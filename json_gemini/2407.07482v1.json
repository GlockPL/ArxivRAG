{"title": "Rigorous Probabilistic Guarantees\nfor Robust Counterfactual Explanations", "authors": ["Luca Marzari", "Francesco Leofante", "Ferdinando Cicalese", "Alessandro Farinelli"], "abstract": "We study the problem of assessing the robustness of\ncounterfactual explanations for deep learning models. We focus on\nplausible model shifts altering model parameters and propose a novel\nframework to reason about the robustness property in this setting. To\nmotivate our solution, we begin by showing for the first time that\ncomputing the robustness of counterfactuals with respect to plausible\nmodel shifts is NP-complete. As this (practically) rules out the exis-\ntence of scalable algorithms for exactly computing robustness, we\npropose a novel probabilistic approach which is able to provide tight\nestimates of robustness with strong guarantees while preserving scal-\nability. Remarkably, and differently from existing solutions targeting\nplausible model shifts, our approach does not impose requirements\non the network to be analyzed, thus enabling robustness analysis on\na wider range of architectures. Experiments on four binary classifica-\ntion datasets indicate that our method improves the state of the art in\ngenerating robust explanations, outperforming existing methods on a\nrange of metrics.", "sections": [{"title": "1 Introduction", "content": "Understanding and interpreting the decisions of black-box deep\nlearning models has become a dominant goal of Explainable AI\n(XAI). Several strategies have been proposed to this end. In this pa-\nper, we focus on counterfactual explanations (CFX) (see [30, 15] for\nrecent surveys on the topic), which aim to demystify the decision-\nmaking of a Deep Neural Network (DNN) by showing how an input\nneeds to be changed to yield a different, typically more desirable,\ndecision. Consider the widely used example of a loan application,\nwhere a mortgage applicant represented by an input x with features\nunemployed status, 25 years of age, and low credit rating applies for\na loan and is rejected by the bank's AI. A CFX for this decision could\nbe a slightly modified input, where increasing credit rating to medium\nwould result in the loan being granted.\nAs CFXs have the potential to influence decisions with strong so-\ncietal implications, their reliability has become the subject of in-\ntensive study (see [13] for a survey). In particular, recent work\nhas highlighted issues related to the robustness of CFXs against\nPlausible Model Shifts (PMS) [32, 12], showing that the validity\nof CFXs is likely to be compromised when bounded perturbations"}, {"title": "2 Related Work", "content": "Various methods for generating CFXs for DNNs have been proposed.\nThe seminal work of [36] framed the task of generating CFXs as a\ngradient-based optimization problem and proposed a loss that pro-\nmotes CFX validity (i.e., the CFX successfully changes the classi-"}, {"title": "3 Background", "content": " (Neural) Classification model. Let \\(X \\subseteq \\mathbb{R}^d\\) denote the input\nspace of a classifier \\(M_{\\theta}: X \\rightarrow [0,1]\\) mapping an input \\(x \\in X\\)\nto an output probability between 0 and 1. We consider classifiers im-\nplemented by feed-forward DNNs parameterized by a (parameter)\nvector \\(\\theta \\in \\Theta \\subseteq \\mathbb{R}^k\\). Given two parameter vectors \\(\\theta, \\theta' \\in \\Theta\\), we re-\nfer to the corresponding classifiers \\(M_{\\theta}\\) and \\(M_{\\theta'}\\) as instantiations of\nthe same parametric classifier \\(M_{\\theta}\\). We assume concrete valuations\nof \\(\\theta\\) are learned from a set of labeled inputs as customary in super-\nvised learning settings [7]. Once \\(\\theta\\) has been learned, the classifier can\nbe used for inference. Without any loss of generality, we focus on bi-\nnary classification tasks, i.e., the classification decision produced by\n\\(M_{\\theta}\\) for an unlabeled input x is 1 if \\(M_{\\theta}(x) \\geq 0.5\\), and 0 otherwise.\nCounterfactual explanations. Existing methods in the literature\ndefine CFXs as follows.\nDefinition 1. Consider an input \\(x \\in X\\) and a classifier \\(M_{\\theta}\\) s.t.\n\\(M_{\\theta}(x) < 0.5\\). Given a distance metric \\(d : X \\times X \\rightarrow \\mathbb{R}^+\\), a (valid)\ncounterfactual explanation is any \\(x'\\) such that:\n\\[\\begin{aligned}\n&\\underset{x' \\in X}{\\arg \\min}\\ d(x, x')\\\\\n&\\text { subject to } M_{\\theta}(x') \\geq 0.5\n\\end{aligned}\\]\nIntuitively, given an input x for which the classifier produces a\nnegative outcome, a counterfactual explanation is a new input x'\nwhich is similar to x, e.g., in terms of some specified distance be-\ntween features values, and for which the classifier predicts a different\noutcome. Common choices for d include the \\(l_1\\) and \\(l_\\infty\\) norms [36],\nwhich will also be used in this work.\nRobustness to model shifts Among several notions of robustness,\nrecent work has placed emphasis on generating CFXs that remain"}, {"title": "4 Checking Robustness is Hard", "content": "In this section, we study the computational complexity of deciding\nwhether a given counterfactual explanation is robust in the presence\nof model shifts. Our aim here is to better understand the computa-\ntional challenges arising from this problem and to use these results\nto guide the development of novel, more efficient certification proce-\ndures. Without loss of generality, we consider PMS to encode the\nproblem. Deciding whether a given CFX x' is robust to a set of\nPMS \\(\\Delta\\) requires to check whether \\(\\Delta\\) contains at least one realiza-\ntion which yields a classification outcome that is different from the\nintended CFX outcome. For instance, in the case of the loan example,\nthis would correspond to a model rejecting the loan (\\(M_{\\theta'}(x') < 0.5\\))\nas opposed to accepting it as intended. This problem can be formu-\nlated as follows.\nDISTINCT-REALIZATIONS PROBLEM (DRP)\nInput: a classifier \\(M_{\\theta}\\), a set of PMS, an input x, and a\nthreshold T.\nOutput: yes \\(\\exists M_{\\theta_1}, M_{\\theta_2} \\in \\Delta\\) s.t\n\\(M_{\\theta_1}(x) < T < M_{\\theta_2}(x)\\).\nTheorem 1. Deciding DRP is NP-complete.\nProof sketch. The inclusion of DRP in NP requires two forward\npropagations of \\(\\theta\\) through two concretizations (i.e., the certificates)\nchecking if \\(T\\) is between the two computed outputs. This is clearly\npolynomial in the size of the classifier. Regarding the hardness, we\ncan show that 3-SAT reduces to DRP. Given a formula \\(\\phi\\) we can\nproduce a DNN classifier \\(M_{\\theta}\\), an input x and a \\(\\delta\\) (maximum shift on\nthe edge weights), defining a \\(\\Delta\\) such that \\(\\phi\\) is satisfiable if and only\nif there exists another \\(M_{\\theta'}\\) which is also a realization of \\(\\Delta\\) such that\n\\(0.5 \\leq M_{\\theta'}(x)\\) and \\(M_{\\theta}(x) < 0.5\\).\nWe build on the reduction of [16], which, given a formula \\(\\phi\\) pro-\nduces a neural network such that satisfying assignments for \\(\\phi\\) are\nencoded into inputs to the network producing a desired output.\nA first observation is that if we start with the neural network \\(M_{\\theta}\\)\nproduced by the reduction of [16], and replace any weight equal to\n1 with the interval \\([1 - 2\\delta, 1]\\) and any weight -1 with the interval\n\\([-1 - 2\\delta, -1]\\), we obtain \\(\\Delta\\) such that for any \\(M_{\\theta'}\\) being a realiza-\ntion of \\(\\Delta\\) and any input x, \\(M_{\\theta'}(x) \\in [(1 - p_1(\\delta))M_{\\theta}(x), (1 +\np_2(\\delta))M_{\\theta}(x)]\\), where \\(p_1, p_2\\) are some fixed polynomials defined by\nthe number of layers in \\(M_{\\theta}\\). Therefore, we can extend the hardness\nof [16] to the case of plausible model shift, where instead of check-\ning for the output to be t, we could use an additional layer to check\nwhether the output is in the interval \\([(1 - p_1(\\delta))t, (1 + p_2(\\delta))t]\\).\nHowever, with respect to the hardness proof of [16], there is a sub-\nstantial difference in our problem since, besides starting from a DNN\nwith each weight in \\([\\theta_i - \\delta, \\theta_i + \\delta]\\), we want to map satisfying as-\nsignments to realizations of the \\(\\Delta\\), rather than inputs of the network.\nIn fact, encoding assignments to choices of the weights turns out to\nrequire significantly more.\nOur reduction is designed so that the assignments to the variables\nof the CNF are encoded to the choices of some specific weights of the\nDNN, henceforth referred to as the network edge main inputs. These\nare the weights that determine the outputs of a gadget that we call the\ngenerating gadget. A generating gadget has a fixed input (represent-\ning the CFX), uses only intervals of width \\(2 \\cdot \\delta\\), and produces a value\nin [0, 1] where the Boolean false is represented by a value close to\n0 and the Boolean true is represented by a value close to 1.\nThe output of the generating gadget (one per each variable of the\nformula ) is sent both to the network simulating the CNF formula\n(as in [16]) and to a further gadget (the discretizer-gadget) that con-\ntrols whether the output of the generating gadget is a discrete value\nin {0, 1}.\nThe output of the network simulating the formula is then combined\nwith the output of the discretizer-gadgets in such a way that the final\noutput is < 0.5 whenever, either there is one of the outputs of the\ngenerating-gadgets (determined by the choice of the network edge\nmain input) which is not in {0, 1} or the output of the subnetwork\nsimulating the formula implies that the network edge main input en-\ncode and assignment not satisfying some clause of \\(\\phi\\). The complete\nproof can be found in the supplementary material.\nFrom Theorem 1, it follows that deciding whether a CFX x' is not\nrobust to a set of PMS \\(\\Delta\\) is NP-complete. This results lead to the\nfollowing corollary.\nTheorem 2. Given a model \\(M_{\\theta}\\) and a set of plausible model shifts\n\\(\\Delta\\), computing the number of model shifts in \\(\\Delta\\) for which a given CFX\nx' is \\(\\Delta\\)-robust is NP-hard.\nIndeed, deciding non-\\(\\Delta\\)-robustness and \\(\\Delta\\)-robustness are equiv-\nalent under Turing reductions. Therefore, the counting problem for\n\\(\\Delta\\)-robustness is at least as hard as the decision problem for non-\\(\\Delta\\)-\nrobustness. This hardness result motivates an approximate approach\nto estimate the robustness of a counterfactual under a set of PMS \\(\\Delta\\)."}, {"title": "5 Probabilistic Guarantees for Existing Notions of\nModel Shifts", "content": "As we have established in the previous section, exact methods for\ncomputing robustness under model shifts are bound to lack scala-\nbility. This motivates the design of probabilistic approaches to solve\nthe problem. Previous work by Hamman et al. [10] presented an ap-\nproach to obtain counterfactual explanations that are probabilistically\nrobust under NOMS. A natural question that arises then is whether\nguarantees obtained for NOMS also transfer to the PMS setting. As\nwe show for the first time below, this is not the case in general.\nLemma 3. Naturally-occurring model shifts may not be Plausible,\nand vice-versa.\nProof. Consider the DNN \\(M_{\\theta}\\) depicted in Fig. 1 (a) with two input\nnodes, one hidden layer with two ReLU nodes\u00b2 and one single out-\nput. The parameters \\(\\theta = [w_1,..., w_6]\\) are the weights on the edges\nlisted top-bottom and left-right.\nPropagating an input vector \\(x = [x_1,x_2]^T\\) through \\(M_{\\theta}\\), we obtain\n\\(M_{\\theta}(x) = y = w_5 \\max{0, w_1\\cdot x_1 + x_2 \\cdot w_3} + w_6 \\cdot \\max{0, w_2 \\cdot x_1 +\nx_2 \\cdot w_4}\\). Now assume an input vector \\(x = [0.9, 0.9]^T\\) and weights\n\\(w_1 = 1, w_2 = 0, w_3 = 0, w_4 = 0.6, w_5 = 1, w_6 = -1\\). The corre-\nsponding output generated by the DNN is \\(M_{\\theta}(x) = 0.46\\). A coun-\nterfactual for x could be given as a new input vector \\(x' = [1,0.8]^T\\),\n\u00b2 In this proof, we consider a DNN with only ReLU activation functions.\nHowever, we notice that it is possible to have a similar counterexample\neven with other activations, e.g. Tanh, Sigmoid."}, {"title": "6 Robustness under PMS with Probabilistic\nGuarantees", "content": "Jiang et al. [12] proposed to use INNs to enable a compact represen-\ntation of a superset of the models that can be obtained by a perturba-\ntion of the starting model under a set \\(\\Delta\\). By exploiting an exact reach-\nable set computation method, e.g., based on MILP [31], the authors\ncould determine whether or not a CFX is robust under the chosen \\(\\Delta\\)\nvia a single forward propagation of the CFX. However, in view of\nthe NP-hardness of the problem discussed in the \u00a7 4 and the typical\nnon-linear nature of the classifiers, it presents some computational\nlimitations.\nIn general, interval neural networks map inputs to intervals repre-\nsenting an over-approximation of all possible outcomes that can be\nproduced by any shifted model \\(M_{\\theta'}\\), obtained under \\(\\Delta\\). Given this\nproperty, if the output reachable set is completely disjoint from the\ndecision threshold 0.5, then one can assert - in a sound and com-\nplete fashion - whether or not a given CFX is robust (Fig. 2 (a,c)).\nOn the other hand, if we run into a situation such as the one de-\npicted in Fig. 2 (b), one cannot assert robustness with certainty. In\nthis scenario, Jiang et al. [12] propose to classify the CFX as not\nrobust, which preserves the soundness of their result. Nonetheless,\nthis might lead to discarding a CFX even when the actual density of\n(equivalently, the probability that after retraining, we incur in) plau-\nsible model shifts for which the CFX is not robust is extremely low.\nAs we will show in \u00a7 7, this worst-case notion of robustness affects\nthe CFXs generated by [12], which may end up being unnecessarily\nexpensive (in terms of proximity) and having low plausibility. Ad-\nditionally, computing the exact output reachable set of an interval\nabstraction may be costly (e.g., MILP is known to be NP-hard). This\nis expected: Theorems 1 and 2 show that there is no polynomial time\nalgorithm able to return an exact estimate of the fraction of plausible\nshifts for which the CFX is robust (hence a fortiori deciding whether\nit is \\(\\Delta\\)-robust), unless P=NP. In the following, we propose a novel\ncertification approach that aims to alleviate this problem."}, {"title": "6.1 A Provable Probabilistic Approach", "content": "One possible idea to avoid exact reachable set computation to de-\ntermine the robustness of a CFX under PMS is to use naive interval\npropagation. Given an input CFX, we propagate this input through\nthe network, keeping track of all the possible activation values that\ncan be obtained under \\(\\Delta\\) until the output layer is reached. However,\nthe non-linear and non-convex nature of DNNs may result in a sig-\nnificant overestimation of the actual reachable set, thus resulting in a\nspurious decision of non-robustness. In such cases, a CFX may end\nup being labeled as non-robust even though the CFX is actually ro-\nbust. Additionally, even with exact methods, a CFX may be discarded\neven though the fraction of plausible model shifts in \\(\\Delta\\) for which the\nCFX is not robust is negligible.\nTo avoid these problems, we propose an approximate certification\napproach based on Monte-Carlo sampling that draws sample real-\nizations directly from \\(\\Delta\\) to obtain an underestimation of the space\nof possible classifications under PMS. The idea of using a sample-\nbased approach stems from the fact that the \\(\\Delta\\) set, representing all\nthe plausible model shifts, abstracts an infinite number of models to\ntest. As testing this infinite number of models may be impossible\nin practice, efficient sampling-based solutions hold great promise.\nIn detail, given a CFX x' we can compute an underestimation of"}, {"title": "6.2 The APAS Algorithm", "content": "Using the result of Lemma 4, we now present our approximation\nmethod APAS to generate probabilistic robustness guarantees. The\nprocedure, shown in Algorithm 1, receives as input a model \\(M_{\\theta}\\), a\nCFX x' for which robustness guarantees are sought, and the two con-\nfidence parameters a, R. The algorithm then searches for the largest\n\\(\\delta_{max}\\) such that the CFX x' is robust for at least a fraction R of the\nplausible model shifts up to \\(\\delta_{max}\\) with probability a.\nThe algorithm starts by computing the size n of a sample\nof realizations that is sufficient to guarantee the condition in\nLemma 4 (line 3). APAS then initializes a small \\(\\delta_{init}\\) and checks\nif x' is at least robust to a small model shift. To this end, it employs\nrealizations \\((M_{\\theta}, x', \\delta, n)\\) which samples n realizations, per-\nturbating each model parameter by at most a factor \\(\\delta\\) and checks if for\neach of these realization \\(M_{\\theta_i}(x') \\geq 0.5\\), thus computing a robust-\nness rate. If not all these realizations result in a robust outcome, thus"}, {"title": "7 Experimental Analysis", "content": "Section 6 laid the theoretical foundations of a novel sampling-based\nmethod that allows to obtain provable probabilistic guarantees on the"}, {"title": "7.1 APAS in Action", "content": "This experiment is designed to demonstrate how the three main pa-\nrameters of APAS, i.e. n, a and R, can be used to obtain probabilis-\ntic guarantees of robustness. To this end, we focus on the synthetic\nexample depicted in Fig. 3. Weights for the original network \\(M_{\\theta}\\), as\nwell as the input used for testing robustness, are generated randomly.\nConsidering a random input x = -2.57, we use APAS to estimate\na \\(\\delta_{max}\\) for which we seek the guarantee that for at least R = 90%\nof the plausible model shifts induced by such \\(\\delta_{max}\\) the CFX x'\nis robust. Following Proposition 5, we set a confidence level a >\n\\(1 - 10^{-40}\\) (i.e., with certainty, in practice), which yields n = 100k\nrealizations. For this setting, APAS identifies a \\(\\delta_{max}\\) = 0.115.\nTo validate this result, we define a procedure to exactly enumerate\nthe models within \\(\\Delta_{\\delta_{max}}\\) for which the robustness property does not\nhold. The interval abstraction proposed by [12] can be used to exactly\nenumerate the portion of the model shifts from \\(\\Delta\\) for which a CFX x'\nis not robust. In fact, it is possible to build an interval neural network\nusing the \\(\\delta_{max}\\) value identified by APAS, setting each weight \\(w_i\\) in\n\\(\\theta\\) to \\([w_i - \\delta_{max}, w_i + \\delta_{max}]\\). Then, recursively splitting each inter-\nval weight of the network in half allows to identify portions of \\(\\Delta\\) that\nare not robust. Employing this exact enumeration strategy (see Algo-\nrithm 3 in the Appendix for a complete formalization), after s = 7\nsplits, we obtain that for ~ 92% of sub-interval networks, the CFX\nis robust. The remaining 8% produced an unknown answer (i.e., the\nsituation depicted in Fig. 2(b)) that would require further splits, cor-\nresponding to only ten nodes to explore in the next iteration. In the"}, {"title": "7.2 Worst-case vs Average-case Guarantees", "content": "This set of experiments aims to compare the probabilistic guarantees\noffered by APAS with the worst-case guarantees offered by [12].\nWhat we aim to show here is that adopting an average-case certifi-\ncation perspective may be more practical in some circumstances, as\nworst-case guarantees may be unnecessarily conservative. Our ap-\nproach aims to obtain a \\(\\delta_{max}\\) for which the CFX is robust with con-\nfidence a for at least a fraction R of model shifts in \\(\\Delta\\). This is in\nstark contrast with the worst-case reasoning of [12], where even a\nsingle realization of \\(\\Delta\\) for which the CFX is not robust results in the\ncorresponding \\(\\delta\\) being discarded.\nTo show why such strict guarantees may not be needed, we use an\nanalogous experimental setup and the training process of [12], which\nconsiders four datasets: Diabetes (continuous) [29], Credit (hetero-\ngeneous) [4], no2 (continuous) [34] and Small Business Administra-\ntion (SBA) (continuous features) [20]. In particular, the Credit and\nthe SBA datasets are known to contain distribution shifts [32] and\nare typically used to assess robustness under model changes. For the\ntraining procedure of the classifier, we randomly shuffle each dataset\nand split it into two halves, denoted \\(D_1\\) and \\(D_2\\). First, we use \\(D_1\\)\nto train a base neural network; then we use both \\(D_1\\) and \\(D_2\\) to train\na shifted model. We then generate 50 robust CFXs for the base net-\nwork using the MILP-R and the same 8 values as in [12] for a fair\ncomparison. Specifically, we use \\(\\delta = 0.11\\) for Diabetes, \\(\\delta = 0.02\\)\nfor no2, \\(\\delta = 0.11\\) for SBA and \\(\\delta = 0.05\\) for Credit. Subsequently,\nwe evaluate the resulting CFXs by looking at two metrics: (i) VM1,\nthe percentage of CFXs that are valid on the base neural network and\n(ii) VM2, the percentage of CFXs that remain valid for the shifted\nneural network trained using both \\(D_1\\) and \\(D_2\\).\nAs previously observed by Jiang et al. [12], the training procedure\nused to generate shifted models may result in changes that exceed the\n\\(\\delta\\) used to generate provably robust CFXs. Indeed, after inspecting the\nnetworks obtained, we noted that the maximum empirical difference\nobserved after retraining (denoted as \\(\\delta_e\\)) is well above the \\(\\delta\\) values\nused during CFX generation. In particular, we recorded \\(\\delta_e = 0.27\\)\nfor Diabetes, \\(\\delta_e = 0.07\\) for no2, \\(\\delta_e = 0.25\\) for SBA and \\(\\delta_e = 1.28\\)\nfor Credit. Given the magnitude of these shifts, the robustness of the\nCFXs generated by MILP-R cannot be guaranteed in practice. How-\never, the results show a rather intriguing picture: the VM2 metric\nappears to be unaffected by retraining, and all CFXs remain valid on\nthe respective final models."}, {"title": "7.3 Generating Robust CFXs using AP\\(\\Delta\\)S", "content": "The results discussed in the previous section have important im-\nplications on algorithms for the generation of robust CFXs. Recent\nworks [5, 12, 10] have proposed iterative procedures that generate\nprovably robust CFXs by alternating two phases. First, a CFX is gen-\nerated solving (variations of) Definition 1; then, a robustness certifi-\ncation procedure is invoked on the CFX. If the CFX is robust, then\nit is returned to the user; otherwise, the search continues, allowing\nfor CFXs of increasing distance to be found. Clearly, the certification\nstep has the potential to affect the CFXs computed in several ways. A\nrobustness test that is too conservative may discard potentially good\nexplanations and keep relaxing the distance constraint until the CFX\nis deemed robust. Ultimately, this may result in CFXs that exhibit\npoor proximity and plausibility.\nTo test this hypothesis, we adapt the CFX generation algorithm\nof [12] and replace their \\(\\Delta\\)-robustness test with the one performed"}, {"title": "8 Conclusions", "content": "We studied the problem of robustness of CFXs with respect to plau-\nsible model shifts. We proved for the first time that certifying the\nrobustness of CFX with respect to this notion is an NP-complete\nproblem. This results motivates the quest of new scalable algorithm\nto certify robustness under PMS. We then compared with existing\nmethods to generate robust CFXs with probabilistic guarantees and\nshowed that these approaches may not be directly applicable to the\nPMS setting. We then introduced APAS, a novel scalable approach\nfor probabilistic robustness certification, and used it to generate ro-\nbust CFXs under model shifts. Our results demonstrate the advan-\ntages of our approach, outperforming SOTA methods on a range of\nmetrics. This paper opens several avenues for future work. Firstly,\nwhile our experiments only focused on DNNs, there seems to be no\nreason why APAS could not be applied to other parameterized mod-\nels for which the notion of plausible model changes holds. Secondly,\nit would be interesting to investigate improvements to the approxi-\nmation scheme presented here to further tighten the robustness guar-\nantees our framework can offer. Finally, it would be interesting to\napply APAS to other notions of robustness studied in the literature,\nsuch as robustness to noisy execution."}, {"title": "A Supplementary Material", "content": "A.1 The proof of Lemma 3\nLemma 3. Naturally-occurring model shifts may not be plausible,\nand vice-versa.\nProof. We showed already that there are examples of plausible\nmodel shifts that cannot be characterized as naturally occurring\nmodel changes. Here we complete the argument.\nAlthough the existence of naturally occurring model shifts not be-\ning plausible is implicit in the definition, for the sake of complete-\nness, we provide an example network.\nConsider a DNN having a single input value x and a single param-\neter \\(\\theta\\) and computing the function\n\\(M_{\\theta}(x) = ReLU(0.5 - ReLU(x - \\theta))\\)\nFix a data set X and let \\(\\max_{x \\in X} x\\). Let us consider the set of\nmodel shifts \\(\\Sigma = {S_{\\tau} | \\tau \\in \\mathbb{R}^+}\\) defined by \\(S_{\\tau}(M_{\\theta}) = M_{\\theta+\\tau}\\).\nClearly for any \\(\\tau \\geq 0\\), we have\n\\(M_{\\tau+\\theta}(x) = M_{\\theta}(x) = 0.5\\),\nfor any \\(x \\in X\\). This trivially implies that \\(\\Sigma\\) is a set of naturally\noccurring model shifts (all shifts considered have exactly the same\nvalue in all points in X).\nThe claim now follows by observing that there is no finite \\(\\delta\\) such\nthat the corresponding set of plausible model shifts \\(\\Delta = {S |\\nd(\\theta, S(M_{\\theta})) < \\delta}\\) contains \\(\\Sigma\\).\nA.2 Plausible model shifts vs naturally-occurring\nmodel shifts: additional results\nWe considered three binary classification datasets commonly used in\n\u03a7\u0391\u0399:\n\u2022 the credit dataset [4], which is used to predict the credit risk of a\nperson (good or bad) based on a set of attribute describing their\ncredit history;\n\u2022 the spambase dataset [11] is used to predict whether an email is\nto be considered spam or not based on selected attributes of the\nemail;\n\u2022 the online news popularity dataset [6], referred to as news in the\nfollowing, is used to predict the popularity of online articles.\nWe trained a neural network classifier with two hidden layers (20\nand 10 neurons, respectively) for each dataset and used a Nearest-\nNeighbor Counterfactual Explainer [8] to generate counterfactual ex-\nplanations for 10 different inputs. After generating a counterfactual,\nwe produce n different perturbations \\(M_{\\theta'}\\) of the original neural net-\nwork \\(M_{\\theta}\\) for \\(n \\in {1000, 10000}\\) under plausible model shift with\n\\(\\Delta \\in {0.05, 0.1, 0.2, 0.3}\\). We then considered two measures:\n\u2022 average difference in output between \\(M_{\\theta}\\) and \\(M_{\\theta'}\\), for each of\nthe n model \\(M_{\\theta'}\\) and across all CFXs;\n\u2022 for each counterfactual, we perform a one-sided t-test to check\nwhether the average prediction generated by n models \\(M_{\\theta'}\\)\nequals the original prediction of \\(M_{\\theta}\\). We report the percentage\nof CFXs for which the null hypothesis was rejected (p-value used\n0.05).\nTable 2 reports our results. We observe that the requirement that\nthe expected output of perturbed models remains equal to the orig-\ninal prediction is often violated. This complements the result of\nLemma 3, confirming that the two notions of model changes are or-\nthogonal. In the following we focus on certification approaches for\nrobustness under plausible model shifts.\nA.3 The proof of Theorem 1\nTheorem 1. DRPIA is NP-complete.\nProof. We can show that 3-SAT reduces to DRPIA. In particular,\ngiven a formula \\(\\phi\\) we can produce an DNN \\(M_{\\theta}\\), an input x and a\nshifts on the edges, defining a INN I such that \\(\\phi\\) is satisfiable if and\nonly if there exists another DNN \\(M_{\\theta'}\\) which is also a realization of I\nsuch that \\(0.5 \\leq M_{\\theta'}(x)\\) and \\(M_{\\theta}(x) < 0.5\\).\nThe INN I is an interval abstraction under a set of plausible shifts\n\\(\\Delta\\). Precisely, this means that the reduction produces an INN where\nall intervals on the edges are of the same width 2\\(\\delta\\)"}, {"title": "B Exact enumeration method", "content": "As sketched in Section 7.1, a possible approach to exactly enumerate\nthe portion of an interval abstraction for which the CFX x' is not\nrobust is to recursively split in half each INN's interval into two equal\nparts until each INN resulting from the split allows to determine that\nx' is entirely robust or non-robust. Algorithm 3 below formalizes\nthese ideas.\nIn lines 3-5 we initialize data structures needed to keep track on\nthe portion of INNs that are robust, non-robust and yet to be de-\ncided. We iteratively compute the output reachable set of the INN\nand check whether the robustness condition is met (lines 9-11) or\nviolated (lines 12-14) and remove the corresponding INN from the\nstack of undecided INNs. Otherwise, we choose an interval in I and\nsplit it into half to create two new INNs, which we add to the stack.\nWe repeat this procedure until the stack of undecided INNs is empty\nor an threshold e on the precision of this procedure is met. Once the\nloop terminates, we return the set of INNs for which the input CFX\nis robust."}, {"title": "C Computing a provable \u0410\u0442\u0430\u0445", "content": "Algorithm 1 can be modified to compute the maximum admissible\nshift under worst-case guarantees. In a nutshell, this simply requires\nreplacing the robustness test performed by APAS with the MILP-\nbased certification procedure. For completeness we report the result-\ning procedure in Algorithm 4."}]}