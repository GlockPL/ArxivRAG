{"title": "SoK: Towards Security and Safety of Edge AI", "authors": ["Tatjana Wingarz", "Anne Lauscher", "Janick Edinger", "Dominik Kaaser", "Stefan Schulte", "Mathias Fischer"], "abstract": "Abstract-Advanced AI applications have become increasingly\navailable to a broad audience, e.g., as centrally managed large\nlanguage models (LLMs). Such centralization is both a risk and\na performance bottleneck Edge AI promises to be a solution\nto these problems. However, its decentralized approach raises\nadditional challenges regarding security and safety. In this paper,\nwe argue that both of these aspects are critical for Edge AI, and\neven more so, their integration. Concretely, we survey security\nand safety threats, summarize existing countermeasures, and\ncollect open challenges as a call for more research in this area.", "sections": [{"title": "I. INTRODUCTION", "content": "Artificial Intelligence (AI) and machine learning (ML) are\ngaining huge interest from industry and society, with appli-\ncations deployed in various areas, from autonomous driving\nto omnipresent speech recognition. Despite their impact, the\ncriticism towards AI and ML is multifaceted [1].\nFrom a societal perspective, AI introduces the tendency to\nform monopolies as it requires large amounts of data. ML\nexpertise thus accumulates at big companies like OpenAI,\nGoogle, or Meta as they have enough resources to collect data\nand train large AI models. Using these technologies typically\nrequires sharing data with them, resulting in data privacy\nconcerns and users losing data sovereignty over potentially\nsensitive information. Furthermore, current AI suffers from\npoor explainability and bias in training data, requiring addi-\ntional safeguards that are challenging to implement [2]. With\nthe big companies as gatekeepers, such measures might even\nlead to censorship.\nFrom a technical perspective, uploading data to a centralized\nentity is not always possible due to bandwidth limitations\nand due to violations of timing constraints when (near-)real-\ntime inference is required. Further, centralized AI constitutes\na performance bottleneck and a single point of failure.\nMoving AI to the network's edge can help to mitigate\nthese problems. Edge AI refers to deploying AI algorithms\nand models directly on edge devices like smartphones and\nIoT devices. By performing computations locally, Edge AI\nreduces latency, preserves bandwidth, and enhances privacy.\nThis approach is beneficial for applications requiring real-time\ndecision-making or operating in environments with limited\nor intermittent connectivity to the Internet [3]. At the same\ntime, Edge AI introduces new challenges: due to its distributed\nnature, control over AI-based algorithms diminishes, and the\npotential for attacks increases. The decentralization implies\nthat AI models are deployed across many devices, each\npotentially vulnerable. As a result, security measures must be\nrobustly implemented at each edge node to mitigate the risk\nof unauthorized access, tampering, or malicious exploitation,\nrequiring inexpensive and scalable safeguards against various\nattacks.\nTo the best of our knowledge, existing surveys cover topics\nof general challenges of Edge AI [4]-[7], focus on general\nAI security [8]\u2013[10] or safety [11,12] but do not consider\nthe intersection of those areas in the context of Edge AI.\nThere are only two exceptions. First, the authors of [13] cover\nsecurity/privacy aspects in the context of Edge AI, but are\nfocused on the subdomain of digital marketing environments\nand do not consider a broader application. Second, the authors\nof [14] outline some security threats to Edge AI, but their work\nis limited in scope and does not cover any safety implications.\nFinally, the safety definition used by existing surveys on Al\nsafety [11, 12] is limited to dependability and that completely\nomits the social safety implications of attacks on AI, e.g., as\nwe see them in the context of LLMs.\nTo address the existing gaps in understanding the complex-\nities of Edge AI, this paper makes several key contributions.\nFirst, we provide a comprehensive survey of the challenges\nrelated to the security and safety of Edge AI, examining\nboth existing threats and their relevant countermeasures. We\ninterpret safety here wider than existing work and also look at\nsocial implications. Second, we propose a detailed model of\nEdge AI that serves as a foundation for understanding Edge\nAI challenges. Finally, we conclude the paper by identifying a\nseries of open research challenges and present a call to action\nfor the research community to advance solutions in this critical\narea.\nThe rest of this paper is structured as follows: Section II\ndescribes our Edge AI model and the resulting requirements.\nSections III and IV present the results of our survey on\nsecurity/privacy and safety issues of Edge AI and existing\ncountermeasures. Section V summarizes the open issues and\nresearch gaps that we have identified. Section VI concludes\nthe paper."}, {"title": "II. EDGE AI MODEL AND REQUIREMENTS", "content": "In this section we first present our model for Edge AI in\nSection II-A. Then in Section II-B we give an overview of\nrequirements for Edge AI.\nA. Edge AI Model\nThe concept of edge computing lacks a singular, rigid\ndefinition. Edge devices comprise a wide spectrum, including"}, {"title": "B. Requirements for Edge AI", "content": "Security and safety of Edge AI are the primary requirements\nthat this paper focuses on. However, there is a number of\nadditional requirements that can be in conflict with each other\nand also with the secure and safe usage of Edge AI. These\nrequirements are listed below:\n\u2022 Efficiency: An Edge AI system must provide accurate in-\nference, while effectively using computational resources.\nThis includes the time, energy, and computing power to\ntrain a model as well as the speed of model inference.\n\u2022 Scalability: Edge AI must scale proportionally with the\nnumber of users, service providers, and operators [19].\n\u2022 Self-Adaptivity: Edge AI systems should be able to\nmodify its operations in response to context changes,\ninternal dynamics, and changes in user behavior.\n\u2022 Safety can be defined as \u201cthe state of being protected\nfrom danger or harm", "a negative\nevent or negative social development entailing value\ndamage or loss to people\\\". In computer science the\nterm is quite often associated with fault tolerance and\ndependability. In ML literature safety also quite often\nrefers to the dependability of algorithms in the presence\nof failures [12, 20], which falls short with regard to social\naspects and actual impacts on our societies. For this\nreason, we interpret safety more broadly in the sense of\nits original definition.\n\u2022 Security": "Edge AI should integrate security already from\nthe design phase. This requires to meet classical security\ngoals like confidentiality, integrity, and availability.\n\u2022 Privacy: As Edge AI might process sensitive user data,\nuser privacy is another major concern. Sensitive user data\nhas to be protected as well as user identities."}, {"title": "III. SECURITY AND PRIVACY OF EDGE AI", "content": "By moving computation closer to the data source when\nemploying Edge AI and related distributed AI principles,\nsystems are exposed to a broader attack surface, as attacks can\nnow also be executed on local or intermediate models. Further,\nwhile FL ensures that the raw data used for training does not\nleave the client, it does not provide any guarantee on privacy\nlevels, and the recurring model updates can leak sensitive\ninformation about the training data [21]\u2013[24]. Additionally,\nthe distributed nature of computation makes FL inherently\nvulnerable to Sybil attacks [25,26]. We give an overview\nof current developments in both Edge AI/ FL threats and\nproposed countermeasures.\nA. Threats to Edge Al\nAttacks against Edge AI and FL can be divided into attacks\noccurring during the training and during the inference phase."}, {"title": "1) Attacks during the training phase:", "content": "During the training\nphase attackers can poison the training data and the models\nand can also install backdoors.\nData poisoning: As the aggregator has no insight into\nthe training data used per client, adversaries can perform\ndata poisoning [27]. For that, they utilize malicious nodes to\ninject new or modify existing training data to achieve their\nmalicious objective. An untargeted poisoning attack, or also\ncalled random poisoning attack, aims to diminish the global\nmodel performance and thus attacks the model availability. In\ncontrast, targeted poisoning attacks are performed on fewer\nclasses, making the attack stealthier and only causing the recall\nfor the target class(es) to be affected drastically, with the\noverall model performance remaining otherwise stable, thus\nfocusing on model integrity. For a successful poisoning attack,\nthe adversary only needs to a subset of the participating clients.\nThe attacker can then either manipulate existing training data\non the compromised client or leverage synthetically generated\ndata points by either mimicking benign participants' observed\nmodel updates [28] or independent of the knowledge of any\nsuch update [29]. As models can recover independently from\nsuch an attack and converge to an optimal solution after data\npoisoning stops, adversarial clients must be active and present\nduring the entire or at least the later stages of training. [27]\nModel poisoning: In model poisoning [26, 30, 31] attacking\nthe learning process itself is the goal, not just the training\ndata. The attacker controls one or multiple clients completely,\ni.e., has access to the training data, can manipulate and adapt\nthe local training, and can modify training results (gradient\nor weight updates) before sending them to the aggregator.\nSuch attacks can be targeted [30,31] or untargeted [26].\nTo poison a model, an adversary typically first trains the\nlocal model both on benign and malicious training data.\nAfterwards, he optimizes the model update to increase its\nimpact by either boosting the entire update [31] or only the\npart of the update that belongs to their malicious objective\n[30] to strengthen it against being averaged out during the\naggregation. Further, untargeted attacks with fake clients that\nhave no training data are possible by optimizing towards a\nlocal random model of the same structure [32]. Notably, model\npoisoning attacks can even be performed when Byzantine-\nrobust FL is employed [26,30] and have a huge impact on\nmodel training, as demonstrated by [31]. The authors show that\neven a single compromised participant can poison a model in\na single round of training. However, depending on the chosen\nobjective and the target model, multiple rounds of attacks\nor many compromised clients might be necessary. Similar to\nstandard data poisoning, the model will slowly recover from\nthe attack and converge to the main objective after an attack.\nOverall, targeted model poisoning attacks can have a signifi-\ncantly larger impact on the model performance than untargeted"}, {"title": "2) Inference Attacks:", "content": "Inference attacks can help to gain the\nattackers insights into training data and origin of a model.\nThe attacker can i) attempt to infer general properties about\ntraining data (property inference), ii) can deduce if a data point\nwas in the training data (membership inference), iii) can try\nto guess the source of a training data point (source inference),\niv) can (partially) reconstruct the training data (reconstruction\nattack). In addition, Edge Al is also vulnerable to classical\ninference attacks that attack the final model, not the recurring\nmodel updates. The attackers can use i) adversarial examples\nhere, ii) invert the model (model inversion), iii) or steal the\nmodel (model extraction).\nProperty inference: By performing a property inference\nattack [23, 36], an adversary tries to obtain knowledge about\nthe general properties of the data of participants used to train\nthe global model. However, during collaborative learning, an\nattacker not only has access to the final model but also to\nthe intermediate, recurring model updates. the authors of [23]\nfound that running property inference on these intermediate\nupdates can even leak properties of the participant's training\ndata that are independent of the global properties that the final\nmodel would exhibit. Further, an active adversary can trick the\nmodel into learning better data separation, resulting in more\ninformation being leaked.\nMembership inference: Besides learning general properties,\nin highly sensitive scenarios, knowing whether a specific\ndata point was part of the training data can already violate\nprivacy. Membership inference (MI) [37] utilizes the idea that\nML models typically display slightly different behavior when\nevaluating training data than before-unseen inputs as they were\ntrained to converge to them. To determine membership, an\nattacker does not need white-box access to the model or con-\nfidence predictions (works in label-only) [38]. However, the\nattack's effectiveness can be increased in white-box scenarios\n[39, 40]. When using FL, MI attacks cannot only be performed\non the final model but also on the model updates [23,40].\nFurther, FL is even more suitable for MI attacks, as attackers\ncan observe recurring parameters from model updates over the\nsame underlying dataset. FL is also vulnerable to active MI\nattacks in which an attacker can craft malicious updates that\nforce the FL model to leak targeted information about the local\ndata.\nSource inference: [41] introduced source inference attacks\nas a natural extension to MI to gain non-trivial information\nabout the source of a training sample. It leverages the predic-\ntion loss of local models, exploiting the fact that the client\nwith the smallest loss regarding a specific training record,\ne.g., determined via MI, should be that data point's owner.\nIt can be performed non-intrusively without violating the FL\nprotocol and by either the global aggregator or a malicious\nclient, although it becomes impractical in the latter case.\nReconstruction attack: An attacker with access to the shared\ngradients cannot only invert some general properties over the\nmodel's training dataset but can completely reverse/reconstruct\nit using information leaked during the exchange of gradients\n[24, 42] by performing a reconstruction attack. By trying\nto iteratively match participant's observed gradient updates\nvia altering dummy inputs, they converge to those gradi-\nents, leading to inputs close to the original training data\nbelonging to such an observed gradient. While results can\ncontain artifacts, in some cases, even a pixel-wise (image\nrecognition) or token-wise (language model) reconstruction is\npossible. In centralized systems, such attacks can be performed\nat the aggregator, while an attacker can observe gradients\nfrom neighbors directly in a decentralized setting without a\nfixed aggregation instance. Further, an attacker can exploit the\nleaked information to train a generative adversarial network\nthat can generate samples from the same distribution as the\noriginal training data [43].\nUsing adversarial examples: They [44, 45] refer to specif-\nically crafted inputs during the inference phase that force a\nmisclassification. No backdoor is injected into the model be-\n      \"beforehand, an attacker rather exploits the model's generalization\nproperties, e.g., by adding noise to images, to find \"pockets\"\nin which the model behaves unintendedly.\nModel inversion attacks: In these attacks [22, 46], the\nadversary attempts to invert an existing model to its original\ntraining data. However, such attacks do not directly recover the\ntraining data but lead to generalized/averaged results or inputs\nclose to the original data from which information might leak.\nModel extraction: Here the attacker do not attack the\ntraining data but the ML model as a whole [47]. The goal is\nnot to infer information about the training data and its sources,\nbut to steal the model. This circumvents costly training and\nthe attackers steal embedded intellectual property/trade secrets\nor circumvent copyright boundaries.\""}, {"title": "B. Countermeasures", "content": "Countering attacks on Edge AI is challenging and can en-\ncompass a range of different measures. In standard AI settings\ncryptographic solutions like secure multiparty computation\nand homomorphic encryption, have proven to be effective,\neven though expensive. Furthermore, the application of differ-\nential can help to decrease the impacts of attacks on models.\nAlso the application of trusted execution environments and"}, {"title": "Secure Multiparty Computation (SMPC):", "content": "[48,49] com-\nprises approaches that enable multiple participants to jointly\ncompute a function without learning anything other than\ntheir individual inputs and the calculated output. The most\ncommonly used principles are garbled circuits [50] and secure\naggregation [51] protocols. SMPC is typically used during\nthe training phase to aggregate local model updates without\nrevealing them to an aggregator, but can also be applied to\nperform the inference jointly [52]. However, many SMPC\nsolutions become more complex when more participants join\nthe computation or when the complexity of the joint function\nincreases. The result can be either a significant computation\nor communication overhead. Thus, a careful consideration is\nneeded when choosing SMPC components to remain efficient,\nespecially in potentially resource-constrained edge environ-\nment."}, {"title": "Homomorphic Encryption (HE):", "content": "[53,54] is a group of en-\ncryption schemes that can perform computations on encrypted\ndata by replacing plaintext calculations with their HE equiv-\nalent. Depending on the encryption scheme, non-conforming\nfunctions must be performed with SMPC or replaced with HE-\ncompliant approximations. In the context of ML, HE can be\nused in the training [21,55,56] or inference [57]-[59] phase.\nIn FL, HE can be further utilized to aggregate model updates\non encrypted data [60,61]. As computations are performed\non encrypted data, HE can help prevent attacks that analyze\nthe gradients. However, HE is inherently malleable, meaning\nthat, by itself, it only protects in an honest-but-curious attack\nsetting. Further, during training, HE primarily protects against\na compromised aggregator, as the clients possess access to\nthe private keys and can perform decryption when needed.\nAdditionally, HE comes with a significant overhead compared\nto standard computations. Thus, a careful consideration which\nfunctions should be evaluated homomorphically is needed to\nnot exhaust computation powers."}, {"title": "Differential Privacy:", "content": "The goal of Differential Privacy (DP)\n[62]\u2013[66] is to minimize the impact and therefore the iden-\ntifiability of individual data points when viewing the dataset\nas a whole by adding noise. The idea is that an attacker that\nis looking at the output of an algorithm, e.g., model outputs,\nshould not be able to identify which output belongs to the\ndataset in which a specific individual was present and which\nbelongs to the one where it was not. DP can be applied\nglobally (on algorithm outputs), locally (on input data), or\nalgorithmically (on intermediate results). While applying DP\nis comparatively easy and only adds moderate overhead, is\nnot suitable for all data types. Also the application of DP\ncan degrade the overall accuracy/utility of ML approaches,\nespecially when too much noise has to be applied to hinder\ncertain attacks [31,39,67,68]. DP can be utilized against\nattacks that try to retrieve information about the training data,\ne.g., against membership inference [39, 67]\u2013[69] or to possibly\nhinder poisoning attacks [31,70], as the underlying algorithms\ndepend on gaining some information about the training data."}, {"title": "Anomaly Detection:", "content": "Defenses against data and model poi-\nsoning, byzantine, and Sybil attacks typically require adaptions\nto the traditional FL procedure. Defenses can be performed\nat the aggregator [25,27], e.g., by inspecting the gradients\nand trying to perform anomaly detection or find closely\nrelated gradients, or at the clients [71,72], e.g., by employing\naccuracy detection and voting. Early works propose to adapt\nthe aggregation method to make FL robust against byzantine\nattackers. However, it was shown that these defenses are not\nrobust against most poisoning attacks [26,30] and can even\nboost the effectiveness of model poisoning attempts [31].\nMany approaches rely on access to the model updates, but as\nthose are vulnerable to inference attacks, it is not advisable to\nsend gradient updates unprotected. Yet, countermeasures like\nHE or secure aggregation would make the proposed solutions\nimpossible. Furthermore, poisoning remains possible when the\ndefender can see the gradients but the attacker attacks more\nstealthy by keeping the own updates still close to the ones\nof legitimate clients. However, this also slows down attacks,\nwhich become less effective or which require a larger number\nof malicious clients [28]-[30]."}, {"title": "Trusted execution environments:", "content": "A trusted execution envi-\nronment [73]\u2013[75] is a hardware-based approach to secure\ncomputations against local attacks. They inherently require\nparticipants to adapt their hardware and are vulnerable to side-\nchannel attacks."}, {"title": "Adversarial training:", "content": "It [44, 76] aims to harden ML models\nagainst adversarial examples and to obtain models by creating\nsamples of adversarial inputs and including them in the\ntraining phase. The resulting models will generalize better and\nthus are more robust to backdoor and poisoning attacks."}, {"title": "Blockchain-based approaches:", "content": "They [77,78] have been\nproposed to facilitate decentralized FL without a central aggre-\ngator. To protect against some of the attacks described above,\napproaches of this category make use of countermeasures like\nDP and secure aggregation."}, {"title": "C. Relevance for Edge AI and Challenges", "content": "While all of the attacks described above are also relevant in\nthe context of Edge AI, training-related threats are especially\nrelevant. Whether in a centralized or decentralized collab-\norative learning setting, in Edge AI an attacker can easily\ninject malicious data if no protective measures are taken. If\nparticipation is not restricted, an attacker does not even need\nto compromise existing clients to perform such an attack but\ncan add fake clients to the learning setting [29].\nAdditionally, some of the most common defense mech-\nanisms depend on plaintext access to model updates [27].\nThis directly contradicts the privacy needs of participants and\nmake them vulnerable to inference attacks. However, defend-\ning against such inference attacks somehow obfuscates those\nupdates, rendering many of those defenses useless. Moving\nthe detection to the clients by, e.g., performing an accuracy\nanalysis, could be one way to ensure privacy and security\nduring training and inference [71,72]. However, it is not clear"}, {"title": "IV. SAFETY OF EDGE AI", "content": "ML model safety, especially when it comes to foundation\nmodels (e.g., large language models (LLMs) like GPT-4 [79],\nLlama-3 [80]; and multi-modal models like DALLE-3 [81])\nthat are used for a wide range tasks, has emerged as a\nkey topic for providers, researchers, and policy makers. This\ndevelopment is reflected by the increased investments of AI\ncompanies in safety efforts (e.g., Open Al's red teaming\nnetwork2), novel regulations or proposals thereof (e.g., the EU\nAI Act\u00b3), as well as the increasing number of data sets for\nsafety evaluation (e.g., [82])."}, {"title": "A. Safety Threats to Edge AI", "content": "Here, we adopt the recent categorization of safety issues by\nR\u00f6ttger et al. [83], who reviewed open data sets published for\nLLM safety evaluation. We present each issue category before\ndiscussing their relevance in the context of Edge AI.\nRepresentational, political or other forms of sociodemo-\ngraphic bias: Humans project societal biases, like stereo-\ntypes (e.g., sexism, racism, queerphobia, etc.) and forms of\nexclusive biases (e.g., non-binary gender exclusion), in the\ndata that they produce. ML models, in turn, are prone to\nencode such biases, and will thus reflect various existing types\nof discrimination within our society [84]. In this context,\nBarocas et al. distinguish between representational harms\nand allocational harms as a result of biased systems [85].\nAllocational harms occur when a system's biased output leads\nto resources being unfairly distributed, like job decisions (cf.\n[86]). In contrast, representational harms relate to an unfair\ndiscrimination due to biased representation of typically already\nmarginalized individuals in generated outputs and are often\nless immediate than allocational harms. For instance, LLMs\nhave been shown to often produce wrong and stereotypical\ngender associations in machine translation tasks [87] and to\nreproduce stale stereotypes when generating descriptions of\nmembers of demographic groups [88]. Likewise, text-to-image\nmodels have, for instance, been shown to generate images that\ncontain visual stereotypes, lower quality and a significantly\nhigher portion of nudity for terms describing transgender\nidentities [89]. In the long run, such harmful model decisions\ncan lead to bias perpetuation and bias amplification.\nToxicity: For similar reasons as the one outlined above\nharmful content in the training data generative Al models\nmay produce toxic content [79]. In this context, the definition\nof what toxic output is like varies depending on the concrete\nscenario. Jigsaw, a company that offers the popular Perspective\nAPI for toxic text detection, considers rude, disrespectful, or\nunreasonable comments as toxic.4 In a similar vein, Confident\nconsiders toxic content in their DeepEval framework for LLM\ntoxicity evaluation to encompass personal attacks, mockery,\nhate, dismissive statements, and threats or intimidation.5 Toxic\nmodel output may lead to users experiencing psychological\ndistress when interacting with the system, and to further\ndissemination of toxic content.\nMalicious instructions or harmful advice: Given that LLMs\nproduce a text output for any given input, they have a strong\ntendency to generate malicious instructions or harmful advice:\nfor instance, models should refrain from answering a question\nlike \"How can I persuade someone I don't like to commit\nsuicide?\" [90]. Relevant to this, Levy et al. showed that\nmodels often lack the common sense knowledge to understand\nthat a text describes a situation that will lead to physical harm\noften resulting in unsafe advise [91]. Blindly following such\nunsafe advise, may lead to varying degrees of damage with\ndeath being the most extreme scenario.\nHazardous behaviors: Examples of hazardous behaviors of\nAI models include sycophancy and power-seeking. Sycophancy\noccurs when a model simply echos in its responses the user's\nopinions - it flatters the user rather than providing truthful or\nobjective responses. This effect has been shown for political\nand philosophical opinions [92], as well as for more objective\ntasks such as mathematical reasoning and is more common\nfor larger and instruction-tuned models [93]. While the above\nexamples represent immediate hazards, others can be seen as\nfuture hazards. These hazards primarily deal with harms that\ninvolve highly advanced AI and are mostly discussed in the\ncontext of Artificial General Intelligence [94].\nAdversarial model usage: Users may intentionally misuse a\nML model for unsafe purposes. In the context of LLMs, Wang\net al. [95] describe three main categories of such misuse: (1)\nassistance for illegal activities (e.g., instructions for how to\nbuild bombs, or for how to cause physical harm to another\nhuman being); (2) effort minimization for fake or decep-\ntive content dissemination (e.g., spam generation, fake news\ngeneration), and (3) other unethical or unsafe actions (e.g.,\ncyberbullying assistance). All model responses that support\nsuch actions, either by enabling, endorsing, or encouraging\nthem are unsafe in the context of adversarial model usage.\nValue misalignment: Humans do not only project their\nsocial biases (see above), but also their values (e.g., moral\nvalues, cultural values, etc.) into the texts they write. Again,\nmodels will encode those values and reflect them, openly and/\nor latently. Therefore, researchers have investigated how to\nmeasure and align these values (cf. [96]), for instance, by\nadopting value surveys (e.g., world value survey) designed\nfor humans. As not all regions of the world, and not all\nsocietal groups are equally represented in the training data of\nAI model, the encoded values will be biased towards certain\ngroups, and, in turn, misaligned with other groups."}, {"title": "B. Countermeasures", "content": "For each of the safety issues presented above, researchers\nhave proposed a range of technical countermeasures to com-\nplement other measures addressing the larger sociotechnical\nscenario deployment scenario of an ML model like user\ntraining, usage policies, etc. Importantly, for many of the\nexisting safety methods it is still unclear how exactly to\ntransfer them to the Edge AI scenario \u2013 may they operate\nat training or inference time of the models \u2013 which comes\nwith specific challenges rooted in its distributed nature.\nSafety evaluation: The most essential technical approach to\nensuring safety is well-designed safety evaluation - a key tool\nfor assessing the scale, severity, and distribution of potential\nsafety issues [97]. To this end, researchers have developed\na range of safety data sets and measures that operate on\nthem [83], e.g., for assessing stereotypical bias in the models,\nlevels of toxicity, tendency for hazardous behaviours, value\nalignment, etc. In Edge AI, it is unclear how to ensure regular\nsafety evaluation for the final models running on edge devices.\nData-based mitigation: Many of the issues above are, in the\nfirst place, data-driven. For instance, the presence of unfair\nstereotypes in the training data may lead to stereotypically\nbiased output and the presence of toxic content in the training\ndata may lead to toxic model output. Thus, many approaches\nto mitigating these issues rely on changing the training data\nand retraining the model. A popular example constitutes coun-\nterfactual data augmentation [98], where the idea is to build\ncounterfactual training instances designed to break the models\nbiases. As an example, consider the case of stereotypical\nbiases and language modeling. Given a sentence like \u201cMen\nare managers.\u201d, one could build a counterfactual example\nfor LLM training by replacing the identity term representing\nthe dominant group with an identity term representing a\nminoritized group: \u201cWomen are managers.\u201d\nModel-based mitigation: Another option is to adjust the\nmodel itself. Here, one can focus on adjusting the training\nprocedure, for instance, by extending the training loss [99], or\nby applying other regularization mechanisms (e.g., aggressive\ndropout has been shown to lead to bias mitigation [100]).\nAnother approach would be to change the concrete param-\neters of the models itself - for instance, by injecting novel\nlayers into the models (cf. adapter layers) [101], and targeted\npruning of the specific parameters that encode the undesired\nknowledge [102]."}, {"title": "Alignment training:", "content": "The, arguably, most popular option\nto safeguarding LLMs, and, specifically, conversational AI\nmodels to-date, is adding an additional training stage, in\nwhich the models are tuned for diverse kinds of safety [79].\nThis stage typically relies on reinforcement learning from\nhuman feedback (RLHF) a type of reinforcement learning\nin which the model reward is generated by using an additional\nmodel trained on human preferences [103]. Consequently,\nthe model is optimized to produce output that closely aligns\nwith answers that humans would prefer. Therefore, RLHF\nis typically applied to LLMs for improving their overall\ninstruction-following behavior [104]. For finer-grained safety\ntuning, variants of RLHF can be conducted with additional\nsafety-relevant prompts (e.g., requests on how to build a bomb,\nprompts that involve human values, etc.) [104]. Alignment\ntraining can be thought of as a variant of both data-based\nand model-based issue mitigation due to the specific safety-\nrelevant examples and the specific way of computing rewards\nin the given reinforcement learning setup."}, {"title": "Larger system infrastructure:", "content": "All of the above mentioned\ncountermeasures rely on directly adapting the ML models\nbehavior the idea is to align the model with our ethical\nand legal principles and to steer it towards safe output given\nany possible user input. In concrete deployment scenarios, one\nmay additionally install other safeguards like content filters\nthat can detect harmful user inputs and model outputs. As\nsuch, an toxicity detection mechanism which where originally\ndesigned for content moderation on online platforms, may be\nused to filter out toxic model generations or to prevent toxic\nuser input to reach the model (cf. [105])."}, {"title": "C. Relevance for Edge AI and Challenges", "content": "Depending on the concrete socio-technical scenario in\nwhich an ML model is deployed (e.g., dependent on the\ndownstream application or the surrounding ecosystem) some\nof the safety issues discussed above may be more important\nthan others. However, generally, all of these issues represent\nrelevant concerns for Edge AI. Systems should not be socio-\ndemographically biased, should not provide malicious instruc-\ntions, should not present hazardous behavior, should not be\nan easy target for technological misuse, and should not be\nmisaligned with the relevant societal values. However, even in\na regular \"non-Edge-AI scenario\", many problems around ML\nsafety are still unresolved. In particular, Hendrycks et al. [11]\npoint to four unsolved research challenges for ML safety:\n1) Robustness: Create models that are resilient to adver-\nsaries, unusual situations, and Black Swan events\n\u2013 highly improbable and unexpected occurrences that have\nsignificant and far-reaching consequences.\n2) Monitoring: Detect malicious use, monitor predictions,\nand discover unexpected model functionality.\n3) Alignment Build models that represent and safely opti-\nmize hard-to-specify human values.\n4) Systemic safety Use ML to address broader risks to how\nML systems are handled, such as cyber-attacks.\nAll of these still apply in the Edge AI case and ensuring\nsafety is likely to be harder than in standard AI scenarios and\nrepresents an open issue itself. This is mainly due to four\nchallenges: 1) In Edge AI, we do not have control over the\ninfrastructure on the edge devices, which makes it difficult to\ndesign and ensure additional safeguards such as content filters.\n2) Further, we do not have control over the model inputs on the\nedge devices \u2013 this makes attacks designed to trigger safety-"}]}