{"title": "STRONGLY-POLYNOMIAL TIME AND VALIDATION ANALYSIS OF POLICY GRADIENT METHODS", "authors": ["CALEB JUT", "GUANGHUI LAN"], "abstract": "Reinforcement learning lacks a principled measure of optimality, causing research to rely on algorithm-to-algorithm or baselines comparisons with no certificate of optimality. Focusing on finite state and action Markov decision processes (MDP), we develop a simple, computable gap function that provides both upper and lower bounds on the optimality gap. Therefore, convergence of the gap function is a stronger mode of convergence than convergence of the optimality gap, and it is equivalent to a new notion we call distribution-free convergence, where convergence is independent of any problem-dependent distribution. We show the basic policy mirror descent exhibits fast distribution-free convergence for both the deterministic and stochastic setting. We leverage the distribution-free convergence to a uncover a couple new results. First, the deterministic policy mirror descent can solve unregularized MDPs in strongly-polynomial time. Second, accuracy estimates can be obtained with no additional samples while running stochastic policy mirror descent and can be used as a termination criteria, which can be verified in the validation step.", "sections": [{"title": "1. Introduction", "content": "The increasing interest in applying reinforcement learning (RL) to real-world applications over the last decade is fueled not only by its success in domains like robotics, resource allocation, and optimal control [3,13,18], but more recently in strategic game play (i.e., artificial intelligence for video games) and training large-language models via reinforcement learning from human feedback [29, 34, 40]. Despite the flurry of success, there have been numerous concerns regarding the development of principled RL algorithms, including (but not limited to) performance evaluation [2] and hyperparameter tuning [8, 11, 12]. Without a clear understanding of these fundamental issues, training RL can result in superfluous work, exacerbating the already huge energy consumption de-manded by large models [41]. While solving these issues is highly non-trivial, the goal of this paper is to gain a better understanding into the former issue by leveraging ideas from stochastic optimization.\nTo motivate our solutions, we revisit current practice for RL performance evaluation. Because the training of (deep) RL is expensive - taking thousands to millions of simulation steps - the typical approach is to run a small number of seeds (e.g., 3 or 5) and plot confidence intervals, or in some cases a point estimate or a single statistic; see [2] and references therein. This was noticed and rightful critiqued in [2], and the authors proposed more robust and informative statistics, e.g., interval estimates in place of point estimates, interquartile mean in place of mean.\nYet, several important issues remain. First, despite the plethora of samples generated, the common way to measure the performance of an RL algorithm is the average performance in the last few iterations (e.g., last 100 training epochs). This metric is a random variable that depends on a fraction of samples. Consequently, a majority of samples are not utilized, resulting in poor sample usage. Second, RL training performance plots often exhibit faster initial improvement followed by stagnated performance. This stagnation can be viewed potentially as convergence to a local solution, and training stops shortly after. However, this practice provides no guarantees on the optimality of the policy, and it can hinder the model from identifying better solutions. Third, most algorithms are compared relative to other algorithms or to some pre-defined threshold, e.g., based on a-priori knowledge or in the case of video games, the performance of an expert player. Similar to the second issue, algorithm-to-algorithm comparisons provide no meaningful optimality guarantees, and in more complex, unknown environments, defining a reasonable threshold is non-trivial and heuristic at best. See [2,8,11,12,29,37] and references therein for some examples\u00b9. While we cannot possibly (or hope to) address all these issues, our goal is to shed some light on a few issues and provide potential remedial solutions.\nTo possibly address these issues, our main and key idea is a new gap function. More concretely, we show for finite state and action Markov decision processes (MDP), there is a simple and efficiently computable gap function that provides both upper and lower bounds on the optimality gap. More specifically, the gap function involves finding the largest element of a vector or solving a convex"}, {"title": "1.1. Notation", "content": "For a Hilbert space (e.g. real Euclidean space), let $\\| \\cdot \\|$ be the induced norm and let $\\|\\cdot \\|_*$ be its dual norm. When appropriate, we specify the exact norm (e.g., $l_2$ norm, $l_1$ norm).\nWe denote the probability simplex over n elements as\n$\\Delta_n := \\{x \\in \\mathbb{R}^n: \\sum_{i=1}^n x_i = 1, x_i \\geq 0 \\forall i\\} .$"}, {"title": "2. Markov decision process, a gap function, and connections to (non-)linear programming", "content": "An infinite-horizon discounted Markov decision process (MDP) is a five-tuple $(S, A, P, c, \\gamma)$, where $S$ is finite state space, $A$ is a finite action space, $P: S \\times S \\times A \\rightarrow \\mathbb{R}$ is the transition kernel where given a state-action $(s, a)$ pair, it reports probability of the next state being $s'$, denoted by $P(s'|s,a)$. The cost is $c:S \\times A \\rightarrow \\mathbb{R}$ and $\\gamma\\in [0,1)$ is a discount factor. A feasible policy $\\pi: A \\times S \\rightarrow \\mathbb{R}$ determines the probability of selecting a particular action at a given state. We denote the space of feasible policies by $\\Pi$. Now, we write Bregman's divergence between any two policies at state $s$ as\n$D'_\\pi(s) := D(\\pi(\\cdot|s), \\pi'(\\cdot|s)) = \\omega(\\pi'(\\cdot|s)) - \\omega(\\pi(\\cdot|s)) - \\langle\\nabla\\omega(\\pi(\\cdot|s)), \\pi'(\\cdot|s) - \\pi(\\cdot|s)\\rangle, \\forall \\pi, \\pi' \\in X$.\nWe measure a policy $\\pi$'s performance by the action-value function $Q^\\pi: S \\times A \\rightarrow \\mathbb{R}$ defined as\n$Q^\\pi(s, a) := \\mathbb{E}[\\sum_{t=0}^\\infty \\gamma^t [c(s_t, a_t) + h^\\pi(s_t)] | s_0 = s, a_0 = a, a_t = \\pi(s_t), s_{t+1} \\sim P(\\cdot|s_t,a_t)].$\nThe function $h^\\pi(s): \\Pi \\rightarrow \\mathbb{R}$ is closed convex function w.r.t. the policy and there exists some $\\mu_h \\geq 0$ such that\n$h^\\pi(s) - h^{\\pi'}(s) - \\langle (h^{\\pi'})'(s, \\cdot), \\pi(\\cdot|s) - \\pi'(\\cdot|s)\\rangle \\geq \\mu_h D_\\pi(s), \\forall s \\in S,$\nwhere $\\langle\\cdot, \\cdot\\rangle$ denotes the inner product of the action space, and $(h^{\\pi'})'(s, \\cdot)$ denotes a subgradient of $h^\\pi(s)$ at $\\pi'$. Clearly, if $h^\\pi(s) = 0$, then $Q^\\pi$ becomes the classic action-value function. Recently, it has been observed entropy regularization (i.e., choice of $h^\\pi(s) = \\mu D_\\pi(s)$ for some $\\mu > 0$ and starting policy $\\pi_0$) helps incentivize policies that have safe exploration and learn risk-sensitive policies [6, 25, 32]. By incorporating a general function $h^\\pi$, our framework covers these settings and more generalized ones too, e.g., barrier functions for constraints. Here we separate $h^\\pi$ from the cost $c(s, a)$ to take the advantage of its strong convexity in the design and analysis of algorithms. Moreover, we define the state-value function $V^\\pi: S \\rightarrow \\mathbb{R}$ associated with as\n$V^\\pi(s) := \\mathbb{E}[\\sum_{t=0}^\\infty \\gamma^t [c(s_t, a_t) + h^{a_t}(s_t)] | s_0 = s, a_t = \\pi(s_t), s_{t+1} \\sim P(\\cdot|s_t, a_t)].$\nIt can be easily seen from the definitions of $Q^\\pi$ and $V^\\pi$ that\n$V^\\pi(s) = \\sum_{a\\in A} \\pi(a|s)Q^\\pi(s,a) = \\langle Q^\\pi(s,\\cdot), \\pi(\\cdot|s)\\rangle$\n$Q^\\pi(s, a) = c(s, a) + h^\\pi(s) + \\gamma\\sum_{s'\\in S} P(s'|s, a)V^\\pi(s').$\nThe main objective in MDP is to find an optimal policy $\\pi^*: A \\times S \\rightarrow \\mathbb{R}$ such that\n$V^{\\pi'}(s) \\leq V^{\\pi^*}(s), \\forall \\pi(\\cdot|s) \\in \\Delta_{|A|}, \\forall s \\in S$.\nSufficient conditions that guarantee the existence of $\\pi^*$ have been intensively studied (e.g., [4,35]). Note that (2.4) can be formulated as a nonlinear optimization problem with a single objective function. Given an initial state distribution $\\rho\\in \\Delta_{|s|}$, let $f_\\rho$ be defined as\n$f_\\rho(\\pi) := \\sum_{s\\in S} \\rho(s) \\cdot V^\\pi(s)$.\nOur goal is to solve the following policy optimization problem $\\min_{\\pi \\in \\Pi} f_\\rho(\\pi)$. While the distribution $\\rho$ can be arbitrarily chosen (as long as the probability is positive for all states), prior policy gradient methods typically select $\\rho$ to be the stationary state distribution induced by the optimal policy $\\pi^*$, denoted by $v^* = \\kappa^{\\pi^*} [22,23,28]. As such, the problem reduces to $\\min_{\\pi \\in \\Pi} f_{v^*}(\\pi)$. Although this choice leads to the objective function depending on the (unknown) $v^*$, the implementation of the algorithm can be oblivious to $v^*$; it is simply for analysis' sake."}, {"title": "2.1. Performance difference and gap function", "content": "Given a policy $\\pi(\\cdot|s) \\in \\Delta_{|A|}$, we define the discounted state visitation distribution $\\kappa^\\pi: S \\rightarrow \\mathbb{R}$ by\n$\\kappa^\\pi (s) := (1 - \\gamma) \\sum_{t=0}^\\infty \\gamma^t Pr^\\pi \\{s_t = s|s_0 = q\\},$\nwhere $Pr^\\pi \\{s_t = \\cdot|s_0 = q\\}$ is the distribution of state $s_t$ when following policy $\\pi$ and starting at state $q \\in S$. In the finite state case, we can also view $\\kappa^\\pi \\in \\mathbb{R}^{|S|}$ as a vector. Clearly, $\\kappa^\\pi \\in \\Delta_{|S|}$, and one can show the lower bound $\\kappa(s) \\geq \\frac{1}{1-\\gamma}$.\nWe now state an important \"performance difference\" lemma which tells us the difference on the value functions for two different policies.\nLEMMA 2.1. Let $\\pi$ and $\\pi'$ be two feasible policies. Then we have\n$V^{\\pi'}(s) - V^{\\pi}(s) = \\frac{1}{1-\\gamma} \\sum_{q\\in S} \\psi^\\pi (q, \\pi') \\kappa^{\\pi'}(q), \\forall s \\in S,$\nwhere the advantage function is defined as\n$\\psi^\\pi (q, \\pi') := \\langle Q^\\pi(q, \\cdot), \\pi'(\\cdot|q)\\rangle - V^\\pi(q) + h^{\\pi'}(q) - h^\\pi(q)$.\nProof. By the definition of $V^{\\pi'}$ in (2.1), we have\n$V^{\\pi'}(s) - V^{\\pi}(s)$\n$= \\mathbb{E}[\\sum_{t=0}^\\infty \\gamma^t [c(s_t, a_t) + h^{\\pi'}(s_t)] | s_0 = s, a_t \\sim \\pi'(\\cdot|s_t), s_{t+1} \\sim P(\\cdot|s_t, a_t)] - V^{\\pi}(s)$\n$= \\mathbb{E}[\\sum_{t=0}^\\infty \\gamma^t [c(s_t, a_t) + h^{\\pi'}(s_t) + \\gamma V^{\\pi}(s_{t+1}) s_{t+1} \\sim P(\\cdot|s_t, a_t)]\n+ \\mathbb{E} [V^{\\pi}(s_0) | s_0 = s, a_t \\sim \\pi'(\\cdot|s_t), s_{t+1} \\sim P(\\cdot|s_t, a_t)] - V^{\\pi}(s)$\n$= \\mathbb{E} [\\sum_{t=0}^\\infty \\gamma^t [c(s_t, a_t) + h^{\\pi'}(s_t) + \\gamma V^{\\pi}(s_{t+1}) - V^{\\pi}(s_t) + h^{\\pi'}(s_t) - h^{\\pi}(s_t)]\n| s_0 = s, a_t \\sim \\pi'(\\cdot|s_t), s_{t+1} \\sim P(\\cdot|s_t, a_t)],$\nwhere the second identity follows from the cancelation of the terms by taking telescoping sum, and the last identity follows from $\\mathbb{E} [V^{\\pi}(s_0) | s_0 = s,a_t = \\pi'(s_t), s_{t+1} \\sim P(\\cdot|s_t, a_t)] = V^{\\pi}(s)$. Now using the above identity, (2.3) and (2.6), we have\n$V^{\\pi'}(s) - V^{\\pi}(s) = \\mathbb{E}[\\sum_{t=0}^\\infty \\gamma^t [Q^{\\pi}(s_t, a_t) - V^\\pi(s_t) + h^{\\pi'}(s_t) - h^\\pi(s_t)]\n| s_0 = s, a_t \\sim \\pi'(\\cdot|s_t), s_{t+1} \\sim P(\\cdot|s_t, a_t)]$\n$=\\frac{1}{1-\\gamma} \\sum_{q\\in S} [\\langle Q^{\\pi}(q,\\cdot), \\pi'(\\cdot|q)\\rangle - V^\\pi(q) + h^{\\pi'}(q) - h^\\pi(q)] \\kappa^{\\pi'}(q)$.$\nWe are ready to establish our main and simple result: the gap function.\nPROPOSITION 2.2. For any policy $\\pi$,\n$\\max_{p\\in \\pi}\\{-\\psi^\\pi (s,p)\\} \\leq V^\\pi (s) - V^{\\pi^*}(s) \\leq (1 - \\gamma)^{-1} \\max_{p\\in \\pi}\\{-\\psi^\\pi (s',p)\\}, \\forall s \\in S.$\nProof. First, we prove the lower bound. Let $\\hat{\\pi} \\in \\arg\\max_{p\\in \\pi}\\{-\\psi^\\pi (s,p)\\}$. This choice implies $-\\psi^\\pi (s, \\hat{\\pi}) = \\max_{p\\in \\pi} -\\psi^\\pi (s,p) \\geq -\\psi^\\pi (s, \\pi^*) = 0$. Therefore,\n$V^\\pi (s) - V^{\\pi^*}(s) \\geq V^\\pi (s) - V^{\\pi^*}(s)$\n$\\overset{(2.7)}{=} \\frac{1}{1-\\gamma} \\sum_{q\\in S} -\\psi^{\\pi} (q, \\hat{\\pi})\\kappa^{\\pi}(q)$\n$\\overset{\\kappa^\\pi (q) \\geq 0}{>} \\frac{1}{1-\\gamma} -\\psi^\\pi (s,\\hat{\\pi}),$\nwhich by construction of $\\hat{\\pi}$ establishes the lower bound.\nAs for the upper bound, we recall $\\kappa^{\\pi^*}$ from (2.6) is a distribution over states. So,\n$V^\\pi (s) - V^{\\pi^*}(s) \\overset{(2.7)}{=} \\frac{1}{1-\\gamma} \\sum_{q\\in S} -\\psi^{\\pi} (q, \\pi^*)\\kappa^{\\pi^*} (q)$\n$\\leq \\frac{1}{1-\\gamma} \\max_{p\\in \\pi}\\{-\\psi^{\\pi} (s,p)\\} \\sum_{q\\in S} \\kappa^{\\pi^*} (q)$\n$=\\frac{1}{1-\\gamma} \\max_{p\\in \\pi}\\{-\\psi^{\\pi} (s,p)\\}$."}, {"title": "2.2. Convex programming and duality theory of (regularized) RL", "content": "For a given distribution $\\rho\\in \\Delta_{|s|}$ and policy $\\pi(\\cdot|s) \\in \\Delta_{|A|}$, we introduce the weighted visitation $\\eta^\\pi: S \\rightarrow \\mathbb{R}$,\n$\\eta^\\pi (s) := (1 - \\gamma)^{-1} \\sum_{q\\in S} \\rho(q)\\cdot \\kappa^\\pi(s),$\nwhere recall $\\kappa^q$ is the state-visitation vector from (2.6).\nLEMMA 2.4. For any policy $\\pi$ and distribution over states $\\rho$,\n$f_\\rho(\\pi) = \\sum_{s\\in S} \\rho(s)V^\\pi(s) = \\sum_{s\\in S}[c(s, \\pi(\\cdot|s)) + h^\\pi (s)] \\cdot \\eta^\\pi (s)$.\nProof. Similar to Lemma 2.1,\n$V^\\pi(s) = \\mathbb{E}[\\sum_{t=0}^\\infty \\gamma^t [c(S_t, \\pi(S_t)) + h^{\\pi(s_t)} (S_t)]|S_0 = s, a_t \\sim \\pi(S_t), S_{t+1} \\sim P(\\cdot|s_t, a_t)]$\n$= \\sum_{t=0}^\\infty \\mathbb{E}[c(S_t, \\pi(S_t)) + h^{\\pi(s_t)} (S_t)|S_0 = s, a_t \\sim \\pi(S_t), S_{t+1} \\sim P(\\cdot|s_t, a_t)]$\n$= \\sum_{t=0}^\\infty \\sum_{q\\in S} Pr\\{S_t = q|S_0 = s\\}\\cdot \\mathbb{E}[c(S_t, \\pi(S_t)) + h^{\\pi(s_t)} (S_t)|S_t = q, S_0 = s, a_t \\sim \\pi(S_t), S_{t+1} \\sim P(\\cdot|s_t, a_t)]$\n$= \\sum_{q\\in S} \\sum_{t=0}^\\infty  Pr^\\pi \\{S_t = q|S_0 = s\\} \\cdot [c(q, \\pi(q)) + h^{\\pi(q)} (q)].$\nNoticing $\\sum_{t=0}^\\infty  Pr^\\pi \\{S_t = q|S_0 = s\\} = (1 - \\gamma)^{-1}\\kappa^\\pi(q)$, we have\n$f_\\rho(\\pi) = \\sum_{q\\in S}[c(s', \\pi(q)) + h^{\\pi(q)} (q)] \\sum_{s\\in S}(1 - \\gamma)^{-1}\\rho(s)d(q)$.\nThe proof is complete by observing the last term $\\sum_{s\\in S}(1 - \\gamma)^{-1}\\rho(s)d^{\\pi}(q) = \\eta^\\pi (q)$."}, {"title": "2.3. Related works", "content": "RL methods can generally be divided into model-based and model-free approaches. In model-based methods, one seeks to approximate the underlying model by using various statistical learning techniques and then to solve an approximate problem to the original MDP. For example, assuming the model is known a priori, the classical linear programming (LP) formulation is"}, {"title": "3. Distribution-free convergence for PMD and strongly-polynomial runtime", "content": "Our goal is to show basic policy mirror descent (PMD) method can achieve distribution-free convergence that matches the best rates for bounding just the aggregated optimality gap. That is, we aim to show one"}, {"title": "3.1. Basic PMD method", "content": "We consider a basic policy mirror descent (PMD) method. Unless otherwise mentioned, we assume the initial policy $\\pi_0(s)$ is the uniform distribution over actions, i.e.,\n$\\pi_0(a|s) = \\frac{1}{|A|}, \\forall s \\in S,$\nand Bregman's divergence $D_\\pi(s)$ is the KL-divergence $KL(\\pi'(\\cdot|s)|\\pi(\\cdot|s))$. With this setup, then for any policy $\\pi(\\cdot|s) \\in \\Delta_{|A|}$, it holds $D_\\pi(s) \\leq \\log |A| [23, Eq 34].\nThe following can be derived by the optimality conditions of (3.2), see for example [22, Lemma 3.1].\nLEMMA 3.1. If the step size $\\eta_t$ satisfies $\\mu_h + \\eta_t^{-1} \\geq 0$,\n$\\psi^{\\pi_t}(s, \\pi_{t+1}(s)) + \\eta_t^{-1}D_{t+1} (s))\n+(\\mu_h + \\eta_t^{-1})D_{t+1}(s) (s) \\leq \\psi^{\\pi_t}(s, \\pi(\\cdot|s)) + \\eta_t^{-1}D_{\\pi_t}(s), \\forall \\pi(\\cdot|s) \\in \\Delta_{|A|}, s \\in S$.\nNext, monotonicity of PMD is shown. In the sequel, a step size $\\eta_t = 1/0$ simply means the Bregman divergence $D_\\pi (s)$ is set to 0 in the subproblem (3.2) at iteration $t$. We skip the proof, which can be found in [22, Proposition 3.2].\nLEMMA 3.2. For any $t \\in [0, +\\infty) \\cup \\{1/0\\}$, $V^{\\pi_{t+1}}(s) - V^{\\pi_t}(s) \\leq \\psi^{\\pi_t}(s, \\pi_{t+1}(s)) \\leq 0$.\nNow we show that a direct application of the PMD method achieves a sublinear rate of convergence of the value function for all states.\nTHEOREM 3.3. Let $t = \\eta > 0$ in the PMD method. Then\n$V^k (s) - V^{\\pi^*} (s) \\leq \\eta[f_s(\\pi_0)-f_s(\\pi^*)]+\\log|A|, \\forall s \\in S,$\nwhere $f_s(\\pi) := \\sum_{q\\in \\kappa^*} (q)V^{\\pi} (q)$.\nProof. We have for any policy $\\pi(\\cdot|s) \\in \\Delta_{|A|}$\n$(1 - \\gamma)[V^\\pi_t(s) - V^{\\pi^*} (s)]$\n$\\overset{(2.7)}{=} \\sum_{q\\in S} \\kappa^{\\pi} (q)(-\\psi^{\\pi_t} (q, \\pi(q)))$\n$\\overset{Lemma 3.1}{\\leq} \\sum_{q\\in S} \\kappa^{\\pi} (q)[-\\psi^{\\pi_t} (q, \\pi_{t+1}(q)) + \\eta^{-1}D_{\\pi_t} (q) - \\eta^{-1}D_{\\pi_{t+1}}(q)]$\n$\\overset{Lemma 3.2}{\\leq} \\sum_{q\\in S} \\kappa^{\\pi} (q) [V^{\\pi_t} (q) - V^{\\pi_{t+1}} (q) + \\eta^{-1}D_{\\pi_t} (q) - \\eta^{-1}D_{\\pi_{t+1}}(q)]$.\nTaking a telescopic sum from $t = 0, . . ., k - 1$, using Lemma 3.2, and then fixing $\\pi = \\pi^*$ and applying $V^k (q) \\leq V^{\\pi^*} (q)$,\n$\\gamma)[V^k(s) - V^{\\pi^*}(s)]$\n$\\leq \\sum_{q\\in S} \\kappa^{\\pi^*} (q) [V^{\\pi_0} (q) - V^{\\pi^*} (q) + \\eta^{-1}D_{\\pi_0} (q) - \\eta^{-1}D_{\\pi_k}(q)]$.\nRecalling $D_{\\pi_0} (q) \\leq \\log|A|$ finishes the proof."}, {"title": "3.2. Linear distribution-free convergence for PMD", "content": "We will show by directly using PMD with a step size that increases geometrically at fixed intervals, then one can obtain linear convergence of the value function over any state. Note that this result applies to general convex and strongly convex regularizers, i.e., $\\mu_h \\geq 0$.\nTHEOREM 3.4. Let $N := [4(1 - \\gamma)^{-1}]$. By using the step size\n$\\eta_t = 4^{\\lfloor t/N \\rfloor} D_0/\\Delta_0,$\nwhere $\\Delta_0 := (1 - \\gamma)^{-1} \\max_{s\\in S, p\\in \\pi}\\{-\\psi^{\\pi_0}(s,p)\\}$ and $\\max_s D_{\\pi_0} (s) < D_0$, then\n$V_t (s) - V^{\\pi^*} (s) \\leq 2^{-\\lfloor t/N \\rfloor}\\Delta_0, \\forall s \\in S.$\nProof. To simplify our analysis, we say epoch $i$ is the set of iterations $t = iN,iN +1,..., (i + 1)N - 1$.\nOur proof will be by mathematical induction over epoch $i$. We will prove for any $s\\in S$ and integer $i \\geq 0$,\n$V^{\\pi_{iN}} (s) - V^{\\pi^*} (s) \\leq 2^{-i}\\Delta_0$\n$\\sum_{q \\in S} \\kappa^\\pi (q) D^{\\pi_{iN}} (q) \\leq 2D_0$.\nIn view of Lemma 3.2, then (3.5) implies (3.4).\nThe base case of $i = 0$, (3.5) is from Proposition 2.2, while (3.6) is from the assumption $D^{\\pi_0} (q) \\leq D_0$ and $\\kappa$ is a distribution over states. We consider $i + 1$ for some $i \\geq 0$. Applying (3.3) over $t = iN, . . ., (i + 1)N - 1$, which uses a constant step size of $\\eta(i) := 4^{i} \\log |A|/\\Delta_0$,\n$N(1 - \\gamma)[V^{\\pi_{(i+1)N}} (s) - V^{\\pi^*} (s)] + \\sum_{q \\in S} \\kappa^\\pi (q) D^{\\pi_{(i+1)N}} (q)$\n$< \\sum_{q \\in S} \\kappa^\\pi (q)[V^{\\pi_{iN}} (q) - V^{\\pi^*} (q)] + \\eta^{-1} \\sum_{q \\in S} \\kappa^\\pi (q)D^{\\pi_{iN}} (q)$\n$\\overset{(3.5), (3.6)}{<}  2^{-i}\\Delta_0 + \\eta_i 2D_0$\n$\\overset{N \\mbox{ and } \\eta(i)}{<} 2^{-(2-1)} \\Delta_0.$\nIn view of $N$ and $\\eta(i)$, the above clearly implies (3.5) and (3.6) for epoch $i + 1$, which completes the proof by induction."}, {"title": "3.3. A strongly polynomial PMD", "content": "Recall an algorithm is strongly polynomial when the number of artihmetic operations is polynomial in the input size, and the memory usage/data transfer is as well. For (unregularized) MDPs, an algorithm is strongly polynomial for a fixed discount factor $\\gamma$ if its runtime is polynomial in the number of state and action pairs and size of the data input2. Our"}, {"title": "4. Distribution-free convergence for stochastic PMD", "content": "We assume throughout that given a policy $\\pi_t$, we are given an estimator $Q^*_t$ generated by random vectors $\\Xi_t$ instead of the true advantage function $Q^{\\pi_t}$. Our goal is to shown basic policy mirror descent (PMD) method also can achieve distribution-free convergence when only given $Q^*_t$.\nWe make the following assumption regarding the underlying noise. It covers independent and identically distributed (iid) random data and bounded stochastic estimates, as well as non-iid with time-dependent noise (e.g., Markovian noise [20, 30]) and noise with bounded moments. This latter setup is more common in reinforcement learning and stochastic optimal control, where data is generated along a single trajectory and subject to some (possibly Gaussian) noise [15].\nAssumption 4.1. There exists $\\zeta, \\sigma, \\mathcal{Q} \\geq 0$\n$||\\mathbb{E}_{\\Xi_t} [Q^{\\pi_t}] - Q^{\\pi_t} ||_* \\leq \\zeta$\n$\\mathbb{E}_{\\Xi_t}||Q^*_t - Q^{\\pi_t} ||^2 \\leq \\sigma^2$\n$\\mathbb{E}_{\\Xi_t}||Q^*_t || \\leq \\mathcal{Q}^2$.\nThe assumption (4.1) bounds the bias, while (4.2) bounds the variance. Clearly, when $\\sigma = 0$, then we have exact information about $Q^{\\pi_t}$. Our results are meaningful in the regime where the bias is small"}, {"title": "4.1. Basic stochastic policy mirror descent", "content": "Stochastic policy mirror descent (SPMD) is the same as PMD (Algorithm 3.1) except the exact Q-function in (3.2) is replaced with a stochastic one", "h^p(s)": "D_\\pi(s)\\"}, "n= argmin_p{\\in \\pi}\\{\\eta_t \\psi^*_t (s,p) + D_\\pi(s)\\}, \\forall s \\in S,$\nwhere for any $p\\in \\Delta_{|A|}$ and $s \\in S$, the stochastic advantage function is defined as $\\psi^*_t(s,p) = \\langle Q^*_t (s,\\cdot), p\\rangle - \\langle Q^{\\pi_t}(s,\\cdot), \\pi_t(\\cdot|s)\\rangle + h^p(s) - h^{\\pi_t(s)}$.\nWe start by proving a descent lemma under noise.\nLEMMA 4.4. For any fixed $\\pi$,\n$(1 - \\gamma) [V^{\\pi_t}(s) - V^{\\pi}(s)"], "pi)": "Q^*_t (q, \\cdot) - Q^{\\pi_t} (q, \\cdot), \\pi_t(\\cdot|q) - \\pi(\\cdot|q))$.\nProof. We have\n$(1 - \\gamma)[V^{\\pi_t}(s) - V^{\\pi}(s)]$\n$\\overset{(2.7)}{=} \\mathbb{E}_{\\kappa^\\pi} [\\langle Q^{\\pi_t} (q,\\cdot), \\pi_t(\\cdot|q) - \\pi(\\cdot|q)\\rangle + h^{\\pi_t} (q) - h^{\\pi} (q)]$\n$= \\mathbb{E}_{\\kappa^\\pi} [\\langle Q^{\\pi_t} (q, \\cdot), \\pi_{t+1}(\\cdot"}