{"title": "TIME-MOE: BILLION-SCALE TIME SERIES FOUNDATION MODELS WITH MIXTURE OF EXPERTS", "authors": ["Xiaoming Shi", "Shiyu Wang", "Yuqi Nie", "Dianqi Li", "Zhou Ye", "Qingsong Wen", "Ming Jin"], "abstract": "Deep learning for time series forecasting has seen significant advancements over the past decades. However, despite the success of large-scale pre-training in language and vision domains, pre-trained time series models remain limited in scale and operate at a high cost, hindering the development of larger capable forecasting models in real-world applications. In response, we introduce TIME-MOE, a scalable and unified architecture designed to pre-train larger, more capable forecasting foundation models while reducing inference costs. By leveraging a sparse mixture-of-experts (MoE) design, TIME-MOE enhances computational efficiency by activating only a subset of networks for each prediction, reducing computational load while maintaining high model capacity. This allows TIME-MOE to scale effectively without a corresponding increase in inference costs. TIME-MOE comprises a family of decoder-only transformer models that operate in an auto-regressive manner and support flexible forecasting horizons with varying input context lengths. We pre-trained these models on our newly introduced large-scale data Time-300B, which spans over 9 domains and encompassing over 300 billion time points. For the first time, we scaled a time series foundation model up to 2.4 billion parameters, achieving significantly improved forecasting precision. Our results validate the applicability of scaling laws for training tokens and model size in the context of time series forecasting. Compared to dense models with the same number of activated parameters or equivalent computation budgets, our models consistently outperform them by large margin. These advancements position TIME-MOE as a state-of-the-art solution for tackling real-world time series forecasting challenges with superior capability, efficiency, and flexibility.", "sections": [{"title": "1 INTRODUCTION", "content": "Time series data is a major modality in real-world dynamic systems and applications across various domains. Analyzing time series data is challenging due to its inherent complexity and distribution shifts, yet it is crucial for unlocking insights that enhance predictive analytics and decision-making. As a key task in high demand, time series forecasting has long been studied and is vital for driving various use cases in fields such as energy, climate, education, quantitative finance, and urban computing. Traditionally, forecasting has been performed in a task-specific, end-to-end manner using either statistical or deep learning models. Despite their competitive performance, the field has not converged on building unified, general-purpose forecasting models until recently, with the emergence of a few foundation models (FMs) for universal forecasting. Although promising, they are generally small in scale and have limited task-solving capabilities compared to domain-specific models, limiting their real-world impact when balancing forecasting precision against computational budget.\nAnswering this question drives the design of TIME-MOE, a scalable and unified architecture for pre-training larger, more capable forecasting FMs while reducing computational costs. TIME-MOE consists of a family of decoder-only transformer models with a mixture-of-experts architecture, operating in an auto-regressive manner to support any forecasting horizon and accommodate context lengths of up to 4096. With its sparsely activated design, TIME-MOE enhances computational efficiency by activating only a subset of networks for each prediction, reducing computational load"}, {"title": "2 RELATED WORK", "content": "Time Series Forecasting. Deep learning models have become powerful tools for time series forecasting over the past decade, which can be broadly categorized into two types: (1) univariate models, such as DeepState, DeepAR, and N-BEATS, which focus on modeling individual time series, and (2) multivariate models, which include both transformer-based approaches and non-transformer models, designed to handle multiple time series simultaneously. While these models achieve competitive in-domain performance, many are task-specific and fall short in generalizability when applied to cross-domain data in few-shot or zero-shot scenarios.\nLarge Time Series Models. Pre-training on large-scale sequence data has significantly advanced modality understanding in language and vision domains. Building on this progress, self-supervised learning has been extensively developed for time series, employing masked reconstruction or contrastive learning. However, these methods are limited in both data and model scale, with many focused on in-domain learning and transfer. Recently, general pre-training of time series models on large-scale data has emerged, though still in its early stages with insufficient exploration into sparse solutions. We discuss the development more in Appendix A. Unlike these dense models, TIME-MOE introduces a scalable and unified architecture for pre-training larger forecasting foundation models, which is also more capable while maintaining the same scale of activated parameters or computational budget as dense models.\nSparse Deep Learning for Time Series. Deep learning models are often dense and over-parameterized, leading to increased memory and computational demands during both training and inference. However, sparse networks, such as mixture-of-experts models, which dynamically route inputs to specialized expert networks, have shown comparable or even superior generalization to dense models while being more efficient. In time series research, model sparsification has received less attention, as time series models have traditionally been small in scale, with simple models like DLinear and SparseTSF excelling in specific tasks prior to the advent of large-scale, general pre-training. The most relevant works on this topic include Pathformer , MoLE, and IME. However, none of them delve into"}, {"title": "3 METHODOLOGY", "content": "Our proposed TIME-MOE, illustrated in Figure 2, adopts a mixture-of-experts-based, decoder-only transformer architecture, comprising three key components: (1) input token embedding, (2) MoE transformer block, and (3) multi-resolution forecasting. For the first time, we scale a sparsely-activated time series model to 2.4 billion parameters, achieving significantly better zero-shot performance with the same computation. This marks a major step forward in developing large time series models for universal forecasting.\nProblem Statement. We address the problem of predicting future values in a time series: given a sequence of historical observations $X_{1:T} = (x_1,x_2,...,x_T) \\in \\mathbb{R}^T$ spanning $T$ time steps, our objective is to forecast the next $H$ time steps, i.e., $X_{T+1:T+H} = f_\\theta (X_{1:T}) \\in \\mathbb{R}^H$. Here, $f_\\theta$ represents a time series model, where $T$ is the context length and $H$ is the forecasting horizon. Notably, both $T$ and $H$ are arbitrary when inferencing TIME-MOE, distinguishing it from task-specific models with fixed configurations. Additionally, channel independence is adopted to transform a multivariate input into univariate series, allowing TIME-MOE to handle any-variate forecasting problems in real-world applications."}, {"title": "3.1 TIME-MOE OVERVIEW", "content": "Input Token Embedding. We utilize point-wise tokenization for time series embedding to ensure the completeness of temporal information. This enhances our model's flexibility and broad applicability in handling variable-length sequences. Then, we employ SwiGLU to embed each time series point:\n$h_t = SwiGLU(x_t) = Swish (Wx_t) \\odot (Vx_t),$                                                       (1)\nwhere $W \\in \\mathbb{R}^{D\\times1}$ and $V \\in \\mathbb{R}^{D\\times1}$ are learnable parameters, and $D$ denotes the hidden dimension."}, {"title": "MoE Transformer Block.", "content": "Our approach builds upon a decoder-only transformer and integrates recent advancements from large language models. We employ RMSNorm to normalize the input of each transformer sub-layer, thereby enhancing training stability. Instead of using absolute positional encoding, we adopt rotary positional embeddings, which provide greater flexibility in sequence length and improved extrapolation capabilities. In line with , we remove biases from most layers but retain them in the QKV layer of self-attention to improve extrapolation. To introduce sparsity, we replace a feed-forward network (FFN) with a mixture-of-experts layer, incorporating a shared pool of experts that are sparsely activated.\n$u_t = SA (RMSNorm (h_{t-1})) + h_{t-1}$,                                                   (2)\n$\\overline{u}_t = RMSNorm (u_t),$                                                                             (3)\n$h_t = Mixture (\\overline{u}_t) + u_t.$                                                                           (4)\nHere, $SA$ denotes self-attention with a causal mask, and $Mixture$ refers to the mixture-of-experts layer. In practice, $Mixture$ comprises several expert networks, each mirroring the architecture of a standard FFN. An individual time series point can be routed to either a single expert or multiple experts. One expert is designated as a shared expert to capture and consolidate common knowledge across different contexts.\n$Mixture (\\overline{u}_t) = g_{N+1,t} FFNN_{N+1} (\\overline{u}_t) + \\sum_{i=1}^{N}(g_{i,t} FFNN_i (\\overline{u}_t)),$               (5)\n$g_{i,t} = \\begin{cases}S_{i,t}, S_{i,t} \\in Topk({s_{j,t}|1 \\leq j \\leq N}, K), \\\\0,  otherwise,\\end{cases}$           (6)\n$g_{N+1,t} = Sigmoid (W\\overline{u}_t),$                                                                         (7)\n$s_{i,t} = Softmax (W\\overline{u}_t),$                                                                           (8)\nwhere $W\\in \\mathbb{R}^{1\\times D}$ denotes the trainable parameters, and $N$ and $K$ respectively denote the numbers of non-shared experts and activated non-shared experts per MoE layer."}, {"title": "Multi-resolution Forecasting.", "content": "We introduce a novel multi-resolution forecasting head, which allows for forecasting at multiple scales simultaneously, in contrast to existing foundation models that are limited to a single fixed scale. This capability enhances TIME-MOE 's flexibility by enabling it to generate forecasts across various horizons. The model uses multiple output projections from single-layer FFNs, each designed for different prediction horizons. By incorporating a simple greedy scheduling algorithm (see Appendix B), TIME-MOE efficiently handles predictions across arbitrary horizons. This design also boosts prediction robustness through multi-resolution ensemble learning. Additionally, TIME-MOE aggregates forecasting errors from different horizons to compute a composite loss (Section 3.2.2), thereby improving the model generalization."}, {"title": "3.2 MODEL TRAINING", "content": ""}, {"title": "3.2.1 TIME-300B DATASET", "content": "Training time series foundation models requires extensive, high-quality data. However, large-scale datasets that are adequately processed remain scarce. Recent advancements have facilitated the collection of numerous time series datasets from various sources. Nonetheless, data quality remains a challenge, with prevalent issues such as missing values and invalid observations that can impair model performance and destabilize training. To mitigate these issues, we developed a streamlined data-cleaning pipeline (Appendix C) to filter and refine raw data, and constructed the largest open-access, high-quality time series data collection named Time-300B for model pre-training.\nTime-300B is composed of a diverse range of publicly available datasets, spanning multiple domains such as energy, retail, healthcare, weather, finance, transportation, and web, as well as a portion of synthetic data to enhance the data quantity and diversity. Time-300B covers a wide spectrum of sampling frequencies, spanning from seconds to yearly intervals, and has over 300 billion time points after being processed by our data-cleaning pipeline, as summarized in Table 1."}, {"title": "3.2.2 LoSS FUNCTION", "content": "Pre-training time series foundation models in large scale presents significant challenges in training stability due to the massive datasets and the vast number of parameters involved. To address this, we use the Huber loss, which provides greater robustness to outliers and improves training stability.\n$L_{ar} (x_t, \\hat{x}_t) = \\begin{cases} \\frac{1}{2} (x_t - \\hat{x}_t)^2, & \\text{if } |x_t - \\hat{x}_t| < \\delta, \\\\  \\delta \\times (|x_t - \\hat{x}_t| - \\frac{1}{2} \\times \\delta), & \\text{otherwise,} \\end{cases}$                                                                              (9)\nwhere $\\delta$ is a hyperparameter that balances the L1 and L2 loss components. When training the model with a MoE architecture, focusing solely on optimizing prediction error often leads to load imbalance issues among the experts. A common problem is routing collapse, where the model predominantly selects only a few experts, limiting training opportunities for others. To mitigate this, following the approaches of , we achieve expert-level balancing with an auxiliary loss to reduce routing collapse:\n$L_{aux} = N \\sum_{i=1}^{N} f_i r_i, f_i = \\frac{1}{KT} \\sum_{t=1}^{T} \\mathbb{I} (\\text{Time point } t \\text{ selects Expert } i), r_i = \\frac{1}{T} \\sum_{t=1}^{T} s_{it}$                                                              (10)\nwhere $f_i$ represents the fraction of tokens assigned to expert $i$, and $r_i$ denotes the proportion of router probability allocated to expert $i$. $\\mathbb{I}$ is the indicator function. Finally, we combine the auto-regressive losses across all multi-resolution projections with the auxiliary balance loss to form the final loss:\n$L = \\frac{1}{P} \\sum_{i=1}^{P} L_{ar} (X_{t+1:t+p_i}, \\hat{X}_{t+1:t+P_i}) + L_{aux},$                                                    (11)"}, {"title": "3.2.3 MODEL CONFIGURATIONS AND TRAINING DETAILS", "content": "Informed by the scaling laws demonstrated by Llama, which show that a 7- or 8-billion parameter model continues to improve performance even after training on over one trillion tokens, we chose to scale TIME-MOE up to 2.4 billion parameters with around 1 billion of them activated. This model, TIME-MOEultra, supports inference on consumer-grade GPUs with less than 8GB of VRAM. We have also developed two smaller models: TIME-MOEbase, with 50 million activated parameters, and TIME-MOElarge, with 200 million activated parameters, both specifically designed for fast inference on CPU architectures. These streamlined models are strategically developed to ensure broader accessibility and applicability in resource-constrained environments. The detailed model configurations are in Table 2. Each model undergoes training for 100, 000 steps with a batch size of 1024, where the maximum sequence length is capped at 4096. This setup results in the consumption of 4 million time points per iteration. We choose {1, 8, 32, 64} as different forecast horizons in the output projection and set the factor of the auxiliary loss $\\alpha$ to 0.02. For optimization, we employ the AdamW optimizer, configured with the following hyperparameters: lr = 1e-3, weight_decay = 0.1, $\\beta_1$ = 0.9, $\\beta_2$ = 0.95. A learning rate scheduler with a linear warmup over the initial 10,000 steps followed by cosine annealing is also utilized. Training is conducted on 128 \u00d7 NVIDIA A100-80G GPUs using BF16 precision."}, {"title": "4 MAIN RESULTS", "content": "TIME-MOE consistently outperforms state-of-the-art forecasting models by large margins across six well-established benchmarks and settings (Appendix B). We compared TIME-MOE against a wide range of time series models, including several recently released foundation models for forecasting. To ensure a fair comparison, we adhered to the experimental configurations from for out-of-distribution forecasting and for in-distribution forecasting, applying a unified evaluation pipeline that we developed.\nBaselines. We evaluate TIME-MOE against 14 different baselines, representing state-of-the-art models in long-term forecasting. These baselines are categorized into two groups. The first group, for zero-shot forecasting evaluation, includes pre-trained foundation models such as Moirai, TimesFM, Timer , Moment, and Chronos . The second group, for in-distribution (full-shot) forecasting evaluation, consists of deep time series models such as iTransformer, TimeMixer, TimesNet, PatchTST, Crossformer , TiDE, DLinear, and FEDformer."}, {"title": "4.1 ZERO-SHOT FORECASTING", "content": "Setup. Time series foundation models have recently demonstrated impressive zero-shot learning capabilities . In this section, we conducted experiments on the six well-known long-term forecasting benchmarks for which datasets were not included in the pre-training corpora. We use four different prediction horizons, which are {96,192, 336, 720}, with the corresponding input time series lengths {512, 1024, 2048, 3072}. The evaluation metrics adopt mean square error (MSE) and mean absolute error (MAE).\nResults. Detailed results of zero-shot forecasting are in Table 3. TIME-MOE achieves consistent state-of-the-art performances, improving a large margin as MSE reduction in average exceeding 23% over the other most competitive baselines. Importantly, as the model size scales (e.g., base \u2192 ultra), it continuously exhibits enhanced performance across all datasets, affirming the efficacy of scaling laws within our time series foundation models. Furthermore, in comparisons with robust baselines possessing a similar number of activated parameters, TIME-MOE demonstrates significantly superior performance. Compared to Moirailarge and Chronoslarge, TIME-MOE achieved"}, {"title": "4.2 IN-DISTRIBUTION FORECASTING", "content": "Setup. We fine-tune the pre-trained TIME-MOE models on the train split of the above-mentioned six benchmarks and set the number of training epochs to only one.\nResults. The full results are in Table 4. TIME-MOE exhibits remarkable capabilities, comprehensively surpassing advanced deep time series models from recent years, achieving an average MSE reduction of 25%. Fine-tuning on downstream data with only one epoch significantly improves predictive performance, showcasing the remarkable potential of large time series models built on the MoE architecture. Similar to zero-shot forecasting, as the model size increases, the scaling law continues to be effective, leading to continuous improvements in the performance of the TIME-MOE."}, {"title": "4.3 ABLATION STUDY", "content": "To validate our designs in TIME-MOE, we conducted detailed ablation studies on key architectural components and loss functions across all experimental benchmarks, as shown in Table 5.\nModel Architecture. Replacing the MoE layers with standard FFNs (w/o mixture-of-experts) led to an average performance drop from 0.245 to 0.255, highlighting the performance boost provided by the sparse architecture. A detailed comparison of dense and sparse models is presented in Section 4.4. We retained only the horizon-32 output layer by eliminating the other multi-resolution"}, {"title": "4.4 SCALABILITY ANALYSIS", "content": "Dense versus Sparse Models. To assess the performance and efficiency benefits of sparse architectures in time series forecasting, we replaced the MoE layer with a dense layer containing an equivalent number of parameters as the activated parameters in the MoE layer. Using identical training setup and data, we trained three dense models corresponding to the sizes of the three TIME-MOE models. A zero-shot performance comparison between the dense and sparse models is shown in Figure 3. Our approach reduced training costs by an average of 78% and inference costs by 39% compared to dense variants. This clearly demonstrates the advantages of TIME-MOE, particularly in maintaining exceptional performance while significantly reducing costs.\nModel and Data Scaling. We save model checkpoints at intervals of every 20 billion time points during training, allowing us to plot performance traces for models of different sizes trained on various data scales. The right side of Figure 3 shows that models trained on larger datasets consistently outperform those trained on smaller datasets, regardless of model size. Our empirical results confirm that as both data volume and model parameters scale, sparse models demonstrate continuous and substantial improvements in performance, as well as achieve better forecasting accuracy compared to the dense counterparts under the same scales.\nTraining Precision. We trained a new model, TIME-MOEbase (FP32), using identical configurations but with float32 precision instead of bfloat16. As shown in Table 6, the forecasting performance of both models is comparable. However, the bfloat16 model achieves a 12% improvement in training speed and reduces memory consumption by 20% compared to the float32 model. Moreover, the"}, {"title": "4.5 SPARSIFICATION ANALYSIS", "content": "Activation Visualization. As shown in Figure 4, TIME-MOE dynamically activates different experts across various datasets, with each expert specializing in learning distinct knowledge. This leads to diverse activation patterns across datasets from different domains, showcasing TIME-MOE'S strong generalization capabilities. The heterogeneous activations indicate that the model adapts its learned representations to the specific characteristics of each dataset, contributing to its great transferability and generalization as a large-scale time series foundation model.\nNumber of Experts. We also performed a sensitivity analysis on the number of experts, represented as topk, within the MoE architecture, as shown in Table 7. As k increases, performance shows only marginal changes, with minimal improvements in average MSE. However, inference time increases noticeably as more experts are utilized. This indicates that increasing sparsity within the MoE architecture does not compromise performance but significantly enhances computational efficiency. This balance is critical for scaling large models, where optimizing performance and computational cost is essential. Sparse MoE architectures inherently offer advantages in these areas."}, {"title": "5 CONCLUSION", "content": "In this paper, we introduced TIME-MOE, a scalable and unified architecture for time series foundation models that leverages a sparse design with mixture-of-experts to enhance computational efficiency without compromising model capacity. Pre-trained on our newly introduced large-scale time series dataset, Time-300B, TIME-MOE was scaled to 2.4 billion parameters, with 1.1 billion activated, demonstrating significant improvements in forecasting accuracy. Our results validate the scaling properties in time series forecasting, showing that TIME-MOE consistently outperforms dense models with equivalent computational budgets across multiple benchmarks. With its ability"}, {"title": "A FURTHER RELATED WORK", "content": "In this section, we delve deeper into the related work on large time series models. Current research efforts in universal forecasting with time series foundation models can be broadly classified into three categories, as summarized in Table 8: (1) encoder-only models, such as Moirai and Moment, which employ masked reconstruction and have been pre-trained on datasets containing 27B and 1B time points, respectively, with model sizes reaching up to 385M parameters; (2) encoder-decoder models, exemplified by Chronos, which offers pre-trained models at four scales, with up to 710M parameters; and (3) decoder-only models, including TimesFM, Lag-Llama, and Timer, with the largest models containing up to 200M parameters. In contrast to these dense models, TIME-MOE introduces a scalable, unified architecture with a sparse mixture-of-experts design, optimized for larger time series forecasting models while reducing inference costs. Trained on our Time-300B dataset, comprising over 300B time points, TIME-MOE is scaled to 2.4B parameters for the first time. It outperforms existing models with the same number of activated parameters, significantly enhancing both model efficiency and forecasting precision, while avoiding limitations such as fixed context lengths or hardcoded heuristics."}, {"title": "B IMPLEMENTATION DETAILS", "content": "Training Configuration. Each model is trained for 100,000 steps with a batch size of 1,024, and a maximum sequence length capped at 4,096. This setup processes 4 million time points per iteration. We use forecast horizons of {1, 8, 32, 64} in the output projection and set the auxiliary loss factor $\\alpha$ to 0.02. For optimization, we apply the AdamW optimizer with the following hyperparameters: lr = 1e-3, weight_decay = 1e-1, $\\beta_1$ = 0.9, and $\\beta_2$ = 0.95. A learning rate scheduler with a linear warmup for the first 10,000 steps, followed by cosine annealing, is used. Training is performed on 128 \u00d7 NVIDIA A100-80G GPUs with BF16 precision. To improve batch processing efficiency and handle varying sequence lengths, we employ sequence packing, which reduces padding requirements.\nBenchmark Details. We evaluate the performance of various models for long-term forecasting across eight well-established datasets, including the Weather, Electricity, and ETT datasets (ETTh1, ETTh2, ETTm1, ETTm2). A detailed description of each dataset is provided in Table 9.\nMetrics. We use mean square error (MSE) and mean absolute error (MAE) as evaluation metrics for time-series forecasting. These metrics are calculated as follows:\n$MSE = \\frac{1}{H} \\sum_{i=1}^{H} (x_i - \\hat{x_i})^2,$\n$MAE = \\frac{1}{H} \\sum_{i=1}^{H} |x_i - \\hat{x_i}|,$\nwhere $x_i, \\hat{x_i} \\in \\mathbb{R}$ are the ground truth and predictions of the i-th future time point."}, {"title": "C PROCESSED DATA ARCHIVE", "content": "Going beyond previous work, we organized a comprehensive large-scale time series dataset from a vast collection of complex raw data. To ensure data quality, we addressed issues by either imputing missing values or discarding malformed time series. Inspired by data processing techniques from large language models, we developed a fine-grained data-cleaning pipeline specifically designed for time series data:\nMissing Value Processing. In time series data, missing values often appear as 'NaN' (Not a Number) or 'Inf' (Infinity). While previous studies commonly address this by replacing missing values"}, {"title": "D ADDITIONAL RESULTS", "content": ""}, {"title": "D.1 ABLATION STUDY", "content": "As shown in Table 11, replacing the MoE layers with standard FFNs (denoted as \"w/o mixture-of-experts \") led to a noticeable performance decline, with the MSE worsening from 0.245 to 0.255. This highlights the significant contribution of the sparse architecture to the model's overall performance, as its dynamic routing enables more specialized processing of diverse input patterns.\nWe also conducted experiments by retaining only the horizon-32 forecasting head from the TIME-MoEbase (denoted as \"w/o multi-resolution layer\"), excluding the multi-task optimization. The performance of this modified model was slightly inferior to the complete TIME-MoEbase, highlighting the importance of multi-resolution outputs in capturing various temporal dependencies."}, {"title": "D.2 TRAINING PRECISION ANALYSIS", "content": "To optimize model performance and efficiency, we conducted a comparative study examining the impact of numerical precision during training. We trained two versions of our model under identical configurations, with the only difference being the precision: one using bfloat16 and the other using float32. The model trained with float32 precision is referred to as TIME-MOEbase w/ FP32.\nAs detailed in Table 6, our analysis reveals that the forecasting performances of these two models are remarkably comparable. This finding is significant as it demonstrates that the use of reduced precision (e.g., bfloat16) does not compromise the predictive capabilities of our model."}, {"title": "E FORECAST SHOWCASES", "content": "To visualize the performance differences among various large-scale time series models, we present the forecasting results of our model, TIME-MOE, in comparison to the ground truth across six real-world benchmarks. These benchmarks include ETTh1, ETTh2, ETTm1, ETTm2, Weather, and Electricity datasets. Alongside TIME-MOE's results, we also show the performance of other large-scale baseline models at different scales, providing a comprehensive view of their comparative capabilities (Figures 5 \u2013 10). In all figures, the context length is set to 512, and the forecast horizon is 96. To enhance clarity and aesthetics, we display the full forecast output, complemented by a portion of the preceding historical input data, ensuring a more intuitive comparison.\nThe results clearly demonstrate the superiority of TIME-MOE over the other foundational models. Its ability to consistently produce more accurate forecasts across a range of datasets underscores the effectiveness of its architecture and design. The performance gains are especially noticeable in long-term prediction scenarios, where TIME-MOE's handling of temporal dependencies proves more robust than its counterparts. These visual comparisons highlight the practical advantages of TIME-MOE in large-scale time series forecasting, reinforcing its status as a state-of-the-art model."}]}