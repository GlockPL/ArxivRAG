{"title": "Archetypal Analysis for Binary Data", "authors": ["A. Emilie J. Wedenborg", "Morten M\u00f8rup"], "abstract": "Archetypal analysis (AA) is a matrix decomposition method that\nidentifies distinct patterns using convex combinations of the data\npoints denoted archetypes with each data point in turn reconstructed\nas convex combinations of the archetypes. AA thereby forms a poly-\ntope representing trade-offs of the distinct aspects in the data. Most\nexisting methods for AA are designed for continuous data and do\nnot exploit the structure of the data distribution. In this paper, we\npropose two new optimization frameworks for archetypal analysis\nfor binary data. i) A second order approximation of the AA likeli-\nhood based on the Bernoulli distribution with efficient closed-form\nupdates using an active set procedure for learning the convex combi-\nnations defining the archetypes, and a sequential minimal optimiza-\ntion strategy for learning the observation specific reconstructions. ii)\nA Bernoulli likelihood based version of the principal convex hull\nanalysis (PCHA) algorithm originally developed for least squares\noptimization. We compare these approaches with the only existing\nbinary AA procedure relying on multiplicative updates and demon-\nstrate their superiority on both synthetic and real binary data. No-\ntably, the proposed optimization frameworks for AA can easily be\nextended to other data distributions providing generic efficient opti-\nmization frameworks for AA based on tailored likelihood functions\nreflecting the underlying data distribution.", "sections": [{"title": "1. INTRODUCTION", "content": "Archetypal Analysis (AA) [1] is a matrix decomposition technique\nknown for identifying unique characteristics called archetypes.\nThese archetypes, which represent the vertices of an optimally\nlearned polytope, are constrained to lie within the convex hull of the\ndata, forming what is known as the principal convex hull [2]. This\ndistinctive feature of AA enables it to capture the most prominent\naspects of the data, offering an easily interpretable model where each\ndata point is represented as a combination of trade-offs between the\narchetypes. Despite its potential, finding the optimal archetypes is\na challenging problem that involves solving a non-convex optimiza-\ntion problem. A solution is to split the problem into smaller convex\nquadratic programming (QP) subproblems that are iteratively al-\nternatingly solved to converge to a (local) minima. Some of the\nattempts to optimize this procedure include fast gradient projection\nmethods based on the principal convex hull analysis (PCHA)[2],\noptimized loss functions, where a Huber loss function is used to-\ngether with an iterative, reweighed least squares strategy [3], faster\nQP-solvers [3], decoupling of data and archetypes, enforcing unit\nnormalization and learning the convex hull inside the unit sphere [4]\nand data driven selection and processing steps for faster convergence\n[5, 2, 6, 7]. These extensions have been tailored for continuous data\nsources based on least squares minimization. Discrete data types\nhave not been explored as extensively in Archetypal Analysis. While\nprevious efforts have aimed at expressing the model in probabilistic\nterms [8], with extensions to discrete distributions, these endeavors\nhave relied on multiplicative updates that are known to converge\nslowly [9, 10], involved transforming the data [11], or used AA\nwith archetypes constrained to actual cases [12]. In this paper,\nwe propose two efficient optimization frameworks for archetypal\nanalysis for binary data. Firstly, we propose a novel framework for\nAA that exploits i) the sparsity of the convex combinations learned\nto define the archetypes by use of an active set algorithm [13] tai-\nlored to convex constraints and ii) the low dimensional structure\nof the matrix forming their reconstruction which we demonstrate\ncan be efficiently solved by sequential minimal optimization (SMO)\noriginally used in the context of support vector machines [14]. We\nthereby establish efficient closed-form updates for the two convex\nsub-problems used alternatingly to solve for AA. We exploit how\nthese updates can be applied to arbitrary likelihood specifications by\nuse of second order likelihood expansions. While our focus is on\nBernoulli distributed binary data, the framework can easily be ex-\npanded to any likelihood function. We identify and demonstrate the\nsame attribute in the Principal Convex Hull Algorithm (PCHA) [2]\nwhere we derive new gradients tailored for a Bernoulli Likelihood.\nSpecifically, we:\n\u2022 Derive an efficient procedure for AA inference exploring\nsparsity of active set defining the convex combinations form-\ning the archetypes and SMO updates for the low-dimensional\nobservation specific reconstruction defining convex combi-\nnations of the archetypes.\n\u2022 Use the approach for generic AA optimization based on\nquadratic expansion of AA likelihoods.\n\u2022 Extend the PCHA algorithm to Bernoulli likelihood optimiza-\ntion enabling principled analyses using PCHA of binary data.\n\u2022 Showcase on synthetic and real datasets the superiority of the\noptimization procedure when compared to the existing multi-\nplicative updates for Bernoulli likelihood optimization."}, {"title": "2. METHODS", "content": "2.1. Likelihood Optimized Archetypal Analysis\nFor a data matrix of features by observations $X \\in \\mathbb{R}^{M \\times N}$, the pri-\nmary goal of archetypal analysis (AA) is to decompose the data ac-\ncording to [1]:"}, {"title": "2.2. S-update by Sequential Minimal Optimization", "content": "Sequential Minimal Optimization (SMO) was first proposed in [14].\nThis method splits large Quadratic Problems (QP) into the smallest\npossible QP problems. The method is especially suited for updating\nthe S matrix as this is typically a low-dimensional dense matrix, rep-\nresenting the data in terms of convex combinations of the archetypes.\nIn eq. (4) (10) the generic framework for an arbitrary distribution\nis shown. By applying a second order Taylor expansion to the likeli-\nhood functions specified in the equations 2 and 3, the expression for\nthe likelihood in terms dependent on S decouples into independent\ncolumn specific terms $f (s_j)$ given by"}, {"title": "2.3. C-update by an Active Set Procedure", "content": "To optimize C a fast non-negative least squares algorithm (FNNLS)[13]\nis used. This enables us to utilize the sparsity of C. As originally\nproposed by [15] we apply a linear constraint directly into an active\nset algorithm imposing a quadratic penalty $1(1 \u2212 \\sum_j C_{jk})^2$. We\ninclude a small regularization $\\epsilon \\Sigma_j c_{jk}^2$ to ensure the associated\nHessian of the QP is full rank. To define a suitable QP problem\nfor the optimization method a Taylor expansion is applied to the\nlikelihood, similar to the S-update the general form becomes:"}, {"title": "2.4. Principal Convex Hull Algorithm for Bernoulli Likelihood", "content": "For completeness, we propose a simple modification to the princi-\npal convex hull algorithm (PCHA) [2], where we replace the least\nsquares loss and derived gradients with the following gradients and\nloss based on the Bernoulli log-likelihood (eq. 3) denoted B-PCHA\nBased on these gradients, the update rules become"}, {"title": "2.5. Normalized Mutual Information", "content": "AA is in general unique [2] and to evaluate model consistency Nor-\nmalized Mutual Information (NMI) is used. This method was in-\ntroduced to AA in [16]. This method evaluates the columns in S\nas probability distributions and compares the consistency across 10\nruns. The Mutual Information is normalized between 0 and 1, there-\nfore an NMI of one indicates that the model converges to the same\nsolution (up to permutation of the components) regardless of how\nthe C and S matrices are initialized."}, {"title": "3. RESULTS AND DISCUSSION", "content": "To demonstrate the proposed optimization framework's efficiency\nand convergence properties we first evaluate it on two synthetic data\nsets respectively based on continuous data (Gaussian distribution\nlikelihood corresponding to conventional least squares optimization)\nand binary data (Bernoulli distribution likelihood corresponding to\ncross-entropy minimization). The data was generated to have eight\narchetypes, 800 features and 1000 samples. We compare our pro-\nposed inference procedure to two existing AA inference method-\nologies - the PCHA algorithm for least squares (Gaussian) [2] and\nmultiplicative update procedure for binary (Bernoulli) [8] data. The\nresults from the synthetic study can be seen in Fig. 1. It is clear that\nour SMO-AS and B-PCHA frameworks exhibits faster convergence\nthan the multiplicative updates [8] while producing similar quality\nof loss-solutions (L(X, R)) Fig. 1(b) and (e) as well as model con-\nsistencies (NMI) Fig. 1(c) and (f). We further evaluated the models\non a binary data set of drugs and their side effects (SIDER) [17] in\nwhich each entry defines the presence or absence of a given side-\neffect for a given drug. The data matrix, after being filtered for the\nMedDRA preferred terms, contains 1347 samples (drugs) and 5868\nfeatures (side effects) with a sparsity of 98.3%. The results for the\nreal dataset can be seen in Fig. 3. We again observe that our generic\noptimization procedures clearly outperforms the multiplicative up-\ndate method in terms of speed and convergence.\nDetermining the optimal number of archetypes remains an open\nproblem. For the simulated data in Fig. 1 in which we know the\ntrue number of simulated archetypes we observe that the correct\nnumber of archetypes follows a regime in which the model sub-\nstantially improves in loss (L(X, R)) by inclusion of archetypes\nwhereas model consistency (NMI) is high. For the real dataset the\nnumber of archetypes considering these two aspects is less evident\nbut inspecting L(X, R) and NMI we observe that the three compo-\nnent models exhibits high NMI and substantial loss improvements.\nWe therefore display the inferred model structure for K = 3 compo-\nnents (Fig. 3(a) and (e)). In Fig 3(e) we observe that the archetypes\nprimarily delineate drugs' tendencies to include side effects such that\narchetype 1 and 3 represent drugs with many as opposed to archetype\n1 representing drugs with few side effects. Whether or not further in-\nsights about the archetypes can be made we leave for future work.\nThe code was run on a Intel Xeon Gold 6226R[18]. While both\nthe B-PCHA and the SMO-AS models are shown to converge faster\nboth in terms of speed and iterations (fig. 3 (a) and (d)) when com-\npared to the multiplicative updates it is nearly impossible to do a\nfair runtime comparison. Our SMO-AS framework allows for triv-\nial parallelization and supports GPU acceleration. We have chosen\nnot to include this in order to make the comparison as fair as possi-\nble and we find that the SMO-AS approach in run-time outperforms\nboth the multiplicative updates and B-PCHA procedures. Notably,\nthe Sequential Minimal Optimization (SMO) framework for S is re-\nmarkably efficient for a small number of archetypes. However, the\nefficiency of the active set algorithm used to update C heavily de-\npends on the size of the active set and in situations where the ac-\ntive set becomes large our active set procedure is inefficient. One\napproach to control the maximal size of the active set could be to\nexplore the furthestsum method proposed in [2] to select a reduced\nset of observations used to form the archetypes. This method can\nbe used to substantially restrict the number of observations used to\ndefine the archetypes and thereby the maximal size of the active set,\nbut it may render the model susceptible to archetypes simply de-\nfined by outliers. Alternatively, the active set size could be restricted\nduring the analysis such that the active set is only allowed to grow\nto a certain size. Future work should investigate how this can be\nimplemented while ensuring convergence. The most important ad-\nvantage of the proposed optimization frameworks for AA is their\ngenerality, enabling the optimization to work for arbitrary data dis-\ntribution. This both has the benefit that the model is easy to use, but\nalso that convergence is guaranteed from the convexity of the alter-\nnating S and C optimization problems and associated closed-form\nupdates not relying on any hyper-parameter tuning such as gradient\nstep sizes [2] or necessitating likelihood tailored convergence deriva-\ntions as required for multiplicative updates [8, 19]."}, {"title": "4. CONCLUSION", "content": "We presented two novel frameworks for archetypal analysis, one\nthat leverages the structure of the data distribution to derive efficient\nclosed-form updates for the AA model exploring that C when sparse\ncan be efficiently inferred using an active set procedure whereas\nS when low-dimensional can be efficiently inferred using sequen-\ntial minimal optimization (SMO). The second method expanded the\nPCHA framework to Bernoulli data. We showed how to instantiate\nour frameworks for continuous (Gaussian) and binary (Bernoulli)\ndata and demonstrated its effectiveness on both synthetic and real.\nThe SMO approach due to its $K^2$ scaling is especially suitable in\nthe typical scenario where $K < 50$. The active set procedure to\nestimate C is suitable, provided that the active set remains small. In\nsituations where this is not the case, we recommend using the pro-\nposed gradient-based B-PCHA approach for binary data. We also\ncompared our approach with two prominent existing methods for\narchetypal analysis for Gaussian [2] and Bernoulli [8] likelihood\ninference and confirmed that our procedure exhibits efficient opti-\nmization as function of alternating iterations updating C and S. Our\nframework can be easily extended to other data distributions by use\nof appropriate likelihood functions."}], "equations": [{"equation": "\\min_{c,s} L(X, R) \\text{ s.t. }\\\\\nR = XCS\\\\\nC_{j,k} \\geq 0, S_{k,j} \\geq 0\\\\\n\\sum_j C_{j,k} = 1, \\sum_k S_{k,j} = 1"}, {"equation": "L = \\sum_{i,j} ||(X_{i,j} - r_{i,j})||_F,"}, {"equation": "L = -\\sum_{i,j} x_{i,j}\\ln(r_{i,j}) \u2013 (1 \u2013 x_{i,j}) \\ln(1 \u2013 r_{i,j})."}, {"equation": "f(s_j) \\approx \\text{const.} \u2013 d_s s_j + s_j^T H s_j"}, {"equation": "d_{k,j} = -2(\\sum_{i,m} C_{m,k}x_{i, m}x_{i, j} + \\sum_{l} C_{l,k}X_{i, l}r_{i,l}) \u2013 (\\sum_{k\u2019,k\u2019\u2019}h_{k\u2019,k\u2019\u2019}S_{j,k\u2019}),"}, {"equation": "h_{k\\\u2019,k\u2019\u2019}^{(j)} = \\sum_{l,l\u2019,i} C_{l,k\\\u2019}x_{i,l\u2019}x_{i,l\\\u2019}C_{l\\\u2019,k\u2019\u2019},"}, {"equation": "d_{k,j} = \\sum_{i}(\\frac{x_{i,j}}{r_{i,j}} - \\frac{(1 - x_{i,j})}{(1 - r_{i,j})}) S_{j,k},"}, {"equation": "h_{k\\\u2019,k\u2019\u2019}^{(j)} = \\sum_{i}(\\frac{x_{i,j}}{r_{i,j}^2} + \\frac{(1 \u2013 x_{i,j})}{(1 \u2013 r_{i,j})^2}) P_{i,m\u2019}C_{m\u2019,k\u2019} P_{i,m\u2019\u2019}C_{m\u2019\u2019,k\u2019\u2019}."}, {"equation": "h_{kk\\\u2019}^{(j)} = \\sum_{i}(\\frac{x_{i,j}}{r_{i,j}^2} + \\frac{(1 \u2013 x_{i,j})}{(1 \u2013 r_{i,j})^2}) (\\sum_{P_{i,m\u2019}C_{m\u2019,k\u2019}})^2"}, {"equation": "\\begin{bmatrix}\nt_j \\alpha_j \\\\\nt_j (1-\\alpha_j)\n\\end{bmatrix}^T\\begin{bmatrix}\nh_{kk\u2019}^{(j)} & h_{k\\\u2019,k\u2019\u2019}^{(j)} \\\\\nh_{k\\\u2019,k\u2019\u2019}^{(j)} & h_{kk\u2019\u2019}^{(j)} \\\\\n\\end{bmatrix}\\begin{bmatrix}\nt_j \\alpha_j \\\\\nt_j (1-\\alpha_j)\n\\end{bmatrix} + \\begin{bmatrix}\nt_j \\alpha_j \\\\\nt_j (1-\\alpha_j)\n\\end{bmatrix}^T\\begin{bmatrix}\nd_{k\u2019j}^{(j)}\\\\\nd_{k\u2019\u2019j}^{(j)}\n\\end{bmatrix} + \\text{~const}"}, {"equation": "\u03b1^{(j)} = \\frac{t_j^2(h_{kk\u2019}^{(j)} + h_{kk\u2019\u2019}^{(j)} - 2h_{k\u2019,k\u2019\u2019}^{(j)}) -2t_jd_{kj}^{(j)} + 2t_jd_{k\u2019\u2019j}^{(j)}}{2t_j(h_{kk\u2019}^{(j)}+h_{kk\u2019\u2019}^{(j)} - 2h_{k\u2019,k\u2019\u2019}^{(j)})}"}, {"equation": "f(c_k) \\approx \\text{const.}\u2212 (d_k+\\lambda_k1)^T c_k+\\frac{1}{2} c_k^T (H^{(k)} +A\\lambda_{11}+ \\epsilon_kI) c_k"}, {"equation": "d_{j,k} = \\sum_{i,j\u2019}x_{i,j\u2019}x_{i,j}S_{k,j\u2019} \u2013 \\sum_{i,j\u2019}x_{i,j\u2019}r_{i,j}S_{k,j\u2019} + \\sum_{j\u2019j\u2019\u2019}h_{j\u2019j\u2019\u2019}C_{j,k},"}, {"equation": "h_{j\\\u2019,j\u2019\u2019}^{(j)} = \\sum_{i}x_{i, j\u2019}x_{i,j\u2019\u2019}S_{k,j\u2019}."}, {"equation": "d_{j,k} = \\sum_{i,j\u2019}P_{i,j} (\\frac{x_{i,j\u2019}}{r_{i,j\u2019}} - \\frac{(1 - x_{i,j\u2019})}{(1 - r_{i,j\u2019})})S_{k,j\u2019} + \\sum_{j\u2019j\u2019\u2019}h_{j\u2019j\u2019\u2019}C_{j,k}, h_{j\\\u2019,j\u2019\u2019}^{(j)} = \\sum_{i}P_{i,j\u2019}P_{i,j\u2019\u2019} (\\frac{x_{i,j\u2019}}{r_{i,j\u2019}^2} + \\frac{(1 \u2013 x_{i,j\u2019})}{(1 \u2013 r_{i,j\u2019})^2})S_{k,j}."}, {"equation": "g_{j,k}= \\sum_{i,j\u2019}P_{ij\u2019}(\\frac{x_{i, j\u2019}}{r_{i,j\u2019}} \u2013 \\frac{(1-x_{i,j\u2019})}{(1-r_{i,j\u2019})})"}, {"equation": "g_{S}=\\sum_{i,j\u2019}P_{i,j\u2019} (\\frac{r_{i,j}}{r_{i,j\u2019}} - \\frac{(1 - x_{i,j\u2019})}{(1-r_{i,j\u2019})})C_{j\u2019l,k}"}, {"equation": "C_{j,k} \\leftarrow \\max \\{ \\tilde{C}_{j,k} \u2013 \u03bc_{C} g_{j,k}, 0 \\} (1 \u2013 \\sum_{l} C_{jl,k})"}, {"equation": "S_{k,j} \\leftarrow \\max \\{ \\tilde{S}_{k,j} \u2013 \u03bc_{S} g_{kj}, 0 \\} (1 \u2013 \\sum_{k\u2019} S_{k\u2019,j})"}]}