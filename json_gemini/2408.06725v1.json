{"title": "Enhancing Visual Dialog State Tracking through Iterative Object-Entity Alignment in Multi-Round Conversations", "authors": ["Wei Pang", "Ruixue Duan", "Jinfu Yang", "Ning Li"], "abstract": "Visual Dialog (VD) is a task where an agent answers a series of image-related questions based on a multi-round dialog history. However, previous VD methods often treat the entire dialog history as a simple text input, disregarding the inherent conversational information flows at the round level. In this paper, we introduce Multi-round Dialogue State Tracking model (MDST), a framework that addresses this limitation by leveraging the dialogue state learned from dialog history to answer questions. MDST captures each round of dialog history, constructing internal dialogue state representations defined as 2-tuples of vision-language representations. These representations effectively ground the current question, enabling the generation of accurate answers. Experimental results on the VisDial v1.0 dataset demonstrate that MDST achieves a new state-of-the-art performance in generative setting. Furthermore, through a series of human studies, we validate the effectiveness of MDST in generating long, consistent, and human-like answers while consistently answering a series of questions correctly.", "sections": [{"title": "1 Introduction", "content": "Vision-language based multi-modal tasks have gained significant attention at the intersection of computer vision and natural language processing. Tasks such as Visual Question Answering (VQA) and Visual or Video Dialogue require the fusion of visual and textual information. Among these tasks, Visual Dialog (VD) poses a unique challenge that goes beyond simple question answering grounded in an image. VD involves comprehending conversational language, navigating through multi-round dialog history, and reasoning based on visual and textual contents to generate coherent answers.\nWhile previous methods in VD have made progress, they often overlook the inherent information flows and round-level interactions within the dialog history. Existing models commonly concatenate the entire dialog history into a single text sequence, lacking explicit focus on the most relevant"}, {"title": "2 Related Work", "content": "Dealing with dialog history as a simple text input in Visual Dialog (VD) has been a prevailing practice since the works of LF, HCIAE, and LTMI. However, recent research has highlighted the limitations of such approaches in explicitly capturing round-level interactions between dialog rounds. Existing work can be categorized into three groups based on their handling of dialog history.\nFirstly, attention-based models typically encode each round of history separately to obtain a set of history embeddings. Sequential attention is applied in HCIAE to attend to the history and image sequentially. MCA leverages modular co-attention to fuse visual and textual modalities. DMRM utilizes dual-attention mechanisms to resolve textual and visual"}, {"title": "3 Model", "content": "Figure 1 presents an overview of our Multi-Round Dialogue State Tracking (MDST) model, which consists of four main modules: Question Encoder (QEncoder), Question Grounding on Dialogue State (QGDS), Answer Decoder (ADer), and Postdiction on Dialogue State (PDS). In the remainder of this section, we go into more detail on each module.\nProblem Formulation Given an image I and a multi-round dialogue history H = C, (q(1), a(1)), . . ., (q(t-1), a(t-1)) up to round t \u2212 1, where C represents the image caption and (q(), a()) denotes the previously experienced question-answer pairs, the dialogue agent aims to respond to the current question q(t) at round t. This response involve generating a free-form natural language answer in a generative setting.\nWe first extract object-level image features using Faster-RCNN. For each image, we select the top-ranked N objects, each of them is of size 2048-dim and projected to low-dimension features with size d through a linear layer as:\n$O^I = RCNN(I)$,\n$O^{(0)} = LayerNorm(ReLU(W_oO^I + b_o))$,\nwhere LayerNorm is the layer normalization, $W_o$ and $b_o$ are learnable parameters, $O^{(0)} \\in R^{N \\times d}$ represents a set of object-level image features. Furthermore, we insert two special pseudo-object features: NULL ($\\epsilon$) and ALL ($\\chi$), where NULL is a zero vector of size d, and ALL denotes the representation of the whole image by taking the mean of $O^{(0)}$. Thus, we get a new set of N +2 object features of $O^{(0)} \\in R^{(N+2)\\times d} = O^{(0)} \\cup \\{\\epsilon\\} \\cup \\{\\chi\\}$. For clarity, we omit all the biases in the remainder of this section.\nLet $<O^{(0)}, S^{(0)}>$ denote the initial dialogue state, where $S^{(0)} \\in R^{(N+2)\\times d}$ is initialized as a set of zero vectors of the same size. In our approach, the image caption is treated as the zeroth round QA pair $C^{(0)}$, which serves as the initialization for $S^{(0)}$ at the beginning of the dialogue.\nQuestion Encoder (QEncoder) For encoding both the question and the image caption, we employ a standard Transformer encoder. This encoder generates contextual representations, denoted as $q^{(t)}$ and $C^{(0)}$, where I represents the length of the question or caption. It is worth noting that, for simplicity, we use the same symbol to represent both the textual string and its corresponding representation.\nQuestion Grounding on Dialogue State (QGDS) QGDS aims to ground current question $q^{(t)}$ in dialogue state, yielding question-related textual clues on language states and visual clues on vision states. To better associate question with vision and language states, three probability distributions are designed: word-entity alignment between question words and language states, word-object alignment between question words and vision states, and switching probability. Before going into detail, we introduce a notation to express a non-linear transformation layer, to which dropout regularization and layer normalization are applied:\n$MLP(x) = LayerNorm(Dropout(GELU(Wx)))$,\nwhere x is the input: a vector or a matrix, with learnable weight W of the size varying with the input.\nBecause many textual relations (e.g., co-reference) existing in question and previous history, our model will associate current question words with its most related dialog entities in language states using a learnable word-entity alignment distribution $\\pi^{(t)} \\in R^{l \\times N}$ in Eq.4. To ground current question in an image, we then calculate a cross-modal matching as in Eq.5,\n$\\pi_l^{(t)} = softmax(MLP(q^{(t)}) \\cdot MLP(S^{(t)})^T/\\sqrt{d})$,  \n$\\pi_v^{(t)} = softmax(MLP(q^{(t)}) \\cdot MLP(O^{(0)})^T/\\sqrt{d})$,   \nwhere $\\pi_v \\pi^{(t)} \\in R^{l \\times N}$ represents word-object alignment distribution between question words and objects in vision states.\nSwitching probability is designed to 1) determine whether current question is related to previous dialog history; 2) provide a weight to fuse two alignment distributions because there is one-to-one correspondence (i.e., object-entity) between vision and language states:\n$\\varphi^{(t)} = sigmoid(\\frac{w(\\frac{(MLP(q^{(t)})MLP(S^{(t)})^T)}{\\sqrt{N+2}}).mean}{11})$,  \nwhere mean takes the mean on the I dimension with trainable parameter w E $\\mathbb{R}^{(N+2)\\times 1}$. $\\varphi^{(t)} \\in [0,1]$ is a weight measured the relationship between question and dialog history. The larger value of $\\varphi^{(t)}$, the less relevant current question is to dialog history. Experiment shows introducing $\\varphi^{(t)}$ can contribute to better the final performance.\nThe question-guided textual context $\\Delta q_l^{(t)}$ is obtained by a weighted sum of language stats over both word-entity and word-object alignment distributions, as denoted in Eq.7:\n$\\Delta q_l^{(t)} = S^{(t)} (\\pi_l^{(t)} + \\varphi^{(t)}\\pi_v^{(t)})$, \n$\\Delta q_v^{(t)} = O^{(0)} (\\pi_v^{(t)} + (1 - \\varphi^{(t)})\\pi_v^{(t)})$,  \nwhere $\\Delta q_l^{(t)} \\in R^{l \\times d}$ represents history context relevant to current question composed of two parts. The first part consists of an explicit history attention directly from question to language states, while the second part contains an aligned history attention indirectly from question, via vision states, to language states, weighted by switching probability. Similarly, the question-guided visual context is written as in Eq.8, where $\\Delta q_v^{(t)} \\in R^{l \\times d}$ represents the focused visual regions relevant to current question from two parts, including an explicit visual attention from question to vision states and an implicit ones via the cross-modal alignment.\nFinally, we use the sum of the three components to denote the final question representation as in $q^{(t)} + \\Delta q_l^{(t)} + \\Delta q_v^{(t)}$, which is decoded in next ADer module.\nAnswer Decoder (ADer) In ADer, we utilize a standard Transformer decoder as the backbone for the generative setting. It takes the final question representation, obtained by combining the question representation q(t) with the question-guided textual context $\\Delta q_l^{(t)}$ and the question-guided visual context $\\Delta q_v^{(t)}$, as input. The decoder autoregressively generates the next word one by one until it encounters an end-of-sequence token, producing a free-form natural answer. Formally, the ADer module can be expressed as:\n$a^{(t)} = Decoder(q^{(t)} + \\Delta q_l^{(t)} + \\Delta q_v^{(t)})$, \nwhere $a^{(t)}$ represents the output of the decoder, which not only represents the free-form natural answer of length l, but also denotes its contextualized representations over the words: $a^{(t)} \\in R^{l \\times d}$. The decoder progressively generates each word based on the input representation."}, {"title": "4 Experiment", "content": "Datasets and Evaluation We conduct our experiments on the VisDial v1.0 dataset, which consists of a standard train/val/test split. To evaluate the performance, we employ NDCG and retrieval metrics, including MRR, Mean rank, and R@1, 5, 10, following the conventions of previous studies . Additionally, we assess the quality of generated answers by generating 2064 dialogues for 10 rounds on the VisDial v1.0 validation set and calculating the following metrics: Joint Answer Accuracy (JACC) measures the percentage of correct QA pairs among all the generated QA pairs. It assesses whether the generated answers are correct given the corresponding images. Average Answer Length (AvgLen) calculates the average length of generated answers.\nImplementation Details We utilize a Transformer encoder-decoder architecture as the backbone. The encoder and decoder consist of 12 layers with 12 heads"}, {"title": "5 Conclusions", "content": "In this paper, we introduce a novel approach called Multi-Round Dialogue State Tracking Network (MDST) for the task of Visual Dialog (VD). Unlike previous methods that treat dialog history as a simple text input, MDST tracks and updates dialogue states, which are 2-tuple aligned vision-language representations. By modeling the inherent interactions at the round level, MDST aims to capture dynamics of the conversation more effectively. Experimental results on VisDial v1.0 dataset demonstrate that MDST achieves state-of-the-art performance across most evaluation metrics. Additionally, extensive human studies further validate MDST can generate long, consistent, and human-like answers while maintaining the ability to provide correct responses to a series of questions.\nOverall, our proposed MDST framework represents a significant advancement in visual dialog systems, showcasing the importance of modeling dialogue states in capturing the complex nature of visual conversations."}]}