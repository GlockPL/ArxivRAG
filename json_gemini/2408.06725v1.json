{"title": "Enhancing Visual Dialog State Tracking through Iterative Object-Entity Alignment in Multi-Round Conversations", "authors": ["Wei Pang", "Ruixue Duan", "Jinfu Yang", "Ning Li"], "abstract": "Visual Dialog (VD) is a task where an agent answers a series of image-related questions based on a multi-round dialog history. However, previous VD methods often treat the entire dialog history as a simple text input, disregarding the inherent conversational information flows at the round level. In this paper, we introduce Multi-round Dialogue State Tracking model (MDST), a framework that addresses this limitation by leveraging the dialogue state learned from dialog history to answer questions. MDST captures each round of dialog history, constructing internal dialogue state representations defined as 2-tuples of vision-language representations. These representations effectively ground the current question, enabling the generation of accurate answers. Experimental results on the VisDial v1.0 dataset demonstrate that MDST achieves a new state-of-the-art performance in generative setting. Furthermore, through a series of human studies, we validate the effectiveness of MDST in generating long, consistent, and human-like answers while consistently answering a series of questions correctly.", "sections": [{"title": "1 Introduction", "content": "Vision-language based multi-modal tasks have gained significant attention at the intersection of computer vision and natural language processing. Tasks such as Visual Question Answering (VQA) [2], and Visual or Video Dialogue [27,11,18] require the fusion of visual and textual information. Among these tasks, Visual Dialog (VD) [11] poses a unique challenge that goes beyond simple question answering grounded in an image. VD involves comprehending conversational language, navigating through multi-round dialog history, and reasoning based on visual and textual contents to generate coherent answers.\nWhile previous methods in VD have made progress, they often overlook the inherent information flows and round-level interactions within the dialog history. Existing models [12,15,21,28,20,7,8,5] commonly concatenate the entire dialog history into a single text sequence, lacking explicit focus on the most relevant history clues. Although attention mechanisms, such as sequential attention [19], co-attention [1,31], dual-attention[9], and multi-view attention [25], have been proposed, they still treat each round of the dialog history independently.\nTo address these limitations, we propose the multi-round dialogue state track-ing model (MDST) for Visual Dialog. Unlike prior work, MDST explicitly models the round-level interactions in dialog history. We define the dialogue state in VD as a 2-tuple of vision and language states, where vision states capture object-level representations and language states represent dialog entity-level representations.\nIn MDST, each round question from the dialog history is processed, ground-ing the question in the dialogue state to yield question-guided visual-textual clues. These clues are then used to decode accurate answers, while updating the dialogue states accordingly. Notably, vision states remain unchanged throughout the dialogue, while language states are updated in each round. We align the vi-sion and language representations in dialogue states in an object-entity fashion, facilitating the grounding of follow-up questions.\nExperimental results on the VisDial v1.0 dataset demonstrate that our pro-posed model achieves state-of-the-art performance in generative setting. Further examinations reveal that MDST consistently answers questions correctly, with a joint answer accuracy (JACC) of 79.8% in the generative setting. Moreover, MDST generates human-like responses, as validated through human studies. To summarize, our contributions are three-fold:\nWe propose a novel multi-round dialogue state tracking model (MDST) for Visual Dialog. The MDST, including representations of image objects and representations of dialog entities, models the inherent interactions in dialog history at the round level.\nWe achieve new state-of-the-art results on most evaluation metrics on Vis-Dial v1.0, and find that the alignment of vision-language in dialogue states could improve the final performance significantly.\nWe introduce JACC to evaluate the answer quality, and find that our MDST can continuously generate correct answers as proved by JACC of 79.8% that means about 8 rounds in 10 are correct on VisDial v1.0 val."}, {"title": "2 Related Work", "content": "Dealing with dialog history as a simple text input in Visual Dialog (VD) has been a prevailing practice since the works of LF [11], HCIAE [19], and LTMI [21]. However, recent research has highlighted the limitations of such approaches in explicitly capturing round-level interactions between dialog rounds [23,24]. Existing work can be categorized into three groups based on their handling of dialog history.\nFirstly, attention-based models [11,19,29,22,9,13,17,30,25,1] typically encode each round of history separately to obtain a set of history embeddings. Sequential attention is applied in HCIAE [19] to attend to the history and image sequen-tially. MCA [1] leverages modular co-attention to fuse visual and textual modal-ities. DMRM [9] utilizes dual-attention mechanisms to resolve textual and visual co-references. MVAN [25] introduces multi-view attention to fuse the question and history at both the sentence and word level.\nSecondly, graph-based models [14,6,32] construct a graph representation of the entire dialog history, where each node represents a question-answer (QA) pair. KBGN [14] and LTMI-GoG [6] establish edges between nodes to indicate coreference relations between QA pairs. However, these graph-based approaches can suffer from scalability issues as the graph size grows with the dialogue.\nThirdly, concatenation-based models treat the entire dialog history as a single sentence. DualVD [15] packs the dialog history into a long string encoded by an LSTM. UTC [5], ICMU [8], and LTMI [21] concatenate each QA pair as a text sequence, separated by a special token (e.g., [SEP]), and input them into a transformer encoder.\nIn summary, prior approaches for handling dialog history in VD have not explicitly modeled interactions at the round level of granularity. This limitation hinders their ability to capture the nuanced dynamics of multi-round dialogues."}, {"title": "3 Model", "content": "Figure 1 presents an overview of our Multi-Round Dialogue State Track-ing (MDST) model, which consists of four main modules: Question Encoder (QEncoder), Question Grounding on Dialogue State (QGDS), Answer Decoder (ADer), and Postdiction on Dialogue State (PDS). In the remainder of this sec-tion, we go into more detail on each module.\nProblem Formulation Given an image I and a multi-round dialogue history \\(H = C, (q^{(1)}, a^{(1)}), . . ., (q^{(t-1)}, a^{(t-1)})\\) up to round \\(t \u2212 1\\), where C represents the image caption and \\((q^{(.)}, a^{(.)})\\) denotes the previously experienced question-answer pairs, the dialogue agent aims to respond to the current question \\(q^{(t)}\\) at round t. This response involve generating a free-form natural language answer in a generative setting.\nWe first extract object-level image features using Faster-RCNN [3]. For each image, we select the top-ranked N objects, each of them is of size 2048-dim and projected to low-dimension features with size d through a linear layer as:\n\\[O^{f} = RCNN(I),\\]\n\\[O^{(0)} = LayerNorm(ReLU(W_{o}O^{f} + b_{o})),\\]\nwhere LayerNorm is the layer normalization [4], \\(W_{o}\\) and \\(b_{o}\\) are learnable param-eters, \\(O^{(0)} \u2208 R^{N\u00d7d}\\) represents a set of object-level image features. Furthermore, we insert two special pseudo-object features: NULL (\\(e\\)) and ALL (\\(x\\)), where NULL is a zero vector of size d, and ALL denotes the representation of the whole image by taking the mean of \\(O^{(0)}\\). Thus, we get a new set of \\(N + 2\\) object features of \\(O^{(0)} \u2208 R^{(N+2)\u00d7d} = O^{(0)} \u222a {e} \u222a {x}\\). For clarity, we omit all the biases in the remainder of this section.\nLet \\(<O^{(0)}, S^{(0)}>\\) denote the initial dialogue state, where \\(S^{(0)} \u2208 R^{(N+2)\u00d7d}\\) is initialized as a set of zero vectors of the same size. In our approach, the image caption is treated as the zeroth round QA pair \\(C^{(0)}\\), which serves as the initialization for \\(S^{(0)}\\) at the beginning of the dialogue.\nQuestion Encoder (QEncoder) For encoding both the question and the im-age caption, we employ a standard Transformer encoder [26]. This encoder gen-erates contextual representations, denoted as \\(q^{(t)}\\) and \\(C^{(0)}\\), where I represents the length of the question or caption. It is worth noting that, for simplicity, we use the same symbol to represent both the textual string and its corresponding representation.\nQuestion Grounding on Dialogue State (QGDS) QGDS aims to ground current question \\(q^{(t)}\\) in dialogue state, yielding question-related textual clues on language states and visual clues on vision states. To better associate question with vision and language states, three probability distributions are designed: word-entity alignment between question words and language states, word-object alignment between question words and vision states, and switching probability. Before going into detail, we introduce a notation to express a non-linear trans-formation layer, to which dropout regularization and layer normalization are applied:\n\\[MLP(x) = LayerNorm(Dropout(GELU(Wx))), \\]\nwhere x is the input: a vector or a matrix, with learnable weight W of the size varying with the input.\nBecause many textual relations (e.g., co-reference) existing in question and previous history [11], our model will associate current question words with its most related dialog entities in language states using a learnable word-entity alignment distribution \\(\u03c0^{(t)} \u2208 R^{l\u00d7N}\\) in Eq.4. To ground current question in an image, we then calculate a cross-modal matching as in Eq.5,\n\\[\u03c0_{l}^{(t)} = softmax(MLP(q^{(t)}) \u00b7 MLP(S^{(t)})^{T}/\\sqrt{d}),\\]\n\\[\u03c0_{v}^{(t)} = softmax(MLP(q^{(t)}) \u00b7 MLP(O^{(0)})^{T}/\\sqrt{d}),\\]\nwhere \\(\u03c0_{v}\u03c0^{(t)} \u2208 R^{l\u00d7N}\\) represents word-object alignment distribution between question words and objects in vision states.\nSwitching probability is designed to 1) determine whether current question is related to previous dialog history; 2) provide a weight to fuse two alignment distributions because there is one-to-one correspondence (i.e., object-entity) be-tween vision and language states:\n\\[\u03c6^{(t)} = sigmoid(\\frac{w(MLP(q^{(t)})MLP(S^{(t)})^{T}).mean}{\\sqrt{N+2}}),\\]\nwhere mean takes the mean on the I dimension with trainable parameter \\(w \u2208 IR^{(N+2)\u00d71}\\). \\(\u03c6^{(t)} \u2208 [0,1]\\) is a weight measured the relationship between question and dialog history. The larger value of \\(\u03c6^{(t)}\\), the less relevant current question is to dialog history. Experiment shows introducing \\(\u03c6^{(t)}\\) can contribute to better the final performance.\nThe question-guided textual context \\(\u2206q_{l}^{(t)}\\) is obtained by a weighted sum of language stats over both word-entity and word-object alignment distributions, as denoted in Eq.7:\n\\[\u2206q_{l}^{(t)} = S^{(t)} (\u03c0_{l}^{(t)} + \u03c6^{(t)}\u03c0_{v}^{(t)}),\\]\n\\[\u2206q_{v}^{(t)} = O^{(0)} (\u03c0_{v}^{(t)} + (1 \u2212 \u03c6^{(t)})\u03c0_{v}^{(t)}),\\]\nwhere \\(\u2206q_{l}^{(t)} \u2208 R^{l\u00d7d}\\) represents history context relevant to current question composed of two parts. The first part consists of an explicit history attention directly from question to language states, while the second part contains an aligned history attention indirectly from question, via vision states, to language states, weighted by switching probability. Similarly, the question-guided visual context is written as in Eq.8, where \\(\u2206q_{v}^{(t)} \u2208 R^{l\u00d7d}\\) represents the focused visual regions relevant to current question from two parts, including an explicit visual attention from question to vision states and an implicit ones via the cross-modal alignment.\nFinally, we use the sum of the three components to denote the final question representation as in \\(q^{(t)} + \u2206q_{l}^{(t)} + \u2206q_{v}^{(t)}\\), which is decoded in next ADer module.\nAnswer Decoder (ADer) In ADer, we utilize a standard Transformer de-coder as the backbone for the generative setting. It takes the final question representation, obtained by combining the question representation \\(q^{(t)}\\) with the question-guided textual context \\(\u2206q_{l}^{(t)}\\) and the question-guided visual context \\(\u2206q_{v}^{(t)}\\), as input. The decoder autoregressively generates the next word one by one until it encounters an end-of-sequence token, producing a free-form natural answer. Formally, the ADer module can be expressed as:\n\\[a^{(t)} = Decoder(q^{(t)} + \u2206q_{l}^{(t)} + \u2206q_{v}^{(t)}),\\]\nwhere \\(a^{(t)}\\) represents the output of the decoder, which not only represents the free-form natural answer of length l, but also denotes its contextualized representations over the words: \\(a^{(t)} \u2208 R^{l\u00d7d}\\). The decoder progressively generates each word based on the input representation."}, {"title": "4 Experiment", "content": "Datasets and Evaluation We conduct our experiments on the VisDial v1.0 dataset, which consists of a standard train/val/test split. To evaluate the per-formance, we employ NDCG and retrieval metrics, including MRR, Mean rank, and R@1, 5, 10, following the conventions of previous studies [8,5]. Additionally, we assess the quality of generated answers by generating 2064 dialogues for 10 rounds on the VisDial v1.0 validation set and calculating the following metrics: Joint Answer Accuracy (JACC) measures the percentage of correct QA pairs among all the generated QA pairs. It assesses whether the generated answers are correct given the corresponding images. Average Answer Length (AvgLen) calculates the average length of generated answers.\nImplementation Details We utilize a Transformer encoder-decoder architec-ture as the backbone. The encoder and decoder consist of 12 layers with 12 heads since the dialogue states are updated in PDS but used in QGDS. The combi-nation of the QGDS and PDS modules provides strong support for the tracking mechanism.\nWhen removing the switching probability \\(\u03b1^{(t)}\\) in the QGDS module, we ob-serve a significant decrease in overall performance. NDCG and MRR decrease by 1.39 and 0.7 points, respectively. This result underscores the importance of the switching probability in our model. Specifically, the switching probability plays a crucial role in associating the two alignment distributions (\\(\u03c0_{l}^{(t)}\\) and \\(\u03c0_{v}^{(t)}\\)), facilitating the alignment of vision-language dialogue states. In other words, aligning vision-language states brings about a substantial improvement, which aligns with findings from previous studies [25,10].\nFurthermore, when removing the two pseudo-object features, NULL and ALL (Row 4), we observe a slight decline in performance. This finding validates that both pseudo-objects carry useful information about the image. The inclusion of these pseudo-objects is valuable because the upcoming question may be unre-lated to the input image or may involve the entire image."}, {"title": "5 Conclusions", "content": "In this paper, we introduce a novel approach called Multi-Round Dialogue State Tracking Network (MDST) for the task of Visual Dialog (VD). Unlike previ-ous methods that treat dialog history as a simple text input, MDST tracks and updates dialogue states, which are 2-tuple aligned vision-language representa-tions. By modeling the inherent interactions at the round level, MDST aims to capture dynamics of the conversation more effectively. Experimental results on VisDial v1.0 dataset demonstrate that MDST achieves state-of-the-art per-formance across most evaluation metrics. Additionally, extensive human studies further validate MDST can generate long, consistent, and human-like answers while maintaining the ability to provide correct responses to a series of questions. Overall, our proposed MDST framework represents a significant advancement in visual dialog systems, showcasing the importance of modeling dialogue states in capturing the complex nature of visual conversations."}]}