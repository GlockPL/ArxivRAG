{"title": "MAGMAX: Leveraging Model Merging for Seamless Continual Learning", "authors": ["Daniel Marczak", "Bart\u0142omiej Twardowski", "Tomasz Trzci\u0144ski", "Sebastian Cygert"], "abstract": "This paper introduces a continual learning approach named MAGMAX, which utilizes model merging to enable large pre-trained models to continuously learn from new data without forgetting previously acquired knowledge. Distinct from traditional continual learning methods that aim to reduce forgetting during task training, MAGMAX combines sequential fine-tuning with a maximum magnitude weight selection for effective knowledge integration across tasks. Our initial contribution is an extensive examination of model merging techniques, revealing that simple approaches like weight averaging and random weight selection surprisingly hold up well in various continual learning contexts. More importantly, we present MAGMAX, a novel model-merging strategy that enables continual learning of large pre-trained models for successive tasks. Our thorough evaluation demonstrates the superiority of MAGMAX in various scenarios, including class- and domain-incremental learning settings.", "sections": [{"title": "1 Introduction", "content": "Large pre-trained models are considered cornerstones of complex machine learning systems, allowing unprecedented performance improvements across many challenging tasks [1,2,17,31,41,50]. Yet their remarkable ability to generalize to unseen conditions is intrinsically limited by the stationary character of their training data. To keep up with the ever-changing world, these models should adapt continuously and assimilate knowledge from the stream of new data, which is the objective of Continual Learning (CL) [21, 24, 39].\nTraditionally, CL approaches used regularization to retain the knowledge from previous tasks [18,23], grow the network while learning new tasks [32,48], or use a replay buffer to limit the catastrophic forgetting [12, 42, 52]. In this work, we argue that in the era of machine learning systems built on top of large"}, {"title": "2 Related Work", "content": "Continual learning (CL) is a setting where models learn a sequence of tasks with access to the data from the current task only. The goal is to achieve high performance on all the tasks from the sequence, with catastrophic forgetting of knowledge learned from the previous tasks being the main challenge [7,26]. One prominent example of CL approaches are the regularization-based methods. In EWC [18], the authors propose to use the Fisher information matrix to estimate model weight importance (for previous tasks) which is then used to penalize changes of important model weight. On the other hand, regularization can be applied on the data level, e.g. LwF [23] or DER [48] penalizes changes in model predictions or features. Other CL approaches include adding more parameters as the number of tasks increases [32,49], or using memory buffer [12, 42, 48, 52] for data from old tasks, which is often undesirable due to the privacy concerns. In general, it seems that the best results are obtained by CL methods that favor stability, that is the model does not change much between consecutive learning tasks [16,33]. As a result, a plethora of methods were developed for CL scenarios which assumed large first task [30,53], or Large Pre-trained Model (LPM).\nContinual Learning of LPMs became popular as capabilities (e.g., zero shot or out-of-distribution (OOD) performance) of foundation models became apparent [1,2, 17, 31, 41, 50]. A recent study questioned the utility of some CL methods, showing that by using a frozen model and nearest mean classifier can"}, {"title": "3 Background and motivation", "content": "We consider a problem of continual learning of large pre-trained models. We assume access to a pre-trained model parametrized by $d$ weights $\\theta_0 \\in \\mathbb{R}^d$. Our goal is to adapt the model to a sequence of disjoint tasks {$\\mathcal{D}_1, \\mathcal{D}_2, ..., \\mathcal{D}_n$} one task at a time. We investigate exemplar-free scenario which assumes no access to data from previous tasks.\nWe consider two fine-tuning scenarios:\nindependent (Ind FT) - starts from pre-trained weights $\\theta_0$,\nsequential (Seq FT) - starts from the weights of the model fine-tuned on the sequence of previous tasks, i.e. when fine-tuning on task $\\mathcal{D}_t$, we start from $\\theta_{t-1}$ which was trained on {$\\mathcal{D}_1, \\mathcal{D}_2, ..., \\mathcal{D}_{t-1}$}.\nWe use a notion of task vector [13] that is an element-wise difference between the fine-tuned model and the pre-trained model, i.\u03b5. $\\tau_i = \\theta_i - \\theta_0$. Note that independently fine-tuned task vectors contain information about a single task and sequentially fine-tuned task vectors encompass some knowledge about all the tasks in the sequence."}, {"title": "3.2 Motivation", "content": "In this Section, we set and experimentally validate two hypotheses that serve as a motivation for developing a new method for continual learning via model merging."}, {"title": "4 MAXIMUM MAGNITUDE SELECTION", "content": "Based on the motivations introduced in the previous Section, we introduce MAXIMUM MAGNITUDE SELECTION (MAGMAX). It is a novel method for continual learning that utilizes sequential fine-tuning, following H2, and model merging based on selecting the parameters of the highest magnitude, following H1 (see Algorithm 1). Given a new task, $\\mathcal{D}_t$, our method consists of two steps:\n1. Sequential adaptation: We obtain the new weights of the model $\\theta_t$ by fine-tuning it on $\\mathcal{D}_t$. Importantly, we start from the weights of the model fine-tuned on previous tasks $\\theta_{t-1}$.\n2. Knowledge consolidation: We consolidate task-specific knowledge using model merging. Firstly, we create task vectors for all tasks seen so far: {$\\tau_i\\}_{i=1}^t$, where $\\tau_i = \\theta_i - \\theta_0$. Then, for each parameter $p \\in {1, 2, ..., d}$, we select the value $\\tau_{\\text{MAGMAX}}^p$ by the maximum magnitude out of all the task vectors. Lastly, we apply the resulting task vector $\\tau_{\\text{MAGMAX}}$ to the pre-trained model $\\theta_{\\text{MAGMAX}} = \\theta_0 + \\lambda * \\tau_{\\text{MAGMAX}}$, where $\\lambda$ is a scaling factor."}, {"title": "5 Experimental setup", "content": "Datasets. For class-incremental learning (CIL) experiments we use CIFAR100 [20] and ImageNet-R [11] as generic image recognition benchmarks and CUB200 [40] and Cars [19] as fine-grained classification datasets. We split the datasets into N equal subsets of disjoint classes, where N \u2208 {5, 10, 20, 50} for generic benchmarks and N \u2208 {5, 10, 20} for fine-grained benchmarks (which contain less data).\nTo compare between class- and domain-incremental learning (DIL) we use ImageNet-R and DomainNet [29]. For domain-incremental learning experiments, we split DomainNet into 6 tasks by their domain (clipart, infographics, painting, quickdraw, real and sketch) and ImageNet-R into 15 tasks by their renditions"}, {"title": "6 Main results", "content": "Class-incremental learning. Table 1 presents the comparison of MAGMAX with CL methods and merging-based baselines on various class-incremental learning benchmarks. MAGMAX consistently outperforms the competitors across the scenarios that vary in number of tasks and dataset granularity, achiev-ing on average 2.1% better results than the second best method. Interestingly, simple baselines that merge independent fine-tunings by averaging (Avg) or even randomly mixing (RandMix) the weights, are close competitors to CL meth-ods and other merging strategies.\nDomain-incremental learning. Table 2 presents the results on domain- incremental learning benchmarks. MAGMAX outperforms other merging strategies in every scenario. It also achieves results on par with CL methods, outperforming them on ImageNet-R but slightly underperforming on DomainNet. We also observe that the top-performing methods achieve higher performance in domain-incremental scenarios than in class-incremental.\nMerging by k-th magnitude. In this section, we experimentally justify the choice of maximum magnitude when merging models. We perform experiments where we merge task vectors by selecting the parameters that have k-th highest"}, {"title": "10", "content": "performance does not change) or concurrent (they serve concurrent task-specific purposes). However, for sequential fine-tuning, the performance decreases as k increases. It means that parameters with high magnitude are better indicators of the beneficial update direction than parameters with lower magnitude.\nIn this Section we set and verify the following hypothesis: parameters which update directions were consistent across tasks tend to have higher magnitude. We define an update direction as a sign of parameter change when trained on a given task, $sgn(\\tau_i^0) = sgn(\\theta_i - \\theta_{i-1})$. For each parameter in each sequentially fine-tuned task vector, we calculate the number of consistent update directions n. Figure 6 presents the relation of magnitude of task vectors' parameters and the consistency of update directions. We observe that the parameters with higher consis-tency tend to have higher magni-tude. Therefore, we can think of maximum magnitude selection as a proxy for selecting the updates that multiple tasks agree on."}, {"title": "7 Extended analysis", "content": "In this Section, we broaden the scope of our analysis. We investigate the impact of maximum magnitude selection merging on existing CL methods. We also study the impact of sequential fine-tuning on other model merging strategies in both CIL setting and on the popular eight datasets benchmark.\nWe modify MAGMAX and instead of performing sequential fine- tuning, we train the model using one of the regularization-based CL methods. We present the results in Table 3. We observe that adding model merging significantly improves the performance of LwF and EWC in almost every scenario. Interest-ingly, neither of these combinations significantly outperform MAGMAX which uses naive sequential fine-tuning, traditionally known for causing catastrophic forgetting [7,26]. These results show that model merging is a promising technique for consolidating the knowledge after the training instead of during the training.\nTable 4 presents the results of merging independent and sequential fine-tunings with different methods in class-incremental scenarios. We observe that all merging methods benefit from sequential fine-tuning in most of the scenarios, achieving from 1.3% to 3.3% better average results. Table 5 presents the results of a similar experiment on eight dataset benchmark. We observe significant improvement (up to 12 p.p.) introduced by sequential fine-tuning. It shows that sequential fine-tuning can be"}, {"title": "8 Conclusion", "content": "In this paper, we introduced MAGMAX, a novel approach to continual learning that leverages model merging via maximum magnitude selection alongside sequential fine-tuning. Our findings underscore the potential of model merging as a viable solution to the challenges of continual learning. The synergy between sequential fine-tuning and maximum magnitude weight selection emerges as a pivotal factor in this success. It opens up possibilities for future research direc-tion focused on developing fine-tuning methods that facilitate model merging or finding new, more effective strategies for selecting important parameters in realms of continual learning."}]}