{"title": "BaiJia: A Large Scale Role-Playing Agent Corpus of Chinese Historical Charcaters", "authors": ["Ting Bai", "Jiazheng Kang", "Jiayang Fan"], "abstract": "We introduce a comprehensive large-scale role-playing agent corpus, termed BaiJia, that comprises various Chinese historical char-acters. This corpus is noteworthy for being the pioneering compi-lation of low-resource data that can be utilized in large languagemodels (LLMs) to engage in AI-driven historical role-playing agents.BaiJia addresses the challenges in terms of fragmented historicaltextual records in different forms and modalities, integrating vari-ous characters' information, including their biographical, literary,family relations, historical events, and so on. We conduct extensiveexperiments to demonstrate the effectiveness of our BaiJia agentcorpus in bolstering the role-playing abilities of various founda-tional LLMs, and promoting the development and assessment ofLLMs in the context of historical role-playing tasks. The agentcorpus is available at baijia.online\u00b9.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) show great potential to mimic hu-man responses in role-playing research areas, enabling individuals to engage with historical characters in a lifelike and immersive manner. Equipping LLMs with role-playing capabilities provides a distinctive means of communicating with historical characters, fostering a deeper comprehension of their thoughts, actions, and the historical backgrounds of those who have made notable contri-butions to human history.\nTo empower LLMs with role-playing abilities, most existing stud-ies, such as RoleLLM [7], InCharacter [8], CharacterEval [6], and ChatHaruhi [4], have focused on Supervised Fine-Tuning (SFT) basic LLM models using collected or generated dialogues of char-acters. However, all of these approaches encounter the significant challenge of the high costs associated with data collection, which is a crucial resource in facilitating LLMs with role-playing capability. We summarize the data properties in existing role-playing studies, and highlight the differences of our BaiJia corpus in Table 1. We can see that most characters in existing studies are modern, anime, or fictional characters, there has been a notable lack of research dedi-cated for role-playing of historical characters. Building role-playing agents with historical characters raises great challenges from vast historical timelines they inhabit and the intricacies associated with the preservation of historical materials. Besides, the number of characters in existing research used for SFT role-playing agents is limited, which may hinder the model's capacity to fully realize its role-playing potential.\nIn this paper, we provide a low-resource data corpus, termed BaiJia, for role-playing agent construction. The information in our agent corpus spans numerous forms and modalities, including historical documents, ancient books, artworks, folklore, and oral traditions. BaiJia contains 19,281 Chinese historical characters from five dynasties, i.e., Tang, Song, Yuan, Ming, and Qing dynasties. It integrates different source information, including their biograph-ical data, literary works, family relations, official positions, and historical events, providing a robust foundation for simulating the personalities, behaviors, and dialogues of these characters. To the best of our knowledge, we are the first to construct a large-scale role-playing agent corpus for Chinese historical characters. Our contributions are as follows:\n\u2022 We contribute a large-scale Chinese historical character agent corpus termed BaiJia, which firstly collects low-resource data for LLMs to conduct AI-driven historical role-playing.\n\u2022 We design comprehensive evaluation dimensions and re-lease an evaluation benchmark for the role-playing task of historical characters."}, {"title": "2 DATASET CONSTRUCTION", "content": "The pipeline for the construction and evaluation of role-playing agents is shown in Fig. 1. We highlight the key steps of data con-struction, dialogue generation, and model evaluation process. The data corpus that we contributed for role-playing agent construc-tion includes three parts: the collection of character resumes, the generation of dialogues that were used to fine-tune LLMs, and the construction of questions that were used to evaluate the role-playing capability of LLMs."}, {"title": "2.1 Resume Collection", "content": "We collect diverse character information from multiple sources, e.g., CBDB2, Wikipedia\u00b3, Gushiwen website, to construct Informative and comprehensive character resume in role-playing agent con-struction. The characters are from five major dynasties in Chinese history: 3,020 characters from Tang, 5,964 characters from Song, 972 characters from Yuan, 4,564 characters from Ming, and 4,761 characters from Qing dynasties. The resumes of each character consist of information about their profiles, their relationships, and their work. The detailed introduction of the resume template is shown in Table 2, which can be summarized into 15 sub-categories that cover the basic profile, relations, career, and achievement of characters."}, {"title": "2.2 Dialogue Generation", "content": "After constructing the resumes of characters, we generate the dia-logues that are used for SFT LLMs. Following Character-LLM [5], we adopt a two-step dialogue generation approach: (1) extracting the character's dialogue scenes: we adopt GPT-40-mini to extract 10 unique scenes based on the resume of each character. These scenes include palace dialogues, family conversations, and literary debates. The prompts focus on the character's social relations, life events, and works, ensuring an authentic historical setting; (2) generating dialogues related to these scenes: under the background of different scenes, we use GPT-40-mini to automatically generate the ques-tions and simulate responses that align with the historical context of characters.\nFinally, we utilize the LLaMA-Factory framework [9] to perform LoRA fine-tuning on LLMs with resumes and the generated dialogue information of characters. This process enables the LLMs to acquire the ability for role-playing."}, {"title": "2.3 Question Construction", "content": "To evaluate the usefulness of our character agent corpus, we con-struct a question dataset that is used for historical role-playing agent evaluation. For each character, we construct 15 questions from five thematic aspects: Personal Background, Era Background, Family & Social Connections, Thoughts, Personality & Values, Achievements & Contributions. We use GPT-40-mini to generate knowledge-oriented"}, {"title": "3 EXPERIMENT", "content": ""}, {"title": "3.1 Experimental Setup", "content": "3.1.1 Baseline Models. To verify the usefulness of our constructed agent corpus, we conduct experiments on different kinds of LLMs, including the general LLMs: i.e., ChatGLM [3], Qwen, Lama [2], DeepSeek [1], and the role-playing LLMs (RP-LLM): i.e., BaiChuan-NPC6. and Tongyi Xingchen7. Our LLM BaiJia has been fine-tuned on Qwen2.5-7B with resume and dialogue information from our constructed agent corpus, ensuring our LLM remains lightweight and highly specialized. Table 4 summarizes the difference among them according to the size of parameters and applications."}, {"title": "3.1.2 Evaluation Metrics", "content": "We design a comprehensive evaluation benchmark to assess the capabilities of LLMs on six dimensions: Character Consistency (CC), Dialogue Ability (DA), Character Appeal (CA), Emotional Expression & Intellectual Depth (EI), Creativity & Role Depth Expansion (CR), and Cultural & Historical Appropriateness (CHA). Except for the evaluation dimensions, i.e., CC, DA and CA, from existing role-playing benchmarks [4, 6-8], we propose three new dimensions, i.e., EI, CR, and CHA, that are specifically designed to evaluate the deep-level spiritual aspect of historical characters, including their emotion, creation and culture understanding. Each of these six dimensions contains two sub-dimensions, forming a comprehensive assessment of role-playing performance with a to-tal of twelve sub-dimensions. Detailed evaluation dimensions are outlined in Table 5."}, {"title": "3.2 Experimental Results", "content": "To evaluate the effectiveness of our agent corpus, we compare the performance of different LLMs with and without the incorporation of our corpus. The results are shown in Table 3. We can see that: (1) After incorporating the information of character resumes (results marked with underline), the role-playing capabilities of all kinds of LLMs gain significant improvements over six evaluation dimen-sions; (2) Despite the increased capabilities of advanced LLMs, such as Qwen2.5-72B v.s. Qwen2.5-7B and Llama-3.1-70B v.s. Llama-3.1-8B, the enhancements achieved through our data corpus still be significant. This demonstrates that our corpus effectively fills the data gaps of current LLMs in role-playing tasks; (3) For LLMs spe-cialized for role-playing applications, such as BaichuanNPC and Xingchen, we observe that they are unable to portray Chinese his-torical characters effectively. This may be attributed to the limited availability and widespread distribution of historical data. (4) The"}, {"title": "3.3 Experimental Analysis", "content": "3.3.1 Ablation Study. An ablation study is conducted to evaluate the effects of our agent corpus, i.e., the resume information and the generated dialogue information of characters. Our model, i.e., Ours, integrates resumes and is SFT with dialogue information based on the Qwen2.5-7B framework. We compare it with its degradation versions, i.e., w/o SFT: utilizing resume information only, and w/o SFT & Resume: neither conduct SFT with dialogue information nor incorporate resume information. As shown in Figure 3, we can see that the fully optimized LLM \"Ours\" achieves superior performance across all evaluation dimensions. Without SFT or resume informa-tion, it leads to noticeable performance degradation, showing the usefulness of our corpus in enhancing the consistency and compre-hensive abilities of role-playing LLMs. Compared to LLMs that are specialized for role-playing agents, without the data resources, they even perform worse than the general LLMs due to their fine-tuning for distinctive kinds of specific characters from Anime or Novels.\n3.3.2 Case Study. To intuitively show the effectiveness of our agent corpus, we present a case study of a historical character Bai Ben and compare the responses generated from various LLMs. As shown in Fig. 2, for the question \"What is the literary work you are most proud of?\", Baichuan-NPC generates a title of work (i.e., \"Ode to Snow\") in the responses, but it is fictional. GPT-4 and Qwen2.5-7B are unable to provide responses, i.e., \"may not have many grand compositions to be passed down\" from GPT-4 and \"not in literary works\" from Qwen2.5-7B, owing to their deficiency in relevant knowledge. After incorporating our agent corpus, the fine-tuned Qwen2.5-7B accurately responds \"Parrot Song: The Fisherman\" as Bai Ben's most accomplished work. This response aligns with historical records and demonstrates the superiority of our corpus in capturing and reproducing historical character information."}, {"title": "4 CONCLUSION", "content": "We contribute a high-quality agent corpus of Chinese historical characters, which is vital to improving the role-playing capability of large language models. By aggregating fragmented information from diverse data sources and integrating missing data, we collect an invaluable data resource in the realm of historical role-playing agents. This large-scale agent corpus is a groundbreaking contribu-tion to low-resource historical AI role-playing research."}]}