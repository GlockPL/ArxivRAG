{"title": "Static Segmentation by Tracking: A Frustratingly Label-Efficient Approach to Fine-Grained Segmentation", "authors": ["Zhenyang Feng", "Zihe Wang", "Saul Ibaven Bueno", "Tomasz Frelek", "Advikaa Ramesh", "Jingyan Bai", "Lemeng Wang", "Zanming Huang", "Jianyang Gu", "Jinsu Yoo", "Tai-Yu Pan", "Arpita Chowdhury", "Michelle Ramirez", "Elizabeth G. Campolongo", "Matthew J. Thompson", "Christopher G. Lawrence", "Sydne Record", "Neil Rosser", "Anuj Karpatne", "Daniel Rubenstein", "Hilmar Lapp", "Charles V. Stewart", "Tanya Berger-Wolf", "Yu Su", "Wei-Lun Chao"], "abstract": "We study image segmentation in the biological domain, particularly trait and part segmentation from specimen images (e.g., butterfly wing stripes or beetle body parts). This is a crucial, fine-grained task that aids in understanding the biology of organisms. The conventional approach involves hand-labeling masks, often for hundreds of images per species, and training a segmentation model to generalize these labels to other images, which can be exceedingly laborious. We present a label-efficient method named Static Segmentation by Tracking (SST). SST is built upon the insight: while specimens of the same species have inherent variations, the traits and parts we aim to segment show up consistently. This motivates us to concatenate specimen images into a \"pseudo-video\" and reframe trait and part segmentation as a tracking problem. Concretely, SST generates masks for unlabeled images by propagating annotated or predicted masks from the \"pseudo-preceding\" images. Powered by Segment Anything Model 2 (SAM 2) initially developed for video segmentation, we show that SST can achieve high-quality trait and part segmentation with merely one labeled image per species\u2014a breakthrough for analyzing specimen images. We further develop a cycle-consistent loss to fine-tune the model, again using one labeled image. Additionally, we highlight the broader potential of SST, including one-shot instance segmentation on images taken in the wild and trait-based image retrieval.", "sections": [{"title": "1. Introduction", "content": "Understanding sources and patterns of intra-specific variation in traits (e.g., morphological characteristics, such as fin length for a fish, wing size for a beetle) is a central goal of evolutionary and ecological study [11, 18]. Intra-specific trait variation provides a currency for assessing the roles of abiotic and biotic processes on community assembly, as it reflects the mechanisms driving species occurrence and responses to change [82]. Museum specimens present an untapped resource for curating information on intra-specific trait variation of species morphology. Up until now, it has been difficult to harvest trait information from museum specimens due to the sheer amount of manual labor needed to make such measurements. Automatic segmentation of morphological traits from specimen images has the potential to scale up the measurement of traits and free up researchers to focus on analysis and interpretation. This paper originated from an interdisciplinary collaboration between biologists and computer scientists aimed at segmenting images of organismal specimens to measure variation in traits to fill this much-needed knowledge gap."}, {"title": "2. Related Work and Background", "content": "Image segmentation. Image segmentation has been a long-standing problem in computer vision, with various applications spanning across different fields [12, 21, 37, 41, 52]. Semantic and instance segmentation are arguably the most popular segmentation tasks nowadays, aiming to cluster pixels bearing the same semantic meanings and instances [23, 52]. While much of the focus has been on common, coarse-grained objects, several works have begun to explore more fine-grained part-level segmentation within common objects [29, 68]. In this paper, we aim to address a brand-new, extremely fine-grained segmentation problem.\nFew-shot segmentation (FSS). Many state-of-the-art (SOTA) models have demonstrated impressive capabilities in image segmentation [15, 33]. However, one bottleneck to such a task is the laborious labeling efforts in creating pixel-level annotations for model training. To address such limitations, other works attempt to use few-shot learning techniques to provide high-quality segmentation masks to unseen classes, only with one or few image mask pairs as \"support\" set [24, 40, 81, 91, 93]. In this work, we propose a new few-shot learning approach and introduce a novel perspective to tackle the FSS problem.\nSegmentation Anything Model (SAM). SAM [35] is a foundation model for image segmentation, achieving SOTA zero-shot segmentation performance. Previous works have shown its superiority in medical image segmentation [28, 44, 49], camouflaged object detection [79], semantic communication [80], and autonomous driving [76]. Recently, Segment Anything Model 2 (SAM 2) [70] was released, with extended capabilities on video segmentation tasks. Specifically, SAM 2 is designed to take in the prompts on any frame within a video sequence to help track and segment target objects and has been shown as the new SOTA model in video segmentation. We build upon SAM 2's superior video segmentation capability for fine-grained segmentation of non-sequential images in the biology domain.\nCo-segmentation. The concept of co-segmentation was introduced by Rother et al. [71], who aimed to segment out the common foreground objects from multiple images. Most of the previous works focus on whole object instances and require some degrees of joint training [14], supervised clustering [34], or generation and ranking of region proposals [16, 83]. Our work can be viewed as a new approach to co-segmentation, leveraging the capabilities of video segmentation models to efficiently segment the common objects, parts, and traits across images given one labeled image."}, {"title": "3. Proposed Approach", "content": "Problem definition and notation. We study trait and part segmentation from specimens of the same species. Let $x \\in \\mathbb{R}^{W\\times H\\times 3}$ denote a $W \\times H$ image and $y \\in \\{0,1\\}^{W\\times H \\times C}$ denote the corresponding ground-truth masks of $C$ distinct traits or parts. The goal is to develop a segmentation model $f$ such that its output $\\hat{y}=f(x)$ matches $y$."}, {"title": "3.1. Static Segmentation by Tracking (SST)", "content": "At first glance, this looks like an impossible mission. However, the domain-specific properties described in Sec. 1 offer the rescue. Our proposed approach, SST, closely considers these properties and reframes trait and part segmentation as a tracking problem, which is inherently a one-shot task given a set of labeled instances in the first frame.\nMore specifically, let $\\{x_0,\\ldots,x_N\\}$ denote a sequence of video frames; a tracking algorithm aims to track the instances in $x_0$, encoded by the label $y_0$, across the remaining frames. In our context, we do not have a video but a set of $N$ unlabeled images and one labeled image $(x, y)$. Nevertheless, the domain-specific properties motivate us to create a \"pseudo-video\" by treating $x$ as the first frame $x_0$, followed by an arbitrary order of the rest. Please see Sec. 3.3 for a detailed discussion about the pseudo-video creation.\nSAM 2 for SST. We adopt the recently released Segment Anything Model 2 (SAM 2) [70] for tracking, although other tracking algorithms may apply. We briefly introduce its model architecture and inference mechanism, followed by our usage. See also Sec. 2 for the background.\nSAM 2 uses a promptable Transformer encoder-decoder $f$ augmented with a memory bank $B$ to process a video and generate masks (see Fig. 5). Let $\\{x_0,\\ldots,x_N\\}$ denote the video frames and $\\{y_0,\\ldots, y_N \\}$ the corresponding ground-truth labels. When the label of $x_n$ is not available, $y_n = \\emptyset$. Let us denote the predicted mask for $x_n$ by $\\hat{y}_n$ and the updated memory bank by $B_n$, which stores both the feature and mask information. $B_n$ can then be accessed by the next frame $x_{n+1}$ to connect consecutive frames. In the context of tracking, $B_n$ can be interpreted as the updated state estimate after perceiving the measurement $x_n$.\nAt each timestamp $n$, $f$ takes the tuple $[x_n, B_{n-1}, y_n]$ as input, where $y_n$ is treated as the (optional) prompt. It then outputs the tuple $[B_n, \\hat{y}_n]$, where $B_n$ will be used as an input at the next timestamp,\n$[B_n, \\hat{y}_n] = f ([x_n, B_{n-1}, y_n]). \\qquad(1)$\nIn our context, we have $y_n = \\emptyset, \\forall n > 0$. Namely, only the first frame $x_0$ is labeled. Inputting $y_0$ to $f$ at timestamp 0 instructs the model on what to segment\u2014the distinct traits and parts and their extents. The resulting memory bank $B_0$ then carries such information to the successive frames, instructing the model on what to track over frames to generate the masks $\\{\\hat{y}_1,\\ldots, \\hat{y}_N\\}$.\nRemark. According to the original paper [70], SAM 2 is readily applicable to a batch of static, non-sequential images, by setting the memory bank $B_n$ to empty. In essence,"}, {"title": "3.2. Opening-Closing Cycle-Consistent Loss", "content": "SST uses SAM 2 in a plug-and-play fashion without changing its pre-trained weights, even though our use case is beyond the training data distribution. It is thus expected that SST might fail when the transitions across static images are significantly out-of-distribution (OOD).\nOne intuitive way to address this is to fine-tune SAM 2. However, given merely one labeled image $(x_0, y_0)$, fine-tuning risks overfitting. Noticing that we have used $y_0$ to prompt SAM 2, we face another challenge: it is unclear how to use it \"dually\u201d as the label to supervise fine-tuning.\nWe propose a novel approach, which leverages the flexibility of creating pseudo-videos. We can not only concatenate static images in a flexible order but also duplicate them and inject them at different timestamps to obtain multiple predictions for the same image. Specifically, we duplicate the labeled image $x_0$ and append it at the end of the pseudo-video. The resulting video becomes $\\{x_0,\\ldots,x_N,x_{N+1}\\}$, where $x_{N+1} = x_0$. Unlike timestamp 0 where $x_0$ is used as the \"opening frame\" and $y_0$ is inputted as the prompt, at timestamp $N + 1$, $x_{N+1}$ is treated as an unlabeled \u201cclosing frame\u201d without prompts (i.e., $y_{N+1} = \\emptyset$),\n$[B_{N+1}, \\hat{y}_{N+1}] = f ([x_{N+1}, B_N, \\emptyset]). \\qquad(2)$\nSuch an arrangement allows us to use $y_0$, the ground-truth label of $x_{N+1}$, to supervise the fine-tuning of $f$ by minimizing the difference between $\\hat{y}_{N+1}$ and $y_0$. The rationale is if $f$ fails in intermediate frames, $B_N$ will not carry useful information for segmenting the traits or parts in $x_{N+1}$. We employ a combination of the binary cross entropy (BCE) loss and the Dice loss for fine-tuning, which are commonly used in training a segmentation model. We name our fine-tuning objective function Opening-Closing Cycle-"}, {"title": "3.3. Implementation Detail", "content": "Pseudo-video creation. Previous subsections assume that the pseudo-video concatenates the specimen images in an arbitrary order. Intuitively, a smooth transition order should improve SST's performance; in contrast, a non-smooth transition order may degrade SST. So far, using random orders, we have not experienced a degradation. (See Appendix S2.1 for details.) Yet, we believe a dedicated approach to finding a smooth order would make SST more stable, and we leave it as a future work.\nIn this paper, unless stated otherwise, we implement SST by creating multiple short, two-frame videos. Concretely, given the labeled image-masks pair $(x_0, Y_0)$ and $N$ unlabeled images $\\{X_1,\\ldots,X_N\\}$, we create $\\{(x_0,X_1),\\ldots,(x_0,x_N)\\}$ and apply SST independently to each of them. We have two reasons.\n\u2022 It eliminates the randomness in creating pseudo-videos while knowing that it would disregard the potential benefit of long-term memory.\n\u2022 It makes the comparison fair. Conventional models segment each test sample independently, reflecting the real-world online use case where one processes a newly captured image right away. The recent few-shot approaches also process each test image independently. Noticing that considering all test images at once would term the conventional inductive setting into a transductive one, we decide to process each test image independently.\nFine-tuning and memory bank. Instead of performing full fine-tuning, we apply LORA [27] to fine-tune the decoder and the memory encoder of SAM 2 in a parameter-efficient way. It reduces potential overfitting and speeds up training. Since we now apply SST to short videos, i.e., $\\{(x_0, x_n)\\}$, the memory bank could simply carry the prompt $y_0$ inputted at timestamp 0, making the fine-tuning ineffective. We thus propose the following strategy."}, {"title": "3.4. Extension to Trait-Based Retrieval", "content": "Beyond trait segmentation within the same species, SST can also be used to retrieve specimens having a similar trait (e.g., the white band on the forewing or the orange tiger tails on the hindwing) from other species. Given a query image $x_0$ and a target trait $y_0^*$, a single channel in the original $Y_0 \\in \\{0,1\\}^{W\\times H \\times C}$, SST scores each image $x_n$ in the retrieval pool by\n1. creating a palindrome-style cycle $\\{x_0,x_n,x_n^\\dagger,x\\}$;\n2. using $y_0^*$ as the prompt and taking the forward pass introduced in Sec. 3.3 to predict $\\hat{y}^*$;\n3. calculating the reconstruction quality IoU($y_0^*, \\hat{y}^*$).\nNamely, if $x_n$ has the target trait, the mask $y_0^*$ should accurately propagate to $x_n^\\dagger$ and then propagate back to $x_0$."}, {"title": "4. Experiment", "content": "4.1. Experimental Setup\nData. We evaluate SST on three specimen data sources.\n\u2022 Butterfly: For fine-grained trait segmentation, we consider the Cambridge Heliconius Collection [36] gathered from various sources\u00b9. This collection contains 155 butterfly sub-species of the Heliconius genus; each has 4 ~ 14 distinctive traits to tell itself apart from others. Examples include the tiger tails on the hindwings and white bands on the forewings; some have quite complex, disconnected shapes. We consulted with biologists and the field guide [1] to annotate them. Across specimens of the same sub-species, the mask IDs are consistent. An algorithm needs to not only segment them but also label each with an ID. As this dataset is long-tailed (see Fig. 2), we split the 151 sub-species into two parts: major and minor."}, {"title": "4.2. Main Result", "content": "Trait segmentation on Butterfly. As each butterfly sub-species has a distinct set of traits, we evaluate them separately. On each of the five major sub-species, we train a Mask2Former [15] and a YOLOv8 [33] using all training samples. For FSS algorithms, we consider a one-shot setting. On the remaining minor species, we only consider FSS algorithms, again in a one-shot setting. For each minor sub-species, we iterate over the 2 ~ 3 labeled images, using one for training and the others for testing, and report the average mIoU. Tab. 1 summarizes the results. SST outperforms existing FSS algorithms in a one-shot setting, with a margin of at least 16 mIoU on both major and minor sub-species. Surprisingly, SST even surpasses many-shot algorithms trained with at least 150 samples per sub-species.\nPart segmentation. We compare SST to both FSS algorithms (in a one-shot setting) and many-shot algorithms on Fish [50] and Beetle [22] datasets. This is particularly challenging as the algorithm needs to segment the same body parts across species (see Fig. 7), which exhibit huge phenotypic variations. As shown in Tab. 2, most of the FSS methods fail, yet SST can still maintain a fairly high mIoU. We observe further improvement by employing one-shot fine-tuning using OC-CCL, as can be seen in Tab. 2 and Fig. 7."}, {"title": "4.3. Out-of-Distribution (OOD) Robustness", "content": "In the actual application of SST on specimen images, sometimes we might encounter OOD cases, where the specimens are not captured in standard views. That is, the images might be subjected to rotation, translation, or scaling. Accordingly, we manually apply these transformations to the Butterfly test set to create an OOD robustness task. We define transformation levels from 0.0 to 1.0, corresponding to no transformation and the largest degree of transformation we apply, respectively. At level 1.0, we randomly rotate the image between -90\u00b0 and 90\u00b0, translate it up to 60% of its height or width, and scale it down to 50% of its original size. We found that SST tends to lose track of the fine-grained details after a certain level of transformations, likely due to the absence of such huge variations (between consecutive frames) in the pre-training data.\nTo address this, we use OC-CCL to fine-tune the model in a one-shot setting for each test image. OC-CCL consistently improves SST's robustness as seen in Fig. 8. In the extreme rotation cases (the right end of the figure), we boost the mIoU from 40% to over 50%, a more than 10% gain."}, {"title": "4.4. Experiments on Instance Segmentation", "content": "Besides fine-grained trait and part segmentation on specimen images, SST can also be applied to object instance segmentation on images taken in the wild. We use the CUB-200-2011 dataset [84] to demonstrate such a capability. Given one random bird image and its segmentation mask, we examine if FSS algorithms can segment all 200 bird species from the remaining images. The results are shown in Tab. 3. As whole object instance segmentation is the original problem domain for most of the compared methods, they show much better results than Tab. 1 and Tab. 2. In this domain, SST still achieves a competitive segmentation performance across bird species, even with large variations from image to image. Furthermore, fine-tuning SST with the single training image using OC-CCL again shows significant improvement in segmentation quality. We closely analyze the object instances that originally fail to be correctly segmented by SST and categorize them into 3 failure cases: Oversegmentation, where SST correctly segments out the object along with some extra neighboring backgrounds; False Positive, where SST falsely segments out an irrelevant object; and False Negative, where SST completely fails to segment anything from the picture. As shown in the bottom row of Fig. 9, without using any ground truth masks for the test images, fine-tuning with OC-CCL helps substantially mitigate these issues."}, {"title": "4.5. Experiments on Trait-Based Retrieval", "content": "As mentioned in Sec. 3.4, given a target trait, our method can use the reconstruction IoU to find images with similar traits. As shown in Fig. 10, SST + OC-CCL faithfully retrieves sub-species with similar corresponding traits. If we focus on different traits of the same butterfly, we can retrieve different lists of sub-species. For more experiment results and discussions, please see Appendix S4."}, {"title": "5. Conclusion", "content": "We introduce Static Segmentation by Tracking (SST), a frustratingly simple approach to fine-grained segmentation on specimen images. By passing non-sequential specimen images into a tracking algorithm like SAM 2, SST demonstrates remarkable trait and part segmentation given merely one labeled image. Our further investigation shows that SST can go beyond specimen images to segment animal instances in the wild. It can also support trait-level retrieval to discover species with similar local parts and patterns."}]}