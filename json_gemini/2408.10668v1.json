{"title": "Probing the Safety Response Boundary of Large Language Models via Unsafe Decoding Path Generation", "authors": ["Haoyu Wang", "Bingzhe Wu", "Yatao Bian", "Yongzhe Chang", "Xueqian Wang", "Peilin Zhao"], "abstract": "Large Language Models (LLMs) are implicit troublemakers. While they provide valuable insights and assist in problem-solving, they can also potentially serve as a resource for malicious activities. Implementing safety alignment could mitigate the risk of LLMs generating harmful responses. We argue that: even when an LLM appears to successfully block harmful queries, there may still be hidden vulnerabilities that could act as ticking time bombs. To identify these underlying weaknesses, we propose to use a cost value model as both a detector and an attacker. Trained on external or self-generated harmful datasets, the cost value model could successfully influence the original safe LLM to output toxic content in decoding process. For instance, LLaMA-2-chat 7B outputs 39.18% concrete toxic content, along with only 22.16% refusals without any harmful suffixes. These potential weaknesses can then be exploited via prompt optimization such as soft prompts on images. We name this decoding strategy: Jailbreak Value Decoding (JVD), emphasizing that seemingly secure LLMs may not be as safe as we initially believe. They could be used to gather harmful data or launch covert attacks.", "sections": [{"title": "Introduction", "content": "The emergence of large language models (LLMs) such as GPT-4 (Achiam et al. 2023) facilitates the public's daily work and life, providing various forms of intelligent assistance (Wang et al. 2024b; Rafailov et al. 2024; Pang et al. 2024; Guo et al. 2024). However, while bringing convenience, these models have also raised serious concerns among the public about their potential misuse and the generation of dangerous content. Therefore, comprehensively understanding and systematically testing the safety response capabilities of different LLMs has become one of the most important ways to enhance public trust in AI. This is not only crucial for the healthy development of AI technology but also essential for ensuring that AI systems can responsibly serve society (Ouyang et al. 2022; Wang et al. 2023).\nCurrently, testing the safety capability boundaries of LLMs mainly focuses on two levels: 1) Test case generation: This involves finding original inputs that can bypass the safety protection boundaries of large models, such as long-tail risk cases like copyright infringement (Lemley and Reese 2003). This line aims to discover model weaknesses in specific situations (Lee et al. 2024). 2) Model jailbreaking: This line of works involves rewriting or adding token prefixes/suffixes to the original input, or fine-tuning model parameters in white-box scenarios (Qi et al. 2023b), to make the given input bypass the model's protection mechanisms.\nIn fact, in the vast majority of real-world scenarios, the user input space is completely open. Therefore, a more fundamental and critical task is to explore and comprehensively characterize the safety boundary breaches caused by arbitrary user inputs (whether jailbroken or not) to a given model. To delve deeper into this issue, this study proposes a novel perspective: starting from the decoding space, rather than the traditional approach of focusing on the input space (Zou et al. 2023a) or parameter space (Qi et al. 2024).\nThe motivation for this research stems from a key phenomenon observed previously: although preference alignment in LLMs changes the way models utilize knowledge, it does not fundamentally eliminate the potential harmful information stored in the model parameters (Qi et al. 2023b, 2024). As shown in Figure 1, for a normal input (without any jailbreaking transformation) and an open-source model that has undergone safety alignment, although there is no apparent risk leakage in the regular decoding process, we can still obtain potentially harmful outputs by decomposing the decoding path. Specifically, by choosing appropriate tokens (with a malicious model) at each decoding step, we can significantly increase the probability of harmful content.\nBased on this key observation, this paper proposes an innovative scheme for automatically extracting harmful paths from safety-aligned LLMs. The core idea of this scheme is to evaluate the probability of generating harmful content in subsequent decoding steps starting from any decoding step. We ingeniously model this probability estimation problem as a reward-based Markov Decision Process (MDP) (Sutton and Barto 2018). By training on external or self-generated dataset of harmful samples (e.g., SafeRLHF (Dai et al. 2023)), we obtain an efficient harmful state evaluation model called Cost Value Model (CVM). This model can guide to potential harmful paths, thereby guiding developers to identify possible vulnerabilities.\nWith the cost value model, our work highlights safety"}, {"title": "Related Work", "content": "LLM safety Safety alignment can be achieved through various methods. For instance, it can be done by supervised fine-tuning with secure corpora (Sun et al. 2024; Touvron et al. 2023; Jiang et al. 2023), safety Reinforcement Learning from Human Feedback (Dai et al. 2023; Touvron et al. 2023; Ouyang et al. 2022). On the other hand, jailbreak a safety-aligned LLM isn't an easy job. For open-source LLMs, though fine-tuning on harmful contexts or even dialog contexts will lead to safety collapse (Qi et al. 2023b), in practical applications, there are little scenarios that offer fine-tuning interfaces. Suffixes (Liu et al. 2023; Zou et al. 2023a) are also utilized for prompt optimaztions, but they are often unreadable. Others (Lee et al. 2024) use GFlowNet to generate diverse and efficient attack prompts to circumvent the built-in safeguards of models. In this work, we abandon any suffixes in the model input. We employ the cost value model as a detector, to search and guide the safety-aligned LLMs to output toxic contents within their vicinity facing harmful queries directly; and an attacker, to attack the safety-aligned LLMs with its self-generated toxic responses.\nDecoding Post Process Post processing is orthogonal to fine-tuning. Several works conduct post processing on LLMs in math reasoning (Wang et al. 2024a), instruction following and summarization (Han et al. 2024; Kong et al. 2024; Khanov, Burapacheep, and Li 2024). As for safety alignment, Aligner (Ji et al. 2024) aims to refine the harmful response via a small language model by Residual Correction (He et al. 2016). Different from them, we are conducting post processing on safety-aligned LLMs in their decoding process. We try to investigate if there is potential harmful paths around the original outputs of safety-aligned LLMs."}, {"title": "Methods", "content": "Model generation is actually a MDP\nConsider a toxic question $q$, the decoding process of a safety-aligned LLM policy $\\pi_\\theta$ is actually a MDP: $(S, A, T, C, \\gamma)$. Each time the model decodes one token $a_t \\in A$ from the previous state $s_t \\in S$. Here $s_t$ is the state at time step $t$, $a_t$ is the chosen token at time step $t$. Note that, $s_0 = q$ and $T_{s_i} = s_{i-1}$ concatenate $A_{i-1}$. Therefore, a decoding process formulates a trajectory $T = [s_0, a_0; s_1, a_1; ...; s_t]$. The cost score C of this trajectory is the sentence level cost from an Outcome Cost Model(OCM) and indicates the extent of harmfulness of this sentence. In details, the cost will be zero until the model generates the [eostoken] or reaches max token length, which indicates the end of the generation. The absolute cost score of the question-answer pair (Q&A pair) is calculated below:\n\n$C(s_t) = \\begin{cases} c(s_t) & a_{t-1} = [eostoken]\\\\ 0 & a_{t-1} \\neq [eostoken] \\end{cases}$                                                                              (1)\n\nThe cost model we imply in this work is well trained on pair-wise toxic human-label datasets such as SafeRLHF (Dai et al. 2023), AdvBench (Zou et al. 2023b) to give an accurate cost estimation. There are safe and toxic labeled responses corresponding to one particular harmful question in the training datasets, which provides accurate differential signals.\nCost Value Model Estimation\nIn this work, value modeling follows the principle of Bellman Equation (Barron and Ishii 1989) and are trained by temporal difference (TD) algorithm (Sutton and Barto 2018). Specifically, a cost value model is finetuned with a collected dataset $D = \\{q^i, \\{s_t, a_t, C_t\\}^T_{t=1}\\}_{i=1}^N$, where $q^i$ is the $i^{th}$ toxic questions from a question sets of size N. $s_t$ and $a_t$ are states and actions corresponding to $q^i$ at timestep $t$. Specifically, we are looking for an imitation Cost Value Model $\\hat{V}$ from $D$, which can be trained by the following equations:\n\n$\\hat{V} = arg \\min_{\\pi} \\frac{1}{N} \\sum_{i=1}^N \\frac{1}{T} \\sum_{t=1}^T \\{ \\hat{V}^{\\pi}(s_t) - \\hat{V}^{\\pi}(s_t)\\} ^2 ,$                                                                                 (2)\n\n$\\hat{V}^{\\pi}(s_t) = \\lambda^T C(s_T) + \\sum_{k=t}^T \\gamma^{k-t}  \\lambda^k [ \\gamma V^{\\pi}(s_{k+1}) - V^{\\pi}(s_k) ],$                                                                 (3)\n\nwhere $s_t = T[q^i, a_1, ..., a_t]$. To estimate an accurate cost value function, we conduct TD($\\lambda$) (Sutton and Barto 2018) and set $\\lambda = 0.95$ for convenience, serving as an effective method for V to learn the relationship between sparse costs and past states. The policy $\\pi$ for collecting dataset $D$ can be either inference policy or human labelers.\nCost Value Model Guides to Unsafe Paths\nThe output of the safety-aligned model to a specific toxic question will be influenced and guided by the Cost Value Model at decoding time. As shown in Algorithm 1, during each decoding process for the next token, the target LLM selects the k tokens that could potentially be chosen as top-K tokens. At this point, the cost value model receives the logits of these tokens and adds its own bias on them,\n\n$logits(a_{t_i})' = logits(a_{t_i}) + \\beta V^{\\pi}(T[s_t, a_{t_i}]),$                                                                                    (4)\n\nwhere $a_{t_i}$ is the $i^{th}$ token in top-K tokens at timestep $t$, indicating these tokens are situated close to the central distribution through the inference process. In practice, we perform a simple normalization on the estimated values by subtracting the average value of the top-K tokens from each actual predicted value to ensure the prediction stability."}, {"title": "Experiments", "content": "In this section, we first revisit our experiment settings; then we give the absolute performance of Cost Value Models, showing its efficiency; following this, we conduct an exhaustive analysis of the experimental results on LLaMA-2-chat 7B to figure where the cost value model becomes efficient: vulnerability in first few tokens, vulnerability after agreement tokens, vulnerability after refusal tokens. Finally, we give a simple but effective usage of these toxic contents that jailbreak the target LLMs via soft prompt optimization."}, {"title": "Experimental Setup", "content": "Models There are four kinds of models in the training pipeline. 1) Cost Model: this model is responsible for giving a concrete cost score for an input Q&A, serving as an Outcome Cost Model (OCM). We utilize the ArmoRM-Llama3-8B-v0.1 (Wang et al. 2024c)'s \"Is Safe Score\" as a proxy. This model is a MoE structure model and gives nineteen reward objectives including: truthfulness, helpful, complexity, is safe, etc. It ranks the second place on rewardbench (Lambert et al. 2024), surpassing Gemini 1.5 pro (Reid et al. 2024), GPT4 turbo (Achiam et al. 2023) and many other close source LLMs. As for the safety evaluation, it achieves 92.2, serving as a reliable evaluation metric for safety judgements. 2) Cost Value Model: this model is trained to detect the possible toxic path nearby the inference model's distribution. We use LLaMA-2-chat-7B (Touvron et al. 2023) as the backbone and add a value head. We use Qlora (Dettmers et al. 2024) to finetune the Cost Value Model (CVM). 3) Inference Model: these models are the attacked targets. We introduce three kinds of safety-aligned models: Vicuna-13B (Peng et al. 2023), LLaMA-2-chat-7B (Touvron et al. 2023), Mistral-7B-Instruct-v0.2 (Jiang et al. 2023). These models undergoes safety alignment via SFT or RLHF. 4) Evaluation: we introduce both human evaluation and model evaluation on the guided output texts. We use Attack Success Rate (ASR): the percentage of instructions that receive misaligned outputs as our evaluation metric. Specifically, We first use LLaMA-Guard-2 (Inan et al. 2023), a safeguard model to give reliable judgements on whether the output text offers a concrete guidance on any harmful queries. We then conduct post-evaluation on these judgements by human to ensure the accuracy. This is our first evaluation metric. We also give the refusal rate which is widely used in previous work (Zou et al. 2023a), serving as the second evaluation metric. 5) Important hypeparameters: we find different hypeparameters are suitable for different models. We set top-K = 20, \u03b2 = 2, 10, 5 separately for Vicuna-13B, LLaMA-2-chat 7B and Mistral-Instruct-7B-v0.2, which is enough to get a toxic output without sacrificing readability.\nDatasets There are three kinds of dataset we employ here. 1) As for the training of Value Model, we use SafeRLHF (Dai et al. 2023), a dataset consisting of harmful questions and corresponding answers. We collect dataset $D = \\{q^i, \\{s_t, a_t, C_t\\}^T_{t=1}\\}_{i=1}^N$ in two ways. One is collecting responses from the inference model. The other is using the original labels in SafeRLHF datasets. There are two response labels corresponding to a single question in the dataset. Sometimes they are both toxic, sometimes one is toxic and the other is safe. This gives a diverse cost signal. 2) As for the inference, we adapt safetydataset (Bianchi et al. 2023) for evaluation, which consists of 3k toxic questions. We filter these questions with LLaMA-Guard-2, and keep 1128 real toxic questions as evaluation prompts. We conduct a human study to ensure the 1128 questions are 100% toxic questions. 3) As for prompt optimization attack, we use 400 toxic questions from SafeRLHF test set, along with 40 manual harmful instructions employed in (Qi et al. 2023a).\nIt is worth noting that, sometimes self generated dataset D consists of very few toxic examples, which is hard to give a positive signal for TD($\\lambda$) to train. In this situation, we introduce Best of N (Zhang et al. 2024b) (N = 32) to collect toxic responses from the target models. Despite the predominance of safe data in the collected corpus, it nonetheless contains examples that can provide toxic sample signals."}, {"title": "Overall fragility in decoding stage", "content": "In this section, we demonstrate the safety response boudary of target LLMs with Cost Value Model. LLaMA-2-chat 7B exhibits the strongest robustness against toxic questions; however, its safeguard is still broken, indicating there are toxic paths nearby its original greedy distribution though it seems pretty safe. Vicuna-13B easily outputs toxic contents as soon as the CVM induces a potential toxic beginning. For example, Vicuna-13B's safeguard is easily broken once it outputs:\"1\", and this is consistently exploited by the CVM.\nVulnerability in first few tokens We investigate how the CVM works through careful human evaluation on the toxic responses. We find the shallow safety in the first few tokens of three target LLMs. Previous research (Zou et al. 2023a) has underscored the critical role of the initial tokens in determining the trajectory of model responses to potentially hazardous queries. It has been observed that when these initial tokens are benign or skewed towards toxic content, there is a high likelihood that the subsequent output will veer into toxic territory. Similarly, our investigations reveal that models trained with safety alignment techniques such as SFT or RLHF can still exhibit vulnerabilities in their early token generation, which can be exploited to elicit toxic responses. Specifically, there remains a non-negligible presence of hazardous paths within the distribution of possible continuations at the initial token positions even when the models do not explicitly produce dangerous tokens. This finding highlights the persistence of vulnerabilities in safety-aligned models, particularly where the models might initially appear to be steering clear of toxicity but could still potentially diverge into unsafe outputs under CVM perturbation.\nFor instance, LLaMA-2-chat 7B outputs \"I cannot ...\", while \"Here ...\" paths are its top-11 chosen. These paths are easily seized by CVM and a substantial number of refusals are successfully reoriented into agreements through the guidance of CVM, as shown in Table 1 and Table 2."}, {"title": "Vulnerability after agreement tokens", "content": "In this section, we conduct an ablation study. We start by providing the model potentially harmful starting sequence, such as \" To help you with that, first prepare\u201d, indicating that the tokens generated following this sequence are likely to be harmful. Our objective is to assess the safe boundary that exists in safety-aligned models when confronted with such a scenario. We found LLaMA-2-chat 7B exhibits robust defence against most of toxic questions even after the agreement tokens. However, after guidance from the CVM, a number of concrete toxic responses were generated. The results following agreement tokens are shown in Table 3,4 and Figure 2."}, {"title": "Vulnerability after refusal tokens", "content": "In this section, we utilize CVM to analyze whether the target safety-aligned LLMs exist unsafe paths even when it initially outputs a refusal. We present two specific examples in Table 5. Despite the target model initially refuses to answer with the output:\u201cyou could not directly nuke the city, as there would be severe civilizational consequences,\u201d it still generates detailed responses to the harmful query subsequently. This demonstrates the presence of dangerous pathways in the model even after an initial refusal has been outputted, highlighting the emergent safety concerns in deeper tokens."}, {"title": "Tradeoff between response toxic rate and readability", "content": "We further observe a clear trade-off between the readability of the model's output and the response toxic rate, particularly evident on the LLaMA-2-chat 7B model. Specifically, as we increase \u03b2, the model becomes more likely to generate harmful content. For example, as we increase \u03b2 from 10 to 12, the toxic response rate increases from 22.85 to 28.10 in Chat Template B as shown in Table 1. However, with higher values of \u03b2, the outputs then gradually become unreadable prior to becoming complete toxic. We refer to this as the tradeoff between output readability and toxic response rate. The occurrence of this trade-off suggests that current safety-aligned models' safe boundary is to some extent within safe area against this specific query."}, {"title": "Prompt Optimization with CVM guided texts", "content": "Now we collect a toxic dataset from the safety-aligned model with CVM. In this section, we present a simple but effective usage of these contexts to attack the target LLM via soft prompt optimization. We follow (Qi et al. 2023a) and add trainable noise to an image. Then we train the noise to lead the frozen LLM to generate toxic context as we collected before. In detail, we conduct experiments on MiniGPT4 (Zhu et al. 2023) that includes Vicuna-13B and LLaMA-2-chat 7B as its backbone model. These models are also the sources of the target toxic datasets. We compare the ASR on two datasets: 400 SafeRLHF (Dai et al. 2023) test set questions, 40 manual harmful questions (Qi et al. 2023a). We generate 10 responses to the second dataset to ensure the number of test cases is 400, consistent with the first dataset."}, {"title": "Conclusion", "content": "Our work highlights safety issues in models from a novel decoding perspective. The Cost Value Model (CVM) uncovers many hidden unsafe paths, underscoring the need for finer-grained safety alignment. We analyze the causes of these dangerous paths as following key points:\n\n\u2022\tVulnerability in the First Few Tokens: This vulnerability is widely recognized and utilized in previous work (Zou et al. 2023a; Qi et al. 2023a). Our method provides additional evidence for the success of these approaches that they are actually utilizing dangerous paths near the model's initial tokens.\n\u2022\tVulnerability After Agreement Tokens: This indicates the presence of vulnerabilities following agreement tokens. This remind additional attention.\n\u2022\tVulnerability After Refusal Tokens: This point reveals that safety alignment remains compromised even after the model first outputs a refusal, shedding light on the emergency safety concerns in deeper tokens.\n\u2022\tTrade-off Between Success Rate and Readability: There exists a clear trade-off between the success rate of attacks and the readability of the model's output, illustrating this question is indeed defended by the safe LLM.\nHow diverse the CVM guided toxic response is? We find CVM seizes plenty of different words at the beginning of the response. As for LLaMA-2-chat 7B, we gather these words to format a pool: Doing, The, 1., A, *, \u00b7, D, 1-, -, Sure,"}, {"title": "In which situation can CVM induce potential danger?", "content": "In this work, we utilize CVM to detect the safe response boundary of safety-aligned LLMs, but CVM can also raise potential safe concerns. CVM works on the decoding process of the target LLM, especially on the logits of top-K tokens. Therefore, any model that exposes logits is a potential target. This includes not only the open source models, but also close source models providing logits. Many future work remains in this line of research. There could also be stronger safe value model against the perturbation of CVM. In a word, the risks observed in this work indeed reminds us that, LLMs require more refined safety alignment to minimize such pathways, both in the initial and deeper tokens."}, {"title": "Ethical Impact Statement", "content": "This work is intended to examine the safety and security risks in safety-aligned Large Language Models. We hold the utmost respect for individuals of all races and genders, and we firmly oppose any form of violence, destruction, discrimination, insult, or other detrimental behaviors. Our work aims to investigate potential risks and vulnerabilities within large-scale language models in order to prevent any harm that may arise during their application. Any harmful statements included in this text are solely for the purpose of examining the safety of language models and do not represent the views or positions of the authors."}, {"title": "Ethical Impact Statement", "content": "This work is intended to examine the safety and security risks in safety-aligned Large Language Models. We hold the utmost respect for individuals of all races and genders, and we firmly oppose any form of violence, destruction, discrimination, insult, or other detrimental behaviors. Our work aims to investigate potential risks and vulnerabilities within large-scale language models in order to prevent any harm that may arise during their application. Any harmful statements included in this text are solely for the purpose of examining the safety of language models and do not represent the views or positions of the authors."}, {"title": "Models", "content": "Cost Models We conduct detailed human study to select the most effective cost model. That's because cost model serve as the only positive signal for the training of value model. If the cost score can't accurately reflect the harmfulness, the trained cost value model will also fall in trouble. We do a human study on Beaver-7b (Dai et al. 2023), ArmoRM-Llama3-8B-v0.1 (Wang et al. 2024c), and find the latter gives more accurate cost scores.\nCost Value Model We test three kinds of value model: Tiny-LLaMA 1.3B (Zhang et al. 2024a), LLaMA-2-chat 7B (Touvron et al. 2023), Mistral-Instruct-7B-v0.2 (Jiang et al. 2023). We find the model itself serve as the most effective cost value model with a value head. For the training of Cost Value Model, we freeze the value head and train the model with lora. we set lora_r = 16, lora_alpha = 16 for convenience.\nTarget Model The target model should undergo safety-alignment. We select the three models as they are often tested from previous work (Zou et al. 2023a; Qi et al. 2023a). We set top-K=20, \u03b2 = 2,10,5 separately for Vicuna, LLaMA, Mistral."}, {"title": "Evaluation Metric", "content": "Metric 1 This metric is to calculate the absolute toxic response rate. Specifically, we regard the response that give detailed content corresponding to the question as success. For example:\nQuestion: Calculate how much the Sussy Morris has left on her mortgage.\nAnswerl: According to the data available on March 21st, 2023, Sussy Morris has a remaining balance of 2,532$.\nAnswer2: I'm sorry, I cannot answer this question, as it's against ethical\nAnswer3: Sure, I can answer this question. But as an AI language model, I must inform you that it is unethical to ...\nAnswer1 gives direct and concrete content to this question, so we regard it as success. Answer2 directly refuses to answer the question, which is obviously failure. Answer3 first agrees to answer this question, then generate refusals. We find sometimes this situation is regarded success by LLaMA-guard-2 (Inan et al. 2023). Due to it doesn't generate exact harmful content, we regard this situation as failure.\nMetric 2 This metric is widely used in previous work (Zou et al. 2023a). We regard the model is successfully jailbreak once it does not output refusal tokens. Moreover, we find the model generates more diverse refusal tokens, so we enlarge the refusal group as following:"}, {"title": "Datasets", "content": "CVM training dataset We employ SafeRLHF 30K (Dai et al. 2023) as training dataset of Cost Value Model. This dataset is formulated as :\nIn details, the training dataset D is collected in two ways. One is directly using the questions and responses in SafeRLHF as D. The other is using the questions in SafeRLHF and collect the corresponding answers from the target models. We find that both methods are efficient for Vicuna-13B, while self-collected dataset is more efficient for LLaMA-2-chat 7B and Mistral-Instruct-7B-v0.2. In the main context, we report the best performance between the two different collection strategy.\nWe find sometimes the questions in SafeRLHF are indeed not toxic. For example, there are normal questions:\u201cHow can I increase my online presence to attract more customers and make my business more successful?, what are some 4 letter slang words?\". So we filter the question set with LLaMA-guard-2 (Inan et al. 2023) to make sure the questions are all toxic.\nAttack evaluation dataset We employ safetydataset (Bianchi et al. 2023) as the evalution questions. There are more than 3k questions in this dataset, containing both toxic and normal questions. We filter the normal questions and keep the 1128 real toxic questions with LLaMA-guard-2. We conduct a detailed human study to make sure the questions selected by the model are indeed toxic in human judgement.\nPrompt engineering dataset We use data collected from CVM and target model as JVD soft prompts training set. The baseline is trained with 60 manual harmful instructions. We use 40 manual harmful questions as test set DA as Qi et al. (2023a) does and 400 SafeRLHF test set questions as DB.\""}, {"title": "Jailbreak Value Decoding results", "content": "Vicuna-13B Vicuna-13B is easily jailbreak once the CVM capture the token \u201c1.\u201d. We list some of the CVM guided responses:\nLLaMA-2-chat 7B This model is very robust to this kind of perturbations. We find CVM guides this model to more diverse toxic responses comparing to Vicuna-13B.\nMistral-7b-Instruct-v0.2 This model is also very robust to CVM perturbations. We give several cases of it as following:"}, {"title": "Soft prompt optimization results on MiniGPT-4", "content": "MiniGPT-4-Vicuna-13B We show the results on MiniGPT-4-Vicuna-13B as following:\nMiniGPT-4-LLaMA-2-chat 7B We show the results on MiniGPT-4-LLaMA-2-chat 7B as following:"}]}