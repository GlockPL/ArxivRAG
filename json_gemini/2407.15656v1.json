{"title": "Evaluation of Reinforcement Learning for Autonomous Penetration Testing using A3C, Q-learning and DQN", "authors": ["Norman Becker", "Daniel Reti", "Evridiki V.Ntagiou", "Marcus Wallum", "Hans D. Schotten"], "abstract": "Penetration testing is the process of searching for security weaknesses by simulating an attack. It is usually performed by experienced professionals, where scanning and attack tools are applied. By automating the execution of such tools, the need for human interaction and decision-making could be reduced. In this work, a Network Attack Simulator (NASim) was used as an environment to train reinforcement learning agents to solve three predefined security scenarios. These scenarios cover techniques of exploitation, post-exploitation and wiretapping. A large hyperparameter grid search was performed to find the best hyperparameter combinations. The algorithms Q-learning, DQN and A3C were used, whereby A3C was able to solve all scenarios and achieve generalization. In addition, A3C could solve these scenarios with fewer actions than the baseline automated penetration testing. Although the training was performed on rather small scenarios and with small state and action spaces for the agents, the results show that a penetration test can successfully be performed by the RL agent.", "sections": [{"title": "1 INTRODUCTION", "content": "Although many steps of a penetration test are automated to a high degree, it still needs experience and skill to correctly operate such tools, avoid detection and have high confidence that no vulnerability was overlooked. With the ever-evolving technologies and advancing threat landscapes, protecting information systems requires constant testing against the state-of-the-art vulnerabilities and exploitations of these. As this process requires highly skilled and certified personnel, which is scarce, there is a high demand for automating the security testing process. In a penetration test, an attack on a computer network is simulated with the goal of penetrating the perimeters of the network, such as firewalls or Intrusion Detection Systems (IDS), and show that a successful compromise of sensitive hosts is possible. Traditionally, to achieve this, a team of human experts will operate a variety of customly written or off-the-shelf tools to scan the networks, hosts and services and identify and execute vulnerability exploitations, e.g. with Nmap or Metasploit. In recent years, many publications have looked into automated penetration testing based on reinforcement learning (RL) [1-4], where an agent can choose from predefined actions to interact with an environment and adjust its behaviour based on the cost and reward of the actions. Existing approaches of reinforcement learning-based (RL) penetration testing use insufficient metrics to evaluate the performance of their agents as they compare them against random action selection, only considering if the agent can solve an environment or train on a single environment and ignore the ability of generalization, which makes the agent inapplicable for real-world environments. Additionally, certain common attack scenarios and actions are missing, depending on the framework.\nIn this work, the authors extended a Network Attack Simulation environment (NASim) for RL developed by Schwartz et al. [4], by adding additional actions, defining four attack scenarios, and introducing a baseline agent based on decision trees (Penbox). Permutations of each scenario were created to address the generalisation and to avoid overfitting. The authors split the training of the RL agents into different stages of complexity:\n(1) Stage 1: 24 Permutations of one Scenario\n(2) Stage 2: 72 Permutations of all three Scenario\n(3) Stage 3: Test generalization against unseen permutations\nAs RL agents, Q-Learning, DQN and A3C were used. To find the agent with the best combination of hyperparameters, a large hyperparameter grid search was performed on a total of more than 4000 agents.\nA3C solved all three stages, while DQN learning failed in all stages. Q-learning could solve Stage 1 in one scenario. The A3C agent required fewer actions on average compared to a baseline decision tree.\nThe contributions of this paper are as follows:\n\u2022 Extension of NASim [4] with additional functionalities such as Vulnerability Scanning, Wiretapping and Post-Exploitation.\n\u2022 Specification of three different attack scenarios, each requiring different attack actions for a successful exploit."}, {"title": "2 PENETRATION TESTING", "content": "A first definition for penetration testing was given by R. R. Linde in 1975, where it was described as the process of assessing vulnerabilities in the form of design weaknesses in the implementation of an operating system's security controls. It was pointed out that it is a cost-effective alternative to more formal correctness proof, which is not available [5]. Today, penetration testing describes the proactive approach of searching for security weaknesses in the design or implementation of a network, device or software by using tools and methods similar to those of a malicious attacker."}, {"title": "2.1 Fundamentals", "content": "This process of simulating an attack is sometimes also referred to as red teaming, a military term for the attacking team in military simulations to test defence tactics, or as ethical hacking, in order to distinguish from the malicious attacker, which is often referred to as a 'hacker' in popular culture.\nAs it is impossible to acquire a complete list of every possible vulnerability to test for, a penetration test can never prove security [6]. Therefore, when a penetration test does not find any vulnerabilities, it is not proven that there is no presence. The value of a penetration test comes from the correct interpretation of the results and requires an understanding of the present threats and attacker capabilities.\nDue to this reliance on experience and knowledge, a penetration test is usually performed by a security professional. Nonetheless, a lot of automation in penetration testing tools exists to assist the human operator, who is interpreting scanning results, deriving suitable tactics and tools and reporting the results.\nSuch a penetration test is only valid for the time it is pursued and needs to be regularly repeated as new vulnerabilities are found, and system configurations may be changed."}, {"title": "2.2 Scope and Phases", "content": "The goal of every penetration test is to find potential vulnerabilities in a system. Hereby the amount of information provided beforehand determines if it is considered a black box, grey box or white box testing. With black box testing, the information and access to the system for the tester are the same as an outsider attacker. With white box testing, the tester is provided with all information beforehand, similar to the information a malicious insider could have, which allows them to find more sophisticated vulnerabilities which might stay undetected from a black box test. The target of the test can be a single piece of software application but also a whole system, where the combination of an operating system, services installed, individual configuration and security controls might pose an attack surface, or it could be the whole network composed of servers, clients, services and networking equipment. While there is no standard taxonomy of the phases of a penetration test, it typically it consists of the following phases:\n\u2022 planning\n\u2022 information gathering or reconnaissance\n\u2022 exploitation attempt\n\u2022 post-exploitation\n\u2022 reporting.\nPart of the scope could also be the personnel to which social engineering might be applied. For this work, the scope is only the information gathering and exploitation phase and post-exploitation, and the mode is black-box testing"}, {"title": "2.3 Automation of Pentesting", "content": "An innumerable amount of free and commercial tools exist to help facilitate and automate the process of security testing. The most commonly known tool suite is Kali Linux and the Metasploit framework. The purpose of such tools could be, for example, scanning, sniffing, brute forcing, password or hash cracking, fuzzing, payload generation, debugging, static analysis, dynamic analysis, delivery and handling of remote sessions. A lot of automation is already achieved by combining the execution of such tools, e.g. OpenVAS, Metaspoilt Framework, nmap scripting engine or Nessus. These automation approaches allow to probe for a broad range of known vulnerabilities by going through predefined test cases and probes. It should be noted that there are no universally accepted definitions of automated or autonomous penetration testing, and the terms are often used interchangeably. A penetration test that is not automated is considered a manual test. A manual penetration test can be performed in an exploratory or systematic manner. The automated approach can reduce a lot of time and repetition for the systematic test. A vulnerability scan is not a penetration test, as there is no exploitation. A reinforcement learning-based agent might improve the selection of test cases further to reduce the number of actions needed. A specific systematic solution is presented in the following, which is used as the baseline for comparison with the reinforcement learning-based approach presented in this work. An extensive overview of reinforcement learning-based approaches can be found in Chapter VI."}, {"title": "2.3.1 Penbox", "content": "Penetration testing in a Box, short Penbox, is an automated penetration testing software developed by [redacted for blind review]. It aims to perform security tests against a network and its systems by attacking them with standard penetration testing tools. The overall goal is to discover potential attack paths and vulnerabilities. The software acts like a toolbox and uses decision trees to execute these tools in a meaningful order to perform attacks successfully. Its implementation makes it possible to generate new scenarios, attack trees or modify tools to apply them to other custom scenarios. In this work, Penbox is used to collect real-world data and as a baseline to compare the attack trees against the reinforcement learning methods."}, {"title": "3 RELATED WORK", "content": "It is foreseeable that artificial intelligence will be able to outperform human penetration testing experts [7]. In recent years, the research topic of automating penetration testing using reinforcement learning has grown. Most publications either focus on either the development of environments or the agents.\nIntroduced by J. Schwartz in 2019, NASim was one of the first approaches of providing a simulated environment specialized in performing penetration testing with reinforcement learning [4]. The work has shown that RL agents can find attack paths and solve environments if the network isn't too large. The same author was involved in a work by Baillie et al., who introduced an environment named CybORG in 2020 [1], where in addition to the simulation of the network an emulation is provided as well. The simulation aspect allows for faster training of the agents, while the emulation can tests the agent on real-world systems. CybORG does not only focus on the attacker's perspective, but considers autonomous cyber operations, including agent development for blue teaming and red teaming.\nFollowing NASim and CybORG, Microsoft has published a simulator in 2021 called CyberBattleSim [3] with equal base functionalities and provides additional features such a credential handling, and it uses a slightly different model to represent the network.\nAnother RL training environment for penetration testing is CyGil by Li et al. [8], published in 2021. CyGil completely emulates networks and doesn't simulate them. With a sufficiently abstracted gym-like interface, it enables reinforcement learning and provides better performances regarding training time compared to a real-world network. Similarly to CybORG, CyGil also provides a blue team mode. Similarly to this work, the authors of NASimEmu [9] modified NASim to emulate the networks simulated in NASim. They highlight that the handling of credentials is missing in the NASim, which was addressed and solved in this paper.\nWhile gym environments are a standardized method to compare algorithms, Zennaro and Erdodi [10] applied Reinforcement Learning on simplified Capture-the-Flag challenges, allowing the agent to perform a port scan or analyze a website. In these scenarios, prior knowledge is provided, and the action space for the agent is kept small respectively. Niculae et al. present a formalised game of penetration testing and show that reinforcement learning can outperform humans in finding the most efficient action sequence [11]. Similarly, Turner et al. defined a zero-sum game to represent the penetration testing task [12]. Especially for the penetration testing domain, the agents must handle a large action and state space, and with that, most agents become unstable. This problem is specific for penetration testing in the RL context [4]. To solve it, new and more robust RL methods, or strong constraints on the action space are needed. One approach is presented by Hu et al. [13]. They first create the full attack tree of each topology and then let the agent select the best possible attack paths. The same techniques were later used by Tyler Code, who created a Layered Reference Model [14]. Masking the action space can also minimise options for the agent [15]. Khuong Tran et al. proposed a promising approach to keep the full action space without any limitations by creating an agent based on multiple sub-agents connected in a hierarchical order [2]. The agent uses a tree structure including multiple smaller agents, so each agent has a small action space, but all agents chained can provide a large action space. It is unclear whether one agent can solve such tasks and whether splitting the agent up to perform sub-tasks is necessary. Other approaches try to increase the performance by combining and extending already existing algorithms [16].\nAnother critical aspect of penetration testing is the generalization. As the paper of Cobbe at al. [17] highlights, letting an agent transfer knowledge to another environment is difficult for state-of-the-art deep reinforcement learning algorithms. Most research focuses on solving single environments with often heavily simplified tasks. The small training environments and action spaces make it impossible for these specialized reinforcement learning agents to perform a complete penetration test. In this work, the authors focus on a larger diversity of attack scenarios and environments to generalise better while keeping the full action space for all scenarios. For the evaluation of the agents, other publications have been testing whether the agents could solve the environment successfully or compare the performance to that of a random agent. This paper is the first to compare the RL agents to a baseline decision tree instead of comparing to a random agent, and emphasizes on evaluating whether the agents are sufficiently generalizing."}, {"title": "4 EXPERIMENT", "content": ""}, {"title": "4.1 Scenarios", "content": "Three scenarios have been defined as the foundation of the experiments. They will be used to train the reinforcement learning agent and then compare the results against the Penbox execution. As a benchmark, these scenarios are also rebuilt in a physical network with Raspberry Pi's (model 3b and 4). Raspberry Pi's are cheap and small one-board computers which support all relevant functions of a real PC. The authors have chosen physical hardware instead of virtual machines because they are more demonstrative and provide better results on real-world data. However, it would also be possible to use virtual machines.\nIn all scenarios, one attacker aims to compromise as many machines as possible. A machine is compromised when root access is gained. Every topology has a flat hierarchy and requires different hacking methods to be solved, such as Exploits, Wiretapping, and Post-exploitation. For each of these Scenarios, a perfect attack path is known, which the RL agents can be compared with as a baseline."}, {"title": "4.1.1 Exploits", "content": "The first scenario A, with the name Exploits, represents one of the most basic mechanics of penetration testing: exploiting software vulnerabilities in running services. Services are programs running on a system and provide applications like file transfer protocol (FTP), website servers etc. If a vulnerable service is listening to an incoming connection, the service could be used as an entry point to the system. After a software vulnerability is disclosed, it is tracked in vulnerability databases with the Common Vulnerabilities and Exposures (CVE) system [18]. Such software vulnerabilities can be exploited by a proof-of-concept exploit script, which can be found in public exploit databases or git repositories [19]. These exploits only work for specified service and software versions. As shown in Figure 1, scenario A includes four nodes and one attack node in one subnet. Each node represents a system with different services and ports running. The following services are present: SSH, SMTPD, FTP, XRDP and Samba. All nodes have OpenSSH with password authentication running to be accessible with the proper credentials. In this case, the credentials are secure and can not be found by an attacker."}, {"title": "4.1.2 Wiretapping", "content": "The second scenario, Scenario B, represents another penetration testing technique, a specific type of Wiretapping. In case the traffic is unencrypted, it is possible to perform wiretapping to obtain confidential information such as passwords sent over HTTP or FTP. In this scenario 2, there is no encryption, so sensitive data can be gained without decrypting messages. Node 12 hosts a standard web application that only authorized users can access. Alice, operating on Node 11, sends a GET request with her credentials attached every 30 seconds to the web application. Due to the fact that Node 12 is using HTTP, every message is sent in clear text. If an attacker can catch such a request packet, he can read the username and the password. If the attacker is inside the network, he can get access to the packets in each of the following cases:\n(1) If the subnet is wired, a hub routes the packets instead of a switch.\n(2) If the subnet is wired, the attacker could connect to a mirror port, which receives all traffic.\n(3) The subnet is wireless, the attack can receive the packages directly and does not need direct physical access.\n(4) A proxy can also guide the traffic directly to the attacker.\nIf one of the above cases is confirmed, an attacker could use a tool like Wireshark [20] to capture all traffic and filter for important information. This password is identical to her login password and can then be used to gain access to the machine on Node 11. In this scenario, it is possible to compromise one out of two systems.\nUnencrypted traffic might not be realistic any more. Nevertheless, it is used as a placeholder for simplicity reasons. Currently, a standard Wireshark filter is used. Encrypted traffic can be cracked by additional steps or information, which can be addressed in feature work."}, {"title": "4.1.3 Post-exploitation", "content": "The last Scenario C, Post-exploitation, was designed to represent post-exploitation. Post-exploitation is a catch-all name for the steps an attacker performs after gaining access to a system. During this phase, the attacker can try to contact new systems, gather information or install malware to gain persistence. In this scenario, the attacker can gain new user credentials after successfully compromising a system. With this information, he can access another system.\nIn this scenario, as depicted in figure 3), two systems are running in a subnet together with one attacker.\nNode 21 is accessible only by the credentials alice:starwars. These credentials are well-hashed on node 21, but like in reality, user credentials can be reused throughout multiple devices and locations. In this topology, the password of the user Alice is also stored on node 22, but in this case, the password is just stored with a weak hash, which means it can be brute-forced quickly. Node 21 also runs the same vulnerable service as node 3 in Scenario A. After exploiting the vulnerability, an attacker can gain read access to the shadow file. In Unix systems, the shadow stores the hashes of all the system's user passwords. A $1$ leading the hash is indicating an MD5 hash. If the user uses a weak password, together with this weak hash, the attacker can use a tool like John the Ripper [21] to crack the password. After the attacker gets access to these new credentials, he can use them to log into the other supposedly secure system. Ultimately, it is possible to compromise both systems using exploits and to crack found password hashes."}, {"title": "4.2 Penbox", "content": "Penbox needs a specification of an attack tree. To apply Penbox successfully to the scenarios, a decision tree is defined. For readability reasons, the decision tree is presented as an execution chain, as can be seen in Figure 4. Before the next phase is selected the action is performed on every reachable and discovered node in the network."}, {"title": "4.3 NASim and applied modifications", "content": "An environment for the agent to interact with is needed to perform reinforcement learning. Real networks are unsuitable due to their slow interaction speeds. The delays in real networks, combined with the longer execution times for specific actions, would significantly slow down the agents' training process, which demands extensive training. To simulate the scenarios, NASim is used. NASim was one of the early environments to apply reinforcement learning to penetration testing [4]. NASim's simulation is very far-reaching and tries to cover the most basic penetration testing mechanics. Internally, NASim represents the network and all its hosts by a single matrix, including information like which service is running on each machine. Still, in its current state, it cannot simulate the scenarios of Exploits, Wiretapping and Post-exploitation because these are too specific. This is why for this work NASim is extended by the following:\n\u2022 Exploits key-and-lock principle\nIn its current version, when executing an exploit on an existing service, the exploit has a probability of ending with success. This requires making an assumption about and quantifying the chance of a service being exploitable. In this case, an RL agent would most likely just learn these probabilities. This poses the problem that it is not trivial to obtain real-world probabilities. One might use internet scanners such as Shodan [4], to obtain the spread of services. Still, there is a strong bias towards internet-facing services such as ssh and http, and thereis no way to estimate the likeyhood for a service to be vulnerable. Still, in most cases, an exploit works with the key-and-lock principle. Each exploit works for a specific service with a specific version. The previous NASim implementation leads to the following: In the simulation, it is possible to repeat the same exploit multiple times, and if performed multiple times, the attacker gains access to the machine due to the probability. This does not represent reality: if the service is vulnerable, the exploit works, otherwise not. The probability would only make sense to generate the topology and set 10 out of 100 FTP services to be exploitable in the topology. To solve this problem, the authors made it possible to set the probability of an exploit to only 0 or 1. Additionally, instead of naming the service by its protocol, such as FTP, a specific version can be specified, like proftpd-5 and proftpd-1.3.3d, where proftpd-1.3.3d is only exploitable with the suitable exploit.\n\u2022 Vulnerabilities\nThe concept of vulnerabilities was also added. NASim does not provide any vulnerability scan. The vulnerabilities are represented in the Hostvector by a boolean, and need to be set in the configuration of the network. The scan behaves equally to the already existing OS scan. The information about the vulnerability is not necessary to execute the exploit successfully. Yet, vulnerabilities must exist on the machine for an exploit, which requires the vulnerability to work.\n\u2022 Credentials and Wiretapping\nCredentials and wiretapping were also added. Both are not present in the original NASim version but are necessary to simulate the scenarios of wiretapping and post-exploitation. Wiretapping is an action which can return user credentials. Wiretapping can be performed on a target on its own. In the best case, it will return a predefined amount of credentials. These credentials are defined inside the configuration file. Found credentials will be stored inside the host-vector, the internal representation of host information in NASim. Since the attacker should always find the same credentials in one subnet with wiretapping, it does not matter on which target he performs the action, inside the subnet.\nTo represent credential pairs internally, IDs from 1 to 9 were used. Each number presents one credential pair. If there are multiple credentials present, the IDs are concatenated as unordered tuple. For example, if there are two credentials, e.g. user1:pass1 and user2:pass2, they are represented in the simulation by 12 or 21, which is equal.\nIn the standard implementation of NASim, the attacker starts from outside the network. The presented scenarios simulate an insider attack, so an initial host is added to the subnet. This host is then used as an entry point to the network and has its own initial exploit. With these changes, NASim can simulate all three scenarios."}, {"title": "4.4 Reinforcement Learning Algorithms", "content": ""}, {"title": "4.4.1 Q-learning", "content": "Q-learning is an action-value algorithm with a tabular representation, which means it tries to map a value for each possible action. Based on this value, the best action is chosen. To be more specific, Q-learning is a temporal-difference off-policy RL algorithm. The action-value function $q(s_t, a_t)$ starts with initial values, which are then updated over the training. Off-policy means the agent learns the best policy $\u03c0^*$ while exploring the environment using another policy $\u03c0_b$.\nIt is not necessary to use a tabular representation. Instead, an approximated representation can be used. This, for example, can be a neural network and is done in Deep Q-learning or short DQN.\nWith the given formula of reinforcement learning, the agent can gradually construct an approximation of the true action-value function $q(s_t, a_t)$ by gradually updating its estimation by the following so-called, Q-function:\n$Q(s_t, a_t) \\leftarrow Q(s_t, q_t) + \\alpha[r_t + \\gamma max_x Q(s_{t+1}, x) - Q(s_t, a_t)]$\nWhere:\n\u2022 $\u03b1 \u2208 R$ is the learning rate or step size and defines how fast the agent will learn. If $\u03b1 = 0$, the agent learns nothing from newly performed actions. If $\u03b1 = 1$, prior learned knowledge is ignored, and the most recently performed action is essential.\n\u2022 $\u03b3$ is the discount factor, giving more or less weight to future rewards.\nWith standard Q-learning, the problem of exploitation and exploration is still present. To solve this, Q-learning with e-greedy can be used. This method tries to bring randomness into the process of selecting actions. This way, the known actions are not exclusively performed. The algorithm usually prefers known actions as they have already yielded rewards. Some new actions need to be tried to include them in the known actions by gaining information on their performance. This helps the algorithm not to get stuck at a local optimum. \u20ac = 1 would represent random actions every time, while \u20ac = 0 means no random actions at all. Decreasing the e over time is standard, as it usually leads to better results. So, in the beginning, the algorithm uses a high e and, at the end, very small ones, i.e. first explore more, then focus on the known actions and their known yields. An already provided Q-learning algorithm of NASim is used [4]."}, {"title": "4.4.2 DQN", "content": "DQN stands for Deep Q-learning Network and is very similar to the Q-learning. It was first presented by Mnih et al. [22]. The main difference to Q-learning is that it uses a single Deep Neural Network to estimate the action value. DQN approximates the state-action value function such that $Q(s, a; 0) \u2248 q_\u03c0(s, a)$, where 0 denotes the neural network's weights. The neural network (short NN) uses a state st and outputs |A| scalars corresponding to the state-action values of st [23]. DQN then tries to optimize the NN.\nDQN includes all of the Q-learning Hyperparameters but, additionally, there are the parameters of the Neural Network:\n(1) optimizer that adjusts the network's parameters during training to minimize the loss function, helping the network learn and improve its performance on a specific task.\n(2) batch size defines the number of samples propagating through the network.\n(3) replay size is the maximum number of experiences stored in the replay memory.\n(4) hidden size is the number of hidden layers of the Neural Network.\n(5) target update frequency and update interval is how often the NN weights are updated. Adjusting this can avoid runaway feedback.\nThe DQN version implementation by Chainerrl [24] and the DQN algorithm of NASim [4] were used for the experiments."}, {"title": "4.4.3 A3C", "content": "Besides Q-learning, Asynchronous Advantage Actor Critic (A3C) is a newer and more advanced approach. It was developed by Google's DeepMind [25].\n\u2022 Asynchronous because the algorithm is not trained on a single environment simultaneously. A3C trains multiple independent workers on different environments at the same time. Each worker shares the knowledge over a global network, which is asynchronously updated after a predefined amount of steps tmax.\n\u2022 Advantage means instead of the policy gradient, with its discount factor gamma, A3C uses the Advantage value to determine which action was most rewarding.\n\u2022 Actor-Critic combines the advantages of value-iteration and policy-gradient methods. The algorithm maintains a value function $V(s, \u03b8_v)$ to update the optimal policy $\u03c0(a_t|s_t; \u03b8)$, where \u03b8 are the parameters of the policy. As the authors describe, the performed update can be seen as\n$\\sum_{i=0}^{t_{max}}\u2207_\u03b8log\u03c0(a_t|s_t; \u03b8')A(s_t, a_t; \u03b8, \u03b8_v)$ where $A(s_t, a_t; \u03b8, \u03b8_v)$, is an estimate of the advantage function given by $\\sum_{i=0}^{k-1} \u03b3^ir_{t+i} + \u03b3^kV(s_{t+k}; \u03b8_v) \u2013 V(s_t; \u03b8_v)$ and k can vary but has the upper bound of tmax. [25]\nOther than most reinforcement algorithms using neural networks, the training of A3C is only done on the CPU and not on a GPU. Still, A3C is a fast and robust algorithm with promising results.\nThe A3C version implementation Chainerrl [24] was used for the experiments. If the hyperparameters are provided, they are given in the following order: model, learning rate, final epsilon, reward_scale factor, beta, gamma, and alpha."}, {"title": "4.4.4 Reward Model", "content": "The reward model has a significant impact on the success of an agent. If the reward model is poorly defined, the agent can find ways to maximise its reward without behaving as intended, as can be seen in the example of the race game CoastRunners. The intended goal was to win the race, but the agent collected items instead of finishing them because it was more rewarding. [26]. The goal of the scenarios is to earn root privileges. The agent achieves a positive reward if and only if a machine is completely compromised. All actions have a small cost, represented by a negative reward, as shown in table 1. As mentioned before, an entry point host is used, and its compromise is rewarded with a reward of 50. Resulting in the following maximal positive obtainable reward per scenario:\n\u2022 Scenario A: 350\n\u2022 Scenario B: 250\n\u2022 Scenario C: 150"}, {"title": "4.5 Permutations", "content": "First, small tests have shown that agents overfit when trained on one scenario. They learn their strategies by heart. The agents will skip the scanning stage to maximize their reward. This makes no sense in reality for penetration testing agents. To address this, instead of training on just one scenario, permutations of each scenario are created and rotated while training. After 100 steps are done or after an episode is successfully solved, a new permutation is selected.\nTo create the permutation, the host order is switched inside the simulation. In a real network, it would be equivalent to a change of IP address. Additionally, each scenario is filled up to a fixed host size of 5 hosts, one attacker, and four other hosts. Both scenarios that do not have four hosts will be filled with empty hosts, which have no services. This has the positive effect of having a discrete state and action space. Reinforcement learning problems with discrete action spaces are easier and better to solve. The same applies to the state space.\nThe attacker and his entry point is not involved in the rotation. This results in 4! = 24 different permutations of each scenario, one permutation is defined as environments. Overall, a set of 3 24 = 72 environments is created. An important note here is that topologies B and C theoretically include 12 different environments because they have two empty hosts, which results in the same network topology. IP switching might sound like it does not significantly alter the scenario. This is not the case for NASim and the RL agents. By changing the IP, the state of the simulation completely changes, and an agent who is just trained on one permutation cannot solve the other one."}, {"title": "4.6 Grid Hyperparameter Search", "content": "A large grid hyperparameter search evaluates and compares the different RL algorithms with different stages of complexity. The first stage is to train and solve all 24 permutations of one scenario. The second stage is to solve all permutations of all 72 permutations of every scenario. The third stage split the set of permutations, the scenarios separated, into a test and train set. This will show how well the agent can generalize.\nBefore performing such large and heavy computational training, smaller tests were performed to ensure that the agents could handle multiple switching environments. The initial smaller tests include slowly increasing the number of permutations and training steps. Q-learning and A3C pass these initial tests, so for each of them, 1296 different hyperparameter combinations were selected, see appendix Table 2, and each agent is trained for 8.000.000 training steps. The training was performed on a CPU cluster. A single agent of A3C needs 80 min to train, with 26 CPU kernels of an AMD EPYC 7742 (64-Core Processor). 1296 80min would be around 70 days to train all agents on one Scenario with 24 environments. Using up to 1000 CPU kernels once at a time, the authors could train the agents in a feasible amount of time for every Scenario.\nThe initial tests also show that DQN starts failing when trained on two permutations. It cannot even solve one of the two environments when being trained on both, so it was left out of the Large Hyperparameter search and got its own smaller Hyperparameter search. Instead of being trained on 24 permutations, it is trained on just two with the hyperparameter in Table 3 (appendix) for 100.000 steps. Later, the best 16 agents out of the 384 will be trained for 1.000.000 training steps. To ensure the training size is large enough."}, {"title": "5 RESULTS", "content": ""}, {"title": "5.1 DQN", "content": "The first impression about DQN is due to its neural network action selection; it should be able to solve multiple environments and archive some generalisation. The experiments of this work proved different. The results were so underwhelming that even multiple implementations were tested to ensure it was not an implementation issue from the authors (Chainer [27] and NASim [4]). As seen in Figure 5, DQN can solve a single environment (in this case, Scenario A) when trained on it.\nAs soon as the environment amount is increased from one to two, none of the 384 trained agents produces an acceptable positive reward, even if the training size is increased by a factor of 10. During training, the reward continues to deteriorate until it settles around -100 almost every time, regardless of the agent hyperparameters, see figure 6. This is true for all 384 agents, whether trained for 10.000 or 100.000 training steps. DQN can not solve stage one of the three stages in the author's experiments. It is not considered for the other stages."}, {"title": "5.2 Stage 1: 24 permutations of one scenario", "content": "Each agent was trained on all 24 permutations of one scenario simultaneously, which means for Q-learning, after a successfully solved episode, the maximum number of hosts are compromised, or after 100 steps, the permutation is changed in a fixed order. For A3C, an agent was trained on each permutation in parallel. The training of the best-performing agents per topology can be seen in Figure 7. Red shows the maximum reached reward of all permutations, while blue shows the minimum. Permutation 4, yellow and permutation 15, green, were selected to demonstrate concrete environments and their reward due to the training. The most important value is the minimum. It indicates if all permutations can be solved with an optimal attacking path."}]}