{"title": "Evaluation of Reinforcement Learning for Autonomous Penetration Testing using A3C, Q-learning and DQN", "authors": ["Norman Becker", "Daniel Reti", "Evridiki V.Ntagiou", "Marcus Wallum", "Hans D. Schotten"], "abstract": "Penetration testing is the process of searching for security weak-nesses by simulating an attack. It is usually performed by experi-enced professionals, where scanning and attack tools are applied.By automating the execution of such tools, the need for humaninteraction and decision-making could be reduced. In this work, aNetwork Attack Simulator (NASim) was used as an environmentto train reinforcement learning agents to solve three predefinedsecurity scenarios. These scenarios cover techniques of exploita-tion, post-exploitation and wiretapping. A large hyperparametergrid search was performed to find the best hyperparameter com-binations. The algorithms Q-learning, DQN and A3C were used,whereby A3C was able to solve all scenarios and achieve general-ization. In addition, A3C could solve these scenarios with feweractions than the baseline automated penetration testing. Althoughthe training was performed on rather small scenarios and withsmall state and action spaces for the agents, the results show that apenetration test can successfully be performed by the RL agent.", "sections": [{"title": "1 INTRODUCTION", "content": "Although many steps of a penetration test are automated to a highdegree, it still needs experience and skill to correctly operate suchtools, avoid detection and have high confidence that no vulnera-bility was overlooked. With the ever-evolving technologies andadvancing threat landscapes, protecting information systems re-quires constant testing against the state-of-the-art vulnerabilitiesand exploitations of these. As this process requires highly skilledand certified personnel, which is scarce, there is a high demand forautomating the security testing process. In a penetration test, anattack on a computer network is simulated with the goal of pene-trating the perimeters of the network, such as firewalls or IntrusionDetection Systems (IDS), and show that a successful compromiseof sensitive hosts is possible. Traditionally, to achieve this, a team"}, {"title": "2 PENETRATION TESTING", "content": "A first definition for penetration testing was given by R. R. Lindein 1975, where it was described as the process of assessing vulner-abilities in the form of design weaknesses in the implementationof an operating system's security controls. It was pointed out thatit is a cost-effective alternative to more formal correctness proof,which is not available [5]. Today, penetration testing describesthe proactive approach of searching for security weaknesses in thedesign or implementation of a network, device or software by usingtools and methods similar to those of a malicious attacker."}, {"title": "2.1 Fundamentals", "content": "This process of simulating an attack is sometimes also referred toas red teaming, a military term for the attacking team in militarysimulations to test defence tactics, or as ethical hacking, in orderto distinguish from the malicious attacker, which is often referredto as a 'hacker' in popular culture.\nAs it is impossible to acquire a complete list of every possiblevulnerability to test for, a penetration test can never prove security[6]. Therefore, when a penetration test does not find any vulner-abilities, it is not proven that there is no presence. The value of apenetration test comes from the correct interpretation of the resultsand requires an understanding of the present threats and attackercapabilities.\nDue to this reliance on experience and knowledge, a penetrationtest is usually performed by a security professional. Nonetheless,a lot of automation in penetration testing tools exists to assist thehuman operator, who is interpreting scanning results, derivingsuitable tactics and tools and reporting the results.\nSuch a penetration test is only valid for the time it is pursuedand needs to be regularly repeated as new vulnerabilities are found,and system configurations may be changed."}, {"title": "2.2 Scope and Phases", "content": "The goal of every penetration test is to find potential vulnerabilitiesin a system. Hereby the amount of information provided beforehand determines if it is considered a black box, grey box or whitebox testing. With black box testing, the information and accessto the system for the tester are the same as an outsider attacker."}, {"title": "2.3 Automation of Pentesting", "content": "An innumerable amount of free and commercial tools exist to helpfacilitate and automate the process of security testing. The mostcommonly known tool suite is Kali Linux and the Metasploit frame-work. The purpose of such tools could be, for example, scanning,sniffing, brute forcing, password or hash cracking, fuzzing, payloadgeneration, debugging, static analysis, dynamic analysis, deliveryand handling of remote sessions. A lot of automation is alreadyachieved by combining the execution of such tools, e.g. OpenVAS,Metaspoilt Framework, nmap scripting engine or Nessus. Theseautomation approaches allow to probe for a broad range of knownvulnerabilities by going through predefined test cases and probes.It should be noted that there are no universally accepted definitionsof automated or autonomous penetration testing, and the termsare often used interchangeably. A penetration test that is not auto-mated is considered a manual test. A manual penetration test can beperformed in an exploratory or systematic manner. The automatedapproach can reduce a lot of time and repetition for the systematictest. A vulnerability scan is not a penetration test, as there is noexploitation. A reinforcement learning-based agent might improvethe selection of test cases further to reduce the number of actionsneeded. A specific systematic solution is presented in the following,which is used as the baseline for comparison with the reinforce-ment learning-based approach presented in this work. An extensiveoverview of reinforcement learning-based approaches can be foundin Chapter VI."}, {"title": "2.3.1 Penbox", "content": "Penetration testing in a Box, short Penbox, is anautomated penetration testing software developed by [redacted forblind review]. It aims to perform security tests against a networkand its systems by attacking them with standard penetration testingtools. The overall goal is to discover potential attack paths andvulnerabilities. The software acts like a toolbox and uses decisiontrees to execute these tools in a meaningful order to perform attackssuccessfully. Its implementation makes it possible to generate newscenarios, attack trees or modify tools to apply them to other custom"}, {"title": "3 RELATED WORK", "content": "It is foreseeable that artificial intelligence will be able to outper-form human penetration testing experts [7]. In recent years, theresearch topic of automating penetration testing using reinforce-ment learning has grown. Most publications either focus on eitherthe development of environments or the agents.\nIntroduced by J. Schwartz in 2019, NASim was one of the firstapproaches of providing a simulated environment specialized inperforming penetration testing with reinforcement learning [4].The work has shown that RL agents can find attack paths and solveenvironments if the network isn't too large. The same author wasinvolved in a work by Baillie et al., who introduced an environmentnamed CybORG in 2020 [1], where in addition to the simulationof the network an emulation is provided as well. The simulationaspect allows for faster training of the agents, while the emulationcan tests the agent on real-world systems. CybORG does not onlyfocus on the attacker's perspective, but considers autonomous cy-ber operations, including agent development for blue teaming andred teaming.\nFollowing NASim and CybORG, Microsoft has published a simula-tor in 2021 called CyberBattleSim [3] with equal base functionalitiesand provides additional features such a credential handling, and ituses a slightly different model to represent the network.\nAnother RL training environment for penetration testing is CyGilby Li et al. [8], published in 2021. CyGil completely emulates net-works and doesn't simulate them. With a sufficiently abstractedgym-like interface, it enables reinforcement learning and providesbetter performances regarding training time compared to a real-world network. Similarly to CybORG, CyGil also provides a blueteam mode. Similarly to this work, the authors of NASimEmu [9]modified NASim to emulate the networks simulated in NASim.They highlight that the handling of credentials is missing in theNASim, which was addressed and solved in this paper.\nWhile gym environments are a standardized method to comparealgorithms, Zennaro and Erdodi [10] applied Reinforcement Learn-ing on simplified Capture-the-Flag challenges, allowing the agentto perform a port scan or analyze a website. In these scenarios,prior knowledge is provided, and the action space for the agent iskept small respectively. Niculae et al. present a formalised gameof penetration testing and show that reinforcement learning canoutperform humans in finding the most efficient action sequence[11]. Similarly, Turner et al. defined a zero-sum game to representthe penetration testing task [12]. Especially for the penetrationtesting domain, the agents must handle a large action and statespace, and with that, most agents become unstable. This problemis specific for penetration testing in the RL context [4]. To solveit, new and more robust RL methods, or strong constraints on theaction space are needed. One approach is presented by Hu et al.[13]. They first create the full attack tree of each topology andthen let the agent select the best possible attack paths. The same"}, {"title": "4 EXPERIMENT", "content": "Three scenarios have been defined as the foundation of the experiments. They will be used to train the reinforcement learning agentand then compare the results against the Penbox execution. As abenchmark, these scenarios are also rebuilt in a physical networkwith Raspberry Pi's (model 3b and 4). Raspberry Pi's are cheap andsmall one-board computers which support all relevant functions ofa real PC. The authors have chosen physical hardware instead ofvirtual machines because they are more demonstrative and providebetter results on real-world data. However, it would also be possibleto use virtual machines.\nIn all scenarios, one attacker aims to compromise as many ma-chines as possible. A machine is compromised when root access isgained. Every topology has a flat hierarchy and requires differenthacking methods to be solved, such as Exploits, Wiretapping, andPost-exploitation. For each of these Scenarios, a perfect attack pathis known, which the RL agents can be compared with as a baseline."}, {"title": "4.1.1 Exploits", "content": "The first scenario A, with the name Exploits, represents one of the most basic mechanics of penetration testing:exploiting software vulnerabilities in running services. Services areprograms running on a system and provide applications like filetransfer protocol (FTP), website servers etc. If a vulnerable serviceis listening to an incoming connection, the service could be usedas an entry point to the system. After a software vulnerability is"}, {"title": "4.1.2 Wiretapping", "content": "The second scenario, Scenario B, representsanother penetration testing technique, a specific type of Wiretap-ping. In case the traffic is unencrypted, it is possible to performwiretapping to obtain confidential information such as passwordssent over HTTP or FTP. In this scenario 2, there is no encryption,so sensitive data can be gained without decrypting messages.Node 12 hosts a standard web application that only authorizedusers can access. Alice, operating on Node 11, sends a GET requestwith her credentials attached every 30 seconds to the web applica-tion. Due to the fact that Node 12 is using HTTP, every message is"}, {"title": "4.1.3 Post-exploitation", "content": "The last Scenario C, Post-exploitation, wasdesigned to represent post-exploitation. Post-exploitation is a catch-all name for the steps an attacker performs after gaining access toa system. During this phase, the attacker can try to contact newsystems, gather information or install malware to gain persistence.In this scenario, the attacker can gain new user credentials aftersuccessfully compromising a system. With this information, he canaccess another system."}, {"title": "4.2 Penbox", "content": "Penbox needs a specification of an attack tree. To apply Penbox suc-cessfully to the scenarios, a decision tree is defined. For readabilityreasons, the decision tree is presented as an execution chain, as canbe seen in Figure 4. Before the next phase is selected the action isperformed on every reachable and discovered node in the network."}, {"title": "4.3 NASim and applied modifications", "content": "An environment for the agent to interact with is needed to performreinforcement learning. Real networks are unsuitable due to theirslow interaction speeds. The delays in real networks, combined withthe longer execution times for specific actions, would significantlyslow down the agents' training process, which demands extensivetraining. To simulate the scenarios, NASim is used. NASim wasone of the early environments to apply reinforcement learning topenetration testing [4]. NASim's simulation is very far-reaching"}, {"title": "4.4 Reinforcement Learning Algorithms", "content": ""}, {"title": "4.4.1 Q-learning", "content": "Q-learning is an action-value algorithm with atabular representation, which means it tries to map a value for eachpossible action. Based on this value, the best action is chosen. Tobe more specific, Q-learning is a temporal-difference off-policy RLalgorithm. The action-value function \\(q(s_t, a_t)\\) starts with initialvalues, which are then updated over the training. Off-policy means the agent learns the best policy \\(\\pi^*\\) while exploring the environmentusing another policy \\(\\pi_b\\).\nIt is not necessary to use a tabular representation. Instead, an ap-proximated representation can be used. This, for example, can be aneural network and is done in Deep Q-learning or short DQN.\nWith the given formula of reinforcement learning, the agent cangradually construct an approximation of the true action-value func-tion \\(q(s_t, a_t)\\) by gradually updating its estimation by the followingso-called, Q-function:\n\\(Q(s_t, a_t) \\leftarrow Q(s_t, q_t) + \\alpha[r_t + \\gamma \\max_x Q(s_{t+1}, x) - Q(s_t, a_t)]\\)\nWhere:"}, {"title": "4.4.2 DQN", "content": "DQN stands for Deep Q-learning Network and is verysimilar to the Q-learning. It was first presented by Mnih et al. [22].The main difference to Q-learning is that it uses a single DeepNeural Network to estimate the action value. DQN approximatesthe state-action value function such that \\(Q(s, a; \\theta) \\approx q_*(s, a)\\), where\\(\\theta\\) denotes the neural network's weights. The neural network (shortNN) uses a state \\(s_t\\) and outputs |A| scalars corresponding to thestate-action values of \\(s_t\\) [23]. DQN then tries to optimize the NN.DQN includes all of the Q-learning Hyperparameters but, addi-tionally, there are the parameters of the Neural Network:"}, {"title": "4.4.3 A3C", "content": "Besides Q-learning, Asynchronous Advantage ActorCritic (A3C) is a newer and more advanced approach. It was devel-oped by Google's DeepMind [25]."}, {"title": "4.4.4 Reward Model", "content": "The reward model has a significant impacton the success of an agent. If the reward model is poorly defined,the agent can find ways to maximise its reward without behaving as"}, {"title": "4.5 Permutations", "content": "First, small tests have shown that agents overfit when trained on onescenario. They learn their strategies by heart. The agents will skipthe scanning stage to maximize their reward. This makes no sensein reality for penetration testing agents. To address this, insteadof training on just one scenario, permutations of each scenario arecreated and rotated while training. After 100 steps are done or afteran episode is successfully solved, a new permutation is selected.\nTo create the permutation, the host order is switched inside thesimulation. In a real network, it would be equivalent to a change ofIP address. Additionally, each scenario is filled up to a fixed host sizeof 5 hosts, one attacker, and four other hosts. Both scenarios thatdo not have four hosts will be filled with empty hosts, which haveno services. This has the positive effect of having a discrete stateand action space. Reinforcement learning problems with discreteaction spaces are easier and better to solve. The same applies to thestate space.\nThe attacker and his entry point is not involved in the rotation.This results in 4! = 24 different permutations of each scenario, onepermutation is defined as environments. Overall, a set of 3 * 24 = 72environments is created. An important note here is that topologiesB and C theoretically include 12 different environments becausethey have two empty hosts, which results in the same networktopology. IP switching might sound like it does not significantlyalter the scenario. This is not the case for NASim and the RL agents.By changing the IP, the state of the simulation completely changes,and an agent who is just trained on one permutation cannot solvethe other one."}, {"title": "4.6 Grid Hyperparameter Search", "content": "A large grid hyperparameter search evaluates and comparesthe different RL algorithms with different stages of complexity. Thefirst stage is to train and solve all 24 permutations of one scenario.The second stage is to solve all permutations of all 72 permutationsof every scenario. The third stage split the set of permutations, thescenarios separated, into a test and train set. This will show howwell the agent can generalize.\nBefore performing such large and heavy computational training,smaller tests were performed to ensure that the agents could handlemultiple switching environments. The initial smaller tests includeslowly increasing the number of permutations and training steps.Q-learning and A3C pass these initial tests, so for each of them, 1296different hyperparameter combinations were selected, see appendixTable 2, and each agent is trained for 8.000.000 training steps. Thetraining was performed on a CPU cluster. A single agent of A3Cneeds 80 min to train, with 26 CPU kernels of an AMD EPYC 7742(64-Core Processor). 1296 * 80min would be around 70 days to trainall agents on one Scenario with 24 environments. Using up to 1000CPU kernels once at a time, the authors could train the agents in afeasible amount of time for every Scenario.\nThe initial tests also show that DQN starts failing when trainedon two permutations. It cannot even solve one of the two environ-ments when being trained on both, so it was left out of the LargeHyperparameter search and got its own smaller Hyperparametersearch. Instead of being trained on 24 permutations, it is trained onjust two with the hyperparameter in Table 3 (appendix) for 100.000steps. Later, the best 16 agents out of the 384 will be trained for1.000.000 training steps. To ensure the training size is large enough."}, {"title": "5 RESULTS", "content": ""}, {"title": "5.1 DQN", "content": "The first impression about DQN is due to its neural network actionselection; it should be able to solve multiple environments andarchive some generalisation. The experiments of this work proveddifferent. The results were so underwhelming that even multipleimplementations were tested to ensure it was not an implemen-tation issue from the authors (Chainer [27] and NASim [4]). Asseen in Figure 5, DQN can solve a single environment (in this case,Scenario A) when trained on it.\nAs soon as the environment amount is increased from one totwo, none of the 384 trained agents produces an acceptable positivereward, even if the training size is increased by a factor of 10. Duringtraining, the reward continues to deteriorate until it settles around-100 almost every time, regardless of the agent hyperparameters,see figure 6. This is true for all 384 agents, whether trained for10.000 or 100.000 training steps. DQN can not solve stage one ofthe three stages in the author's experiments. It is not consideredfor the other stages."}, {"title": "5.2 Stage 1: 24 permutations of one scenario", "content": "Each agent was trained on all 24 permutations of one scenariosimultaneously, which means for Q-learning, after a successfullysolved episode, the maximum number of hosts are compromised, orafter 100 steps, the permutation is changed in a fixed order. For A3C,"}, {"title": "5.3 Stage 2: 72 permutations of all three scenarios", "content": "Q-learning cannot solve all permutations of every scenario, so onlyA3C is selected for the second stage. The next step is to solve allscenarios and their permutations with just one agent. The agentsare trained on all 72 permutations instead of just 24. None of the"}, {"title": "5.4 Stage 3: Generalization of A3C on one scenario", "content": "Lastly, the generalization is tested. The best agent for each scenariofrom stage 1 is selected and trained. Instead of all 24 permutations,the permutations get split into a train and test set. To see howmany train environments are required to solve all 24 permutations.Multiple trainings were performed, starting with 12 permutationsinside the train and test set. If the resulting agent cannot solveall permutations, the training is repeated, and one environmentfrom the test set is moved to the training set. The results are thefollowing:"}, {"title": "6 DISCUSSION", "content": "The experiments show that the A3C agent can solve all permuta-tions and outperform Penbox, a standard automated penetrationtesting tool, regarding the number of actions needed. The performedactions are very close, if not perfect, to the optimal attack path.Additionally, the A3C agent was able to generalize and even solveunseen permutations. It is important to not just consider successfulsolves as an evaluation metric, but to also keep track of the numberof actions needed to solve the scenario. If the agent can solve anenvironment, but requires more actions than a random agent or apredefined decision tree, the RL-based approach offers no advan-tage. In a real-world setting the amount of actions is relevant, as anincreased amount of actions results in a longer testing time and alouder and therefore easier detectable attack, which decreases thechance of success.\nWith Penbox, a potential interface is provided to execute the ac-tions in real networks, and the gathered information can be passedback into the simulated NASim environment. However, when ap-plying the agents to real-world environments, the following pointsneed to be considered: (1) The agents are trained in a perfect world,without connectivity problems, crashes, timeouts or defence mech-anisms. (2) The topologies are similar, and their action and statespace is fixed. (3) The agents are limited to the methods learned inthe training.\nIn reality, an attacker might face connectivity problems, crashesor timeouts. These issues should be handled by a wrapper inter-face and not by the RL agents to reduce the action and state space.Additionally, the experiments don't provide any active defences orcountermechanisms. This makes it much easier for an attacker toreach their goals. In the real world, many networks provide intru-sion detection systems and firewalls or even honeypots and blueteams. The authors were not emphasising on the stealthiness of theagents, so there are no counter defence actions applied.\nThe discrete action and state space limit the agent's capabilityof interacting with networks. The agent can handle networks witha size of exactly the amount of hosts it was trained for and a fixedamount of actions. To address this, the agent needs to be trainedusing a continuous action and state space, which can negativelyimpact the agent's performance. Aside from that, the discrete actionand state space can be expanded to common network subnet sizes,like 256. This would result in an explosion of the possible actionsand states, thus greatly impacting the performance. ConsideringScenario A, an exploit service mapping could be enough to makereinforcement learning superfluous. The mapping of exploits tovulnerable services could be provided and not be necessary to belearned and performed by the agent. This will speed up the training,increase performance and allow the agent to focus on other tasks.Although the provided tasks require rather trivial actions to besolved, showing that these dependencies could be found by theagents is a necessary requirement for the success in scenarios withmore complex dependencies.\nThe fact that the agents can only solve similar topologies andare limited to their trained attacking methods has to be consideredwhen applying the agent to real networks. Similar to human testers,the RL algorithms are limited to a specific skill set, which they canimprove by new learning. While it would be interesting to mea-sure how the agents perform in other scenarios than they weretrained on, in this work each scenario requires different exploitationmethods, leading to different sets of actions required. Therefore,the agents would not perform well in the other scenarios withoutretraining. These problems of retraining and knowledge transferare broad topics and may be the scope for feature research. Thesescenarios in this work only represent a small set of penetrationtesting techniques, and the resulting agents are limited to the ac-tions from the training, requiring retraining to extend an agentwith new techniques. The amount of 72 environments used in thiswork are not sufficient to achieve greater generalization. This wasalready indicated by the overfitting in Stage 3. To achieve a moregenerally applicable agent, a large number of different topologies"}, {"title": "7 CONCLUSION", "content": "In this paper, the authors have developed three different penetration testing scenarios. To solve these scenarios with reinforcementlearning, the Network Attack Simulator, NASim, was modified torepresent these scenarios inside a simulation. A3C, Q-learning andDQN were used as algorithms compared against an already existingautomated penetration testing method using decision trees. Anextensive hyperparameter grid search was performed on two al-gorithms to find the best agents. The experiments show that A3Ccan solve all scenarios and find the optimal attack paths. In most"}]}