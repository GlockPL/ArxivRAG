{"title": "Investigating Context Effects in Similarity Judgements in Large Language Models", "authors": ["Sagar Uprety", "Amit Kumar Jaiswal", "Haiming Liu", "Dawei Song"], "abstract": "Large Language Models (LLMs) have revolutionised the capability\nof AI models in comprehending and generating natural language\ntext. They are increasingly being used to empower and deploy\nagents in real-world scenarios, which make decisions and take\nactions based on their understanding of the context. Therefore re-\nsearchers, policy makers and enterprises alike are working towards\nensuring that the decisions made by these agents align with human\nvalues and user expectations. That being said, human values and\ndecisions are not always straightforward to measure and are subject\nto different cognitive biases. There is a vast section of literature in\nBehavioural Science which studies biases in human judgements.\nIn this work we report an ongoing investigation on alignment of\nLLMs with human judgements affected by order bias. Specifically,\nwe focus on a famous human study which showed evidence of\norder effects in similarity judgements, and replicate it with vari-\nous popular LLMs. We report the different settings where LLMs\nexhibit human-like order effect bias and discuss the implications of\nthese findings to inform the design and development of LLM based\napplications.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language models (LLMs) like GPT-4 [1] and LLaMa fam-\nily [21] have been at the forefront of ushering a new wave of re-\nsearch, funding, investments and public opinion in Artificial Intelli-\ngence (AI). This is due by their remarkably improved capabilities in\nreasoning [12, 25], cross-domain generalisation [7] and the emer-\ngence of a new type of learning - in-context learning [24]. Some\nresearchers have gone as far as suggesting that LLMs are the pre-\ncursor to a generalist form of AI [8], whereas there is another school\nof thought which says that the autoregressive training and next\nword prediction architecture of LLMs are not enough to imitate\nhuman cognitive intelligence [20]. Nevertheless, it is accepted that\nLLMs are useful tools to augment our workflows and assist in differ-\nent tasks like coding assistant\u00b9, copywriting, marketing and sales\u00b2,\nsystematic reviews\u00b3, search engines\u2074, etc. There is ongoing research\nin developing specialised and generalist LLM agents which can use\nother tools like browsers and APIs to perform tasks like booking\nholidays, reserving tables, online shopping, etc. Increasingly, many\nof our daily workflows, including our professional work will be\neither automated or augmented with the help of LLM-based agents.\nThese agents will interact with and impact the society in general.\nThus, it is imperative that the decisions made by these agents\nthrough LLMs align with not only the intentions of the users of\nthese agent applications, but also societal values and conventions.\nThere is a parallel line of research in this direction under the um-\nbrella of AI Alignment [11, 13].\nThere are several facets of AI alignment, depending upon the ap-\nplication domain of the AI model. However, the underlying theme\nof current alignment research is the focus on avoiding negative or\nincorrect outcomes from LLMs and also the focus on values [19].\nThere is a large class of human judgements where the outcome\nis neither negative nor incorrect, rather there is no single correct\noutcome. This is because the outcome depends on the context of\njudgement. In this work, we focus on the alignment of context-\nsensitive judgements produced by LLMs with those produced by\nhumans. There are several implications of aligning human-LLM"}, {"title": "2 RELATED WORK", "content": "Research at the intersection of cognitive science and LLMs is based\non chiefly two perspectives. Although both perspectives seek to\nknow how close are LLMs to humans, one line of work is interested\nin utilising LLMs to study cognitive models and theories on a large\nscale by replicating human behaviour with synthetic data generated\nfrom LLMs [4-6]. The other research direction concerns with the\napplication of LLMs in automation and augmentation, wherein it\nis essential that decisions and judgements by LLM based agents\nalign with human preferences and judgements [2, 16]. In either\ncase, while comparing humans and LLMs in terms of responses,\njudgements and decisions, it is important to know whether the un-\nderlying mechanisms of representation of concepts and reasoning\nare similar or not. In general, it is seen that even in cases where\nLLM judgements are similar to humans, the underlying processes\nof reasoning and representation are different [4, 17]. For e.g., [5] re-\nveals that while LLMs are effective at understanding and explaining\nvariability in human decisions under risk, their risk assessments\nare purely data-driven, lacking the emotional and psychological\nfactors influencing human decisions.\nA large array of recent work investigates presence of different\nhuman-like cognitive biases in LLMs [3, 9, 10, 17]. In [26], the\nauthors repeatedly prompt LLMs with the same query and study\nthe variability in the responses. It is found that at temperature =\n1.0, the variability displays similar patterns to human judgements\nvariability. In [17] and [3], LLMs are tested against popular tasks\nlike Wason selection and Conjunction fallacy and found to contain\nhuman-like bias in some of the tasks. Another important study [10]\nfound presence of common cognitive biases like anchoring, framing,\ngroup atttribution in 4 LLMs - GPT-3.5-Turbo, GPT-4, Llama2 7B,\nand Llama2 13B. However, they were able to devise strategies to\nmitigate these biases to a good extent.\nHuman judgements typically optimise over a lot of different\nfactors thus leading to a lot of trade-offs (Bounded rationality). One\nsuch trade-off between truthfulness and helpfulness to user goals\nof LLM responses is studied in [14]. This balance is crucial because,\nlike humans, LLMs must sometimes prioritise helpfulness over strict\nhonesty to be effective communicators. For instance, an LLM might\nsimplify or approximate numerical values to aid understanding,\nakin to how humans might round numbers in conversation to\nmake information more comprehensible. It was found that LLM\nresponses also display human-like patterns, which can be further\ntuned towards either factor using different techniques like RLHF\nor Chain-of-Though Prompting (CoT)."}, {"title": "3 TVERSKY AND GATI'S SIMILARITY\nEFFECTS STUDY", "content": "In this study [23], participants were shown pairs of 21 countries\nand asked to rate the similarity between them on a scale of 0 to\n20. The participants were divided into two groups, and each group\nrated similarity of 21 countries. The only difference between the\ngroups was the order of countries in the pair. It was found that\nthere were statistically significant differences in similarity scores\nfor the country pairs in the two groups, thus providing evidence\nthat human given similarity judgements may not always follow the\nsymmetry assumption.\nTversky and Gati consider similarity judgements to be an ex-\ntension of similarity statements (a is like b), which are in general\ndirectional. When two entities are compared, one is usually the\nsubject and the other is the referent. The referent is always the more\nprominent of the two entities (i.e. has with more salient features). In\nthe example given earlier in the paper, China would be the referent\nand North Korea the subject. Tversky and Gati hypothesise that if\n$s(a, b)$ and $s(b, a)$ are the similarity of a pair of subject and referent\nin the two orders, then $s(a, b) > s(b, a)$. Similarity of two entities\nwould be more if the first entity is the less prominent one of the\npair. This is validated in their experiment for the 21 country pairs"}, {"title": "4 EXPERIMENTS", "content": "We conduct investigations into order effect in similarity using 8\nLLMs - Mistral 7B6, Llama2 7B, Llama2 13B, LLama2 70B, Llama3\n8B, Llama3 70B [21], GPT-3.5 and GPT-4 [1]. For the open-source\nmodels we use their non-quantised 16 bit versions, with default\nvalues of top_p (0.9) and top_k (50) parameters, while the temper-\nature parameter is varied. The different temperature parameters\nused are 0.001, 0.5, 1.0, 1.5. Note that we did not use temperature\n= 0, as the Llama models require a non-zero temperature value,\nunlike OpenAI models. We thus keep the lowest temperature value\nto as low as possible, and avoid using temperature = 0 just for the\nOpenAI models.\nThe prompts for each of the LLMs consist of a system message\nwhich also instructs the model to generate response in a structured\njson format, and the specific questions which are part of the study."}, {"title": "4.1 Models", "content": "We conduct investigations into order effect in similarity using 8\nLLMs - Mistral 7B6, Llama2 7B, Llama2 13B, LLama2 70B, Llama3\n8B, Llama3 70B [21], GPT-3.5 and GPT-4 [1]. For the open-source\nmodels we use their non-quantised 16 bit versions, with default\nvalues of top_p (0.9) and top_k (50) parameters, while the temper-\nature parameter is varied. The different temperature parameters\nused are 0.001, 0.5, 1.0, 1.5. Note that we did not use temperature\n= 0, as the Llama models require a non-zero temperature value,\nunlike OpenAI models. We thus keep the lowest temperature value\nto as low as possible, and avoid using temperature = 0 just for the\nOpenAI models.\nThe prompts for each of the LLMs consist of a system message\nwhich also instructs the model to generate response in a structured\njson format, and the specific questions which are part of the study."}, {"title": "4.2 Experiment 1 - Single pair prompts", "content": "In the original study the participants were asked to \"assess the\ndegree to which country A similar to country B\". Therefore, the\nequivalent prompt to this question is \"On a scale of 0 to 20, where\n0 means no similarity and 20 means complete similarity, assess the\ndegree to which {country_1} is similar to {country_2}?\". Moreover,\neach prompt is preceded by a system message and we assert the"}, {"title": "4.3 Experiment 2 - Dual pair prompts", "content": "In this experimental setting we prompt the LLMs with both the\norders of the two countries present in the same prompt, thus elimi-\nnating the context created by changing orders. Although there will\nalways be a particular order in which the countries are presented,\nif the other order immediately follows the first order, the context\neffect is negated.\nEach prompt consist of two statements for each order. The ex-\npected output is a json with two keys - score_1 and score_2. We\nagain use the three different wording styles in both statements. The\nthree prompts with their labels are:\n\u2022 Dual Similar Degree (DSD) -\nQuestion 1: On a scale of 0 to 20, where 0 means no simi-\nlarity and 20 means complete similarity, assess the degree to\nwhich {country_1} is similar to {country_2}?\nQuestion 2: On a scale of 0 to 20, where 0 means no simi-\nlarity and 20 means complete similarity, assess the degree to\nwhich {country_2} is similar to {country_1}?\n\u2022 Dual Similar To (DST) -\nQuestion 1: On a scale of 0 to 20, where 0 means no similar-\nity and 20 means complete similarity, is {country_1} similar\nto {country_2}?\nQuestion 2: On a scale of 0 to 20, where 0 means no similar-\nity and 20 means complete similarity, is {country_2} similar\nto {country_1}?\n\u2022 Dual Similar And (DSA) -\nQuestion 1: On a scale of 0 to 20, where 0 means no sim-\nlarity and 20 means complete similarity, how similar are\n{country_1} and {country_2}?\nQuestion 2: On a scale of 0 to 20, where 0 means no sim-\nlarity and 20 means complete similarity, how similar are\n{country_2} and {country_1}?"}, {"title": "5 RESULTS AND DISCUSSION", "content": "We perform the above experiments for all 21 pairs of countries,\nfor all 8 models, 4 temperature settings per model and 3 different\nsingle-prompt styles, totalling in 96 different instances of models-\ntemperature-prompt settings for the set of 21 country pairs. Figure 1\nshows the distribution of difference in scores for all country pairs for\nall models, temperature settings and single prompt styles. We also\nplot the human similarity score differences from Tversky and Gati's\nstudy. It is evident that all models show asymmetry in similarity\njudgements, a departure from the mathematical definition. However,\nas will be detailed below, not all settings have statistically significant\ndifferences. For the dual prompt setting, the hypothesis is that all\nsimilarity judgements should be symmetrical, as when deliberately\nshown both orders, humans should give the same rating to the\ncountry pairs. The similarity differences for dual prompt settings\nare plotted in figure 3.\nSimilar to [23], an order effect is calculated when the mean\ndifference between the similarity scores is statistically significant\n(a < 0.05) based on a one-sided paired t-test. The direction being\nthe increase in similarity score when the less prominent country is\nplaced at first in the pair. According to our second alignment criteria,\nthe scores across the dual prompt styles should be symmetric for all\nmodels. Of the 96 different settings, we find only 9 settings where\nboth the criteria are met. Table 1 lists all these settings. We list the\nt-statistic and p-value for each model-temperature-single prompt\nstyle setting. We also check the effects in the corresponding dual\nprompt version for the same model-temperature setting. Either\nthere are no similarity score differences for all 21 countries in the\ntwo orders for that setting, or the differences are not statistically\nsignificant. These could be due to other reasons such as prompt\nsensitivity [15].\nWe can see that only 3 models have perfectly symmetric similar-\nity scores for all the dual prompt styles and temperature settings\n- Llama3 8B, Llama3 70B and GPT-4. There are three instances of\nmodels scoring perfectly symmetrically in the dual prompt settings\n- GPT-4 for the original study's SSD prompt style and Llama3_8B\nfor the SST prompt style for two different temperature settings.\nWe can thus say that GPT-4 is aligned with human judgements for\nthis similarity task (for temperature = 0.5) as the SSD prompt is\nclosest to the original study setting. However, the order effect in the\noriginal study is not hypothesised to be due to the wording of the\nquestion itself, but rather due to the order of the countries in the\npair. It is safe to assume that if the questions in the original study\nwere of style SST, the order effects would have still been present in\nthe similarity judgements. Therefore, we can say that Llama3_8B\nmodel also aligns with human judgements for this task. Moreover,\nit is aligned for two temperature values versus one value for GPT-4.\nHowever, largely the LLMs do not show significant asymmetry in\nsimilarity judgements. Only some models with certain temperature\nsettings do so. This means that LLMs can be made to align their\nbehaviour with human behaviour by tuning these settings. There\nmight be certain applications where this alignment is beneficial\nand certain where it is not. For example, consider an e-commerce\nLLM-based chat agent asked by whether Phone A is similar to\nPhone B or not. If another user asks the agent the same question in\nanother order (perhaps they are more inclined towards the other\nphone), the agent should ideally come up with the same response.\nShould the LLM exhibit order effects in similarity judgements, it\nwould report inconsistent results to different users solely based\non the order of items in query. On the other hand, there might be\ncertain applications e.g. in dating, mental health support where\nLLMs might be better-off aligned to human order bias."}, {"title": "6 CONCLUSION & FUTURE WORK", "content": "In this paper, we explored the how aligned are LLM judgements\nwith human judgements via similarity ratings to pairs of countries\nin different orders. Our investigation involves studying the effect\nof context-driven asymmetry in human judgements utilising Tver-\nsky's framework of similarity judgements [23]. We find that out\nof eight LLMs studied across different temperature settings and\nprompting styles, only Llama3 8B and GPT-4 models report statisti-\ncally significant order effects in line with human judgements. Even\nwith these models, changing the temperature setting can lead to dis-\nappearance of the effect. In the future, we aim to elicit and compare\nthe reasoning generated by these models by different prompting\napproaches like Chain of Thought prompting [25]. We can then\ncompare the crtieria used by LLMs to arrive at their similarity score\nand whether and how this criteria is different for different order of\ncountries in the pair."}]}