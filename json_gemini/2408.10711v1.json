{"title": "Investigating Context Effects in Similarity Judgements in Large Language Models", "authors": ["Sagar Uprety", "Amit Kumar Jaiswal", "Haiming Liu", "Dawei Song"], "abstract": "Large Language Models (LLMs) have revolutionised the capability of AI models in comprehending and generating natural language text. They are increasingly being used to empower and deploy agents in real-world scenarios, which make decisions and take actions based on their understanding of the context. Therefore researchers, policy makers and enterprises alike are working towards ensuring that the decisions made by these agents align with human values and user expectations. That being said, human values and decisions are not always straightforward to measure and are subject to different cognitive biases. There is a vast section of literature in Behavioural Science which studies biases in human judgements. In this work we report an ongoing investigation on alignment of LLMs with human judgements affected by order bias. Specifically, we focus on a famous human study which showed evidence of order effects in similarity judgements, and replicate it with various popular LLMs. We report the different settings where LLMs exhibit human-like order effect bias and discuss the implications of these findings to inform the design and development of LLM based applications.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language models (LLMs) like GPT-4 [1] and LLaMa family [21] have been at the forefront of ushering a new wave of research, funding, investments and public opinion in Artificial Intelligence (AI). This is due by their remarkably improved capabilities in reasoning [12, 25], cross-domain generalisation [7] and the emergence of a new type of learning - in-context learning [24]. Some researchers have gone as far as suggesting that LLMs are the precursor to a generalist form of AI [8], whereas there is another school of thought which says that the autoregressive training and next word prediction architecture of LLMs are not enough to imitate human cognitive intelligence [20]. Nevertheless, it is accepted that LLMs are useful tools to augment our workflows and assist in different tasks like coding assistant\u00b9, copywriting, marketing and sales\u00b2, systematic reviews\u00b3, search engines\u2074, etc. There is ongoing research in developing specialised and generalist LLM agents which can use other tools like browsers and APIs to perform tasks like booking holidays, reserving tables, online shopping, etc. Increasingly, many of our daily workflows, including our professional work will be either automated or augmented with the help of LLM-based agents. These agents will interact with and impact the society in general. Thus, it is imperative that the decisions made by these agents through LLMs align with not only the intentions of the users of these agent applications, but also societal values and conventions. There is a parallel line of research in this direction under the umbrella of AI Alignment [11, 13].\nThere are several facets of AI alignment, depending upon the application domain of the AI model. However, the underlying theme of current alignment research is the focus on avoiding negative or incorrect outcomes from LLMs and also the focus on values [19]. There is a large class of human judgements where the outcome is neither negative nor incorrect, rather there is no single correct outcome. This is because the outcome depends on the context of judgement. In this work, we focus on the alignment of context-sensitive judgements produced by LLMs with those produced by humans. There are several implications of aligning human-LLM"}, {"title": "2 RELATED WORK", "content": "Research at the intersection of cognitive science and LLMs is based on chiefly two perspectives. Although both perspectives seek to know how close are LLMs to humans, one line of work is interested in utilising LLMs to study cognitive models and theories on a large scale by replicating human behaviour with synthetic data generated from LLMs [4-6]. The other research direction concerns with the application of LLMs in automation and augmentation, wherein it is essential that decisions and judgements by LLM based agents align with human preferences and judgements [2, 16]. In either case, while comparing humans and LLMs in terms of responses, judgements and decisions, it is important to know whether the underlying mechanisms of representation of concepts and reasoning are similar or not. In general, it is seen that even in cases where LLM judgements are similar to humans, the underlying processes of reasoning and representation are different [4, 17]. For e.g., [5] reveals that while LLMs are effective at understanding and explaining variability in human decisions under risk, their risk assessments are purely data-driven, lacking the emotional and psychological factors influencing human decisions.\nA large array of recent work investigates presence of different human-like cognitive biases in LLMs [3, 9, 10, 17]. In [26], the authors repeatedly prompt LLMs with the same query and study the variability in the responses. It is found that at temperature = 1.0, the variability displays similar patterns to human judgements variability. In [17] and [3], LLMs are tested against popular tasks like Wason selection and Conjunction fallacy and found to contain human-like bias in some of the tasks. Another important study [10] found presence of common cognitive biases like anchoring, framing, group atttribution in 4 LLMs - GPT-3.5-Turbo, GPT-4, Llama2 7B, and Llama2 13B. However, they were able to devise strategies to mitigate these biases to a good extent.\nHuman judgements typically optimise over a lot of different factors thus leading to a lot of trade-offs (Bounded rationality). One such trade-off between truthfulness and helpfulness to user goals of LLM responses is studied in [14]. This balance is crucial because, like humans, LLMs must sometimes prioritise helpfulness over strict honesty to be effective communicators. For instance, an LLM might simplify or approximate numerical values to aid understanding, akin to how humans might round numbers in conversation to make information more comprehensible. It was found that LLM responses also display human-like patterns, which can be further tuned towards either factor using different techniques like RLHF or Chain-of-Though Prompting (CoT)."}, {"title": "3 TVERSKY AND GATI'S SIMILARITY EFFECTS STUDY", "content": "In this study [23], participants were shown pairs of 21 countries and asked to rate the similarity between them on a scale of 0 to 20. The participants were divided into two groups, and each group rated similarity of 21 countries. The only difference between the groups was the order of countries in the pair. It was found that there were statistically significant differences in similarity scores for the country pairs in the two groups, thus providing evidence that human given similarity judgements may not always follow the symmetry assumption.\nTversky and Gati consider similarity judgements to be an ex-tension of similarity statements (a is like b), which are in general directional. When two entities are compared, one is usually the subject and the other is the referent. The referent is always the more prominent of the two entities (i.e. has with more salient features). In the example given earlier in the paper, China would be the referent and North Korea the subject. Tversky and Gati hypothesise that if s(a, b) and s(b, a) are the similarity of a pair of subject and referent in the two orders, then s(a, b) > s(b, a). Similarity of two entities would be more if the first entity is the less prominent one of the pair. This is validated in their experiment for the 21 country pairs"}, {"title": "4 EXPERIMENTS", "content": "We conduct investigations into order effect in similarity using 8 LLMs - Mistral 7B6, Llama2 7B, Llama2 13B, LLama2 70B, Llama3 8B, Llama3 70B [21], GPT-3.5 and GPT-4 [1]. For the open-source models we use their non-quantised 16 bit versions, with default values of top_p (0.9) and top_k (50) parameters, while the temperature parameter is varied. The different temperature parameters used are 0.001, 0.5, 1.0, 1.5. Note that we did not use temperature = 0, as the Llama models require a non-zero temperature value, unlike OpenAI models. We thus keep the lowest temperature value to as low as possible, and avoid using temperature = 0 just for the OpenAI models.\nThe prompts for each of the LLMs consist of a system message which also instructs the model to generate response in a structured json format, and the specific questions which are part of the study."}, {"title": "4.1 Models", "content": "We conduct investigations into order effect in similarity using 8 LLMs - Mistral 7B6, Llama2 7B, Llama2 13B, LLama2 70B, Llama3 8B, Llama3 70B [21], GPT-3.5 and GPT-4 [1]. For the open-source models we use their non-quantised 16 bit versions, with default values of top_p (0.9) and top_k (50) parameters, while the temperature parameter is varied. The different temperature parameters used are 0.001, 0.5, 1.0, 1.5. Note that we did not use temperature = 0, as the Llama models require a non-zero temperature value, unlike OpenAI models. We thus keep the lowest temperature value to as low as possible, and avoid using temperature = 0 just for the OpenAI models.\nThe prompts for each of the LLMs consist of a system message which also instructs the model to generate response in a structured json format, and the specific questions which are part of the study."}, {"title": "4.2 Experiment 1 - Single pair prompts", "content": "In the original study the participants were asked to \"assess the degree to which country A similar to country B\". Therefore, the equivalent prompt to this question is \"On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, assess the degree to which {country_1} is similar to {country_2}?\". Moreover, each prompt is preceded by a system message and we assert the LLM to strictly return a number between 0 and 20 in a json format. However, apart from this prompt, we also try two other variants by changing the language of the prompt. It is well-known that LLM outputs, even at zero temperature, are sensitive to choice of words in a prompt [15]. The expected output is a json of shape: {'score': }. The three variants of single pair prompts with appropriate labels are:\n\u2022 Single Similar Degree (SSD) - On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, assess the degree to which {country_1} is similar to {country_2}?\n\u2022 Single Similar To (SST) - On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is {country_1} similar to {country_2} ?\n\u2022 Single Similar And (SSA) - On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, how similar are {country_1} and {country_2}?\nIn the other order of these prompts, the country names are swapped."}, {"title": "4.3 Experiment 2 - Dual pair prompts", "content": "In this experimental setting we prompt the LLMs with both the orders of the two countries present in the same prompt, thus eliminating the context created by changing orders. Although there will always be a particular order in which the countries are presented, if the other order immediately follows the first order, the context effect is negated.\nEach prompt consist of two statements for each order. The expected output is a json with two keys - score_1 and score_2. We again use the three different wording styles in both statements. The three prompts with their labels are:\n\u2022 Dual Similar Degree (DSD) -\nQuestion 1: On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, assess the degree to which {country_1} is similar to {country_2}?\nQuestion 2: On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, assess the degree to which {country_2} is similar to {country_1}?\n\u2022 Dual Similar To (DST) -\nQuestion 1: On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is {country_1} similar to {country_2}?\nQuestion 2: On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, is {country_2} similar to {country_1}?\n\u2022 Dual Similar And (DSA) -\nQuestion 1: On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, how similar are {country_1} and {country_2}?\nQuestion 2: On a scale of 0 to 20, where 0 means no similarity and 20 means complete similarity, how similar are {country_2} and {country_1}?"}, {"title": "5 RESULTS AND DISCUSSION", "content": "We perform the above experiments for all 21 pairs of countries, for all 8 models, 4 temperature settings per model and 3 different single-prompt styles, totalling in 96 different instances of modelstemperature-prompt settings for the set of 21 country pairs."}, {"title": "6 CONCLUSION & FUTURE WORK", "content": "In this paper, we explored the how aligned are LLM judgements with human judgements via similarity ratings to pairs of countries in different orders. Our investigation involves studying the effect of context-driven asymmetry in human judgements utilising Tversky's framework of similarity judgements [23]. We find that out of eight LLMs studied across different temperature settings and prompting styles, only Llama3 8B and GPT-4 models report statistically significant order effects in line with human judgements. Even with these models, changing the temperature setting can lead to disappearance of the effect. In the future, we aim to elicit and compare the reasoning generated by these models by different prompting approaches like Chain of Thought prompting [25]. We can then compare the crtieria used by LLMs to arrive at their similarity score and whether and how this criteria is different for different order of countries in the pair."}]}