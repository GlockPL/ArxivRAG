{"title": "Toward Efficient Permutation for Hierarchical N:M Sparsity on GPUs", "authors": ["Seungmin Yu", "Xiaodie Yi", "Hayun Lee", "Dongkun Shin"], "abstract": "N:M sparsity pruning is a powerful technique for compressing deep neural networks\n(DNNs), utilizing NVIDIA's Sparse Tensor Core technology. This method benefits\nfrom hardware support for sparse indexing, enabling the adoption of fine-grained\nsparsity to maintain model accuracy while minimizing the overhead typically asso-\nciated with irregular data access. Although restricted to a fixed level of sparsity due\nto its reliance on hardware, N:M sparsity can be combined with coarser sparsity\ntechniques, such as vector-wise sparsity, to achieve diverse compression ratios.\nInitially, column-wise vector sparsity is applied to a dense model, followed by\nrow-wise N:M sparsity on the preserved column vectors. We call this multi-level ap-\nproach as hierarchical N:M (HiNM) sparsity. Similar to earlier single-level sparsity\ntechniques, HiNM sparsity necessitates an effective channel permutation strategy\nto maximize the accuracy of the compressed networks. However, it introduces\nfurther complexities by requiring the rearrangement of both input and output chan-\nnels, addressing challenges such as permutation sequence, HiNM-sparsity-aware\npermutation, and maintaining consistency in channel ordering across layers. In this\npaper, we introduce a channel permutation method designed specifically for HiNM\nsparsity, named gyro-permutation. This method is crafted to exploit the unique\ncharacteristics of HiNM pruning, incorporating a strategic policy in each permuta-\ntion phase, including channel sampling, clustering, and assignment, to circumvent\nlocal minima. Additionally, we have developed a GPU kernel that facilitates in-\ndependent layer permutation during the execution of HiNM sparse networks. Our\nextensive experimental evaluations on various DNN models demonstrate that our\ngyro-permutation significantly enhances the accuracy of HiNM sparse networks,\nallowing them to reach performance levels comparable to those of unstructured\nsparse networks.", "sections": [{"title": "1 Introduction", "content": "Deep Neural Networks (DNNs) have been rapidly increasing in size, supporting the Scaling Law [1]\nthat suggests larger networks usually yield higher accuracy. For instance, an image task model,\nDALL-E, contains approximately 12 billion parameters, and GPT-3 has over 175 billion parameters.\nHowever, this growth significantly raises the costs associated with memory, storage, and computation,\ncreating substantial challenges for deploying DNNs on standard hardware for practical applications."}, {"title": "2 Related Work", "content": "Weight pruning Given the vast number of parameters in modern DNN models, weight parameter\npruning is crucial to reach the constraints on computation and memory resources. Various weight\npruning strategies have been developed, incorporating different sparsity patterns [3, 9, 5], salience\nscore estimation methods [10\u201312], and fine-tuning techniques such as one-shot pruning [13] and\ngradual pruning [12].\nTypically, irregular and fine-grained sparsity patterns minimize the accuracy loss associated with\npruning but increase the overhead of sparse indexing. Vector-wise sparsity [7, 4, 14], which prunes\nvectors of shape V \u00d7 1, offers an effective balance between accuracy drop and indexing overhead.\nIn case of N:M sparsity, N weight elements in a 1 \u00d7 M vector can be selected irregularly. However,\nthanks to the hardware-level indexing capabilities of NVIDIA's Sparse Tensor Core (STC), the\noverhead of sparse indexing can be substantially reduced. This kind of advantage has spurred\nnumerous research initiatives on N:M sparsity, including the development of effective training\nmethods [15, 16], layer-wise sparsity [17], and optimized GPU kernel designs [18].\nCurrently, since STC supports a fixed sparsity ratio, some researchers proposed new hardware designs\nfor hierarchical sparsity [19, 20] to enable various levels of sparsity. These designs accommodate\nN:M sparsity in a layered approach but often involve complex and costly hardware modifications. We\ncan also consider a software-based hierarchical sparsity technique. Venom [21] combines vector-wise\nsparsity and N:M sparsity, while the sparse index for vector-wise sparsity is handled by software,\nand N:M sparsity is managed by hardware. We target the software-based hierarchical sparsity and\npropose a channel permutation technique to minimize the accuracy drop.\nChannel permutations Channel permutation techniques [7, 4, 8, 22] preprocess weight matrices\nby reordering output or input channels. This reorganization aligns with the required sparsity pattern,\nenabling more efficient removal of unimportant elements. It involves partitioning the channels\naccording to the target sparsity pattern and grouping appropriate channels within each partition to\nfacilitate the pruning process.\nIn previous research, specific permutation strategies have been developed for single-level sparsity\npatterns such as column-wise vector sparsity and N:M sparsity. For column-wise vector sparsity,\noutput channel permutation [4] employs a balanced K-means clustering algorithm to categorize\nchannels with similar distributions, thereby concentrating less significant elements for vector-by-\nvector removal. Conversely, input channel permutation [8] for N:M sparsity utilizes a channel\nswapping technique to balance the distribution of significant elements across each row vector.\nAdditionally, Tetris [22] tackles the challenge of rearranging both the output and input channels of\nweight matrices for block-wise sparsity. Similar to the approach in [8], Tetris implements channel\nswapping to modify the order of channels along different axes. However, Tetris introduces further\nindex translation operations between layers to manage inconsistent channel orders, which significantly\nincreases the overhead during GPU inference."}, {"title": "3 Hierarchical N:M sparsity", "content": "3.1 Pruning policy of multi-level sparsity\nHierarchical N:M(HiNM) sparsity is established through a multi-layered pruning approach, compris-\ning column-wise vector pruning followed by N:M pruning, allowing the adoption of varied pruning\nstrategies. Practitioners can opt to initiate with column-wise vector pruning, commence with N:M\npruning, or integrate both methods concurrently. The sequence from column-wise vector pruning to\nN:M pruning, similar to the VENOM [21] method, is our standard approach, as it directly facilitates\nthe generation of hierarchical N:M sparsity patterns. Conversely, beginning with N:M pruning requires\nsubsequent adjustments for column-wise vector pruning, an unconventional strategy given the initial\nfocus on finer granularity. Simultaneously considering both pruning methods necessitates a novel\nstrategy that addresses the structural inter-dependencies of each approach. We adopt a foundational\npruning policy that progresses from column-wise vector pruning to N:M pruning, demonstrating that\nour gyro-permutation technique achieve high performance without considering the pruning policy.\n3.2 Sparse matrix multiplication with HiNM sparsity patterns\nIn the GPU-based sparse matrix multiplication (SpMM) process involving a sparse weight matrix with\nHiNM sparsity patterns and a dense input matrix, a pivotal observation is the movement of column\nvector data. As shown in Figure 2, during the transition from global memory to shared memory, input\ndata is loaded according to the indices of the corresponding column vectors which are aligned along\nthe input channel axis-from 1 to \u2461. This alignment enhances memory consistency and maximizes\ninput reuse during computation, both essential for efficient model inference.\nMoreover, to fully exploit the parallel computing capabilities of GPUs, each thread block processes a\n\"tile\"-a collection of contiguous output channels equal in size to the column vector. As demonstrated\nin Figure 2, two distinct tiles are assigned to different thread blocks. This arrangement guarantees\nthat as each tile is computed independently, altering the order of vector indices within a tile does not\nimpact the final computation results. This insight is advantageous for two primary reasons:\nTile-wise input channel permutation: By focusing permutation efforts within the granularity of\ncolumn vectors in tiles rather than altering the order of larger input channels, optimization is more\nreadily attainable. This method simplifies the permutation process and reduces the complexity\nassociated with reaching optimal configurations.\nMaintaining layer consistency: Traditionally, modifying the sequence of both output and input\nchannels [22] necessitates additional permute operations during runtime to preserve consistency\nacross different layers. However, if adjustments are made to the order of output channels and column\nvectors, these can be configured offline. By pre-ordering all layers according to the output channel\nsequence, during runtime, input data are loaded from global to shared memory using the reordered\nvector indices, thus obviating the need for additional indexing operations."}, {"title": "4 Gyro-Permutation", "content": "4.1 Optimization challenges in permutation\nIn a layer of a pretrained dense model, a pruning method applies a mask M to a weight matrix\nW\u2208 Rm\u00d7n, aiming to maximize the retention of important weight elements based on their saliency\nwhile complying with specific structural constraints. The hierarchical N:M (HiNM) sparsity pattern,\ncharacterized by a two-step pruning process of column-wise vector pruning followed by row-wise 2:4\npruning, presents considerable challenges. This approach necessitates the permutation of both output\nand input channels, significantly increasing the complexity of determining optimal permutations."}, {"title": "4.2 Permutation algorithm", "content": "In the development of the Gyro-Permutation technique for HiNM sparsity, we delineated two sub-problems as outlined in section 4.1: output channel permutation and tile-wise input channel permutation. These sub-problems address distinct aspects of the sparsity structure yet are managed through a unified algorithmic framework within the Gyro-Permutation method.\nGyro-Permutation operates in three critical phases in an iteration step: sampling, clustering, and assignment, each pivotal to the permutation process.\nSampling. During the sampling process, we consistently extract an equal number of channels from all partitions to promote global optimization at each iteration. The effectiveness of permutations is significantly influenced by the number of samples extracted from each partition, akin to the effect of learning rates in model training. Generally, extracting a larger number of samples aids in avoiding local minima but may hinder achieving the absolute optimum. Conversely, extracting fewer samples can facilitate reaching the optimum but at an increased risk of encountering local minima. In the case of output channel permutation, we dynamically adjust the number of samples from each partition in every iteration, analogous to how learning rates are adjusted during model training. For tile-wise input channel permutation, where each partition typically contains only four column vectors, we are constrained to extract just one column vector per partition. This restriction is informed by the reduced likelihood of falling into local minima in this phase, which diminishes the need to vary the number of samples.\nClustering. Clustering is strategically employed to synchronize the number of sampled channels with the partition count. In tile-wise input channel permutations, where the sample count naturally aligns with the partition number, the clustering phase is bypassed, thereby simplifying the procedure. Conversely, for output channel permutation, we adopt an approach informed by prior research [4]. In this approach, sampled output channels are organized using the Balanced K-means clustering algorithm. This technique groups channels with similar weight distributions, enhancing the probability of aggregating less critical elements together.\nAssignment. During this phase, samples that have been clustered are placed within the designated partitions based on a carefully defined cost function. This function aims to minimize the saliency of pruned elements, thereby optimizing the sparsity pattern achieved through permutation. The cost function is articulated as follows:\nCi,j = p - |M\u2299 p|, where p CPU Sj\nWhere Pi represents the i-th partition, and sj is the j-th sample (or cluster). The term p denotes the saliency scores of the weights, and M is the mask reflecting the target sparsity pattern driven by the permutation. The primary goal of this cost function is to minimize the saliency of the weights that are pruned according to the targeted pruning method when the j-th sample is integrated into the i-th partition.\nAfter calculating the cost for all combinations of partitions and samples, the Hungarian algorithm is employed to find the combination that minimizes the total cost. This pre-optimization of the cost during the permutation process makes it possible to specifically target the least important elements during actual pruning operations. By preemptively optimizing the permutations in this manner, the method effectively reduces the impact of removing critical elements, enhancing the overall efficiency and effectiveness of the pruning process."}, {"title": "5 Experiments", "content": "For our model selection, we opted for ResNet18, ResNet50, DeiT-base, and BERT-base as experi-mental models, demonstrating the effectiveness of the HiNM technique through enhancements in accuracy and F1 scores. We utilized the ImageNet-1K dataset for the CNN and DeiT-base models, and the SQUAD 1.1 dataset for the BERT model.\nIn Section 5.1, we first evaluate the accuracy improvement by gyro-permutation under different sparsity ratios. We examined two scenarios: one-shot pruning with fine-tuning and gradual pruning. In Section 5.2, we conduct an ablation study to assess the superiority of our permutation techniques over previously established methods. In Section 5.3, we benchmark the end-to-end latency of the inference task using the BERT-base model to ascertain the speed improvements facilitated by HiNM pruning. The experiments were conducted on an NVIDIA RTX 4090 GPU, leveraging its Ampere architecture."}, {"title": "5.1 Accuracy improvement by Gyro-permutation", "content": "To assess the efficiency of gyro-permutation, we begin by employing one-shot pruning complemented by fine-tuning. To demonstrate the superiority of HiNM, we compare its outcomes with those of the vector-wise sparsity method [4] and the element-wise pruning method.\nFor estimating the importance of weight (or vector) elements, two approaches can be utilized: the magnitude (L1 norm) technique [9] and the second-order information method [23, 24]. We apply the simpler magnitude technique for CNN models due to its straightforwardness. However, for the transformer-based DeiT-base model, we employ the second-order technique, considering the increased complexity of transformer models."}, {"title": "5.1.1 One-shot pruning with fine-tuning", "content": "Environments. We apply HiNM pruning to all the Conv2d layers, setting the vector size to 32 for ResNet models sourced from the torchvision library [25]. During the fine-tuning phase, we utilize a cosine learning rate strategy with a value of 0.05 over 60 epochs. For the DeiT-base model, we implement the second-order pruning across all Linear modules within the attention, intermediate, and output layers. We adopt an exponential learning rate scheme set at 10-4 and continue fine-tuning for 60 epochs.\nWe evaluate HiNM against four other pruning techniques. As indicated in the legends of Figures 3 and 4, Dense refers to the accuracy of the original, unpruned model; HiNM-NoPerm describes a variant of our proposed strategy that omits the gyro-permutation component; 0VW represents traditional out-vector-wise sparse pattern pruning [4]; Unstructured corresponds to element-wise pruning."}, {"title": "5.1.2 Gradual pruning", "content": "Environments We conducted gradual pruning on the Bert-base model to compare its accuracy with VENOM [21], which employs the same sparsity pattern. VENOM adjusts its saliency scores using a pair-wise approach within a second-order pruning technique [12] and modifies both the vector sparsity ratio and N:M sparsity ratio with each gradual pruning step. In contrast, to demonstrate the efficiency of gyro-permutation, we based our permutations on saliency scores using a second-order pruning technique and aligned our pruning steps with the HiNM sparsity pattern. Initially, we applied only column-wise vector pruning during the early stages of gradual pruning. Once the target vector sparsity ratio is achieved, we then proceeded with N:M pruning. According to Table 2, the F1 score for the HiNM sparsity with gyro-permutation showed improvements of 0.81% and 0.93% over VENOM's performance in the Bert-base model."}, {"title": "5.2 Ablation study on various permutation methods", "content": "Our gyro-permutation technique features innovative approaches in both output channel permutation (OCP) and tile-wise input channel permutation (ICP), distinguishing it from previous single-level permutation techniques that were limited to either output or input channels and often faced issues with the probability of local minima. To assess the efficiency of individual permutation algorithms within our gyro-permutation framework, we conducted an ablation study. This study involved substituting the OCP and ICP with prior permutation techniques, leading to the creation of two variants: HiNM-V1, which modifies the OCP, and HiNM-V2, which alters the ICP.\nHiNM-V1 Unlike our OCP, where the number of samples varies across iterations, the OVW does not involve a sampling phase and instead utilizes a balanced K-means clustering algorithm to partition all output channels into each group uniformly. Consequently, as illustrated in Table 3, our HiNM achieves an accuracy improvement of 4.53% and 0.49% over HiNM-V1 for ResNet18 and ResNet50, respectively. This performance enhancement can be attributed to the factors discussed in Section 4. Specifically, clustering more channels in a single iteration during the permutation process complicates the identification of the optimal order. Moreover, the balanced K-means clustering algorithm does not explicitly identify elements to be pruned, which can lead to significant accuracy drop if used alone.\nHiNM-V2 The input channel permutation technique from NVIDIA-Apex [8] rearranges input channels according to their target N:M sparsity pattern. To benchmark against this technique, we altered the granularity of the permutation unit from input channels to column vectors. As shown in Table 3, the accuracy of HiNM exceeds that of HiNM-V2 by 2.5% and 0.87%, respectively. This suggests that a small sample size can be prone to falling into local optima, even when using strategies from [8] designed to avoid such pitfalls."}, {"title": "5.3 Latency overhead by Gyro-permutation", "content": "We adapted the GPU kernel previously utilized by VENOM [21] to accommodate HiNM sparsity with gyro-permutation. In contrast to VENOM, which employed padding techniques to mitigate bank conflict during the storage of partial sums in shared memory, we implemented NVIDIA's swizzle operator to address this conflict. Additionally, for maintaining inter-layer consistency, the output channel permutation of gyro-permutation systematically prearranges all inter-layer channels offline. Meanwhile, the input channel permutation dynamically adjusts vector indices at runtime. To assess the runtime overhead of gyro-permutation across various sparsity ratios, we integrated HiNM sparsity into the Bert-base model on a RTX 3090 GPU and recorded the inference times. As illustrated in Figure 5, gyro-permutation introduces no detectable runtime overhead across all tested sparsity ratios and vector sizes."}, {"title": "6 Conclusions", "content": "In this study, we introduce the novel gyro-permutation technique tailored for HiNM sparsity, specifically reorders both output channels and input channels. This method not only maintains layer-to-layer consistency but also proves the feasibility of synchronizing the ordering of both output and input channels by observing the data movement across column vectors during GPU computations. By methodically sequencing the permutations\u2014initiating with output channels and progressing to tile-wise input channels\u2014we significantly improve the efficiency of both column-wise vector pruning and N:M pruning within a hierarchical pruning framework.\nFurthermore, our permutation algorithm, which encompasses sampling, clustering, and assignment phases, brings unique advantages that collectively boost the effectiveness of the entire system."}, {"title": "7 Limitations", "content": "Due to the diverse pruning policies available within Hierarchical N:M sparsity, the sequence of permutations can vary based on the chosen pruning policy. We adhere to the conventional approach, initiating with column-wise vector pruning before proceeding to N:M pruning. However, we anticipate that future research will demonstrate enhanced performance when pruning policies and permutations are strategically integrated."}]}