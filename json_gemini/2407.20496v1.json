{"title": "Toward Efficient Permutation for Hierarchical N:M Sparsity on GPUs", "authors": ["Seungmin Yu", "Xiaodie Yi", "Hayun Lee", "Dongkun Shin"], "abstract": "N:M sparsity pruning is a powerful technique for compressing deep neural networks (DNNs), utilizing NVIDIA's Sparse Tensor Core technology. This method benefits from hardware support for sparse indexing, enabling the adoption of fine-grained sparsity to maintain model accuracy while minimizing the overhead typically asso- ciated with irregular data access. Although restricted to a fixed level of sparsity due to its reliance on hardware, N:M sparsity can be combined with coarser sparsity techniques, such as vector-wise sparsity, to achieve diverse compression ratios. Initially, column-wise vector sparsity is applied to a dense model, followed by row-wise N:M sparsity on the preserved column vectors. We call this multi-level ap- proach as hierarchical N:M (HiNM) sparsity. Similar to earlier single-level sparsity techniques, HiNM sparsity necessitates an effective channel permutation strategy to maximize the accuracy of the compressed networks. However, it introduces further complexities by requiring the rearrangement of both input and output chan- nels, addressing challenges such as permutation sequence, HiNM-sparsity-aware permutation, and maintaining consistency in channel ordering across layers. In this paper, we introduce a channel permutation method designed specifically for HiNM sparsity, named gyro-permutation. This method is crafted to exploit the unique characteristics of HiNM pruning, incorporating a strategic policy in each permuta- tion phase, including channel sampling, clustering, and assignment, to circumvent local minima. Additionally, we have developed a GPU kernel that facilitates in- dependent layer permutation during the execution of HiNM sparse networks. Our extensive experimental evaluations on various DNN models demonstrate that our gyro-permutation significantly enhances the accuracy of HiNM sparse networks, allowing them to reach performance levels comparable to those of unstructured sparse networks.", "sections": [{"title": "1 Introduction", "content": "Deep Neural Networks (DNNs) have been rapidly increasing in size, supporting the Scaling Law [1] that suggests larger networks usually yield higher accuracy. For instance, an image task model, DALL-E, contains approximately 12 billion parameters, and GPT-3 has over 175 billion parameters. However, this growth significantly raises the costs associated with memory, storage, and computation, creating substantial challenges for deploying DNNs on standard hardware for practical applications."}, {"title": "2 Related Work", "content": "Weight pruning Given the vast number of parameters in modern DNN models, weight parameter pruning is crucial to reach the constraints on computation and memory resources. Various weight pruning strategies have been developed, incorporating different sparsity patterns [3, 9, 5], salience score estimation methods [10\u201312], and fine-tuning techniques such as one-shot pruning [13] and gradual pruning [12].\nTypically, irregular and fine-grained sparsity patterns minimize the accuracy loss associated with pruning but increase the overhead of sparse indexing. Vector-wise sparsity [7, 4, 14], which prunes vectors of shape $V \\times 1$, offers an effective balance between accuracy drop and indexing overhead.\nIn case of N:M sparsity, N weight elements in a $1 \\times M$ vector can be selected irregularly. However, thanks to the hardware-level indexing capabilities of NVIDIA's Sparse Tensor Core (STC), the overhead of sparse indexing can be substantially reduced. This kind of advantage has spurred numerous research initiatives on N:M sparsity, including the development of effective training methods [15, 16], layer-wise sparsity [17], and optimized GPU kernel designs [18].\nCurrently, since STC supports a fixed sparsity ratio, some researchers proposed new hardware designs for hierarchical sparsity [19, 20] to enable various levels of sparsity. These designs accommodate N:M sparsity in a layered approach but often involve complex and costly hardware modifications. We can also consider a software-based hierarchical sparsity technique. Venom [21] combines vector-wise sparsity and N:M sparsity, while the sparse index for vector-wise sparsity is handled by software, and N:M sparsity is managed by hardware. We target the software-based hierarchical sparsity and propose a channel permutation technique to minimize the accuracy drop.\nChannel permutations Channel permutation techniques [7, 4, 8, 22] preprocess weight matrices by reordering output or input channels. This reorganization aligns with the required sparsity pattern, enabling more efficient removal of unimportant elements. It involves partitioning the channels according to the target sparsity pattern and grouping appropriate channels within each partition to facilitate the pruning process.\nIn previous research, specific permutation strategies have been developed for single-level sparsity patterns such as column-wise vector sparsity and N:M sparsity. For column-wise vector sparsity, output channel permutation [4] employs a balanced K-means clustering algorithm to categorize channels with similar distributions, thereby concentrating less significant elements for vector-by-vector removal. Conversely, input channel permutation [8] for N:M sparsity utilizes a channel swapping technique to balance the distribution of significant elements across each row vector.\nAdditionally, Tetris [22] tackles the challenge of rearranging both the output and input channels of weight matrices for block-wise sparsity. Similar to the approach in [8], Tetris implements channel swapping to modify the order of channels along different axes. However, Tetris introduces further index translation operations between layers to manage inconsistent channel orders, which significantly increases the overhead during GPU inference."}, {"title": "3 Hierarchical N:M sparsity", "content": "Our gyro-permutation differentiates itself by seamlessly integrating index translation operations into the native indexing process of the HiNM sparsity pattern. This integration occurs during the transfer of column-wise vectors from global to shared GPU memory, effectively eliminating any additional computational overhead. This novel approach not only simplifies the computational process but also enhances the overall efficiency and scalability of sparsity implementations in neural networks, as demonstrated in our experiments."}, {"title": "3.1 Pruning policy of multi-level sparsity", "content": "Hierarchical N:M(HiNM) sparsity is established through a multi-layered pruning approach, compris- ing column-wise vector pruning followed by N:M pruning, allowing the adoption of varied pruning strategies. Practitioners can opt to initiate with column-wise vector pruning, commence with N:M pruning, or integrate both methods concurrently. The sequence from column-wise vector pruning to N:M pruning, similar to the VENOM [21] method, is our standard approach, as it directly facilitates the generation of hierarchical N:M sparsity patterns. Conversely, beginning with N:M pruning requires subsequent adjustments for column-wise vector pruning, an unconventional strategy given the initial focus on finer granularity. Simultaneously considering both pruning methods necessitates a novel strategy that addresses the structural inter-dependencies of each approach. We adopt a foundational pruning policy that progresses from column-wise vector pruning to N:M pruning, demonstrating that our gyro-permutation technique achieve high performance without considering the pruning policy."}, {"title": "3.2 Sparse matrix multiplication with HiNM sparsity patterns", "content": "In the GPU-based sparse matrix multiplication (SpMM) process involving a sparse weight matrix with HiNM sparsity patterns and a dense input matrix, a pivotal observation is the movement of column vector data. As shown in Figure 2, during the transition from global memory to shared memory, input data is loaded according to the indices of the corresponding column vectors which are aligned along the input channel axis-from 1 to \u2461. This alignment enhances memory consistency and maximizes input reuse during computation, both essential for efficient model inference.\nMoreover, to fully exploit the parallel computing capabilities of GPUs, each thread block processes a \"tile\"-a collection of contiguous output channels equal in size to the column vector. As demonstrated in Figure 2, two distinct tiles are assigned to different thread blocks. This arrangement guarantees that as each tile is computed independently, altering the order of vector indices within a tile does not impact the final computation results. This insight is advantageous for two primary reasons:\nTile-wise input channel permutation: By focusing permutation efforts within the granularity of column vectors in tiles rather than altering the order of larger input channels, optimization is more readily attainable. This method simplifies the permutation process and reduces the complexity associated with reaching optimal configurations.\nMaintaining layer consistency: Traditionally, modifying the sequence of both output and input channels [22] necessitates additional permute operations during runtime to preserve consistency across different layers. However, if adjustments are made to the order of output channels and column vectors, these can be configured offline. By pre-ordering all layers according to the output channel sequence, during runtime, input data are loaded from global to shared memory using the reordered vector indices, thus obviating the need for additional indexing operations."}, {"title": "4 Gyro-Permutation", "content": "In the development of the Gyro-Permutation technique for HiNM sparsity, we delineated two sub- problems as outlined in section 4.1: output channel permutation and tile-wise input channel permutation. These sub-problems address distinct aspects of the sparsity structure yet are managed through a unified algorithmic framework within the Gyro-Permutation method.\nGyro-Permutation operates in three critical phases in an iteration step: sampling, clustering, and assignment, each pivotal to the permutation process.\nSampling. During the sampling process, we consistently extract an equal number of channels from all partitions to promote global optimization at each iteration. The effectiveness of permutations is significantly influenced by the number of samples extracted from each partition, akin to the effect of learning rates in model training. Generally, extracting a larger number of samples aids in avoiding local minima but may hinder achieving the absolute optimum. Conversely, extracting fewer samples can facilitate reaching the optimum but at an increased risk of encountering local minima. In the case of output channel permutation, we dynamically adjust the number of samples from each partition in every iteration, analogous to how learning rates are adjusted during model training. For tile-wise input channel permutation, where each partition typically contains only four column vectors, we are constrained to extract just one column vector per partition. This restriction is informed by the reduced likelihood of falling into local minima in this phase, which diminishes the need to vary the number of samples.\nClustering. Clustering is strategically employed to synchronize the number of sampled channels with the partition count. In tile-wise input channel permutations, where the sample count naturally aligns with the partition number, the clustering phase is bypassed, thereby simplifying the procedure. Conversely, for output channel permutation, we adopt an approach informed by prior research [4]. In this approach, sampled output channels are organized using the Balanced K-means clustering algorithm. This technique groups channels with similar weight distributions, enhancing the probability of aggregating less critical elements together.\nAssignment. During this phase, samples that have been clustered are placed within the designated partitions based on a carefully defined cost function. This function aims to minimize the saliency of pruned elements, thereby optimizing the sparsity pattern achieved through permutation. The cost function is articulated as follows:\n$C_{i,j} = p - |M \\odot p|$, where $p \\in P_i \\cup S_j$\nWhere $P_i$ represents the i-th partition, and $s_j$ is the j-th sample (or cluster). The term $p$ denotes the saliency scores of the weights, and $M$ is the mask reflecting the target sparsity pattern driven by the permutation. The primary goal of this cost function is to minimize the saliency of the weights that are pruned according to the targeted pruning method when the j-th sample is integrated into the i-th partition.\nAfter calculating the cost for all combinations of partitions and samples, the Hungarian algorithm is employed to find the combination that minimizes the total cost. This pre-optimization of the cost during the permutation process makes it possible to specifically target the least important elements during actual pruning operations. By preemptively optimizing the permutations in this manner, the method effectively reduces the impact of removing critical elements, enhancing the overall efficiency and effectiveness of the pruning process."}, {"title": "5 Experiments", "content": "For our model selection, we opted for ResNet18, ResNet50, DeiT-base, and BERT-base as experi- mental models, demonstrating the effectiveness of the HiNM technique through enhancements in accuracy and F1 scores. We utilized the ImageNet-1K dataset for the CNN and DeiT-base models, and the SQUAD 1.1 dataset for the BERT model.\nIn Section 5.1, we first evaluate the accuracy improvement by gyro-permutation under different sparsity ratios. We examined two scenarios: one-shot pruning with fine-tuning and gradual pruning. In Section 5.2, we conduct an ablation study to assess the superiority of our permutation techniques over previously established methods. In Section 5.3, we benchmark the end-to-end latency of the inference task using the BERT-base model to ascertain the speed improvements facilitated by HiNM pruning. The experiments were conducted on an NVIDIA RTX 4090 GPU, leveraging its Ampere architecture."}, {"title": "5.1 Accuracy improvement by Gyro-permutation", "content": "To assess the efficiency of gyro-permutation, we begin by employing one-shot pruning complemented by fine-tuning. To demonstrate the superiority of HiNM, we compare its outcomes with those of the vector-wise sparsity method [4] and the element-wise pruning method.\nFor estimating the importance of weight (or vector) elements, two approaches can be utilized: the magnitude (L1 norm) technique [9] and the second-order information method [23, 24]. We apply the simpler magnitude technique for CNN models due to its straightforwardness. However, for the transformer-based DeiT-base model, we employ the second-order technique, considering the increased complexity of transformer models."}, {"title": "5.1.1 One-shot pruning with fine-tuning", "content": "Environments. We apply HiNM pruning to all the Conv2d layers, setting the vector size to 32 for ResNet models sourced from the torchvision library [25]. During the fine-tuning phase, we utilize a cosine learning rate strategy with a value of 0.05 over 60 epochs. For the DeiT-base model, we implement the second-order pruning across all Linear modules within the attention, intermediate, and output layers. We adopt an exponential learning rate scheme set at $10^{-4}$ and continue fine-tuning for 60 epochs.\nWe evaluate HiNM against four other pruning techniques. As indicated in the legends of Figures 3 and 4, Dense refers to the accuracy of the original, unpruned model; HiNM-NoPerm describes a variant of our proposed strategy that omits the gyro-permutation component; 0VW represents traditional out- vector-wise sparse pattern pruning [4]; Unstructured corresponds to element-wise pruning.\nThe results indicate a notable decline in accuracy for HiNM pruning without permutation as the sparsity ratio increases. However, integrating permutation technology significantly enhances accuracy, restoring it to levels comparable to those of unstructured pruning. Particularly at a 75% sparsity rate, our gyro channel permutation technique improves accuracy by 5.12% for ResNet18 and 3.62% for ResNet50.\nNotably, even with finer-grained N:M sparsity, HiNM-NoPerm underperforms compared to OVW, which benefits from a channel permutation technique tailored for vector-wise pruning. Nevertheless, the introduction of gyro-permutation in HiNM enables it to surpass OVW in accuracy.\nSpecifically, at a sparsity rate of 75%, HiNM achieves an accuracy of 68.91% on ResNet18 and 74.45% on ResNet50, exceeding the OVW's accuracy of 65.21% and 70.91%, respectively, as shown in Figures 3 and 4. Consequently, HiNM sparsity maintains approximately 99% of the dense ResNet18 model's accuracy and 98% of the dense ResNet50 model's accuracy, even with a 75% reduction in model complexity."}, {"title": "5.1.2 Gradual pruning", "content": "Environments We conducted gradual pruning on the Bert-base model to compare its accuracy with VENOM [21], which employs the same sparsity pattern. VENOM adjusts its saliency scores using a pair-wise approach within a second-order pruning technique [12] and modifies both the vector sparsity ratio and N:M sparsity ratio with each gradual pruning step. In contrast, to demonstrate the efficiency of gyro-permutation, we based our permutations on saliency scores using a second-order pruning technique and aligned our pruning steps with the HiNM sparsity pattern. Initially, we applied only column-wise vector pruning during the early stages of gradual pruning. Once the target vector sparsity ratio is achieved, we then proceeded with N:M pruning. According to Table 2, the F1 score for the HiNM sparsity with gyro-permutation showed improvements of 0.81% and 0.93% over VENOM's performance in the Bert-base model."}, {"title": "5.2 Ablation study on various permutation methods", "content": "Our gyro-permutation technique features innovative approaches in both output channel permutation (OCP) and tile-wise input channel permutation (ICP), distinguishing it from previous single-level permutation techniques that were limited to either output or input channels and often faced issues with the probability of local minima. To assess the efficiency of individual permutation algorithms within our gyro-permutation framework, we conducted an ablation study. This study involved substituting the OCP and ICP with prior permutation techniques, leading to the creation of two variants: HiNM-V1, which modifies the OCP, and HiNM-V2, which alters the ICP.\nHiNM-V1 Unlike our OCP, where the number of samples varies across iterations, the OVW does not involve a sampling phase and instead utilizes a balanced K-means clustering algorithm to partition all output channels into each group uniformly. Consequently, as illustrated in Table 3, our HiNM achieves an accuracy improvement of 4.53% and 0.49% over HiNM-V1 for ResNet18 and ResNet50, respectively. This performance enhancement can be attributed to the factors discussed in Section 4. Specifically, clustering more channels in a single iteration during the permutation process complicates the identification of the optimal order. Moreover, the balanced K-means clustering algorithm does not explicitly identify elements to be pruned, which can lead to significant accuracy drop if used alone.\nHiNM-V2 The input channel permutation technique from NVIDIA-Apex [8] rearranges input channels according to their target N:M sparsity pattern. To benchmark against this technique, we altered the granularity of the permutation unit from input channels to column vectors. As shown in Table 3, the accuracy of HiNM exceeds that of HiNM-V2 by 2.5% and 0.87%, respectively. This suggests that a small sample size can be prone to falling into local optima, even when using strategies from [8] designed to avoid such pitfalls."}, {"title": "5.3 Latency overhead by Gyro-permutation", "content": "We adapted the GPU kernel previously utilized by VENOM [21] to accommodate HiNM sparsity with gyro-permutation. In contrast to VENOM, which employed padding techniques to mitigate bank conflict during the storage of partial sums in shared memory, we implemented NVIDIA's swizzle operator to address this conflict. Additionally, for maintaining inter-layer consistency, the output channel permutation of gyro-permutation systematically prearranges all inter-layer channels offline. Meanwhile, the input channel permutation dynamically adjusts vector indices at runtime. To assess the runtime overhead of gyro-permutation across various sparsity ratios, we integrated HiNM sparsity into the Bert-base model on a RTX 3090 GPU and recorded the inference times. As illustrated in Figure 5, gyro-permutation introduces no detectable runtime overhead across all tested sparsity ratios and vector sizes."}, {"title": "6 Conclusions", "content": "In this study, we introduce the novel gyro-permutation technique tailored for HiNM sparsity, specifi- cally reorders both output channels and input channels. This method not only maintains layer-to-layer consistency but also proves the feasibility of synchronizing the ordering of both output and input channels by observing the data movement across column vectors during GPU computations. By me- thodically sequencing the permutations\u2014initiating with output channels and progressing to tile-wise input channels\u2014we significantly improve the efficiency of both column-wise vector pruning and N:M pruning within a hierarchical pruning framework.\nFurthermore, our permutation algorithm, which encompasses sampling, clustering, and assignment phases, brings unique advantages that collectively boost the effectiveness of the entire system."}, {"title": "7 Limitations", "content": "Due to the diverse pruning policies available within Hierarchical N:M sparsity, the sequence of permutations can vary based on the chosen pruning policy. We adhere to the conventional approach, initiating with column-wise vector pruning before proceeding to N:M pruning. However, we anticipate that future research will demonstrate enhanced performance when pruning policies and permutations are strategically integrated."}]}