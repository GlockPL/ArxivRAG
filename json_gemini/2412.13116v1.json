{"title": "Equity in the Use of ChatGPT for the Classroom:\nA Comparison of the Accuracy and Precision of\nChatGPT 3.5 vs. ChatGPT4 with Respect to\nStatistics and Data Science Exams", "authors": ["Monnie McGee", "Bivin Sadler"], "abstract": "A college education historically has been seen as method of moving upward\nwith regards to income brackets and social status. Indeed, many colleges recognize\nthis connection and seek to enroll talented low income students. While these students\nmight have their education, books, room, and board paid; there are other items that\nthey might be expected to use that are not part of most college scholarship packages.\nOne of those items that has recently surfaced is access to generative AI platforms. The\nmost popular of these platforms is ChatGPT, and it has a paid version (ChatGPT4)\nand a free version (ChatGPT3.5). We seek to explore differences in the free and paid\nversions in the context of homework questions and data analyses as might be seen in\na typical introductory statistics course. We determine the extent to which students\nwho cannot afford newer and faster versions of generative AI programs would be\ndisadvantaged in terms of writing such projects and learning these methods.", "sections": [{"title": "1 Introduction", "content": "The association of social mobility with a college education has been studied since the\nearly 1950's [1]. Although there are some indications that a college education is not\nas effective as it once was in helping graduates climb the social ladder [2], it is still\nthe most reliable way of doing so. US News & World Report updated its rankings\nin 2023 to include social mobility [3], and many institutions of higher education are\npaying more attention to recruitment of first-generation college students and talented\nstudents from disadvantaged backgrounds. With the inclusion of such students in the\ntypical college class comes some important considerations. For example, a student from\ndifficult financial circumstances with an academic background to match the profile of\nany student an elite institution will have more difficulty paying for textbooks, a laptop,\na smartphone, and other items that are almost essential to current college life [2].\nAs of November 2022, one such item that students from advantaged backgrounds\nwill have access to that those from lower income brackets will not is ChatGPT4 [4]. It\ncurrently costs $20 per month for a subscription and has been called a \"significant leap\nforward\" compared to ChatGPT3.5 [5], which is free [6]. While use of generative AI is\nprohibited in some college classrooms, this is hard to police, and many students use it\nregardless of classroom restrictions [7]. When generative AI is allowed, there is a wide\naray of platforms from which students can choose. The platforms include Anthropic's\nClaude [8], Google Gemini [9], Microsoft Co-Pilot [10], Pi [11], ChatGPT3.5 [5], and\nChatGPT4 [4]. As of the writing of this article, Pi and ChatGPT3.5 are free for use.\nClaude and Gemini have both free and \"pro\" versions, where the pro version promises\ngreater functionality and accuracy for the price of approximately $20 per month, much\nlike ChatGPT4. Copilot is currently free with a license for Office 365 [12].\nRegardless of the platform used, some students will be able to pay for newer, faster,\nmore equipped versions, while others will need to use the free version. Many believe\nthat generative AI platforms have the potential to revolutionize education and shrink\nor even close the gap between the haves and have nots with respect to education [13].\nThis belief assumes that each student has access to the same version of a generative AI\nplatform. A significant difference in performance between the free and paid versions\nof generative AI platforms would, ironically, widen the knowledge gap even further.\nAdmittedly, lack of financial resources is not the only reason someone would not\nwant to purchase a paid version of any generative AI platform. Some students or\nfaculty simply do not want to give their credit card information to an entity that is\nuntested. Some individuals might not think they will use it enough to justify the cost.\nOthers might not want the extra complexity and services that come with a monthly\nsubscription. In addition, there are other issues of equity and access besides the ability\nor desire to pay for course materials or study aids. One accessibility issue involves\nthe visual nature of these platforms. All of them involve typing text and reading\nthe output. Both typing and reading might be barriers to some. To overcome these\nbarriers, some platforms have text-to-speech capability. Pi, Gemini, and ChatGPT"}, {"title": "2 Literature Review", "content": "Much of the existing literature has explored the ethics of potential general uses for gen-\nerative AI in the classroom, as well as some of the risks [16]. Ethical issues, reliability,\nand robustness have been identified as considerations for the use of AI generated con-\ntent (AIGC) in engineering management [17]. Specifically, a rigorous system of checks\nand balances on AIGC is recommended to avoid potential safety concerns. Such a\nsystem does not yet exist. A review paper on the use of AI in education gives more\ndetails [18].\nMost comparisons of ChatGPT3.5 to ChatGPT4 have been done in the context of\nperformance on standardized exams. For example, ChatGPT3.5 performs worse than\nChatGPT4 on the American Academy of Opthomology exam [19]. The performance\nof ChatGPT3.5 vs. ChatGPT4 has also been compared using the ACT, SAT, and\nother college entrance exams, in addition to many other typical student assignments\n[20]. ChatGPT3.5 received a score of 68% on the Bar Exam [21], while a different\nstudy showed ChatGPT4 passed the bar with a score much better than the average for\nhumans [22]. Physicians have tested the platforms' ability to diagnose certain ailments\n[23]. In all of the studies of this nature done thus far, ChatGPT4 has been the clear\nwinner."}, {"title": "3 Methods", "content": "We selected four different exams representing various levels of statistics knowledge to\nuse in this study. Two of them were nationally normed exams, the Comprehensive\nAssessment of Outcomes in Statistics (CAOS) [15] and the AP statistics exam. The\nCAOS exam was originally written to gauge student learning outcomes in college-\nlevel introductory statistics courses. The AP exam is meant to replace a college level\nstatistics course; therefore, it can be considered as a level above the CAOS exam. We\nalso a standardized test designed by the Arkansas Council of Teachers of Mathematics\n(ACTM) [14], which is meant to quantify understanding of statistical concepts at the\nhigh school level. Finally, we used questions from a homemade exam given in a first-\nyear graduate course in statistical methods for PhD students in statistics, biostatistics,\nand data science. Questions from this last exam are meant to represent assessment of\nstatistical knowledge at the graduate level. Regardless of the level of exam, all exams\nconsisted of two main types of questions: multiple choice and free response. Each type\nof question has questions where an image was displayed or not. Therefore, there were\nfour types of questions: multiple choice without an image (MC, n = 43), multiple\nchoice with an image (MCI, n = 22), free response without an image (FR, n = 20)\nand free response with an image (FRI, n = 8). Examples of each type of question are\ngiven in Appendix A."}, {"title": "3.1 McNemar's Test", "content": "McNemar's test is a x2 test designed for examining the relationship between two\nvariables (ChatGPT3.5 and ChatGPT4, in this case) with two categories each where\nthe units are paired. The multiple choice questions fit this scenario. The units are\nthe questions, and each question was scored as either correct or incorrect. Because we\nentered the same questions into each version of ChatGPT, the outcomes are paired.\nMcNemar's test examines the discrepancy between discordant versus concordant pairs.\nA discordant pair is one in which one the platforms provide different outcomes (ie.\none of the platforms provides a correct solution while the other does not). A question\ngenerates a concordant pair when both platforms provide the same outcome (either\nboth answer that question correctly or both incorrectly.)\nThe null and alternative hypotheses of McNemar's test are:\nHo: The probability of ChatGPT4 getting a question right and ChatGPT3.5 getting\nit wrong is the same as ChatGPT4 getting a question wrong and ChatGPT3.5\ngetting it right."}, {"title": "3.2 Ordinal Logistic Regression", "content": "Ordinal logistic regression is an extension of binary logistic regression. It is used to\nmodel the relationship between an ordinal response variable and one or more predic-\ntor variables. The predictor variables can be either categorical or quantitative. It is\nsuitable when the response variable has ordered categories but the intervals between\ncategories are not assumed to be equal.\nThe model can be expressed as:\nlog$\\frac{P(Y \u2264 j)}{P(Y > j)}$ = \u03b1; \u2013 \u03a7\u03b2\nwhere P(Y < j) is the cumulative probability of the response Y being less than or\nequal to category j, $\\frac{P(Y \u2264 j)}{P(Y > j)}$ are known as the odds, aj are the threshold parameters\nfor each category j, X is a matrix of predictor variables, \u03b2 is a vector of coefficients\nassociated with the predictor variables.\nWith logistic regression models, the odds ratio of an event is often calculated as\npart of the output. The odds represent the likelihood of an event occurring compared\nto the likelihood of it not occurring. In ordinal logistic regression, the odds ratio\ncompares the odds of the response being in one category versus the odds of it being\nin a lower category.\nTo estimate the odds ratio from the parameter estimates (\u03b2), the coefficient asso-\nciated with the predictor variable is back transformed using the exponential function.\nFor example, if \u03b2\u2081 is the coefficient for a predictor variable X1, the odds ratio associ-\nated with a one-unit increase in X1 is e\u03b21. The mathematical relationship is shown in\nEquation 1.\ne\u03b21 = $\\frac{P(Y < j | X1 + 1),P(Y < j | X1)}{P(Y > j | X1 + 1)\u2032 P(Y > j | X1)}$  (1)\nThis study employs ordinal logistic regression to evaluate the relationship between\na set of predictors and an ordinal response variable, which has ordered categories but\nunequal intervals. To accommodate the paired nature of the data, the response variable\nwas defined as the difference between scores for the each question between ChatGPT4\nand ChatGPT3.5. The scores for each question were given using the rubric in Table\n1. The differences ranged from scores < 0 (indicating ChatGPT3.5 received a higher\nscore than ChatGPT4) to 4 (ChatGPT4 was completely correct and ChatGPT3.5\nwas completely incorrect). We translated the response variable, indexed by j into the\nfollowing comparative evaluations of ChatGPT4 and ChatGPT3.5:\nMuch: ChatGPT4 was \"Much better\" than ChatGPT3.5.\nSomewhat: ChatGPT4 was \"Somewhat better\" than ChatGPT3.5.\nSame: Both versions performed \"the same.\"\nWorse: ChatGPT4 performed \"Worse\" than ChatGPT3.5.\nThe ordinal logistic regression model is formulated as:\nlog$\\frac{P(Y \u2264 j)}{P(Y > j)}$ = aj + \u03b2\u2081FR + B2Image\nwhere aj are thresholds (intercepts) that define the boundaries between categories,\nand \u03b21 and B2 are coefficients for the predictors:\nFR: Indicator of Free Response (1) versus Multiple Choice (0) questions.\nImage: Presence (1) or absence (0) of an image in the question.\nInterpretation of Exponentiated Coefficients: The exponentiated coeffi-\ncients, e\u03b21 and e\u03b22, represent the multiplicative change of the odds of ChatGPT4\nproviding a higher quality response than ChatGPT3.5 for different levels of the Type\nquestion (FR or MC) and if the question has an image (0 or 1), respectively. This\nmultiplicative change is also known as the odds ratio."}, {"title": "4 Results", "content": "This investigation began with curiosity about the performance of ChatGPT3.5 and\nChatGPT4 on an exam given to seven first-year graduate students in a statistical\nmethods course in the fall of 2022. This is the same \"homemade\" graduate exam men-\ntioned in Section 2. Exam questions were loaded one-by-one into either ChatGPT3.5\nand ChatGPT4, and the answers from each generative AI platform was graded with\nthe same rubric as the student exams were graded. Grades were collected from the\nstudents' answers and from the two versions of ChatGPT.\nWe then extended the investigation to include the overall performance of Chat- GPT3.5 and ChatGPT4 on each of the nationally normed exams (CAOS, AP, and ACTM). The overall scores for the first-year exam shown in Fig 1 are given in the last row for comparison. While each exam has its own official purpose and scale, if we use the traditional passing threshold of 70% correct, we see that ChatGPT3.5 would have failed all tests while ChatGPT4 would have passed these same tests. Note that if Chat- GPT3.5 and ChatGPT4 provided solutions with the same quality, the probability of getting this result by chance is only 6.25% (1/16)."}, {"title": "4.1 Examining Questions with and without Images", "content": "As noted in Table 5, of the 63 questions that did not have images, there were 17\nquestions that ChatGPT4 answered correctly that ChatGPT3.5 did not while there\nwere only 6 that ChatGPT3.5 answered correctly that ChatGPT4 did not. McNemar's\ntest yields a p-value of .03 for these data indicating a 3 in 100 chance of seeing a\ntest statistic as large or larger than the one calculated if the null hypothesis is true.\nThis p-value is sufficient evidence of a difference in performance of ChatGPT3.5 and\nChatGPT4. This difference is reflected in the fact that ChatGPT4 answered 88% of\nquestions without images correctly while ChatGPT3.5 only answered 71% of the same\nquestions correctly."}, {"title": "4.2 Analysis by Question Type", "content": "In this section we analyze correct and incorrect results separately for free response\nquestions and multiple choice questions. Answers to free response questions were\ndichotomized into \"correct\" if the score on a question was \u2265 3 as given by Table 1 and\n\"incorrect\" if the score was < 3. As noted in Table 7, of the 28 free response questions,\nthere were 18 that ChatGPT4 answered correctly that ChatGPT3.5 did not and only 2\nthat ChatGPT3.5 answered correctly that ChatGPT4 did not. McNemar's test yields\na p-value of .010 for these data indicating sufficient evidence of a difference in perfor-\nmance of ChatGPT3.5 and ChatGPT4 for free response questions. This difference is\nreflected in the fact that ChatGPT4 answered 82% (23/28) of free response correctly\nwhile ChatGPT 3.5 only answered 43% (12/28) of the same questions correctly."}, {"title": "4.3 Ordinal Logistic Regression Results", "content": "Figs 2 and 3 showed answers from ChatGPT3.5 and ChatGPT4 graded as either\n\"correct\" or \"incorrect\". In other words, the focus was on the accuracy of the answer\nto each questions. In this section, we focus on the quality of the answer between\nChatGPT3.5 to ChatGPT4 for through the scale for free response questions introduced\nin Table 1.\nTo determine the effect of type of question (free response or multiple choice) and\npresence of an image within the question (Image or Not), we performed an ordinal\nlogistic regression where the response was given on a scale of 0 to 4 as shown in Table\n1. The parameter estimates and associated statistics for each coefficient in the model\nare shown in Table 9."}, {"title": "5 Discussion", "content": "Sal Kahn of Khan Academy has, since 2008, advocated for every child in the World to\nhave a personal tutor. In that year, he put his educational videos on YouTube in an\neffort to democratize education for all. His mission to provide everyone with their own\ntutor was given a great boon by the emergence of ChatGPT's potential to provide\ninsights into a wide variety of subjects with the knowledge of what it was trained on,\nnearly the entire Internet. OpenAI, the company that developed ChatGPT, saw this\ngreat potential early and made its ground breaking GPT3 technology available to Dr.\nKahn in June of 2022 [33], six months before its blockbuster release in November 2022.\nThe eventual product to emerge was named Kahnmigo and was hailed as a great leap\nforward towards accomplishing his mission of a personal tutor for all. [34]\nWhile Sal Kahn's goal was to democratize education and bridge the educational\ndivide between different socioeconomic classes, ironically, the introduction of a more\npowerful paid version of this technology (ChatGPT4) introduces the potential to\nincrease this divide. The premise in which we address the potential equity issue posed\nhere is that good tutoring is delivered in two ways:\nA. Through good general tips, hints, metaphors and examples\nB. Through thorough hints and solutions to specific practice problems.\nSimilar to how a human tutor is often assessed, a proxy for these two tenants\nof tutoring will be measured performance on practice problems in statistics that are\nsimilar to those that would be seen in class and that are aimed at facilitating the\nmastery of the desired skill and methods. In order to detect and measure a difference in\nquality between ChatGPT3.5 and ChatGPT4 in their responses to potential practice\nquestions, we have partitioned the question bank into those with images and those\nwithout images.\nChaptGPT3.5 is not able to ingest images; thus for questions for which reference\nto an image is part of the question, ChatGPT3.5 is limited to the text. This limitation\nis shown in that, of the 30 questions that had images, both platforms answered 2 of\nthem correctly (upper left in Table 6), both platforms answered 10 of them incorrectly\n(lower right in Table 6) and ChatGPT4 provided correct responses to the remaining\n18 questions (lower left in Table 6) that ChatGPT3.5 subsequently could not provide\na response and were thus scored as \"incorrect\" (upper right in Table 6). There is\noverwhelming evidence to suggest that of the questions that the platforms disagree\non (the discordant pairs). ChatGPT4 will be more likely to answer correctly with\nrespect to ChatGPT3.5 (p-value < .0001 from McNemar's test). For reference, in our\nstudy ChatGPT4 answered 66% of questions correctly while ChatGPT3.5 was only\nable to to provide correct solutions to 6% of these questions. Furthermore, recall our\nordinal logistic regression analysis yielded estimated probabilities of over 60% that\nChaptGPT4 would provide a higher quality response that ChatGPT3.5 for questions\nwith images (free response and multiple choice). However, we did not examine the\neffect of alt text, which is text embedded within a visualization to be read aloud\nby screen readers, contained within plots on the interpretability of images for either\ngenerative AI platform.\nWhile questions with images exposed an important difference in quality between\nthe two platforms, there was also evidence of a difference between the platforms for\nquestions that did not have images. Overall, ChatGPT4 answered 88% of questions\nwithout images correctly while ChatGPT3.5 only answered 71% of the same questions\nwith a correct response. If translated into grades, ChatGPT4 would receive a B+ while\nChatGPT3.5 would receive a C-. For the discordant pairs, there was again sufficient\nevidence to suggest that ChatGPT4 is more likely to provide a correct solution (p-\nvalue from McNemar's test was .03) and it was estimated that for questions without\nimages there was a greater than 20% chance that ChatGPT4 would provide a higher\nquality response than ChatGPT3.5 for questions without images regardless of whether\nthe quesiton was free response or multiple choice.\nInterestingly, while ChatGPT4 was found to be more likely to provide higher qual-\nity solutions to both questions with and without images, we are 95% confident that\nthe odds of ChatGPT4 providing higher quality solutions than ChatGPT3.5 for ques-\ntions with images is between 176% and 1575% more than the odds of a higher quality\nresponse for questions without images (p-value = .00007 from an ordinal logistic\nregression).\nIt is also intuitive that platforms may have different advantages when it comes\nto the modality of the question: multiple choice versus free response. For instance, it\nmay be that both platforms perform similarly well on multiple choice questions and\nthat equity concerns are minimized for these types of questions. The evidence strongly\nsuggested that ChatGPT4 provided higher quality responses (increased likelihood of\nbeing correct) for both multiple choice (McNemar p-value = .0009) and free response\nquestions (McNemar p-value = .010). Furthermore, the estimated probability of Chat-\nGPT4 providing a higher quality response than ChatGPT3.5 was at least 23% for free\nresponse questions and at least 33% for multiple choice questions.\nA limitation of this study is that the sample size was not large enough to sta-\ntistically exclude the plausibility that the difference detected in the modality of the\nquestion is confounded with the presence of an image; however, the empirical evi-\ndence is compelling. Of the eight free response questions that had images, ChatGPT4\nprovided a correct solution for five (62.5%) while ChatGPT3.5 did not answer any\ncorrectly. Additionally, of the remaining 20 free response questions without images,\nChatGPT4 answered 18 (90%) correctly while ChaptGPT3.5 was only able to answer\nten (50%) correctly. Furthermore, ChatGPT4 answered 80% of the the questions that\nmade up the 10 discordant pairs correctly.\nAnother potential confounding variable in the study is the difficulty of the ques-\ntion; a proxy for this is the exam to which each question belongs. The 93 questions\nin this study were sourced from four different tests of variable difficulty. The most\nintroductory was the ACTM as it focused on assessing high school statistical skill.\nThe next level up was the CAOS exam which is focused on introductory college level\nstatistics student, while the AP exam is aimed at more advanced students who are\nlooking to earn credit for having completed a introductory college statistics course.\nThe span of difficulty was capped at the graduate statistics level from one of the\nauthor's graduate statistics exams. Again, while this study did not have enough power\nto statistically test for these effects, the empirical evidence was very convincing as\nChatGPT4 outperformed ChatGPT3.5 on every exam (see Table 3).\nTo this point, this study has provided evidence to suggest that ChaptGPT4 is more\nlikely to provide higher quality responses to questions regardless of their image content\nand modality. We submit that it is reasonable to conclude that those that must use"}, {"title": "6 Extensions and Future Work", "content": "On May 13, 2024, Open AI announced the release of ChatGPT40. According to\nOpenAI, the release of ChatGPT40 will make more features of ChatGPT4 avail-\nable to without a (paid) subscription. These features include access to \"GPT-4 level\nintelligence\" and the ability to read and upload images [35]. However, they also\nmentioned,\nThere will be a limit on the number of messages that free users can send with GPT-40\ndepending on usage and demand. When the limit is reached, ChatGPT will automatically\nswitch to GPT-3.5 so users can continue their conversations. [35]\nAs of the submission of this paper, this limit has yet to be defined; however,\nsince paid users will not experience usage limits, the existence of the limit means\nthe persistence of the equity concerns described in this study. Therefore, the most\nimmediate extension of this paper will be to assess just how much better ChatGPT40\nis than ChatGPT4 at \"understanding\" images with respect to statistics questions.\nCurrently, the performance of ChatGPT4o is quite impressive. Users can hold a smart\nphone above a paper on which there is a hand-written question, and ChatGPT40\ncan read the equation and give hints on how to solve it. This remarkable ability was\ndemonstrated in a video using a simple algebraic question with one variable [35]. It\nis unclear how ChatGPT4o would respond to a system of hand-written equations, or\na complicated multiple integral, or in determining differences in medians among a set\nof hand-drawn boxplots.\nIn terms of equity for visually impaired individuals, the video demonstration of\nChatGPT40 showed remarkable text to speech capability, both in ChatGPT40's ability\nto understand speech and to respond in clear speech. ChatGPT40 can vary pitch and\ntone to be more dramatic or to speak like a robot. Therefore, it seems that visually\nimpaired individuals can use ChatGPT40 with little trouble. However, during the\ndemonstration of ChatGPT40's ability to read handwritten equations on paper, the"}, {"title": "Appendix A Samples of the Four Types of\nQuestions Entered into ChatGPT3.5\nand ChatGPT4", "content": "In this Appendix, we provide examples of the four types of questions examined in this\nresearch."}]}