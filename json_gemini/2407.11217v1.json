{"title": "Almost-linear Time Approximation Algorithm to Euclidean\nk-median and k-means", "authors": ["Max Dupr\u00e9 la Tour", "David Saulpic"], "abstract": "Clustering is one of the staples of data analysis and unsupervised learning. As such,\nclustering algorithms are often used on massive data sets, and they need to be extremely\nfast. We focus on the Euclidean k-median and k-means problems, two of the standard ways\nto model the task of clustering.\nFor these, the go-to algorithm is k-means++, which yields an O(logk)-approximation in\ntime \u00d5(nkd). While it is possible to improve either the approximation factor [Lattanzi and\nSohler, ICML19] or the running time [Cohen-Addad et al., NeurIPS 20], it is unknown how\nprecise a linear-time algorithm can be.\nIn this paper, we almost answer this question by presenting an almost linear-time algo-\nrithm to compute a constant-factor approximation.", "sections": [{"title": "Introduction", "content": "The k-means objective function was introduced by Lloyd in 1957 (and published later in [Llo82])\nas a measure of the quality of compression. Given a set of points P and an integer k, minimizing\nthe k-means objective yields a set of k centers that provide a good compressed representation\nof the original dataset P. Lloyd's original motivation was to compress analog audio signals into\nnumerical ones: numerical signals have to be discrete, and Lloyd proposed a method to find which\nfrequencies should be kept in the discretization. His method was a heuristic trying to minimize\nwhat he called the quantization error, which is the sum, for each point, of the squared distance to\nits representative. This is precisely the k-means cost, and the goal of the k-means problem is to\nfind the set of k representatives (or centers) that minimizes this cost. In contrast, the k-median\ncost function is the sum, for each point, of the distance to its closest center, inherently giving\nless weight to the outliers in the dataset.\nSince 1957, these compression methods have been widely adopted, extended to clustering tasks,\nand have become one of the prominent unsupervised learning techniques. This has entirely\nchanged the size and shape of the datasets involved. It is now common to have billions of input\npoints and a target compression size k in the hundreds of thousands, and to solve k-means or\nk-median to extract the essence of the dataset.\nThis fundamentally changes the nature of the algorithms that can be used to solve the compres-\nsion task. Simple polynomial-time algorithms, even with small polynomial running times like\nLloyd's original method (which runs in time $(ndk)$), are no longer applicable, and there is a\ncrucial need for linear-time algorithms. The question we ask in this paper is: How fast can we\nsolve k-means and k-median?\nThe complexity of these problems naturally depends on the metric space from which the input is\ndrawn. In general metric spaces, this complexity is well understood: it is not possible to compute\nany approximation in time o(nk), and there is a constant-factor approximation algorithm-i.e.,\nan algorithm that computes a solution with a cost O(1) times the optimal cost-with this running\ntime [MP04].\nThe picture is not as clear in Euclidean space, which is arguably the most common case in\npractice. The problem remains NP-hard even in dimension 2 (see [MNV12] for k-means, [MS84]\nfor k-median) or when k = 2 [DF09], but there exist fast approximation algorithms. The success\nof the famous k-means++ algorithm [AV07] is due to its O(nkd) running time combined with\nan approximation guarantee of O(log k). This has become the method of choice for practitioners\nand is often used as a baseline when evaluating the quality of a clustering algorithm.\nBoth the running time and the approximation guarantee of k-means++ can be improved. The\nrunning time of this algorithm has been improved to almost linear time $\\tilde{O} (nd + (n \\log \\Delta)^{1+\\epsilon})$\nby [CLN+20], with an approximation factor of $O(f(\\epsilon) \\log k)$ and an extra small dependency on\nthe aspect ratio of the dataset A, which is the ratio between the largest and smallest distance\nbetween input points. The algorithm of [CHH+23] has a slightly better running time $\\tilde{O}(nd)$\nbut an exponentially worse approximation ratio of $O(k^4)$. On the other hand, it is possible to\nimprove the approximation guarantee from k-means++ by combining it with local search: this\nprovides a constant-factor approximation while preserving a running time of $\\tilde{O}(nkd)$ [LS19].\nHowever, no algorithm combines the best of both worlds, namely, a really fast algorithm with a\nconstant-factor approximation guarantee."}, {"title": "Our contribution", "content": "We provide an almost optimal answer to our question and demonstrate the existence of an almost\nlinear-time algorithm that computes a constant-factor approximation for the (k, z)-clustering\nproblem. This problem is a generalization of k-median and k-means, which seeks to minimize\nthe sum of the z-th power of the distance from each client to its center (k-means for z = 2,\nk-median for z = 1).\nFurthermore, our algorithm not only computes a solution to (k, z)-clustering, but it also provides\nan ordering of the input points $p_1, ..., p_n$ such that for any k, the set ${p_1,..., p_k }$ forms an O(1)-\napproximation to (k, z)-clustering. This variant of (k, z)-clustering is referred to as online [MP03]\nor incremental [She16][CH11] in the literature. This is a shared feature with algorithms based\non k-means++, where each prefix is an O(polylogk) approximation.\nTheorem 1.1. For any $c > 5$, there exists an algorithm with running time $\\tilde{O}(d\\log \\Delta \\cdot n^{1+1/c^2+o(1)})$\nthat computes a poly(c)-approximation to the incremental (k, z)-clustering problem.\nWe note that, in time $\\tilde{O}(nd)$, the dimension can be reduced to $O(\\log n)$ using Johnson-\nLindenstrauss embedding [JL84]. Furthermore, in time $\\tilde{O}(nd)$, the aspect ratio can be reduced\nto poly(n) (by combining the approximation algorithm from [CHH+23] and the diameter\nreduction from [DSS24]). This leads to the following corollary.\nCorollary 1.2. For any $c > 5$, there exists an algorithm with running time $\\tilde{O} \\left(nd+n^{1+1/c^2+o(1)}\\right)$\nthat computes a poly(c)-approximation to the (k, z)-clustering problem.\nNote that we lose the incremental property in this corollary because the reduction of the aspect\nratio depends on an estimate of the (k, z)-clustering cost and therefore on k.\nVery recently, [DITHS24] showed how to maintain a coreset of size O(k) in the fully dynamic\nsetting with update time $\\tilde{O}(d)$ and query time $\\tilde{O}(kd)$. Combined with our theorem, this leads\nto the following corollary.\nCorollary 1.3. There is an algorithm in the fully dynamic setting on $R^d$ that maintains a\npoly(c)-approximation of the (k, z)-clustering with update time $\\tilde{O}(d)$ and query time $\\tilde{O}(kd +\nk^{1+1/c^2+o(1)})$.\nThis means the query time is almost the same as the running time to output a solution."}, {"title": "Sketch of our techniques", "content": "Our algorithm builds upon the hierarchically greedy algorithm developed by Mettu and Plaxton\n[MP03] for the incremental k-median problem. Their algorithm selects k sequences of balls\ngreedily: each sequence has geometrically decreasing radii, and the algorithm places a center in\nthe last ball of each sequence. The process begins by selecting the first ball in a sequence to\nmaximize a function ValueMp among balls that are \"very distant\" from all previously placed\ncenters. Then, the (i + 1)-th ball in the sequence is chosen as the one with the highest ValueMp\nvalue among balls with a radius geometrically smaller than the i-th ball and located \"close\" to\nit. A sequence terminates when a ball contains a unique point that is \"very distant\" from all\nother points. This unique point is then designated as a center, and the algorithm initiates a new\nsequence.\nThe original ValueMp in Mettu and Plaxton's algorithm comes from the dual of the Lagrangian\nrelaxation of the standard k-median linear program. For a ball B(x, R), it is $\\text{ValueMP} (B(x, R)) :=$"}, {"title": "Preliminaries", "content": ""}, {"title": "Definitions", "content": "The Euclidean (k, z)-clustering problem is defined as follows: the input is a multiset P\u2286 Rd,\nan integer k, and a z > 1. The goal is to find a set of k points S that minimizes $\\text{COST}(P, S) :=\n\\sum_{p\\in P} \\text{dist}(p, S)^z$, where $\\text{dist}(p, S) := \\min_{s \\in S} \\text{dist}(p, s)$ and dist is the Euclidean distance. We\nsay that a set of k points Ck is an \u03b1-approximation to (k, z)-clustering when $\\text{COST}(P, C_k) \\leq\n\\alpha \\min_{|S|=k} \\text{COST}(P, S)$.\nA list of n points $C_1, ..., C_n$ is an \u03b1-approximation to the incremental (k, z)-clustering problem on\ninput P when for any k = 1,..., n, the prefix $C_1, ..., C_k$ is an \u03b1-approximation to (k, z)-clustering\non P.\nIn the following, we fix a c \u2265 5, which will govern the trade-off between run-time and approxi-\nmation ratio.\nWe let A be an upper bound on the diameter of the input P (i.e., the largest pairwise distance).\nWe assume for simplicity that \u2206 is a power of 2c, and that the smallest pairwise distance is at\nleast 1.\nFor (k, z)-clustering, we can assume \u2206 = poly(n): [DSS24] showed how to transform any input P\nto reduce the diameter. Their algorithm runs in time $O (nd \\log \\log \\Delta)$, which is the running-time\nof their algorithm to compute a poly(n)-approximation: this has been improved to $\\tilde{O}(nd)$ by\n[CHH+23], hence we can reduce to \u2206 = poly(n) in time $\\tilde{O}(nd)$."}, {"title": "Basic tools", "content": "The first tool we use to speed-up the algorithm is Locallity-sensitive hashing [AI06]. The precise\nresult we use is the following:\nLemma 2.1 (See section D in [CLN+20]). Let P \u2286 Rd, and $l = (n/\u03b4)^{1/c^2}$; there is a family\nof hash functions from Rd to some universe U such that, with probability $1 \u2212 \u03b4$, if $f_1, ..., f_l$ are\ndrawn from this family:\n\\begin{itemize}\n    \\item For any p,q \u2208 P with $\\text{dist}(p,q) \\geq c \\cdot R$, then for all i = 1, ..., l $f_i(p) \\neq f_i(q)$\n    \\item For any p, q \u2208 P with $\\text{dist}(p,q) \\leq R$, then there exists $i \\in \\{1, ..., l\\}$ with $f_i(p) = f_i(q)$.\n\\end{itemize}\nFurthermore, the hash functions satisfy the following:\n\\begin{itemize}\n    \\item for any i, p \u2208 Rd, computing $f_i(p)$ takes time $O (dn^{o(1)})$, \n    \\item after preprocessing time $O (ld \\cdot n^{1+o(1)})$, one can compute for any i,p the set $T_i[u] := \\{p :\nf_i(p) = u\\}$ in time $O(|T_i[u]|)$.\n\\end{itemize}\nWe use the previous lemma in two ways: first, it allows us to compute an approximate neigh-\nborhood of each point quickly, and second, combined with streaming techniques, to estimate the\nsize of this neighborhood efficiently. We start with the former (where we replaced, for simplicity\nof notation, the success probability 1 \u2212 \u03b4 with 1 \u2212 1/n\u00b2):\nCorollary 2.2. For any R\u2208 R+ and P \u2286 Rd, there is a datastructure with preprocessing time\n$O \\left(dn^{1+3/c^2+o(1)}\\right)$ that can, with probability 1 \u2212 1/n\u00b2:"}, {"title": "The Algorithm", "content": "Here, we describe our algorithm, for which we give a pseudocode in Algorithm 1.\nInput: The algorithm is given a set of points PC Rd and a target number of clusters k.\nOutput: A set $C_k = \\{C_1,..., c_k\\}$ of centers.\nPreprocessing: The algorithm will consider all the balls of the form B(p, R), where p\u2208 P is\nan input point and R is a power of 2c such that $1/(2c)^7 < R \u2264 \u0394$. All these balls are marked as\navailable when the algorithm starts. Using Lemma 2.3, the algorithm computes Value(B(p, R))\nfor each ball B(p, R). The algorithm then preprocesses the data structure of Corollary 2.2 in\norder to have access to all N(p, 10c R) and $N(p, 100c^4R)$ for all radii R power of 2c such that\n$1/(2c)^7 < R \u2264 \u0394$.\nIterative Greedy Choices: The algorithm selects the centers one by one. After one center\nis chosen, the balls that are close to this center (relative to their radius) are removed from the\nset of available balls. More formally:\nj-th Iteration Selecting a Center:\nEach iteration starts by selecting the available ball\nB(x), R) that maximizes Value. If no balls are available, the algorithm stops and outputs\nthe current set of centers. Otherwise, the algorithm inductively constructs a sequence of balls\n$(B(x_l, R_l))$, beginning with B (x), R). While Rl is not the minimum radius $1/(2c)^7$, the\nalgorithm computes the set $N (x_l,10c\\cdot R_l)$ (using the data structure from corollary 2.2) and\nselects the next ball of the sequence $B (x_{l+1}, R_{l+1})$ that maximizes Value among the balls of the\nform B (p, R/2) with $p \\in N (x_l, 10c R)$. At the end, the center of the last ball, cj, is selected\nas the j-th center for the clustering solution.\nj-th Iteration Removing Balls:\nAfter selecting cj, the algorithm computes, for all R\npowers of 2c such that $1/(2c)^7 < R \u2264 \u2206$, the set $N (c_j, 100c^4 \\cdot R)$ (using again the data structure\nfrom corollary 2.2). It removes from the set of available balls all balls of the form B(p, R)\nwith $p\\in N(c_j, 100c^4R)$ and removes p from the data structure of Corollary 2.2 computing\n$N(c_j, 100c^4\\cdot R)$."}, {"title": "Basic properties of the algorithm", "content": "We start with a simple property of the function Value.\nLemma 3.1. For any point p \u2208 P the function $l \\rightarrow \\text{Value}(B(p, \\Delta/(2c)^l))$ is decreasing."}, {"title": "Running time analysis", "content": "The running time of the algorithm is mostly determined by the running time of the two inner\nloops, namely the while loop line 8 of Algorithm 1, and the for loop line 14.\nWe first show in Fact 4.1 that any p that appears in a set N(x, 10c\u00b7 R) for some iteration j and\nl of the algorithm, is close to the center cj. This will be used to show that B(p, R/2c), and all\nits ancestors by Lemma 3.3, is removed from the available balls line 17 after cj is selected."}, {"title": "Proof of Correctness", "content": "The goal of this section is to prove that the outcome of the algorithm is a good approximation\nto (k, z)-clustering, which combined with Lemma 4.2, concludes the proof of Theorem 1.1. This\nis formally stated in the following theorem:\nTheorem 5.1. For any k, the set Ck output by the algorithm gives a O(poly(c))-approximation\nof the optimal (k, z)-clustering solution."}, {"title": "Putting things together: proof of Theorem 5.1", "content": "We conclude the proof of our main theorem as follows:\nProof of Theorem 5.1. Given the matching \u03c6 of Lemma 5.9, we can conclude as follows. Sum-\nming the inequality of the third property of \u03c6 gives"}]}