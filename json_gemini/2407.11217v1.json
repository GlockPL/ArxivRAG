{"title": "Almost-linear Time Approximation Algorithm to Euclidean\nk-median and k-means", "authors": ["Max Dupr\u00e9 la Tour", "David Saulpic"], "abstract": "Clustering is one of the staples of data analysis and unsupervised learning. As such,\nclustering algorithms are often used on massive data sets, and they need to be extremely\nfast. We focus on the Euclidean k-median and k-means problems, two of the standard ways\nto model the task of clustering.\nFor these, the go-to algorithm is k-means++, which yields an O(logk)-approximation in\ntime \u00d5(nkd). While it is possible to improve either the approximation factor [Lattanzi and\nSohler, ICML19] or the running time [Cohen-Addad et al., NeurIPS 20], it is unknown how\nprecise a linear-time algorithm can be.\nIn this paper, we almost answer this question by presenting an almost linear-time algo-\nrithm to compute a constant-factor approximation.", "sections": [{"title": "1 Introduction", "content": "The k-means objective function was introduced by Lloyd in 1957 (and published later in [Llo82])\nas a measure of the quality of compression. Given a set of points P and an integer k, minimizing\nthe k-means objective yields a set of k centers that provide a good compressed representation\nof the original dataset P. Lloyd's original motivation was to compress analog audio signals into\nnumerical ones: numerical signals have to be discrete, and Lloyd proposed a method to find which\nfrequencies should be kept in the discretization. His method was a heuristic trying to minimize\nwhat he called the quantization error, which is the sum, for each point, of the squared distance to\nits representative. This is precisely the k-means cost, and the goal of the k-means problem is to\nfind the set of k representatives (or centers) that minimizes this cost. In contrast, the k-median\ncost function is the sum, for each point, of the distance to its closest center, inherently giving\nless weight to the outliers in the dataset.\nSince 1957, these compression methods have been widely adopted, extended to clustering tasks,\nand have become one of the prominent unsupervised learning techniques. This has entirely\nchanged the size and shape of the datasets involved. It is now common to have billions of input\npoints and a target compression size k in the hundreds of thousands, and to solve k-means or\nk-median to extract the essence of the dataset.\nThis fundamentally changes the nature of the algorithms that can be used to solve the compres-\nsion task. Simple polynomial-time algorithms, even with small polynomial running times like\nLloyd's original method (which runs in time $O(ndk)$), are no longer applicable, and there is a\ncrucial need for linear-time algorithms. The question we ask in this paper is: How fast can we\nsolve k-means and k-median?\nThe complexity of these problems naturally depends on the metric space from which the input is\ndrawn. In general metric spaces, this complexity is well understood: it is not possible to compute\nany approximation in time o(nk), and there is a constant-factor approximation algorithm-i.e.,\nan algorithm that computes a solution with a cost O(1) times the optimal cost-with this running\ntime [MP04].\nThe picture is not as clear in Euclidean space, which is arguably the most common case in\npractice. The problem remains NP-hard even in dimension 2 (see [MNV12] for k-means, [MS84]\nfor k-median) or when k = 2 [DF09], but there exist fast approximation algorithms. The success\nof the famous k-means++ algorithm [AV07] is due to its O(nkd) running time combined with\nan approximation guarantee of O(log k). This has become the method of choice for practitioners\nand is often used as a baseline when evaluating the quality of a clustering algorithm.\nBoth the running time and the approximation guarantee of k-means++ can be improved. The\nrunning time of this algorithm has been improved to almost linear time \u00d5 (nd + (n log \u2206)1+\u20ac)\nby [CLN+20], with an approximation factor of O(f(s) logk) and an extra small dependency on\nthe aspect ratio of the dataset A, which is the ratio between the largest and smallest distance\nbetween input points. The algorithm of [CHH+23] has a slightly better running time \u00d5(nd)\nbut an exponentially worse approximation ratio of O(k4). On the other hand, it is possible to\nimprove the approximation guarantee from k-means++ by combining it with local search: this\nprovides a constant-factor approximation while preserving a running time of \u00d5(nkd) [LS19].\nHowever, no algorithm combines the best of both worlds, namely, a really fast algorithm with a\nconstant-factor approximation guarantee."}, {"title": "1.1 Our contribution", "content": "We provide an almost optimal answer to our question and demonstrate the existence of an almost\nlinear-time algorithm that computes a constant-factor approximation for the (k, z)-clustering\nproblem. This problem is a generalization of k-median and k-means, which seeks to minimize\nthe sum of the z-th power of the distance from each client to its center (k-means for z = 2,\nk-median for z = 1).\nFurthermore, our algorithm not only computes a solution to (k, z)-clustering, but it also provides\nan ordering of the input points P1, ..., Pn such that for any k, the set {P1,..., pk } forms an O(1)-\napproximation to (k, z)-clustering. This variant of (k, z)-clustering is referred to as online [MP03]\nor incremental [She16][CH11] in the literature. This is a shared feature with algorithms based\non k-means++, where each prefix is an O(polylogk) approximation.\nTheorem 1.1. For any c > 5, there exists an algorithm with running time \u00d5(dlog \u2206.n1+1/c\u00b2+o(1)))\nthat computes a poly(c)-approximation to the incremental (k, z)-clustering problem.\nWe note that, in time \u00d5(nd), the dimension can be reduced to O(logn) using Johnson-\nLindenstrauss embedding [JL84]. Furthermore, in time \u00d5(nd), the aspect ratio can be reduced\nto poly(n) (by combining the approximation algorithm from [CHH+23] and the diameter\nreduction from [DSS24]). This leads to the following corollary.\nCorollary 1.2. For any c > 5, there exists an algorithm with running time \u014c $\\left(nd+n^{1+1/c^{2}+o(1)}\\right)$\nthat computes a poly(c)-approximation to the (k, z)-clustering problem.\nNote that we lose the incremental property in this corollary because the reduction of the aspect\nratio depends on an estimate of the (k, z)-clustering cost and therefore on k.\nVery recently, [DITHS24] showed how to maintain a coreset of size O(k) in the fully dynamic\nsetting with update time \u00d5(d) and query time \u00d5(kd). Combined with our theorem, this leads\nto the following corollary.\nCorollary 1.3. There is an algorithm in the fully dynamic setting on Rd that maintains a\npoly(c)-approximation of the (k, z)-clustering with update time \u00d5(d) and query time \u00d5(kd +\nk1+1/c\u00b2+o(1)).\nThis means the query time is almost the same as the running time to output a solution."}, {"title": "1.2 Sketch of our techniques", "content": "Our algorithm builds upon the hierarchically greedy algorithm developed by Mettu and Plaxton\n[MP03] for the incremental k-median problem. Their algorithm selects k sequences of balls\ngreedily: each sequence has geometrically decreasing radii, and the algorithm places a center in\nthe last ball of each sequence. The process begins by selecting the first ball in a sequence to\nmaximize a function ValueMp among balls that are \"very distant\" from all previously placed\ncenters. Then, the (i + 1)-th ball in the sequence is chosen as the one with the highest ValueMp\nvalue among balls with a radius geometrically smaller than the i-th ball and located \"close\" to\nit. A sequence terminates when a ball contains a unique point that is \"very distant\" from all\nother points. This unique point is then designated as a center, and the algorithm initiates a new\nsequence.\nThe original ValueMp in Mettu and Plaxton's algorithm comes from the dual of the Lagrangian\nrelaxation of the standard k-median linear program. For a ball B(x, R), it is ValueMP (B(x, R)) :=\n$\\Sigma_{p \\in P \\cap B(x,R)} (R^{z} - d(x,p)^{z})$ where P is the input dataset.\nOur first and crucial improvement is to replace their value ValueMP(B(x, R)) with simply the\nnumber of points in the ball: Value(B(x, R)) = |P\u2229B(x, R)|\u00b7Rz. These two quantities are related:\nValueMP (B(x, R)) < Value(B(x, R)) \u2264 ValueMP (B(x, 2R)). Our main technical contribution is\nto show that the algorithm that uses Value instead of ValueMp still computes a constant-factor\napproximation, if we carefully adjust the definitions of \"close\" and \"very distant\u201d.\nAs our proof shows that, in the particular case of the algorithm we analyze, the quantities\nValueMp and Value are equivalent, we believe that our results may shed new light on the standard\nlinear program.\nWe start by briefly explaining why such a greedy algorithm provides a good solution and will\nexplain later how to implement it in linear time. We let Ck = {C1,...,Ck} be the set of centers\ncomputed by the algorithm. The first part of the proof relates the cost of each cluster of the\noptimal solution to the value of a particular ball. For a center y in the optimal solution with\ncluster Py, the analysis chooses a radius Ry such that Ry = dist(\u03b3, Ck)/\u03b1, where a is a large\nconstant: points in B(y, Ry) are much closer to y than to any center in C. On the other hand,\npoints of Py that are not in B(\u03b3, R\u2084) are roughly at the same distance from y and from Ck, and\ntheir cost is well approximated by Ck. Thus, we only focus on points lying in that ball we call\nthem In(Py).\nThe first crucial property is that the cost of the points in the solution Ck is at most O(1).\nValue(B(\u03b3, R\u2084)) (i.\u0435., |In(P\u2084)|\u00b7 R). This is due to the definition of Ry: there is a center of Ck\nat distance a. Ry from y. Thus, by the triangle inequality, each point of In(Py) is at distance\nO(R) from Ck. Therefore, the cost of In(Py) in the solution C is roughly Value(B(\u03b3, R\u2084)): this\nis where the values come into play.\nThe clustering cost of Ck is therefore essentially bounded by the sum of values of the balls\nB(, R), for all centers y of the optimal solution \u0393. To bound this sum, we must relate the\nvalue of balls to their cost in the optimal solution: this is easy to do when the ball is far\nfrom F. Indeed, for any point x, if dist(x, \u0393) \u2265 2R we then say that B(x, R) is uncovered\nthen each point in the ball B(x, R) pays at least R in the optimal solution, and therefore\nValue(B(x, R)) \u2248 COST(B(x, R) \u2229 \u0420, \u0413).\nAll the above is true regardless of the algorithm used to compute Ck. To continue the proof,\nthe issue is that the balls B(y, R\u2084) are not far from \u0393, and we cannot directly relate their values\nto the cost in \u0413. This is precisely where we rely on the greedy choices of the algorithm, which\nselects balls with maximal value, in order to match each B(y, R\u2084) with balls that have larger\nvalue. We also want two extra properties: (1) that those balls are far from \u0393-to be able to apply\nthe previous inequality to relate their value to the cost in \u0393-and (2) that they are disjoint-so\nthat the sum of their costs is at most the total cost in \u0413.\nBuilding this matching is the key ingredient of the proof and heavily relies on the structure of\nthe greedy choices. Our proof shows that k balls satisfying the above conditions can be found,\none in each sequence of balls chosen by the algorithm.\nThis concludes the proof of the accuracy of our algorithm. This proof is similar to the original\none by [MP03]; however, the proof requires some crucial and non-trivial adjustments to work\nwith Value instead of ValueMP, and we believe these make the proof more understandable.\nMore than a mere simplification of the algorithm, this change allows for very fast computation of\nthe values of balls: using locality-sensitive hashing (LSH) and sketching techniques, we show how"}, {"title": "2 Preliminaries", "content": "The Euclidean (k, z)-clustering problem is defined as follows: the input is a multiset PC Rd,\nan integer k, and a z > 1. The goal is to find a set of k points S that minimizes COST(P, S) :=\n$\\Sigma_{p\\in P} dist(p, S)^{z}$, where dist(p, S) := mins\u2208s dist(p, s) and dist is the Euclidean distance. We\nsay that a set of k points Ck is an a-approximation to (k, z)-clustering when COST(P, Ck) \u2264\namins,|S|=k COST(P, S).\nA list of n points C1, ..., Cn is an a-approximation to the incremental (k, z)-clustering problem on\ninput P when for any k = 1,..., n, the prefix C1, ..., Ck is an a-approximation to (k, z)-clustering\non P.\nIn the following, we fix a c \u2265 5, which will govern the trade-off between run-time and approxi-\nmation ratio.\nWe let A be an upper bound on the diameter of the input P (i.e., the largest pairwise distance).\nWe assume for simplicity that A is a power of 2c, and that the smallest pairwise distance is at\nleast 1.\nFor (k, z)-clustering, we can assume \u2206 = poly(n): [DSS24] showed how to transform any input P\nto reduce the diameter. Their algorithm runs in time O (nd log log \u2206), which is the running-time\nof their algorithm to compute a poly(n)-approximation: this has been improved to \u00d5(nd) by\n[CHH+23], hence we can reduce to \u2206 = poly(n) in time \u00d5(nd).\nThe first tool we use to speed-up the algorithm is Locallity-sensitive hashing [AI06]. The precise\nresult we use is the following:\nLemma 2.1 (See section D in [CLN+20]). Let P \u2286 Rd, and l = (n/8)1/c\u00b2; there is a family\nof hash functions from Rd to some universe U such that, with probability 1 \u2013 8, if f1, ..., fe are\ndrawn from this family:\n\u2022 For any p,q \u2208 P with dist(p,q) \u2265 c. R, then for all i = 1, ..., l fi(p) \u2260 fi(q)\n\u2022 For any p, q \u2208 P with dist(p,q) \u2264 R, then there exists i \u2208 {1, ..., l} with fi(p) = fi(q).\nFurthermore, the hash functions satisfy the following:\n\u2022 for any i, p \u2208 Rd, computing fi(p) takes time O (dn\u00ba(1)),\n\u2022 after preprocessing time O (ld \u00b7 n1+0(1)), one can compute for any i,p the set Ti[u] := {p :\nfi(p) = u} in time O(|Ti[u]|).\nWe use the previous lemma in two ways: first, it allows us to compute an approximate neigh-\nborhood of each point quickly, and second, combined with streaming techniques, to estimate the\nsize of this neighborhood efficiently. We start with the former (where we replaced, for simplicity\nof notation, the success probability 1 \u2013 8 with 1 \u2013 1/n\u00b2):\nCorollary 2.2. For any R\u2208 R+ and P \u2286 Rd, there is a datastructure with preprocessing time\nO (dn1+3/c\u00b2+0(1)) that can, with probability 1 \u2013 1/n\u00b2:\n\u2022 remove a point from P in time O $\\left(n^{3/c^{2}}\\right)$"}, {"title": "3 The Algorithm", "content": "Here, we describe our algorithm, for which we give a pseudocode in Algorithm 1.\nInput: The algorithm is given a set of points PC Rd and a target number of clusters k.\nOutput: A set Ck = {C1,..., ck} of centers.\nPreprocessing: The algorithm will consider all the balls of the form B(p, R), where p\u2208 P is\nan input point and R is a power of 2c such that 1/(2c)7 < R \u2264 \u0394. All these balls are marked as\navailable when the algorithm starts. Using Lemma 2.3, the algorithm computes Value(B(p, R))\nfor each ball B(p, R). The algorithm then preprocesses the data structure of Corollary 2.2 in\norder to have access to all N(p, 10c R) and N(p, 100c4R) for all radii R power of 2c such that\n1/(2c)7 < R \u2264 \u0394.\nIterative Greedy Choices: The algorithm selects the centers one by one. After one center\nis chosen, the balls that are close to this center (relative to their radius) are removed from the\nset of available balls. More formally:\nj-th Iteration Selecting a Center: Each iteration starts by selecting the available ball\nB(x), R) that maximizes Value. If no balls are available, the algorithm stops and outputs\nthe current set of centers. Otherwise, the algorithm inductively constructs a sequence of balls\n(B(x, R)), beginning with B (x), R). While R is not the minimum radius 1/(2c)7, the\nalgorithm computes the set N (x,10c. R) (using the data structure from corollary 2.2) and\nselects the next ball of the sequence B (x+1, R+1) that maximizes Value among the balls of the\nform B (p, R/2) with p \u2208 N (x, 10c R). At the end, the center of the last ball, cj, is selected\nas the j-th center for the clustering solution.\nj-th Iteration Removing Balls: After selecting cj, the algorithm computes, for all R\npowers of 2c such that 1/(2c)7 < R \u2264 \u2206, the set N (cj, 100c4 \u00b7 R) (using again the data structure\nfrom corollary 2.2). It removes from the set of available balls all balls of the form B(p, R)\nwith p\u2208 N(cj, 100c4R) and removes p from the data structure of Corollary 2.2 computing\nN(cj, 100c4. R)."}, {"title": "3.1 Basic properties of the algorithm", "content": "We start with a simple property of the function Value.\nLemma 3.1. For any point p \u2208 P the function l \u2192 Value(B(p, \u2206/(2c)l)) is decreasing."}, {"title": "4 Running time analysis", "content": "The running time of the algorithm is mostly determined by the running time of the two inner\nloops, namely the while loop line 8 of Algorithm 1, and the for loop line 14.\nWe first show in Fact 4.1 that any p that appears in a set N(x, 10c\u00b7 R) for some iteration j and\nl of the algorithm, is close to the center cj. This will be used to show that B(p, R/2c), and all\nits ancestors by Lemma 3.3, is removed from the available balls line 17 after cj is selected."}, {"title": "5 Proof of Correctness", "content": "The goal of this section is to prove that the outcome of the algorithm is a good approximation\nto (k, z)-clustering, which combined with Lemma 4.2, concludes the proof of Theorem 1.1. This\nis formally stated in the following theorem:\nTheorem 5.1. For any k, the set Ck output by the algorithm gives a O(poly(c))-approximation\nof the optimal (k, z)-clustering solution."}, {"title": "5.1 Bounding the sum of values", "content": "To do so, we start by showing a simple lemma that provides a lower bound for the cost of the\nballs that do not intersect \u0413. We say that a ball B(x, R) is covered by \u0393 if B(x, 2c \u2022 R) \u2229 \u0393 \u2260 0.\nLemma 5.6. If a ball B(x, R) is not covered by F, then COST(B(x, c \u2022 R) \u2229 P, \u0413) \u2265 c\u00b2/3 \u00b7\nValue(B(x, R))."}, {"title": "5.2 Putting things together: proof of Theorem 5.1", "content": "We conclude the proof of our main theorem as follows:\nProof of Theorem 5.1. Given the matching & of Lemma 5.9, we can conclude as follows. Sum-\nming the inequality of the third property of & gives"}]}