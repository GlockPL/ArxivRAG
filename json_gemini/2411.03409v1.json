{"title": "STEER: Flexible Robotic Manipulation via Dense Language Grounding", "authors": ["Laura Smith", "Alex Irpan", "Montserrat Gonzalez Arenas", "Sean Kirmani", "Dmitry Kalashnikov", "Dhruv Shah", "Ted Xiao"], "abstract": "The complexity of the real world demands robotic systems that can intelligently adapt to unseen situations. We present STEER, a robot learning framework that bridges high-level, commonsense reasoning with precise, flexible low-level control. Our approach translates complex situational awareness into actionable low-level behavior through training language-grounded policies with dense annotation. By structuring policy training around fundamental, modular manipulation skills expressed in natural language, STEER exposes an expressive interface for humans or Vision-Language Models (VLMs) to intelligently orchestrate the robot's behavior by reasoning about the task and context. Our experiments demonstrate the skills learned via STEER can be combined to synthesize novel behaviors to adapt to new situations or perform completely new tasks without additional data collection or training.", "sections": [{"title": "I. INTRODUCTION", "content": "Consider the breadth of situations a human encounters on a daily basis, from pouring a cup of coffee in their kitchen to grabbing objects from a cluttered supply closet. Designing robot systems that can navigate these varied, nuanced scenarios is a major challenge, requiring systems that can adapt to complex and dynamic situations. This has led many roboticists to explore learning-based solutions that may generalize better than hand-engineered ones. Imitation learning (IL) is a widely-used, data-driven approach that distills expert demonstrations into learned policies, enabling fine-grained manipulation of high-dimensional robot systems in the real world [1], [2], and has been shown to scale with more data in language-conditioned, multi-task [3]\u2013[6] and even multi-robot [7]-[9] regimes. While these works have shown remarkable promise, the resulting robot systems remain fairly limited to situations seen during training. And the span of these training scenarios is significantly narrower compared to those of other domains, such as vision and language, where large-scale supervised learning has excelled as real-world embodied data collection is significantly more expensive and bottlenecked by physical constraints.\nHumans, on the other hand, can adapt to very complex situations without any previous first-hand experience. We exhibit 'common sense' generalization\u2014using our inherent understanding of complex, high-level concepts like object affordances, intuitive physics, and compositionality to adapt our past experiences intelligently as new situations arise. This deliberate, analytical thinking has been termed 'System 2' processing, in contrast to reactive 'System 1', reflexive low-level behaviors that are less cognitively demanding but equally essential for our behavior [10]. Emulating this blend of reasoning and reflex in robotic systems is challenging, and various approaches have been developed to bridge the gap. One notable example is SayCan [11], which leverages a large language model (LLM) to plan over and sequence learned policies to perform long-horizon tasks. SayCan compensates for the LLM's lack of direct physical grounding by using the policies' value functions to assess feasibility. Moreover, this approach is limited to sequencing the original tasks demonstrated, while many realistic tasks require finer, more nuanced control of low-level policies (as illustrated in Figure 1, right side). Subsequent works have focused on enabling LLMs or VLMs to interface at a finer granularity with pre-programmed System 1 behaviors through representations like code [12] or semantic keypoints [13]. Another strategy that has emerged is to fine-tune VLMs on embodied data [5], [6],"}, {"title": "II. RELATED WORK", "content": "Imitation Learning. Imitation learning (IL) has emerged as the most popular paradigm for training real-world manipulation policies [2], [3], [14]; however, deploying these models in unstructured scenarios remains a challenge. Robot policies trained on human-collected demonstration data can have trouble adapting to \u201cout-of-distribution\u201d scenarios where demonstrations are sparse [15]. This is fundamentally due to the expensive-to-collect small scale robot data, in comparison to the web-scale text and image datasets for training today's foundation models [16]-[18]. Due to practical constraints on obtaining more robot data, a large body of work has explored using text and vision foundation models to improve generalization in robotics from existing datasets. This includes expanding IL policies to open-world object grasping by using open-vocabulary object detection [19] and relabeling episode-level instructions via CLIP [20] or other foundation models [21], [22]. Our work is closely related to aforementioned works in dataset relabeling [20]-[22]: our framework expands robot capabilities by relabeling different behavior modes in existing heterogenous robot demo datasets to train more effective policies.\nPolicy Conditioning for Generalization. Prior works have sought to enable test-time generalization by exploring expressive modalities for policy conditioning, such as goal target poses [23], goal images [24], [25], trajectories [26], [27], code [12], or combinations of vision and language [28], [29]. However, while these modalities all show promise in action generalization, it is challenging for off-the-shelf high-level reasoning systems like VLMs or humans to plan over these creative modalities; natural language remains the main modality utilized in complex planning by state-of-the-art LLMs and VLMs. Thus, STEER focuses on improving language-conditioned action prediction, by studying granular instructions such as \"grasp from the side,\" or \"lift up,\" that can be easily composed at test-time.\nSkill Learning. There is extensive research in utilizing learned 'skills' to accelerate learning new tasks by exploring with temporally-extended, semantically meaningful action sequences [30]-[40]. These works often use a hierarchical approach, where a high-level policy is learned through in-teraction with reinforcement learning through environment interaction to compose the learned skills [35], [38]-[43]. EXTRACT [40] in particular uses VLMs to label skills from offline data to enable learning new tasks. Our work also exploits the intuition about skills being transferable to synthesize new behaviors. However, we use common-sense reasoning in off-the-shelf VLMs to choose appropriate skills for the situation without training a separate policy.\nAffordances. Our work leverages VLMs' common-sense reasoning capabilities to plan for longer-horizon tasks and provide strategies for the robot to approach novel configurations of objects. The model does this by reasoning about how humans would approach tasks from visual image input. Prior work investigates how to represent human priors for how to act in scenes using affordances [44], [45] in image space, and even shown how to deploy these on real robot systems for guiding exploration [46], [47]. While effective, these affordances are often represented in the form of keypoint coordinates in pixel space, and make particular assumptions on the kinds of tasks to be performed. Our approach can be thought of reasoning about such affordances in language, which makes it amenable for off-the-shelf VLMs or humans to naturally interact with and opens the door to express more sophisticated descriptions than in visual space."}, {"title": "III. SYSTEM DESIGN", "content": "We present STEER, a robot learning framework that aims to expand the capabilities of a robot trained on a set of expert demonstrations by extracting flexible skills from existing datasets, then relying on a module with strong reasoning capabilities to orchestrate the skills intelligently. Our system consists of two main components: low-level 'System 1' skill-training and high-level 'System 2' high-level reasoning. In this section, we describe design decisions for acquiring and then integrating them into a practical end-to-end system.\nA. Learning Flexible, Composable Manipulation Skills\nOur goal is to extract skills that can be easily reasoned about and composed by either humans of foundation models. Thus, we aim for skills that are language-indexed and object-centric, allowing a foundation model with knowledge about how the state of an object should evolve to accomplish a certain task to be able to steer despite not having direct motor control capabilities. To achieve this, we densely annotate existing datasets, then train a language-conditioned RT-1 [4] policy with these segmented and relabeled instructions.\nOur key idea is to break down basic composable skills into semantically identifiable categories that can be associated with a language description. As intuition, many have observed that human-collected behavior data is challenging to learn from in part due to different data collectors having different 'styles' or strategies [48]. For example, human driving styles\u2014aggressive, cautious, smooth, or jerky\u2014are highly variable. Prior works have dealt with this heterogene-ity by using latent variables to explain modes [49], new algo-rithms [50], or more expressive generative models [2], [51]. We expose these different styles as adjustable parameters, allowing robots to flexibly adapt their behavior. We focus on shared, object-relational skills such as grasping, lifting, placing, and rotating. These skills, originally demonstrated using templates like pick <object>, move <object1> near <object2>, knock <object>, place <object> upright, can be executed with varying strategies. We identify the following key factors:\nGrasp Angle. Objects can be grasped in multiple stable positions, and the particular way indeed impacts the ability to perform downstream tasks. However, grasp positions are rarely prescribed (and therefore labeled), as they are often implicit. We hypothesize that controlling grasp angle can improve task composition and adaptability. We use a simple approach to label the grasp approach by manually labeling a relatively small set of 'anchor' grasp poses. We then label an arbitrary grasp with the label of its nearest neighbor 'anchor' pose as measured by cosine similarity. We represent a grasp pose as a 3D unit vector by rotating [0,1,0] by the wrist quaternion and we identify the time of a grasp where the gripper changed from fully open to fully closed. To define and label the anchor poses, we took 3D unit vectors that are linear combinations of the elementary 3D basis vectors (resulting in 27 directions). We then clustered 1000 grasps from our dataset and visualized the clusters in order to label them. This only requires visualizing and labeling roughly\nB. Orchestrating Learned Skills\nA key capability of the System 2 component is its ability to reason about the visual observation of the scene, the task description, and the robot's low-level skills to effectively select and sequence appropriate actions. While a human can perform this reasoning, we also demonstrate how it can be automated using a VLM. Our automated System 2 component is implemented as a code-writing VLM agent, enabling it to autonomously execute verbalized plans without additional modules or a human in the loop. To facilitate this, we define an API for the action primitives accessible by the VLM to interface with the System 1, reactive low-level RT-1 policy skills as described in Subsection III-A. The"}, {"title": "IV. EXPERIMENTS", "content": "To understand the efficacy afforded by multiple strategies extracted and learned from the offline data, we test whether our model enables more effective grasping in unseen scenar-ios. We test this by manipulating objects that do not appear in the offline dataset and require specific grasp strategies to succeed. We then test STEER's ability to perform new behaviors-requiring complex reasoning and reliable motor control. This is first demonstrated by having a human expert create a plan for STEER, to show our design decisions lead to composable primitives. We then leverage a VLM to automate the planning. We aim to answer the following concrete questions through our experiments:\n1) Does learning multiple modes of behaviors used to solve a task improve the adaptability of a robotic manipulation system to novel situations?\n2) Can combining extracted skills from heterogeneous human demos enable entirely new tasks?\n3) To what degree can a state-of-the-art VLM plan orches-trate these skills autonomously?\nA. Experimental Setup\nWe use a mobile manipulator with a 7 DoF arm, a two-fingered gripper, and a mobile base, as used in RT-1 [4]. We target a tabletop manipulation environment, where objects on a counter need to be moved or arranged according to natural language instruction. We use the multi-task (6 semantic categories) demonstration dataset used in RT-1 [4] (70K demonstrations) and the dataset of grasping-only data featured in MOO [19] (15K demonstrations). As we discuss in Section III, the exact architectures of the low-level (System 1) and high-level (System 2) components can vary, as long as the high-level component can reason over skills expressed in language. We choose RT-1 [4] for our System 1 component and Gemini 1.5 Pro [53] in VLM experiments. Videos and setup details are available on the project website: https://lauramsmith.github.io/steer.\nB. Improving Test-Time Adaptability\nSetup. One of our central hypotheses is that explicitly extracting, labeling, and training expressive primitives from heterogeneous demonstrations affords our system with im-proved robustness. When faced with unfamiliar situations at test time, we expect some skills to generalize better or be more suitable than others. To test this, we present the robot with three unseen object-grasping scenarios: a kettle with a handle extending above it, a potted plant, and grasping a fruit out of clutter. We choose unseen test objects specifically to simulate challenging real-world settings in which na\u00efve grasping without a strategy is unlikely to succeed.\nWe first compare STEER against the baseline RT-1 [4] model, with the same architecture, trained on the same demo data as STEER, but with the original language instructions. We condition STEER on the templated language it is trained on, with the grasp strategy chosen by a human in these experiments. We condition RT-1 on the templated language it is trained on, i.e. \u201cpick <object>\". For STEER and RT-1, we train with a 50/50 split of the RT-1 and MOO datasets since MOO comprises diverse grasping-only data.\""}, {"title": "C. Performing Novel Behaviors", "content": "Setup. Having demonstrated that STEER learns more flex-ible skill primitives, we study how well we can engineer behavior for a new everyday task without collecting new demonstrations or additional fine-tuning. To test this, we consider an everyday task, pouring, that is out of the distri-bution of demonstrated tasks but should be achievable with the motions that exist in the data. Pouring requires grasping the cup from the side, such that the robot can easily tilt the cup once lifted\u2014avoiding singularities or spilling onto the robot-then setting the cup back down onto the table upright.\nWe perform an extensive evaluation on the pouring task, comparing against the best-case version of each baseline and comparison. Each method exposes a different control interface-for example, RT-1 is commanded with the natural language instructions it has been trained on, while a BC policy conditioned on goal images is commanded by giving images of the sub-goals required to reach the desired end state. For each method, if human assistance is used, the hu-man coaxes the model to perform the task by providing what they deem to be the best command that method supports, in a closed-loop fashion. The human is not allowed to move the objects or robot on their own, and a trial is halted if the robot reaches an irrecoverable state (e.g. objects fall off the table). For this task, we train RT-1 and STEER with a mixing ratio proportional to the respective dataset sizes (roughly 85% RT-1 dataset 15% MOO dataset) as we are not testing generalization to new objects, rather maneuvering seen objects in new ways. We compare to the following 4 baselines and prior methods:\n1) RT-1 [4], which acts as our baseline set of action primitives given by the original tasks in the datasets.\n2) Language motions from RT-H [55], defined by narrating end-effector movement to give language like move arm left and rotate arm right.\n3) A goal-image conditioned variant of RT-1. This tests whether language is a better abstraction than goal im-ages. For this comparison, we first perform a demon-stration of the new task, then run a goal-image condi-tioned policy passing images from that demonstration as subgoals at the granularity of our extracted skills.\n4) OpenVLA [8], to compare to a state-of-the-art model pre-trained on web data and fine-tuned on robot data.\nResults. Human orchestration with a STEER policy achieves"}, {"title": "D. VLM Orchestration", "content": "Now, we test whether a VLM can effectively select or sequence appropriate skills afforded by STEER by reason-ing about the context, in the visual observation and task description, as well as the skills exposed through the API without any examples (i.e. 0-shot). For these experiments, we compare to human orchestration of the same STEER policy as an upper bound on the performance. For each trial, we query the VLM with the initial scene, task description, and maintain the same system prompt. Exact inputs and outputs for all experiments are on the project website.\nSeen task, new scenarios. We see that the VLM successfully produces the same high-level plans as the human expert very reliably for the grasping tasks. However, as shown in Figure 6c we see that there is a degradation in end-to-end task performance compared to human orchestration when executing the code produced by the VLM, and we analyze these failures. For the kettle picking task, we note that the low-level policy appears to be sensitive to the specific naming of objects. That is, the VLM often produced code to grasp the 'black and white kettle' from the top instead of grasping the 'black and white object' from the top, and with further analysis find that this instruction has a noticeable degradation across all low-level language-conditioned policies. So, while the VLM reasonably commands the policy to grasp from above, the low-level policy is less reliable. We expect this to be improved with denser annotation or augmentation on the entity-level, whereas STEER is concerned with the motion-level. For the Fruit in Clutter grasping task, the VLM did"}, {"title": "E. Additional Experiments", "content": "Self-improvement with in-context learning. For the pour-ing task, the VLM orchestrated policy succeeded in 6/10 trials. We tested whether feeding the VLM-generated pro-grams that resulted in on-robot success as examples in the prompt would lead to a higher success rate. Indeed, with the self-generated in-context examples, the newly generated programs succeeded in 8/10 trials for the same pouring task, improving upon the 6/10 in 0-shot. This only requires human labels for task success and no model fine-tuning.\nCup unstacking and flipping. We also test our ability to use STEER to perform a longer-horizon task of unstacking and reorienting a cup upright, so as to be able to dispense a drink. This task also requires grasping with intention for the future object reorientation steps. We demonstrate that we can guide STEER to perform this new task (see Figure 7).\nV. DISCUSSION\nWe presented STEER, a robotic system that can follow natural language instructions for manipulation. Rather than collecting data for new tasks, STEER uses a novel methodol-ogy for relabeling existing data with flexible and composable manipulation primitives. This relabeled data is used to train a small language-conditioned policy, which can be controlled using either a high-level VLM agent or a human. STEER can be steered to perform specific manipulation tasks, which in conjunction with the commonsense-reasoning afforded by a VLM agent can perform intelligent multi-step manipulation without ever collecting new data. We report that our simple recipe can outperform large, end-to-end models like Open-VLA (despite using a 100\u00d7 smaller model and data). Since the performance of STEER is driven by our re-annotation of behavior modes, this suggests existing robot datasets could also be re-annotated to produce more steerable action primitives, with little changes to the model architecture and training pipelines. In the future, it would be useful to scale up the discovery and labeling of dataset attributes, investigate automatic relabeling, and more directly optimize annotations to maximize the high-level agent's skill composability."}]}