{"title": "CODEV-BENCH: How Do LLMS UNDERSTAND DEVELOPER-CENTRIC CODE COMPLETION?", "authors": ["Zhenyu Pan", "Rongyu Cao", "Yongchang Cao", "Yingwei Ma", "Binhua Li", "Fei Huang", "Han Liu", "Yongbin Li"], "abstract": "Code completion, a key downstream task in code generation, is one of the most frequent and impactful methods for enhancing developer productivity in software development. As intelligent completion tools evolve, we need a robust evaluation benchmark that enable meaningful comparisons between products and guide future advancements. However, existing benchmarks focus more on coarse-grained tasks without industrial analysis resemble general code generation rather than the real-world scenarios developers encounter. Moreover, these benchmark often rely on costly and time-consuming human annotation, and the standalone test cases fails to leverage minimal tests for maximum repository-level understanding and code coverage. To address these limitations, we first analyze business data from an industrial code completion tool and redefine the evaluation criteria to better align with the developer's intent and desired completion behavior throughout the coding process. Based on these insights, we introduce Codev-Agent, an agent-based system that au-tomates repository crawling, constructs execution environments, extracts dynamic calling chain from existing unit tests, and generates new test samples to avoid data leakage, ensuring fair and effective comparisons. Using Codev-Agent, we present the Code-Development Benchmark (Codev-Bench), a fine-grained, real-world, repository-level, and developer-centric evaluation framework. Codev-Bench as-sesses whether a code completion tool can capture a developer's immediate intent and suggest appropriate code across diverse contexts, providing a more realistic benchmark for code completion in modern software development.", "sections": [{"title": "1 INTRODUCTION", "content": "With the rapid development of large language models (LLMs), LLM4X gains popularity cross various domains, including healthcare [8], question answering [23, 24], and education [13]. In software engineering, code LLMs such as Codeqwen [28], Codegeex [37], and Starcoder [20] also demonstrate strong capability in code generation [22, 21]. Several code assistants including TONGYI Lingma [4] and Copilot [17], begin integrating these code LLMs into industrial products. Among these tools, code completion remains one of the most frequent and effective functionalities for boosting developer productivity in daily software development [5]. With the emergence of various tools, there is an urgent need for a comprehensive benchmark to compare their performance [31].\nInitially, general code generation benchmarks like HumanEval [4] and ClassEval [6] are used to evaluate completion abilities in form-closed, self-constrained Python functions and classes. However, these benchmarks are too isolated to reflect real-world development practices. In response, DevEval [15] and CrossCodeEval [9] introduce repository-level benchmarks that evaluate the code completion about cross-file contextual understanding and retrieval capabilities, making them more aligned with actual development environments. The latest work, RepoMasterEval [30], further conducts an study to analyze the correlation between user acceptance and benchmark performance, demonstrating that a well-designed benchmark can guide the product improvement in practical settings. However, three key challenges remain: (1) the coarse-grained tasks without industrial analysis resemble general code generation rather than the real-world scenarios developers face; (2) limited extensibility, as human annotation of data samples and test cases is time-consuming and costly, making continuous updates cumbersome and inflexible; and (3) the standalone generation of test samples fails to leverage minimal tests for maximum repository-level understanding and code coverage.\nTo address these limitations, we first analyze feedback from an industrial code completion tool and redefine the evaluation criteria to better align with the developer's intent and desired completion behavior throughout the coding process, tackling challenge (1), as discussed in Section 3.1. Based on these insights, we introduce Codev-Agent to address challenge (2) and (3), as detailed in Section 3.2. Codev-Agent is an agent-based system that automates repository crawling, constructs execution environments, extracts dynamic call chains from existing unit tests, and generates test samples to avoid data leakage, ensuring fair and effective comparisons. Furthermore, it supports user customization and optimization, allowing users to tailor the benchmark to specific needs and scenarios, making it more adaptable and comprehensive than previous work. With Codev-Agent, we present the developer-centric Code-Development Benchmark (Codev-Bench), a fine-grained, real-world, and repository-level evaluation framework, as outlined in Section 3.3. It assesses whether a code completion tool can capture a developer's immediate intent and suggest appropriate code cross diverse contexts, offering a more realistic and actionable evaluation for code completion in modern software development.\nAfter evaluating state-of-the-art general and code-specific LLMs on Codev-Bench, we gain significant insights into their performance and applicability in developer-centric code completion scenarios. Common issues observed include incorrect code indentation, predictions that erroneously span multi-ple code blocks, incomplete code generation, failure to stop appropriately at the right point, redundant code that repeats the surrounding context, incorrect API calls or parameter values, and misidentifi-cation of the correct block type to generate. These problems severely impact the user experience when using code completion tools in practical settings, yet existing benchmarks fail to capture this diverse range of errors. This highlights our core innovation an automated evaluation framework that assesses code completion from a developer's perspective, ensuring a more comprehensive and realistic evaluation of code completion tools in real-world development environments.\nIn summary, this paper makes the following contributions to the community:\n\u2022 We conduct a comprehensive business analysis from an industrial code completion product to redefine evaluation criteria that better align with developers' intent and desired completion behavior throughout the coding process.\n\u2022 We introduce Codev-Agent, a unified system that automates the entire process: crawling repositories, setting up execution environments, analyzing call chains from existing unit tests, extracting scenario samples, and evaluating LLMs. Codev-Agent is also adaptable, supporting user customization and optimization to tailor the benchmark to specific needs.\n\u2022 We propose Codev-Bench, a developer-centric, fine-grained, real-world, and repository-level evaluation framework. Codev-Bench assesses whether a code completion tool can capture a developer's immediate intent and suggest appropriate code snippets across diverse contexts, offering a more realistic evaluation for code completion in modern software development.\n\u2022 We present evaluation results on several state-of-the-art LLMs and provide insights that can guide future research and improvements in the development of LLMs for code completion."}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "We begin by introducing existing LLMs designed for code generation. Next, we outline the bench-marks for general code generation task and the more specific code completion task. It is important to note that code completion is a key downstream task in code generation, as it is one of the most frequent and impactful methods for enhancing developer productivity in software development. Unlike coarse-grained tasks such as class or function generation, code completion often requires finer granularity. As a result, its evaluation demands more fine-grained and developer-centric scenarios, distinct from those used in general code generation."}, {"title": "2.1 LLM FOR CODE GENERATION", "content": "The evolution of Code LLMs starts from basic code generation to highly specialized models designed for complex programming tasks. Codeqwen-1.5b [28] marks a step forward with its extensive pretraining on programming languages, refining its precision in code generation. Following this, Deepseek-coder-v2-lite [38] introduces a lightweight, efficient model suited for rapid completion and debugging in constrained environments. The development of Codegeex-4-9b [37] highlights a shift toward tackling real-world scenarios with its Fill-In-Middle strategy, excelling in mid-segment code generation. Starcoder-2-7b [20] further advances multi-language support, increasing its utility in popular languages like Python and JavaScript. Finally, Codegemma-7b [27] exemplifies the trend toward enterprise-scale code generation, focusing on managing and refining large codebases, meeting the growing needs of software maintenance and development. Together, these models showcase the continuous innovation in Code LLMs, expanding their capabilities and real-world applications."}, {"title": "2.2 BENCHMARK FOR CODE GENERATION", "content": "Early works introduce benchmarks [35, 10] to evaluate code generation on form-closed and self-constrained Python functions, such as HumanEval [4] and MBPP [3]. ClassEval [6] proposes a class-level code generation dataset containing 100 human-crafted, self-contained Python classes. CoderEval [34] extends the evaluation to non-standalone programs. DevEval [15] and CrossCodeEval [9] align these evaluations with real-world code repositories, addressing more complex code generation scenarios. Meanwhile, XCodeEval [12] and HumanEval-X [37] expand the scope to multilingual programming beyond just Python. In these works, the correctness of generated code snippets, measured by test cases (e.g., Pass@k [34]), and the semantic similarity between generated code and the ground truth (e.g., CodeBLEU [25]) are common evaluation metrics. Test cases derive either from human annotations or from general-purpose LLMs (GPT-4 [1]). The former (human annotation) is time-consuming and costly, while the latter (GPT-4-generated test cases) tends to be unstable and often fails to capture key correlations across a repository."}, {"title": "2.3 BENCHMARK FOR CODE COMPLETION", "content": "With the emergence of various code assistant tools, such as Copilot [4], Visual Studio IntelliCode [7], and TONGYI Lingma [17], people start to incorporate code LLMs in real-world development. Code completion is one of the most frequent and useful functionalities. CrossCodeEval [5], RepoBench [19], and RepoEval [36] propose benchmarks to evaluate the repository-level code completion across different dimensions, such as cross-file contextual understanding, retrieval capability, and various levels of generation granularity. The most recent work, RepoMasterEval [30], also conducts an industrial study to evaluate the benchmark in a practical setting. However, these work face three common challenges: (1) human annotation of data samples and test cases is time-consuming and costly, (2) coarse-grained tasks such as class and function completion resemble general code generation more than the real-world scenarios developer faced, and (3) unsatisfying and standalone generated test cases fail to reveal code LLMs' ability to understand context at the repository level. To address these gaps, we first analyze the feedback data from industrial code completion tool and redefine the evaluation criteria to better capture a developer's intent and desired completion behavior throughout the coding process. Finally, we deliver the Codev-Agent and Codev-Bench."}, {"title": "3 METHODOLOGY", "content": "As mentioned earlier, there are three common challenges: (1) coarse-grained tasks resemble general code generation more than real-world scenarios faced by developers, (2) human annotation of data samples and test cases is time-consuming and costly, and (3) standalone generated test cases fail to demonstrate LLMs' ability to understand context at the repository level. To address challenge (1), we request business data from a real-world deployed code completion product through our partner company, which helps us understand the actual needs of developers when using code assistants."}, {"title": "3.1 PRODUCT BUSINESS DATA ANALYSIS", "content": "The diverse scenarios in which users trigger code completions involve many variables that affect code assistants' performance. Collaborating with a partner company, we access business data from a real-world code completion product. Analyzing this data helps us understand developers' actual needs and typical usage contexts, guiding the design of a test dataset that mirrors real development environments for more realistic evaluation. Based on the product data, we identify key patterns in how users need code completions:\nCode Block Categories Pending Completion (Figure 1.A): The majority of completions, about 68.51%, are triggered for general code statements, reflecting a developer's need for broad, context-aware suggestions. Other significant categories include single-line comments (9.06%) and more specific structures like functions (5.71%) and con-trol logic (6.53%). These distributions highlight the diversity of developer needs and the impor-tance of covering a wide range of code structures in the test dataset.\nNumber of Completion Lines (Figure 1.B): The data shows that in nearly half of the cases (49.31%), the entire line is completed by the assis-tant, with a significant portion (32.35%) complet-ing part of a line. This suggests that developers often rely on code assistants for both line comple-tion and generating larger blocks of code, support-ing the importance of testing across varying code lengths and completion contexts.\nLength of User Prompts (Figure 1.C): We ob-serve that 41% of user prompts contain more than 10 tokens, indicating that developers often provide detailed input to guide code completions. How-ever, there are also cases where prompts contain fewer tokens or even none (31.51%), suggesting that code assistants need to be flexible in handling both detailed and minimal input. This informs our decision to test completions in a variety of prompt conditions, from highly specific to more ambiguous inputs.\nBy analyzing these key statistics, we ensure that our test dataset aligns closely with real-world us-age, covering a broad range of code completion scenarios, varying completion lengths, and differ-ing levels of user input. This data-driven approach enables us to construct a benchmark that accurately reflects the diversity and complexity of developer workflows, resulting in a more comprehensive and practical evaluation of code completion tools."}, {"title": "3.2 CODEV-AGENT", "content": "LLM-based Codev-Agent aims to automatically update our benchmark to avoid data leakage, ensuring fair and effective comparisons with minimal cost. It can minimize the human effort but keep the stability in repositories selection, execution environment setup, test samples extraction based on dynamic data flow of existing unit test files. Figure 2 visualizes the pipeline."}, {"title": "Automated Repository Crawling", "content": "As shown in Figure 2a, a LLM-based crawler discovers and crawls up-to-date repositories, gathering the most relevant and current data for analysis and test case generation. The crawler operates based on the following criteria: (1) only repositories created within the last four months are considered to ensure data timeliness; (2) only repositories with a high star count (more than 50 stars) are selected to guarantee community engagement and project popularity; (3) after scanning the file directory, only repositories containing unit test files are retained, ensuring the presence of relevant testing infrastructure; (4) by leveraging the Qwen to understand and analyze the README file, the crawler filters for repositories that require only lightweight configuration environments for execution, thus optimizing for efficiency and reducing complexity."}, {"title": "Execution Environment Setup", "content": "After gathering the qualifying repositories and their associated unit tests from the crawling phase, Codev-Agent sets up the corresponding execution environments through an iterative process. As illustrated in Figure 2b, the first step involves detecting and extracting the unit tests from each repository (Step 0). Next, the agent utilizes Qwen to analyze the README.md file and generate the necessary setup commands for the current environment (Step 1). After the initial setup, the agent executes the extracted unit tests to verify whether they run without errors or warnings (Step 3). Based on the logs from the test executions and command outputs, Qwen determines whether to repeat the process (Steps 1\u20134), regenerating setup commands if needed, or to finalize the environment setup with executable unit tests."}, {"title": "Dynamic and Static Call Chain Analysis", "content": "After completing the environment setup and extracting runnable units, Codev-Agent first parses the code files and unit tests in each repository, transforming them into static Abstract Syntax Trees (AST) that include various node types such as Class, Method, If, While, For, Try, Catch, Expression, Statement, and Import, as shown at the bottom of Figure 2c. Simultaneously, Codev-Agent traces the dynamic data flow during the execution of the unit tests, capturing the program's overall behavior and generating the corresponding call chains, depicted at the top of Figure 2c. By fusing the dynamic call chains with the static AST, we can easily extract specific scenarios from the unit test execution process. This module is also adaptable for repositories with few or no existing unit tests, making it an efficient tool for enhancing our benchmark. We can simply design minimal unit tests, and Codev-Agent will automatically and efficiently extract the desired scenarios, saving time spent combing code details through the entire repository."}, {"title": "Test Sample Generation", "content": "As shown in Figure 2d, once Codev-Agent obtain the dynamic call chains from the unit tests and the static AST structures of the repositories, we can ex-tract the corresponding test samples for each real-world scenario summarized in Section 3.1."}, {"title": "3.3 CODEV-BENCH", "content": "With Codev-Agent's support, we deliver a fine-grained, real-world, repository-level, and developer-centric evaluation framework, Codev-Bench. It assesses whether a code completion tool can capture a developer's immediate intent and suggest appropriate code snippets across diverse contexts."}, {"title": "3.3.1 FEATURES OF CODEV-BENCH", "content": "Extensibility: Codev-Bench stands out as the most extensible benchmark compared to others. Other baselines rely on manually or LLM-generated datasets where the data samples need to be extracted one by one, and corresponding unit tests must be generated for validation. This process is time-consuming and labor-intensive. In contrast, Codev-Bench uses a dynamic-static analysis module of Codev-Agent as shown in Figure 2.c, which starts from existing unit tests, extracting samples directly from the data flow via simple string-matching operations. This method minimizes effort in validating feasibility. Even for repositories with few or no unit tests, we only need to write the test cases, and the system can generate numerous test data samples from the data flow, making it scalable.\nAuto Annotation: Codev-Bench does not rely on LLMs or human intervention for annotation. It extracts annotations directly from the dynamic data flow of the existing unit tests by parsing code. This process ensures that all necessary annotations are derived without any manual effort, unlike other baselines that require manual or LLM-assisted annotations.\nDeveloper-Centric Benchmark with Industrial Analysis: Most prior benchmarks are constructed based on the researchers' understanding of what is needed, without any developer-centric focus. None of the baselines analyze real-world code completion products or utilize real user feedback to identify actual scenarios that developers face. Codev-Bench is the first benchmark to analyze top-tier, industry-level, developer-centric data from an already deployed code assistant tool. As discussed in Section 3.1, this ensures that Codev-Bench aligns closely with real-world usage. Although RepoMasterEval does touch on some industry analysis, it only explores the relationship between tool's suggestion acceptance and benchmark performance without diving deep into user needs.\nGranularity: Prior datasets have a coarse, one-size-fits-all granularity, focusing on completing a function body based on its signature, which is far from industrial code completion. Codev-Bench, on the other hand, includes a wide variety of granular, real-world code completion tasks. These range from completing logic blocks like if, for, and while, to completing individual statements, filling in comment sections, completing argument lists for function calls, and more. The diversity and flexibility of these tasks make Codev-Bench much more reflective of actual code completion needs.\nAgent Integration: Previous benchmarks are based on manually curated datasets, which limits them to small-scale repositories and only provides the final dataset. Codev-Bench, however, offers a fully integrated framework with Codev-Agent, which automates the entire process. This includes crawling repositories, setting up execution environments, analyzing call chains, extracting scenario samples, and evaluating LLMs\u2014all in one unified architecture. It also supports user customization and optimization, allowing them to tailor the benchmarks to specific needs and scenarios, making it far more adaptable and comprehensive than any previous efforts.\nIn summary, Codev-Bench surpasses existing benchmarks in every key area, offering high extensibil-ity, real-world relevance, comprehensive automation, and fine-grained scenario-based evaluations."}, {"title": "3.3.2 BENCHMARK STATISTICS, AND FUTURE EXTENSIONS", "content": "Statistic Current statistics of Codev-Bench are in Table 2. For this submis-sion, we select 10 repositories due to storage limitations. These repositories contain a total of 862 code files and 191 existing test cases. After extrac-tion, our test samples cover 55 code files, all 191 test files, and 296 code blocks. In the future, we plan to pro-cess additional repositories and make them publicly available on both Hugging Face and GitHub.\nFuture Extensions Codev-Bench currently deliver test samples on Python languages. As our Codev-Agent is adaptable for every language, Codev-Bench can be extended to more languages. In addition, our Codev-Agent allow users to only design new unit tests for every repository for augmenting the benchmark with minimal human effort, further enhancing the extensibility."}, {"title": "4 EXPERIMENTS", "content": "4.1 EXPERIMENTAL SETUP\nScenario 1 - Full block completion: In this scenario, the model is tasked with completing a full code block (e.g., function, if, for, try, or statement) based on a complete, unbroken surrounding context. To pass, the model must accurately complete the block and stop at the correct point, ensuring it passes the unit test successfully. Scenario 2 - Inner block completion: In this scenario, the model is required to complete a portion of code block based on a complete, unbroken surrounding context. In addition, 20% of the samples in this scenario have an empty ground truth, evaluating"}, {"title": "4.2 BASE MODELS", "content": "General LLMs. General LLMs are evaluated in the code understanding. They exhibit diverse strengths in tasks involving complex reasoning and context handling. Claude-3.5-Sonnet [2]: Anthropic designs it for tasks requiring nuanced reasoning, multilingual support, and high-level coding proficiency. It excels in complex multistep workflows, achieving top benchmarks in reasoning (90.4% MMLU) and coding (92% HumanEval). Deepseek-v2 [16]: A 236B parameter Mixture-of-Experts model, with only 21B activated per token, reducing training costs by 42.5% and boosting generation throughput by 5.76x. Mistral-123b [11]: A 123B parameter model with a 128k context window, supporting multi-languages and 80+ coding languages. It excel in code and reasoning tasks. Yi-1.5-34b [33]: An enhanced version of Yi, pre-trained on 500B tokens and fine-tuned on 3M diverse samples, improving in coding, math, reasoning, and instruction-following. Qwen-2-54b-moe & Qwen-2-72b [32]: They incorporate mixture-of-experts (MoE), allowing dynamic routing to improve performance while maintaining efficiency. Llama-3.1-70b & Llama-3.1-405b [29]: They are iterated with expanding parameter count, resulting in enhanced reasoning and multitask capabilities in long-context scenarios. GPT4 & 4o [1]: The most powerful LLM from OpenAI.\nCode LLMs. We also evaluate specialized code LLMs designed to handle programming tasks. They implement strategies like Fill-In-Middle to enhance coding accuracy and generation. Codeqwen-1.5b [28]: Qwen series, tuned specifically for code completion tasks. It leverages extensive pretraining on programming languages to excel in code generation. Deepseek-coder-v2-lite [38]: A lightweight version of Deepseek's code models, optimized for rapid completion and debugging in constrained environments. Codegeex-4-9b [37]: With its focus on Fill-In-Middle tasks, Codegeex is tailored for real-world code completion scenarios, excelling in solving mid-segment coding problems. Starcoder-2-7b [20]: This model builds upon the original Starcoder, aiming to improve multi-language support and efficiency in common code languages like Python and JavaScript. Codegemma-7b [27]: A relatively new model, Codegemma focuses on generating and refining large codebases, making it ideal for enterprise-level code maintenance and development."}, {"title": "4.3 EVALUATION METRICS", "content": "Test Case Pass Rate: We generate code snippets by prompting LLMs and inserting them back into the original code, then rerun the corresponding unit tests. If the test passes without errors, the generated completion is considered successful. We use Pass@1 as the metric, which represents the probability that the LLM generates a code snippet that passes on the first attempt. This metric provides a clear evaluation of how effectively the LLM generates valid, executable code completions.\nEdit Similarity (ES) Levenshtein distance quantifies the number of single-character ed-its-insertions, substitutions, or deletions\u2014required to transform one sequence of tokens into another [26]. In practice, developers accept approximate code completions and make manual edits. Therefore, ES is a crucial metric for evaluating how closely a generated completion matches the intended result. The Levenshtein distance between two strings is calculated using dynamic programming."}, {"title": "4.4 EXPERIMENTAL RESULS", "content": "Full Block Completion in Full Context We evaluate the performance of general and code-specific LLMs on completing full code blocks based on complete context (Scenario 1) with results in Table 3 and Table 4. General LLMs struggle with function and try blocks, with a Pass@1 of 0.00% for function completion across all general LLMs except GPT-4 and GPT-40. However, some models perform well on simpler constructs like statements, where Llama-3.1-405b achieves 78.00%. Those code LLMs in fill-in-the-middle mode show much stronger performance. Codegemma leads with a 53.85% on average, significantly outperforming general LLMs across all block types.\nInsight 1: Simpler blocks like statements are easier to complete, while complex blocks (e.g., functions, condition logic blocks, loop logic blocks) remain challenging. Code LLMs outperform general models, but there is still room for improvement, especially in generating accurate and complete function bodies and logical blocks.\nWe found that many models struggle to stop generating content at the appropriate position (shown in Appendix E.1). Consequently, even though the models might generate high-quality code, their overall accuracy remains very low-particularly with models like Starcoder-2-7b and Yi-1.5-34b. This is further corroborated by the average lines of generated codes shown in Figure 5. This issue becomes especially pronounced during real user interactions with code completion models. Users not only face longer waiting times for the model's predictions but also need to remove a considerable amount of extraneous code generated by the model, leading to a significant decline in user experience.\nSensitivity to Internal Block Completion We evaluate general and code LLMs to complete internal parts of code blocks based on a complete context (Scenario 2) with results in Table 3 and Table 5. Among general LLMs, performance is inconsistent, with lower scores for try and statement completions. Claude-3.5 performs best with an average Pass@1 of 48.87%, while models like GPT-40-mini and Mistral underperform, particularly on try and statement blocks. In contrast, code LLMs excel in this scenario, with Codegemma achieving the highest average Pass@1 score of 77.36%, and Deepspeek-coder-v2-lite performing particularly well, scoring 100.00% on function completions.\nInsight 2: Code LLMs significantly outperform general LLMs in internal block completion tasks, particularly on complex code structures. While general LLMs struggle with try and statement blocks, code LLMs show greater sensitivity to internal block context across all block types.\nMany models struggle to effectively recognize whether the current scenario requires code completion within a block (shown in Appendix E.2) or if the existing code snippet is already complete and does not need any additions (shown in Appendix E.3). This subset is designed to assess a model's ability to effectively complete code within a block and to recognize when no completion is necessary. Models that perform poorly in this scenario may cause significant disruption for users in real-world applications of code completion.\nCompletion with Incomplete Suffix We evaluate completing code blocks when the suffix content following the cursor is incomplete or missing (Scenario 3) in Table 3 and Table 6. General LLMs perform poorly in this task, especially for more complex block types. For example, most general models achieved a 0.00% Pass@1 on function completions, while the highest performing general model, Llama-3.1-405b, only managed 6.77% on average. On the other hand, code LLMs show\nslightly better performance. Codegemma achieves the highest average Pass@1 of 6.86% across all block types, with a stronger performance on statements and try blocks, scoring 11.94% and 9.77%, respectively. However, the overall performance of both general and code LLMs remains limited in this scenario, indicating the difficulty of completing code accurately when future context is missing.\nInsight 3: Both general and code LLMs fail in incomplete suffix completions, particularly for complex structures such as functions and try blocks. Code LLMs show a slight advantage, but overall performance highlights the challenge of this completion scenario with missing or incomplete suffixes.\nThis subset aligns more closely with real user interactions with code completion tools. Our statistics indicate that in over 20% of cases, the cursor position within the function the user is currently writing has no content following it, and sometimes even the entire file has nothing after the cursor position. Therefore, the model's performance in this scenario is crucial for users utilizing code completion tools. We found that both general LLMs and code LLMs do not perform well in this scenario. This is primarily because the contextual information provided by the surrounding code may be insufficient for completing the current content. This also highlights that the code completion task continues to present significant challenges.\nRAG-Based Completion We evaluate the RAG-based completion (Scenario 4) in Table 3 and Table 7. Among general LLMs, the performance is mixed, with Llama-3.1-405b achieving the highest average Pass@1 score of 40.38%, and notable performance on functions (33.33%) and try blocks (19.57%). However, most other general LLMs struggle with this task, especially on function completions, achieving a Pass@1 score of 0.00%. Code LLMs, as expected, demonstrate better overall performance. Codegemma leads the group with an average Pass@1 score of 55.77%, showing strong results on statements (70.00%) and try blocks (44.44%). Deepspeek-coder-v2-lite also performs well, with an average score of 52.17%, excelling in the retrieval of for and try blocks.\nInsight 4: General LLMs still struggle with RAG-based tasks, particularly for more complex block types, where code LLMs show a clear advantage. Codegemma and Deepspeek can leverage RAG more effectively, achieving significantly better performance across most block types.\nIn real project-level code development, referencing code snippets from other files is quite common. The improvements observed in this scenario test set compared to the results from Scenario 1 reflect the model's ability to recognize and utilize similar code snippets from other files for code completion.\nFinally, we examine the correlation between the two metrics: test pass rate (Pass@1) and edit similarity (ES). By calculating the results of each model across four scenarios for these two metrics, we found that, apart from Scenario 2, which has a correlation of 0.991, the correlation in the other three scenarios is below 0.85, specifically 0.793 from Scenario 1, 0.529 from Scenario 3, and 0.822 from Scenario 4. The detailed results are presented in the Appendix C. This indicates that, in most cases, using edit similarity as a statistical measure does not objectively reflect the quality of the model's generation in the code completion task. We also show some"}, {"title": "5 SUMMARY AND FUTURE WORK", "content": "In this work, we redefined evaluation criteria for code completion tools through a comprehensive busi-ness analysis and introduced Codev-Agent, an automated system for repository crawling, environment setup, call chain analysis, and LLM evaluation. We also proposed Codev-Bench, a developer-centric, real-world framework that assesses whether code completion tools capture developers' intent across diverse contexts. Our evaluation of several state-of-the-art LLMs offered insights for future improve-ments in code completion models. Looking forward, we aim to extend Codev-Agent to support multiple programming languages, generating a multilingual dataset, while also refining Codev-Bench to handle more complex developer scenarios."}, {"title": "A MORE EVALUATION RESULTS OF CODEV-BENCH", "content": "In contrast to Table 3, we present a more detailed evaluation of the results in Table 4, Table 5, Table 6 and Table 7. For each scenario, we assess the test pass rates corresponding to various types of code blocks, including functions, logical condition blocks, loop blocks, exception handling blocks, and ordinary statements. Furthermore, we compare the performance of general LLMs with code LLMs. The general LLMs utilize a prompt format based on natural language, while the code LLMs employ a fill-in-middle prompt format."}, {"title": "B AVERAGE LINES OF GENERATED CODE IN FOUR SCENARIOS", "content": "We found that many models struggle to stop generating content at the appropriate position (shown in Appendix E.1). Consequently, even though the models might generate high-quality code, their overall accuracy remains very low-particularly with models like Starcoder-2-7b and Yi-1.5-34b. This is further corroborated by the average lines of generated codes shown in Figure 5. This issue becomes especially pronounced during real user interactions with code completion models. Users not only face longer waiting times for the model's predictions but also need to remove a considerable amount of extraneous code generated by the model, leading to a significant decline in user experience."}, {"title": "C THE CORRELATION BETWEEN PASS@1 AND ES", "content": "In this paper, we analyze the prediction results of various models across four scenarios, evaluating them based on test pass rates and edit distance similarity. Our goal in this section is to assess whether there is consistency between the predictions of the test pass rates and edit distance similarity. An introduction to the test pass rates and edit distance similarity is provided in Section 4.3.\nIn the four scenarios, we calculate the average Pass@1 and average Edit Similarity (ES) values predicted by each general LLMs and code LLMs. To visualize these results, we plot each model's outcomes in Figure 6 using a scatter plot, with the X-axis representing the average Pass@1 and the Y-axis representing the average ES value. Additionally, we calculate the Pearson correlation between the average Pass@1 and average Edit Similarity (ES) values predicted by each model in the four\nscenarios as follows,\n$r=\\frac{\\sum_{i=1}^{n}(P_{i}-\\overline{P})(E_{i}-\\overline{E})}{\\sqrt{\\sum_{i=1}^{n}(P_{i}-\\overline{P})^{2} \\sum_{i=1}^{n}(E_{i}-\\overline{E})^{2}}}$ (1)\nwhere, $P_{i}$ refers to the Pass@1 value of i-th model and $E_{i}$ refers to the ES value of i-th model, $\\overline{P}$ and $\\overline{E}$ refer to the average value of each model's Pass@1 and ES value, n refers to the number of models to be tested."}, {"title": "Finally", "content": "the correlations in all the four scenarios are 0.793, 0.991, 0.529"}]}