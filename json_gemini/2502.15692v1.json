{"title": "ACL-rlg: A Dataset for Reading List Generation", "authors": ["Julien Aubert-B\u00e9duchaud", "Florian Boudin", "B\u00e9atrice Daille", "Richard Dufour"], "abstract": "Familiarizing oneself with a new scientific field\nand its existing literature can be daunting due to\nthe large amount of available articles. Curated\nlists of academic references, or reading lists,\ncompiled by experts, offer a structured way to\ngain a comprehensive overview of a domain\nor a specific scientific challenge. In this work,\nwe introduce ACL-rlg, the largest open expert-\nannotated reading list dataset. We also provide\nmultiple baselines for evaluating reading list\ngeneration and formally define it as a retrieval\ntask. Our qualitative study highlights the fact\nthat traditional scholarly search engines and\nindexing methods perform poorly on this task,\nand GPT-40, despite showing better results, ex-\nhibits signs of potential data contamination.", "sections": [{"title": "1 Introduction", "content": "As the volume of scientific publications contin-\nues to grow, gaining insights into a field becomes\nincreasingly time-consuming. Although existing\ntools for browsing the literature (academic search\nengines, paper recommendation systems, etc.) help\nresearchers avoid missing relevant papers, they of-\nten return an overwhelming number of results. This\nabundance of information makes the familiariza-\ntion process daunting and inefficient, particularly\nfor junior researchers who lack effective paper-\nskimming skills or experienced scholars transition-\ning to a new field.\nOne solution is to consult survey papers, which\noffer comprehensive reviews of the current state\nof research in a particular area. However, survey\npapers have several limitations: they are not avail-\nable for all fields, may become outdated quickly,\nand may not address the specific needs of novice\nresearchers as surveys are too broad and without\nexplicit instructions of reading order. Another way\nto familiarize oneself with a new field is through\nreading lists, which are curated lists of academic\nreferences compiled by experts that provide an\norganized overview of a field (Siddall and Rose,\n2014). Compared to surveys, creating reading lists\nrequires significantly less effort, yet they can still\nhelp novice researchers navigate key literature and\nreduce the time needed to begin their research (Jar-\ndine, 2014). As a result, interest in automated meth-\nods for generating reading lists has increased (Ek-\nstrand et al., 2010; Jardine, 2014; Gordon et al.,\n2017; Figueira et al., 2019). However, progress\nhas been limited due to the scarcity and quality\nof available datasets. Indeed, existing datasets are\neither too small, as exemplified by the eight lists\ncompiled by Jardine (2014), or are of low quality\nbecause they have been automatically constructed\nfrom survey papers (Ekstrand et al., 2010; Ding\net al., 2022; Figueira et al., 2019).\nTo address this gap, we introduce ACL-rlg, a\nnew dataset of expert-curated reading lists derived\nfrom tutorials accepted at major Natural Language\nProcessing (NLP)-related conferences, along with\nmanually annotated queries that novice researchers\nmight use to locate relevant papers from the lists.\nWe conducted experiments with various meth-\nods for generating reading lists, including academic\nsearch engines, commercial LLMs, and ad-hoc re-\ntrieval models, to validate our dataset and establish\na benchmark for future research. Our contributions\nare:\n\u2022 We introduce ACL-rlg, the largest expert-\ncurated reading list dataset available with 85\nreading lists;\n\u2022 We formally define the reading list generation\ntask as a retrieval task and set up an evaluation\nframework with metrics and baselines;\n\u2022 We compare existing models and show that\nthere are signs of potential data contamination\nusing systems such as GPT-40.\nOur code and data are openly available\u00b9."}, {"title": "2 Reading List Generation", "content": "In the literature, reading list generation is broadly\ndefined as the task of creating a list of references\nthat serves as a starting point for familiarizing one-\nself with a new field (Ekstrand et al., 2010; Jardine,\n2014; Gordon et al., 2017; Sesagiri Raamkumar\net al., 2017; Figueira et al., 2019). The lack of a\nstandardized definition for the task makes it chal-\nlenging to compare the results between these stud-\nies, as the proposed models operate with varying\ninputs and outputs. Here, we address this issue\nby defining reading list generation as an article\nretrieval task:\nGiven a collection of scientific articles \\(C =\n\\{\u03b1_1, \u03b1_2, ..., \u03b1_\u03bd\\}\\) and a query q formulated by\na novice researcher (e.g., using keywords or a\nnatural language expression), the task of reading\nlist generation is to retrieve a concise, ordered list\nof papers L that helps the user efficiently grasp\nthe topic of q. The list should be compact for\nquick consumption, with an order that ideally\nsupports the user's learning curve.\nReading lists should offer an overview of a re-\nsearch field while remaining concise, balancing\nthe relevance of references with general coverage\nof the existing literature (Thompson et al., 2004).\nAlthough the maximum size of a reading list is\ncommonly set at 20 articles (Jardine, 2014; Sesa-\ngiri Raamkumar et al., 2017; Figueira et al., 2019;\nDing et al., 2022), there is no consensus on the\nminimum number of references. Based on the re-\nquirements outlined by Sesagiri Raamkumar et al.\n(2017), we assume that a reading list should include\nat least three references.\nOne notable aspect that distinguishes the genera-\ntion of reading lists from other ad hoc retrieval tasks\nis the meaningful ordering of the papers within the\nlists. In a reading list, articles are arranged in a\nsequence that reflects the optimal learning path,\nguiding the reader through a knowledge progres-\nsion to better understand the new field (Ding et al.,\n2022). Therefore, generating a reading list not only\ninvolves retrieving relevant papers but also deter-\nmining the best order in which to present them.\nBenchmark datasets for reading list generation\nare scarce. Most existing datasets are constructed\nusing references from survey papers as proxies for\nreading lists. For instance, Ekstrand et al. (2010)\nused survey papers from the ACM Computing Sur-\nveys, Figueira et al. (2019) relied on reviews and\nsurvey papers from Scopus, and Ding et al. (2022)\ncollected survey papers by querying S2ORC (Lo\net al., 2020) and Google Scholar. It is worth noting\nthe lack of uniformity in the queries associated with\nthese reading lists. Ekstrand et al. (2010) used a\nset of initial papers as a query, while Figueira et al.\n(2019) and Ding et al. (2022) relied on keywords,\neither provided by the survey authors or extracted\nfrom their titles, respectively. These automatically\nconstructed datasets can be extended to a large\nscale, with Ding et al. (2022) aggregating more\nthan 9,000 reading lists. However, their quality is\nlimited as the purpose of surveys is to be compre-\nhensive, which contrasts with the conciseness and\nfocus required in reading lists.\nPerhaps the most relevant work for us is by Jar-\ndine (2014), who introduced a dataset of reading\nlists created by experts using the ACL Anthology\nNetwork (Radev et al., 2013). NLP-related PhDs\nwith several years of research experience were\ntasked with creating reading lists from research\ntopics (e.g. statistical parsing). While the resulting\ndataset is high in quality, its small size (only 8 lists)\nand its age significantly render it less useful for con-\ntemporary research, showing the need for newer,\nmore robust datasets for reading list generation."}, {"title": "3 Dataset Description", "content": "We gathered reading lists from tutorials presented\nat events sponsored by the Association for Compu-\ntational Linguistics (ACL). Tutorials are structured\neducational sessions presented by experts and de-\nsigned to provide in-depth guidance in a specific\nfield. Since 2021, tutorial presenters have been ex-\nplicitly instructed to include a reading list in their\ndescription papers\u00b2. These instructions suggest\nthat the lists should be concise, recommending 4-\n10 articles, and include a range of authorship to\nensure diverse perspectives. We reviewed all tu-\ntorial descriptions and identified those containing\nsections explicitly titled \u201cReading List\" or \"Pre-\nrequisite Readings\". References were manually\nextracted from these sections, and their metadata\nwas enriched using the Semantic Scholar API. We\nmaintained the original ordering of the references\nand preserved any structural organization provided,\nsuch as sections or subsections. A total of 27 read-\ning lists included structured sections, and among\nthose, 3 included subsections"}, {"title": "3.2 Query Annotation", "content": "Annotation protocol. In the context of automatic\nreading list generation, an initial query is required\nto retrieve a set of relevant references. To achieve\nthis, we need to formulate a query that accurately\nreflects the field of the tutorial from which the read-\ning list was derived. A simple approach might in-\nvolve extracting keywords directly from the tutorial\ntitles. However, titles may omit critical concepts\nsince they are crafted primarily to capture attention.\nInstead, we opted for a more controlled and quali-\ntative approach, asking annotators to create queries\nthey would use to find a reading list relevant to\nthe field of each tutorial, with novice researchers\nas target users (See annotation guidelines in Ap-\npendix A)."}, {"title": "Inter-annotator agreement.", "content": "Table 3 shows the\ninter-annotator agreement, measured by the exact\nword match between queries using stemming and\nlowercase. A key observation is the significant\ndifference in query length, particularly between\nA2 and the other two annotators, which partly ex-\nplains the mismatch. The discrepancies may also\narise from the fact that some terms annotated in"}, {"title": "4 Experiments", "content": "As researchers often rely on\nacademic search engines to find relevant articles,\nwe investigate whether the top results from these\nengines align with expert-curated reading lists.\nSpecifically, we compare the outputs of two widely\nused search engines: Semantic Scholar (S2) and\nGoogle Scholar (GS). Given the growing popular-\nity of LLMs as an alternative, we also measure\nGPT-40 (OpenAI, 2024) and Gemini 1.5 (Gemini\nTeam, 2024) performances against these search\nengines for the same task with different output\nstrategies (Basic, JSON-mode (JM), Structured\nOutputs (SO)). The temperature of LLMs was set\nto 0 in our experiments. We evaluate the generated\nreading lists using commonly-used IR metrics:\nRecall@k, NDCG@k (Normalized Discounted\nCumulative Gain), and MRR@k (Mean Reciprocal\nRank), with k set to 20. Recall measures the\nmodel's ability to predict reading lists that closely\nmatch those curated by experts. In contrast, NDCG\nand MRR assess the quality of the ranking within\nthe predicted lists, focusing on how well the\nrelevant items are ordered. Search engines are\nqueried using manually annotated keywords, while\nLLMs are prompted with instruction-based queries,\nfollowing the procedures detailed in Appendix B.\nAnalysis of results. As shown in Table 4 (full\ndetails in Table 8), GPT-40 outperforms other sys-\ntems in generating reading lists. Notably, GPT-40\ndemonstrates significant improvements in prioritiz-\ning articles from expert-curated lists, as reflected by\nits MRR@20 scores. However, the overall scores\nremain low, highlighting the need for systems ded-"}, {"title": "4.2 Comparison of Retrieval Models on ACL Anthology Collection", "content": "Additional experiments. We conducted additional\nexperiments by indexing the collection of papers\nfrom the ACL Anthology. We compared three\nretrieval systems: Semantic Scholar (Any article\nfrom the S2 collection for comparison purpose,\nACL articles only, and most cited ACL articles) as\na baseline, as well as sparse (BM25 (Robertson and\nZaragoza, 2009)) and dense (SPECTER2 (Singh\net al., 2023)) retrieval models (see Table 5, full\ndetails in Table 9). Overall, we observe lower\nscores compared to those achieved with search\nengines but found performance comparable to\nSemantic Scholar when using the same index."}, {"title": "Article popularity impact.", "content": "Interestingly, the pre-\ndicted results from Semantic Scholar exhibit popu-\nlarity trends that are significantly lower compared\nto the articles featured in the ACL-rlg reading lists\n(Figure 2). Given these differences, we derived\nadditional results from Semantic Scholar using a\n\"Most Cited\" strategy, selecting the 20 most-cited\narticles from the 100 relevant results for a given\nquery. The performance improvement observed\nwith this strategy suggests that prioritizing highly\ncited articles could be an important factor to con-\nsider when creating a reading list for a specific\nfield."}, {"title": "4.3 Analysis of LLMs Predictions", "content": "Given that some of the\ntutorials used to extract our reading lists may be\npart of GPT-40's training data (up to October\n2023), the higher scores achieved by GPT-4o raise\nconcerns about potential data contamination with\nLLMs (Sainz et al., 2023). To gain insights into\nthis issue, we evaluated the performance of search\nengines and LLMs based on the reading lists\nextracted from tutorials for each specific year (see\nFigure 3). While there is no definitive evidence,\nour results show a drop in performance for LLMs\nfrom 2023 that is not observed with the search\nengines, strongly suggesting data contamination."}, {"title": "Hallucinations.", "content": "Another concern with the use of\nLLMs is the potential generation of hallucinated\nreferences. Here, we consider references that are\nnot indexed in Semantic Scholar as hallucinations.\nFigure 4 shows the percentage of hallucinated ref-\nerences in the generated reading lists for each year.\nWe observe that \u2248 1/3 of the references generated\nby LLMs could not be found in Semantic Scholar,\nwith this number increasing after the 2023 cutoff.\nThis result raises doubts about the current effec-\ntiveness of LLMs for the reading list generation\ntask, suggesting that such models rely on memo-\nrization and could perform poorly on unseen topics\ncompared to academic search engines."}, {"title": "5 Conclusion", "content": "We introduced ACL-rlg, the largest expert-\nannotated benchmark dataset for reading list gener-\nation. We conducted experiments with commonly\nused academic search engines and commercial\nLLMs. Empirical results highlight the limited per-\nformance of search engines on the task and raise\nconcerns about data contamination and hallucina-\ntions in LLMs, notably GPT-40. This work lays\nthe groundwork for developing new methods for\nreading list generation. Future directions include\nexploring Retrieval Augmented Generation (RAG)\nto address the identified issues in LLMs, and evalu-\nation metrics that consider the ordering of reading\nlists."}, {"title": "6 Limitations", "content": "Reading lists content and subjectivity. ACL-rlg\ncontains reading lists collected from tutorial\ndescriptions presented at ACL main confer-\nences. While we ensured that the lists met our\nrequirements, their content reflects the individual\ninterpretations of the authors. We were unable\nto manually verify what defines a good reading\nlist for each specific field. The initial assumption\nseemed to be that reading all articles in a curated\nreading list is necessary to fully understand a\nfield. While experts design these lists under the\npremise that the included articles are valuable\nfor understanding their tutorial sessions, it is\npossible that only a subset of the references is truly\nessential. This raises questions about whether all\nreferences presented in reading lists are necessary\nfor conveying domain knowledge.\nDataset limitations. Although ACL-rlg represents\nthe largest collection of expert-curated reading\nlists, its size is insufficient for training supervised\nmodels and is better suited for use as an evaluation\nset. Future updates will try to address this\nlimitation by incorporating new publications from\nthe ACL Anthology or other potential data sources.\nHowever, as tutorial descriptions currently serve as\nthe primary source of reading lists, the dataset's\ngrowth remains dependent on the availability of\nsuch publications until alternative sources are\nidentified.\nQueries discrepancies. Inter-annotator agreement\nhighlights notable discrepancies between A2 and\nthe other annotators. While these variations were\nanticipated due to the open-ended nature of our\nannotation campaign, it would be useful to explore\nwhether they arise from individual annotator biases\nor differences in expertise. Understanding this\ncould help the development of systems tailored to\ndifferent levels of expertise.\nEvaluation of article substitutes. Lack of\nevaluation whether the predicted articles can\nserve as effective substitutes for items in the\nexpert-curated reading lists may hinder models\nfrom accurately generating reading lists that\nalign with the standards and comprehensiveness\nof those created by human experts. Addressing\nthis issue in future research should explore the\ninterchangeability of articles, as multiple sources\nmay cover similar key aspects of a field."}, {"title": "7 Ethical considerations", "content": "We aim to\ntackle the automatic generation of reading lists\nand have conducted experiments using LLMs to\ngenerate them. Given the nature of large language\nmodels, automated systems based on these\ntechnologies could produce lists that misrepresent\nthe impact of an article within a field or even\nprovide incorrect information. To mitigate these\nrisks, additional verification processes should be\nimplemented if such systems would be deployed to\na wider audience. Additionally, providing easily\ndigestible reading lists for every field may reduce\nthe curiosity to explore the broader literature\nor engage with articles from other disciplines,\npotentially limiting their exposure to diverse\nperspectives and knowledge.\nCompilation of reading lists. The reading lists\nin ACL-rlg are curated by human experts and\npublished within the ACL Anthology. We did not\nimpose any criteria on the individuals compiling\nthese lists or the specific fields of the referenced\nworks when collecting our dataset. Our only\nselection criterion was the presence of a document\nthat clearly resembled a reading list.\nHuman annotations. The annotation campaign\nfor the request queries was carried out by three\nannotators chosen based on their expertise in NLP.\nTwo of the annotators are authors of this paper,\nwhile the third is a member of the same laboratory.\nThe annotation process required approximately 7\nhours per annotator, with an average of 5 minutes\nspent per sample.\nReproducibility. We provide detailed descriptions\nof our methodology and make code and data pub-\nlicly available under MIT license."}]}