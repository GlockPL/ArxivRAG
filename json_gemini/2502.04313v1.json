{"title": "Great Models Think Alike and this Undermines AI Oversight", "authors": ["Shashwat Goel", "Joschka Str\u00fcber", "Ilze Amanda Auzina", "Karuna K Chandra", "Ponnurangam Kumaraguru", "Douwe Kiela", "Ameya Prabhu", "Matthias Bethge", "Jonas Geiping"], "abstract": "As Language Model (LM) capabilities advance, evaluating and supervising them at scale is getting harder for humans. There is hope that other language models can automate both these tasks, which we refer to as \u201cAI Oversight\". We study how model similarity affects both aspects of AI oversight by proposing Chance Adjusted Probabilistic Agreement (CAPA): a metric for LM similarity based on overlap in model mistakes. Using CAPA, we first show that LLM-as-a-judge scores favor models similar to the judge, generalizing recent self-preference results. Then, we study training on LM annotations, and find complementary knowledge between the weak supervisor and strong student model plays a crucial role in gains from \"weak-to-strong generalization\". As model capabilities increase, it becomes harder to find their mistakes, and we might defer more to AI oversight. However, we observe a concerning trend - model mistakes are becoming more similar with increasing capabilities, pointing to risks from correlated failures. Our work underscores the importance of reporting and correcting for model similarity, especially in the emerging paradigm of AI oversight.", "sections": [{"title": "1. Introduction", "content": "Machine Learning model capabilities have improved immensely over the last few years. Scaling up the amount of data used for training has played a crucial role in these improvements (Kaplan et al., 2020). Initially, most of the gains in Language Model (LM) capabilities came from scaling pretraining data (Llama Team, 2024a). Recently, there is increasing interest in post-training, either with human preferences (Ouyang et al., 2022), or task-specific expert annotations (Lightman et al., 2023). Collecting human preferences or annotations is slow and expensive. Therefore, with increasing model capabilities, an attractive alternative is to use LMs to annotate training data (Gilardi et al., 2023) and score model outputs (Zheng et al., 2023), to boost both training (Stiennon et al., 2020) and evaluation (Li et al., 2024b). In this paper, we refer to both these techniques together as Al oversight\u00b9.\nCan we rely on AI oversight going forward? This remains a topic of much debate. In this work, we study oversight from the perspective of model similarity. When assessing or teaching humans, it is well recognized that individuals have different strengths and weaknesses. Similarly, two models with 50% accuracy may misclassify completely different samples and thus be highly dissimilar (having different 'strengths'). To measure model similarity, we build on error consistency (Geirhos et al., 2020), which measures overlap in the samples where two models err beyond what is expected by chance due to the two models' accuracies."}, {"title": "2. Methodology: Measuring LM Similarity", "content": "We begin by describing how we quantify model similarity.\nBackground\nFunctional similarity: Prior work on model similarity has focused on two classes of similarity measures: representational and functional similarity (see Klabunde et al. (2024) for a recent survey). Representation similarity metrics focus on the weights and activations of the networks (Kornblith et al., 2019), comparing how features are represented internally. In contrast, functional similarity metrics focus on the input-output behavior of the model. Functional similarity metrics are more broadly applicable than representation metrics as (1) they allow comparisons across model families and architectures and (2) are applicable for models behind an API (where weights are not released). Functional similarity metrics are more interpretable because they operate on data samples rather than noisy, complex internal representations (Golechha & Dao, 2024). Despite large architectural differences across models and model families, their outputs can still be fairly similar. Moreover, Geirhos et al. (2020) argue models with similar internal mechanisms make similar mistakes, and thus mistakes can proxy whether models use similar internal mechanisms. Therefore, in the present work we focus on functional similarity metrics.\nError Consistency: A popular similarity metric designed in the context of comparing mistakes of image-classifiers to humans is error consistency (Geirhos et al., 2020). It quantifies the overlap on samples where two models make mistakes while normalizing for chance overlap due to accuracy. First, they define \\(C_{obs}\\) as the \u201cobserved error overlap\" i.e., the fraction of samples on which both models are correct or both models are wrong. This itself is used a metric in recent work on LM similarity, Dutta et al. (2024). However, as Geirhos et al. (2020) point out, \\(C_{obs}\\) has a crucial shortcoming: two independent models with high accuracy will have a higher \\(C_{obs}\\) by chance than two models with low accuracy (1). An independent model here is one that is correct on a uniform random subset (size corresponding to accuracy) of samples, and wrong on the others. For instance, two independent models with 90% accuracy will agree on at least 81% of the samples by chance, whereas for two models with 50% accuracy, the lower-bound on chance agreement drops to 25%. Consequently, to account for this, Geirhos et al. (2020) calculate the \u201cexpected error overlap\" (\\(C_{exp}\\)) as \\(C_{exp} = acc_1 \\cdot acc_2 +(1 - acc_1)(1 - acc_2)\\) where acc is the accuracy of model i. Similar to Cohen's K (Cohen, 1960), error consistency (Eq. 1) is then defined as the fraction of excess agreement observed (\\(C_{obs} - C_{exp}\\)) from what is possible beyond chance (\\(1 \u2013 C_{exp}\\)): \n\\[k = \\frac{C_{obs} - C_{exp}}{1 - C_{exp}}\\]"}, {"title": "2.2. Our Contribution", "content": "We identify two key limitations of error consistency (\\(k\\)):\nDoes not distinguish differing mistakes (2): If two models make wrong but different predictions, error consistency still counts that as an agreement. For example, two models that are always wrong, even in different ways, have perfect error consistency (\\(k = 1\\)). It thus overestimates similarity.\nDoes not capture probability information (3): For comparison to humans, error consistency assumes a single top prediction, whereas models inherently output a probability distribution. Ignoring these probabilities can lead to incorrect conclusions about model similarity. Consider two models whose outputs are [0.49, 0.51] and [0.51, 0.49]. Despite their small differences, binary labels would classify them as making entirely different predictions (0 and 1). Conversely, models with predictions [0.99,0.01] and [0.51,0.49] may share the same binary output (0 and 0) but differ significantly in confidence distribution.\nNovel Metric. We redefine \\(C_{obs}\\) and \\(C_{exp}\\) to address the above limitations. For clarity we adjust the notation of our agreement metrics to \\(c_{obs}\\) and \\(c_{exp}\\). To compute \\(c_{obs}\\) we directly use the model output probabilities (Eq.2), thus accounting for disagreement on incorrect options and better capturing model similarity. This approach lets us calculate \\(C_{obs}\\) without sample-wise ground-truth annotations. For \\(c_{exp}\\), we take into account that the model output predictions can span over multiple options rather than only looking at sample-wise accuracy.\nDefinition. We define \\(k_p\\) in the context of Multiple Choice Questions (MCQs), which is the format of many popular benchmarks for LMs. We provide a detailed derivation in Appendix A.1, with extensions to classification and exact match evaluations in Appendix A.3.\nObserved Agreement (\\(c_{obs}\\)): It represents the probability of agreement if the model's predictions were sampled based on the observed likelihoods assigned over options. Formally,\n\\[c_{obs} = \\frac{1}{|D|} \\sum_{x \\in D} \\sum_{o_i \\in O(x)} p_1(o_i) p_2(o_i),\\]\nwhere \\(p_1(o_i)\\) and \\(p_2(o_i)\\) are the output probabilities for model 1 and 2, respectively, on a data sample x for option \\(o_i\\). \\(O(x)\\) are the possible options: \\(O(x) = [o_i, ..., o_n]\\), and \\(|D|\\) is the total number of data points.\nChance Agreement (\\(c_{exp}\\)): To account for higher accuracies inflating \\(c_{obs}\\), we normalize by the agreement expected from two independent models. First, we define \\(p_j\\) as the average probability model j assigns to the correct option across all samples. For a perfectly calibrated model \\(p_j\\) approaches accuracy, thus aligning with the motivations in error consistency. Then, we define independent models as assigning \\(P_j\\) probability to the correct option, and uniformly distributing the remaining 1 - \\(P_j\\) probability over the incorrect options. The latter is necessary, as there is no coherent concept of \"classes\" for MCQ data, i.e. the options can be permuted. This prevents us from computing class marginals for the remaining options, such as in inter-annotator agreement metrics like Cohen's Kappa, Scott's Pi (Scott, 1955), Fleiss' Kappa (Fleiss et al., 1981). Formally,\n\\[c_{exp} = \\frac{P_1 \\cdot P_2}{\\text{chance agreement on correct option}} + \\frac{\\frac{1}{\\mid O(x) \\mid -1}}{\\sum_{x \\in D}  \\frac{(1-p_1) \\cdot (1-p_2)}{\\frac{1}{\\mid O(x) \\mid -1}}}{\\text{chance agreement on incorrect option}}\\]\nwhere |O(x)| is the number of options in question x.\nFinally, the equation for CAPA is:\n\\[\\kappa_p = \\frac{c_{obs} - c_{exp}}{1 - c_{exp}}\\]\nInterpretation. We prove \\(\\kappa_p\\) is bounded between -1 and 1 in Appendix A.6. A value of 0 means the models have"}, {"title": "3. Affinity Bias in LLM-as-a-Judge", "content": "Evaluating free-response model generations automatically at scale is tricky (Biderman et al., 2024). This has led to popular leaderboards like Arena-hard-auto (Li et al., 2024b), AlpacaEval 2.0 (Dubois et al., 2024), and Aidan-Bench (McLaughlin et al., 2024) adopting LLM-as-a-Judge for scoring. Recent work cautions that language models are biased towards their own outputs (Liu et al., 2024; Panickssery et al., 2024), and these leaderboards assume that excluding the judge model from the rankings circumvents this problem. However, it has been shown that human interviewers are biased towards candidates with similar knowledge and traits, a phenomenon called affinity bias (Bagues & Perez-Villadoniga, 2012). We study whether LMs also exhibit a bias toward similar models. This would indicate that it is not sufficient to just use a held-out LM as the judge; one must account for the confounder of similarity.\nExperimental Setup\nTo study whether LM judges prefer more similar models, we evaluate a large set of judges and models on MMLU-Pro (Wang et al., 2024), a benchmark for hard problem solving questions across 14 domains. We filter 8,707 questions that can also be answered in a free-text response style,"}, {"title": "3.2. Results & Discussion", "content": "Q1: Do LM Judges favor more similar models? As a motivating example, Qwen2.5-72B-Instruct as a judge scores Qwen2.5-7B-Instruct as being 71% correct, while the more capable (41% vs 51% MCQ accuracy) Meta-Llama-3.1-70B-Instruct is deemed less accurate at 67%. In Figure 2 we show that model favoritism extends beyond self- or family- preference to all models that are functionally similar. We find a significant (p < 0.01) positive correlation (average Pearson r=0.84) between LLM-as-a-judge scores and model similarity (\\(\\kappa_p\\)) for all judges.\nQ2: Is this merely due to better accuracy? Note that while \\(\\kappa_p\\) adjusts for inflation in agreement of highly accurate models, we do expect models with lower accuracy to be less similar with highly capable judge models, and thus have lower \\(\\kappa_p\\). To control for the accuracy of the evaluated model we perform multiple regression and partial correlation analysis (see Table 2). The multiple regression analysis shows that both, accuracy and similarity, have a significant positive effect on the judge score. The coefficient of accuracy increases for more capable judge models, consistent with prior work showing improved alignment with human judges (Thakur et al., 2024). We find that especially for small models (<32B) the effect of similarity is greater. The partial correlation results control for accuracy and confirm that there is still a significant effect of similarity on judgment scores even for the best judge models. Altogether, the statistical analysis confirms that judgment scores are confounded by affinity bias."}, {"title": "4. Learning from Complementary Knowledge of LM Annotators", "content": "We now study the role of similarity in AI supervising training. This can allow scaling up training data by reducing reliance on costly and time-intensive expert human inputs. There is hope that models can learn from other models to improve further even on tasks where they surpass human capabilities (Hughes et al., 2024). Where could this improvement come from? We hypothesize that the existence of complementary knowledge or complementary capabilities between two LMs can be one mechanism for driving improvements from LM annotations, if effectively leveraged. This complement can exhibit itself in the form of differing predictions on training data, and can thus be quantified using functional similarity between the supervisor and student model. Lower \\(\\kappa_p\\) is indicative of more complementary knowledge, and as an implication of our hypothesis, should inversely correlate with the performance gain of a student model when trained on the supervisor's annotations."}, {"title": "4.1. Experimental Setup", "content": "Burns et al. (2024) study training a larger student model on annotations from a small task-finetuned \u201cexpert\" teacher. They find the student can outperform the supervisor, a phenomenon they call \"weak to strong generalization\". We study this setup as it can seem counter-intuitive when viewed from the lens of accuracies. How can training a 60% accuracy student on a 70% accuracy task-finetuned teacher lead to 75% accuracy? We adopt a lens of complementary knowledge to understand weak-to-strong generalization.\nWe measure similarity between the weak supervisor and base student model on the validation set. We then perform weak-to-strong training on the student model, using the confidence-weighted finetuning objective proposed in Burns et al. (2024). We investigate if similarity is an apriori predictor of performance gained on the test set. For our experiments, we study 4 weak models in the 1 - 3B parameter range, and 3 strong models in the 7 - 9B parameter range, for a total of 12 model pairs, and 15 of the NLP tasks studied in Burns et al. (2024), specified in Table 17. The full setup is consistent with the open-weight model reproduction by (Scherlis et al., 2024), and is described in Appendix C.1."}, {"title": "4.2. Results & Discussion", "content": "Q1: Does Complementary Knowledge Influence Performance Gain? Figure 3 shows that for all model combinations, similarity between the weak supervisor and initial strong student inversely correlates with the improvement obtained from weak-to-strong training (r = -0.85). Even after using partial correlation analysis to control for the accuracy gap between the weak supervisor and strong student, similarity is inversely correlated with weak-to-strong gain (r = -0.35, p < 0.01). Thus, tasks where the supervisor and student make less correlated errors tend to yield greater improvements. This contributes towards understanding why gains from weak to strong training vary across tasks, an open question posed by Burns et al. (2024).\nQ2: Does Complementary Knowledge Add Beyond Elicitation? The original explanation for performance gains from weak-to-strong generalization is that the weak supervisor \"elicits\" the latent knowledge in the superior representations of the stronger student (Burns et al., 2024). To investigate whether complementary knowledge adds to this explanation or is subsumed within it, we first obtain the strong model with \u201cupper-bound\" elicitation by finetuning it on ground-truth annotations. We refer to this as the strong elicited model. We can then separate the test data into four parts based on whether the strong elicited and weak supervisor model were correct or wrong, measuring average accuracy of the weak-to-strong model on each part to disentangle gains from different factors. The experiment setup is discussed further in Appendix C.2.\nFigure 4 reports aggregate values across 15 tasks for 12 model pairs. Accuracy on the bottom-left quadrant (avg. 71.9%) can only be due to successful elicitation, as here the weak supervisor was wrong. Accuracy on the top-right quadrant (avg. 42.2%) can only be due to complementary knowledge transfer as here the upper-bound elicitation model was wrong. This confirms that elicitation plays an important role in weak-to-strong generalization, with complementary knowledge transfer from the weak supervisor also contributing to significant gains.\nQ3: Where can weak-to-strong training improve? The strong elicited model is considered to represent upper-bound performance, but as shown in Table 3, the actual ceiling is significantly higher if complementary knowledge of the weak supervisor is fully leveraged. Interestingly, on the training set, the weak-to-strong trained model shows similar accuracy on the top-left and bottom-right quadrants as shown in Figure 12. Yet, when generalizing to unseen samples, it falls back more often to its initial priors. We hope this analysis guides future work on improving weak-to-strong training methodology, by highlighting leveraging complementary knowledge as a concrete avenue for improvement."}, {"title": "5. Models are making more similar mistakes as capabilities increase", "content": "The previous two sections highlighted two major advantages of having access to more diverse LMs: a) it leads to less biased judges, b) it can drive more performance gains from training on LM annotations. This points to the importance of diversity, or lower model similarity, for AI oversight. As AI oversight becomes increasingly relevant with advancing capabilities, we now study similarity trends in existing LMs across different levels of capability. It has been shown model representations across modalities are converging with increasing capabilities (Huh et al., 2024). Does this also lead to more similar mistakes?\nExperimental Setup\nWe collect sample-wise evaluation files for 130 official models from the OpenLLM Leaderboard 2 released by Hugging-Face, listed in Appendix D.5. We use MMLU-Pro (Wang et al., 2024) and Big Bench Hard (BBH) (Suzgun et al., 2023) as they measure a broad range of capabilities using MCQ, and frontier models have reasonable accuracies while not saturating these datasets. We first bucket these models into five performance percentile ranges. Then, for each model, we compute its mean similarity (\\(\\kappa_p\\)) with models in the same bucket from different developers, to prevent confounding from distillation or continual training. More setup details are provided in Appendix D.1. In Appendix D.3 we also report pairwise results, and using the extension of \\(\\kappa_p\\) for sets of \\(M > 2\\) models."}, {"title": "5.2. Results & Discussion", "content": "Q1: Are model errors becoming more correlated with improving capabilities? Figure 5 shows a strong positive correlation between model capabilities and \\(\\kappa_p\\), which measures similarity beyond chance agreement due to accuracy. In Appendix D.4 we find this also holds across individual categories in both datasets, not just in aggregate.\nPotential Implications. If this trend continues, it could mean greater affinity bias when using LM judges, and lower potential for gains from inter-LM training in the context of our earlier results. It could undermine benefits from using LM juries by compromising independence and amplifying collective biases. Most concerningly, our results indicate that as model blind-spots get harder to detect, making us defer more to AI oversight, models also make more similar mistakes, posing safety risks from correlated failures.\nQ2: Why are model errors becoming more correlated? This is an interesting research direction in itself. We perform a preliminary analysis in Appendix D.2, summarizing key conclusions here. First, we observe only a slight increase in similarity for harder questions in our datasets, indicating difficulty is not a significant confounder for this trend. We find this trend is stronger in instruction-tuned models, and using alternative architectures like Mamba (Gu & Dao, 2023) may not be enough to increase diversity."}, {"title": "6. Related Work", "content": "There is increasing interest in finding differences between models for applications like visual tools for comparative analytics (Strobelt et al., 2021; Kahng et al., 2024), efficient human evaluation (Boubdir et al., 2023), comparing learning algorithms (Shah et al., 2023), identifying side-effects of API updates (Eyuboglu et al., 2024) or quantization (Dutta et al., 2024). Prior work has also looked at qualitatively describing differences between data distributions (Zhong et al., 2022; 2023; Dunlap et al., 2024b;a). Our work proposes metrics to quantify LM differences (or similarity). Huh et al. (2024) used representation similarity metrics (Kornblith et al., 2019; Bansal et al., 2021) to show convergence in visual representations and their alignment with language representations. In contrast, we show model mistakes are becoming more correlated as capabilities improve. We measure differences in input-output behaviour, which leverages sample level evaluations (Burnell et al., 2023) such as those available on OpenLLMLeaderboard (Myrzakhan et al., 2024) and HELM (Bommasani et al., 2023). Geirhos et al. (2020) proposed measuring \"error consistency\" between image classifiers and humans, with Geirhos et al. (2021) showing an early trend of data-rich models making more similar mistakes to humans. We enrich this metric, distinguishing between different mistakes and incorporating probabilistic information."}, {"title": "7. Conclusion, Limitations, Future Work", "content": "Our paper shows the importance of measuring functional similarity for language models. We derive a novel, probabilistic metric for model similarity, CAPA (\\(\\kappa_p\\)). We then use it to study the implications of similarity for AI oversight - showing affinity bias in AI judges, and the role of complementary knowledge when training on LM annotations, such as in weak-to-strong generalization. Al oversight will become more relevant as capabilities improve, so our finding that increasing capabilities could lead to more correlated errors is particularly concerning. Thus, we believe measuring and accounting for model similarity is going to be increasingly important. We now list some limitations of our work, along with avenues for future work that can help develop a better understanding of model similarity and its implications.\nEstablishing Causation: We established similarity correlates with both aspects of AI oversight \u2013 evaluation and training supervision. To establish causality, we need methods to make a model less similar without harming capabilities, which is itself a challenging open problem.\nExtending to similarity metrics for free-text outputs: Everyday use of generative models is based on their free-text responses. Like much work on benchmarking, we had to limit to MCQ tasks as the science of precisely evaluating free-text is still evolving (Biderman et al., 2024). For example, both model-free (Papineni et al., 2002) and model-based metrics (Pillutla et al., 2021) suffer from a wide range of syntax and style sensitivity issues (Kocmi et al., 2021; He et al., 2023). We hope the community takes up the challenge of designing similarity metrics for free-response text and reasoning. This would allow studying the role of similarity using more promising oversight setups like debate (Kenton et al., 2024) and process supervision (Lightman et al., 2023).\nGenerator-Verifier gap: AI oversight has recently shown promise for tasks where it is easier to validate a solution than generate it (Song et al., 2024). Similarity may continue to play a role here. (1) In evaluations, similarity in stylistic preferences of the generated solution may influence judge scores. (2) In training, the generator-verifier gap may be larger if models are more different.\nSafety implications: Researchers separately develop many post-training interventions to reduce harmfulness, dual-use knowledge, dishonesty etc. In real world model deployments, all these problems have to be tackled at once, which can benefit from composing interventions (Kolbeinsson et al., 2024). If the benefits are decorrelated, composing would lead to greater compound safety. If the side effects are correlated, composing would lead to lower accumulation. More broadly, measuring LM similarity post-intervention can help characterize decorrelation in research bets, for granters (Canton, 2025). For example, LM unlearning was recently found to be functionally similar to refusal training (\u0141ucki et al., 2024), even though it was proposed as a complementary safeguard (Li et al., 2024a). Finally, as we transition towards language agents, similarity can help understand collective \u201cblind spots\" (He et al., 2023), and could lead to better cooperation (Lowe et al., 2017) but also scheming (Balesni et al., 2024) between multiple agents.\nQualitative analysis of model differences: We developed quantitative methods for measuring LM similarity on a given data distribution. One exciting direction is to use these metrics to provide qualitative difference descriptors (Dunlap et al., 2024a) between models, by describing data distributions where models are least similar."}]}