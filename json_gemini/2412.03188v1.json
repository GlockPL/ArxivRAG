{"title": "Semi-decentralized Training of Spatio-Temporal Graph Neural Networks for Traffic Prediction", "authors": ["Ivan Kralj", "Lodovico Giaretta", "Gordan Je\u017ei\u0107", "Ivana Podnar \u017darko", "\u0160ar\u016bnas Girdzijauskas"], "abstract": "In smart mobility, large networks of geographically distributed sensors produce vast amounts of high-frequency spatio-temporal data that must be processed in real time to avoid major disruptions. Traditional centralized approaches are increasingly unsuitable to this task, as they struggle to scale with expanding sensor networks, and reliability issues in central components can easily affect the whole deployment. To address these challenges, we explore and adapt semi-decentralized training techniques for Spatio-Temporal Graph Neural Networks (ST-GNNs) in smart mobility domain. We implement a simulation framework where sensors are grouped by proximity into multiple cloudlets, each handling a subgraph of the traffic graph, fetching node features from other cloudlets to train its own local ST-GNN model, and exchanging model updates with other cloudlets to ensure consistency, enhancing scalability and removing reliance on a centralized aggregator. We perform extensive comparative evaluation of four different ST-GNN training setups-centralized, traditional FL, server-free FL, and Gossip Learning-on large-scale traffic datasets, the METR-LA and PeMS-BAY datasets, for short-, mid-, and long-term vehicle speed predictions. Experimental results show that semi-decentralized setups are comparable to centralized approaches in performance metrics, while offering advantages in terms of scalability and fault tolerance. In addition, we highlight often overlooked issues in existing literature for distributed ST-GNNs, such as the variation in model performance across different geographical areas due to region-specific traffic patterns, and the significant communication overhead and computational costs that arise from the large receptive field of GNNs, leading to substantial data transfers and increased computation of partial embeddings.", "sections": [{"title": "I. INTRODUCTION", "content": "Smart mobility is integral to urban planning, particularly in transportation systems, influencing development and traffic management [1]. Traffic prediction is a key component of smart mobility [2]\u2013[6] and includes tasks such as vehicle speed and public transport forecasting. As demand for these services rises, the need for accurate and efficient traffic forecasting grows, enabling effective resource management, congestion reduction, and route optimization.\nLike other prediction tasks, traffic forecasting methods can generally be divided into classical statistical methods like autoregressive integrated moving average (ARIMA) [7] and Deep Learning (DL)-based methods such as Long Short-Term Memory (LSTM) [8] and Diffusion Convolutional Recurrent Neural Networks (DCRNN) [9]. However, both approaches only use temporal dependencies, overlooking spatial ones.\nGraph Neural Networks (GNNs) [10] address these issues by modeling traffic data as a graph, where nodes represents a physical location, and edges spatial connections between these locations. Specifically, Spatio-Temporal Graph Neural Networks (ST-GNNs) [11] extend GNNs to capture both spatial and temporal dependencies, making them more effective than other DL-based approaches for traffic forecasting. Thus, numerous ST-GNN architectures have been developed and tested for this task [11].\nHowever, most previous works have focus on ST-GNN trained in centralized environments, where data from all sensors is continuously collected. But the task of collecting data, performing DL training and inference, and sending back commands is challenging to perform reliably in real-time in a centralized system. As sensors networks expand, scaling a central control system to handle the increased volume of data in real-time with low-latency becomes increasingly difficult. Furthermore, any issues in the central control system can easily result in a complete interruption of the service, requiring significant investments to maximize reliability. Given the key role played by mobility infrastructure in our societies, a centralized system can also become a significant target for hostile actors and a major cyber-weakness in strategic infrastructure management.\nIn this paper, we explore and adapt distributed training techniques for ST-GNNs in the smart mobility domain through semi-decentralized methods at the network edge, ensuring scalability and reliability. This approach utilizes geographically distributed cloudlets, hosted on edge devices with moderate computation and communication capabilities, as shown in Figure 1-a. Each cloudlet manages a subgraph of the overall traffic graph (Fig. 1-b), shares node features necessary for local training step and model updates with other cloudlets to maintain consistency (Fig. 1-c), and constructs ST-GNN subgraph required for training on its local nodes (Fig. 1-d).\nThe contributions of this paper can be summarized as follows:\n\u2022 To our knowledge, we are the first to investigate semi-"}, {"title": "II. PRELIMINARIES", "content": "Traffic prediction is one of the most common and critical time-series forecasting tasks, as it estimates traffic metrics, such as speed, volume, and density to monitor current traffic conditions and predict future trends. It is generally classified into three forecasting horizons: short-term (15 min), mid-term (30 min), and long-term (60 min). In the context of ST-GNNs, the traffic system is represented as a spatial graph at the t-th time step, denoted by $G_t = (V_t, E, W)$, where $v_i, v_j \\in V_t$ is a finite set of nodes corresponding to observations from n IoT-devices in the traffic network, $(v_i,v_j) \\in E$ is a set of edges connecting the IoT-devices, and $W \\in \\mathbb{R}^{n \\times n}$ is a weighted adjacency matrix that encodes the spatial relationships between these IoT-devices. The spatial component is defined by the physical locations of IoT devices, while the temporal component arises from traffic features that evolve over time, influencing the graph's dynamics. While this work focuses on traffic prediction, methods for other smart mobility tasks can be generalized to this domain due to their shared reliance on spatio-temporal data."}, {"title": "B. Spatio-Temporal Graph Neural Networks", "content": "ST-GNNs are DL models designed to capture both spatial and temporal dependencies in structured data, allowing it to simultaneously account for spatial correlations between nodes and temporal correlations over time, making it particularly suited for traffic prediction. By leveraging spatial dependencies, ST-GNNs provide a more accurate representation of how local traffic conditions at one location impact neighbouring locations.\nMultiple ST-GNNs have been developed [11], such as Graph Recurrent Network (GRN) [13], Spatio-Temporal Graph Convolutional Network (ST-GCN) [14], Graph Attention LSTM Network [15], and many more [11]. Each technique addresses specific challenges or offers unique advantages over others.\nIn this paper, we utilize ST-GCN due to their superior ability to handle traffic prediction tasks in the IoT context compared to other techniques. Experiments demonstrated that ST-GCN consistently outperforms other models in key evaluation metrics (MAE, MAPE, and RMSE) [14]."}, {"title": "C. Traditional Federated Learning", "content": "Traditional FL [16] is a collaborative machine learning (ML) approach where multiple clients work together to train a model, coordinated by a central aggregator. Instead of sharing raw data, which remains stored locally on each client device, the clients exchange only model updates with the server. These updates contain just the minimum information needed for the learning task, carefully scoped to limit the exposure of client data. The central aggregator then aggregates these focused updates as soon as they are received, ensuring that the learning process is both efficient and compliant with data minimization principles."}, {"title": "D. Server-free Federated Learning", "content": "Server-free FL [17] is a decentralized variant of the traditional FL approach. Unlike traditional FL, where a central aggregator is responsible for coordinating the model updates, server-free FL operates without a central entity. Instead, participating devices communicate directly with one another to exchange model parameters and perform local updates. After initializing their models, devices exchange model parameters with their neighbours, aggregate these received models with their own local model, and proceed to train on a subset of data. This iterative process allows for continuous refinement of models across the network, as the updated models are exchanged again between neighbouring devices."}, {"title": "E. Gossip Learning", "content": "Gossip Learning [18] is a decentralized protocol for training ML models in distributed settings without the need for a central coordinator. It is designed to be highly scalable and robust, allowing model updates to be propagated efficiently across a network of devices or nodes. Each device stores two models in its memory (as a FIFO buffer), typically representing the most recent models received from neighbouring devices. At each iteration of the gossip protocol, a device averages the weights of these two models to create an aggregated model. The device then performs one local training step using its own data, refining the aggregated model based on its local observations. Once the model is updated, the device forwards it to a randomly chosen device in the entire network. This process is repeated across the network, ensuring that models continuously evolve and improve as they traverse different nodes, collecting knowledge from the data they encounter at each location."}, {"title": "III. METHODOLOGY", "content": "The traffic prediction scenario involves IoT-devices (i.e., sensors) deployed along highways to monitor real-time traffic conditions such as vehicle speed, volume, and density at fixed time intervals. To streamline data processing, IoT-devices are assigned to local base stations (BS), referred to as cloudlets, based on their physical proximity and communication range. The communication between IoT-devices and its BS is facilitated using Low-Power Wide-Area Network (LPWAN) protocols, such as LoRa or NB-IoT, which enable efficient, long-range, and low-power data transmission. Base stations, in addition to serving as communication hubs, are equipped with moderate computational resources. This existing computational power can be leveraged for training ML models, such as ST-GNNs."}, {"title": "B. Distributed Training Setups", "content": "We adopt a semi-decentralized ST-GNN architecture inspired by Nazzal et al. [19], as it's one of the most effective architectures for distributed training. Figure 1 shows key components of such architecture. In Fig. 1-a, traffic network is represented as a graph, where nodes correspond to IoT-devices, and edges represent road connections based on physical proximity. In Fig. 1-b, the graph is partitioned into subgraphs based on geographical proximity, each managed by a local cloudlet. In Fig. 1-c, cloudlets form a communication network for exchanging necessary node features and model updates. After communicating with each other, each cloudlet constructs ST-GNN subgraph required for training on its local nodes, as shown in Fig. 1-d.\nUsing this architecture, we evaluate four different training setups-centralized, traditional FL, server-free FL, and Gossip Learning for ST-GNNs in the context of traffic prediction, with the centralized setup serving as a baseline for comparing the semi-decentralized approaches."}, {"title": "C. GNN Partitioning in Distributed Training", "content": "Partitioning the graph into subgraphs for local training in distributed GNN scenarios presents unique challenges due to their reliance on spatial dependencies. Unlike traditional ML models that process localized data independently, GNNs require information from neighbouring nodes to compute effective representations. Specifically, in an l-layer GNN, the representation of a node depends not only on its own features but also on the features of its l-hop neighbours.\nThis spatial dependency introduces a need for inter-cloudlet communication, as the required node features often extend beyond the boundaries of a single cloudlet's subgraph. Since IoT-devices typically have limited communication capabilities and cannot directly exchange data with distant cloudlets, the necessary node features are shared between the cloudlets themselves.\nEach cloudlet, based on the underlying sensor network connectivity, is aware of which neighbouring cloudlets require its node features for local computations. When features are collected from the IoT-sensors, the cloudlet proactively broadcasts these features to the neighbouring cloudlets that need them for training (Fig. 1-d). This proactive exchange of node features ensures that each cloudlet has sufficient information to perform local computations without compromising the accuracy of the GNN model."}, {"title": "IV. EXPERIMENTAL SETUP", "content": "To evaluate the performance of our setups, we utilize two real-world public traffic datasets, PeMS-BAY\u00b9 and METR-LA\u00b9 [12], which are collected by Caltrans performance measurement system. Both datasets consist of traffic data aggregated in 5-minute intervals, with each sensor collecting 288 data points per day. The details of datasets are listed in Table I."}, {"title": "B. Evaluation Metrics", "content": "In our experiments, we evaluate the performance of different setups using Weighted Mean Absolute Percentage Error (WMAPE), alongside the widely adopted Mean Absolute Error (MAE) and Root Mean Square Error (RMSE). We omit the formulas for MAE and RMSE due to their wide adoption. The WMAPE formula is:\n$WMAPE(x, \\hat{x}) = \\frac{\\sum |x_i - \\hat{x_i}|}{\\sum x_i} * 100%$         (1)\nWhere $x = x_1,..., x_n$ denotes the ground truth values of vehicle speed, and $\\hat{x} = \\hat{x_1},..., \\hat{x_n}$ represents their predicted values.\nWe compute these metrics across three forecasting horizons-short-, mid-, and long-term-to capture performance variations over time. For centralized and FL setups, metrics are derived from the main server's aggregated model, while for server-free FL and Gossip Learning, they are computed as a weighted average of individual cloudlet predictions. This ensures a consistent and fair comparison across all approaches."}, {"title": "C. Experimental Setup", "content": "All experiments were run with a fixed number of 40 epochs, utilizing the High-Performance Computing (HPC) system Leonardo Booster, which features 4x NVIDIA A100 SXM6 64GB GPUs, with only 1 being used during training. The software environment used for the experiments included Python 3.10.14, PyTorch 2.3.1, and PyTorch Geometric 2.5.3. For model architecture, we used 2 spatio-temporal blocks (ST-blocks), with GLU activation function, a learning rate of 0.0001, dropout of 0.5, weight decay of 0.00001, and a batch size of 32. MAE is used as the training loss, Adam was chosen as the optimization algorithm, and learning rate scheduler was applied with the StepLR strategy, where the step size was set to 5, and the decay factor was set to 0.7. The temporal and spatial kernel size was set to 3 for all experiments, with Chebyshev convolution as convolutional layer. All experiments use 60 minutes as the historical time window, i.e., 12 observed data points.\nAdditionally, sensors were distributed across 7 cloudlets based on proximity and communication range. Each cloudlet can communicate with other cloudlets and IoT-devices only if they are within an 8 km range. This limitation directly impacts the server-free FL approach by restricting the number of cloudlets that can exchange model updates. In contrast, this constraint does not affect Gossip Learning, as model updates are sent to a randomly selected cloudlet across the entire network, regardless of proximity. Fig. 2 shows how sensors are assigned to cloudlets and their respective communication ranges for both the METR-LA and PeMS-BAY datasets. This partitioning reflects the semi-decentralized structure used for our distributed training approach."}, {"title": "V. EXPERIMENTAL RESULTS", "content": "Table II presents the results of all setups, datasets, and forecast horizons. Centralized training consistently outperforms semi-decentralized methods across all metrics and horizons. However, the performance gap is minimal, especially for MAE and RMSE. For instance, in the METR-LA dataset, the MAE difference between centralized and the best semi-decentralized setup is only 0.1 mph for short-term predictions-negligible compared to typical traffic flow variations, often measured in tens of mph. While WMAPE shows a slightly larger gap as the forecast horizon increases, particularly for METR-LA, the difference remains narrow across all horizons. This indicates that semi-decentralized setups offer competitive performance with no significant practical drawbacks for traffic prediction."}, {"title": "B. Cloudlet Metric Analysis", "content": "Figure 3 shows WMAPE variability across cloudlets for all setups and datasets, focusing on short- and long-term predictions, providing a comprehensive view of how errors distribute geographically across cloudlets for different prediction horizons. WMAPE is highlighted as it provides a clear, percentage-based measure of relative error, making it easier to compare variability across cloudlets.\nThe figure reveals consistent WMAPE spread across cloudlets, regardless of training setup, dataset, or horizon, indicating that performance variability stems from the datasets' geographical characteristics and sensor partitioning, rather than the training methods. Each cloudlet's performance is influenced by the distribution of sensors assigned to it and the corresponding traffic patterns in that geographical region.\nAdditionally, while global weighted averages reported in Table II provide a summary of performance, they fail to capture the significant differences in performance across individual cloudlets. Previous works have primarily focused on global averages, overlooking notable challenge of heterogeneous performance among cloudlets. This oversight can lead to an incomplete understanding of model performance in distributed traffic forecasting systems, resulting in unexpected discrepancies, suboptimal decision-making and potential disruptions in real-world usage compared to testing."}, {"title": "C. Analysis of semi-decentralized overheads", "content": "Semi-decentralized learning naturally introduces several overheads, primarily stemming from the need to communicate and aggregate models, increasing communication and computational costs compared to centralized training. Additionally, due to graph partitioning, node features must be exchanged between cloudlets, further contributing to communication and compute demands. Table III breaks down these overheads across three key aspects: model transfer, node feature transfer, and floating-point operations (FLOPs).\nA significant communication cost arises from node feature transfers. Unlike centralized training, where each sensor's features are sent once to a central server, distributed setups require sending features to multiple cloudlets. This increase is due to the dense graph created by our distance-based weighted adjacency matrix and the 2-hop GNN, which requires each cloudlet to process features from both direct and indirect neighbors. Consequently, each cloudlet processes a substantial graph portion, leading to several-fold higher communication costs compared to centralized setups. However, as the network grows larger, the portion of the graph stored in each cloudlet will decrease, as cloudlets farther away will not have connected sensors. Despite the increase, the overall feature transfer per cloudlet remains manageable, typically just a few megabytes.\nIn contrast, model transfer per epoch is relatively small due to the compact size of ST-GCN models. While model transfers accumulate over time, contributing to total communication overhead, the per-epoch cost remains in the order of megabytes. As shown in Figure 4, distributed setups, particularly server-free FL, require more epochs to converge than centralized training, increasing total model transfer costs. Despite this, even in server-free FL's worst-case scenario, where all cloudlets communicate with each other, model transfer remains a minor overhead.\nComputation-wise, aggregation costs are many orders of magnitude smaller training costs, so they do not represent a reason to avoid semi-decentralized training. However, distributed setups incur significantly higher training costs than centralized ones, mirroring the feature transfer pattern. Each cloudlet runs the model on its local subgraph, often with duplicated nodes and features due to overlapping receptive fields. This duplication results in additional computations, as cloudlets must compute partial embeddings for nodes they do not own. In contrast, centralized training efficiently processes the entire graph without redundancies, avoiding duplicated calculations.\nOur analysis shows that the primary overheads in semi-decentralized training stem not from the distributed learning algorithms themselves, but from the highly interconnected spatial nature of traffic data and the need to partition these spatial connections across cloudlets. If these algorithms were applied to non-spatial models, the overheads would be minimal. While communication and computation costs are higher than in centralized setups, they remain within acceptable limits, making semi-decentralized training a practical solution for scalable traffic prediction in smart mobility applications."}, {"title": "VI. RELATED WORK", "content": "Training and testing GNN models on large graphs demand substantial memory and processing resources due to the interdependence of graph nodes [21]. This creates a scalability and reliability issues for centralized GNN operations [22], especially when dealing with large-scale graphs or sensitive data collected from distributed sensors in real-time traffic prediction.\nHowever, the majority of existing approaches for training ST-GNNs are based on centralized architectures [13]\u2013[15], [23]. While this approach allows for the direct management of both spatial and temporal correlations, ensuring that the full context of the data can be leveraged to improve the models accuracy and performance, it comes with significant limitations as described above."}, {"title": "B. Traditional Federated Learning ST-GNN training", "content": "Several works have applied traditional FL to train ST-GNN models for traffic prediction [24]-[27]. One of the primary advantages of traditional FL is enhanced scalability. By distributing the training process, it leverages the computational resources of multiple devices, which allows larger datasets to be processed without overloading any single server. While this approach improves on centralized training, it still relies on a central aggregator, meaning that it shares the same issues as centralized approach, as was already described above."}, {"title": "C. Decentralized and semi-decentralized ST-GNN training", "content": "To the best of our knowledge, there has been no prior research specifically focused on decentralized or even semi-decentralized training of ST-GNNs for traffic prediction.\nOne relevant work by L. Giaretta et. al. [28] explores the challenges and solutions associated with fully decentralized training. However, their solution is not tailored to ST-GNNs, but rather to Graph Convolutional Network (GCN), limiting its applicability to traffic prediction tasks that require both spatial and temporal correlations.\nWhile there is no prior research work for decentralized training for ST-GNNs, M. Nazzal et. al. [19] developed Heterogeneous Graph Neural Network with LSTM (HetGNN-LSTM) for taxi-demand prediction and tested it in a semi-decentralized environment, where nodes were partitioned into cloudlets, without any central aggregator. However, their solution only enables semi-decentralized inference by deploying a pre-existing model, with no support for semi-decentralized training."}, {"title": "VII. FUTURE WORK", "content": "Future work should address the significant communication overhead arising from node feature transfers, which dominate the bandwidth in semi-decentralized training, and increased computation of partial embeddings during training. While communication is feasible in these setups, the cost is significant compared to centralized training. Future work should aim to shift the trade-off between scalability and reliability by reducing both network overhead and computational cost without compromising model accuracy by exploring these directions:\n\u2022 Changing the network connectivity to be more sparse, replacing distance-based node connectivity with road network-based connectivity, with minimal impact on the model accuracy\n\u2022 Restricting the receptive field of GNNs by adopting layer-wise GNN training, where the receptive field is constrained to avoid unnecessary feature propagation, as proposed by L. Giaretta et. al. [28]."}, {"title": "B. Cloudlet personalization", "content": "Another promising direction is implementing individual cloudlet personalization to address performance variability across cloudlets. Our results have highlighted a notable gap in error rates, especially in certain cloudlets with consistently higher error values. Personalizing cloudlets that have lower performance compared to the rest of the cloudlets, tailoring them to local traffic conditions or unique spatial-temporal patterns, could help reduce these disparities. Such a personalized approach may involve adjusting model parameters or incorporating local fine-tuning for cloudlets with higher error rates, potentially enhancing prediction accuracy across the network while maintaining decentralized resilience."}, {"title": "VIII. CONCLUSION", "content": "In this paper, we have explored semi-decentralized approaches for traffic prediction in the smart mobility domain, comparing centralized, traditional FL, server-free FL, and Gossip Learning setups. By distributing ST-GNN training across multiple cloudlets, we aimed to address scalability and resilience challenges associated with traditional centralized approaches.\nExperimental results on the METR-LA and PeMS-BAY dataset show that semi-decentralized setups are comparable to centralized approaches in performance metrics, while offering advantages in terms of scalability and fault tolerance. We also highlight often overlooked issues in existing literature for distributed ST-GNNs, such as the variation in model performance across different geographical areas due to region-specific traffic patterns, and the significant communication overhead and computational costs that arise from the large receptive field of GNNs, leading to substantial data transfers and increased computation of partial embeddings."}]}