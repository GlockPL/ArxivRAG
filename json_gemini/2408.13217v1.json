[{"title": "HBIC: A Biclustering Algorithm for Heterogeneous Datasets", "authors": ["Ad\u00e1n Jos\u00e9-Garc\u00eda", "Julie Jacques", "Cl\u00e9ment Chauvet", "Vincent Sobanski", "Clarisse Dhaenens"], "abstract": "Biclustering is an unsupervised machine-learning approach aiming to cluster rows and columns simultaneously in a data matrix. Several biclustering algorithms have been proposed for handling numeric datasets. However, real-world data mining problems often involve heterogeneous datasets with mixed attributes. To address this challenge, we introduce a biclustering approach called HBIC, capable of discovering meaningful biclusters in complex heterogeneous data, including numeric, binary, and categorical data. The approach comprises two stages: bicluster generation and bicluster model selection. In the initial stage, several candidate biclusters are generated iteratively by adding and removing rows and columns based on the frequency of values in the original matrix. In the second stage, we introduce two approaches for selecting the most suitable biclusters by considering their size and homogeneity. Through a series of experiments, we investigated the suitability of our approach on a synthetic benchmark and in a biomedical application involving clinical data of systemic sclerosis patients. The evaluation comparing our method to existing approaches demonstrates its ability to discover high-quality biclusters from heterogeneous data. Our biclustering approach is a starting point for heterogeneous bicluster discovery, leading to a better understanding of complex underlying data structures.", "sections": [{"title": "1 Introduction", "content": "Unsupervised machine learning techniques such as clustering have become widely used due to their ability to provide new insights from unlabeled datasets [1]. These algorithms aim to find global patterns (homogeneous groups) from observations based on their coherence found along all attributes [31]. However, the recent big-data phenomenon has massively increased the number and type of observations and attributes, leading to the emergence of high-dimensional and heterogeneous datasets [10, 35]. For instance, in biomedicine, electronic health record (EHR)-based research using unsupervised learning is central to fulfilling the vision of personalized medicine [19]. In these heterogeneous data problems, specific observations are usually locally correlated on a subset of attributes. In this context, biclustering is an unsupervised learning technique that simultaneously groups observations and attributes, forming biclusters, leading to unique conceptual benefits [21, 18]: (i) can unravel local patterns, i.e., observations meaningfully correlated on a subset of attributes; (ii) provide flexibility in the definition of different types of biclusters; (iii) allow overlapping biclusters, such that one observation can belong to more than one bicluster; and (iv) help alleviate the curse of dimensionality problem, as the space volume grows exponentially with the number of observations.\nBiclustering is an active research field with application to diverse problems, such as clinical and biological [35, 33], gene expression and microarray data [22], text mining [4], and time series analysis [5]. During the last 20 years, many biclustering algorithms and tools have been developed to provide insights into large high-dimensional datasets [21, 35, 18]. However, current biclustering algorithms mainly operate on numeric attributes, which poses challenges when dealing with highly heterogeneous datasets containing multiple data types, such as binary, numeric, and categorical attributes. In practice, feature transformation is a commonly used technique [34, 3]; unfortunately, this early transformation step largely pre-determines the results and can cause information loss, as the relative importance of different attributes is not considered [1].\nThis paper proposes a biclustering approach capable of discovering meaningful biclusters from datasets having binary, numeric, and categorical attributes. The algorithm consists of two main stages, bicluster generation, and model selection. In the generation stage, a heuristic-based iterative process is used to generate multiple candidate biclusters, which may consist of different data types. Then, a fitness function is proposed based on intra-cluster variance to measure the quality of obtained biclusters with mixed-type attributes. In the second stage, we propose two strategies for selecting the most meaningful biclusters generated in the first stage. The resulting biclustering approach not only has the capability to automatically identify the number of biclusters, but can also leverage domain-specific knowledge if it is available.\nThe organization of this paper is as follows. Section 2 presents background concepts and discusses related works. Our proposed biclustering approach is described in Section 3. Section 4 provides the experimental setup, benchmarks, and performance assessment. The results and main findings are discussed in Section 5. Finally, Section 6 concludes and presents future research directions."}, {"title": "2 Background and related work", "content": "This section presents some basic concepts, summarizes the related works, and discusses relevant literature."}, {"title": "2.1 Biclusters", "content": "A data matrix X is defined by N observations (rows), R = {1,..., N}, and M attributes (columns), C = {1,..., M}. A bicluster B = (I, J) is a subset of rows I\u2286 R and columns J\u2286 C of the original matrix, and bij represents a value in the bicluster corresponding to the i-th row and the j-th column.\nThere exist several definitions of biclusters, each specifically tailored to different applications and data characteristics [21, 7]. We can identify four types of biclusters based on their assumed correlations between the bicluster values:\n1.  Constant biclusters: an overall-constant bicluster has all the values equal to a constant \u03c0, i.e. bij = \u03c0; constant-rows has all values equal per row, i.e. bij = ai; and constant-columns has all values equal per column, i.e. bij = Aj.\n2.  Coherent biclusters: an additive coherent or shifting bicluster is defined as bij = \u03c0 + \u03b1\u03af + \u03bbj, and a multiplicative coherent or scaling bicluster is defined as bij = \u03c0\u00d7 \u03b1\u00a1 \u00d7 \u03bbj.\n3.  Order preserving biclusters: the rows or columns in the bicluster represent general trends in data (such as up-down-up) rather than explaining well-defined values.\n4.  Composed biclusters: is a bicluster combining different types of biclusters. For instance, a bicluster with two sub-biclusters: an order preserving bicluster with numeric attributes and one constant-columns bicluster with categorical attributes.\nIn the previous definitions, ai (1 \u2264 i \u2264 |I|) and 1; (1 \u2264 j \u2264 |J|) refers to constant values used in the different bicluster definitions."}, {"title": "2.2 Biclustering algorithms", "content": "In real-world problems, biclustering algorithms are used to identify multiple biclusters, generating a biclustering solution, denoted as B = {B1,..., Bq}, where q represents the number of biclusters. The relationships between the biclusters covering the input data matrix are determined based on two criteria: exclusivity and exhaustivity [21, 18]. Exclusivity indicates that a row or column belongs only to one of the q bicluster in the biclustering solution. In contrast, the exhaustivity criterion specifies that every row and column belongs to at least one bicluster. Exhaustivity refers to covering the input matrix, while exclusivity is associated with biclustering all rows and columns in X. Therefore, the bicluster types to be discovered will depend on both the problem domain and the data types involved [23].\nBiclustering data analysis has gained much interest since the seminal work of Cheng and Church to investigate gene expression data [8]. Nowadays, many biclustering algorithms exist in the literature, particularly for biological data such as gene expression [21, 25, 18, 17]. In the more general case, these biclustering algorithms can be categorized into heuristic-based and metaheuristic-based approaches. The reader is referred to the following surveys for a detailed classification of biclustering algorithms and their applications [21, 7, 18]."}, {"title": "2.3 Biclustering algorithm for mixed-type data", "content": "We discuss existing biclustering algorithms from two crucial design aspects: their capacity to address multiple heterogeneous attributes and their ability to determine the number of biclusters automatically. To the best of our knowledge, the works of Vandromme et al. [32, 33] and Selosse et al. [28] are the only biclustering approaches specifically designed to extract biclusters from heterogeneous datasets.\nVandromme et al. [32] proposed a biclustering algorithm called HBC to extract biclusters with constant values in the columns from medical data matrices, where the attributes can be numerical, symbolic, and binary. Subsequently, the HBC algorithm was extended for temporal data [33] (i.e., data from multiple patient visits). Although this approach is representative of the literature, it lacks a comprehensive evaluation against other traditional biclustering algorithms, and the ability of HBC to find other types of biclusters (i.e., overlapping) is not investigated. Selosse et al. [28] presented a model-based biclustering approach for data with different types of attributes. They extended the latent block model (LBM) from numerical data to other data types, such as categorical and binary. In addition, a distribution type was defined for each data type and modeled using a multiple LBM. A major limitation of this model is that the mixed-type attributes cannot be part of the same column cluster, as the model is based on the assumption that the attributes of the same block share the same distribution. Additionally, the multi-parameter configuration of each distribution used in the model is a complex task that varies for each data subset.\nThe biclustering approaches discussed above either require as input the maximum number of biclusters to be fixed in advance [8, 20], or they keep this parameter variable and generate a certain number of biclusters (usually several) without making an informed selection of the most representative ones and leave this task only to the decision maker or data expert [32, 33, 28]. The former approach is only sometimes realistic, as the most suitable number of biclusters is unknown in real-world applications. The latter is more generally applicable but completely disregards any insight or domain expertise available. To address these difficulties, our approach outlined in Section 3 can extract various types of biclusters from heterogeneous data and automatically identify the most representative."}, {"title": "3 Proposed algorithm: HBIC", "content": "This section introduces our proposed approach HBIC (Heterogeneous BIClustering) to address datasets with mixed-type attributes such as binary, numeric, and categorical. First, HBIC generates candidate biclusters iteratively from a discretized search space. Then, HBIC selects meaningful biclusters by ranking them according to their heterogeneous intra-bicluster variance, measured in the original heterogeneous data space.\nThe overall framework, principal components, complexity analysis, and source code availability of our proposed algorithm HBIC, are discussed in the following subsections."}, {"title": "3.1 Overall framework", "content": "The overall functioning of HBIC is outlined in Algorithm 1. In the first stage, the numeric attributes of the input data matrix are discretized; then, a heuristic search strategy generates a set of candidate biclusters, and subsequently, all repeated biclusters are removed. Then, in the second stage, the problem of determining the most representative biclusters is addressed (see Section 3.3).\nHBIC operates over a discretized search space to generate candidates during the first stage. In HBIC, the numeric attributes are discretized, simplifying the bicluster search since these attributes will have a finite number of possible values (a limited number of possible states). Since HBIC is an unsupervised learning algorithm, only unsupervised data discretization methods can be used in the Discretization step (line 1 in Algorithm 1). HBIC uses a basic, standard equal-width binning method, which divides the data"}, {"title": "3.2 Bicluster quality", "content": "The initial phase of the HBIC algorithm generates multiple candidate biclusters, B, for a given heterogeneous data problem. In the second phase, the quality of the candidate biclusters is computed in the original heterogeneous data space, enabling the selection of the most representative biclusters. In the literature, there is a range of metrics defined for numeric biclusters [26], including the well-known minimum squared error and the average correlation function. To measure the quality of heterogeneous biclusters, we propose the heterogeneous intra-bicluster variance (HIV), which is comparable to the definitions used in HBC [32].\nLet X be the input data matrix with N rows and M columns, and let B = (I, J) a bicluster expressed as a tuple of two nonempty sets. Thus, HIV evaluates the intra-bicluster homogeneity in the presence of heterogeneous attributes as follows:\n$$HIV (B) = ANV (I, Jnum) + ACF (I, Jcat)$$, (1)\nwhere ANV (,) denotes the average numeric variance for the numeric attributes such that Jnum \u2286 J, and ACF (,) represents the average categorical frequency for the categorical and binary attributes such that Jcat \u2286 J. Firstly, the ANV function is given by\n$$ANV (I, Jnum) = \\frac{1}{|J_{num}|} \\sum_{j \\in J_{num}} \\frac{var (b_{Ij})}{var (x_{Rj})}$$, (2)\nwhere  $|J_{num}|$ is the number of numeric attributes; var(b1j) denotes the variance of the numeric values for the j-th column in B; and var(xRj) denotes the variance of the numeric values for the j-th column in X. Secondly, the ACF function estimates the extent to which a discrete value is correctly expressed to its biclusters. This is evaluated by counting the most frequent value in a certain discretized attribute (e.g., categorical or binary). The ACF is computed as\n$$ACF (I, Jcat) = \\frac{1}{|J_{cat}|} \\sum_{j \\in J_{cat}} (1 - \\frac{freq (b_{Ij})}{|I|})$$, (3)\nwhere  $|J_{cat}|$  indicates the number of categorical and binary attributes, |I| is number of rows, and freq (b1j) denotes the number of the most frequent value for the j-th attribute.\nThe above expressions describe a quality measure for heterogeneous biclusters with numeric, binary, and categorical data types. However, if other data types exist in X, this function will require further adaptations and analysis. HIV is an optimization function that takes values in the range [0, inf] and has to be minimized, i.e., values close to zero indicate more homogeneous biclusters."}, {"title": "3.3 Model selection of biclusters", "content": "The selection of the number of clusters is an important problem in unsupervised machine learning [31, 15]. In bicluster analysis, biclustering algorithms usually require defining this parameter a priori [8, 21, 14]; however, in real-world applications, this parameter is usually unknown [14, 2]. In this work, we propose two selection strategies for automatically determining the number of biclusters:\nDistance-based selection The fitness of a heterogeneous bicluster, B = (I, J), is computed as\n$$fitness(B) = a \u00d7 HIV(B) + (1 \u2212 a) \u00d7 (1 \u2013 Size(B))$$, (4)\nwhere a is a constant weight representing a preference towards either the heterogeneous intra-bicluster variance (HIV) or the bicluster size (Size). HIV is determined using Eq. (1), whereas the bicluster size is computed as the number of rows times the number of columns, |I| \u00d7 |J|. The values of HIV(.) and Size() should be normalized to the range [0, 1], and Eq. (4) should be minimized so that values close to zero indicate highly homogeneous biclusters with large sizes. Thus, based on the previous definitions, the distance-based selection is summarized in four main steps: (i) the fitness of all the candidate biclusters are computed using Eq. (4); (ii) the biclusters are sorted according to their scores; (iii) the difference between the scores of the biclusters is estimated; (iv) the index of the most significant change (largest) of the consecutive score indicates the number of biclusters to be preserved.\nPareto-based selection Let us consider two objectives to be minimized: f1 = HIV(B) and f2 = (1 - Size(B)), as they were defined separately in Eq. (4). Let F = (f1, f2) be the vector of objective functions. A bicluster solution B\u00b9 \u2208 B is said to be better than B2 \u2208 B (also known as B\u00b9 dominates B2, denoted as B\u00b9 < B\u00b2) if and only if fi(B\u00b9) < fi(B\u00b2) for all i \u2208 {1,...,m} and fi(B\u00b9) < fi(B2) for at least one i \u2208 {1, . . ., m}, where m = 2 denotes the number of objectives. A bicluster solution B* is Pareto optimal in case there does not exist a solution B\u2208 B that dominates B*. The set of all Pareto-optimal solutions is called the Pareto Set (PS), and FP = {F(B) | B \u2208 PS} is called the Pareto Front [9]. In our case, all the non-dominated solutions in the Pareto Front represent the most relevant biclusters."}, {"title": "3.4 Complexity analysis", "content": "The main operation of the HBIC heuristic described above is AddColumn (see Algorithm 2), where the best column is added to a bicluster, and rows are removed to keep this bicluster perfectly homogeneous. The cost of this procedure is to search which rows of a column to be added are present in the current bicluster. This translates into O(r x log(r)) worst-case complexity since looking for an element in a list has O(log(r)) complexity, where r is the number of rows in the data matrix. At each step, the best column is added, which means all possible columns must be added to determine the optimal choice. Therefore, O(c\u00b2) AddColumn operations are performed during the construction of each bicluster, where c is the number of columns. This whole process is performed for each column and each possible value of this column as a starting point; therefore, O(cx v) iterations are performed, where v is the number of possible values for an attribute. This leads to a worst-case time complexity of O(rxlog(r) x c\u00b3 xv). However, fewer than O(c\u00b2) AddColumn operations are performed in practice, as the method usually reaches the stopping criterion after adding a few dozen columns (even on heterogeneous real-world datasets). Finally, regarding both model selection strategies presented, the most time-consuming step is the computation of HIV, which is of the order of O(\u1e9e \u00d7 r \u00d7 c), where \u1e9e is the number of candidate biclusters. This complexity analysis, along with experimental observations, indicates that the proposed method scales mainly with the number of columns. The total number of rows; however, does not significantly impact the execution time."}, {"title": "3.5 Source code availability", "content": "The source code of HBIC (written in Matlab and Python) and the datasets used in our experiments are available to the research community in the following repositories: https://github.com/adanjoga/hbic and https://github.com/clementchauvet/py-hbic."}, {"title": "4 Experimental setting", "content": null}, {"title": "4.1 Datasets", "content": "Heterogeneous synthetic datasets The following collection of heterogeneous synthetic datasets was generated using the G-bic tool recently proposed by Castanho et al. [6], which is available through the repository: https://github.com/jplobo1313/G-Bic/. This collection comprises 21 data configurations organized into five categories. Each category aims to evaluate a unique aspect of datasets in medical applications. In the following descriptions, unless otherwise noted, each data set contains 1000 rows and 500 columns, with five planted biclusters, where 50% of the attributes are numeric and 50% categorical. The five categories of datasets are:\n\u2022 Heterogeneity level (HL). The percentage of categorical attributes in the dataset was modified as follows: HL \u2208 {0, 25, 50, 75, 100}, where HL=0% indicates a complete numeric dataset, and HL=100% denotes a categorical dataset.\n\u2022 Number of biclusters (NB). The number of planted bicluster, \u03b2*, was modified as: \u03b2* \u2208 {3,5,8,10}.\n\u2022 Size of biclusters (SB). The bicluster size (|I|\u00d7|J|) was modified as follows: SB \u2208 {(25\u00d725), (50\u00d750), (75\u00d775), (100\u00d7100). For each configuration, three biclusters were planted: one numeric, one categorical, and one mixed.\n\u2022 Size of datasets (SM). The dataset size (N \u00d7 M) was modified as: \u2208 {(500 \u00d7 250), (1000 \u00d7 500), (1500 \u00d7 750), (2000 \u00d7 1000).\n\u2022 Noise level (NL). The percentage of noise level was modified both in the dataset and in the planted biclusters: NL \u2208 {5, 10, 15, 20}.\nThis collection contains a total of 315 heterogeneous data problems, as for each of the 21 data configurations, 15 datasets were generated. The generation of the datasets and biclusters follows a uniform distribution, where the numerical variables are in the range of [-10, +10]. The categorical attributes follow the alphabet {\u201ca\u201d, \u201cb\u201d, \u201cc\u201d, \u201cd\u201d, \u201ce\u201d, \u201cf\u201d, \u201cg\u201d, \u201ch\u201d, \u201ci"}, "."]}, {"title": "4.2 Biclustering reference algorithms", "content": "Regarding the comparison of our proposed approach with state-of-the-art algorithms, HBIC works on heterogeneous attributes (numeric, categorical, combination of both); however, most existing biclustering methods work exclusively on numerical data. For this reason, the following well-known algorithms are considered: Cheng and Church's Algorithm (CCA) [8] and Large Average Submatrices (LAS) [29]. These two algorithms are compared to three versions of the HBIC algorithm, namely: HBICdis, HBICpar, and HBICall, corresponding to the model selection strategies distance- and Pareto-based as described in Section 3.3. We performed the following data preprocessing approach to use CCA and LAS in heterogeneous data problems. First, categorical data was encoded into binary using the one-hot encoding technique. Then, the algorithms were applied to this new numeric representation. Finally, a decoding step was performed to recover the column indices from the resulting biclusters."}, {"title": "4.3 Evaluation metrics", "content": "Section 5 reports statistics computed from several independent executions performed for each biclustering method studied for every data problem considered. The ability of an algorithm to produce high-quality biclusters is assessed using three metrics. In the following definitions, let B = (I, J) be a bicluster represented by a tuple of two nonempty sets such that I \u2286 R and J\u2286 C, where N is the number of rows and M the columns in the input data matrix. Also, let us assume that B = {Bi}i=1 and B* = {B*i}i=1 are the obtained solution by the algorithm and the reference biclustering solution, respectively. The relevance and recovery metrics are defined as in [27]:\n$$relevance(B,B^*) = {S_R(B, B^*) \u00d7 S_C(B,B^*)^{1/2}}$$, (5)\n$$recovery(B, B^*) = relevance(B^*, B)$$, (6)\nwhere SR(,) and Sc(,) are defined as follows\n$$S_R(B, B^*) = \\frac{1}{k} \\sum_{B_i \\in B} \\max_{B^*_j \\in B^*} \\frac{|I_i \\cap I^*_j|}{|I_i \\cup I^*_j|}$$, and (7)\n$$S_C(B, B^*) = \\frac{1}{k} \\sum_{B_i \\in B} \\max_{B^*_j \\in B^*} \\frac{|J_i \\cap J^*_j|}{|J_i \\cup J^*_j|}$$, (8)\nThe recovery metric evaluates the ability of an algorithm to discover the true biclusters, regardless of the total number of biclusters found by the algorithm. Meanwhile, the relevance metric quantifies the similarity between the obtained and the true biclusters, imposing penalties if the number of biclusters does not match between the two solutions [27, 18]. Additionally, to further understand the performance and behavior of biclustering algorithms, we analyze a biclustering solution using the Clustering Error metric presented by Patrikainen and Meila [24] (herein renamed as biclustering error), which is defined as:\n$$biclustering \\ error (B, B^*) = \\frac{D_{max}(B, B^*)}{|Uni(B) \\cup Uni(B^*)|}$$, (9)\nwhere Uni() is the union set of a biclustering solution defined as\n$$Uni (B) = \\bigcup_{B_i \\in B} I_i \u00d7 J_i$$, (10)\nDmax (\u00b7,\u00b7) represents a unique relation {xi, Yi}min(k,q):\n$$D_{max} (B, B^*) = \\sum_{i=1}^{min(k,q)} |I_{xi} \u00d7 J_{xi} \\cap I^*_{Yi} \u00d7 J^*_{Yi}|$$, (11)\nThese three metrics analyze the pairwise co-assignment of data entities between the solution obtained and the correct partition (ground truth, which is known for the synthetic problems considered). The measures are defined in the [0, 1] range, and higher values indicate better biclustering performance. Horta and Campello [13] present a comparative study of these three and other biclustering metrics and didactically illustrate their implementation.\nThe non-parametric Kruskal-Wallis test is used to investigate the statistical significance of the performance differences observed between the approaches compared. A significance level of a = 0.05 is considered in all the cases. The Bonferroni correction is applied to account for multiple testing issues."}, {"title": "5 Experimental results", "content": "This section discusses the results of a series of experiments conducted to investigate the performance of the proposed biclustering algorithm for heterogeneous datasets, HBIC."}, {"title": "5.1 Results on heterogeneous synthetic datasets", "content": "This section presents a comparative analysis of the results obtained by HBIC and its performance in comparison to two reference biclustering algorithms (see Section 2.2). The results of this comparison are summarized in Fig. 1, while Table 1 presents detailed results for each problem configuration and category, accompanied by a statistical significance analysis.\nIn the case of heterogeneous synthetic datasets, the HBICall algorithm obtained the best performance in terms of the metric recovery, i.e., the capacity to find the true planted biclusters, where HBICpar"}, {"title": "5.2 Application to systemic sclerosis", "content": "A primary study of SSc patients suggested the existence of two (general perspective) and six (specific perspective) patient clusters [30, 16]. It showed that there is no clear dichotomy between SSc patients with diffuse and limited cutaneous forms, and that other homogeneous groups can be found beyond skin involvement. However, these groups may not adequately represent the homogeneity of the patients because a generic clustering algorithm was used that transformed categorical variables into numerical values.\nWe analyzed the CHUL database of SSc patients (see Section 4.1) using CCA, LAS, and HBIC biclustering algorithms. On the one hand, a total of 39 biclusters were generated using the proposed HBIC. These heterogeneous biclusters are detailed in the Supplementary Material. The variability and differences of the obtained biclusters are illustrated in Fig. 2 in terms of four well-known metrics in"}, {"title": "6 Conclusions", "content": "Our work proposes a biclustering approach based on a greedy heuristic for heterogeneous datasets. The resulting algorithm, HBIC, can find meaningful biclusters from heterogeneous data and automatically determines the number of biclusters when this parameter is unknown. Experiments with various datasets of different characteristics and complexities underline the robustness of the proposed approach to solving complex heterogeneous data problems. Overall, our algorithm reports a competitive performance to existing biclustering algorithms. Our approach excels in identifying meaningful biclusters from medical datasets with multiple data types.\nOur analysis focused on heterogeneous data with numeric, binary, and categorical attributes. In this scenario, HBIC can retrieve constant and composed types of biclusters. However, further analysis is needed to extend this capability to other bicluster patterns and data types (i.e., longitudinal and text data). Our algorithm's design prioritized adopting straightforward yet effective techniques; nevertheless, HBIC can incorporate more sophisticated methods. For example, the selection strategies can be modified to improve the algorithm's performance. Furthermore, extending the applicability of HBIC to data with missing values is an interesting path for future investigation."}, {"title": "A Supplementary Material", "content": null}, {"title": "A.1 Impact of the Discretization Strategy in HBIC", "content": "This section aims to analyze the impact of the discretization function and its parameter nbins in our proposed HBIC algorithm. The details of the DISCRETIZATION() function are described in the main paper, and its implementation in HBIC is indicated in Algorithm 1, line 1. The parameter nbins in the equal-width binning discretization helps to transform numeric attributes into discrete categories. This strategy helps to simplify the analysis of heterogeneous data; however, choosing the right intervals (nbins) can be challenging, as different values lead to different performances of the HBIC algorithm. In this section, we study the impact of the nbins parameter in the HBIC algorithm when dealing with numerical datasets.\nIn our study, we considered the numerical data problems described in Section 2, and we varied the value of the parameter nbins in the discretization function as nbins = {2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20}. Note that the nbins parameter only affects datasets with numeric attributes, since the discretization step is unnecessary for categorical and binary variables. For this reason, this study of the nbins parameter was performed on datasets where all attributes are numeric and represent the worst case for the HBIC algorithm, where in the best case, all attributes in the dataset are categorical or binary.\nAs a first step to visually understand the effect of the nbins parameter on numeric datasets, Figure 3 exemplifies the visualization of a numerical matrix with five biclusters and different discretization levels, nbins = {2, 5, 10, 15}. We can observe that the higher the level of discretization, the closer to the original data matrix distribution. Therefore, this indicates that a higher value of the nbins parameter helps better to approximate the frequency distribution of the original data matrix; however, as the value of nbins increases, the computational complexity of the HBIC algorithm will also increase.\nThe results of the impact of the discretization strategy in the HBIC algorithm are summarized in Figures 4 and 5. Overall, from Fig. 4, we can observe that as we increase the parameter nbins the biclustering performance in terms of the metrics recovery and relevance also tends to increase. On the one hand, for the recovery metric (the ability of the algorithm to retrieve exactly the inserted biclusters), acceptable performance close to the unity is obtained from nbins = 5, while for lower values the performance starts very low and increases monotonically until it becomes stable. On the other hand, regarding the relevance metric (the ability to recover good candidate biclusters, i.e., compact with low variance), acceptable performance close to the unity is obtained from nbins = 6. These results suggest that the numeric datasets considered a value superior or equal to six will lead to robust biclustering performance."}, {"title": "A.2 Supplementary Results", "content": "Table 4 summarizes the 39 characteristics of the biclusters generated by HBIC when using the CHUL database. This medical dataset comprises 530 observations and 40 attributes (22 binary, 16 numeric, and two categorical). The proposed algorithm obtained several biclusters with different degrees of homogeneity (where a bicluster can contain attributes of different types) and of varying sizes. Also, these biclusters cover different regions of the input data matrix. Finally, although these subclusters contain interesting properties, an expert analysis is needed in order to derive medical interpretations of their potential impact on the SSc disease."}]