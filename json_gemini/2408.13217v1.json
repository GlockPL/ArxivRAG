{"title": "HBIC: A Biclustering Algorithm for Heterogeneous Datasets", "authors": ["Ad\u00e1n Jos\u00e9-Garc\u00eda", "Julie Jacques", "Cl\u00e9ment Chauvet", "Vincent Sobanski", "Clarisse Dhaenens"], "abstract": "Biclustering is an unsupervised machine-learning approach aiming to cluster rows and columns simultaneously in a data matrix. Several biclustering algorithms have been proposed for handling numeric datasets. However, real-world data mining problems often involve heterogeneous datasets with mixed attributes. To address this challenge, we introduce a biclustering approach called HBIC, capable of discovering meaningful biclusters in complex heterogeneous data, including numeric, binary, and categorical data. The approach comprises two stages: bicluster generation and bicluster model selection. In the initial stage, several candidate biclusters are generated iteratively by adding and removing rows and columns based on the frequency of values in the original matrix. In the second stage, we introduce two approaches for selecting the most suitable biclusters by considering their size and homogeneity. Through a series of experiments, we investigated the suitability of our approach on a synthetic benchmark and in a biomedical application involving clinical data of systemic sclerosis patients. The evaluation comparing our method to existing approaches demonstrates its ability to discover high-quality biclusters from heterogeneous data. Our biclustering approach is a starting point for heterogeneous bicluster discovery, leading to a better understanding of complex underlying data structures.", "sections": [{"title": "1 Introduction", "content": "Unsupervised machine learning techniques such as clustering have become widely used due to their ability to provide new insights from unlabeled datasets [1]. These algorithms aim to find global patterns (homogeneous groups) from observations based on their coherence found along all attributes [31]. However, the recent big-data phenomenon has massively increased the number and type of observations and attributes, leading to the emergence of high-dimensional and heterogeneous datasets [10, 35]. For instance, in biomedicine, electronic health record (EHR)-based research using unsupervised learning is central to fulfilling the vision of personalized medicine [19]. In these heterogeneous data problems, specific observations are usually locally correlated on a subset of attributes. In this context, biclustering is an unsupervised learning technique that simultaneously groups observations and attributes, forming biclusters, leading to unique conceptual benefits [21, 18]: (i) can unravel local patterns, i.e., observations meaningfully correlated on a subset of attributes; (ii) provide flexibility in the definition of different types of biclusters; (iii) allow overlapping biclusters, such that one observation can belong to more than one bicluster; and (iv) help alleviate the curse of dimensionality problem, as the space volume grows exponentially with the number of observations.\nBiclustering is an active research field with application to diverse problems, such as clinical and biological [35, 33], gene expression and microarray data [22], text mining [4], and time series analysis [5]. During the last 20 years, many biclustering algorithms and tools have been developed to provide insights into large high-dimensional datasets [21, 35, 18]. However, current biclustering algorithms mainly operate on numeric attributes, which poses challenges when dealing with highly heterogeneous datasets containing multiple data types, such as binary, numeric, and categorical attributes. In practice, feature transformation is a commonly used technique [34, 3]; unfortunately, this early transformation step largely pre-determines the results and can cause information loss, as the relative importance of different attributes is not considered [1].\nThis paper proposes a biclustering approach capable of discovering meaningful biclusters from datasets having binary, numeric, and categorical attributes. The algorithm consists of two main stages, bicluster generation, and model selection. In the generation stage, a heuristic-based iterative process is used to generate multiple candidate biclusters, which may consist of different data types. Then, a fitness function is proposed based on intra-cluster variance to measure the quality of obtained biclusters with mixed-type attributes. In the second stage, we propose two strategies for selecting the most meaningful biclusters generated in the first stage. The resulting biclustering approach not only has the capability to automatically identify the number of biclusters, but can also leverage domain-specific knowledge if it is available.\nThe organization of this paper is as follows. Section 2 presents background concepts and discusses related works. Our proposed biclustering approach is described in Section 3. Section 4 provides the experimental setup, benchmarks, and performance assessment. The results and main findings are discussed in Section 5. Finally, Section 6 concludes and presents future research directions."}, {"title": "2 Background and related work", "content": "This section presents some basic concepts, summarizes the related works, and discusses relevant literature."}, {"title": "2.1 Biclusters", "content": "A data matrix X is defined by N observations (rows), $R = \\{1,..., N\\}$, and M attributes (columns), $C = \\{1,..., M\\}$. A bicluster B = (I, J) is a subset of rows $I\\subseteq R$ and columns $J\\subseteq C$ of the original matrix, and $b_{ij}$ represents a value in the bicluster corresponding to the i-th row and the j-th column.\nThere exist several definitions of biclusters, each specifically tailored to different applications and data characteristics [21, 7]. We can identify four types of biclusters based on their assumed correlations between the bicluster values:\n1. Constant biclusters: an overall-constant bicluster has all the values equal to a constant \u03c0, i.e. $b_{ij} = \\pi$; constant-rows has all values equal per row, i.e. $b_{ij} = a_i$; and constant-columns has all values equal per column, i.e. $b_{ij} = A_j$.\n2. Coherent biclusters: an additive coherent or shifting bicluster is defined as $b_{ij} = \\pi + a_i + \\lambda_j$, and a multiplicative coherent or scaling bicluster is defined as $b_{ij} = \\pi \\times a_i \\times \\lambda_j$.\n3. Order preserving biclusters: the rows or columns in the bicluster represent general trends in data (such as up-down-up) rather than explaining well-defined values.\n4. Composed biclusters: is a bicluster combining different types of biclusters. For instance, a bicluster with two sub-biclusters: an order preserving bicluster with numeric attributes and one constant-columns bicluster with categorical attributes.\nIn the previous definitions, $a_i (1 \\leq i \\leq |I|)$ and $\\lambda_j (1 \\leq j \\leq |J|)$ refers to constant values used in the different bicluster definitions."}, {"title": "2.2 Biclustering algorithms", "content": "In real-world problems, biclustering algorithms are used to identify multiple biclusters, generating a biclustering solution, denoted as $B = \\{B_1,..., B_q\\}$, where q represents the number of biclusters. The relationships between the biclusters covering the input data matrix are determined based on two criteria: exclusivity and exhaustivity [21, 18]. Exclusivity indicates that a row or column belongs only to one of the q bicluster in the biclustering solution. In contrast, the exhaustivity criterion specifies that every row and column belongs to at least one bicluster. Exhaustivity refers to covering the input matrix, while exclusivity is associated with biclustering all rows and columns in X. Therefore, the bicluster types to be discovered will depend on both the problem domain and the data types involved [23].\nBiclustering data analysis has gained much interest since the seminal work of Cheng and Church to investigate gene expression data [8]. Nowadays, many biclustering algorithms exist in the literature, particularly for biological data such as gene expression [21, 25, 18, 17]. In the more general case, these biclustering algorithms can be categorized into heuristic-based and metaheuristic-based approaches. The reader is referred to the following surveys for a detailed classification of biclustering algorithms and their applications [21, 7, 18]."}, {"title": "2.3 Biclustering algorithm for mixed-type data", "content": "We discuss existing biclustering algorithms from two crucial design aspects: their capacity to address multiple heterogeneous attributes and their ability to determine the number of biclusters automatically. To the best of our knowledge, the works of Vandromme et al. [32, 33] and Selosse et al. [28] are the only biclustering approaches specifically designed to extract biclusters from heterogeneous datasets.\nVandromme et al. [32] proposed a biclustering algorithm called HBC to extract biclusters with constant values in the columns from medical data matrices, where the attributes can be numerical, symbolic, and binary. Subsequently, the HBC algorithm was extended for temporal data [33] (i.e., data from multiple patient visits). Although this approach is representative of the literature, it lacks a comprehensive evaluation against other traditional biclustering algorithms, and the ability of HBC to find other types of biclusters (i.e., overlapping) is not investigated. Selosse et al. [28] presented a model-based biclustering approach for data with different types of attributes. They extended the latent block model (LBM) from numerical data to other data types, such as categorical and binary. In addition, a distribution type was defined for each data type and modeled using a multiple LBM. A major limitation of this model is that the mixed-type attributes cannot be part of the same column cluster, as the model is based on the assumption that the attributes of the same block share the same distribution. Additionally, the multi-parameter configuration of each distribution used in the model is a complex task that varies for each data subset.\nThe biclustering approaches discussed above either require as input the maximum number of biclusters to be fixed in advance [8, 20], or they keep this parameter variable and generate a certain number of biclusters (usually several) without making an informed selection of the most representative ones and leave this task only to the decision maker or data expert [32, 33, 28]. The former approach is only sometimes realistic, as the most suitable number of biclusters is unknown in real-world applications. The latter is more generally applicable but completely disregards any insight or domain expertise available. To address these difficulties, our approach outlined in Section 3 can extract various types of biclusters from heterogeneous data and automatically identify the most representative."}, {"title": "3 Proposed algorithm: HBIC", "content": "This section introduces our proposed approach HBIC (Heterogeneous BIClustering) to address datasets with mixed-type attributes such as binary, numeric, and categorical. First, HBIC generates candidate biclusters iteratively from a discretized search space. Then, HBIC selects meaningful biclusters by ranking them according to their heterogeneous intra-bicluster variance, measured in the original heterogeneous data space.\nThe overall framework, principal components, complexity analysis, and source code availability of our proposed algorithm HBIC, are discussed in the following subsections."}, {"title": "3.1 Overall framework", "content": "The overall functioning of HBIC is outlined in Algorithm 1. In the first stage, the numeric attributes of the input data matrix are discretized; then, a heuristic search strategy generates a set of candidate biclusters, and subsequently, all repeated biclusters are removed. Then, in the second stage, the problem of determining the most representative biclusters is addressed (see Section 3.3).\nHBIC operates over a discretized search space to generate candidates during the first stage. In HBIC, the numeric attributes are discretized, simplifying the bicluster search since these attributes will have a finite number of possible values (a limited number of possible states). Since HBIC is an unsupervised learning algorithm, only unsupervised data discretization methods can be used in the Discretization step (line 1 in Algorithm 1). HBIC uses a basic, standard equal-width binning method, which divides the data domain into equal-size intervals (or bins); however, note that other discretization strategies, such as frequency-based and pattern-mining techniques [11], can be used at this step. Data discretization has proven to be a helpful strategy in various data mining tasks and machine learning algorithm applications [33]. An analysis of the impact of the discretization step is included in the Supplementary Material.\nThe bicluster generation process is based on a greedy constructive heuristic (comprising lines 2-15 in Algorithm 1). The method starts by creating an initial bicluster with one column and all rows containing the same value for this column. Subsequently, at each step, it adds the column with the highest number of identical values for the rows of the current bicluster and deletes the rows whose values are different (see Algorithm 2). This process is repeated for each distinct value of each column. The process terminates when all columns have been processed. During this stage, the discretized matrix, M, is used; therefore, the method operates on a finite number of possible values for each attribute. This adaptation simplifies the counting of identical values while generating candidate biclusters. At the end of the first stage, the UniqueBiclusters step removes the repeated biclusters since HBIC might rediscover the same bicluster when analyzing a different column of the bicluster in the data matrix."}, {"title": "3.2 Bicluster quality", "content": "The initial phase of the HBIC algorithm generates multiple candidate biclusters, B, for a given heterogeneous data problem. In the second phase, the quality of the candidate biclusters is computed in the original heterogeneous data space, enabling the selection of the most representative biclusters. In the literature, there is a range of metrics defined for numeric biclusters [26], including the well-known minimum squared error and the average correlation function. To measure the quality of heterogeneous biclusters, we propose the heterogeneous intra-bicluster variance (HIV), which is comparable to the definitions used in HBC [32].\nLet X be the input data matrix with N rows and M columns, and let B = (I, J) a bicluster expressed as a tuple of two nonempty sets. Thus, HIV evaluates the intra-bicluster homogeneity in the presence of heterogeneous attributes as follows:\n$HIV (B) = ANV (I, J_{num}) + ACF (I, J_{cat})$         (1)\nwhere ANV (,) denotes the average numeric variance for the numeric attributes such that $J_{num} \\subseteq J$, and ACF (,) represents the average categorical frequency for the categorical and binary attributes such that $J_{cat} \\subseteq J$. Firstly, the ANV function is given by\n$ANV (I, J_{num}) = \\frac{1}{|J_{num}|} \\sum_{j \\in J_{num}}  \\frac{var (b_{Ij})}{var (x_{Rj})}$        (2)\nwhere $|J_{num}|$ is the number of numeric attributes; $var(b_{Ij})$ denotes the variance of the numeric values for the j-th column in B; and $var(x_{Rj})$ denotes the variance of the numeric values for the j-th column in X. Secondly, the ACF function estimates the extent to which a discrete value is correctly expressed to its biclusters. This is evaluated by counting the most frequent value in a certain discretized attribute (e.g., categorical or binary). The ACF is computed as\n$ACF (I, J_{cat}) =  \\frac{1}{|J_{cat}|} \\sum_{j \\in J_{cat}} (\\frac{freq (b_{Ij})}{|I|})$      (3)\nwhere $|J_{cat}|$ indicates the number of categorical and binary attributes, |I| is number of rows, and $freq (b_{Ij})$ denotes the number of the most frequent value for the j-th attribute.\nThe above expressions describe a quality measure for heterogeneous biclusters with numeric, binary, and categorical data types. However, if other data types exist in X, this function will require further adaptations and analysis. HIV is an optimization function that takes values in the range [0, inf] and has to be minimized, i.e., values close to zero indicate more homogeneous biclusters."}, {"title": "3.3 Model selection of biclusters", "content": "The selection of the number of clusters is an important problem in unsupervised machine learning [31, 15]. In bicluster analysis, biclustering algorithms usually require defining this parameter a priori [8, 21, 14]; however, in real-world applications, this parameter is usually unknown [14, 2]. In this work, we propose two selection strategies for automatically determining the number of biclusters:"}, {"title": "3.4 Complexity analysis", "content": "The main operation of the HBIC heuristic described above is AddColumn (see Algorithm 2), where the best column is added to a bicluster, and rows are removed to keep this bicluster perfectly homogeneous. The cost of this procedure is to search which rows of a column to be added are present in the current bicluster. This translates into O(r x log(r)) worst-case complexity since looking for an element in a list has O(log(r)) complexity, where r is the number of rows in the data matrix. At each step, the best column is added, which means all possible columns must be added to determine the optimal choice. Therefore, O($c^2$) AddColumn operations are performed during the construction of each bicluster, where c is the number of columns. This whole process is performed for each column and each possible value of this column as a starting point; therefore, O(c x v) iterations are performed, where v is the number of possible values for an attribute. This leads to a worst-case time complexity of O($r x log(r) x c^3 x v$). However, fewer than O($c^2$) AddColumn operations are performed in practice, as the method usually reaches the stopping criterion after adding a few dozen columns (even on heterogeneous real-world datasets). Finally, regarding both model selection strategies presented, the most time-consuming step is the computation of HIV, which is of the order of O(\u03b2 \u00d7 r \u00d7 c), where \u03b2 is the number of candidate biclusters. This complexity analysis, along with experimental observations, indicates that the proposed method scales mainly with the number of columns. The total number of rows; however, does not significantly impact the execution time."}, {"title": "3.5 Source code availability", "content": "The source code of HBIC (written in Matlab and Python) and the datasets used in our experiments are available to the research community in the following repositories: https://github.com/adanjoga/hbic and https://github.com/clementchauvet/py-hbic."}, {"title": "4 Experimental setting", "content": ""}, {"title": "4.1 Datasets", "content": "Heterogeneous synthetic datasets The following collection of heterogeneous synthetic datasets was generated using the G-bic tool recently proposed by Castanho et al. [6], which is available through the repository: https://github.com/jplobo1313/G-Bic/. This collection comprises 21 data configurations organized into five categories. Each category aims to evaluate a unique aspect of datasets in medical applications. In the following descriptions, unless otherwise noted, each data set contains 1000 rows and 500 columns, with five planted biclusters, where 50% of the attributes are numeric and 50% categorical. The five categories of datasets are:\n\u2022 Heterogeneity level (HL). The percentage of categorical attributes in the dataset was modified as follows: HL \u2208 {0, 25, 50, 75, 100}, where HL=0% indicates a complete numeric dataset, and HL=100% denotes a categorical dataset.\n\u2022 Number of biclusters (NB). The number of planted bicluster, \u03b2*, was modified as: \u03b2* \u2208 {3,5,8,10}.\n\u2022 Size of biclusters (SB). The bicluster size (|I|\u00d7|J|) was modified as follows: SB \u2208 {(25\u00d725), (50\u00d750), (75\u00d775), (100\u00d7100). For each configuration, three biclusters were planted: one numeric, one categorical, and one mixed.\n\u2022 Size of datasets (SM). The dataset size (N \u00d7 M) was modified as: \u2208 {(500 \u00d7 250), (1000 \u00d7 500), (1500 \u00d7 750), (2000 \u00d7 1000).\n\u2022 Noise level (NL). The percentage of noise level was modified both in the dataset and in the planted biclusters: NL \u2208 {5, 10, 15, 20}.\nThis collection contains a total of 315 heterogeneous data problems, as for each of the 21 data configurations, 15 datasets were generated. The generation of the datasets and biclusters follows a uniform distribution, where the numerical variables are in the range of [-10, +10]. The categorical attributes follow the alphabet {\"a\", \u201cb\u201d, \"c\", \"d\", \u201ce\", \u201cf\u201d, \u201cg\u201d, \u201ch\u201d, \u201ci\", \u201cj\"}. For each data configuration, different biclusters were planted, including numeric, categorical, and mixed. These problems are detailed in the Supplementary Material.\nHeterogeneous data from systemic sclerosis patients The proposed biclustering algorithm is designed to find biclusters in highly heterogeneous data problems. Medical databases inherently contain clinical information from different data types. In this study, we considered the systemic sclerosis (SSC) database of the Centre Hospitalier Universitaire de Lille (herein referred to as CHUL\u00b9 database). SSc is the most severe systemic autoimmune disease with the highest morbidity and mortality [30] and is characterized by significant heterogeneity with varying degrees of organ involvement among patients. This complicates the search for therapeutic targets, the design of clinical trials, and the management of patients. The current classification of patients is based on the extent of skin"}, {"title": "5 Experimental results", "content": "This section discusses the results of a series of experiments conducted to investigate the performance of the proposed biclustering algorithm for heterogeneous datasets, HBIC."}, {"title": "5.1 Results on heterogeneous synthetic datasets", "content": "This section presents a comparative analysis of the results obtained by HBIC and its performance in comparison to two reference biclustering algorithms (see Section 2.2). The results of this comparison are summarized in Fig. 1, while Table 1 presents detailed results for each problem configuration and category, accompanied by a statistical significance analysis.\nIn the case of heterogeneous synthetic datasets, the HBICall algorithm obtained the best performance in terms of the metric recovery, i.e., the capacity to find the true planted biclusters, where HBICpar obtained statistically similar results. Concerning the relevance metric, i.e., the general ability to find biclusters as good as the planted ones, the HBICdis performed better than its counterparts. Regarding the bicluster error, which is a compromise between the recovery and relevance metrics, the HBICdis version also performed the best, with HBICpar and HBICall achieving statistically similar performance. Overall, we observed that HBIC versions performed better than the CCA and LAS algorithms. The Pareto-based selection strategy performed slightly better than the distance-based.\nRegarding the performance of the algorithms on the five datasets categories (see Table1), we observe that HBIC performs remarkably well on heterogeneous datasets (heterogeneity level, HL), in data with different numbers and sizes of biclusters (NB and SM), and on datasets of different sizes (NL). However, HBIC and the benchmark algorithms present difficulties if some noise exists in the input data."}, {"title": "5.2 Application to systemic sclerosis", "content": "A primary study of SSc patients suggested the existence of two (general perspective) and six (specific perspective) patient clusters [30, 16]. It showed that there is no clear dichotomy between SSc patients with diffuse and limited cutaneous forms, and that other homogeneous groups can be found beyond skin involvement. However, these groups may not adequately represent the homogeneity of the patients because a generic clustering algorithm was used that transformed categorical variables into numerical values.\nWe analyzed the CHUL database of SSc patients (see Section 4.1) using CCA, LAS, and HBIC biclustering algorithms. On the one hand, a total of 39 biclusters were generated using the proposed HBIC. These heterogeneous biclusters are detailed in the Supplementary Material. The variability and differences of the obtained biclusters are illustrated in Fig. 2 in terms of four well-known metrics in SSc. A summary of 10 relevant biclusters with key clinical features is listed in Table 2. On the other hand, the CCA algorithm generated 11 biclusters, while LAS found only one bicluster. It should be noted that for CCA and LAS, the categorical variables were transformed into binary variables using the one-hot-encoding technique. Details of these biclusters are reported in Table 3.\nAccording to the results obtained by the three algorithms, CCA, LAS, and HBIC, it can be observed that HBIC obtained a greater diversity of biclusters of different sizes and homogeneity. It was observed that HBIC obtained an almost complete coverage of the entire clinical dataset, 95% of the patients and 100% of the clinical variables. While CCA obtained a patient coverage of 43%, and its biclusters tended to cover the same region of the clinical data matrix. LAS performed very poorly on the medical data, generating only one valid bicluster (see Table 3, bicluster B*).\nIn general, our preliminary results using the HBIC on the CHUL suggest that biclustering algorithms on heterogeneous real-world datasets can provide valuable information on clinical subsetting of a disease not sufficiently explored by classical clustering methods."}, {"title": "6 Conclusions", "content": "Our work proposes a biclustering approach based on a greedy heuristic for heterogeneous datasets. The resulting algorithm, HBIC, can find meaningful biclusters from heterogeneous data and automatically determines the number of biclusters when this parameter is unknown. Experiments with various datasets of different characteristics and complexities underline the robustness of the proposed approach to solving complex heterogeneous data problems. Overall, our algorithm reports a competitive performance to existing biclustering algorithms. Our approach excels in identifying meaningful biclusters from medical datasets with multiple data types.\nOur analysis focused on heterogeneous data with numeric, binary, and categorical attributes. In this scenario, HBIC can retrieve constant and composed types of biclusters. However, further analysis is needed to extend this capability to other bicluster patterns and data types (i.e., longitudinal and text data). Our algorithm's design prioritized adopting straightforward yet effective techniques; nevertheless, HBIC can incorporate more sophisticated methods. For example, the selection strategies can be modified to improve the algorithm's performance. Furthermore, extending the applicability of HBIC to data with missing values is an interesting path for future investigation."}, {"title": "A Supplementary Material", "content": ""}, {"title": "A.1 Impact of the Discretization Strategy in HBIC", "content": "This section aims to analyze the impact of the discretization function and its parameter nbins in our proposed HBIC algorithm. The details of the DISCRETIZATION() function are described in the main paper, and its implementation in HBIC is indicated in Algorithm 1, line 1. The parameter nbins in the equal-width binning discretization helps to transform numeric attributes into discrete categories. This strategy helps to simplify the analysis of heterogeneous data; however, choosing the right intervals (nbins) can be challenging, as different values lead to different performances of the HBIC algorithm. In this section, we study the impact of the nbins parameter in the HBIC algorithm when dealing with numerical datasets.\nIn our study, we considered the numerical data problems described in Section 2, and we varied the value of the parameter nbins in the discretization function as nbins = {2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20}. Note that the nbins parameter only affects datasets with numeric attributes, since the discretization step is unnecessary for categorical and binary variables. For this reason, this study of the nbins parameter was performed on datasets where all attributes are numeric and represent the worst case for the HBIC algorithm, where in the best case, all attributes in the dataset are categorical or binary.\nAs a first step to visually understand the effect of the nbins parameter on numeric datasets, Figure 3 exemplifies the visualization of a numerical matrix with five biclusters and different discretization levels, nbins = {2, 5, 10, 15}. We can observe that the higher the level of discretization, the closer to the original data matrix distribution. Therefore, this indicates that a higher value of the nbins parameter helps better to approximate the frequency distribution of the original data matrix; however, as the value of nbins increases, the computational complexity of the HBIC algorithm will also increase.\nThe results of the impact of the discretization strategy in the HBIC algorithm are summarized in Figures 4 and 5. Overall, from Fig. 4, we can observe that as we increase the parameter nbins the biclustering performance in terms of the metrics recovery and relevance also tends to increase. On the one hand, for the recovery metric (the ability of the algorithm to retrieve exactly the inserted biclusters), acceptable performance close to the unity is obtained from nbins = 5, while for lower values the performance starts very low and increases monotonically until it becomes stable. On the other hand, regarding the relevance metric (the ability to recover good candidate biclusters, i.e., compact with low variance), acceptable performance close to the unity is obtained from nbins = 6. These results suggest that for the numeric datasets considered a value superior or equal to six will lead to robust biclustering performance."}, {"title": "A.2 Supplementary Results", "content": "Table 4 summarizes the 39 characteristics of the biclusters generated by HBIC when using the CHUL database. This medical dataset comprises 530 observations and 40 attributes (22 binary, 16 numeric, and two categorical). The proposed algorithm obtained several biclusters with different degrees of homogeneity (where a bicluster can contain attributes of different types) and of varying sizes. Also, these biclusters cover different regions of the input data matrix. Finally, although these subclusters contain interesting properties, an expert analysis is needed in order to derive medical interpretations of their potential impact on the SSc disease."}]}