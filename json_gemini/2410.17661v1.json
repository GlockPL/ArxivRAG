{"title": "PETAH: Parameter Efficient Task Adaptation for Hybrid Transformers in a resource-limited Context", "authors": ["Maximilian Augustin", "Syed Shakib Sarwar", "Mostafa Elhoushi", "Sai Qian Zhang", "Yuecheng Li", "Barbara De Salvo"], "abstract": "Following their success in natural language processing (NLP), there has been a shift towards transformer models in computer vision. While transformers perform well and offer promising multi-tasking performance, due to their high compute requirements, many resource-constrained applications still rely on convolutional or hybrid models that combine the benefits of convolution and attention layers and achieve the best results in the sub 100M parameter range. Simultaneously, task-adaptation techniques that allow for the use of one shared transformer backbone for multiple downstream tasks, resulting in great storage savings at negligible cost in performance, have not yet been adopted for hybrid transformers. In this work, we investigate how to achieve the best task-adaptation performance and introduce PETAH: Parameter Efficient Task Adaptation for Hybrid Transformers. We further combine PETAH adaptation with pruning to achieve highly performant and storage-friendly models for multi-tasking. In our extensive evaluation on classification and other vision tasks, we demonstrate that our PETAH-adapted hybrid models outperform established task-adaptation techniques for ViTs while requiring fewer parameters and being more efficient on mobile hardware.", "sections": [{"title": "Introduction", "content": "In recent years, transformers [87] have dominated many natural language and computer vision applications, including classification [19, 74, 22, 21], semantic segmentation [13] and object detection [4, 109, 58]. Recently, [59] have shown promising results for transferring large-scale transformer models to multiple downstream applications without requiring extensive retraining of the entire transformer model. Instead, [59] use their DinoV2 model as a fixed feature extractor and achieve good downstream performance by training a task-specific head. While this seems promising for low-resource applications, these transfer capabilities have only been demonstrated for large-scale vision transformer (ViT) models [87] with hundreds of millions of parameters that are trained on massive datasets. In natural language processing (NLP), task-adaptation techniques such as Adapter [28] or Low-Rank Adapatation (LoRA) and its variants [30, 20, 84, 7] have shown great success at adapting massive large language models to new tasks in a parameter efficient way and help with multi-tasking [93, 64]."}, {"title": "Related Work", "content": "Vision architectures: For nearly a decade after the first AlexNet paper [36], computer vision research has been focused on convolutional networks such as ResNets [25], MobileNets [69, 29] or EfficientNets [79]. Transformers were initially proposed for handling long text sequences [87] but were quickly adopted to the vision domain, most prominently in the form of the vision transformer (ViT) [19]. Due to their architecture, ViTs have fewer inductive biases and thus necessitated the"}, {"title": "Preliminaries", "content": "To fairly evaluate different task-adaptation methods for hybrid transformers and compare them to task-adaptation on vision transformers, we have to select a hybrid architecture and a pre-training framework that we can use to train all models. For most of our experiments, we will use the EfficientFormer (EF) [44] in both the L3 variant with 31M and the L7 variant with 80M parameters. Unlike some of the later hybrid variants [43, 86, 94], the EF contains transformer blocks that are very similar to that of a standard ViT which makes it easy to adopt NLP task-adaptation techniques and compare to a ViT. In particular, the EF consists of a convolutional stem and three stages of 4D MetaBlocks, that while inspired by the transformer design, use convolutions to operate on a batch of 3D feature maps (C\u00d7 H \u00d7 W with channels C', width W and height H). In the final stage, the 3D feature maps are flattened into a 2D sequence of size (HW) \u00d7 C which is processed by multiple 3D MetaBlocks that contain a standard multi-head attention layer with an additional linear projection and a two-layer MLP. In contrast, the standard ViT only consists of a patchify layer and multiple transformer blocks, each containing a multi-head attention module and a two-layer MLP. Thus the fourth stage of the EF model strongly resembles the standard ViT architecture, however, the first three stages are built using faster and more parameter-efficient convolutional layers. While the EF is not the most parameter-efficient hybrid model [43, 86], it is much more efficient in terms of number of floating point operations and on-device latency than ViTs."}, {"title": "Task Adaptation for Hybrid Transformers", "content": "We begin this section by briefly recapping some existing methods for transformers which we consider to be baselines for our work. We note that for transformers, it is common [27, 30, 81] to only adapt the attention modules without adapting the MLP modules. For this section, unless otherwise stated, we assume that we want to adapt a linear transformation f(x) = Wox + b parameterized by the weight matrix Wo \u2208 RP\u00d7q and bias vector b \u2208 RP where p and q are the output and input dimensions.\nLORA - Low-Rank Adaptation: LoRA [30] is one of the most popular parameter-efficient methods introduced for the adaptation of large language models. Given a pre-defined rank r the modified forward pass of the linear transformation is defined via the weight update AW which is given as the outer product of two low-rank matrices A \u2208 R\u2033\u00d79 and B \u2208 Rp\u00d7r as \u2206W = BA, thus:\nImportantly, since the updated weight matrix Wo + BA can be computed while loading the model weights and adapter parameters, LoRA does not introduce any computation overhead during inference.\nLORA for Convolutional Layers: Due to their origins in NLP, most PEFT methods target the transformer architecture and its attention layers. A less known fact is that it is also possible to adapt convolutional layers using such decompositions [37, 63]. Assume we are given a convolutional layer with kernel size k, p output- and q input channels parameterized by the weight tensor W4D \u2208 [[Rp\u00d7q\u00d7k\u00d7k and bias b \u2208 RP. We use conv2D(, ) to denote the function that applies a 2D convolution specified by the kernel in the second argument to the input given as the first argument."}, {"title": "How to adapt a hybrid transformer", "content": "We can now investigate the problem of adapting hybrid transformers. For standard language and vision transformers, it is common to only adapt the linear transformations in attention layers [30, 27]. However, in the case of the EfficientFormer and many other hybrid architectures that contain convolutional stages followed by attention-based stages [18, 86, 43], such a procedure would only adapt the last part of the network and keep a large part of the signal path unchanged. It is thus questionable if such an adaptation is flexible enough to allow the model to adapt to various computer vision tasks or if it is beneficial to also adapt the convolutional layers. For this experiment, we adapt the EF L7 backbone to three fine-grained classification datasets: FGVC-Aircraft [53], Food101 [1] and the Describable Textures Dataset (DTD) [14]. For each task, we add a linear head on top of the frozen backbone and use PEFT to adapt specific modules of the backbone. As baselines, we compare to linear probing, where we keep the backbone completely frozen and only fit the linear head on top of it. We also use LoRA with varying ranks where we adapt either only the attention weights or the attention weights and the MLP layers in the transformer block. Note that when we adapt the attention weights, we always refer to the WQ, WK, Wv weight matrices and the surrounding projection layers in the EfficientFormer. We extend attention LoRA with convolutional LoRA where we adapt all the convolutional layers in the stem and first three stages of the model using the approach outlined in\nEq. (2), also see Fig. 1. We tune the learning rate and weight decay for each method using a grid search on a separate validation set and report the test accuracy for the best-performing configuration. Since Food101 does not have an extra validation split, we create one from the train set by separating 50 examples for each class. Results can be found in Table 2.\nSeveral interesting findings differ from the PEFT literature for transformers. First, we note that for the EF L7, adapting the MLP weights does increase downstream task performance, which was found to not be the case for ViTs [27]. However, while such adaptation can be beneficial to performance, due to the high dimensionality of the MLP matrices, this adaptation significantly increases the"}, {"title": "PETAH: Parameter Efficient Task Adaptation for Hybrid Transformers", "content": "We now introduce PETAH, our PEFT framework for hybrid transformers. It uses standard LORA adaptation of the attention layers combined with a low-rank convolutional adaptation for all convo-lutional layers. In particular, we define PETAH-n as adapting the linear layers inside the attention module with LoRA of rank r = 8 and all convolutions with convolutional LoRA of rank n. Any other fully connected layers outside of the attention layer are not modified. This setup strikes a good balance between performance and parameter efficiency. Our method exploits the fact that, unlike MLP modules, convolutional layers can be adapted in a low-rank fashion (rc = 1 or 2) and significantly boost performance with a relatively small amount of additional parameters."}, {"title": "Experiments", "content": "In the following Section, we will evaluate PETAH on several vision benchmarks, including clas-sification, object detection, and semantic segmentation, and compare it to other task-adaptation techniques. We will also compare different model sizes and architectures and, in particular, evaluate the performance of PETAH for hybrid models vs adapting ViTs."}, {"title": "Classification", "content": "For fine-grained classification, we again use the Aircraft, DTD, and Food101 datasets and additionally evaluate on CUB200 [88], Oxford-IIIT Pets [61] and Stanford Cars [35]. We use the validation split to find the learning rate and weight decay with each method's best performance to ensure that hyperparameter choices do not cause our findings (Appendix A). Since CUB, Pets, and Cars do not have a separate validation split and are relatively small, we reuse the best-performing parameters from Aircraft and DTD since they are most similar in size. We use standard data augmentation including random crops and horizontal flipping. After hyperparameter selection, we run each experiment with"}, {"title": "Object Detection and Semantic Segmentation", "content": "While other works on PEFT either focus on LLMs, classification [27] or generation [97], our goal is to demonstrate that hybrid backbones with PETAH can handle a wide range of computer vision applications. We thus evaluate common detection and segmentation benchmarks. For object detection and instance segmentation, we follow a similar setup to [44, 40] and use a Cascade R-CNN [3] with a feature pyramid network [45] on COCO [46] in 640 \u00d7 480. For semantic segmentation, we use the challenging ADE20K [107] dataset with 20K training images covering 150 classes. We use our pre-trained backbone and fit a Semantic FPN [33] on top in combination with different PEFT methods. One major advantage of hybrid models over ViTs is their hierarchical feature representation that allows for multi-scale representations used by many classic computer vision algorithms [45, 13]. Since ViTs do not have hierarchical feature maps at different resolutions and are typically not used for dense prediction tasks without large adapters [12, 42], we exclude them from this experiment. Results for the EF L7 can be found in Table 6. While full fine-tuning is overall the best performing method, our PETAH approach clearly outperforms LoRA and attention fine-tuning and is a close second to standard full fine-tuning, which requires more than 170 times as many parameters."}, {"title": "Conclusion and Limitations", "content": "In this work, we introduce PETAH, a PEFT framework for hybrid transformers which modifies not only the attention layers but also the convolutional layers and clearly outperforms baseline PEFT approaches. The resulting adapted models can beat ViTs of comparable size while being more compute-efficient. In addition, we demonstrate that for sparse hybrid backones, PETAH adaptation outperforms even full fine-tuning and can recover part of the performance loss caused by pruning. We also demonstrate the implicit regularization effects of PEFT methods on vision tasks by comparing them to explicit regularisation techniques such as dropout or data augmentation. Due to their hierarchical feature maps, hybrid backbones can easily be adapted to non-classification tasks such as detection or segmentation, which was missing from previous works [27]. In terms of limitations, we note that we restrict most of our analysis to the EfficientFormer backbone since the backbone and resulting PEFT adaptations are comparable in terms of the number of parameters. While it is possible to manually extend PETAH to other hybrid backbones and more efficient PEFT factorizations [20, 97], ideally one should combine convolutional adaptation with a random-search-based approach like Glora [7] to automatically find the ideal variant without the need for a manual configuration."}], "equations": ["Wox + \u2206Wx + b = Wox + BAx + b = (Wo + BA)x + b\\qquad (1)", "conv2D(x, W4D) + conv2D(conv2D(x, A4D), B4D) + b.\\qquad (2)"]}