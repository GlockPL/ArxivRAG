{"title": "ALPI: Auto-Labeller with Proxy Injection for 3D Object Detection using 2D Labels Only", "authors": ["Saad Lahlali", "Nicolas Granger", "Herv\u00e9 Le Borgne", "Quoc-Cuong Pham"], "abstract": "3D object detection plays a crucial role in various applications such as autonomous vehicles, robotics and augmented reality. However, training 3D detectors requires a costly precise annotation, which is a hindrance to scaling annotation to large datasets. To address this challenge, we propose a weakly supervised 3D annotator that relies solely on 2D bounding box annotations from images, along with size priors. One major problem is that supervising a 3D detection model using only 2D boxes is not reliable due to ambiguities between different 3D poses and their identical 2D projection. We introduce a simple yet effective and generic solution: we build 3D proxy objects with annotations by construction and add them to the training dataset. Our method requires only size priors to adapt to new classes. To better align 2D supervision with 3D detection, our method ensures depth invariance with a novel expression of the 2D losses. Finally, to detect more challenging instances, our annotator follows an offline pseudo-labelling scheme which gradually improves its 3D pseudo-labels. Extensive experiments on the KITTI dataset demonstrate that our method not only performs on-par or above previous works on the Car category, but also achieves performance close to fully supervised methods on more challenging classes. We further demonstrate the effectiveness and robustness of our method by being the first to experiment on the more challenging nuScenes dataset. We additionally propose a setting where weak labels are obtained from a 2D detector pre-trained on MS-COCO instead of human annotations.", "sections": [{"title": "1. Introduction", "content": "In various fields such as autonomous vehicles, robotics, and augmented reality, achieving accurate 3D scene understanding stands as a critical task. This necessitates leveraging a diverse array of sensors, including cameras and LiDAR. Deep learning methods processing images [9, 15, 25] or point clouds [8,20,23] can extract valuable 3D spatial information. Their effectiveness largely depends on the availability of large-scale, diverse and precisely annotated data. For 3D object detection annotation, the process is nevertheless tedious, especially for occluded objects, with an average annotation time of 2 minutes per object [14].\nIn light of these challenges, weak supervision seeks to leverage annotations that do not fully correspond to the targeted task, but that are substantially cheaper to label. In the case of the 3D object detection task, instead of annotating the complete 3D box, a cheaper annotation can be the center of the 3D box [32], the center in bird-eye-view [14] or a 2D box annotated from a synchronized camera view [12, 13, 16, 27, 30]. These weakly supervised models are often used as 3D pseudo-labelers providing 3D box supervision for off-the-shelf fully supervised 3D detectors.\nWeak supervision led to considerable progress in 2D-related tasks like 2D object detection [7,28,33], but remains at a relatively nascent stage for 3D-related tasks. Among existing 3D object detection approaches supervised with weak annotations (Table 1), one can identify two major limitations:\n\u2022 Most of existing approaches [12-14, 16, 32] are actually semi-weakly supervised since they require a small amount of 3D box annotations;\n\u2022 Approaches that do not require any 3D box annotation [27, 30] are specific to the class Car since they rely on heuristics that make them tedious or difficult to adapt to other classes.\nWe propose the first approach to address both limitations by being multi-class and requiring no 3D box annotation at all. More specifically, we supervise the model solely with readily available class size priors and 2D bounding boxes around objects in the camera views.\nTo be able to detect different object classes, we adopt a learning-based approach, leading to easier and better generalization across various object classes. Nevertheless, the"}, {"title": "2. Related Work", "content": "Weak supervision yields substantial practical benefits when manual annotation is costly. In the context of 3D object detection, several annotation paradigms have been proposed.\nSemi-weakly supervised 3D object detection methods adopt a mixed-annotation strategy where a small portion of the train set is 3D box annotated and the rest is weakly labeled. Different types of weak annotations have been proposed in this supervision setting. For instance, MTA [12], CAT [16] and MAP-Gen [13] adopt a semi-weak annotation strategy where 13% of the scenes are fully annotated while the rest are annotated with 2D bounding boxes. While these approaches achieve performance very close to fully supervised models, they focus on only one class and require a non-negligible amount of 3D box annotations. Methods working on more object classes and requiring fewer fully annotated scenes have been proposed but they require weak labels with 3D information. In particular, WS3D [14] used the object's center in bird's eye view and ViT-WSS3D [32] used the object's 3D center as weak annotation. While these weak annotations are appealing due to their valuable 3D information, it is important to note that they require annotating within the point cloud which increases the annotation time compared to 2D box annotation. As mentioned in [14], it is necessary to draw a 2D box on either the image or the point cloud to zoom in for easier and more precise point-annotation in a point cloud.\nAn original paradigm was proposed by Tang et al. [21]: a subset of object classes is fully annotated, while another distinct subset is weakly annotated with 2D boxes. The underlying concept involves extracting valuable 3D information from fully annotated classes to help detecting weakly annotated classes. By supervising weak classes with 2D boxes, the model learns their specific features. Based on the empirical results of this method, we get the intuition that cross-class transfers depends more on size similarity than shape similarity. In a similar vein, our approach leverages cross-class transfer but from synthetic objects, thus requiring no manual annotations.\nWeakly supervised 3D object detection methods work under a more challenging setting where only weak labels are provided. Notably, FGR [27] and GAL [30] opted for 2D bounding boxes as weak annotations to detect Cars in 3D. Both methods rely on the standard pattern shape of Cars in bird's-eye view to handcraft heuristics. Therefore, designing new handcrafted heuristics for novel classes would be time-consuming, especially if these classes are not rigid (Pedestrian, Cyclist). In opposition to these methods, we opt for a learning-based approach for easier and better generalization across diverse object classes.\nWeak annotation paradigm. Several paradigms have been proposed as they offer unique trade-offs between annotation costs and achieved performance. Approaches using few fully annotated scenes are motivated by the inherent challenges encountered when attempting to learn the detection of objects in 3D using only weakly annotated data. While these approaches outperform the ones which use no 3D box annotations, the selection of which scenes are fully annotated can significantly affect performance. This aspect was not investigated in semi-weakly supervised literature; however, Wang et al. [22] observed notable performance discrepancies when training a 3D detector with different small subsets of fully annotated scenes. For this purpose, we set our method to be weakly supervised to lower dependency on the sampling of fully annotated scenes and opt for"}, {"title": "3. Method", "content": "We adopt the typical weak supervision pipeline which trains a 3D annotator model in a weakly supervised fashion (Figure 2 part 2), then pseudo-annotates the dataset with 3D boxes (Figure 2 part 3) and finally trains an off-the shelf model [3, 18, 19] with these pseudo-labels (Figure 2 part 4).\nWorking in a frustum point cloud. By using the 2D box in an image and the point cloud, we can narrow down the search space for an object from the entire scene to a truncated pyramid (Figure 1) referred to as frustum."}, {"title": "3.1. Dataset Construction with Proxy Objects", "content": "Within this frustum, a majority of points usually belong to the object of interest while a small portion is held by the ground, occluding objects and the background. To better isolate the object from its background, the following step is to estimate its depth along the frustum axis.\nObject depth estimation. To determine the approximate depth of an object in a scene, we use the annotated 2D bounding box, the average object height H and the LiDAR/camera projection matrix $P \\in R^{3\\times4}$.\nThe projection equation for the two points in red and purple in Figure 1 can be written as: $8_{1,2}[x, y_{1,2},1]^T = P[X, Y_{1,2}, Z, 1]^T$\nBy approximating the object's height as the average height i.e. $Y_2 - Y_1 = H$, we can estimate the depth of the object $X$ as:\n$X = (K^{-1}8_2 [x, y_2, 1, 1/8_2])_1$ where $8_2 = \\frac{H(P_{22} - Y_1P_{12})}{Y_1 - Y_2}$ and $K = [0 0 0 ; 0 0 0 ; 0 0 0 ; 1]$.\nBackground extraction. We proceed to a coarse segmentation of the object within the frustum in order to extract the background. We design a simple approach which confidently filters out the target object points while potentially excluding some associated with occluding objects, the ground, or background. Points within the frustum are considered part of the background if they fall outside a cylinder centered at a depth $X$ on the optical line and with a radius equal to the average length of the object $L$. This segmentation is not used for supervision but to filter out the object from the background scene which is used in the next part as context for object insertion.\nObject injection in the background. To insert an object in the background scene, a 3D center position is needed. Since we want to substitute the previous object by a new one, we simply define the center as the median value w.r.t. each dimension of the points previously removed. The inserted object can either be a proxy or a pseudo-annotated object. The injection of pseudo-labeled objects is explained in subsection 3.3.\nProxy objects. A proxy object is designed with a cuboid-shaped appearance defined by a 3D center (as defined previously), a 3D size and a heading. The proxy's size is sampled using the size prior mean and standard deviation (part 1 in Figure 2). These statistics are provided in the supplementary materials. Next, we choose a heading. Among 12 possible headings in [0, \u03c0), we select the one that best aligns the projection of the 3D box with the previous 2D box annotation. After positioning the proxy in the scene, points are randomly sampled on the surfaces visible to the LiDAR (Figure 3)."}, {"title": "3.2. 3D Annotator Architecture and Training", "content": "Architecture. We opt for Frustum-ConvNet [26] as architecture for the 3D annotator. This model estimates the 3D box of an object within a frustum. At each training iteration, two types of frustums are fed to the 3D annotator. As shown in Figure 2, in green the first type being frustums with target objects (Cars, Pedestrians and Cyclists) which are supervised in 2D only. The second type, in blue, are frustums with injected object (proxies or pseudo-labeled) which are supervised both in 2D and 3D. The annotator model predicts a Box3D which encompasses the 3D coordinates of the center, sizes, and heading of the object. The 8 corners of this Box3D are projected in 2D and the minimum and maximum w.r.t each dimension produces the Box2D. We minimize a multi-task loss that is the sum of several terms: Classification loss. We apply a focal loss function [10] to the class predictions of the model. Such loss has already been used by [32] for semi-weakly supervised 3D object detection.\nLosses on Box3D. When the frustums contain proxy objects or pseudo-labelled objects that provide a 3D pseudo ground-truth, we apply regression losses on the estimated"}, {"title": "3.3. 3D Pseudo-Annotation of the Dataset", "content": "The 3D annotator is used to annotate the dataset in two cases. First, in order to pseudo-label objects before inject-"}, {"title": "3.4. Towards Zero New Human Annotation", "content": "To reduce annotation costs, we propose using 2D boxes generated by a pretrained 2D detector instead of an Oracle capable of annotating complete 2D boxes. We use the Faster R-CNN detector [17], pre-trained on MS-COCO [11] from the MMDet toolbox [1], to generate these weak labels.\nCompared to Oracle 2D boxes from KITTI, typical 2D detector boxes outline only the visible portion of objects, not the full 3D projection. This difference is particularly significant in occluded scenarios, as shown in Figure 4, where the extrapolated 2D box is green, and the Faster R-CNN box is purple. Consequently, incomplete weak annotations of occluded objects cannot directly train the 3D annotator, necessitating method adaptation.\nAt iteration 0, we refine the training set by excluding occluded objects with incomplete 2D boxes, using a 2D detector confidence threshold of 0.95. The 3D annotator then pseudo-labels the entire dataset. In subsequent iterations, we inject these pseudo-labeled objects into background scenes, filtered by a 0.95 confidence score from the 2D detector. Instead of weak annotations from the 2D detector, we use the more accurate 2D projections inferred by the 3D annotator (orange frustum lines in Figure 4).\nThe frustum-based architecture [26] allows selective ex-"}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Dataset and Evaluation Metric", "content": "We validate our approach on the KITTI dataset [4], which provides 7,481 pairs of RGB images and point clouds for training and 7,518 pairs for testing. The dataset includes annotations for three object categories (Car, Pedestrian, and Cyclist), each evaluated under three difficulty levels (easy, moderate, and hard) based on object occlusion and truncation. As the test set ground truth is hidden, we follow existing works [2] and split the training set into 3,712 training and 3,769 validation instances.\nWe evaluate 3D detection performance using mean Average Precision (mAP). mAP measures the accuracy of object localization and classification by averaging precision across different classes and IoU thresholds, reflecting a model's ability to detect and localize objects in 3D space accurately. We compute mAP at 40 recall points with a 3D IoU threshold of 0.7 for Car and 0.5 for Pedestrian and Cyclist."}, {"title": "4.2. Implementation details.", "content": "The training of the 3D annotator follows the settings set by the original F-ConvNet model [26]. We changed the Adam optimizer to a weight decay of 0.0001 and a learning rate that starts from 1e-4 and decays by a factor of 10 every 20 epochs for a total of 50 epochs. We set a classic depth range of [0, 70] meters in KITTI. For the proxy objects, we randomly sample between 50 and 200 points on 6 different horizontal lines. We consider the 3D annotator to have converged after 2 pseudo-labeling iterations. At iteration 0, 1 and 2, the proportions of pseudo-labeled instances over proxy objects during injection are respectively 0, 0.3 and 0.3. After iteration 2, we apply the same refinement as in the original F-ConvNet model and use the whole pseudo-labeled train set for training."}, {"title": "4.3. Comparisons with the State of the Art", "content": "Following previous methods [27,30,32], we measure the effectiveness of our 3D annotator by training different off-the-shelf 3D detectors (PointRCNN [19], PV-RCNN [18], Voxel-RCNN [23]) with the obtained annotated 3D box pseudo-labels. We report results on the PointRCNN under the multi-class setting and the per-class setting (all classes at once and one class at a time, respectively). Indeed, we noticed that training the 3D detector with all classes at once helps harder classes (Pedestrian, Cyclist) but penalizes simpler classes (Car).\nIn Table 2, we report the results on KITTI validation set. Compared to previous weakly supervised methods,"}, {"title": "4.4. Evaluation on nuScenes dataset", "content": "To demonstrate the broad applicability of our method across various object classes and scenarios, we conducted experiments on the nuScenes dataset under the weak label setting, being the first to do so, and present the results in Table 4. We employed CenterPoint [31] as the 3D detector model, trained with the 3D pseudo-labels generated by our 3D annotator. Since no weak or semi-weak approaches have reported performance on nuScenes, we compared our method with the fully supervised CenterPoint [31] and the state-of-the-art semi-supervised approach [24] which also uses CenterPoint. The nuScenes dataset is more challenging than KITTI due to a higher number of objects per scene and sparser point clouds, resulting in a larger performance gap between a 3D detector trained with oracle 3D annotations and one trained with our pseudo-annotations. In a true weakly supervised fashion, we decided to leave all hyperparameters of our method unmodified. Despite not using temporal consistency, which significantly enhances performance [24], our method outperforms the semi-supervised approach on average. For a few classes like trucks with ambiguous shapes, our method performs lower, likely due to the resemblance of such objects to two connected 3D boxes. These findings aim to open evaluation for weakly supervised 3D object detection across a wider range of classes and datasets."}, {"title": "4.5. Ablations studies", "content": "Role of proxy objects. In the absence of proxy objects, the model can't converge well during training due to the ambiguities left by the 2D supervision alone. Proxy objects serve to initiate and enhance the model's detection capabilities, effectively transferring to real object classes to the extent that inferred pseudo-labels become viable for subsequent training iterations.\nEffect of the depth normalized 2D bounding box loss. The loss $L_{norm}^{2D}$ defined by Equation 2 takes into account how far the object is from the LiDAR sensor. Therefore, to investigate how much it increases the performances, we"}, {"title": "Effect of pseudo-labeling iterations.", "content": "In Table 6 (with manual weak annotations), we report the performances of our method on the KITTI validation set after each of the pseudo-labeling iterations. It shows that the pseudo-labeling improves the model's performance after each iteration. Visualisation are provided in Figure 4 and Figure 5. Additional quantitative results can be found in the supplementary materials."}, {"title": "Weak labels from any automatic 2D detector", "content": "In Table 6 (automatic weak annotation), we demonstrate the impact of pseudo-labeling iterations using imperfect weak labels obtained from Faster R-CNN (subsection 3.4). For the moderate difficulty, performance doubled from iteration 0 to the last, reaching a mAP of 70%, compared to a 14% increase to 76% with manual weak annotations. Additionally, our 3D annotator struggles more with harder examples compared to using oracle weak annotations, consistent with the fact that hard examples are often occluded. The bounding box predicted by Faster R-CNN, which encloses only the visible part of the object, differs from the extrapolated box containing the whole object. Unlike pure geometrical approaches like GAL [27] and FGR [27], where the 3D box boundaries are tied to the 2D weak label, our method, based on"}, {"title": "Effect of noisy size prior", "content": "The impact of employing noisy size priors is presented in Table 7. Specifically, the average length is shifted by \u00b1 one standard deviation (40 cm) from the average value (3.88 meters). This experiments reflects the influence of a noisy length size prior, shows the robustness of our method to the variation of variation across different car models and regions where scenes are gathered (more details in the supplementary materials)."}, {"title": "5. Conclusions, Limitations and Perspectives", "content": "In the vein of Weakly Supervised 3D Vehicle Detection approaches [27,30], the method we propose does not need any 3D annotation, contrary to most works in the literature that are Semi-weakly supervised 3D object detection methods. These two previous works were nevertheless limited to the Car class, while our approach can easily adapt to other ones (Pedestrian, Cyclist) simply by adding a corresponding proxy object class during training. The results on nuScenes motivate us to explore in future works methods which leverage temporal consistency to improve detection.\nBeing frustum-based, our method is better suited to large-scale scenes (ex: outdoor contexts), thus we will investigate to adapt it in order to relax its dependency to the frustum model."}]}