{"title": "ALPI: Auto-Labeller with Proxy Injection for 3D Object Detection using 2D\nLabels Only", "authors": ["Saad Lahlali", "Nicolas Granger", "Herv\u00e9 Le Borgne", "Quoc-Cuong Pham"], "abstract": "3D object detection plays a crucial role in various ap-\nplications such as autonomous vehicles, robotics and aug-\nmented reality. However, training 3D detectors requires a\ncostly precise annotation, which is a hindrance to scaling\nannotation to large datasets. To address this challenge, we\npropose a weakly supervised 3D annotator that relies solely\non 2D bounding box annotations from images, along with\nsize priors. One major problem is that supervising a 3D de-\ntection model using only 2D boxes is not reliable due to am-\nbiguities between different 3D poses and their identical 2D\nprojection. We introduce a simple yet effective and generic\nsolution: we build 3D proxy objects with annotations by\nconstruction and add them to the training dataset. Our\nmethod requires only size priors to adapt to new classes. To\nbetter align 2D supervision with 3D detection, our method\nensures depth invariance with a novel expression of the 2D\nlosses. Finally, to detect more challenging instances, our\nannotator follows an offline pseudo-labelling scheme which\ngradually improves its 3D pseudo-labels. Extensive experi-\nments on the KITTI dataset demonstrate that our method not\nonly performs on-par or above previous works on the Car\ncategory, but also achieves performance close to fully su-\npervised methods on more challenging classes. We further\ndemonstrate the effectiveness and robustness of our method\nby being the first to experiment on the more challenging\nnuScenes dataset. We additionally propose a setting where\nweak labels are obtained from a 2D detector pre-trained on\nMS-COCO instead of human annotations.", "sections": [{"title": "1. Introduction", "content": "In various fields such as autonomous vehicles, robotics,\nand augmented reality, achieving accurate 3D scene under-\nstanding stands as a critical task. This necessitates lever-\naging a diverse array of sensors, including cameras and Li-\nDAR. Deep learning methods processing images [9, 15, 25]\nor point clouds [8,20,23] can extract valuable 3D spatial in-\nformation. Their effectiveness largely depends on the avail-\nability of large-scale, diverse and precisely annotated data.\nFor 3D object detection annotation, the process is neverthe-\nless tedious, especially for occluded objects, with an aver-\nage annotation time of 2 minutes per object [14].\nIn light of these challenges, weak supervision seeks to\nleverage annotations that do not fully correspond to the tar-\ngeted task, but that are substantially cheaper to label. In\nthe case of the 3D object detection task, instead of an-\nnotating the complete 3D box, a cheaper annotation can\nbe the center of the 3D box [32], the center in bird-eye-\nview [14] or a 2D box annotated from a synchronized cam-\nera view [12, 13, 16, 27, 30]. These weakly supervised mod-\nels are often used as 3D pseudo-labelers providing 3D box\nsupervision for off-the-shelf fully supervised 3D detectors.\nWeak supervision led to considerable progress in 2D-\nrelated tasks like 2D object detection [7,28,33], but remains\nat a relatively nascent stage for 3D-related tasks. Among\nexisting 3D object detection approaches supervised with\nweak annotations (Table 1), one can identify two major lim-\nitations:\n\u2022 Most of existing approaches [12-14, 16, 32] are actu-\nally semi-weakly supervised since they require a small\namount of 3D box annotations;\n\u2022 Approaches that do not require any 3D box annota-\ntion [27, 30] are specific to the class Car since they\nrely on heuristics that make them tedious or difficult to\nadapt to other classes.\nWe propose the first approach to address both limitations\nby being multi-class and requiring no 3D box annotation at\nall. More specifically, we supervise the model solely with\nreadily available class size priors and 2D bounding boxes\naround objects in the camera views.\nTo be able to detect different object classes, we adopt a\nlearning-based approach, leading to easier and better gen-\neralization across various object classes. Nevertheless, the"}, {"title": "2. Related Work", "content": "Weak supervision yields substantial practical benefits\nwhen manual annotation is costly. In the context of 3D ob-\nject detection, several annotation paradigms have been pro-\nposed.\nSemi-weakly supervised 3D object detection methods\nadopt a mixed-annotation strategy where a small portion of\nthe train set is 3D box annotated and the rest is weakly la-\nbeled. Different types of weak annotations have been pro-\nposed in this supervision setting. For instance, MTA [12],\nCAT [16] and MAP-Gen [13] adopt a semi-weak annotation\nstrategy where 13% of the scenes are fully annotated while\nthe rest are annotated with 2D bounding boxes. While these\napproaches achieve performance very close to fully super-\nvised models, they focus on only one class and require a\nnon-negligible amount of 3D box annotations. Methods\nworking on more object classes and requiring fewer fully\nannotated scenes have been proposed but they require weak\nlabels with 3D information. In particular, WS3D [14] used\nthe object's center in bird's eye view and ViT-WSS3D [32]\nused the object's 3D center as weak annotation. While these\nweak annotations are appealing due to their valuable 3D in-\nformation, it is important to note that they require anno-\ntating within the point cloud which increases the annotation\ntime compared to 2D box annotation. As mentioned in [14],\nit is necessary to draw a 2D box on either the image or the\npoint cloud to zoom in for easier and more precise point-\nannotation in a point cloud.\nAn original paradigm was proposed by Tang et al. [21]: a\nsubset of object classes is fully annotated, while another dis-\ntinct subset is weakly annotated with 2D boxes. The under-\nlying concept involves extracting valuable 3D information\nfrom fully annotated classes to help detecting weakly anno-\ntated classes. By supervising weak classes with 2D boxes,\nthe model learns their specific features. Based on the em-\npirical results of this method, we get the intuition that cross-\nclass transfers depends more on size similarity than shape\nsimilarity. In a similar vein, our approach leverages cross-\nclass transfer but from synthetic objects, thus requiring no\nmanual annotations.\nWeakly supervised 3D object detection methods work\nunder a more challenging setting where only weak labels are\nprovided. Notably, FGR [27] and GAL [30] opted for 2D\nbounding boxes as weak annotations to detect Cars in 3D.\nBoth methods rely on the standard pattern shape of Cars\nin bird's-eye view to handcraft heuristics. Therefore, de-\nsigning new handcrafted heuristics for novel classes would\nbe time-consuming, especially if these classes are not rigid\n(Pedestrian, Cyclist). In opposition to these methods, we\nopt for a learning-based approach for easier and better gen-\neralization across diverse object classes.\nWeak annotation paradigm. Several paradigms have\nbeen proposed as they offer unique trade-offs between an-\notation costs and achieved performance. Approaches us-\ning few fully annotated scenes are motivated by the inher-\nent challenges encountered when attempting to learn the de-\ntection of objects in 3D using only weakly annotated data.\nWhile these approaches outperform the ones which use no\n3D box annotations, the selection of which scenes are fully\nannotated can significantly affect performance. This aspect\nwas not investigated in semi-weakly supervised literature;\nhowever, Wang et al. [22] observed notable performance\ndiscrepancies when training a 3D detector with different\nsmall subsets of fully annotated scenes. For this purpose,\nwe set our method to be weakly supervised to lower depen-\ndency on the sampling of fully annotated scenes and opt for"}, {"title": "3. Method", "content": "We adopt the typical weak supervision pipeline which\ntrains a 3D annotator model in a weakly supervised fashion\n(Figure 2 part 2), then pseudo-annotates the dataset with\n3D boxes (Figure 2 part 3) and finally trains an off-the shelf\nmodel [3, 18, 19] with these pseudo-labels (Figure 2 part 4).\nWorking in a frustum point cloud. By using the 2D\nbox in an image and the point cloud, we can narrow down\nthe search space for an object from the entire scene to a\ntruncated pyramid (Figure 1) referred to as frustum.\n3.1. Dataset Construction with Proxy Objects\nWithin this frustum, a majority of points usually belong\nto the object of interest while a small portion is held by the\nground, occluding objects and the background. To better\nisolate the object from its background, the following step is\nto estimate its depth along the frustum axis.\nObject depth estimation. To determine the approximate\ndepth of an object in a scene, we use the annotated 2D\nbounding box, the average object height H and the Li-\nDAR/camera projection matrix \\(P\u2208R^{3\u00d74}\\).\nThe projection equation for the two points in red and\npurple in Figure 1 can be written as: \\(8_{1,2}[x, y_{1,2},1]^{T} =\nP[X, Y_{1,2}, Z, 1]^{T}\\)\nBy approximating the object's height as the average\nheight i.e. \\(Y_{2} - Y_{1} = H\\), we can estimate the depth of\nthe object X as:\n\\(X = (K^{-1}8_2[x, y_2, 1, 1/8_2])_1\n\\ \\frac{H(P_{22} - Y_1P_{12})}{(Y_1 - Y_2)}  + K\\),\nwhere \\(8_2 = \\frac{X}{Y_1 - Y_2}\\) and \\(K = [0 0 0 1]\\)\nBackground extraction. We proceed to a coarse seg-\nmentation of the object within the frustum in order to ex-\ntract the background. We design a simple approach which\nconfidently filters out the target object points while poten-\ntially excluding some associated with occluding objects, the\nground, or background. Points within the frustum are con-\nsidered part of the background if they fall outside a cylinder\ncentered at a depth X on the optical line and with a radius\nequal to the average length of the object L. This segmen-\ntation is not used for supervision but to filter out the object\nfrom the background scene which is used in the next part as\ncontext for object insertion.\nObject injection in the background. To insert an object\nin the background scene, a 3D center position is needed.\nSince we want to substitute the previous object by a new\none, we simply define the center as the median value w.r.t.\neach dimension of the points previously removed. The in-\nserted object can either be a proxy or a pseudo-annotated\nobject. The injection of pseudo-labeled objects is explained\nin subsection 3.3.\nProxy objects. A proxy object is designed with a cuboid-\nshaped appearance defined by a 3D center (as defined pre-\nviously), a 3D size and a heading. The proxy's size is sam-\npled using the size prior mean and standard deviation (part\n1 in Figure 2). These statistics are provided in the supple-\nmentary materials. Next, we choose a heading. Among 12\npossible headings in [0, \u03c0), we select the one that best aligns\nthe projection of the 3D box with the previous 2D box an-\notation. After positioning the proxy in the scene, points\nare randomly sampled on the surfaces visible to the LiDAR\n(Figure 3)."}, {"title": "3.2. 3D Annotator Architecture and Training", "content": "Architecture. We opt for Frustum-ConvNet [26] as archi-\ntecture for the 3D annotator. This model estimates the 3D\nbox of an object within a frustum. At each training iteration,\ntwo types of frustums are fed to the 3D annotator. As shown\nin Figure 2, in green the first type being frustums with tar-\nget objects (Cars, Pedestrians and Cyclists) which are su-\npervised in 2D only. The second type, in blue, are frustums\nwith injected object (proxies or pseudo-labeled) which are\nsupervised both in 2D and 3D. The annotator model pre-\ndicts a Box3D which encompasses the 3D coordinates of\nthe center, sizes, and heading of the object. The 8 corners\nof this Box3D are projected in 2D and the minimum and\nmaximum w.r.t each dimension produces the Box2D. We\nminimize a multi-task loss that is the sum of several terms:\nClassification loss. We apply a focal loss function [10] to\nthe class predictions of the model. Such loss has already\nbeen used by [32] for semi-weakly supervised 3D object\ndetection.\nLosses on Box3D. When the frustums contain proxy ob-\njects or pseudo-labelled objects that provide a 3D pseudo\nground-truth, we apply regression losses on the estimated"}, {"title": "BOX3D of these instances", "content": "The 3D center coordinates are\nregressed with the Huber loss [6], while the size is super-\nvised via a smooth L1 loss. To handle the range of object\nheadings within the interval [0, \u03c0), we discretize it into 12\nbins and pose the problem of estimating the heading as a\nclassification task, with a cross-entropy loss. The residual\nheading offset is supervised through a smooth L1 loss.\nProposed Loss on Box2D. For both target and injected ob-\njects, we supervise the projection of the predicted 3D box\nin the image (Box2D) using available 2D bounding box la-\nbels. Typically, a 2D detection regression loss would use a\nsmooth L1 loss (l) on each Box2D coordinate, giving a to-\ntal loss \\(L_{2D} = l(x_1,\\hat{x_1})+l(x_2, \\hat{x_2})+l(y_1, \\hat{y_1})+l(y_2,\\hat{y_2})\\),\nwhere x., y. and \\( \\hat{x}.\\hat{y}. \\) are the coordinates of the annotated\nand predicted 2D boxes, respectively. However, when a\npredicted Box3D is shifted from its ground truth by a con-\nstant distance and translated across different depths, the 3D\nIoU error remains constant, but the 2D projection causes\nthe L2D loss to vary with object depth. This is problem-\natic in outdoor scenes with diverse object depths (0 to 70\nmeters), as changing depth distributions in training batches\ncan cause large variations in mean loss, disrupting smooth\ntraining. To address this, we propose a depth-normalized\n2D loss, leveraging the relationship between the 2D box size\nand object depth discussed in section 3.1. The new loss is:\n\\(L^{norm}_{2D} =  \\frac{l(x_1,\\hat{x_1}) + l(x_2,\\hat{x_2})}{ \\hat{x_2}-\\hat{x_1}} +\\frac{l(y_1,\\hat{y_1}) + l(y_2,\\hat{y_2})}{\\hat{y_2}-\\hat{y_1}}\\)\nThis formulation normalizes the 2D loss by the height\nand width of the ground-truth 2D bounding box, ensuring\nconsistent loss values regardless of object depth."}, {"title": "Size regularization", "content": "We apply a regularization loss on the\nsize of the 3D boxes predicted by the model at each training\nbatch computed as:\n\\(L^{regu} = \\sum_{k\u2208{h,l,w}} l(\\hat{\\sigma}_k, \\sigma_k) + l(\\hat{v}_k, v_k)\\)\nwhere \u03c3., v. and \\( \\hat{\\sigma}.\\hat{v}. \\) denote the standard deviation and av-\nerage sizes in a training batch and in the validation set re-\nspectively, l is a smooth L1 loss and h, l, w are respectively\nthe object's height, length and width."}, {"title": "3.3. 3D Pseudo-Annotation of the Dataset", "content": "The 3D annotator is used to annotate the dataset in two\ncases. First, in order to pseudo-label objects before inject-"}, {"title": "ing them in the background scene", "content": "Secondly, for training a\n3D detector during deployment. To improve its robustness\nin detecting occluded samples, we inject confident pseudo-\nlabeled objects into background scenes and iteratively re-\ntrain the annotator.\nAt iteration 0, the 3D annotator trains on target and proxy\nobjects only. After each iteration, we save a catalog of\npseudo-annotated objects, defined by the predicted 3D box\nand the points within this box (Figure 3). We trust only\npseudo-labels where the projected Box2D closely matches\nthe ground-truth 2D box, indicating likely correct predic-\ntions. These trusted pseudo-labeled instances are injected\ninto background scenes later. We select pseudo-labeled ob-\njects with an azimuth angle close to that of the removed ob-\nject, referring to the angle between the ego Car and the ob-\nject's center in a bird's eye view. The pseudo-labeled object\nis then translated along the azimuth to its new 3D position\n(section 3.1). To enhance model robustness to occlusions\nthe injected object is augmented (Figure 3).\nWhile previous studies have explored augmentation with\nobject injections [5,29], we specifically design the injection\nof pseudo-labeled objects to improve robustness for cases\nwith imperfect 2D box annotations (Figure 3). We simu-\nlate a cropped version of the 2D box from the 3D box's 2D\nprojection and use this cropped 2D box to extract the frus-\ntum, integrating objects with accurate 3D box estimations\nand reproducing the appearance of occluded objects in the\npoint cloud."}, {"title": "3.4. Towards Zero New Human Annotation", "content": "To reduce annotation costs, we propose using 2D boxes\ngenerated by a pretrained 2D detector instead of an Oracle\ncapable of annotating complete 2D boxes. We use the Faster\nR-CNN detector [17], pre-trained on MS-COCO [11] from\nthe MMDet toolbox [1], to generate these weak labels.\nCompared to Oracle 2D boxes from KITTI, typical 2D\ndetector boxes outline only the visible portion of objects\nnot the full 3D projection. This difference is particularly\nsignificant in occluded scenarios, as shown in Figure 4\nwhere the extrapolated 2D box is green, and the Faster R-\nCNN box is purple. Consequently, incomplete weak an-\nnotations of occluded objects cannot directly train the 3D\nannotator, necessitating method adaptation.\nAt iteration 0, we refine the training set by excluding\noccluded objects with incomplete 2D boxes, using a 2D\ndetector confidence threshold of 0.95. The 3D annotator\nthen pseudo-labels the entire dataset. In subsequent iter-\nations, we inject these pseudo-labeled objects into back-\nground scenes, filtered by a 0.95 confidence score from the\n2D detector. Instead of weak annotations from the 2D de-\ntector, we use the more accurate 2D projections inferred by\nthe 3D annotator (orange frustum lines in Figure 4).\nThe frustum-based architecture [26] allows selective ex-"}, {"title": "clusion of frustums with low-confidence 2D box annotations", "content": "simplifying the filtering process compared to more\ncomplex scene-based solutions."}, {"title": "4. Experiments", "content": "4.1. Dataset and Evaluation Metric\nWe validate our approach on the KITTI dataset [4],\nwhich provides 7,481 pairs of RGB images and point clouds\nfor training and 7,518 pairs for testing. The dataset includes\nannotations for three object categories (Car, Pedestrian, and\nCyclist), each evaluated under three difficulty levels (easy,\nmoderate, and hard) based on object occlusion and trunca-\ntion. As the test set ground truth is hidden, we follow ex-\nisting works [2] and split the training set into 3,712 training\nand 3,769 validation instances.\nWe evaluate 3D detection performance using mean Average\nPrecision (mAP). mAP measures the accuracy of object lo-\ncalization and classification by averaging precision across\ndifferent classes and IoU thresholds, reflecting a model's\nability to detect and localize objects in 3D space accurately.\nWe compute mAP at 40 recall points with a 3D IoU thresh-\nold of 0.7 for Car and 0.5 for Pedestrian and Cyclist.\n4.2. Implementation details.\nThe training of the 3D annotator follows the settings set\nby the original F-ConvNet model [26]. We changed the\nAdam optimizer to a weight decay of 0.0001 and a learn-\ning rate that starts from 1e-4 and decays by a factor of 10\nevery 20 epochs for a total of 50 epochs. We set a classic\ndepth range of [0, 70] meters in KITTI. For the proxy ob-\njects, we randomly sample between 50 and 200 points on 6\ndifferent horizontal lines. We consider the 3D annotator to\nhave converged after 2 pseudo-labeling iterations. At itera-\ntion 0, 1 and 2, the proportions of pseudo-labeled instances\nover proxy objects during injection are respectively 0, 0.3\nand 0.3. After iteration 2, we apply the same refinement as\nin the original F-ConvNet model and use the whole pseudo-\nlabeled train set for training.\n4.3. Comparisons with the State of the Art\nFollowing previous methods [27,30,32], we measure the\neffectiveness of our 3D annotator by training different off-\nthe-shelf 3D detectors (PointRCNN [19], PV-RCNN [18],\nVoxel-RCNN [23]) with the obtained annotated 3D box\npseudo-labels. We report results on the PointRCNN under\nthe multi-class setting and the per-class setting (all classes\nat once and one class at a time, respectively). Indeed, we\nnoticed that training the 3D detector with all classes at once\nhelps harder classes (Pedestrian, Cyclist) but penalizes sim-\npler classes (Car).\nIn Table 2, we report the results on KITTI validation\nset. Compared to previous weakly supervised methods,"}, {"title": "4.4. Evaluation on nuScenes dataset", "content": "To demonstrate the broad applicability of our method\nacross various object classes and scenarios, we conducted\nexperiments on the nuScenes dataset under the weak label\nsetting, being the first to do so, and present the results in\nTable 4. We employed CenterPoint [31] as the 3D detec-\ntor model, trained with the 3D pseudo-labels generated by\nour 3D annotator. Since no weak or semi-weak approaches\nhave reported performance on nuScenes, we compared our\nmethod with the fully supervised CenterPoint [31] and the\nstate-of-the-art semi-supervised approach [24] which also\nuses CenterPoint. The nuScenes dataset is more challeng-\ning than KITTI due to a higher number of objects per scene\nand sparser point clouds, resulting in a larger performance\ngap between a 3D detector trained with oracle 3D annota-\ntions and one trained with our pseudo-annotations. In a true\nweakly supervised fashion, we decided to leave all hyper-\nparameters of our method unmodified. Despite not using\ntemporal consistency, which significantly enhances perfor-\nmance [24], our method outperforms the semi-supervised\napproach on average. For a few classes like trucks with am-\nbiguous shapes, our method performs lower, likely due to\nthe resemblance of such objects to two connected 3D boxes.\nThese findings aim to open evaluation for weakly super-\nvised 3D object detection across a wider range of classes\nand datasets."}, {"title": "4.5. Ablations studies", "content": "Role of proxy objects. In the absence of proxy objects,\nthe model can't converge well during training due to\nthe ambiguities left by the 2D supervision alone. Proxy\nobjects serve to initiate and enhance the model's detection\ncapabilities, effectively transferring to real object classes\nto the extent that inferred pseudo-labels become viable for\nsubsequent training iterations.\nEffect of the depth normalized 2D bounding box loss.\nThe loss \\(L^{norm}_{2D}\\) defined by Equation 2 takes into account\nhow far the object is from the LiDAR sensor. Therefore,\nto investigate how much it increases the performances, we"}, {"title": "propose to experiment on the KITTI validation set using a", "content": "usual smooth L1 loss \\(L^{2D} \\) and compare it with our normal-\nized \\(L^{norm}_{2D}\\). The \\(L^{norm}_{2D}\\) improves detection performances\non the class Car in Table 5 and also on the other classes in\nthe supplementary material, both with and without the reg-\nularization \\(L^{regu}\\), defined by Equation 3.\nEffect of pseudo-labeling iterations. In Table 6 (with\nmanual weak annotations), we report the performances\nof our method on the KITTI validation set after each of\nthe pseudo-labeling iterations. It shows that the pseudo-\nlabeling improves the model's performance after each iter-\nation. Visualisation are provided in Figure 4 and Figure 5.\nAdditional quantitative results can be found in the supple-\nmentary materials.\nWeak labels from any automatic 2D detector In Table 6\n(automatic weak annotation), we demonstrate the impact of\npseudo-labeling iterations using imperfect weak labels ob-\ntained from Faster R-CNN (subsection 3.4). For the moder-\nate difficulty, performance doubled from iteration 0 to the\nlast, reaching a mAP of 70%, compared to a 14% increase to\n76% with manual weak annotations. Additionally, our 3D\nannotator struggles more with harder examples compared\nto using oracle weak annotations, consistent with the fact\nthat hard examples are often occluded. The bounding box\npredicted by Faster R-CNN, which encloses only the visible\npart of the object, differs from the extrapolated box contain-\ning the whole object. Unlike pure geometrical approaches\nlike GAL [27] and FGR [27], where the 3D box bound-\naries are tied to the 2D weak label, our method, based on"}, {"title": "learned object representations", "content": "is better at recovering from\nnoisy labels, especially in occluded situations (see Fig-\nure 4). FGR's pipeline, due to extensive filtering, cannot\neffectively use automatic 2D weak labels, resulting in par-\ntial scene annotations unsuitable for training 3D detectors.\nEffect of noisy size prior The impact of employing noisy\nsize priors is presented in Table 7. Specifically, the aver-\nage length is shifted by \u00b1 one standard deviation (40 cm)\nfrom the average value (3.88 meters). This experiments re-\nflects the influence of a noisy length size prior, shows the\nrobustness of our method to the variation of variation across\ndifferent car models and regions where scenes are gathered\n(more details in the supplementary materials)."}, {"title": "5. Conclusions, Limitations and Perspectives", "content": "In the vein of Weakly Supervised 3D Vehicle Detection\napproaches [27,30], the method we propose does not need\nany 3D annotation, contrary to most works in the literature\nthat are Semi-weakly supervised 3D object detection meth-\nods. These two previous works were nevertheless limited\nto the Car class, while our approach can easily adapt to\nother ones (Pedestrian, Cyclist) simply by adding a corre-\nsponding proxy object class during training. The results on\nnuScenes motivate us to explore in future works methods\nwhich leverage temporal consistency to improve detection.\nBeing frustum-based, our method is better suited to\nlarge-scale scenes (ex: outdoor contexts), thus we will in-\nvestigate to adapt it in order to relax its dependency to the\nfrustum model."}]}