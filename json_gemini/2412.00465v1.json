{"title": "AgriBench: A Hierarchical Agriculture Benchmark for Multimodal Large Language Models", "authors": ["Yutong Zhou", "Masahiro Ryo"], "abstract": "We introduce AgriBench, the first agriculture benchmark designed to evaluate MultiModal Large Language Models (MM-LLMs) for agriculture applications. To further address the agriculture knowledge-based dataset limitation problem, we propose MM-LUCAS, a multimodal agriculture dataset, that includes 1,784 landscape images, segmentation masks, depth maps, and detailed annotations (geographical location, country, date, land cover and land use taxonomic details, quality scores, aesthetic scores, etc.), based on the Land Use/Cover Area Frame Survey (LUCAS) dataset, which contains comparable statistics on land use and land cover for the European Union (EU) territory. This work presents a groundbreaking perspective in advancing agriculture MM-LLMs and is still in progress, offering valuable insights for future developments and innovations in specific expert knowledge-based MM-LLMs.", "sections": [{"title": "1 Introduction", "content": "Agriculture is an important foundation for human existence. Nearly half of the terrestrial land is used for agriculture in Europe, contributing to food, fiber, and bio-energy resource production [32]. Agriculture depends on agroecosystems comprising subterranean soil, soil organisms, habitats for wild flora, and animals in and around the fields. Traditional agricultural practices heavily depended on the empirical knowledge and expertise of farmers to achieve productive yields. To increase efficiency and optimize production, digitalization has been an important agenda in agriculture for increasing efficiency and optimization, including the utility of Artificial Intelligence (AI). AI has rapidly developed and is widely used to promote agricultural automation. This advancement has significant promise for enhancing agricultural processes, including efficient assessment, explanation, informed decision-making, and understanding of agricultural systems.\nThere have been several significant advancements in Machine Learning (ML) and Deep Learning (DL) domains within the agriculture and biodiversity research field in the past decade, such as species identification [21], wildlife monitoring and protection [39], plant disease detection [22,48], plant phenotyping [30], crop classification [42], weed detection [35], intelligent spraying [15, 40], robotic harvesting [20, 41], etc. Despite these achievements, conventional ML and DL models still have certain limitations: require extensive, task-specific, and well-labeled datasets for effective training; only adapt to specific tasks but cannot generalize to other tasks or unseen data. Due to these limitations, several approaches have been examined, including transfer learning [34], few-shot learning [37], label-efficient learning [27], self-supervised learning [56], to name a few.\nRecently, benefiting from the advancements in Large Language Models (LLMS) [8,11,44], MultiModal Large Language Models (MM-LLMs) have rapidly become a new paradigm bridging the fields of Natural Language Processing (NLP) and Computer Vision (CV). MM-LLMs preserve the inherent reasoning and decision-making capabilities of LLMs and showcase remarkable versatility and efficiency across a diverse range of multimodal (MM) tasks [55], such as emotional understanding [52], image captioning [28,38], Visual Question Answering (VQA) [19], etc. However, the agricultural domain involves more complex and expert tasks, including multimodality and human-nature interactions, which present significant challenges even for advanced MM-LLMs. Therefore, it is essential to develop benchmarks to evaluate the performance of the existing models specialized for agriculture. To our knowledge, no such MM-LLM benchmarks currently exist.\nHere, we present four key questions about MM-LLMs in the agriculture domain, which will be discussed in detail in the following sections.\nQ1: How to evaluate the MM-LLM's capacities in agriculture?\nA1: [Sec. 3] Due to the unique needs and complexity of agricultural research, it is important to build an agriculture-specified benchmark to evaluate the MM-LLMs. Thus, we propose a novel Agriculture Benchmark: AgriBench.\nQ2: How to design an agriculture multimodal dataset?\nA2: [Sec. 4] To address the lack of agricultural datasets, we create MM-LUCAS with 1,784 annotated agricultural scenery images from 27 EU countries.\nQ3: How effective are advanced MM-LLMs in solving agricultural problems without extra fine-tuning?\nA3: [Sec. 5] MM-LLMs understand general agricultural content well but struggle with specific problems like diagnosing plant diseases without fine-tuning.\nOur contributions can be summarized as follows:\n1. AgriBench is the first hierarchical benchmark to evaluate the comprehension and reasoning abilities of existing MM-LLMs regarding agriculture.\n2. An innovative MM-LUCAS dataset is also newly designed for the proposed AgriBench, which contains corresponding multi-modality annotations."}, {"title": "2 Related Works", "content": "Multimodal Large Language Models (MM-LLMs) The rapid developments and remarkable achievements of LLMs have driven a growing research interest in MM-LLMs. These models enhance multimodal comprehension by aligning visual features from pre-trained image encoders with LLMs on image-text datasets. Groundbreaking MM-LLMs, such as Flamingo [9], GPT-4 [8], ModaVers [47], Cambrian-1 [43] and LLaVA-OneVision [26] have successfully fused visual data and text and adapted to various multimodal tasks. However, regarding specialized research fields and reality applications, such as industry, agriculture, and healthcare, existing advanced MM-LLMs still face significant challenges in accurately and comprehensively handling domain-specific tasks.\nBenchmarks for Multimodal Large Language Models Current benchmarks mainly focus on the ability to predict the understanding of vision-text inputs. For instance, MMBench [31] creates extensive question sets to enhance the objective evaluation of MM-LLMs. GVTBench [46] is designed for two new tasks for MM-LLMs (object counting and multi-class identification), however, its evaluations are limited to some specific aspects of visual understanding. For specialized domains, evaluation requires a level of expertise and precision that current benchmarks often fail to achieve. Therefore, it is essential to develop MM-LLM benchmarks specifically customized for these research domains.\nLand Use/Cover Area Frame Survey (LUCAS) The European Union (EU) encompasses a wide variety of landscapes and ecosystems, from densely populated urban centers to sparsely inhabited rural areas. Knowing the patterns in \"Land Cover\u201d and \u201cLand Use\" is important for human activity and geography. The Land Use/Cover Area Frame Survey (LUCAS) is one of the most extensive and authoritative in-situ field surveys across Europe. Land cover (LC) denotes the visible physical and biological features, such as cropland and water-body. Land use (LU) refers to how humans utilize the land for socio-economic purposes, such as residential living and agriculture [3]. From 2006 to 2018, LUCAS was conducted every three years across EU member states, providing a standardized framework for collecting statistics on LC and LU and other information such as soil physico-chemical parameters. Data was collected from 1,351,293 points at 651,780 unique locations, covering 106 variables, and including 5.4 million scenery photos [14]. The LUCAS data includes (1) Microdata on LC, LU, and environmental parameters; (2) Landscape images from the four cardinal directions (north, south, east, and west); and (3) Statistical tables aggregating estimates of LC and LU at the geographical level based on the microdata. This survey has significantly contributed to understanding agriculture, environmental conditions, and sustainable development across the EU. Previous studies"}, {"title": "3 AgriBench", "content": "We introduce AgriBench, the first hierarchical benchmark designed to assess the visual comprehension capabilities of MM-LLMs in the agricultural domain. Most existing MM-LLM benchmarks mainly focus on multimodal complexity. However, given the unique characteristics and requirements of specific domains, we argue that task complexity must also be taken into account. Multimodal complexity and task complexity should be considered as two distinct but correlated axes. It is feasible to encounter complex tasks involving a single modality or simple tasks requiring multimodal data. To address this, AgriBench designs diverse scenario tasks that reflect real-world agricultural challenges, ensuring a robust assessment of model performance. Furthermore, AgriBench evaluates models across five levels of task complexity and various modalities, providing a comprehensive framework for advancements in agricultural AI."}, {"title": "3.1 Hierarchical Standard", "content": "Agriculture requires a broad range of tasks, from simple object detection to complex decision-making (e.g., fertilization strategy). In particular, high-stakes decision-making (e.g., planning and management) typically has no objectively single correct answer. There will be several answers equally convincing for domain experts. Therefore, this ambiguity demands to be addressed within the context of human-centered AI. As illustrated in Fig. 1, we define the capabilities of MM-LLMs in agriculture into hierarchical levels standards ranging from L1 (Basic Recognition) to L5 (Human-AI Aligned Suggestion)."}, {"title": "3.2 The 5 Levels of Agriculture MM-LLMs Evaluation Strategy", "content": "We propose the 5 levels of MM-LLM capability that can assess to which extent the model can address multiple modalities and various tasks. For each level, we first describe the definition and some agriculture task examples, ranging from perception to cognition, simple to complex. Then, we summarize totally 17 main tasks with detailed descriptions as follows, with the template: \u201cT\u2192T / I\u2192T / I\u2192I / T+I\u2192T / T+I\u2192I agriculture tasks: details.\u201d\nLevel 1: Basic Recognition At the lowest level, the model should accurately describe visual content in images. The textual descriptions must be immediately clear to the user based on human intuition, without additional justification to evaluate performance. This includes object detection (e.g., identifying fruits on a tree, flowering plants, and weeds on soil), boundary delineation (e.g., distinguishing soil from plants), and species classification (e.g., categorizing visually distinct common crops such as sunflowers, lavenders, wheat, maize, and cotton).\nT\u2192T Basic Question Answering: Describe the basic information on crops or scenes based on broad knowledge from LLMs without complex inference.\nT+I\u2192T Species Classification: Detect and classify common crops.\nT+I\u2192T Image Captioning: Describe the visible agriculture objects.\nLevel 2: Coarse-grained Recognition The model can describe clearly definable properties. These properties should be objectively extractable from the MM-LLMs, although the extraction process may require time for the user. While the task complexity is higher than Level 1, no inference is still necessary, and users should consistently agree on the results. This includes object counting (e.g., counting the number of fruits in an image or on a specific branch), dominant class detection (e.g., identifying the most prevalent crop type in the image if multiple crops are seen), and classification (e.g., predicting widely recognized basic phenological growth stages such as seedling, vegetative, flowering, and ripening).\nT+I\u2192T Object counting: Count the number of user-specified objects.\nT+I\u2192T Basic Scene Analysis: Generate detailed textual descriptions of multiple visible agriculture objects and the overall scene accurately.\nLevel 3: Fine-grained Recognition The model at this level can describe fine-grained properties, though the estimation involves some subjectivity. Unlike Level 2, while a common consensus on the answer can be reached, there may still be some disagreement. This includes tasks such as image enhancement (e.g., multiple enhancement methods are possible) and grounded image captioning (e.g., the explanation focus could be various if multiple objects are present).\nT\u2192T Advanced Question Answering: Describe the in-depth, detailed information on crops or scenes based on specific knowledge from existing LLMs that may require some degree of inference."}, {"title": "Level 4: Knowledge-guided Inference", "content": "The model can describe elements that are not directly visible, requiring educated guesses based on expert insights or domain-specific knowledge. Experienced users can make informed guesses using visible properties as hints, often relying on empirical correlations with a high degree of flexibility. Thus, adding additional data sources can enhance the accuracy of these predictions. This includes tasks such as regression (e.g., estimating crop yield, plant health, soil health), classification (e.g., identifying plant disease, crop phenotype, and geographic region), and scene generation.\nI\u2192T Surrounding Scene Analysis: Predict crop yield based on visible properties (plant size, leaf color, and density) with historical data and weathers.\nI\u2192I Automatic Image Enhancement: Automatically enhance images from a comprehensive scene analysis, without relying on additional guidance.\nT+I\u2192T Environmental Impact Prediction: Combine visual images with information on fertilizer use, irrigation practices, and local biodiversity to predict the environmental impact.\nT+I\u2192T Camouflaged Object Detection: Identify camouflaged objects blended with surroundings, aiding in pest activity risk assessment.\nT+I\u2192I Scene Generation: Generate scenes for future scenarios based on current observations."}, {"title": "Level 5: Human Aligned Suggestion", "content": "Level 5 extends beyond inference based on expert insights. The model should be capable of suggesting actions or scenarios for future implementation. These suggestions could influence high-stakes decision-making, requiring the model to provide comprehensive justifications, which ensures that users can assess the validity and feasibility of the suggestions before taking action.\nT\u2192T Strategic Planning: Suggest long-term strategies using continuously updated current and historical data, such as recommending crop rotation and irrigation schedules to maximize yield and soil health over multiple seasons.\nI\u2192T Scene Projection: Evaluate various \"what-if\" scenarios, such as the impact of changing a particular farming practice or responding to an unexpected event (e.g., sudden weather changes), and suggest the best course of action with a detailed analysis of potential outcomes and probabilities.\nT+I\u2192T Sustainability Recommendations: Propose sustainable farming practices that balance productivity with environmental conservation, such as reducing chemical use, or adopting no-till farming, based on accurate prediction and detailed analysis results."}, {"title": "4 MM-LUCAS Dataset", "content": "Image acquisition and computer vision play a central role in advancing agricultural digitalization and optimization. Images are commonly captured using handheld cameras [16], Unmanned Aerial Vehicles (UAV)-mounted cameras [18], and earth observatories [45]. While multispectral (e.g., near-infrared) and hyperspectral imaging techniques are increasingly popular for estimating plant and soil conditions across various wavelengths, RGB images still serve as the basis for various agricultural applications.\nHowever, there is a lack of publicly available and well-annotated agricultural image datasets with multimodal information. To address this gap and enhance the agricultural knowledge understanding capabilities of MM-LLMs, we propose a novel multimodal agriculture dataset with specialized annotations, designed to follow the proposed AgriBench, named MM-LUCAS. As shown in Fig. 2, our dataset has the following properties: (1) It contains 1,784 scenery images and some basic information from the original LUCAS dataset. (2) It includes 1,784 corresponding semantic segmentation masks and depth maps. (3) For each image, we assess the quality and aesthetic. (4) We generate landscape question answering with 4 topics. MM-LUCAS was developed based on the existing segmentation LUCAS dataset [33]. We describe the detailed information below."}, {"title": "4.1 Data Collection", "content": "Image & Basic Information We collect 1,784 scenery images (1600\u00d71200 pixels) from [33] for MM-LUCAS. These images were taken horizontally at the eye-height level across 1,784 different sites in 27 EU countries (see Tab. 3). The sites primarily represent agricultural and rural landscapes to ensure a diverse and comprehensive dataset. This selection aims to capture the variety and complexity of rural environments across Europe, providing a robust basis for precise analyses. Additionally, we selected specific microdata (geographical location, date, LC, LU) from [33], as shown in Tab. 2 (Top part)."}, {"title": "4.2 Data Processing", "content": "Depth Estimation We adopt the advanced Depth Anything V2-Large model (335.3M parameters) [51], which achieves robust and fine-grained depth predictions by using synthetic images, with high efficiency and accuracy.\nVisual Assessment We adopt Q-Align [49], which trained LLMs for visual rating by emulating human discrete-level rating processes. Compared with other similar models that rely on numerical score scaling, Q-Align aligns more with human cognition. We only evaluate image quality and aesthetics in our dataset."}, {"title": "4.3 Data Analysis", "content": "As shown in Fig. 3a, we present the top 10 land cover (LC) categories in our MM-LUCAS dataset. Tab. 6 list the detailed LC labels, which show a diverse range of agricultural and natural land cover types across the EU regions. Common wheat is the most frequently occurring. The importance of crops like wheat, sunflowmai4qgand, barlausbighlightanthasericUjwalefiles in our MM-LUCAS dataset, which indicate Agriculture (excluding fallow land and kitchen gardens), overwhelmingly the most prevalent. Forestry is the second most common LU that is significantly less frequent than agriculture. Tab. 7 list the detailed LU labels, indicating that although agriculture is prevailing, there is still a variety of land use types represented, including forestry, natural areas, andfaldosedandhouovidings fadivassenvianof the bardskamequality and visual appeal of agriculture, we analyze the relationship between the microdata [33] (segmentation classes, geographical information and date) and quality scores (QS) / aesthetic scores (AS). This analysis provides insights that can help optimize agricultural practices, improve crop management, and enhance marketability.\nFigure 4 presents the relationship between the number of different segmentation classes (see Tab. 4) and their average AS. Despite the Orchard is infrequent, have a relatively high average AS (2.04). Natural elements, like flower and cropfiled have high AS (2.45 and 2.46), showing the importance of contributing to the scenery beauty.\nFigure 5 illustrates the significant regional variability of QS and AS across the EU, which helps understand regional strengths and weaknesses, guiding local farmers and agricultural policymakers to focus on best practices that lead to higher quality and more aesthetically pleasing crops. France, Germany, and the United Kingdom have higher QS. Romania and Bulgaria have a mix of high and low-quality images, which could be due to diverse geographical features and environmental conditions, as shown in Fig. 5 (left). France and parts of Romania and Bulgaria have higher AS, while Finland, Estonia, and Lithuania have lower AS, as shown in Fig. 5 (right).\nWe further analyze seasonal changes in AS for the five most frequent and five least frequent classes, which provides insights into how different agricultural products and related surrounding environments are affected by seasonal changes. As demonstrated in Fig. 6(a), the top 5 classes are highly influenced by natural cycles of growth and harvest, with spring and summer generally showing higher and more variable scores. Strong seasonal dependency flowers and fruit show the highest variability, while field margins maintain more"}, {"title": "5 Experiments and Analysis", "content": "5.1 Mainstream MM-LLMs for Evaluation and Results\nWe evaluate 2 open-source MM-LLMs (InterVL2-26B [13], and mPLUG-Owl2 [53]) and 3 closed-source MM-LLMs (GPT-40 [7], Gemini-1.0 Pro [6], and Claude 3.5 Sonnet [5]) based on the official implementation sources in the following 2 tasks. Experimental results are shown as: \u201cLevel X: Modal Task Question Models: Results\". Some details are omitted due to page limitations.\nLevel 1: T\u2192T Basic Question Answering Q1: Please briefly introduce the common vegetable crops in Europe.\n\u2022 InterVL2-26B [13]: Tomatoes, cucumbers, peppers, onions, carrots, potatoes, and lettuce...\n* mPLUG-Owl2 [53]: Potatoes, onions, carrots, tomatoes, peas, and lettuce.\n* Claude 3.5 Sonnet [5]: Potatoes; Tomatoes; Carrots; Onions; Cabbage; Lettuce; Cucumbers; Bell peppers; Peas; Beans; Broccoli and cauliflower; Spinach; Asparagus; Zucchini (courgette); Garlic...\n* Gemini-1.0 Pro [6]: Mass Production Leaders: Tomatoes; Onions; Carrots ... Other Widespread Crops: Brassicas; Solanaceae Family ...\n* GPT-40 [7]: Tomatoes; Potatoes; Carrots; Onions; Cabbage; Cucumbers; Peppers (Bell and Chili); Spinach; Zucchini (Courgette) ...\nLevel 3: T+I\u2192T Contextual Scene Analysis Q2: Rate the aesthetics (0-5) of this image and identify key features enhancing the visual appeal.\n* InterVL2-26B [13]: Score: Composition (4/5), Lighting Conditions (4/5)\u21924/5. Key features: Central tree; Contrast between sky & ground.\n*mPLUG-Owl2 [53]: Score: 4/5. Key features: the large tree in the middle of the field, the lush green grass, and the yellow flowers surrounding the tree.\n* Claude 3.5 Sonnet [5]: Score: 4.5/5. Key features: Focal point; Composition; Color palette; Lighting; Depth perception; Seasonal beauty; Sky drama; Natural framing; Contrast; Symmetry; Rural charm.\n* Gemini-1.0 Pro [6]: Score: 4/5. Key features: Simplicity and Minimalism: (4/5); Centralized Subject: (4/5); Contrasting Colors: (4/5); Rule of Thirds: (4/5); Natural Light: (4/5); Depth of Field: (4/5).\n* GPT-40 [7]: Score: 4.5/5. Key features: Tree as the Focal Point; Color Contrast; Depth and Layers; Natural Symmetry and Balance; Texture and Detail; Light and Shadow; Serene Atmosphere."}, {"title": "5.2 Results Analysis", "content": "For Basic Question Answering, InterVL2-26B and mPLUG-Owl2 provide concise lists of 6-7 common vegetables, offering straightforward and relevant responses. In contrast, Claude 3.5 Sonnet and GPT-40 present more comprehensive lists, reflecting a broader knowledge. Gemini-1.0 Pro uniquely categorizes crops into mass production leaders and other widespread crops, highlighting the economic significance and scale of different crops.\nFor Contextual Scene Analysis, the five MM-LLMs consistently rate the AS between 4 and 4.5 out of 5. Key visual features include the tree, color contrast, and depth. While InterVL2-26B, mPLUG-Owl2, and Gemini-1.0 Pro focus on the tree and contrast as central elements. Claude 3.5 Sonnet and GPT-40 provide a more comprehensive analysis, especially highlighting seasonal beauty and light, offering a deeper understanding of the scene aesthetic."}, {"title": "6 Discussion", "content": "Correlation of MM-LUCAS and AgriBench. Semantic segmentation masks and depth maps play an important role in AgriBench, evaluating the agricultural MM-LLM tasks by offering detailed visual and spatial information. They provide an extra condition for accurately assessing tasks such as species classification, object counting, and dense object counting within complex scenes. They also improve contextual scene analysis and environmental impact prediction by offering deeper insights into the relationships between objects and their surroundings. Additionally, they support scene projection, scene generation, and sustainability recommendations by ensuring realistic spatial relationships and providing key data for informed decision-making.\nResponsible AI. Another important challenge is the requirement for MM-LLMs to be interpretable and explainable, even in advanced models. Responsible AI considers various aspects, such as bias, fairness, transparency, human oversight, etc., which ensures that users can understand how and why a model makes certain decisions, as this is crucial for decision-making and gaining the trust of stakeholders. For instance, in healthcare, understanding the decision-making process of an AI model can help doctors trust and effectively use AI recommendations. Similarly, in agriculture, farmers need clear insights into AI predictions to ensure regulatory compliance and to make informed decisions. This trust ensures that AI systems are effective, trustworthy, and ethical, which is foundational for the widespread implementation and acceptance of AI technologies in various research domains and real-world applications.\nLimitations and Future Works. Our initial evaluation is primarily qualitative. We plan to extend our benchmark by including evaluation metrics, from perception (user satisfaction scores and expert reviews) to cognition (accuracy rate and consistency score), to provide a comprehensive evaluation that combines both qualitative and quantitative."}, {"title": "7 Conclusion", "content": "This paper introduces AgriBench, the first agriculture benchmark evaluating MM-LLMs across multi-modality and multi-tasks dimensions, supporting a broader range of agriculture scenarios and applications. We further introduce MM-LUCAS, the first multimodal agriculture dataset with multiple annotations. This initial exploration is still a work in progress and represents the first step designed to align with AgriBench. Finally, we compare 5 MM-LLMs on AgriBench and highlight significant insights in specialized domain MM-LLMs for further exploration. AgriBench represents the initial step and a significant contribution to developing agricultural MM-LLMs. We aim to boost the progress of AI technologies specifically for agricultural needs, benefiting both researchers and practitioners."}]}