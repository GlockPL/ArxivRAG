{"title": "Can OpenAI o1 Reason Well in Ophthalmology? A 6,990-Question Head-to-Head Evaluation Study", "authors": ["Sahana Srinivasan", "Xuguang Ai", "Minjie Zou", "Ke Zou", "Hyunjae Kim", "Thaddaeus Wai Soon Lo", "Krithi Pushpanathan", "Yiming Kong", "Anran Li", "Maxwell Singer", "Kai Jin", "Fares Antaki", "David Ziyou Chen", "Dianbo Liu", "Ron A. Adelman", "Qingyu Chen", "Yih Chung Tham"], "abstract": "Importance: OpenAl's recent large language model (LLM), 01, has been designed with enhanced reasoning capabilities. However, this advancement remains untested in specialised medical fields like ophthalmology. Findings from this study provide valuable insights into o1's performance in ophthalmology, addressing whether a domain-specific LLM is still needed.\nObjective: To determine the performance and reasoning ability of OpenAl 01 compared to other commonly used LLMs in addressing ophthalmology-specific questions.\nDesign and Setting: This evaluation included Open Al's o1, GPT-40, GPT-4, GPT-3.5, along with Llama 3, and Gemini 1.5 Pro. A total of 6,990 questions from the ophthalmology subset of the open-source MedMCQA examination dataset was input into each LLM in a standardised format to assess medical knowledge accuracy and reasoning. Subgroup analyses were performed based on ophthalmic subtopics and ground truth explanation lengths.\nMain Outcomes and Measures: The models were evaluated on performance (using accuracy and macro-F1 score) and reasoning abilities (measured using text-generation metrics: ROUGE-L, BERTScore, BARTScore, AlignScore, METEOR, and a weighted normalised aggregate score).", "sections": [{"title": "Key Points", "content": "Question:\nWhat is the performance and reasoning ability of OpenAl 01 compared to other large language models in addressing ophthalmology-specific questions?\nFindings:\nThis study evaluated OpenAl 01 and five LLMs using 6,990 ophthalmological questions from MedMCQA. O1 achieved the highest accuracy (0.88) and macro-F1 score but ranked third in reasoning capabilities based on text-generation metrics. Across subtopics, o1 ranked first in \u201cLens\u201d and \u201cGlaucoma\u201d but second to GPT-40 in \u201cCorneal and External Diseases,\u201d \u201cVitreous and Retina,\u201d and \u201cOculoplastic and Orbital Diseases\u201d. Subgroup analyses showed o1 performed better on queries with longer ground truth explanations.\nMeaning:\nO1's reasoning enhancements may not fully extend to ophthalmology, underscoring the need for domain-specific refinements to optimize performance in specialized fields like ophthalmology."}, {"title": "Results", "content": "Among the six models, o1 achieved the highest accuracy (0.88 \u00b1 0.33) and macro-F1 score (0.70) (all P<0.001). However, in reasoning evaluation, based on the weighted aggregate score, o1 ranked third behind GPT-40 and GPT-4. O1 performed poorer than GPT4o and GPT-4 in ROUGE-L (0.12 \u00b1 0.07), BERTScore (0.66 \u00b1 0.06), and AlignScore (0.16 \u00b1 0.13) (all P<0.001). Nevertheless, O1 excelled in BARTScore (-4.79 \u00b1 1.09) and METEOR (0.22 \u00b1 0.10). Across subtopics, o1 ranked first in \u201cLens\u201d and \u201cGlaucoma\u201d but was predominantly second to GPT-40 in \u201cCorneal and External Diseases,\u201d \u201cVitreous and Retina,\u201d and \u201cOculoplastic and Orbital Diseases.\u201d Additionally, o1's reasoning showed improved performance on MCQ items with longer ground truth explanations (\u2265100 words).\nConclusions and Relevance: OpenAl o1 demonstrated superior accuracy in answering ophthalmology-related MCQs. However, its reasoning abilities lagged behind those of GPT-40 and GPT-4. These findings suggest that o1's general reasoning enhancements may not extend equivalently to ophthalmology, underscoring the ongoing need for domain-specific refinements to optimise LLM performance in specialised medical fields like ophthalmology."}, {"title": "Introduction", "content": "The emergence of large language models (LLMs) represents a transformative advancement in natural language processing (NLP). LLMs generate \u201chuman-like\u201d responses by prediction of the natural language token based on their extensive training.\nSince the introduction of OpenAl's GPT-3.5 in late 2022,1\u20136 LLMs have made significant advancements the last two years. In healthcare, LLMs have shown promise to revolutionise or optimise current clinical practice and medical education delivery.3,5\u201311 For example, LLMs have shown potential in simplifying and summarising medical reports in radiology and intensive care, triaging in emergency medicine and answering patient health queries. 12\u201315\nIn the field of ophthalmology, LLMs have demonstrated potential utility in improving healthcare delivery by optimising disease diagnosis, patient education and clinical workflow. 16-21 However, a critical limitation of existing LLMs lies in their susceptibility to generating inaccurate outputs, and their inconsistent performance in medical-specific tasks.1,5,6,22\u201325 For this reason, LLMs are constantly being improved to generate outputs that are more accurate and with more up-to-date knowledge.\nThe rapid development of LLMs has culminated in the release of OpenAl o1 in September 2024. This recent iteration is designed with enhanced reasoning capabilities, aimed at tackling complex tasks with greater accuracy.26 This iteration retains the hallmark features of earlier LLMs such as \u201chuman-like\u201d conversation while introducing advancements in reasoning and problem-solving processes.26,27 With a longer time for reasoning and response generation, it is claimed that o1 can now \u201cthink before it responds\". 27 This enhancement positions of as a model anticipated to outperform its predecessors, potentially enabling it to handle more complex tasks. 26,28 Nevertheless, while recent studies demonstrated o1's improved performance in answering general medicine-related questions, 28-31 its reasoning and problem-solving performance in the specialised field, in particular ophthalmology remains unexplored.\nTo address this gap, this study aimed to evaluate and compare the performance of OpenAl 01 with five other commonly used LLMs - GPT-40, GPT-4, GPT-3.5, Llama-3-8b and Gemini 1.5 Pro, in addressing ophthalmology-related questions. An ophthalmological exam benchmark dataset was first curated by extracting ophthalmological questions from the Medical Multiple-Choice Question Answering (MedMCQA) dataset. The models were then comprehensively assessed for both the accuracy of their responses and the quality of their reasoning. These findings may offer valuable insights into the capabilities and limitations of the o1 model in ophthalmology and contribute to answering the critical question of whether a domain-customised, ophthalmology-specific LLM is still necessary.\""}, {"title": "Methods", "content": "This study was a comparative evaluation of six LLMs, o1, GPT-40, GPT-4, GPT-3.5, Llama-3-8b, and Gemini 1.5 Pro, assessing their performance on a set of multiple-choice questions (MCQs) using both accuracy- and reasoning-focused metrics.\nThe models included in the analysis were OpenAl's 01 (01-preview-2024-09-12), GPT-40 (gpt-40-2024-05-13), GPT-4 (gpt-4-0613), GPT-3.5 (gpt-35-turbo-0613, as well as Llama-3-8b (Meta-Llama-3-8B-Instruct) and Gemini 1.5 Pro (gemini-1.5-pro-001). All models were accessed via their respective APIs.\nThis study utilized the ophthalmology subset of the MedMCQA dataset, which comprised of 6,990 multiple-choice questions (MCQs).32 Each MCQ item consisted of a correct answer and an accompanying explanation, which served as the ground truth reasoning for comparison with the outputs generated by the various LLMs in this study.\nSince prompt phrasing can influence model performance, we standardised the input format, consistent with the styles adopted in previous studies. 33,34 As illustrated in Figure 1, the 6,990 questions were reformatted into prompts, before being presented to each model with a zero-shot approach. Each prompt instructed the model to respond as an \"assistant specializing in ophthalmology\u201d, select the most appropriate answer option, and provide an explanation for its choice (Figure 1). We conducted greedy decoding with a temperature set to 0 across all models to minimise response variability. 35\nWe evaluated outputs of o1 and the five other benchmark LLMs using a variety of metrics to assess accuracy and reasoning. Accuracy was evaluated using traditional accuracy which was defined as the proportion of correct predictions made out of the total predictions mad, while macro-F1 score was used to provide the unweighted harmonic mean of precision and recall. Reasoning abilities were evaluated using five text- generation metrics: Recall-Oriented Understudy for Gisting Evaluation (ROUGE-L), BERTScore, BARTScore, AlignScore and the Metric for Evaluation of Translation with Explicit Ordering (METEOR).36\u201340 These metrics evaluate the quality of the model's generated reasoning by comparing it to the ground truth (reference) reasoning in MedMCQA, assessing aspects such as similarity and overlap in key phrases and words (See Supplementary Table 1 for definitions). To summarise reasoning performance, a weighted normalised aggregate score was calculated. For each text-generation metric, scores were first normalised on a scale from 0 to 1, with 0 representing the poorest performance and 1 being the best. The normalized scores across the five metrics were then averaged to derive a final weighted score for each LLM.\nWe conducted three subgroup or sensitivity analyses to further explore how different factors may influence o1's reasoning capabilities. The ophthalmology subset of the MedMCQA dataset, with its vast 6,990 items, provides a comprehensive resource for evaluating LLM performance. However, it is unavoidable that some ground truth explanations may be suboptimal. For instance, we observed a small subset of items where explanations merely listed the answer options, restated the correct answer verbatim, or consisted of disjointed keywords rather than coherent reasoning (Supplementary Table 2). Such suboptimal reasoning could confound performance on text-generation metrics, which rely on comparing model outputs to reference explanations. To address this, we conducted a sensitivity analysis on 100 questions with complete and detailed explanations for both correct and incorrect options.\nSecond, we additionally performed subgroup analyses stratified by ground truth explanations' length: i) those with \u226550 words, ii) \u2265100 words.\nThirdly, we explored whether o1's performance varied across various ophthalmic sub- topics. The original ophthalmology subset of MedMCQA contained 123 overlapping subtopics. For better clarity, we regrouped the 123 sub-topics into distinct topic groups (Supplementary Table 3). To ensure robust evaluation, we then focused on five ophthalmic topic groups with the largest sample sizes: \u201cCorneal and external diseases\" (n=534), \u201cLens\u201d (n=336), \u201cVitreous and retina\u201d (n=319), \u201cOculoplastic and orbital diseases\u201d (n=256), and \u201cGlaucoma\u201d (n=213).\nText-generation metrics alone may not entirely reflect the reasoning abilities of these models. 25,41 To address this, a qualitative review was conducted by two human evaluators (SS, MJW) on a randomly selected subset of 40 items. Specifically, the evaluators assessed responses generated by OpenAl o1 and its predecessor, GPT-40, to identify similarities or differences in reasoning quality and coherence.\nAll statistical analyses were conducted using Python (Python version 3.9.0, Python Software Foundation), with statistical significance set at p<0.05. To compare accuracy, the paired t-test was used to assess any statistical significance between o1 and the other models' performances. The z-test was used for macro-F1 as it provides a single aggregate performance score. Although the distribution of the dataset is non-parametric, its large sample size (n=6,990) allows for the difference in means to be approximated as a normal distribution, justifying the use of the paired t-test and z-test. Additionally, the Bonferroni correction was applied to control the family-wise error rate in multiple test comparisons.\nFor the text-generation metrics, the two-tailed Wilcoxon rank-sum test was used to assess statistical significance between 01 and the other models' performances at the item level. This test was chosen due to the non-parametric nature of the text-generation metrics'"}, {"title": "Performance on Accuracy and Macro-F1 Scores", "content": "Overall, 01 demonstrated the highest accuracy (0.88 \u00b1 0.33) among all models, significantly outperforming the others (all p < 0.001). It was followed by Llama-3-8b (0.83\u00b1 0.38, GPT-40 (0.82\u00b1 0.39), GPT-4 (0.75\u00b1 0.43), Gemini 1.5 Pro (0.71\u00b1 0.45), and GPT- 3.5 (0.58\u00b1 0.49). Similarly, o1 achieved the highest macro-F1 score (0.70), followed by Llama-3-8b (0.66), GPT-40 (0.65), GPT-4 (0.60), Gemini 1.5 Pro (0.57), and GPT-3.5 (0.46) (all p < 0.001)."}, {"title": "Performance on Text-Generation Metrics", "content": "In reasoning evaluations, based on the weighted normalised score 01 ranked third overall, behind GPT-40 (first) and GPT-4 (second). Based on ROUGE-L score, o1 (0.12\u00b10.07) ranked fourth, behind GPT-4 (0.15\u00b10.08), GPT-40 (0.14\u00b10.07), and GPT-3.5 (0.13\u00b10.07). Similarly, for BERTScore, o1 (0.66\u00b10.06) ranked third, behind GPT-40 (0.67\u00b10.05) and GPT-4 (0.67\u00b10.05) (all p<0.001). A similar trend was observed for AlignScore (o1: 0.16\u00b10.13, GPT-40: 0.18\u00b10.14, GPT-4: 0.19\u00b10.15; p<0.001). However, o1 excelled in BARTScore, achieving the best performance with a score of -4.79\u00b11.09, outperforming all other models (all p<0.001). For METEOR, 0o1's performance (0.22\u00b10.10) was joint-best with GPT-40 (p=0.06) and significantly surpassed the other models (all p<0.001). In an additional sensitivity analysis where we only included MCQ items which were correctly answered by all LLMs, we consistently observed that o1 ranked first for BARTScore and METEOR, while ranking behind GPT-4 (first) and GPT-40 (second) for ROUGE-L, BERTScore and AlignScore (all p\u22640.01, except for comparison in BERTScore). Similarly, o1 performed comparably to Llama-3-8b which ranked third for ROUGE-L and BERTScore.\nSub-analyses were conducted on questions with ground truth explanations of \u226550 words and \u2265100 words . Scores across all metrics generally improved as the ground truth explanation length increased, Notably, 01 outperformed the other models when the ground truth explanation length exceeded 100 words, achieving the highest scores in BERTScore, BARTScore, and METEOR (p \u22640.001 for all comparisons, except with GPT- 40 in BERTScore). Similarly, 01 outperformed GPT-4 and Gemini 1.5 Pro in ROUGE-L and tied first with the rest (P<0.001) However, for AlignScore, even when the ground truth explanation length was \u2265100 words, o1 (0.30\u00b10.12) still ranked third behind GPT-40 (0.33\u00b10.14) and GPT-4 (0.33\u00b10.15)."}, {"title": "Performance Across Ophthalmic Subtopics", "content": "When evaluating o1's performance across the five ophthalmic topic groups, the model consistently achieved the highest accuracy, largely outperforming all other models.\nO1's performance varied across the text-generation metrics in different topic groups, mirroring trends seen in the entire dataset. Overall, using the weighted normalised scores, o1 ranked first in \u201cLens\u201d (01: 0.75, others: 0.05-0.65) and \u201cGlaucoma\u201d (01: 0.76, others: 0.13-0.72). However, overall, o1 ranked second to GPT-40 in \u201cCorneal and external diseases\u201d (01: 0.75, GPT-40: 0.76), \u201cVitreous and retina\u201d (01: 0.73, GPT-40: 0.80), and \u201cOculoplastic and orbital diseases\u201d (01: 0.74, GPT-40: 0.76). Across the topic groups, o1 consistently ranked first in terms of BARTScore and METEOR, but still ranked second or third behind GPT4o and GPT4 for the ROUGE-L, BERTScore and AlignScore metrics."}, {"title": "Human Expert Review of LLM-generated Responses", "content": "This review on a randomly selected subset of MCQ items revealed distinct response patterns between 01 and GPT-40. We observed that o1 produced more structured and detailed responses maintaining a methodical format even for incorrect outputs. Additionally, o1 also incorporated more clinical terminology, aligning with its higher accuracy scores. In contrast, GPT-40 tended to generate concise, summary-like responses. However, qualitative evaluation revealed instances where 01 occasionally confused similar yet distinct clinical concepts or methodologies, leading to inaccurate outputs. For instance, o1 seemed to have misinterpreted phenol red thread test (PRTT) to Schirmer-2 test, leading to choosing the incorrect option when asked about characteristics of PRTT."}, {"title": "Discussion", "content": "Using a large dataset of 6,990 ophthalmology-related MCQ items, our study pioneers the evaluation of OpenAl's latest 01 model's performance and reasoning capabilities, benchmarking it against five other commonly used LLMs. While recent studies primarily examined o1 in general medical discipline and its accuracy performance28,29, our study distinguishes itself through a more comprehensive and multifaceted evaluation approach which spanned across performance assessment (accuracy and macro-F1 scores), reasoning capabilities (using standard text-generation metrics), and qualitative evaluation via human expert reviews. Altogether, these methodologies provided a multidimensional assessment of o1's domain-specific knowledge and inferential reasoning processes in ophthalmology. We found that o1 demonstrated superior accuracy but largely ranked behind GPT-40 and GPT-4 in terms of the text-generation metrics. Contrary to expectations that o1's enhanced reasoning capabilities would extend to ophthalmology, our findings suggest that o1's reasoning capability may not translate equivalently to ophthalmology. This underscores the ongoing need for customised ophthalmology- specific LLMs to optimize performance in this specialised field.\nIn contrast to previous study, o1's ranking as third behind GPT-40 and GPT-4 in text- generation metrics in this ophthalmological evaluation differs from earlier reported findings. For instance, in an evaluation involving general medical contexts, Xie et al., demonstrated o1's superior performance in ROUGE-1 compared to other general LLMs.29 This discrepancy could be due to the highly specialised nature of ophthalmology, where general reasoning improvements may not translate directly. Moreover, o1 tended to generate longer responses compared to other models and the ground truth explanations, likely as a result of its chain-of-thought reasoning approach.26,27 On this note, text- generation metrics, which primarily evaluate lexical and semantic similarity between model outputs and ground truth explanations, are generally sensitive to length mismatches. Thus, overly verbose outputs may reduce similarity scores in these text- generation metrics, partly explaining o1's inconsistent performance across these text- generation metrics. However, in subgroup analyses based on ground truth explanation length, 01 showed improved performance on queries with ground truth reasonings exceeding 100 words. Taken together, these observations warrant the need for further investigations into the reasoning processes.\nIn our qualitative review of the randomly selected subset of MCQ items, o1 was observed to produce more detailed and methodical responses compared to GPT-40, particularly when o1 selected the correct answer. For instance, 01 provided a more comprehensive breakdown of the visual cascade compared to GPT-40, demonstrating a comparatively deeper grasp of ophthalmological knowledge. Similarly, 01 accurately identified key details in a step-by-step manner, leading to the correct answer. These examples highlight how o1's chain-of-thought reasoning can enhance its ability to address complex medical scenarios.\nHowever, o1's detail-oriented approach is not without challenges. In some instances, misplaced assumptions led to errors , where 01 confused two similar tests for dry eyes, resulting in an incorrect output. Despite these limitations, o1's detailed approach provides insights into the model's reasoning process and highlights opportunities for targeted refinement in the future, by incorporating ophthalmology-specific datasets for fine-tuning. In contrast, GPT-40 produces more succinct, general responses, prioritising clarity and coherence, often avoiding over- interpretation . While this succinctness mitigates risks of over- analysis, it may omit important medical nuances necessary for accurate judgment in complex cases.\nThe detailed clinical language and step-by-step reasoning in o1's responses suggest potential for tasks requiring depth and technical accuracy, such as medical staff training, patient education, or for providing second opinions for patients . However, its chain-of-thought approach also presents challenges. Notably, this approach was associated with slower response times, increased token usage and cost compared to other models.29 Hence, this detailed qualitative review also highlight a trade- off between the depth of o1's responses and their practical utility in different clinical scenarios. Furthermore, in this study, overall, we did not observe notable reasoning improvements of o1 over its previous iteration GPT-40. This raises questions about o1's efficiency, practical applicability, and cost-effectiveness, particularly in time-sensitive ophthalmological practice, where speed and precision are critical.\nOur study has several key strengths. Notably, it pioneers a comprehensive and multidimensional evaluation of o1 in ophthalmology. Secondly, to our knowledge, we utilised the largest ophthalmological MCQ dataset to date, comprising of 6,990 questions, providing a robust foundation for assessing model accuracy and reasoning. Third, we conducted a thorough head-to-head comparison of o1 with five other general LLMs, offering a detailed performance benchmark. Furthermore, our assessment extended beyond accuracy, incorporating five distinct text-generation metrics to provide a multi- faceted evaluations of o1's strengths and limitations. Lastly, our detailed qualitative review of 40 randomly selected outputs offered qualitative insights into the differences in reasoning patterns and outputs between o1 and GPT-40.\nNevertheless, this study has a few limitations. Firstly, the five text-generation metrics used in this study are general and not specific to medical context. This warrants the need for medical-specific text generation metric for future study. Secondly, the use of MedMCQA's examination questions forms only the first step towards understanding the applicability of LLMs in ophthalmological clinical practice. Moving forward, there is a need for benchmarks that can be used for the systematic evaluation of their clinical value42. Future studies should aim to create standardised and diverse ophthalmology-specific validation datasets by curating diverse tasks, further including patient queries, and clinical management tasks25,42."}, {"title": "Conclusion", "content": "01 demonstrated superior accuracy in answering ophthalmology-related MCQs but showed inconsistencies in reasoning capabilities, largely ranking behind GPT-4o and GPT-4 in text-generation metrics. Contrary to expectations that o1's enhanced reasoning capabilities would extend to ophthalmology, its performance highlighted limitations in addressing domain-specific challenges. These findings suggest that a customised ophthalmology-specific LLM may still be necessary and underscore the importance of domain-specific evaluations."}]}