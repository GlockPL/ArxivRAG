{"title": "Deep Time Warping for Multiple Time Series Alignment", "authors": ["Alireza Nourbakhsh", "Hoda Mohammadzade"], "abstract": "Time Series Alignment is a crucial task in signal processing with wide-ranging applications. Real-world signals often suffer from temporal shifts and scaling, leading to errors in raw data classification. This paper presents a novel Deep Learning-based approach for Multiple Time Series Alignment (MTSA). Unlike existing methods, which mainly focus on Multiple Sequence Alignment (MSA) for biological sequences, there is a notable lack of alignment techniques for numerical time series. Traditional methods also typically address pairwise alignment, whereas our approach aligns all signals simultaneously, improving both alignment efficiency and computational speed. By decomposing signals into piece-wise linear sections, we introduce varying complexity into the warping function while ensuring compliance with three key constraints: boundary, monotonicity, and continuity conditions. Leveraging a deep convolutional network, we propose a new loss function that overcomes some limitations of Dynamic Time Warping (DTW). Experiments on the UCR Archive 2018, involving 129 time series datasets, show that our method significantly enhances classification accuracy, warping average, and runtime efficiency across most datasets.", "sections": [{"title": "1 Introduction", "content": "Multiple Sequence Alignment (MSA) and Multiple Time Series Alignment (MTSA) are essential in machine learning, data analysis, and bioinformatics, both aiming to align multiple inputs to identify patterns. The key difference lies in the data type: MSA aligns symbolic, discrete sequences like DNA, RNA, or proteins, while MTSA aligns continuous numerical signals, such as time series representing temporal or spatial measurements.\nBoth MSA and MTSA achieve alignment through a series of pairwise alignments. However, MTSA's numerical nature and higher computational complexity have restricted research in this area, whereas MSA has received extensive attention in the literature.\nThis paper addresses the research gap in MTSA by employing a multiple alignment algorithm instead of pairwise alignments, which leads to a better performance. Given the strong conceptual and methodological links between MSA and MTSA, we also review MSA approaches in the literature to gain insights for advancing MTSA methods."}, {"title": "1.1 Applications", "content": "The applications of MSA and MTSA can be categorized as follows:\nClassification: Time series classification presents challenges due to shifts and rescaling in similar signals. A proper pre-warping stage can improve accuracy, as shown in the Experiments section. Studies [1-3] have combined DTW and its extensions with Nearest Neighbor (NN) for classification, but DTW+NN requires computing DTW between each test and training sample. The Nearest Centroid (NC) algorithm [4] reduces this by aligning test samples with a representative signal per class, with [5] further refining this into a classifier. Selecting the representative signal is crucial, commonly performed using Dynamic Barycenter Averaging (DBA) [6], which iteratively aligns and updates the barycenter. Instead, we employ MTSA algorithms, achieving superior quality and performance, as demonstrated in the Experiments section.\nHuman Activity Recognition: HAR is a specialized classification task involving motion signals, widely used in surveillance, healthcare, assistive robotics, and human-machine interfaces. Here signal alignment is crucial due to variations in speed and initial phase across individuals performing activities like running or walking. Several time warping-based methods for HAR have been proposed in [1, 7\u201311].\nBiological Signal Analysis: Signals such as ECG, EEG, EMG, and PPG serve as the primary channels in an intelligent system aimed at understanding human health situations. Due to variations in amplitude and morphology among biological signals, the absence of labeled datasets, and the difficulty of labeling, even by experts, the development of unsupervised warping approaches becomes imperative. Authors in [12] employ an algorithm based on DTW to identify sub-patterns in signals, utilized for signal prediction. In [13] and [14], DTW is applied to eliminate unwanted noise from ECG signals. Additionally, [15] approximates DTW using a neural network on EEG signals.\nRecently DTW and alignment methods have also been used for applications such as video alignment [12, 16, 17] and time series forecasting [18, 19]. While there are numerous"}, {"title": "1.2 Methods", "content": "MSA is widely used in genomics, particularly for protein sequence analysis, leading to the development of numerous methods in this field. The first method discussed is ClustalW [20]. It performs pairwise alignment between signals to build a guide tree based on Progressive Alignment [21], which assumes that aligning two similar signals allows them to be treated as one. Through iterative pairwise alignment, a set of time series can be aligned, but the signals need to be homogeneous, such as motion or ECG signals.\nHidden Markov Model (HMM) is used for MSA in literatures like [22-24]. In [25], an unsupervised approach models each time series as a non-uniformly distributed sample from a latent trace, accounting for local rescaling and noise. For MTSA, alignment is conducted separately using DTW between each signal and the latent trace. Notably, [25] is one of the few works directly addressing numerical time series in MTSA.\nIn all the aforementioned works, Multiple Alignment is achieved through a series of pairwise alignments. Additionally, some studies like [26], propose a method for aligning two signals and then extend it to MSA by aligning each signal with the average signal."}, {"title": "2 Background", "content": "This section covers key concepts of MTSA, starting with warping and its definitions. It then explores DTW as the most common warping method, discusses its limitations, and presents novel approaches derived from it. Finally, the section outlines our contributions to the field."}, {"title": "2.1 Overview of Useful Definitions", "content": "Warping: Consider two time series X and Y with lengths N and M, respectively. The warping path, denoted as P, is a sequence with length $L \\in \\mathbb{N}$ defined as follows:\n$P = (p_1, ..., p_L)$ (1)\nIn Eq. 1 for $l \\in [1 : L]$ we have $p_l = (n_l, m_l) \\in [1 : N] \\times [1 : M]$. Clearly $L = max(N, M)$ and $p_l = (n_l, m_l)$ signifies that the index $n_l$ from X is warped to index $m_l$ from Y. Thus, the warping path encapsulates all the necessary information for aligning the two signals.\nTypically, three warping constraints are considered:"}, {"title": "2.2 DTW Problems", "content": "DTW stands as the most widely method used for aligning time series. For brevity, we omit the introduction of DTW, and the reader is directed to [27]. In this section, we address the challenges of DTW.\nPolynomial computational complexity: The main limitation of DTW is its polynomial computational complexity, making it unsuitable for large datasets. To address this, various extensions have been developed to reduce the complexity from polynomial to linear.\nSpeedup strategies fall into two categories: constraint addition and data abbreviation. In"}, {"title": "2.3 After DTW", "content": "In an attempt to address the limitations of DTW, several alternative methods have been proposed:\n\u2022 Generalized Time Warping (GTW) [8]: GTW addresses the polynomial complexity of DTW by introducing a linear-time algorithm that models the warping path as a linear combination of basis functions.\n\u2022 Trainable Time Warping (TTW) [32]: TTW enhances warping by operating in the continuous time domain with convolutional kernels, offering better performance for complex warpings.\n\u2022 Neural Time Warping (NTW) [10]: NTW relaxes the original DTW optimization problem to a continuous convex problem and finds the solution using a neural network.\nBoth TTW and NTW serve as approximations of the original DTW problem. Additionally, studies [33] and [34] introduce modifications to DTW to enhance its effectiveness in time series classification."}, {"title": "2.4 Using Deep Learning", "content": "Integrating deep neural networks, such as Convolutional Neural Networks (CNN) or Recurrent Neural Networks (RNN), into time series alignment provides significant advantages due to their structural flexibility, adaptable loss functions, and tunable hyperparameters."}, {"title": "2.5 Contributions", "content": "In our work, we have introduced the following contributions:\n\u2022 Linear Computational Complexity: Our model achieves linear inference complexity, addressing the polynomial complexity issue found in many previous MSA/MTSA methods.\n\u2022 Grouped MTSA Algorithm: Instead of performing multiple pairwise alignments like many previous MSA/MTSA methods, our proposed grouped MTSA algorithm enhances efficiency and scalability.\n\u2022 Deep Neural Network Utilization: By leveraging a deep neural network with an appropriate loss function, we address some drawbacks of DTW, improving the model's ability to capture complex time series relationships.\n\u2022 Decomposition of Nonlinear Warpings: We break down complex nonlinear warpings into piecewise linear segments, enabling varying levels of complexity through simple linear warpings for a more flexible and adaptive approach.\n\u2022 Warping Constraints Guarantee: Our approach ensures compliance with the three warping constraints, maintaining proper chronological order and continuity in alignment.\n\u2022 Improved Classification Accuracy: Using our MTSA method before classification has led to increased accuracy across nearly all UCR Archive 2018 datasets."}, {"title": "3 The Proposed Method", "content": ""}, {"title": "3.1 MTSA Problem Definition", "content": "Suppose N time series $X_1, X_2, ..., X_N$, where for $i \\in [1 : N]$, $X_i \\in \\mathbb{R}^{d_i \\times T_i}$ with $d_i$ and $T_i$ representing the dimension and length of $X_i$, respectively. Two models can be employed to express time warping:\n\u2022 Matrix Multiplication: Defining warping matrices as $W_i$ for $i \\in [1 : N]$, the warped form of $X_i$ can be expressed as $W_iX_i$, as detailed in Section 2.1. One possible MSE cost function for the MTSA problem can be formulated as shown in Eq. 4:\n$J_{MTSA1}(\\{W_i\\}) = \\sum_{i=1}^N \\sum_{j=1}^N ||W_iX_i - W_jX_j||$ (4)\n\u2022 Function Composition: Utilizing warping functions $\\tau_i$ for $i \\in [1 : N]$, the warped form of $X_i$ is $X_i \\circ \\tau_i = X_i(\\tau_i(t))$ and the associated cost function can be expressed as shown in Eq. 5:\n$J_{MTSA2}(\\{\\tau_i\\}) = \\sum_{i=1}^N \\sum_{j=1}^N ||X_i(\\tau_i(t)) - X_j(\\tau_j(t)) ||$ (5)"}, {"title": "3.2 Warping Function and Constraints", "content": "A linear warping function $\\tau(t) = at + b$ can be implemented using a neural network with two output parameters (a and b). However, this function is too simplistic for real-world scenarios. Instead, we adopt a more generalizable piece-wise linear function.\nIncreasing K introduces more non-linearity into the model. In this case, the neural network must output 2K non-negative parameters: $\\{a_1, a_2, ..., a_K, t_1, t_2, ..., t_K\\}$. The mathematical formulation of the warping function $\\tau(t)$ is given in Eq. 6.\n$\\tau(t) = \\begin{cases}\na_1t & t < t_1 \\\\\na_1t_1 + a_2(t - t_1) & t_1 \\leq t < t_1 + t_2 \\\\\n... \\\\\n\\sum_{k=1}^{K-1} a_kt_k + a_K(t - \\sum_{k=1}^{K-1}t_k) & \\sum_{k=1}^{K-1} t_k \\leq t < \\sum_{k=1}^{K} t_k\n\\end{cases}$ (6)\nThe warping constraints: We verify the validity of the three warping constraints in the warping function."}, {"title": "3.3 Non-differentiability Problem", "content": "Consider a neural network is trained to implement the warping function $\\tau(\\cdot)$, and let signal X with length T be inputted to the network. The warped signal is obtained as $X_{warp} = X(\\tau(\\cdot))$. Consequently, $X(\\tau(t))$ should be calculated for each $t \\in [1,T]$.\nHowever, if $\\tau(t)$ is not an integer, standard (hard) warping approximates it to the nearest integer since X is defined only at discrete time steps. This makes the loss function non-differentiable, as small changes in time ($t_k$) or amplitude ($a_k$) parameters may result in non-integer $\\tau(t)$, causing $X(\\tau(t))$ and the loss function to be undefined. Consequently, gradient-based optimization cannot be applied.\nTo solve this, soft warping is introduced, allowing $\\tau(t)$ to be a floating-point value. The warped signal $X_{warp}$ is then computed using interpolation. This interpolation is modeled through matrix multiplication (Eq. 4), where the warping matrix W contains values in the range [0,1]."}, {"title": "3.4 Neural Network Structure", "content": "The overall structure of the neural network is illustrated in Fig. 2. The input time series $X_1(t), X_2(t), ..., X_N(t)$ are assumed to have the same length at this stage; considerations for different-length time series will be addressed later. The primary network is a CNN with an input, three convolutional, a flatten and two dense layers."}, {"title": "3.5 Loss Function", "content": "As discussed in Section 2, DTW faces issues like computational complexity and singularity. To address singularity, we propose two solutions: First, using convolutional kernels in CNNs for feature extraction, allowing local patterns at each temporal point to influence adjacent points, creating relationships between them. Second, instead of relying on traditional DTW algorithms with MSE loss functions, which can cause singularity due to their point-wise nature, we implement a more robust loss function that captures the overall similarity between two signals, rather than just point-to-point proximity."}, {"title": "3.6 Training and Testing Procedure", "content": "In this section, we explain how our framework extends to the multiple time series case for the MTSA problem. Consider Fig. 2, where the signals in the input dataset $X_i$ for $i \\in [1 : N]$ have the same length T. If their lengths differ, a pre-processing stage will equalize them.\nBelow is the proposed algorithm for the training procedure:\n1. Apply each time series $X_i$ to the network input.\n2. Obtain amplitude parameters $\\{a_1, a_2, a_3, a_4\\}$ and time parameters $\\{t_1, t_2, t_3, t_4\\}$ from the network.\n3. Utilize the warper block to generate the warping matrix associated with these values and multiply it with the input time series to construct $X_{i,warp}$.\n4. The loss function block calculates the average final loss between $X_{i,warp}$ and each of the other N-1 signals according to Eq. 12.\n5. Replace the original $X_i$ with its warped version $X_{i,warp}$.\n6. Repeat steps 1-5 for all N signals, completing one epoch of training.\n7. Perform an appropriate number of epochs to gradually align signals to each other.\nSubstituting signals with their warped versions is essential in our MTSA framework. However, early in training, the network may lack meaningful warpings. Delaying substitution until the model learns more relevant information ensures stable and informed dataset updates.\nUltimately, the network aligns N input signals, enabling accurate warping of homogeneous test time series. During testing (illustrated in Fig. 2), the process remains the same except for omitting the loss function block. The input test signal $X_i$ is processed by the network, producing the warped test signal $X_{i,warp}$, via the warper block.\nA key benefit of using deep neural networks for time series alignment is the elimination of backpropagation during testing. Unlike conventional methods such as DTW, which require repeated optimization for each alignment, our approach uses a parameterized network that learns to align signals efficiently."}, {"title": "4 Experiments", "content": "This paper conducts four experiments using the UCR Time Series Classification Archive [37], which includes 128 univariate time series datasets. The first experiment addresses the MTSA problem by aligning test signals to training signals. The second experiment explores warped averaging as a key MTSA application, highlighting notable cases to evaluate the method's performance. The third experiment involves a classification test on 90 datasets, reporting accuracy for a Nearest Neighbor classifier in four scenarios: no warping, DTW, DBA, and the proposed approach. The fourth experiment validates the method's superiority by measuring classification rate and error using a deep ResNet classifier.\nThe convolutional neural network consists of three layers with filter sizes of 13, 7, and 3, and filter counts of 128, 64, and 32, respectively. Each convolutional layer is followed by an average pooling layer (stride 1, sizes 6, 4, and 2). After the third layer, the tensor is flattened and processed by two parallel dense layers, each with 4 output neurons representing $\\{a_1, a_2, a_3, a_4\\}$ and $\\{t_1, t_2, t_3, t_4\\}$. ReLU activation ensures non-negative, unbounded outputs for a and t.\nThe hyperparameters $\u03bb_1$ and $\u03bb_2$ in Eq. 12 are set to 0.5 for most datasets. Although optimizing them individually could improve results, we avoided this due to its time-intensive nature. The learning rate is fixed at 10-3. Training runs for 25 epochs, with checkpoints saved every 5 epochs to account for potential early stopping benefits. The best model is chosen based on validation accuracy. The implementation uses the PyTorch library."}, {"title": "4.1 The Multiple Time Series Alignment (MTSA)", "content": "A key application of MTSA is computing a warped average to represent a set of signals, as a simple arithmetic average cannot handle temporal shifts or scale variations. DBA [6], a robust MTSA method, iteratively uses DTW to align signals with an evolving average. In this study, DBA is used as the baseline for MTSA (in this section) and warped averaging (in the next section) to demonstrate the advantages of our proposed time series alignment approach.\nFor each dataset, signals with the same label are inputted into the model to ensure homogeneity. Standard UCR dataset train-test splits are used, with the training set for model training. The goal is to optimally align five test signals with their corresponding training signals. Fig. 4 illustrates results for various datasets and labels, showing red signals warped to align with gray signals, producing green signals. In cases like \"Plane: 4\" and"}, {"title": "4.2 Representative and Warped Averaging", "content": "In this section, we provide visual comparisons demonstrating the advantages of our approach over the DBA algorithm in computing the warped average signal and effectively addressing various challenges.\nOverall Comparison: An overall test on the GunPoint dataset evaluates our method's performance. The results highlight that the simple average fails to capture slightly complex trends, particularly for label 2, while DBA introduces unwanted spikes. In contrast, our method aligns signals effectively, producing a warped average that preserves the trend of its signals and serves as a representative for each class.\nPreserve Signal Shapes: Preserving signal shapes is crucial in warped averaging, especially for challenging datasets like Trace. Simple averaging fails to capture the true shape of signals. While DBA improves the results, our approach,"}, {"title": "4.3 The Comprehensive Classification Test", "content": "This section and the next aim to show how our proposed warper network enhances classification quality, using classification accuracy as the metric. Since classification is not the main focus, we employ the simplest classifier, nearest neighbor (NN), and evaluate"}, {"title": "4.4 Deep Network Classification", "content": "After evaluating our method's effectiveness in enhancing the accuracy of a simple nearest neighbor classifier, this section examines its performance with a more advanced and complex classifier."}, {"title": "5 Conclusion", "content": "We present a novel deep learning-based framework for MTSA, addressing a largely overlooked problem in the literature. Unlike traditional MSA methods that rely on pairwise alignments, leading to high computational complexity, our approach introduces a grouped multiple alignment algorithm that aligns all signals together. Additionally, we decompose complex non-linear warpings into simpler linear sections, ensuring a general time warping that adheres to three essential constraints. By optimizing cost functions and training procedures, our method achieves promising results in both time series classification and warped averaging."}]}