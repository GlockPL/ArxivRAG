{"title": "Learn-by-interact: A Data-Centric Framework for Self-Adaptive Agents in Realistic Environments", "authors": ["Hongjin Su", "Ruoxi Sun", "Jinsung Yoon", "Pengcheng Yin", "Tao Yu", "Sercan \u00d6. Ar\u0131k"], "abstract": "Autonomous agents powered by large language models (LLMs) have the potential to enhance human capabilities, assisting with digital tasks from sending emails to performing data analysis. The abilities of existing LLMs at such tasks are often hindered by the lack of high-quality agent data from the corresponding environments they interact with. We propose Learn-BY-INTERACT, a data-centric framework to adapt LLM agents to any given environments without human annotations. LEARN-BY-INTERACT synthesizes trajectories of agent-environment interactions based on documentations, and constructs instructions by summarizing or abstracting the interaction histories, a process called backward construction. We assess the quality of our synthetic data by using them in both training-based scenarios and training-free in-context learning (ICL), where we craft innovative retrieval approaches optimized for agents. Extensive experiments on SWE-bench, WebArena, OSWorld and Spider2-V spanning across realistic coding, web, and desktop environments show the effectiveness of LEARN-BY-INTERACT in various downstream agentic tasks \u2013 baseline results are improved by up to 12.2% for ICL with Claude-3.5 and 19.5% for training with Codestral-22B. We further demonstrate the critical role of backward construction, which provides up to 14.0% improvement for training. Our ablation studies demonstrate the efficiency provided by our synthesized data in ICL and the superiority of our retrieval pipeline over alternative approaches like conventional retrieval-augmented generation (RAG). We expect that LEARN-BY-INTERACT will serve as a foundation for agent data synthesis as LLMs are increasingly deployed at real-world environments.", "sections": [{"title": "1 Introduction", "content": "Pre-trained large language models (LLMs) offer great potential for assisting humans with various tasks in digital settings, such as editing images, performing data analysis, resolving software engineering issues, and navigating commercial platforms (Jimenez et al., 2023; Xie et al., 2023, 2024; Yao et al., 2022a). By streamlining these, LLM agents can greatly enhance human efficiency and productivity, allowing users to shift their focus toward higher-level, creative, and strategic endeavors. To explore this potential, many benchmarks (Cao et al., 2024; Jimenez et al., 2023; Koh et al., 2024; Xie et al., 2024; Zhou et al., 2023b) and agentic frameworks (Chen et al., 2024a; Gur et al., 2023; Yang et al., 2024, 2023; Zhan and Zhang, 2023) have been established based on realistic digital environments, spanning web applications, code development, desktop computing, etc. However, LLMs often fall short of expected performance in these tasks, consistently displaying a significant gap compared to human capabilities. As a result, they remain less practical and reliable for real-world applications.\nEfficient adaptation to new environments can be a key part of the performance improvements. Prior works have explored various prompt-based approaches (Gur et al., 2023; Yang et al., 2024; Yao et al., 2022b; Zhan and Zhang, 2023), that are constrained by the capabilities of underlying foundation models. Other studies on training LLMs with human-labeled examples (Chen et al., 2023,"}, {"title": "2 Learn-by-interact", "content": "We introduce the proposed LEARN-BY-INTERACT framework to synthesize agent data in an autonomous way by leveraging interactions between LLMs and environments. We first formalize the canonical agentic tasks (\u00a72.1), and introduce the detailed synthesis (\u00a72.2) and filtering (\u00a72.3) procedures. We then describe the application of the synthesized data in adapting LLMs in both training-free and training-based settings (\u00a72.4)."}, {"title": "2.1 Task formulation", "content": "Given an environment E and a task instruction I, the objective of an agent A is to achieve the target G through multi-step interactions with E. At each step i, A predicts the next action ai based on the instruction I and the previous history H = (o0, a1, o1, a2, ..., oi\u22121), which is then executed in the environment E to get a new observation oi. The interactions terminated until A predicts the action stop or the maximum number of steps m is reached."}, {"title": "2.2 Agentic data synthesis", "content": "The essential idea of LEARN-BY-INTERACT is manifested in synthesizing environment-specific agent data with zero human effort. In Algorithm 1, we show the overall process with pseudo-code. Given an environment for a downstream application (e.g., Visual Studio Code), we first leverage commonly-accessible resources such as documentation to generate diverse task instructions using self-instruct (Wang et al., 2022b) (line 5). These resources are usually created by human experts to address common concerns and provide usage suggestions, e.g., how to navigate a website or operate a software. Intuitively, such references often cover representative use cases of an application. Therefore,"}, {"title": "2.3 Filtering", "content": "To further enhance the data quality, we design the following criteria to filter inferior synthesized data: (1). Remove duplicate states: We remove duplicate (ai, oi) from T' if (ai, oi)=(ai\u22121, oi\u22121), which is potentially introduced by the invalid action or the environment error (inactivity). (2). LLM committee check: We feed the generated instruction-trajectory pair (I', T') into a committee of LLMs, and only classify it of high-quality if all LLMs consider the trajectory coherent, natural, reasonable and aligned with the instruction. The listed criteria are all fully-autonomous and canonically-applicable for filtering data synthesized in general agent scenarios. Additionally, we employ iterative prompting to augment LLMs with high-quality examples to enhance their capabilities in data generation. See Table 31 for our prompts used in LLM committee check."}, {"title": "2.4 Adaptation", "content": "After obtaining the synthesized data D, we apply it to both ICL and training. Given the unique characteristics of multi-round interactions with environments in agent settings, we design agentic retrieval (pseudo-code in Algorithm 2) to maximize the effectiveness of the synthesized data. Specifically, we propose two retrieval pipelines: observation-based (line 5-14) and model-based retrieval (line 15-17). In observation-based retrieval, we compare the current observation o to the trajectory of each example e in the synthesized data, where e = [I', [o0, a1, o1, ..., an, On]]. If o matches one of the observations in e, i.e., o = oi, then we consider e as a helpful example to the current task. For the model-based retrieval, we leverage LLMs to first write queries based on the instruction, the interaction history and the current observation (line 16), and then employ retrieval models to retrieve non-duplicate examples (line 17). LLMs are then augmented with the retrieved examples to predict the next action (line 18). Refer to Table 32 to 35 for prompts to write queries and predict actions.\nApart from using the synthesized data as demonstration examples in ICL, we further utilize them to fine-tune models. For a given generated example, we convert it to the format of action prediction (Table 32), and prepare input-output pairs for supervised fine-tuning. More details on the experimental settings can be found in \u00a73.3."}, {"title": "3 Experiments", "content": null}, {"title": "3.1 Baselines", "content": "We compare ICL with agentic retrieval to the following prompt-based approaches.\n\u2022 Baseline: The vanilla prediction pipeline in each benchmark that includes the task instruction, interaction history and the state observation in the prompt. See more implementation details in Appendix A.\n\u2022 RAG: The conventional RAG pipeline that first retrieves from the resources like documentation based on the instruction, and augments LLMs with the retrieved content.\n\u2022 Data distill: We follow the same pipeline to synthesize data in Algorithm 1 except backward construction (replace lines 15-22 with D.append(I, T)), and follow Algorithm 2 during the evaluation.\n\u2022 Reflexion (Shinn et al., 2024): A general framework to reinforce language agents through linguistic feedback from both executors and LLMs.\n\u2022 Language Agent Tree Search (LATS) (Zhou et al., 2023a): It integrates the combinatorial tree search into expanding ReAct (Yao et al., 2022b) and combine agent online reasoning, acting and planning throughout the trajectory.\nFor the training-based evaluation, we primarily compare to the data distillation, which also constructs"}, {"title": "3.2 Datasets", "content": "We consider the four agentic datasets that involve multi-round interactions with realistic environments. They span diverse domains of code, web, computer desktop and professional software. Appendix B illustrates details of each dataset with examples.\n\u2022 SWE-bench (Jimenez et al., 2023) is an evaluation benchmark on realistic software engineering problems from realistic Github issues. We use the verified version by default throughout the experiments.\n\u2022 Webarena (Zhou et al., 2023b) evaluates agent capabilities to perform tasks in the web environments such as e-commerce, social forum discussion, and beyond.\n\u2022 OSWorld (Xie et al., 2024) is an integrated environment for assessing open-ended computer tasks, which involve diverse applications like Terminal, Chrome, etc.\n\u2022 Spider2-V (Cao et al., 2024) is a multimodal agent benchmark focusing on professional data science and engineering workflows, which includes BigQuery, Airbyte and more."}, {"title": "3.3 Settings", "content": "We synthesize one separate set of environment-specific data for each evaluated benchmark. Throughout the data synthesis process, we employ the Claude-3.5-sonnet (Anthropic, 2024) as the generator model and both Gemini-1.5-pro (Reid et al., 2024) and Claude-3.5-sonnet as the LLM committee for filtering low-quality data. For each document, we sample three task instructions from LLMs. The statistics for generated raw trajectories, examples before and after filtering are shown in Table 1. In Appendix D, we list document sources used for each benchmark. During ICL, we retrieve examples until the maximum length of LLMs and set an upper bound of 5 for both model-based and observation-based retrieval (m1 = 5, m2 = 5 in Algorithm 2). We leverage Gemini-1.5-pro (Reid et al., 2024) and Claude-3.5-sonnet (Anthropic, 2024)1, Codegemma-7B (Team, 2024a) and Codestral-22B (Team, 2024b) in the ICL evaluation, and tune Codegemma-7B and Codestral-22B with LoRA (Hu et al., 2021) to evaluate the data quality as training sources. By default, we do not include retrieval content in evaluating the trained model to avoid the confusion in understanding the effectiveness of our synthesized data in training. We include more detailed hyper-parameter settings (both existing approaches and LEARN-BY-INTERACT) and machine information in Appendix C."}, {"title": "3.4 Evaluation", "content": "We follow the default evaluation metrics designed by the original benchmarks. On SWE-bench (Jimenez et al., 2023), we apply the generated patch program to the repository codebase, and measure the agent performance by execution accuracy (pass@1). On WebArena (Zhou et al., 2023b), we employ both LLM-based fuzzy match and string match that checks keywords in predictions. Slightly different from the original work that uses gpt-4-0613 as the LLM judge, we use Claude-3.5-sonnet as a similar replacement. On OSWorld (Xie et al., 2024), we leverage the sample-specific evaluation scripts to assess the functional correctness of the task completion, which processes environment states and checks if agents finish the task as expected. On Spider2-V (Cao et al., 2024), we utilize file-based comparison, information-based validation, execution-based verification to determine whether a task is successfully completed. All performance numbers throughout the paper are shown in the percentage of resolved instances with % omitted for brevity."}, {"title": "3.5 Results", "content": null}, {"title": "3.5.1 Training-free Evaluation", "content": "We first consider LEARN-BY-INTERACT in the training-free setting, where the proposed methods can be applied to the commercial LLMs even with prediction-only API access.\nResults on Table 2 show marginal improvement of RAG compared to the baseline, which suggests limited effectiveness by simply concatenating standard resources to LLM prompts. By retrieving examples from distilled data, we observe better performance compared to RAG, but still no more than 2% improvement over the baseline, which indicates that the distilled data tend to be noisy in the setting with multi-round agent-environment interactions. This highlights the critical role of backward construction, which corrects the misalignment between instructions and trajectories by curating new task objectives.\nBoth Reflexion and LATS consistently improve over the baseline across 4 benchmarks, which"}, {"title": "3.5.2 Training-based Evaluation", "content": "We consider the data synthesized by LEARN-BY-INTERACT in the scenario of LLM tuning, which is applicable to the LLMs with access to weight updates.\nThe results presented in Table 3 reveal that LEARN-BY-INTERACT substantially surpasses both the baseline and data distillation, suggesting its capacity to generate high-quality training data that enables language models to learn and adapt efficiently. We discover that utilizing our synthesized data for model training yields better results compared to using it as in-context learning (ICL) examples. A notable instance is in WebArena, where Codestral-22B's performance jumps from 4.7% to 24.2% when trained on our synthesized data, while only showing a 5.5% improvement in the ICL scenario. Remarkably, the Codestral-22B model trained with our synthesized data even outperforms Gemini when the latter uses our data as demonstration examples."}, {"title": "4 Analysis", "content": null}, {"title": "4.1 Inference Efficiency", "content": "We compare the efficiency of different pipelines at inference. We analyze the trade-off between downstream task performance and the required computational costs. We focus on measuring the number of LLM calls and consumed tokens per example, which are averaged across four evaluated datasets (\u00a73.2) using Claude-3.5-sonnet. As illustrated in Fig. 2, while Reflexion and LATS demonstrate"}, {"title": "4.2 The Impact of Retrieval", "content": "As mentioned in \u00a72.4, we employ both model-based and observation-based retrieval in our evaluation with ICL. We analyze their effectiveness by incorporating only one of them (skip lines 5-14 in Algorithm 2 for model-based retrieval only and skip lines 15-17 for observation-based retrieval only). In addition, we compare to two baselines: (1) no retrieval: LLMs predict each action in the zero-shot setting; and (2) instruction-based: only use instructions to retrieve synthesized data and apply the same demonstration examples in every action prediction throughout the trajectory.\nThe results presented in Table 4 illustrate how various retrieval methods impact LLMs when using the synthetic data as the retrieval source. Despite having access to the same example pool (except the baseline without using retrieval), there are notable differences in performance across different retrieval strategies, highlighting the crucial role of agentic retrieval in effectively utiliz-"}, {"title": "4.3 Data granularity", "content": "As mentioned in \u00a72.2, we synthesize data by taking contiguous sub-trajectories from the full generation paths of LLMs, i.e. T' = T[i : j], which results in trajectories of diverse lengths in the synthesized data. We divide the synthetic data into three groups: (1). trajectory steps < 5 (short); (2). 5 \u2264 trajectory steps < 10 (medium); (3). trajectory steps \u2265 10 (long), and leverage each group and their combinations in both the training-free and the training-based process. To ensure a fair comparison, we constraint the data size in each group and combined group to 200M tokens2, utilizing Su et al. (2022) for sub-sampling. Table 5 presents the results. In both training-free and training-based evaluation, LLMs derive greater advantages from short-trajectory data, as demonstrated by its consistently superior performance compared to medium and long-trajectory data with Claude-3.5-sonnet and Codestral-22B. This can be attributed to the versatility of short-trajectory data, which usually serves as a sub-step or a partial workflow in downstream tasks. The combination of any two data groups proves more effective than relying on a single group, showcasing the complementary nature of diverse data sets. For instance, in Webarena with Codestral-22B, incorporating examples with both short and medium-length trajectories shows additional improvement over using either one exclusively. This underscores the value of considering the trajectory length as a unique dimension of agentic data synthesis."}, {"title": "4.4 Scaling Laws", "content": "We examine how the model performance improves as the synthetic data size scales up. Figure 3 presents two sets of results, with training-free (where Claude, Gemini, Codegemma and Codestral use retrieval augmentation without training) and with training-based (where fine-tuned Codegemma and Codestral models are evaluated without retrieval). All results are averaged across Webarena and OSworld due to computational resource constraints. The findings indicate that both learning paradigms benefit from larger data, suggesting the synthetic data is diverse and high-quality. In the training-free evaluation, more substantial improvements are observed for larger models (Claude and Gemini) compared to smaller ones (Codegemma and Codestral), possibly due to the their enhanced in-context learning abilities. Our analysis also reveals that for a given amount of synthetic data, fine-tuning smaller models is more effective than using the data as demonstration examples during evaluation."}, {"title": "5 Related work", "content": "Various agents based on LLMs have been developed (Huang et al., 2022; Shinn et al., 2024; Wang et al., 2023a, 2024a, 2023b; Zhang et al., 2024). React (Yao et al., 2022b) proposes to synergize reasoning and acting in LLMs. By integrating Monte Carlo Tree Search (Coulom, 2006; Kocsis and Szepesv\u00e1ri, 2006), Zhou et al. (2023a) leverages LLM-powered value functions and self-reflection (Madaan et al., 2024) to encourage proficient exploration and decision-making. However, it comes with increased computational costs and relies on the premise that the environment allows for state reversals. In contrast, LEARN-BY-INTERACT removes such assumptions and improves both agent efficiency and performance by synthesizing high-quality data in advance.\nAnother line of research to improve agent models relies on training on human-labeled examples (Chen et al., 2024b; Deng et al., 2024; Wang et al., 2022a; Yin et al., 2023; Zeng et al., 2023) or data distilled from LLMs like GPT-4 (Chen et al., 2023; Zhao et al., 2024). AgentGen (Hu et al., 2024) explores automatic synthesis of both environments and tasks and then leverages FastDownward\u00b3 to generate trajectory data. AgentTuning (Zeng et al., 2023) utilizes both existing datasets and self-instruct (Wang et al., 2022b) to derive instructions and then samples trajectories from GPT-4 (Achiam et al., 2023). In contrast, LEARN-BY-INTERACT focuses on realistic environments and generate tasks and trajectories using backward construction. Some other researchers are also exploring ways to use data more efficiently with reinforcement learning (Ball et al., 2023; Nachum et al., 2018; Schwarzer et al., 2020, 2021; Thomas and Brunskill, 2016). Gulcehre et al. (2023) suggests using"}, {"title": "6 Conclusion", "content": "We introduce LEARN-BY-INTERACT, a data-centric framework to adapt LLM agents to any given environments without human annotations. Based on commonly-accessible resources like documentaion, LLMs propose downstream tasks and complete them with multi-round interactions with environments. We address the misalignment between instructions and trajectories by updating objectives with new instructions derived from trajectories. Additionally, we design innovative retrieval approaches that leverage agent instructions, interaction histories, and current observations to retrieve synthesized examples. Through extensive experiments, we demonstrate that the synthetic data from LEARN-BY-INTERACT significantly enhances model performance with both ICL and training. Compared with other leading approaches in agent tasks, LEARN-BY-INTERACT shows much better performance with lower latency and computational costs, which make it particularly suitable for large-scale deployment. Further analysis has also shown the superiority of LEARN-BY-INTERACT over the classical RAG. In future work, we plan to explore multi-modal settings and train general agent models widely applicable in realistic environments. We anticipate that LEARN-BY-INTERACT will inspire future research to push the state-of-the-art in this direction."}, {"title": "7 Limitations", "content": "Although LEARN-BY-INTERACT effectively synthesizes high-quality agentic data with trajectories, it requires a lot of LLM calls in generation and filtering. We hope that future works will explore more efficient approaches to complete annotations without sacrificing quality. Additionally, LEARN-BY-INTERACT leverages the environment-related resources to generate instructions. In some scenarios, however, these resources may be incomplete or not available."}, {"title": "A Baseline implementations", "content": "We follow the existing frameworks to set up baselines in each benchmark. In SWE-bench (Jimenez et al., 2023), we follow CodeAct (Wang et al., 2024b), where LLMs interact with environments to solve problems. In WebArena (Zhou et al., 2023b), we follow the implementation in Drouin et al. (2024), which concatenates task objectives, action space descriptions, general instructions (e.g., output formats) and webpage observations in the prompt, and ask LMs to predict the next action. By default, we use the accessibility tree\u2074 as the observation space. In OSWorld (Xie et al., 2024) and Spider2-V (Cao et al., 2024), we follow the original prompt style designed by the benchmark, which also concatenates task objectives, action space descriptions, general instructions and computer observations in the prompt. By default, we use the accessibility tree as the observation space for OSWorld, and use the set-of-mark for Spider2-V due to the significant information loss of the accessibility tree in the original benchmark. See an example in Table 18 and 19 for more details."}, {"title": "B Dataset examples", "content": "From Table 8 to 17, we provide one example for each dataset with full instructions, interaction history with the environment."}, {"title": "C Experimental settings", "content": "We retrieve documents until the maximum length of LLMs for RAG and set an upper bound number of 50 documents, where the retrieved documents remain unchanged throughout agent interaction trajectory because only instructions are used as the query for retrieval. For Reflexion (Shinn et al., 2024), we use the maximum trials 3. In LATS (Zhou et al., 2023a), we use the number of generated action 5, depth limit 15, value function weight 0.8, following the original setting in paper with WebShop (Yao et al., 2022a), which is also an agent task based on website. By default, we use https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings as the dense retriever for model-based retrieval. We use the temperature 0 throughout the experiments to ensure better reproductivity of the experiments. During training, we the batch size 128, learning rate 0.00002, warmup ratio 0.03 and maximum length 8192, and tune the model for 3 epochs. All experiments are conducted in H100 machines with 80GB memeory."}, {"title": "D Document sources", "content": "We use all the non-repeated python files in SWE-bench-Verified (Jimenez et al., 2023) as the document sources. Although we may not always find abundant documentations and tutorials for each environment, we believe that documentations in the same domain still have a good coverage of frequent operations. For example, one subset of WebArena (Zhou et al., 2023b) focuses on the navigation of the shopping website OneStopMarket, we use the Amazon documentation as a good replacement. Regardless of the shopping websites, the frequent tasks usually include order change, product search, delivery checking, etc. Therefore, we use other documentations in the same domain to sample task"}, {"title": "E Synthesized data examples", "content": "From Table 20 to 26, we provide a complete example of data synthesis. To begin with, an LLM generates instructions based on standard resources like tutorials, documentations and FAQs: Upload CSV data in Google Drive to BigQuery. (See prompt in Table 29) It then attempts solve the task by predicting actions and collecting feedback from environments (interactions). This produces a long trajectory showing how LLMs try to achieve the goal.\nHowever, it is not guaranteed that the trajectory successfully achieves the target. In our example, the LLM makes a wrong prediction in the action 4. It selects the table source Google Cloud Storage, while the correct action should select \"Drive\" to align with the instruction that reuiqres to upload CSV data in Google Drive. This results in wrong actions in the subsequent predictions, and the generated trajectory is not aligned with the initial instruction, which leads to noisy data in this case."}, {"title": "F Case study on filtered examples", "content": "In Table 36-45, we demonstrate the representative synthesized examples that fail to meet our designed criteria. The example in Table 36-41 is filtered because the trajectory shows detour in accomplishing the goal, i.e. Action 1-6 are not necessary. The example in Table 42-45 is filtered because it goes back and forth in states, i.e. repeat the actions of clicking \"My Orders\" and clicking \"View Order\". We filter these low-quality examples to avoid their negative influences in the downstream applications."}, {"title": "G Synthesized data from environments", "content": "We compare LEARN-BY-INTERACT with the version without relying on existing resources in WebArena. Except for sampling task instructions from LLMs based on given environments, we follow the same procedures in Learn-by-interact to synthesize 10k examples. The results of in-context learning with Claude-3.5-sonnet are shown in Table 6\nHowever, we note the following potential concerns regarding the version without replying on existing resources: the distribution and the diversity of the generated data are hard to control. Without conditioning on prior documents, one will need intensive prompt engineering to guide LLMs in generating diverse task instructions. On the other hand, the related resources are usually crafted by experts or written by real users, which cover most important scenarios that people who interact with the environment are interested in."}, {"title": "H Cross-website generalization", "content": "To evaluate the generalization capabilities of LEARN-BY-INTERACT, in WebArena, we consider the content management systems (CMS) as a held-out test set and leverage the synthetic data from the"}]}