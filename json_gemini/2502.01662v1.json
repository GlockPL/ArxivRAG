{"title": "Speculative Ensemble: Fast Large Language Model Ensemble via Speculation", "authors": ["Jiale Fu", "Yuchu Jiang", "Junkai Chen", "Jiaming Fan", "Xin Geng", "Xu Yang"], "abstract": "Ensemble methods enhance Large Language Models (LLMs) by combining multiple models but suffer from high computational costs. In this paper, we introduce Speculative Ensemble, a novel framework that accelerates LLM ensembles without sacrificing performance, inspired by Speculative Decoding\u2014where a small proposal model generates tokens sequentially, and a larger target model verifies them in parallel. Our approach builds on two key insights: (1) the verification distribution can be the ensemble distribution of both the proposal and target models, and (2) alternating each model as the proposer and verifier can further enhance efficiency. We generalize this method to ensembles with n models and theoretically prove that SE is never slower than a standard ensemble, typically achieving faster speed. Extensive experiments demonstrate speed improvements of 1.11x-2.23x over standard ensemble techniques without compromising generation quality.", "sections": [{"title": "1. Introduction", "content": "Recent advancements in Large Language Models (LLMs) have been substantial, with both closed-source and open-source frameworks pushing the boundaries of performance. Closed-source models, such as GPT-4 and the Gemini series, have set high-performance benchmarks, while open-source models like Llama, Deepseek, and the Qwen series not only approach these benchmarks but also bring diversity in architecture, training data, and strategies. This variability has spurred interest in ensemble approaches that leverage the strengths of multiple models. Beyond traditional methods that average the probabilities of LLMs with similar capabilities, more sophisticated ensemble strategies have emerged. These include integrating three or more models to enhance performance and using contrastive decoding, which involves subtracting the logits of a smaller LLM from a larger one to reduce biases.\nDespite the significant progress in ensemble techniques, a persistent challenge remains. Specifically, when combining multiple models for prediction, each LLM must sequentially compute the token distribution, leading to a considerable slowdown compared to the inference time of a single model. This raises the crucial question of whether ensemble speed can be increased without sacrificing the ensemble quality. To address this issue, we introduce Speculative Ensemble (SE), a novel ensemble approach inspired by Speculative Decoding (SD).\nSD is a technique designed to accelerate LLM inference without sacrificing performance. As depicted in Figure 1 (b), it uses a smaller but more efficient proposal model Mq to rapidly generate proposal tokens, which are then"}, {"title": "2. Related Work", "content": "Large Language Model Ensemble. LLM ensembles combine multiple pretrained models to enhance output quality and robustness. Existing approaches can be categorized based on the application stage: pre-inference, post-inference, and during-inference (Lu et al., 2024a)."}, {"title": "3. Speculative Ensemble", "content": "Speculative decoding (SD) is a technique designed to speed up inference while maintaining the quality of generated outputs. It involves two phases: the proposal phase and the verification phase. During the proposal phase, a lightweight proposal model sequentially generates proposal tokens. In the verification phase, a larger target model verifies these tokens in parallel. Furthermore, by incorporating appropriate acceptance-rejection criteria, the technique ensures that the generated tokens align precisely with the target model's distribution, thus maintaining high-quality results.\nSpecifically, in the proposal phase, the proposal model Mq generates a sequence of length \\(y\\), denoted as:\n\\((x_{i+1}, x_{i+2}, ..., x_{i+y}) \\sim \\prod_{j=1}^{y} q_{i+j}(x).\\) (1)\nHere, \\(x_{i+j}\\) represents the token generated at position \\(i + j\\), and \\(q_{i+j}(x) \\equiv q(x_{i+j} | X_{\\leq i+j-1})\\) is the conditional probability distribution computed by Mq over \\(x_{i+j}\\), given the previously generated sequence \\(X_{<i+j-1}\\).\nIn the verification phase, the target model Mp executes a forward pass, producing \\(y + 1\\) target distributions: \\(p_{i+1}(x),..., p_{i+y}(x), p_{i+y+1}(x)\\). The first \\(y\\) distributions are subsequently used to validate the proposal tokens generated in the proposal phase. Specifically, a proposal token \\(x_{i+j}\\) is accepted if the following condition holds:\n\\(u_j \\leq min \\left(1, \\frac{p_{i+j}(x)}{q_{i+j}(x)} \\right)\\) (2)\nwhere \\(u_j \\sim U(0, 1)\\) represents a uniformly distributed random variable. If the token \\(x_{i+j}\\) is rejected, the subsequent tokens \\(x_{i+j+1},..., x_{i+y+1}\\) are discarded, and \\(x_{i+j}\\) is sampled from the distribution norm(max(0, \\(p_{i+j} - q_{i+j}\\))). If all \\(y\\) tokens are accepted, an additional token is directly sampled from \\(p_{i+y+1}\\) and appended to the generated sequence, referred to as the bonus token in our paper.\nBy iteratively alternating between the proposal and verification phases, SD improves inference speed while ensuring the generated tokens align with the target model's distribution."}, {"title": "3.2. Speculative-based Ensemble", "content": "As discussed above, vanilla SD can only accelerate the inference of a single model. In this subsection, we will introduce how to apply SD to scenarios involving an arbitrary ensemble of two models. Specifically, let \\(q_i(x)\\) and \\(p_i(x)\\) denote the distributions of token \\(x_i\\) given by the proposal model and the target model, respectively, and let \\(l^q\\) and \\(l^p\\) be the corresponding logits. Then, the ensemble distribution \\(r_i(x)\\) can be expressed as\n\\(r_i(x) = E(q_i(x), p_i(x)) \\text{ or } E'(l^q, l^p),\\) (3)\nwhere \\(E(\\cdot)\\) represents the ensemble function at the probability level, while \\(E'(\\cdot)\\) is at logits level. For example, the common weighted ensemble form that uses probability for weighted summation can be expressed as\n\\(r(x) = E(q(x), p(x)) = \\lambda q(x) + (1 - \\lambda) p(x),\\) (4)\nwhile contrastive decoding can be represented as\n\\(r(x) = E'(l^q, l^p) = \\text{Softmax}(l^p - \\mu l^q).\\) (5)"}, {"title": "3.3. Alternate Proposal Framework", "content": "In Section 3.2, we explore the application of speculative decoding to LLM ensembles. However, we don't consider the bonus token, that is, the additional token generated when all proposal tokens are accepted. This is because the bonus token follows the distribution of the verifier rather than the ensemble distribution and can not be directly appended to the output sequence. In this subsection, we introduce an ensemble framework, termed the alternate proposal framework, which effectively leverages the bonus token and demonstrates superior performance.\nAs shown in Figure 2, in the alternate proposal framework, the generation of a bonus token is treated as a proposal from the current verifier, which is subsequently verified by the current proposer. Specifically, let the proposer be denoted as Mq and the verifier as Mp with proposal lengths \\(\\gamma_q\\) and \\(\\gamma_p\\), respectively. If all tokens proposed by Mq are accepted, a total of \\(\\gamma_q + 1\\) tokens will be generated. The first \\(\\gamma_q\\) tokens follows the distribution \\(r_{i+j}(x) = E(q_{i+j}(x), p_{i+j}(x))\\), for \\(j = 1,...,\\gamma_q\\), while the \\(\\gamma_q + 1\\)-th token is drawn from \\(p_{i+\\gamma_q+1}(x)\\). At this stage, the \\(\\gamma_q + 1\\)-th token, referred to as bonus token, is treated as the initial token in Mp's proposal. Subsequently, Mp will generate an additional \\(\\gamma_p - 1\\) tokens to complete its proposal.\nIf any proposed tokens are rejected and no bonus token is generated, the default proposal model will take over as the proposal model. This default model is predefined and fixed. As outlined above, the two models alternate as proposers during the decoding process, which is why this approach is called the alternate proposal framework.\nAnalysis of speed improvement. We now analyze the speed improvement achieved by the alternative proposal framework. For the sake of clarity, we focus on a single cycle, which encompasses one proposal and one verification. In this cycle, both the proposer and verifier are fixed. Given that the decoding process is composed of multiple such cycles, the overall decoding performance can be inferred from the behavior of a single cycle.\nFirst, similar to Theorem 3.1, we provide the expected speed"}, {"title": "3.4. Generalize to More Models", "content": "In this subsection, we extend SE to the n-model ensemble scenario. The core principles remain similar to the two-model case, with acceleration driven by two key factors. First, each model can score the proposals of other models in parallel, where scoring refers to computing the probability distribution of a proposal from other models."}, {"title": "4. Experiments", "content": "4.1. Experimental Setups\nDatasets and evaluation. We test SE across multiple tasks including code generation, mathematical reasoning, multitask understanding, and text summarization on HumanEval , GSM8K, MMLU and CNNDM, respectively. We measure each method's speed by the average tokens generated per second and compute the speedup ratio relative to the standard ensemble.\nEnsemble functions and methods. We experiment with two ensemble functions: weighted ensemble (WE) at the distribution level (Equation (4)) and contrastive decoding (CD) at the logits level (Equation (5)). For WE, in the two-model case, we set \\(\\lambda\\) = 0.5 and temperature T = 1; in the three-model case, each model's coefficient was set to 1/3. For CD, we set \\(\\mu\\) = 0.1, which is the most common setting, and set T to both 0 and 1. WE with T = 0 is not tested due to its uncommon use, as it leads to a one-hot distribution, reducing information. Among two ensemble functions, three methods are compared: (1) the standard ensemble (WE, CD); (2) an accelerated version with speculative decoding (SD), using the smallest model as the proposal and the ensemble model as the target (WE-SD, CD-SD); and (3) Speculative Ensemble (WE-SE, CD-SE).\nModel pair configuration. We experiment on different types of LLMs, including Llama-2, Vicuna, Llama-3, Qwen-2.5, and OPT. Model pair configurations for each ensemble are in Table 1. We also test a three-model ensemble using Qwen2.5-1.5B-Instruct and its code and math versions in the WE setting.\nConfiguration of \\(\\gamma\\). The proposal length \\(\\gamma\\) is the only hyperparameter in SD, affecting the algorithm's acceleration. In the two-model SE setting, \\(\\gamma\\) corresponds to the proposal length of the smaller model, with the larger model fixed at 1. For simplicity, we refer to the smaller model with \\(\\gamma\\) > 1 as the proposal model of SE, since it typically serves this role. We tested \\(\\gamma\\) = 5 and \\(\\gamma\\) = 1 for SE and SD speeds, reporting the optimal results. \\(\\gamma\\) = 5 is the common setting,"}, {"title": "4.2. Main Results", "content": "Table 2 and Table 3 display the speedup ratios for each method relative to the standard ensemble in the WE and CD settings, respectively. From these two tables, we have the following findings. First, SE not only consistently achieves the highest speedup in all settings, it also gets speedup across all settings, which supports the findings in Corollary 3.7. In contrast, SD may reduce the ensemble speed in some cases. For example, when using the Llama-2 model pair with T = 1 on HumanEval in Table 3, applying SD reduces the speed to 0.94x of the standard ensemble. A similar speed reduction was also observed in the three-model scenario in Table 2. This is because vanilla SD does not inherently ensure acceleration. When the acceptance rate is low, SD may perform slower than standard decoding.\nSecond, compared to the CD scenario, the WE scenario ensures a higher minimum speedup for SE. In the two-model case, SE achieves a minimum speedup of 1.34x, while in the three-model case, it reaches at least 1.27x. In contrast, the CD scenario has a speedup as low as 1.11x. This difference arises because SE maintains a consistently high acceptance rate in the WE scenario, as outlined in Corollary 3.3.\nThird, the speedup varies across tasks and is influenced by the determinism of task outputs. For example, in the WE scenario, SE achieved the highest speedup on HumanEval, averaging 1.65x, as code generation demands strictly formatted outputs. Conversely, SE has a lower speedup of 1.36x on a text summarization task, where output flexibility is higher. This difference stems from the alignment between the proposal and target models: in highly deterministic tasks, their outputs exhibit greater similarity, leading to a higher acceptance rate and, consequently, stronger acceleration."}, {"title": "4.3. Analysis", "content": "Impact of proposal length \\(\\gamma\\). We evaluate the influence of various \\(\\gamma\\) values, ranging from 1 to 5, on speedup across both the WE and CD scenarios, utilizing the HumanEval and GSM8K datasets. The results in Figure 6 show that when the models are similar in size, the speedup ratio remains stable across \\(\\gamma\\) values, as seen in the WE scenario Figure 6 (a). This is because the high cost of invoking the proposal model offsets the speedup from increasing \\(\\gamma\\). However, when the models differ significantly in size, the speedup ratio varies considerably with \\(\\gamma\\), as shown in the CD scenario Figure 6 (b) and (c).\nAdditionally, we observe that speedup initially increases with \\(\\gamma\\) before decreasing. For example, in the experiment with the Llama-3 model pair on GSM8K (Figure 6 (b)), speedup improves as \\(\\gamma\\) rises from 1 to 5, peaks at \\(\\gamma\\) = 5, and then declines. This behavior is explained by two factors: increasing \\(\\gamma\\) boosts the expected number of accepted tokens, which improves acceleration; while later proposal tokens depend on earlier, unverified tokens, making them less accurate and more likely to be rejected, which wastes computation. Thus, the optimal speedup is achieved at a specific \\(\\gamma\\). In some cases, however, speedup either monotonically increases or decreases due to high or low acceptance rates. For instance, this is observed in the experiment experiments with the Llama-3 pair on HumanEval (Figure 6 (b)) and the Llama-2 pair on HumanEval (Figure 6 (c)).\nSpeedup ratio for different weight \\(\\lambda\\) in WE. We examine the speedup effect of SE when \\(\\lambda\\) takes values other than just 0.5. Specifically, we conduct experiments with \\(\\lambda\\) values ranging from 0.1 to 0.9, using the Llama-Vicuna and Qwen-3b model pairs on the HumanEval and GSM8K datasets. The results, presented in Figure 4, show that SE consistently achieves a high speedup of at least 1.5x across all tested \\(\\lambda\\) values. This consistent speedup occurs because, when the two models are of similar sizes, for any \\(\\lambda\\), an appropriate proposal model can be selected to maintain a high acceptance rate during SE process (as explained in Corollary 3.3), ensuring the observed speedup.\nSpeedup ratio for different weight values of \\(\\mu\\) in CD. Similarly, we examine the speedup effect of CD when \\(\\mu\\) takes other values. Specifically, we conduct experiments with \\(\\mu\\) ranging from 0.1 to 0.5, using the Llama-3 model pair on the HumanEval and GSM8K datasets. The results, presented in Figure 5, show that SE consistently accelerates the CD process across all tested \\(\\mu\\).\nFurthermore, we find that an increase in \\(\\mu\\) results in a reduced speedup. This occurs because CD computes the ensemble distribution by subtracting the proposal model's information from the target model's. As \\(\\mu\\) grows, the gap between these distributions widens, lowering the acceptance rate (Equation (7)). Despite this, the speedup ratio remains above 1.00x, confirming that CD-SE always accelerates, consistent with Corollary 3.7."}, {"title": "5. Conclusion", "content": "This paper introduces Speculative Ensemble (SE), an extension of speculative decoding that accelerates ensemble inference while maintaining output quality. SE refines the verification mechanism for direct ensemble sampling and introduces an alternate proposal framework to further boost efficiency. We demonstrate the effectiveness of SE through both theoretical analysis and empirical validation."}]}