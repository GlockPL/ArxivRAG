{"title": "FINALLY: fast and universal speech enhancement with studio-like quality", "authors": ["Nicholas Babaev", "Kirill Tamogashev", "Azat Saginbaev", "Ivan Shchekotov", "Hanbin Bae", "Hosang Sung", "WonJun Lee", "Hoon-Young Cho", "Pavel Andreev"], "abstract": "In this paper, we address the challenge of speech enhancement in real-world recordings, which often contain various forms of distortion, such as background noise, reverberation, and microphone artefacts. We revisit the use of Generative Adversarial Networks (GANs) for speech enhancement and theoretically show that GANs are naturally inclined to seek the point of maximum density within the conditional clean speech distribution, which, as we argue, is essential for the speech enhancement task. We study various feature extractors for perceptual loss to facilitate the stability of adversarial training, developing a methodology for probing the structure of the feature space. This leads us to integrate WavLM-based perceptual loss into MS-STFT adversarial training pipeline, creating an effective and stable training procedure for the speech enhancement model. The resulting speech enhancement model, which we refer to as FINALLY, builds upon the HiFi++ architecture, augmented with a WavLM encoder and a novel training pipeline. Empirical results on various datasets confirm our model's ability to produce clear, high-quality speech at 48 kHz, achieving state-of-the-art performance in the field of speech enhancement.", "sections": [{"title": "1 Introduction", "content": "Speech recordings are often contaminated with background noise, reverberation, reduced frequency bandwidth, and other distortions. Unlike classical speech enhancement (Ephraim & Malah, 1984; Pascual et al., 2017), which considers each task separately, universal speech enhancement (Serr\u00e0 et al., 2022; Su et al., 2021; Liu et al., 2022) aims to restore speech from all types of distortions simultaneously. Thus, universal speech enhancement seeks to generalize across a wide range of distortions, making it more suitable for real-world applications where multiple distortions may coexist.\nRecent studies have categorized the problem of speech enhancement as a task of learning the clean speech distribution conditioned on degraded signals (Lemercier et al., 2023; Serr\u00e0 et al., 2022; Richter et al., 2023). This problem is often addressed using diffusion models (Ho et al., 2020; Song et al., 2020), which are renowned for their exceptional ability to learn distributions. Diffusion models have recently achieved state-of-the-art results in universal speech enhancement (Serr\u00e0 et al., 2022)."}, {"title": "2 Mode Collapse and Speech Enhancement", "content": "The first question that we address is what is the practical purpose of a speech enhancement model. The practical goal of a speech enhancement model is to restore the audio signal containing the speech characteristics of the original recording, including the voice, linguistic content, and prosody. Thus, loosely speaking, the purpose of the speech enhancement task for many applications is not \"generative\" in its essence, in the sense that the speech enhancement model should not generate new speech content but rather \u201crefine\" existing speech as if it was recorded in ideal conditions (studio-like quality). From the mathematical point of view, this means that the speech enhancement model should retrieve the most probable reconstruction of the clean speech y given the corrupted version x, i.e.,\n$y = \\arg \\max_y P_{\\text{clean}}(y|x)$.\nThis formulation re-considers the probabilistic speech enhancement formulation, which is widely used in the literature. In such formulation, the speech enhancement model is aimed to capture the entire conditional distribution $P_{\\text{clean}}(y|x)$. This formulation might be especially appealing in situations with high generation ambiguity, e.g., a low SNR scenario where clean speech content"}, {"title": "3 Perceptual Loss for Speech Generation", "content": "Adversarial training is known for its instability issues (Brock et al., 2018). It often leads to suboptimal solutions, mode collapse, and gradient explosions. For paired tasks, including speech enhancement, adversarial losses are often accompanied by additional regressive losses to stabilize training and guide the generator towards useful solutions (Kong et al., 2020; Su et al., 2020). In the context of GAN mode-seeking behaviour discussed above, regressive losses could be seen as a merit to push the generator towards the \u201cright\u201d (most-probable) mode. Therefore, finding an appropriate regression loss to guide adversarial training is of significant importance.\nHistorically, initial attempts to apply deep learning methods to speech enhancement were based on treating this problem as a predictive task (Defossez et al., 2020; Hao et al., 2021; Chen et al., 2022a;"}, {"title": "4 FINALLY", "content": "Our method is based on HiFi++ (Andreev et al., 2022). The HiFi++ generator is a four-component neural network consisting of SpectralUNet, Upsampler, WaveUNet, and SpectralMaskNet modules. SpectralUNet is responsible for initial preprocessing of audio in the spectral domain using two-dimensional convolutions. The Upsampler is a HiFi-GAN generator-based module that increases the temporal resolution of the input tensor, mapping it to the waveform domain. WaveUNet performs"}, {"title": "4.1 Architecture", "content": "post-processing in the waveform domain and improves the output of the Upsampler by incorporating phase information gleaned directly from the raw input waveform. Finally, SpectralMaskNet is applied to perform spectrum-based post-processing and, thus, remove any possible artefacts that remained after WaveUNet. Thus, the model alternates between time and frequency domains, allowing for effective audio restoration.\nWe introduce two modifications to the HiFi++ generator's architecture. First, we modify the generator by incorporating WavLM-large model output (last hidden state of the transformer) as an additional input to the Upsampler. Prior works (Hung et al., 2022; Byun et al., 2023) have demonstrated the usefulness of Self-Supervised Learning (SSL) features for speech enhancement tasks, and we validate this by observing significant performance gains from using SSL features. Second, we introduce the Upsample WaveUNet at the end of the generator. This module acts as a learnable upsampler of the signal sampling rate. For its architecture, we use the WaveUNet with an additional convolutional upsampling block in the decoder that upsamples the temporal resolution by 3 times. This allows the model to output a 48 kHz signal while taking a 16 kHz signal as input."}, {"title": "4.2 Data and training", "content": "We use LibriTTS-R (Koizumi et al., 2023b), DAPS-clean (Mysore, 2014) as the sources of clean speech data. LibriTTS-R is used at 16 kHz, while DAPS at 48 kHz. Noise samples were taken from the DNS dataset (Dubey et al., 2022). After mixing with noise, we apply several digital distortion effects (see Appendix C.2 for details).\nWe train the model in three stages. The first two stages concentrate on restoring the original speech content, and the final stage aims to enhance the aesthetic perception of the speech. The multi-stage approach is necessary due to the characteristics of the employed datasets: the LibriTTS-R dataset has a lot of samples but limited perceptual quality, whereas the DAPS dataset is high-quality but contains a smaller number of samples. Consequently, we utilize the LibriTTS-R dataset for learning speech content restoration and the DAPS dataset for aesthetic stylization.\nThe loss functions that we use can be written as follows:\n$L_{\\text{LMOS}} (\\theta) = E_{x,y\\sim p(x,y)} [100\\cdot ||\\phi(y) - \\phi(g_{\\theta}(x))||_2 + |||\\text{STFT}(y)| - |\\text{STFT}(g_{\\theta}(x))|||_1]$,\n$L_{\\text{gen}} (\\theta) = \\lambda_{\\text{LMOS}} L_{\\text{LMOS}} (\\theta) + \\lambda_{\\text{GAN}} \\cdot L_{\\text{GAN-gen}}(\\theta) + \\lambda_{\\text{FM}} \\cdot L_{\\text{FM}}(\\theta) + \\lambda_{\\text{HF}} \\cdot L_{\\text{HF}}(\\theta)$,\n$L_{\\text{disc}} (\\gamma_i) = L_{\\text{GAN-disc}} (\\gamma_i), i = 1, . . ., k$.\nHere, $\\phi$ denotes the WavLM-Conv feature mapping, $g_{\\theta}(x)$ denotes the generator neural network with parameters $\\theta$, $L_{\\text{GAN-gen}}(\\theta)$ denotes the LS-GAN generator loss (Mao et al., 2017), $L(\\theta)$ denotes the combined generator loss, $L_{\\text{GAN-disc}} (\\gamma_i)$ denotes the LS-GAN discriminator (Mao et al., 2017) loss for the i-th discriminator with parameters $\\gamma_i$, $L_{\\text{FM}}$ denotes the feature matching loss (Kumar et al., 2019; D\u00e9fossez et al., 2023), $L_{\\text{HF}}$ denotes the human feedback loss, and $\\lambda_*$ denotes the corresponding loss"}, {"title": "5 Related Work", "content": "Self-supervised features for speech enhancement Several works have employed self-supervised (SSL) features (Baevski et al., 2020; Chen et al., 2022b; Hsu et al., 2021) for training of speech enhancement models as intermediate representation (Wang et al., 2024; Koizumi et al., 2023c), auxiliary space for loss computation (Sato et al., 2023; Hsieh et al., 2020; Close et al., 2023a,b) or input features (Hung et al., 2022; Byun et al., 2023). Sato et al. (2023); Close et al. (2023a,b); Hsieh et al. (2020) proposed to use features of self-supervised models as an auxiliary space for regression loss computation. In our work, we similarly study the effect of SSL features on speech enhancement models; however, our study systematically compares different feature backbones and develops criteria for probing the structure of different feature spaces. In particular, we show that features of the WavLM's convolutional encoder are the most effective for the loss function, while recent work (Sato et al., 2023) has used the outputs of transformer layers. Close et al. (2023a,b) proposed a similar loss function for speech enhancement based on convolutional features of HuBERT (Hsu et al., 2021), our work can be considered as extension of this work as we show that additional spectrogram loss greatly benefits the quality and provide experimental evidence for the advantages of LMOS-loss usage for adversarial training.\nGAN-based approaches The HiFi GAN works (Su et al., 2020, 2021) consider a GAN-based approach to speech enhancement, which is similar to ours. Importantly, these works base their generator architectures on feed-forward WaveNet (Rethage et al., 2018), a fully-convolutional neural network that operates at full input resolution, leading to slow training and inference times. We show that our model is able to achieve superior quality compared to this model while being much more efficient.\nKoizumi et al. (2023c) proposes to use w2v-BERT features (Chung et al., 2021) as intermediate representations for speech restoration. The features extracted from the noisy waveform are processed by a feature cleanser which is conditioned on the text representation extracted from transcripts via PnG-BERT, and speaker embedding extracted from the waveform. The feature cleanser is trained"}, {"title": "6 Results", "content": "We evaluate our models using the following sources of data:\nVoxCeleb Data: 50 audio clips selected from VoxCeleb1 (Nagrani et al., 2017) to cover the Speech Transmission Index (STI) range of 0.75-0.99 uniformly and balanced across male and female speakers.\nUNIVERSE Data: 100 audio clips randomly generated by the UNIVERSE (Serr\u00e0 et al., 2022) authors from clean utterances sampled from VCTK and Harvard sentences, together with noises/backgrounds from DEMAND and FSDnoisy18k. The data contains various artificially simulated distortions including band limiting, reverberation, codec, and transmission artefacts. Please refer to (Serr\u00e0 et al., 2022) for further details.\nThe validation data of UNIVERSE was artificially simulated from clean speech recordings using the same pipeline that the authors utilized for training. Therefore, we must note that the comparison is conducted in a manner advantageous to UNIVERSE, since our data simulation pipeline is different.\nVCTK-DEMAND: we use validation samples from popular Valentini denoising benchmark (Valentini-Botinhao et al., 2017). This dataset is used for a broad comparison with a wide range of speech enhancement models. The test set (824 utterances) includes artificially simulated noisy samples from 2 speakers with 4 SNR (17.5, 12.5, 7.5, and 2.5 dB).\nWe utilize DNSMOS (Reddy et al., 2022), UTMOS (Saeki et al., 2022), and WV-MOS (Andreev et al., 2022) as non-intrusive metrics to objectively assess the samples generated by our speech enhancement model on the VoxCeleb dataset. The non-intrusive nature of these metrics is essential since the dataset comprises recordings from real-life scenarios, lacking ground-truth samples.\nIn addition, we compute the Phoneme Error Rate (PhER) and Word Error Rate (WER) by comparing ground truths with the generated samples (see Appendix D for details) for both the UNIVERSE and VCTK-DEMAND datasets. For subjective quality assessment, we conduct 5-point Mean Opinion Score (MOS) tests. All audio clips are normalized to ensure volume differences do not influence the raters' evaluations. The raters are required to be English speakers using appropriate listening equipment (more details in Appendix E).\nThe Real-Time Factor (RTF) is determined by measuring the processing time (in seconds) for a 10-second audio segment on a V100 GPU and then dividing this time by 10. All confidence intervals are calculated using bootstrapping method.\nFurther, we evaluate our model on the VCTK-DEMAND dataset (Valentini-Botinhao et al., 2017) with additional metrics such as PESQ (Rix et al., 2001a), STOI (Taal et al., 2011), and SI-SDR (Roux et al., 2018). These metrics are included to ensure consistent comparison with previous works."}, {"title": "6.1 Evaluation", "content": "We evaluate our models using the following sources of data:"}, {"title": "6.2 Comparison with existing approaches", "content": "We consider BBED (Lay et al., 2023), STORM (Lemercier et al., 2023), and UNIVERSE (Serr\u00e0 et al., 2022) diffusion models, along with Voicefixer and DEMUCS regression models, as our baselines. In addition, we consider our closest competitor, HiFi-GAN-2, as a GAN-based baseline. The data for comparison with HiFi-GAN-2 and UNIVERSE were taken from their demo pages, since the authors did not release any code. We conduct comparisons with BBED, STORM, Voicefixer, DEMUCS, and HiFi-GAN-2 on real-world VoxCeleb1 samples, and the comparison with UNIVERSE on the simulated data provided by the authors of this work. The results are presented in Table 2. We also compare these models in VCTK-DEMAND dataset, provided in Table 3. We complement this table by two additional models: MetricGAN+ (Fu et al., 2021) and DB-AIAT (Yu et al., 2021).\nImportantly, our study confirms the observation from Serr\u00e0 et al. (2022) that speech enhancers based on generative models significantly outperform regression-based approaches. Our model performs comparably in terms of perceptual MOS quality to all the considered baselines, while being more than five times as efficient as the closest competitors, HiFi-GAN-2 and UNIVERSE. We have also found that our model is less prone to hallucinating linguistic content than UNIVERSE, delivering a lower Phoneme Error Rate (PhER) value.\nLastly, we would like to comment on the results in Table 3. Our model outperforms baselines in subjective evaluation and no-reference metrics, e.g. UTMOS, but significantly in terms of PESQ (Rix et al., 2001a), STOI (Taal et al., 2011) and SI-SDR (Roux et al., 2018). However, there have been numerous works consistently reporting low correlation of reference-based metrics with human perceptual judgment (Manocha et al., 2022; Manjunath, 2009; Andreev et al., 2022). In particular, the study (Manocha et al., 2022) reports that no-reference metrics (including DNSMOS, reported in our work) correlate significantly better with human perception and therefore have higher relevance for objective comparison between methods. Furthermore, in our study, we report the MOS score, which directly reflects human judgments of restoration quality."}, {"title": "6.3 Ablation study", "content": "To validate the improvements proposed in this work, we conduct an ablation study assessing the effectiveness of the design choices made.\nFirstly, we compare the LMOS loss against two other regression losses in the context of training a small speech enhancement model (10 times smaller than the final model; please see the Appendix D.2 for details). The first regression loss is the Mel-Spectrogram loss, which was proposed by Kong et al. (2020). As the second alternative, we employ the Reconstruction loss (RecLoss) proposed by D\u00e9fossez et al. (2023). It consists of a combination of L1 and L2 distances between mel-spectrograms computed with different resolutions. For these experiments, we conducted a grid search for the weights of each reconstruction loss (details are in Appendix D.2) and report results for the best option in Table 4.\nThe other proposed improvements bring incremental gains in perceptual MOS quality. Firstly, we add the features of the WavLM encoder as an additional inputs to HiFi++. Next, we scale the architecture by increasing the number of channels within the model. After that, we concatenate the Upsample WaveUNet with the FINALLY-16 architecture and implement the 3rd stage of training on the DAPS-clean dataset (48 kHz). Finally, we use additional human feedback losses (HF Loss) during the 3rd stage to further improve perceptual quality.\nSince Table 4 does not clearly demonstrate the advantages of using the WavLM (Chen et al., 2022b) encoder in terms of MOS, we have conducted additional ablation study on the more challenging UNIVERSE (Serr\u00e0 et al., 2022) validation dataset. The results of these additional experiments, which are presented in Table 5, clearly demontarte the importance of WavLM encoder."}, {"title": "7 Conclusion", "content": "In conclusion, we theoretically demonstrate that LS-GAN training encourages the selection of the point of maximum density within the conditional clean speech distribution, aligning naturally with the objectives of speech enhancement. Our empirical investigation identifies WavLM as an effective backbone for perceptual loss, supporting adversarial training. By integrating WavLM-based perceptual loss into the MS-STFT adversarial training pipeline and enhancing the HiFi++ architecture with a WavLM encoder, we develop a novel speech enhancement model, FINALLY, which achieves state-of-the-art performance, producing clear and high-quality speech at 48 kHz."}, {"title": "A Additional results for perceptual losses", "content": "From the probabilistic point of view, minimization of the point-wise distance leads to an averaging effect. For example, optimization of the mean squared error between waveforms delivers the expectation of the waveform over the conditional distribution of clean speech given its degraded version $g_{\\theta}(x) = E_{y\\sim P_{\\text{clean}}(y|x)} [y]$. The key thing is that the expectation over the distribution is not guaranteed to lie in the regions of high density of this distribution"}, {"title": "A.1 More on motivation", "content": "Mathematically speaking, let $\\phi(x)$ be the mapping to this space, and assume there exists a reverse mapping $\\phi^{-1}(z)$ such that $\\phi^{-1}(\\phi(x)) = x$. We would like to note that in practice, for regression purposes, there is no need to explicitly know the reverse mapping $\\phi^{-1}(z)$ as long as $\\phi(x)$ is differentiable. The regression in the space produced by the mapping leads to averaging in this space; thus, after minimizing the MSE loss\n$E_{x,y\\sim P_{\\text{clean}}(y)p(x|y)}||\\phi(y) - \\phi(g_{\\theta}(x))||^2$,\none would obtain the solution\n$g_{\\theta}(x) = \\phi^{-1} (E_{y\\sim P_{\\text{clean}}(y|x)} [\\phi(y)])$.\nTherefore, a desirable property for the $\\phi(x)$ mapping to be used as the regression space is that\n$\\phi^{-1} (E_{y\\sim P_{\\text{clean}}(y|x)} [\\phi(y)]) = \\arg \\max_y p_{\\text{clean}}(y|x)$,\ni.e., we would like averaging in $\\phi$-space to provide a representation corresponding to the most likely solution (the most probable reconstruction as discussed in the previous section). In practice, this property is difficult to verify. Moreover, finding such a mapping is likely to be a task which is not easier than the original problem. Therefore, based on this intuition, we propose some heuristic rules for assessing the structure of regression spaces produced by different mappings."}, {"title": "A.2 Influence of losses on final audio", "content": "To illustrate the influence of LMOS loss, as well as the importance of adversarial training, we examine the impact of different training setup using vocoding and discuss the results using spectrograms in Figure 5. We start with training the vocoding model using only convolutional features from WavLM. This training turns out to be suboptimal, as the model produces noticeable artefacts, visible in the top spectrogram in Figure 5. Next, we complement the WavLM feature loss with the STFT L1 loss, which is a L1 distance between spectrograms of reference and predicted audios. This method yields better quality, however some artefacts still remain. Notice, that such training particularly struggles to reconstruct high frequencies and capture the harmonic content. Finally, we train the"}, {"title": "C Implementation details", "content": "Our model is based on HiFi++ architecture (Andreev et al., 2022), for which code is available in https://github.com/SamsungLabs/hifi_plusplus. We used several open-source datasets for training. More specifically, we used DAPS, LibriTTS-R and Deep Noise Suppression Challenge (DNS) (only noises). We rely on official implementations of MS-STFT discriminators, which can be found in https://github.com/facebookresearch/encodec. For LMOS loss and HiFi++ conditioining, we used WavLM large model from Hugging Face, which can be found in https://huggingface.co/microsoft/wavlm-large."}, {"title": "C.1 Reproducibility statement", "content": "Before mixing with noise, we convolve the speech signal with a randomly chosen microphone impulse response from the Microphone Impulse Response project\u00b3 and apply other digital distortions. With a probability of 0.8, we also convolve the signal with a room impulse response randomly chosen from those provided in the DNS dataset. We additionally apply audio effects from the torchaudio library (Hwang et al., 2023), trying to simulate digital distortions. Parameter values are chosen randomly; only one codec is applied."}, {"title": "C.2 Augmentations", "content": "In this section we detail parameters for our speech enhancement models. We use HiFi++ (Andreev et al., 2022) as backbone. This architecture consists of four parts: Spectral UNet, HiFi Upsampler, WaveUNet and SpectalMaskNet. In addition to that we also use WavLM-large feature encoder with transformer features, which takes waveform as an input and outputs last hidden state of the transformer, which is stacked with the SpectralUNet output and fed into the HiFi Upsampler. In the next paragraphs we thoroughly discuss the architecture of every part. Architecture summary is presented in Table 9\nResidual blocks To better understand Spectral UNet, WaveUNet and SpectralMaskNet we first discuss the key building block. Each residual block is composed of convolution, followed by LeakyReLU. We use weight norm (Salimans & Kingma, 2016) instead of BatchNorm, we found it to be more efficient in our preliminary experiments. We use kernel size 3 for 2D UNets and kernel size 5 for 1D UNet. Each residual block has additive skip connection. In our architectures we use a stacked composition of residual blocks. The number of residual blocks stacked together in one layer is called depth.\nSpectralUNet architecture SpectralUNet processes a mel-spectrogram input with dimensions [B, 80, T] and outputs a hidden state with dimensions [B, 512, T]. The input mel-spectrogram is combined with positional encoding. The architecture is based on a 2D UNet (Ronneberger et al., 2015) with five layers, each having respective channels of [16, 32, 64, 128, 256] and a depth of 4. Additional convolutions are applied after these layers to transform 2D data from shape [B, 16, W, T] to [B, 1, W, T], and then to [B, 512, T]. This output is concatenated along the channel dimension with"}, {"title": "C.3 Model architecture", "content": "the final transformer hidden state of WavLM (Chen et al., 2022b), which has been interpolated using the nearest-neighbour method to match the output length of SpectralUNet in the time dimension. The concatenated result then passes through a Residual block with kernel size 3 and with a width of 1536 (1024 from the final transformer hidden state and 512 from SpectralUNet output), than passes through 1d Convolution with kernel size 1 and LeakyReLU to obtain size 512 in the channel dimension, and is subsequently fed into the HiFi Upsampler.\nHiFi Upsampler architecture For HiFi Upsampler we use HiFi generator architecture from (Kong et al., 2020). In our implementation we use 4 layer model with upsample rates [8, 8, 2, 2], kernel size [16, 16, 4, 4], hidden sizes [512, 256, 128, 64]. For each layer we use residual blocks with kernels [3, 7, 11] and dilations [(1, 3, 5), (1, 3, 5), (1, 3, 5)].\nWaveUNet architecture We implement WaveUNet following (Stoller et al., 2018). We use 4-layer WaveUNet with channels [128, 128, 256, 512], each layer has depth 4. Our WaveUNet takes the concatenation of input waveform and HiFi Upsampler as input. In this way we provide a residual connection for Spectral UNet and HiFi Upsampler.\nSpectralMaskNet architecture The final Stage of our pipeline is SpectralMaskNet. It applies channel-wise Short Time Fourier Transform (STFT) to the input, decomposes it into phase and amplitude, and then processes amplitude with 2D UNet. Finally, it multiplies processed amplitude and phase and applies inverse STFT to obtain the final audio. For processing amplitude 2D UNet with channels [64, 128, 256, 512] as a backbone, each layer has depth 1. The output of SpectralMaskNet is the final output of our 16kHz model.\nWaveUNet Upsampler In order to enable the model to output samples in 48kHz we also add WaveUNet Upsampler, that upsamples the final audio. For that task we use WaveUNet with 5 layers and channels [128, 128, 128, 128, 256], each layer has depth 3. The outputs of each layer are downsampled with the scale 4. After the output of the final upsample layer, we have an additional head that has 512 features and is used for directly upscaling the WaveUNet output into 48kHz."}, {"title": "C.4 Limitations", "content": "The primary area for further developing in the proposed model lies in improving perceptual quality, particularly in instances when the speech is severely degraded. Although our model is capable of capturing the content of such signals, even in situations that may be impossible for humans, some artifacts remain. These artifacts could possibly be attributed to the high uncertainty of the initial signal, such as the speaker's voice, for instance.\nAnother crucial aspect to consider is the streaming mode. While the proposed model is fast, it doesn't suit low latency scenarios, which are widely applicable in telecommunications. The big obstacle to such improvement could be the WavLM (Chen et al., 2022b) model that utilizes context for identifying the content of speech. This model has the boundaries of the required context and, consequently, the minimum latency achievable."}, {"title": "D Evaluation details", "content": "To compare models in their ability to correctly reconstruct the speech we use Phoneme Error Rate (PhER) and Word Error Rate (WER). Given the reference audio and audio, produced by the speech enhancement model, metrics are computed in the following way. First, Automatic Speech Recognition (ASR) model is applied to both audios, yielding phoneme representations. Then, the editing distance is computed over predicted phonemes (or words for WER) for the reference and the predicted audio. The distance is normalized to the length of the reference phoneme representation (or the length of the sentence for WER) and averaged over the validation set."}, {"title": "D.1 Metric computation", "content": "For the ablation study, we utilized a smaller version of our original model to expedite evaluations, with the reduced model containing approximately 22 million parameters. The dataset used for this ablation was provided by (Nagrani et al., 2017). The ablation process was carried out in several stages. Initially, we compared the performance of our LMOS loss with L1Spec loss (Kong et al., 2020), MS-STFT loss (Defossez et al., 2020), and RecLoss (D\u00e9fossez et al., 2023) during the pretraining phase. Each loss function was used independently to train the model in a regressive manner for 500,000 steps with a batch size of 12, utilizing two Nvidia P40 GPUs. Our findings indicated that LMOS consistently outperformed the other losses. Detailed results are presented in Appendix D.2. It's important to note that MOS evaluations were not conducted during the first stage, as the enhanced audio typically displayed artifacts following this stage of training."}, {"title": "D.2 Ablation details", "content": "Next, we ablated the use of mentioned losses in purely adversarial setup without pretraining. The same small model was trained from scratch using one of the discussed reconstruction losses with adversarial LS-GAN loss (Mao et al., 2017) and feature matching loss (Kong et al., 2020). We used MS-STFT discriminators (D\u00e9fossez et al., 2023) for adversarial training. We fixed coefficients for the feature matching loss at 5 and for the LS-GAN loss at 15, and then grid searched the coefficient for each reconstruction loss. We experimented with the possible coefficient to be 1, 3, 10, 20, 50 and found that is all cases the best coefficient is 3. We found that LMOS loss outperforms other reconstruction losses in this setup as well.\nIn the first stage we experimented with the addition of SSL features to boost model quality. For these experiments we took our best model trained with LMOS loss. As our ablation indicates, transformer features of WavLM significantly increase the model quality.\nFinally, we train our big model, using the ablated architecture. We report the results in 'Scaling' row in Table 4. We also show how the introduction of the WaveUNet Upsampler and Human Feedback (HF) losses influence the final quality of our model.\nIn conclusion, we found that the best model is the one, trained with WavLM transformer features and LMOS reconstruction loss. Moreover, we found, that using HF losses also significantly increases the final quality.\nComparison with HiFi++. Since our model is based on HiFi++ architecture (Andreev et al., 2022), use explicitly show how the improvements we introduced increase the quality of enhanced audio compared to HiFi++ baseline using VoxCelex dataset (Nagrani et al., 2017). The results are presented in Table 12"}, {"title": "E Subjective evaluation", "content": "We measure mean opinion score (MOS) of the model using a crowdsourcing adaptation of the standard absolute category rating procedure. Our MOS computing procedure is as follows."}]}