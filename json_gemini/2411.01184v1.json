{"title": "Guiding Multi-agent Multi-task Reinforcement Learning by a Hierarchical Framework with Logical Reward Shaping", "authors": ["Chanjuan Liu", "Jinmiao Cong", "Bingcai Chen", "Yaochu Jin", "Enqiang Zhu"], "abstract": "Multi-agent hierarchical reinforcement learning (MAHRL) has been studied as an effective means to solve intelligent decision problems in complex and large-scale environments. However, most current MAHRL algorithms follow the traditional way of using reward functions in reinforcement learning, which limits their use to a single task. This study aims to design a multi-agent cooperative algorithm with logic reward shaping (LRS), which uses a more flexible way of setting the rewards, allowing for the effective completion of multi-tasks. LRS uses Linear Temporal Logic (LTL) to express the internal logic relation of subtasks within a complex task. Then, it evaluates whether the subformulae of the LTL expressions are satisfied based on a designed reward structure. This helps agents to learn to effectively complete tasks by adhering to the LTL expressions, thus enhancing the interpretability and credibility of their decisions. To enhance coordination and cooperation among multiple agents, a value iteration technique is designed to evaluate the actions taken by each agent. Based on this evaluation, a reward function is shaped for coordination, which enables each agent to evaluate its status and complete the remaining subtasks through experiential learning. Experiments have been conducted on various types of tasks in the Minecraft-like environment. The results demonstrate that the proposed algorithm can improve the performance of multi-agents when learning to complete multi-tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep reinforcement learning (DRL) has shown remarkable success in solving decision-making problems that surpass human-level performance, such as the Atari game [16], chess confrontation [30], [23], and real-time strategy game (RTS) [14]. However, as the environments become increasingly com- plex, some limitations (such as low learning efficiency and quality) may appear in single-agent DRL systems. To address this, there is an urgent need for multi-agent DRL [9], where multiple agents can solve complex tasks through collaboration [8]. However, multi-agent learning [26] for complex tasks suffers from an exponential growth of the action and state spaces, which is known as the curse of dimensionality [7]. To overcome this, hierarchical reinforcement learning (HRL) [40] has been introduced into multi-agent DRL, giving rise to multi-agent hierarchical reinforcement learning (MAHRL) [44], [11]."}, {"title": "A. The Challenges", "content": "Most existing MAHRL algorithms follow the traditional way of setting the reward functions, which is not appropriate when multiple tasks need to be completed in complex environ- ments. For instance, in the Minecraft environment, in order to complete the task of making bows and arrows, agents have to find wood to make the body of bows and arrows, spider silk to make bowstrings, as well as feathers to make arrow fletchings. To learn the strategies for completing the task, an appropriate reward is needed for the agent. However, designing a reward function for one task is challenging and difficult to generalize for other tasks [15]. Moreover, in the task of making bows and arrows, if an agent only finds some of the required materials, it can not get a reward; thus, it is challenging for agents to learn how to complete the remaining tasks. In MAHRL, the decision of each agent is typically treated as a black box, making it difficult to understand the logic behind this decision, leading to the untrustworthiness of the system. Hence, it is essential to develop a general and effective way of shaping rewards with a description of the internal logic of the tasks, which helps the agents easily understand the progress of the task and make reasonable decisions."}, {"title": "B. Our Contributions", "content": "This work explores a flexible approach to setting rewards, called logic reward shaping (LRS), for multi-task learning. LRS uses the Linear Temporal Logic (LTL) [24], [3], [5], [42] to represent environmental tasks, making use of its precise semantics and compact syntax to clearly show the internal logical construction of the tasks and provide guidance for the agents. A reward structure is appropriately defined to give rewards, based on whether LTL expressions are satisfied or not. To promote strategy learning, a technique of value iteration is used to evaluate the actions taken by each agent; after that, a reward shaping mechanism is utilized to shape a reward"}, {"title": "D. Organization of the Paper", "content": "The rest of this article is organized as follows. Section II introduces the preliminaries of reinforcement learning and LTL. Section III describes the algorithm model. Section IV presents the experimental design and results. Finally, the last section summarizes this work with future research directions."}, {"title": "II. PRELIMINARIES", "content": ""}, {"title": "A. Single Agent Reinforcement Learning", "content": "The single agent reinforcement learning (SARL) [25] is based on the idea that the agent can learn to select appro- priate actions by interacting with a dynamic environment and maximizing its cumulative return. This process is similar to how humans acquire knowledge and make decisions. Deep reinforcement learning has made significant progress in AI research, as demonstrated by the successes of AlphaGo [29], AlphaGo Zero [31], and AlphaStar [37], which have shown the ability of reinforcement learning to solve complex decision- making problems.\n\nThe interaction between the agent and the environment in SARL follows the Markov Decision Process (MDP). MDP is generally represented by a tuple of (S, A, R, T, \u03b3). In this tuple, S represents the state space of the environment. At the beginning of an episode, the environment is set to an initial state $s_0$. At timestep t, the agent observes the current state $s_t$. The action space is represented by A, and $a_t \\in A$ represents the action the agent performs at timestep t. The function R: S\u00d7A\u00d7S\u2192 R defines the instantaneous return of an agent from state $s_t$ to state $s_{t+1}$ through action $a_t$. The total return from the beginning time t to the end of the interaction at time K can be expressed as $R_t = \\sum_{t'=t}^{K}\\gamma^{t'-t}r_{t'}$. Here, \u03b3 \u0395 [0,1] is the discount coefficient, which is used to ensure that the later return has a more negligible impact on the reward function. It depicts the uncertainty of future returns and also limits the return function. The function T : S\u00d7A\u00d7S \u2192 [0, 1], defines the probability distribution of the transition from $s_t$ to $s_{t+1}$ for a given action $a_t \\in A$. According to the feedback reward of the environment, the agent uses positive and negative feedback to indicate whether the action is beneficial to the learning goal. The agent constantly optimizes action selection strategies through trial and error and feedback. Eventually, the agent learns a goal-oriented strategy."}, {"title": "B. Multi-Agent Reinforcement Learning", "content": "When faced with large-scale and complex decision-making problems, a single-agent system cannot comprehend the re- lationship of cooperation or competition among multiple decision-makers. Therefore, the DRL model is extended to a multi-agent system with cooperation, communication, and competition among multiple agents. This is called multi-agent reinforcement learning (MARL) [2], [21]. The MARL frame- work is modeled as a Markov Game (MG): (N, S, A, R, T, Y). N stands for the number of agents. A = $a_1\u00d7, ..., \u00d7a_n$ is the joint action space for all agents. For i \u2208 [1, . . ., N], $R_i : S \u00d7 A \u00d7S \u2192 R$ is the reward function for each agent. The reward function is assumed to be bounded. $T : S \u00d7 A \u00d7 S \u2192 [0, 1]$ is the state transition function. y is the discount coefficient. The multi-agent reinforcement learning scheme is shown in Figure 1."}, {"title": "C. Linear Temporal Logic", "content": "Linear Temporal Logic (LTL) is a type of propositional logic with temporal modalities. Let AP be a set of propositions. An LTL formula consists of finite propositions p\u2208 AP, logical connectives V (disjunctive), \u2227 (conjunctive), \u2192 (implication), \u2192 (negation), unary sequence operators \u2610 (next), (always), \u25c7 (eventually), and binary operators U (until), R (release). The following formula gives the syntax of LTL formula 4 on proposition set AP, where p\u2208 AP:\n\n$\n\\varphi ::= p | \\neg\\varphi | \\varphi \\wedge \\varphi | \\varphi \\vee \\varphi | \\bigcirc\\varphi | \\Box\\varphi | \\Diamond\\varphi | \\varphi U \\varphi ' | \\varphi R\\varphi '.\n$   (1)\n\nThese temporal operators have the following meanings: $\\bigcirc$ indicates that 4 is true in the next time step; \u2610 indicates that 4 is always true; \u25c7 indicates that 6 will eventually be true; $\\varphi U \\varphi '$ means that $ must remain true until & becomes true;\ntherefore, $\u25c7\\varphi =$ trueU\u03c6. $\\varphi R\\varphi '$ means that $ is always true or & remains true until both and are true.\n\nIn general, LTL describes propositions with infinite length,\nbut this work focuses on the tasks that are completed in a finite time. Therefore, we use the co-safe Linear Temporal Logic (Co-safe LTL) [38], which extends LTL to specify and characterize system behaviors. Co-safe LTL specifications are typically of finite lengths, so the truth value for a given specification can be determined within a finite number of steps. For instance, the formula 1 meaning \u201cEventually 41 is true\u201d is co-safe because once 41 is true, what happens afterward is irrelevant. Note that \u00ac\u25c72 meaning \u201c42 must always be false\u201d is not co-safe because it can only be satisfied if 42 is never true in an infinite number of steps. Nonetheless, we can define a co-safe task \u00ac\u25c72U41, which means \"41 must always be false before 42 is true\". Thus, co-safe LTL formulae can still define some subtasks to be completed while ensuring finite attributes. In the following, unless specified, LTL formulae generally refer to co-safe LTL formulae.\n\nDeterministic Finite Automaton (DFA) is a finite state machine that accepts or rejects a finite number of symbol strings [27]. According to Laccrda's work [19], any co-safe formula can be transformed into a corresponding DFA Dy = <Q,$q_0$,QF, 2AP, $\u03b4_D$\u201e). Here, Q represents a finite set of states, $q_0$\u2208Q is the initial state, QFQ is the set of final ac- ceptance states, 2AP is the alphabet, and $\u03b4_D$\u201e : Q \u00d7 2AP \u2192 Q is a transition function. The DFA D accepts only the finite traces that satisfy 6. The transformation process demonstrates double-exponential complexity, but it significantly improves the feasibility and automation of LTL formulae verification. Practical tools for such transformations have been developed and demonstrated to be effective in practice."}, {"title": "III. ALGORITHM MODEL", "content": "The algorithm MHLRS comprises of three modules, namely, the environment module, agents module, and logic reward shaping (LRS) module. The architecture is shown in Figure 2. The environment consists of various components that are combined using logical and temporal connectives to create LTL formulae. These formulae act as tasks for the agents and are stored in a set called \u03a6. Each task is represented by an LTL formula that needs to be completed by the agents. Each agent in the environment adopts a two-layer hierarchical structure. The high-level meta-controller proposes strategies, which guide the agent to perform actions, while the low-level controller is responsible for executing actions to complete the target tasks. The reward shaping component assigns appropriate rewards to agents as they complete LTL tasks, after LTL progression and value iteration. To efficiently collaborate among agents, each agent can share LTL task information for the strategy learning of common tasks. It is important to note that this structure is scalable and flexible in terms of the number of agents.\n\nThe following is a detailed description of the main modules of the algorithm and the training method of MHLRS."}, {"title": "A. Logic Reward Shaping Module", "content": "Each task can be formally specified in LTL syntax. For example, Equation (2) represents the task of making an axe, which is composed of four propositions and two conjunctions.\n\n$\n\\P_{axe} \\vDash \\Diamond(got\\_wood \\wedge \\Diamond used\\_workbench)\n                 (got\\_iron \\wedge \\Diamond used\\_factory).\n$   (2)\n\nIn general, LTL describes propositions with infinite length, but this work focuses on the tasks that are completed in a\nfinitetime.Therefore,weusetheco-safeLinearTemporalLogic (Co-safe LTL) [38], which extends LTL to specify and characterize system behaviors. Co-safe LTL specifications are typically of finite lengths, so the truth value for a given\nTo determine the truth of the LTL formulae, a label function\n$L : S \u2192 2^{AP}$ is introduced, where S is the state set,\nand AP is the atomic proposition set. The label function L"}, {"title": "B. Multi-Agent Reinforcement Learning", "content": "When faced with large-scale and complex decision-making problems, a single-agent system cannot comprehend the re- lationship of cooperation or competition among multiple decision-makers. Therefore, the DRL model is extended to a multi-agent system with cooperation, communication, and competition among multiple agents. This is called multi-agent reinforcement learning (MARL) [2], [21]. The MARL frame- work is modeled as a Markov Game (MG): (N, S, A, R, T, Y). N stands for the number of agents. A = $a_1\u00d7, ..., \u00d7a_n$ is the joint action space for all agents. For i \u2208 [1, . . ., N], $R_i : S \u00d7 A \u00d7S \u2192 R$ is the reward function for each agent. The reward function is assumed to be bounded. $T : S \u00d7 A \u00d7 S \u2192 [0, 1]$ is the state transition function. y is the discount coefficient. The multi-agent reinforcement learning scheme is shown in Figure 1."}, {"title": "C. Linear Temporal Logic", "content": "Linear Temporal Logic (LTL) is a type of propositional logic with temporal modalities. Let AP be a set of propositions. An LTL formula consists of finite propositions p\u2208 AP, logical connectives V (disjunctive), \u2227 (conjunctive), \u2192 (implication), \u2192 (negation), unary sequence operators \u2610 (next), (always), \u25c7 (eventually), and binary operators U (until), R (release). The following formula gives the syntax of LTL formula 4 on proposition set AP, where p\u2208 AP:\n\n$\n\\varphi ::= p | \\neg\\varphi | \\varphi \\wedge \\varphi | \\varphi \\vee \\varphi | \\bigcirc\\varphi | \\Box\\varphi | \\Diamond\\varphi | \\varphi U \\varphi ' | \\varphi R\\varphi '.\n$   (1)\n\nThese temporal operators have the following meanings: $\\bigcirc$ indicates that 4 is true in the next time step; \u2610 indicates that 4 is always true; \u25c7 indicates that 6 will eventually be true; $\\varphi U \\varphi '$ means that $ must remain true until & becomes true;\ntherefore, $\u25c7\\varphi =$ trueU\u03c6. $\\varphi R\\varphi '$ means that $ is always true or & remains true until both and are true.\n\nIn general, LTL describes propositions with infinite length,\nbut this work focuses on the tasks that are completed in a finite time. Therefore, we use the co-safe Linear Temporal Logic (Co-safe LTL) [38], which extends LTL to specify and characterize system behaviors. Co-safe LTL specifications are typically of finite lengths, so the truth value for a given specification can be determined within a finite number of steps. For instance, the formula 1 meaning \u201cEventually 41 is true\u201d is co-safe because once 41 is true, what happens afterward is irrelevant. Note that \u00ac\u25c72 meaning \u201c42 must always be false\u201d is not co-safe because it can only be satisfied if 42 is never true in an infinite number of steps. Nonetheless, we can define a co-safe task \u00ac\u25c72U41, which means \"41 must always be false before 42 is true\". Thus, co-safe LTL formulae can still define some subtasks to be completed while ensuring finite attributes. In the following, unless specified, LTL formulae generally refer to co-safe LTL formulae.\n\nDeterministic Finite Automaton (DFA) is a finite state machine that accepts or rejects a finite number of symbol strings [27]. According to Laccrda's work [19], any co-safe formula can be transformed into a corresponding DFA Dy = <Q,$q_0$,QF, 2AP, $\u03b4_D$\u201e). Here, Q represents a finite set of states, $q_0$\u2208Q is the initial state, QFQ is the set of final ac- ceptance states, 2AP is the alphabet, and $\u03b4_D$\u201e : Q \u00d7 2AP \u2192 Q is a transition function. The DFA D accepts only the finite traces that satisfy 6. The transformation process demonstrates double-exponential complexity, but it significantly improves the feasibility and automation of LTL formulae verification. Practical tools for such transformations have been developed and demonstrated to be effective in practice."}, {"title": "III. ALGORITHM MODEL", "content": "The algorithm MHLRS comprises of three modules, namely, the environment module, agents module, and logic reward shaping (LRS) module. The architecture is shown in Figure 2. The environment consists of various components that are combined using logical and temporal connectives to create LTL formulae. These formulae act as tasks for the agents and are stored in a set called \u03a6. Each task is represented by an LTL formula that needs to be completed by the agents. Each agent in the environment adopts a two-layer hierarchical structure. The high-level meta-controller proposes strategies, which guide the agent to perform actions, while the low-level controller is responsible for executing actions to complete the target tasks. The reward shaping component assigns appropriate rewards to agents as they complete LTL tasks, after LTL progression and value iteration. To efficiently collaborate among agents, each agent can share LTL task information for the strategy learning of common tasks. It is important to note that this structure is scalable and flexible in terms of the number of agents.\n\nThe following is a detailed description of the main modules of the algorithm and the training method of MHLRS."}, {"title": "A. Logic Reward Shaping Module", "content": "Each task can be formally specified in LTL syntax. For example, Equation (2) represents the task of making an axe, which is composed of four propositions and two conjunctions.\n\n$\n\\P_{axe} \\vDash \\Diamond(got\\_wood \\wedge \\Diamond used\\_workbench)\n                 (got\\_iron \\wedge \\Diamond used\\_factory).\n$   (2)\n\nIn general, LTL describes propositions with infinite length, but this work focuses on the tasks that are completed in a\nfinitetime.Therefore,weusetheco-safeLinearTemporalLogic (Co-safe LTL) [38], which extends LTL to specify and characterize system behaviors. Co-safe LTL specifications are typically of finite lengths, so the truth value for a given\nTo determine the truth of the LTL formulae, a label function\n$L : S \u2192 2^{AP}$ is introduced, where S is the state set,\nand AP is the atomic proposition set. The label function L"}, {"title": "Agents Module", "content": "Agent 1\nEnvironment Module\nmeta-\ncontroller\noption\ncontroller\nmeta-\ncontroller\noption\ncontroller\nAgentn\nEnvironment LTL Formula Tasks Set\nActions\nEnvironment\n\u06f7,\u06f8, \u039f\n\u03a6\nEnv state LTL task\nmeta-\ncontroller\noption\ncontroller\nagent 2"}, {"title": "LTL task. The Q-value function of policy II can be defined over the state sequences, as shown in Equation (5):", "content": "$\nQ^{\\Pi}((s_{0:t}), A) = E_{\\pi}\\left[\\sum_{t'=t}^{K}\\gamma^{t'-t}R_{\\varphi}((s_{0:t})) | A_t = A \\right]$\n(5)\n\nwhere $(s_{0:t})$ is the abbreviation of $(s_0,..., s_t)$. $A_t = A$ is the joint actions taken by the agents in state $s_t$.\n\nThe goal of the agents is to learn an optimal policy\n$\\Pi^*(a_t|s_t, \\varphi)$ to maximize the Q value. However, one challenge\nin the learning process is that the reward function depends on\nthe sequence of states and is non-Markov. In order to formally\ndefine this problem, we use the concept of the Non-Markovian\nReward Game (NMRG).\n\nDefinition 1. (NMRG). A Non-Markovian Reward Game\n(NMRG) is modeled as a tuple M = (N, S, A, R, T, \u03b3),\nwhere N, S, A, T, are defined as in the MG, but the R is\ndefined over state histories, $R_{\\varphi} : (L(s_1, a_1), ..., L(s_t, a_t)) \u2192\nR$.\n\nThe optimal policy $\u03a0^*$ must consider the state sequences\nof the whole training process, which brings about the issue\nof long-term dependencies. To deal with this issue, we use\nthe method of LTL progression, which works in the following\nway.\n\nLTL progression [13], [35] is a rewriting process that\nretains LTL's semantics. It receives an LTL formula and the\ncurrent state as input and returns a formula that needs to be\nfurther dealt with. The LTL formula is progressed according\nto the obtained truth assignment sequence {$\u03c3_0,..., \u03c3_t$}. In the training process, the formula is updated in each step to reflect the satisfied parts in the current state. The progressed formula will only include expressions on tasks that are uncompleted. For example, the formula ($\\varphi_1 \\wedge \\bigcirc \\Diamond \\varphi_2$) can be progressed to$\\bigcirc \\Diamond \\varphi_2$ when $\\varphi_1$ is true.\n\nGiven an LTL formula $\\varphi$ and a truth assignment $\u03c3_i$, the LTL\nprogression prog($\u03c3_i,\\varphi$) on the atomic proposition set AP is\nformally defined as follows:\n\n$\\bullet \\ prog(\u03c3_i,p) = true$ if $p \\in \u03c3_i$, where $p \\in AP$.\n\n$\\bullet \\ prog(\u03c3_i,p) = false$ if $p \\notin \u03c3_i$, where $p \\in AP$.\n\n$\\bullet \\ prog(\u03c3_i, \\neg\\varphi) = \\neg prog(\u03c3_i, \\varphi)$.\n\n$\\bullet \\ prog(\u03c3_i, \\varphi_1 \\wedge \\varphi_2) = prog(\u03c3_i,\\varphi_1) \\wedge prog(\u03c3_i, \\varphi_2)$.\n\n$\\bullet \\ prog(\u03c3_i, \\varphi_1 \\vee \\varphi_2) = prog(\u03c3_i,\\varphi_1) \\vee prog(\u03c3_i, \\varphi_2)$.\n\n$\\bullet \\ prog(\u03c3_i, \\bigcirc\\varphi) = \\varphi$.\n\n$\\bullet \\ prog(\u03c3_i, \\Box\\varphi) = true$.\n\n$\\bullet \\ prog(\u03c3_i, \\varphi_1U\\varphi_2) = prog(\u03c3_i, \\varphi_2) \\vee (prog(\u03c3_i, \\varphi_1) \\wedge\\varphi_1U\\varphi_2)$.\n\n$\\bullet \\ prog(\u03c3_i, \\varphi_1R\\varphi_2) = prog(\u03c3_i,\\varphi_1) \\wedge (prog(\u03c3_i, \\varphi_2) \\vee \\varphi_1R\\varphi_2)$.\n\nLTL progression can endow the reward function $R_\u03c6$ with\nMarkov property, shown as Equation (6), and this is achieved\nmainly through two aspects: (1) process the task formula $\\varphi$\nby progression after each action of the agents; (2) when$\\varphi$\nbecomes true by progression, the agent obtains the reward of\n1; -1 otherwise.\n\n$\nR'((s, \u03c6), a, (s', \u03c6')) = \n\\begin{cases}\n1 & \\text{if prog(L(s), \u03c6) = true,}\n\\\\\n-1 & \\text{otherwise.}\n\\end{cases}$\n\n(6)"}, {"title": "where s' is the transition state and \u03c6' = prog(L(s), \u03c6).", "content": "By applying the $R_\u03c6$ with Markov property to NMRG, the\nNMRG can be transformed into an MG problem that the multi-\nagent system can solve. For this, we apply LTL progression\nto the Markov Game (MG) to obtain an enhanced MG for\nlearning LTL tasks, which is named the Enhanced Markov\nGame with LTL Progression (EMGLP).\n\nDefinition 2. (EMGLP). An enhanced MG with LTL progres-\nsion (EMGLP) is modeled as a tuple G = (N, S, A, R'\u2030,T,\nAP, L, \u03a6, \u03b3), where N, S, A, T, and y are defined as in MG or\nNMRG, R', is the reward function to output suitable reward to\nthe agents after acting, shown as Equation (6), AP is the set\nof propositional symbols, L : S \u2192 $2^{AP}$ is the label function,\nand I = {1,2,..., m}, the set of tasks, is a finite non-\nempty set of LTL formulae over AP.\n\nEquation (6) is derived from Equation (4) by applying LTL\nprogression such that $R_{\\varphi}$ exhibits the Markov property. This\nallows for the learning of LTL formulae to be transformed into\na reinforcement learning (RL) problem that can be handled by\nagents. By using R with the Markov property, the NMRG\ncan be transformed into the EMGLP, which is a Markovian\nmodel.\n\nDefinition 3. (Transformation). A NMRG M = (N, S, A,\n$R_\u03c6$, \u03a4, \u03b3) can be transformed into an EMGLP G = (N, S, A,\nR',T,L, \u03a6, \u03b3), if apply LTL progression to Rp.\n\nIf an optimal joint strategy \u220f exists for M, then there\nshould also be a joint strategy II' for G that ensures the same\nreward. Thus, we can find optimal strategies for M by solving\nG instead."}, {"title": "B. Hierarchical Structure of Single Agent", "content": "This section explains the hierarchical reinforcement learning architecture of each agent. The algorithm is based on the framework of options. An option is a hierarchical reinforce- ment learning method proposed by Sutton [32]. An option provides a policy of subgoal, guiding an agent to achieve specific tasks or sub-objectives through a sequence of actions."}, {"title": "agents_module", "content": "Agent 1\nEnvironment Module\nmeta-\ncontroller\noption\ncontroller\nmeta-\ncontroller\noption\ncontroller\nAgentn\nEnvironment LTL Formula Tasks Set\nActions\nEnvironment\n\u03a6\nEnv state LTL task\nmeta-\ncontroller\noption\ncontroller\nagent 2\nReward\nLRS Module\nr' Reward Shaping(r)\nr = R' ((s, \u03c6'), \u03b1)\n\u2191\nLTL Progression (\u03c6)"}, {"title": "The training process of MHLRS is presented in Algorithm", "content": "The training process of MHLRS is presented in Algorithm (1).\n\n(1).\n\n$\n1 \\text{  :   Initialize replay buffers }\\{D_1, D_2\\} \\text{and parameters }\\{\\theta_1,\n\\theta_2\\} \\text{for each agent's controller and meta-controller}.\n$\n\n$\n2 \\text{  :   Initialize exploration probability }\\epsilon_{1,g} = 1 \\text{for each agent's}\n\\text{controller for all goals g and }\\epsilon_2 = 1 \\text{for each agent's meta-}\n\\text{controller}.\n$\n\n$\n3 \\text{  :   for }j = 1, num\\_episodes \\text{ do}\n$\n\n$\n4 \\text{  :      \u03a6 \u2190 CurriculumLearner(\u03a6)}\n$\n\n$\n5 \\text{  :      s\u2190 GetInitialState()}\n$\n\n$\n6 \\text{  :      if random() <  \u03f5 then}\n$\n\n$\n7 \\text{  :      g\u2190 random element from set G}\n$\n\n$\n8 \\text{  :      else}\n$\n\n$\n9 \\text{  :      g \u2190 argmax }g_{t\u2208G}Q(s, g_t)\n$\n\n$\n10 \\text{  :   end if}\n$\n\n$\n11 \\text{  :   while }\\varphi \\\\text{true, false}\\} \\text{and not EnvDeadEnd(s) do}\n$\n\n$\n12 \\text{  :       F\u2190 0}\n$\n\n$\n13 \\text{  :       so \u2190 s}\n$\n\n$\n14 \\text{  :       while not (EnvDeadEnd(s) or goal g reached) do}\n$\n\n$\n15 \\text{  :        while i < n do}\n$\n\n$\n16 \\text{  :          a\u2190 agent.GetActionEpsilonGreedy($Q_{\\varphi}$,s)}\n$\n\n$\n17 \\text{  :          Execute a and obtain next state s' and extrinsic}\n$\n\n$\n18 \\text{  :          reward R}(s, a, s')  \\text{ from environment}\n$\n\n$\n19 \\text{  :          Obtain intrinsic reward }r_t  \\text{ from internal critic}\n$\n\n$\n20 \\text{  :          Vi = \u00a7(Ri(s, a, s') + V(s|a))}\n$\n\n$\n21 \\text{  :          Store transition }\\{\\{s, g\\}, a, r_t, \\{s', g\\}\\} \\text{ in D\u2081}\n$\n\n$\n22 \\text{  :          agent.UPDATEPARAMS(L1($\u03b8_{1,j}$), D\u2081)}\n$\n\n$\n23 \\text{  :          agent.UPDATEPARAMS(L2($\u03b8_{2,j}$), D\u2082)}\n$\n\n$\n24 \\text{  :          s\u2190 s'}\n$\n\n$\n25 \\text{  :          i += 1}\n$\n\n$\n26 \\text{  :         end while}\n$\n\n$\n27 \\text{  :         Fi\u2190 Ri(s, a, s') + Vmin \u2013 YVmax}\n$\n\n$\n28 \\text{  :         Store transition (so, g, Fi, s' in D2)}\n$\n\n$\n29 \\text{  :       end while}\n$\n\n$\n30 \\text{  :       if s is not terminal then}\n$\n\n$\n31 \\text{  :        g \u2190 prog(L(s), 4)}\n$\n\n$\n32 \\text{  :        if random() <  \u03f5 then}\n$\n\n$\n33 \\text{  :        g \u2190 random element from set G}\n$\n\n$\n34 \\text{  :        else}\n$\n\n$\n35 \\text{  :        g \u2190 argmax }g_{t\u2208G}Q(s, g_t)\n$\n\n$\n36 \\text{  :        end if}\n$\n\n$\n37 \\text{  :       end if}\n$\n\n$\n38 \\text{  :       Anneal }\\epsilon_2 \\text{ and }\\epsilon_1\n$\n\n$\n39 \\text{  :   end while}\n$\n\n$\\\nThe environment is comprised of a set of tasks called \u03a6,\nwhich is made up of LTL formulae. Each agent initializes its\nmeta-controller and controller. Once initialization is complete,\na curriculum learner [20] selects tasks from the task set.\nSuppose the task set contains m tasks: $\u03c8_0,\u03c8_1,..., \u03c8_{m\u22121}$.\nThe curriculum learner selects tasks based on the given task\norder and tracks the success rate of each task in training:\n\n$P_{succ}(i) = \\frac{Num\\_Succ(\\varphi i)}{Num(episodes)}.\n$\n(12)"}, {"title": "IV. EXPERIMENTS", "content": "We conducted experiments on a grid map similar to\nMinecraft, which was suggested by [1]. This map is well-\nsuited for a formal language that represents environmental\ntasks and is widely used in the literature. In order to test the\neffectiveness of our proposed multi-agent learning method, we\nexpanded the environment to include multiple agents. These\nagents worked together to complete multiple tasks."}, {"title": "A. Baseline Algorithms", "content": "We use three state-of-the-art algorithms as the baselines.\nThe first baseline is the FALCON algorithm based on the\ncommander-unit structure proposed by [44]. This hierarchi-\ncal structure includes a high-level commander model and"}, {"title": "B. Experimental Setup", "content": "1) Maps: Environmental maps consist of two types: ran- dom maps and adversarial maps. Random maps have raw materials and agent locations generated randomly. Adversarial maps", "Features": "All of the tested algo- rithms consider the same features, actions, network architec- ture, and optimizer. The input features are stored in a vector that records the distance from each object to the agents. The implementation of DQN and DDQN is based on the OpenAI benchmark [12"}]}