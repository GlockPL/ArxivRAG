{"title": "Knowledge as a Breaking of Ergodicity", "authors": ["Yang He", "Vassiliy Lubchenko"], "abstract": "We construct a thermodynamic potential that can guide training of a generative model defined on a set of binary degrees of freedom. We argue that upon reduction in description, so as to make the generative model computationally-manageable, the potential develops multiple minima. This is mirrored by the emergence of multiple minima in the free energy proper of the generative model itself. The variety of training samples that employ N binary degrees of freedom is ordinarily much lower than the size 2N of the full phase space. The non-represented configurations, we argue, should be thought of as comprising a high-temperature phase separated by an extensive energy gap from the configurations composing the training set. Thus, training amounts to sampling a free energy surface in the form of a library of distinct bound states, each of which breaks ergodicity. The ergodicity breaking prevents escape into the near continuum of states comprising the high-temperature phase; thus it is necessary for proper functionality. It may however have the side effect of limiting access to patterns that were under-represented in the training set. At the same time, the ergodicity breaking within the library complicates both learning and retrieval. As a remedy, one may concurrently employ multiple generative models\u2014up to one model per free energy minimum.", "sections": [{"title": "Motivation", "content": "Training sets and empirical data alike are often processed using representations that do not have an obvious physical meaning or are not optimized for the specific application computation-wise. Of particular interest are binary representations of information as would be pertinent to digital computation. Not only do the values of the binary variables depend on the detailed digitization recipe, but the number of training samples will usually be vastly smaller than the size 2 of the full phase space available, in principle, to N binary variables. Given this, one is justified in asking whether a reduced description exists that uses a relatively small number of variables and parameters to efficiently document the empirically relevant configurations. At the same time, it is desirable for the reduced description to be robust with respect to the choice of a discretization procedure that is used to present the original dataset.\nThe problem of finding reduced descriptions is relevant for all fields of knowledge, of course, and is quite difficult in general. For example, the state of an equilibrated collection of particles is unambiguously specified by the expectation value of local density in a broad range of temperature and pressure (Evans, 1979). Hereby particles exchange places on times comparable to or shorter than typical vibrational times, implying it is unnecessary to keep track of the myriad coordinates of individual particles. The equilibrium density profile is a unique, slowly varying function of just three spatial coordinates. Yet under certain conditions the translational symmetry becomes broken: One may no longer speak of an equilibrium density profile that is unique or smooth. Instead, one must keep track of a large collection of distinct, rapidly-varying density profiles each of which corresponds to a metastable solid; these profiles can be regarded as equilibrated with respect to particles' vibrations but not translations. For instance in a glassy melt (Lubchenko, 2015; Lubchenko & Wolynes, 2007), the number of alternative, metastable structures scales exponentially with the system size while the free energy surface becomes a vastly degenerate landscape that breaks ergodicity.\nHere we address the problem of finding reduced descriptions in the context of machine learning. The complete description in the present setup is realized, by construction, through a generative model that is a universal approximator to an arbitrary digital dataset. That is, one can always choose such values for the model's parameters that the model will eventually have generated any given ensemble of 2N distinct binary sequences of length N. The generative model is in the form of an Ising spin-based energy function, each spin representing a binary number. Ising spin-based generative models have been employed for decades (Hopfield, 1982; Laydevant, Markovi\u0107, & Grollier, 2024; Mohseni, McMahon, & Byrnes, 2022), of course. The present energy function has the functional form of the higher-order Boltzmann machine (Sejnowski, 1986) and generally contains every possible"}, {"title": "Thermodynamics of Learning and Retrieval", "content": "combination of the spins. The learning rules are, however, different in that the coupling constants are deterministically expressed through the log-weights of individual sequences in the ensemble we want to reproduce. The retrieval is performed by Gibbs-sampling the Boltzmann distribution of the resulting energy function at a non-vanishing temperature.\nThe present study begins by constructing effective thermodynamic potentials whose arguments are the parameters of the complete generative model. Each of these potentials is uniquely minimized by the optimal values of the coupling constants in the complete description\u2014and can be generalized so as to reflect correlations among distinct sub-ensembles. We note that effective thermodynamic potentials for a variety of generative models have been considered in the past. These are exemplified by the Helmholtz machine (Dayan, Hinton, Neal, & Zemel, 1995) or the loss function for the restricted Boltzmann machine (Mont\u00fafar, 2018), among others.\nSpecific inquires during retrieval in the present generative model are made by imposing a constraint of user's choice. Of particular interest are constraints in the form of an additive contribution to the energy function that can stabilize a particular combination of the spins. This is analogous to how in a particle system, one can use an external\u2500or\u201csource\"\u2014field to stabilize a desired density profile (Lubchenko, 2015; Evans, 1979). If the system is ergodic, one may then use a Legendre transform to obtain a free energy as a function of the density profile. The latter procedure is a way to obtain a description in terms of variables of interest. At the same time, it represents a type of coarse-graining. Likewise, here we employ appropriate source-fields to produce a description in terms of variables of interest. The new degrees of freedom reflect weighted averages of the original spin degrees; thus their energetics are governed by a free energy. We show that when the source fields are turned off, this free energy becomes the aforementioned thermodynamic potential for the coupling constants. Thus learning and retrieval, respectively, can be thought of as minimizations on a conjoint free energy surface.\nThe total number of the coupling constants in the complete description, 2N, becomes impractically large already for trivial applications, which then prompts one to ask whether the description can be reduced in some controlled way. The most direct way to reduce the description is to simply omit some terms from the energy function; the number of such terms increases combinatorially with the order of the interaction. We show that following a reduction in description, however, the free energy will increase non-uniformly, over the phase space, so as to develop multiple minima of comparable depth. By co-opting known results from Statistical Mechanics, we argue that, depending on the application, the amount of minima could be so large as to scale exponentially with the number of variables. The multiplicity of minima makes the choice of an optimal description ambiguous. Conversely, a successful reduction in description (Merchan & Nemenman,\""}, {"title": "Setup of the generative model", "content": "By construction, we consider a machine that operates on N binary variables in the form of Ising spins \\( \\sigma_{\\alpha} = \\pm 1 \\), \\( \\alpha = 1,2,..., N \\). The \u201cpositive\u201d(\u201cnegative\") polarization state of each binary variable may be referred to as the \u201cup\u201d (\u201cdown\u201d) state. These names are meant to specify the state of a binary register or a Boolean variable, as in the table below:\n\\begin{equation}\n\\begin{array}{|c|c|c|c|c|}\n\\hline\n\\sigma & \\text { polarization } & \\text { spin-state } & \\text { arrow } & \\text { Boolean } \\\\\n\\hline\n+1 & \\text { positive } & \\text { up } & \\uparrow & 1 \\\\\n\\hline\n-1 & \\text { negative } & \\text { down } & \\downarrow & 0 \\\\\n\\hline\n\\end{array}\n\\end{equation}\nThe 2N possible configurations of N Ising spins \\( \\sigma_{\\alpha}, \\alpha = 1,...N \\) cover the corners of a hyper-cube in an N-dimensional Hamming space. A configuration i"}, {"title": null, "content": "will be denoted as \u1ee1i:\n\\begin{equation}\n(\\sigma_1^{(i)}, \\sigma_2^{(i)}, ..., \\sigma_N^{(i)}) = \\vec{\\sigma}_i\n\\end{equation}\nWe will use Greek indexes to distinguish the N directions in the Hamming space\u2014these directions pertain to the individual bits (spins) themselves and correspond to the subscripts on the l.h.s. of Eq. (2).\nWe define a dataset by assigning a number Zi to configuration i of the N binary variables. We further define the normalized weight of configuration i according to\n\\begin{equation}\nx_i = \\frac{Z_i}{Z}\n\\end{equation}\nwhere\n\\begin{equation}\nZ = \\sum_{i=1}^{2^N}Z_i\n\\end{equation}\nthe summation being over the 2 points comprising the Hamming space. We will consistently label the latter points in the Hamming space using Latin indices, as well as any other quantities assigned to those points.\nBy construction, the weight xi is intended to specify what a chemist would call the \"mole fraction\" of configuration i. The weight xi may or may not be associated with a probability, depending on the context; the weights are assigned using a user-defined convention as would be mole fractions in Chemistry, where one must explicitly specify what is meant by a \u201cspecies,\u201d \u201cparticle\u201d etc. We set aside until Section 4 the obvious issue that already for a very modestly-sized system, obtaining or storing 2N-worth of quantities Zi is impractical. For now, we simply assume that for those configurations not represented in the dataset, the respective weights are assigned some values of one's choice. In any event, the quantities Zi do not have to be integer. For instance, pretend we are teaching a machine, by example, the behavior of the inverter gate. Four distinct configurations are possible, in principle:(1)\u2191\u2193, (2)\u2193\u2191, (3) \u2191\u2191, and (4) \u2193\u2193, where one arrow stands for the input bit and the other arrow for the output bit. For concreteness, let us set Zi at the number of times configuration i was presented in the set. Suppose the training set is Z\u2081 = 32, Z2 = 37, Z3 = 2, Z4 = 0. It will be useful to regard Z4 as an adjustable parameter, even if we eventually adopt for it a fixed value that is very small relative to the rest of the Zi's.\nWe will sometimes refer to expressions of the type\n\\begin{equation}\n\\vec{f} = \\sum_{i=1}^{2^N} x_i f_i\n\\end{equation}\nas \"weighted sums,\u201d or \u201caverages,\u201d or \u201cexpectation values.\u201d\nWe define a generative model in the form of an energy-like function acting on N binary degrees of freedom. Because \u03c32n+1 = \u03c3 for integer n, the most general"}, {"title": null, "content": "function of the spin variables \u03c3\u03b1 can be written as a linear combination of all possible products of the variables, where in each product, a particular component \u03c3\u03b1 is present at most once. Thus one may define the following function\n\\begin{equation}\n\\begin{aligned}\nE(\\vec{\\sigma}) &= -J_0 - \\sum_{\\alpha_1}^N J_{\\alpha_1} \\sigma_{\\alpha_1} - \\sum_{\\alpha_1<\\alpha_2}^N J_{\\alpha_1 \\alpha_2} \\sigma_{\\alpha_1} \\sigma_{\\alpha_2} - \\sum_{\\alpha_1<\\alpha_2<\\alpha_3}^N J_{\\alpha_1 \\alpha_2 \\alpha_3} \\sigma_{\\alpha_1} \\sigma_{\\alpha_2} \\sigma_{\\alpha_3} ...\\\\\n&=-J_0-\\sum_{n=1}^N \\sum_{\\alpha_1<...<\\alpha_n} J_{\\alpha_1\\alpha_2...\\alpha_n} \\prod_{i=1}^n \\sigma_{\\alpha_i}\\\\\n&=-(\\vec{J} \\vec{\\sigma})\n\\end{aligned}\n\\end{equation}\nA parameter \\( J_{\\alpha_1\\alpha_2...\\alpha_n} \\) is the coupling constant for the interaction that couples the n spins \\( \\sigma_{\\alpha_1}, \\sigma_{\\alpha_2}, ..., \\sigma_{\\alpha_n} \\). The last line in Eq. (6) purveys a useful short-hand whereby we present the energy function as the inner product of two 2N-dimensional vectors J and \u03c3. By construction, vector \u03c3 is the Kronecker product of all pairs (1, \u03c3\u03b1):\n\\begin{equation}\n\\vec{\\sigma} = (1, \\sigma_N) \\otimes (1, \\sigma_{N-1}) \\otimes ... \\otimes (1, \\sigma_1).\n\\end{equation}\nWe will consistently order all Kronecker products so that the spin label a increases right to left. For instance, for three spins one has \\( \\vec{\\sigma} = (1, \\sigma_3) \\otimes (1, \\sigma_2) \\otimes (1, \\sigma_1) = (1, \\sigma_1, \\sigma_2, \\sigma_1 \\sigma_2, \\sigma_3, \\sigma_1 \\sigma_3, \\sigma_2 \\sigma_3, \\sigma_1 \\sigma_2 \\sigma_3) \\). The components of the 2-dimensional vector J are labeled so as to match the combination of spins the component in question multiplies, per Eq. (6). (Jo multiplies the 1.) And so for three spins one has J = (Jo, J1, J2, J12, J3, J13, J23, J123) etc.\nOwing to the mixed-product property of the Kronecker product, two vectors \\( \\vec{\\sigma}_i \\) and \\( \\vec{\\sigma}_j \\) are orthogonal, if \\( \\sigma_{\\alpha}^{(i)} \\ne \\sigma_{\\alpha}^{(j)} \\) at least for one \u03b1. Thus,\n\\begin{equation}\n(\\vec{\\sigma}_i \\vec{\\sigma}_j) = 2^N \\delta_{ij}.\n\\end{equation}\nThis orthogonality relation, in turn, implies the following completeness relation:\n\\begin{equation}\n\\sum_i (\\vec{\\sigma}_i) \\rangle \\langle (\\vec{\\sigma}_i) = 2^N \\mathbb{1},\n\\end{equation}\nwhere |a\\rangle \\langle b| denotes the outer product of vectors a and b and 1 is the unit matrix of size 2.\nBecause the total number \\( \\binom{N}{n} = \\frac{N!}{n!(N-n)!} = 2^N \\) of the coupling constants matches the total number of configurations available to our spin system, one may inquire if there is a set of coupling constants such that the set of the 2 values of the function E(\u2642\u00bf), i = 1, ..., 2N, can match exactly an arbitrary set of energies Ei, i = 1, ..., 2N: E(\u2642) = E\u00bf. Eq. (6), then, implies the coupling constants would have to solve the following system of 2N linear equations:\n\\begin{equation}\n(\\vec{J} \\vec{\\sigma}_i) = -E_i \\quad (i=1,...2^N).\n\\end{equation}"}, {"title": null, "content": "The solution indeed always exists, is unique, and is straightforwardly obtained by multiplying the above equation by (\u03c3\u00bf on the right, summing over i, and using Eq. (9), see also (Gresele & Marsili, 2017). This yields\n\\begin{equation}\n\\vec{J} = \\frac{1}{2^N} \\sum_{i=1}^{2^N} E_i \\vec{\\sigma}_i,\n\\end{equation}\nor, more explicitly,\n\\begin{equation}\n\\begin{aligned}\nJ_0&= \\frac{1}{2^N} \\sum_i E_i \\\\\nJ_\\alpha &= \\frac{1}{2^N} \\sum_i \\sigma_{\\alpha}^{(i)} E_i \\\\\nJ_{\\alpha\\beta} &= \\frac{1}{2^N} \\sum_i \\sigma_{\\alpha}^{(i)} \\sigma_{\\beta}^{(i)} E_i \\\\\nJ_{\\alpha\\beta\\gamma} &= \\frac{1}{2^N} \\sum_i \\sigma_{\\alpha}^{(i)} \\sigma_{\\beta}^{(i)} \\sigma_{\\gamma}^{(i)} E_i \\\\\n...&\n\\end{aligned}\n\\end{equation}\nIt is not immediately obvious how much the coefficients J from Eq. (11)\u2014and hence the generative model itself\u2014would vary among datasets originating from distinct sources/experiments that one may, nonetheless, deem being qualitatively similar or even equivalent. In the happy event that the variation is indeed small, such robustness could be thought of, by analogy with thermodynamics, as the couplings J being subject to a smoothly varying, free-energy surface. With the aim of constructing such a free-energy surface, we next discuss a variety of ways to connect our generative model with a dataset and, conversely, how to retrieve patterns learned by the model."}, {"title": "Data retrieval and calibration. Learning rules", "content": "We consider a candidate ensemble in which the numbers Zi are driven by source fields in the form of the energies E\u2081 themselves. The couplings J-which are ultimately of interest can be then determined using the relations (11). We are specifically interested in the ability to drive the distribution Zi with respect to the energy E\u2081 relative to some preset reference value E:\n\\begin{equation}\nZ_i = Z_i^\\Theta e^{-(E_i - E^\\Theta)/T} = e^{-[E_i - (E^\\Theta - T \\ln Z_i^\\Theta)]/T}\n\\end{equation}\nWe will call the reference values \u201cstandard\u201d and consistently label them with the symbol . By construction, Ze and E are independent of temperature."}, {"title": "Free energy of learning", "content": "Consider the following object:\n\\begin{equation}\nA(\\lbrace E_i \\rbrace, T) = - T \\ln \\sum_i Z_i = -T \\ln \\sum_i Z_i^\\Theta e^{-(E_i - E^\\Theta)/T},\n\\end{equation}\nwhere we treat each quantity \\( Z_i = Z_i^\\Theta e^{-(E_i - E^\\Theta)/T} \\) and the quantity A itself as functions of the energies E\u00a1 and temperature T. We next consider the following quantity:\n\\begin{equation}\n\\begin{aligned}\n\\tilde{S} &= - \\frac{\\partial A}{\\partial T} = \\ln Z + \\frac{1}{T} \\sum_i (E_i - E^\\Theta) \\frac{Z_i}{Z} = \\ln Z + \\frac{1}{T} \\sum_i (E_i - E^\\Theta) \\frac{Z_i/Z}{Z_i^\\Theta/Z^\\Theta} \\frac{Z_i^\\Theta}{Z^\\Theta} \\\\\n&= \\ln Z + \\sum_i \\frac{E_i - E^\\Theta}{T} x_i = \\sum_i x_i \\ln \\frac{Z_i}{Z} = \\sum_i x_i \\ln \\frac{x_i}{x_i^\\Theta},\n\\end{aligned}\n\\end{equation}\n\\begin{equation}\n< \\tilde{S}^\\Theta\n\\end{equation}\nIn Eq. (14), we reflected that the sum vanishes for the standard weights and used the definition (3) of the weights xi. The inequality in Eq. (15) holds because the sum in the above equation is non-negative. (The latter sum also happens to be the Kullback-Leibler divergence between the distributions defined by xi and x, respectively.) The divergence is minimized\u2014and hence the quantity S is maximized along the line Zi/Z = Z/Z\u00ae = x. If one imposes an additional constraint, for instance by fixing the energy-like quantity\n\\begin{equation}\n\\bar{E} = A + T \\tilde{S} = \\sum_i x_i (E_i - E^\\Theta),\n\\end{equation}\nthe quantity S is now maximized by a unique configuration of {Z;} or, equivalently, of the energies {E;}. Thus the quantity S is an entropy.\nClearly E = 0; consequently\n\\begin{equation}\nA^\\Theta = A(\\lbrace E_i^\\Theta \\rbrace, T) = -T \\tilde{S}^\\Theta.\n\\end{equation}\nThis implies the standard entropy Se is temperature independent, since according to Eq. (13), the standard value A\u00ba\n\\begin{equation}\nA^\\Theta = -T \\ln \\sum_i Z_i^\\Theta = -T \\ln Z^\\Theta\n\\end{equation}\nis strictly proportional to temperature. Also, Eqs. (17) and (13) can be used to show that in the gauge Z = e-E/T\u00b0, where T\u00ba is a positive constant, the standard entropy is given by the following expression:\n\\begin{equation}\n\\tilde{S}^\\Theta = - \\sum_i x_i^\\Theta \\ln x_i^\\Theta = \\sum_i x_i^\\Theta E_i^\\Theta /T^\\Theta, \\quad if \\quad Z_i^\\Theta = e^{-E_i^\\Theta/T^\\Theta}.\n\\end{equation}"}, {"title": null, "content": "The entropy above has the form of the Shannon entropy of the dataset, if one sets \u03a3\u0395 = 0. The latter can be done without loss of generality but will have the effect of fixing the energy reference.\nOne may also readily define a heat capacity-like quantity:\n\\begin{equation}\n\\tilde{C} = \\frac{\\partial \\bar{E}}{\\partial T} = T \\frac{\\partial \\tilde{S}}{\\partial T} = \\sum_i x_i \\frac{(E_i - E^\\Theta)^2 - \\bar{E}^2}{T^2}.\n\\end{equation}\nThe quantities E and C can be regarded as analogs of the conventional energy and heat capacity, respectively, but generalized for a local \u201cgauge\u201d in the form of the state-specific energy references E. Conversely, if we adopt a global gauge, by adopting a uniform E = const, we obtain the conventional forms E = E - const and \\( \\tilde{C} = (\\bar{E}^2 - \\bar{E}^2)/T^2 \\), respectively.\nAccording to Eqs. (13) and (3):\n\\begin{equation}\n\\frac{\\partial A}{\\partial E_i}|_{E_{j \\ne i}, T} = \\frac{Z_i}{Z} = x_i.\n\\end{equation}\nThus we obtain that the function A is a free energy:\n\\begin{equation}\ndA = -\\tilde{S} dT + \\sum_i x_i dE_i\n\\end{equation}\nConsistent with this identification, \\( A(\\lbrace E_i \\rbrace, T = const) \\) is a convex-up function of the variables {E;}. The curvature vanishes in exactly one direction, Zi/Z = const; movement in the latter direction leaves the weights xi invariant.\nA more useful description is afforded by a free energy that operates on the weights xi-which are the actual \u201cobservable\u201d quantities; we accomplish this using Legendre transforms. Choose any number M of configurations of interest, and assign to them labels 1 through M. One may then define the following Legendre transform, whereby one removes the energetic contribution to the shift of the free energy off its standard value, while retaining exclusively the entropic contribution, for configurations 1 through M:\n\\begin{equation}\n\\tilde{A}^{(M)}(\\lbrace x_i \\rbrace, T) = A - \\sum_{i=1}^M (E_i - E^\\Theta) x_i,\n\\end{equation}\nso that\n\\begin{equation}\nd \\tilde{A}^{(M)} = - \\tilde{S}^{(M)} dT - \\sum_{i=1}^M (E_i - E^\\Theta) dx_i.\n\\end{equation}"}, {"title": null, "content": "Thus,\n\\begin{equation}\n\\frac{\\partial \\tilde{A}^{(M)}}{\\partial x_i}|_{x_j, E_k, T} = E_i - E^\\Theta \\quad (i, j \\le M, j \\ne i; k > M)\n\\end{equation}\nand we have defined\n\\begin{equation}\n\\tilde{S}^{(M)} = -(\\frac{\\partial \\tilde{A}^{(M)}}{\\partial T})_{x_i, E_k} \\quad (i \\le M, k > M).\n\\end{equation}\nWe note that\n\\begin{equation}\n\\tilde{S}^{(M)} = \\tilde{S} \\quad \\text{when} \\quad M = 2^N.\n\\end{equation}\nAlso, \\( \\sum_i^M Z_i = Z \\sum_i^M x_i \\); thus \\( Z = Z'/(1 - \\sum_i^M x_i) \\) and, subsequently,\n\\begin{equation}\nZ_i = \\frac{Z'}{1 - \\sum_i^M x_i} x_i\n\\end{equation}\nwhich can be used to express \\( \\tilde{A}^{(M)} \\) explicitly in terms of xi:\n\\begin{equation}\n\\tilde{A}^{(M)} = \\begin{cases}\nT \\sum_i^M x_i \\ln \\frac{x_i}{x_i^\\Theta} + (1 - \\sum_i^M x_i) \\ln \\frac{x'}{x'} - T \\ln Z^\\Theta, \\quad (M < 2^N) \\\\\nT \\sum_i x_i \\ln \\frac{x_i}{x_i^\\Theta} - T \\ln Z^\\Theta, \\quad (M = 2^N)\n\\end{cases}\n\\end{equation}\nwhere\n\\begin{equation}\nx' = Z'/Z.\n\\end{equation}\nand \\( \\sum_i^M x_i = 1 \\), of course. Throughout, summation over configurations spans the range 1 through either the explicitly indicated upper limit or, otherwise, through 2N. We will use the following shorthand below:\n\\begin{equation}\nA= \\tilde{A}^{(M)} \\quad \\text{when} \\quad M = 2^N\n\\end{equation}\nFurther, Eqs. (26) and (29) imply\n\\begin{equation}\n\\tilde{A}^{(M)} = - T \\tilde{S}^{(M)}\n\\end{equation}\nConsequently, \\( d \\tilde{A}^{(M)} = - T d \\tilde{S}^{(M)} - \\tilde{S}^{(M)} dT \\). Combining this with Eq. (24) yields\n\\begin{equation}\nd \\tilde{S}^{(M)} = - \\sum_{i=1}^M \\frac{E_i - E^\\Theta}{T} dx_i.\n\\end{equation}"}, {"title": null, "content": "Thus the entropy \\( \\tilde{S}^{(M)} \\) is uniquely maximized at \\( E_i = E^\\Theta \\) and, hence at \\( x_i = x_i^\\Theta \\). Consequently, the function \\( \\tilde{A}^{(M)} \\) is uniquely minimized at \\( x_i = x_i^\\Theta \\), by Eq. (32):\n\\begin{equation}\n\\tilde{A}^{(M)} > A = -T \\ln Z^\\Theta,\n\\end{equation}\nfor any M. These notions can be also established directly by differentiating Eq. (29) with respect to the xi's. Thus the function A is analogous to a thermodynamic potential governing the relaxation of a single-phase physical system. Note Eqs. (29) and (34) are consistent with Eq. (2.6) of (Dayan et al., 1995).\nThe bound in Eq. (34) is connected to the familiar Gibbs inequality. Indeed, in view of Eq. (23), inequality (34) becomes\n\\begin{equation}\nA^\\Theta < A + \\sum_i^M x_i (E_i^\\Theta - E_i).\n\\end{equation}\nAt M = 2N, the above equation is the traditional way to express the Gibbs inequality (Girardeau & Mazo, 1973):\n\\begin{equation}\nA^\\Theta < A + \\sum_i x_i (E_i^\\Theta - E_i).\n\\end{equation}\nIn other words, the free energy of a system of interest\u2500represented by the standard model in this case\u2014is bounded from above by the free energy for a trial energy function plus the expectation value of the correct energy, relative to the trial energy, where the weights are computed using the trial energy function.\nAccording to Eqs. (11) and (12), there is one-to-one correspondence between the quantities Zi and the coupling constants J. Thus Eqs. (11) can be viewed as the solution of a minimization problem in which one minimizes the thermodynamic potential A with respect to the coupling constants J, while keeping exactly one coupling constant fixed. (It is most convenient to fix Jo, which simply specifies an overall multiplicative factor for the numbers Zi and, hence, does not affect the weights xi.). In this sense, one can think of A as being able to guide a learning process, in principle.\nWhat is the meaning of the distribution e\u2212A({x})/T, which one may nominally associate with the potential A? Analogous distributions arise in Thermodynamics as a result of the canonical construction. Hereby one effectively considers an infinite ensemble of distinct but physically equivalent replicas of the system (McQuarrie, 1973). The width of the distribution e-A/T then should be formally thought of as variations of the weights xi among distinct replicas. To generate such ensembles in practice one can, for instance, break up a very large dataset into smaller but still large\u2014partial datasets. In some cases data may exhibit correlations due to implicit or hidden variables, such as the time and place of"}, {"title": "Coarse-graining, choice of description, and inquiries", "content": "collection. If so, breaking datasets into pertinent subsets may reveal correlations among fluctuations of the weights xi-from subset to subset\u2014that are not necessarily captured by the ideal mixture-like expression (29). Adopting a particular calibration scheme for the inputs of the sensors may also introduce correlations among weight fluctuations. We outline, in Appendix A, a possible way to modify the free energy so as to account for the latter correlations."}, {"title": null, "content": "Because spins are intrinsically discrete degrees of freedom, equivalent yet distinct datasets may result in spin arrangements that are similar in some course-grained sense, yet may be rather dissimilar at the level of individual spins owing, for instance, to noise. It may, then, be advantageous to specify the state of the system at a coarse-grained level, i.e., using not the discrete variables \u03c3\u03b1\u2014themselves or combinations thereof but, instead, their averaged values that retain essential features of the pattern while not being overly sensitive to detailed variations in the polarization pattern. Here we use the coarse-graining recipe underlying the canonical construction whereby one applies a source field h to the spins, which yields the following, equilibrium Gibbs energy:\n\\begin{equation}\nG = -T \\ln \\sum_i Z_i e^{(\\vec{h} \\vec{\\sigma}_i)/T}.\n\\end{equation}\nThe source field h itself is independent of spin configuration and can be used to stabilize spin arrangements of interest. Throughout, we will limit ourselves to source fields in the form of a sum of one-spin, onsite fields:\n\\begin{equation}\n<h \\sigma> = \\sum_{\\alpha} h_\\alpha \\sigma_\\alpha.\n\\end{equation}\nIn a straightforward manner, one can determine the typical magnetization of spin a by varying the free energy G with respect to the source field ha:\n\\begin{equation}\n\\frac{\\partial G}{\\partial h_\\alpha} = \\sum_i x_i \\sigma_{\\alpha}^{(i)} = \\overline{\\sigma_\\alpha} = m_\\alpha,\n\\end{equation}\nwhere the weights xi now reflect the additional bias due to the external field:\n\\begin{equation}\nx_i = \\frac{Z_i e^{(\\vec{h} \\vec{\\sigma}_i)/T}}{\\sum_j Z_j e^{(\\vec{h} \\vec{\\sigma}_j)/T}},\n\\end{equation}"}, {"title": null, "content": "The simple type of source field on the r.h.s. of Eq. (38) is just one particular\u2014 albeit instructive\u2014functional form for a source field. An arbitrary linear combi- nation of spin products \u03c3\u03b1", "way": "n\\begin{equation"}, "nA = G + \\sum_\\alpha h_\\alpha m_\\alpha,\n\\end{equation}\nwhere the source fields ha can be thought of as external fields that would be necessary to induce a magnetization pattern of interest:\n\\begin{equation}\nh_\\alpha = \\frac{\\partial A}{\\partial m_\\alpha}.\n\\end{equation}\nAt the same time,\n\\begin{equation}\n\\frac{\\partial G}{\\partial E_i} = x_i\n\\end{equation}\nc.f. Eq. (21), and so the quantity\n\\begin{equation}\n\\tilde{G} = G - \\sum_i x_i (E_i - E^\\Theta)\n\\end{equation}\nshould be recognized as the Gibbs counterpart of \\( \\tilde{A} \\).\nWe make a connection with the thermodynamic potential A from Eq. (29) by repeating the steps that led to Eq. (29) while having replaced E\u00bf by the quantity \\( E_i^{(h)} \\):\n\\begin{equation}\nE_i^{(h)} = E_i - (\\vec{h} \\vec{\\sigma}_i)\n\\end{equation}\nin Eq. (23) and A by G. One thus obtains\n\\begin{equation}\nG - \\sum_i (E_i^{(h)} - E^\\Theta) x_i = \\tilde{A}(\\{x_i"]}