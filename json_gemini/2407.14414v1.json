{"title": "System-1.x: Learning to Balance Fast and Slow Planning with Language Models", "authors": ["Swarnadeep Saha", "Archiki Prasad", "Justin Chih-Yao Chen", "Peter Hase", "Elias Stengel-Eskin", "Mohit Bansal"], "abstract": "Language models can be used to solve long-horizon planning problems in two distinct modes. In a fast 'System-1' mode, models directly generate plans without any explicit search or backtracking, and in a slow 'System-2' mode, they plan step-by-step by explicitly searching over possible actions. While System-2 planning is typically more effective, it is also more computationally expensive, often making it infeasible for long plans or large action spaces. Moreover, isolated System-1 or System-2 planning ignores the user's end goals and constraints (e.g., token budget), failing to provide ways for the user to control their behavior. To this end, we propose the System-1.x Planner, a controllable planning framework with language models that is capable of generating hybrid plans and balancing between the two planning modes based on the difficulty of the problem at hand. System-1.x consists of (i) a controller, (ii) a System-1 Planner, and (iii) a System-2 Planner. Based on a user-specified hybridization factor x governing the degree to which the system uses System-1 vs. System-2, the controller decomposes a planning problem into sub-goals, and classifies them as easy or hard to be solved by either System-1 or System-2, respectively. We fine-tune all three components on top of a single base LLM, requiring only search traces as supervision. Experiments with two diverse planning tasks \u2013 Maze Navigation and Blocksworld \u2013 show that our System-1.x Planner outperforms a System-1 Planner, a System-2 Planner trained to approximate A* search, and also a symbolic planner (A* search), given an exploration budget. We also demonstrate the following key properties of our planner: (1) controllability: by adjusting the hybridization factor x (e.g., System-1.75 vs. System-1.5) we can perform more (or less) search, improving performance, (2) flexibility: by building a neuro-symbolic variant composed of a neural System-1 planner and a symbolic System-2 planner, we can take advantage of existing symbolic methods, and (3) generalizability: by learning from different search algorithms (BFS, DFS, A*), we show that our method is robust to the choice of search algorithm used for training.", "sections": [{"title": "1 Introduction", "content": "A key feature of intelligence \u2013 both human and artificial \u2013 is the ability to plan. Indeed, planning has been a major topic of research in AI for most of its history [Russell and Norvig, 2016], with applications ranging from navigation and robotics to manufacturing and story-telling. These days, auto-regressive Large Language Models (LLMs) are increasingly used for various long-horizon planning"}, {"title": "2 System-1.x: Controllable and Hybrid Planning with LLMs", "content": "We start with the requisite background on planning problems and our setup (\u00a72.1), followed by an overview and a detailed description of our method System-1.x Planner in \u00a72.2 and \u00a72.3, respectively."}, {"title": "2.1 Background and Problem Setup", "content": "A planning problem can be modeled as a Markov Decision Process M = (S, A, T, R) defined with a set of states S, a set of actions A on any given state, a transition function T : S \u00d7 A \u2192 S defining transitions between pairs of states based on an action, and a reward function R : S \u2192 R assigning a reward for reaching the goal state. Given such a planning problem, a start state so \u2208 S and a goal state sg \u2208 S, the objective of classical planning is to generate a plan P = (a1, ..., an), as a sequence of n actions ar \u2208 A that helps transition from the start state to the goal state. Using this notation, we define some salient concepts in a planning problem below:"}, {"title": "2.2 System-1.x Overview", "content": "Using the setup described in \u00a72.1, we develop our System-1.x Planner that consists of three trained components: (i) a Controller for decomposing planning problems into sub-goals and classifying them into easy or hard sub-goals, (ii) a System-1 Planner for fast planning of easy sub-goals, and (iii) a System-2 Planner for slow planning of hard sub-goals. See Fig. 2 for an overview. We train all three components by automatically generating data from the gold search traces in a way that also respects the user's constraints (specified in the form of the hybridization factor). In the following two subsections, we describe how we train each component and how all three join to make System-1.x."}, {"title": "2.3 Training Data Generation for System-1.x", "content": "System-1 Planner Training Data. The training data for the System-1 Planner is Dsys1 = {(si), s)), P(i)}\u2081 where the input is a natural language description of the planning problem consisting of a start state s) and a goal state si) and the output is the corresponding plan in natural language P(i) (see Appendix C for examples). For the Maze domain, these gold plans are also optimal plans while for Blocksworld, they may include additional explorations, making them sub-optimal at times (though they are always valid).\nSystem-2 Training Data. Similar to System-1, the input to a System-2 Planner is also a natural language description of the planning problem. However, the target output of the System-2 Planner is a verbalized search trajectory T(i) as described in \u00a72.1 (refer to Appendix C for examples). This yields a training dataset of Dsys2 = {(s(i), si)), T()}1.\nController Training Data. Recall that the System-1.x Planner is designed to minimize state explorations by only performing System-2 planning as needed, i.e., saving states by employing System-1 for easier sub-goals and reserving search with System-2 for harder sub-goals. Hence, the role of the controller is two-fold: (i) decomposing a problem into sub-goals characterized by a start and end state (si, sj), and (ii) classifying those sub-goals as easy or hard so that System-2 can be invoked only for the hard sub-goals. However, most planning domains lack gold annotations of sub-goal decompositions and their easy and hard classifications. Therefore, we propose an automatic data generation process for training our controller, as described below and more formally in Algorithm 1. The inputs to preparing the controller data are (1) the gold plans {P(i)}\\\u2081 that will be decomposed into sub-goals, (2) the user-defined hybridization factor x, and (3) a hardness function h(,) for (sub-)goals.\n\u2022 Step 1: Defining and Ranking using a Hardness Function. A hardness function h(si, sj) takes a start state si and a goal state s; as input, and returns a scalar value estimating the hardness of the goal or the sub-goal. An example of a hardness function for a Maze Navigation task is the number of obstacles in a sub-maze, where a higher value indicates greater difficulty. Sections \u00a73.4, and \u00a74.3 more concretely describe and compare these hardness functions for different tasks,"}, {"title": "2.4 Training and Inference of System-1.x Planner", "content": "Using the data generation process described above, we finetune a single base LLM to behave as a System-1 Planner, a System-2 Planner, or a controller. During inference, we first use the controller to generate a meta-plan a list of sub-goals and their easy versus hard assignments. After solving each with the appropriate planning model, we concatenate the sub-plans from the two planners, in order, to construct the final plan (see Fig. 2 for an example of a concatenated plan, denoted by '+').\nTrain- and Test-time Controllability. We design System-1.x such that there is a single point of control for training the system, i.e., the hybridization factor x, and this control lies with the end-user. Setting it adjusts the training data for the controller, i.e., what is annotated as easy versus hard, both across samples (Step 2 of the algorithm) and within samples (with sub-goals; Step 3 of the algorithm). This ultimately yields the user-specified level of hybridization. Hence, System-1.x offers training-time control of compute (e.g., one could train a System-1.25, 1.5, or a 1.75 Planner). Furthermore, the controller's design also offers an additional degree of test-time control: since the controller classifies sub-goals as System-1 or System-2, the end-user could also re-balance the model towards System-1 or 2 based on the controller confidence threshold. By default, the controller generates System '1' or '2' based on which token has a higher probability mass. However, the user can add a bias term to these token probabilities to solve a greater or smaller number of sub-goals using System-1 or System-2, adjusting the behavior of an already-trained System-1.x Planner. This means that a System-1.5 Planner, for example, can be biased to behave as a System-1.25 Planner or a System-1.75 Planner at test time. In our results in Figs. 3 and 5 to 7, we explore how both train-time and test-time control impacts performance."}, {"title": "3 Experimental Setup", "content": ""}, {"title": "3.1 Tasks", "content": "We evaluate System-1.x Planner on two classical planning tasks that are challenging for LLMs [Valmeekam et al., 2023, Lehnert et al., 2024]: (1) Maze Navigation and (2) Blocksworld.\nMaze Navigation. We first experiment with a robotic maze navigation task \u2013 given a 2D maze with some cells containing obstacles and four permissible actions {'left', 'right', 'up', 'down'}, the objective is to find a path from a start state to a goal state by avoiding the obstacles. We randomly generate a balanced dataset of 4K planning problems for maze navigation using the following configuration - 5x5 maze, 40% of the cells containing obstacles, and an equal number of problems (500) with optimal plan lengths between 1 to 8. Controlling for the plan length allows us to study a planner's ability to solve harder planning problems. We remove duplicates (problems where the placement of the obstacles, the start state, and the goal state match exactly), and construct a 3200/400/400 (train/validation/test) balanced split for training our models.\nBlocksworld. Additionally, to test out-of-distribution (OOD) generalization to longer plans, we experiment with Blocksworld, a classical planning problem from the International Planning Conference (IPC)-2000. The task consists of a table and a few blocks stacked on the table or on top of each other in one configuration. A robot is tasked to move blocks to another goal configuration by only moving one unstacked block at a time. Following the data creation algorithm in Bohnet et al. [2024], we generate problems consisting of 4-7 blocks (without repetition). From there, we create a train/validation/test split of 3000/250/200 samples where the train and the validation split consist of samples with plan lengths 1-6 and the test split consists of samples with plan lengths 7-10. Different from maze which has a fixed action space of 4, Blocksworld has a much larger action space that also increases with the number of blocks."}, {"title": "3.2 Baselines", "content": "We compare our System-1.x Planner with (1) a System-1 Planner, (2) a System-2 Planner, (3) a symbolic planner, specifically A*, and (4) a System-1.x Planner without sub-goal decomposition."}, {"title": "3.3 Evaluation Metrics", "content": "We compare all methods along two axes: accuracy and cost. As defined in \u00a72.1, accuracy is given by the fraction of valid plans generated by a planner while for cost, we compute the average #States-Explored by a planner before reaching the goal.\nReporting Results. Note that in order to fairly compare the performance of Systems 1, 2, and 1.x, their #States-Explored should be matched, such that we compare performance at a fixed budget for each system. First, since System-1 does not do any exploration, it is constant w.r.t. budget and hence has the same accuracy irrespective of #States-Explored. Next, to adapt System-2 and System-1.x to lower budgets, we truncate the search when the maximum number of states allowed by the budget has been reached. This value of maximum allowed states is given by the highest possible value such that the average #States-Explored matches the desired budget. We refer to this as truncation below. Finally, to adapt System-1.x to higher budgets, we use System-1.x's test-time controllability, increasing the bias term on System-2 reasoning (cf. \u00a72.4). As this term increases, the model uses more and more System-2 planning, i.e., x \u2192 1 in System-1.x. We refer to this as test-time control.\nWe perform this matching and report plan validity (i.e., accuracy) for the following values of #States-Explored. First, we vary the maximum budget of states explored in intervals of 5 for Maze and 10 for Blocksworld, up to the number of states used by System-2 (the most compute-intensive system we consider). We also consider two specific points in the range of #States-Explored: we match System-2's #States-Explored to the number of states that System-1.x uses by default. In the other direction, we match System-1.x's #States-Explored to the number of states used by System-2. These points let us directly compare the two systems without any truncation or test-time control. In the main paper, we present our results with plots while we refer to the Appendix D for detailed tables."}, {"title": "3.4 Implementation Details", "content": "We choose Mistral-7B-Instruct-v0.2 [Jiang et al., 2023] as the base LLM and fine-tune all our components with LoRA [Hu et al., 2021] with a rank of 8 for a maximum of 3 epochs and a batch size of 4, resulting in three adapters for System-1, System-2, and the controller. For the purposes of \u00a74.1, System-1.x will specifically refer to a System-1.5 Planner (i.e., x=0.5, which is a balanced hybrid between a System-1 Planner and System-2 Planner) and fine-tuned from A* traces.\nHardness Function for Maze Navigation. Given two maze states si = (a, b) and sj = (c, d) we define hardness of a sub-goal h(si, sj) as the number of obstacles in the rectangular sub-maze given the end-points. More obstacles will mean that the model might have to search more and would likely benefit from System-2 planning.\nHardness Function for Blocksworld. A state in Blocksworld is characterized by the configuration of the blocks. Our hardness function computes a distance metric for a sub-goal: For every block that is not in its correct position in the goal state (i.e., the blocks above and below it are different between the start and goal states), we add a cost of 1. Additionally, for a block that is not on the table and also not in its correct position, we add 1 more to the cost, indicating greater hardness for moving blocks that are in the middle of a stack."}, {"title": "4 Results and Analysis", "content": ""}, {"title": "4.1 System-1.x outperforms System-1 and 2", "content": "Study Setup. In this first set of experiments, we compare our System-1.x Planner with the following baselines (cf. \u00a73.2) (1) a System-1 Planner, (2) a System-2 Planner, and (3) a System-1.x Planner without sub-goal decomposition.\nResults for Maze Navigation. Fig. 3(a) (cf. Table 3) shows the plan accuracy obtained by each planner at different #States-Explored. The values of #States-Explored are as described previously in \u00a73.3. Based on this, we summarize our key findings.\n\u2022 System-1.x outperforms all baselines at all budgets. Without truncation or test-time control, System-1.x (x=0.5) uses 13.6 states on average. At this number of states, it obtains an accuracy of 70.4%, outperforming the System-1 Planner by 21% (48.7% \u2192 70.4%), the System-2 Planner by 33% (37.2% \u2192 70.4%), and the System-1.x variant without sub-goal decomposition by 20% (50.0% \u2192 70.4%). Next, to compare all methods at lower budgets (i.e., with #States-Explored of 5 and 10), we do state truncation (as described in \u00a73.3) by limiting the search (generation) to a maximum number of states. We find that even at these lower budgets, System-1.x continues to be significantly more accurate compared to baselines, pointing to an effective allocation of resources. Finally, System-1.x's test-time control capability lets us increase compute to match that of System-2 by biasing the controller and solving more sub-goals with System-2. Doing so eventually transforms a System-1.x Planner into a System-2 Planner with sub-goal decomposition, outperforming vanilla System-2 by 3% and achieving the highest accuracy of 96.7% at 27.3 states. This highlights the effectiveness of our sub-goal decomposition (more details below).\n\u2022 System-1.x benefits from sub-goal decomposition. We validate the utility of sub-goal decomposition from two observations. First, the full System-1.x outperforms the System-1.x variant that does not perform sub-goal decomposition. Second, System-2 also benefits from sub-goal decomposition as shown by the maximum accuracy of 96.7% which is achieved via test-time control and biasing the controller to solve every sub-goal using System-2. However, note that System-2 with decomposition still applies System-2 planning to every step, making System-1.x more performant at any given budget.\n\u2022 System-1 is cheaper but not accurate. On one extreme of the plot, System-1 Planner is cheaper since it only explores the states that are part of its plan, amounting to an average of 3 states. However, this comes at a cost to performance: it also only achieves an accuracy of 49%. Another limitation of a System-1 Planner is that unlike System-1.x, it is not controllable and hence, there is no easy affordance for improving performance by exploring more states."}, {"title": "4.2 Neuro-symbolic System-1.x outperforms or matches A* at all budgets", "content": "Study Setup. While we primarily designed System-1.x as a fully neural planner with all three components (System-1, 2, and the controller) developed on top of the same LLM, we note that System-1.x can also act as a neuro-symbolic planner. This can be achieved by using a symbolic solver like A* as the System-2 component. We follow this setup to report results on the Maze Navigation task, again setting x=0.5.\nResults. Table 5 (cf. Table 5) compares the accuracy of all planners at different #States-Explored. Our main result is that the conclusions drawn with a fully neural System-1.x also transfer to a neuro-symbolic System-1.x. Notably, at an average of 11.6 states, neuro-symbolic System-1.x outperforms A* by a large margin of 39% (31.0% \u2192 70.5%). This can again be explained by the ineffectiveness of #States-Explored truncation for"}, {"title": "4.3 Analyses and Ablations of System-1.x Planner", "content": "We conduct all analyses and ablations on the maze navigation task.\nTrain-time Control: System-1.75 trades off efficiency for accuracy compared to System- 1.5. One of the core strengths of our planner is its training-time controllability. The hybridization factor x allows the user to specify the System-1 to System-2 balance they want in the final planner. To demonstrate this capability, we train a System-1.75 Planner by setting x = 0.75. Table 6 (cf. Table 6) shows our results on the maze task. By default, System- 1.75 uses 16.6 states on average and obtains an accuracy of 75.7%, compared to System-1.5's 70.4% with a default average of 13.6 states (Table 3) i.e., a 5% gain at the cost of 3 more states. Thus, as we increase the value of x, the System-1.x Planner will use more states and improve performance, converging to a System-2 Planner that additionally performs sub-goal decomposition. Compared to System-2, System-1.75 is again significantly more performant at all budgets (e.g., by 28% at 16.6 states) and eventually obtains the highest accuracy of 96.7% when acting as a System-2 with sub-goals. This echoes our System-1.5 Planner findings, which also demonstrated similar gains at high budgets.\nBroadly, these results showcase our controller's effectiveness, which in turn validates our hardness measures and sub-goal decompositions. We further ablate features of the controller in Appendix B, comparing against a random controller, testing the effectiveness of the sliding window, and contrasting different hardness functions. These experiments confirm our choices in \u00a73.4.\nGeneralizability: System-1.x beats System-2 with all search algorithms (A*, DFS, BFS). We conducted our main experiments by leveraging supervision from A* traces. In this experiment, we show that System-1.x is robust to different search algorithms, working with other options like"}, {"title": "5 Discussion and Limitations", "content": "Comparison to Dual-Process Theories. While System-1.x draws on dual process theories of human cognition, it deviates in important ways. Crucially, most dual-process accounts hold that System-1 thinking occurs all the time, with System-2 occurring some of the time [Evans, 2003], whereas in our planning framework, the two are mutually exclusive. It is worth noting that, whether mutually exclusive or not, dual-process accounts require some control mechanism that triggers System-2; errors in this controller can lead to compounding errors downstream, which has also been posited as a theory for breakdowns in human cognition [Tversky and Kahneman, 1974, Gilovich et al., 2002]. We also do not take a position on whether performing search by next-token prediction is optimal; indeed, our neuro-symbolic results in Fig. 5 indicate that System-1.x is compatible with both neural implementations of search and symbolic search. Dual-process accounts also typically rely on some form of metacognition, where agents reason not only about the task, but also form a model of reasoning itself [Ganapini et al., 2022]. This entails having not only a world model \u2013 in"}, {"title": "6 Related Work", "content": "Combining LLMs and Symbolic Planners. Despite the widespread success of LLMs in complex problem-solving, a large body of recent work has highlighted their shortcomings on long-horizon planning problems [Valmeekam et al., 2023, 2024, Pallagani et al., 2023, Momennejad et al., 2024, Hirsch et al., 2024, Zheng et al., 2024, Aghzal et al., 2023], persisting across popular prompting techniques like Chain-of-Thought [Wei et al., 2022], ReAct [Yao et al., 2022], and Reflexion [Shinn et al., 2024]. These methods can largely be grouped into System-1 inference-time methods in which the LLM does not search, backtrack, or learn from incorrect actions. Systematic search is either not performed at all or delegated to symbolic solvers [e.g., Liu et al., 2023, Pan et al., 2023, Xie et al., 2023, discussed further below], thus deviating from the focus of this work on equipping LLMs to solve harder planning problems via search.\nMore related to our work are inference-time methods that do involve planning, tree search, and backtracking. These System-2-inspired methods have a search algorithm that operates on top of and guides an LLM with reward models, value functions, or verifiers [Hao et al., 2023, Yao et al., 2024, Besta et al., 2024, Zhou et al., 2023, Sel et al., 2023, Koh et al., 2024]. In a similar spirit, neuro-symbolic planning systems like LLM-Modulo Frameworks [Kambhampati et al., 2024] have emerged that combine LLMs with symbolic planners [Nye et al., 2021, Liu et al., 2023, Pan et al., 2023, Xie et al., 2023, Kambhampati et al., 2024, Zuo et al., 2024] System-1.x differs from this class of inference-time methods in two major ways. Firstly, it is a training-time method that teaches the LLM to search, with the goal of enhancing the LLM's intrinsic planning capabilities and discovering more effective search strategies, making it self-contained (as opposed to complex inference-time methods relying on external modules). Secondly, it is also a hybrid system that can alternate between System-1 and System-2-style planning, whereas past work is capable of one or the other, but not both. Notably, this kind of hybridization distinguishes System-1.x from other work that either creates hybrids between weaker and stronger models in the form of model cascades [Lin et al., 2024, Yue et al., 2023] or between neural and symbolic methods (e.g., LLM-Modulo frameworks).\nLLMs Generating Plans. Methods combining LLMs and symbolic planners need some channel of communication between the LLM and the symbolic planning component. This is usually a value function or reward model producing a single score for a state, making the success of the method contingent on the quality of the reward model, or the LLM's ability to act as a value function (as in Yao et al. [2024], Lightman et al. [2023]). Furthermore, off-loading planning or search adds additional modules at test-time that may not always be available for certain domains. It also assumes"}, {"title": "7 Conclusion", "content": "We introduced the System-1.x Planner, a novel neural planner that adaptively interleaves quick and cost-efficient System-1 planning with more costly but performant System-2 planning. On two tasks, we showed that our method results in strong performance across a range of budgets: on Mazes, System-1.x beats obtains the best performance at every budget, and on Blocksworld it outperforms System-2 at all budgets except the highest, where it is comparable. System-1.x Planner achieves this by decomposing problems into sub-goals and intelligently allocating System-2's additional resources to harder sub-goals while allowing System-1 to quickly solve the easier ones. Furthermore, our method is adaptable, allowing users to control the cost and performance by specifying the degree to which the planner relies on System-2. In our analysis, we find that System-1.x is controllable, generalizes to various search algorithms, and performs better out-of-distribution. We also ablate choices to our training data creation, including the choice of the hardness function and the search algorithm, finding System-1.x is robust to such changes."}]}