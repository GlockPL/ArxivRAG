{"title": "CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation", "authors": ["Ingo Ziegler", "Abdullatif K\u00f6ksal", "Desmond Elliott", "Hinrich Sch\u00fctze"], "abstract": "Building high-quality datasets for specialized tasks is a time-consuming and resource-intensive process that often requires specialized domain knowledge. We propose Corpus Retrieval and Augmentation for Fine-Tuning (CRAFT), a method for generating synthetic datasets, given a small number of user-written few-shots that demonstrate the task to be performed. Given the few-shot examples, we use large-scale public web-crawled corpora and similarity-based document retrieval to find other relevant human-written documents. Lastly, instruction-tuned large language models (LLMs) augment the retrieved documents into custom-formatted task samples, which then can be used for fine-tuning. We demonstrate that CRAFT can efficiently generate large-scale task-specific training datasets for four diverse tasks: biology question-answering (QA), medicine QA and commonsense QA as well as summarization. Our experiments show that CRAFT-based models outperform or achieve comparable performance to general LLMs for QA tasks, while CRAFT-based summarization models outperform models trained on human-curated data by 46 preference points.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) demonstrate strong generalization capabilities across diverse tasks (Dubey et al., 2024; Ouyang et al., 2022), but optimizing these models for specific tasks remains a considerable challenge. Although zero-shot and few-shot prompting methods provide some degree of adaptability (Dong et al., 2024), task-specific fine-tuning generally delivers better performance, particularly for specialized and out-of-domain tasks (Liu et al., 2022). A key challenge for effective fine-tuning is obtaining high-quality task-specific examples at large scale.\nTraditionally, creating high-quality datasets for specific tasks involves a time-consuming and resource-intensive process, often requiring extensive manual curation and annotation (e.g. Marcus et al. (1993)). This challenge is particularly acute for low-resource domains or novel tasks where existing datasets may be limited or non-existent.\nOn the other hand, \"raw\" (i.e., unannotated, free-text) web-crawled corpora are known for their diversity and potential utility for various tasks (Maini et al., 2024). Prior work has used raw data by targeted crawling of recipe websites (Bie\u0144 et al., 2020) or word-specific filtering of crawling metadata to gather examples from pre-training corpora for sentiment analysis and summarization tasks via ratings (Maas et al., 2011) and bullet point summaries found in news articles (See et al., 2017). These approaches either rely on a predefined task definition based on keywords, or on the targeted crawling of websites which are expected to contain the desired content. This reliance hinders the generalization of these methods to tasks where such prior knowledge is unavailable, difficult to define, or highly context-dependent.\nIn this work, we propose Corpus Retrieval and Augmentation for Fine-Tuning (CRAFT) to curate task-specific samples from raw data for a wide variety of tasks. CRAFT only requires a small set of few-shot examples from a user to initiate the process of crawling and structuring task examples. CRAFT first detects relevant corpus examples from large-scale unannotated corpora using similarity-based retrieval. Then it uses LLMs to structure these examples into a proper task format, effectively transforming free-text documents into custom-formatted task samples for fine-tuning.\nWe demonstrate the effectiveness of CRAFT on four diverse tasks: three QA tasks \u2013 in biology, medicine and commonsense as well as a text summarization generative task. Our results show that models fine-tuned on CRAFT-generated datasets achieve performance that is either better than or comparable to instruction-tuned LLMs. This holds across diverse tasks, LLMs, and dataset sizes, highlighting the effectiveness of our approach. We publicly release the code to craft datasets for other tasks as well as all datasets and checkpoints at github.com/ziegler-ingo/CRAFT."}, {"title": "Related Work", "content": ""}, {"title": "Optimizing LLMs for Specific Tasks", "content": "Prompting: Prompts are added to the input to provide additional context that guides the computation and output of a model (Gu et al., 2023). A prompt usually takes the form of a natural language instruction (Radford et al., 2019; Brown et al., 2020). Prompting is commonly used with instruction-tuned models to define tasks and extract responses from language models, using natural language, without gradient updates.\nZero-Shot Inference: Originally discovered in the vision domain, zero-shot inference (Larochelle et al., 2008) is a technique that allows models to generalize their learned knowledge from pre-training to previously unseen classes, tasks, or sample instance variants at inference time without gradient updates. Pre-training LLMs on large corpora produces semantic representations that are generally applicable to multiple downstream tasks. GPT-2 (Radford et al., 2019) demonstrated that the acquired capabilities can then be activated by prompting a new task in natural language. However, zero-shot inference often falls short of the performance achieved by few-shot learning (Brown et al., 2020).\nFew-Shot Learning: In few-shot learning, the model is provided with a small number of task-specific examples at inference time. The few-shot examples are given to the model in the prompt, in a technique known as in-context learning (Brown et al., 2020). While full fine-tuning generally requires a substantial amount of labeled data, few-shot learning offers an inexpensive alternative to adapt a model to a new task with a limited number of examples (Dong et al., 2024). Nonetheless, few-shot learning faces several challenges, including inaccurate assessment of the underlying data distribution (Song et al., 2023), biases related to small sample sizes (Song et al., 2023), and sensitivity to shot length (Liu et al., 2024), shot quality and noise (Perez et al., 2021; Chang et al., 2021; Chen et al., 2022).\nFull Fine-Tuning: During full fine-tuning, all model parameters are updated on a large dataset with the goal of adapting the model to a domain, task or dataset (Howard and Ruder, 2018). This approach usually provides the best performance by learning task-specific patterns and relationships that may not be captured by pre-training and zero-or few-shot learning alone. However, it requires a dataset of appropriate size.\nInstruction Tuning: Instruction tuning (Wei et al., 2022) is a type of full fine-tuning that optimizes a model to produce more relevant answers to questions or instructions (Leike et al., 2018; Askell et al., 2021). This approach enables language models to better understand and follow user intents rather than simply continuing the input text. Instruction-tuned models regularly produce answers that are preferred by humans for tasks ranging from question-answering to summarization (Ouyang et al., 2022). The main challenge is to obtain a large high-quality dataset that is both task-specific and in the desired instruction-output format.\nLow-Rank Adaptation: Full fine-tuning may be too expensive for LLMs but the difference between pre-trained weights and their fine-tuned counterparts often has low rank (Li et al., 2018; Aghajanyan et al., 2021). Low-Rank Adaptation (Hu et al., 2021, LoRA) approximates these low-rank matrices during fine-tuning, and is efficient because it freezes the full model and only learns the low-rank matrices, which typically results in learning the equivalent of 2% of the model's parameters."}, {"title": "Synthetic Data Generation", "content": "Synthetic data refers to artificially generated data that mimics the characteristics of real-world data (Little et al., 1993). It can be generated using statistical (Sue, 1987; Maqsud, 2015) or deep neural approaches (Sutskever et al., 2011) with the aim of replicating the patterns, distributions, and structures found in real-world datasets.\nFully Synthetic Data Generation: A dataset is fully synthetic if the question or instruction, the possible context, as well as the answers are generated synthetically. For instance, Self-Instruct (Wang et al., 2023b), Unnatural Instructions (Honovich et al., 2023), Alpaca (Taori et al., 2023), and Evol-Instruct (Xu et al., 2023) are examples of fully synthetic general-purpose data generated by LLMs. More focused approaches for task-specific fine-tuning data generation have also been proposed, especially based around the rephrasing of already existing task-specific datasets (Yin et al., 2023; Gandhi et al., 2024). Methods that use general-purpose corpora have recently been proposed for generating pre-training data (Maini et al., 2024). When used for fine-tuning data generation, these methods are either based around complex and resource-intensive multi-agent workflows (Mitra et al., 2024) or are restricted to a small set of tasks, as the generation process relies on a model that has been fine-tuned for those tasks (Nayak et al., 2024).\nThe two greatest drawbacks of current approaches to fully synthetic data generation are repetition and low quality. Unnatural Instructions reported that a majority of their samples have a BERTScore (Zhang et al., 2020) of above 45% when compared to other samples in the generated dataset. Self-Instruct faces similar issues, with generated instructions often having ROUGE-L scores (Lin, 2004) greater than 0.4 compared to the provided seed instructions. Both approaches also only contain about 54%-56.5% correct samples, while the correctness rate in Alpaca is as low as 17% (Chen et al., 2024a). This suggests that a large portion of the samples in these datasets may not be useful for fine-tuning models.\nPartially Synthetic Data Generation: In partially synthetic data generation, a portion of the input, context, or output is generated synthetically, while the remaining portion is human-curated. It is distinct from approaches that combine fully synthetic and purely human-curated samples at the dataset level, such as Phi (Gunasekar et al., 2023).\nOne recent approach is reverse instruction generation (K\u00f6ksal et al., 2023), where a language model, provided with a human-curated output in context, generates the instruction that would have prompted this output. This produces more coherent and correct input-output pairs because the LLM does not need to generate the longer and more complex component of the data sample. There are also approaches where, conversely, the output is synthetically generated from human-curated input samples. Such methods employ distillation techniques to extract patterns from larger, more capable models to teach those patterns and skills to smaller models (Mukherjee et al., 2023; Mitra et al., 2023).\nPartially synthetic data generation alleviates some of the quality and diversity concerns of fully synthetic data generation. However, taking a raw corpus document as the output can result in generating noisy or unnecessary information (Agarwal et al., 2007), but data augmentation can mitigate these problems when generating pre-training data (Maini et al., 2024). However, when data augmentation was used to generate fine-tuning data, it required GPT-4 (OpenAI, 2023) to build an intermediate synthetic dataset to fine-tune a sample creator model (Chen et al., 2024b). This approach can result in a sample creator model that is a distilled version of the larger model's knowledge and data, while also limiting the model's task flexibility, depending on the synthesized training data.\nIn contrast, CRAFT produces fully synthetic data but leverages the quality and diversity advantages of human-written documents from partially synthetic data generation approaches while removing noise through augmentation. Our approach does not require intermediate datasets, nor a separately fine-tuned model, nor knowledge distillation from a larger model; instead, it relies only on a small number of human-curated examples, retrieval, and in-context learning."}, {"title": "The CRAFT Approach", "content": ""}, {"title": "Architecture Overview", "content": "CRAFT is used to fine-tune language models by generating task-specific synthetic datasets, given a few human-curated examples of the task. During CRAFT (see Figure 1), we retrieve human-written, free-text documents from a large collection of corpora by calculating their similarity to the provided few-shots and transforming them into the task-specific format through augmentation. The only human effort required is in writing a small number of high-quality examples of the target task. CRAFT has two phases: In the initial phase, an embedding database is created from large corpora. While this phase can be resource-intensive, its cost is incurred only once for all subsequent tasks, and it can be easily expanded with new corpora. In the second phase, the user-generated, task-specific few-shot examples are embedded, enabling the retrieval of relevant documents by calculating similarity measures between few-shots and corpus documents. Once relevant documents are retrieved, an instruction-tuned LLM is used to augment the retrieved free-text documents into a task-specific design, generating synthetic task samples in the layout that is needed for instruction-tuning (illustrated in Figure 1). Finally, the synthetic dataset is used to fine-tune a task-specific language model. We report implementation details for the whole CRAFT framework in Appendix A."}, {"title": "Few-Shot Examples", "content": "A small number of human-curated few-shots serve as the \"definition\" of the task, i.e., they indicate how the task is to be performed. The few-shot samples consist of three elements: (i) a long text that mirrors in language, content, and accuracy what a high-quality corpus sample from the web should look like, (ii) a natural language instruction for the task to be performed, which can take the form of a direct instruction or a question about the text, and (iii) an output that satisfies the instruction or answers the question the way that the final model should later respond. Length statistics for texts, instructions, and outputs of our few-shots can be found in the XS row of Appendix C.\nWe note that the task does not need to be explicitly specified. For example, there is no need to state the task as \u201cbiology question-answering\"; it is sufficient for the human-curated few shots to focus on QA in the domain of biology. If multiple-choice questions or single-letter outputs are in the few-shots, this will result in a corresponding dataset and fine-tuned model behavior. These examples show that CRAFT is highly customizable: Few-shot examples enable users to tailor the model's behavior to specific formats, use cases, or domains. Users can create few-shots with unique terminology, style preferences, or domain-specific constraints, optimizing the retrieval and the final model's performance for particular tasks."}, {"title": "Corpora and Embedding Database", "content": "The embedding database is a key element of CRAFT as it provides, for all corpora, embeddings of human-written documents that should be retrievable for task-specific augmentation. It is, therefore, important that the embedding database encompasses a wide variety of linguistically and semantically diverse documents. This diversity can be achieved by including corpora that exhibit different writing styles, tones, and vocabularies. Task-specific, task-agnostic, public, and also private documents can provide a comprehensive coverage of relevant information. The more varied the documents in the embedding database, the better the coverage will be for diverse or rare tasks. Notably, CRAFT can also handle sensitive company data, as the encoding, storage, and retrieval can be performed on-site."}, {"title": "Document Retrieval", "content": "Our retrieval system is task agnostic, both in terms of domain and complexity, in contrast to previous approaches (Ein-Dor et al., 2020; Dai et al., 2022; Lewis et al., 2021). The CRAFT approach relies on human-curated few-shot examples as query documents and can dynamically retrieve any document of the base corpora. As the few-shot samples include a text containing the domain, the instruction or question, as well as the output, the resulting embedding representation of the sample contains contextualized (Reimers and Gurevych, 2019) semantic information about both the domain and the nature of task to be performed. Relevant text documents that contain similar latent features as the few-shots are retrieved from the corpora by calculating similarity scores based on the embedded few-shots and corpus samples.\nAs corpus size increases, the risk of retrieving redundant or similar corpus samples also increases. This is partly due to the growing volume of documents, but also because the diversity of documents within the corpora may plateau, resulting in a higher proportion of similar documents. Designing few-shots that are sufficiently diverse in topic may alleviate this issue. For example, when creating few-shots for biology question-answering, various subtopics of biology, such as genetics, anatomy, or physiology, should be covered to broaden the range of retrieved documents."}, {"title": "Task Sample Synthesis", "content": "The retrieved documents naturally contain noise (Agarwal et al., 2007) and lack the formatting required for fine-tuning. Therefore, it is necessary to convert these free-text documents into appropriate task samples by removing noise and undesired sections.\nTo address this, we utilize instruction-tuning prompt templates (Sanh et al., 2022; Maini et al., 2024) to augment the free-text documents into task-specific training data while simultaneously eliminating noise. A few-shot task template consists of three elements: (i) one or more few-shots, (ii) a corpus sample, (iii) and a brief instruction for the model to generate instruction-output pairs from the content of the corpus sample. Aside from the brief instruction, it is easy to assemble these templates from material we already have. The template only structures all information from the instruction, the few-shots, and the retrieved corpus samples to generate one continuous string that serves as input for the model generating the synthetic task samples. In this setup, the contents of the few-shots serve as in-context examples for the completion of the instruction. Figure 2, step 3, shows an example of how these templates guide the model in augmenting the corpus samples into synthetic task samples. Any instruction-tuned language model can be used for this purpose. This augmentation step not only rephrases the text but also condenses the retrieved document down to the essential information required for the task. The result of this step produces the final synthetic instruction-output pairs that can be used to fine-tune a language model. Figure 2, step 4, shows an actual example output from the generated pool of synthetic training samples, and Appendix C provides an overview of length statistics from the stages of corpus retrieval up to the synthesized input-output pairs."}, {"title": "Experimental Setup", "content": ""}, {"title": "Tasks", "content": "We generate datasets for all tasks in sizes 100, 500, 5000, and 25,000. We refer to the few-shots as a dataset of size XS, and to the sizes ranging from 100 to 25,000 as S, M, L, and XL, respectively. Implementation details related to fine-tuning can be found in Appendix A.6.\nMultiple-Choice QA: We generate three synthetic QA datasets for biology (BioQA), medicine (MedQA), and commonsense (CSQA). The datasets all follow the MMLU multiple-choice format (Hendrycks et al., 2021), where each question is accompanied by a number of answer options. Exactly one of these options is correct, and the task is to identify the correct answer. The expected output is the corresponding letter label of the correct answer.\nGenerative: In addition, we develop two synthetic datasets for the generative tasks of summarization and recipe generation (RecipeGen). The goal of summarization is to convey accurate and concise information; recipe generation is additionally focused on creating coherent and structured text that adheres to specific formatting and stylistic conventions (Wang et al., 2022). To build a synthetic summarization dataset, we first select a corpus sample and instruct the model to extract an extensive but unsummarized section of text. In the second step, the extracted section is transformed into a summary format, optionally incorporating elements from the raw text, such as abstracts, conclusions, or TLDRs. This approach avoids using the original corpus samples as to-be-summarized text documents, which can be lengthy and overly broad and may result in uninformative summaries."}, {"title": "Evaluation", "content": ""}, {"title": "Metrics", "content": "QA Tasks: All multiple-choice QA tasks are evaluated using accuracy. We follow the evaluation approach of MMLU and assess the logarithmic probabilities for the vocabulary options corresponding to the letter labels of the answer choices (Hendrycks et al., 2021). Accordingly, greedy decoding without temperature scaling is performed. Depending on the number of answer choices, this evaluation can range from options A and B to options A through E.\nGenerative Tasks: Automated metrics like ROUGE (Lin, 2004) and METEOR (Banerjee and Lavie, 2005) are resource-efficient as evaluation metrics but face limitations in generative tasks. They rely heavily on n-gram overlap, which may not accurately reflect the true quality of the generated text (Barbella et al., 2021). They also assume that the reference text provides a complete and accurate representation of the desired output, which is not always guaranteed (Graham, 2015; Sai et al., 2019). Reference texts can be low-quality, contain errors and ambiguities, which leads to unreliable evaluation results. This issue is illustrated by the findings of Sottana et al. (2023), who demonstrated that human reviewers frequently rate gold-standard benchmark answers among the worst answer options. This issue worsens when generated texts differ significantly in length from references, as subsequence-based metrics struggle to capture such variations (Celikyilmaz et al., 2020).\nAs an alternative, we opt to evaluate generations using LLMs as a judge (Eldan and Li, 2023). In this setup, the LLM effectively acts as a human annotator, providing a binary preference score for each pair of outputs, resulting in a win rate as the final metric (Chiang et al., 2024). This approach has become increasingly common: LLMs have been successfully employed as annotators while demonstrating high inter-rater reliability (Hackl et al., 2023; Sottana et al., 2023; Liu et al., 2023).\nFor general-purpose outputs, we use the popular Alpaca-Eval benchmark (Li et al., 2023) that evaluates multiple LLMs on about 650 human-curated questions (Dubois et al., 2023). We select Llama 3 70B (Dubey et al., 2024) as our annotator model due to its open nature and cost-efficiency for high-volume experiments. As of July 2024, Llama 3 70B ranks 4th in human agreement with a score of 67.5, close to customized GPT-4 versions at 69.2."}, {"title": "Datasets", "content": "To assess the quality of our synthetically generated datasets, we compare the performance of models trained on them to those trained on human-curated datasets.\nBioQA: We use the 800 sample test split from the biology subsection of ScienceQA (Lu et al., 2022). ScienceQA sourced expert-curated question-answer pairs from online learning platforms, ensuring a high level of quality and accuracy. The dataset's answer options range from two to five, have a single correct answer per question, and are randomized to prevent pattern recognition.\nMedQA: We use the 4183 samples from the MedMCQA validation split (Pal et al., 2022). The dataset is comprised of entrance exam questions to two of India's postgraduate institutions. All dataset samples are sourced from preparation or real exams created by medical professionals. All questions have 4 answer options with one correct answer option per question.\nCSQA: We benchmark on the 2541 validation set samples in CommonsenseQA 2.0 (Talmor et al., 2021). The dataset was generated through a gamified but controlled question generation process, where players were incentivized to design challenging yes/no questions by earning points when beating an AI model. The generated questions were then cross-validated by other players, independent validators, and another model, to ensure that the questions were well-formed, answerable, and representative of common sense.\nRecipeGen: We randomly sample 1000 samples from the higher-quality subsection of the RecipeNLG dataset (Bie\u0144 et al., 2020). The recipes were scraped from cooking websites and post-processed using a fine-grained and standardized cleaning and formatting process to ensure correctness. Each recipe features a title, the list of required ingredients, as well as the steps to produce the meal. We only include samples not present in C4, based on the URL in each dataset.\nSummarization: We use 1000 samples from the test split of the CNN-DailyMail dataset (See et al., 2017). The dataset is commonly used for summarization because it consists of articles and stories from CNN and DailyMail alongside their highlights in abstract or bullet point format presented at the top of newspaper pages or websites."}, {"title": "Results", "content": ""}, {"title": "Baselines", "content": "We compare the performance of CRAFT, trained on our synthetic datasets, against three baselines. The few-shot baseline is a model fine-tuned only on the XS size CRAFT dataset, with human-curated few-shots. It serves as the primary baseline since this model uses all human-curated data available in our pipeline.\nThe second baseline is the instruction-tuned model, Mistral 7B Instruct v0.2 (Jiang et al., 2023), which has been fine-tuned on proprietary instruction-following datasets that mix various tasks and sources. This baseline provides a meaningful comparison, as it is similar in size and instruction-tuned like CRAFT models, though it is trained on undisclosed datasets of unknown quality and quantity. Thus, matching or exceeding the performance of instruction-tuned models with our synthetic data would indicate that CRAFT can produce high-quality datasets.\nThe upper bound of expected performance is fine-tuning the models on the in-domain human-curated training splits from the chosen evaluation datasets. This baseline represents the optimal performance achievable with human-quality datasets."}, {"title": "Scaling the Data", "content": "We report the performance gain when scaling up our training data in Figure 3. We report the mean and standard deviation across three seeds. We observe consistent improvements across four tasks as we increase the data size. Relative to the baseline models trained with only few-shot examples, we see improvements of 17% (from 66.9 to 78.1), 12% (from 55.3 to 62.1), 23% (from 39.1 to 48.0), and 124% (from 43.7 to 97.9) for BioQA, CSQA, MedQA, and Summarization, respectively. This shows that CRAFT can be used for diverse tasks, starting with just a few curated examples. We also find appropriate scaling for each set of examples, ranging from 100 to 25,000 across all tasks. Additionally, that models trained with fewer examples (32, 100) exhibit much more variance than those trained with 5,000 and 25,000 examples, as indicated by the gray regions in the plots that visualize the standard deviation.\nFor all tasks, we achieve results that are are clearly better of comparable to Mistral Instruct. It is worth noting that CRAFT uses an LLM in a limited way (to restructure and rewrite existing corpora) that seem to exclude the possibility that distillation may have played a role here. However, even if distillation were to be considered the reason for good CRAFT performance, the results indicate otherwise: we use the same model (Mistral Instruct) to paraphrase existing corpora examples but achieve even stronger results. Finally, we observe that CRAFT models outperform those trained with official human-curated large-scale data in Summarization. For other tasks, while we observe lower performance than with official data, we speculate that this could be due to in-domain evaluation for official human-curated data. We use their test split to evaluate our models, which may give these models an unfair advantage. We investigate this further in the next section."}, {"title": "OOD Generalization and Data Contamination", "content": "We now report experiments to understand the level of data contamination or similarity between test and training examples in the experiments introduced in Figure 3. We conduct 5-gram weighted Jaccard similarity analyses between CRAFT datasets and the test data. For each sample, we combine the instruction and output and gather 5-gram frequencies for the whole dataset. We then calculate the Jaccard similarity between the 5-gram frequency distributions of the respective CRAFT and test dataset, where 5-grams receive weight proportional to their frequency.\nThis analysis shows that all CRAFT datasets have less than 0.4% similarity with the task test set, whereas the original human-authored datasets show much higher similarities: BioQA (17.9%); CSQA (4.4%), MedQA (1.1%), and Summarization (0.3%); this may indicate some overlap between train and test splits. Since we have curated the few-shots manually, rather than copying from existing datasets, our low overlap (0.4%) may be expected. To further investigate CRAFT'S performance improvement, we select four out-of-domain datasets for the biology question answering task and compare CRAFT with the human-curated baseline.\nIn Table 1, we compare the human-curated baseline with CRAFT on the in-domain dataset for the baseline and four out-of-domain (OOD) datasets selected from MMLU (Hendrycks et al., 2021). Although the baseline outperforms CRAFT by more than 11% in the biology subset of the ScienceQA test set (in-domain for the baseline), CRAFT outperforms the baseline on 3 out of 4 OOD datasets. On average, CRAFT outperforms the baseline by 4%."}, {"title": "Negative Results: Recipe Generation", "content": "Out of the five tasks we selected, we observe non-scaling behavior in one task: Recipe Generation. While our manually curated few-shots are of high quality, we see a drop when scaling from 32 to 25,000 examples, as illustrated in Figure 4. CRAFT's performance is still better than the baseline with official human data, which means that the final dataset is usable. However, we explore why this reverse scaling occurs and examine the drop in performance.\nAn initial analysis suggested that the CRAFT pipeline tends to find less relevant examples over time. We conducted automated data quality analysis to analyze this on a larger scale. For 500 randomly sampled instructions from different sizes of CRAFT datasets (i.e., the training sets), we used the Llama 3 8B Instruct model to answer the instructions. Then, using Llama 3 70B Instruct as a judge, we compared win rates, i.e., which output the model preferred: the gold output in the CRAFT dataset or the output generated by Llama 3 8B Instruct. We report the average win rate against the Llama outputs as the data quality metric. Higher scores indicate that the pipeline created higher quality output than Llama 3 8B Instruct's answers.\nWe observe that data quality drops when scaling up to 25,000 examples. The 100 and 500 example sets have win rates around 0.4, while the win rate for 25K drops to 0.3. We believe this is the cause of the performance drop with scaling. While the final dataset is still useful (it outperforms the baseline with official human data), the next version of CRAFT should include either effective stopping criteria or additional validators for quality."}, {"title": "Base Model Comparison", "content": "In previous sections, we fine-tuned CRAFT models using the pretrained Mistral 7B model. Now, we repeat the experiments using the pretrained Llama 3 8B model. We observe similar trends across all tasks, and the relative improvement is comparable when scaling up from few-shots to 25,000 examples, as illustrated in Table 2.\nIn all experiments, we manually curated 32-shot examples and expanded our synthetic data examples from that point. However, even curating 32 examples can be time-consuming. We can limit the number of few-shots to just eight examples to bootstrap the CRAFT process. Figure 3 shows the results compared to the CRAFT pipeline with 32-shots. While the final results with 25,000 examples are slightly lower for 8-shots, the trend is similar for both 8- and 32-shot examples. We observe that the model trained with 25,000 samples, based on running CRAFT with 8 few-shots, significantly outperforms the model trained with only 32 few-shots (i.e., no extra synthetic data). This suggests that if there are time and resource limitations, using CRAFT with fewer initial examples leads to better models than trying to curate and train only on more human-curated few-shot examples."}, {"title": "Conclusion", "content": "In this work, we introduced CRAFT (Corpus Retrieval and Augmentation for Fine-Tuning), a framework for generating task-specific synthetic datasets grounded in text corpora. CRAFT requires only a small set of human-curated few-shot examples to bootstrap the creation of large-scale training data by leveraging existing corpora and instruction-tuned language models. Our experiments across multiple tasks, including biology, medicine, and commonsense question-answering, as well as summarization, demonstrate that models fine-tuned on CRAFT-generated datasets can match or outperform strong baselines, including instruction-tuned models and those trained on human-curated datasets. Notably, CRAFT-based models showed better generalization capabilities on out-of-domain datasets compared to models trained on human-curated data, highlighting the robustness of our approach.\nWhile CRAFT shows promising results for most tasks, we also identified limitations in scaling performance for recipe generation, emphasizing the need for careful quality control and potential stopping criteria in future iterations. Nevertheless, the overall success of CRAFT in producing high-quality synthetic datasets with minimal human effort opens up new possibilities for efficient and adaptable model fine-tuning across a wide range of domains and tasks."}, {"title": "Implementation Details", "content": ""}, {"title": "Few-Shot Design", "content": "The BioQA few-shot text samples were compiled from a diverse range of sources, including textbooks (Alberts, 2017; Malmquist and Prescott, 2022; Wilkin and Brainard, 2016; Rye et al., 2016), openly accessible materials, and Encyclopedia Britannica. For MedQA, we primarily drew upon openly accessible websites, such as the National Institutes of Health, National Health Service, Food and Drug Administration, Mayo and Cleveland Clinics, and other medicine-related websites, to generate our few-shot text samples. The CSQA few-shot text samples were sourced from a variety of online resources, including blogs, articles, and other websites tailored to the specific topic at hand. From sources that are not openly accessible through websites, continuous text snippets were directly extracted and used as texts, while all other material was shortened, rephrased, or restructured by the authors. This process ensures that articles which may have been crawled through C4 (Raffel et al."}]}