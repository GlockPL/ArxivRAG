{"title": "AI-Driven Review Systems:\nEvaluating LLMs in Scalable and Bias-Aware Academic Reviews", "authors": ["Keith Tyser", "Ben Segev", "Gaston Longhitano", "Xin-Yu Zhang", "Zachary Meeks", "Jason Lee", "Uday Garg", "Nicholas Belsten", "Avi Shporer", "Madeleine Udell", "Dov Te\u02bceni", "Iddo Drori"], "abstract": "Automatic reviewing helps handle a large volume of papers,\nprovides early feedback and quality control, reduces bias, and\nallows the analysis of trends. Paper reviews are used by re-\nsearchers and academics, students, lecturers, innovators and\nentrepreneurs, policymakers and funding agencies, science\njournalists, and the general public to navigate research, ana-\nlyze trends, find educational purposes, and find collaborators.\nWe evaluate the alignment of automatic paper reviews with\nhuman reviews using an arena of human preferences by pair-\nwise comparisons. Gathering human preference may be time-\nconsuming; therefore, we also use an LLM to automatically\nevaluate reviews to increase sample efficiency while reducing\nbias. In addition to evaluating human and LLM preferences\namong LLM reviews, we fine-tune an LLM to predict human\npreferences, predicting which reviews humans will prefer in a\nhead-to-head battle between LLMs. We artificially introduce\nerrors into papers and analyze the LLM's responses to iden-\ntify limitations, use adaptive review questions, meta prompt-\ning, role-playing, integrate visual and textual analysis, use\nvenue-specific reviewing materials, and predict human pref-\nerences, improving upon the limitations of the traditional re-\nview processes. We make the reviews of publicly available\narXiv and open-access Nature journal papers available on-\nline, along with a free service which helps authors review and\nrevise their research papers and improve their quality. This\nwork develops proof-of-concept LLM reviewing systems that\nquickly deliver consistent, high-quality reviews and evaluate\ntheir quality. We mitigate the risks of misuse, inflated review\nscores, overconfident ratings, and skewed score distributions\nby augmenting the LLM with multiple documents, including\nthe review form, reviewer guide, code of ethics and conduct,\narea chair guidelines, and previous year statistics, by finding\nwhich errors and shortcomings of the paper may be detected\nby automated reviews, and evaluating pairwise reviewer pref-\nerences. This work identifies and addresses the limitations of\nusing LLMs as reviewers and evaluators and enhances the\nquality of the reviewing process.", "sections": [{"title": "1 Introduction", "content": "The academic community acknowledges the acute need for\nhaving foundation models assist reviewing of papers at scale\n(Liu and Shah 2023; Robertson 2023; Petrescu and Krishen\n2022; Schulz et al. 2022; Checco et al. 2021; Bao, Hong,\nand Li 2021; Vesper 2018; Latona et al. 2024; Kuznetsov\net al. 2024), along with the risks involved (Kaddour et al.\n2023; Spitale, Biller-Andorno, and Germani 2023; Zou et al.\n2023). Previous work addresses the limitations of LLM's\nability to perform reviewing (Liu and Shah 2023) and their\ncapabilities to review academic papers (Liang et al. 2023).\nLarge language models demonstrate surprising creative ca-\npabilities in text (Koivisto and Grassini 2023), though they\nmay hallucinate (Zhang et al. 2023), and demonstrate the\npower to persuade humans even when inaccurate (Spitale,\nBiller-Andorno, and Germani 2023). This makes controlling\nthe quality and appropriateness of LLM-augmented review-\ning highly challenging. At least 15.8% of reviews for ICLR\n2024 were written with AI assistance (Latona et al. 2024).\nRecently, an attempt has been made to automate the en-\ntire scientific endeavor including generating research ideas,\nwriting code, running experiments, visualizing results, writ-\ning scientific papers, and reviewing (Lu et al. 2024).\nMeta-prompting (Suzgun and Kalai 2024) uses multiple\nLLM instances for managing and integrating multiple inde-\npendent LLM queries. Utilizing meta-prompting, the LLM\nbreaks down complex tasks into smaller subtasks handled\nby expert instances with tailored instructions, significantly\nenhancing performance across various tasks. This approach\noutperforms conventional prompting methods across mul-\ntiple tasks, enhancing LLM functionality without requiring\ntask-specific instructions. Multi-agent review generation for\nscientific papers (D'Arcy et al. 2024) improves LLM re-\nviewing by using multiple instances of LLMs, providing\nmore specific and helpful feedback by distributing the text\nacross specialized agents that simulate an internal discus-\nsion. This reduces generic feedback and increases the gener-\nation of good comments. Recent work formulates the peer-\nreview process as a multi-turn dialogue between the different\nroles of authors, reviewers, and decision-makers (Tan et al.\n2024), and finds that both reviews (Latona et al. 2024) and\nmeta-reviews written by LLMs (Santu et al. 2024) are pre-\nferred by humans over human reviews and meta-reviews.\nWhy do the Artificial Intelligence, Machine Learning, and\nComputer Vision communities need AI-based reviews of pa-\npers? (i) AI-based reviews provide early feedback to authors\nfor their work in progress, allowing authors to learn and im-\nprove their work; (ii) AI-based reviews would help confer-\nences maintain high-quality and timely reviews for the in-\ncreasing number of papers in these fields, as shown in Figure\n9 in Appendix A. (iii) For quality control of reviews gener-"}, {"title": "2 Review Systems", "content": "We present OpenReviewer 1, Papers with Reviews 2, and Re-\nviewer Arena 3.\nOpenReviewer\nOpenReviewer is a platform designed to augment the tradi-\ntional peer review process by using LLMs to review papers.\nResearchers and authors may upload their work to instantly\nreceive peer-review feedback at any point in the writing pro-\ncess. It aims to address the inherent delays and variability in\nquality of human reviews by providing instant, consistent,\nhigh-quality early reviews of academic papers.\nWe identify requirements for system acceptance by au-\nthors and the community, such as journal editors and confer-\nence chairs. One requirement is a human-level quality of the\nreview; the other is adherence to community norms of re-\nviewing, e.g., ethical and unbiased reviewing. Human-level\nreviews are required to make the system trustworthy and in-\nformative to the author wishing to revise the paper accord-\ning to the review. To achieve human-level quality, we design\nOpenReviewer to consider text and figures in the papers and\nto adapt the review questions to the journal type and the pa-\nper content. To ensure adherence to community norms, we\nspecify appropriate code of ethics and correct for bias in re-\nviewing.\n\"A picture is worth a thousand words\" expresses suc-\ncinctly the idea that images convey information more effi-\nciently and persuasively than text. However, when reading\nscientific papers that include both text and images, it is es-\nsential to consider not only pictures or words alone, but the\ncombination of both, which is more powerful than either one\nalone (Schnotz and Bannert 2003). The theory of dual cod-\ning (Paivio 2010) explains how humans process images and\ntext so that the combined effect results in better and more\nefficient comprehension. For instance, image processing is\nusually fast and holistic, while text (verbal) processing is\nslow and sequential. For some tasks, but not all, images may\nbe more efficient than text or tables, e.g., detecting a trend.\nAs images and text do not utilize the same processors, read-\ners can process them simultaneously to understand one in the\ncontext of the other. Moreover, users often prefer text with\npictures over text alone and may regard pictures as more\npersuasive (Powell et al. 2015; Tal and Wansink 2016). For\nthese reasons, images are often added to text even though\nthey may not add information already stated in the text.\nPrevious work on automated reviewing does not consider\nvisual items in papers or map out the errors and shortcom-\nings of papers that LLM reviews can reliably detect. Fig-\nures are a significant source of information, and a human\nreviewer would responsibly review a paper while analyzing\nits figures. In this work, we consider the paper text and fig-\nures within the context of the entire paper while performing"}, {"title": "3 Review Evaluation Methods", "content": "Anonymous Human and LLM Evaluation\nTo evaluate the quality of the LLM-generated reviews, five\nexpert evaluators were provided with 150 papers together\nwith two anonymous reviews for each paper. Each paper was\nrandomly assigned two reviewers from the list of five poten-\ntial reviewers: Human, GPT-4 (Turbo-2024-04-09), Claude\n3 Opus, Gemini Pro (Bard), and Command R+. The human\nreviews were obtained from OpenReview submissions. The\nevaluators were asked which of the two reviews for each\npaper they preferred. Therefore, this methodology evaluates\nthe relative quality of each reviewer as determined by human\nevaluators through a series of one-on-one comparisons.\nThis work quantifies and ranks reviewers based on ob-\nserved match outcomes using a win matrix, Bradley-Terry\n(BT) model coefficients, and logistic regression. The win\nmatrix represents the outcomes of matches between com-\npetitors. For N competitors, the matrix W is an N \u00d7 N\nmatrix where each element wij represents the probability\nof competitor i winning against competitor j, defined as\n$\\frac{\\text{# wins of i against j}}{\\text{total matches between i and j}}$. This matrix is constructed by\niterating over a list of match results, updating both the win\ncount and the total match count for each pair of competi-\ntors. The resultant win matrix for our experiment is show in\nFigure 4.\nThe Bradley-Terry model provides a parametric approach\nto estimating the relative strengths of competitors based on\npairwise comparisons. The probability P that competitor\nm beats competitor m' follows a logistic function, where\n$P_H = \\frac{1}{1+e^{\u03be_{m'}-\u03be_m}}$ where $\u03be$ represents the vector of BT"}, {"title": "4 Review Generation Methods", "content": "Review Questions\nWe explored four types of review questions: (i) Fixed ques-\ntions for a conference or journal: for example, ICLR and\nNeurIPS papers (Appendix B) have fixed review forms with\nquestions; (ii) Fixed questions for a type of paper: for ex-\nample, sets of questions for survey, empirical, opinion, etc,\npapers; (iii) Adaptive choice from a bank of questions based\non the paper content: Given the paper and 40 review ques-\ntions, the LLM selects the top 10 review questions for the\npaper; and (iv) Adaptive generation of the questions based\non the paper content: Given the paper, the LLM generates\nthe top 10 review questions. We applied the fixed questions\nfor maintaining an equal footing and scale. The Appendix K\nprovide details about the review questions."}, {"title": "5 User Feedback and Limitations", "content": "User feedback is used to assess the quality and trustworthi-\nness of the automated feedback and to continually improve\nthe system design. We collect feedback from users in Papers\nwith Reviews. The feedback is on the automated review gen-\nerated by OpenReviewer for specific papers. The feedback\nconsists of five quantitative questions that evaluate paper re-\nviews (Goldberg et al. 2023) and an open-ended question as\ndescribed with summary statistics in Appendix C.\nWe use multiple documents related to the review as LLM\ncontext: the previous year's statistics, reviewer and area\nchair guidelines, code of ethics and code of conduct, and\nthe formal review form. These venue-dependent documents\nresult in our review score distributions being similar to hu-\nman distributions and yielding quality reviews using the full\nrange of scores; however, they require yearly updates. A\nproblem with applying the prediction of human preferences\nto reviews is that different people may prefer different re-\nviews. A Kaggle competition over a dataset of human prefer-\nences provides a common ground for prediction. Correcting\nfor human bias helps partially mitigate this gap, as personal\npreferences are driven by human behavioral bias.\nFuture research will extend our evaluation to examine\nhow authors use and trust LLM reviews. Our analysis of the\ncapabilities of LLMs by classifying and testing various re-\nviewing criteria and types of errors and shortcomings indi-\ncates the limits of our current application. These limitations\nare essential for knowing how to use the application, partic-"}, {"title": "6", "content": "Conclusions\nOur aim is to improve scientific writing, research, and com-\nmunication by providing fast and reliable in-depth reviews\non demand. This work evaluates the limitations and capabil-\nities of GPT-4 to review papers and suggest revisions. LLM-\ngenerated reviews align well with human reviewers when\nevaluated by blind human evaluation and an automatic GPT-\n4 comparison. We present our LLM reviewer system, Open-\nReviewer, and the associated Papers with the Reviews site.\nTo our knowledge, we are the first to report on such a large-\nscale empirical evaluation of LLM reviewing.\nUsing human reviews as a baseline, we evaluated value\nalignment and the process alignment of LLM reviews, i.e.,\nwe compared the quality of reviews and the adherence of\nthe reviewing process to conference guidelines and scien-\ntific norms of practice. Prior work on LLM academic capa-\nbilities suggests that LLMs are now ready for specific re-\nviewing tasks and appear to be more effective for some aca-\ndemic domains and less effective for others (Checco et al.\n2021; Schulz et al. 2022; Liu and Shah 2023; Lu et al. 2024).\nTherefore, we conducted ablation studies and determined\nthe types of errors and shortcomings the LLM can detect\nand review. When supplied with information about previous\neditorial decisions, the LLM aligns well with human review-\ners. Furthermore, the LLM performs well in detecting spe-\ncific errors and shortcomings, such as overclaiming, but not\nothers, such as detecting cases in which the authors needed\nto follow expected norms. We find that iterative design and\nlarge-scale empirical evaluation are essential to calibrate the\napplication of LLMs.\nThis work leverages LLMs in the review process, ad-\ndressing challenges and offering proof-of-concept LLM re-\nview tools. We introduce and evaluate systems designed\nto streamline handling tens of thousands of academic pa-\npers, from initial collection to reviewing and evaluation. Our\nmethods offer novel approaches to automating academic re-\nviews, improving upon traditional reviews. Our analysis re-\nveals that the system facilitates a more efficient review pro-\ncess and enhances the accessibility and quality of academic\nliterature for both authors and the broader scholarly commu-\nnity. Using papers from arXiv and open-access Nature, cou-\npled with our methods, shows promise in identifying high-\nquality papers and emerging research trends. In conclusion,\nour systems and methods represent the different levels of\nautonomy in the academic review process, detailed in Ap-\npendix Q, and a step forward in automation improvement.\nWith continued development and community involvement,\nit holds the potential to transform how academic literature is\ncollected, reviewed, disseminated, and evaluated, making it\naccessible and valuable to researchers worldwide. We hope\nthat our work paves the way for more efficient, consistent,\nhigh-quality reviews, accelerating scientific progress while\nmaintaining responsible conduct of research."}, {"title": "I", "content": "Automatic Comparisons\nWe compare consistency of summaries raised in reviews. The average overlap between human reviewers is 3.05, with a standard\ndeviation of 1.56, which indicates a slight consensus among human reviewers about specific aspects of the papers they reviewed.\nThe average overlap between human reviewers and the LLM is 3.67, with a standard deviation of 1.58, which is higher than\nthe human-human average, suggesting that the LLM often aligns better with multiple points raised by human reviewers. The\noverlaps between human reviewers and LLM are diverse, with some papers having up to 6 points of overlap with the LLM.\nThis suggests that the LLM often aligns with the feedback or points raised by human reviewers.\nGiven two sets of review points A and B with similarity scores s(Ai) and s(Bj) for elements Ai \u2208 A and Bj \u2208 B the\nweighted Jaccard similarity is defined as $J_w (A, B) = \\frac{\\sum_{i, j}  min(s(A_i), s(B_j))}{\\sum_{i,j} max(s(A_i), s(B_j))}$, where Ai and Bj are overlapping elements in A and\nB. The weighted Jaccard similarity heatmap shown in Figure 27 considers common points' presence and similarity scores. The\ndarker shades highlight pairs with higher similarity. The average weighted Jaccard similarity across all pairs is 0.214. Overall,\nthe Human-LLM Jaccard similarities are higher than the Human-Human values. The overlaps between human reviewers and\nLLMs suggest that LLMs can assist or augment the peer-review process, capturing critical points that human reviewers also\nidentify. The most common overlaps between review summaries are experimental validation, clarity in methodology, and po-\ntential real-world applications. This suggests areas where reviewers often converge in their feedback and may provide insights\nfor improvements for authors."}, {"title": "J Editorial Review Process", "content": "The LLM is set to play different roles: program chair (PC), senior area chair (SAC), area chair (AC), and reviewers (R). The\nhuman editorial process is simulated given corresponding prompts described in Table 9 in the Appendix, reducing the editorial\nprocess time from human months to machine minutes."}, {"title": "Q", "content": "Levels of Autonomy in Reviewing\nWe currently do not want to fully replace human reviews and their evaluation by AI. Problems with solely using LLMs include\nevaluation bias, the risk of LLMs favoring results from similar LLMs, potential bias against specific user groups, misinfor-\nmation, and hallucinations. Our goal is to avoid such biases and ensure factual accuracy. We propose combining humans and\nLLMs for evaluation by understanding the broad spectrum between human evaluation and full automation by LLMs. Currently,\nhumans are the sole reviewers without any AI interference. It is common practice for humans to maintain complete control\nwhile being supported by AI which summarizes and highlights paper and review texts. LLMs may help humans make decisions\nby generating summaries for human evaluation. Beyond summaries, humans and LLMs may collaborate by having each make\ndecisions they are good at, such as by role-playing and dialogue. Moving closer to automation is achieved by humans in the\nloop, having humans prefer between automated evaluations or decisions or having a human verify and accept or reject an au-\ntomated decision. Role-playing and dialogue may consider LLMs as crowd workers to be supervised by humans. Finally, fully\nautomated reviewing may be beneficial in automating the entire scientific process, but it is unsuitable for the current academic\nreview process."}]}