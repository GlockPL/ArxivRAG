{"title": "2024 BRAVO Challenge Track 1 \u2014 1st Place Report: Evaluating Robustness of Vision Foundation Models for Semantic Segmentation", "authors": ["Tommie Kerssies", "Daan de Geus", "Gijs Dubbelman"], "abstract": "In this report, we present our solution for Track 1 of the 2024 BRAVO Challenge, where a model is trained on Cityscapes and its robustness is evaluated on several out-of-distribution datasets. Our solution leverages the powerful representations learned by vision foundation models, by attaching a simple segmentation decoder to DINOv2 and fine-tuning the entire model. This approach outperforms more complex existing approaches, and achieves 1st place in the challenge. Our code is publicly available\u00b9.", "sections": [{"title": "1. Introduction", "content": "The 2024 BRAVO Challenge \u201caims to benchmark semantic segmentation models on urban scenes undergoing diverse forms of natural degradation and realistic-looking synthetic corruption\" [17]. In this report, we present our solution for Track 1 of this challenge, where a model is trained on a single labeled semantic segmentation dataset, Cityscapes [4], and evaluated on a range of other datasets with out-of-distribution image conditions and semantic content, to evaluate the robustness of the model.\nWhen trained on a single dataset like Cityscapes, vanilla semantic segmentation models typically only perform well on data that is similar to this training dataset. Under a distribution shift \u2013 e.g., changing weather conditions, different geography or image distortion \u2013 the performance drops. To address this problem, we do not propose a new algorithmic approach or model design, but instead aim to leverage the power of Vision Foundation Models (VFMs). VFMs are pre-trained on broad datasets, allowing them to learn powerful representations for a large variety of downstream tasks [1, 8, 12, 14, 18, 21]. As these VFMs learn representations from such diverse data, captured under a wide array of conditions, we hypothesize that they are perfectly suited for robust semantic segmentation with simple, vanilla fine-tuning."}, {"title": "2. Method", "content": "The concept of our approach is to fine-tune pre-trained Vision Foundation Models (VFMs) for semantic segmentation, leveraging the robust representations these models provide. The meta-approach is visualized in Figure 1. Given a pre-trained VFM, we attach an off-the-shelf segmentation decoder, and fine-tune the entire model for semantic segmentation. We evaluate this meta-architecture in several different configurations.\nOur primary solution, which achieves 1st place in the challenge, uses the DINOv2 VFM [12]. We selected DINOv2 due to its demonstrated effectiveness in domain-generalized semantic segmentation for urban scenes [7, 10]."}, {"title": "2.1. Training", "content": "When training the model with a linear decoder, we bilinearly upsample the segmentation logits $L \\in \\mathbb{R}^{C \\times \\frac{H}{P} \\times \\frac{W}{P}}$ to $L' \\in \\mathbb{R}^{C \\times H \\times W}$, and then apply a categorical cross-entropy loss to these logits and the semantic segmentation ground truth to fine-tune the model.\nWhen using Mask2Former, the decoder outputs a set of mask logits $M \\in \\mathbb{R}^{N \\times \\frac{H}{P} \\times \\frac{W}{P}}$ and corresponding class logits $C \\in \\mathbb{R}^{N \\times (C+1)}$, where $N$ is the number of masks and $C$ includes an additional \"no-object\" class. During training, following Mask2Former, these mask and class logits are matched to the ground truth using bipartite matching. The predicted masks are then supervised with a cross-entropy loss and a Dice loss, and the predicted classes are supervised with a categorical cross-entropy loss."}, {"title": "2.2. Testing", "content": "During inference with the linear decoder, we compute per-pixel class confidence scores by applying a softmax function to the upsampled class logits $L'$. For each pixel, the predicted class is the one with the highest confidence score, and we also output this confidence score.\nDuring inference with the Mask2Former decoder [3], we first bilinearly upsample the mask logits $M \\in \\mathbb{R}^{N \\times \\frac{H}{P} \\times \\frac{W}{P}}$ to the original resolution, resulting in $M' \\in \\mathbb{R}^{N \\times H \\times W}$. We then apply a sigmoid function to $M'$ to obtain the mask scores: $P_M = \\text{sigmoid}(M')$.\nThe class logits $C \\in \\mathbb{R}^{N \\times (C+1)}$ are converted to class scores using a softmax function, excluding the \"no object\" class: $P_C = \\text{softmax}(C)[..., :-1]$.\nThe overall per-pixel class confidence scores $P' \\in \\mathbb{R}^{C \\times H \\times W}$ are computed by multiplying the mask scores with the class scores across all masks. Specifically, for each class $c$ and pixel $(h, w)$, we have:\n$P'_{c,h,w} = \\sum_{n=1}^{N} P_{Mn,h,w} P_{Cn,c}$ (1)\nFor each pixel, the predicted class is the one with the highest value in $P'$, and we also output this maximum value as the confidence score."}, {"title": "3. Experimental Setup", "content": ""}, {"title": "3.1. Datasets", "content": "Track 1 of the 2024 BRAVO Challenge focuses on single-domain training, assessing the robustness of models trained with limited supervision and geographical diversity against real-world distribution shifts. Only the Cityscapes dataset [4] is permitted for training. This dataset features homogeneous urban scenes with fine-grained annotations for 19 semantic classes.\nThe BRAVO benchmark dataset, used for evaluation, consists of six subsets:\n\u2022 ACDC [15]: Real scenes captured in adverse weather conditions, such as fog, rain, night, and snow.\n\u2022 SMIYC [2]: Real scenes featuring out-of-distribution (OOD) objects that are rarely encountered on the road.\n\u2022 Out-of-context [9]: Augmented scenes with random backgrounds, generated by replacing the backgrounds of 329 validation images from Cityscapes with random ones.\n\u2022 Synflare [20]: Augmented scenes with synthesized light flares, produced by adding random light flares to 308 validation images from Cityscapes.\n\u2022 Synobjs [11]: Augmented scenes with inpainted synthetic OOD objects, created by adding 26 different OOD objects to 656 validation images from Cityscapes.\n\u2022 Synrain [13]: Augmented scenes with synthesized raindrops on the camera lens, generated by augmenting 500 validation images from Cityscapes."}, {"title": "3.2. Evaluation metrics", "content": "The 2024 BRAVO Challenge evaluates methods based on a variety of metrics to assess their performance in semantic segmentation and out-of-distribution (OOD) detection. The metrics are grouped into three categories: semantic metrics, OOD metrics, and summary metrics. The semantic metrics evaluate the overall quality of the semantic segmentation predictions, while the OOD metrics assess the model's ability to detect OOD objects. The summary metrics combine the semantic and OOD metrics to provide an overall ranking of the methods.\nSemantic metrics. The semantic metrics are computed on all subsets, except SMIYC, for valid pixels only. Valid pixels are those not \u201cinvalidated\u201d by extreme uncertainty, such as pixels obscured by the brightest areas of a flare or covered by an OOD object. The semantic metrics include:\n\u2022 Mean Intersection over Union (mIoU): Measures the proportion of correctly labeled pixels among all pixels. It is the only semantic metric that does not rely on prediction confidence. Higher mIoU values indicate better segmentation accuracy.\n\u2022 Expected Calibration Error (ECE): Quantifies the difference between predicted confidence and actual accuracy. Lower ECE values indicate better calibration of the model's confidence scores.\n\u2022 Area Under the ROC Curve (AUROC): Represents the area under the curve plotting the true positive rate against the false positive rate, using the predicted confidence level to rank pixels. Higher AUROC values indicate better ability to distinguish between correct and incorrect predictions.\n\u2022 False Positive Rate at 95% True Positive Rate (FPR@95): Measures the false positive rate when the true positive rate is 95% computed in the ROC curve above. Lower FPR@95 values indicate better robustness against false positives.\n\u2022 Area Under the Precision-Recall Curve (AUPR): Represents the area under the curve plotting precision against recall, using the predicted confidence level to rank pixels. Higher AUPR-Success and AUPR-Error values indicate better ability to identify correct and incorrect predictions, respectively.\nOOD metrics. The OOD metrics are computed on the SMIYC and Synobjs subsets for invalid pixels only. Invalid pixels in these datasets are those that are obscured by OOD objects. The OOD metrics include:\n\u2022 Area Under the Precision-Recall Curve (AUPRC): AUPRC, over the binary criterion of a pixel being invalid, ranked by the reversed predicted confidence level for the pixel.\n\u2022 Area Under the ROC Curve (AUROC): Area Under the ROC Curve, over the binary criterion of a pixel being invalid, ranked by the reversed predicted confidence level for the pixel.\n\u2022 False Positive Rate at 95% True Positive Rate (FPR@95): False Positive Rate when True Positive Rate is 95% computed in the ROC curve above.\nSummary metrics. The summary metrics include:\n\u2022 Semantic: The harmonic mean of all semantic metrics, where ECE and FPR@95 are reversed.\n\u2022 OOD: The harmonic mean of all OOD metrics, where FPR@95 is reversed.\n\u2022 BRAVO: The harmonic mean of the semantic and OOD harmonic means, serving as the official ranking metric for the challenge."}, {"title": "3.3. Implementation Details", "content": "We use the following models from the timm library [19] to initialize the VFM:\n\u2022 deit3_small_patch16_224.fb_in1k;\n\u2022 vit_small_patch14_dinov2;\n\u2022 vit_base_patch14_dinov2;\n\u2022 vit_large_patch14_dinov2;\n\u2022 vit_giant_patch14_dinov2.\nThe models are fine-tuned for 40 epochs using two A6000 GPUs, with a batch size of 1 per GPU and gradient accumulation over 8 steps, resulting in an effective batch size of 16. Our implementation follows the details provided in [10]. Notably, the learning rate for the VFM weights is set to be 10\u00d7 smaller than the overall learning rate, as this configuration empirically yields better results. For the Mask2Former decoder, we employ a variant specifically adapted for use with a single-scale ViT encoder, as introduced in [10]."}, {"title": "4. Results", "content": "BRAVO index. The official ranking metric, calculated as the harmonic mean of the semantic and OOD harmonic means for each method, is shown in Table 1. Our best-performing model, DINOv2 with a ViT-L/8 backbone and a linear decoder, achieves the highest BRAVO index of 77.9, which is +10.1 higher than the next best method not submitted by us, which uses multiple different specialist models, i.e., PixOOD YOLO (=\"Model Selection\").\nSubset harmonic means. The harmonic means of semantic and OOD metrics for each subset in the BRAVO benchmark dataset are shown in Table 2. Although our DINOv2-based models perform well across most subsets, they do not consistently outperform all other methods. Specifically, on the Out-of-context, Synobjs, and Synrain subsets, the method named Model selection outperforms our models. Interestingly, compared to other methods, our DINOv2-based models work particularly well on the datasets with OOD objects; SMIYC and Synobjs. On SMIYC, our best models achieve scores above 89.0, while the next best method scores 74.9, and others score significantly lower. This indicates that the VFM pre-training is particularly helpful for out-of-distribution detection in real-world scenarios.\nSemantic metrics. Table 3 presents the performance metrics for valid pixel predictions and their confidence, averaged across all subsets except SMIYC.\nWe observe that the model with the highest mIoU, DINOv2 with a ViT-g/16 backbone and a Mask2Former decoder, performs relatively poorly on the other metrics. As the other metrics take into account the confidence score, this suggests that while Mask2Former is effective at predicting the correct class, it is less adept at estimating the confidence of its predictions, at least in the out-of-the-box manner in which we used it. In our setup, models with a simple linear decoder provide the best trade-off between segmentation accuracy and confidence estimation.\nA similar result is observed when changing the patch size. A smaller patch size of 8 \u00d7 8 results in better mIoU, but the other metrics are worse. This indicates that a smaller patch size allows the model to capture more fine-grained details, which improves the accuracy of the predicted class labels, but that this somehow makes the confidence scores less reliable.\nAnother noteworthy observation is that all our models have relatively low ECE values, indicating that they are well-calibrated, even though no explicit calibration techniques were applied. Even the DeiT-III-based model, which scores low on the overall BRAVO score, achieves a low ECE value of 1.8. Therefore, further investigation is needed to understand why the ECE values are so low.\nFinally, the results suggest that more accurate models in terms of mIoU tend to be worse at identifying their own errors, as indicated by the AUPR-Error metric. However, they excel at identifying correct predictions, as shown by the AUPR-Success metric. It is possible that this happens simply because errors by accurate models are rarer, making it harder to identify them.\nOverall, the results show that mIoU, which does not depend on prediction confidence, does not correlate well with the other metrics that do.\nOOD metrics. Table 4 presents the performance metrics for detecting OOD objects by identifying invalid pixels based on prediction confidence, averaged over the SMIYC and Synobjs subsets.\nSurprisingly, our configuration with worst confidence estimation for valid pixels (see Table 3), DINOv2 with ViT-g/16 and Mask2Former, achieves the highest AUPRC of 84.1, the highest AUROC of 98.8, and the lowest FPR@95 of 4.5 for detecting invalid pixels. This suggests that the mask classification framework used by Mask2Former, where per-class masks are predicted separately, allows this decoder to more accurately identify which pixels belong to the mask of an in-distribution class and which do not.\nAdditionally, while a smaller patch size results in worse confidence estimation for valid pixels (see Table 3), it helps in identifying invalid pixels. Qualitative analyses show that the smaller patch size enables the model to better separate valid and invalid pixels, as it can capture more fine-grained details.\nOverall, the results indicate that the models best at identifying invalid pixels are not necessarily the same ones that excel at correctly classifying valid pixels or accurately estimating their confidence for valid pixels."}, {"title": "5. Conclusion", "content": "This report presents the winning solution for Track 1 of the 2024 BRAVO Challenge. Our approach is based on fine-tuning Vision Foundation Models (VFMs) for semantic segmentation, leveraging the strong representations learned during pre-training. The results demonstrate that, just through better pre-training, a fine-tuned VFM is more robust under distribution shifts than complex specialist models, and can both accurately classify pixels and reliably estimate its confidence in these predictions. We evaluated our approach in several configurations, which has lead to several new observations. While we identify potential causes of these observations, future work is necessary to explore this further. Particularly, we believe it would be interesting to investigate (a) the cause of the low calibration error of our ViT-based models, and (b) the reason for the difference in semantic and OOD metrics with the Mask2Former decoder compared to the linear decoder. Additionally, future works could explore whether some of the more specialized out-of-distribution detection or calibration methods are still effective when used in combination with VFMs."}]}