{"title": "Benchmarking Complex Instruction-Following with Multiple Constraints Composition", "authors": ["Bosi Wen", "Pei Ke", "Xiaotao Gu", "Lindong Wu", "Hao Huang", "Jinfeng Zhou", "Wenchuang Li", "Binxin Hu", "Wendy Gao Jiaxin Xu", "Yiming Liu", "Jie Tang", "Hongning Wang", "Minlie Huang"], "abstract": "Instruction following is one of the fundamental capabilities of large language models (LLMs). As the ability of LLMs is constantly improving, they have been increasingly applied to deal with complex human instructions in real-world scenarios. Therefore, how to evaluate the ability of complex instruction-following of LLMs has become a critical research problem. Existing benchmarks mainly focus on modeling different types of constraints in human instructions while neglecting the composition of different constraints, which is an indispensable constituent in complex instructions. To this end, we propose COMPLEXBENCH, a benchmark for comprehensively evaluating the ability of LLMs to follow complex instructions composed of multiple constraints. We propose a hierarchical taxonomy for complex instructions, including 4 constraint types, 19 constraint dimensions, and 4 composition types, and manually collect a high-quality dataset accordingly. To make the evaluation reliable, we augment LLM-based evaluators with rules to effectively verify whether generated texts can satisfy each constraint and composition. Furthermore, we obtain the final evaluation score based on the dependency structure determined by different composition types. COMPLEXBENCH identifies significant deficiencies in existing LLMs when dealing with complex instructions with multiple constraints composition.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have proven their remarkable abilities in addressing various NLP tasks [1]. Among these, instruction following is one of the most crucial requirements for LLM applications as it determines how well LLMs align with human intents [2]. In real-world use of LLMs, almost all the tasks are formulated as instruction following, where human instructions impose different constraints on the model output to specify the requirement of specific tasks [3].\nHence, how to accurately measure the quality of instruction following has become an essential problem. While early works focused on simple and direct human instructions in traditional NLP tasks, such as translation and text classification [4, 5, 6], These complex instruction-following benchmarks aim to measure whether the generated text can meet every constraint in the input instruction.\nHowever, we argue that existing complex instruction-following benchmarks neglect to model the composition of constraints, causing insufficient evaluation of the LLMs' ability to follow complex instructions."}, {"title": "2 Related Work", "content": "Evaluation of Instruction-Following. Instruction following remains one of the most important factors determining the practicality of LLMs [18]. Therefore, numerous studies have attempted to evaluate it from various aspects. Earlier works used to focus on simple human instructions formed with mostly a single constraint, such as semantic [5, 4, 6] and format [16, 19, 20] constraints. Since LLMs have been gradually applied to address complex real-world tasks, users have to form complex instructions, which naturally call for the evaluation of the LLMs' ability in complex instruction following [3, 13]. WizardLM [14] employs two strategies, In-Breadth Evolving and In-depth Evolving, to form complex instructions from simple ones. CELLO [15] defines complex instructions from task descriptions and input text, and evaluates LLMs with real-world scenarios data. Unlike our work, which includes subjective and objective constraints and combines LLM-based and rule-based evaluations, CELLO focuses only on objective, rule-verifiable constraints and uses rule-based scoring functions for evaluation. Nonetheless, we argue that these benchmarks neglect to model the composition of constraints, which is an important character in complex instructions and brings non-negligible structural complexity that is crucial to assessing LLMs' abilities.\nCompositionality in NLP. Previous studies have explored compositionality across traditional NLP tasks, including semantic parsing [21, 22, 23], machine translation [23, 24], style transfer [25], and data-to-text generation [26]. However, in the task of instruction-following, how the LLMs deal with the compositionality in instructions is still under-explored. CompMCTG [27] investigates the compositionality of multiple control attributes for LLMs, which is a topic neighboring ours. Nevertheless, our work studies more complex composition types beyond simple coordination between different constraints, such as Chain and Selection and their nested structures, which form the basis of many real-world complex tasks for LLMs."}, {"title": "3 COMPLEXBENCH Framework", "content": "3.1 Overview\nTo comprehensively evaluate the ability of LLMs to follow complex instructions, we propose a hierarchical taxonomy to define constraints and composition types. For constraints, we extend common constraints in controlled text generation tasks to the instruction-following tasks and consider a two-level structure including coarse-grained types and fine-grained dimensions (Section 3.2). As for compositions that indicate structural combinations of constraints, we consider the characteristics of instruction-following tasks to define the composition types according to existing works on compositionality in traditional NLP tasks (Section 3.3).\n3.2 Constraints\nFollowing existing works on controlled text generation and instruction following [28, 29, 30, 31, 16, 32], we propose a two-level structure for constraints including 4 constraint types (i.e., Lexical,"}, {"title": "3.3 Composition", "content": "As shown in Figure 3, we propose 4 composition types that indicate typical structural combinations of constraints.\nSingle. The output is required to satisfy a single constraint.\nAnd. The output needs to satisfy multiple constraints simultaneously. This simple composition type commonly appears in most of the existing benchmarks on complex instruction-following [3, 16, 13].\nChain. The output is required to complete multiple tasks in the instruction sequentially, each of which may contain several constraints. Formally, Chain contains n tasks {T1, T2, . . ., Tn}, which need to be completed sequentially. The output of Tk+1 depends on that of Tk (k = 1,2,\u2026\u2026\u2026, n \u2212 1).\nSelection. The output is required to select different branches according to certain conditions, fulfilling the constraints of the corresponding branch. Formally, Selection contains m branches {B1, B2, . . ., Bm}, each of which is a task with expected outputs Y1, Y2, . . ., Ym respectively. We denote a selection function as S with a range {1,2,\u2026\u2026\u2026, m}, taking the selection condition cond as input. Finally, the expected output of the instruction is Ys (cond).\nIt's worth noting that the above composition types can be nested to construct more complex structures. Each task in Chain and each branch in Selection may also contain other composition types. As shown in the last row of Figure 3, a branch of Selection can also contain Selection, thus forming a nested selection composition type.\nTo verify the necessity and comprehensiveness of the composition types considered in COMPLEXBENCH, we analyze the distribution of composition types in real-world scenarios. We collect instructions with high demand and representativeness from two application scenarios including general and professional instructions. Professional instructions may contain specialized domains, such as business and academic research. For each category of instructions, we randomly sample 300 instructions and count the number of instructions containing each composition type. We found that the taxonomy of COMPLEXBENCH fully covers present composition types. As shown in Figure 4, although the composition types of general instructions are relatively simple and have already been covered by current benchmarks, professional instructions include more complex composition types, such as Selection and nested structures of multiple composition types, which are rarely been considered by current benchmarks. As LLMs have been gradually applied to deal with complex instructions in professional scenarios, it is necessary to evaluate their ability to follow instructions with multiple constraints composition."}, {"title": "4 COMPLEXBENCH Construction", "content": "4.1 Data Collection\nWe manually construct COMPLEXBENCH based on the taxonomy described in Section 3. The detailed construction pipeline consists of four steps, i.e., Reference Instructions Collection, Task Allocation, Data Annotation and Validation, and Selection Branch Expansion.\nReference Instruction Collection. Considering the difficulty of constructing complex instructions from scratch, annotators are required to create new complex instructions based on provided reference instructions. We collect reference instructions from real-world application scenarios and open-source instruction following benchmarks [16, 3, 13]. We conduct strict desensitization of privacy and carefully filter these instructions using category and quality classifiers.\nTask Allocation. To ensure comprehensive coverage of each constraint and composition type, we partition the entire dataset construction into multiple annotation tasks. Each annotation task has different requirements for the minimal number of constraint dimensions in each constraint type and composition type. Annotators are required to modify reference instructions to meet the requirements of corresponding tasks. To alleviate the annotation cost, especially when the constraint dimensions in the reference instructions and task requirements are different, we leverage GPT-4 [37] to automatically acquire the constraint dimensions in reference instructions and assign them to corresponding annotation tasks according to minimal editing distance.\nData Annotation and Validation. Given reference instructions and corresponding annotation task requirements, annotators are expected to construct new complex instructions and annotate the constraint dimensions and composition types. After the data annotation, newly constructed"}, {"title": "4.2 Evaluation Protocol", "content": "To conduct a detailed evaluation of how well each constraint and composition type is satisfied, we draw inspiration from previous works that transform text evaluation into multiple question-answering tasks [11, 12, 13]. For each constraint and composition type specified in an instruction, we manually craft a scoring question that can be succinctly answered with either \"yes\" or \"no.\"\nCurrent mainstream evaluation methods contain LLM-based [14, 3, 13] and rule-based methods [15, 16, 17]. In our preliminary experiments, we find that LLM-based methods are effective at answering open-ended scoring questions, but they demonstrate a significant deficiency in those involving numerical computation, counting, and other objective rule-defined areas, such as keyword inclusion and text length. Simultaneously, rule-based methods perform well in rule-defined areas but are powerless against open-ended scoring problems. To address their limitations, we design a Rule-Augmented LLM-based (RAL) evaluation method to equip LLM evaluators with rules to answer scoring questions in both rule-defined and open-ended areas. For the instruction I, the generated response to be evaluated o, and the scoring problem q, if q is verifiable by rules, we first use the LLM to automatically extract segments e of o, which is related to scoring question q. Subsequently, we use the rule Rq written for q to obtain the evaluation result rq \u2208 {0, 1}, that is:\ne = Mext(I, q, 0)\n   (1)\nrq = Rg(e)\n (2)\nwhere Mext indicates the LLM with the prompt used for extraction. Otherwise, if q cannot be verified by rules, we directly use the LLM to measure the quality of 0:\nrq = Meva(I, q, 0)\n  (3)\nwhere Meva denotes the LLM with the prompt used for evaluation. For composition types, considering that their satisfaction is a prerequisite for satisfying some constraints, we model the dependencies of its scoring questions. Specifically, for Chain, all the scoring questions of the subsequent task depend on the answers to those of the preceding task. And for Selection, all the scoring questions of the selection branch depend on whether the correct selection branch is selected. If a scoring question is judged as \"no\", all the scoring questions depending on it will also be directly judged as \"no\". Formally, we denote the set of scoring questions that q depends on as Dep(q). After all scoring questions have been independently verified, Dependency Aggregation will be performed, and the result of q will be calculated as follows:\nra = ra \u039b\npeDep(q) rp\n (4)\nFinally, following InfoBench [13], we calculate Decomposed Requirements Following Ratio (DRFR) as the final score during Score Aggregation. Considering a benchmark dataset has N instructions, the instruction i has mi scoring questions, and the result of the j-th scoring question is rij, the metric is calculated as: DRFR = \u2211i,jrij/\u03a3i mi. Figure 5 shows a framework of our evaluation protocol."}, {"title": "4.3 Benchmark Statistics", "content": "COMPLEX BENCH contains 1,150 instructions and 5,306 scoring questions, as shown in Table 2. Nesting depth means the maximum depth of composition types. In addition to three basic composition types including And, Chain, and Selection, we adopt a separate category whose instructions simultaneously contain Chain and Selection, aiming to use these two challenging types to explore the boundary of LLMs' ability in complex instruction-following2. We also present the task distribution of COMPLEXBENCH in Appendix B."}, {"title": "5 Experiments", "content": "5.1 Agreement Evaluation\nTo measure the agreement between our evaluation method and manual evaluation, we randomly sample 200 instructions from COMPLEXBENCH to construct a meta-evaluation dataset. Five LLMs are involved in this evaluation as generation models. We employ GPT-4-1106 [37] as our primary judge and adopt two metrics to confirm the reliability of our method: (1) Overall Pairwise Agreement: Given an instruction, two model responses (denoted as A and B), the human annotators are instructed to compare the quality and choose from 3 options, namely A better than B, tie, B better than A. Subsequently, the automatic evaluation scores for two model responses are converted into pairwise comparisons to measure agreement with human annotators. (2) Question-level Agreement: Given an instruction and a model response, human annotators are instructed to judge whether each scoring question is satisfied respectively. Then, we calculate the agreement between automatic evaluation results and human-annotated ones.\nFor the Overall Pairwise Agreement, we sample 500 pairs from the outputs of 5 LLMs. Direct Scoring serves as a baseline, which adopts a scoring prompt [5] to assign a score to the response with a scale of 1-10. As shown in Table 3, our method can improve the agreement with manual evaluations compared to Direct Scoring with a large margin. Dependency Aggregation also shows its important contribution to our method due to its modeling of composition structures."}, {"title": "5.2 Automatic Evaluation", "content": "5.2.1 Setup\nWe use GPT-4-1106 [37] as our judge to evaluate 15 LLMs: (1) Closed-source LLMs: GPT-4-1106, Claude-3-Opus [38], GLM-4 [39], ERNIEBot-4, GPT-3.5-Turbo-1106. (2) Open-source LLMs: Qwen1.5-Chat [40], Llama3-Instruct [41], InternLM2-Chat [42], Baichuan2-Chat [43], Mistral-Instruct [44], InternLM2-Chat [42], ChatGLM3-Chat [45]. The sizes of these models vary from 6B to 72B. We use greedy search for reproducibility, and the maximum generation length is 8,192.\n5.2.2 Main Results\nThe main results are shown in Table 5. Firstly, the widely recognized powerful GPT-4 still fails to complete 20% of complex instructions, highlighting the necessity of complex instruction evaluation. Secondly, as the complexity of composition types within instruction increases, the performance of all LLMs significantly drops, especially on Selection and Chain. This aligns with our motivation for constructing complex composition types. Thirdly, the performance of most open-source LLMs falls short compared to closed-source LLMs especially on complex composition types, indicating that open-source LLMs still have a large room for improvement in chasing the capabilities of closed-source LLMs.\nTo dissect the ability of LLMs to follow specific constraint and composition types, we calculate the average accuracy of scoring questions for each type. The results are shown in Figure 6.\nFirstly, for constraints, LLMs generally perform better on Semantic and Utility constraints but struggle with the Format and Lexical constraints that have explicit evaluation standards. Secondly, for compositions, Chain presents severe challenges while Selection come second. We speculate that the main difficulty in Selection lies not only in choosing the correct branch but in executing it without interference from irrelevant branches. More results and analyses are in Appendix F and G.\n5.2.3 Analysis\nDecomposition of instructions with composition types. To explore whether decomposing complex instructions and executing them through multi-round interactions can improve the performance of LLMs, we manually decompose COMPLEXBENCH instructions based on composition types (e.g., Chain into sequential tasks, Selection into selection and execution branches, while And remains intact) and compare the performance of LLMs between executing decomposed instructions step-by-step"}, {"title": "6 Conclusion", "content": "In this work, we propose COMPLEXBENCH, a systematical benchmark for complex instruction-following. We firstly propose a hierarchical taxonomy for complex instructions, including 4 constraint types, 19 constraint dimensions and 4 composition types and manually collect a high-quality dataset accordingly. Along with the dataset, we propose a structure-aware automatic evaluation method for complex instruction-following with constraints composition and further enhance the evaluation accuracy by equipping LLM-based evaluators with rules. Finally, we conduct extensive experiments to evaluate the performance of current representative LLMs on complex instruction-following and uncover their significant deficiencies in dealing with complex composition types. In summary, we posit that COMPLEXBENCH can serve as a valuable tool for benchmarking the complex instruction-follow ability of LLMs and provide useful insights for further work to improve this ability of LLMs."}, {"title": "A Limitation", "content": "The limitations of our work are summarized as follows:\nMonolingual Capability. COMPLEXBENCH comprises exclusively Chinese content, which may neglect some elements in other languages and cultures that can influence the complexity of instructions. Recognizing this constraint, we plan to expand COMPLEXBENCH by incorporating multiple languages to investigate the disparities in complex instruction-following ability of LLMs across different linguistic environments in future iterations.\nLLM-based Evaluation. The evaluation method based on LLM is widely used in the automatic evaluation process of COMPLEXBENCH. Although experiments show that our evaluation method achieves satisfactory agreement with human judgment generally, the potential biases of LLM-as-Judge, such as verbosity and self-enhancement [5], may affect the overall evaluation correctness. Additionally, we utilize GPT-4-1106 commercial APIs for evaluation, which presents challenges such as high costs and potential data leakage. We leave the development of more accurate and efficient methods for evaluating complex instruction-following as important future work."}, {"title": "B Task Distribution of COMPLEXBENCH", "content": "We refer to the taxonomy of AlignBench [6] to categorize the task types of instructions in the COMPLEXBENCH. Taking into account that instructions about mathematics have relatively fixed answers and are difficult to construct complex instructions, as well as the coarse granularity of the writing ability category. We remove mathematical and use 4 subcategories of writing ability in AlignBench: practical writing, creative writing, professional writing, and custom writing. When annotators construct instructions, they also provide task category labels simultaneously, the results are shown in Table 7."}, {"title": "C Details of Constraint Dimensions", "content": "C.1 Lexical Constraint\nWord Matching. The response should accurately find the corresponding content of certain keywords in the given instruction.\nKeywords. The response should (not) include certain keywords, or include several words from a keyword list.\nC.2 Format Constraint\nJSON Format. The entire response should be wrapped in JSON format."}, {"title": "C.3 Semantic Constraint", "content": "Language Style. The response should adhere to a specific language style. We use the taxonomy of CharacterGLM [32], which defines language style from multiple aspects such as formality, imitation of celebrities, context-specific scenes, and discourse features (like using style from a certain website, emoji, etc.).\nPersonalization. The response should align with certain character attributes.\nTopic. The response should focus on a specific topic.\nSentiment. The response should contain specific emotions. We refer to the six fine-grained categories of ECM [36] for sentiment, named as Like, Happy, Sad, Disgust, Angry, Other."}, {"title": "C.4 Utility Constraint", "content": "Helpfulness. The response should follow task descriptions.\nTarget Language. The response should be in a specific language, such as simplified Chinese, traditional Chinese or English.\nSupportiveness. The response should be faithful to input texts, answering based on the information provided in the text completely.\nConsistency. The content of the response should be consistent and free of contradictions.\nFactuality. The response should correspond with facts, which primarily applies to instructions with definitive answers such as mathematical and logical reasoning."}, {"title": "D Prompts for Extractor in Rule-Augmented LLM-based Evaluation", "content": "Table 8 provides the prompt template we used for the LLM extractor in Rule-Augmented LLM-based evaluation. And Table 9 provides an example of scoring object extraction. To improve performance, we use 6 manually constructed in-context examples in the prompt. Considering that the extraction of content differs significantly when there are multiple scoring objects (e.g., scoring question \u201cDoes each shot's dialogue in the model output start with an interrogative sentence?\u201d), compared to when there is only one scoring object (e.g., scoring question \u201cDoes the title of the speech given by the model have no more than 10 characters?\u201d). We use different sets of in-context examples for these two situations."}, {"title": "E Prompts for Evaluator in Rule-Augmented LLM-based Evaluation", "content": "Table 10 provides the prompt template we used for the LLM evaluator in Rule-Augmented LLM-based evaluation. And Table 11 provides an example of automatic evaluation. We have also explored different settings where all scoring questions from the instruction are presented to the evaluation model simultaneously, or asking the evaluation model to choose \"YES\" or \"NO\" without analysis. Ultimately, we found that the current settings achieve the highest level of agreement with humans."}, {"title": "F The Influence of Composition Types Nested Methods", "content": "Table 12 presents DRFR of GPT-3.5-Turbo-1106 on instructions with different numbers of each composition type. Nested multiple Selection seems to be significantly more difficult than other composition type nested methods. And the addition of And has a limited impact on the overall complexity of instructions. The result reveals the weakness in the ability of LLMs to follow complex instructions with multi-layer tree structures, highlighting the importance of further efforts to improve LLMs in these areas."}, {"title": "G Detailed Results of Each Constraint and Composition Type", "content": "Table 13 presents the average accuracy of LLMs on diverse constraint dimensions and composition types. Topic, Markdown Format, Consistency, Sentiment, and Personalization seem to be the easiest constraint dimensions for LLMs overall, while Length, Punctuation, Keywords, End with, and Factuality pose the greatest challenges. It is worth noting that the performance of all LLMs on Length is far from satisfactory, with even the strongest model achieving only an accuracy rate of 0.532. This result indicates that there is still significant room for improvement in the ability of current LLMs to precisely control and plan the output content."}, {"title": "H Detailed Results of Each Task Type", "content": "Table 14 presents the DRFR of the selected LLMs for each task type. We find that the performance of LLMs across tasks is balanced overall. Relatively, LLMs perform better on tasks related to writing and role-playing, while they have shortcomings in Logical Reasoning, Advanced Chinese Understanding, and Open-ended Questions. All LLMs exhibit significant weaknesses in Logical Reasoning, which is consistent with the Reasoning Drawbacks found in AlignBench [6]."}]}