{"title": "DISCO: DISCovering Overfittings as Causal Rules for Text Classification Models", "authors": ["Zijian Zhang", "Vinay Setty", "Yumeng Wang", "Avishek Anand"], "abstract": "With the rapid advancement of neural language models, the deployment of overparameterized models has surged, increasing the need for interpretable explanations comprehensible to human inspectors. Existing post-hoc interpretability methods, which often focus on unigram features of single input textual instances, fail to capture the models' decision-making process fully. Additionally, many methods do not differentiate between decisions based on spurious correlations and those based on a holistic understanding of the input. Our paper introduces DISCO, a novel method for discovering global, rule-based explanations by identifying causal n-gram associations with model predictions. This method employs a scalable sequence mining technique to extract relevant text spans from training data, associate them with model predictions, and conduct causality checks to distill robust rules that elucidate model behavior. These rules expose potential overfitting and provide insights into misleading feature combinations. We validate DISCO through extensive testing, demonstrating its superiority over existing methods in offering comprehensive insights into complex model behaviors. Our approach successfully identifies all shortcuts manually introduced into the training data (100% detection rate on the MultiRC dataset), resulting in an 18.8% regression in model performance-a capability unmatched by any other method. Furthermore, DISCO supports interactive explanations, enabling human inspectors to distinguish spurious causes in the rule-based output. This alleviates the burden of abundant instance-wise explanations and helps assess the model's risk when encountering out-of-distribution (OOD) data.", "sections": [{"title": "1. Introduction", "content": "Over-parameterized transformer models for natural language tasks have demonstrated remarkable success. However, these inherently statistical models are prone to overfitting, particularly in terms of the correlation between input phrases and prediction labels, known as \u201cshortcuts\u201d, which can lead to biased outcomes [1, 2]. Our goal is to identify these shortcuts in text classification tasks and enhance human understanding of the model's predictive reasoning. We propose a post-hoc, model-agnostic method designed to reduce the amount of human effort needed to evaluate the justification of the model's decisions.\nIn this paper, we introduce DISCO, a method designed to extract a concise set of global rules using longer text sequences, which helps identify undesirable causal shortcuts learned in text classification tasks."}, {"title": "2. Related Work", "content": "In this section, we introduce existing works related to ours, highlight their limitations, and describe how our approach resolves them."}, {"title": "2.1. Local Interpretability", "content": "Considerable work has been done on post-hoc interpretability of language tasks based on token selection [3, 4, 5, 6]. Interpretable-by-design approaches also often select specific input tokens as rationales for tasks, using these as intermediate inputs for the prediction model [7, 8, 9]. These approaches focus on interpreting individual instances, necessitating labor-intensive, human-driven analysis to identify problematic prediction reasons. Our approach, in contrast, globally extracts rules internalized by the language model. Other works analyze model behavior using composition operators over primitive concepts aligned with human-understandable concepts [10]. Despite their global perspective, these methods do not incorporate causal patterns. Attribution patterns from local interpretability methods lack inherent causality and may fail to capture the causal relationships internalized by the model. Recent approaches that aggregate rules from local explanations [11, 12] are also unsuitable for language tasks due to their reliance on single terms and inability to produce causal rules. SEARs [13] is closer to our work, detecting semantically equivalent adversarial replacement rules leading to prediction changes. However, our method identifies patterns consistently leading the model to specific predictions under counterfactual conditions."}, {"title": "2.2. Causal Inference on Language Tasks", "content": "Most research in this area focuses on creating \u201ccounterfactual instances\", altered or minimally disturbed instances, to gain insights into model behavior. These counterfactuals are developed through human"}, {"title": "2.3. Rule Extraction for Model Debugging", "content": "Recent research characterizes model deficiencies through rules by dataset contamination [17, 2], but fails to identify human-comprehensible text sequences with high statistical capacity, which is precisely our aim. Furthermore, our methods are post-hoc and non-intrusive. Anchor [11] identifies local n-gram phrases with high explanability, but its time complexity results in intractable calculations on the entire training set. [18] involves a white-box, rule-based method, and [19] identifies spurious correlations rather than all shortcuts, making them less suitable for direct comparison with our approach. [20] is word-based and, therefore, not suitable for n-gram rules. These methods adopt a local perspective, aggregating explanations on an instance-by-instance basis without considering context awareness or causality. Our approach, in contrast, is n-gram-based, causal, and context-aware, providing a more comprehensive and insightful analysis.\nAtwell et al. [21] aims to evaluate the risk associated with models when exposed to test data with distribution shifts compared to their original training data. However, their research goal differs from ours. While their approach yields evaluation scores characterized by bias and h-discrepancy across datasets from different domains, our approach identifies possible shortcut n-grams learned from the original training data, offering more intuitive and interpretable shortcut rules.\nTraditional research on developing n-gram classifiers focuses on highly interpretable algorithms leveraging frequent n-grams to discern between different topics [22, 23, 24]. Unfortunately, these classifiers either do not achieve performance comparable to modern neural models or lack universality. Our approach bridges the gap between interpretability and performance by effectively identifying high-support n-gram patterns from underlying neural models."}, {"title": "3. Causal Rule Mining", "content": ""}, {"title": "3.1. Problem Statement", "content": "We consider an underlying model M trained on a classification dataset represented as $D \\subset X \\times Y$. Here, X represents the input space, and y represents the labels. An input $x \\in X$ is an ordered sequence of terms $(x_1,x_2,...,x_{|x|}),$ where each term $x_i$ comes from the vocabulary V. The prediction made by M on input x is denoted as $\\hat{y} = \\underset{y \\in Y}{\\operatorname{argmax}} P_M(y|x)$. For simplicity, we abbreviate this as $\\hat{y} = M(x)$ throughout this paper. Our research focuses exclusively on binary classification tasks.\nWe define $s = (s_1, s_2, ..., s_n)$ as an n-gram sub-sequence of x (represented as $s \\subseteq x)$. The remaining content in x is denoted as c, i.e., $x = \\langle s, c \\rangle$, where $\\langle\\cdot,\\cdot\\rangle$ is the sequence combination operator. Note that we do not assume sequence continuity in either c or s. The support of s within D is defined as $Sup(s, D) = | \\{x \\in D : s \\subseteq x\\} |$.\nAdditionally, we define a rule r as a tuple $(s \\rightarrow \\hat{y})$, where the sequence s is its pattern and $\\hat{y}$ is its consequent label. For instance, the rule:\nindicates that \u201cthe best movie\u201d is a shortcut for M to predict POSitive. In this context, we say that M predicts y primarily relying on the presence of the sequence s, rather than comprehending the overall input.\nOur objective is to discover a globally representative set of rules, denoted as $G = \\{r = (s, \\hat{y})\\},$ where each rule represents a shortcut learned by \u041c."}, {"title": "3.2. DISCO: Approach Overview", "content": "To streamline the identification process, we begin by extracting all high-frequency n-gram patterns from the training data (Section 3.3). We then retain the candidates that pass the causality check (Section 3.4) as the final output rules. Our approach is designed to verify the (non-)existence of confounding variables, serving as a statistical test to establish causality in classification tasks."}, {"title": "3.3. Generation of Candidate Sequences", "content": "In the initial step, our primary objective is to extract frequent n-gram sequences that exhibit a high correlation with specific model predictions.\nSequence Mining. Empirical studies such as [25] emphasize that a pattern is more likely to influence a model's prediction as a shortcut if it occurs frequently in the training set. Therefore, we first select all frequent patterns using an efficient approach known as DESQ-COUNT [26]. For a detailed explanation of DESQ-COUNT, please refer to [27].\nNPMI Evaluation. We further evaluate the pattern-prediction correlation using their NPMI (Normalized Pointwise Mutual Information) score. Initially, we list all input data x from the training set together with their corresponding predictions from the model $y = M(x)$. Then we calculate P(y, s), P(y|s), and P(y) from these predictions. It is worth mentioning that these probabilities are different from the model's prediction $P_M(\\hat{y}|x)$. Using these terms, we calculate the NPMI scores for all frequent s identified by DESQ-COUNT:\n$NPMI(s; y) = \\frac{PMI(y; s)}{h(s, y)} = \\frac{log\\frac{P(y|s)}{P(y)}}{h(s, y)}$\nwhere h(s, y) = \u2014 log P(s, y) is the entropy of P(s, y). The resulting NPMI score falls within the range of [-1, 1], capturing the spectrum from \u201cnever occurring together (-1)\u201d to \u201cindependence (0)\" and ultimately \"complete co-occurrence (1)\" between the pattern and the label. We retain only those pairs that demonstrate a substantial level of correlation in their NPMI scores."}, {"title": "3.4. Causality Check", "content": "Such correlation alone, however, does not guarantee a direct causal relationship, as it could also arise from a confounding factor [28]. In our context, we assume the confounding factor is the latent semantic representation z of the input. The presence of sequence pattern s and the context c of the input x are conditioned on z. An ideal machine learning model should comprehend this structure and capture z, rather than relying solely on the statistical correlation between s and \u0177, also referred to as the \"shortcut\" [29, 2]."}, {"title": "3.5. A Toy Example", "content": "At the end of this section, we provide a toy example to assist our readers in understanding the full process of our approach. We consider an extreme situation as follows to help illustrate. Assume a sentiment analysis problem where all reviews on books are positive, and all reviews on movies are negative in the training data. A model trained on such data might incorrectly predict positive for a review like \"this book is badly written\" due to its overfitting to the correlation between the sequence \u201cthis book\u201d and the label positive. It is worth mentioning that such sequences may appear semantically senseless and therefore \u201cnon-causal\u201d to humans. The resulting rules reflect the rational basis of the model's prediction, rather than convincing a human inspector of its causality.\nIn DISCO, we apply DESQ first to identify the correlation between the sequence \u201cthis book\u201d and the label positive from the training data. This pair is then subjected to an NPMI check to decide whether it is a candidate sequence (Section 3.3). Then, in the causality check (Section 3.4), we keep \"this book\" constant and vary its contexts to other neutral contexts (Section 3.4) like \"was played in the cinema\" or \"is on the table\". If the prediction predominantly remains positive, we infer that \u201cthis book\u201d \u2013 positive is a shortcut."}, {"title": "4. Experimental Evaluation", "content": ""}, {"title": "4.1. Research Questions", "content": "Our experiments aim to answer the following research questions (RQs):\n\u2022 RQ1. Faithfulness: Are the global rules faithful to the model's local explanations?\n\u2022 RQ2. Recall: If the model is known to have learned some shortcuts, can DISCO identify them?\n\u2022 RQ3. Human Utility: Are the shortcut rules useful for humans in detecting the model's wrong reasons?"}, {"title": "4.2. Models and Datasets", "content": "Our approach is model-agnostic. Therefore, we conduct experiments on multiple models to answer RQ1 and RQ3, including an LSTM model and two over-parameterized transformer models, BERTBASE and SBERT [31].\nThe experiments are conducted on one document classification and three multi-task datasets. Given the foundational role of document classification in information retrieval (IR) and natural language processing (NLP), we employ a unified approach, transforming all datasets into binary classification: Movies from the ERASER benchmark [32] is originally a binary sentiment classification dataset. MultiRC from the same benchmark is converted following the recipe presented in [32]. For SST-2 (Stanford Sentiment Treebank) [33], we binarize the sentiment assigned to each input sentence. As for CLIMATE-FEVER, a fact-checking dataset from ir_datasets [34] with queries and documents regarding climate change, we combine each query with each of its relevant/irrelevant documents as the inputs, while assigning \"relevant\"/\"irrelevant\" as their labels."}, {"title": "4.3. The Agreement Score as a Metric of Faithfulness", "content": "Local interpretation approaches, such as LIME [6] and ExPred [9], provide relatively faithful instance-wise explanations. Although researchers are questioning the quality of LIME explanations [35], LIMEbalances time efficiency and faithfulness well, to the best of our knowledge. Our global rules are considered faithful to the local explanations if they agree with the local explanations in all applicable instances. We define an input x as applicable to a rule $r = (s \\rightarrow \\hat{y})$ if $s \\subseteq x$. Additionally, an applicable input x further satisfies the rule r if its prediction matches the rule's consequent, i.e., $\\hat{y} = M(x)$.\nFor an input-prediction pair (x, y), an instance-wise explainer attributes the prediction $P_M(\\hat{y}|x)$ to $x_i$ as attribution score $a \\in R$. The gathering of all attribution scores of x is represented using $\\vec{a}$. For clarity, we ignore the superscripts of \u0177 in the rest of this section. We rank all terms based on their attribution scores in descending order, denoted as $R^a(x) = (x_{k_1}, x_{k_2},..., x_{k_n})$, where $a_{k_1} \\ge a_{k_2} \\ge ... \\ge a_{k_n}$ are re-ranked token indices.\nFor an input x that satisfies a rule r, we define the agreement score between r and $R^a(x)$ as:\nagreement(r, $R^a(x)$) = ranking score($R^a(x); s),\nwhere the semicolon in the ranking score calculation separates the ranking sequence $R^a(x)$ from the subsequence s.\nWe borrow the nDCG score [36] from ranking evaluation tasks as the ranking score function here and consider the pattern terms as the \u201cground truth\u201d terms. The intuition behind this metric is that the terms selected by the rule (ground truth) should be assigned the highest attribution scores and thus ranked the highest. A higher agreement score indicates that the rule is more faithful to a local explanation. For example, given x = \u201ca b c\u201d with corresponding attribution scores a = [0.1, 0.5, 0.4]. The tokens are therefore ranked as \u201cb \u2013 c \u2013 a\u201d. If s = \u201ca b\u201d, the agreement score is therefore nDCG@k((\u201cab\u201d \u2192 \u0177), b \u2212 c \u2212 a) = $\\frac{0.5/log_2(1+1)}{0.5/log_2(1+1)+0.1/log_2(2+1)} = 0.89$ for k = 2."}, {"title": "4.4. Experiment Environment", "content": "Our approach is implemented in Python 3.7.3, utilizing PyTorch version 1.12.1+cu133. All experiments are conducted on a Linux server equipped with an AMD\u00aeEPYC\u00ae7513 processor and an Nvidia\u00aeA100 GPU with 40 GB of display memory."}, {"title": "5. Results", "content": ""}, {"title": "5.1. RQ1. Faithfulness", "content": "We address this research question through two experiments: explanation alignment and an ablation study. In this section, we mine rules from BERTBASE [37] models fine-tuned on different datasets."}, {"title": "5.1.1. Agreement with Local Explanations", "content": "We aim to evaluate whether the global rules are consistent with the local explanations by measuring the agreement scores between them. Overall, we find a high degree of alignment between the global rules and the local explanations across all three datasets, with low variance (Fig. 3). It is worth mentioning that the lowest agreement score appears on Movies with ExPred, being 0.695, which is the only outlier. The remaining scores range from 0.81 to 0.923. For exact results, we refer to Table 2. This indicates that our rules faithfully represent the model's explanations.\nMoreover, we observed a slight exception in the SST-2 dataset, where the low frequency of sequences leads to a small number of dominant rules and relatively higher variance. Nevertheless, upon manual examination of the rules, we found that most high-coverage rules in this dataset are correct and result in the right prediction. For a detailed evaluation, please refer to Section 5.3.\nIt should be noted that the CLIMATE-FEVER dataset is not included in this analysis because it provides no rationale annotations, making it impossible to train the ExPred model on it. Based on our results, we can conclude that for Movies, SST-2, and MultiRC, the rules with the highest satisfaction are usually the correct reasons for the model's predictions, as they tend to have high alignment with local explanations. However, some rules, such as don't even \u2192 NEG for Movies and in its \u2192 POS for SST-2, suggest that the model has also learned some incorrect shortcuts. Relying on incorrect shortcuts could be even more detrimental to the model's performance when deployed in the field and encountering out-of-distribution"}, {"title": "5.1.2. Ablation Study", "content": "To the best of our knowledge, our work is pioneering in the extraction of global causal rules learned by the model, making it challenging to establish appropriate baseline methods. Instead, we conduct ablation studies on different components of our approach, DISCO, to assess its ability to discover causal rules, as summarized in Table 2. We select the top-15 (s, \u0177) pairs based on their coverage under three conditions: 1) NPMI score filtering only, 2) DISCO with both NPMI and causality checks, and 3) the intersection (\u2229) between 1) and 2). We measure the average agreement scores among these configurations."}, {"title": "5.1.3. Hyperparameters", "content": "For the Movies dataset, we mine sequences with lengths ranging from 4 to 10, and a support value of 20. During the causality check, we consider rules where the average prediction over all synthetic instances is greater than 0.7, serving as the mean threshold.\nFor SST-2, the sequence lengths range from 2 to 10, the support value is 100, and the mean threshold is 0.7.\nBoth datasets are sentiment analysis datasets containing no queries\u00b9.\nOn the other hand, MultiRC and CLIMATE-FEVER datasets consist of instances that include a query and a document. The pattern of their rules is (sq, sd) tuples, indicating a combination of a sequence sq from the query and a sequence sa from the document. During sequence mining, sq and s\u0105 are jointly extracted from the query and document for each instance.\nFor MultiRC, the lengths of sq and sd are constrained within the ranges of 3 to 10 and 4 to 10, respectively. The support value for tuples is set to 200, and the mean threshold is 0.7. For CLIMATE-FEVER, the sequence lengths of sq and s\u0105 are within the ranges of 2 to 10. The tuple support is set to 200, and the mean threshold remains at 0.7."}, {"title": "5.1.4. Statistics", "content": "The statistics of the rules are summarized in Table 3, showcasing key metrics such as #(frequent), #(NPMI), #(rules), and avg(|s|). These columns represent the number of frequent sequences mined by"}, {"title": "5.2. RQ2. Recall", "content": "This research question serves two purposes: 1) to validate our assumption that highly correlated patterns and labels lead to the model learning shortcuts, and 2) to demonstrate the capability of DISCO in identifying these shortcuts.\nQuantitatively evaluating the retention rate of shortcuts by DISCO poses a challenge as it requires knowledge of the ground-truth correlated pattern-label pairs. This challenge is common in the evaluation of explanations [38, 39]. To overcome this issue, we deliberately introduce decoys [1] into the dataset to entice the model into learning shortcuts. All decoys are presented in Table 4. Following a similar methodology to that of [25], we contaminate the original training set with decoy patterns, varying the contamination rate and bias. It is important to note that we only contaminate the training and validation sets, keeping the test set intact. This setup simulates a scenario where the model performs well on a biased dataset but lacks generalization due to learned shortcuts. If our approach can successfully identify the injected decoys, we consider it a success."}, {"title": "5.2.1. Contamination Rate, Bias, and Retention Rate", "content": "The extent of contamination is described by the contamination rate and the bias.\nWe define contamination rate as the ratio of instances containing the decoy, namely $\\frac{Y}{X}$. We further define bias as the label imbalance when adding the decoy, namely $\\underset{y \\in Y}{max} \\Sigma_{y_i \\in Y^d} 1(Y_i=y)$, where $Y^d$ indicates the labels corresponding to all contaminated instances. The label y selected by the $\\underset{y \\in Y}{max}$ operator is referred to as the dominant label.\nThe retention rate is the fraction of decoys that can be detected. A decoy is considered detected if the output of our approach contains the rule constructed by the decoy and its corresponding label. To the best of our knowledge, our study is also the first to systematically investigate the retention rate of decoys under different contamination rates and biases."}, {"title": "5.2.2. Contamination-Bias Settings", "content": "To evaluate the retention rate of DISCO across various scenarios, we examine four different settings that produce different contamination rates and biases: {80%, 20%} \u00d7 {60%, 90%}. Figure 4 illustrates the retention rate and task performance for each of these settings."}, {"title": "5.2.3. Observations", "content": "Figure 4 (third row) demonstrates that adding decoys to the training set has minimal effect on test performance, indicating that the introduced decoys do not significantly alter the data distribution. We"}, {"title": "5.3. RQ3. Human Utility", "content": "A shortcut rule can be a good reason for a model decision, but can also be a wrong one. To measure the human perception of model-generated rules, and to see whether the rules help humans detect wrong reasons for a decision, we conducted experiments using the uncontaminated four training sets with three different models: BERTBASE, LSTM, and SBERT. The extracted rules were independently shown to four machine learning developers who were asked to assess whether a rule was a \u201cwrong reason\". A wrong reason is an explanation that is either non-understandable or implausible, given the underlying language task. For example, the pattern \u201c? ||\u201d of a rule is non-understandable as it contains no meaningful words, while the rule \u201cin its \u2192 POS\u201d is implausible for a sentiment classification task."}, {"title": "5.3.1. Results", "content": "To report the inter-annotator agreement, we utilized Fleiss' kappa, a metric assessing the reliability of agreement between raters\u00b2 (see Figure 5). We observed a high inter-annotator agreement of \u2265 0.54 for BERTBASE and SBERT on the CLIMATE-FEVER dataset, and complete agreement for the MultiRC dataset. Interestingly, for the SST-2 dataset, we observed a low inter-rater agreement of -0.041 for the LSTM model. This was primarily due to the extraction of rules with extremely short sequences, such as \"n't \u2192 NEG\" by DISCO. Low Fleiss' \u043a among human evaluators on particular datasets and models indicates the subjective nature of distinguishing between \u201cright\u201d and \u201cwrong\u201d shortcuts in terms of semantic"}, {"title": "6. Conclusion", "content": "This paper introduces DISCO, a method designed to identify causal rules internalized by neural models in natural language tasks. DISCO produces a concise and statistically robust set of causal rules, enabling users to scrutinize and understand the underlying knowledge captured by the model. The intrinsic causal orientation of our approach ensures that the resultant rules are faithful to the inputs where they are applicable. We demonstrate the efficacy of DISCO by identifying shortcuts learned by prominent models, including BERTBASE, SBERT, and LSTM. Our approach not only reveals these shortcuts but also provides insights into the model's decision-making process. In essence, DISCO stands as an instrumental resource for those aiming to gain deeper insights into the interactive explainability of AI models."}, {"title": "7. Limitations", "content": "One limitation of our approach arises from the context selection when constructing the counterfactual. Reusing neutral contexts is a straightforward method to generate human-understandable replacements for counterfactual contexts. However, this strategy possesses three inherent limitations:\nFirst, the availability of context is constrained. We only employ contexts present in the training data, limiting the sampling space and potentially compromising the effectiveness of the do-operator. Furthermore, selecting neutral contexts further narrows the sampling space and may introduce discrepancies between the sampled contexts and the training contexts, affecting the data distribution.\nAdditionally, compared to related works like [19], we do not differentiate between \"spurious\" and \"genuine\" reasons for predictions. However, this distinction is of lesser concern as our objective is to identify globally overfit shortcut patterns within the model, rather than pinpointing specific reasons for individual predictions, nor do we care about their faithfulness.\nA third limitation concerns the experiments conducted. Although the theory and approach of our work do not require sequence continuity, all experiments are based on consecutive sequences. Exploring efficient methods to identify sequences with gaps or even more complex patterns remains a potential avenue for future research."}]}