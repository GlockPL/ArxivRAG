{"title": "Semantic-Driven Topic Modeling Using Transformer-Based Embeddings and Clustering Algorithms", "authors": ["Melkamu Abay Mershaa", "Mesay Gemeda yigezub", "Jugal Kalitaa"], "abstract": "Topic modeling is a powerful technique to discover hidden topics and patterns within a collection of documents without prior knowledge. Traditional topic modeling and clustering-based techniques encounter challenges in capturing contextual semantic information. This study introduces an innovative end-to-end semantic-driven topic modeling technique for the topic extraction process, utilizing advanced word and document embeddings combined with a powerful clustering algorithm. This semantic-driven approach represents a significant advancement in topic modeling methodologies. It leverages contextual semantic information to extract coherent and meaningful topics. Specifically, our model generates document embeddings using pre-trained transformer-based language models, reduces the dimensions of the embeddings, clusters the embeddings based on semantic similarity, and generates coherent topics for each cluster. Compared to ChatGPT and traditional topic modeling algorithms, our model provides more coherent and meaningful topics.\nKeywords: Topic Modeling, Semantic, Cluster, Transformer-Based Embeddings, Transformer, Topic Extraction, Semantic-Driven, Deep Learning, Natural Language Processing.", "sections": [{"title": "1. Introduction", "content": "Topic modeling is a powerful technique used to discover hidden topics or latent thematic patterns within a collection of documents without prior knowledge [1]. Topic modeling helps extract significant and meaningful topics from documents and provides valuable insights into the document's ideas. Topic modeling is essential in natural language processing and machine learning for reasons such as data exploration and understanding [2], document organization and summarization [3], information retrieval [4], recommendation systems, content analysis [5], market research and customer insights [6], and textual data preprocessing [7].\nTraditional topic modeling methods such as Latent Dirichlet Allocation (LDA) [8], Non-Negative Matrix Factorization (NMF) [9], Latent Semantic Analysis (LSA) [10], and some BERT-based topic models work based on the bag-of-words approach to extract topics. Due to reliance on the bag-of-words technique, they suffer from the limitation that they treat all words in isolation without considering contextual relevance and relationships of words to the document. Traditional and even some Transformer-based topic models [11] encounter challenges in contextual understanding at the topic extraction stage, potentially leading to less accurate and meaningful topic representations from the document collection.\nIn this study, we present a novel semantic-driven topic modeling approach that leverages the Transformer's ability to capture contextual information about words within the document throughout the end-to-end topic extraction process. We ensure that the model focuses only on the most relevant words within each document, disregarding non-relevant ones. This unique feature of our"}, {"title": null, "content": "model sets it apart from others and enhances its ability to extract accurate and meaningful topics for each group of documents. We hypothesize that a unique word with no contextual relevance to the document is not a good topic representative for that document. This enables the proposed model to extract more accurate and meaningful topics for each group of documents. To the best of our knowledge, this semantic-driven end-to-end topic extraction approach is our innovative work.\nOur model, designed with four layers, plays a pivotal role in utilizing the contextual information generated by Transformers for words and sentences from the given documents during topic extraction. This not only allows for a deeper understanding of documents but also significantly improves the quality of extracted topics. By combining these four layers and leveraging the power of Transformer's contextual embeddings, our model outperforms existing topic techniques such as LDA [8], Embedded Topic Model (ETM) [12], Correlated Topic Model (CTM) [13], and BERTopic [11]. Our work makes the following contributions.\n\u2022 Developing a novel semantic-driven topic modeling technique for an end-to-end topic extraction process.\n\u2022 We extract quality and coherent topics leveraging rich contextual information about word usage available within the document.\n\u2022 We further improve the model's performance by eliminating non-relevant topic representative words in a second layer of processing once again based on the contextual information.\nThe paper is organized as follows: a review of the most recent related works is presented in Section 2. The model architecture and functions of the components are discussed in Section 3. Section 4 covers the experimental setup, results, and analysis. Finally, the paper concludes with the findings in Section 5."}, {"title": "2. Related Works", "content": "The current state-of-the-art topic modeling methodologies can be classified into two main categories: probabilistic and embedding-based. Probabilistic models like LDA [8], NMF [9], LSA [10], and other variants of LDA work based on the statistical properties of data. However, these probabilistic models have a few limitations when using bag-of-words representation. The embedding-based models use text embeddings and can overcome the limitations of the traditional probabilistic-based models.\nIn recent years, topic modeling has shown improvement by exploiting the power of neural network models to enhance traditional techniques, resulting in improved performance and the ability to capture more complex relationships within large document collections [14] and [15]. The integration of word embeddings into classical probabilistic models has shown effective and promising topic representations [16] and [17]. There has been a substantial surge in the development of topic-modeling techniques, primarily focused on embedding-based models [12, 18, 19]. Embedding-based models have achieved good performance because of their capability to capture the contextual meaning and the semantic relationship among words in a document. Angelov (2020) introduced an advanced topic modeling approach that utilizes clusters of pre-trained word embeddings instead of traditional probabilistic topic model methods [20]. The authors achieved faster and more efficient topic extraction, generating promising results with accurate topics for each cluster. Bianchi et al. (2020) also demonstrated the utilization of word embeddings to enhance the topic extraction process [18]. They introduced a method that leverages contextualized document embeddings, resulting in improved topic quality and coherence. The study demonstrated that contextualized word embeddings produce more meaningful and coherent topic representations."}, {"title": null, "content": "Researchers have also used hybrid approaches in recent years, leading to remarkable improvements in topic extraction. Grootendors (2022) and Zhang et al. (2022) adopt an innovative approach that combines TF-IDF and word embeddings [11], [21]. This hybrid model uses BERT embeddings to group documents into distinct clusters and extract coherent and meaningful topics from each cluster based on TF-IDF scores.\nThe model proposed in this paper enhances the topic modeling process by leveraging contextual information from SBERT embeddings [22] of candidate topic words within each cluster [11]. Our new technique leverages an end-to-end semantic-driven approach using Sentence-BERT [22, 23] to generate better topic representations, outperforming TF-IDF, probabilistic, and other methods. This results in more coherent and meaningful topics for each cluster."}, {"title": "3. Model Architecture", "content": "The model we introduce has four modules: embedding, dimension reduction, clustering, and topic extraction."}, {"title": "3.1. Document Embedding", "content": "In this paper, a document refers to a unit of text that can be any piece of textual content ranging from a single phrase, sentence, paragraph, or a collection of these text units or documents. The initial task in the model is creating a sentence-level vector space representation. SentenceTransformer-BERT (SBERT) [22, 24] is used for this purpose. SBERT converts collections of documents into high-quality sentence embeddings in a dense vector space by leveraging the BERT pre-trained language model [25], which provides fixed-length vector representations. In this module, any other document embedding method can be employed if it produces better vector representations and improves the quality of document clustering. Since the clustering quality will improve as new and enhanced language models continue to emerge, the performance of the model will also improve; it is a potential benefit of our model."}, {"title": "3.2. Dimension Reduction", "content": "Studies have shown that the proximity to the nearest data point tends to approach the distance to the farthest data point when the dimensionality of data increases [26]. As a result, the hypothesis of spatial locality becomes poorly defined in high-dimensional space, leading to diminished differences between different distance measures. This high-dimensional Sentence BERT vector"}, {"title": null, "content": "space representation may challenge clustering algorithms [27]. Therefore, applying dimension reduction techniques is the straightforward solution for this high-dimensionality challenge to get a better clustering result [28]. We employed UMAP as a dimension reduction technique that shows remarkable improvements in clustering documents, providing a significant milestone for the overall topic extraction process [11]. We adjust UMAP's parameters, such as the number of neighbors and minimum distance, to balance the preservation of global and local structures. Furthermore, using some model explainability techniques may help to interpret UMAP output [29], which is not done in this study."}, {"title": "3.3. Document Clustering", "content": "Clustering is essential in our topic extraction process. We use reduced document embeddings, clustered based on semantic similarity, to identify and extract coherent and unique topics from a document collection. HDBSCAN is chosen for its robustness, scalability, and ability to find clusters of varying densities [30]. This method is particularly effective for diverse document structures and noisy data, providing hierarchical insights to uncover hidden topics and subtopics across the entire collection."}, {"title": "3.4. Topic Extraction", "content": "Topic modeling studies have demonstrated that the documents within a cluster exhibit a clear association with a specific topic [11]. However, it is essential to realize that the documents within a cluster may contain multiple topics and subtopics, indicating a certain level of topic diversity within clusters. Once the HDBSCAN clustering algorithm is applied and clusters are identified, the next step is detecting topic words for each cluster, building a vocabulary, and extracting topics, which involves a few steps. First, to build a vocabulary for each cluster, sentences within each cluster are split into individual words, and these words are mapped to their corresponding contextual embedding values, helping eliminate topic-representative words that do not have any semantic contribution to the sentence. Secondly, unique candidate words are extracted from each sentence, and an independent vocabulary is constructed for each cluster. Subsequently, contextually non-relevant unique words are eliminated from each vocabulary, resulting in a vocabulary composed of unique words associated with their embeddings. In the third step, the average semantic similarity of each unique word within the cluster is computed by comparing it with each sentence's semantic information. This process provides an average of representative semantic similarity values for each topic word in that cluster (Equation 1). A cluster consists of a collection of n unique words, represented as vocabulary W, accompanied by a set of N contextually similar sentences denoted as S. To determine the representativeness of each word within the cluster, we calculate the average similarity between each word and all the sentences in the cluster; we can use cosine/Jaccard/Euclidea similarity measurement, defined by:\n\n$\\text{ave cos sim}(w_i) = \\frac{1}{N} \\sum_{j=1}^{N} \\text{cos}(w_i, s_j)$\n\nwhere, $w_i$ is the embedding vector of the $i^{th}$ word in the vocabulary W and $s_j$ is the embedding vector of the $j^{th}$ sentence in the set S.\nThe candidate topic words are organized and sorted based on the average semantic similarity values. The top k words are selected from each cluster. This process enables the extraction of topics from each cluster with enhanced accuracy and relevance of topic words specific to that cluster. After the topics are extracted, it is essential to consider how much each topic differs from others. Hence, we"}, {"title": null, "content": "merge the least ranked topic with its most similar counterparts through an iterative process using similarity measures. This iterative process helps reduce the number of topics to a user-specified value. Algorithm 1 presents a high-level overview of our model."}, {"title": "4. Experiments and Results", "content": "In this section, we briefly discuss the experimental setup, including details about the dataset and preprocessing procedures, the model evaluation metrics employed, the performance and results of our proposed model, and the results of various model comparisons."}, {"title": "4.1. Experiment setup", "content": "We used all-MiniLML6-v2 (MiniLM) and all-mpnet-base-v2 (MPNET), two different SBERT models, in the experiments to encode documents [22]. OCTIS (Optimizing and Comparing Topic Models is Simple) is an open-source Python package designed to help optimize and compare topic models [15, 31]. It comprises a suite of tools and metrics, including topic coherence. We utilized OCTIS to conduct the model comparison experiment and validation process."}, {"title": "4.2. Datasets", "content": "The 20NewsGroups, BBC News, and Trump's tweets datasets are used to validate our model. The 20NewsGroups dataset comprises 16,309 news articles categorized into 20 different groups [32]. The BBC News dataset contains 2,225 documents, categorized into four distinct classes, from the BBC News website between 2004 and 2005 [33]. The 20newsgroup and BBC News datasets are a collection of short and long texts. We used Trump's tweets to represent more recent and short textual data [11]. Trump's collection of tweets contains 44,253 tweets between 2009 and 2021. All these datasets are retrieved from the Kaggle repository."}, {"title": "4.3. Model Evaluation", "content": "Widely accepted and easily computable topic coherence measures, such as Cv, Cnpmi, UMass, and Cuci, are used to evaluate the interpretability of topics.\n1) C_V Coherence: The C_V coherence metric evaluates the coherence and interpretability of topics based on context vectors instead of relying on the co-occurrence frequency of words [34]. These context vectors calculate the Normalized Pointwise Mutual Information (NPMI) between a chosen word and the frequency counts of the top topic words within the vector. The C_V topic coherence measure correlates well with human judgment [34]. A C_V score of 1 indicates perfect coherence, whereas 0 indicates no coherence.\n2) C_npmi: C_npmi (Normalized Pointwise Mutual Information coherence) works by analyzing the semantic relationships between words within a topic [35]. It computes NPMI between pairs of words in each topic, measuring how strongly they are correlated with each other. C_npmi overcomes the limitation of Cuci by replacing PMI with normalized PMI. The C_npmi measure correlates better with human judgment [36].\nC_npmi scores typically range from -1 to 1, where a score of 1 indicates perfect coherence.\nC_uci [36] and U_mass [37] measure topic coherence by observing how topic words co-occur within a topic in a reference corpus of text data. They do not depend on any other word embeddings or complex statistics like C_npmi and C_V. High Cuci and U_Mass scores indicate that the words within a topic are more coherent and have a higher likelihood of co-occurring together.\nWe computed the coherence of each topic separately, and each cluster-based topic showed an excellent coherence score. These individual scores indicate that the top k words in each topic have a stronger semantic relationship and a high probability of co-occurring within the given topic's context. The overall topic coherence score is computed by averaging these individual topic coherence scores. Topic Coherence (TC) is computed for each topic model, varying the number of topics from 10 to 50 with increments of 10. We averaged the outputs from three separate runs at each interval to enhance consistency, resulting in an average score derived from a cumulative total of 15 distinct runs using fixed parameters for HDBSCAN and UMAP."}, {"title": "4.4. Model Comparison", "content": "We compare our model with the existing traditional topic modeling approaches and ChatGPT."}, {"title": "4.4.1. Traditional Models", "content": "We conduct extensive performance comparisons between our proposed model and well-known, established models, including (LDA) [8] Latent Dirichlet Allocation, (CTM) Correlated Topic Model [13], ETM (Topic Modeling in Embedding Spaces) [12], and BERTopic [11].\nTopic Coherence is computed for each topic model, varying the number of topics from 10 to 50 with increments of 10. We averaged the outputs from three separate runs at each interval to enhance consistency, resulting in an average score derived from a cumulative total of 15 distinct runs."}, {"title": "4.4.2. ChatGPT", "content": "GPT, developed for various NLP tasks such as translation, language processing, and question-answering, is described in [38]. While GPT is not explicitly designed for topic modeling and lacks integrated topic modeling algorithms, ChatGPT can generate topics and explanations by leveraging the rich information base in its embedding space. We conducted extensive experiments through programming and conversation to compare our model with ChatGPT. We split a large dataset into smaller chunks to overcome the token limit, resulting in other challenges. First, we lose critical latent themes and patterns in the document. Second, ChatGPT is stateless; it does not remember past API interactions for each chunk, particularly in multi-turn conversations, and it is difficult to process sequential data. We broke down a similar section of the 20 newsgroup datasets into chunks and extracted one topic from each chunk. The topics generated in each chunk may not provide document-wise hidden themes and patterns. ChatGPT does not use any evaluation metrics like topic coherence and topic diversity to assess topic quality. ChatGPT generates granular topics that may need merging or splitting, but it lacks this capability. Our model allows easy topic refinement through adjustable parameters and hyperparameters.\nOur experiments revealed that while ChatGPT performs adequately for small-size input texts, it falls short for large datasets and measuring topic quality and scalability. Compared to our models, it lacks reliability, extendability, and security for sensitive information. These limitations highlight the importance of traditional algorithms and ChatGPT and the need for enhanced techniques in topic modeling."}, {"title": "4.5. Results", "content": "Our model consistently achieves high topic coherence scores across all datasets, as various metrics show. The model exhibits strong coherence scores when applied to preprocessed datasets. Experimental results demonstrate that our proposed model outperforms traditional and embedding-based methods, including LDA, ETM, CTM, and BERTopic. For a visual representation, Figure 1(a) displays the word embedding spaces of the input dataset in"}, {"title": "4.6. Model Performance", "content": "Our model exhibits several notable strengths compared to the other topic models we compared with this study. The utilization of end-to-end embedding approaches for topic modeling provides many advantages to our model. First, our model is adaptable to different language models since it depends on embedding spaces for clustering, enabling it to stay at the forefront of advances in embedding techniques, ensuring its continuous upgrading and scalability in line with the latest developments in the field. Second, the most significant strength lies in cluster-based vocabulary construction and contextual similarity computation. These processes leverage the inherent contextual similarity among words and sentences within clusters, empowering the model to generate coherent and meaningful topics consistently."}, {"title": "4.7. Discussion", "content": "We have presented a novel model, an unsupervised learning algorithm designed to discover topics within a semantic space that leverages the embedding of documents. We have demonstrated how the semantic vector space is used for the representation of topics, enabling the computation of topics by identifying dense regions of highly semantically similar documents. To understand"}, {"title": null, "content": "our model comprehensively, it is essential to understand the contextual importance of each word within a document and sentence from the Transformer model. The model centers on each word's and sentence's contextual meaning and contribution within its corresponding cluster or semantic space. These central concepts offer two main advantages to the model. Firstly, we employ Sentence-Transformer's word embedding values to extract topics based on the relevance of each word within its cluster using some similarity measure. Secondly, we exclude non-relevant words from the topic extraction process by utilizing similarity score values, enhancing the model's performance. HDBSCAN identifies highly semantically similar dense and sparse sentence areas in the sentence vector space on the UMAP dimensionally reduced sentence vector. Those semantically similar dense areas are where we are interested in finding the underlying topics. In our finding, sparse sentence areas are semantically less similar to each other and also to the dense sentence areas. These sparse areas are considered as noise, and no significant underlying topic exists, and we exclude them from the topic extraction process. The minimum cluster size is the most critical hyperparameter in HDBSCAN. In our experiments, we determined that a minimum cluster size of 10 returns the best results for 20 newsgroup and BBCNews datasets and 8 for Trump's Twitt dataset. We notice that larger values increase the likelihood of merging unrelated sentence clusters. Using cosine similarity, we computed the topics"}, {"title": null, "content": "for each identified dense area or cluster. Topics exhibiting high cosine similarity values, indicating close to 1, are considered highly similar. Depending on the desired level of reduction, the users can set a threshold similarity score for the user-specified values to their preferences. For example we can merge topics 7 and 17 into the 'hardware' category, and merge topics 16 and 18 into the 'software' category."}, {"title": "4.8. Limitation of the Study", "content": "Traditional topic modeling techniques depend on the frequency of words. Our semantic-driven topic modeling technique focuses on the meaning of words and documents instead of their surface characteristics, which is our study's greatest strength and new paradigm shift in the topic modeling study. Our model has a limitation in detecting latent subtopics. Latent subtopics are topics that are not directly stated but are suggested. For example, consider the customer feedback about the Apple Smartphone and the model identified explicit topics such as camera quality, screen size, battery life, storage, and processing speed. However, our model does not detect latent subtopics like the user's overall satisfaction. This subtopic identification is a common challenge for many topic modeling techniques and is an open research area."}, {"title": "5. Conclusion", "content": "We have introduced a novel approach to topic modeling that leverages the rich contextual information provided by transformer models to generate topics from a collection of documents. The model employs the SBERT to obtain sentence embeddings, reduces the dimensions of these sentence embeddings, identifies semantically similar dense sentence vector spaces using a density-based clustering algorithm, and extracts coherent topics that represent these semantically dense areas or clusters. Our experiments have shown that the proposed model achieves competitive results and performance compared to various existing models across different datasets."}]}