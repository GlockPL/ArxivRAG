{"title": "Reasoning based on symbolic and parametric knowledge bases: a survey", "authors": ["Mayi Xu", "Yunfeng Ning", "Yongqi Li", "Jianhao Chen", "Jintao Wen", "Yao Xiao", "Shen Zhou", "Birong Pan", "Zepeng Bao", "Xin Miao", "Hankun Kang", "Ke Sun", "Tieyun Qian"], "abstract": "Reasoning is fundamental to human intelligence, and critical for problem-solving, decision-making, and critical\nthinking. Reasoning refers to drawing new conclusions based on existing knowledge, which can support various applications like\nclinical diagnosis, basic education, and financial analysis. Though a good number of surveys have been proposed for reviewing\nreasoning-related methods, none of them has systematically investigated these methods from the viewpoint of their dependent\nknowledge base. Both the scenarios to which the knowledge bases are applied and their storage formats are significantly different.\nHence, investigating reasoning methods from the knowledge base perspective helps us better understand the challenges and future\ndirections. To fill this gap, this paper first classifies the knowledge base into symbolic and parametric ones. The former explicitly\nstores information in human-readable symbols, and the latter implicitly encodes knowledge within parameters. Then, we provide\na comprehensive overview of reasoning methods using symbolic knowledge bases, parametric knowledge bases, and both of them.\nFinally, we identify the future direction toward enhancing reasoning capabilities to bridge the gap between human and machine\nintelligence.", "sections": [{"title": "1 Introduction", "content": "Reasoning refers to inferring new conclusions from existing knowledge [1], which is fundamental to\nhuman intelligence and essential for complex tasks such as problem-solving, decision-making, and critical\nthinking. The cognitive process of reasoning involves using evidence, arguments, and logic to draw\nconclusions or make judgments [2], which can provide back-end support for various real-world applications,\nsuch as clinical diagnosis [3-5], basic education [6-8], and financial analysis [9-11]. Reasoning ability is\ncentral to human intelligence, yet modern natural language processing systems still struggle to reason\nbased on the information they are given or have already learned [12-15]. The study of reasoning is\nessential in fields like neurosciences [16], psychology [17], philosophy [18,19], and computer science [20],\nas it helps to narrow the gap between human and machine intelligence [15]. Hence, building an artificial\nintelligence system capable of reasoning is both the goal of the research community and the way to\nimprove the performance of complex applications [1].\nWith the rapid development of reasoning technology, some surveys [1, 2, 15, 21-30] summarized the\nreasoning methods from different perspectives. For instance, there was a survey [1] researching reasoning\nusing natural language format, including classical logical reasoning, natural language inference, multi-\nhop question answering, and commonsense reasoning. A few of them [22-25] emphasized the reasoning\nbased on the structured facts in knowledge graphs (KGs), like temporal knowledge graph reasoning and\nmulti-modal knowledge graph reasoning. Some studies [26-30] paid attention to the knowledge sources\nthat the reasoning methods used for question answering, e.g., Wikidata KG and Wikipedia corpus. More\nrecent surveys [2, 15, 21] summarized the reasoning methods by prompting large language models, such\nas chain-of-thought series and self-reflection series methods."}, {"title": "2 Background", "content": "In this section, we first introduce the concepts of symbolic and parametric knowledge bases. In artificial\nintelligence, the symbolic knowledge bases and parametric knowledge bases represent different paradigms\nof knowledge representations that align with symbolism [32] and connectionism [33], respectively. The\nsymbolic knowledge base involves explicit knowledge and logical structures for reasoning. It is also\nfundamental to symbolic AI, which focuses on rule-based manipulation of symbols [34]. In contrast, the\nparametric knowledge base is associated with connectionism, where neural networks capture knowledge\nimplicitly through learned parameters, emphasizing adaptability and pattern recognition [35]. Finally,\nwe will introduce the taxonomy of reasoning in detail."}, {"title": "2.1 Symbolic knowledge bases", "content": "The symbolic knowledge bases contain KGs, tables, and text, the first two are structured and the\nlast one is unstructured. The structured KGs can be partitioned into static knowledge graphs (SKGs),\ntemporal knowledge graphs (TKGs), and multi-modal knowledge graphs (MMKGs). The following part\nof this subsection introduce the details of these symbolic knowledge bases.\nStatic knowledge graph: As shown in Figure 2 (a), an SKG is a structured semantic knowledge\nbase that can express various associations between entities in a graphical manner [36]. An SKG contains\nmany factual triplets (h, r, t), where h, r, and t represent the head entity, the relation, and the tail entity,\nrespectively. For example, (Microsoft, located within, Washington State) represents \u201cMicrosoft is located\nwithin Washington State\". An SKG is represented as (E, R, F), where E, R, and F denote the entity\nset, the relation set, and the triplet set {(h, r, t)} \u2286 E \u00d7 R \u00d7 E, respectively.\nTemporal knowledge graph: As shown in Figure 2 (b), TKGs can store temporal information such\nas events that evolve over time. The quadruples (h, r, t, time) are the basic unit of TKGs, where h, r,\nt, and time represent the head entity, the relation, the tail entity, and the timestamp, respectively. For\nexample, the quadruple (Apple Inc., product, iPad, 2010-01-27) means \"On January 27, 2012, Apple\nInc. produced iPad\". A TKG is represented as (E, R, T, F), where E, R, T, F denote the entity set,\nrelation set, the timestamp set, and the quadruple set {(h, r, t, time)} \u2286 E \u00d7 R \u00d7 E \u00d7 T, respectively.\nAnother representation way of TKGs is (Gt1, Gt2, ..., Gtn), where Gti denotes the SKG containing all\ntriplets happened in timestamp ti.\nMulti-modal knowledge graph: The MMKG integrates various multi-modal data into one SKG,\nsuch as text, images, and audio. As shown in Figure 2 (c) and (d), MMKGs are generally divided into\ntwo types [36]. One represents multi-modal data as new entities, and the other one describes them as\nentities' attributes.\nStructured table: As shown in Figure 2 (e), a structured table contains n records and m attributes.\nEach record can be represented as a vector ri = (ai1, Ai2,\u2026,aim), where aij denotes the value of the\nj-th attribute in the i-th record. The entire table can be viewed as a set of n records {r1,r2,\u2026,rn}.\nThe format of table makes data easy to store, manage, and analyze [37]. Hence, it is widely used in\nvarious fields such as financial statements, scientific research data, and inventory management [38,39].\nUnstructured text: As shown in Figure 2 (f), unstructured text primarily refers the text information\nwithout a standardized format or organization, such as books and articles. Unstructured text can also\nstore knowledge and be widely used in various applications with the help of current large language models."}, {"title": "2.2 Parametric knowledge bases", "content": "Parametric knowledge bases mainly refer to pre-trained language models (PLMs). PLMs are pre-\ntrained on large-scale text corpora via self-supervised learning and store generalized knowledge in pa-"}, {"title": "2.3 Taxonomy of reasoning", "content": "From the perspective of the dependent knowledge bases, reasoning methods can be divided into three\ncategories: reasoning methods based on symbolic knowledge bases, reasoning methods based on paramet-\nric knowledge bases, and collaborative reasoning methods based on symbolic and parametric knowledge\nbases."}, {"title": "3 Reasoning based on symbolic knowledge base", "content": "In this section, we investigate the reasoning methods based on symbolic knowledge bases. Based on\nthe structural types of KGs, we investigate static knowledge graph reasoning, temporal knowledge graph\nreasoning, and multi-modal knowledge graph reasoning. The overall taxonomy of reasoning methods\nbased on symbolic knowledge bases is shown in Figure 4."}, {"title": "3.1 Static knowledge graph reasoning", "content": "Static knowledge graph reasoning refers to the completion of incomplete triplets (incomplete knowledge)\nbased on the given fact triplets (existing knowledge) in the SKG, thereby obtaining new complete factual\ntriplets (new knowledge). According to the differences in query form and output form, static knowledge\ngraph reasoning tasks can be divided into three sub-tasks: traditional SKG reasoning, multi-hop SKG\nreasoning, and SKG complex logical query answering. In general, multi-hop SKG reasoning requires\nproviding an explicit reasoning path while completing the triplets. SKG complex logical query answering\nrequires modeling the complex logical symbols."}, {"title": "3.1.1 Traditional SKG reasoning", "content": "Task definition: Given the query (h,r,?), (?,r,t), or (h,?, t), traditional SKG reasoning methods\naim to predict the missing entity or relation directly. Generally, the traditional SKG reasoning methods\nfirst learn the representations of relations and entities in SKGs. Then, it constructs a scoring function\nto calculate the validity of possible triples. According to the representation approaches, traditional SKG\nreasoning methods can be categorized into three types: translation-based, tensor decompositional, and\nneural-based [36,163]."}, {"title": "3.1.1.1 Translation-based methods", "content": "Translation-based methods map the entities and relations to a vector space, where entities with similar\nsemantics are close in the distance. The role of the relation is to project the head entity representation\nonto the tail entity representation. A common approach is to represent relations as translation vectors\nfrom the head entity to the tail entity.\nThe first translation-based method TransE [74] regards the relation as a translation operation, that is,\nh+r\u2248t, where h, r, and t belong to the same vector feature space. However, it cannot handle certain\nspecific relations, such as one-to-many, many-to-one, symmetric, and transitive relations. To adapt to the\nmultiple meanings of entities and relations, TransH [82] interprets relations as transformation operations\non hyperplanes, TransR [83] models different atoms in the SKG in different vector spaces with the\nprojection matrix M, for each relation r, and TransD [84] dynamically constructs mapping matrices for"}, {"title": "3.1.1.2 Tensor decompositional methods", "content": "Tensor decompositional methods focus on capturing the pairwise interaction between atoms in an SKG\nto exploit the similarity of the latent features. For example, RESCAL [88] models entities as vectors and\nrelations as matrices to capture the interaction amounts between the elements at corresponding positions\nof the head and tail entity vectors. However, RESCAL is high in computational complexity and cannot\nmodel asymmetric relations. Therefore, many methods like DistMult [89], SimplE [90], Complex [91],\nQuatE [92], TuckER [93] utilize tensor decomposition to better model the semantic similarity of triples\nand reduce the computational complexity of the model."}, {"title": "3.1.1.3 Neural-based methods", "content": "Translation-based and tensor decompositional methods create a vector or matrix for every entity\nand relation, which requires large storage space for large-scale SKGs. To address the challenge, neural\nnetwork methods utilize multi-layer neural networks to learn the representations of entities and relations,\nwhich avoids the storage of vectors or matrices. For example, ConvE [94] presents a simple multi-layer\nconvolutional architecture. Inputting the head entity and relation, the encoder of ConvE calculates their\ndeep features, and the decoder of ConvE maps them into the entity space to predict the target tail entity.\nBesides, since graph neural network (GNN) [164] has proven to be effective in mining the structural\ninformation of graph, it is widely applied for traditional SKG reasoning. For example, RGCN [95]\nencodes entities by aggregating neighbor information instead of optimizing with a single entity vector,\nresulting in significant performance improvements."}, {"title": "3.1.2 Multi-hop SKG reasoning", "content": "Task definition: Given a query (h, r, ?), multi-hop SKG reasoning methods aim to predict the target\ntail entity t through a n-hop reasoning path 7: h \"\u2192 e\u2081 \"2> e2... \"n> en, where e\u00a1 and ri represent\nthe entity and the relation in the path 7. The last entity en in Tis treated as the predicted target tail\nentity t. Compared to traditional SKG reasoning methods, multi-hop SKG reasoning methods provide\nbetter better interpretability. The multi-hop SKG reasoning methods can be categorized into rule-based,\nreinforcement learning-based (RL-based), and generation-based."}, {"title": "3.1.2.1 Rule-based methods", "content": "Rule-based methods automatically induce logical rules from SKG and predict missing entities or\nrelations by matching queries to the rules, typically without model training. The rules exist in the form\nof symbolic chains, such as mother_of(m, c) \u2227 married_to(m, f) \u21d2 father_of (f, c), in which some facts\ncan be inferred from other facts. Some rule-based methods such as AMIE [96], RuleN [97], AnyBURL [98],\nLoGRe [99] aim to efficiently mine and utilize rules in KGs. Although these methods achieve remarkable\nperformance, they are hard to generalize in practice due to the limitation of symbolic representation."}, {"title": "3.1.2.2 Reinforcement learning-based methods", "content": "RL-based methods model the multi-hop reasoning process as a markov decision process (MDP), where\nthe agent walks on SKG to search the target entities. DeepPath [100] first adopts the RL framework\nto search the reasoning paths and target relations given to the head and tail entities. MINERVA [101]\nproposes a more complex and practical scenario to find the target tail entities given the relations and\nhead entities. Following MINERVA, most RL-based methods are devoted to tackling the sparse rewards\nproblem and trying to design a more efficient policy network in incomplete SKG. For instance, Multi-\nHopKG [102] is one of the first to use embedding models to estimate the rewards of unobserved targets,\nthereby reducing the impact of incorrect negative samples. Similar methods include DacKGR [103],\nExKGR [104], RARL [105], and RuleGuider [106].\nMoreover, several RL-based methods have integrated meta-learning to address the decreased reasoning\ncapability of reinforcement learning in few-shot relation scenarios. For instance, Meta-KGR [107] is the"}, {"title": "3.1.2.3 Generation-based methods", "content": "Generation-based methods adopt a generative framework to generate the reasoning paths step by step.\nFor instance, SQUIRE [112] utilizes an encoder-decoder model to translate the query to a reasoning path.\nBy leveraging the rule-enhanced and iterative training strategy, SQUIRE performs better than rule-based\nand RL-based methods. In cold-start multi-hop reasoning, the model always lacks precise guidance and\nexplicit paths. To overcome these challenges, SelfHier [113] designs an effective generation-based model\nto explore the reasoning paths by hierarchical guidance and self-verification strategies. Overall, the\ngeneration-based methods not only obtain high performance but also has low time complexity."}, {"title": "3.1.3 SKG complex logical query answering", "content": "Task definition: Given an incomplete first-order logical (FOL) query, SKG complex logical query an-\nswering methods aim to predict an target entity set. For instance, the FOL query C?. \u2203P : assoc(d1, P) ^\nassoc(d2, P)\u0245target(P, C?) means that \u201cidentify potential drugs C? that can act on proteins P associated\nwith the disease d\u2081 and d2\". SKG complex logical query answering methods first transform the given\nFOL query into a region in embedding space. Then, they output the entities within the region as the\ntarget entity set. Based on the difference of embedding space, the SKG complex logical query answering\nmethods can be categorized into geometric-based, probability-based, and neural-based.\""}, {"title": "3.1.3.1 Geometric-based methods", "content": "Geometric-based methods transform queries into geometric regions with clear boundaries. The com-\nposition and transformation of queries align with the composition and transformation of their geometric\nregions. For example, GQE [114] proposes the geometric projection operator and the geometric intersec-\ntion operator, which embeds basic query into a single point and combines basic queries into complex FOL\nquery, respectively. Query2Box [115] effectively models query graph embeddings through hypergeometric\nbox embeddings, where the query is represented as a box using the center and the boundary offset. All\nentities that fall within the box region of the query are considered the target entity set. In addition, Hy-\npeE [116] learns representations of entities and relations as hyperboloids in a Poincar\u00e9 ball. ConE [117]\nrepresents entities and queries as Cartesian products of two-dimensional cones."}, {"title": "3.1.3.2 Probability-based methods", "content": "Geometric-based methods necessitate that target entities strictly fall within the region defined by the\nquery. However, this requirement often fails to reflect real-world scenarios where entities exhibit semantic\ndiversity. Probability-based methods address this issue by modeling queries and entities as more flexible\nprobability distributions. For example, BetaE [118] uses probabilistic distributions with bounded support,\nspecifically the Beta distribution, and embeds queries/entities as distributions, which allows it to faithfully\nmodel uncertainty. Similarly, GammaE [119] utilizes Gamma distribution to capture more features of\nentities and queries. Based on the linear property and strong boundary support of Gamma distribution,\nGammaE effectively avoids generating ambiguous answers."}, {"title": "3.1.3.3 Neural-based methods", "content": "Neural-based methods model the correspondence between queries and entities using neural network.\nFor example, NNMLR [120] uses multilayer perceptron (MLP) and MLP-Mixer to model the basic queries\nand their combinations. It computes the distance between the query and the entities to rank the answers.\nSimilar methods include EmQL [121], CQD [122], GNN-QE [123], FuzzQE [124], Var2Vec [125], and\nTEMP [126]."}, {"title": "3.2 Temporal knowledge graph reasoning", "content": "The objective of temporal knowledge graph reasoning is to leverage existing events and knowledge to\nreason about unseen events or predict future events. Previous research [36] primarily categorizes reasoning\ntasks into two scenarios: interpolation and extrapolation, depending on whether the model has seen the\ntimestamps in the query. From a reasoning perspective, interpolation generally completes missing facts\nby analyzing known knowledge in TKGs, while extrapolation focuses on predicting unknown events by\nlearning embeddings of entities and relations from historical facts on continuous TKGs."}, {"title": "3.2.1 Interpolation-based TKG reasoning", "content": "Task definition: Given a TKG with facts from time to timer, the Interpolation-based TKG rea-\nsoning method aims to complete missing quadruple (h,r,?,timei) or (?,r,t,time\u017c) in history (timeo <\ntime; < timer). Interpolation-based TKG reasoning methods can be divided into dependent-based,\nfunction-based, and neural-based."}, {"title": "3.2.1.1 Dependent-based methods", "content": "Dependent-based methods generally do not involve direct manipulation of timestamps. Instead, they\nassociate each timestamp with the relevant entity or relation, capturing the evolution of entities or rela-\ntions over time. For example, TTransE [75] extends the traditional TransE [74] model by jointly encoding\nrelations and timestamps within a unified space. Building on TTransE, ST-TransE [127] introduces a\nspecialized time embedding method that constrains the representation learning of entities and relations.\nHowever, TTransE and ST-TransE struggle to manage evolving facts over time effectively. In contrast,\nT-SimplE [128] leverages a fourth-order tensor to model interactions within quadruples, improving its\nability to capture temporal associations."}, {"title": "3.2.1.2 Function-based methods", "content": "Function-based methods use specialized functions to learn embeddings for timestamps or model the\ntemporal evolution of entities and relations. Specifically, BoxTE [129] extends the static BoxE [166]\nmodel by incorporating temporal information through a relation-specific transfer matrix, facilitating the\nexploration of more complex inference patterns over time. ChronoR [130] associates timestamps with\nrelations, considering each relation-timestamp pair as a rotation that maps the head entity to the tail\nentity. TComplEx and TNTComplEx [131] extend the third-order tensor to a fourth-order tensor in\ncomplex space for reasoning. Notably, TNTComplex assumes that certain facts remain static over time,\nseparating the TKG into temporal and non-temporal components. Similarly, TeRo [132] incorporates\ntimestamps into the embeddings of head and tail entities in complex space to capture their temporal\nevolution, and represents the relation as a rotation that maps the head entity to the tail entity."}, {"title": "3.2.1.3 Neural-based methods", "content": "Neural-based methods typically use convolutional neural networks (CNNs) or long short-term memory\n(LSTM) networks to encode timestamps. These encoded timestamps help model the evolution of entities\nand relations by capturing their intrinsic correlations and temporal dependencies. For instance, TA-\nTRANSE [133] is a temporal-aware version of TrasnE [74]. It utilizes LSTM to learn time-aware represen-\ntations of relation, and represents quadruples as a set of triples in the form of (h, rseq, t), where rseq means\nrelation that may include temporal information with a temporal suffix. Similarly, TA-DISTMULT [133] is\na temporal extension of DistMult [89], considering the relation with temporal information as a sequence.\nAdditionally, HyTE [134] is an extension of TransH [82], DE-SimplE [135] is an extension of SimplE [90].\nThese methods often consider temporal constraints to enhance temporal reasoning capabilities. For ex-"}, {"title": "3.2.2 Extrapolation-based TKG reasoning", "content": "Task definition: Given a TKG with facts from timeo, to timer, the Extrapolation-based TKG rea-\nsoning method aims to predict unknown facts (h,r,?, timej) or (?,r,t,time;) that occur in the future\n(time; > time\u0442). Extrapolation-based TKG reasoning methods can be categorized into RL-based, rule-\nguided, and GNN-based."}, {"title": "3.2.2.1 Reinforcement learning-based methods", "content": "RL-based methods model path reasoning process through a reinforcement learning framework. For\ninstance, TPath [137] adds the time information as a separate vector to participate in the iteration of\nenvironment and agent. TAgent [138] filters the embeddings of candidate actions through a novel gate\nmechanism based on temporal information to capture temporal evolutionary patterns. TITer [139] defines\na relative time encoding function to capture the information of timestamps and designs a time-shaped\nreward based on the Dirichlet distribution to guide the model's learning. CluSTeR [140] proposes a beam\nsearch strategy to elicit multiple clues from historical facts and uses graph convolutional networks to\ndeduce answers from the clues."}, {"title": "3.2.2.2 Rule-guided methods", "content": "Rule-guided methods derive some temporal logic rules from TKGs and utilize them to predict future\nfacts. The rules provide a structured framework to infer logical conclusions, thereby generating more\naccurate predictions about future states or events. For example, TLogic [141] automatically mines cyclic\ntemporal logical rules by extracting temporal random walks from the graph. LCGE [142] mines the\ntemporal rules with several time constraint patterns to construct a rule-guided predicate embedding reg-\nularization strategy for learning the causality among events. Rule-guided methods can also be integrated\nwith RL-based methods to reduce semantic noise during reasoning and enhance the stability of the model.\nFor example, TPRG [143] proposes a similar concept of temporal rules and has made improvements based\non TPath [137], achieving certain improvements. DREAM [144] proposes a reinforcement learning frame-\nwork where the agent can receive adaptive rewards by imitating demonstrations at both the semantic\nand rule levels to eliminate the issue of sparse rewards."}, {"title": "3.2.2.3 GNN-based methods", "content": "Recent advancements in temporal knowledge graphs have leveraged GNNs to manage structural and\ntemporal dependencies. Know-Evolve [145] is a classic and the first temporal knowledge graph rea-\nsoning model that models the occurrence of facts (edges) as a multivariate point process over time,\nthereby learning non-linearly evolving entity representations with a deep recurrent network. As with\nSKGs, GCN [167] is beneficial for TKGR. For example, RE-NET [146] applies GCN [167] to interpret\nevent occurrences as sequences of subgraphs within TKGs, employing an autoregressive model with a\nneighborhood aggregation function to enhance interpretability. Both RE-GCN [147] and its advanced\nversion, the CEN [148], holistically treat the entire KG sequence to capture the evolutionary dynamics\nof entities and relations, effectively pinpointing local historical dependencies. Meanwhile, CyGNet [149]\nfocuses on identifying high-frequency entities by exploiting repetitive patterns in historical data using a\ncopy-generation network. On the other hand, CENET [150] differentiates between historical and non-\nhistorical dependencies to better identify entities suited for specific queries. While these models excel\nat capturing specific long-range facts, they often lack high-order connectivity information and dynamic\nsequential patterns required for a deeper understanding. To address these shortcomings, TiRGN [151]\ndevelop different structural encoders to capture sequential and recurring patterns within historical data.\nFurthermore, HGLS [152] designs a hierarchical graph framework to model long-term dependencies of\nentities across different timestamps."}, {"title": "3.3 Multi-modal knowledge graph reasoning", "content": "Task definition: The goal of MMKG reasoning methods is similar to SKG reasoning methods, which\naims to complete the triplet (h,r,t) when one of h, r, or t is missing. In particular, the entity (h and\nr) could be text or images or has attributes of text and images. The MMKG reasoning methods can be\ndivided into naive fusion MMKG reasoning and multi-modal pre-trained transformer-based (MPT-based)\nMMKG reasoning methods."}, {"title": "3.3.1 Naive fusion MMKG reasoning methods", "content": "Naive fusion MMKG reasoning methods evolve from traditional SKG reasoning methods. They concen-\ntrate on the efficient encoding and integration of multi-modal data. The representation of multi-modal\ndata is achieved by either combining each individual modality's representation within its own feature\nspace or by projecting different modal representations into a shared latent space.\nThe earliest work IKRL [153] uses a neural image encoder to construct representations for all im-\nages of an entity. Then, the multiple image representations of an entity are combined with the original\nstructure-based representations and trained like TransE [74], thereby learning multi-modal knowledge\nrepresentations for reasoning. Inspired by IKRL [153], a translation-based Visual and Linguistic Rep-\nresentation Model (VALR) [154] has been proposed, which defines the energy of the triple as the sum\nof sub-energy functions that leverage both visual, linguistic and structural representations. Similarly,\nTransAE [155] combines multi-modal autoencoder with TransE [74] model, where the hidden layer of the\nautoencoder is used to encode multi-modal data. MKBE [156] designs specialized encoding layers, scor-\ning modules, and decoding layers for data in different modals. THCR [157] complements the relational\nknowledge by learning a shared latent representation that integrates information across those modalities.\nMMKRL [158] designs a joint learning framework that can be easily extended to any modality and uses\nan adversarial strategy to enhance its robustness. RSME [159] automatically encourages or filters the\ninfluence of visual context to avoid encoding too much irrelevant information."}, {"title": "3.3.2 MPT-based MMKG reasoning methods", "content": "MPT-based MMKG reasoning methods use MPT to encode textual and visual features in a uni-\nfied architecture. Then, it employs graph encoders to integrate structural knowledge with multi-modal\nknowledge. For example, VBKGC [160] focuses on the co-design of the structural KG model and negative\nsampling. It consists of an encoding module with VisualBERT [168], a projection module, and a scoring\nmodule like TransE [74]. MKGformer [161] utilizes a hybrid transformer architecture with unified input-\noutput and reduces noise from irrelevant images/objects through token-level modal fusion. SGMPT [162]\ndesigns a structure-guided fusion module that uses weighted summation and alignment constraint to\ninject the structural information into both the textual and visual features."}, {"title": "3.4 Datasets", "content": "In this section, we have compiled statistics on some commonly-used datasets related to reasoning based\non symbolic knowledge bases, including (1) # Ent.: Entity number; (2) # Rel.: Relation number; (3) #\nTime.: Timestamp number; (4) # Facts: Fact number; (5) Type: Knowledge graph type. In particular,\nthe MMKG is represented by specific combinations of modalities. Taking \u201cKG+TXT+IMG\" for example,\n\"KG\" means the entity has a simple name or ID, \"TXT\" means the entity has a textual description as\nattributes, \"IMG\" means the entity has single or multiple corresponding images as attributes; (6) Domain:\nThe domain of knowledge stored in the KGs; (7) Source: The source of the KGs, and (8) Links: The\nstorage address of the KGs. The statistical results are shown in Table 1."}, {"title": "4 Reasoning based on parametric knowledge bases", "content": "Since reasoning methods based on parametric knowledge bases are often task-independent, this section\nreviews them from the perspective of their equipped techniques rather than tasks. These methods mainly\nperform reasoning in the form of question answering."}, {"title": "4.1 CoT-based reasoning", "content": "Recent studies [61, 203, 226] find that generating a series of intermediate reasoning steps (also known\nas CoT and rationale) significantly improves the ability of LLMs to perform complex reasoning. The\nintermediate reasoning steps of CoT-series methods contribute to enhancing the logical consistency of the\nreasoning processes before reaching a conclusion. In this way, CoT-series methods significantly improve\nthe LLMs' ability to perform tasks that require multi-step reasoning and deep understanding. There are\nthree types of CoT-series methods: CoT optimization, CoT engineering, and automatic CoT methods."}, {"title": "4.1.1 CoT optimization", "content": "CoT optimization methods construct the question-rationale pairs as demonstrations to guide the LLMs\nto reason step by step. Few-shot-cot [61] adopts some questions and manually constructs CoT as demon-\nstrations for the first time. Following this line, the CoT optimization methods try to optimize the CoT\nin demonstrations from different perspectives. For instance, Complex-cot [194] finds that demonstra-\ntions with higher reasoning complexity achieve substantially better performance on multi-step reasoning.\nHence, it constructs CoT with more reasoning steps in demonstrations. Inspired by how humans can"}, {"title": "4.1.2 CoT engineering", "content": "CoT engineering methods are devoted to designing more complex CoT reasoning procedures on the\ntop single reasoning process to help LLMs generate more accurate final answers. Inspired by cognitive\nscience, which characterizes problem-solving as a search through a combinatorial problem space, ToT [198]\nactively maintains a tree of thoughts, where each thought is a coherent language sequence that serves as\nan intermediate step toward problem-solving. GoT [199] models the information generated by LLMs as\nan arbitrary graph, where information units are vertices, and edges correspond to dependencies between\nthese vertices. GoT enables combining arbitrary LLM thoughts into synergistic outcomes, distilling\nthe essence of whole networks of thoughts, or enhancing thoughts using feedback loops. The information\nneeded to solve the task is not initially given in many reasoning-related applications. To enhance LLMs in\nactively seeking information, UoT [200] incentivizes a model to seek information in a way that maximally\nreduces the amount of information it does not know. To optimize the generated answer progressively,\nPHP [201] performs automatic multiple interactions between queries and LLMs by using previously\ngenerated answers as hints."}, {"title": "4.1.3 Automatic CoT", "content": "The aforementioned two types of methods achieve excellent performance, but they rely on manually\nconstructed demonstrations, which may generalize poorly between data from different domains. Hence,\nthe automatic CoT methods try to design general instructions to trigger multi-step reasoning or con-\nstruct pseudo-demonstrations to guide LLMs under a zero-shot setting. For instance, Zero-shot-cot [203", "Let's think step by step\" after question, which can ac-\ntivate the inherent multi-step reasoning capability of LLMs. To solve the problem that performance\ndrops significantly when no demonstration is available, Z-ICL [204": "constructs pseudo-demonstrations\nfrom a raw text corpus. It retrieves relevant text from the corpus using the nearest neighbor search"}]}