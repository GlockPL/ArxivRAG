{"title": "SMOSE: Sparse Mixture of Shallow Experts for Interpretable Reinforcement Learning in Continuous Control Tasks", "authors": ["M\u00e1ty\u00e1s Vincze", "Laura Ferrarotti", "Leonardo Lucio Custode", "Bruno Lepri", "Giovanni Iacca"], "abstract": "Continuous control tasks often involve high-dimensional, dynamic, and non-linear environments. State-of-the-art performance in these tasks is achieved through complex closed-box policies that are effective, but suffer from an inherent opacity. Interpretable policies, while generally underperforming compared to their closed-box counterparts, advantageously facilitate transparent decision-making within automated systems. Hence, their usage is often essential for diagnosing and mitigating errors, supporting ethical and legal accountability, and fostering trust among stakeholders. In this paper, we propose SMOSE, a novel method to train sparsely activated interpretable controllers, based on a top-1 Mixture-of-Experts architecture. SMOSE combines a set of interpretable decision-makers, trained to be experts in different basic skills, and an interpretable router that assigns tasks among the experts. The training is carried out via state-of-the-art Reinforcement Learning algorithms, exploiting load-balancing techniques to ensure fair expert usage. We then distill decision trees from the weights of the router, significantly improving the ease of interpretation. We evaluate SMOSE on six benchmark environments from MuJoCo: our method outperforms recent interpretable baselines and narrows the gap with non-interpretable state-of-the-art algorithms.", "sections": [{"title": "Introduction", "content": "Over the last decade, Artificial Intelligence (AI) has achieved dramatic success. However, the increasing adoption of AI systems in real-world use cases has been raising concerns related to the trustworthiness of these systems (Wexler 2017; McGough 2018; Varshney and Alemzadeh 2017; Rudin, Wang, and Coker 2019; Huang et al. 2020; Smyth et al. 2021; He 2021). One of the most promising paths toward trustworthy AI involves developing methods to enhance our understanding of AI decision-making processes. In this context, eXplainable AI (XAI) introduces Al systems enabling the generation of post-hoc explanations for their behavior (Dwivedi et al. 2023). Although such methods do provide insights into the decision-making processes, their post-hoc analysis explains just an approximation of the original closed-box behavior. This approximation cannot be trusted in high-stakes scenarios, which are frequent for instance in robot control, autonomous driving, emergency response, and public decision-making. Indeed, given that interpretable policies distilled for explanation are typically less complex than closed-box ones, they might require adopting entirely different strategies, disrupting the link between the original behavior and its interpretation (Rudin 2019). To overcome this issue, research efforts have been channeled towards Interpretable AI (IAI) (Rudin 2019; Akrour, Tateo, and Peters 2021). Interpretable AI focuses on systems whose decision-making process is inherently understandable to humans, without the need for distilled explanations (Arrieta et al. 2020). These models prioritize clarity and simplicity, enabling users to directly comprehend how inputs are transformed into outputs, thus facilitating transparency, trust, and validation ease (Rudin 2019). Due to its sequential nature, Reinforcement Learning (RL) benefits more from directly interpretable policies than post-hoc explanations (Rudin et al. 2021). Discrepancies between a policy and its interpretable approximation can grow over time due to state distribution drift, making local explanations less meaningful (Akrour, Tateo, and Peters 2021). Moreover, post hoc XAI methods, when used in RL to tackle goal misalignment problems, might mislead observers into believing that closed-box agents select the correct actions for the appropriate reasons, even though their decision-making process may actually be misaligned (Chan, Kong, and Liang 2022; Delfosse et al. 2024). This paper addresses the challenge of Interpretable Reinforcement Learning (IRL) in continuous action spaces. In particular, the main advances presented in this work can be summarized as follows:\n\u2022 We propose SMOSE, a novel, high-performing method, that exploits a sparse, interpretable Mixture-of-Experts (MoE) architecture. Our method simultaneously learns a set of simple yet effective continuous sub-policies, and an interpretable router. The sub-policies are sequentially selected by the router, one per each decision step, to control the system, based on the current state.\n\u2022 We showcase the capabilities of SMOSE through results obtained on six well-known continuous control benchmarks from the MuJoCo environment (Todorov, Erez, and Tassa 2012). For all the considered environments we analyze performance both in training and in evaluation.\n\u2022 We include a full interpretation for all the trained policies, demonstrating the effectiveness of SMOSE in providing extensive insight on the learned controllers. By combining router and the experts interpretation, we analyze the high-level and low-level alignment of the policy.\n\u2022 We compare SMOSE with interpretable and non-interpretable state-of-the-art RL models, considering both larger model size competitors, and models with comparable size (in terms of the number of active and overall parameters). Results highlight that the proposed architecture, while retaining interpretability, improves the performance w.r.t. other interpretable methods, tightening the gap with non-interpretable approaches.\nThe paper is structured as follows: the next two sections summarize the relevant background and related contributions, while the Methods section details SMOSE, our proposed approach. Then, the Results section presents the experimental setup and the performance achieved by our method, including the interpretation of the learned policies. Finally, we draw the conclusions and discuss the future directions of this work."}, {"title": "Background", "content": "As recently surveyed in (Glanois et al. 2024), several works study IRL models. Some approaches exploit neural logic-based policies to achieve interpretability (Jiang and Luo 2019; Kimura et al. 2021; Delfosse et al. 2023; Sha et al. 2024). Older approaches (McCallum 1996; Pyeatt, Howe et al. 2001) use existing methods for decision tree (DT) induction, adapting them to the RL domain. In (Roth et al. 2019), the authors introduce a heuristic to keep the size of the trees smaller while still achieving good performance. However, these algorithms suffer from the curse of dimensionality, i.e., they do not scale well with the dimensionality of the state space. More recent approaches address this issue. In (Silva et al. 2020), the authors employ soft DTs (Irsoy, Y\u0131ld\u0131z, and Alpayd\u0131n 2012) as policies for RL agents. This simplifies training and allows the use of widely known deep RL algorithms. However, soft trees are difficult to interpret, and discretizing them into \"hard\" DTs, the policies obtained can suffer from a significant loss in performance.\nOther approaches make use of evolutionary principles to optimize DTs. (Dhebar et al. 2020) propose a method for learning a non-linear DT from a neural network trained on the target environment. This allows for choosing the desired properties of the resulting DT (e.g., the depth). On the other hand, this methodology hinders online learning (and thus adaptation to novel scenarios). In (Custode and Iacca 2023), the authors combine evolutionary techniques with Q-Learning to produce DTs that can learn online, while still being interpretable. The DTs produced by this approach achieve performance comparable to non-interpretable state-of-the-art algorithms in a number of simple benchmarks. However, this approach has an extremely high computational cost, requiring a large number of interactions with the environment. Finally, in (Custode and Iacca 2024), the authors leverage principles from social learning to significantly improve both the computational complexity and the performance of evolutionary methods."}, {"title": "Related work", "content": "IRL methods tailored to environments with continuous action space are heavily needed in a wide variety of real-world scenarios, e.g., robot manipulation and control, as showcased by many benchmarking examples (Todorov, Erez, and Tassa 2012). So far, however, only a few works have investigated the use of IRL for continuous control. A branch of research is dedicated to the learning of interpretable programs as RL policies (Verma et al. 2019, 2021; Liu et al. 2023; Kohler et al. 2024). In (Custode and Iacca 2021) the authors propose a cooperative co-evolutionary approach in order to independently evolve a population of binary DTs (generated via Grammatical Evolution) and a population of sets of actions, both optimized w.r.t. the fitness associated with the combined use of the two. (Videau et al. 2022) explore methods for constructing symbolic RL controllers, utilizing parse trees and Linear Genetic Programming (LGP) to represent the programs as a vector of integers. Additionally, a multi-armed bandit strategy distributes the computational budget across generations. LGP is also used in (Nadizar, Medvet, and Wilson 2024), along with Cartesian Genetic Programming (CGP), where programs are instead represented as directed acyclic graphs. In (Paleja et al. 2023), an IRL algorithm for continuous control is introduced, exploiting fuzzy DTs combined with nodes and leaves with differentiable crispification, that can hence be directly learned via gradient descent. As previously mentioned, our method takes advantage of an interpretable MoE architecture for the control policy. MoEs can be found in RL literature, employed to tackle different problems, such as parameter scaling in deep RL (Obando-Ceron et al. 2024), handling of multiple optimal behaviors (Ren et al. 2021), and multi-task learning (Cheng et al. 2023; Willi et al. 2024), to name a few. In the realm of interpretability, a kernel-based method employing MoE is proposed in (Akrour, Tateo, and Peters 2021). In this work, the selection of a set of centroids from trajectory data is optimized. Each state is associated with an expert policy modeled as a Gaussian distribution around a linear policy, while retaining an internal complex function approximator. According to this approach, a learned combination of experts handles the control task. This is obtained by considering fuzzy memberships to clusters and employing a learned set of cluster weights. In our method, instead, we can tune the number of experts employed at every timestep for the decision, and, for instance, force the policy to exploit only one expert, for maximum interpretability (as explained in more detail in the Methods section). Moreover, while the policies in (Akrour, Tateo, and Peters 2021) are updated via approximate policy iteration, the centroids, which must be elements of the replay buffer, require separate iterations of discrete optimization. SMOSE, instead, learns a router that distributes the control tasks among experts. This compact representation, while being fully interpretable, can be learned via backpropagation, simultaneously to the experts."}, {"title": "Method", "content": "We seek to solve RL problems with continuous actions structured as Markov Decision Processes (MDPs), i.e., tu-\nples ($S, A, P, R, \u03b3, S_0$), where $S$ is the set of the states\nin the problem, $A \\in \\mathbb{R}^{n_a}$ is the set of (continuous) ac-\ntions, $P(s, a, s') : S \\times A \\times S \\rightarrow [0, 1]$ associates a\nprobability to each transition from $(s, a)$ to each state $s'$;\n$R(s,a,s') : S \\times A \\times S \\rightarrow \\mathbb{R}^+$ assigns a reward to\neach triplet $(s, a, s')$; $\\gamma$ is a discount factor, used for denoting the\nimportance of future rewards w.r.t. the current one, and $S_0$\nis the set of the initial states. Solving such a problem re-\nquires the design of a learning strategy to fit a policy func-\ntion $\\pi : S \\times A \\rightarrow [0, 1]$, optimizing:\n$\\underset{\\pi}{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t, s_{t+1}) \\mid s_0 \\sim S_0, s_{t+1} \\sim P_t, a_t \\sim \\pi_t\\right]$\nthrough interaction with the environment. Good results tack-\nling this problem in non-trivial environments are often\nachieved by complex, not directly interpretable policies.\nThe main idea behind our method is to decompose a com-\nplex behavior by identifying a set of basic skills (or decision\nstrategies) that are easier to interpret when combined with\na policy capable of selecting which skill to employ in each\nstate. Decomposing the decision process with this divide-\nand-conquer approach can provide insight into the interpre-\ntation of the learned controller's behavior. Thus, we struc-\nture the control policy as a composition of:\n\u2022 A set $\\{\\pi_m(\\cdot | \\theta_m) : S \\rightarrow A | m = 1, . . ., M\\}$ of $M$ pa-\nrameterized continuous policy functions, representing $M$\ndecision-makers, each trained to be expert in an individ-\nual useful basic skill. To ensure overall interpretability,\nwe consider continuous policies $\\pi_m$ that are shallow and\ndirectly interpretable, such as linear ones.\n\u2022 A planner, capable of assigning to each state $s \\in S$ an\nexpert policy among the available set, in order to attain\noptimal behavior. This module is a parameterized router\n$g(s|\\Theta) : S \\rightarrow [0, 1]^M$ producing a one-hot encoded\n$M$-dimensional vector as output.\nSMOSE, summarized in Figure 1, combines the experts\nand the router according to a MoE architecture of the form:\n$\\pi(s) = \\sum_{m=1}^{M} [g(s|\\Theta)]_m \\pi_m(s|\\theta_m),$\t\t\t\t(1)\nwhere $[g(s)]_m$ is the $m$-th component of vector:\n$g(s|\\Theta) = TOP_1 (softmax(\\hat{g}(s|\\Theta))).$\t\t\t\t\t(2)\nIn the equation above, we indicate as $\\hat{g}(s|\\Theta) \\in \\mathbb{R}^{M}$ the\ninner parameterization of the router that produces as output\nan $M$-dimensional vector. The softmax function then trans-\nforms $\\hat{g}(s)$ in a preference vector, where the $m$-th ele-\nment measures the preference in choosing the $m$-th expert\nas actor in state $s$. The function $TOP_1$ assigns value 1 to the\nvector component with maximum preference, and 0 to all\nthe others, allowing for an extremely sparse MoE structure\nin which only one expert is activated in each state.\nRemark. The proposed method can also accommodate the\nuse of $TOP_k$ with $k > 1$, replacing $TOP_1$ in Eq. (2) and\nthereby enabling the selection of a combination of $k$ experts\nat each timestep to make control decisions. While the result-\ning policy would remain interpretable, in this work we in-\ntentionally set $k = 1$ to evaluate the performance of a truly\nsparse MoE architecture and to maximize interpretability\nwithin this framework.\nThis architecture takes inspiration from (Shazeer et al.\n2017; Riquelme et al. 2021), where sparse MoE neural lay-\ners are stacked to obtain both computational efficiency in\ninference and parameters specialized on subsets of states.\nHere, such architecture is tuned to retain interpretability in\ncontinuous control, by removing the non-linearities and acti-\nvation functions, employing a single-layer shallow structure,\nand shaping the inner router $\\hat{g}$ and the experts $\\pi_m$ as linear\nfunctions, that is:\n$\\pi_m(s|\\theta_m) = \\theta_m\\cdot s$ and $\\hat{g}(s | \\Theta ) = \\Theta\\cdot s$\nwith $\\theta_m \\in \\mathbb{R}^{n_a \\times n_s}$ and $\\Theta \\in \\mathbb{R}^{M \\times n_s}$.\nThe control policy in Eq. (1) is trained via RL. The\nparameters characterizing $\\pi_m$ and $g$ are hence simultane-\nously learned. It is important to notice that composing the\n$TOP_1$ and $softmax$ functions in the order expressed in Eq.\n(2) permits a larger propagation of the gradients among\nthe router weights, if compared with the opposite order-\ning. At every update, indeed, each of the $M$ components\nof $softmax(\\hat{g}(s |\\Theta))$ depends on all the weights $\\Theta$, due to\nthe action of softmax. Hence, all the components in $\\Theta$ will\nbe updated, not only the ones associated with the expert se-\nlected by $TOP_1$ (Riquelme et al. 2021). We ensure to explore\nthe action space of each expert $\\pi_m$ by injecting stochasticity\nin the choices in training, i.e.,\n$\\pi_m(s|\\theta_m,\\sigma_m) = \\mathcal{N}(\\theta_m\\cdot s, \\sigma^2 I)$.\nOur method can be seamlessly integrated with any RL al-\ngorithm for the learning of continuous controllers. In this\nwork, we rely for exploration on Soft Actor-Critic (SAC)\n(Haarnoja et al. 2018), a state-of-the-art algorithm that bal-\nances the objective function with a term promoting higher\nentropy policies. We structure the actor module according to\nthe interpretable architecture in Eq. (1), while we maintain\nthe classic neural critic introduced in (Haarnoja et al. 2018).\nIn order to ensure balanced workloads among the M ex-\nperts, we augment the SAC actor objective function with\nadditional penalties introduced in (Riquelme et al. 2021),\nweighted by a tunable parameter $\\lambda$. In particular, per each\nmini-batch $S = \\{s_k\\} \\subseteq S$ of states, we consider its impor-\ntance for $m = 1, ..., M$:\n$Imp_m (S) = \\sum_{s_k \\in S} softmax(\\pi_m(s_k | \\theta_m, \\sigma_m ))$\nand we compute the importance loss:\n$f_{imp} (S) =  \\frac{ 1 }{ 2} \\frac{std(Imp(S)) } {mean(Imp(S)) } $;\t\t\t(3)\nThen, we consider the load of $S$ for $m = 1,..., M$:\n$Load_m (S) = \\sum_{s_k \\in S} \\mathbb{P}(noise \\ge \\tau(s_k) - \\pi_m (s_k | \\Theta_m, \\sigma_m))$\nwith $\\tau(s_k) = \\underset{m}{max}(\\pi_m(s_k | \\Theta_m, \\sigma_m))$, and compute the\nload-balancing loss:\n$f_{load} (S) =  \\frac{ 1 }{ 2} \\frac{std(Load(S)) } {mean(Load(S)) } $.           (4)\nWhen computing the load-balancing loss on each mini-batch\n$S$, noise to the inner router, i.e., $\\hat{g}( s | \\Theta) = \\Theta \\cdot s + \\epsilon$, with\n$\\epsilon \\sim \\mathcal{N}(0, 1/M^2)$, in order to ensure proper exploration\nof experts employment. Finally, once the policy training is\ncomplete, we produce a useful support for interpretation by\ndistilling a multiclass classifier using Decision Trees (DTs),\nwhich are trained on a router-labeled replay buffer. By de-\ncomposing the original router into M binary classifiers of\nlimited depth, we create an easily readable representation\nof the interpretable router's decision-making process, where\neach DT determines whether a specific expert should control\nthe task for a given state. After balancing the data, we train\nthe DTs in a supervised manner, using the CART algorithm\n(Timofeev 2004). More details on this can be found in the\nAppendix."}, {"title": "Results", "content": "In this section, we present the evaluation of SMOSE on six\nwidely-known continuous-control environments from Mu-\nJoCo (Todorov, Erez, and Tassa 2012), namely Walker2d-\nv4, Hopper-v4, Ant-v4, HalfCheetah-v4, Reacher-v4, and\nSwimmer-v4 (described in Appendix), which are commonly\nused as benchmarks in the RL field. The following sub-\nsections detail the experimental setup, performance eval-\nuation, and comparative analysis with interpretable and\nnon-interpretable baselines, highlighting the effectiveness of\nSMOSE in delivering both interpretability and high perfor-\nmance. As it is common in RL literature, performance is\nmeasured considering the attained episodic rewards (ER),\ni.e., the accumulated rewards achieved at each timestep of\nan episode, in order to compare well with the state of the art.\nTraining performance\nIn this section, we include the results in terms of perfor-\nmance achieved in training by SMOSE, tuned with $M = 8$\nexperts, and weighing the load-balancing losses in Eq.s (3)-\n(4) with $\\lambda = 0.1$. The value for the $M$ parameter has been\nempirically tuned to strike a balance between agents' per-\nformance and easiness of interpretation, as shown in the ab-\nlation study included in Appendix. The weight $\\lambda$ was tuned\nbetween 0.01 and 1.0 on a three-dimensional grid, to ensure\nfair load and reduce expert collapse. Details on the computa-\ntional setup are available in the Appendix. To achieve statis-\ntical reliability in our results, we perform $N_{train} = 10$ inde-\npendent training runs, seeded from 0 to 9. Every training run\nconsists of a sequence of episodes with a maximum horizon\nof $H = 1000$ interactions with the environment, which is\nachieved if the controller does not incur catastrophic failure.\nWe train for a total of one million environmental interac-\ntions (i.e., timesteps), to be comparable with the closed-box"}, {"title": "Policy interpretation", "content": "This section includes the interpretation of the best policy learned for the Reacher-v4 environment. We include interpretations for the other five environments in the Appendix. Figure 3 presents a graphical representation of the weights of the MoE policy, including both the router and the experts. Details on both are included in the following, but we can already notice from the figure that the controller employs almost exclusively a small subset of variables:\n\u2022 coordinates of the target ($T$): $x_T$, $y_T$\n\u2022 coordinates' difference between the fingertip ($f$) and $T$:\n$\\Delta x = x_f - x_T$, $ \\Delta y = y_f - y_T$\nThe two control variables, the torques applied to the first\nand second joint, are indicated as $\\tau_1$ and $\\tau_2$, respectively. We\nindicate as $S_m$ the score of the $m$-th expert, i.e., the \"weight\"\ncomputed by the corresponding column of the router.\nExpert 1 ($S_1 \\approx 2.7 y_T - 5.6 \\Delta y$) Expert 1 is called when\nits score $S_1$ is greater than all the other scores. Its policy can\nbe described as:\n$\\begin{cases}\n\\tau_1 \\approx x_T, \\\\\n\\tau_2 \\approx y_T -4\\Delta x.\n\\end{cases}$\t(5)\nAccording to this, the first joint is strongly accelerated in\na direction that is opposite to that of $x_y$, while the second\njoint's control signal is composed of two terms, one that\nmoves the joint in the opposite direction of $y_T$, and the other\nthat tries to minimize the distance on the x-axis between the\nfingertip and the target.\nExpert 2 ($S_2 \\approx -3.5 y_T + 8.3 \\Delta y$) It can be noted that\nExpert 2's score $S_2$ has opposite signs w.r.t. $S_1$, indicating\nthat these two experts are likely working in opposite states.\nIts policy can be described as follows:\n$\\begin{cases}\n\\tau_1 \\approx 3.1 y_T - 2.7 \\Delta x - 4.5 \\Delta y, \\\\\n\\tau_2 \\approx 2.8 y_T + 3.0 \\Delta x - 3.4 \\Delta y.\n\\end{cases}$\t\t\t\t (6)\nshows that the two joints' control signals have partially sim-\nilar behaviors. Indeed, both of them have a dependency on\n$y_T$, which can be seen as a feed-forward control scheme\ncombined with a feed-back control scheme (Tao, Kosut, and\nAral 1994). This is suggested by the presence, in both con-\ntrollers, of an amplified version of $y_T$, and a negative depen-\ndency on \\Delta y (please, note that in this environment \\Delta can\nbe seen as the opposite of the control error). We note, though, a\nstrong difference between the two joints' controllers: while\n$\\tau_1$ has a negative dependency on \\Delta x (suggesting that it tries\nto reach the target also on the x-axis), $\\tau_2$ shows the oppo-\nsite. Interestingly, the magnitude of the two contributions is\ncomparable, suggesting that these terms are used to balance\nthe fingertip by simultaneously moving the two joints.\nExpert 3 ($S_3 \\approx 3.9 y_T - 5.8 \\Delta y$) The score $S_3$ has similar\ncoefficients to $S_1$, but with higher weight to $y_T$, meaning\nthat Expert 3 is preferred over Expert 1 when $y_T$ has a high\nmagnitude. The policy of this expert can be summarized as:\n$\\begin{cases}\n\\tau_1 \\approx 2.9 y_T + 4.8 \\Delta x, \\\\\n\\tau_2 \\approx -3.6 x_T + 2.4 x + 5.0 \\Delta y.\n\\end{cases}$\t\t(7)\nMost of the terms in (7) try to move away from the target\n(i.e., positive dependencies on $\\Delta x$ and $\\Delta y$, as well as a neg-\native dependency on $x_T$. However, the positive dependency\non $y_T$ in $\\tau_1$, shows an attempt to strike a balance between\nmoving towards the desired $y_T$ and moving away from $x_T$.\nThis likely builds momentum for reaching the target in the\nfollowing steps, through the use of other experts.\nExpert 4 ($S_4 \\approx 4.1 y_T + 2.1 \\Delta x - 6.0 \\Delta y$) Expert 4 im-\nplements a simple policy, described by:\n$\\begin{cases}\n\\tau_1 \\approx -3.9 x_T, \\\\\n\\tau_2 \\approx 2.9 \\Delta y.\n\\end{cases}$\t\t\t(8)\nThis policy tends to move away from the target. In fact, in $\\tau_1$\nwe have a negative dependency on $x_T$, and in $\\tau_2$ a positive\none w.r.t. $\\Delta y = y_f - y_T$, thus a positive weight pushing $y_f$\nfarther away from $y_T$.\nExpert 5 ($S_5 \\approx 3.5 \\Delta x$) This expert's policy is:\n$\\begin{cases}\n\\tau_1 \\approx 5.9 y_T - 5.2 \\Delta y, \\\\\n\\tau_2 \\approx -3.4 x + 13 y_T.\n\\end{cases}$\t\t (9)\nThe torque $\\tau_1$ aims for $y_f$ to reach $y_T$ through combined\nfeed-forward and feed-back control approaches, while $\\tau_2$"}, {"title": "Conclusions", "content": "In this paper, we introduced SMOSE, a novel approach for training sparsely activated and interpretable controllers using a top-1 Mixture-of-Experts architecture. By integrating interpretable shallow decision-makers, each specializing in different basic skills, and an interpretable router for task allocation, SMOSE strikes a balance between performance and interpretability. The evaluation of SMOSE presented in this work demonstrates its competitive performance, outperforming existing interpretable baselines and narrowing the performance gap with non-interpretable state-of-the-art methods. The transparency of SMOSE is also showcased in this work, through an in-depth interpretation. Additionally, by distilling DTs from the learned router, we supply an additional tool to facilitate the interpretation of the trained models, making SMOSE a compelling choice for scenarios where both high performance and interpretability are required. As future work, we plan to explore the potential of SMOSE in more complex environments and extend it to multi-agent scenarios, exploiting socially-inspired reward designs to achieve interpretable cooperative and coordinated AI policies."}, {"title": "Appendix", "content": "Computational setup\nAll experiments were run on a single Intel Ice Lake CPU\nwith 32 GB RAM on RedHat Enterprise Linux 8.6 operating\nsystem. In each environment", "segments": "the\ntorso at the top, the thigh in the middle, the lower leg beneath\nthe thigh, and a single foot supporting the entire body. The\nobjective is to achieve forward (rightward) motion by gener-\nating hops through the application of torques (ranging from\n-1 to +1) at the three joints connecting these four segments.\nThe structure of the observations and rewards is similar to\nthe ones described for Walker2d-v4.\nSwimmer-v4 (Fig. 5b) consists of three segments con-\nnected by two articulation joints (called rotors), each of\nwhich connects exactly two links to form a linear chain.\nThe swimmer is suspended in a two-dimensional pool and\nalways starts from a similar position (with minor deviations\ndrawn from a uniform distribution). The objective is to move\nas quickly as possible to the right by applying torques (rang-\ning from -1 to +1) to the joints and utilizing fluid friction.\nThe structure of the observations is similar to the ones de-\nscribed for Walker2d-v4. A positive reward is assigned if the\nswimmer moves forward, while a penalty is associated with\nexcessively large actions.\nHalfCheetah-v4 (Fig. 6a) is a 2-dimensional robot com-\nposed of 9 body parts connected by 8 joints, including two\npaws. The objective is to apply torques (ranging from -1\nto +1) to the joints to make the cheetah run forward (to\nthe right) as quickly as possible. Positive rewards are given\nbased on the distance traveled forward, while negative re-\nwards are given for moving backwards. The cheetah's torso\nand head are fixed, and torques can only be applied to the 6\njoints located at the front and back thighs (connected to the\ntorso), shins (connected to the thighs), and feet (connected\nto the shins).\nAnt-v4 (Fig. 6b) is a 3-dimensional robot composed of a central torso, which can rotate freely, and four legs, each\nconsisting of two segments. The objective is to coordinate\nthe movement of the four legs to achieve forward (rightward)\nmotion by applying torques (ranging from 1 to +1) to the\neight hinges that connect the leg segments to each other and\nto the torso. In total, the robot has nine body parts and eight\nhinges.\nNumber of experts ablation\nThe number of experts employed by SMOSE within the\nMoE architecture is represented by the M parameter. Such\nparameter has been empirically tuned considering five dif-\nferent values ranging from 3 to 32, to balance agents' per-\nformance"}]}