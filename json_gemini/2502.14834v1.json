{"title": "LongWriter-V: Enabling Ultra-Long and High-Fidelity Generation in Vision-Language Models", "authors": ["Shangqing Tu", "Yucheng Wang", "Daniel Zhang-Li", "Yushi Bai", "Jifan Yu", "Yuhao Wu", "Lei Hou", "Hui-Qin Liu", "Zhiyuan Liu", "Bin Xu", "Juanzi Li"], "abstract": "Existing Large Vision-Language Models (LVLMs) can process inputs with context lengths up to 128k visual and text tokens, yet they struggle to generate coherent outputs beyond 1,000 words. We find that the primary limitation is the absence of long output examples during supervised fine-tuning (SFT). \u03a4\u03bf tackle this issue, we introduce LongWriter-V-22k, a SFT dataset comprising 22,158 examples, each with multiple input images, an instruction, and corresponding outputs ranging from 0 to 10,000 words. Moreover, to achieve long outputs that maintain high-fidelity to the input images, we employ Direct Preference Optimization (DPO) to the SFT model. Given the high cost of collecting human feedback for lengthy outputs (e.g., 3,000 words), we propose IterDPO, which breaks long outputs into segments and uses iterative corrections to form preference pairs with the original outputs. Additionally, we develop MMLongBench-Write, a benchmark featuring six tasks to evaluate the long-generation capabilities of VLMs. Our 7B parameter model, trained with LongWriter-V-22k and IterDPO, achieves impressive performance on this benchmark, outperforming larger proprietary models like GPT-40.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in Large Vision-Language Models (LVLMs) have significantly enhanced their capabilities in processing visual and textual in-puts (Alayrac et al., 2022; Zhang et al., 2024). Notably, there have been substantial breakthroughs in the long-context capabilities of VLMs (Xue et al., 2024; Shu et al., 2024). For instance, Qwen2-VL (Wang et al., 2024a) can now understand videos up to 20 minutes, with a context window of 32k tokens. This progress has significantly expanded the scope of tasks that VLMs can handle, making them more applicable to real-world scenarios.\nHowever, despite the increased input context window, the effective output length of VLMs re-mains limited. To verify this limitation, we col-lect a benchmark comprising six tasks that require VLMs to generate long texts based on visual inputs (as shown in Figure 1). By adjusting the required output length in the instructions, we found that all existing models struggle to generate outputs ex-ceeding 1,000 words (Section 2). In real-world sce-narios, such long-output queries are common user demands (Chou et al., 2024). For example, (1) cre-ative writing tasks may require generating detailed stories or essays based on visual prompts (Hong et al., 2023), and (2) professional writing tasks may involve writing comprehensive reports or analyses from visual data (Hartsock and Rasool, 2024). \u03a4\u03bf meet these practical needs, it is essential to enhance the long-output capabilities of VLMs.\nTo investigate the reasons behind the limited long-output capability of VLMs, we are inspired by the LongWriter (Bai et al., 2024), which adjusts the output length distribution of the supervised fine-tuning (SFT) data to observe changes in model output length. Our experiments revealed that the proportion of long-output examples in the SFT data determines the model's output length. This find-ing explains why VLMs typically have an output length limit of around 1,000 words. Existing vi-sual instruction tuning datasets (Schuhmann et al., 2022), such as LLaVA (Liu et al., 2024a), mainly contain tasks like grounding (Liu et al., 2024b) and caption generation (Wang et al., 2022), with most outputs being less than 300 words (Lin et al., 2014).\nTo fill the gap, we select long-output instruction-image pairs from MMEvol (Luo et al., 2024) as inputs. In addition to single-image inputs, we also constructed other forms of data, including multi-image inputs and backtranslated instructions (Wang et al., 2024b), to enrich the diversity of the input data. To generate long outputs, we propose a plan-and-write approach: LongWriter-Agent-V. This"}, {"title": "2 Preliminaries", "content": "In the preliminary experiments, we first collect MMLongBench-Write, a benchmark with visual inputs and long-output requirements. Then, we conduct an evaluation on this benchmark to explore the maximum output length of VLMs. Besides, we reveal that the main reason for bounded output length lies in the length distribution of SFT data.\nMMLongBench-Write. The ability to write long texts based on visual inputs is a fundamental skill in various real-world applications and can be broadly categorized into professional writing and creative writing, depending on whether specialized knowl-edge is required (Taavitsainen and Pahta, 2000). \u03a4\u03bf evaluate how well that VLMs master the two skills, we design three specific tasks for each skill. For each task, we curate 20 representative instructions with input images as test data. To ensure diver-sity, half instructions are in English and half are in Chinese. Figure 1 shows six examples of the benchmark and data distribution. It highlights that professional writing tasks typically involve more input images and require longer output lengths.\nLong Write-V-Ruler. To explore the maximum output length of VLMs, we select 8 examples from MMLongBench-Write benchmark, with four"}, {"title": "3 LongWriter-V: Data and Training", "content": "In this section, we will introduce the data collec-tion and training process for unlocking the long generation capability of vision-language models.\n3.1 Data Collection\nFigure 4 depicts the overall pipeline of our data collection process, which consists of two phases: SFT and DPO data collection.\n3.1.1 SFT Data Collection\nExisting VLMs fail to directly generate texts ex-ceeding 1k tokens, so we develop a two-stage method to generate long texts as SFT data.\nLongWrite Agent-V. Before introducing our method, we first formalize the task's objective. Given several input images v and an user instruc-tion x, our goal is to generate a text y* that aligned with user's length and quality requirements:\ny* = arg max(s_l(y) + s_q(y))P_0(y|v,x) (1)\ny\nwhere $s_l$ and $s_q$ is the scoring function for judging the length and quality of the output, respectively. $P_0$ is the function representing the end-to-end so-lution, while existing VLMs may not be directly applied as their maximum output lengths are be-low 1k. To utilize off-the-shelf VLMs, we propose\na two-stage method for generating long texts. In-spired by the plan-and-write method from Long-Writer (Bai et al., 2024), we first prompt the VLM to generate an outline o that structures the output, plans the content, and specifies the word count for each paragraph. This outline breaks down the com-plex long-output task into manageable sub-tasks. Next, we use the VLM to fill in each paragraph and concatenate them to form the final output:\ny^* = arg max_o P_1(o|v, x) arg max_y (s_l(y)+s_q(y))P_2 (2)\ny\n P_2(y|v, x, o) = \\prod_{i=0}^{n} [P(y_i|v, x, o_i, y_{<i}) (3)\nwhere $P_1$ is the modeling function for first stage, which takes input images and instruction to write an n-paragraph outline o = {o_i, i = 1, ..., n}. $P_2$ refers to the second stage, where the VLM outputs the content $y_i$ paragraph by paragraph based on the input information, outline $o_i$ and previous para-graphs $y_{<i}$. In practice, we design two detailed prompts for guiding VLM to implement the two\nstages, which are listed in Appendix B.2.\nVisual Instruction Collection. To collect long-output visual instructions for SFT, we choose MMEvol (Luo et al., 2024) as our primary data source. MMEvol is a large-scale, open-domain dataset containing 480k image-text in-struction pairs, sourced from diverse datasets such as LLaVA-Instruct (Liu et al., 2024a) and ShareGPT4V (Chen et al., 2024a). However, the average output length in this dataset is relatively short (54.85 tokens), necessitating a filtering pro-cess to identify long-output instructions. We first check the original response length of each example and select those with output length over 128, yield-ing 55,835 valid data. Next, we utilized GPT-40 to verify whether each instruction genuinely requires a long output and whether the associated image was sufficiently relevant to the instruction. Finally, we get 8,115 single-image instructions.\nMulti-image Instruction Generation. As the orig-inal data in MMEvol only has one image for each instruction, we synthesize some multi-image in-structions to increase the diversity of SFT data."}, {"title": "3.1.2 DPO Data Collection", "content": "The SFT data aims to extend VLMs' output length. But the longer outputs may bring more hallucina-tions and repetitions. So the follow up question is: how to improve the generation fidelity of long out-put VLM? Previous works often adapt direct prefer-ence optimization (Rafailov et al., 2024; Liu et al., 2024c) to correct the hallucinations of VLMs. We follow the data format in RLHF-V (Yu et al., 2024a) which utilizes the human-annotated segment-level corrections on VLM's outputs as feedback.\nVLM Output Collection. To collect long re-sponses, we select 100 slides that were not in-cluded in LongWrite-V-22k for VLM to gener-ate scripts. These slides were previously used for teaching on MOOC platforms (Yu et al., 2020) and cover 10 subjects such as Computer Science, Math and Physics. Each subject may contain 4 to 16\nslides and each slide may have 10 to 30 pages. We use LongWriter-V-7B, the VLM trained on our SFT data, to generate scripts for each slide. The long scripts are segmented by sections and aligned with each page of the given slide. We find that LongWriter-V-7B tends to output fewer sections than the number of total pages, which is one of the issues that we would ask human annotators to fix.\nHuman Revision Collection. To get high-quality feedback on the flawed output of SFT model, we hire 10 college students from 10 different majors corresponding to the subjects of our slides. We required annotators to have a GPA above 3.8 to ensure their expertise. To facilitate the annota-tion process, we build an online platform (See Ap-pendix C.1). Each annotator will get slides that match with their major. The platform displays each slide page alongside the corresponding script seg-ment generated by the SFT model. We ask anno-tators to check and revise each page's script for the following error types: factual errors, missing information, relevance to the image, coherence of sentences, and repetition of words. After complet-ing the annotation of a slide, our authors will review the annotation quality. Ultimately, we get 72 valid scripts with fine-grained human corrections."}, {"title": "3.2 Training", "content": "Supervised Fine-tuning. We conduct model train-ing based on two open-source VLMs with dif-ferent parameter sizes: Qwen2.5-VL-7B-Instruct and Qwen2.5-VL-72B-Instruct (Team, 2025). We choose Qwen2.5-VL series as base model because they support a context window of 32k tokens. By resizing the input image's width and height to 280x280, the Qwen2.5-VL models can process up to 30 images. As shown in Figure 5, the output length in LongWrite-V-22k are distributed between 0 and 10k with two peaks around 0 and 1.5k. The\npeak at 0 indicates some short output data is mixed in the Long Write-V-22k, which are mainly the re-sults of those simple instructions. To get a bet-ter length distribution, we sample 10k data from LongWrite-V-22k with an average output length of 2.8k as training data. We then fine-tune the two models for 3 epochs with a learning rate of le-5 for Qwen2.5-VL-7B-Instruct and 7e-6 for Qwen2.5-VL-72B-Instruct, resulting in two SFT models: LongWriter-V-7B and LongWriter-V-72B.\nIterative Direct Preference Optimization. After SFT phase, DPO (Rafailov et al., 2024) is a widely-used method to optimize VLM's output quality, which learns from a dataset of preference pairs D = {(v, x, y_w, y_l)}, where the winning output $y_w$ is preferred over the losing output $y_l$ given the same visual input v and text input x. The optimization objective of DPO is to maximize the difference between likelihood of preference pairs:\nL_{DPO}(\\pi_\\theta; \\pi_{ref}) = -E_{(v,x,y_w,y_l)~D}[\nlog \\sigma(\\beta log \\frac{\\pi_\\theta(y_w |v, x)}{\\pi_{ref}(y_w|v, x)} - \\beta log \\frac{\\pi_\\theta(y_l|v, x)}{\\pi_{ref}(y_l|v, x)})\n] (4)\nIn our annotation process, v represents the images of a slide, x is the instruction for generating scripts, $y_l$ is the flawed output script of VLM and $y_w$ is the slide's lecture after human revision. However, collecting human feedback on long output is very time-consuming and expensive. As mentioned in Section 3.1.2, we gather 72 preference pairs on the scripts, which costs one week and around 1,000 $ to finish. To make most use of these data, we propose to iteratively learn the fine-grained human correctional feedback on the long output. As the $y_w = {y_{w_i}, i = 1, ...N}$ is a revised script for an N page slide, we increasingly view each page's script $y_{w_i}$ as a winning segment over the flawed script:\nL_{IterDPO}(\\pi_\\theta; \\pi_{ref}) = -E_{(v,x,y_w,y_l)~D}[\nlog \\sigma(\\beta log \\frac{\\pi_\\theta(y^{w_i}|v_{<=i}, x)}{\\pi_{ref}(y^{w_i}|v_{<=i}, x)} - \\beta log \\frac{\\pi_\\theta(y^{l_i}|v_{<=i}, x)}{\\pi_{ref}(y^{l_i}|v_{<=i}, x)})\n] (5)\nwhere $y^{w_i}, y^{l_i}$ is the revised and unrevised scripts until page i, and $v_{<=i}$ are the corresponding images. We view $y^{w_i}$ as a new wining response over the flawed output $y^{l_i}$, this can help VLM learn the fine-grained feedback on the long output and ex-tend the number preference pairs for N times. In this way, we get 1,477 iterative pairs for training. Apart from human feedback, we also utilize AI"}, {"title": "4 Experiments", "content": "4.1 Experimental Setup\nMetric. Following Bai et al. (2024), we evaluate the VLM's output length and quality using two metrics: $S_l$ and $S_q$. $S_l$ is the output score that measures how close that the VLM's output length $l_v$ is to the required length $l_r$:\nS_i =\n\\begin{cases}\n100 \\cdot max \\Big(0, 1-\\frac{(l_v/l_r -1)}{3} \\Big) & \\text{if } l_v > l_r,\n\\\\\n100 \\cdot max \\Big(0, 1-\\frac{(l_r/l_v -1)}{2} \\Big) & \\text{if } l_v < l_r.\n\\end{cases} (6)\nWe also use gpt-4o-2024-08-06 to assign the qual-ity score $S_q$ for six aspects: Relevance, Accuracy, Coherence, Clarity, Breadth and Depth, and Read-ing Experience. We list the scoring prompt in Ap-pendix D. Note that we have asked gpt-4o not to take the output length into account so that the qual-ity score is independent with the length score. The overall score S is the mean of $S_l$ and $S_q$.\nBaselines. We evaluate 3 proprietary VLMs, 3 open-source VLMs and 4 LLMs on MMLongBench-Write (model details about models are listed in Table 3). Given that LLMs can also process visual instructions via reading the image caption (Ma et al., 2024), we first use gpt-40 to describe the input images and then feed the caption and writing instruction to the LLM.\n4.2 Main Results\nWe report the performance of baselines and our trained models in Table 1. To study the ef-fective output length of models, we divide the MMLongBench-Write benchmark into four subsets based on the instruction's required word count: 0-1500 words, 1500-2000 words, 2000-3000 words, and over 3000 words. The highest length and qual-ity scores for each subset among models are in bold. We have three observations on the results: (1) Most existing models struggle to satisfy the length requirement over 2000 words, while LongWriter-V models can generate enough words for such in-structions. By checking the length score $S_l$ across different length intervals, we find that most models"}, {"title": "4.3 Human Evaluation", "content": "As the quality score $S_q$ is assigned by the GPT-40 automatically, the evaluation results may have bias as LLM tends to favor the responses generated by itself (Wang et al., 2023; Li et al., 2024a). To get a more fair quality comparison for the mod-"}, {"title": "4.4 Ablation Study", "content": "We conduct ablation experiments on both the SFT and DPO process of LongWriter-V models. For the LongWriter-V-7B model trained on LongWrite-V-22k data, we control the three data sources of Long Write-V-22k to observe how they contribute to the final performance of the SFT model. We run the SFT process on Qwen2.5-VL-7B-Instruct with-"}, {"title": "5 Related Work", "content": "Recent advancements in Vision-Language Mod-els have focused on enhancing their ability to process long-context inputs (Ge et al., 2024; Li et al., 2024b; Chen et al., 2024c). There are abundant benchmarks and datasets that designed for multimodal long context understanding includ-ing MMLongBench-Doc (Ma et al., 2024), Long-"}, {"title": "6 Conclusion", "content": "Our work introduces MMLongBench-Write, a comprehensive benchmark for evaluating long-generation tasks with visual inputs, and LongWriter-V-22k, a novel supervised fine-tuning dataset designed to enhance the long-output capabilities of VLMs. Furthermore, our proposed IterDPO method effectively leverages human feedback to improve the fidelity of long outputs, addressing issues such as hallucination. Future"}, {"title": "Limitations", "content": "We acknowledge some limitations in our work, which are listed below: 1. Dataset Size: The size of our LongWriter-V-22k dataset may not be suffi-ciently large to fully capture the diversity of long-output generation tasks. While this dataset size is adequate for initial exploration and training, it may limit the robustness of our findings and the gener-alizability of our model's performance. Expanding the dataset to include more examples would require significant additional resources, both in terms of data collection and annotation costs. 2. Language Limitation: The current dataset and benchmark are limited to English and Chinese only. This re-stricts our ability to evaluate the performance of VLMs across multiple languages, which is crucial for real-world applications where multilingual sup-port is often required. Future work should con-sider expanding the dataset to include other lan-guages to provide a more comprehensive evalua-tion of VLMs' long-output capabilities. 3. Human Feedback Efficiency: While our IterDPO method significantly improves the efficiency of utilizing human feedback for long outputs, the process of collecting high-quality human corrections remains time-consuming and costly. This limits the scalabil-ity of our approach and the frequency with which we can update and refine the training data. Fu-ture work should explore more efficient methods for obtaining and incorporating human feedback to further enhance model performance."}, {"title": "Ethical Consideration", "content": "While our work on enhancing the long-output ca-pabilities of Vision-Language Models represents a significant advancement, VLMs may still generate inaccurate or misleading information, especially when dealing with common knowledge not explic-itly present in the context. This can lead to the propagation of false information if not properly managed. Therefore, additional safeguards and ver-ification mechanisms should be implemented when deploying these models in user-facing applications.\nOur training data has been desensitized to pro-tect individual privacy. All the data sources we used are public available with permissible licenses, including MMEvol (Luo et al., 2024) and Zen-"}]}