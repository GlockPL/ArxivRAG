{"title": "INSIGHT: Enhancing Autonomous Driving Safety through Vision-Language Models on Context-Aware Hazard Detection and Edge Case Evaluation", "authors": ["Dianwei Chen", "Zifan Zhang", "Yuchen Liu", "Xianfeng Terry Yang"], "abstract": "Autonomous driving systems face significant chal-lenges in handling unpredictable edge-case scenarios, such as adversarial pedestrian movements, dangerous vehicle maneuvers, and sudden environmental changes. Current end-to-end driving models struggle with generalization to these rare events due to limitations in traditional detection and prediction approaches. To address this, we propose INSIGHT (Integration of Semantic and Visual Inputs for Generalized Hazard Tracking), a hierarchical vision-language model (VLM) framework designed to enhance hazard detection and edge-case evaluation. By using multimodal data fusion, our approach integrates semantic and visual representations, enabling precise interpretation of driving scenarios and accurate forecasting of potential dangers. Through supervised fine-tuning of VLMs, we optimize spatial hazard localization using attention-based mechanisms and coordinate regression techniques. Experimental results on the BDD100K dataset demonstrate a substantial improvement in hazard prediction straightforwardness and accuracy over existing models, achieving a notable increase in generalization performance. This advancement enhances the robustness and safety of autonomous driving systems, ensuring improved situational awareness and potential decision-making in complex real-world scenarios.", "sections": [{"title": "I. INTRODUCTION", "content": "In autonomous driving scenarios, there exists a possibility that unpredictable edge cases, such as pedestrian crossing and construction zones, will occur in the next time steps [1]. These unforeseen edge cases can lead to dangerous consequences, including vehicle crashes, pedestrian injuries, etc. Therefore, it is necessary to develop a precise and generalized model to interpret the current driving scenario and draw out unpredictable dangerous scenarios. By connecting end-to-end and prediction models, autonomous vehicles are able to prevent effects from possible dangerous edge cases with advanced autonomous driving algorithms [2].\nHowever, connecting current autonomous driving models and edge cases prediction models is challenging [1]. The traditional detection algorithms, such as the sensor-based au-tonomous driving model [3], do not perform well on edge"}, {"title": "II. RELATED WORKS", "content": "Recently, the development of autonomous driving algo-rithms has significantly improved with various approaches addressing scene understanding, prediction, and control [6]- [8]. This section reviews relevant work in edge case sce-nario exploration, end-to-end autonomous driving models, and VLMs in autonomous vehicles."}, {"title": "A. Edge Case Scenario Exploration", "content": "Edge case scenarios in autonomous driving refer to rare, unpredictable, or extreme situations that challenge the ro-bustness and safety of autonomous driving algorithms [9], [10]. Verifying both the vehicle and its algorithms in these edge cases is critical to ensuring safety and reliability [1]. Testing in such scenarios involves subjecting the vehicle and its systems to high-risk or failure-prone conditions, allowing for a comprehensive evaluation of their performance under adverse circumstances [11], [12]. These simulations play a vital role in rigorously testing and refining algorithms and vehicle systems, enabling the identification and resolution of potential weaknesses [13]. After verifying and even training in such scenarios, these vehicles are able to not only enhance the robustness of autonomous systems but also accelerate their deployment in real-world settings by ensuring they meet safety and performance standards under a wide range of conditions Extensive research has been conducted to model and simulate edge case scenarios, focusing on conditions such as extreme weather, erratic pedestrian behavior, and unconventional vehicle movements [14]\u2013[16]. By addressing these issues in simulated environments, developers can ensure a higher level of safety and performance before deploying the systems in real-world applications [17]. This proactive approach is essential for building trust in autonomous driving technologies and mitigating risks associated with edge cases."}, {"title": "B. End-to-End Autonomous Driving Model", "content": "End-to-end models for autonomous driving utilize deep learning architectures to map raw sensor inputs, such as images and LiDAR data, directly to control outputs like steering, ac-celeration, and braking. Convolutional neural network (CNN) is a common model employed to process visual data by extracting spatial features from camera inputs, enabling the recognition of lanes, vehicles, and other road elements [18], [19]. Recurrent neural network (RNN) and temporal convo-lutional network (TCN) handle data sequences effectively, capturing dynamic changes in the environment over time [20]. Moreover, transformer utilizes self-attention mechanisms to effectively model long-range dependencies and complex interactions for both visual and sequential data processing [21], [22], offering improved performance in tasks such as object detection, trajectory prediction, and scene understanding [23]. While effective in many scenarios, these models often face challenges in rare or hard-predictable edge cases due to their reliance on the data used during training which is collected from daily common driving scenarios or simple AV simulator scenarios [24], which may not fully represent real-world variability."}, {"title": "C. Vision-Language Model in Autonomous Vehicle", "content": "In recent years, VLM has made breakthroughs in natural language processing and multimodal tasks [4], [25]. Applying it to autonomous driving can effectively improve scene un-derstanding and decision-making capabilities [5], [26]. VLMs enhance the system's comprehensive perception capabilities by combining visual data, such as cameras and LiDAR, and textual data, such as traffic signs and navigation instructions [27]\u2013[29]. The effective integration of visual features and language representation through visual language adapters can improve the ability to understand complex driving scenar-ios [30]. With large-scale pre-trained VLMs, the model can learn common representations and knowledge from massive multi-modal data, assisted with zero-shot learning [31], [32], thus having strong generalization capabilities [33], [34]. The model can infer semantic information such as the meaning of the potential movement of a pulled-over vehicle with law enforcement on the side from visual features in an image [35]. This ability allows the model to use pre-trained knowledge to make inferences about unknown data when encountering new scenarios, without relying on task-specific annotated data [36]. In our work, we use VLM to unify visual and language under-standing in autonomous driving, enabling real-time decisions, better scene comprehension, and improved edge case handling for safer and more efficient driving automation.\nOur proposed method consists of four main components: modal feature extraction, shared latent space projection, se-mantic alignment, and hazard region localization. It includes"}, {"title": "III. SUPERVISED FINE-TUNING VLMS", "content": null}, {"title": "A. Shared Latent Space", "content": "Both visual and textual features are projected into a shared latent space $V \\subset R^k$ through projection layers, enabling cross-modal understanding:\n$E_{img} = Projector_{img}(F_{img}), E_{text} = Projector_{text}(F_{text})$ (1)\nThe projection layer maps high-dimensional visual and textual features into a shared lower-dimensional latent space using a linear transformation or MLP. This shared space ensures that visual and textual inputs are comparable and seamlessly fused."}, {"title": "B. Semantic Alignment Between Image and Text", "content": "The proposed method achieves strong semantic alignment between images and text using contrastive learning. It can align embeddings of similar data (e.g., matching image and text) by bringing them closer in a shared space while pushing apart dissimilar data. The objective of contrast learning is aligning embeddings of positive pairs (e.g., matching image and text) and negative pairs (e.g., mismatched image and text) are computed and the loss function is shown below:\n$L_{contrastive} = - log \\frac{exp(sim(E_{img}, E_{text}))}{\\Sigma_j exp(sim(E_{img}, E_{l_{ext}}))}$ (2)\nwhere $E_{text}$ represents the embedding of the matching text, $E_{ext}$ represents the embedding of non-matching text, and $sim(x, y) = \\frac{x \\cdot y}{|x||y|}$ is the cosine similarity. This alignment ensures that relevant image-text pairs share a similar semantic representation, allowing the model to better integrate multi-modal information and handle complex scenarios."}, {"title": "C. Attention Maps for Spatial Localization", "content": "Our proposed method employs a transformer-based archi-tecture that generates multimodal embeddings by integrating visual and textual features. To enable spatial localization, we extract attention maps from its vision encoder, which inher-ently capture the regions of the input image that contribute most to the model's understanding. These attention maps, denoted as $A \\in R^{H \\times W}$, represent a spatial distribution of importance over the image, where H and W are the height and width of the feature map.\n1) Extracting Coordinates from Attention Maps: To local-ize specific regions of interest, the method will identify the most prominent areas in the attention map. By computing attention weights for each spatial location using the outputs of the last layer of the vision encoder. The spatial coordinates of the maximum attention weight, (x, y), are identified as the predicted location of the target:\n$(x, y) = arg\\ \\max_{(x,y)} A(x, y)$ (3)\n2) Integration with Text Generation: The extracted coor-dinates are further integrated with the language generation module to produce descriptive outputs. For instance, the model generates text such as: \"The area around (544, 459) should be paid more attention to.\" by conditioning the language generation layers on the localized regions."}, {"title": "D. Loss Function:", "content": "The training process adopts a multi-task loss function to simultaneously optimize coordinate prediction and text genera-tion. The total loss is defined as $L_{total} = \\lambda_{coord}L_{coord} + \\lambda_{text}L_{text}$ which is shown in Algorithm 1, particularly in lines4, 10. with the following parameters.\n$\n* L_{coord}: Coordinate regression loss, computed as the Mean\nSquared Error (MSE) between the attention and true\ncoordinates. It is formally represented as $L_{coord} = \\frac{1}{N} \\Sigma_{i=1}^{N} ||loc_i - \\hat{loc_i}||^2$, where $\\hat{loc_i} = (\\hat{x_i}, \\hat{y_i})$ represents the attention map coordinates, and $loc_i = (x_i, y_i)$ represents the true coordinates.\n* L_{text}: Text generation loss, computed as the cross-entropy\nLoss between the generated sequence and the ground\ntruth sequence. It is formally represented as $L_{text} =\\frac{-1}{NT} \\Sigma_{i=1}^{N} \\Sigma_{t=1}^{T} s_{i,t} log(\\hat{s_{i,t}})$, where T is the sequence length, $s_{i,t}$ is the ground truth token, and $\\hat{s_{i,t}}$ is the predicted probability for the corresponding token.\n* $\\lambda_{coord}$ and $\\lambda_{text}$: Balancing coefficients to control the relative importance of the two tasks.\n$\nThis approach solves key challenges in multimodal learning by explicitly aligning localization with textual descriptions. $L_{coord}$ stabilizes attention-based localization, addressing un-certainty in soft probabilistic outputs, while $L_{text}$ ensures contextually relevant text aligned with detected regions. The balancing coefficients allow dynamic emphasis on either task, improving robustness across datasets and use cases. Attention-based localization method removes the need for explicit bounding box annotations, while the multi-task loss ensures end-to-end optimization, making it both efficient and effective for scene understanding compared to traditional end-to-end black box model. This methodology as shown in Fig. 1 enables accurate prediction of potential hazard regions in autonomous driving scenarios by utilizing fine-tuned vision-language align-ment, precise localization mechanisms, and a combined focus on coordinate regression and textual explanation."}, {"title": "E. Dataset Preprocessing", "content": "1) Dataset Overview: The dataset used in this study is a subset of the BDD100K dataset, containing 1000 images selected from front-facing camera recordings in autonomous driving scenarios. These images cover diverse environments, including urban, suburban, and highway scenes, with varying lighting and weather conditions. A custom dataset is created by manually annotating this selective portion of images, where each image is labeled with potential hazard points (x, y) based on human judgment and driving experience (Fig. 2). The annotation criteria focused on identifying areas likely to pose risks, such as pedestrians, vehicles in proximity, or sudden obstacles. The subset of 1000 images is selected to maintain efficiency and reduce computational costs while still enabling effective model evaluation and training on key scenarios.\n2) Annotation Method: Manual annotation is performed to label potential hazard areas within each image. The hazard area is defined based on the experimental subjects' driving experience and judgment, including areas where pedestrians, vehicles, or other obstacles pose potential risks to safe driving. In general, these potential hazard areas can be divided into two categories: predictable surrounding behavior (e.g., a vehicle in an adjacent lane attempting to merge) and unpredictable surrounding behavior (e.g., a pedestrian suddenly stepping onto the road).\nFor consistency, one experimental subject annotated all 1000 images in this dataset. During this annotation process, each frame is displayed for a maximum of 5 seconds, during which the subject identified the area with the highest probability of collision risk. A bounding box is drawn around the identified hazard area, and its center point recorded as (x, y) coordinates.\nEach annotated image contains exactly one identified hazard area. To validate the reliability of these annotations, a subset of 100 images is cross-checked by two additional reviewers; discrepancies are discussed and resolved, resulting in a final consensus annotation for the entire dataset."}, {"title": "F. Model Setup", "content": "1) Pre-trained Model: Qwen2-VL-7B is chosen as the baseline pre-trained model [37], while it can be easily extended to other pre-trained VLM models. This VLM is designed for multi-modal tasks, combining robust image and text process-ing capabilities. Also with the incorporation of the vision transformer for visual encoding and the transformer-based text encoder for language understanding, it's easy to enable the model to handle complex multi-modal tasks effectively. The pre-trained weights are sourced from the HuggingFace library, and the base architecture is kept unchanged during fine-tuning, except for specific layers adapted using LoRA.\n2) Fine-tuning Strategy: To adapt the pre-trained model for hazard region prediction, LoRA is employed as a parameter fine-tuning technique. LoRA is applied to the multi-head attention layers (Query and Value projection matrics in the attention mechanism) and feed-forward layers (Selected layer of the output feed-forward network). The other weights in the pre-trained model remained frozen during training. The Rank r of the LORA matrices is set to 8. The pre-trained weights $W \\in R^{d \\times k}$ are adapted as follows:\n$W' = W + \\Delta W, \\Delta W = A \\cdot B, A \\in R^{d \\times r}, B \\in R^{r \\times k}$, (4)\nwhere A and B are learnable matrices, and r is the low rank, significantly reducing the number of trainable parameters."}, {"title": "IV. EXPERIMENTS", "content": null}, {"title": "A. Hyperparameter Configuration", "content": "The AdamW optimizer which includes the weight decaying for regularization is used in the experiment. The initial learning rate is set to $1 \\times 10^{-4}$ and is decayed based on the cosine schedule as $\\eta_t = \\eta_0 \\cdot 0.5 \\cdot (1 + cos(\\frac{t}{T} \\cdot \\pi))$, where $\\eta_t$ is learning rate at step t, and T is total number of training steps. Also due to the memory constraints, a single image per batch is processed with gradient accumulation performed over 8 steps before parameter updates. The model is trained for 3 epochs, ensuring sufficient convergence without overfitting. Gradients are clipped at a maximum norm of 1.0 to prevent exploding gradients. BF16 mixed precision is enabled to reduce memory usage and improve training speed."}, {"title": "B. Experiment Equipment", "content": "The proposed experiment is conducted on a lab server running Ubuntu 20.04.6 LTS (Focal Fossa). The system is equipped with a 13th Gen Intel Core i9-13900KF processor featuring 16 cores and 32 threads. For graphical computation, the server utilizes an NVIDIA GPU GeForce RTX 4090 24 GB with the proprietary NVIDIA 535.183.01 driver. The system includes 64 GB of RAM and a 1 TB NVMe SSD for primary storage, supplemented by a 14.55 TB external USB drive for dataset storage and backups."}, {"title": "C. Training Result", "content": "1) Training Process: The training process is monitored using key metrics, including training loss, learning rate, and Validation accuracy, to ensure effective convergence and robust model performance on unseen data. presents these metrics: the training loss consistently decreases, indicating effective learning; the learning rate follows a cosine decay schedule for smooth optimization; and the gradient normal-ization remains controlled, ensuring stable training. During the training process, the loss L exhibits a rapid decline from approximately 2.0 at the beginning to around 0.6 by 35 steps, after which it stabilizes, indicating convergence. The learning rate \u03b7 follows a scheduled warm-up and cosine annealing strat-egy, starting at $3 \\times 10^{-5}$ and progressively decreasing to near 0 by 300 steps, aligning with designed learning rate function to enhance model stability. The gradient norm ||g|| shows an initial high value of ~ 2.0, decreasing over time but exhibiting fluctuations, suggesting adjustments in weight updates during training. This overall trend indicates effective convergence of the model while maintaining controlled parameter updates.\n2) Core Indicators and Performance: The fine-tuned model's performance is evaluated using several core indicators, including BLEU-4, ROUGE-L, ROUGE-1, ROUGE-2, and Mean Squared Error (MSE). The BLEU-4 measures the pre-cision of up to 4-gram overlaps between a fine-tuned vision-language model's generated text and reference outputs, making it useful for tasks like image captioning comparison, ROUGE-1 captures the unigram-level overlap, indicating how well the model covers key information in text summarization tasks, ROUGE-2 extends this to bigrams, offering a finer-grained measure of contextual coverage for summarizing or describing visual content, and ROUGE-L uses the longest common sub-sequence to evaluate the sequence-level match, emphasizing overall recall of critical information in the model's output. The fine-tuned model is verified in the highway and urban driving scenarios of the BDD100K human annotation dataset. Each dataset contains 250 frames of driving scenarios and the max_samples is also 250. The model achieved outstanding performance on standard scenarios, as shown in Table I. The model achieved outstanding performance in highway driving scenarios and also great performance in urban driving scenar-ios. The MSE calculation function is shown below:\n$MSE = \\frac{1}{n} \\Sigma_{i=1}^{n} ((x_i - x_{true})^2 + (y_i - y_{true})^2)$ (5)\nWhere $x_i$ and $y_i$ represent the prediction from the fine-tuned Qwen2-VL and the $x_{true}$ and $y_{true}$ represent the human-annotation ground truth pixels location. The model's qualita-tive performance is illustrated in , where predicted hazard regions (yellow) closely match the ground truth annotations"}, {"title": "V. CONCLUSION", "content": "This study demonstrates the effectiveness of using fine-tuned VLMs, such as Qwen2-VL, in addressing the chal-lenges of unpredictable edge case scenarios in autonomous driving. By connecting visual and textual modalities, IN-SIGHT enhances situational edge-case awareness and hazard detection. Experimental results show significant improvements in accuracy for hazard region localization and text genera-tion, outperforming baselines and approaching human-level performance. INSIGHT not only advances the robustness of autonomous systems but also provides a framework for integrating multimodal learning to handle complex and rare driving conditions effectively. Future research could further explore real-time implementations and expand the model's adaptability to broader datasets, CARLA simulators, and real-world scenarios."}]}