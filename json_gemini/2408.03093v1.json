{"title": "Learning Provably Robust Policies in Uncertain Parametric Environments", "authors": ["Yannik Schnitzer", "Alessandro Abate", "David Parker"], "abstract": "We present a data-driven approach for learning MDP policies that are robust across stochastic environments whose transition probabilities are defined by parameters with an unknown distribution. We produce probably approximately correct (PAC) guarantees for the performance of these learned policies in a new, unseen environment over the unknown distribution. Our approach is based on finite samples of the MDP environments, for each of which we build an approximation of the model as an interval MDP, by exploring a set of generated trajectories. We use the built approximations to synthesise a single policy that performs well (meets given requirements) across the sampled environments, and furthermore bound its risk (of not meeting the given requirements) when deployed in an unseen environment. Our procedure offers a trade-off between the guaranteed performance of the learned policy and the risk of not meeting the guarantee in an unseen environment. Our approach exploits knowledge of the environment's state space and graph structure, and we show how additional knowledge of its parametric structure can be lever-aged to optimize learning and to obtain tighter guarantees from less samples. We evaluate our approach on a diverse range of established benchmarks, demonstrating that we can generate highly performing and robust policies, along with guarantees that tightly quantify their performance and the associated risk.", "sections": [{"title": "1 Introduction", "content": "Ensuring the safety and robustness of autonomous systems in safety-critical tasks, such as unmanned aerial vehicles (UAVs), robotics or autonomous control, is paramount. A standard model for sequential decision making in these settings is a Markov decision process (MDP), which provides a stochastic model of the environment. However, real-world dynamics are complex, with parameters that are unknown and may evolve. Therefore, models with epistemic uncertainty [6, 7, 16, 17, 21, 25, 30, 37, 51] are considered which account for uncertainty resulting from a lack of knowledge of the system or the environment under investigation. Uncertain models help construct robust policies that perform well across the various possible stochastic environments.\nConsider the UAV motion planning problem shown in Figure 5, adapted from [7]. The goal is to navigate the drone safely to the target zone, without obstacle collisions. The drone's dynamics are influenced by weather conditions such as wind or precipitation. In low disturbance conditions (e.g., low wind), the drone can safely take the shorter route to the target. Under high disturbances, a\nrobust policy might take a detour through a less cluttered region of the environment. The (stochastic) dynamics of the drone are impacted by environmental parameters such as wind speed or direction. The values of these parameters are unknown - they can vary over time and may be difficult to observe - but some conditions may be more probable than others.\nIn this paper, we aim to learn robust policies for stochastic environments whose probabilistic dynamics are defined in terms of parameters with an unknown underlying distribution. Furthermore, we want to provide guarantees on their performance (e.g., the probability of safe mission completion) in unseen scenarios. We model environments as parametric Markov decision processes (pMDPs) [23, 34], where state transition probabilities are uncertain and depend on (potentially related) parameters from a set \u0398 with an unknown distribution P. This forms an uncertain pMDP (upMDP) MD [7, 8, 42].\nAn overview of our procedure is illustrated in Figure 1. We assume access to multiple samples M[\u03b8] of the environment, each corresponding to a parameter valuation \u03b8i from distribution P. The value \u03b8i is not directly observed. Instead, we are able to access the partially unknown MDP M[\u03b8i]: we know the model's underlying structure (and, potentially, dependencies between some transition probabilities that share parameters) but the exact dynamics are unknown can only be inferred from sample trajectories of M[\u03b8]. In our UAV example, this equates to taking the drone outside on a new day and encountering a new set of environmental conditions (or a simulation of this)."}, {"title": "2 Preliminaries", "content": "We introduce the fundamental formalisms underpinning our approach. Let \u2206(S) = {\u03bc: S \u2192 [0, 1] |\n\u03a3s \u03bc(s) = 1} denote the set of all probability distributions over a finite set S.\nDefinition 2.1 (Parametric MDP). A parametric Markov decision process (pMDP) is a tuple M\u0473 =\n(S, 81, A, P\u0473), where S and A are finite state and action spaces, s\u2081 \u2208 \u2206(S) is the initial state\ndistribution, and P\u0473 : \u04e8 \u00d7 S \u00d7 A \u2192 \u25b3(S) is the parametric transition probability function over the\nparameter space \u04e8. Fixing a parameterization 0 \u2208 \u04e8 induces a standard MDP M\u0473[0], or M[0] for\nshort, with transition kernel Po: S \u00d7 A \u2192 \u2206(S) defined as Pe(s, a, s') = Pe(0, s, a, s').\nFigure 4a shows a simple 4-state pMDP with 3 parameters. We remark that pMDPs can be seen\nas modelling a set of MDPs, i.e., the instantiations induced by all possible parameters \u03b8\u2208 \u0398. \u0399n\ndoing so, they are closely related to the most general class of uncertain MDPs [30, 51], where each\ntransition is associated with (potentially interdependent) sets of possible values.\nDefinition 2.2 (Uncertain Parametric MDP). An uncertain parametric Markov decision process\n(upMDP) M = (M\u0473, P) is a pMDP M\u0473 with a probability measure P over the parameters \u0398.\nWe assume that upMDPs are graph preserving, meaning that the underlying graph of all in-\nduced MDPs is the same: \u2200s,s' \u2208 S,a \u2208 A: (0 \u2208 supp(P): Pe(s,a,s') = 0) v (\u03b8\u03b5\nsupp(P): Pe(s, a, s') > 0), and that this graph structure is known. This is a common assump-\ntion when using these models (and indeed the closely related interval MDPs models [51]).\nPolicies are mappings \u03c0 : (S \u00d7 A)* \u00d7 S \u2192 \u2206(A) from (finite) histories of states and actions to a\ndistribution over the next actions. In this work, we mainly focus on learning memoryless-deterministic\npolicies \u03c0: S \u2192 A, but all results carry over to other classes of policies. We show that memoryless-\ndeterministic policies achieve high performances on relevant benchmarks and elaborate on cases\nwhere more expressive policy classes can be advantageous (see Section 3.2).\nDefinition 2.3 (Evaluation Function). For a upMDP M = (M\u0473, P), an evaluation function\nJ: \u041f \u00d7 \u04e8 \u2192 R maps a policy \u03c0 and a parameter value 0 onto a performance value of \u03c0 on M[0].\nWe also use J(\u03c0, M) for the evaluation of policy \u03c0 on MDP M, i.e., J(\u03c0, 0) = J(\u03c0, \u039c[\u03b8]).\nCommonly used evaluation functions are, e.g., (un-)discounted expected reward [47] or the probability\nof satisfying requirement over trajectories [10]. For a fixed policy \u03c0 of an upMDP MD, an evaluation\nfunction J only depends on the parameters 0. Hence, J is a random variable with measure IP (see\nFigure 2). One possibility, ignoring stochasticity, is to learn policies for M that are robust against\nall possible parameters. The robust value of a policy is the minimal (worst-case) achieved under any\npossible parameterization (see Figure 2): J(\u03c0) = infe\u2208supp(P) J(\u03c0, \u03b8).\nEven with full knowledge of and P, finding J(\u03c0) is a challenging problem. While computation can\nbe feasible under certain convexity and rectangularity assumptions on the parametric structure [30, 51],\nthese approaches do not apply to unknown, arbitrary O and P. Furthermore, we specifically exploit\nthe probabilistic nature of upMDPs M, by not necessarily considering a policy's worst-case\nperformance, but rather its violation risk.\nDefinition 2.4 (Violation Risk). The violation risk of policy \u03c0 and performance guarantee \u0134 \u2208 R is:\nr(\u03c0, \u00ce) = P {\u03b8\u2208 \u04e8: J(\u03c0,\u03b8) < \u00ce}. \n(1)\nThe violation risk is the probability that policy \u03c0 achieves a value less than a stated performance\nguarantee J on MPD (see Figure 3). There is an inherent trade-off between violation risk and\nperformance guarantee: a higher performance guarantee is associated with a higher risk, regardless\nof M and J. Quantifying the violation risk offers more flexibility than computing the worst-case\nperformance J(\u03c0), which ignores the likelihood of worst-case scenarios and can lead to overly\nconservative guarantees based on unlikely parameterizations.\nSince we do not assume any knowledge of the parameter space or the probability measure P, it\nis unrealistic to assume we could compute the policy violation risk exactly. Thus, our goal is to\nprovide a sound and tight upper bound by analyzing sample MDPs from the underlying upMDP. Our\napproach builds and solves model approximations represented as interval MDPs.\nDefinition 2.5 (Interval Markov Decision Processes). An interval Markov decision process (IMDP)\nMI = (S, 81, A, PI) is an MDP with a probabilistic interval transition function PI : S \u00d7 A \u00d7 S \u2192 I,"}, {"title": "3 Learning Provably Robust Policies", "content": "In this paper, we approximate sampled unknown environments by analysing trajectories. Using these\nlearned estimates, we construct robust policies, evaluate their performance and bound their risk in\nunseen environments. The overall problem can be stated as follows.\nProblem 1. Given a upMDP M with unknown parameter distribution P, an evaluation\nfunction J, and a confidence level n > 0, find a robust policy \u03c0, a performance guarantee J,\nand a risk bound \u025b > 0, such that:\nPr {r(\u03c0, J) \u2264 \u03b5} \u2265 1 - \u03b7.\nFrom here on, we consider a fixed upMDP M. The input to our procedure is a set D = {M[0] |\n0 ~ P} of partially unknown MDPs sampled from M. We split this set into disjoint training\nand verification sets TM UVN = D of sizes M and N. The training set is used to learn a robust\npolicy, which is then evaluated on the verification set to derive a performance guarantee and bound\nthe violation risk when deployed in an unseen environment sampled from P. Since the MDPs in D\nare partially unknown, we estimate the unknown parts from sample trajectories. For the verification\nset, we build IMDPs that include the unknown MDPs with high confidence (see Fig. 4c)."}, {"title": "3.1 PAC IMDP Learning", "content": "We follow the established approach for PAC learning of IMDP approximations introduced by Strehl\nand Littman [44]. Consider a partially unknown MDP M[0]. We assume access to trajectories from\nM[0], consisting of sequences of triples (s, a, s') representing states, chosen actions, and successor\nstates. Leveraging the Markov property of MDPs, we treat each triple as an independent Bernoulli\nexperiment to estimate the transition probability to state s'. We denote the number of times action a\nwas chosen in state s across all sample trajectories as #(s, a), and the number of times this choice\nled to a transition to s' as #(s, a, s'). The resulting probability point estimate is thus given by:\nP(s,a,s') = #(s,a, s')\n#(s,a)  ,\n(3)\nfor #(s, a) > 0. We construct an IMDP MY [0] by transforming the point estimates into PAC\nintervals using Hoeffding's inequality [39]. Let 1 \u2013 \u03b3 with \u03b3 > 0 be the desired confidence level for\nM[0] to be included in MY[0]. This confidence is distributed over all nu unknown transitions as\n\u03b3\u03c1 = \u03b3/nu. Let H = #(s, a), for each unknown transition, the interval is given by:\nP'(s, a, s') = [max(\u00b5, P(s, a, s') \u2013 \u03b4), min(P(s, a, s') + \u03b4, 1)],\n(4)\nwith d = \u221a(log (2/\u03b3\u03c1)/2H and \u03bc > 0 being an arbitrarily small quantity to preserve the known\ngraph structure. For unvisited state action pairs with #(s, a) = 0, we set P(s, a, s') = [\u03bc, 1],\nfor all s'. According to Hoeffding's inequality, PV(s, a, s') contains the true transition probability\nP(s, a, s') with a probability of at least 1 Yp. By applying a union bound over the unknown\ntransitions, Strehl and Littman [44] derive the following overall guarantee:\nLemma 3.1. The true, partially unknown MDP M[0i] is contained in its IMDP overapproximation\nMY [0] with probability at least 1 \u2013 \u03b3."}, {"title": "3.1.1 Model-based Optimizations", "content": "Our procedure does not require any model knowledge beyond its graph structure. However, we\ncan leverage available information on the environment's parametric structure to achieve tighter\napproximations from fewer samples. Consider two transitions sharing the same parameterization\nP(s,a,s') = P(t,b,t') = \u00f4, though \u00ee\u00ee is unknown (see Figure 4b). Parameter tying [32, 33]\ncan be employed to combine counts from both transitions. Let sim(s,a,s') = {(t,b,t') |\nP(s,a,s') = P(t,b,t')} denote the set of transitions with equal parameterization. Define\n#T (s, a, s') = \u2211(t,b,t')\u2208sim(s,a,s') #(t, b, t') and HT (s, a, s') = \u2211(t,b,t')\u2208sim(s,a,s') #(t, b). The\nparameter-tied point estimate and interval width are given by:\nP(s, a, s') =\n#T(s, a, s')\nHT (s, a, s')  and Sp =\nlog (2/\u03b3)\n2HT(s,a) \n(5)\nAppendix E investigates the impact of parameter tying on the tightness of the obtained approximation\nand demonstrates that it can accelerate learning by multiple orders of magnitude."}, {"title": "3.2 Robust Policy Learning", "content": "We use the training set TM of partially unknown MDPs to learn a policy that achieves high perfor-\nmance across all training samples. Since risk evaluation will be conducted ex post on the PAC IMDPS\nfrom the verification set, we have flexibility in our choice of method. With full knowledge of the\nsampled MDPs, the robust policy problem coincides with finding the optimal policy for multiple-\nenvironment MDPs [35], uncertain MDPs with only finitely many possible environments. It was\nshown by Raskin and Sankur [35] that the optimal policy may require for infinite memory. Intuitively,\nthe optimal policy explores and determines which of the environments it is operating in and follows\nthe optimal policy for that environment. Despite being infeasible to compute even under full knowl-\nedge, this policy can be seen as perfectly overfitted to the training set and may not generalize well to\nunseen environments. Computing the optimal robust memoryless policy is NP-hard, even for just two\nfully known environments [35]. Rickard et al. [36] introduced an approximate procedure based on\npolicy gradient, which was shown to be able to produce good, randomized policies. However, the\npolicy gradient algorithm requires solution of every MDP sample in every iteration of the algorithm\nfor the gradient step, rendering it feasible only on small training sets consisting of small MDPs.\nWhile any of the procedures above can be used to find robust policies within our framework, we opt\nfor a simple yet effective overapproximation approach that aligns well with our setup of sampled\npartially unknown MDPs M[0]. Similar to PAC IMDP learning, we analyze trajectories for each\npartially unknown MDP in the training set to construct an IMDP. However, since we use these\nIMDPS solely for policy learning, not for uncertainty quantification, we are not restricted to PAC\nlearning. This flexibility allows for the use of various interval learning algorithms that, while lacking\ninclusion guarantees, offer empirically tighter approximations from fewer samples. In Section 4, we\ncompare IMDP learning using: (1) PAC learning as described in Section 3.1; (2) Linearly Updating\nIntervals [46], a recent Bayesian approach; (3) the UCRL2 reinforcement learning algorithm [3]; and\n(4) Maximum a-posteriori (MAP) point estimates [46]. Detailed descriptions of these IMDP learning\nalgorithms are given in Appendix B. All methods can be enhanced by model-based optimizations."}, {"title": "3.3 Risk Evaluation", "content": "We use the verification set from Section 3.1 to bound the violation risk of the learned policy \u03c0 when\napplied in an unseen environment. We analyze \u03c0 on the PAC IMDPs V\u00a5 = {M\u00a5[0i]}1<i<N to\nbound the policy's performance on the underlying unknown MDPs. From Equation 2, we have\nM[0] \u2286 M\u00b2 [0] \u21d2 J(\u03c0, \u039c^[0i]) \u2264 J(\u03c0, \u039c[\u03b8]). From Lemma 3.1, it follows that\nP{J(\u03c0, \u039c\u0384 [\u03b8]) \u2264 J(\u03c0, \u039c[\u03b8i])} \u2265 1 \u2013 \u03b3.\n(7)\nTo obtain a bound on the violation risk, we formulate the problem as a convex optimization problem\nwith randomized constraints\u2014a scenario program [15]. We leverage the generalization theory of\nthe scenario approach by Campi and Garatti [13], adapting it to incorporate the uncertainty of\nperformance values obtained on the learned IMDPs to under-approximate the performances on the\nunderlying MDPs (see Equation 7). We detail the formulation and the derivation of the generalization\nfrom approximations of unknown samples to the entire unknown distribution in Appendix A.\nTheorem 3.2. Given N i.i.d. sample MDPs M[0i] and IMDPs MV[0i], such that P{M[0] \u2286\n\u039c\u03a5 [\u03b8]} \u2265 1 \u2013 \u03b3. For any policy \u3160 and confidence level 1 \u03b7, with \u03b7 > 0, it holds that:\nPN {r(\u03c0, J) \u2264 \u03b5(\u039d, \u03b3, \u03b7)} \u2265 1 -\u03b7,\nwhere J = min\u2081 J(\u03c0, \u039c^[0i]), and \u025b(N,\u03b3, \u03b7) is the solution to the following, for any M < N:\n(8)\n\u03a3(4) (1-1) - - (1-1) = \u03a3() \u03b5\u1f30 (1 \u2013 \u03b5) \u039d-\u03af,\ni=M\n(9)\ni=0\nTheorem 3.2 bounds the violation risk for the policy to achieve a performance less than the minimum\nperformance on any verification IMDP. This bound only depends on values we can observe from\nthe learned approximations. The theorem includes a tuning parameter M < N. To obtain the\ntightest bound, our procedure enumerates possible values for M and solves the resulting equation\n(see Appendix A). For a fixed M, the left-hand side of Equation 9 is constant, and the right-hand\nside is the cumulative distribution function of a beta distribution with M + 1 and N - M degrees of\nfreedom [15], which is easy to solve numerically for its unique solution in the interval [0, 1] using\nbisection [7, 39]. Figure 6 shows the obtained risk bounds for various combinations of confidences\nand verification set sizes. Appendix D extends this with an empirical analysis of the bound tightness.\nWe extend Theorem 3.2 to govern the trade-off between performance guarantee and violation risk by\nallowing for the discarding of samples [14]. Instead of bounding the risk for the policy to achieve a\nperformance less than the minimum, we state a bound for the (k + 1)-th-smallest performance of the\nverification set (see Figure 7). Users can choose k for a permissible risk level and a potentially higher\nperformance guarantee, avoiding constraints from samples in the unlikely tail of the distribution."}, {"title": "4 Experimental Evaluation", "content": "We implement our approach on top of the PRISM verification tool [27], which provides trajectory\ngeneration and (robust) value iteration for MDPs and IMDPs. We compare several IMDP policy learn-\ning approaches for the training set (see Section 3.2). These include: (1) PAC learning (Section 3.1),\n(2) Linearly Updating Intervals [46], (3) UCRL2 reinforcement learning [3], and (4) Maximum\na-posteriori (MAP) point estimates [46]. We adapt a modified version of UCRL2 from Suilen et al.\n[46] for IMDPs. Details of each learning algorithm are provided in Appendix B.\nOur approach involves two sampling dimensions: sampling unknown MDPs and sampling trajectories\nwithin each unknown MDP. Increasing the amount of MDP samples reduces the risk for the perfor-\nmance guarantee. Increasing the number of sampled trajectories leads to tighter approximations,\naligning the observable performance guarantee closer to the actual performance. Both dimensions are\nessentially independent. The risk follows from the results in Section 3.3 and is detailed in Appendix D.\nThe following empirical analysis addresses the second dimension, and for each method we consider:\n\u2022 Robust Performance: Minimum performance on the true MDPs of the verification set,\nunknown to the algorithm. As the optimal robust performance is infeasible to compute, we\ncompare against the existential performance, the individual optimal policy of each MDP [7].\n\u2022 Robust Performance Guarantee: Robust performance guarantee J obtained on the IMDPs.\nThis is compared against the robust value of the policy computed under full knowledge of\nthe MDPs, which is the highest achievable performance guarantee using our approach.\nWe benchmark our method on a range of established environments: Aircraft Collision Avoidance [26],\nthe Chain Problem [1], a Betting Game [11], a Semi-Autonomous Vehicle [25, 45], and the Au-\ntonomous Drone [7]. We provide detailed descriptions of each environment in Appendix C. We use\nthe Autonomous Drone and the Chain Problem to highlight key properties and observations.\n\u2022 Autonomous Drone: The drone maneuvers in a 3D environment, starting from the origin (see\nFigure 5). It aims to reach a target zone while avoiding obstacles. There are 15 parameters\npi, one for each x-coordinate, influencing the probabilities of the drone drifting off. The\nevaluation function is the probability of reaching the goal without crashing into an obstacle.\n\u2022 Chain Problem: A chain of 6 states with three actions: (1) progressing to the next state with\nprobability p and falling back to the initial state with probability 1 p, (2) analogous with\ninverse probabilities, and (3) both scenarios with probability 1/2. The evaluation function is\nthe expected number of steps required to reach the last state.\nWe use a confidence level y = 10-4 for PAC learning, and M = 300 and N = 200 training and\nverification set samples. Each experiment is repeated 10 times and reported with standard deviations.\nAll experiments were conducted on a 3.1GHz Intel Xeon CPU with 4 cores and 16GB of memory."}, {"title": "5 Conclusion", "content": "We have presented a novel approach to learn provably robust policies for MDPs with epistemic\nuncertainty modelled using transition probabilities defined by parameters with unknown distributions.\nWe show that it yields good policies with sound and tight performance guarantees on a range\nof benchmarks. Future work includes learning robust policies in partially-observable uncertain\nenvironments. We do not see any plausible negative social impacts or risks of misuse for this work."}, {"title": "A Derivations and Proofs", "content": "We detail the proofs and derivations of our main theoretical contributions. We state our lemmas and theorems for\nthe case of maximizing the evaluation functions, which has implications for the linear programs and inequalities\nused. However all our results apply to the minimizing case by swapping the inequalities and directions of\noptimization.\nFirst, we present a range of results for incorporating additional uncertainty into the scenario approach [13, 14].\nWe then show how to formulate our problem as a randomized convex program-the scenario program-and\napply the adapted scenario theorems to derive our main results."}, {"title": "A.1 The Scenario Approach with Uncertain Constraints", "content": "We first present the basic setup for the scenario approach introduced by Campi and Garatti [13]. The ingredients\nfor the scenario approach are:\n(i) A cost function cx,\n(ii) An admissible region CC Rd,\n(iii) A family of convex constraints indexed by an uncertain parameter {Co \u2286 Rd | 0 \u2208 \u0398},\n(iv) A probability measure IP over \u0398.\nGiven a sample (01,..., 0v) of independent random parameters drawn from (\u0398, P), a scenario program is a\nlinear program over the corresponding convex constraints:\nmax\nxEC\ncTx\nsubject to\nx \u0395\n(12)\ni=1,...,N C\u03b8i\nwhich we specialise to c7 x = x. Let x* be the solution to the scenario program in Equation 12. The scenario\napproach provides the generalization theory to bound the violation probability defined as\nV(x) = P {\u03b8\u2208 \u0398: x \u2209 Co} .\n(13)\nThe violation probability is the probability that the solution to the scenario program is violating an unseen\nconstraint sampled from . From now on, we assume the sampled constraints C01,..., Con are unknown.\nInstead, we are given a set of known convex constraints \u0108e\u2081,..., \u0108en, for which \u0108o\u2081 \u2286 Co\u2081, \u22001 \u2264 i \u2264 N.\nFurthermore, we assume that all constraints, known or unknown are one-dimensional intervals of the form\nCo\u2081 = [a, be], for some constant a < bo\u2081, \u22001 < i < N. Note that a can be chosen as -\u221e. All our results hold\nfor this special case of constraints as one-dimensional intervals. Note that in the minimization case, the intervals\nwould be of the form [ao\u2081, b]. We first show that the violation probability of the solution obtained for the more\nconservative constraints Co, cannot be higher than for the unknown constraints Co.\nLemma A.1. Let a < x < y \u2208 R, it holds that V(x) < V(y).\nProof. Given a new constraint Ce = [a, be], we have that\nx & Co \u21d2 x > be \u21d2 y > be \u21d2 y & Co.\n(14)\nIt follows that\nV(x) = P {\u03b8\u2208 \u0398: x F Co} < P {\u03b8\u2208 \u04e8: y \u00a3 Co} = V(y).\n(15)\n\u03a0\nLemma A.2. Let x* \u2208 R and \u00ee* \u2208 R be the solutions to the scenario program in Equation 12 with constraints\nC01,..., Con and C01,..., \u0108on with \u0108o; \u2286 Co\u2081, \u22001 \u2264 i \u2264 N. It holds that\nV(x*) \u2264 V(x*).\n(16)\nProof. We show that the claim holds for our case of convex constraints of the form Co\u2081 = [a, bez]. It is easy to\nsee that the optimal solutions to the scenario program in Equation 12, with finitely many interval constraints are\nx* = min be, and * = min min bo\ni\n(17)\nNow \u0108o\u2081 \u2286 Co\u2081 is equivalent to [a, be\u2081] \u2286 [a, be\u2081], which implies that be\u2081 \u2264 be, and therefore \u00ee* < x*. The\nclaim follows by Lemma A.1.\nFurthermore, we show that a solution obtained for constraints C01,..., Con cannot have a higher violation\nprobability than the solution for any subset of the constraints.\nLemma A.3. Let x \u2208 R and x \u2208 R be the solutions to the scenario program in Euqation 12 with constraints\nC01,..., Con and C051,\u06f0\u06f0\u06f0, C0jM, for some R = {r1,...,rm} \u2286 {1,...,N} = I. It holds that\nV(x*) \u2264 V(x).\n(18)\nProof. Given that all constraints are of the form [a, bo,], it is easy to see that the optimal solutions to the scenario\nprogram in Equation 12, with finitely many interval constraints are\nx* = min be, and XR = min ber.\n\u0395\u0399\nrER\n(19)\nNow since RC I it follows that x* < xr. The claim follows by Lemma A.1.\nAs the last ingredient for the first main theorem, we show that violation risk with the solution obtained when\nremoving k arbitrary constraints cannot be higher than the solution obtained when removing the k worst-case\nconstraints.\nLemma A.4. Let XR ER be the solution to the scenario program in Equation 12 with constraints\nCor\u2081,..., Corm, for some R = {r1,...,rm} \u2286 {1,...,N} = I. Let XN\u201ak be the solution for constraints\nC01,..., Con that violates exactly k = N \u2013 M of the N constraints. It holds that\nV(x) \u2264 V(XN,k).\n(20)\nProof. Given that all constraints are of the form [a, bo,], it is easy to see that the optimal solution to the scenario\nprogram in Equation 12, with finitely many interval constraints Cor\u2081,..., Cerm is\nXR = min be.\nrER\n(21)\nFurther, the maximum solution that violates exactly k of the N constraints Co\u2081,..., CON is the (k + 1)-th\nsmallest be. Since |R| = M and k = N - M, it follows that XR \u2264 XNk. The claim follows by Lemma A.1."}, {"title": "A.2 Derivation of Theorem 3.2", "content": "We show that Theorem 3.2 follows as a special case of Theorem A.5. Consider an upMDP M and a policy \u03c0.\nGiven a evaluation function J, sampled parameters 0\u2082 ~ P induce convex constraints of the form\nCo\u2081 = (-\u221e, J(\u03c0, \u039c[\u03b8\u03ad])].\nSimilarly, learned IMDP over-approximations M^ [0i], where M[0] \u2286 MV[0i], imply that J(\u03c0, \u039c^[0]) \u2264\nJ(\u03c0, \u039c[\u03b8]), inducing under-approximations of the constraints\n\u0108e\u2081 = (-\u221e, J(\u03c0, \u039c^[0])] \u2286 (\u2212\u221e, J(\u03c0, \u039c[\u03b8i])].\nSince P{M[0] \u2286 M^[0]} \u2265 1 \u2013 y by construction of the IMDPs, Theorem A.5 becomes applicable to the\nsolution * = J = min\u2081 J(\u03c0, \u039c [\u03b8]).\nTheorem A.6 (3.2). Given N i.i.d. sample MDPs M[0i] and IMDPs M' [0i], such that P{M[0i] \u2286 M^ [0]} \u2265\n1 \u2013 \u03b3. For any policy \u3160 and confidence level 1 \u2013 \u03b7, with \u03b7 > 0, it holds that\nPN {r(\u03c0, \u00ce) \u2264 \u03b5(\u039d, \u03b3, \u03b7) } \u2265 1 -\u03b7,\n(36)\nwhere V = mini J(\u03c0, M\u00b2 [0i]), and \u025b(\u039d, \u03b3, \u03b7) is the solution to\nN\n\u03a3 (4.7) () (1 \u2212 \u03b3)\u03ad\u03b3\u039d\u03ad = 1 \u2212 (37)\ni=M\nfor any M < N.\nProof. By applying Theorem A.5, we obtain\n(48) (47) - \u03b2 (38)\ni=M\nwith k N-M, M < N.\nEquating the right-hand side to 1 \u03b7, we obtain the following range of permissible \u03b2:\n\u03b2\u2264\u03a3\n\u03a3\u03b5 (4-7) (1 - \u03b3)\u03ad\u03b3- \u2013 (1 - \u03b7).\ni=M\nSince the risk &(N, k, \u03b2) for fixed N and k increases as \u1e9e decreases, the smallest risk is obtained for the largest\npossible \u00df, which corresponds to the smallest possible confidence that adds up to the desired confidence 1 \u03b7.\nTherefore, substituting\nN\n4 (48) (1 - \u03b3)\u03ad\u03b3- \u2013 (1 - \u03b7)\ninto Equations 29 and 38 concludes the proof."}, {"title": "A.3 Derivation of Theorem 3.3", "content": "To incorporate sample discarding into the setup with uncertain constraints, we adapt the reasoning above to\nexclude a fixed number I of the N observable constraints \u0108\u0259\u2081,..., \u0108on. Let L \u2286 {1, ..., N} with |L| = l be\nthe indices of the discarded constraints. The probability that there exists a subset of constraints with indices\nR= {1,...,r} \u2286 {1, ..., N} \\ L, which all contain their under-approximations \u0108o, \u2286 Cor, \u2200r \u2208 Ris\nPN {\u2203R \u2286 {1, ..., N} \\ L: |R| = M and Vr \u2208 R: \u0108o, \u2286 Co} \u03a3 (1)\ni=M\n(1 \u2212 \u03b3)\u03af\u03b3N-l-\u03ad (39)\nAnalogous to Equation 23, we transform Equation 39 into\nPN {V(N,1) \u2264 V(x,m)} \u2265 \u03a3 () (1 \u2212 \u03b3)\u03ad\u03b3-\u03b9-\u03af, (40)\ni=M\nwhere x1 is the solution for constraints \u0108\u04e9\u2081,...,\u0108en without the indices L, and m = N \u2013 M."}, {"title": "B Interval MDP Learning Algorithms", "content": "We detail the used learning algorithms (1) PAC learning (Section 3.1), (2) Linearly Updating Intervals [46], (3)\nUCRL2 reinforcement learning [3], and (4) Maximum a-posteriori (MAP) point estimates [46]. PAC learning is\nfully described in Section 3.1 and is applied in policy learning the exact same way we use it in the learning of\nIMDP overapproximations of the verification set."}, {"title": "B.1 Linearly Updating Intervals", "content": "Linearly Updating Intervals (LUI) is a recent approach for learning IMDPs from sample trajectories of an\nunknown MDP, introduced by Suilen et al. [46]. It exploits the Bayesian approach of intervals with linearly\nupdating conjugate priors [46, 49]. Although the learned IMDP does not guarantee inclusion of the underlying\nMDP, it has been empirically shown to be tighter while remaining sound. For each uncertain transition P(s, a, s'),\nLUI updates the interval P\u00b9 = [P\u00b9,P\u00b9], known as the prior interval, and the prior strength n.\nGiven state-action count N = #(s, a) and transition count k = #(s, a, s') from sample trajectories, the prior\ninterval is updated to the posterior interval, as follows:\nP =\nnPI + k\nn+N  ,\nP =\nnP + k\nn+N ,\nwith the posterior strength n' = n + N. In our experiments, we initialize the prior intervals for each unknown\ntransition as [\u03b5, 1] and set the prior strength to n = 0."}, {"title": "B.2 Maximum A-Poster"}]}