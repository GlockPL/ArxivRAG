{"title": "Learning Provably Robust Policies\nin Uncertain Parametric Environments", "authors": ["Yannik Schnitzer", "Alessandro Abate", "David Parker"], "abstract": "We present a data-driven approach for learning MDP policies that are robust across\nstochastic environments whose transition probabilities are defined by parameters\nwith an unknown distribution. We produce probably approximately correct (PAC)\nguarantees for the performance of these learned policies in a new, unseen envi-\nronment over the unknown distribution. Our approach is based on finite samples\nof the MDP environments, for each of which we build an approximation of the\nmodel as an interval MDP, by exploring a set of generated trajectories. We use the\nbuilt approximations to synthesise a single policy that performs well (meets given\nrequirements) across the sampled environments, and furthermore bound its risk (of\nnot meeting the given requirements) when deployed in an unseen environment. Our\nprocedure offers a trade-off between the guaranteed performance of the learned\npolicy and the risk of not meeting the guarantee in an unseen environment. Our\napproach exploits knowledge of the environment's state space and graph structure,\nand we show how additional knowledge of its parametric structure can be lever-\naged to optimize learning and to obtain tighter guarantees from less samples. We\nevaluate our approach on a diverse range of established benchmarks, demonstrating\nthat we can generate highly performing and robust policies, along with guarantees\nthat tightly quantify their performance and the associated risk.", "sections": [{"title": "1 Introduction", "content": "Ensuring the safety and robustness of autonomous systems in safety-critical tasks, such as unmanned\naerial vehicles (UAVs), robotics or autonomous control, is paramount. A standard model for sequential\ndecision making in these settings is a Markov decision process (MDP), which provides a stochastic\nmodel of the environment. However, real-world dynamics are complex, with parameters that are\nunknown and may evolve. Therefore, models with epistemic uncertainty [6, 7, 16, 17, 21, 25, 30, 37,\n51] are considered which account for uncertainty resulting from a lack of knowledge of the system or\nthe environment under investigation. Uncertain models help construct robust policies that perform\nwell across the various possible stochastic environments.\nConsider the UAV motion planning problem shown in Figure 5, adapted from [7]. The goal is to\nnavigate the drone safely to the target zone, without obstacle collisions. The drone's dynamics are\ninfluenced by weather conditions such as wind or precipitation. In low disturbance conditions (e.g.,\nlow wind), the drone can safely take the shorter route to the target. Under high disturbances, a"}, {"title": "2 Preliminaries", "content": "We introduce the fundamental formalisms underpinning our approach. Let \u2206(S) = {\u03bc: S \u2192 [0, 1] |\n\u03a3s\u2208S \u03bc(s) = 1} denote the set of all probability distributions over a finite set S.\nDefinition 2.1 (Parametric MDP). A parametric Markov decision process (pMDP) is a tuple M\u0473 =\n(S, s\u2081, A, P\u0473), where S and A are finite state and action spaces, s\u2081 \u2208 \u2206(S) is the initial state\ndistribution, and P\u0473 : \u0398 \u00d7 S \u00d7 A \u2192 \u25b3(S) is the parametric transition probability function over the\nparameter space \u0398. Fixing a parameterization \u03b8 \u2208 \u0398 induces a standard MDP M\u0473[0], or M[0] for\nshort, with transition kernel P\u0473: S \u00d7 A \u2192 \u2206(S) defined as P\u0473(s, a, s') = P\u0473(\u03b8, s, a, s').\nFigure 4a shows a simple 4-state pMDP with 3 parameters. We remark that pMDPs can be seen\nas modelling a set of MDPs, i.e., the instantiations induced by all possible parameters \u03b8\u2208 \u0398. \u0399n\ndoing so, they are closely related to the most general class of uncertain MDPs [30, 51], where each\ntransition is associated with (potentially interdependent) sets of possible values.\nDefinition 2.2 (Uncertain Parametric MDP). An uncertain parametric Markov decision process\n(upMDP) M = (M\u0473, P) is a pMDP M\u0473 with a probability measure P over the parameters \u0398.\nWe assume that upMDPs are graph preserving, meaning that the underlying graph of all in-\nduced MDPs is the same: \u2200s,s' \u2208 S,a \u2208 A: (0 \u2208 supp(P): P\u0473(s,a,s') = 0) v (\u03b8\u03b5\nsupp(P): P\u0473(s, a, s') > 0), and that this graph structure is known. This is a common assump-\ntion when using these models (and indeed the closely related interval MDPs models [51]).\nPolicies are mappings \u03c0 : (S \u00d7 A)* \u00d7 S \u2192 \u2206(A) from (finite) histories of states and actions to a\ndistribution over the next actions. In this work, we mainly focus on learning memoryless-deterministic\npolicies \u03c0: S \u2192 A, but all results carry over to other classes of policies. We show that memoryless-\ndeterministic policies achieve high performances on relevant benchmarks and elaborate on cases\nwhere more expressive policy classes can be advantageous (see Section 3.2).\nDefinition 2.3 (Evaluation Function). For a upMDP M = (M\u0473, P), an evaluation function\nJ: \u041f \u00d7 \u04e8 \u2192 R maps a policy \u03c0 and a parameter value 0 onto a performance value of \u03c0 on M[0].\nWe also use J(\u03c0, M) for the evaluation of policy \u03c0 on MDP M, i.e., J(\u03c0, 0) = J(\u03c0, \u039c[\u03b8]).\nCommonly used evaluation functions are, e.g., (un-)discounted expected reward [47] or the probability\nof satisfying requirement over trajectories [10]. For a fixed policy \u03c0 of an upMDP MD, an evaluation\nfunction J only depends on the parameters 0. Hence, J is a random variable with measure IP (see\nFigure 2). One possibility, ignoring stochasticity, is to learn policies for M that are robust against\nall possible parameters. The robust value of a policy is the minimal (worst-case) achieved under any\npossible parameterization (see Figure 2): J(\u03c0) = inf\u03b8\u2208supp(P) J(\u03c0, \u03b8).\nEven with full knowledge of and P, finding J(\u03c0) is a challenging problem. While computation can\nbe feasible under certain convexity and rectangularity assumptions on the parametric structure [30, 51],\nthese approaches do not apply to unknown, arbitrary O and P. Furthermore, we specifically exploit\nthe probabilistic nature of upMDPs M, by not necessarily considering a policy's worst-case\nperformance, but rather its violation risk.\nDefinition 2.4 (Violation Risk). The violation risk of policy \u03c0 and performance guarantee \u0134 \u2208 R is:\n\\begin{equation}\nr(\\pi, \\hat{J}) = \\mathbb{P} \\{\\theta\\in \\Theta: J(\\pi,\\theta) < \\hat{J}\\}.\n\\end{equation}\n(1)\nThe violation risk is the probability that policy \u03c0 achieves a value less than a stated performance\nguarantee J on MPD (see Figure 3). There is an inherent trade-off between violation risk and\nperformance guarantee: a higher performance guarantee is associated with a higher risk, regardless\nof M and J. Quantifying the violation risk offers more flexibility than computing the worst-case\nperformance J(\u03c0), which ignores the likelihood of worst-case scenarios and can lead to overly\nconservative guarantees based on unlikely parameterizations.\nSince we do not assume any knowledge of the parameter space or the probability measure P, it\nis unrealistic to assume we could compute the policy violation risk exactly. Thus, our goal is to\nprovide a sound and tight upper bound by analyzing sample MDPs from the underlying upMDP. Our\napproach builds and solves model approximations represented as interval MDPs.\nDefinition 2.5 (Interval Markov Decision Processes). An interval Markov decision process (IMDP)\nMI = (S, s\u2081, A, PI) is an MDP with a probabilistic interval transition function PI : S \u00d7 A \u00d7 S \u2192 I,"}, {"title": "3 Learning Provably Robust Policies", "content": "In this paper, we approximate sampled unknown environments by analysing trajectories. Using these\nlearned estimates, we construct robust policies, evaluate their performance and bound their risk in\nunseen environments. The overall problem can be stated as follows.\nProblem 1. Given a upMDP M with unknown parameter distribution P, an evaluation\nfunction J, and a confidence level \u03b7 > 0, find a robust policy \u03c0, a performance guarantee J,\nand a risk bound \u025b > 0, such that:\n\\begin{equation}\n\\mathbb{P} \\{r(\\pi, J) \\leq \\varepsilon\\} \\geq 1 - \\eta.\n\\end{equation}\nFrom here on, we consider a fixed upMDP M. The input to our procedure is a set D = {M[0] |\n0 ~ P} of partially unknown MDPs sampled from M. We split this set into disjoint training\nand verification sets TM UVN = D of sizes M and N. The training set is used to learn a robust\npolicy, which is then evaluated on the verification set to derive a performance guarantee and bound\nthe violation risk when deployed in an unseen environment sampled from P. Since the MDPs in D\nare partially unknown, we estimate the unknown parts from sample trajectories. For the verification\nset, we build IMDPs that include the unknown MDPs with high confidence (see Fig. 4c)."}, {"title": "3.1 PAC IMDP Learning", "content": "We follow the established approach for PAC learning of IMDP approximations introduced by Strehl\nand Littman [44]. Consider a partially unknown MDP M[0]. We assume access to trajectories from\nM[0], consisting of sequences of triples (s, a, s') representing states, chosen actions, and successor\nstates. Leveraging the Markov property of MDPs, we treat each triple as an independent Bernoulli\nexperiment to estimate the transition probability to state s'. We denote the number of times action a\nwas chosen in state s across all sample trajectories as #(s, a), and the number of times this choice\nled to a transition to s' as #(s, a, s'). The resulting probability point estimate is thus given by:\n\\begin{equation}\n\\mathbb{P}(s,a,s') = \\frac{\\#(s,a, s')}{\\#(s,a)}, \n\\end{equation}\n(3)\nfor #(s, a) > 0. We construct an IMDP MY [0] by transforming the point estimates into PAC\nintervals using Hoeffding's inequality [39]. Let 1 \u2013 \u03b3 with \u03b3 > 0 be the desired confidence level for\nM[0] to be included in MV[02]. This confidence is distributed over all nu unknown transitions as\nYP = y/nu. Let H = #(s, a), for each unknown transition, the interval is given by:\n\\begin{equation}\n\\mathbb{P}'(s, a, s') = [\\text{max}(\\mu, \\mathbb{P}(s, a, s') \u2013 \\delta), \\text{min}(\\mathbb{P}(s, a, s') + \\delta, 1)],\n\\end{equation}\n(4)\nwith \u03b4 = \u221alog (2/\u03b3\u03c1)/2H and \u03bc > 0 being an arbitrarily small quantity to preserve the known\ngraph structure. For unvisited state action pairs with #(s, a) = 0, we set P(s, a, s') = [\u03bc, 1],\nfor all s'. According to Hoeffding's inequality, PV(s, a, s') contains the true transition probability\nP(s, a, s') with a probability of at least 1 Yp. By applying a union bound over the unknown\ntransitions, Strehl and Littman [44] derive the following overall guarantee:\nLemma 3.1. The true, partially unknown MDP M[0i] is contained in its IMDP overapproximation\nMY [0] with probability at least 1 \u2013 \u03b3."}, {"title": "3.1.1 Model-based Optimizations", "content": "Our procedure does not require any model knowledge beyond its graph structure. However, we\ncan leverage available information on the environment's parametric structure to achieve tighter\napproximations from fewer samples. Consider two transitions sharing the same parameterization\nP(s,a,s') = P(t,b,t') = \u00f4, though \u00ee\u00ee is unknown (see Figure 4b). Parameter tying [32, 33]\ncan be employed to combine counts from both transitions. Let sim(s,a,s') = {(t,b,t') |\nP(s,a,s') = P(t,b,t')} denote the set of transitions with equal parameterization. Define\n#T (s, a, s') = \u2211(t,b,t')\u2208sim(s,a,s') #(t, b, t') and HT (s, a, s') = \u2211(t,b,t')\u2208sim(s,a,s') #(t, b). The\nparameter-tied point estimate and interval width are given by:\n\\begin{equation}\n\\mathbb{P}(s, a, s') = \\frac{\\#_T(s, a, s')}{\\#_T(s, a, s')} \\quad \\text{and} \\quad \\delta_P = \\sqrt{\\frac{\\text{log } (2/\\gamma)}{2H_T(s,a)}}\n\\end{equation}\n(5)\nAppendix E investigates the impact of parameter tying on the tightness of the obtained approximation\nand demonstrates that it can accelerate learning by multiple orders of magnitude."}, {"title": "3.2 Robust Policy Learning", "content": "We use the training set TM of partially unknown MDPs to learn a policy that achieves high perfor-\nmance across all training samples. Since risk evaluation will be conducted ex post on the PAC IMDPS\nfrom the verification set, we have flexibility in our choice of method. With full knowledge of the\nsampled MDPs, the robust policy problem coincides with finding the optimal policy for multiple-\nenvironment MDPs [35], uncertain MDPs with only finitely many possible environments. It was\nshown by Raskin and Sankur [35] that the optimal policy may require for infinite memory. Intuitively,\nthe optimal policy explores and determines which of the environments it is operating in and follows\nthe optimal policy for that environment. Despite being infeasible to compute even under full knowl-\nedge, this policy can be seen as perfectly overfitted to the training set and may not generalize well to\nunseen environments. Computing the optimal robust memoryless policy is NP-hard, even for just two\nfully known environments [35]. Rickard et al. [36] introduced an approximate procedure based on\npolicy gradient, which was shown to be able to produce good, randomized policies. However, the\npolicy gradient algorithm requires solution of every MDP sample in every iteration of the algorithm\nfor the gradient step, rendering it feasible only on small training sets consisting of small MDPs.\nWhile any of the procedures above can be used to find robust policies within our framework, we opt\nfor a simple yet effective overapproximation approach that aligns well with our setup of sampled\npartially unknown MDPs M[0]. Similar to PAC IMDP learning, we analyze trajectories for each\npartially unknown MDP in the training set to construct an IMDP. However, since we use these\nIMDPS solely for policy learning, not for uncertainty quantification, we are not restricted to PAC\nlearning. This flexibility allows for the use of various interval learning algorithms that, while lacking\ninclusion guarantees, offer empirically tighter approximations from fewer samples. In Section 4, we\ncompare IMDP learning using: (1) PAC learning as described in Section 3.1; (2) Linearly Updating\nIntervals [46], a recent Bayesian approach; (3) the UCRL2 reinforcement learning algorithm [3]; and\n(4) Maximum a-posteriori (MAP) point estimates [46]. Detailed descriptions of these IMDP learning\nalgorithms are given in Appendix B. All methods can be enhanced by model-based optimizations."}, {"title": "3.3 Risk Evaluation", "content": "We use the verification set from Section 3.1 to bound the violation risk of the learned policy \u03c0 when\napplied in an unseen environment. We analyze \u03c0 on the PAC IMDPs V\u00a5 = {M\u00a5[0i]}1<i<N to\nbound the policy's performance on the underlying unknown MDPs. From Equation 2, we have\nM[0] \u2286 M\u00b2 [0] \u21d2 J(\u03c0, \u039c^[0i]) \u2264 J(\u03c0, \u039c[\u03b8]). From Lemma 3.1, it follows that\n\\begin{equation}\n\\mathbb{P}\\{J(\\pi, \\mathbb{M}^V[\\theta]) \\leq J(\\pi, \\mathbb{M}[\\theta_i])\\} \\geq 1 \u2013 \\gamma.\n\\end{equation}\n(7)\nTo obtain a bound on the violation risk, we formulate the problem as a convex optimization problem\nwith randomized constraints\u2014a scenario program [15]. We leverage the generalization theory of\nthe scenario approach by Campi and Garatti [13], adapting it to incorporate the uncertainty of\nperformance values obtained on the learned IMDPs to under-approximate the performances on the\nunderlying MDPs (see Equation 7). We detail the formulation and the derivation of the generalization\nfrom approximations of unknown samples to the entire unknown distribution in Appendix A.\nTheorem 3.2. Given N i.i.d. sample MDPs M[0i] and IMDPs MV[0i], such that P{M[0] \u2286\n\u039c\u03a5 [\u03b8]} \u2265 1 \u2013 \u03b3. For any policy \u3160 and confidence level 1 \u03b7, with \u03b7 > 0, it holds that:\n\\begin{equation}\n\\mathbb{P}^N \\{r(\\pi, J) \\leq \\varepsilon(N, \\gamma, \\eta)\\} \\geq 1 -\\eta,\n\\end{equation}\nwhere J = min\u2081 J(\u03c0, \u039c^[0i]), and \u025b(N,\u03b3, \u03b7) is the solution to the following, for any M < N:\n\\begin{equation}\n\\sum_{i=M}^{N} {N \\choose i} (1 - \\gamma)^i \\gamma^{N - i} - \\eta = \\sum_{i=0}^{N-M} {N \\choose i} \\varepsilon^{i} (1 \u2013 \\varepsilon)^{N-i},\n\\end{equation}\n(9)\nTheorem 3.2 bounds the violation risk for the policy to achieve a performance less than the minimum\nperformance on any verification IMDP. This bound only depends on values we can observe from\nthe learned approximations. The theorem includes a tuning parameter M < N. To obtain the\ntightest bound, our procedure enumerates possible values for M and solves the resulting equation\n(see Appendix A). For a fixed M, the left-hand side of Equation 9 is constant, and the right-hand\nside is the cumulative distribution function of a beta distribution with M + 1 and N - M degrees of\nfreedom [15], which is easy to solve numerically for its unique solution in the interval [0, 1] using\nbisection [7, 39]. Figure 6 shows the obtained risk bounds for various combinations of confidences\nand verification set sizes. Appendix D extends this with an empirical analysis of the bound tightness.\nWe extend Theorem 3.2 to govern the trade-off between performance guarantee and violation risk by\nallowing for the discarding of samples [14]. Instead of bounding the risk for the policy to achieve a\nperformance less than the minimum, we state a bound for the (k + 1)-th-smallest performance of the\nverification set (see Figure 7). Users can choose k for a permissible risk level and a potentially higher\nperformance guarantee, avoiding constraints from samples in the unlikely tail of the distribution."}, {"title": "4 Experimental Evaluation", "content": "We implement our approach on top of the PRISM verification tool [27], which provides trajectory\ngeneration and (robust) value iteration for MDPs and IMDPs. We compare several IMDP policy learn-\ning approaches for the training set (see Section 3.2). These include: (1) PAC learning (Section 3.1),\n(2) Linearly Updating Intervals [46], (3) UCRL2 reinforcement learning [3], and (4) Maximum\na-posteriori (MAP) point estimates [46]. We adapt a modified version of UCRL2 from Suilen et al.\n[46] for IMDPs. Details of each learning algorithm are provided in Appendix B.\nOur approach involves two sampling dimensions: sampling unknown MDPs and sampling trajectories\nwithin each unknown MDP. Increasing the amount of MDP samples reduces the risk for the perfor-\nmance guarantee. Increasing the number of sampled trajectories leads to tighter approximations,\naligning the observable performance guarantee closer to the actual performance. Both dimensions are\nessentially independent. The risk follows from the results in Section 3.3 and is detailed in Appendix D.\nThe following empirical analysis addresses the second dimension, and for each method we consider:\n\u2022 Robust Performance: Minimum performance on the true MDPs of the verification set,\nunknown to the algorithm. As the optimal robust performance is infeasible to compute, we\ncompare against the existential performance, the individual optimal policy of each MDP [7].\n\u2022 Robust Performance Guarantee: Robust performance guarantee J obtained on the IMDPs.\nThis is compared against the robust value of the policy computed under full knowledge of\nthe MDPs, which is the highest achievable performance guarantee using our approach.\nWe benchmark our method on a range of established environments: Aircraft Collision Avoidance [26],\nthe Chain Problem [1], a Betting Game [11], a Semi-Autonomous Vehicle [25, 45], and the Au-\ntonomous Drone [7]. We provide detailed descriptions of each environment in Appendix C. We use\nthe Autonomous Drone and the Chain Problem to highlight key properties and observations.\n\u2022 Autonomous Drone: The drone maneuvers in a 3D environment, starting from the origin (see\nFigure 5). It aims to reach a target zone while avoiding obstacles. There are 15 parameters"}, {"title": "5 Conclusion", "content": "We have presented a novel approach to learn provably robust policies for MDPs with epistemic\nuncertainty modelled using transition probabilities defined by parameters with unknown distributions.\nWe show that it yields good policies with sound and tight performance guarantees on a range\nof benchmarks. Future work includes learning robust policies in partially-observable uncertain\nenvironments. We do not see any plausible negative social impacts or risks of misuse for this work."}, {"title": "A Derivations and Proofs", "content": "We detail the proofs and derivations of our main theoretical contributions. We state our lemmas and theorems for\nthe case of maximizing the evaluation functions, which has implications for the linear programs and inequalities\nused. However all our results apply to the minimizing case by swapping the inequalities and directions of\noptimization.\nFirst, we present a range of results for incorporating additional uncertainty into the scenario approach [13, 14].\nWe then show how to formulate our problem as a randomized convex program-the scenario program-and\napply the adapted scenario theorems to derive our main results."}, {"title": "A.1 The Scenario Approach with Uncertain Constraints", "content": "We first present the basic setup for the scenario approach introduced by Campi and Garatti [13]. The ingredients\nfor the scenario approach are:\n(i) A cost function cx,\n(ii) An admissible region CC Rd,\n(iii) A family of convex constraints indexed by an uncertain parameter {Co \u2286 Rd | 0 \u2208 \u0398},\n(iv) A probability measure IP over \u0398.\nGiven a sample (01,..., 0v) of independent random parameters drawn from (\u0398, P), a scenario program is a\nlinear program over the corresponding convex constraints:\n\\begin{equation}\n\\begin{array}{ll}\n\\max & c^{\\top} x \\\\\n\\text{subject to} & x \\in C \\cap \\bigcap_{i=1,...,N} C_{\\theta_i}\n\\end{array}\n\\end{equation}\n(12)\nwhich we specialise to c7 x = x. Let x* be the solution to the scenario program in Equation 12. The scenario\napproach provides the generalization theory to bound the violation probability defined as\n\\begin{equation}\nV(x) = \\mathbb{P} \\{\\theta\\in \\Theta: x \\notin C_{\\theta}\\} .\n\\end{equation}\n(13)\nThe violation probability is the probability that the solution to the scenario program is violating an unseen\nconstraint sampled from . From now on, we assume the sampled constraints C01,..., Con are unknown.\nInstead, we are given a set of known convex constraints \u0108e\u2081,..., \u0108en, for which \u0108o\u2081 \u2286 Co\u2081, \u22001 \u2264 i \u2264 N."}, {"title": "A.2 Derivation of Theorem 3.2", "content": "We show that Theorem 3.2 follows as a special case of Theorem A.5. Consider an upMDP M and a policy \u03c0.\nGiven a evaluation function J, sampled parameters 0\u2082 ~ P induce convex constraints of the form\nCo\u2081 = (-\u221e, J(\u03c0, \u039c[\u03b8\u03ad])].\nSimilarly, learned IMDP over-approximations M^ [0i], where M[0] \u2286 MV[0i], imply that J(\u03c0, \u039c^[0]) \u2264\nJ(\u03c0, \u039c[\u03b8]), inducing under-approximations of the constraints\n\u0108e\u2081 = (-\u221e, J(\u03c0, \u039c^[0])] \u2286 (\u2212\u221e, J(\u03c0, \u039c[\u03b8i])].\nSince P{M[0] \u2286 M^[0]} \u2265 1 \u2013 y by construction of the IMDPs, Theorem A.5 becomes applicable to the\nsolution * = J = min\u2081 J(\u03c0, \u039c [\u03b8]).\nTheorem A.6 (3.2). Given N i.i.d. sample MDPs M[0i] and IMDPs M' [0i], such that P{M[0i] \u2286 M^ [0]} \u2265\n1 \u2013 \u03b3. For any policy \u3160 and confidence level 1 \u2013 \u03b7, with \u03b7 > 0, it holds that\n\\begin{equation}\n\\mathbb{P}^N \\{r(\\pi, \\hat{J}) \\leq \\varepsilon(N, \\gamma, \\eta) \\} \\geq 1 -\\eta,\n\\end{equation}\n(36)\nwhere V = mini J(\u03c0, M\u00b2 [0i]), and \u025b(\u039d, \u03b3, \u03b7) is the solution to\n\\begin{equation}\n\\sum_{i=M}^{N} {N \\choose i} (1 - \\gamma)^i \\gamma^{N - i} - \\eta = \\sum_{i=0}^{N-M} {N \\choose i} \\varepsilon^{i} (1 \u2013 \\varepsilon)^{N-i},\n\\end{equation}\n(37)\nfor any M < N.\nProof. By applying Theorem A.5, we obtain\n\\begin{equation}\n\\mathbb{P}^N \\{r(\\pi, \\hat{J}) \\leq \\varepsilon(N, k, \\beta) \\} \\geq \\sum_{i=M}^{N} {N \\choose i} (1 - \\gamma)^i \\gamma^{N - i} - \\beta,\n\\end{equation}\n(38)\nwith k N-M, M < N.\nEquating the right-hand side to 1 \u03b7, we obtain the following range of permissible \u03b2:\n\\begin{equation}\n\\beta \\leq \\sum_{i=M}^{N} {N \\choose i} (1 - \\gamma)^i \\gamma^{N - i} \u2013 (1 - \\eta).\n\\end{equation}\nSince the risk &(N, k, \u03b2) for fixed N and k increases as \u1e9e decreases, the smallest risk is obtained for the largest\npossible \u00df, which corresponds to the smallest possible confidence that adds up to the desired confidence 1 \u03b7.\nTherefore, substituting\n\\begin{equation}\n\\beta = \\sum_{i=M}^{N} {N \\choose i} (1 - \\gamma)^i \\gamma^{N - i} \u2013 (1 - \\eta)\n\\end{equation}\ninto Equations 29 and 38 concludes the proof."}, {"title": "A.3 Derivation of Theorem 3.3", "content": "To incorporate sample discarding into the setup with uncertain constraints, we adapt the reasoning above to\nexclude a fixed number I of the N observable constraints \u0108\u0259\u2081,..., \u0108on. Let L \u2286 {1, ..., N} with |L| = l be\nthe indices of the discarded constraints. The probability that there exists a subset of constraints with indices\nR= {1,...,r} \u2286 {1, ..., N} \\ L, which all contain their under-approximations \u0108o, \u2286 Cor, \u2200r \u2208 Ris\n\\begin{equation}\n\\mathbb{P}^{-N} \\{\\exists R \\subseteq \\{1, ..., N\\} \\backslash L: |R| = M \\text{ and } \\forall r \\in R: \\hat{C}_{\\theta_r} \\subseteq C_{\\theta_r}\\} \\geq \\sum_{i=M}^{N-l} {N-l \\choose i} (1 - \\gamma)^i \\gamma^{N-l-i}.(39)\n\\end{equation}\nAnalogous to Equation 23, we transform Equation 39 into\n\\begin{equation}\n\\mathbb{P}^N \\{V(\\hat{x}_{N,l}) \\leq V(x^*_{N,m})\\} \\geq \\sum_{i=M}^{N-l} {N-l \\choose i} (1 - \\gamma)^i \\gamma^{N-l-i},\n\\end{equation}\n(40)\nwhere x1 is the solution for constraints \u0108\u04e9\u2081,...,\u0108en without the indices L, and m = N \u2013 M."}, {"title": "B Interval MDP Learning Algorithms", "content": "We detail the used learning algorithms (1) PAC learning (Section 3.1), (2) Linearly Updating Intervals [46], (3)\nUCRL2 reinforcement learning [3], and (4) Maximum a-posteriori (MAP) point estimates [46]. PAC learning is\nfully described in Section 3.1 and is applied in policy learning the exact same way we use it in the learning of\nIMDP overapproximations of the verification set."}, {"title": "B.1 Linearly Updating Intervals", "content": "Linearly Updating Intervals (LUI) is a recent approach for learning IMDPs from sample trajectories of an\nunknown MDP, introduced by Suilen et al. [46]. It exploits the Bayesian approach of intervals with linearly\nupdating conjugate priors [46, 49]. Although the learned IMDP does not guarantee inclusion of the underlying\nMDP, it has been empirically shown to be tighter while remaining sound. For each uncertain transition P(s, a, s'),\nLUI updates the interval P\u00b9 = [P\u00b9,P\u00b9], known as the prior interval, and the prior strength n.\nGiven state-action count N = #(s, a) and transition count k = #(s, a, s') from sample trajectories, the prior\ninterval is updated to the posterior interval, as follows:\n\\begin{aligned}\n\\underline{P} &= \\frac{n\\underline{P} + k}{n+N},\\\\\n\\overline{P} &= \\frac{n\\overline{P} + k}{n+N},\n\\end{aligned}\nwith the posterior strength n' = n + N. In our experiments, we initialize the prior intervals for each unknown\ntransition as [\u03b5, 1] and set the prior strength to n = 0."}, {"title": "B.2 Maximum A-Posteriori Point Estimates", "content": "Maximum a-posteriori (MAP) point estimates are a well-known principle from Bayesian statistics and param-\neter estimation [12]. Given an unknown MDP with transition probabilities P(s, a, si) for the m successors\n$1,..., Sm of a state-action pair (s, a), the probability of observing ki = #(s, a, si) transitions for each\nsuccessor, given N = #(s, a) trials, follows a multinomial distribution:\n\\begin{equation}\nf(k_1, ..., k_m | P) = \\frac{N!}{k_1! ... k_m!} \\prod_{i=1}^{m} P(s, a, s_i).\n\\end{equation}"}, {"title": "C Benchmark Environments", "content": "We detail the remaining benchmark environments used in our experimental evaluation in Section 4.\nBetting Game The betting game is a reward maximization benchmark introduced by B\u00e4uerle and Ott [11].\nThe player starts with 10 coins and can sequentially place n bets, risking either 0, 1, 2, 5, or 10 coins. With\nprobability p, the player wins and earns double the bet; with probability 1 \u2013 p, the bet is lost. The goal is to\nmaximize the number of coins after n bets. The evaluation function is the expected number of coins after n bets.\nWe consider a version with n = 8 rounds of betting.\nAircraft Collision Avoidance The aircraft collision avoidance environment is a simplified version of the\nrich set of models introduced by Kochenderfer [26]. We consider a 10 \u00d7 5 grid where two aircraft, one controlled\nby our agent and one adversarial, fly towards each other. In each step, both pilots may choose to fly straight, up,\nor down, succeeding with probabilities p and q, respectively. The goal is for the agent to reach the opposite end\nof the grid without colliding with the adversarial aircraft, which maneuvers arbitrarily. The evaluation function\nis the probability of the agent reaching the goal zone without colliding.\nSemi-Autonomous Vehicle The semi-autonomous vehicle benchmark, introduced by St\u00fcckler et al. [45]\nand formalized as a PRISM model by Junges et al. [25], models an explorer moving through a grid while\ncommunicating with a controller via two faulty channels. The probabilities of each channel losing a message\ndepend on parameters p and q, and the agent's current position. In each step, the agent can either communicate\nover a chosen channel for a limited number of tries or move in a desired direction. The agent can only move a\ncertain number of steps without successful communication; otherwise, the task fails. The evaluation function\nis the probability of the agent reaching a goal zone without exceeding the maximum number of steps without\ncommunication. We consider a 10 \u00d7 5 grid, a maximum of two communication trials, and only two allowed\nmoves without successful communication."}, {"title": "D Risk Bounds", "content": "We present the risk bounds derived from our main theorems in Section 3.3. Figure 9 displays the risk bounds\nwithout discarding any samples, based on Theorem 3.2, for typical values of IMDP inclusion probability 1 \u2013 7\nand overall confidence 1 \u03b7. Figure 10 illustrates the risk when 5% of the verification samples are discarded"}]}