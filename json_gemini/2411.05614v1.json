{"title": "Acceleration for Deep Reinforcement Learning using Parallel and Distributed Computing: A Survey", "authors": ["ZHIHONG LIU", "XIN XU", "PENG QIAO", "DONGSHENG LI"], "abstract": "Deep reinforcement learning has led to dramatic breakthroughs in the field of artificial intelligence for the past few years. As the amount of rollout experience data and the size of neural networks for deep reinforcement learning have grown continuously, handling the training process and reducing the time consumption using parallel and distributed computing is becoming an urgent and essential desire. In this paper, we perform a broad and thorough investigation on training acceleration methodologies for deep reinforcement learning based on parallel and distributed computing, providing a comprehensive survey in this field with state-of-the-art methods and pointers to core references. In particular, a taxonomy of literature is provided, along with a discussion of emerging topics and open issues. This incorporates learning system architectures, simulation parallelism, computing parallelism, distributed synchronization mechanisms, and deep evolutionary reinforcement learning. Further, we compare 16 current open-source libraries and platforms with criteria of facilitating rapid development. Finally, we extrapolate future directions that deserve further research.", "sections": [{"title": "1 INTRODUCTION", "content": "Since the advent of Deep Q Network (DQN)[1] in 2015 which achieved human-level control on Atari video games, deep reinforcement learning (DRL), a powerful machine learning paradigm, has been widely investigated by Artificial Intelligence (AI) researchers. DRL deals with training an agent to learn the optimal policy based on feed-back from interaction with the environment. Through leveraging the power of deep learning and reinforcement learning, there are many merits in DRL. First, DRL is able to handle high-dimensional and large state spaces. Second, DRL has the general self-learning ability without the requirements of labeled datasets. Third, problems that need to be solved by online computation traditionally can be solved in offline training manner with DRL. With these merits, DRL in recent years has led to dramatic breakthroughs in game playing[2-4], robotics[5-8], healthcare [9-11], etc.\nHowever, with the increased complexity of the applications for DRL, more interaction iterations and larger scale of neural network models are required. As a result, the training process becomes very computation-intensive and thus time-consuming. For instance, it needs around 38 days of experience to train DQN on Atari 2600"}, {"title": "1.1 Related surveys", "content": "There are a number of surveys in this field that are related to ours. Arulkumaran et al. [13] provide a brief survey on DRL algorithms, applications, and challenges. Li [14] gives a wide coverage of core elements, important mechanisms, and applications in DRL. Wang et al. [15] present an overview of the theories, algorithms, and research topics of DRL. Moerland et al. [16] provide the taxonomy, key challenges, and benefits of model-based reinforcement learning. Meanwhile, many surveys targeting specific application domains have emerged in recent years, e.g., autonomous driving[17], cyber security[18], networking[19], intelligence transportation[20], multi-agent systems [21] and Internet of Things [22]. However, these surveys either focus on the fundamental"}, {"title": "1.2 Structure of the survey", "content": "The structure of the survey is summarized in Fig. 1 and organized as follows.\n\u2022 Section 2 provides the foundations and challenges related to our study.\n\u2022 Section 3 classifies system architectures in general for parallel and distributed DRL.\n\u2022 Section 4 elaborates on different simulation parallelism strategies to expose simulation concurrency.\n\u2022 Section 5 analyzes computing parallelism patterns used in existing works.\n\u2022 Section 6 discusses distributed synchronization mechanisms for backpropagation-based distributed training methods.\n\u2022 Section 7 explores technologies of deep evolutionary reinforcement learning for evolution-based distributed training methods.\n\u2022 Section 8 compares current open-source libraries and platforms that put parallel and distributed DRL into practice.\n\u2022 Section 9 gives concluding remarks and highlights future directions that deserve further research."}, {"title": "2 BACKGROUND", "content": "Deep reinforcement learning (DRL) integrates deep learning (DL) and reinforcement learning (RL), where DL is leveraged to perform function approximation in RL. Thus, the basic knowledge of DRL and current distributed DL training development lay a foundation for parallel and distributed DRL. This section will discuss the foundations related to this work, and analyze its challenges to motivate this study."}, {"title": "2.1 Preliminaries of deep reinforcement learning", "content": "Formally, DRL can be modeled as a Markov Decision Process (MDP). More specifically, at each time step t, the DRL agent receives a state $s_t \\in S$ and makes its action decision $a_t \\in A$ according to a distribution called policy $\\pi(\\theta | s_t)$, where S and A are the state space and action space, respectively. The policy is basically modeled by a neural network with parameters $\\theta$. After the action execution, the agent receives a reward $r_{t+1}$ and transitions to the next state $s_{t+1}$ according to the reward function $R(s_t, a_t, s_{t+1})$ and transition distribution $P(s_{t+1} | s_t, a_t)$, respectively. The return starting from time t to the end of the interaction is defined by $G_t = \\sum_{i} \\gamma^i r_{t+i}$, where $\\gamma \\in [0, 1]$ is the discount factor. The semantics of DRL are illustrated in Fig. 2. DRL agents interact with the environment to collect experience data, which is usually denoted by $(s_t, a_t, s_{t+1}, r_{t+1})$. Based on experience data, the agent is trained to learn an optimal policy for maximizing the return by updating the parameters $\\theta$ of the neural network.\nDRL can be classified to model-based and model-free. In model-based variants, the transition and reward models (also named MDP dynamics) are known or learned first. Then the policy optimization is based on the models. There are mainly two categories in model-based DRL [16]. First is model-based DRL with a learned model. This category of methods learns both the model and the global policy. Examples are Dyna-style algorithms such as MB-MPO[31] and ME-TRPO[32]. These algorithms learn the model through data from interaction with environments. Then model-free algorithms are utilized to learn the global policy based on the data generated by the learned model. Second is model-based DRL with a known model. This category of methods plans over a known model, and only performs learning for the global policy. Examples are dynamic programming[33] and Monte-Carlo tree search based algorithms (AlphaZero[3]).\nIn contrast, model-free DRL follows trail-and-error patterns and learns the policy directly. Basically, there are mainly three categories in model-free DRL. First is value-based methods. This category learns a value function that represents the expected return for being in a state or taking an action in the state, e.g., $V_\\pi(s)$ and $Q_\\pi(s, a)$. Then an optimal policy can be obtained based on the value function. Value-based methods are suited for discrete action spaces and deterministic policies. Second is Policy-based methods. This category learns the policy directly and extracts actions according to the policy. Since the policy is actually a distribution of possible actions, continuous action spaces, and stochastic policies can also be learned. Compared to value-based methods, policy-based methods have better convergence properties. However, high variance and sample inefficient are common issues in policy-based methods. Third is Actor-critic based methods, which integrates the merits of value-based and"}, {"title": "2.2 Distributed training for deep learning", "content": "In recent years, distributed training acceleration for deep learning has become a hot research topic and various methods have flourished in this field. Due to the tight connection between DL and DRL, knowledge and progress of distributed DL methods play a foundational role in parallel and distributed DRL. Basically, there are mainly three parallelism schemes of training acceleration for deep learning: data parallelism, model parallelism, and pipeline parallelism.\nData parallelism [39-43] is the first proposed and the most common parallelism schemes. It partitions the training dataset (samples) into multiple subsets and dispatches them to multiple workers (e.g., compute nodes and devices). Each worker maintains a replica of the entire neural network model along with its local parameters. During training, each worker processes its subset of data independently and synchronizes parameters or gradients of the model across different workers either in a synchronous or asynchronous manner. This is the main parallelism scheme utilized in distributed DRL. Details can be found in Section 6.\nModel parallelism [44\u201347] slices the neural network model into disjoint partitions and assigns them to multiple workers for computation. Compared to data parallelism, the computations between workers are no longer independent. Neurons with connections across workers require data transfers before continuing the computation to the next layer. Due to the high computation dependency in neural networks, model parallelism is non-trivial for accelerating the training [48]. The main advantage of model parallelism is making training a large model, which cannot fit into the memory of one node, become possible.\nPipelining parallelism [49\u201352] divides the neural network model into stages (consisting of consecutive layers) and assigns the stages to different workers. Each worker performs the computation of its stage for microbatches of samples one by one in a pipeline fashion. This scheme can significantly improve parallel training throughput over the vanilla model parallelism by fully saturating the pipeline. Basically, pipeline parallelism is a special form of model parallelism.\nCombining these parallelism schemes therefore giving full play to their respective advantages is a promising direction[53-59]. Krizhevsky et al. [53] propose a hybrid scheme of using data parallelism for convolutional layers and model parallelism for densely-connected layer. Wu et al.[54] propose to make use of data and model parallelism to accelerate the training of recurrent neural networks. Rasley et al. propose ZeRo [57], a parallelized optimizer based on model and data parallelism, which can train large models with over 100 billion parameters."}, {"title": "2.3 Challenges of parallel and distributed training for DRL", "content": "Although above distributed training methods can serve as good references for training acceleration in DRL, copying these methods to DRL directly is infeasible. That is because there are differences in training schemes between DL and DRL. DL trains from supervised training datasets and seeks to minimize the error between the neural network model and the labeled data. In comparison, DRL does not have labeled data. It learns an optimal policy that leads to the maximum outcome through interacting with the environment. This will unavoidably cause differences in training acceleration between DL and DRL and trigger requirements of new techniques and methodologies. Overall, the challenges of parallel and distributed training for DRL can be summarized as follows.\n\u2022 Training DRL agents in parallel is a complex and dynamic problem that many components, such as actors, learners, and parameter servers, need to operate cooperatively. How to organize the architecture of these components and allocate the training tasks among them so as to maximize the system throughput is a challenging problem[60, 61].\n\u2022 Requiring to interact with environments (mostly simulated) to generate training samples is a distinguished feather of DRL. Training DRL agents for complex applications often need millions or even billions of experience samples. How to parallel simulations so as to increase the sample efficiency is demanding [62, 63].\n\u2022 The workloads in distributed DRL training are heterogeneous, which may cause data movement across devices even nodes frequently. This will degrade the overall throughput and computation efficiency. How to saturate the computation resources by using different hardware architectures and corresponding computing parallelism techniques accordingly is challenging[64].\n\u2022 Since the parallel learning machines may process at different speeds in a heterogeneous environment, obsolete gradients exist and can have a large impact on stability and convergence in training. How to aggregate the gradients and synchronize the model maintained in parallel workers is also non-trivial[65].\n\u2022 The essence of dominant training technology in distributed DRL is stochastic gradient descent based on backpropagation. However, this technology still suffers from local optima and expensive computational cost [66]. How to exploit other optimization or search techniques in DRL to enable more sufficient training is desired.\nIn addressing these challenges, a profusion of studies have been proposed in the field of DRL acceleration using parallel and distributed. There is a necessity to collect, classify, and compare these studies in a structured manner, thereby facilitating researchers understanding the development of this field. In the following sections, we survey the research for parallel and distributed DRL training and demystify the details of the key technologies. This includes system architectures for distributed DRL, simulation parallelism, computing parallelism, distributed synchronization mechanisms (backpropagation-based training), and deep evolutionary reinforcement learning (evolution-based training). In addition, we provide an overview and comparison of current open-source libraries and platforms that put parallel and distributed DRL into practice."}, {"title": "3 SYSTEM ARCHITECTURES FOR DISTRIBUTED DRL", "content": "Training DRL agents in parallel is a system engineering problem that many components need to operate cooperatively. Taking a panoramic view of the parallel and distributed DRL acceleration frameworks that bloomed in recent years, four components can be abstracted in the general case:\n\u2022 Actor: is in charge of interacting with the environment, including performing the actions obtained according to the policy, getting information such as observations and rewards, and producing experience data.\n\u2022 Learner: collects or samples the experience data, and computes the gradients for updating the model. Since the gradient computation is usually based on a batch of experiences, high speed hardware like GPU is commonly used in Learners.\n\u2022 Parameter Server: Maintains the up-to-date parameters of neural network models for Actors or Learners. Note that in some approaches (e.g., APE-X[67] and R2D2[68]), the job of the parameter server is concurrently taken by the Learner.\n\u2022 Replay memory: Stores the experience data produced by Actors. This exists in cases of off-policy reinforcement learning, e.g., Gorila[69], Ape-X[67], R2D2[68], etc.\nAccording to the organization of these components, we classify the parallel and distributed training architectures for DRL into two classes: centralized and decentralized architectures."}, {"title": "3.1 Centralized architecture", "content": "In the centralized architecture, there is a center that maintains the global model of the neural network for learning. Learners optimize the global model by computing gradients based on the experiences from Actors. Both Learners and Actors synchronize their local model from the global model in some period or steps. Compared to Actors who usually have a large scale, the number of Learners can be one or many. We abstract the general form of the centralized architecture as shown in Fig. 3(a). The global model is either maintained by the Parameter Server or a Learner. More specifically, if there are multiple Learners in the architecture, the Parameter Sever serves as the center for updating the global model and distributing it to Actors and Learners. Therefore, the star communication topology is constructed in the centralized architecture. In any event, since a large number of workers as well as iterations could be involved in distributed DRL training, the bottleneck of the center node will have a strong impact on the training scalability.\nSilver et al. [69] propose a massively distributed method for the DQN named Gorila (General Reinforcement Learning Architecture). To the best of our knowledge, this is one of the pioneering works in the area of distributed training for DRL. Gorila is based on the centralized architecture and consists of four components: Actors, Learners, the Parameter Server, and the Replay Memory, as shown in Fig. 4(a). The center of Gorila is the Parameter Server, which is in charge of updating and distributing the network parameters. There can be many Actors and Learners in Gorila. Each Actor has a replica of the Q network and is responsible for generating experiences to the Replay Memory through interacting with the environment. Each learner also has a replica of the Q network and is in"}, {"title": "3.2 Decentralized architecture", "content": "In the decentralized architecture, there is no central node that maintains the global model. Basically, more than one Leaner are deployed. Each Learner not only computes the gradients based on the experiences from the corresponding Actors, but also obtains the gradients from other learners. Then, each Learner updates its model by aggregating all the gradients. Here, All-reduce [73, 74] distributed communication mechanism is utilized to aggregate the gradients, which generally requires a fully connected communication topology for parallel workers. Other than interacting with environments and producing the experiences, Actors update the parameters from Learners. We abstract the general form of the decentralized architecture as shown in Fig. 3(b). Compared to the centralized architecture, by using multiple Learners to aggregate the gradients and maintain the model, the decentralized architecture removes the communication congestion on the Parameter Server [74].\nA decentralized architecture of multiple learners with importance weighted is proposed in IMPALA [61]. In IMPALA, actors utilize CPU cores to interact with the environment, while learners are deployed on GPUs to give play to the gradient computation. To maintain the latest model in the whole system, gradients are aggregated across multiple learners synchronously and actors obtain the latest parameters from learners. Stooke et al. [71, 75] propose an acceleration method of utilizing multiple GPUs for DRL named rlpyt, which is based on the decentralized architecture. There are multiple Learners in the architecture. Each of them is running on a GPU and is in charge of data sampling and model inference. Besides, All-reduce is performed in each iteration to aggregate the gradients so as to keep the model in every Learner identical. This method has many variants that are applied with different DRL algorithms including A3C, PPO, DQN, etc. A similar architecture is adopted in DD-PPO[72], which is also decentralized and leverages multiple GPUs. Other than the method in [75] that synchronizes the models of different GPUs in one machine, DD-PPO extends the architecture across many machines and has better scalability. Similar to A3C[70], DD-PPO encapsulates the work of the Actor and Learner together in a worker. In this way, each worker alternates the workloads of experience collection, gradient aggregation, and optimization to achieve better resource utilization."}, {"title": "3.3 Discussion", "content": "These two classes of architectures are commonly used in recent studies. In the centralized architecture, it is known that the central node may encounter communication congestion that limits the scalability of distributed training [23]. However, its merits on synchronization efficiency and realization simplicity are non-negligible. This is because the global updated model is maintained in a central node, from which all workers can easily obtain and keep consistent. In the decentralized architecture, the scalability issue is relieved by eliminating the single-point issue. Nevertheless, the models optimize separately and need to be synchronized through assembling, which may incur convergence latency and communication overhead [74].\nOverall, there are many open issues and emerging topics from the perspective of the learning system architecture. Firstly, the question of whether Actors should conduct model inference to derive actions remains unresolved. In scenarios where Actors are tasked with model inference, Actors need to maintain the up-to-date copy of the model, obtain the actions through model inference, and transfer the experiences after environment interaction (e.g., IMPALA shown in Fig. 4(d)). Conversely, in other approaches, Actors focus on the interaction with the environment and give up the work of model inference (e.g., rlpyt shown in Fig. 4(e)). Instead, Learners obtain the actions through model inference and transfer them to Actors. Both paths of solutions have two sides. The former path takes full advantage of computational capacities in Actors to share the responsibilities of model inference. However, frequent model update requests from many Actors will significantly decrease the learning efficiency of"}, {"title": "4 SIMULATION PARALLELISM", "content": "Requiring to interact with the environment to collect training samples is one of the main differences between DRL and DL. Simulations that simulate realistic environments are widely used to reduce training cost and time, especially for physics-based applications, e.g., robotics and autonomous driving. There are many simulation platforms that are popular in DRL training, e.g., OpenAI Gym, MoJoCo, Surreal, Unity ML, Gazebo, and AirSim. Basically, the computations in simulations (e.g., physics calculation, rewards calculation, and 3D rendering) are involved in each step of training iteration. As the size of experiences increased for training complex applications, the simulation efficiency becomes more and more important in the training acceleration. For instance, 2.5 billion frames of experience are needed for PointGoal navigation in 3D scanned environments[72]. How to parallel simulations so as to increase the sample efficiency is challenging. To this end, many approaches have been proposed. According to existing reinforcement learning literature, there are mainly two simulation parallelism strategies."}, {"title": "4.1 Distributed simulation based on CPU clusters", "content": "Many simulation platforms in use today are run on CPUs for many existing DRL methods in training, e.g., Atari in [1, 70, 84, 85], MoJoCo in [36, 86-88] and OpenAI Gym in [73, 89, 90]. A straightforward strategy for training acceleration is employing a cluster of CPUs to execute a number of instances of the environment in a distributed manner, where each instance normally runs on a process or a thread. Based on this, the experience data used for training can be generated at a higher speed. Besides, the experience data can also be more diversified by importing stochastic or using different policies when interacting with different instances of the environment. This strategy is adopted by many existing parallel and distributed DRL training methods such as Gorila [69], A3C [70], APE-X [67] and IMPALA [61], and yields impressive results. For example, in APE-X, approximately 50K environmental frames per second (FPS) can be generated by using 360 actor machines; each actor runs an instance of the environment by a CPU core.\nNevertheless, the scaling of these distributed simulation methods is determined by the number of CPU cores in the system. As the number of CPU cores and nodes in the cluster increases, potential overhead may be incurred in communication across nodes [91, 92], synchronization [72, 93] and resource allocation [94, 95]. Furthermore, as GPUs are commonly used in neural network computations, a combination of CPUs and GPUs is popular in most of the DRL researches. In this case, additional context-switching overhead would be introduced. That is because intermediate data needs to be copied from CPUs to GPUs back and forth during training in this case. It is noted that simulation is only one part of the training platform. After a simulation step in the environment, the next state and a reward need to be computed. When using GPUs for neural network computations, the experience data (e.g., the next state and rewards) needs to be transferred from system memory to GPU memory for neural network inference. Then, actions are produced and need to be copied again to system memory for CPUs to perform the simulation step."}, {"title": "4.2 Batch simulation based on GPUs/TPUs", "content": "In view of the limitations of CPU simulations on a large-scale, researchers have been committed to developing batch simulation methods based on specialized hardware architectures such as GPUs or TPUs. Liang et al. [96] propose a GPU-accelerated RL simulator to parallel the simulations, which can achieve thousands of humanoid robots concurrently simulated and generate 60K environmental frames per second in a single machine with one GPU and CPU. With the batch simulation framework, the humanoid robots can be trained to run in less than 20 minutes while using 1000 \u00d7 less CPU cores compared to previous works [97]. While relying on GPU to perform most of the simulation computations, this method still requires CPUs to perform tasks of getting state and applying controls. The performance of this method is also bounded by CPU-GPU communication bandwidth."}, {"title": "4.3 Discussion", "content": "Currently, using a combination of CPUs and GPUs in DRL training is the mainstream in the field. CPUs are used to simulate the environments and perform environmental interaction, while GPUs are used to perform neural network inference as well as weights update. Hence, CPU-GPU communication is an unavoidable performance bottleneck. To this end, zero-copy simulation that contains all the computation in GPUs or TPUs is one of the significant research directions for highly efficient simulations. This provides an alternative way of simulation parallelism, with no requirement of accessing to CPU clusters which most researchers find hard to get. Besides, different from previous works that run each environment instance in the individual simulation, sharing scenes with other robots in a simulation is an effective way to take advantage of batch parallelism and maximize the throughput [62, 96, 98]. Since texture and geometry objects tend to be large in size, naively loading these objects for every environment is unaffordable for GPU memory. For instance, Liang et al. [96] loads all agents (and task-related objects) in the same simulation. Shacklett et al. [98] maintain $K \\ll N$ unique scenes in GPU memory, where N is the number of parallel environments in a batch.\nTransferring the learned policies from simulation to reality is an essential demand for many real-world applications. Simulation-to-reality (Sim-to-real) is a topic of much interest. One straightforward strategy is to build a realistic simulator that can perfectly replicate the reality. Hence, the learned policies can be directly applied in real-world environments. This is referred as the zero-shot transfer. In recent years, the zero-shot transfer has been successfully demonstrated in several domains. Andrychowicz et al. [6] transfer learned policies for dexterous in-hand manipulation to physical robots by performing training entirely in simulation. Rudin et al. [101] demonstrate sim-to-real transfer for quadrupedal robots by using massively parallel DRL. Loquercio et al. [102] achieve the zero-shot transfer for high-speed UAV navigation while some realistic scenarios that were never experienced during training in simulation. The techniques for enabling seamless transfer in these approaches can be summarized as follows. First, add realistic sensor noise to the observations. Second, randomize physical properties in simulations such as friction coefficients and dynamics of objects. Third, learn with disturbances such as pushing the robot randomly and adding noise to rewards during training."}, {"title": "5 COMPUTING PARALLELISM", "content": "Other than simulation, intensive computing workloads are unavoidable in DRL training e.g., network inference, backpropagation, and evolutionary computation (see Section 7). As a result, computing parallelism techniques are widely utilized to speed-up computations in DRL training. There are mainly three ways in the literature: cluster computing, single machine parallelism, and specialized hardware architectures (i.e., GPUs, FPGA, TPUs) acceleration."}, {"title": "5.1 Cluster computing", "content": "Cluster computing usually includes multiple machines working together to perform computation-intensive or data-intensive tasks. Those machines are also called 'nodes' and inter-connected by a high-speed local network. Since the computing resources in individual nodes are relatively constrained, integrating the resources of multiple nodes is a straightforward and effective way for acceleration of large-scale processing. In the early days when deep neural networks were not widely used in reinforcement learning, the classic cluster computing framework (i.e., MapReduce [103]) has been applied to reinforcement learning for acceleration [104]. This method distributes the computation in the large matrix multiplication for Markov decision process (MDP) solutions such as policy evaluation and iteration. Considering the inadequacy of MapReduce for iterative computation involved in neural network training, Dean et al. propose DistBelief[44] to utilize clusters of machines in training and inference for large-scale deep networks. However, this method targets deep network training, which is different from that of DRL. To this end, one of the earliest works on parallel and distributed DRL, Gorila [69], is proposed. Gorila utilizes a cluster of machines to achieve an order of magnitude of reduction in training time than single-machine training. To the best of our knowledge, this is the enlightenment work that many methods, such as Ape-X[67], IMPALA [61], and R2D2[68], spring up in this direction. From the evolution of related works, we can see that cluster computing is roughly a standard mode of accelerating training for DRL."}, {"title": "5.2 Single machine parallelism", "content": "Other than cluster computing that uses multiple machines, single machine parallelism utilizes multiprocessors or multi-core CPUs in a single machine to increase the computing ability. The rationale for applying single machine parallelism is that distributed clusters may not be affordable for most researchers, unlike the widely available commodity workstations. Besides, data transfer, model synchronization across machines during training will also incur noticeable overhead. Multiprocessing and multithreading are the key technologies in single machine parallelism. More specifically, multiprocessing refers to operating different processes simultaneously by more than one CPU processor, while multithreading refers to executing multiple independent threads in parallel by a single CPU, especially the multi-core CPU.\nThe representative method is A3C [70]. In A3C, many actor-learner threads are launched in parallel, and each thread performs a training procedure separately, including environment interaction, experience collection, and model update. In this way, the speed of data generation and the sample efficiency can be improved significantly. The results in [70] showed that A3C using 16 CPU cores surpasses DQN variants using Nvidia K40 GPU in half of the training time and achieves comparative performance to Gorila which uses 100 machines.\nPetrenko et al. present a high-throughput training system named SampleFactory [60], which is designed based on a single machine with a multi-core CPU and a GPU. Though optimizing the efficiency and resource utilization in a single machine setting, Sample Factory can achieve throughout as 130K FPS (Frame per Second) and 4 times speedup over the state-of-the-art baseline SEED_RL [77] with a workstation-level PC. One of the significant contributions of SampleFactory is that it allows large-scale DRL experiments accessible to a wider community by reducing the computational requirements."}, {"title": "5.4 Discussion", "content": "Table 1 compares the computing parallelism types utilized in current parallel and distributed DRL implementations. We can see in the table that all of these parallelism types are widely used, and a combination of different computing parallelism techniques is popular. More specifically, all these three types of parallelism are reported in the latest research such as Ray RLlib[105], SEED_RL[77], Dactyl[6], AlphaZero[3], GATO[121], etc. As parallelism in DRL becomes more and more ambitious, achievements in training efficiency in this area have been proven. The state-of-the-art solution only took 3.6 mins [105] to train Atari 2600 games. In comparison, it took 15 days to train a single Atari game several years ago, which at that time was a significant breakthrough in human history [1].\nBesides, with the parallelization of computing resources, computing task scheduling plays an important role in enhancing the efficiency of distributed DRL systems. The computing workloads involved in distributed DRL training are diverse and heterogeneous, including environment interaction, network interference, gradient backpropagation, and model synchronization, which are handled by workers such as Actors, Learners, or the Parameter Server. Hence, it is essential to effectively schedule these computing tasks across processors even machines with varying resource configurations, thereby minimizing the overall training time. There are many scheduling strategies have been proposed in current distributed DRL systems.\n\u2022 Load balancing. This strategy enables load-aware scheduling that assigns computing tasks to distributed hardware, considering factors such as resource contention and input locality. Ray[125] utilizes fine-grained and coarse-grained load balancing methods for stateless and stateful computations, respectively. rlpyt[75] forms two alternating groups of simulation processes, and schedules GPU to serve each group in turn to keep utilization high. Meng et al. [119] propose inter-load balancing method for two compute units (CUs) involved in training value and policy networks. Li et al. [79] propose to explicit control the frequencies for parallel workers to achieve load balance.\n\u2022 Resource dynamic adjusting. This strategy dynamically scales the resources for workers to accelerate DRL training with minimum costs. MINIONSRL [126] leverages a scheduler that dynamically adjusts the number of Actor workers to optimize the DRL training with minimal training time and costs. Similarly, Meng et al. design a scheduling mechanism that re-allocates CPU threads to processing a sub-batch of training for the Learner in heterogeneous computing environments.\n\u2022 Computation and communication overlapping. In distributed DRL training, computing tasks can be blocked by communication tasks due to their execution dependencies. For example, gradient aggregation has to wait for the completion of gradient transfer for parallel workers. This strategy schedules the computing and communication tasks to reduce the waiting time, thereby improving the computation utilization. PEARL [127], an open-source distributed DRL framework, designs a Learner module that enables scheduling to overlay the communication and computation. Mei et al [114] propose an optimized data transfer scheduling technique that overlaps computation in distributed DRL training.\n\u2022 Preemptive scheduling. This strategy proactively terminates lagging tasks, as each worker must wait for all parallel workers to complete during synchronous training. Wijmans et al. propose a straggler"}, {"title": "6 DISTRIBUTED SYNCHRONIZATION MECHANISMS", "content": "The dominant solution\u00b9 for training acceleration in distributed DRL is employing a number of workers to cooperatively train the model based on distributed stochastic gradient descent (DSGD) [27, 48]. This is based on the data parallelism introduced in Section 2. During DSGD training, each parallel work holds a copy of the model, performs training based on a subset of environmental experiences, and then aggregates the update to the target model cooperatively. It is noted that distributed synchronization among workers is vital to the training efficiency, especially in the heterogeneous environment where the parallel workers may process at different speeds. Variants of synchronization mechanisms are systemically studied in deep learning area, i.e., Bulk Synchronous Parallel (BSP)[129], Asynchronous Parallel (ASP) [130] and Stale Synchronous Parallel (SSP) [131]. These mechanisms lay a good technical foundation for distributed DRL."}, {"title": "6.1 Asynchronous Off-policy Training", "content": "Asynchronous off-policy training is the distributed synchronization mechanism that was initially applied in dis-tributed DRL, and it has been widely used later on, e.g., Gorila[69", "APE-X[67": "R2D2[68", "37": ".", "61": "."}, {"61": "."}]}