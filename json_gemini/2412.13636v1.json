{"title": "Consistency of Compositional Generalization across Multiple Levels", "authors": ["Chuanhao Li", "Zhen Li", "Chenchen Jing", "Xiaomeng Fan", "Wenbo Ye", "Yuwei Wu", "Yunde Jia"], "abstract": "Compositional generalization is the capability of a model to understand novel compositions composed of seen concepts. There are multiple levels of novel compositions including phrase-phrase level, phrase-word level, and word-word level. Existing methods achieve promising compositional generalization, but the consistency of compositional generalization across multiple levels of novel compositions remains unexplored. The consistency refers to that a model should generalize to a phrase-phrase level novel composition, and phrase-word/word-word level novel compositions that can be derived from it simultaneously. In this paper, we propose a meta-learning based framework, for achieving consistent compositional generalization across multiple levels. The basic idea is to progressively learn compositions from simple to complex for consistency. Specifically, we divide the original training set into multiple validation sets based on compositional complexity, and introduce multiple meta-weight-nets to generate sample weights for samples in different validation sets. To fit the validation sets in order of increasing compositional complexity, we optimize the parameters of each meta-weight-net independently and sequentially in a multilevel optimization manner. We build a GQA-CCG dataset to quantitatively evaluate the consistency. Experimental results on visual question answering and temporal video grounding, demonstrate the effectiveness of the proposed framework. We release GQA-CCG at https://github.com/NeverMoreLCH/CCG.", "sections": [{"title": "Introduction", "content": "Compositionality is an important property of human cognition (Fodor and Pylyshyn 1988). Compositional generalization, the capability of a model to understand novel compositions composed of seen concepts, is critical for artificial intelligence systems to mimic the compositionality. Previous work (Pierrot et al. 2019; Liu et al. 2020; Yang et al. 2023; Xu et al. 2023) has shown that novel compositions exist at multiple levels, including phrase-phrase level, phrase-word level and word-word level, as shown in Figure 1, but the consistency of compositional generalization across multiple levels of novel compositions remains unexplored. The consistency refers to the model's ability to generalize to both phrase-phrase level novel compositions and phrase-word/word-word level novel compositions, which are derived from the words within the phrase-phrase structures. For example, when a model generalize to a phrase-phrase level composition like \"golden dog\"+\"white cat\", it should also be able to generalize to phrase-word and word-word level compositions, such as \"golden\"+\"white cat\" and \"golden\"+\"cat\". Understanding \"golden\"+\"white cat\" and \"golden\"+\"cat\" are the premise for understanding \"golden dog\u201d+\u201cwhite cat\". We investigate if existing vision-and-language models exhibit the consistency. Our observations show that the models even with 37B parameters only achieve ~40% in the consistency, which indicates that existing models misunderstands the concepts in novel compositions.\nIn this paper, we propose a meta-learning based framework applicable to different types of models for consistent compositional generalization (CCG) across multiple levels of novel compositions. The basic idea behind our framework is to progressively learn compositions from simple to complex by making models learn difficult compositions only after learning simple compositions. To this end, we explicitly distinguish samples with different compositional complexities by generating different sample weights for them, and adaptively update the sample weights to ensure learning compositions from simple to complex. Specifically, we divide the original training set into multiple validation sets"}, {"title": "Related work", "content": "Compositional Generalization\nCompositional generalization has received increasing attention as its importance in mimicking the fundamental compositionality of human cognition (Fodor and Pylyshyn 1988). Numerous benchmarks (Li et al. 2022, 2024) have been proposed to evaluate the compositional generalization capacity, and a substantial amount of research (Saqur and Narasimhan 2020; Wang et al. 2023a; Li et al. 2023b) has been proposed to boost the compositional generalization capacity.\nAn important property regarding composition is that the process of composition is recursive (Bienenstock 1996), which revealed that compositions exist at multiple levels and compositional generalization capacity can be evaluated at multiple levels. Recently, there have been several attempts that improve compositional generalization capacity at multiple levels. For example, works (Pierrot et al. 2019; Liu et al. 2020) perform recursive reasoning over a decomposed tree layout to achieve compositional generalization at multiple levels. Yang et al. (2023) proposed a coarse-to-fine contrastive ranking loss for learning a composite representation that is sensitive to different levels of granularity of both queries and actions. Xu et al. (2023) optimized models on multiple virtual sets in a bi-level optimization scheme to handle various levels of novel compositions. These works focus on the accuracy on samples with multiple levels of novel compositions. Differently, we explore the consistency of compositional generalization across multiple levels, requiring a model to generalize not only to complex phrase-phrase level novel compositions but also to their associated simple phrase-word/word-word level novel compositions.\nConsistency\nChecking for consistency can be likened to conducting a Turing Test (Radziwill and Benton 2017), and the research community has demonstrated significant interest in assessing consistency. Xu et al. (2018) explored the consistency across image variations by performing adversarial attack on vision systems, while (Shah et al. 2019; Ribeiro, Guestrin, and Singh 2019) measure the consistency across linguistic variations by generating new questions with the same visual facts in the original question. (Ray et al. 2019; Tascon-Morales, M\u00e1rquez-Neila, and Sznitman 2023) focus on logical consistency about logically consistent entailed questions or sufficient/necessary conditions. (Selvaraju et al. 2020; Yuan et al. 2021) test perception consistency on low-level perception questions generated for reasoning questions. Jing et al. (2022) improved reasoning consistency, which requires a VQA model make correct answers for a series of sub-questions about a compositional question. Other works have also looked into consistency, such as spatial-temporal consistency (Wang et al. 2024) in video-related tasks, multi-view consistency (Yang et al. 2024) and 2D-3D relational consistency (Zhang, Luo, and Lei 2024) in 3D-related tasks. By contrast, we focus on the consistency of compositional generalization across multiple levels, which is critical for understanding novel compositions but remains unexplored."}, {"title": "Framework", "content": "Overview\nThe overview of the proposed framework is shown in Figure 2. In our framework, we make models learn compositions from simple to complex by progressively fitting samples in order of increasing compositional complexity. For"}, {"title": "Validation Set Construction", "content": "To learn samples with different levels of compositional complexity, we divide the original training set to construct multiple validation sets. Each validation set is expected to contain samples with a certain level of compositional complexity. For clarity, we introduce how to construct validation sets in the context of VQA, as shown in Figure 3. VQA requires models to learn to provide a correct answer A for a natural language question Q about an image V. For a training set Dt of VQA, compositional generalization is the capacity of a VQA model trained on Dt to correctly answer questions with novel compositions composed of primitives (i.e., words and phrases) seen in Dt.\nSpecifically, given a sample (Q, V, A) \u2208 Dt, we first use the benepar toolkit (Kitaev, Cao, and Klein 2019) to extract phrases in Q. The phrases are denoted as {P}, where Np represent the number of phrases. Generally, the compositional complexity is proportional to the length of the longest phrase in a question, with longer longest phrases indicating more complex questions. As a result, we count the length of the longest phrase in the question as an approximated compositional complexity, and denote it as L(Q). Next, we count the number of samples whose longest phrase length is L(Q) using a function S(\u00b7), and denote the number as S(L(Q)). Finally, we construct validation sets according to two principles: (1) Samples with similar compositional complexity should be placed in the same validation set as much as possible. (2) The number of samples in each validation set should be as close as possible. Based on these two principles, we construct the i-th validation set by using\n$Du_i = \\{(Q, V, A)|(Q, V, A) \\in D_t, \\\\ \\sum_{1 \\le j \\le L(Q)} S(j) \\ge [max(|D_t| \\times (i - 1)/K, 0)],\\quad (1)\n\\sum_{1 \\le j \\le L(Q)} S(j) < [max(|D_t| \\times i/K, |D_t|)]\\},$ \nwhere |Dt| is the sample number of the input dataset, and K is a hyperparameter denoting the number of expected validation sets. In doing so, each sample in the original training set is assigned to a unique validation set and satisfies \u03a31<i<K Dvi = Dt, while samples in different validation sets have different complexities, the larger i is, the more complex the samples in Dv\u2081 are."}, {"title": "Sample Weight Generation", "content": "As different samples have different importance in learning a certain level of compositions,\nwe introduce a set of meta-weight-nets (Shu et al. 2019) to automatically generate sample weights for samples in different validation sets, to explicitly control which samples should be learned. Each meta-weight-net is only responsible for generating sample weights for a specific validation set. These meta-weight-nets use the same architectures-three stacked fully connected layers and a sigmoid layer, but have different parameters."}, {"title": "Multilevel Optimization", "content": "To make the VQA model fit the validation sets progressively from simple to complex, we optimize the model and the meta-weight-nets in a multilevel optimization process. The process consists of two continuously alternating steps: parameter optimization and meta optimization. During parameter optimization, we freeze the meta-weight-nets, and optimize the model to fit current sample weights. During meta optimization, we update sample weights by optimizing meta-weight-nets to fit the validation sets. The two steps are performed in sequence until the model training converges. For model testing, we conventionally test the model and exclude the meta-weight-nets. Below we first introduce the formulation of the multilevel optimization process, and then discuss the details of the parameter optimization and the meta optimization in the process.\nFormulation. Let 0 and wi denote the parameters of the model and the i-th meta-weight-net, respectively, the multilevel optimization process can be formulated as sequentially performing the following nested loops from LOOPO to LOOPK:\nLOOPK : $\\omega_k^*= arg\\underset{\\omega_k}{min}\\mathcal{L}_v(\\theta^*,\\{\\omega_i^*\\}_{i=1}^{K-1},\\omega_k;D_{v_k})$\nLOOP2 : $\\underset{\\omega_2}{s.t.} \\omega_2^* = arg\\underset{\\omega_2}{min}\\mathcal{L}_v(\\theta^*,\\omega_1^*,\\omega_2, \\ldots, \\omega_k;D_{v_2})$\nLOOP1: $\\underset{\\omega_1}{s.t.} \\omega_1^* = arg\\underset{\\omega_1}{min}\\mathcal{L}_v(\\theta^*,\\omega_1, \\ldots, \\omega_k;D_{v_1})$\nLOOPO: $\\underset{\\theta}{s.t.} \\theta^* = arg\\underset{\\theta}{min}\\mathcal{L}_t(\\theta, \\omega_1, \\ldots, \\omega_k;D_t), \\quad (3)$\nwhere Lt and L\u2082 denote the training loss and validation loss, respectively. The Lt is determined by the selected model, as different models are trained by different losses. Given a model trained by loss L, by applying the proposed framework, Lt can be given by\n$L_t(\\theta,\\omega_1,\\omega_2, \\ldots, \\omega_k; D_t) = \\sum_{1 \\le i \\le K} \\sum_{d \\in D_{v_i}} w_d\\mathcal{L}(\\theta; d), \\quad (4)$\nwhere wa is the sample weight of d calculated by Eq. (2). Furthermore, L\u2082 can be written as\n$L_v(\\theta^*, \\omega_1, ..., \\omega_{i-1},\\omega_i, ..., \\omega_k; D_{v_i}) = \\sum_{d \\in D_{v_i}}\\mathcal{L}(\\theta^*(\\omega_i); d).\\quad(5)$\nParameter Optimization. Parameter optimization aims to find the optimal parameters 0* such that minimizing the"}, {"title": "GQA-CCG Dataset", "content": "In this section, we illustrate how we construct GQA-CCG based on the GQA dataset, the overview of which is shown in Figure 4. We use the the train_balanced split and the val_all split of GQA in the process of constructing GQA-CCG, and here we denote them as Dt and D, respectively.\nPreparations\nCandidate Filter. First, we extract words and phrases by benepar (Kitaev, Cao, and Klein 2019) for all questions in Dt and Dv, respectively. Then we count seen compositions including phrase-phrase (pp), phrase-word (pw) and word-word (ww) in Dt. Next, we filter out the samples in D\u03c5, whose question has at least a novel phrase-phrase composition. We collected all question-answer pairs (denoted as [Qpp, App]) of the filtered samples as a candidate set C.\nIn-context Filter. Based on C, we select M questions for each type of question prefix by a diversity maximization method. For a type of question prefix, we first randomly select a QA pair [Qpp, App] of the type to construct an initial set P = {[Qpp, App]}. Then we find a QA pair, in which the question has the lowest average similarity to all questions in P of this type. We add the found QA pair to P, and repeat the above step until |P| = M. The similarities between the two questions are computed by the cosine similarity of their BERT embeddings (Devlin et al. 2019). As a result, we obtain a set of in-context sets {It}tet, where T denotes the set containing all types of question prefix, and It represents the set of selected M QA-pairs with type t. Finally, we update C = C \u2013 I, where I = \u2211tet It, and It can be represented as {[Q, A]}1\nFor a QA pair [Qpp, App] \u2208 I, where Qpp has a novel phrase-phrase composition P1-P2 (notes that p1 and p2 can be exchanged), we manually annotate it with: (1) A QA pair [Qpw, Apw] with a novel phrase-word composition p1-W2. (2) A QA pair [Qww, Aww] with a novel word-word composition W1-W2. The relationship between P1, P2, W1 and W2 satisfies\nW1 P1, W2 P2. (8)\nAfter the manual annotation, we rewrite It as {T}1, where T is a triplet that is denoted as {[Q), A(2)]}te{pp,pw,ww}.\nSample Generation Pipeline\nAutomatic Generation. For a QA pair [Qpp, App] \u2208 C with type t, we first select 3 triplets, in which the questions have the maximum cosine similarity of BERT embeddings with Q from It. We denote the novel phrase-phrase composition in [Qpp, App] as P1-P2, i-th selected triplet as Pi = {[Q), A)]}t\u2208 {pp,pw,ww}, the components of novel compositions in Pi as as pi), p\u2081, phi), P2, W\u2081 wi) and wh\u00b2). Then we iterate over the words in p2 as w2. For each w\u2082, we iterate over the words in p\u2081 as w\u2081. For each pair of w\u2081-W2, we fill associated infos into the prompt template in Figure 4 (b), and then use GPT-3.5 to generate {[Qt, At]}t\u2208{pw,ww} to form a triplet {[Qt, At]}t\u2208{pp,pw,ww}. Eventually, the generated triplets are collected as G.\nPostprocessing. For a triplet {[Qt, At]}te{pp,pw,ww} in G, we denote the novel phrase-phrase composition in Qpp as P1-P2, and retain the triplet that if it satisfies: (1) There is a novel word-word composition W1-W2 and a novel phrase-word composition p1-W2 in Qww and Qpw, respectively. (2) The relationship between P1, P2, W\u2081 and w2 satisfies Eq. (8). (3) There are no novel compositions that are not w\u2081-W2 and P1-W2 in Qww and Qpw, respectively.\nManual Review. We recruited volunteers to verify the correctness of the generated QA pairs according to the image for the original QA pair. For a triplet {[Qt, At]}t\u2208{pp,pw,ww} and the image I for [Qpw, Apw], we retain the triplet if it satisfies both [Qpw, Apw] and [Qww, Aww] are correct based on I. We add the associated images to the retained triplets, forming our dataset DCCG. We get 18983 samples with novel compositions at different levels that consist of 8702 triplets of {[Qt, At, I]}t\u2208{pp,pw,ww}. The number of samples with novel compositions at phrase-phrase, phrase-word and word-word level are 5125, 8102 and 5756, respectively.\nConsistency Score\nTo quantitatively evaluate the consistency on our DCCG, we devise a metric Cons, which is computed by\n$Cons = \\frac{\\sum_{T \\in D_{CCG}} \\prod_{t \\in T} Correct(t)}{triplet \\_ num(D_{CCG})}, \\quad (9)$\nwhere Correct(\u00b7) is an indicator function, triplet_num() is a function that counts the triplet number of the input dataset. The value range of Cons is [0, 1], and the larger Cons, the better the consistency."}, {"title": "Experimental Settings", "content": "Datasets. We apply the proposed framework to two tasks, VQA and TVG, to evaluate its effectiveness. For VQA, we evaluate the framework on our GQA-CCG dataset and the GQA dataset (Hudson and Manning 2019). The GQA-CCG dataset is used to test the consistency of compositional generalization across multiple levels and the accuracy of compositional generalization at multiple levels. We use the GQA dataset to test whether our framework is harmful to the IID generalization capability. For TVG, we use the recently released Charades-CG dataset (Li et al. 2022) that contains compositional referring expressions about real-world videos to further test the compositional generalization capability of our framework on different tasks.\nBaseline Methods. For VQA, we incorporate our framework into five methods including MAC (Hudson and Manning 2018), LCGN (Hu et al. 2019), MMN (Chen et al. 2021), VL-T5 (Cho et al. 2021) and CFR (Nguyen et al. 2022). and dub the incorporated methods X+MLO, where X is a method name. These methods belong to different types, thus the experiments on these methods allow for a comprehensive assessment of the effectiveness of our framework. For TVG, we apply the proposed framework to MS-2D-TAN (Zhang et al. 2021), which uses a 2D temporal map to model the temporal adjacent relations of video moments, and demonstrates good compositional capability."}, {"title": "Compositional Generalization Performance", "content": "We compare with different types of VQA methods including large vision-language models (LVLMs) varies in parameters (7B to 37B) on GQA-CCG, including MAC (Hudson and Manning 2018), LCGN (Hu et al. 2019), MMN (Chen et al. 2021), OpenFlamingo (Awadalla et al. 2023), BLIP-2 (Li et al. 2023c), Otter (Li et al. 2023a), LLaVa-v1.5-Xtuner (Contributors 2023b), XComposer2 (Dong et al. 2024), mPLUG-Owl2 (Ye et al. 2024), LLaVA-1.6 (Liu et al. 2024), CogVLM (Wang et al. 2023b), emu2 (Sun et al. 2024), LXMERT (Tan and Bansal 2019), VL-T5 (Cho et al. 2021), and CFR (Nguyen et al. 2022). We evaluate pretrain-based models with parameters > 7B via VLMEvalKit (Contributors 2023a), which provides model-specific prompts and answer matching rules. We design additional matching patterns for each model with respect to its answer format. For example, we use the matching pattern \"The answer is XXX.\u201d for XComposer2 as it often answers in this format.\nThe experimental results on GQA-CCG are listed in Table 1, where \u201coverall\u201d represents the accuracy on all test samples of GQA-CCG, and \u201cConsistency\u201d is the consistency score computed by Eq. (9). The \u201cphrase-phrase\u201d, \u201cphrase-word\" and \"word-word\u201d denote the accuracy on samples with corresponding levels of novel compositions. We observe that: (1) CFR+MLO achieves the best performance on both the accuracy and consistency (e.g., 74.23% and 49.27% in overall accuracy and consistency, respectively). (2) For all five baseline methods of different types, our framework consistently improves their accuracy and consistency (e.g., 3.28% and 2.81% absolute performance gains in consistency for MAC and CFR, respectively). (3) LVLMs are better at simple word-word level compositions than at complex phrase-phrase level and phrase-word level compositions. Although several LVLMs outperform CFR+MLO in word-word accuracy, they have more than thirty times the scale of parameters of CFR+MLO and have been trained on much more VQA samples. These observations show that the proposed framework is efficient in improving not only the consistency but also the accuracy of compositional generalization at multiple levels for different types of baseline methods. Furthermore, LVLMs still struggle to the consistency of compositional generalization, although they've been trained on a large amount of VQA samples."}, {"title": "IID Generalization Performance", "content": "The experimental results on GQA are listed in Table 2. We can observe that: (1) Overall, CFR+MLO achieves the best performance among state-of-the-art methods. (2) Compared to baseline methods, our framework improves their performance (e.g., 0.4% absolute performance gains in accuracy for MAC). The reason for the limited performance improvement of the proposed framework on GQA is that we mainly focus on the compositional generalization capability, which can be viewed as a capability of out-of-distribution (OOD) generalization, while GQA is used more to evaluate the independent and identically distributed (IID) generalization. The experimental results show that our framework is beneficial for IID setting (e.g., GQA) apart from the OOD setting (e.g., GQA-CCG), compared to most existing methods that provide performance gains in the OOD testing at the expense of IID performance (Cho et al. 2023)."}, {"title": "Ablation Studies", "content": "Firstly, we investigate the effectiveness of different components of our framework on the consistency and accuracy of compositional generalization, and the results are shown in Table 3. We evaluate the effectiveness of meta-weight-nets by training them simultaneously rather than in a multilevel optimization manner. We observe that the performance is better than the baseline methods but worse than the methods"}, {"title": "Analysis of Validation Set Learning Sequence", "content": "We analyze the importance of learning validation sets from simple to complex, to find whether learning from complex to simple is also effective to improve the compositional consistency. The experimental results of using different learning procedures are list in Table 4. which shows that learning validation sets from complex to simple (C \u2192 S) has little improvement over the baseline model, and even suffers a slight decline in some metrics (e.g., 0.14% performance drop in phrase-phrase accuracy). The main reason is that without the accumulation of simple knowledge, it is difficult to directly learn complex knowledge, which is proved in the human cognitive theory (Plass, Moreno, and Br\u00fcnken 2010)."}, {"title": "Qualitative Analysis", "content": "We provide several qualitative examples in the context of VQA in Figure 5. For a triplet that consists of questions with novel compositions at different levels, we provide the predictions of MMN+MLO and MMN. We can obverse that: (1) MMN makes correct predictions for the questions with complex novel phrase-phrase compositions, but fails"}, {"title": "Conclusion", "content": "In this paper, we have explored the consistency of compositional generalization across multiple levels of novel compositions, and have presented that existing vision-and-language models even with 37B parameters struggle to the consistency. We've proposed a meta-learning based framework that can improve the consistency of different models, by making the models progressively learn compositions from simple to complex in a multilevel optimization process. Moreover, a GQA-CCG dataset has been presented to enable the qualitative evaluation of the consistency for VQA models. Experimental results show that our framework can improve not only the consistency of compositional generalization across multiple levels, but also the capacity of compositional generalization at different levels."}]}