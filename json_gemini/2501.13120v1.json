{"title": "Multilinguality in LLM-Designed Reward Functions for Restless Bandits: Effects\non Task Performance and Fairness", "authors": ["Ambreesh Parthasarathy", "Chandrasekar Subramanian", "Ganesh Senrayan", "Shreyash Adappanavar", "Aparna Taneja", "Balaraman Ravindran", "Milind Tambe"], "abstract": "Restless Multi-Armed Bandits (RMABs) have been successfully applied to resource allocation problems in a variety of settings, including public health. With the rapid development of powerful large language models (LLMs), they are increasingly used to design reward functions to better match human preferences. Recent work has shown that LLMs can be used to tailor automated allocation decisions to community needs using language prompts. However, this has been studied primarily for English prompts and with a focus on task performance only. This can be an issue since grassroots workers, especially in developing countries like India, prefer to work in local languages, some of which are low-resource. Further, given the nature of the problem, biases along population groups unintended by the user are also undesirable. In this work, we study the effects on both task performance and fairness when the DLM algorithm, a recent work on using LLMs to design reward functions for RMABs, is prompted with non-English language commands. Specifically, we run the model on a synthetic environment for various prompts translated into multiple languages. The prompts themselves vary in complexity. Our results show that the LLM-proposed reward functions are significantly better when prompted in English compared to other languages. We also find that the exact phrasing of the prompt impacts task performance. Further, as prompt complexity increases, performance worsens for all languages; however, it is more robust with English prompts than with lower-resource languages. On the fairness side, we find that low-resource languages and more complex prompts are both highly likely to create unfairness along unintended dimensions.", "sections": [{"title": "1 Introduction", "content": "Reward functions are crucial to the generation of optimal policies for sequential decision-making via reinforcement learning. Previous works (Behari et al. 2024; Verma et al. 2024) have shown that LLMs are quite useful when it comes to designing reward functions that are aligned with human preferences. However, these existing methods only study this problem for human commands that are in English. Additionally, they only focus on task performance (i.e., how well the given commands are executed).\nHowever, as seen in both papers, a key domain of application is public health. Specifically, the DLM algorithm (Behari et al. 2024) has been used to allocate health worker calls to underprivileged socioeconomic groups. This can be used to empower grassroots health workers to specify commands based on community needs, and the algorithm would automatically tailor allocation decisions based on that. However, two questions remained unanswered: (1) What happens if the prompts are not in English? (2) Is there an impact on the fairness of the allocations? We are not aware of any work that studies these questions for the DLM algorithm.\nThis is especially important to look at in developing countries like India where these workers might prefer to work in local languages, some of which are low-resource languages. Furthermore, given the nature of the problem, unintended biases along population groups are also undesirable.\nThis paper studies the effects on both task performance and fairness when the DLM algorithm (Behari et al. 2024) is prompted in non-English languages.\nRelated work There has been a lot of recent work in using LLMs to design reward functions for reinforcement learning settings. Works by (Sun et al. 2024; Kwon et al. 2023; Ma et al. 2024; Yu et al. 2023; Cao et al. 2024; Behari et al. 2024; Verma et al. 2024), among others, have shown that using LLMs to design reward functions allows the reinforcement learning modules to be more flexible and helps them ground their outcomes in natural language.\nThere has also been work that has looked at the multilinguality of LLMs from the perspectives of improving task performance (Lai, Mesgar, and Fraser 2024) and bias mitigation (Demidova et al. 2024).\nHowever, to our knowledge, ours is the first work that studies the impact on task performance and fairness when prompts to LLMs (which are used to design reward functions) are in non-English languages."}, {"title": "2 Experiment Setup", "content": "Our experiments are intended to answer the following research questions:\n1. How does language affect reward function proposals? We study this in Section 3.\n2. How do non-English prompts affect task performance? We study this in Section 4.\n3. How do non-English prompts affect fairness? We study this in Section 5."}, {"title": "2.1 Environment", "content": "We consider 6 features taken from (Behari et al. 2024) for this paper. However, we use a synthetic environment to perform our analyses instead. We describe the environment below. We generate features based on the schema we have described below. Given the difficulties in acquiring real-world data, we instantiate a synthetic dataset, where the features and effects are reflective of real-world settings. We derive our transition probabilities from these features.\nInitial Feature Distribution We assume that the initial feature distribution is generated from the following graphical model. The structural relationships are described as follows:\nAge \u2192 Income\nEducation_Level \u2192 Income\nIncome \u2192 Phone_Ownership\nPhone-Ownership \u2192 Times_To_Be_Called\nLanguage Spoken is independent of all other variables.\nWe also choose a constant \u03b1 that denotes the strength of the relationships between the variables. We assume that the structural equations are as follows:\nAge ~ Unif[0, 1]\nEducation_Level ~ Unif[0, 1]\nLanguage Spoken ~ Unif[0, 1]\nIncome ~ Clip(\u03b1 \u00b7 (Age + Education) / 2 + (1 \u2212 \u03b1) \u00b7 Unif[0, 1])\nPhone_Ownership ~ Clip(\u03b1 \u00b7 (1 \u2013 Income) + (1 \u2212 \u03b1) \u00b7 Unif[0, 1])\nTimes_To_Be_Called ~ Clip(\u03b1 \u00b7 Phone_Ownership + (1 \u2212 \u03b1) \u00b7 Unif[0, 1])\nThe computed quantities for each feature are then bucketed into its possible values. Refer to Table 4 in Appendix C for the values of each feature.\nSimilar to (Verma et al. 2024), we have assumed that the variables follow a uniform distribution as we do not have any information on the prior distribution of these variables. However, we intend to use more realistic distributions based on real-world data in future work. We have chosen the above structural equations for the following reasons. Age affects Income because older people are more likely to have more revenue streams. Education affects Income because more education allows better income opportunities. Phone_Ownership affects Times_To_Be_Called because, the more likely the phone is owned by a family, the more likely they would like to be called at a time when they have assured privacy. Income affects Phone_Ownership as more income could result in less likelihood of group ownership of the phone. In our structural equations, we assume Language Spoken to be independent of the rest.\nWe consider two possible values for \u03b1: 0.2 and 0.8. The former is intended to simulate a low conditional correlation between nodes of our graphical model, and the latter is intended to simulate a higher conditional correlation.\nTransition Probabilities Transition probabilities are computed in the same way as in (Verma et al. 2024). We use the \u201cweight vector parameters\u201d of the features inspired by Table 2 in the paper. There are two states, 0 and 1. The good state (state 1) is when beneficiaries listen to an automated message for more than 30 seconds, and the bad state (state 0) is when the participant does not engage.\nThe key idea is that the values of the features and the weights are used to calculate a value \u03b4, which is the difference between the active transition probabilities and the passive transition probabilities that we observe in a Restless MAB (Liu, Liu, and Zhao 2013). By expressing this \u03b4 as a function of the feature values and dataset weights, we create a dependency for the active transition probabilities on the features."}, {"title": "2.2 Prompts", "content": "We use a total of 8 prompts (also called 'goal prompts') for our analysis. Prompts 1 through 6 are increasing in complexity, while Prompts 7 and 8 are rephrased versions of Prompts 1 and 2, respectively. These will be used to analyze the effects of prompt complexity and prompt rephrasing. As we will see, outcomes indeed vary along both dimensions and in Section 6, we propose one direction that can be explored in the future to mitigate the issues with respect to phrasing.\nThe full set of prompts is given in Table 2. To view the full prompt given to the LLM, refer to Table 5 in Appendix D."}, {"title": "2.3 Fairness Metrics", "content": "Demographic Parity Demographic Parity (Kusner et al. 2017) ensures that the outcome of a model is independent of a specific protected attribute (e.g., gender, race, or age). It requires that individuals from different demographic groups have the same probability of being allocated.\nConsider the feature Age, which can take values in discrete intervals, such as 10-20, 21-30, 31-40, 41-50, and 51-60. The probability of an arm falling in one of these buckets (say, x) receiving an allocation (which is given by Y = 1) is\nP(Y = 1 | Age = x) = (P(Age = x | Y = 1) \u00b7 P(Y = 1)) / P(Age = x)\nwhere Y \u2208 {0, 1} is the random variable that is 1 if a particular arm of the Restless MAB has received an allocation.\nHere, the age group can take values x_1,x_2,...,x_k, corresponding to the discrete intervals of the feature.\nTo evaluate the fairness of the system across different demographic groups, we compute the variance in the allocation probabilities for different subgroups. This metric, referred to as DP_variance, is defined as:\nDP_variance = (1/k) * \u2211 (P(Y = 1 | Age = x_i) \u2013 P(Y = 1))^2\ni=1\nwhere k is the total number of groups (e.g., age intervals), and P(Y = 1) is the mean probability of receiving an allocation, calculated as:\nP(Y = 1) = (1/k) * \u2211 P(Y = 1 | Age = x_i)\ni=1\nWe consider a lower DP_variance to indicate that the model is closer to satisfying Demographic Parity. This metric can serve as an indicator of fairness, with zero variance implying perfect Demographic Parity."}, {"title": "3 Analysis 1: Proposing Acceptable Reward Functions", "content": "We define an acceptable reward function as a reward proposal by the LLM that contains all the relevant features and no spurious features. In the case where the goal prompt contains multiple features, an acceptable reward function must select the right features for all the conditions. Note that we consider the final reward functions proposed at the end of the evolutionary search of the DLM algorithm.\nWe measure the rate of acceptable reward functions for the languages English, Hindi, Tamil and Tulu and report it as Mean \u00b1 StdError. Note that the error reported is one standard error."}, {"title": "3.1 Result 1: English Outperforms in Proposing Acceptable Reward Functions", "content": "From Table 3, we observe that English has a higher rate of acceptable reward functions proposed by the LLM. We also observe from the allocation plots in Appendix A that this higher rate of acceptable reward functions proposed is directly correlated with task performance. This could be due to the LLM being provided with a better set of allocations for reflection, thereby improving the overall performance."}, {"title": "4 Analysis 2: Task Performance Under Multilingual Prompts", "content": "We measure task performance (i.e., how well the allocations align with the prompt) for multilingual prompts along two dimensions.\n1. Phrasing: We plot the allocation values for relevant features across different languages for two prompts that are semantically identical but phrased differently.\n2. Prompt Complexity: We plot the rate of acceptable allocations across various runs. We delve deeper into what an acceptable allocation is in later sections.\nAll results are averaged over 20 independent runs, and error bars are \u00b1 1 standard error from the sample mean."}, {"title": "4.1 Result 2: Phrasing Makes a Difference in Performance", "content": "We analyze the effects of phrasing on the performance of the algorithm by plotting the allocation percentage values across different languages for prompts that have been phrased differently. We observe from Figures 1, 2, 3, and 4 that changing the phrasing has a noticeable impact on the allocations. While the prompts are semantically the same, the allocations are noticeably different."}, {"title": "4.2 Result 3: Explicitly Stating your Goals Helps Improve Performance", "content": "We observe from Figures 1, 2, 3, and 4 that explicitly stating our goals helps elicit better performance from the algorithm. In fact, in some instances, the improvement is quite significant. Additionally, from Figure 5, we observe that even when the allocations are not as intended, explicitly stating the goal does improve allocations (in some cases, even pushing the needle to the point where the allocation could be considered desirable). It is something to be mindful of when using the DLM system.\nIn Appendix B, we provide plots for \u03b1 = 0.8 that further support our claims on the importance of phrasing and explicitly stating one's goals."}, {"title": "4.3 Result 4 : Performance Degrades with Increasing Prompt Complexity; But English is More Robust", "content": "We analyzed the task success rate (the percentage of our runs that successfully carried out the task) against each prompt, with the complexity increasing as we moved along the x-axis. Here, we define success as an instance where the allocations have deviated from the relevant features to the point where the allocation is higher than what it would have been had the allocation been uniform across feature values. From the plots Figure 6 and Figure 7, we see that as the complexity of the prompt increases, English, more often than not, performs better than the low resource languages. It is also to be noted that beyond a certain threshold of complexity (three feature allocation), performance collapses regardless of language.\nIn Appendix A, we have provided various allocation plots for the same, where we observe that English prompts frequently achieve the patterns of allocation intended by the goal prompt.\nNote: The very poor performance with all languages for Prompts 2 and 3 is due to the failure to provide correct allocations for the features Income and Times_To_Be_Called, both of them being ordered categorical features. We hypothesize that these types of features require explicit expression of the ranges, but further analysis is required for a definitive stance.\nFrom a deployability standpoint, the results of sections 4.1, 4.2 and 4.3 pose interesting questions. The difference in performance for different languages might be a cause for concern, especially in use cases like resource allocation for public health. These disparities would lead to a difference in outcomes for users who use different languages, thereby leading to unfairness. If the algorithm is to be deployed for grassroots-level interventions, where one might see the language dimension come to the fore, this difference in performance for different languages may cause undesirable out-"}, {"title": "5 Analysis 3: Fairness Across Multilingual Prompts", "content": "Each prompt specifies how allocations are distributed across different features. For instance, Prompt 1 demands a higher allocation for individuals with lower age values. In this case, Age is considered an 'intended feature' for Prompt 1, while all other features are 'unintended'. Thus, a higher DP_variance in the unintended features indicates unfairness. We plot two main counts as a measure of unfairness:\n\u2022 Absolute: We plot the number of prompts where DP_variance in unintended features is greater than a certain threshold for various values of thresholds.\n\u2022 Relative: We plot the number of prompts where DP_variance in unintended features is greater than DP_variance in intended feature.\nIn Figure 8, Figure 9, Figure 10 and Figure 11, we plot the average counts across 20 runs."}, {"title": "5.1 Result 5: Low Resource Languages are More Likely to Introduce Unfairness", "content": "From the plots in Figure 8, Figure 9 and Figure 10, we see that for \u03b1 = 0.2, low-resource languages like Tulu are more unfair as evident from the high count values. This is not as pronounced in \u03b1 = 0.8 since the high DP_variance in the intended feature propagates to the unintended features due to high conditional correlation."}, {"title": "5.2 Result 6: More Complex Prompts Tend to Result in More Unfairness", "content": "Prompts become more complex when the number of intended features increases, or when the prompt is not direct and requires a step of reasoning to fully understand it. Prompts 5 and 6 are more complex than prompts 1, 2 and 3 since they have more number of intended features and are harder to interpret. From Figure 11, prompts 5 and 6 seem to be more unfair than prompts 1, 2 and 3, supporting our claim that more complex prompts are more likely to be unfair."}, {"title": "6 Directions for Future Work", "content": "We propose a few directions for future work that can potentially mitigate some of the issues that came up in our experiments and analyses.\nPrompt rewriting component We could have a separate 'prompt rewriting' component in the DLM algorithm that takes the user prompt and rewrites it before sending it to the rest of the DLM algorithm. Specifically, this component could rewrite prompts with the right phrasing and possibly also make feature values desired by the user more explicit in the prompt. The results in our experimental sections indicate that both of these could potentially improve task performance.\nImproved reflection component Ideally, even if one of the multiple proposed reward functions is good, the LLM tasked with reflection is supposed to choose it. However, this frequently does not happen, as we can see in Section 3. Anecdotally as well, we saw that the LLM that does the evaluation sometimes picks a worse reward function even if a much better proposal is available. One possible direction to investigate is to see if we can improve the reflection LLM to increase the chance that it picks the best reward function from multiple proposals.\nPrompt guardrails Given that the system is intended to be used widely in critical settings such as public health, it is important to ensure that malicious users do not introduce undesirable biases through the prompt. We suggest that the system, before deployment, has the right guardrails in place to prevent such issues. One direction to explore is to have some kind of constitution (Bai et al. 2022) against which the prompts can be evaluated."}, {"title": "7 Conclusion", "content": "In this paper, we study the effects on both task performance and fairness when the DLM algorithm (Behari et al. 2024) is prompted in non-English languages. Our results show that the reward function proposals are better when prompted in English. Interestingly, we see that the way a prompt is worded (even if there is no change in meaning) affects task performance. Furthermore, performance degrades with prompt complexity, but it is more robust with English prompts than with lower-resource languages. Finally, our results show that prompts in lower-resource languages and more complex prompts are both more likely to create unfairness along unintended dimensions."}, {"title": "A Appendix: Detailed Allocation Plots", "content": "Figures 12 through 27 for full allocation plots."}, {"title": "B Appendix: Phrasing Plots for \u03b1 0.8", "content": "Please refer to Figures 28 through 32."}, {"title": "C Appendix: Possible Values of Each Feature", "content": "The possible values of each feature is given in Table 4."}]}