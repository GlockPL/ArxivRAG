{"title": "Nearly Lossless Adaptive Bit Switching", "authors": ["Haiduo Huang", "Tian Xia", "Wenzhe zhao", "Zhenhua Liu", "Pengju Ren"], "abstract": "Model quantization is widely applied for compressing and accelerating deep neural\nnetworks (DNNs). However, conventional Quantization-Aware Training (QAT)\nfocuses on training DNNs with uniform bit-width. The bit-width settings vary\nacross different hardware and transmission demands, which induces considerable\ntraining and storage costs. Hence, the scheme of one-shot joint training multiple\nprecisions is proposed to address this issue. Previous works either store a larger\nFP32 model to switch between different precision models for higher accuracy or\nstore a smaller INT8 model but compromise accuracy due to using shared quanti-\nzation parameters. In this paper, we introduce the Double Rounding quantization\nmethod, which fully utilizes the quantized representation range to accomplish\nnearly lossless bit-switching while reducing storage by using the highest integer\nprecision instead of full precision. Furthermore, we observe a competitive inter-\nference among different precisions during one-shot joint training, primarily due\nto inconsistent gradients of quantization scales during backward propagation. To\ntackle this problem, we propose an Adaptive Learning Rate Scaling (ALRS) tech-\nnique that dynamically adapts learning rates for various precisions to optimize the\ntraining process. Additionally, we extend our Double Rounding to one-shot mixed\nprecision training and develop a Hessian-Aware Stochastic Bit-switching (HASB)\nstrategy. Experimental results on the ImageNet-1K classification demonstrate that\nour methods have enough advantages to state-of-the-art one-shot joint QAT in both\nmulti-precision and mixed-precision. We also validate the feasibility of our method\non detection and segmentation tasks, as well as on LLMs task. Our codes are\navailable at https://github.com/haiduo/Double-Rounding.", "sections": [{"title": "1 Introduction", "content": "Recently, with the popularity of mobile and edge devices, more and more researchers have attracted\nattention to model compression due to the limitation of computing resources and storage. Model\nquantization [1; 2] has gained significant prominence in the industry. Quantization maps floating-point\nvalues to integer values, significantly reducing storage requirements and computational resources\nwithout altering the network architecture.\nGenerally, for a given pre-trained model, the quantization bit-width configuration is predefined for a\nspecific application scenario. The quantized model then undergoes retraining, i.e., QAT, to mitigate\nthe accuracy decline. However, when the model is deployed across diverse scenarios with different\nprecisions, it often requires repetitive retraining processes for the same model. A lot of computing"}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Double Rounding", "content": "Conventional separate precision quantization using Quantization-Aware Training (QAT) [25] attain\na fixed bit-width quantized model under a pre-trained FP32 model. A pseudo-quantization node is\ninserted into each layer of the model during training. This pseudo-quantization node comprises two\noperations: the quantization operation $quant(x)$, which maps floating-point (FP32) values to lower-\nbit integer values, and the dequantization operation $dequant(x)$, which restores the quantized integer\nvalue to its original floating-point representation. It can simulate the quantization error incurred\nwhen compressing float values into integer values. As quantization involves a non-differentiable\nRounding operation, Straight-Through Estimator (STE) [26] is commonly used to handle the non-\ndifferentiability."}, {"title": "3.2 Adaptive Learning Rate Scaling for Multi-Precision", "content": "Although our proposed Double Rounding method represents a significant improvement over most\nprevious multi-precision works, the one-shot joint optimization of multiple precisions remains\nconstrained by severe competition between the highest and lowest precisions [10; 4]. Different\nprecisions simultaneously impact each other during joint training, resulting in substantial differences\nin convergence rates between them, as shown in Figure 3 (c). We experimentally find that this\ncompetitive relationship stems from the inconsistent magnitudes of the quantization scale's gradients\nbetween high-bit and low-bit quantization during joint training, as shown in Figure 3 (a) and (b). For\nother models statistical results please refer to Section A.6 in the appendix."}, {"title": "3.3 One-Shot Mixed-Precision SuperNet", "content": "Unlike multi-precision, where all layers uniformly utilize the same bit-width, mixed-precision\nSuperNet provides finer-grained adaptive by configuring the bit-width at different layers. Previous\nmethods typically decouple the training and search stages, which need a third stage for retraining\nor fine-tuning the searched SubNets. These approaches generally incur substantial search costs in\nselecting the optimal SubNets, often employing methods such as greedy algorithms [28; 9] or genetic\nalgorithms [29; 4]. Considering the fact that the sensitivity [30], i.e., importance, of each layer\nis different, we propose a Hessian-Aware Stochastic Bit-switching (HASB) strategy for one-shot\nmixed-precision training."}, {"title": "4 Experimental Results", "content": ""}, {"title": "4.1 Image Classification", "content": "Setup. In this paper, we mainly focus on ImageNet-1K classification task using both classical\nnetworks (ResNet18/50) and lightweight networks (MobileNetV2), which same as previous works.\nExperiments cover joint quantization training for multi-precision and mixed precision. We explore\ntwo candidate bit configurations, i.e., {8,6,4,2}-bit and {4,3,2}-bit, each number represents the\nquantization level of the weight and activation layers. Like previous methods, we exclude batch\nnormalization layers from quantization, and the first and last layers are kept at full precision. We\ninitialize the multi-precision models with a pre-trained FP32 model, and initialize the mixed-precision\nmodels with a pre-trained multi-precision model. All models use the Adam optimizer with a batch\nsize of 256 for 90 epochs and use a cosine scheduler without warm-up phase. The initial learning"}, {"title": "4.1.1 Multi-Precision", "content": "Results. For {8,6,4,2}-bit configuration, the Top-1 validation accuracy is shown in Table 1. The\nnetwork weights and the corresponding activations are quantized into w-bit and a-bit respectively.\nOur double-rounding combined with ALRS training strategy surpasses the previous state-of-the-art\n(SOTA) methods. For example, in ResNet18, it exceeds Any-Precision by 2.7%(or 2.83%) under w8a8\nsetting without(or with) using KD technique, and outperforms MultiQuant by 0.63%(or 0.73%) under\nw4a4 setting without(or with) using KD technique respectively. Additionally, when the candidate\nbit-list includes 2-bit, the previous methods can't converge on MobileNetV2 during training. So, they\nuse {8,6,4}-bit precision for MobileNetV2 experiments. For consistency, we also test {8,6,4}-bit\nresults, as shown in the \"Ours {8,6,4}-bit\" rows of Table 1. Our method achieves 0.25%/0.11%/0.56%\nhigher accuracy than AdaBits under the w8a8/w6a6/w4a4 settings.\nNotably, our method exhibits the ability to converge but shows a big decline in accuracy on Mo-\nbileNetV2. On the one hand, the compact model exhibits significant differences in the quantization\nscale gradients of different channels due to involving Depth-Wise Convolution [32]. On the other\nhand, when the bit-list includes 2-bit, it intensifies competition between different precisions during\ntraining. To improve the accuracy of compact models, we suggest considering the per-layer or\nper-channel learning rate scaling techniques in future work.\nFor {4,3,2}-bit configuration, Table 2 demonstrate that our double-rounding consistently surpasses\nprevious SOTA methods. For instance, in ResNet18, it exceeds Bit-Mixer by 0.63%/0.7%/1.2%(or\n0.37%/0.64%/1.02%) under w4a4/w3a3/w2a2 settings without(or with) using KD technique, and\noutperforms ABN by 0.87%/0.74%/1.12% under w4a4/w3a3/w2a2 settings with using KD tech-\nnique respectively. In ResNet50, Our method outperforms Bit-Mixer by 0.86%/0.63%/0.1% under\nw4a4/w3a3/w2a2 settings.\nNotably, the overall results of Table 2 are worse than the {8,6,4,2}-bit configuration for joint training.\nWe analyze that this discrepancy arises from information loss in the shared lower precision model\n(i.e., 4-bit) used for bit-switching. In other words, compared with 4-bit, it is easier to directly optimize\n8-bit quantization parameters to converge to the optimal value. So, we recommend including 8-bit for"}, {"title": "4.1.2 Mixed-Precision", "content": "Results. We follow previous works to conduct mixed-precision experiments based on the {4,3,2}-bit\nconfiguration. Our proposed one-shot mixed-precision joint quantization method with the HASB tech-\nnique comparable to the previous SOTA methods, as presented in Table 3. For example, in ResNet18,\nour method exceeds Bit-Mixer by 0.83%/0.72%/0.77%/7.07% under w4a4/w3a3/w2a2/3MP settings\nand outperforms EQ-Net [5] by 0.2% under 3MP setting. The results demonstrate the effectiveness\nof one-shot mixed-precision joint training to consider sensitivity with Hessian Matrix Trace when\nrandomly allocating bit-widths for different layers. Additionally, Table 3 reveals that our results do\nnot achieve optimal performance across all settings. We hypothesize that extending the number of\ntraining epochs or combining ILP with other efficient search methods, such as genetic algorithms,\nmay be necessary to achieve optimal results in mixed-precision optimization."}, {"title": "4.2 Object Detection and Segmentation", "content": "Setup. We utilize the pre-trained models as the backbone within the Mask-RCNN [37] detector for\nobject detection and instance segmentation on the MS-COCO 2017 dataset, comprising 118K training\nimages and 5K validation images. We follow the Pytorch official \"code\" and employ the AdamW\noptimizer, conduct training of 26 epochs, use a batch size of 16, and maintain other training settings\nwithout further hyperparameter tuning.\nResults. Table 4 reports the average precision (mAP) performance for both detection and instance\nsegmentation tasks for quantizing the backbone of the Mask-RCNN model on the COCO dataset.\nThe results further confirm the generalization capabilities of our Double Rounding."}, {"title": "4.3 LLMs Task", "content": "We also conduct experiments on Large Language Models (LLMs) to validate the effectiveness of our\nmethod in more recent architectures, as shown in Table 5. We conduct multi-precision experiments\non small LLMs [38] without using ALRS and distillation. Note that, except for not quantizing the\nembedding layer and head layer, due to the sensitivity of the SiLU activation causing non-convergence,\nwe don't quantize the SiLU activation in the MLP and set the batch size to 16. The results demonstrate\nthat our approach applies to more recent and complex models."}, {"title": "4.4 Ablation Studies", "content": "ALRS vs. Conventional in Multi-Precision. To verify the effectiveness of our proposed ALRS train-\ning strategy, we conduct an ablation experiment without KD, as shown in Table 6, and observe overall\naccuracy improvements, particularly for the 2bit. Like previous works, where MobileNetV2 can't\nachieve stable convergence with {4,3,2}-bit, we also opt for {8,6,4}-bit to keep consistent. However,\nour method can achieve stable convergence with {8,6,4,2}-bit quantization. This demonstrates the\nsuperiority of our proposed Double-Rounding and ALRS methods. In addition, we conduct ablation\nstudies of other methods with or without ALRS, as shown in Table 7. The results further validate the\neffectiveness of our proposed ALRS for multi-precision.\nMulti-Precision vs. Separate-Precision in Time Cost. We statistic the results regarding the time\ncost for normal multi-precision compared to separate-precision quantization, as shown in Table 8.\nMulti-precision training costs stay approximate constant as the number of candidate bit-widths.\nPareto Frontier of Different Mixed-Precision Configurations. To verify the effectiveness of our\nHASB strategy, we conduct ablation experiments on different bit-lists. Figure 5 shows the search\nresults of Mixed-precision SuperNet under {8,6,4,2}-bit, {4,3,2}-bit and {8,4}-bit configurations\nrespectively. Where each point represents a SubNet. These results are obtained directly from ILP"}, {"title": "5 Conclusion", "content": "This paper first introduces Double Rounding quantization method used to address the challenges\nof multi-precision and mixed-precision joint training. It can store single integer-weight parameters\nand attain nearly lossless bit-switching. Secondly, we propose an Adaptive Learning Rate Scaling\n(ALRS) method for multi-precision joint training that narrows the training convergence gap between\nhigh-precision and low-precision, enhancing model accuracy of multi-precision. Finally, our proposed\nHessian-Aware Stochastic Bit-switching (HASB) strategy for one-shot mixed-precision SuperNet"}, {"title": "A Appendix / supplemental material", "content": ""}, {"title": "A.1 Overview", "content": "In this supplementary material, we present more explanations and experimental results.\n\u2022 First, we provide a detailed explanation of the different quantization types under QAT.\n\u2022 We then present a comparison of multi-precision and separate-precision on the ImageNet-1k dataset.\n\u2022 Furthermore, we provide the gradient formulation of Double Rounding.\n\u2022 And, the algorithm implementation of both multi-precision and mixed-precision training approaches.\n\u2022 Then, we provide more gradient statistics of learnable quantization scales in different networks.\n\u2022 Finally, we also provide the bit-widths learned by each layer of the mixed-precision with a given\naverage bit-width condition."}, {"title": "A.2 Different Quantization Types", "content": "In this section, we provide a detailed explanation of the different quantization types during\nQuantization-Aware Training (QAT), as is shown in Figure 6."}, {"title": "A.3 Multi-Precision vs. Separate-Precision.", "content": "We provide the comparison of Multi-Precision and Separate-Precision on ImageNet-1K dataset.\nTable 9 shows that our Multi-Precision joint training scheme has comparable accuracy of different\nprecisions compared to Separate-Precision with multiple re-train. This further proves the effectiveness\nof our proposed One-shot Double Rounding Multi-Precision method."}, {"title": "A.4 The Gradient Formulation of Double Rounding", "content": "A general formulation for uniform quantization process is as follows:\n$W\n=\nclip(\\frac{W}{s} + z, -2^{b-1}, 2^{b-1} \u2013 1)$\n(8)\n$W = (W-z) \\times s$\n(9)\nwhere the symbol [.] denotes the Rounding function, $clip(x, low, upper)$ expresses $x$ below $low$\nare set to $low$ and above $upper$ are set to $upper$. $b$ denotes the quantization level (or bit-width),\n$s \\in R$ and $z \\in Z$ represents the quantization scale (or interval) and zero-point associated with each $b$,\nrespectively. $W$ represents the FP32 model's weights, $W$ signifies the quantized integer weights, and\n$W$ represents the dequantized floating-point weights.\nThe quantization scale of our Double Rounding is learned online and not fixed. And it only needs a\npair of shared quantization parameters, i.e., scale and zero-point. Suppose the highest-bit and the\nlow-bit are denoted as h-bit and l-bit respectively, and the difference between them is $\\Delta = h \u2013 l$.\nThe specific formulation is as follows:\n$W_h\n=\nclip(\\frac{W}{S_h} - Z_h ,-2^{h-1},2^{h-1}-1)$\n(10)\n$W_l = clip(\\frac{W_h}{2^\\Delta} , -2^{l-1}, 2^{l-1} \u2013 1)$\n(11)\n$W_l = W_l\\times S_h\\times2^\\Delta + Z_h$\n(12)\nwhere $s_h \\in R$ and $z_h \\in Z$ denote the highest-bit quantization scale and zero-point respectively. $W_h$\nand $W_l$ represent the quantized weights of the highest-bit and low-bit respectively. Hardware shift\noperations can efficiently execute the division and multiplication by $2^\\Delta$. And the $z_h$ is 0 for the\nweight quantization in this paper. The gradient formulation of Double Rounding for one-shot joint\ntraining is represented as follows:\n$\\frac{\\partial Y}{\\partial s_h} ~\n\\begin{cases}\n\\frac{Y-Z_h}{S_h}, & \\text{if } n < \\frac{Y-Z_h}{S_h} < p, \\\\\n0 & otherwise.\n\\end{cases}$\n(13)\n$\\frac{\\partial Y}{\\partial z_h} ~\n\\begin{cases}\n0, & \\text{if } n < \\frac{Y-Z_h}{S_h} < p, \\\\\n1 & otherwise.\n\\end{cases}$\n(14)\nwhere n and p denote the lower and upper bounds of the integer range $[N_{min}, N_{max}]$ for quantizing\nthe weights or activations respectively. Y represents the FP32 weights or activations, and \u0176 represents\nthe dequantized weights or activations. Unlike weights, activation quantization scale and zero-point\nof different precisions are learned independently. However, the gradient formulation is the same."}, {"title": "A.5 Algorithms", "content": "This section provides the algorithm implementations of multi-precision, one-shot mixed-precision\njoint training, and the search stage of SubNets."}, {"title": "A.5.1 Multi-Precision Joint Training", "content": "The multi-precision model with different quantization precisions shares the same model weight(e.g.,\nthe highest-bit) during joint training. In conventional multi-precision, the shared weight (e.g., multi-\nprecision model) computes n forward processes at each training iteration, where n is the number of\ncandidate bit-widths. Then, all attained losses of different precisions perform an accumulation, and\nupdate the parameters accordingly. For specific implementation details please refer to Algorithm A.1.\nHowever, we find that if separate precision loss and parameter updates are performed directly after\ncalculating a precision at each forward process, it will lead to difficulty convergence during training\nor suboptimal accuracy. In other words, the varying gradient magnitudes of quantization scales of"}, {"title": "A.5.2 One-shot Joint Training for Mixed Precision SuperNet", "content": "Unlike multi-precision joint quantization, the bit-switching of mixed-precision training is more\ncomplicated. In multi-precision training, the bit-widths calculated in each iteration are fixed, e.g.,"}, {"title": "A.5.3 Efficient One-shot Searching for Mixed Precision SuperNet", "content": "After training the mixed-precision SuperNet, the next step is to select the appropriate optimal SubNets\nbased on conditions, such as model parameters, latency, and FLOPs, for actual deployment and\ninference. To achieve optimal allocations for candidate bit-width under given conditions, we employ\nthe Iterative Integer Linear Programming (ILP) approach. Since each ILP run can only provide\none solution, we obtain multiple solutions by altering the values of different average bit widths.\nSpecifically, given a trained SuperNet (e.g., RestNet18), it takes less than two minutes to solve\ncandidate SubNets. It can be implemented through the Python PULP package. Finally, these searched\nSubNets only need inference to attain final accuracy, which needs a few hours. This forms a Pareto\noptimal frontier. From this frontier, we can select the appropriate subnet for deployment. Specific\nimplementation details of the searching process by ILP can be found in Algorithm 2."}]}