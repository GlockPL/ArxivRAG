{"title": "GraphicsDreamer: Image to 3D Generation with Physical Consistency", "authors": ["Pei Chen", "Fudong Wang", "Yixuan Tong", "Jingdong Chen", "Ming Yang", "Minghui Yang"], "abstract": "Recently, the surge of efficient and automated 3D AI-generated content (AIGC) methods has increasingly illuminated the path of transforming human imagination into complex 3D structures. However, the automated generation of 3D content is still significantly lags in industrial application. This gap exists because 3D modeling demands high-quality assets with sharp geometry, exquisite topology, and physically based rendering (PBR), among other criteria. To narrow the disparity between generated results and artists' expectations, we introduce GraphicsDreamer, a method for creating highly usable 3D meshes from single images. To better capture the geometry and material details, we integrate the PBR lighting equation into our cross-domain diffusion model, concurrently predicting multi-view color, normal, depth images, and PBR materials. In the geometry fusion stage, we continue to enforce the PBR constraints, ensuring that the generated 3D objects possess reliable texture details, supporting realistic relighting. Furthermore, our method incorporates topology optimization and fast UV unwrapping capabilities, allowing the 3D products to be seamlessly imported into graphics engines. Extensive experiments demonstrate that our model can produce high quality 3D assets in a reasonable time cost compared to previous methods.", "sections": [{"title": "1. Introduction", "content": "Traditional 3D modeling processes heavily relies on manual labor, which significantly hinders the application of 3D content in Internet and gaming industries. Excitingly, with the evolution of generative AI, many works [13, 27, 39, 40, 42,"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. 2D Diffusion for 3D Generation", "content": "Recent advancements has demonstrated that utilizing CLIP model [26, 49, 59, 78] or a 2D diffusion model [60, 61], researchers can directly generate 3D objects from user prompts. The pioneer work DreamFusion [56] leverages score distillation sampling (SDS) to extract prior knowledge from a 2D diffusion model, iteratively optimizing a neural radiance field (NeRF) [47] and achieve zero-shot text-to-3D generation. Concurrently, SJC [70] utilizes score Jacobian chaining to achieve a similar goal. Building on this foundation, Magic3D [37] employs an improved multi-resolution SDS to enhances the precision of 3D generation. Fantasia3D [7] integrates DMTet and SDS, separating textures from geometry, aiming to improve texture quality.\nHowever, due to the lack of multi-view constraints, these methods often produce objects with multi faces. Additionally, the hour-level generation time required for per-shape optimization is usually difficult to accept. Other methods, such as One-2-3-45 [16], Magic123 [57], and Make-it-3d [67], directly generate 3D geometries from image conditions, though they significantly speed up generation, the quality is lower, lacking in geometric and texture details."}, {"title": "2.2. 3D Generative Models", "content": "This type of generative model is trained directly on 3D data, learning to capture the distribution of 3D data, and have achieved convincing results. The forms of 3D repre-sentations can generally be classified into voxels [24, 77], point clouds [44, 52, 82], meshes [17, 41], and neural fields"}, {"title": "2.3. Multi-view Diffusion Models", "content": "Zero-1-to-3 [39] enhances the 2D diffusion by fine-tuning the Stable Diffusion model [60], enabling it to perform novel-view synthesis from specified views. More recent de-velopments have seen significant improvements in the con-sistency of multi-view image generation through multi-view diffusion [40, 63, 64, 66, 69, 76, 81, 86]. A prominent project in this series is MVDream [64], which fine-tunes a pretrained diffusion model using multi-view images ren-dered from 3D objects in the Objaverse dataset [12]. How-ever, because they rely solely on RGB images, often en-counter texture ambiguity during geometric reconstruction.\nSubsequent work, Wonder3D [43] incorporates normal images into its training data, utilizing an RGB-Normal dif-fusion model to enhance the generated geometric details. RichDreamer's [58] Normal-Depth diffusion model enables it to generate rich geometric details. In contrast, our model constructs 3D objects as a joint distribution of images in six domains, comprehensively representing both geometry and materials. By integrating the PBR conditions into both the multi-view image synthesis stage and geometry fusion stage, our approach significantly boosts the generation quality, fully adhering to the computer graphics pipeline."}, {"title": "2.4. Materials and Light Estimation", "content": "Known as intrinsic image decomposition [1, 19] or inverse rendering [54], it's a challenging task to estimate the ge-ometry, intrinsic materials and lighting of observed objects"}, {"title": "3. Method", "content": "As illustrated in Fig. 2, GraphicsDreamer consists of three phases. Given a single input image of the desired object, we first build a diffusion model to generate multi view images (6 views), including RGB color for the overall appearance, normal and depth as geometric information, intrinsic mate-rials for texture details, which are simultaneously controlled by a PBR condition as demonstrated in Sec. 3.1. The gen-erated images are then treated as pseudo ground truth for refining a deep learning-based inverse rendering reconstruc-tion, which is also conditioned by PBR to keep consistent with the generative model. It results in a complete 3D object as a surface mesh with smooth geometry and distinct textures, as described in Sec. 3.2 and Sec. 3.3. At last, our method will product appealing 3D assets with artistically optimized topology and UV textures, which are essential in modern CG workflow, as shown in Sec. 3.4."}, {"title": "3.1. PBR Diffusion Model", "content": "The Distribution of 3D Assets. Previous work, such as Wonder3D [43] and RichDreamer [58], selected color im-ages along with corresponding normal or depth images as learning targets for their 2D diffusion models. While this representation can model 3D geometry, it falls short in ad-"}, {"title": "3.2. Mixed Surface Representation", "content": "With the above generated sparse-view images and materi-als of object, we need to recover the geometry based on which the physically-based rendering and lighting proce-dure is performed. Has been proved its efficiency in cur-rent studies [48, 72] that implicit representation, e.g. signed distance function (SDF), can achieve better differentiability and stability, it is incompatible with the reflection occur-ring on object's surface incorporated in PBR function and thus can not be used directly. In this section, we introduce a mixture representation consists of implicit SDF and ex-plicit surface such that both the differentiability, stability and compatibility can be achieved simultaneously.\nImplicit SDF Initilization. We initially adopt the implicit SDF representation introduced in NeuS [72] that can con-vergent to the zero-level iso-surface S faster. Given a ray $r \\triangleq r_o+t \\cdot r_d$, where $r_o,r_d \\in R^3$ are the ordinary and destination respectively, $t \\in R^+$ is the depth of the current sample distance, we define the SDF $f \\in R$ of a sampled point $x = r_o + t_r \\cdot r_d$ on the ray r as $f(x) = f_m(\\theta,x)$, where $f_m (\\theta,\\cdot)$ consists of MLPs w.r.t. the neural parame-ters $\\theta$. The zero-level isosurface S of the desired object is $f_m (\\theta, x) = 0 \\Leftrightarrow x \\in S$.\nEmpirically, we can represent an intersection point $x_s$, lying on a ray r and iso-surface S simply, i.e., as a weighted sum of all the sampled candidates along this ray,\n$x_s \\triangleq \\sum_{i=0}^N w_i x_i$, where $x_i = r_o + t_i r_d$. (4)\nNote that, this formulation is based on the assumption guar-anteed by Neus [72] that the distribution of weights $\\{w_i\\}_{i \\in [0, 1]}$ along a ray is well-approximated as a unimodal func-tion whose value increases if $x_i$ gets closer to S from the visible view (not back view). Unfortunately, we find that the resulted $x_s$ is not smooth enough when it marches closed to S, somehow suffering from the limitation of geometry con-sistency of generated sparse view images and materials in Sec. 3.1. Therefore, we introduce an simple yet efficient sampling method to extract more smooth $\\{x_s\\}$ in an ex-plicit way, as illustrated in Fig. 5.\nExplicit Surface Sampling. With SDF $f_m(\\theta)$ defined above, an intersection point $x_i \\in r \\cap S$ lies between two"}, {"title": "3.3. Physically-based Rendering", "content": "For training the multi-view generative model and the inverse rendering reconstruction, we both implement a physically-based rendering [55] approximation with the simplified Disney principle BRDF [5] and spherical Gaus-sian [45, 71] lighting as illustrated in Fig. 5.\nThe Rendering Equation. To formulate the intersection of light and object's surface, the rendering equation [28] has been introduced based on the physical law of energy con-servation, accounting the contribution of intrinsic materials of object. Given a surface point x (i.e., xs above) with nor-mal n, the radiance of incident light $L_i$ shining at point x along incident direction $w_i$ is $L_i (w_i, x)$, with an observation direction $w_o$ to the point x, the reflection accounting for ge-ometry and materials can be reduced by the bidirectional reflectance distribution function (BRDF) as $f_r(w_o, w_i; x)$, the observed radiance $L_o(w_o, x)$ is defined as,\n$L_o(w_o, x) = \\int_{\\Omega} L_i(w_i)f_r(w_o, w_i; x)(w_i \\cdot n)dw_i$, (7)\nwhere $\\Omega \\equiv \\{w_i | w_i \\cdot n > 0\\}$ is the hemisphere over which the integral Eqn. (7) is conducted.\nDisney BRDF. The implementation of BRDF plays a core role of Eqn. (7), we utilize the widely used Disney BRDF [5] developed from Cook-Torrance [11] as,\n$f_r(w_o, w_i; x) \\triangleq f_a + f_s(w_o, w_i; x)$ (8)\n$\\triangleq k_d \\frac{a}{\\pi} + \\frac{D(h)F(w_o, w_i)G(W_o, W_i)}{4(w_o \\cdot n)(w_i \\cdot n)}$ (9)\nwhere $k_d$ is the diffuse refraction, $a \\in [0, 1]^3$ is the albedo, $h = (w_o + w_i)/||w_o + W_i||_2$ is the half vector, D, F, G are the normal distribution function (NDF), Fresnel and ge-ometry term, respectively. The integral in Eqn. (7) can be approximately calculated either in discrete integration (e.g., precomputed radiance transfer (PRT)) [65] or closed form [45, 71]. To achieve more efficiency we adopt the im-plementation in closed form, which needs to be approxi-mated as spherical Gaussians described as follows.\nSpherical Gaussian Formulation. An spherical Gaussian (SG) in $R^3$ is defined as [71]:\n$G_s(v; p, \\lambda, \\mu) \\triangleq \\mu e^{\\lambda (v \\cdot p - 1)}$, (10)\nwhere $v \\in S^2$ is the input vector, $p \\in S^2$ is the lobe axis, $\\lambda \\in R^+$ is the sharpness, and $\\mu \\in R^3$ is the amplitude.\nLight as SG. Concretely, the light $L_i$ is represented as a mixture of $N = 16$ SGs as\n$L_i(W_i) \\triangleq \\sum_{j=1}^N G_s (W_i; p_j, d_j, \\mu_j)$. (11)\nBRDF as SG. The NDF term D(h) can be approximately represented as a single spherically wrapped SG [71],\n$D(h) \\approx G_s(h; p_w, \\lambda_w, \\mu_w)$, (12)\nwhere $p_w, \\lambda_w, \\mu_w$ are wrapped as:\n$p_w \\triangleq \\frac{2(w_o \\cdot n)n - w_o}{\\left|| 2(w_o \\cdot n)n - w_o \\right||}, \\qquad \\mu_w \\triangleq \\frac{1}{\\pi \\gamma^4}, \\qquad \\gamma \\triangleq \\frac{r^2+1}{r^4}$. (13)\nwith $r \\in [0, 1]$ is the roughness at point x. Moreover, under the smooth and constant assumption [71], $F(w_o, w_i)$ and $G(w_o, w_i)$ can be calculated as constants $F_o, G_o$ on the sup-port of D(h) with approximation $w_i \\approx 2(w_o \\cdot n)n - w_o$. Thus $f_s (w_o, w_i; x)$ can be rewritten as\n$f_s (w_o, w_i; x) \\approx G_s(h; p_w, \\lambda_w, F_oG_o\\mu)$. (14)\nSee more details about calculating $k_a, F_o, G_o$ w.r.t. $w_o, w_i, n, r, m, s$ in our supplemental materials.\nCosine as SG. Now we consider the only left cosine $w_i \\cdot n$ in Eqn. (7). As proposed in [45], it can be approximated as\n$w_i \\cdot n \\approx G_s (w_i; n, 0.0315, 32.7080) - 31.7003$. (15)\nFinally, the integrand in Eqn. (7) is the product of three SGs, i.e. Eqn. (11), (14) and (15), it's also a spherical Gaus-sian and can be integrated with closed form [45]."}, {"title": "3.4. Asset Enhancement", "content": "Mesh Quadrification and UV Unwrapping. The meshes generated by the Marching Cubes algorithm typically con-sist of millions of uneven triangles and messy topology, making it very difficult for artists to make any edits, such as reducing polygons to create additional levels of de-tail (LOD) variants. To overcome this challenge, we use Blender's [3] Quad Remesher [25] tool to remesh these tri-angular meshes into quad-faced meshes with a reasonable number of faces (e.g. 20k), while preserving sharp edges and flat surfaces. Next, we unwrap the UVs of the remeshed objects automatically, and bake the per-vertex colors from the original high-poly model onto the remeshed low-poly model, ultimately converting it into a 3D asset that meets PBR standards, as shown in Fig. 2. This whole process ef-ficiently and reliably produces high-quality, refined 3D as-sets, facilitates the direct use of the generated digital assets within existing computer graphics (CG) workflows."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Implementation Details", "content": "We wrote a Blender script to filter approximately 32,000 3D objects with complete PBR material maps from the Ob-javerse dataset [12], and then normalized all objects to a unit scale. To create a multi-view image dataset, cameras were placed in six positions: front, back, left, right, front-right, and front-left. We automated the modification of shader node connections in the .glb format 3D objects to render multi-view images of color, normal, depth, albedo, rough-ness, and metallic.\nIn the multi-view synthesis stage, we fine-tuned on the pretrained Stable Diffusion Image Variants Model, which has image-to-image generation capabilities. We employed a batch size of 512 and an image resolution of 256, with the multi-view self-attention training for 30,000 steps and the cross-domain attention training for 20, 000 steps. The entire training process was conducted on a single machine with 8 A100 GPUs, taking nearly 5 days.\nIn the inverse rendering phase, the SDF MLP fm (0) con-sists of 8 nonlinear layers of width 128, with a skip con-nection at 4th layer. The material MLP fc(0') comprises 4 nonlinear layers of width 128, concluding with Sigmoid activation layer. Positional encoding [48] is employed in both of fm (0) and fc(0'), utilizing L = 10 frequency com-ponents. For the parameters {pj, dj, \u03bc;}j of light Li, we uniformly initialize {p; to be distributed on the unit sphere S2 and normalize \u03bc;}j by dividing by the total energy."}, {"title": "4.2. Evaluation", "content": "Baseline We adopt Zero123 [39], One-2-3-45++ [38], SyncDreamer [40], Wonder3D [43], InstantMesh [79],"}, {"title": "4.3. Novel View Synthesis", "content": "We evaluated the quality of novel-view synthesis across dif-ferent methods. Qualitative results can be seen in Fig. 3 and Fig. 6, while the quantitative outcomes are presented in Tab. 1. Wonder3D [43], which lacks depth information as overall geometric prior, sometimes produces distorted ge-ometries and struggles with complex structures. Despite SyncDreamer [40] introduced a volume attention scheme to enhance consistency, their model often yields unreason-able results. In contrast, our method faithfully generates 3D models according to the input images and performs well in terms of both geometry and texture."}, {"title": "4.4. Surface Reconstruction", "content": "We evaluate the effectiveness of the mixed representation of surface proposed in Sec. 3.2, which enhances the smooth-ness and contributes to a more visually appealing surface. Compared to other methods, our approach achieves a SOTA surface accuracy, as shown in Tab. 2 and Fig. 4."}, {"title": "4.5. Materials and Relighting", "content": "We can naturally relight the 3D objects using the materials integrated into the complete 3D surface mesh following the Physically Based Rendering (PBR) process, which has been mapped into UV space. We employ three types of environ-ment maps to simulate dark, ordinary, and bright conditions, respectively. In Fig. 7, we demonstrate that our model can handle special cases such as highlights and metallic mate-rials. And as shown in Fig. 8, it evaluates how the intrin-sic materials extracted by our method interact with varying light environment maps."}, {"title": "5. Conclusions", "content": "In this paper, we introduce GraphicsDreamer, an advanced workflow for 3D objects modeling tailored for modern graphics. It comprises a multi view diffusion genera-tive model that integrates geometry and intrinsic mate-rials, alongside a deep learning-based inverse rendering that aggregates multi view images and intrinsic materials into a complete 3D surface mesh. By employing Physi-cally Based Rendering (PBR) as a condition in both stages, GraphicsDreamer ensures convincing consistency in mate-rials throughout the entire process. Furthermore, Graph-icsDreamer incorporates topology optimization and UV un-wrapping, which are often overlooked in previous works. Experiments demonstrate that our method excels in fidelity and accuracy, particularly regarding geometric and textural details, while also ensuring clean topology.\nIn future work, we plan to increase the amount of training data and simultaneously render objects with a wider variety of lighting conditions using thousands of environment maps. With the improved re-training of generative intrinsic materials, we aim to achieve a more stable and accurate inverse rendering refinement process."}, {"title": "A2.1. Explicit Surface Sampling", "content": "Z-buffer Implementation. As noted in lines 335 \u2013 367, for each ray r, we will first find two specific sample points xp, xn \u2208r following the z-buffer [68] method. Since the original z-buffer method is used for rasterizing a mesh sur-face, we have implemented it with specific revisions in our work.\nConcretely, given N (e.g., N = 64) sample points $\\{x_i\\}_{i=1}^N$ along the ray r, with the definition of xi in Eqn. (4) in line 343, we reorder $\\{x_i\\}_{i}$ w.r.t. an increasing order of $\\{t_i\\}_i$. Next, we compute the SDF values of $\\{x_i\\}$ with the SDF MLPs as $\\{f_m(\\theta,x_i)\\}$, which hold positive val-ues in the exterior space and negative values in the interior space. We also compute the signs of these SDF values as $\\{sign(f_m(\\theta, x_i))\\}_i$. Therefore, we have\n$sign(f_m(\\theta, x_i))=$\\begin{cases}\n+1, & x_i in exterior space,\\\\\n-1, & x_i in interior space.\n\\end{cases}$(16)\nBy this way, the product of signs of two neighbor points xi-1, xi as $sign(f_m(\\theta, x_{i-1})) \\cdot sign(f_m(\\theta, x_i))$ equals -1 means that the there exists an intersection point xs between xi-1 and xi. The rule of z-buffer requires that the inter-section point xs has the minimal depth ts along the ray r. Fortunately, since the depth values $\\{t_i\\}_i$ are arranged in increasing order w.r.t the indices $i = 0,1,...N - 1$, we can compute the desired pair $(x_{i-1},x_i)$ at the first in-stance where $sign(f_m (\\theta, x_{i-1})) \\cdot sign(f_m(\\theta,x_i)) == -1$ occurred. This can be efficiently implemented using the argmax() or argmin() operator. Thus, we have indeed iden-tified a pair $(x_p, x_n) \\triangleq (x_{i-1}, x_i)$."}, {"title": "A2.2. Disney BRDF Calculation", "content": "As demonstrated in lines 421 433, we calculate the sim-plified Disney BRDF as follows.\nGiven an intersection point $x_s$, sampled above, we derive the normal of xs as the normalized gradient of SDF MLPs $f_m (\\theta,\\cdot)$ as\n$n = \\frac{\\nabla_x f_m(\\theta,x_s)}{||\\nabla_x f_m(\\theta,x_s)||_2}$(17)\nAlso, we utilize the materials MLPs $f_c(\\theta',\\cdot)$ to com-pute the correspondent intrinsic materials as $f_c(\\theta', x_s) = [a, r, m, s] \\in [0,1]^6$. Next, we use the following expres-sions to calculate the terms of BRDF inspired by prior work [5].\nFor the diffuse refraction $k_a$ in Eqn. (9) in line 403, we compute $k_a = (1 \u2013 m) k_dk_a$ with\n$w_i \\approx 2(w_o \\cdot n)n \u2013 w_o$, (18)\n$h = \\frac{W_i + W_o}{||W_i + Woll_2}$, (19)\n$F_{D90} = 0.5 + 2(w_i\\cdot h)^2 r$, (20)\n$k_{\\perp} = 1 + (F_{D90} \u2212 1)(1 \u2212 w_i \\cdot n)^5$, (21)\n$k_{\\parallel} = 1 + (F_{D90} \u2212 1)(1 \u2212 w_o \\cdot n)^5$. (22)\nFor the Fresnel term Fo in Eqn. (14) in line 431, we com-pute it by\n$C_s = (1 - m)s + ma$, (23)\n$F_o = C + (1 \u2212 C_s)(1 \u2013 w_i\\cdot h)^5$. (24)\nFor the geometry term Go in Eqn. (14) in line 431, we compute it by\n$k = \\frac{(r + 1)^2}{8}$(25)\n$G_o = \\frac{w_i\\cdot n}{w_i\\cdot n(1-k) + k} \\cdot \\frac{w_o\\cdot n}{w_o\\cdot n(1 \u2212 k) + k}$.(26)"}]}