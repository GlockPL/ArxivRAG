{"title": "CATP-LLM: Empowering Large Language Models for Cost-Aware Tool Planning", "authors": ["Duo Wu", "Jinghe Wang", "Yuan Meng", "Yanning Zhang", "Le Sun", "Zhi Wang"], "abstract": "Utilizing large language models (LLMs) for tool planning has emerged as a promising avenue for developing general Al systems, where LLMs automatically schedule external tools (e.g. vision models) to tackle complex tasks based on task descriptions. To push this paradigm toward practical applications, it is crucial for LLMs to consider tool execution costs (e.g. execution time) for tool planning. Unfortunately, prior studies overlook the tool execution costs, leading to the generation of expensive plans of which the costs outweigh task performance. To fill this gap, we propose the Cost-Aware Tool Planning with LLMs (CATP-LLM) framework, which for the first time provides a coherent design to empower LLMs for cost-aware tool planning. Specifically, CATP-LLM incorporates a tool planning language to enhance the LLM to generate non-sequential plans of multiple branches for efficient concurrent tool execution and cost reduction. Moreover, it further designs a cost-aware offline reinforcement learning algorithm to fine-tune the LLM to optimize the performance-cost trade-off in tool planning. In lack of public cost-related datasets, we further present OpenCATP, the first platform for cost-aware planning evaluation. Experiments on OpenCATP show that CATP-LLM outperforms GPT-4 even when using Llama2-7B as its backbone, with the average improvement of 28.2%-30.2% higher plan performance and 24.7%-45.8% lower costs even on the challenging planning tasks. The codes of CATP-LLM and OpenCATP will be publicly available.", "sections": [{"title": "1. Introduction", "content": "The recent advancement in large language models (LLMs) [1, 11] have highlighted their extraordinary abil-ity in tool planning [9, 35, 47], where LLMs can schedule and execute external tools (e.g. vision models [22, 37] and python programs [12, 15]) to tackle complex tasks based on task descriptions. For instance, when confronted with the task \"How to generate the caption of a low-resolution im-age in German\", the LLM may devise the following tool plan: (1) use a super resolution tool to enlarge the image; (2) employ the image captioning model to generate caption; (3) execute a translator to convert the caption into German. Due to the flexibility of tool usage and composition, this paradigm has shown exceptional performance, especially in solving complex tasks including visual question answer-ing [40, 48], image editing [25, 44, 49] and mathematical reasoning [46, 51], paving the way for the development of more general Al systems [10, 12]. Furthermore, it offers significant practical prospects by effectively utilizing exist-ing tools and models, thereby reducing the need for devel-oping and training new models [37].\nDespite promising, using LLMs for tool planning still faces practical issues related to the plan execution costs. Specifically, the execution of tools will inevitably cause computation costs such as execution time and memory con-sumption. This makes it essential for the LLMs to carefully take the execution overhead of each tool into account when devising tool plans for specific tasks. Otherwise, they may generate expensive tool plans of which the execution costs outweigh task performance, ultimately hindering their prac-tical applications (e.g. Figure 13).\nHowever, empowering LLMs for cost-aware tool plan-ning poses the following challenges.\n\u2022 First, how to enable LLMs for non-sequential tool plan-ning? Non-sequential planning refers to the capability to create nonlinear plans that comprise multiple branches for concurrent tool execution. It not only facilitates more efficient parallel processing of tools to reduce ex-ecution costs (e.g. execution time), but also enhances the ability of LLMs to solve complicated tasks that re-quire complex tool plans [10], as exemplified in Fig-ure 14. Nevertheless, implementing non-sequential plan-ning is challenging. Recent insights have revealed that it can be difficult for LLMs to understand and generate nonlinear-structured contents through natural language prompts [4, 16, 32, 50], not to mention the complex tool dependencies that must be strictly followed during the generation of structured plans.\n\u2022 Second, how to enable LLMs to achieve the optimal performance-cost trade-off in tool planning? The key obstacle lies in the conflicts between plan performance and execution costs. Generally, simple plans often in-cur lower costs but may compromise performance, while complex plans can enhance performance but at the ex-pense of higher costs. Moreover, the costs of execut-ing tool plans can be affected by the input data associ-ated with the task, which further complicates the prob-lem. Consequently, striking a good balance between per-formance and costs in tool planning remains a significant challenge for LLMs, which requires a deep understand-ing of the performance-cost trade-off and the impacts of input data on execution costs.\n\u2022 Last but not least, in lack of public datasets that consider tool execution costs, how to evaluate the effectiveness of LLMs in cost-aware tool planning?\nPrior studies [10, 22, 28, 37] fail to achieve cost-aware tool planning as most of them only support sequential plan-ning and overlook the tool execution costs. To fill this re-search gap, in this paper, we propose the Cost-Aware Tool Planning with LLMs (CATP-LLM) framework, which for the first time provides a coherent design to efficiently enable LLMs for cost-aware tool planning. Specifically, we pro-pose the following techniques to systematically overcome the aforementioned challenges:\n\u2022 First, we design a tool planning language (TPL) to en-hance the non-sequential planning capability of LLMs. TPL transforms tools and their dependencies into learn-able tokens, which, like language tokens, can be trained to learn the optimal representations for LLMs to under-stand their semantic information. TPL then encodes any tool plan, including non-sequential ones, as a sequence of tool and dependency tokens, so that the LLMs can gener-ate diverse tool plans by simply predicting these tokens. Besides, it also introduces some special tokens to describe the structural information of tool plans to facilitate LLMs to generate tool plans of complex structures.\n\u2022 Based on the TPL, we then propose an efficient cost-aware offline reinforcement learning (CAORL) algorithm to address the second challenge. Specifically, CAORL introduces a context augmentation scheme for LLMs to capture the impacts of input data on the tool execution costs for effective cost-aware planning. It then fine-tunes LLMs based on the efficient offline RL algorithm [2, 6, 7] with a dedicated reward model to guide the LLM to min-imize plan execution costs while maximizing plan per-formance. Besides, a data generation method is also de-signed to generate the training data for fine-tuning.\n\u2022 To comprehensively assess the efficacy of LLMs in cost-aware tool planning, we establish a new platform Open-CATP. The key features of OpenCATP lie in three aspects: (1) it allows the measurement of plan costs; (2) it incorpo-rates a unified metric Quality of Plan (QoP) to quantita-tively evaluate the plan quality based on its performance and execution costs; (3) it introduces more complex tasks for the non-sequential planning evaluation (e.g. Figure 1).\nWe compare CATP-LLM with existing methods on OpenCATP. Results demonstrate that CATP-LLM outper-forms GPT-3.5 [29] and GPT-4 [1] with Llama2-7B [41] as the backbone LLM. To be specific, it achieves 1.15x-1.6x and 2.29x-9.13x higher QoP in the case of sequential plan-ning and non-sequential planning, respectively, with lower"}, {"title": "2. Related Work", "content": "Large language models (LLMs) [11, 41] have shown ex-ceptional performance in understanding task descriptions and deriving tool execution plans based on the provided toolkit to address the target tasks [8, 24, 26, 40]. There-fore, tool planning with LLMs has emerged as a promis-ing avenue toward developing general AI systems [10, 12]. In pursuit of this vision, numerous works have been pro-posed to enhance the tool planning abilities of LLMs, which can be categorized into two paradigms. The first paradigm leverages prompt engineering [5, 23, 34] to guide LLMs for tool scheduling, which has been recently extended by adding in-context demonstrations [12, 30, 40, 49], introduc-ing more tools [28, 44], or designing more dedicated plan-ning procedures [22, 26, 37]. Another line of work involves fine-tuning LLMs for tool planning through instruction-tuning [13, 31, 35, 47]. Recent advancements have also in-troduced reinforcement learning techniques [10, 33] to fur-ther augment LLMs for tool planning.\nExisting methods exhibit limited support for non-sequential planning, restricting the full potential of LLMs in cost-aware tool planning. For in-stance, prior work [22] combines formal language with nat-ural language to improve the planning ability of the LLM, but it necessitates domain experts to handcraft specialized prompts, which can be labor-intensive. Although meth-ods proposed in [26, 37] integrate non-sequential plans in JSON-like format into prompts as demonstrations, recent insights [16, 32, 39, 50] suggest that LLMs may not ef-fectively understand and generate nonlinear-structured in-formation through language instructions, thereby hindering these methods to function effectively. In contrast, CATP-LLM incorporates a tool planning language to empower LLMs for non-sequential planning to effectively generate tool plans of various structures. Moreover, while existing methods overlook the execution costs of tool plans, CATP-LLM introduces an efficient algorithm to fine-tune LLMs for cost optimization."}, {"title": "2.1. Tool Planning with LLMS", "content": "Reinforcement learning (RL) [33, 43] has been proposed to enhance LLMs for tool planning, which enables LLMs to understand their actions and adjust behavior accordingly by replaying plan execution feedback to LLMs. However, existing methods [10, 33] necessitate completing plan ex-ecution before replaying feedback to LLMs, resulting in coarse-grained planning. In contrast, CATP-LLM intro-duces an efficient RL algorithm tailored for cost-aware tool planning. It allows LLMs to access the plan quality each time a decision is made (e.g. adding a new tool to the plan). Such fine-grained perception empowers LLMs for more ef-fective cost-aware planning (e.g. selecting a cheaper tool if the current plan costs are already excessive)."}, {"title": "2.2. Reinforcement Learning for Tool Planning", "content": "With the advancement of tool planning with LLMs, numer-ous datasets have been established to access LLMs' capabil-ity in scheduling tools for task solving [21, 27, 38, 42, 54]. From this line of work, OpenAGI [10] is the most rele-vant to this paper, which offers a quantitative scheme to ob-jectively evaluate the performance of LLM-generated tool plans in various tasks. However, existing works, including OpenAGI, overlook the tool execution costs, the crucial as-pect that significantly affects the efficacy of tool planning in real-world scenarios. To bridge this gap, this work devel-ops a new platform OpenCATP that for the first time inte-grates tool cost information and introduces a comprehensive scheme for the evaluation of cost-aware tool planning."}, {"title": "2.3. Datasets for Tool Planning", "content": "We propose CATP-LLM, a general model-agnostic frame-work that enhances the capability of LLMs in cost-aware tool planning. As shown in Figure 2, CATP-LLM consists of two core design building blocks:\n\u2022 Tool planning language (TPL). TPL encodes tools and their dependencies into trainable tokens, and represents"}, {"title": "3. CATP-LLM Design", "content": "Given a set of tools $T = \\{t_1, t_2,\u00b7\u00b7\u00b7\\}$ and their dependen-cies $D = \\{d_1, d_2, \u00b7\u00b7\u00b7 \\}$, a tool plan can be expressed as a di-rected acyclic graph (DAG) $p = (\\{t_i\\}_{i=1}, \\{d_j\\}_{j=1})$, where node $t_i \\in T$ indicates a specific tool and edge $d_j \\in D$ denotes a specific dependency (e.g. a resource dependency where one tool accepts the outputs of another tool as in-puts). To enable the LLM to generate tool plans of various topological structures, the proposed TPL comprises the fol-lowing key designs.\nAnything as token. Inspired by the work in [13], we formulate tools and dependencies as tokens\u00b9 denoted as"}, {"title": "3.1. Tool Planning Language", "content": "$[T] = \\{[t_1], [t_2], \u00b7\u00b7\u00b7 \\}$ and $(D) = \\{(d_1), (d_2), \u00b7\u00b7\u00b7 \\}$, respec-tively. Similar to regular word tokens, each tool or depen-dency token is parameterized by an embedding vector that can be trained to learn the optimal representations for the LLM to understand its semantic information. The key ben-efit of this approach is that it can accommodate new tools or dependencies by simply adding new tokens, allowing the LLM to support massive tools and dependencies.\nPlan as sequence. Due to the complexity of DAG struc-tures, it can be challenging and error-prone for the LLM to directly generate tool plans in the DAG form. To tackle this issue, we explicitly transform a DAG tool plan into a sequence of tool and dependency tokens to cater to the se-quence prediction nature of the LLM:\n$p = \\{[SoP],\u2026\u2026, [t_i], (SoD), (d_1), ..., (d_n), (EoD),..., [EoP]\\},$        (1)\nwhere $[t_i] \\in [T]$ denotes the i-th tool token, and $(d_i) \\in (D)$ represents the dependency between the i-th tool and the pre-vious tool. In our practical implementation, we introduce an additional token $(task)$ to indicate a special dependency that a tool accepts the data provided by the task (e.g. an image) as inputs. In particular, $[SoP], [EoP]$ and $(SoD)$, $(EoP)$ are special structure tokens indicating the start, end of the plan and start, end of the dependencies associated with a tool, respectively. These structure tokens are used to facilitate the LLM to effectively perceive the plan structural information. Through such a flexible representation, our TPL can represent tool plans of diverse structures, enhanc-ing the non-sequential planning ability of the LLM. Figure 3 exemplifies the use of TPL for encoding various tool plans."}, {"title": "3.2. Cost-Aware Offline Reinforcement Learning", "content": "With the proposed TPL, the LLM is capable to devise var-ious tool plans by simply predicting tool and dependency tokens. To optimize the plan generation process, CATP-LLM then fine-tunes the LLM through the efficient CAORL mechanism. Specifically, CAORL includes: i) a context augmentation scheme to effectively integrate tool cost in-formation into the LLM's input context; ii) an offline RL-based fine-tuning algorithm that guides the LLM to achieve the optimal performance-cost trade-off in tool planning."}, {"title": "3.2.1. Context Augmentation", "content": "Figure 4 illustrates the prompt template designed to pro-vide the LLM with rich information for effective planning. We integrate the embedding features of each tool into the prompt for the LLM to understand their functionalities. Moreover, it is also essential to incorporate tool cost in-formation into the prompt for effective cost-aware plan-ning. However, since tool costs can vary with different sizes of task inputs, merely writing the cost information in the prompt fails to establish the connection between input data sizes and tool costs. To address this issue, we further pro-pose a context augmentation scheme that effectively trans-forms the tool cost information into a set of cost-aware em-bedding features, as shown in Figure 4.\nThe details of the context augmentation scheme are elab-orated as follows. First, we categorize the input sizes into $\\{1,\u2026, k\\}$ levels as two very close sizes will not make a big difference (e.g. resolution 480\u00d7520 vs. 490\u00d7510, batch size 32 vs. 36). Subsequently, each tool is assigned with a cost attribute vector $c(t_i) = (c_1,\u00b7\u00b7\u00b7, c_k)$, indicating its exe-cution costs at different levels of input sizes. Afterward, we employ a linear layer to extract features from the tool cost attributes. We then encode the importance of cost features by adding the following importance vector:\n$v_{i-1} \\coloneqq cos\\left(\\frac{\\pi(i - 1)}{2k}\\right) \\in \\{1,..., k\\},$       (2)\nwhere $I$ denotes the level of the current task inputs. This importance vector allows the importance of cost features to"}, {"title": "3.2.2. Learning Through Offline RL", "content": "Despite augmenting the LLM with rich input context, achieving cost-aware tool planning remains challenging due to the inherent trade-off between plan performance and costs. Hence, it is imperative to fine-tune the LLM to ac-quire the domain knowledge for cost-aware planning. No-tably, we observe that the selection of a tool can have cas-cading effects on the subsequent selection. For instance, se-lecting a powerful yet costly tool may lead to avoiding the selection of expensive tools later to prevent the costs from becoming unaffordable. This suggests that the cost-aware tool planning is essentially a sequential decision problem, a domain where reinforcement learning (RL) [43] excels. Thus, we design the fine-tuning algorithm based on RL, as illustrated in Figure 5 and detailed below.\nState. At timestep i, the LLM will receive a state $s_i$ defined as a token sequence of the current tool plan:\n$s_i = \\{t_{k1},\u2026,t_{ki}\\},$     (3)\nwhere $t_{ki}$ denotes a tool, dependency or structure token.\nAction. Based on state $s_i$ and input context, the LLM takes an action $a_i$ representing a tool token or dependency token. We design two output heads for the LLM to generate ac-tions: tool head for tool tokens and dependency head for de-pendency tokens. In particular, we extend the output space of tool head and dependency head with $[EoP]$ and $(EOD)$ tokens, respectively. During plan generation, if the last to-ken in state $s_i$ is $[SoP]$ or $(EoD)$, the LLM will use the tool head to predict the next tool token. This token will be added to the plan along with the $(SoD)$ token, which forms a new state. Afterward, the LLM will switch to the dependency head to predict dependency tokens. It will switch back to the tool prediction head until the $(EoD)$ token is predicted. The plan generation process will terminate as long as the LLM predicts the $[EoP]$ token with the tool head."}, {"title": "4. OpenCATP Platform", "content": "In this section, we describe the OpenCATP platform de-signed to evaluate the efficacy of LLMs in cost-aware tool planning. OpenCATP is built upon OpenAGI [10], which offers a rich toolkit (e.g. image deblurring), diverse tasks for evaluation (e.g. image editing), as well as objective met-rics for performance measurement (e.g. ViT Score [10] and BERT Score [53]). This allows us to focus on cost measure-ment and evaluation in building OpenCATP.\nThe key feature of OpenCATP is that it defines a cost model to comprehensively reflect the costs of tool plans, inspired by the Function as a Service (FaaS) [20, 36] plat-forms. To be specific, FaaS providers have developed a ma-ture pricing model in order to charge for the service of run-ning a function based on the execution time and resource consumption. Hence, we take cues from the FaaS platforms and design a pricing model in OpenCATP to quantify the overall costs of a tool plan, which jointly considers exe-cution time and instant/constant memory consumption (de-tailed in Appendix \u00a7B). Based on this model, we introduce a unified metric Quality of Plan (QoP) in OpenCATP to quan-tify the quality of a tool plan:\n$QoP = \\frac{P_{task}(p)}{(1 + a)C_{price}(p)},$    (5)\nwhere $P_{task}(p)$ quantifies the task performance of a plan with the OpenAGI APIs, $C_{price}(p)$ calculates the execu-tion prices of a tool plan reflecting its overall costs, and $\u03b1 \\in (0,1)$ is the weight parameter. We employ min-max normalization on these two metrics when calculating QoP to make sure their values fall in the same scale. The min/max values are derived from empirical measurement.\nFinally, we introduce more challenging tasks that require non-sequential plans for task solving in OpenCATP. Follow-ing the back-instruct method in [38], we construct diverse non-sequential tasks by following steps: (1) create a tool graph based on the toolkit and dependencies between tools; (2) extract a sub-graph representing a non-sequential plan; (3) prompt the GPT-4 to generate the descriptions of a task that can be solved by this plan."}, {"title": "5. Experiments", "content": "We use the proposed OpenCATP as the evaluation platform, which features diverse compositional tasks that cannot be solved with a single tool. In particular, we evalu-ate the efficacy of CATP-LLM in two scenarios: sequential"}, {"title": "5.1. Setup", "content": "where $p_i = s_i \\cup \\{a_i\\}$ denotes the plan updated with action $a_i$, $P(p_i)$ calculates the performance scores of $p_i$ on the tar-get task (e.g. BERT Score [53] for image captioning), $C(p_i)$ measures the execution costs of $p_i$ (e.g. execution time), and $\u03b1 \\in (0, 1)$ serves as a weight parameter to balance the im-portance of two metrics (a = 0.5 by default). In contrast to prior studies [10, 33] where the LLM only receives feed-back upon completing a plan, we allow the execution of an incomplete plan during the fine-tuning stage to collect its execution costs\u00b2 as the feedback replayed to the LLM. This guides the LLM toward minimizing execution costs while maximizing performance scores in plan generation.\nLearning algorithm. We fine-tune the LLM for plan gen-eration based on the offline RL algorithm decision trans-former (DT) [6] because of its efficiency. DT reformulates RL as a sequence modeling problem, which seamlessly aligns with the sequence modeling nature of LLM and the proposed TPL. As illustrated in Figure 5, in this algorithm, the LLM models the distribution of actions conditioned on the specific states and returns, where return $R_i = \\sum_{j=i}^i r_j$ represents the cumulative rewards expected to receive from state $s_i$. After training, the LLM can generate actions to achieve the desired return [6, 45]. More details about the algorithm can be found in Appendix \u00a7A.\nThe primary challenge of implementing such a learning algorithm lies in the collection of training datasets. In lack of public tool plan datasets for fine-tuning, we propose a dataset generation method to collect the necessary training data. This approach involves sampling tasks from a task pool and instructing the LLM (specifically GPT-4 [1] in"}, {"title": "5.2. Main Results", "content": "The proposed fine-tuning algorithm CAORL plays an significant role in enabling CATP-LLM for cost-aware tool planning. Hence, we explore the contributions of various important techniques in CAORL to CATP-LLM, including context augmentation, intermediate feedback signals, and adaptive masking. As shown in Table 3, the context augmentation"}, {"title": "5.3. Ablation Study", "content": "In this paper, we propose the first cost-aware tool planning framework CATP-LLM. It features a tool planning language to enhance the LLM to create non-sequential plans of multi-ple branches for efficient concurrent execution. Building on this language, it further introduces a cost-aware offline rein-forcement learning algorithm to fine-tune the LLM to opti-"}, {"title": "6. Conclusion", "content": "its plan generation efficiency.\nReward with intermediate feedback. Upon taking action $a_i$, the LLM will receive a reward $r_i$ formally defined as:\n$r_i = \\begin{cases}\\frac{\\alpha P(p_i) \u2013 (1 \u2013 \\alpha)C(p_i)}{\\alpha P(p_i) + (1 \u2013 \\alpha)C(p_i)}, & \\text{if } a_i \\neq [EOP],\\\\-\\frac{(1 \u2013 \\alpha)C(p_i)}{\\alpha P(p_i) + (1 \u2013 \\alpha)C(p_i)}, & \\text{if } a_i = [EOP],\\end{cases}$   (4)\nand intermediate feedback are essential for reducing execu-tion costs, while adaptive masking ensures plan validness and improves generation effectiveness.\nGeneralization to various QoP definitions. By default, we set $a$ in QoP to 0.5, indicating equal importance for plan performance and costs. In fact, CATP-LLM is a general framework that can adapt to different QoP definitions based on practical applications. We verify its generalizability by varying $a$ in QoP and comparing its performance to GPT-4-powered methods. As shown in Figure 6, CATP-LLM consistently yields the highest QoP scores across all cases.\nImpacts of LLM parameter sizes. Finally, we explore the impacts of LLM sizes on CATP-LLM's efficacy. We use TinyLlama-1B [52], Qwen-3B [17], and Llama2-7B [41] as backbone LLMs. As shown in Figure 7, Qwen-3B demon-strates comparable performance to Llama2-7B in sequen-tial planning, but Llama2-7B significantly outperforms oth-ers in complex non-sequential tasks. This suggests that for relatively simple sequential tasks, smaller LLMs like Qwen-3B are sufficient for planning, while larger LLMs like Llama2-7B are recommended for more complex tasks. Due to GPU resource limits, we will leave the exploration of larger LLMs for future work."}]}