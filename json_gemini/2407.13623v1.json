{"title": "Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies", "authors": ["Chaofan Tao", "Qian Liu", "Longxu Dou", "Niklas Muennighoff", "Zhongwei Wan", "Ping Luo", "Min Lin", "Ngai Wong"], "abstract": "Research on scaling large language models (LLMs) has primarily focused on model parameters and training data size, overlooking the role of vocabulary size. We investigate how vocabulary size impacts LLM scaling laws by training models ranging from 33M to 3B parameters on up to 500B characters with various vocabulary configurations. We propose three complementary approaches for predicting the compute-optimal vocabulary size: IsoFLOPs analysis, derivative estimation, and parametric fit of the loss function. Our approaches converge on the same result that the optimal vocabulary size depends on the available compute budget and that larger models deserve larger vocabularies. However, most LLMs use too small vocabulary sizes. For example, we predict that the optimal vocabulary size of Llama2-70B should have been at least 216K, 7 times larger than its vocabulary of 32K. We validate our predictions empirically by training models with 3B parameters across different FLOPs budgets. Adopting our predicted optimal vocabulary size consistently improves downstream performance over commonly used vocabulary sizes. By increasing the vocabulary size from the conventional 32K to 43K, we improve performance on ARC-Challenge from 29.1 to 32.0 with the same 2.3e21 FLOPs. Our work emphasizes the necessity of jointly considering model parameters and vocabulary size for efficient scaling.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) achieve remarkable performance by pre-training on vast text corpora using massive computational resources [43]. Extensive prior work on LLMs has focused on deriving so-called scaling laws: a set of empirical formulas to predict how model performance scales, mainly as computing floating-point operations (FLOPs), model parameters, and quantity of training data change [28, 24, 60, 2, 40, 54]. These works show that power-law fits can effectively predict language modeling loss and by extension downstream performance [21, 51]. However, these scaling laws usually disregard the impact of the vocabulary size. For example, in Kaplan et al. [28] only non-vocabulary parameters are considered in their predictive formula. This negligence has resulted in substantial variability in the vocabulary size of current LLMs. For instance, Llama2-7B employs a vocabulary size of 32K [64], while Gemma-7B [61] adopts a much larger vocabulary size of 256K despite both having a similar number of total parameters. This variability in vocabulary sizes across LLMs raises the research question: What is the compute-optimal vocabulary size for an LLM?\nThe vocabulary size affects performance non-trivially. Intuitively, the optimal vocabulary size should neither be excessively large nor small. A larger vocabulary size improves tokenization fertility, i.e., splitting sentences into fewer tokens, thereby improving the tokenization efficiency. Additionally, a larger vocabulary enhances the representational capacity of the model, enabling it to capture a wider range of concepts and nuances in the corpus. However, the risk of under-fitting representations for rare tokens increases with larger vocabulary sizes, especially in the data-constrained regime [40, 66]. Thus, the optimal vocabulary size needs to be determined by taking the training data and the amount of non-vocabulary parameters into account.\nIn this paper, we show that the effect of vocabulary on scaling laws has been underestimated, and we quantify the effect to derive a prediction for the optimal vocabulary size. We first introduce a normalized loss formulation to ensure a fair comparison across models with varying vocabulary sizes. Utilizing the normalized loss function, we analyze and discuss the underlying rationale behind the existence of an optimal vocabulary size, which depends on the available computational budget.\nTo predict the optimal vocabulary size given a compute budget, we propose three approaches. Approach 1 (Estimating power laws via IsoFLOPs): We pre-train models with non-vocabulary parameters ranging from 33M to 1.13B, with groups of models that share the same FLOPs (\u201cIsoFLOPs\u201d) but varying vocabulary configurations. Then, we fit power laws relating FLOPs to non-vocabulary parameters, vocabulary parameters, and training data, respectively. Our analysis reveals that the optimal vocabulary parameters exhibit a power-law growth with respect to the computational budget, however, at a slower rate than non-vocabulary parameters, as shown in Figure 1. Approach 2 (Derivative-based Estimation): We introduce a derivative-based method that estimates the optimal"}, {"title": "2 Preliminary", "content": "In this section, we first present a general formulation of a commonly used scaling law, and then demonstrate how to modify it to incorporate the vocabulary."}, {"title": "2.1 Scaling law", "content": "Scaling laws consider a computational budget, $C$, which is measured in FLOPs. The goal is to optimally allocate the compute budget to model parameters $N$ and the number of training tokens $D$ [28, 6, 24, 40]. It can be formulated as:\n$(N_{opt}, D_{opt}) = \\arg \\min_{N,D} L(N, D) \\text{ s.t. } FLOPs(N, D) = C,$\nFollowing Radford et al. [47], the loss function is typically the language modeling loss, which can be written as:\n$L = \\frac{1}{T} \\sum_{i=1}^{T} -\\log p(w_i | w_{1:i-1}, V),$\nwhere $p(w_i | w_{1:i-1}, V)$ is the output probability of word $w_i$ given the context $w_{1:i-1}$ and the tokenizer with vocabulary size $V$. Generally, the lower $L$ indicates better performance of the language model. However, due to its dependency on $V$, $L$ cannot be used to compare language models with different vocabulary sizes. Thus, we propose an adaptation later in \u00a72.2. Fitting scaling laws generally requires various models trained for different configurations [21]. A common approach is to select several compute budgets and train models with varying $N$ and $D$ for each budget to find the best one, i.e. the one with the lowest loss (\u201cIsoFLOPs\u201d) [24]. Using fitting techniques we can then estimate a function that maps from the compute budget to the optimal allocation to $N$ and $D$."}, {"title": "2.2 Scaling law with vocabulary", "content": "As prior work generally assumes the vocabulary size to be fixed, we cannot adopt the attributes in their scaling laws and their evaluation metric directly. Thus, we detail several considerations that allow us to investigate vocabulary scaling laws.\nScaling laws commonly deal with the attributes, model parameters ($N$) and number of training tokens ($D$) [24, 40]. We adapt them for our analysis in the context of vocabulary size. (1) We break down the total model parameters ($N$) into non-vocabulary ($N_{nv}$) and vocabulary parameters ($N_v$). To understand the importance of vocabulary parameters, we isolate them from other model parameters, where $N = N_{nv} + N_v$. We use $N_v = Vd$ to represent both the vocabulary parameters in the output layer. Notably, to change $N_v$ we only vary the vocabulary size $V$ and take the embedding dimension $d$ as given based on $N_{nv}$ empirically, see \u00a7A.5.2 for details. This is based on the observation by Kaplan et al. [28] that the performance of models with varying depth-to-width ratios converges to a single trend. (2) We measure data not in tokens ($D$) but in training characters ($H$). The number of tokens depends on the vocabulary of the tokenizer, thus we need a vocabulary-independent measurement of data. By studying training characters, we can better see how the data volume affects the performance regardless of different vocabulary sizes.\nAs detailed above we measure training data in training characters ($H$). Nonetheless, to connect our findings with existing studies on scaling laws [24, 40], we need to be able to map from $H$ to $D$. This mapping is the tokenizer's compression ratio which can be computed via $D/H$. The more tokens the tokenizer needs to represent $H$, the larger $D$, and thus it compresses less. We develop a simple function $f(V)$ to estimate this ratio solely from the chosen vocabulary size, $V$. Specifically, we find that a quadratic function on the logarithmic value of $V$ achieves accurate predictions:\n$f(V) = a \\log_2(V)^2 + b \\log(V) + c$\nBy fitting several tokenizers with $V$ ranging from 1K to 1024K, we obtain $a = 0.0064, b = -0.1581$ and $c = 1.2047$. We find that our function accurately predicts the compression ratio with a low relative mean square error (RMSE) and a high coefficient of determination (R2). In \u00a7A.7, we visualize fitting results and show that our approximation works with different tokenizers and is robust to different $V$. For all our main experiments, we use the BPE algorithm for tokenization [55].\nTo fairly assess models that vary in $V$, the commonly used language model loss in Equation 2 is inappropriate. Models trained with larger $V$ naturally have a higher loss, as there are more possibilities in the vocabulary to predict. However, this does not mean that the model is worse. Thus, we need to normalize the loss with respect to the vocabulary size. We reformulate the unigram-normalized metric [50] as a loss function. Suppose we have a $T$-length sequence $w_{1:T}$, we design the unigram-normalized language model loss as:\n$L_u = \\frac{1}{T} \\sum_{i=1}^{T} \\log \\frac{p(w_i | w_{1:i-1}, V)}{p(w_i | V)},$\nwhere $p(w_i | V)$ is the frequency of word $w_i$ in the tokenized corpus, given the tokenizer with vocabulary size $V$. The loss indicates the improvement in probability that a context-aware language model offers over a unigram model without context, allowing us to assess the language model's efficacy. Based on theory from prior work [50], the normalized loss $L_u$ remains consistent for a given model with a fixed non-vocabulary component across different vocabulary sizes. The difference of $L_u$ comes from the ability of the language model itself. Compared with $L$, the value of $L_u$ is much smaller and can be negative as $L_u$ adds a negative term $\\frac{1}{T} \\sum_{i=1}^{T} \\log p(w_i | V)$. One may also employ the average bits per character (BPC), a common metric for text compression [25], as the"}, {"title": "3 Analysis: Why the optimal vocabulary size is bounded by compute", "content": "In this section, we present analyses to explain why the optimal vocabulary size is constrained by the computational budget."}, {"title": "3.1 Analysis 1: The perspective of fixed normalized loss", "content": "According to Kaplan et al. [28], the FLOPs ($C$) of a Transformer-based language model can be estimated as $C \u2248 6ND$, which can be re-written as:\n$C \u2248 6ND \u2248 6(N_{nv} + Vd) H f(V),$\nwhere $N = N_{nv} + N_v$ and $D = Hf(V)$ based on \u00a72.2. The reasons why model performance first increases and then decreases as the vocabulary size grows are: (1) At small $V$, increasing the vocabulary size easily improves tokenization fertility from $f(V)$. Subsequently, more characters can be learned from the model with a fixed number of tokens, thereby improving model performance. (2) At very large $V$, the gain from tokenization fertility decreases, while the parameters from expanding the vocabulary cannot be adequately trained with limited data, which leads to a decline in model performance. We present an expanded derivation in \u00a7A.1, and show how the corresponding FLOPs change with the vocabulary size in Figure 3 (left)."}, {"title": "3.2 Analysis 2: The perspective of fixed FLOP budget", "content": "Given a fixed FLOPs budget, we isolate the FLOPs and investigate how the vocabulary influences the loss. In practice, we set several fixed FLOP budgets. For each budget, we adopt a group of models with similar total parameters and vary vocabulary sizes from 4K to 96K. In Figure 3 (right) we plot the relationship between the loss w.r.t. the vocabulary size. It reveals that the vocabulary corresponding to the lowest point on the loss curve increases as the FLOPs budget increases. This suggests that with more computational resources, LLMs can effectively harness larger vocabularies to reduce loss. However, merely expanding the vocabulary does not always lower the loss. For a fixed FLOP budget, the loss initially decreases with the increase in vocabulary and then starts to rise, indicating that an optimal point exists for the vocabulary. This suggests a trade-off between model complexity and computational constraints, where an overly large vocabulary cannot be utilized efficiently, leading to sub-optimal model performance."}, {"title": "3.3 Analysis 3: The perspective of parameter growing", "content": "Traditionally, scaling up model parameters in language models has been approached in two ways: increasing depth (i.e., the number of layers) or width (i.e., the hidden size). While extensive research has been conducted on these methods, current empirical practices often involve expanding both simultaneously [60]. This approach, however, may overlook crucial distinctions in how different parameters benefit from these expansions.\nNon-vocabulary parameters can benefit from increases in both depth and width, allowing for more complex hierarchical representations and broader feature capture. In contrast, vocabulary parameters, associated with word embeddings and language model heads, are generally confined to a single layer, limiting their ability to benefit from increases in the model depth. Their primary avenue for expansion is through increasing the width. This disparity in growth potential between non-vocabulary and vocabulary parameters suggests that to maintain a balanced growth rate, it may be necessary to expand the vocabulary size along with the depth. This would allow the vocabulary parameter part to keep pace with the growth of non-vocabulary parameters."}, {"title": "4 Estimating the optimal vocabulary size", "content": "In this section, we describe three complementary approaches to estimate the optimal vocabulary size."}, {"title": "4.1 Approach 1: Estimating power laws via IsoFLOPs", "content": "We define 6 groups of models with $N_{nv}$ ranging from 33M to 1.13B. Within each group, we solely vary the vocabulary size $V$ from 4K to 96K, and evaluate different models under the same FLOPs budget. We evaluate the normalized loss $L_u$ on a held-out validation dataset. This approach allows us to directly answer the question: For a given FLOPs budget, what is the optimal allocation to non-vocabulary parameters, vocabulary parameters, and training data?\nGiven a certain N, the embedding dimension $d$ is fixed, thus $N_v$ increases as $V$ increases. For all experiments, we uniformly sample the training data from different domains in the SlimPajama dataset [57]. All other hyperparameters are fixed with more details in \u00a7A.5.\nWe select data points with the minimum $L_u$ for each FLOP budget, with all runs visualized in Figure 4. These points are the compute-optimal allocation to $(N_{nv}, N_v, H)$. Following Kaplan et al. [28] and Hoffmann et al. [24], we hypothesize that the optimal vocabulary parameters $N_v$ meet a power law w.r.t. the FLOPs $C$, just like the non-vocabulary parameters and the amount of training data. Specifically, $N_{nv} = k_1 C^{\\alpha_1}, N_v = k_2 C^{\\alpha_2}$ and $H = k_3 C^{\\alpha_3}$. As model size and training data should be scaled equally for compute-optimal training [24], we set $\\alpha_1 = \\alpha_3$. As our new attribute $V$ significantly increases the number of possible experimental configurations, we employ interpolation"}, {"title": "4.2 Approach 2: Derivative-based fast estimation", "content": "We propose an alternative approach leveraging insights from the estimation of the FLOPs itself. Prior work [24, 28] usually considers a fixed compute budget in FLOPs and then aims to minimize loss by finding the optimal allocation to model parameters $N$ and training tokens $D$. Here we flip this recipe on its head following recent work [53]. We aim to find the minimum FLOPs to achieve a certain loss $L_u(N_{nv}, V, H) = l$ through optimal allocation of the vocabulary size $V$:\n$V = \\arg \\min_{V|L_u(N_{nv}, V,H)=l} C(N_{nv}, N_v, H)$.\nBy computing the minimum point of FLOPs $C$ with respect to $V$ via derivative:\n$\\frac{\\partial C}{\\partial V} = \\frac{6H (N_{nv} + Vd)}{V} [2a \\log(V) + b] + [a (\\log(V))^2 + b \\log(V) + c] d,$\nwe can estimate the optimal $V$ under the assumption that it can achieve a certain loss $L_u(N_{nv}, V, H) = l$. The parameters $a, b$ and $c$ can be easily obtained from building $f(V)$ (\u00a72.2). In theory, as long as the non-vocabulary parameters $N_{nv}$ are provided, $V$ can be numerically searched via the solution of $\\frac{\\partial C}{\\partial V} = 0$. More details are in \u00a7A.1.\nNote that the optimal vocabulary size should be determined primarily by the normalized loss $L_u$, rather than by the compute budget FLOPs. However, when the compute allocation is near optimal, the loss exhibits a power-law relationship with respect to the FLOPs budget, as described by the scaling law [28]. This relationship allows us to use FLOPs with compute-optimal allocation as a reliable proxy for observing the scaling behavior of the optimal vocabulary parameters. In practise,"}, {"title": "4.3 Approach 3: Parametric fit of loss formula", "content": "Finally, we directly predict the loss given the non-vocabulary parameter, vocabulary parameter and the amount of training characters. Then, the optimal vocabulary configuration can be predicted by finding the minimum point of loss with respect to the vocabulary. Following a classical risk decomposition used in Hoffmann et al. [24], we design the vocabulary-dependent loss formula as:\n$L_u = -E + \\frac{A_1}{N_{nv}^{\\alpha_1}} + \\frac{A_2}{N_v^{\\alpha_2}} + \\frac{B}{D^{\\beta}},$\nwhere $D = Hf(V)$. The first term captures the normalized loss for an ideal generative process on the data distribution. The subsequent terms reflect the effect of the non-vocabulary parameters, vocabulary parameters, and the number of training data on the loss, respectively. $E, A_1, A_2, B, \\alpha_1, \\alpha_2, \\beta$ are learned parameters.\nWe use the points $(N_{nv}, N_v, H)$ collected for experiments in \u00a74.1. Note that we do not only consider the points with the lowest loss for each FLOP budget as we want to predict loss for any combination of $(N_{nv}, N_v, H)$. We add the constraint $\\alpha_1 = \\beta$ following Muennighoff et al. [40]. We also filter out points with very small FLOPs following Hoffmann et al. [24]. Fitting yields $A_1 = 1.831, A_2 = 0.196, B = 2.124, E = 5.533, \\alpha_1 = \\beta = 0.447, \\alpha_2 = 0.671$. The detailed fitting process is written in \u00a7A.5.4.\nAfter fitting the parameters in Equation 8, the optimal vocabulary size can be obtained by finding the lowest loss w.r.t the vocabulary size, with a constraint of FLOPs budget. For example, given $N_{nv}$ and FLOPs budget $C$, by replacing $[Hf(V)]$ with $C/(6(N_{nv} + N_v))$ and finding the solution of $\\frac{\\partial L_u}{\\partial V} = 0$ via numerical search, we can get the prediction. The details of $\\frac{\\partial L_u}{\\partial V}$ is written in \u00a7A.2. Note that all of the proposed approaches can be used in optimally allocating $(N_{nv}, N_v, H)$ altogether, while Approach 3 is more flexible in predicting the locally optimal $N_v$ when $(N_{nv}, H)$ are not following the Chinchilla's law [24], i.e. equally-scaled law. The reason is that the loss formula in Approach 3 does not only considers the combinations $(N_{nv}, N_v, H)$ which reach the optimal given a certain training budget. By fixing $N_{nv}$ and varying $C$ in Approach 3, we can predict the locally optimal vocabulary size with different amount of training characters. This property makes Approach 3 more valuable, since modern LLMs [64, 61, 3, 4, 8] usually leverage overly sufficient training data to build powerful models with relatively low inference costs."}, {"title": "5 Discussion", "content": "Predicting allocations for larger models Table 1 reports the predicted optimal vocabulary parameters and sizes based on the proposed three approaches, where the amount of training data is optimally allocated, i.e. equally scaled with the non-vocabulary parameters [24]. As shown in Figure 1, the predictions from all proposed approaches align closely. $N_{nv}$ should be scaled faster than $N_v$. Notably, mainstream LLMs typically assign fewer parameters to vocabulary than what is optimal. However, the community is starting to shift to larger vocabularies, such as with Llama3 [37] having a 128K vocabulary size up from 32K of Llama2 [64]. However, scaling data is still the most critical part, and solving data scarcity issues should be a focus of future work [66].\nTo empirically verify our prediction, we train models with $N_{nv} = 2.87B$ under a compute-optimal training FLOPs budget and evaluate them using lighteval 2. For the baseline model we use the common vocabulary size of $V = 32K. The other model uses $V_{opt}$ as predicted by Approach 3."}, {"title": "6 Related work", "content": "The Transformer [65] has proven to be a very scalable architecture with consistent performance gains which has led to a series of large language models (LLMs) [12, 15, 48, 43, 18, 27, 49, 64, 67, 37, 9, 4, 34, 23, 58, 61, 8, 35, 30, 79]. Through their training to predict subsequent tokens in a sequence, these models acquire a deep understanding of language enabling them to perform a variety of language tasks directly after pre-training. Their capabilities include code generation [31, 3, 39, 78, 77], mathematical reasoning [69, 5], question answering [44, 41] among others. In our work, we pre-train large language models from scratch on English corpora and focus on their loss during training and downstream performance on common tasks after training.\nVocabulary in language models The vocabulary of a language model influences its performance significantly [59, 68, 71]. A larger vocabulary size helps cover more words thus reducing the likelihood of out-of-vocabulary (OOV) cases [19]. Takahashi and Tanaka-Ishii [59] find that larger vocabularies are better at capturing the true statistical distribution of language. Similarly, expanding vocabulary in multilingual models [68] improves performance, especially for low-resource languages. However, large vocabularies [29] increase the computational overhead during both training and generation phases. Liao et al. [32] show that low-frequency words have few instances in the training data, leading to insufficient information to train robust representations if the vocabulary is too large. To this end, our work fills an under-explored gap: How to optimally allocate the vocabulary size?\nRecent work has explored byte-level language models [74, 70], which offer advantages in decoding efficiency and noise robustness compared to token-level models. However, typically limited to parameters under 1B, these models have not been effectively scaled up. Our scaling laws suggest that the limited vocabulary (i.e., 256 in byte-level language models) may constrain their performance, especially for larger models. The insight provides a potential explanation for the challenges in scaling byte-level models and implies that successful scaling of language models may require proportional increases in vocabulary size."}, {"title": "7 Conclusion", "content": "We investigate the impact of the vocabulary size when scaling language models. We analyze and verify that there exists an optimal vocabulary size for a given FLOPs budget. Subsequently, we"}, {"title": "A.1 The derivation of FLOPs w.r.t the vocabulary size for the Approach 2", "content": "Here we provide the detailed process of how we compute the extreme point of FLOPs C with respect to V. From Kaplan et al. [28], we know that:\n$C \\approx 6ND \\approx 6(N_{nv} + Vd) Hf(V)$.\nWe then compute the derivative as follows:\n$\\frac{\\partial C}{\\partial V} = \\frac{\\partial}{\\partial V} [6(N_{nv} + dV)H (f(V))]$\n$= \\frac{\\partial}{\\partial V} [6(N_{nv} +dV) H (a(\\log(V))^2 + b\\log(V) + c)]$\n$= 6H \\left[ (N_{nv} +dV) \\frac{d}{dV}(a(\\log(V))^2 + b\\log(V) + c) + (a(\\log(V))^2 + b\\log(V) + c) \\frac{d}{dV}(N_{nv} + dV) \\right]$\n$= 6H \\left[ (N_{nv} +Vd) \\frac{2a \\log(V) + b}{V} + (a(\\log(V))^2 + b\\log(V) + c) d \\right].$\nThe solution of $\\frac{\\partial C}{\\partial V} = 0$ corresponds to the minimum point of the FLOPs. Since the variable V in this equation is not separated conveniently, we use a numerical search method, specifically scipy.optimize.fsolve, to find the solution."}, {"title": "A.2 The derivation of loss w.r.t the vocabulary size in Approach 3", "content": "Here we provide how we derive the loss w.r.t the vocabulary size given a FLOPs budget C in Approach 3. After substituting the $[Hf(V)]$ with the $C/(6(N_{nv} + N_v)$ based on Equation 9:"}, {"title": "A.3 More visualizations for the analyses: Why the optimal vocabulary size is bounded by the compute", "content": "Word embeddings in a large vocabulary are hard to learn when FLOPs are constrained Previous studies have shown embeddings suffer from representation degradation, where low-frequency word embeddings cluster together due to limited parameter updating [22]. In Figure 9, we visualize how the word embeddings distribute using different vocabulary sizes. We use the average Euclidean distance among all the embeddings, $D_{avg}$, to quantify the degree of clustering, which is 1.067, 1.011, and 0.952 for V = 4K, V = 16K and V = 64K, respectively. Larger vocabularies (64K) lead to more clustering of embeddings, especially for infrequent words. This clustering suggests that they have been insufficiently trained. Conversely, a small-sized vocabulary (4K) and middle-sized vocabulary (16K) display a more dispersed distribution of embeddings. These observations suggest that there exists an optimal vocabulary size that balances lexicon coverage and sufficient updating of word embedding. Language models with large vocabulary sizes may have better lexicon coverage, but on the other hand, hinder the model's ability to sufficiently update the word embeddings, especially for low-frequency words."}, {"title": "A.4 Exploration of Larger Range of Vocabulary Sizes", "content": "Because of computational resource constraints, the vocabulary sizes we used to fit the scaling laws are in the range of 4K to 96K. This range is sufficient to fit, because the optimal vocabulary sizes for all the training configurations we used fall in this range.\nTo further verify that there is always an optimal vocabulary size holds for a larger range of vocabulary lists, we increase the range of vocabulary sizes from 1K to 512K, with the Nnv fixed as 33M. As depicted in the Figure 10, the model's performance declines consistently as the vocabulary size increases beyond the optimal configuration. This figure shows loss curves for vocabulary sizes up to 512K, given a specific FLOPs budget. The data indicates a consistent degradation in model performance with the vocabulary size away from the optimal one. It suggests that there is a critical point beyond which the model's efficiency in handling the vocabulary diminishes. This exploration"}, {"title": "A.5 Implementation details", "content": "We list the architectures of the models and the corresponding number of training characters in Table 4. For each model family, we fix the non-vocabulary parameters $N_{nv}$ and vary the vocabulary size. We adopt the Llama architecture [63], except for the vocabulary size. For the vocabulary size, we use numbers divisible by 128 for compatibility with NVIDIA's tensor core to accelerate matrix multiplication 3. Specifically, the vocabulary sizes we adopt for each model family are 4096, 6144, 8192, 10240, 16384, 24576, 32768, 48128, 64512 and 96256. The expected number of training tokens D and characters H vary slightly given a fixed number of non-vocabulary parameters and a FLOP budget. We use the middle-sized V of 16384 to determine the number of training characters and the corresponding FLOPs budget, except for $N_{nv} = 2870M$ we use V = 32K."}, {"title": "A.5.2 The relationship between non-vocabulary parameters and embedding dimension", "content": "According to the observation in Kaplan et al. [28], the depth-width ratio has a relatively small effect on performance given the total non-vocabulary parameters. Thus, to ease the modeling of our scaling laws taking vocabulary size into account, we take the width (i.e. embedding dimension) as given following prior work [28, 24, 40, 64, 76]. The relationship between the non-vocabulary parameters Nnv and embedding dimension d used in our experiments are in Table 5."}, {"title": "A.5.3 Training details", "content": "The maximum learning rate is set to 4e-4 and decays to 10% i.e. 4e-5 similar to prior scaling work [24, 40]. We use AdamW [33] as our optimizer and accelerate training with bfloat16 mixed precision training. For models with $N_{nv} < 1130M$, we use a single node with 8 GPUs for training. Otherwise, we adopt the Megatron-LM framework [56] for multi-node training with 8 GPUs on each node. For our experiments with $N_{nv} = 2870M$, it takes about 120 hours to train on over 500B training characters with 64 total GPUs. We use a global batch size of 512 for all runs and run all experiments on 40GB Nvidia-A100 GPUs."}, {"title": "A.5.4 Fitting techniques", "content": "To avoid numerical underflow and overflow of the fitting parameters, we fit the data in a logarithmic form inspired by Hoffmann et al. [24]. Taking $N_{nv}$ as an example, we learn the parameters $k_1, \\alpha_1$ by minimizing:\n$\\min_{K_1,\\alpha_1} Huber(K_1 + \\alpha_1 \\log(C), \\log(N_{nv})),$ \nwhere $K_1 = \\log(k_1)$ and Huber, denotes the Huber loss with delta value $\\delta$ ($\\delta$ is 0.001 in our paper). We use the LBFGS algorithm to find the local minima of the function. The later Approach 2 and 3 use the same optimization algorithm. We initialize all attributes from the same uniform grid where $K \\in [-20, 15]$ and $\\alpha \\in [0, 1]$ with 20 initial guesses respectively. The fitting takes less than half of one minute.\nTo cheaply obtain more experimental data points, we perform interpolation of $(N_{nv}, N_v, H)$ triplets in the logarithmic scale and predict the validation loss based on real data points. Then, we compute the required FLOPs for each data point using Equation 5.\nBy using different N and obtaining the corresponding optimal N based on Equation 7, we have a set of $\\{(N_{nv_i}, N_{v_i})|i = 1,..., n\\}$. Denoting $D_{nv_i} = N_{nv_i} / N_{nv_0}$ and $D_{v_i} = N_{v_i}/ N_{v_0}$, we learn the scaling proportion $\\gamma$ by minimizing:\n$\\min_{\\gamma} Huber(\\gamma * \\log(D_{nv_i}), \\log(D_{v_i})).$\nThe initial guess of $\\gamma$ is uniformly sampled from [0, 1].\nWe recast the designed vocabulary-dependent loss formula here:\n$L_u = -E + \\frac{A_1}{N_{nv}^{\\alpha_1}} + \\frac{A_2}{N_{v}^{\\alpha_2}} + \\frac{B}{[Hf(V)]^{\\beta}},$"}, {"title": "A.6 Details of fitting tokens-character relationship function f(V)", "content": "We train 25 tokenizers with the following vocabulary sizes: 1024, 2048, 3072, 4096, 5120, 6144, 7168, 8192, 9216, 10240, 12288, 16384, 20480, 24576, 28672, 32768, 48128"}]}