{"title": "Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies", "authors": ["Chaofan Tao", "Qian Liu", "Longxu Dou", "Niklas Muennighoff", "Zhongwei Wan", "Ping Luo", "Min Lin", "Ngai Wong"], "abstract": "Research on scaling large language models (LLMs) has primarily focused on model parameters and training data size, overlooking the role of vocabulary size. We investigate how vocabulary size impacts LLM scaling laws by training models ranging from 33M to 3B parameters on up to 500B characters with various vocabulary configurations. We propose three complementary approaches for predicting the compute-optimal vocabulary size: IsoFLOPs analysis, derivative estimation, and parametric fit of the loss function. Our approaches converge on the same result that the optimal vocabulary size depends on the available compute budget and that larger models deserve larger vocabularies. However, most LLMs use too small vocabulary sizes. For example, we predict that the optimal vocabulary size of Llama2-70B should have been at least 216K, 7 times larger than its vocabulary of 32K. We validate our predictions empirically by training models with 3B parameters across different FLOPs budgets. Adopting our predicted optimal vocabulary size consistently improves downstream performance over commonly used vocabulary sizes. By increasing the vocabulary size from the conventional 32K to 43K, we improve performance on ARC-Challenge from 29.1 to 32.0 with the same 2.3e21 FLOPs. Our work emphasizes the necessity of jointly considering model parameters and vocabulary size for efficient scaling.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) achieve remarkable performance by pre-training on vast text corpora using massive computational resources [43]. Extensive prior work on LLMs has focused on deriving so-called scaling laws: a set of empirical formulas to predict how model performance scales, mainly as computing floating-point operations (FLOPs), model parameters, and quantity of training data change [28, 24, 60, 2, 40, 54]. These works show that power-law fits can effectively predict language modeling loss and by extension downstream performance [21, 51]. However, these scaling laws usually disregard the impact of the vocabulary size. For example, in Kaplan et al. [28] only non-vocabulary parameters are considered in their predictive formula. This negligence has resulted in substantial variability in the vocabulary size of current LLMs. For instance, Llama2-7B employs a vocabulary size of 32K [64], while Gemma-7B [61] adopts a much larger vocabulary size of 256K despite both having a similar number of total parameters. This variability in vocabulary sizes across LLMs raises the research question: What is the compute-optimal vocabulary size for an LLM?\nThe vocabulary size affects performance non-trivially. Intuitively, the optimal vocabulary size should neither be excessively large nor small. A larger vocabulary size improves tokenization fertility, i.e., splitting sentences into fewer tokens, thereby improving the tokenization efficiency. Additionally, a larger vocabulary enhances the representational capacity of the model, enabling it to capture a wider range of concepts and nuances in the corpus. However, the risk of under-fitting representations for rare tokens increases with larger vocabulary sizes, especially in the data-constrained regime [40, 66]. Thus, the optimal vocabulary size needs to be determined by taking the training data and the amount of non-vocabulary parameters into account.\nIn this paper, we show that the effect of vocabulary on scaling laws has been underestimated, and we quantify the effect to derive a prediction for the optimal vocabulary size. We first introduce a normalized loss formulation to ensure a fair comparison across models with varying vocabulary sizes. Utilizing the normalized loss function, we analyze and discuss the underlying rationale behind the existence of an optimal vocabulary size, which depends on the available computational budget.\nTo predict the optimal vocabulary size given a compute budget, we propose three approaches. Approach 1 (Estimating power laws via IsoFLOPs): We pre-train models with non-vocabulary pa- rameters ranging from 33M to 1.13B, with groups of models that share the same FLOPs (\u201cIsoFLOPs\") but varying vocabulary configurations. Then, we fit power laws relating FLOPs to non-vocabulary parameters, vocabulary parameters, and training data, respectively. Our analysis reveals that the optimal vocabulary parameters exhibit a power-law growth with respect to the computational budget, however, at a slower rate than non-vocabulary parameters, as shown in Figure 1. Approach 2 (Derivative-based Estimation): We introduce a derivative-based method that estimates the optimal"}, {"title": "Preliminary", "content": "In this section, we first present a general formulation of a commonly used scaling law, and then demonstrate how to modify it to incorporate the vocabulary."}, {"title": "Scaling law", "content": "Scaling laws consider a computational budget, C, which is measured in FLOPs. The goal is to optimally allocate the compute budget to model parameters N and the number of training tokens D [28, 6, 24, 40]. It can be formulated as:\n$(N_{opt}, D_{opt}) = \\arg \\min_{N,D} L(N, D) \\text{ s.t. } FLOPs(N, D) = C,$\nFollowing Radford et al. [47], the loss function is typically the language modeling loss, which can be written as:\n$L = \\frac{1}{T} \\sum_{i=1}^{T} -\\log p(w_i | w_{1:i-1}, V),$\nwhere $p(w_i | w_{1:i-1}, V)$ is the output probability of word $w_i$ given the context $w_{1:i-1}$ and the tokenizer with vocabulary size V. Generally, the lower L indicates better performance of the language model. However, due to its dependency on V, L cannot be used to compare language models with different vocabulary sizes. Thus, we propose an adaptation later in \u00a72.2. Fitting scaling laws generally requires various models trained for different configurations [21]. A common approach is to select several compute budgets and train models with varying N and D for each budget to find the best one, i.e. the one with the lowest loss (\u201cIsoFLOPs\") [24]. Using fitting techniques we can then estimate a function that maps from the compute budget to the optimal allocation to N and D."}, {"title": "Scaling law with vocabulary", "content": "As prior work generally assumes the vocabulary size to be fixed, we cannot adopt the attributes in their scaling laws and their evaluation metric directly. Thus, we detail several considerations that allow us to investigate vocabulary scaling laws.\nAttributes Scaling laws commonly deal with the attributes, model parameters (N) and number of training tokens (D) [24, 40]. We adapt them for our analysis in the context of vocabulary size. (1) We break down the total model parameters (N) into non-vocabulary ($N_{nv}$) and vocabulary parameters ($N_v$). To understand the importance of vocabulary parameters, we isolate them from other model parameters, where $N = N_{nv} + N_v$. We use $N_v = Vd$ to represent both the vocabulary parameters in the output layer. Notably, to change $N_v$ we only vary the vocabulary size V and take the embedding dimension d as given based on $N_{nv}$ empirically, see \u00a7A.5.2 for details. This is based on the observation by Kaplan et al. [28] that the performance of models with varying depth-to-width ratios converges to a single trend. (2) We measure data not in tokens (D) but in training characters (H). The number of tokens depends on the vocabulary of the tokenizer, thus we need a vocabulary-independent measurement of data. By studying training characters, we can better see how the data volume affects the performance regardless of different vocabulary sizes.\nMapping from training characters (H) to tokens (D) As detailed above we measure training data in training characters (H). Nonetheless, to connect our findings with existing studies on scaling laws [24, 40], we need to be able to map from H to D. This mapping is the tokenizer's compression ratio which can be computed via D/H. The more tokens the tokenizer needs to represent H, the larger D, and thus it compresses less. We develop a simple function $f(V)$ to estimate this ratio solely from the chosen vocabulary size, V. Specifically, we find that a quadratic function on the logarithmic value of V achieves accurate predictions:\n$f(V) = a \\log_2(V)^2 + b \\log(V) + c$\nBy fitting several tokenizers with V ranging from 1K to 1024K, we obtain a = 0.0064, b = \u22120.1581 and c = 1.2047. We find that our function accurately predicts the compression ratio with a low relative mean square error (RMSE) and a high coefficient of determination ($R^2$). In \u00a7A.7, we visualize fitting results and show that our approximation works with different tokenizers and is robust to different V. For all our main experiments, we use the BPE algorithm for tokenization [55].\nVocabulary-insensitive loss To fairly assess models that vary in V, the commonly used language model loss in Equation 2 is inappropriate. Models trained with larger V naturally have a higher loss, as there are more possibilities in the vocabulary to predict. However, this does not mean that the model is worse. Thus, we need to normalize the loss with respect to the vocabulary size. We reformulate the unigram-normalized metric [50] as a loss function. Suppose we have a T-length sequence $w_{1:T}$, we design the unigram-normalized language model loss as:\n$L_u = \\frac{1}{T} \\sum_{i=1}^{T} \\log \\frac{p(w_i | w_{1:i-1}, V)}{p(w_i | V)},$\nwhere $p(w_i | V)$ is the frequency of word $w_i$ in the tokenized corpus, given the tokenizer with vocabulary size V. The loss indicates the improvement in probability that a context-aware language model offers over a unigram model without context, allowing us to assess the language model's efficacy. Based on theory from prior work [50], the normalized loss $L_u$ remains consistent for a given model with a fixed non-vocabulary component across different vocabulary sizes. The difference of $L_u$ comes from the ability of the language model itself. Compared with L, the value of $L_u$ is much smaller and can be negative as $L_u$ adds a negative term $\\frac{1}{T} \\sum_{i=1}^{T} \\log p(w_i | V)$. One may also employ the average bits per character (BPC), a common metric for text compression [25], as the"}, {"title": "Analysis: Why the optimal vocabulary size is bounded by compute", "content": "In this section, we present analyses to explain why the optimal vocabulary size is constrained by the computational budget."}, {"title": "The perspective of fixed normalized loss", "content": "According to Kaplan et al. [28], the FLOPs (C) of a Transformer-based language model can be estimated as C \u2248 6ND, which can be re-written as:\n$C \\approx 6ND \\approx 6(N_{nv} + Vd)Hf(V),$\nwhere $N = N_{nv} + N_v$ and $D = Hf(V)$ based on \u00a72.2. The reasons why model performance first increases and then decreases as the vocabulary size grows are: (1) At small V, increasing the vocabulary size easily improves tokenization fertility from f(V). Subsequently, more characters can be learned from the model with a fixed number of tokens, thereby improving model performance. (2) At very large V, the gain from tokenization fertility decreases, while the parameters from expanding the vocabulary cannot be adequately trained with limited data, which leads to a decline in model performance. We present an expanded derivation in \u00a7A.1, and show how the corresponding FLOPs change with the vocabulary size in Figure 3 (left)."}, {"title": "The perspective of fixed FLOP budget", "content": "Given a fixed FLOPs budget, we isolate the FLOPs and investigate how the vocabulary influences the loss. In practice, we set several fixed FLOP budgets. For each budget, we adopt a group of models with similar total parameters and vary vocabulary sizes from 4K to 96K. In Figure 3 (right) we plot the relationship between the loss w.r.t. the vocabulary size. It reveals that the vocabulary corresponding to the lowest point on the loss curve increases as the FLOPs budget increases. This suggests that with more computational resources, LLMs can effectively harness larger vocabularies to reduce loss. However, merely expanding the vocabulary does not always lower the loss. For a fixed FLOP budget, the loss initially decreases with the increase in vocabulary and then starts to rise, indicating that an optimal point exists for the vocabulary. This suggests a trade-off between model complexity and computational constraints, where an overly large vocabulary cannot be utilized efficiently, leading to sub-optimal model performance."}, {"title": "Analysis 3: The perspective of parameter growing", "content": "Traditionally, scaling up model parameters in language models has been approached in two ways: increasing depth (i.e., the number of layers) or width (i.e., the hidden size). While extensive research has been conducted on these methods, current empirical practices often involve expanding both simultaneously [60]. This approach, however, may overlook crucial distinctions in how different parameters benefit from these expansions.\nNon-vocabulary parameters can benefit from increases in both depth and width, allowing for more complex hierarchical representations and broader feature capture. In contrast, vocabulary parameters, associated with word embeddings and language model heads, are generally confined to a single layer, limiting their ability to benefit from increases in the model depth. Their primary avenue for expansion is through increasing the width. This disparity in growth potential between non-vocabulary and vocabulary parameters suggests that to maintain a balanced growth rate, it may be necessary to expand the vocabulary size along with the depth. This would allow the vocabulary parameter part to keep pace with the growth of non-vocabulary parameters."}, {"title": "Estimating the optimal vocabulary size", "content": "In this section, we describe three complementary approaches to estimate the optimal vocabulary size."}, {"title": "Approach 1: Estimating power laws via IsoFLOPs", "content": "We define 6 groups of models with $N_{nv}$ ranging from 33M to 1.13B. Within each group, we solely vary the vocabulary size V from 4K to 96K, and evaluate different models under the same FLOPs budget. We evaluate the normalized loss $L_u$ on a held-out validation dataset. This approach allows us to directly answer the question: For a given FLOPs budget, what is the optimal allocation to non-vocabulary parameters, vocabulary parameters, and training data?\nSetup Given a certain N, the embedding dimension d is fixed, thus $N_v$ increases as V increases. For all experiments, we uniformly sample the training data from different domains in the SlimPajama dataset [57]. All other hyperparameters are fixed with more details in \u00a7A.5.\nFitting We select data points with the minimum $L_u$ for each FLOP budget, with all runs visualized in Figure 4. These points are the compute-optimal allocation to $(N_{nv}, N_v, H)$. Following Kaplan et al. [28] and Hoffmann et al. [24], we hypothesize that the optimal vocabulary parameters $N_v$ meet a power law w.r.t. the FLOPs C, just like the non-vocabulary parameters and the amount of training data. Specifically, $N_{nv} = k_1 C^{\\alpha_1}, N_v = k_2 C^{\\alpha_2}$ and $H = k_3 C^{\\alpha_3}$. As model size and training data should be scaled equally for compute-optimal training [24], we set $\\alpha_1 = \\alpha_3$. As our new attribute V significantly increases the number of possible experimental configurations, we employ interpolation"}, {"title": "Approach 2: Derivative-based fast estimation", "content": "We propose an alternative approach leveraging insights from the estimation of the FLOPs itself. Prior work [24, 28] usually considers a fixed compute budget in FLOPs and then aims to minimize loss by finding the optimal allocation to model parameters N and training tokens D. Here we flip this recipe on its head following recent work [53]. We aim to find the minimum FLOPs to achieve a certain loss $L_u(N_{nv}, V, H) = l$ through optimal allocation of the vocabulary size V:\n$V = \\arg \\min_{V | L_u(N_{nv}, V, H) = l} C(N_{nv}, N_v, H).$\nBy computing the minimum point of FLOPs C with respect to V via derivative:\n$\\frac{\\partial C}{\\partial V} = 6H \\frac{(N_{nv} + Vd)}{V} [2a \\log(V) + b] + [a(log(V))^2 + b \\log(V) + c] d,$\nwe can estimate the optimal V under the assumption that it can achieve a certain loss $L_u(N_{nv}, V, H) = l$. The parameters a, b and c can be easily obtained from building $f(V)$ (\u00a72.2). In theory, as long as the non-vocabulary parameters $N_{nv}$ are provided, V can be numerically searched via the solution of $\\frac{\\partial C}{\\partial V}=0$. More details are in \u00a7A.1.\nUsage Note that the optimal vocabulary size should be determined primarily by the normalized loss $L_u$, rather than by the compute budget FLOPs. However, when the compute allocation is near optimal, the loss exhibits a power-law relationship with respect to the FLOPs budget, as described by the scaling law [28]. This relationship allows us to use FLOPs with compute-optimal allocation as a reliable proxy for observing the scaling behavior of the optimal vocabulary parameters. In practise,"}, {"title": "Approach 3: Parametric fit of loss formula", "content": "Finally, we directly predict the loss given the non-vocabulary parameter, vocabulary parameter and the amount of training characters. Then, the optimal vocabulary configuration can be predicted by finding the minimum point of loss with respect to the vocabulary. Following a classical risk decomposition used in Hoffmann et al. [24], we design the vocabulary-dependent loss formula as:\n$L_u = -E + \\frac{A_1}{N_{nv}^{\\alpha_1}} + \\frac{A_2}{N_v^{\\alpha_2}} + \\frac{B}{D^{\\beta}},$\nwhere $D = Hf(V)$. The first term captures the normalized loss for an ideal generative process on the data distribution. The subsequent terms reflect the effect of the non-vocabulary parameters, vocabulary parameters, and the number of training data on the loss, respectively. $E, A_1, A_2, B, \\alpha_1, \\alpha_2, \\beta$ are learned parameters.\nFitting We use the points $(N_{nv}, N_v, H)$ collected for experiments in \u00a74.1. Note that we do not only consider the points with the lowest loss for each FLOP budget as we want to predict loss for any combination of $(N_{nv}, N_v, H)$. We add the constraint $\\alpha_1 = \\beta$ following Muennighoff et al. [40]. We also filter out points with very small FLOPs following Hoffmann et al. [24]. Fitting yields $A_1 = 1.831, A_2 = 0.196, B = 2.124, E = 5.533, \\alpha_1 = \\beta = 0.447, \\alpha_2 = 0.671$. The detailed fitting process is written in \u00a7A.5.4.\nUsage After fitting the parameters in Equation 8, the optimal vocabulary size can be obtained by finding the lowest loss w.r.t the vocabulary size, with a constraint of FLOPs budget. For example, given $N_{nv}$ and FLOPs budget C, by replacing $[Hf(V)]$ with $C/(6(N_{nv} + N_v))$ and finding the solution of $\\frac{\\partial L_u}{\\partial V} = 0$ via numerical search, we can get the prediction. The details of $\\frac{\\partial L_u}{\\partial V}$ is written in \u00a7A.2. Note that all of the proposed approaches can be used in optimally allocating $(N_{nv}, N_v, H)$ altogether, while Approach 3 is more flexible in predicting the locally optimal $N_v$ when $(N_{nv}, H)$ are not following the Chinchilla's law [24], i.e. equally-scaled law. The reason is that the loss formula in Approach 3 does not only considers the combinations $(N_{nv}, N_v, H)$ which reach the optimal given a certain training budget. By fixing $N_{nv}$ and varying C in Approach 3, we can predict the locally optimal vocabulary size with different amount of training characters. This property makes Approach 3 more valuable, since modern LLMs [64, 61, 3, 4, 8] usually leverage overly sufficient training data to build powerful models with relatively low inference costs.\nIn Figure 6, we remove the assumption [24] for the practical reason that the parameters and training data are not equally scaled. Then, we predict the locally optimal vocabulary parameters. It can be observed that the allocation of vocabulary parameters are typically under-estimated."}, {"title": "Discussion", "content": "Predicting allocations for larger models Table 1 reports the predicted optimal vocabulary parame- ters and sizes based on the proposed three approaches, where the amount of training data is optimally allocated, i.e. equally scaled with the non-vocabulary parameters [24]. As shown in Figure 1, the predictions from all proposed approaches align closely. $N_{nv}$ should be scaled faster than $N_v$. Notably, mainstream LLMs typically assign fewer parameters to vocabulary than what is optimal. However, the community is starting to shift to larger vocabularies, such as with Llama3 [37] having a 128K vocabulary size up from 32K of Llama2 [64]. However, scaling data is still the most critical part, and solving data scarcity issues should be a focus of future work [66].\nTo empirically verify our prediction, we train models with $N_{nv} = 2.87B$ under a compute-optimal training FLOPs budget and evaluate them using lighteval 2. For the baseline model we use the common vocabulary size of V = 32K. The other model uses $V_{opt}$ as predicted by Approach 3."}, {"title": "Related work", "content": "Large language models The Transformer [65] has proven to be a very scalable architecture with consistent performance gains which has led to a series of large language models (LLMs) [12, 15, 48, 43, 18, 27, 49, 64, 67, 37, 9, 4, 34, 23, 58, 61, 8, 35, 30, 79]. Through their training to predict subsequent tokens in a sequence, these models acquire a deep understanding of language enabling them to perform a variety of language tasks directly after pre-training. Their capabilities include code generation [31, 3, 39, 78, 77], mathematical reasoning [69, 5], question answering [44, 41] among others. In our work, we pre-train large language models from scratch on English corpora and focus on their loss during training and downstream performance on common tasks after training.\nVocabulary in language models The vocabulary of a language model influences its performance significantly [59, 68, 71]. A larger vocabulary size helps cover more words thus reducing the likelihood of out-of-vocabulary (OOV) cases [19]. Takahashi and Tanaka-Ishii [59] find that larger vocabularies are better at capturing the true statistical distribution of language. Similarly, expanding vocabulary in multilingual models [68] improves performance, especially for low-resource languages. However, large vocabularies [29] increase the computational overhead during both training and generation phases. Liao et al. [32] show that low-frequency words have few instances in the training data, leading to insufficient information to train robust representations if the vocabulary is too large. To this end, our work fills an under-explored gap: How to optimally allocate the vocabulary size?\nByte-level language models Recent work has explored byte-level language models [74, 70], which offer advantages in decoding efficiency and noise robustness compared to token-level models. However, typically limited to parameters under 1B, these models have not been effectively scaled up. Our scaling laws suggest that the limited vocabulary (i.e., 256 in byte-level language models) may constrain their performance, especially for larger models. The insight provides a potential explanation for the challenges in scaling byte-level models and implies that successful scaling of language models may require proportional increases in vocabulary size."}, {"title": "Conclusion", "content": "We investigate the impact of the vocabulary size when scaling language models. We analyze and verify that there exists an optimal vocabulary size for a given FLOPs budget. Subsequently, we develop 3 approaches to predict the optimal vocabulary size. Our first approach uses a set of empirical training runs across different IsoFLOPs regimes to fit a scaling law. The second approach investigates the FLOPs w.r.t. the vocabulary size and estimates the vocabulary size with derivatives. The third approach consists of a parametric function to directly predict the impact of different attributes on loss. Across all approaches, we find that while vocabulary parameters should be scaled slower than other parameters, they are still critical for performance and we can accurately predict their optimal allocation. We make predictions for larger models and empirically verify our approaches on up to 3B parameters and on varying amounts of training data. Our results show that models trained with an optimal vocabulary size as predicted by our approaches outperform models with a conventional vocabulary size under the same FLOPs budget."}, {"title": "Appendix", "content": null}, {"title": "The derivation of FLOPs w.r.t the vocabulary size for the Approach 2", "content": "Here we provide the detailed process of how we compute the extreme point of FLOPs C with respect to V. From Kaplan et al. [28], we know that:\n$C \\approx 6ND \\approx 6(N_{nv} + Vd)Hf(V).$\nWe then compute the derivative as follows:\n$\\frac{\\partial C}{\\partial V} = \\frac{\\partial}{\\partial V} [6(N_{nv} + dV)H (f(V))] = \\frac{\\partial}{\\partial V} [6(N_{nv} +dV) H (a(\\log(V))^2 + b\\log(V) + c)] = 6H (N_{nv} + dV) \\frac{d}{dV} (a(\\log(V))^2 + b\\log(V) + c) + (a(\\log(V))^2 + b\\log(V) + c) \\frac{d}{dV} (N_{nv} + dV) = 6H (N_{nv} + Vd) \\frac{2a \\log(V) + b}{V} + (a(\\log(V))^2 + b\\log(V) + c)d].$\nThe solution of $\\frac{\\partial C}{\\partial V} = 0$ corresponds to the minimum point of the FLOPs. Since the variable V in this equation is not separated conveniently, we use a numerical search method, specifically scipy.optimize.fsolve, to find the solution."}, {"title": "The derivation of loss w.r.t the vocabulary size in Approach 3", "content": "Here we provide how we derive the loss w.r.t the vocabulary size given a FLOPs budget C in Approach 3. After substituting the [Hf(V)] with the C/(6($N_{nv}$ + $N_v$) based on Equation 9:\n$L_u = -E + \\frac{A_1}{N_{nv}^{\\alpha_1}} + \\frac{A_2}{N_v^{\\alpha_2}} + \\frac{B}{[C/(6(N_{nv} + N_v)]^{\\beta}}.$\nThe loss is solely dependent on the $N_v = Vd$, given a $N_{nv}$. The derivative w.r.t. V is:\n$\\frac{\\partial L_u}{\\partial V} = \\frac{\\partial}{\\partial V} (\\frac{A_2}{(Vd)^{\\alpha_2}}) + \\frac{\\partial}{\\partial V} (\\frac{B}{(\\frac{C}{6(N_{nv} + Vd)})^{\\beta}}) = -\\frac{A_2 \\alpha_2 d}{(Vd)^{\\alpha_2+1}} + \\frac{B \\beta \\frac{Cd}{6(N_{nv} + Vd)^2}}{(\\frac{C}{6(N_{nv} + Vd)})^{\\beta+1}} = -\\frac{A_2d}{(Vd)^{\\alpha_2+1}} + \\beta\\frac{Cd}{6(N_{nv}+Vd)^2}$.\nThe solution of $\\frac{\\partial L_u}{\\partial V} = 0$ corresponds to the optimal V. Similar with Approach 2, we use scipy.optimize.fsolve to find the solution."}, {"title": "More visualizations for the analyses: Why the optimal vocabulary size is bounded by the compute", "content": "Word embeddings in a large vocabulary are hard to learn when FLOPs are constrained Previous studies have shown embeddings suffer from representation degradation, where low-frequency word embeddings cluster together due to limited parameter updating [22]. In Figure 9, we visualize how the word embeddings distribute using different vocabulary sizes. We use the average Euclidean distance among all the embeddings, $D_{avg}$, to quantify the degree of clustering, which is 1.067, 1.011, and 0.952 for V = 4K, V = 16K and V = 64K, respectively. Larger vocabularies (64K) lead to more clustering of embeddings, especially for infrequent words. This clustering suggests that they have been insufficiently trained. Conversely, a small-sized vocabulary (4K) and middle-sized vocabulary (16K) display a more dispersed distribution of embeddings. These observations suggest that there exists an optimal vocabulary size that balances lexicon coverage and sufficient updating of word embedding. Language models with large vocabulary sizes may have better lexicon coverage, but on the other hand, hinder the model's ability to sufficiently update the word embeddings, especially for low-frequency words."}, {"title": "Exploration of Larger Range of Vocabulary Sizes", "content": "Because of computational resource constraints, the vocabulary sizes we used to fit the scaling laws are in the range of 4K to 96K. This range is sufficient to fit, because the optimal vocabulary sizes for all the training configurations we used fall in this range.\nTo further verify that there is always an optimal vocabulary size holds for a larger range of vocabulary lists, we increase the range of vocabulary sizes from 1K to 512K, with the $N_{nv}$ fixed as 33M. As depicted in the Figure 10, the model's performance declines consistently as the vocabulary size increases beyond the optimal configuration. This figure shows loss curves for vocabulary sizes up to 512K, given a specific FLOPs budget. The data indicates a consistent degradation in model performance with the vocabulary size away from the optimal one. It suggests that there is a critical point beyond which the model's efficiency in handling the vocabulary diminishes. This exploration"}, {"title": "Implementation details", "content": null}, {"title": "Setting of model architecture, vocabulary size and training characters", "content": "We list the architectures of the models and the corresponding number of training characters in Table 4. For each model family, we fix the non-vocabulary parameters $N_{nv}$ and vary the vocabulary size. We adopt the Llama architecture [63], except for the vocabulary size. For the vocabulary size, we use numbers divisible by 128 for compatibility with NVIDIA's tensor core to accelerate matrix multiplication 3. Specifically, the vocabulary sizes we adopt for each model family are 4096, 6144, 8192, 10240, 16384, 24576, 32768, 48128, 64512 and 96256. The expected number of training tokens D and characters H vary slightly given a fixed number of non-vocabulary parameters and a FLOP budget. We use the middle-sized V of 16384 to determine the number of training characters and the corresponding FLOPs budget, except for $N_{nv} = 2870M$ we use V = 32K."}, {"title": "The relationship between non-vocabulary parameters and embedding dimension", "content": "According to the observation in Kaplan et al. [28], the depth-width ratio has a relatively small effect on performance given the total non-vocabulary parameters. Thus, to ease the modeling of our scaling laws taking vocabulary size into account, we take the width (i.e. embedding dimension) as given following prior work [28, 24, 40, 64, 76]. The relationship between the non-vocabulary parameters $N_{nv}$ and embedding dimension d used in our experiments are in Table 5."}, {"title": "Training details", "content": "The maximum learning rate is set to 4e-4 and decays to 10% i.e. 4e-5 similar to prior scaling work [24, 40]. We use AdamW [33] as our optimizer and accelerate training with bfloat16 mixed precision training. For models with $N_{nv} < 1130M$, we use a single node with 8 GPUs for training. Otherwise, we adopt the Megatron-LM framework [56] for multi-node training with 8 GPUs on each node. For our experiments with $N_{nv} = 2870M$, it takes about 120 hours to train on over 500B training characters with 64 total GPUs. We use a global batch size of 512 for all runs and run all experiments on 40GB Nvidia-A100 GPUs."}, {"title": "Fitting techniques", "content": "Approach 1 To avoid numerical underflow and overflow of the fitting parameters, we fit the data in a logarithmic form inspired by Hoffmann et al. [24]. Taking $N_{nv}$ as an example, we learn the parameters $k_1, \\alpha_1$ by minimizing:\n$\\min_{K_1,\\alpha_1} Hubers(K_1 + \\alpha_1 \\log(C), \\log(N_{nv})),$\nwhere $K_1 = \\log(k_1)$ and $Huber$, denotes the Huber loss with delta value \u03b4 (\u03b4 is 0.001 in our paper). We use the LBFGS algorithm to find the local minima of the function. The later Approach 2 and 3 use the same optimization algorithm.```json\nApproach 2 By using different Nnv and obtaining the corresponding optimal N\u2082 based on Equation 7, we have a set of {($N_{nv_i}$, $N_{v_i}$)|i = 1,..., n}. Denoting $D_{nv_i} = N_{nv_i}/N_{nvo}$ and $D_{ui} = N_{vi}/N_{vo}$, we learn the scaling proportion y by minimizing:\n$\\min_\\gamma Hubers(\\gamma * \\log(D_{nv_i}), \\log(D_{ui})),$\nThe initial guess of y is uniformly sampled from [0, 1].\nApproach 3 We recast the designed vocabulary-dependent loss formula here:\n$L_u = -E + \\frac{A_1}{N_{nv}^{\\alpha_1}} + \\frac{A_2}{N_v^{\\alpha_2}} + \\frac{B}{[Hf(V)]^{\\beta}},$"}, {"title": "Robustness of the tokens-characters relationship function f(V)", "content": "Robustness to the type of tokenizers Besides the widely adopted BPE tokenizer used in our experiment, we also consider the unigram tokenizer and the word-based tokenizer. We visualize their tokens-characters ratio and corresponding predictive function in Figure 11. We find that our proposed formula of f(V) is a good predictor for the tokens-character ratio, regardless of which tokenizer is used. This verifies the effectiveness of our proposed formula. The tokenization fertility of the unigram tokenizer is close to that of the BPE tokenizer as seen in their similar y-axis values, since they both employ subword-based tokenization. Meanwhile, the tokenization fertility of word-based tokenization is poor, thus requiring more tokens on average to compress characters.\nRobustness to the range of the vocabulary size The quadratic function on the logarithmic value of vocabulary size that we propose can precisely predict the tokens-characters ratio with an RMSE of 1.5e-6 and $R^2$ of 0.99. However, as a quadratic function is single-peaked, increasing V will increase the output value of $f(V) = a log_2(V)^2 + b log V + c$ when V is very large, e.g. V > exp(-b/2a) \u2248 218K in our case.\nFortunately, when V is sufficiently large, the tokenization fertility improvement of the tokenizer decays sharply, which results in almost no change to the value of f(V). This is because the words in the training corpus can already be effectively covered by the vocabulary list when the vocabulary size is sufficiently large. In this extreme, the tokenization fertility of the corresponding"}, {"title": "Experimental verification on the fairness of the unigram-normalized language modeling loss", "content": "In \u00a72.2, we have explained that we use a unigram-normalized loss, Lu, to fairly evaluate models that vary in vocabulary size. Here we empirically verify this choice. We train models with a fixed number of non-vocabulary parameters Nnv and embedding dimension d but varying vocabulary sizes V. Thus, their vocabulary parameters N also vary. We plot the final language model loss and unigram-normalized loss of these models compared to downstream performance in Figure 12. The language modeling loss exhibits a positive correlation with downstream performance: Models with a higher language modeling loss have better downstream performance. This is because our models with larger vocabularies naturally have a higher loss due to the objective function, yet they can be actually better models with better downstream performance. Our unigram-normalized loss solves this problem and exhibits the expected negative correlation between loss and downstream performance: a lower loss comes with better downstream performance. This empirically justifies our use of Lu throughout this work."}, {"title": "Limitation and future work", "content": null}, {"title": "Limitations of our proposed approaches", "content": "Approach 1 The Approach 1 provides a broader solution by predicting the allocation of computa- tional resources across non-vocabulary parameters, vocabulary parameters, and training data based on experimental data points. This method's strength lies in its holistic view, allowing for a balanced resource distribution that potentially enhances model efficiency and performance. However, this"}, {"title": "Larger models and different architectures", "content": "We have shown that our predictions hold for models with up to three billion parameters (\u00a75). However, LLMs are often orders of magnitude larger, such as the 400-billion parameter Llama-3 model [37]. Further, we have decided to focus on dense transformer language models, as they are most commonly used for LLMs. However, many non-transformer models have been proposed and scaled up to billions of parameters [45, 46]. Exploring to what extent our findings hold in even larger models and with different architectures is a promising direction for future work."}, {"title": "Parametric function for the loss when considering the vocabulary", "content": "Researchers [24, 40] consider modeling the language modeling loss with parametric functions in the form of L = P1 + P2/N\u00b0 + P3/D\u00b3, where {P1, P2, P3, \u03b1, \u03b2} are learnable variables. The first term of loss represents the minimum achievable loss, and the second and third terms represent the contribution to the loss from the model size N and number of training tokens D. The parametric function allows predicting the loss L given N and D even if (N,D) are not optimally allocated. In prior work, this loss formula accounts for changes in model size and training data but does not explicitly address the complexities introduced by varying vocabulary sizes. Incorporating vocabulary size into the loss predictor is challenging: Vocabulary size affects the model directly as well as the number of tokens and the quality of tokenization by the tokenizer. A tokenizer with a large vocabulary size makes it easier to capture semantic information in raw text and reduces the frequency of out-of-vocabulary words. For instance, a large vocabulary size may cover common phrases, common subwords, and specialized terminology. Therefore, even if the same number of tokens are trained, the performance of the model trained on tokens with different qualities will be different.\nFuture work in this area could explore various parametric non-linear loss functions to predict the interactions between vocabulary size, model size, and training data with different compute allocations, not just the case of optimal compute allocation. Additionally, empirical studies on different datasets could help in understanding how vocabulary size impacts loss under varied data conditions, guiding the development of more adaptive loss prediction models."}, {"title": "Extensions to multilingual and multimodal scenarios", "content": "Future work could extend the proposed approaches to encompass multilingual and multimodal scenarios. Multilingual models require a nuanced understanding of vocabulary due to linguistic diversity, which may affect the optimal vocabulary size and the computation of FLOPs differently across languages. Adapting these methods to consider linguistic features and tokenization variations could lead to more tailored and efficient resource allocations for multilingual models. Different languages compete with each other for the model's ability to allocate to that language [11], which makes it necessary to take into account the relationship between different languages when setting the size of word lists for different languages in a multilingual scenario.\nFor multimodal models that integrate text with other data types such as images or video, the optimal vocabulary size might interact uniquely with non-linguistic parameters. Recent work [1, 62] models visual concepts in an autoregressive manner with tokenization like the processing of text data. It is interesting to explore the size of visual vocabulary size, i.e., the codebook size [20], in the visual tasks and vision-language tasks. How to set the vocabulary size and the compute resource efficiently for different modalities remains an open issue."}, {"title": "Potential social impact", "content": "The positive potential social impact of this research on vocabulary size in language model scaling is substantial. By optimizing large language models with the consideration of the vocabulary size and other attributes jointly, the paper provides a foundational understanding that can lead to more lightweight and cost-effective pre-trained large language models. This efficiency can democratize access to advanced language processing technologies, making it feasible for smaller organizations and the general public to benefit from powerful AI tools. Such advancements can benefit various domains, for example, improve accessibility features for individuals with disabilities, where efficient language models can be used to analyze medical records and assist in diagnostics. Furthermore, the reduction in computational requirements for training these models can lead to a decrease in energy usage, contributing positively to environmental sustainability efforts.\nOn the other hand, the misuse of pretrained language models may pose risks, including the creation of highly realistic deepfakes that can spread disinformation and undermine trust in media and institutions. These models can generate misleading content, automate cyberattacks through convincing phishing schemes, and produce large-scale spam, degrading online communication. Additionally, they can be used to generate harmful or abusive content, such as hate speech, which perpetuates discrimination and harms vulnerable populations. To mitigate these risks, it is crucial to develop trustworthy language models, implement robust monitoring systems, and foster collaboration among researchers, policymakers, and users."}]}