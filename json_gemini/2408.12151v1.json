{"title": "A Tighter Complexity Analysis of SparseGPT", "authors": ["Xiaoyu Li", "Yingyu Liang", "Zhenmei Shi", "Zhao Song"], "abstract": "In this work, we improved the analysis of the running time of SparseGPT [Frantar, Alistarh ICML 2023] from $O(d^3)$ to $O(d^2 + d^{2+a+o(1)} + d^{1+w(1,1,a)-a})$ for any $a \\in [0, 1]$, where $w$ is the exponent of matrix multiplication. In particular, for the current $w \\approx 2.371$ [Alman, Duan, Williams, Xu, Xu, Zhou 2024], our running times boil down to $O(d^{2.53})$. This running time is due to the analysis of the lazy update behavior in iterative maintenance problems, such as [Deng, Song, Weinstein 2022, Brand, Song, Zhou ICML 2024].", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have been widely applied in many AI applications. Accelerating\nits inference speed is crucial to reduce the latency time during user usage. One of the recent\nbrilliant works, SparseGPT [FA23], uses calibration data to prune the parameters of GPT-family\nmodels [BMR+20, ZRG+22], by using the optimal brain damage technique [LDS89, HS92]. Their\nalgorithm (Algorithm 1) can prune at least 50% parameters with structure patterns, while the\nperplexity increase is negligible. Thus, SparseGPT can reduce the running time and GPU memory\nusage while keeping high performance for LLMs' applications.\nSparseGPT [FA23] claims their main pruning algorithm (Algorithm 1) takes running time $O(d^3)$\nwhere $d$ is the model's hidden feature dimensions. In our work, we improved the running time\nanalysis to get a tighter running time complexity bound, i.e., $O(d^{2.53})$, for Algorithm 1. Formally,\nwe state our main results as follows.\nTheorem 1.1 (Main result (Restatement of Theorem 4.1)). Let lazy update block size $B = d^a$ for\nany $a \\in [0,1]$. Then Procedure SPARSEGPT in Algorithm 1 achieves the running time\n$O^*(d^w + d^{2+a} + d^{1+w(1,1,a)-a})$.\nFor the current $w \\approx 2.371$, according to the Table 1 in [ADW+24], we should choose $a \\approx 0.5275$\nin order to balance the terms $d^{2+a+o(1)}$ and $d^{1+w(1,1,a)-a}$. Then the running time boils down to\n$O(d^{2.53})$, which is better than $O(d^3)$ as claimed in [FA23].\nThe key technique in our analysis is to compute the complexity of SparseGPT by using lazy\nupdate. It comes from an interesting fact of fast rectangular matrix multiplication: the time\ncomplexity of multiplying a $d\\times d$ matrix by a $d \\times 1$ matrix is the same as the times complexity of\nmultiplying a $d\\times d$ matrix by a $d\\times d^a$ matrix for any nonnegative $a \\leq \\tilde{a}$ where $\\tilde{a}$ is the dual exponent\nof matrix multiplication and currently $\\tilde{a} \\approx 0.321$ [WXXZ24, LG24, LGU18]. The lazy update has\nbeen widely used to speed up the training of neural networks [CLP+20, BPSW21, SZZ24] and\nmaintain dynamic attention in large language models [BSZ24]. The high-level intuition of lazy\nupdate is that during iterative one-rank updates for a $d\\times d$ matrix, we use an external matrix\n$d\\times d^a$ to save the input for updates while we conduct fast matrix multiplication only when $d^a$\nupdates happen. Based on the lazy update idea, we achieve our tighter complexity bound."}, {"title": "Related Work", "content": "Model Acceleration and Pruning\nModel acceleration is critical and urgently needed in many practical applications. There are many\ntechniques for model acceleration. One line of work is to change model architecture to sup-\nport fast inference, e.g., Mamba [GD23, DG24], PolySketchFormer [KMZ23], Linearizing Trans-\nformers [ZBKR24, MVK+24], Hopfield Model [HCW+24, HCL+24, WHHL24, XHH+24, HLSL24,\nWHL+24, HYW+23] and so on. Another line of work is accelerating model computation on system\nlevel, e.g., FlashAttetnion [DFE+22, Dao23, SBZ+24, SY24], parallel decoding [SSU18], quantiza-\ntion [ZLC+24, TZZ+24, LTT+24] and so on. To accelerate LLMs training and inference, there is\na line of work to approximate attention matrix computation [AS23, AS24, HJK+24, ZHMK24,\nLSSZ24, PMN+23, CTWC24, LSSY24, LLS+24,GSWY23] in almost linear time. Some spe-\ncific technique is developed to accelerate LLMs generation, e.g., KV-Cache compression [GZL+23,\nLDLG23, XTC+23, ZSZ+24, LWD+23, DYZ+24, SCY+24] and speculative decodings [CLG+24,\nLCH+24, SCY+24, ESL+24]."}, {"title": "Lazy Update", "content": "Pruning is a technique aimed at reducing the number of weights in a neural network by se-\nlectively removing certain neurons [LDS89, HS92]. This approach has gained considerable atten-\ntion in recent years as a method to enhance the efficiency and scalability of deep learning mod-\nels [HPTD15, FC18, LAT19, WZG19, BGOFG20, BMBE20, LZ20, TKYG20, CJD+21, HABN+21,\nHCI+21, JCR+22, FA22, FA23, SLBK24]. Pruning can be categorized based on its stage in the\ntraining process: pre-training pruning and post-training pruning. Pre-training pruning involves\npruning the network at initialization. [FC18] demonstrated that a neural network pruned at ini-\ntialization could be trained to achieve performance comparable to a dense model, a phenomenon\nreferred to as the lottery ticket hypothesis. This discovery motivated a line of research focused\non developing methods to reduce the computational cost of pruning neural networks at initializa-\ntion [LAT19, WZG19, LZ20, TKYG20, CJD+21]. More recently, [YLG+23] provided theoretical\nevidence that pre-training pruning can enhance a model's generalization ability. Post-training prun-\ning, initially popularized through its application in quantization [NAVB+20, HNH+21, LGT+21,\nSLBK24], was later extended to pruning by [HCI+21, FA22, KKM+22]. Post-training pruning aims\nto compress a well-optimized model using a small set of calibration data. This process typically\ninvolves layer-wise pruning of the neural network. Notably, [HCI+21] proposed a provable and\nefficient method for compressing a model by \"stitching together\" individually compressed layers.\nLazy Update\nIn recent years, the lazy update idea has emerged as a crucial technique to effectively and effi-\nciently solve various optimization problems including linear programming [CLS19, Bra20, BLSS20,\nJSWZ21, SY21, LSZ+23], semi-definite programming [JKL+20, HJS+22, GS22, SYZ23], empiri-\ncal risk minimization [LSZ19, GSZ23, QSZZ23], cutting plane methods [JLSW20], neural network\ntraining [CLP+20, BPSW21, SZZ24], discrepancy minimization [DSW22], dynamic attention prob-\nlems [BSZ24] and so on.\nWe highlight several previous papers that share similar running time complexity. For compar-\nison, we assume the input size of these problems is dominated by a parameter $d$. In the line of\ndevelopments of fast linear program solvers, [CLS19] first introduced the lazy update idea to de-\nvelop efficient dynamic inverse structures for implementing interior point methods in solving linear\nprogramming problems, and it achieves the running time $O^*(d^w + d^{2.5-a/2} + d^{1.5+a})$. [SY21] uses a\ndifferent method to achieve linear programming in the same complexity $O^*(d^w + d^{2.5-a/2} + d^{1.5+a})$\nby incorporating the sparse sketching matrix to speed up the online matrix-vector multiplication.\n[JSWZ21] improves the linear programming to $O^*(d^w + d^{2.5-a/2} + d^{1.5+a-\\tilde{a}/2} + d^{0.5+a+(w-1)\\tilde{a}})$ time\nwhere $a \\in [0, \\tilde{a}]$ by designing a two-level lazy update framework for efficiently maintaining a pro-\njection matrix. Later [LSZ19] generalizes the algorithm for linear programming to empirical risk\nminimization, which also takes $O^*(d^w + d^{2.5-a/2} + d^{1.5+a})$ time. Different from [CLS19], which\nemploys a stochastic central path method that updates weights using a random sparse vector,\n[LSZ19] introduces a robust deterministic central path method. Additionally, [LSZ19] proposes an\nefficient data structure capable of handling updates efficiently even when the weight update vector\nis dense. Further, [QSZZ23] uses a different method to achieve empirical risk minimization in the\nsame complexity $O^*(d^w + d^{2.5-a/2} + d^{1.5+a})$ via online projection matrix-vector multiplication.\nThere are numerous other applications of the lazy update technique. For instance, [DSW22]\nemploys this approach to solve a subroutine of the discrepancy minimization problem in $O^*(d^w +$\n$d^{2+a} + d^{1+w(1,1,a)-a})$. This is achieved by designing a data structure that efficiently implements\nthe iterative Edge-Walk partial-coloring algorithm proposed by [LM15], utilizing the lazy update\nconcept. More recently, [BSZ24] demonstrates how the lazy update concept can be applied to a\ndynamic attention problem, achieving an amortized update time of $O(d^{w(1,1,a)-a})$."}, {"title": "Preliminary", "content": "Notations. For a matrix $A$, we denote its transpose by $A^T$. We use $I_{d\\times d}$ to denote a $d\\times d$ identity\nmatrix. We use $1_{n\\times d}$ to denote an $n\\times d$ matrix where all entries are ones and $0_{n\\times d}$ to denote an\n$n \\times d$ matrix where all entries are zeros. For a matrix $A$, the notation $A_{[i_1,i_2],[j_1,j_2]}$ refers to the\nsubmatrix of $A$ corresponding to the rows from $i_1$ to $i_2$ (including $i_1$ and $i_2$) and columns from $j_1$\nto $j_2$ (including $j_1$ and $j_2$). When we write $A_{*,[j_1,j_2]}$, it denotes the submatrix of $A$ that includes\nall rows and restricts the columns to those from $j_1$ to $j_2$. For two matrices $A, B$ of the same size,\nwe denote the Hadamard product of $A$ and $B$ by $A\\circ B$. We use $O^*(f(d))$ to hide $f(d)^{o(1)}$ factor.\nDefinitions and Facts\nWe now introduce some key definitions and key facts.\nDefinition 3.1. For three positive integers $d_1,d_2,d_3$, we use $T_{\\text{mat}}(d_1,d_2,d_3)$ to denote the time of\nmultiplying a $d_1 \\times d_2$ matrix with a $d_2 \\times d_3$ matrix.\nThe following fact shows that the order of $d_1, d_2, d_3$ only results in a constant factor difference.\nFact 3.2 ([BCS13, Bl\u00e413]). It holds that\n$T_{\\text{mat}}(d_1, d_2, d_3) = O(T_{\\text{mat}}(d_1, d_3, d_2)) = O(T_{\\text{mat}}(d_2, d_1, d_3)).$\nWe provide a definition of $w(\\cdot, \\cdot, \\cdot)$ here.\nDefinition 3.3. For $a,b,c > 0$, we use $d^{w(a,b,c)}$ to denote the time complexity of multiplying a $d^a \\times d^b$\nmatrix with a $d^b \\times d^c$ matrix. We define $w := w(1,1,1)$ as the exponent of matrix multiplication.\nWe use $\\tilde{a}$ to denote the dual exponent of matrix multiplication, which is the largest value such that\n$w(1, \\tilde{a}, 1) = 2 + o(1)$.\nIn other words, $w$ means that multiplying two $d \\times d$ matrices require time $O(d^w)$, and $\\tilde{a}$ is the\nlargest number such that we can multiply a $d \\times d^{\\tilde{a}}$ matrix with a $d^{\\tilde{a}} \\times d$ in the subquadratic time.\nLemma 3.4 ([ADW+24, WXXZ24, LG24]). Currently, we have $w \\approx 2.371$ and $\\tilde{a} \\approx 0.321.$"}, {"title": "Main Results", "content": "In this section, we present our principal findings. Our analysis demonstrates that SparseGPT (refer\nto Algorithm 1) attains the desired running time of $O(d^w +d^{2+a} + d^{1+w(1,1,a)-a})$ (Theorem 4.1). For\nthe sake of clarity, we assume that both the weight matrix $W$ and the input feature matrix $X$ are\nof dimensions $\\mathbb{R}^{d\\times d}$. However, our analysis remains valid for more general cases where $W \\in \\mathbb{R}^{n\\times d}$\nand $X \\in \\mathbb{R}^{d\\times N}$, provided that $n = O(d)$ and $N = O(d)$.\nTheorem 4.1 (Main result). Let lazy update block size $B = d^a$ for any $a \\in [0,1]$. Then Procedure\nSPARSEGPT in Algorithm 1 achieves the running time\n$O^*(d^w + d^{2+a} + d^{1+w(1,1,a)-a})$.\nProof. We split the analysis of running time in the following\nLine 6 takes time $O(d^2)$ to initiate $M$ and takes time $O(d^{1+a})$ to initiate $E."}, {"title": "Full Complexity Analysis", "content": "From now on, we let the lazy update block size $B = d^a$ for any $a \\in [0,1]$ without defining it in the\nstatement of lemmas. We first analyze the running time of the subroutine Procedure MASKSELECT\nin Algorithm 1.\nLemma 4.2. The running time of Procedure MASKSELECT in Algorithm 1 is $O(rd \\log d)$. Hence\nthe running time of Line 11 in Algorithm 1 over all iterations is $O(d^2 \\log d)$.\nProof. We first analyze the running time of Procedure MASKSELECT. We split the analysis of\nrunning time in the following\nLine 24 takes time $O(dr)$ to initialize $M'$.\nIn every iteration, Line 26 takes time $O(d)$ to initialize $w$. Since there are $r$ iterations, the\ntotal running time is $O(dr)$.\nIn every iteration, Line 27 takes time $O(1)$ to compute $(H_{s+k,s+k})^2$ and then takes time $O(d)$ to\ncompute $(w\\circ w)/a$. Since there are $r$ iterations, the total running time is $O(dr)$.\nIn every iteration, Line 28 takes time $O(d \\log d)$ to sort $w$ (without overwriting) and takes\ntime $O(d)$ to read the indices of top $(1 \u2013 p)d$ largest entries. Since there are $r$ iterations, the\ntotal running time is $O(r(d \\log d + d)) = O(rd \\log d)$.\nIn every iteration over $k$, 30 takes time $O(d)$ to update $M'$. Since there are $r$ iterations, the\ntotal running time is $O(dr)$.\nHence the Procedure MASKSELECT takes time\n$O(dr + dr + r + dr + rd \\log d + dr) = O(rd \\log d)$.\nNow, we analyze the running of Line 11 over all iterations in Algorithm 1. Let $r = B_s$. The\ntotal number calls to MASKSELECT is $d/B_s$. Hence the running over all iterations is\n$O((d/B_s) \\cdot (B_s d \\log d)) = O(d^2 \\log d)$.\nThus we complete the proof.\nNext, we analyze the running time of several key steps of Procedure SPARSEGPT in Algorithm 1.\nWe show that the inverse Hessian can be computed in time $O(d^w)."}, {"title": "Conclusion", "content": "We improved the complexity analysis of SparseGPT from $O(d^3)$ to $O(d^{2.53})$, using techniques from\nfast matrix multiplication and lazy updates. This tighter bound demonstrates that large language\nmodels can be pruned more efficiently than previously thought. Future work could explore further\nimprovements or extensions to other model compression algorithms."}]}