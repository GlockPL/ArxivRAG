{"title": "A Tighter Complexity Analysis of SparseGPT", "authors": ["Xiaoyu Li", "Yingyu Liang", "Zhenmei Shi", "Zhao Song"], "abstract": "In this work, we improved the analysis of the running time of SparseGPT [Frantar, Alistarh ICML 2023] from $O(d^3)$ to $O(d^\u03c9 + d^{2+a+o(1)} + d^{1+\u03c9(1,1,a)-a})$ for any $a \u2208 [0, 1]$, where $\u03c9$ is the exponent of matrix multiplication. In particular, for the current $\u03c9 \u2248 2.371$ [Alman, Duan, Williams, Xu, Xu, Zhou 2024], our running times boil down to $O(d^{2.53})$. This running time is due to the analysis of the lazy update behavior in iterative maintenance problems, such as [Deng, Song, Weinstein 2022, Brand, Song, Zhou ICML 2024].", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have been widely applied in many AI applications. Accelerating its inference speed is crucial to reduce the latency time during user usage. One of the recent brilliant works, SparseGPT [FA23], uses calibration data to prune the parameters of GPT-family models [BMR+20, ZRG+22], by using the optimal brain damage technique [LDS89, HS92]. Their algorithm (Algorithm 1) can prune at least 50% parameters with structure patterns, while the perplexity increase is negligible. Thus, SparseGPT can reduce the running time and GPU memory usage while keeping high performance for LLMs' applications.\nSparseGPT [FA23] claims their main pruning algorithm (Algorithm 1) takes running time $O(d^3)$ where d is the model's hidden feature dimensions. In our work, we improved the running time analysis to get a tighter running time complexity bound, i.e., $O(d^{2.53})$, for Algorithm 1. Formally, we state our main results as follows.\nTheorem 1.1 (Main result (Restatement of Theorem 4.1)). Let lazy update block size $B = d^a$ for any $a \u2208 [0,1]$. Then Procedure SPARSEGPT in Algorithm 1 achieves the running time\n$O^*(d^\u03c9 + d^{2+a} + d^{1+\u03c9(1,1,a)-a})$.\nFor the current $\u03c9 \u2248 2.371$, according to the Table 1 in [ADW+24], we should choose $a \u2248 0.5275$ in order to balance the terms $d^{2+a+o(1)}$ and $d^{1+\u03c9(1,1,a)-a}$. Then the running time boils down to $O(d^{2.53})$, which is better than $O(d^3)$ as claimed in [FA23].\nThe key technique in our analysis is to compute the complexity of SparseGPT by using lazy update. It comes from an interesting fact of fast rectangular matrix multiplication: the time complexity of multiplying a $d\u00d7d$ matrix by a $d \u00d7 1$ matrix is the same as the times complexity of multiplying a $d\u00d7d$ matrix by a $d\u00d7d^a$ matrix for any nonnegative $a \u2264 a$ where $a$ is the dual exponent of matrix multiplication and currently $a \u2248 0.321$ [WXXZ24, LG24, LGU18]. The lazy update has been widely used to speed up the training of neural networks [CLP+20, BPSW21, SZZ24] and maintain dynamic attention in large language models [BSZ24]. The high-level intuition of lazy update is that during iterative one-rank updates for a $d\u00d7d$ matrix, we use an external matrix $d\u00d7 d^a$ to save the input for updates while we conduct fast matrix multiplication only when $d^a$ updates happen. Based on the lazy update idea, we achieve our tighter complexity bound."}, {"title": "Related Work", "content": null}, {"title": "Model Acceleration and Pruning", "content": "Model acceleration is critical and urgently needed in many practical applications. There are many techniques for model acceleration. One line of work is to change model architecture to sup-port fast inference, e.g., Mamba [GD23, DG24], PolySketchFormer [KMZ23], Linearizing Trans-formers [ZBKR24, MVK+24], Hopfield Model [HCW+24, HCL+24, WHHL24, XHH+24, HLSL24,WHL+24, HYW+23] and so on. Another line of work is accelerating model computation on systemlevel, e.g., FlashAttetnion [DFE+22, Dao23, SBZ+24, SY24], parallel decoding [SSU18], quantization [ZLC+24, TZZ+24, LTT+24] and so on. To accelerate LLMs training and inference, there isa line of work to approximate attention matrix computation [AS23, AS24, HJK+24, ZHMK24,LSSZ24, PMN+23, CTWC24, LSSY24, LLS+24,GSWY23] in almost linear time. Some spe-cific technique is developed to accelerate LLMs generation, e.g., KV-Cache compression [GZL+23,LDLG23, XTC+23, ZSZ+24, LWD+23, DYZ+24, SCY+24] and speculative decodings [CLG+24,LCH+24, SCY+24, ESL+24]."}, {"title": "Lazy Update", "content": "In recent years, the lazy update idea has emerged as a crucial technique to effectively and effi-ciently solve various optimization problems including linear programming [CLS19, Bra20, BLSS20,JSWZ21, SY21, LSZ+23], semi-definite programming [JKL+20, HJS+22, GS22, SYZ23], empiri-cal risk minimization [LSZ19, GSZ23, QSZZ23], cutting plane methods [JLSW20], neural networktraining [CLP+20, BPSW21, SZZ24], discrepancy minimization [DSW22], dynamic attention prob-lems [BSZ24] and so on.\nWe highlight several previous papers that share similar running time complexity. For compar-ison, we assume the input size of these problems is dominated by a parameter d. In the line ofdevelopments of fast linear program solvers, [CLS19] first introduced the lazy update idea to de-velop efficient dynamic inverse structures for implementing interior point methods in solving linearprogramming problems, and it achieves the running time $O^*(d^\u03c9 + d^{2.5-a/2} + d^{1.5+a})$. [SY21] uses adifferent method to achieve linear programming in the same complexity $O^*(d^\u03c9 + d^{2.5-a/2} + d^{1.5+a})$by incorporating the sparse sketching matrix to speed up the online matrix-vector multiplication.[JSWZ21] improves the linear programming to $O^*(d^\u03c9 + d^{2.5-a/2} + d^{1.5+a\u2212\u00e3/2} + d^{0.5+a+(\u03c9\u22121)\u00e3})$ time where $a \u2208 [0, a]$ by designing a two-level lazy update framework for efficiently maintaining a pro-jection matrix. Later [LSZ19] generalizes the algorithm for linear programming to empirical riskminimization, which also takes $O^*(d^\u03c9 + d^{2.5-a/2} + d^{1.5+a})$ time. Different from [CLS19], whichemploys a stochastic central path method that updates weights using a random sparse vector,[LSZ19] introduces a robust deterministic central path method. Additionally, [LSZ19] proposes anefficient data structure capable of handling updates efficiently even when the weight update vectoris dense. Further, [QSZZ23] uses a different method to achieve empirical risk minimization in thesame complexity $O^*(d^\u03c9 + d^{2.5-a/2} + d^{1.5+a})$ via online projection matrix-vector multiplication.\nThere are numerous other applications of the lazy update technique. For instance, [DSW22]employs this approach to solve a subroutine of the discrepancy minimization problem in $O^*(d +d^{2+a} + d^{1+\u03c9(1,1,a)-a})$. This is achieved by designing a data structure that efficiently implementsthe iterative Edge-Walk partial-coloring algorithm proposed by [LM15], utilizing the lazy updateconcept. More recently, [BSZ24] demonstrates how the lazy update concept can be applied to adynamic attention problem, achieving an amortized update time of $O(d^{\u03c9(1,1,a)-a})$."}, {"title": "Preliminary", "content": "Notations. For a matrix A, we denote its transpose by $A^T$. We use $I_{dxd}$ to denote a $d\u00d7d$ identitymatrix. We use $1_{nxd}$ to denote an $n\u00d7 d$ matrix where all entries are ones and $O_{nxd}$ to denote an$n \u00d7 d$ matrix where all entries are zeros. For a matrix A, the notation $A_{[i_1,i_2],[j_1,j_2]}$ refers to thesubmatrix of A corresponding to the rows from $i_1$ to $i_2$ (including $i_1$ and $i_2$) and columns from $j_1$to $j_2$ (including $j_1$ and $j_2$). When we write $A_{*,[j_1,j_2]}$, it denotes the submatrix of A that includesall rows and restricts the columns to those from $j_1$ to $j_2$. For two matrices A, B of the same size,we denote the Hadamard product of A and B by $A o B$. We use $O^*(f(d))$ to hide $f(d)^{o(1)}$ factor.\nDefinitions and Facts\nWe now introduce some key definitions and key facts.\nDefinition 3.1. For three positive integers $d_1,d_2,d_3$, we use $T_{mat}(d_1,d_2,d_3)$ to denote the time ofmultiplying a $d_1 \u00d7 d_2$ matrix with a $d_2 \u00d7 d_3$ matrix.\nThe following fact shows that the order of $d_1, d_2, d_3$ only results in a constant factor difference.\nFact 3.2 ([BCS13, Bl\u00e413]). It holds that\n$T_{mat} (d_1, d_2, d_3) = O(T_{mat}(d_1, d_3, d_2)) = O(T_{mat}(d_2, d_1, d_3))$.\nWe provide a definition of $\u03c9(\u00b7,\u00b7, \u00b7)$ here.\nDefinition 3.3. For $a, b, c > 0$, we use $d^{\u03c9(a,b,c)}$ to denote the time complexity of multiplying a $d^a \u00d7d^b$matrix with a $d^b \u00d7 d^c$ matrix. We define $\u03c9 := \u03c9(1,1,1)$ as the exponent of matrix multiplication.We use $a$ to denote the dual exponent of matrix multiplication, which is the largest value such that$\u03c9(1, a, 1) = 2 + o(1)$.\nIn other words, $\u03c9$ means that multiplying two $d \u00d7 d$ matrices require time $O(d^\u03c9)$, and $a$ is thelargest number such that we can multiply a $d \u00d7 d^a$ matrix with a $d^a \u00d7 d$ in the subquadratic time.\nLemma 3.4 ([ADW+24, WXXZ24, LG24]). Currently, we have $\u03c9 \u2248 2.371$ and $a \u2248 0.321$."}, {"title": "Main Results", "content": "In this section, we present our principal findings. Our analysis demonstrates that SparseGPT (referto Algorithm 1) attains the desired running time of $O(d^\u03c9 +d^{2+a} + d^{1+\u03c9(1,1,a)\u2212a})$ (Theorem 4.1). For the sake of clarity, we assume that both the weight matrix W and the input feature matrix X areof dimensions $R^{d\u00d7d}$. However, our analysis remains valid for more general cases where $W \u2208 R^{n\u00d7d}$and $X \u2208 R^{d\u00d7N}$, provided that $n = O(d)$ and $N = O(d)$.\nTheorem 4.1 (Main result). Let lazy update_block size $B = d^a$ for any $a \u2208 [0,1]$. Then ProcedureSPARSEGPT in Algorithm 1 achieves the running time\n$O^*(d^\u03c9 + d^{2+a} + d^{1+\u03c9(1,1,a)-a})$.\nProof. We split the analysis of running time in the following\n\u2022 Line 6 takes time $O(d^2)$ to initiate M and takes time $O(d^{1+a})$ to initiate E."}, {"title": "Full Complexity Analysis", "content": "From now on, we let the lazy update block size $B = d^a$ for any $a \u2208 [0,1]$ without defining it in thestatement of lemmas. We first analyze the running time of the subroutine Procedure MASKSELECTin Algorithm 1.\nLemma 4.2. The running time of Procedure MASKSELECT in Algorithm 1 is $O(rdlog d)$. Hence the running time of Line 11 in Algorithm 1 over all iterations is $O(d^2 log d)$.\nProof. We first analyze the running time of Procedure MASKSELECT. We split the analysis ofrunning time in the following\n\u2022 Line 24 takes time $O(dr)$ to initialize $M'$.\n\u2022 In every iteration, Line 26 takes time $O(d)$ to initialize w. Since there are r iterations, thetotal running time is $O(dr)$.\n\u2022 In every iteration, Line 27 takes time $O(1)$ to compute $(H_{s+k})^{2}$ and then takes time $O(d)$ tocompute $(w ow)/a$. Since there are r iterations, the total running time is $O(dr)$.\n\u2022 In every iteration, Line 28 takes time $O(dlogd)$ to sort w (without overwriting) and takestime $O(d)$ to read the indices of top $(1 \u2013 p)d$ largest entries. Since there are r iterations, thetotal running time is $O(r(dlog d + d)) = O(rdlog d)$.\n\u2022 In every iteration over k, 30 takes time $O(d)$ to update $M'$. Since there are r iterations, thetotal running time is $O(dr)$.\nHence the Procedure MASKSELECT takes time\n$O(dr + dr + r + dr + rd log d + dr) = O(rd log d)$.\nNow, we analyze the running of Line 11 over all iterations in Algorithm 1. Let $r = B_s$. Thetotal number calls to MASKSELECT is $d/B_s$. Hence the running over all iterations is\n$O((d/B_s) \u00b7 (B_s d log d)) = O(d^2 log d)$.\nThus we complete the proof."}, {"title": "Conclusion", "content": "We improved the complexity analysis of SparseGPT from $O(d^3)$ to $O(d^{2.53})$, using techniques fromfast matrix multiplication and lazy updates. This tighter bound demonstrates that large languagemodels can be pruned more efficiently than previously thought. Future work could explore furtherimprovements or extensions to other model compression algorithms."}]}