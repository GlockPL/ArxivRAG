{"title": "A Simple Model of Inference Scaling Laws", "authors": ["Noam Levi"], "abstract": "Neural scaling laws have garnered significant interest due to their ability to predict model performance as a function of increasing parameters, data, and compute. In this work, we propose a simple statistical ansatz based on memorization to study scaling laws in the context of inference, specifically how performance improves with multiple inference attempts. We explore the coverage, or pass@k metric, which measures the chance of success over repeated attempts and provide a motivation for the observed functional form of the inference scaling behavior of the coverage in large language models (LLMs) on reasoning tasks. We then define an \"inference loss\", which exhibits a power law decay as the number of trials increases, and connect this result with prompting costs. We further test our construction by conducting experiments on a simple generative model, and find that our predictions are in agreement with the empirical coverage curves in a controlled setting. Our simple framework sets the ground for incorporating inference scaling with other known scaling laws.", "sections": [{"title": "Introduction", "content": "Advancements in deep learning have demonstrated that the performance of neural networks scales predictably as a function of model size, data size, and computational resources [Hestness et al., 2017, Kaplan et al., 2020a, Rosenfeld et al., 2020, Henighan et al., 2020a]. These trends, known as neural scaling laws, have motivated research into understanding how scaling influences model performance in a range of domains, in particular, Large Language Models (LLMs) [Brown et al., 2020, Hoffmann et al., 2022].\nHowever, scaling during inference\u2014the process by which a trained model makes predictions on new data-has received less attention. Recent works have shown empirically that LLMs can gain substantial benefits from repeated prompts to perform better on difficult tasks such as coding and formal proofs, where verification of the correct answer can be done [Brown et al., 2024, Snell et al., 2024, Bansal et al., 2024]. These works demonstrate that the performance of weaker models can be amplified without further training, by simply repeating inference trials. A natural question then arises:\nCan we interpret, or predict the inference scaling behavior of a model with repeated attempts?\nTo answer this question, we propose a simple toy model that isolates the inference scaling laws which dictate how certain performance metrics improve as a function of the number of inference attempts. Inspired by the work of Hutter [2021], which introduced a model to study scaling behavior for memorization and generalization, we devise a simple setting to capture the effect of repeated inference attempts, focusing on the coverage metric, also known as pass@k.\nIn this work, we present analytical predictions for coverage from a probabilistic perspective and demonstrate how inference improves with the number of repeated trials in a predictable way, which matches the observed behavior in Brown et al. [2024] and Snell et al. [2024]. We use two different approaches to obtain the predicted pass@k, and highlight the connection between coverage and total inference cost. Additionally, we define a simple \"inference loss\", similar to the familiar test loss, but allowing for repeated trials, and predict its scaling. Lastly, we verify our predictions empirically by appealing to a simple generative model, a Variational Autoencoder (VAE), which is trained to generate reconstructions of its training data by sampling from a latent space with an associated temperature, mimicking some of the complex generative properties of LLMs. Given that our results are isolated from the effects of other neural scaling laws, they could be incorporated into a broader exploration to find the optimal train/inference point. In particular, we hope this work sets the ground for exploring the optimal trade-off between training and inference attempts, such that the total performance, as well as cost, is minimized."}, {"title": "Related Work", "content": "Scaling laws for neural networks have been extensively studied in recent years. Empirical research has shown that error rates decrease predictably as a function of increased data, parameters, and compute, following power-law relationships. Notable contributions in this space include work by Kaplan et al. [2020b], who demonstrated consistent scaling behavior across language models, and Henighan et al. [2020b], who extended these ideas to multimodal models, as well as Cabannes et al. [2024], Maloney et al. [2022], Bordelon et al. [2020], Spigler et al. [2020], Caponnetto and De Vito [2007], Steinwart et al. [2009], Fischer and Steinwart [2020], Cui et al. [2021] who derived scaling laws for solvable yet sufficiently complex models, ranging from generalized linear regression on random feature models to kernel ridge regression. While most scaling laws focus on training, the study of inference scaling remains under-explored. Our work tries to fill this gap by studying how performance improves with repeated inference attempts."}, {"title": "Memorizing Ansatz", "content": "In the following, we first briefly recall the simplest model which produces the known data scaling law prediction by appealing to a memorizing construction, then consider our proposed model for repeated inference attempts. The Hutter model is a probabilistic framework originally introduced to study the scaling of learning curves, focusing on the relationship between memorization and generalization. It assumes a model which perfectly memorizes features during training, allowing it to correctly match sets of features and labels {i, yi}, such that only unseen features can incur an error. The set of features is assumed to follow a Zipf power law decay with parameter \u03b1, where the probability of encountering feature i is \u03b8i \u00d7 i\u22121\u2212\u03b1 and decreases with its rank. For n training samples, the expected single feature error is\n$E_n = E[E_i] = \\sum_{i=1}^{\\infty} \\theta_i (1- \\theta_i) \\approx n^{-\\beta}, \\quad \\beta = \\frac{1}{1 + \\alpha}$"}, {"title": "Perfect Sample Memorization, Imperfect Predictions", "content": "In contrast to the Hutter model, we focus on a scenario where all samples have been memorized, hence there is no notion of test error coming from unseen data. Instead, failure during inference or data generation could arise from the need to follow a sequence of steps to reach the correct answer."}, {"title": "Power Law Distributions Predict Pass@k for LLMs", "content": "To construct the distribution of failure probabilities pi, we assume that different samples may have different inference complexity levels, incorporating some \"easy\" and some \"difficult\" samples with respect to the inference model. One way to model the different complexities is done by appealing to the so called Beta distribution. We think of $p = p_i$ as a random variable, drawn from $p \\sim Beta(\\alpha, \\beta)$, where $ \\alpha \\in \\mathbb{R}^+$ controls how often we encounter \"easier\" problems, where smaller \u03b1 pushes the distribution mass towards zero, while $\u03b2 \\in \\mathbb{R}^+$ dictates how often we encounter \"harder\" problems. Namely, a lower \u03b2 parameter increases the distribution mass towards the right tail (high failure probabilities). The $Beta(\\alpha, \\beta)$ PDF is given explicitly by\n$Beta(\\alpha, \\beta; p) = \\frac{p^{-1+\\alpha}(1-p)^{-1+\\beta}}{B(\\alpha, \\beta)}$"}, {"title": "Interpretability", "content": "The result in Eq. (7) not only allows one to predict a threshold for inference in order to get an increase in model performance on difficult problems, but could also offer some interpretation for the difficulty of tasks with respect to the trained model. We can gain some insights regarding the real-world models by fitting the pass@k metric according to Eq. (6) to the ones given in Brown et al. [2024], and attempt to interpret the properties of the test data from the parameters \u03b1 and \u03b2. In particular, note that the ratio $\\frac{\\alpha}{\\alpha+\\beta}$ gives the mean of the Beta distribution, which represents the average failure probability across the samples. If \u03b1 > \u03b2, the mean failure probability is high (i.e., most samples are harder). On the other hand, if \u03b1 < \u03b2, the mean failure probability is low, implying that most samples are easy. Furthermore, the denominator \u03b1 + \u03b2 governs the concentration of the distribution, such that when \u03b1 + \u03b2 is large, the failure probabilities pi are more tightly clustered around the mean (more homogeneity in difficulty), and conversely for small \u03b1 + \u03b2.\nFrom Fig. 1, we can see that the typical values for the \"harder\" problem parameter \u03b2 ~ 0.35, while the tail parameter is 2 < \u03b1 < 20. This implies that most of the problems in the datasets used to measure the pass@k curves were indeed difficult in terms of the model's perception, while the existence of a left tail implies that the easy samples are covered quickly with less than 100 trials. This is reflected in the right panel of Fig. 1.\nAnother point of interest is that the average performed in Eq. (6) can be thought of as a Laplace transform from the variable $\\sigma = \\log (1/p)$ to the trials space k, in the sense that\n$\\mathcal{F}(k) = \\langle p^{k} \\rangle = \\int_0^{\\infty} d \\sigma \\frac{e^{-\\sigma k} e^{-1+\\alpha} (1-e^{-\\sigma})^{-1+\\beta}}{B(\\alpha, \\beta)} = \\int_0^{\\infty} d \\sigma e^{-\\sigma k} f(\\sigma)$"}, {"title": "Correlated Trials and Effective k Approach", "content": "In the previous section, we showed that correlated samples drawn iid from a varying complexity distribution can effectively describe the pass@k metric for memorizing models. Here, we take a converse approach, where do not assume that samples are correlated through their failure rate distribution, but instead that trials themselves are correlated.\nOne can conjecture that dependencies between trials arise due to the internal model structure and the data itself. To capture these correlations, we suggest a model where the correlation between trials decays as a power law, implying that successive trials become less independent as we increase the number of trials.\nIn order to incorporate the correlation between trials, we define the notion of an effective number of independent trials, denoted keff. This adjusts the original k to account for the decay in trial independence. The correlation between trials is modeled via a power-law decay in the eigenvalues of the correlation matrix, such that the effective number of independent trials is given by\n$k_{eff} = \\sum_{i=1}^k i^{-\\kappa} = kH_k(\\kappa) \\approx \\frac{k}{\\kappa - 1} - \\frac{1}{2} k^{-\\kappa} + \\zeta(\\kappa), \\quad \\kappa \\in \\mathbb{R}_{\\geq 0}$"}, {"title": "Connection to Compute Scaling", "content": "Here, we would like to translate our results from the attempts variable to the inference cost. A natural proxy for the inference cost may be the number of required Floating Point Operations Per Second (FLOPS)\u00b9. For concreteness, we adapt the total inference cost formula suggested in Brown et al. [2024], given by\ntotal inference FLOPS \u2248 num prompt tokens \u00d7 FLOPS per token\n+ num decoded tokens \u00d7 FLOPS per token \u00d7 num completions\n\u2192 C = Np \u00d7 F + Nd \u00d7 F \u00d7 k,"}, {"title": "Experiments on a Simple Generative Model", "content": "To further validate some of our analytical understanding in a controllable setting, we perform a series of exper-iments in which we train a simple generative model to reconstruct images taken from Fashion-MNIST [Xiao et al., 2017]. Our goal is to connect the theoretical memorizing model and the behavior of more complex generative models by attempting to accurately reconstruct \"memorized\" examples, heuristically shown in Fig. 4.\nTo do this, we train a variational autoencoder (VAE) with a temperature parameter to study how errors propagate over multiple trials and to compare empirical pass@k with theoretical predictions under correlated trials. We refer to this as the VAE reconstruction task.\nTo quantify the error probability of the model over multiple samples, we define the error per sample using the norm of the difference between the reconstructed and original image:\n$error(i) = \\frac{||y_i - \\hat{y_i}||}{||y_i||}$"}, {"title": "Conclusion", "content": "In this paper, we have proposed a simple statistical explanation for a so-called inference scaling law, which describes the scaling of the coverage (pass@k) metric with number of repeated attempts for generative models. We presented two possible models which lead to inference scaling: One based on introducing a sample space distribution of \"easy\" and \"difficult\" problems, and the other on an effective Zipf-like correlation structure between trials. Using these simple constructions, we were able to derive analytical predictions for the pass@k, as well the test loss as a function of repeated attempts, which we dubbed the inference loss. We then verified our predictions empirically both through previous experimental results for LLMs and for a simple generative VAE construction.\nWe stress that the merit of our construction is in its simplicity, and there are many other models who can give rise to the same functional behavior. We view this as a positive rather than a negative, since it means that this simple model captures a universal behavior, which should not depend much on the modeling itself. For instance, another way to arrive at a similar scaling law would be to choose a different modeling for the failure distribution, based perhaps on program length, and introducing the notion of a distribution of program lengths corresponding to different samples, similar to Ringel and de Bem [2018]. In the end, this type of construction will have a similar interpretation in terms of task complexity w.r.t the model.\nWe believe our toy model offers a simple yet effective phenomenological framework for understanding how inference quality improves with more opportunities to predict correctly. Future work could extend this framework to more complex models, including applying similar methodology as Maloney et al. [2022] to generalized linear regression, kernel regression and neural networks, and investigate how it interacts with existing scaling laws based on model size and training data."}]}