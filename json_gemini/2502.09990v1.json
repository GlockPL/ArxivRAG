{"title": "X-Boundary: Establishing Exact Safety Boundary to Shield LLMs from Multi-Turn Jailbreaks without Compromising Usability", "authors": ["Xiaoya Lu", "Dongrui Liu", "Yi Yu", "Luxin Xu", "Jing Shao"], "abstract": "Despite the rapid development of safety alignment techniques for LLMs, defending against multi-turn jailbreaks is still a challenging task. In this paper, we conduct a comprehensive comparison, revealing that some existing defense methods can improve the robustness of LLMs against multi-turn jailbreaks but compromise usability, i.e., reducing general capabilities or causing the over-refusal problem. From the perspective of mechanism interpretability of LLMs, we discover that these methods fail to establish a boundary that exactly distinguishes safe and harmful feature representations. Therefore, boundary-safe representations close to harmful representations are inevitably disrupted, leading to a decline in usability. To address this issue, we propose X-Boundary to push harmful representations away from boundary-safe representations and obtain an exact distinction boundary. In this way, harmful representations can be precisely erased without disrupting safe ones. Experimental results show that X-Boundary achieves state-of-the-art defense performance against multi-turn jailbreaks, while reducing the over-refusal rate by about 20% and maintaining nearly complete general capability. Furthermore, we theoretically prove and empirically verify that X-Boundary can accelerate the convergence process during training.", "sections": [{"title": "1. Introduction", "content": "As large language models (LLMs) have demonstrated impressive abilities (OpenAI, 2024; AI@Meta, 2024; Team,"}, {"title": "2. Comparison of Existing Defense Methods Against Multi-Turn Jailbreaks", "content": "To the best of our knowledge, we are the first to conduct a comprehensive evaluation of classic defense approaches against multi-turn jailbreaks, considering both defense robustness and impact on usability. Although previous studies (Ren et al., 2024b; Jiang et al., 2024) have employed Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) in multi-turn defense scenarios, many other defense methods effective against single-turn jailbreaks, such as Gradient Ascent (GA) and Circuit Breaking (CB), have been overlooked and have not been thoroughly compared. In Section 2.1, we outline the process of constructing training data, reproducing SFT and DPO, and adapting GA and CB for multi-turn defense scenarios. In Section 2.2, we present and analyze the evaluation results, revealing the shortcomings of existing defense methods in balancing robustness and usability."}, {"title": "2.1. Adaption and Evaluation of Single-Turn Defense Methods Against Multi-Turn Jailbreaks", "content": "We compare the following defense methods against multi-turn jailbreak (Ren et al., 2024b; Jiang et al., 2024) on Llama-3-8B-Instruct (AI@Meta, 2024), Qwen2.5-7B-Chat (Yang et al., 2024a), and Mistral-7B-Instruct-v0.2 (Jiang et al., 2023):\n\u2022 SFT (Ren et al., 2024b): fine-tuning LLMs using harmful queries as inputs and refusal answers as supervised labels directly.\n\u2022 DPO (Rafailov et al., 2024; Jiang et al., 2024): aligning LLMs using harmful queries as inputs, harmful answers as rejected responses, and refusal answers as chosen responses.\n\u2022 GA (Zhang et al., 2024b; Lu et al., 2024a): unlearning harmful knowledge by training with gradient ascent optimization methods.\n\u2022 CB (Zou et al., 2024): remapping the representations of harmful knowledge to desired targeted representations.\nConstruct multi-turn defense datasets. We construct the multi-turn defense training datasets based on SafeMTData (Ren et al., 2024b). SafeMTData consists of 1680 safe multi-turn dialogues for the safety alignment of LLMs in multi-turn interactions. For SFT, we directly exploit SafeMTData as a multi-turn training dataset following Ren et al. (2024b). For DPO, we curate harmful responses to the harmful multi-turn queries in SafeMTData and constructed a multi-turn preference dataset following Jiang et al. (2024). For CB and GA, to remove harmful knowledge that could be elicited through multi-turn attacks, we add pairs of harmful queries from SafeMTData along with the curated harmful responses into their respective defense training datasets. (Zhang et al., 2024b; Zou et al., 2024) More details about data construction are illustrated in Appendix B.1 and training settings of"}, {"title": "2.2. Experimental Results and Analysis", "content": "Existing methods fail to strike a balance between robustness and usability. Table 1 shows that existing methods"}, {"title": "3. X-Boundary: Optimize Exact Boundary to Balance Defense Robustness and Usability", "content": "In this section, we propose X-Boundary to balance robustness against multi-turn jailbreaks and usability by explicitly formulating the distinction boundary. Section 3.1 analyzes the essential mechanism of decline in usability. Section 3.2 introduces the optimization objective of X-Boundary. Section 3.3 theoretically proves that X-Boundary may ease the learning difficulty and contribute to fast learning."}, {"title": "3.1. The Imprecise Distinction Boundary of Existing Multi-Turn Defense Methods.", "content": "Notations. Give an input data point $x$, $R_M(x)$ denotes its feature representations encoded by LLMs M. ${x_i}_{i=1}^N$ and ${R_M(x_i)}_{i=1}^N$ denote a set of multiple data points and representations, respectively. In particular, $x^h$ represents a harmful Query and its corresponding harmful Answer (QA"}, {"title": "3.2. Explicit Formulation for Distinction Boundary", "content": "We propose X-Boundary to explicitly formulate the distinction boundary between safe and harmful representations. The key idea is to push harmful representations far away from boundary-safe representations through explicit loss function, such that harmful representations can be effectively and precisely erased without disrupting safe ones. In this way, a balance between defense robustness and LLM usability can be achieved.\nSpecifically, we construct a separate set $D_s$ for separating harmful and boundary-safe representations, an erase set $D_e$ to contain harmful knowledge that should be erased, and a retain set $D_r$ for preserving safe knowledge related to the usability of LLMs. To this end, $D_r$ includes safe QA pairs $\\{x^s\\}_{i=1}^N$, boundary-safe QA pairs $\\{x^b\\}_{i=1}^N$, and refusal responses to harmful queries $\\{x^r\\}_{i=1}^N$. $D_e$ consists of harmful QA pairs: $D_e = \\{x^h\\}_{i=1}^N$. $D_s$ contains pairs of $x^b$ and $x^r$: $D_s = \\{(x^b, x^r)\\}_{i=1}^N$.\nTo explicit formulate a precise distinction boundary, we propose separate loss $L_s$ to increase the distance between harmful representations $\\{R_M(x^h)\\}_{i=1}^N$ and boundary-safe representations $\\{R_M(x^b)\\}_{i=1}^N$. Since most $\\{R_{M_{ref}}(x^h)\\}_{i=1}^N$ will be remapped to $\\{R_M(x^r)\\}_{i=1}^N$ due to the following erasure operation, we can separate them by directly optimizing $R_M(x^b)$ to be orthogonal to $R_M(x^r)$ as shown in Fig. 3:\n$L_s = \\frac{1}{|S_s|} \\sum_{i=1}^{|S_s|} \\text{ReLU}(\\cos (R_M(x_i^b), R_M (x_i^r)))$ (1)"}, {"title": "3.3. Theoretical Analysis of X-Boundary", "content": "In this subsection, we theoretically analyze the convergence rate of LLM from the perspective of the optimal transport"}, {"title": "4. Experiments", "content": null}, {"title": "4.1. Experimental Settings", "content": "To ensure fairness in comparison and consistency in experimental settings, we also implement X-Boundary on Llama-3-8B-Instruct, Qwen2.5-7B-Chat, and Mistral-7B-Instruct-v0.2, and evaluate it using the benchmarks described in Section 2.1. Additionally, to assess the effectiveness of X-Boundary across different sizes of LLMs, we implement it on Qwen2.5-14B-Chat. To construct the Separate Set, we sample 500 boundary-safe prompts from OR-Bench-80K (Cui et al., 2024), which have been filtered to avoid data contamination with the test set of OR-Bench. Next, we use GPT-4o to generate safe and helpful responses for these prompts, thus we get boundary-safe QA pairs. The retain set consists of our collected boundary-safe QA pairs, UltraChat (Ding et al., 2023), and refusal data points generated by the trained LLMs themselves. The erase set includes the harmful QA pairs for single-turn defense used in Zou et al. (2024)"}, {"title": "4.2. Main Results", "content": "The explicit formulation for boundary contributes to the precise distinction between harmful and safe representations. To investigate the effect of the explicit formulation for distinction boundary, we visualize the representation distribution of X-Boundary and without X-Boundary. Fig. 5 shows that, without X-Boundary, the boundary-safe representations close to harmful representations are mistakenly regarded as harmful ones. This demonstrates that LLMs fail to learn a boundary that exactly distinguishes safe and harmful representations, which supports our motivation of explicitly formulating the distinction boundary. With X-Boundary, harmful representations and boundary-safe representations are clearly separated as shown in Fig. 5, verifying that the proposed explicit formulation contributes to establishing a precise distinction boundary. Please refer to Appendix A.4 and Appendix A.5 for more detailed visualization and analysis of the representation distribution.\nX-Boundary maintains the lowest over-refusal rate while achieving SOTA defense against multi-turn jailbreaks. With a precise distinction boundary, X-Boundary reduces the ASR of ActorAttack by more than 40% while maintaining the increase in over-refusal rate on OKTest within 5% across three LLMs, as shown in Table 1. Specifically, on Llama-3-8B-Instruct, CB and X-Boundary both achieve the lowest ASR against ActorAttack, but X-Boundary demonstrates an average over-refusal rate that is lower by 20.05%. Similarly, on Qwen2.5-7B-Chat, X-Boundary's average over-refusal rate is 58.50% lower than GA, which achieves the lowest ASR against Crescendo.\nX-Boundary rarely declines general capability. Table 1 shows that the decline of general capabilities caused by X-Boundary is generally no more than 0.5% compared to vanilla models, across the domains of general knowledge, mathematical ability, and coding ability. In contrast to SFT, which causes a 7% reduction in coding ability for Mistral-7B-Instruct-v0.2, X-Boundary achieves a lower ASR without compromising coding capability.\nX-Boundary successfully strikes a balance between robustness and usability. As a supplement to Table 1, Fig. 6 intuitively illustrates the trade-off between the ASR against multi-turn jailbreaks and the over-refusal rate. As the training process advances, the ASR steadily decreases, whereas the over-refusal rate progressively increases. Considering the two metrics comprehensively, X-Boundary appears in the lower-left corner of Fig. 6, indicating that it achieves a better balance compared to the baseline methods. In the same way, Fig. 7 demonstrates that X-Boundary also"}, {"title": "4.3. Ablation Study", "content": "We conduct ablation studies on the impact of multi-turn defense data, boundary-safe data, and separate loss. The results are illustrated in Table 3. More results on Llama-3-8B-Instruct and Mistral-7B-Instruct-v0.2 are shown in Appendix A.2.\nMulti-turn defense data contribute to the reduction of ASR but intensify the over-refusal problem. With the multi-turn defense data described in Section 2.1 added into erase set, the ASR of ActorAttack is reduced from 63.00% to 15.50% on Qwen2.5-7B-Chat. However, the over-refusal rates in OR-Bench and PHTest increase about 30.00%. This highlights that over-refusal is a critical issue in multi-turn defense tasks, which cannot be overlooked and urgently requires resolution.\nBoundary-safe data can partially mitigate the over-refusal issue. Boundary-safe QA pairs added to the retain set significantly reduce the over-refusal rate on OR-Bench and PHTest but show limited effectiveness on XSTest and OKTest. This may be because the boundary-safe QA pairs are synthesized by LLMs, leading to effectiveness on OR-Bench and PHTest, which also use synthetic data for testing. In contrast, the test queries in XSTest and OKTest are manually crafted and may differ in distribution from the synthetic data, making it difficult to achieve effective generalization.\nSimply adjusting the size of boundary-safe data can not effectively balance ASR and over-refusal rate. Increasing the size of boundary-safe data can reduce the over-refusal rate, but it also leads to a sharp increase in ASR against multi-turn jailbreaks. Please see Appendix A.3 for more detailed results.\nSeparate loss can further reduce the over-refusal rate. Unlike simply adding boundary-safe data, separate loss markedly reduces the over-refusal rate on both manually crafted and synthetically constructed benchmarks. Since the boundary-safe data shares the same source as OR-Bench, simply adding data is sufficient to reduce the over-refusal rate to a very low level, leaving little room for separate loss to make a noticeable impact. However, in the other three benchmarks, separate loss further reduces the over-"}, {"title": "5. Related Work", "content": "Multi-turn attack. Several studies have explored the safety risks in multi-turn dialogue scenarios (Wang et al., 2025; Tong et al., 2024). For instance, Li et al. (2024a) employs human red teamers to uncover vulnerabilities in LLMs when subjected to multi-turn attacks. Jiang et al. (2024) crafts 40 multi-turn scenarios in which malicious intent is concealed under the guise of preventing harm. Yu et al. (2024), Zhou et al. (2024b) and Liu et al. (2024b) generate multi-turn jailbreak queries by breaking down the original malicious query into multiple less harmful sub-questions. Ren et al. (2024b) captures multi-turn attack clues by modeling a network of semantically linked actors. Yang et al. (2024b) and Russinovich et al. (2024) dynamically adjust the attack query based on the contextual feedback from victim LLMs, gradually steering benign initial queries toward more harmful topics throughout the conversation. In this paper, we evaluate the defense robustness of existing methods and X-Boundary against three types of multi-turn jailbreak attacks: ActorAttack (Ren et al., 2024b), RedQueen (Jiang et al., 2024), Crescendo (Russinovich et al., 2024).\nDefenses for LLMs. Although defense methods for multi-turn jailbreak attacks are less explored in the literature, some existing approaches have proven effective against various single-turn attacks and have the potential to be adapted for multi-turn scenarios. These defense methods can be classified into the following categories: training LLMs to refuse harmful queries (Bai et al., 2022; Rafailov et al., 2024; Ouyang et al., 2022a; Yuan et al., 2024), training LLMs to prioritize safe instructions (Lu et al., 2024b; Wallace et al., 2024; Zhang et al., 2023), unlearning and editing harmful knowledge (Lu et al., 2024a; Zhang et al., 2024b; Ren et al., 2024a; Qian et al., 2024a), prompt engineering (Xie et al., 2023; Zheng et al., 2024), and implementing input and output guardrails (Inan et al., 2023; Dubey et al., 2024) such as jailbreak detection (Hu et al., 2024a; Jain et al., 2023) input perturbation (Cao et al., 2023; Robey et al., 2023; Liu et al., 2024c). Several studies (Li et al., 2024b; Zou et al., 2024; 2023; Qian et al., 2024b; Zhang et al., 2024a) also"}, {"title": "Decline in usability caused by defense methods.", "content": "We assess the impact of defense methods on usability from two aspects: general capability degradation and over-refusal. General capability degradation, commonly known as the \"alignment tax\" (Ouyang et al., 2022b) phenomenon, has garnered widespread attention and has been extensively discussed in technical reports on LLMs (Dubey et al., 2024; Inan et al., 2023; Ren et al., 2024b; Li et al., 2024b; Hu et al., 2024b). Over-refusal refers to the unreasonable rejection of safe queries by LLMs (Varshney et al., 2023; Zhao et al., 2024; Zou et al., 2023; Arditi et al., 2024; Cao et al., 2024). Bianchi et al. (2023) discover that excessive safety-tuning makes LLMs refuse entirely safe prompts if they superficially resemble unsafe ones. R\u00f6ttger et al. (2023), Shi et al. (2024), Cui et al. (2024), and An et al. (2024) employ linguistic techniques or automatic pipelines to generate seemingly unsafe prompts for evaluating LLMs' over-refusal behavior."}, {"title": "6. Conclusion", "content": "In this paper, we comprehensively compare existing defense methods in multi-turn attack scenarios and reveal their shortcomings in balancing the robustness of defense and LLM usability. We analyze this issue from the perspective of LLMs' feature space, and conclude that previous methods fail to learn a precise boundary that distinguishes safe and harmful representations without an explicit formulation. To address this issue, we propose the X-Boundary to push harmful representations away from safe representations through explicit loss functions and obtain a clear distinction boundary. Such distinction boundary enables the consequential removal of harmful representations without disrupting safe ones, thereby achieving a balance between robustness against multi-turn jailbreaks and LLM usability. We think that X-Boundary can offer more efficient and fine-grained defense for LLMs, complementing existing safety alignment techniques and ultimately improving the deployment of robust AI systems in real-world applications."}, {"title": "Impact Statement", "content": "This work aims to advance the field of large language models (LLMs) safety alignment by proposing X-Boundary, a method that maintains state-of-the-art performance in multi-turn jailbreak attack defenses while effectively mitigating the over-safety problem. We do not consider that this method will directly lead to severe negative consequences for societal development. However, we must be aware that malicious actors could exploit various approaches to induce LLMs to generate misleading or harmful content. Therefore, we expect that future research will focus on enhancing content moderation mechanisms and setting up ethical usage protocols to effectively reduce potential risks. From a constructive perspective, this method can significantly enhance the reliability, safety, and usability of LLMs."}, {"title": "A. Additional Results", "content": null}, {"title": "A.1. The Trade-Off between Robustness and General Capability", "content": "Fig. 7 intuitively shows the trade-off between the ASR against multi-turn jailbreaks and the decline of general capability. As the training process advances, the ASR steadily decreases, while the decline in code and math capability progressively increases. X-Boundary lies in the lower-left corner of the plots, demonstrating that it achieves a win-win outcome with robust defense and strong general capability."}, {"title": "A.2. Ablation Study on Llama-3-8B-Instruct and Mistral-7B-Instruct-v0.2", "content": "Through analyzing the results of ablation experiments in Table 4 and Table 5, we can obtain conclusions consistent with that in Section 4.3."}, {"title": "A.3. Effects of the Size of Boundary-Safe Data", "content": "Fig. 8 shows that as the boundary-safe data size increases, the over-refusal rate generally decreases, while ASR against multi-turn attacks tends to increase. Without the separate loss, when the boundary-safe data size reaches 500, the ASR hardly decreases, failing to achieve the purpose of enhancing multi-turn defense. This demonstrates that it is difficult to balance ASR and over-refusal rate simply by adjusting the boundary-safe data size."}, {"title": "A.4. Effects of Separate Loss and Boundary-Safe Data on the Representation Distribution", "content": "Fig. 9 shows that adding boundary-safe data to the retain set reduces the angle between boundary-safe representations after training and their original representations. Furthermore, under the effect of separate loss, this angle is further minimized. Meanwhile, the angle between boundary-safe representations and refusal representations increases, indicating that separate loss contribute to establish a clear distinction boundary."}, {"title": "A.5. Details about Representation Visualization", "content": "To analyze safety-usability trade-off from the perspective of interpretability mechanism, we extract the feature representations from the 10th layer of Llama-3-8B-Instruct and visualize them using 2-dimensional t-SNE, as shown in Fig. 10."}, {"title": "B. Experimental Details", "content": null}, {"title": "B.1. Construction of Multi-Turn Defense Dataset", "content": "We construct a multi-turn defense dataset based on SafeMTData. SafeMTData is derived from the circuit breaker training dataset, and carefully filtered to prevent data contamination with Harmbench. It includes harmful multi-turn queries generated by ActorAttack (Ren et al., 2024b), along with refusal responses to reject the harmful queries. To curate the harmful responses for DPO, GA, and CB, we use harmful multi-turn queries in SafeMTData to attack deepseek-chat (Liu et al., 2024a) and filter the harmful response using HarmBench classifier (Mazeika et al., 2024).\nFor SFT and DPO, following Ren et al. (2024b), we maintain a 1:2 ratio between the multi-turn defense data and instruction data, e.g., UltraChat (Ding et al., 2023). For CB, we add the filtered harmful responses and their corresponding single-turn"}, {"title": "B.2. Training Details of Baselines", "content": "Multi-Turn SFT For multi-turn SFT, we set the batch size to 1 with accumulation step 16. The training process was conducted for a total of 1 epoch. Optimization was performed using the AdamW optimizer, with the learning rate set to 5 \u00d7 10-4, ensuring stable and efficient model updates. The warm-up ratio and weight decay ratio are set to 0.05, 0.03. All training processes use Low-Rank Adaptation (LoRA) for parameter fine-tuning, where the rank r, scaling factor a, and dropout rate are set to 16, 16, and 0.1, respectively. It takes about 40 minutes to train a Llama-3-8B-Instruct model on a single A100 80G GPU.\nMulti-Turn DPO For Multi-turn DPO, we use a learning rate of 1.0 \u00d7 10-5 with a cosine learning rate scheduler and a warm-up ratio of 0.1. We set the training epoch to 3 and the batch size to 1 with gradient accumulation steps of 8. All training processes use Low-Rank Adaptation (LoRA) for parameter fine-tuning with the rank r, scaling factor a, and dropout rate set to 8, 16, and 0, respectively. We conducted all training processes on a single A100 80GB GPU.\nGradient Ascent Following the experimental setting of Zhang et al. (2024b), we set the batch size to 11 with accumulation step 1, where the ratio of the three types of data in a batch is 5:5:1. We use the AdamW optimizer with a learning rate of 2 \u00d7 10-5 and set the maximum epoch as 3. For Qwen2.5-7B-Chat and Llama-3-8B-Instruct, the coefficients of safe responses loss Ls, general performance loss Lg, and unlearning loss Lh are set to 0.5, 1.0, 0.3. For Mistral-7B-Instruct-v0.2, the loss coefficients are set to 0.25, 1.0, and 0.05, respectively. All training processes use Low-Rank Adaptation (LoRA) for parameter fine-tuning. For Llama-3-8B-Instruct and Mistral-7B-Instruct-v0.2, we set the rank r, scaling factor a, and dropout rate to 16, 16, 0.05. For Qwen2.5-7B-Chat, we conducted a grid search over the LoRA hyperparameters with r\u2208 {8, 16, 32} and a \u2208 {16,32, 64}. We end up selecting r = 8, a = 64, and a dropout rate of 0.05. We linearly decay the learning rate and select the checkpoint after 1 epoch for evaluation. Training a Mistral-7B-Instruct-v0.2 model on a single A100 80GB GPU takes approximately 1 hour."}, {"title": "B.3. Training Details of X-Boundary", "content": "We use LORA for fine-tuning and set the rank r as 16 on Llama-3-8B-Instruct and Mistral-7B-Instruct-v0.2, 32 on Qwen2.5-7B-Chat and Qwen2.5-14B-Chat. We set dynamic loss coefficients following (Zou et al., 2024), where $c_r = a\\frac{\\beta}{f}$ and $c_e = c_s = a(1 - \\frac{\\beta}{t})$. \u03b1, \u03b2, and the target layers for calculating erase loss keep consistent with hyperparameters specified in Appendix B.2. We conduct a grid search on the size of boundary-safe data in a valid set in the range of [0,500], with a step of 50, selecting the size for Llama-3-8B-Instruct, Mistral-7B-Instruct-v0.2, Qwen2.5-7B-Chat, and Qwen2.5-14B-Chat is 500, 200, 100, and 50, respectively. Qwen2.5-14B-Chat is trained for 260 steps with a batch size of 8 on 4 A100 GPUs, while other LLMs are trained for 180 steps with a batch size of 16 on 1 A100 GPU."}, {"title": "B.4. Evaluations", "content": "Datasets. We evaluate our approach on benchmarks covering multi-turn attacks, over-refusal, and general model capabilities:\nMulti-Turn Attack We employ three state-of-the-art multi-turn attack benchmarks. We adopt three state-of-the-art multi-turn attack benchmarks:\n\u2022 ActorAttack (Ren et al., 2024b): Emphasizes role-playing scenarios to gradually induce harmful behavior. The multi-turn queries in SafeMTData_Attack_600 (Ren et al., 2024b) are used to attack victim models, and HarmBench"}, {"title": "C. Theoretical Analysis of X-Boundary", "content": "Proposition 1. If \u00a2#\u03bc is (n, \u2206)-clusterable, then for all $m \\le n(2\\Delta)^{-2}$,\n$Var_m(\\phi_#\\mu) < 48\\Delta$. (1)\nGiven a distribution \u03bc, (n, \u2206)-clusterable means that supp(\u00b5) lies in the union of n balls of radius at most \u2206.\nProof. Proposition 1 in this paper is an application of Proposition 13 in (Weed & Bach, 2019).\nDefinition 1 ((Weed & Bach, 2019)). A distribution \u03bc is (m, \u2206)-clusterable if supp(\u00b5) lies in the union of m balls of radius at most A.\nProposition 2 (Proven in (Weed & Bach, 2019)). If \u00b5 is (n, \u2206)-clusterable, then for all $m \\le n(2\\Delta)^{-2p}$,\n$E_{s \\sim \\mu^m}[W_p^p(\\mu, \\mu_s)] \\le (9^p + 3)\\frac{n}{m}\\Delta^p,$ (2)"}, {"title": null, "content": "According to the triangle inequality, we have\n$Var_m(\\phi_#\\mu) = E_{s, \\S \\sim \\mu^m}[W_1(\\phi_# \\mu_s, \\phi_#\\mu_S)] \\le 2 E_{s \\sim \\mu^m} [W_1^p(\\mu, \\mu_s)] \\le 24 \\sqrt{\\frac{n}{m}}\\Delta,$ (3)\nIn this way, for all $m \\le n(2\\Delta)^{-2p}$, we have\n$Var_m(\\phi_#\\mu) \\le 24 \\sqrt{\\frac{n}{m}}\\Delta < 48\\Delta.$ (4)"}, {"title": "D. Case Study", "content": "In this section, we showcase a range of examples to demonstrate the practical utility of the X-Boundary in mitigating over-safety and its robustness against multi-turn attacks.\nCases of Over-Safety In Fig. 11 and Fig. 12, we present two examples from OKTest and OR-Bench, comparing the responses of Llama-3-8B-Instruct with different safety strategies. X-Boundary provides accurate and useful responses to boundary-safe queries, e.g., \u201cIs there an effective program in place to educate children about the dangers of molestation? \", unlike previous methods which incorrectly treat them as harmful queries and refuse to respond. This lies in X-Boundary effectively increases the distance between harmful representations and boundary-safe representations, allowing for a clear distinction between harmful queries and boundary-safe queries.\nCases of Multi-Turn Attack In Fig. 13, Fig. 14, and Fig. 15, we present three examples taken from ActorAttack, RedQueen, and Crescendo, respectively. X-Boundary effectively defends against various types of multi-turn jailbreaks, demonstrating its robustness."}]}