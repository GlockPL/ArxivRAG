{"title": "CRAB: Cross-environment Agent Benchmark for Multimodal Language Model Agents", "authors": ["Tianqi Xu", "Linyao Chen", "Dai-Jie Wu", "Yanjun Chen", "Zecheng Zhang", "Xiang Yao", "Zhiqiang Xie", "Yongchao Chen", "Shilong Liu", "Bochen Qian", "Philip Torr", "Bernard Ghanem", "Guohao Li"], "abstract": "The development of autonomous agents increasingly relies on Multimodal Lan- guage Models (MLMs) to perform tasks described in natural language with GUI environments, such as websites, desktop computers, or mobile phones. Existing benchmarks for MLM agents in interactive environments are limited by their focus on a single environment, lack of detailed and generalized evaluation methods, and the complexities of constructing tasks and evaluators. To overcome these limitations, we introduce Crab, the first agent benchmark framework designed to support cross-environment tasks, incorporating a graph-based fine-grained evalua- tion method and an efficient mechanism for task and evaluator construction. Our framework supports multiple devices and can be easily extended to any environ- ment with a Python interface. Leveraging Crab, we developed a cross-platform Crab Benchmark-v0 comprising 100 tasks in computer desktop and mobile phone environments. We evaluated four advanced MLMs using different single and multi-agent system configurations on this benchmark. The experimental results demonstrate that the single agent with GPT-4o achieves the best completion ratio of 35.26%. All framework code, agent code, and task datasets are publicly available at https://github.com/camel-ai/crab.", "sections": [{"title": "1 Introduction", "content": "The development of autonomous agents for human-centric interactive systems such as desktop OS [54], websites [59, 19], smartphones [55, 50], and games [42, 43]-has long been an impor- tant goal of AI research, aiming to convert natural language instructions into concrete operations. Traditionally, these challenges have been addressed using reinforcement learning [31]. Recently, Large Language Models (LLMs) have demonstrated remarkable proficiency in natural language understanding and commonsense reasoning, making them vital tools for developing autonomous agents. This utility is further enhanced by Multimodal Language Models (MLMs), which improve the ability to interpret visual information from GUIs [5]. To effectively develop MLM-based autonomous agents for real-world applications, it is essential to create suitable benchmarks for standardized performance evaluation. However, existing benchmarks still have limitations in terms of interaction methods, platform diversity, evaluation metrics, static task dataset that prevent them from closely mirroring complex real-world applications. First, existing benchmarks that interact with the environments through pre-collected observation data from system environments [40, 30, 6] fail to capture the dynamic nature of real-world scenarios without interactive exploration where data and conditions can change unpredictably. Second, existing benchmarks are"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Autonomous Agents", "content": "Recently, with the great advancement of Large Language Models (LLMs), a great number of LLM- based agents [45, 13, 48] have secured significant accomplishments across an expanding array of complicated tasks. Some works [43, 39, 4] apply LLMs to the planning of embodied agents under complicated environments. Some researches [34, 24] simulate the human behaviors and social communications by leveraging the stunning human-like understanding and generation capability of LLMs. LLMs are also widely used in other domains, including web navigation [28], game playing [20], office assistant [22], and code generation [57]. Following are some related categories of agents to our system."}, {"title": "Multi-Agent", "content": "These researches utilize interactions between multiple agents to better problem solv- ing. Camel [21] firstly models the complex task-solving as collaboration among different agents playing different roles. Metagpt [11] encodes the Standardized Operating Procedures (SOPs) into a streamlined collaboration workflow, leveraging multiple agents as different roles in each sections."}, {"title": "Multimodal Agent", "content": "Recent researches [12, 19] achieve impressive capacity in processing image- text inputs in complicated environments. LLaVA-Plus [26] introduces different tools to obtain better understanding in visual input. WebGUM [8] tackles the web navigation task considering the web pages as image input, achieving great results via corresponding fine-tuning. VisualWebArena [19] receives HTML screenshots and additional figures as the visual input, applying Set-of-Mark annotation to improve the performance of the agent."}, {"title": "Operating System Control Agent", "content": "With the large models demonstrating profound knowledge of operating systems and formidable planning capabilities, operating system control agent [47, 44] has emerged as a new focal point. Synapse [58] collects computer control trajectories as the exemplars for in-context learning of LLMs, addressing the challenge of limited context length. SceenAgent [32] broadens the computer system control tasks into a wider range over the web-related tasks. Cradle [41] gives attention on a more laborious multi-modal environment, the digital game environment, validating the potential of VLMs in solving complex tasks. MobileAgent [44] pays attention to control android system via LLMs and VLMs and tests on a wider range of android apps. Android Arena [50] underlines the collaboration among android applications and expands simple android tasks into cross-App and constrained tasks, which verifies the potential of LLM-based complicated android system control. these researches are limited to specific domains and lack the capability for cross-platform control."}, {"title": "2.2 Benchmark for Language Model Agents", "content": "Various benchmarks are developed to validate the performance of autonomous agents based on the reproducible environments. Miniwob++ [38] analyzes the open-domain web tasks, builds corresponding web environment, and produces high-quality datasets considering extensive website and operation categories. GAIA [30] proposes a benchmark which considers the challenges of emergency cases. Mind2Web [6] proposes a benchmark for the real-world websites which are genuine and unpredictable, with a high coverage of domains, websites, tasks, and user-interactions. WebArena [59] provides a realistic and reproducible web environment to simulate sufficiently complex web tasks. Several works [19, 10] further broaden the web tasks, considering the visual tasks to build the benchmark for multi-modal autonomous agents. SWEBench [15] builds a benchmark based on the Github, focusing on the coding capacity of understanding and solving github issues. AgentBench [27], significantly expands the scope of agent applications within the domain of computer interaction tasks. This expansion is particularly noteworthy as it encompasses the examination of these tasks across a diverse array of complex and challenging environments. OMNIACT [16] incorporates the visual information of OS screen UI via segmentation and corresponding tagging, which creates corresponding tasks upon the basic elements. OSWorld [49] pays attention to the simulations across diverse computer systems, taking XML and screenshots as both inputs and meticulously delineating a standardized format for both the environment and the evaluation process. Contemporary studies not only focus on tasks related to control within web and computer systems but also extend their scope to encompass control tasks within mobile systems. MetaGUI [40] divides the mobile system control tasks into dialogues and GUI operation traces, collecting GUI traces based on the collected dialogues. AITW [36] produces a large dataset upon a large dataset of real-world scenarios, and builds challenging multi-steps tasks based on the annotated single-step tasks as a two-stage manner. MobileAgent [44] proposes tasks based on Ant Intelligent Assistant(AIA) system, which integrates Standing Operating Procedure(SOP) information for the creation of subtasks. AITZ [56] constructs datasets with Chain-of-Thought (COT) considerations, adding semantic annotations according to visual models at each step, and developing the operational procedure for selected tasks."}, {"title": "3 Definitions", "content": ""}, {"title": "3.1 Problem Formulation", "content": "Consider autonomous agents performing a task on a digital device (i.e. desktop computer). Such a device typically has input devices (i.e. mouse and keyboard) for human interaction and output devices (i.e. screen) to allow human observation of its state. In Crab, we represent this type of device as an"}, {"title": "3.2 Graph of Task Decomposition", "content": "Decomposing a complex task into several simpler sub-tasks has been proved to be an effective prompting method for LLMs [17]. Some studies represent sub-tasks in a graph structure. For instance, PLaG [23] uses a graph-based structure to enhance plan reasoning within LLMs, while DyVal [60] employs directed acyclic graphs (DAGs) to facilitate dynamic evaluation of LLMs. By introducing this concept into the realm of benchmarks, naturally, decomposing a complex task into sub-tasks that have both sequential and parallel connections forms a DAG. Therefore, we introduce the Graph of Decomposed Tasks (GDT), which provides a new task decomposition method representing decomposed sub-tasks within a DAG structure. In GDT, each node is a sub-task, formalized as a tuple (m, i, r), where m specifies the environment in which the sub-task is performed, i provides the natural language instruction, and r represents the reward function. This function evaluates the state of m and outputs a boolean value to determine if the sub-task is completed. The edges within GDT represent the sequential relationship between sub-tasks. An example GDT is shown in Fig. 2."}, {"title": "4 The Crab Framework", "content": ""}, {"title": "4.1 Cross-environment Agent Interaction", "content": "Compared to single-environment tasks, cross-environment tasks offer three main advantages for benchmarking agents. First, cross-environment tasks reflect real-world scenarios where humans use multiple devices simultaneously to accomplish tasks. Second, these tasks require sophisticated message processing and information transfer between environments. Such tasks demand that the agent plan actions, construct outputs for each environment, and remember what needs to be transferred, showcasing a high-level understanding of real-world and ability to solving complex tasks. Lastly, role-playing multi-agent systems have proven to be effective in executing complex tasks [21, 11]. The underlying principle of their effectiveness is the division of responsibilities. Cross-environment tasks are suited to multi-agent, as they can be divided by distinct observation spaces, action spaces, and specialized knowledge in each environment, as shown in Fig. 1. Crab uses a unified interface for agents to operate in all environments. We define an action by its name, the environment it belongs to, a concrete description of its functionality, and the parameters with descriptions. The agent must provide the action name, parameters, and the target environment in each turn. Crab translates the action into its corresponding function and routes it to the physical or virtual device through the network. Implementation details are in the Appendix A.1."}, {"title": "4.2 Graph Evaluator", "content": "To assess the capabilities of MLM agents, most benchmarks [38, 6, 19, 59] evaluate agents based on solely the final states of the environment after agent operations. Typically, they only judge whether the final goal is success or fail. However, this approach does not capture incremental progress made by the agents. For instance, consider two agents tasked with installing a new application on a computer: agent a successfully downloads the installer but fails during the installation process, whereas agent b does not even try to find the installer. Despite Agent a making more progress, both are deemed"}, {"title": "4.3 Metrics", "content": "Given a Graph Evaluator synchronized with the environment state, it becomes possible to track agent progress through the current status of sub-task completions. Beyond the traditional Success Rate (SR), which marks a task as success only when all sub-tasks are completed, we introduce three metrics aiming at assessing both performance and efficiency of agents, leveraging the detailed"}, {"title": "4.4 Task and Evaluator Construction", "content": "Despite the graph evaluator offers detailed evaluations, one challenge is the complexity in creating each evaluator. Creating a graph evaluator requires: (1) adequately decomposing a task into multiple sub-tasks, each with a well-defined graph structure; and (2) engaging an expert who is well-acquainted with the target platform to carefully craft an evaluator for each sub-task. To streamline the creation of tasks and the development of evaluators, we consider to build GDTs by sub-tasks. There are two primary challenges in constructing GDT: (1) Sub-tasks still require manual creation, necessitating a method to quickly generate them on a large scale; (2) Properly modeling the sequential and parallel relationships between sub-tasks, ensuring that the edges connecting sub-task nodes are semantically meaningful and systematically applicable. A template-based approach is commonly used to address the first issue by generating a large number of tasks efficiently. To tackle the second challenge, we employ the message transferring concept (Sec. 4.1). Specifically, if a sub-task a produces an output message that serves as an input for another sub-task B, then a can be considered a legitimate prerequisite of \u00df, allowing us to connect a and \u00df with an directed edge in the GDT. To further refine our approach, we introduce a sub-task template structure. Each sub-task is described using a natural language instruction template that includes several replaceable input attributes. The types of each input attribute and the task output should be defined carefully. To generate a GDT, input attributes can be filled with either a hand-crafted value corresponding to their type or linked to a task with the same output type as the input type. From the evaluator's perspective, each sub-task template is linked to an evaluator generator that uses the input attribute value to generate evaluator subgraphs. Once a GDT is constructed, the graph evaluator is created by interlinking each subgraph. The description for the composed task is initially generated by GPT-4 using the sub-task descriptions as prompts and subsequently refined and polished by human reviewers."}, {"title": "5 The Crab Benchmark", "content": "We build an agent benchmark Crab Benchmark-v0 featuring with cross-environment, graph evaluator, and task generation through Crab framework. The environments consists of an Android smart- phone emulator and a Ubuntu Linux desktop virtual machine. We establish both environments in a reproducible and standalone manner: the Ubuntu environment is launched on a QEMU/KVM [3, 18] Virtual Machine, and the Android environment employs the Google Android Emulator\u00b9. Both environments utilize snapshots to ensure a consistent state across all sessions. This allows each experiment to start from an identical state, providing a controlled setup for all test agents. Interaction with the Ubuntu environment is facilitated using PyAutoGUI\u00b2 and MSS\u00b3, which provide high-level commands for mouse and keyboard control and screen capture, respectively. For the Android environment, we use the Android Debug Bridge (ADB)4. Observation Space. The observation space consists solely of the current system screen for both environments, captured in image format at each step of the agent's interaction. We employ the Set-of-Marks visual prompt method [51] to label each interactive element on the screen. Interactive elements are identified using the GroundingDINO [25] with icon.logo. text prompt to locate all"}, {"title": "5.1 Task Construction", "content": "In Crab Benchmark-v0, we meticulously construct 16 sub-task templates for the Android environment and 19 sub-task templates for the Ubuntu environment. The Ubuntu templates encompass a variety of tasks such as Command Line Interface (CLI) operations, file system management, search engine usage, desktop configurations, and map navigation. Conversely, the Android sub-task templates are primarily focused on the storage and transmission of messages via various applications. Each sub-task template is linked to a graph evaluator consisting of one to four nodes. Each sub-task is verified by at least two related field experts. To ensure the accuracy of the evaluators, the benchmark includes a human mode, which periodically activates the graph evaluators during manual operation by a user. We ascertain the correctness of each task by ensuring that all evaluators in the graph can be passed in human mode. The task dataset is consisted by 35 single tasks generated by a single sub-task, 53 simple tasks and 12 challenging tasks obtained by sub-task composition aiming to verify the different levels of challenges in daily usage scenarios. The dataset has 29 android tasks, 53 Ubuntu tasks and 18 cross-platform tasks. Besides, he sub-task pool has 19 in Ubuntu and 17 in Android. Evaluator Design. To assess the intermediate states of sub-tasks as described in Sec. 4.2, we have implemented a comprehensive suite of execution-based reward functions (evaluators) [49]. These evaluators retrieve and assess specific current states, such as the edited content of a file or a modified setting, thereby determining the successful completion of a sub-task. For each evaluator, input attributes are carefully selected to interpret software information or system settings relevant to the scenario defined for the sub-task. For instance, evaluators use file paths before and after edits as input parameters to verify the completion of file editing sub-tasks."}, {"title": "6 Experiments", "content": ""}, {"title": "6.1 Baseline Agent System", "content": "At the core of MLM Agents are back-end Multimodal Language Models that provide natural language and image understanding, basic device knowledge, task planning, and logical reasoning abilities. To run in Crab Benchmark-v0, the back-end model needs to support: (1) Accept multimodal mixed input, as the system provides both screenshots and text instructions as prompts; (2) Handle multi-turn conversations, as most tasks require the agent to take multiple actions, necessitating the storage of history messages in its context; (3) Generate structured output through function calling, ensuring the proper use of provided actions with type-correct parameters. We selected four MLMs that meet these criteria for our experiments: GPT-4o (gpt-4o-2024-05-13) [33], GPT-4 Turbo (gpt-4-turbo-2024-04-09) [1], Gemini 1.5 Pro (May 2024 version) [37], Claude 3 Opus (claude-3-opus-20240229) [2]. These models serve as the backend models for our agents. Beyond the MLM backend, the structure of agent systems significantly influences overall performance. To examine how different multi-agent structures impact performance, we design three agent system structures: single agent, multi-agent by functionality, and multi-agent by environment. In the single agent structure, one agent manages all responsibilities, including observation analysis, planning, reasoning, and format the output action. The multi-agent by functionality structure splits tasks between a main agent, responsible for analysis and planning, and a tool agent that translates instructions into actions without accessing environmental observations. This division allows the main agent to concentrate on high-level tasks without managing functional call formats. Meanwhile, in the multi-agent by environment setup, responsibilities are further distributed. A main agent processes all environmental observations for high-level planning, while each environment-specific sub-agent executes actions based on the main agent's instructions, incorporating observations from their respective environments. For all models, we utilized the default API parameters and retained only two turns of historical messages to ensure messages do not exceed the context window. The interaction turns are limited to 15 and the task will terminated because reaching max turns. The agent can also terminate the task ahead if it thinks the task is completed. The screenshots do not descale and passed through PNG format with the highest quality that the APIs provide. Detailed agent and prompt designs are shown in Appendix B. In the experiment, we deployed four cloud machines cloned from the same disk image to ensure a consistent environment for all agents. Running a single agent setting in the benchmark requires at least 30 hours to complete on one machine. This duration depends on the API call times and the necessity for manual resets in certain tasks."}, {"title": "6.2 Results", "content": "The primary outcomes are detailed in Table 3. The GPT-4o and GPT-4 Turbo models, developed by OpenAI, achieve the highest average success rates and completion ratios among the tested models. Specifically, GPT-40 slightly outperforms GPT-4 Turbo. This result suggests a tiny difference in their underlying architectures or training data, but GPT-40 possibly be trained on more GUI data. Claude 3 outperforms Gemini 1.5 according to CR. The multi-agent structures' performances on all"}, {"title": "7 Conclusion", "content": "We propose the Crab framework introducing cross-environment automatic task performing problem, featuring advanced graph-based task generation and evaluation methods, which reduce the manual effort in task step and provide a more dynamic and accurate agent assessments. Based on the framework, we propose Crab Benchmark-v0, including a set of high quality cross-environment tasks for a smart phone and desktop, equipped with visual prompting strategy. We test various backend models and agent system structures on the dataset. The result reflects preference of different agent settings. Despite our work contributing to better cross-environment agent research, there are still some limitations. We build sub-tasks upon the original apps in the Ubuntu system and the Android system on Pixel, which cannot cover a wider range of applications. Moreover, the visual information is not used in the evaluation on the sub-tasks in Android System. Future works can focus on expanding datasets and environments and testing more models, prompts, structure of agents upon the benchmark."}, {"title": "A Benchmark Detail", "content": "The framework code and task dataset are open source under Apache-2.0 licences. Section A.2 describes the our experiment settings in detail. Section A.3 describes the specific format defined in our framework that ease data extension and how to use them. We provides a detailed document to setup experiment environments and reproduce our results. Fig. 3 shows the structure of modules inside Crab Benchmark-v0."}, {"title": "A.1 Framework Design", "content": "Crab offers a modular and extensible framework for evaluating agent performance in diverse tasks. At the heart of the framework lies the action, a unit operation representing the fundamental operation within the benchmark. The action is essentially an executable Python function that can be defined with explicit typed parameters and a clear description. actions serve not only as building blocks but also as interfaces through which agents interact with the environment. The evaluator is a specialized action restricted to returning boolean values, signifying the success or failure of an agent's task. It enhances the actions by analyzing the state of the environment and the sequence of actions executed by the agent, providing a decisive metric of task accomplishment. Additionally, multiple evaluators can be interconnected to form a graph evaluator for complex tasks (Sec. 4.2). The benchmark is a key definition in the framework. A benchmark includes multiple environments and cross-environment tasks. The environment is formed by an action space and an observation space, which are both defined by a list of actions, and other essential parameters necessary for its"}, {"title": "A.2 Configuration by Modules", "content": "Building on the declarative and modular design of our framework, this section explains the configura- tion and potential extensibility of each module. Environment The environments in Crab are a combination of multiple different uses of actions with some environment metadata, such as name and natural language description. In Crab Benchmark-v0, we use a computer desktop environment and a smartphone environment both based on virtual machine technology. The computer desktop environment, named Ubuntu, is installed from an ISO image of Ubuntu 22.04.4 LTS (Jammy Jellyfish) downloaded from the Ubuntu Official website. Necessary applications such as the LibreOffice suite (Writer, Calc, and Impress) and Slack are installed later via snap and apt, according to the task dataset requirements. The smartphone environment, named Android, is installed using pre-defined devices (Google Pixel 8 Pro with release name R) provided in Google Android Studio. We install additional required applications such as Keep Notes, Tasks, and Docs from Google Play. The descriptions of the two environments in Crab Benchmark-v0, which are inserted in the agent prompts, are as follows: \u2022 Ubuntu: An Ubuntu 22.04 Linux desktop operating system. The interface displays a current screenshot at each step and primarily supports interaction via mouse and keyboard. You must use searching functionality to open any application in the system. This device includes system-related applications including Terminal, Files, Text Editor, Vim, and Settings. It also features Firefox as the web browser, and the LibreOffice suite-Writer, Calc, and Impress. For communication, Slack is available. The Google account is pre-logged in on Firefox, synchronized with the same account used in the Android environment. \u2022 Android: A Google Pixel smartphone runs on the Android operating system. The interface displays a current screenshot at each step and primarily supports interaction through tapping and typing. This device offers a suite of standard applications including Phone, Photos, Camera, Chrome, and Calendar, among others. Access the app drawer to view all installed applications on the device. The Google account is pre-logged in, synchronized with the same account used in the Ubuntu environment. Action Action implementation in Crab Benchmark-v0 utilize the dynamic feature of Python. It provides an intuitive method to define actions through Python function. Here is an example of action search_application in the Ubuntu environment:"}, {"title": "Observation", "content": "The observation space is represented by a set of actions. These observation actions are designed to be parameter-free and return an observation result. For instance, within the Ubuntu environment, the sole observation action available is the screenshot function, defined as follows:"}, {"title": "Evaluator", "content": "The evaluator in Crab Benchmark-vo is crafted to assess the outcome of actions performed by the agent within the environment. The evaluator is defined as an action that outputs a boolean value. An example of an evaluator in the Ubuntu environment is the check_text_in_current_window_name function, outlined below:"}, {"title": "Task", "content": "Following a declarative programming paradigm, the task is defined as a data model. Here is an example of a cross-platform task in the dataset:"}, {"title": "Sub-task", "content": "The sub-task in Crab is the unit component of in task construction. The following example is a sub-task template that we used to easily generate sub-tasks:"}, {"title": "A.3 Task Dataset", "content": "We use a JSON format to save the composed tasks, which includes the task ID, overall task description, sub-tasks with their attribute values, and a graph structure represented in an adjacency list. The entire task dataset is defined by the sub-task pool in Python code and the task composition JSON files categorized by task platform."}, {"title": "B Agent system", "content": ""}, {"title": "B.1 Agent Implementation", "content": "In this section, we outline the implementation of the agents used in our experiments, which leverage advanced multimodal language models from OpenAI, Anthropic, and Google. Each agent is designed to function in multi-environment setups, interacting with various action spaces defined by different environments. General Framework All agents share a common architecture but are tailored to the specific APIs and capabilities of each language model provider. Initialization Each agent is initialized with several key parameters, including a description, an action space, the model type, maximum tokens, history message length, and an optional environment description. The initialization process involves: \u2022 Action Space Conversion: Actions defined for each environment are converted into a schema compatible with the respective API. This ensures that the actions can be correctly interpreted and executed by the language models. \u2022 System Message Setup: Depending on whether the agent is configured for single or multiple environments, a system message is formatted to provide the model with context about the tasks and environments."}, {"title": "Interaction (Chat Method)", "content": "The core functionality of each agent is encapsulated in its ability to interact with users through a chat method. This involves: \u2022 Content Parsing: Input content is parsed and formatted to match the requirements of the respective API. This includes structuring user messages and any necessary contextual information. \u2022 Request Construction: The request payload is constructed, incorporating the system message, chat history, and the newly parsed user input. \u2022 API Communication: The constructed request is sent to the appropriate API, which generates a response. The agents handle API-specific constraints such as rate limits and response formats. \u2022 Response Handling: The response from the API is processed to extract any tool calls suggested by the model. These are then appended to the chat history, maintaining a coherent conversation state."}, {"title": "Multi-Environment Support", "content": "For agents configured to operate in multiple environments, additional logic ensures that actions are correctly associated with their respective environments. This involves modifying action names and descriptions to reflect their environmental context and handling responses accordingly. Utilities and Shared Functions Several utility functions support the operation of these agents, facil- itating tasks such as content parsing, action prompt generation, and schema conversion. These shared functions ensure consistency and reduce redundancy across the different agent implementations."}, {"title": "B.2 Inter-agent Communication Strategies", "content": "In this section we introduce the details of two multi-agent communications methods, which are introduced in 6.1. Multi-agent Communication by Functionality This setting involves two agents: a main agent prompted with the task description and a tool agent with the entire action space. The main agent generates the instruction for the next step and sends it to the tool agent. The tool agent chooses the proper action with parameters and a target environment, then feeds it back to the system. Multi-agent Communication by Environment This setting involves four agents in our benchmark setting: a main agent prompted with the task description and three tool agents, each corresponding to the environments of Android, Ubuntu, and Root, with the respective action spaces. The main agent generates the instruction for the next step and sends it to the tool agents. Each sub-environment agent receives the message containing the instruction and environment observation information. The environment agents process the message using their specialized models and action schemas, performing the required actions within their environments."}, {"title": "B.3 Agent Prompt", "content": ""}, {"title": "B.3.1 Single Agent", "content": "You are a helpful assistant. Now you have to do a task as described below: {task_description}. And this is the description of each given environment: {env_description}. A unit operation you can perform is called action in a given envi- ronment. For each environment, you are given a limited action space as function calls: {action_descriptions} You may receive a screenshot of the current system. The interactive UI elements on the screenshot are labeled with numeric tags starting from 1. For each step, You must state what actions to take, what the parameters are, and you MUST provide in which environment to perform these actions. Your answer must be a least one function call. please do not output any other information. You must make sure all function calls get their required parameters."}, {"title": "B.3.2 Multi-Agent by Functionality", "content": "You are a helpful assistant. Now you have to do a task as described below: {task_description}. And this is the description of each given environment: {env_description}. A unit operation you can perform is called action in a given envi- ronment. For each environment, you are given a limited action space as function calls: {action_descriptions} You may receive a screenshot of the current system. The interactive UI elements on the screenshot are labeled with numeric tags starting from 1. For each step, You must state what actions to take, what the parameters are, and you MUST provide in which environment to perform these actions."}, {"title": "B.3.3 Multi-Agent by Environment", "content": "You are a main agent, and your goal is to plan and give instructions to sub-agents in each environment to complete the final task. Now you have to do a task as described below: {description}. The description of each given environment: {env_description}. For each step, you are required to provide high-level instructions detailing the next actions to be taken. Additionally, you must specify which sub-agent in the designated environment should execute these instructions. If a sub-agent is not needed for a particular step, you may instruct it to skip that step. You are a sub-agent responsible for the crab benchmark root environment. Your goal is to assist the main agent in completing the whole task: \"{description}\". You can only complete the task or submit the result when the main agent tells you the whole task has been completed. Otherwise, you can only call SKIP. You are a sub-agent responsible for the environment environment. The description of the environment} environment is: {env_description}. Your goal is to assist the main agent in completing the final task by performing actions in the environment} environ- ment according to the instructions from the main agent. The final task is described be- low: {task_description}. A unit operation you can perform is called action in a given environment. You can only execute action in the environment} environment. For the {environment} environment, you are given a limited action space as function calls: {action_descriptions} The interactive UI elements on the screenshot are labeled with numeric tags starting from 1. For each step, You will receive an instruction telling you what you need to do next. After analyzing the instruction you received and the current {environment system, if you think you don't need to do anything in the current {environment} system, you should choose SKIP action. Otherwise, you must state what actions to take, what the parameters are, and you MUST provide in which environment to perform these actions. Your answer must be function calls. Please do not output any other information. You must make sure all function calls get their required parameters."}, {"title": "C Further Result Analysis", "content": "This section further discusses our experimental results in detail. Section C.1 categorizes the results into three types of tasks: Ubuntu, Android, and cross-platform, and provides further analysis. Section C.2 examines three specific tasks and analyzes the performance of different agent settings on each."}, {"title": "C.1 Result by Platforms", "content": "Table 4, 5 and 6 show the experiment results on Ubuntu Tasks, Android Tasks, and cross-platform Tasks, respectively. We find that certain models demonstrate a distinct preference or better alignment with specific platforms. The GPT-40, Gemini, and Claude models, for instance, show notably better outcomes on"}, {"title": "C.2 Case Study", "content": "To better understand how different agents perform the same task and exhibit varied properties, we present visual results along with detailed metrics and logs for three cases by platform. The screenshots illustrate the progress of agents executing tasks according to specific natural language instructions."}, {"title": "C.2.1 Cross-platform Task", "content": "Task: Open the \"Tasks\" app on an Android device, check the first incomplete task, and then execute it as described. The first task, found incomplete in the \"Tasks\" app, involves switching the system to dark mode in Ubuntu via the \"Settings\" application. This task exemplifies message passing across different environments, where the \"incomplete task\" serves as the critical information that the agent must relay and apply in the Ubuntu setting. These two phases-retrieving the task details via the phone and executing the task on a computer-are inseparably linked and cannot be treated as distinct tasks. The agent can only proceed to the second stage after successfully acquiring information from the first. In this task, GPT-40 (single agent), GPT-4 Turbo (single agent), and GPT-4 Turbo (multi-agent by functionality) all successfully complete the task using the minimal steps necessary to locate and execute the task, demonstrating their efficiency in managing multiple environments simultaneously. On the other hand, both GPT-40 (multi-agent by functionality) and GPT-40 (multi-agent by environment) also perform commendably, completing the task up until the final step. However, after incorrectly performing the last step, they both erroneously conclude the task is completed and exit. This indicates a communication breakdown, where the sub-agents misinterpret the instructions from the main agent. The remaining four agents fail to complete the task. Agents equipped with the Gemini model do not even manage to open the \"Tasks\" app within the allocated step limit, whereas agents with the Claude model quickly open the \"Tasks\" app to complete the first step but"}]}