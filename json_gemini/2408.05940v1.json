{"title": "Spb3DTracker: A Robust LiDAR-Based Person\nTracker for Noisy Environment", "authors": ["Eunsoo Im", "Changhyun Jee", "Jung Kwon Lee"], "abstract": "Person detection and tracking (PDT) has seen significant\nadvancements with 2D camera-based systems in the autonomous vehicle\nfield, leading to widespread adoption of these algorithms. However, grow-\ning privacy concerns have recently emerged as a major issue, prompting a\nshift towards LiDAR-based PDT as a viable alternative. Within this do-\nmain, \"Tracking-by-Detection\" (TBD) has become a prominent method-\nology. Despite its effectiveness, LiDAR-based PDT has not yet achieved\nthe same level of performance as camera-based PDT. This paper ex-\namines key components of the LiDAR-based PDT framework, including\ndetection post-processing, data association, motion modeling, and lifecy-\ncle management. Building upon these insights, we introduce SpbTrack,\na robust person tracker designed for diverse environments. Our method\nachieves superior performance on noisy datasets and state-of-the-art re-\nsults on KITTI Dataset benchmarks and custom office indoor dataset\namong LiDAR-based trackers. Project page at anonymous.", "sections": [{"title": "1 Introduction", "content": "Lidar-based person detection and tracking (PDT) is crucial for ensuring safety\nand reliability in various domains, including autonomous driving, industrial\nsafety, and crowd management [35]. While image-based trackers have been widely\nused, recent concerns about personal information protection have highlighted\ntheir limitations [14]. As a result, LiDAR technology has emerged as a promising\nalternative, as LiDAR data does not contain personally identifiable information,\nmaking it impossible to distinguish specific individuals solely from this data [15].\nHowever, LiDAR-based person tracking presents its own set of challenges. To ad-\ndress these, we delve into the tracking module and propose improved algorithms.\nLiDAR-based PDT generally fall into two categories: neural network-based ap-\nproaches and tracking-by-detection (TBD) methods. Neural network-based ap-\nproaches, such as those utilizing Graph Neural Networks (GNNs), offer attractive\nend-to-end solutions [23]. However, they often require ground truth (GT) labels\nfor object IDs, which are scarce in real-world datasets and challenging to ob-\ntain [18]. TBD has emerged as the predominant and most effective paradigm for"}, {"title": "2 Related Works", "content": "Many recent person trackers are designed based on end-to-end deep learning\nnetworks. A common approach involves integrating recurrent layers with detec-\ntor modules. For instance, ROLO combines the convolutional layers of YOLO\nwith LSTM recurrent units [16], while TrackR-CNN extends multi-object track-\ning to include instance segmentation [20]. Tractor++ offers an efficient track-\ning solution by utilizing bounding box regression to predict object positions in\nsubsequent frames without requiring training on tracking data [2]. Other ap-\nproaches focus on enhancing tracking performance through various techniques.\nDeep SORT integrates appearance information with the Simple Online and Re-\naltime Tracking (SORT) technique, employing a recursive Kalman filter and\nframe-by-frame data association [25] [28]. This method incorporates an offline\npre-training stage to learn a deep association metric using large-scale person\nre-identification datasets [12]. Similarly, some trackers adopt a single-stage ap-\nproach, learning target detection and appearance embedding in a shared man-\nner, often complemented by Kalman filters for location prediction [22]. While 2D\ntrackers can leverage rich RGB information for appearance models, a resource\nnot typically available in LiDAR-based 3D tracking, they often struggle with\naccurately representing 3D scene dynamics [3]. Conversely, 3D tracking methods\ncan better handle scale variations and provide more accurate spatial informa-\ntion [9]. Ultimately, the design of person tracking methods should align with"}, {"title": "2.2 Lidar-based Tracking", "content": "Recent developments in Lidar-Based Tracking (LBT) have explored diverse strate-\ngies to enhance tracking performance in dynamic environments [32]. Some ap-\nproaches focus solely on 3D bounding boxes detected from point clouds, offering\nrapid processing and comprehensive scene dynamics capture, albeit potentially\nunderutilizing point cloud appearance features [32]. Alternative methods have in-\ncorporated custom point cloud features or employed deep networks to estimate\n3D bounding boxes from single-view images [32][17]. More recent approaches\nhave shown promising results by combining features extracted from both RGB\nimages and point clouds [34]. Many LBT techniques rely on rule-based compo-\nnents. For instance, AB3DMOT, a widely-used baseline, employs Intersection\nover Union (IoU) for association and a Kalman filter for motion modeling [32].\nSubsequent enhancements have focused on improving the association step, such\nas substituting IoU with Mahalanobis or L2 distance metrics [34]. Researchers\nhave also recognized the significance of lifecycle management in tracking systems\n[34]. Additionally, recent studies have investigated the application of graph neu-\nral networks (GNN) to address LBT in an end-to-end manner, with a focus on\ndata association and active tracklet classification [27][11]. As the field progresses,\nthere is an increasing need for comprehensive studies on 3D Multi-Object Track-\ning (MOT) methods to identify areas for improvement and guide future research\ndirections [34]. This systematic approach will help in addressing the challenges\nunique to LBT and advancing the state-of-the-art in this domain [32][34]."}, {"title": "2.3 Kalman Filter", "content": "Kalman filters (KF) are widely used in tracking-by-detection methods for object\ntracking in autonomous driving due to their effectiveness in filtering noise from\nstate estimations while handling noisy measurements [33]. However, standard\nKFs have limitations in real-world scenarios. They assume linear system models\nand Gaussian noise distribution, which may not hold in complex environments\n[33][29]. They struggle with non-linear motion and poor observation conditions\ndue to fixed system covariance, leading to error accumulation in state predic-\ntions, especially during periodic occlusions of objects [29]. Additionally, spatial\ndistortions in 3D sensor detections can result in imprecise motion parameter es-\ntimations [33]. To address these issues, advanced filtering techniques such as the\nUnscented Kalman Filter (UKF) and Cubature Kalman Filter (CKF) have been\nproposed [29][30]. These filters are designed to better handle non-linearities and\napproximate state distributions more accurately than standard KFs [30]. Fur-\nthermore, adaptive filters can adjust system covariance dynamically to better\nrespond to changing environments, improving tracking performance in complex,\nreal-world autonomous driving scenarios [30][31][13]."}, {"title": "3 SpbTracker", "content": "Our approach diverges from conventional algorithms that typically retain only\nhigh-confidence 3D detection boxes. Instead, we implement a more inclusive\nstrategy that avoids strict confidence thresholds [33]. This method aims to pre-\nserve potential tracklets and enhance recall performance. In our pipeline, we\nemploy the DSVT (Dynamic Sparse Voxel Transformer) detection model[21]\nand apply Non-Maximum Suppression (NMS) to refine the detection results.\nUnlike image-based approaches, our LiDAR-based method focuses on 3D space,\nsimplifying the post-processing steps. By eschewing a fixed confidence threshold,\nour approach seeks to strike a balance between retaining potentially valid de-\ntections and mitigating false positives. This strategy is particularly effective in\nchallenging scenarios where low-confidence detections may still represent genuine\nobjects. As illustrated in Fig. 3, our experiments reveal an inverse relationship\nbetween the confidence threshold and tracking performance metrics (A\u041c\u041e\u0422\u0420\nand AMOTA). Lower thresholds correspond to improved metric scores, under-\nscoring the importance of managing \"ghost\" objects (false positives) in enhanc-\ning recall.\nGiven the limited availability of person data in the KITTI dataset[8], we\nadopted a strategy to enhance our model's performance through transfer learn-"}, {"title": "3.2 Motion Model", "content": "Most previous methods apply simplistic motion models that fail to adequately\naccount for the complexity of a person's movement. persons can move freely\nalong the X and Y axes and rotate around the Z-axis (yaw direction) according\nto the right-hand rule. Furthermore, analysis of person detection results reveals\nthe difficulty in distinguishing between front and back views of individuals. Con-\nsequently, prediction models relying solely on heading angle may be inaccurate.\nTo address these limitations, we propose formulating the state of an object tra-\njectory as an 11-dimensional vector:\n$[x, y, z, \u03b8, \u03a7\u03c5, Yv, Xa, Ya, w, l, h]$", "latex": ["[x, y, z, \u03b8, \u03a7\u03c5, Yv, Xa, Ya, w, l, h]"]}, {"title": "3.3 Dynamic Unscented Kalman Filter", "content": "The standard Kalman Filter is widely used in Track-By-Detection (TBD) track-\ning modules for associating motion model systems with measurement systems\n(detection systems). Most previous methods assume linear object motion and\nGaussian distribution. However, person motion is inherently non-linear, and the\ntypical LiDAR sensor frame rate of 10Hz, coupled with TBD detection module\nlatency of 0.3-0.7 seconds, often violates the Gaussian distribution assumption in\nthe tracking module. To address these issues, we propose a Dynamic Unscented\nKalman Filter (D-UKF). We leverage the D-UKF to estimate the trajectory\nstate. The prediction process can be described by:\n$(Xt-1, Wt) = (Tt\u22121, Pt-1, \u043a)$,\n$(Tt, Pt) = UT(f(xt\u22121), Wt, Q)$,\n$(Dt, Pz,t) = UT(h(Xt\u22121), Wt, R)$,\nwhere T denotes the prediction model, D means the detection model, P is the\ncovariance matrix of the previous frame, Q is the prediction model system's", "latex": ["(Xt-1, Wt) = (Tt\u22121, Pt-1, \u043a)", "(Tt, Pt) = UT(f(xt\u22121), Wt, Q)", "(Dt, Pz,t) = UT(h(Xt\u22121), Wt, R)"]}, {"title": "3.4 Association", "content": "The Generalized 3D Generalized IoU (GIoU) is a widely adopted association\nmetric in object tracking. This metric enhances the traditional IoU (Intersection\nover Union) by accounting for the spatial relationship between non-overlapping\nbounding boxes, thereby providing a more comprehensive measure of object over-\nlap. However, GIoU solely reflects the proportion of overlap, neglecting crucial\nfactors such as volume size and object viewpoint. To address these limitations,\nwe incorporate the concept of Complete-IoU (CIoU) into our association frame-\nwork. CIoU extends the capabilities of GIoU by considering additional geometric\nproperties, thus offering a more nuanced approach to object association. We sug-\ngest new metric by modifying CIoU metric as Modified Complete-IoU(MCIOU).\nMCIOU is specifically designed to reflect the unique characteristics of person\ntracking, particularly taking into account the ratio of person height:\n$\u03c5 = \\frac{4.0}{\u03c0}(arctan(\\frac{h_{s}}{Areas}) - arctan(\\frac{h_{t}}{Areat}))$,\n$\u03b1 = \\frac{\u03c5}{v(1-GIOU + 1)}$,\n$MCIOU = GIOU + \u03b1$\nNevertheless, IoU-based association methods inherently face challenges in Re-\nIdentification (Re-ID) tasks, particularly when dealing with objects that have\nbeen absent from the scene for extended periods. This limitation arises from the\nreliance on threshold distances for matching, which can fail to associate objects\nthat have undergone significant spatial displacement over time. To mitigate this\nissue, we augment our association strategy by incorporating feature similarity\nmeasures. This additional dimension allows for more robust matching, especially\nin scenarios where spatial proximity alone is insufficient. we utilize bird's eye\nview(BEV) features sampled via region of interest (ROI) as feature similarity.\nConsidering that the matched object ROI features of the two branches, we design\nthe feature similarity (FS) as:\n$FS = exp(\\frac{f_{s}(f_{t})^{T}}{||f_{s}||_{2}||f_{t}||_{2}})$", "latex": ["\u03c5 = \\frac{4.0}{\u03c0}(arctan(\\frac{h_{s}}{Areas}) - arctan(\\frac{h_{t}}{Areat}))", "\u03b1 = \\frac{\u03c5}{v(1-GIOU + 1)}", "MCIOU = GIOU + \u03b1", "FS = exp(\\frac{f_{s}(f_{t})^{T}}{||f_{s}||_{2}||f_{t}||_{2}})"]}, {"title": "3.5 Life Cycle Management", "content": "While previous approaches often rely exclusively on high-confidence 3D bound-\ning boxes, our method incorporates all predicted 3D boxes to enhance recall\nmetrics. To address the challenges associated with this comprehensive approach,\nwe adopt a holistic strategy as follows: Firstly, we utilize the F1 score as the\nclassification criterion. We associate the track pool with all detection results us-\ning our previously described association method. This approach, however, can\npotentially lead to decreased precision due to the inclusion of low-confidence de-\ntections and increased computational complexity in the association step, which\nemploys the Hungarian algorithm. In contrast to conventional algorithms that\nuse distance-based thresholds for association, we employ a Feature Similarity\n(FS) metric. Among pairs exceeding a distance threshold, we compare their FS\nvalues. Pairs with FS values above a predetermined threshold are retained in the\npair memory pool. Matched pairs undergo processing through a Dynamic Un-\nscented Kalman Filter (UKF) and a Low-Pass Filter (LPF) for confidence score\nsmoothing. The LPF mitigates rapid fluctuations in confidence scores, applying\nto both matched trajectory scores and detection result scores. If the LPF result"}, {"title": "4 Experiments", "content": "We implemented our proposed methodology using Python and C++, and con-\nducted experiments on an Intel Xeon Gold 5218R CPU (2.10GHz) with 125GB\nRAM. All reported results reflect online performance.\nDatasets We evaluated our method on two datasets: the KITTI tracking dataset,\nand a our own dataset. The KITTI tracking dataset was recorded in Karlsruhe,\nGermany, using a 64-beam LiDAR sensor at a 10Hz frame rate. Following the\nconvention in, we used sequences 1, 6, 8, 10, 12, 13, 14, 15, 16, 18, and 19 as\nthe validation set. Our custom Office dataset was captured using a Hesai QT128\nLiDAR sensor. This sensor features a vertical Field of View (FOV) of 105.2\u00b0\n[-52.6\u00b0, +52.6\u00b0], a horizontal FOV of 360\u00b0, a range of 0.05 to 50 meters, and\noperates at a frame rate of 10Hz. We collected person tracking data in an indoor\nenvironment using this LiDAR sensor.\nEvaluation Metrics and Criteria For the KITTI dataset, we conducted 3D\nMOT evaluation on the validation set, as the test set only supports 2D MOT\nevaluation and its ground truth is not publicly available. We followed the KITTI\nconvention, presenting results for Pedestrian. Regarding matching criteria, we\nadopted the convention from the KITTI 3D object detection benchmark, us-\ning 3D IoU to determine successful matches. Specifically, we employed 3D IoU\nthresholds (IoU thres) of 0.25 for pedestrian.\nOn KITTI, we also report SAMOTA [56] and HOTA [31] metrics, which\nallows us to analyze detection and association errors separately. We also report\nthe number of identity switches (IDs) at the best performing recall."}, {"title": "4.2 Benchmark results", "content": "KITTI 3D Pedestrian validation set benchmark In Table 5, we present a\nsummary of the pedestrian tracking results. Our proposed system demonstrates\nsuperior performance compared to other modern tracking algorithms. Across\nall metrics, with the exception of ID switches (IDs), our method achieves the\nbest results. It's worth noting that AB3DMOT shows the lowest number of\nID switches. However, this is primarily due to its significantly lower detection\ncapability. To provide a fair comparison, we modified AB3DMOT to use the same\ndetector as our SpbTracker, denoted as AB3DMOT*. With this modification,\nAB3DMOT* exhibits the highest number of ID switches.\nOffice 3D person tracking In Table 6, AB3DMOT* denotes the AB3DMOT\nalgorithm using our detector instead of its original one. The office dataset presents\na more challenging environment compared to KITTI due to the presence of\ncrowded objects. Additionally, the LiDAR sensor used in our office dataset has\na low range than KITTI's sensor and the frame rate is lower. These factors con-\ntribute to the overall lower performance results compared to those obtained on\nthe KITTI dataset. It's important to note that all tracking algorithm parame-\nters, including those for our method and the other algorithms evaluated, remain\nconsistent with the settings used in the KITTI experiments. This comparison\nhighlights the robustness of our tracking system across different environmental\nconditions and sensor configurations, while also demonstrating the challenges\nposed by more complex scenarios such as crowded indoor environments."}, {"title": "KITTI 2D Pedestrian Tracking", "content": "In Table 4, we present the 2D Pedestrian\ntracking results obtained on the KITTI test set. Although our tracking is per-\nformed in 3D space, we can report 2D tracking results by projecting the 3D\nbounding boxes onto the image plane using camera intrinsic and extrinsic pa-\nrameters. We then report the minimal axis-aligned 2D bounding boxes that fully\nenclose these projections as the tracks' 2D positions. Despite focusing on 3D\ntracking and using 2D detections only as a secondary cue, our method achieves\nstate-of-the-art results in terms of SMOT\u0391, \u039c\u039f\u03a4\u0391, \u039cOTP, and IDSW metrics\nfor the 3D modality(except for models considering only 2D image space). No-\ntably, our SMOTA and IDSW metrics are state-of-the-art even when compared\nto 2D+3D multi-modal models. This performance demonstrates the effective-\nness of our 3D tracking approach, which can compete with and even surpass\nmethods that directly operate in 2D or use both 2D and 3D information. Our\nresults highlight the potential of 3D-focused tracking methods in achieving high\nperformance across both 3D and 2D evaluation metrics."}, {"title": "5 Conclusion", "content": "In this paper, we analyzed the \"TBD\" 3D tracking algorithm and proposed\nseveral enhancements for improved person tracking. Our contributions include\na person-biased detector, MCIOU, feature similarity measures, a specific person\nmotion model, a robust filter, and long-term life-cycle memory, all of which\nlead to significant performance improvements. However, the use of large-scale\nmemory in the life-cycle model results in increased matching time. To address\nthis, we optimized the code using C++, but further research is needed to develop\na more compact algorithm. Future work will explore learning-based methods for\nend-to-end tracking with multi-modal, moving away from the rule-based TBD\napproach."}]}