{"title": "Hashing based Contrastive Learning for Virtual Screening", "authors": ["Jin Han", "Yun Hong", "Wu-Jun Li"], "abstract": "Virtual screening (VS) is a critical step in computer-aided drug discovery, aiming to identify molecules that bind to a specific target receptor like protein. Traditional VS methods, such as docking, are often too time-consuming for screening large-scale molecular databases. Recent advances in deep learning have demonstrated that learning vector representations for both proteins and molecules using contrastive learning can outperform traditional docking methods. However, given that target databases often contain billions of molecules, real-valued vector representations adopted by existing methods can still incur significant memory and time costs in VS. To address this problem, in this paper we propose a hashing-based contrastive learning method, called DrugHash, for VS. DrugHash treats VS as a retrieval task that uses efficient binary hash codes for retrieval. In particular, DrugHash designs a simple yet effective hashing strategy to enable end-to-end learning of binary hash codes for both protein and molecule modalities, which can dramatically reduce the memory and time costs with higher accuracy compared with existing methods. Experimental results show that DrugHash can outperform existing methods to achieve state-of-the-art accuracy, with a memory saving of $32\\times$ and a speed improvement of $3.5\\times$.", "sections": [{"title": "Introduction", "content": "Due to the high failure rates of drug candidates, drug discovery requires a long development cycle and substantial costs [1]. Virtual screening (VS) is a critical step in computer-aided drug discovery, aiming to identify molecules that bind to a specific target receptor. Representative receptors include biological macromolecules such as protein, DNA and RNA. High-quality VS could identify promising lead compounds, thereby reducing time and resource costs in drug discovery. The size of the molecular database is an important factor affecting the effectiveness of VS. Increasing the database size can typically include more candidate molecules. However, as shown in [2], a poorly performing VS method will increase the false positive rate when the database is enlarged. This poses two requirements for effective VS: a large-scale molecular database and an effective VS algorithm.\nNumerous large-scale molecular databases have been developed for drug discovery. For instance, ZINC [3], a commercially available molecular database, boasts a vast collection of over 1.4 billion compounds in its latest version ZINC20 [4]. The Enamine REAL database [5] serves as a robust tool for large-scale virtual screening, featuring a substantial repository of over 6.75 billion molecules in its current release. However, despite the availability of these diverse databases, existing VS methods are not equipped to screen such large-scale databases. Traditional VS methods, such as molecular docking, are often too time-consuming for screening large-scale molecular databases. Taking the AutoDock Vina [6] as an example, when the exhaustiveness is set to 8, it takes about 70 seconds to dock a molecule on a single-core CPU. Hence, docking millions of molecules, the scale of which is comparable with or even smaller than those in many real applications, would take Vina more than two years. Although increasing the number of CPUs can reduce docking time, this inevitably leads to substantial resource consumption. The learning-based VS methods [7\u201311] are relatively resource-effective. However, most of these methods attempt to predict the binding affinity or interactions between the protein and molecule, which fail to outperform docking methods on VS benchmarks.\nRecently, DrugCLIP [12] proposes to treat VS as a retrieval task rather than learning the binding affinity or interactions of protein-molecule pairs. DrugCLIP allows for the use of more unlabeled training data and enables the pre-encoding of molecular representations into vectors to accelerate the retrieval process. By learning real-valued vector representations for both proteins and molecules through contrastive learning, DrugCLIP can outperform traditional docking methods. However, given that target databases often contain billions of molecules, the real-valued vector representations for molecular databases can still incur significant memory costs in VS. For example, DrugCLIP encodes molecules into 128-dimensional real-valued vectors, and hence the real-valued vector representations for the Enamine REAL database would have a memory cost of more than 3TB, which is extremely large for local computer memory. Moreover, calculating the similarity between billions of real-valued vectors and ranking the results is time-consuming.\nIn this paper, we propose a hashing-based contrastive learning method, called DrugHash, for VS. DrugHash also treats VS as a retrieval task, but it uses efficient binary hash codes for retrieval. The contributions of DrugHash are outlined as follows:\n\u2022 To the best of our knowledge, DrugHash is the first hashing method for VS.\n\u2022 DrugHash designs a simple yet effective hashing strategy to enable end-to-end learning of binary hash codes for both protein and molecule modalities, which can dramatically reduce the memory and time costs.\n\u2022 DrugHash can also outperform existing methods in terms of accuracy. This seems to be counter-intuitive but is actually reasonable, because binary hash codes can act as a constraint (regularization) for improving generalization ability.\n\u2022 Experimental results show that DrugHash can outperform existing methods to achieve state-of-the-art accuracy, with a memory saving of $32\\times$ and a speed improvement of $3.5\\times$."}, {"title": "Related Works", "content": "Virtual Screening Existing VS methods can be categorized into two main classes: docking- based methods and learning-based methods. Docking-based methods like FlexX [13], Glide [14] and AutoDock Vina [15] involves predicting the molecule conformation and pose given a target protein [6]. Docking relies on complex scoring functions to estimate the binding affinity or strength of the connection across the molecule and the target protein [16]. Random approaches like Monte Carlo and genetic algorithms are performed to search the vast conformation space to find the optimal docking pose. Hence, docking-based methods are typically very time-consuming.\nLearning-based methods can be further categorized into supervised and unsupervised methods. Supervised methods are supervised by the binding affinities or interactions between molecules and proteins. Methods such as OnionNet [7], FAST [8], Planet [9], and CAPLA [17] treat the problem as a regression task that predicts the binding affinities of the protein-ligand complex. On the other hand, a bunch of methods such as BridgeDPI [10], TransformerCPI [18], and AttentionSiteDTI [11] predict whether or not a molecule could bind to the target protein, which is a binary classification task. However, due to the shortage of labeled data, the performance of these supervised methods is limited [19]. Recently, DrugCLIP [12] proposes to treat VS as a retrieval task which is unsupervised. By learning real-valued vector representations for proteins and molecules in a contrastive learning approach and applying data argumentation, DrugCLIP can surpass traditional docking methods on VS benchmarks. However, the real-valued vectors for feature representations will bring large memory and time costs to screen billion-scale molecular databases."}, {"title": "Method", "content": "In this section, we present the details of our proposed method called DrugHash, which is a hashing- based contrastive learning method. DrugHash treats VS as a retrieval task. Although the techniques in DrugHash can be used for various VS problems, we focus on protein-ligand complexes [27\u201329] in this paper. Hence, the term protein actually refers to the protein pocket. For brevity, we will continue to use protein in the following content. The term ligand refers to a small molecule that binds to a target protein. The complexes reveal the structural and functional relationships between proteins and molecules. We treat the complexes as data of two modalities: proteins and molecules. DrugHash uses protein queries to retrieve molecules in the database.\nThe hash function can be manually designed or learned from the training data. We focus on learning hash functions for proteins and molecules. DrugHash contains the protein and molecule encoder and the objective function which includes a contrastive learning objective and a cross-modal hashing objective."}, {"title": "Protein and Molecule Encoder", "content": "Various protein and molecule encoders can be adopted in DrugHash. To show the effectiveness of our proposed hashing strategy, we adopt the same encoder as DrugCLIP [12], which is a pre-trained SE(3) Transformer proposed in Uni-Mol [31]. The input of the encoder are atom representation which is initialized from atom type and pair representation which is initialized by encoding the Euclidean distance between pairs of atoms using a Gaussian kernel [32]. We denote the pair representation of ij in layer l as $q_{ij}^l$. $q_{ij}^l$ is first updated by the Query-Key product in attention mechanism [33]:\n$q_{ij}^{l+1} = q_{ij}^l + \\{\\frac{Q_h^l (K_h^l)^T}{\\sqrt{d}} \\vert h \\in [1, H]\\},$\\nwhere $Q_h^l$ and $K_h^l$ is the Query and Key of i-th and j-th atom representation in layer l for the h-th attention head, H is number of attention heads, d is the dimension of Key. The pair representation serves as a bias term in self-attention to update the atom representation:\n$\\text{Attention}(Q_h, K_h, V_h) = \\text{softmax}(\\frac{Q_h (K_h)^T}{\\sqrt{d}} + q_{ij}^{l+1} V_h),$\nwhere $V_h$ is the Value of j-th atom representation in layer l for the h-th attention head. The above encoder is pre-trained through 3D position recovery and atom-type recovery tasks. The input atom coordinates are randomly corrupted, and the model is trained to predict the correct positions and pairwise distance of the atoms. The atom types of corrupted atoms are also masked by a [CLS] token, and the model is trained to predict the masked atom type. The protein encoder is pre-trained on 3.2 million protein pockets and the molecule encoder is pre-trained on 19 million molecules. The encoder of DrugHash is initialized by the parameter of the pre-trained encoder.\nFormally, we denote the above encoding process of protein and molecule as $E_p$ and $E_m$ respectively. Considering a set of n complexes, we denote the set of proteins as $P = \\{p_1, p_2, \\dots, p_n\\}$ and the set of molecules as $M = \\{m_1, m_2, \\dots, m_n\\}$. For any index k, $p_k$ and $m_k$ indicate the protein and molecule data originating from the same complex. For a protein-molecule pair $(p_k, m_k)$, their vector representation $(y_p^k, y_m^k)$ could be obtained by:\n$(y_p^k, y_m^k) = (E_p(p_k), E_m(m_k)).$"}, {"title": "Objective Function", "content": "Our objective function includes a contrastive learning objective and a cross-modal hashing strategy. The contrastive learning objective aims to align the representation of proteins and molecules in a shared embedding space. The cross-modal hashing strategy aims to enable end-to-end learning of the binary hash codes for both protein and molecule modalities.\nContrastive Learning We aim to maximize the similarity between correct pairs and minimize the similarity between incorrect pairs. In this context, the word \u201ccorrect\u201d means that the protein and molecule belong to the same complex. The encoded protein set $E_p(P) = \\{y_p^1, y_p^2, \\dots, y_p^n\\}$ and the encoded molecule set $E_m(M) = \\{y_m^1, y_m^2, \\dots, y_m^n\\}$. As shown in Figure 1, there are $n^2$ protein-molecule pairs in total, but only the pairs in the diagonal are supposed to be similar. We use cosine similarity to define the similarity between $y_p^k$ and $y_m^k$:\n$\\text{sim}(y_p^k, y_m^k) = \\frac{y_p^k \\cdot (y_m^k)}{||y_p^k|| ||y_m^k||}.$\nWe use infoNCE loss [34] to define our contrastive learning objective, which aims to minimize the negative log-likelihood of similar protein-molecule pairs. From the perspective of protein modality, we aim to distinguish the true ligand that binds to the given protein:\n$L_c^p = - \\log \\frac{\\exp(\\text{sim}(y_p^k, y_m^k)/\\tau)}{\\sum_j \\exp(\\text{sim}(y_p^k, y_m^j)/\\tau)},$\nwhere $\\tau$ denotes a temperature hyperparameter. From the perspective of molecule modality, we aim to distinguish the true receptor that accepts the given molecule:\n$L_c^m = - \\log \\frac{\\exp(\\text{sim}(y_p^k, y_m^k)/\\tau)}{\\sum_j \\exp(\\text{sim}(y_p^j, y_m^k)/\\tau)}.$\nThe above loss function has been utilized in previous works like CLIP [35] and DrugCLIP [12]. The overall contrastive learning objective is defined as:\n$L_c = \\frac{1}{2} \\sum_{k=1}^n (L_c^p + L_c^m).$\nCross-Modal Hashing Note that the vector representation $y_p^k$ and $y_m^k$ of protein and molecule is still real-valued at this stage. Unlike the previous methods, we aim to learn the binary hash codes for both protein and molecule modality. Let $b_p^k \\in \\{-1, 1\\}^d$ and $b_m^k \\in \\{-1, 1\\}^d$ denote the binary hash codes for protein $p_k$ and molecule $m_k$, where d is the code length, which is the same as the embedding dimension of $y_k$. We define our loss function as follows:\n$L_{hash} = \\frac{1}{n} \\sum_{k=1}^n (||y_p^k - b_p^k||^2 + ||y_m^k - b_m^k||^2).$"}, {"title": "Training and Inference", "content": "We denote the whole parameter of $E_p$ and $E_m$ as $\\theta$. In the training stage, the model parameter $\\theta$ and binary hash codes $b_p^k$ and $b_m^k$ can be optimized alternately. When $\\theta$ is fixed, the $b_p^k$ and $b_m^k$ could be obtained by\n$b_p^k = \\text{sign}(y_p^k),$\n$b_m^k = \\text{sign}(y_m^k),$\nwhere the function sign(\u00b7) returns the signs of the elements. When the binary hash codes are fixed, $\\theta$ can be optimized by backpropagation.\nIn the inference stage, as illustrated in Figure 2, we aim to retrieve molecules that would bind to the target protein from the molecular database. Given a target protein p and molecular database $D_M = \\{m_1, m_2, m_3, \\dots \\}$, the binary hash code $b_p^o$ of protein and binary vector database $B_{D_M}$ of molecules can be obtained as:\n$b_p^o = \\text{sign}(E_p(p)),$\n$B_{D_M} = \\{b_m^1, b_m^2, b_m^3, \\dots \\} = \\{\\text{sign}(E_m(m_1)), \\text{sign}(E_m(m_2)), \\text{sign}(E_m(m_3)), \\dots \\}.$\nThe number -1 in hash codes can be easily changed to 0 to get the final hash code representation. We rank the molecules most likely to bind with the target protein based on Hamming distance, which can be calculated as the different bits of binary hash codes."}, {"title": "Experiment", "content": "We adopt several metrics to evaluate the accuracy of VS: the area under the receiver operating characteristic curve (AUROC), the Boltzmann-enhanced discrimination of receiver op- erating characteristic (BEDROC), and the enrichment factor (EF). AUROC is a commonly used metric to evaluate the ranking performance. BEDORC is a metric proposed in [36], which is sen- sitive to \"early recognition\". The BEDROC is formulated as $\\text{BEDROC} = R_a \\frac{e^{aR_a}(1 - e^{-a})}{(e^{a/N}-1)} \\times \\frac{\\text{sinh}(a/2)}{\\text{cosh}(a/2)-\\text{cosh}(a/2-aR_a)} + \\frac{1-e^{a(1-R_a)}}{1-e^a}$, where n is the number of actives, N is the total number of molecules, $R_a = n/N$, $r_i$ is the rank position of the i-th active and a is set to 80.5 in our experiment."}, {"title": "Results", "content": "Evaluation on DUD-E We compare the AUROC, BEDORC, and EF of DrugHash with baselines on the DUD-E dataset. The results are shown in Table 1. DrugCLIP has several versions in the original paper. For a fair comparison, we train DrugCLIP using the same dataset and evaluate the zero-shot results on DUD-E, and the results are consistent with that of the original paper. DrugHash does also not perform any finetuning on the DUD-E dataset and evaluates the zero-shot results. We can find that DrugHash outperforms the baselines across all metrics. Notably, DrugHash has used the same encoder and training data as DrugCLIP, but it outperforms DrugCLIP by a large margin across all metrics, which shows the superiority of our hashing strategy.\nEvaluation on LIT-PCBA We compare the AUROC, BEDORC, and EF of DrugHash with baselines on the LIT-PCBA dataset. The results are shown in Table 2. LIT-PCBA is a more challenging dataset compared to DUD-E, and we can find that the performance of all methods has declined. DrugHash outperforms all other methods on metrics of BEDROC, $EF^{0.5\\%}$ and $EF^{1\\%}$. The performance of DrugHash on AUROC is not outstanding. However, as pointed out in [36], AUROC is not sensitive to early recognition, which means a successful VS method should rank actives very early among all molecules, because only a small proportion of potential actives will be tested in experiments. So AUROC is less meaningful compared with the other metrics. Although DrugHash may not surpass Banana on $EF^{5\\%}$, it significantly outperforms Banana on $EF^{0.5\\%}$ and $EF^{1\\%}$, and $EF^{0.5\\%}$ and $EF^{1\\%}$ are more aligned with the requirement of early recognition than $EF^{5\\%}$. Moreover, Banana generalizes poorly on the DUD-E dataset. Overall, the experiment results show the effectiveness of DrugHash.\nMemory and Time cost We show the memory and time cost of Planet, Banana, DrugCLIP, and DrugHash when adopting ZINC and Enamine REAL as the target molecule library in Table 3. Unlike the learning-based methods, Vina could only store the raw molecule files. The memory cost of raw molecule files is extremely high, so we do not present them in the table. For the ZINC database, which contains around 2.3 million ready-to-dock molecule files, the memory cost of real-valued vectors is still acceptable for the three real-valued methods. But when the database size increases to the size of the REAL database, which contains 6.5 billion molecules, the memory cost for real-valued vectors rises to more than 3080GB, which is unacceptable for the memory of most computers. However, DrugHash only needs 96GB to store the REAL database, which is about 32 times less than Banana and DrugCLIP, and 75 times less than Planet.\nWe tested the time cost of the above methods when using only one target protein as the query. The time cost of Vina to dock the total ZINC dataset takes around 1863 days, which is already impractical in real-world applications, so we do not show the time cost for the more massive REAL database. Though Planet and Banana could pre-encode the proteins and molecules to vector representations, they have to further fuse the protein and molecule representations to predict the final binding affinity or interaction. On a database with the same size of ZINC, their required time is still acceptable, but on REAL, their time cost rises to tens or even thousands of hours. DrugCLIP takes much less time on both databases compare with the above methods. DrugHash is the fastest method compared with others and achieves more than three times faster than DrugCLIP on the REAL database. DrugHash has more advantages as the number of the target protein increases."}, {"title": "Analysis of Hashing Strategy", "content": "In the experiments mentioned above, we demonstrated the comprehensive advantages of our method in terms of accuracy, storage, and retrieval speed for VS. All these improvements can be attributed to the hashing strategy we designed. In this subsection, we will conduct a detailed analysis of how the hashing strategy works.\nSensitivity Analysis of $\\lambda$ We study the sensitivity of the important hyperparameter $\\lambda$ in DrugHash. The results are shown in Figure 3b. When we set $\\lambda$ to 0, it means we do not adopt any hashing strategy to train the model, but in the evaluation phase, we still use the sign(\u00b7) function to obtain the binary hash codes. It could be seen as the ablation study of the hashing strategy. In this case, we can find that the accuracy dropped by a large margin, which proves the necessity of our hashing strategy. When we tune the $\\lambda$ from 0 to 1.0, the accuracy of the DrugHash initially improves and then declines, with the turning point at 0.2. The sensitivity experiment of $\\lambda$ suggests a careful choice of $\\lambda$ to make the output of the model closer to binary hash codes while avoiding excessive regularization constraints. When the value of $\\lambda$ is between 0.1 and 0.4, the accuracy of binary retrieval can consistently exceed that of real-valued vectors.\nCode Length Experiment We study the influence of the length of the output binary hash codes. The results are shown in Table 4. DrugHash-128 is marked with * to denote our original implementation. We study the output code length of $\\{64, 128, 256, 512\\}$, and the accuracy of DrugHash continuously improves as the code length increases. Therefore, the accuracy of DrugHash could be improved by simply increasing code length. However, a larger code length will bring larger memory and time costs. But due to the efficiency of binary hash codes, DrugHash is capable of adopting larger code lengths. Even with the code length of 512, DrugHash requires less than 200GB of memory cost to store the REAL database. DrugHash offers a good trade-off between accuracy and cost in real-world applications.\nOverfitting Analysis We provide an explanation of how the hashing strategy enhances model accuracy. We demonstrate the loss and BEDROC curves of DrugHash on the validation set with and without the hashing strategy in Figure 4a and Figure 4b. In Figrue 4a, We can find that the model without the hashing strategy begins to overfit after 35k steps, with the loss on the validation set continuing to increase. However, after adding the hashing strategy, the loss on our validation set continues to decrease and reaches a lower validation loss compared to when hashing is not used. A similar phenomenon can be observed in Figure 4b. The model without the hashing strategy shows a decline in BEDROC in the later stages of training, while the model with the hashing strategy maintains a higher BEDROC. The experiment indicates that the hashing strategy could alleviate the model overfitting and improve the accuracy."}, {"title": "Conclusion", "content": "In this paper we propose a hashing-based contrastive learning method, called DrugHash, for VS. DrugHash treats VS as a retrieval task that uses efficient binary hash codes for retrieval. In particular, DrugHash designs a simple yet effective hashing strategy to enable end-to-end learning of binary hash codes for both protein and molecule modalities, which can dramatically reduce the memory and time costs with higher accuracy. Experimental results show that DrugHash can outperform other existing methods to achieve state-of-the-art accuracy, with a memory saving of $32\\times$ and a speed improvement of $3.5\\times$. We also conduct detailed experiments about how the hashing strategy works."}, {"title": "Limitations & Broader Impact", "content": "There exists some limitations in DrugHash. Firstly, in DrugHash, we designed a simple yet effective hashing strategy to conduct a preliminary exploration of the efficacy of hashing in VS tasks. Actually, more hashing strategies could be explored to address the binarization problem. Secondly, the effect of the size of training data could be further explored. In this paper, we use PDBBind as the training set. Larger complex datasets like BioLip and ChEMBL could be used to train the model. Thirdly, since different encoders could be adopted in DrugHash, a more expressive encoder could be designed. We leave the above directions for future work."}]}