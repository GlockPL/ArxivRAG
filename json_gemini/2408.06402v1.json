{"title": "PHAGO: PROTEIN FUNCTION ANNOTATION FOR\nBACTERIOPHAGES BY INTEGRATING THE GENOMIC CONTEXT", "authors": ["Jiaojiao Guan", "Cheng Peng", "Xubo Tang", "Yongxin Ji", "Wei Zou", "Jiayu Shang", "Yanni Sun"], "abstract": "Bacteriophages are viruses that target bacteria, playing a crucial role in microbial ecology. Phage\nproteins are important in understanding phage biology, such as virus infection, replication, and\nevolution. Although a large number of new phages have been identified via metagenomic sequencing,\nmany of them have limited protein function annotation. Accurate function annotation of phage\nproteins presents several challenges, including their inherent diversity and the scarcity of annotated\nones. Existing tools have yet to fully leverage the unique properties of phages in annotating protein\nfunctions. In this work, we propose a new protein function annotation tool for phages by leveraging\nthe modular genomic structure of phage genomes. By employing embeddings from the latest protein\nfoundation models and Transformer to capture contextual information between proteins in phage\ngenomes, PhaGO surpasses state-of-the-art methods in annotating diverged proteins and proteins with\nuncommon functions by 6.78% and 13.05% improvement, respectively. PhaGO can annotate proteins\nlacking homology search results, which is critical for characterizing the rapidly accumulating phage\ngenomes. We demonstrate the utility of PhaGO by identifying 688 potential holins in phages, which\nexhibit high structural conservation with known holins. The results show the potential of PhaGO to\nextend our understanding of newly discovered phages.", "sections": [{"title": "1 Introduction", "content": "Bacteriophages (phages) are viruses that can infect bacterial cells. They are highly prevalent and abundant in the\nbiosphere, being found in various environmental matrices, including gastrointestinal tracts of animals, water bodies,\nand soil [7, 33, 35]. Accumulating studies have demonstrated the important role of phages in microbial communities.\nFor example, phages have been observed to facilitate the horizontal transfer of genes between bacteria, which can\ninfluence bacterial adaptation, evolution, and acquisition of new functionalities [12]. In addition, they can modulate\nthe abundance and diversity of bacterial populations by killing their host [9]. Due to the increasing threats posed by\nantibiotic resistance, phages have gained significant attention as potential alternatives to traditional antibiotics, as they\ncan lyse pathogenic bacteria. [4, 18, 19].\nDespite the significance of phages, the efficacy of their applications heavily relies on prior knowledge of protein\nfunctions. Understanding the protein function enables us to identify phage proteins that can target and disrupt essential\nbacterial processes, offering the potential for the development of targeted antimicrobial therapies [26]. For example,\nholin proteins, known for their cell-killing capabilities and broad host range, have gained significant attention for their\npotential applications in bacterial control. [25, 27]. To accelerate the application of phages, it is crucial to figure out the\nannotation of the proteins in phages.\nGene Ontology (GO) terms are widely used to annotate the phage proteins. They are standardized vocabulary and\nhierarchical frameworks comprising three key dimensions: biological process (BP), cellular component (CC), and\nmolecular function (MF) [8]. BP encompasses the sequences of events or pathways in which proteins participate, such\nas cellular signaling or metabolic processes, while CC pertains to the subcellular locations or structures where proteins\nare localized, such as the nucleus or plasma membrane. The MF aspect centers on the distinct activities and tasks\ncarried out by proteins, such as enzyme catalysis or receptor binding.\nHowever, there are two major challenges to using GO terms to annotate phage protein. First, the number of phage\nproteins with known GO labels is limited. Until February 27, 2024, the total number of phage proteins from the National\nCenter for Biotechnology Information Reference Sequence Database (NCBI RefSeq) is 541,060, derived from 5, 160\ncomplete genomes. However, only 20.85% percent of proteins have GO labels. This scarcity of labeled proteins results\nin an insufficient database for comprehensive functional annotation. Second, although phage encodes a small number\nof proteins compared to their hosts, these proteins exhibit a remarkable degree of functional diversity. For example,\namong the 1173 phage proteins provided by the UniProtKB database, there are a total of 912 GO terms. This means\nthat on average each GO label contains less than two supporting samples and will bring challenges to computational\nmethods. Moreover, the distribution of these GO terms is imbalanced, with certain terms being more prevalent or\nspecific compared to others. This imbalanced label distribution poses a significant challenge to accurate classification.\nThese obstacles impose great requirements on annotation tools."}, {"title": "2 Methods and materials", "content": "The proteins in the phage sequences are similar to the words in the natural language. Thus, the phage genomes can be\nviewed as a language of phage life that exhibits distinct features. One notable observation of these phage languages is\nthat phage proteins within the same genus tend to maintain a consistent arrangement. These characteristics inspire us to reformat the\nphage genomes into sentences with contextual proteins and predict the annotations based on the surrounding information.\nIn the following section, we will detail how PhaGO leverages the contextual information for phage protein annotation."}, {"title": "2.1 Embedding protein sequences", "content": "Fig. 2 shows the architecture of the PhaGO model. In Fig. 2A, let the number of proteins of a phage genome in the\ntraining process be n. The first step is to encode the phage genomes by generating the embedding of the n proteins.\nTo obtain protein embedding, we employ the ESM2 model which is pre-trained on protein sequences sourced from\nUR50/D. During training, ESM2 selects 15% amino acids for masking and predicts amino acids at the masked position.\nBased on a third-party benchmark result [20], ESM2-33 has a better performance than the ProtT5 family. Moreover,\nthe performance of the ESM2-33 is comparable with ESM2-36 and ESM2-48, but the latter two models have more\nparameters, leading to a significant increase in runtime. Specifically, the ESM2-33 model consists of approximately 650\nmillion parameters, while the ESM2-36 and ESM2-48 models contain 3 billion and 15 billion parameters, respectively.\nTherefore, we chose ESM2-33 to embed the proteins.\nWe define $d_e$ as the dimension of per-residue embedding and impose a maximum limit of 1,024 residues for each protein\nsequence, which aligns with the default setting of the ESM2 model. By applying the ESM2 embedding to the protein\nsequences and considering these constraints, we generate an embedding matrix $X_1$ with dimensions of 1,024 \u00d7 $d_e$ for\neach protein shown in Fig. 2B. In the ESM2-33 model, the default value of $d_e$ is 1,280. Subsequently, we pass $X_1$\nthrough a fully connected (FC) layer, resulting in a one-dimensional feature set denoted as $X_2$."}, {"title": "2.2 Learning the relationship of context proteins using Transformer", "content": "As words and sentences in human language derive meaning through their context and relationships with other linguistic\nelements, proteins can also be better understood by considering their interactions, dependencies, and roles within the\ngenome. Therefore, we annotate the protein functions by considering the context neighbors. This goal is achieved\nthrough two steps: preparing the context protein embedding and learning the relationships among proteins within the\nsame genome. The sequential steps are illustrated in Fig. 2C.\nTo obtain the context protein embedding, first, we treat each protein as a token and contigs can be seen as sentences\ncomposed of multiple tokens. Then, we combine the embeddings of each protein into a single embedding with\ndimensions of n \u00d7 $d_e$. This integration process takes into account the order in which the proteins appear in the contigs\nand allows us to preserve the contextual relationships among the proteins within the same genomic context.\nTo incorporate positional information, we utilize position embedding. The position embedding component takes the\nindex of each protein as its input and generates an embedding vector that encodes the relative position of the token\nwithin the sequence. This allows the model to understand the relative distance of the proteins. The final output $X_3$ of\nthe embedding layer is obtained by summing the context protein embedding and position embedding results, resulting\nin a comprehensive representation of each protein in the sequence.\nAfter embedding the context proteins into an n \u00d7 $d_e$ matrix, we introduce a crucial component in our architecture:\nthe self-attention layer. This layer plays a vital role in learning intricate connections between proteins. To perform\nself-attention computations in Eqn. 1, we transform the input matrix into three separate matrices: Query (Q), Key (K),\nand Value (V) through three independent FC layers. The n \u00d7 n attention matrix is computed by multiplying the Q\nand K, representing the strength of protein associations. To prevent excessive values, we scale the attention matrix by\ndividing it by the square root of the dimension of matrix K (denoted as $\\sqrt{d_k}$). Next, we normalize the attention matrix\nusing the softmax function, assigning weights to protein pairs to indicate their relative importance. Finally, we score the\nproteins in the sequence by multiplying the V with the weight matrix.\nIn order to collectively focus on information stemming from diverse representation subspaces, we employ a multi-head\nmechanism in Eqn. 2, where each head represents a separate self-attention layer. Computation is performed in parallel\nacross all heads, and then the concatenated head is input into an FC layer shown in Eqn. 3, $W^M \\in R^{1280\u00d71280}$.\nFollowing the multi-head attention block, the resulting output $X_4$ is subsequently passed through a feed-forward layer."}, {"title": "2.3 Predicting the GO terms", "content": "We formulate the protein function annotation task as a multi-label binary classification task. The goal is to assign a\nprobability to each GO term, indicating the likelihood of the protein being associated with that specific function. The\nresults of the feed-forward layer $X_5$ are input into a fully connected layer with the sigmoid activation function and the\noutput is an m-dimensional vector, where m represents the number of GO terms shown in Eqn. 4."}, {"title": "2.4 Integrating PhaGO with alignment-based method", "content": "Considering that proteins with significant alignment usually have high-precision GO prediction results[6, 14], we\nintroduce a hybrid mode named PhaGO+ by incorporating DiamondScore into the PhaGO to enhance the predictive\ncapabilities for phage protein annotations."}, {"title": "3 Results", "content": "We evaluate the performance of PhaGO following previous work. Specifically, we present two sets of metrics,\ncorresponding to the prediction accuracy of protein-centric and GO term-centric evaluation, which are used in the\nCritical Assessment of Functional Annotation (CAFA) competitions. The protein-centric evaluation focuses on\ndetermining the function prediction accuracy, whereas the term-centric evaluation aims to examine whether the model\ncan correctly identify proteins associated with a particular functional term [22]. The latter can provide the performance\nof different function terms.\nFirst, we introduce protein-centric metrics. Let $P_i(t)$ be the set of GO terms for a protein i returned by the model under\nthe score cutoff t, while $T_i$ represents the true GO term set for protein i. Then recall and precision for each protein i\nwith threshold t are calculated in Eqn. 7 and Eqn. 8. To calculate the average recall and precision on all proteins, we\ndefine n as the total number of proteins and $n_t$ as the number of proteins that have at least one predicted GO term when\nthe threshold is t. The equations are shown in Eqn. 9 and Eqn. 10, respectively. We record the F1-score calculated for\neach threshold t, ranging from 0 to 1, and obtain the maximum F1-score as $F_{max}$ shown in Eqn. 11. To compute the\nArea Under the Precision-Recall Curve (AUPR), the prediction scores of proteins are concatenated and input into the\nscikit-learn Python package.\nThen, we present the term-centric evaluation. To calculate the term-centric $F_{max}$, we follow a three-step process. First,\nwe calculate the precision and recall for GO term l under threshold t, as defined in Eqn. 12 and 13. In the second step,\nwe calculate $F_{max}(l)$, which is the maximum F1-score for label l under different score cutoffs (Eqn. 14). Finally, we"}, {"title": "3.2 Dataset", "content": "We downloaded the reference genomes and proteins under the Caudoviricetes class from the NCBI RefSeq database.\nDue to the lack of GO terms in the Refseq database, we mapped the protein accessions into UniProt database [1] using\nthe 'ID mapping' tool and retrieved annotations.\nTo ensure an adequate number of labeled proteins for training, we labeled the proteins with no GO terms using the\nProkaryotic Virus Remote Homologous Groups (PHROG) database [29] based on HHsuite tool [28]. The database\ncontains 38,880 PHROGs, which encompass a total of 868,340 proteins derived from complete genomes of viruses\ninfecting bacteria or archaea. Moreover, we saved the hits that demonstrated a probability of the template being\nhomologous to query sequences exceeding 80%, ensuring the reliability and high confidence of the matches between the\nphage proteins and the entries in the PHROG database. Although we used the pairwise alignment to extend the dataset,\nthe proteins with significant alignments were only 15.51%. The remained 63.64% proteins still lacked annotations,\nwhich further demonstrated the necessity and importance of developing an effective phage protein annotation tool.\nBecause of the requirement for rich contextual information for proteins, we selectively focused on proteins from genera\nwith high annotation rates. The annotation rate for each genome is calculated below."}, {"title": "3.3 PhaGO outperforms the state-of-the-art predictors", "content": "In this experiment, we compared PhaGO with four tools: DiamondScore[14], DeepGOCNN[14], DeepGOPlus[14],\nand PFresGO[21]. These tools are the most widely used pipelines for general protein function annotation and have\nbeen demonstrated as state-of-the-art predictors. The same training dataset was utilized for retaining the learning-based\nmethods (DeePGOCNN, DeepGOPlus, and PFresGO) or constructing the database for the alignment-based methods\n(DiamondScore). The performance evaluation was then carried out using the same test dataset, which ensured a fair and\ncomparable assessment for all methods.\nThe performance based on term-centric is presented in Table. 2, while the results obtained from protein-centric\nevaluation can be found in Supplementary Table 3. PhaGO+ outperforms the second-best method, regarding both\nAUPR and Fmax scores with notable improvements across all three categories. Specifically, the improvements of 9.94%,\n6.50%, and 10.67% in AUPR and 6.49%, 4.67%, and 6.65% in Fmax scores for BP, CC, and MF, respectively.\nComparing PhaGOBASE and PhaGOLARGE, the results reveal that using a larger protein foundation model has a\nbetter performance because of the larger foundation model's ability to capture and learn more intricate biological\nsignals. The most significant improvement is observed in the MF category, with a notable increase of 7.57% in AUPR\nand 4.69% in Fmax.\nAdditionally, integrating DiamondScore with PhaGO through hybrid approaches can further improve the performance\nin protein function prediction. Comparing PhaGO and PhaGO+, the BP category exhibits a highest improvement of\n6.49% and 4.49% in AUPR and Fmax for PhaGOBASE and 2.54% and 2.26% in AUPR and Fmax for PhaGOLARGE\n+.\nTaken together, utilizing a deeper foundation model and integrating homologous search methods can help PhaGO\nachieve the best performance in protein function prediction. In addition, based on the performance, PhaGO BASE and\nPhaGOLARGE are recommended for users. However, in scenarios where computational resources are constrained,\nPhaGO BASE is the preferable choice.\n+"}, {"title": "3.4 PhaGO improves annotation of proteins by utilizing the contextual information", "content": "In this section, we designed two experiments to evaluate how contextual proteins impact function prediction. In the\nfirst experiment, we compare two different usages of the protein embeddings from the foundation model: 1) using the"}, {"title": "3.5 PhaGO shows superior performance in annotating remote homologous proteins", "content": "In this section, we evaluate PhaGO's predictive capability with different levels of sequence identity. The test dataset\nwas partitioned into three distinct groups based on alignment with the training data: 'no-alignment' for proteins lacking\nalignment, 'min-40%' for those below 40% identity, and \u201840%-100%' for proteins with 40-100% identity. These\ncategories represent proteins with no similarity, low similarity, and moderate to high similarity to the training set,\nrespectively. As shown in Fig. 5(b), the AUPR of all methods improved with increased sequence identity for all\nthree GO categories. For the high-similarity dataset, the alignment-based method exhibits excellent performance, and\nPhaGO+ demonstrates comparable results in three ontologies. It suggests that both methods can effectively predict\nprotein functions when the dataset aligns well with the training dataset. However, for the dataset that has no alignment\nwith the training dataset, PhaGO+ stands out with impressive AUPR. Specifically, PhaGO+ achieves AUPR values\nof 0.7524, 0.8478, and 0.8210 for the BP, CC, and MF. These values represent improvements of 5.68%, 6.78%, and\n5.75% compared to the performance of the second-best method. The term-centric results are shown with a similar trend\nin supplementary Fig. 1(a). Additionally, the percentage of no-alignment proteins accounts for 27.93%, 55.70%, and\n27.62% of the test dataset for BP, CC, and MF, respectively. These results highlight the robustness and effectiveness of\nPhaGO+ in predicting protein functions, especially for low-similarity proteins.\nWe continue to analyze the impact of contextual protein information on different level-similarity groups. The results are\nshown in the Supplementary Fig. 1(b). Focusing on the no-alignments dataset, both PhaGOBASE and PhaGOLARGE\ndemonstrate improvements compared to their respective counterparts. On one hand, PhaGOBASE shows performance\ngains of 10.18%, 6.5%, and 1.11% for BP, CC, and MF categories, respectively. On the other hand, PhaGOLARGE\nexhibits improvements of 5.33%, 3.87%, and 7.91% for BP, CC, and MF categories, respectively."}, {"title": "3.6 PhaGO enhances annotation with focus on minority class GO terms", "content": "To examine the ability of PhaGO on the GO terms of different popularities, we split GO terms into three groups based\non the information content (IC) of GO shown in Eqn. 17. f (l) is the frequency of the GO term l in the training dataset.\nHigher IC values mean fewer proteins annotated by the GO term labels.\nThe experiment results in Fig. 5(b) demonstrate that all methods consistently performed well in the majority labels of\nGO terms. However, PhaGO+ demonstrates a distinct advantage in predicting minority GO terms, surpassing the other\nmethods and achieving the highest performance across all three ontologies. Specifically, PhaGO+ achieves medium\nAUPR of 0.8801, 0.9043, and 0.8105 for BP, CC, and MF in the smallest GO terms group, respectively. This indicates\nthat even for infrequently occurring GO terms, PhaGO+ can make an accurate prediction.\nWe also further investigate the impact of the context proteins on the different GO terms. The results are shown in\nSupplementary Fig. 1(c). Focusing on the smallest GO terms group, both PhaGOBASE and PhaGOLARGE demonstrate\nimprovements in performance. For the BP and CC ontologies, PhaGOBASE shows performance gains of 4.5% and\n3.55% in AUPR, respectively. Moreover, it achieves comparable results for MF. In addition, PhaGOLARGE exhibits\nimprovements of 3.71%, 2.29%, and 2.80% for BP, CC, and MF categories, respectively. These results highlight the\nbenefits of incorporating context proteins in predicting fewer GO terms."}, {"title": "3.7 PhaGO enables protein function annotation without relying on homology search", "content": "To showcase the utility of PhaGO in annotating proteins that lack homology search results, we explore its application in\nthe analysis of phage's holin proteins. The holin protein is a small membrane protein that plays a crucial role in lysing\nbacterial hosts by triggering the formation of pores that disrupt the host cell membrane [23]. It controls the release of\nphages and the completion of the lytic cycle, underscoring the significance of the intricate interplay between phages\nand their host organisms. However, according to the protein annotation of phages in the RefSeq database, over 448\ngenera have no annotated holin proteins, indicating that holin proteins may be very diverse across different phages. In\nthis experiment, we apply PhaGO to annotate possible holin proteins.\nAccording to statistical analysis of GO terms for the well-studied holin proteins from UniproKB, we manually selected\nsix GO terms as their indicator. The details of selecting GO terms are shown in the Supplementary file. We input\nall proteins from 448 genera into PhaGO+ and identified 688 potential holin proteins spanning 262 genera. After\nidentifying possible holins, we clustered them to analyze their relationship. To accomplish this, we aligned them\nall against all and selected alignment with identity and coverage larger than 90. Gephi was used to represent the\nrelationships among proteins visually. The results depicting the top 10 phage genera in Fig. 6(a). The genera of phage\nare from the RefSeq annotations. An evident observation is the high conservation of holin proteins within the same\ngenus, mirroring a common pattern observed among known holin proteins in phage genomes.\nIn addition, we aligned them with the known holin proteins using BLASTP [3] with e-value 1e-5. 590 proteins have no\nalignment, indicating the high diversity of holin proteins. Then, we searched the annotation of 688 proteins from the\nUniProtKB database. Out of these, 335 proteins are recommended for annotation as holin, 171 proteins are labeled as\nuncharacterized proteins, and 87 proteins are categorized as membrane proteins. These top three annotations collectively\naccount for 86.2% of the total proteins.\nTo further examine the identified holin proteins without alignment, we employed ESMFold [16] to predict their three-\ndimensional (3D) structures, which are very fast and can get comparable predictions with AlphaFold [13]. We found\nthat despite having low sequence similarity, 590 identified holin proteins exhibit structural homology with the known\nholin proteins. The result is shown in 6(b). The TM-score and the Root Mean Square Deviation (RMSD) value are\ncalculated by the TM-align tool [36]. Fig. 6(c) and (d) are visualizations of the two putative holin proteins identified\nby our tool. In conclusion, the experiments provide further evidence of the great potential of PhaGO as a valuable\ntool for viral protein annotation. In addition, the information and 3D structure of the 688 holins are available in the\nSupplementary data."}, {"title": "4 Conclusion and Discussion", "content": "In this work, we proposed a method named PhaGO/PhaGO+ for protein function annotation of phages. The major\nimprovement in our approach can be attributed to utilizing the properties of phages and the foundation model. The\nTransformer model is used to learn the relationship of the genomic context proteins. Our experiments compared four"}, {"title": "5 Data availability", "content": "PhaGO is implemented in Python, which can be downloaded from https://github.com/jiaojiaoguan/PhaGO."}, {"title": "6 SUPPLEMENTARY DATA", "content": "Supplementary Data are available at NAR Online."}, {"title": "7 Competing interests", "content": "No competing interest is declared."}]}