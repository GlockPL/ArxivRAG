{"title": "Word Sense Disambiguation in Native Spanish: A Comprehensive Lexical Evaluation Resource", "authors": ["Pablo Ortega", "Jordi Luque", "Luis Lamiable", "Rodrigo L\u00f3pez", "Richard Benjamins"], "abstract": "Human language, while aimed at conveying meaning, inherently carries ambiguity. It poses challenges for speech and language processing, but also serves crucial communicative functions. Efficiently solve ambiguity is both a desired and a necessary characteristic. The lexical meaning of a word in context can be determined automatically by Word Sense Disambiguation (WSD) algorithms that rely on external knowledge often limited and biased toward English. When adapting content to other languages, automated translations are frequently inaccurate and a high degree of expert human validation is necessary to ensure both accuracy and understanding. The current study addresses previous limitations by introducing a new resource for Spanish WSD. It includes a sense inventory and a lexical dataset sourced from the Diccionario de la Lengua Espa\u00f1ola which is maintained by the Real Academia Espa\u00f1ola. We also review current resources for Spanish and report metrics on them by a state-of-the-art system.", "sections": [{"title": "1. Introduction", "content": "The goal of language is the communication of meaning but the human language is inherently ambiguous, with uncertainty brought on by pragmatic, syntactic and lexical factors. A term becomes lexically ambiguous when it has several meanings. For instance, the Spanish term \"banco\" can be used to describe both a financial organization and a bench in a public space. If the intended meaning cannot be determined from the context, this form of ambiguity may cause misunderstanding and misinterpretation. In the field of Natural Language Processing (NLP), the difficult task of computationally determining the appropriate meaning of a word in a particular context is known as word sense disambiguation (WSD). The ability to navigate and resolve ambiguity is crucial for successful computational systems. Despite the intended lexical meaning of a word in context can be, to some extend, determined automatically by WSD algorithms, unfortunately, external knowledge like a predefined sense inventory is a fundamental component for these algorithms. It provides the essential data to associate senses with words, and usually is a scarce resource, mainly in English, an issue known as the knownledge acquisition bottleneck [1].\nThe NLP community tends to maintain the working assumption that word meaning can be discretized in a finite number of classes [2], thus casting polysemy resolution as a multi-class classification problem, where the classes, e.g. the senses are specific to a word. Senses are registered in a dictionary like resource called the sense inventory. In WSD the sense inventory is virtually always The Princeton WordNet (WNG) [3] together with some parallel corpus annotated with senses, like MultiSemCor and Babel, just multilingual versions of WordNet. Nonetheless they suffer by the use of semi-automatic methods for data harvesting [4], non-accurate automatic translations or by the semi-automatic validation of the word senses and granularity [5]. A drawback stemming from the fact that every language is inherently subject to change and interpretation which undoubtedly requires a native speaker validation and a high level of expertise.\nHowever, as mentioned in a previous study [3], all of these inventories encounter a challenge known as the fine-granularity problem. This issue arises when distinguishing between various meanings of the same word becomes challenging, even for humans. For instance, WordNet lists 29 senses for the noun line, including two that differentiate between a set of horizontally laid out things and one laid out vertically. To address the excessive granularity, coarser-grained inventories have been suggested, but mainly performed on the English language with few extensions to other main languages, such as French, German, Italian and Spanish. Moreover, the meanings of non-English words are translated from English, ignoring many of the nuances of different senses in specific situations.\nThe Spanish language has several specific characteristics that justifies a deeper look in relation to WSD. It is spoken by almost 600 million\u00b9 people in the world of which about 100 million are non-native speakers. A distinguishing factor of the Spanish language, compared to most other languages, is that it is a globally \"regulated\" language with respect for local variations. The Spanish Royal Language Academy (Real Academia Espa\u00f1ola, RAE 2) is a 300-year-old Institution that, in collaboration with all local Spanish language academies, is actively monitoring and managing the Spanish language in all its geographical regions. The main materialisation of this work are the official dictionary of Spanish language DLE, with more than 94,000 entries, 30% more than other commercial Spanish dictionaries, and an average of 2.5 meanings per entry and the Student's Dictionary in Spanish (SDS) a lexicographic work specially designed for students of Spanish. The wide usage of Spanish across the world, combined with the normative approach for its evolution, clearly justifies a Spanish-specific approach for WSD.\nAmong the contributions in this work, we provide a new lexicon resource for WSD in native-Spanish. Furthermore, we provide a comprehensive review of the existing resources and approaches to Spanish WSD and a specific approach by finetuning BERT and RoBERTa based models, by using different combinations of Spanish WSD resources. Finally, we report the"}, {"title": "2. Related Work", "content": "Early WSD experiments included manually crafted lexicons and rules [6]. But as this field's study has advanced, more complex answers have been put forth. There are different approaches in the literature to tackle WSD, including supervised methods, unsupervised methods and knowledge or graph-based methods [3].\nSupervised methods use labeled training data, e.g., sets of encoded examples together with their sense classes or labels, to learn a classifier that can predict the correct meaning of a word in context assigning, e.g., by maximizing the similarity to a single meaning like a single-label classification problem or as a soft multi-label classification problem in which multiple senses can be assigned to each target word [7]. On the other hand, unsupervised methods are based on unlabeled corpora and do not exploit any manually sense-tagged corpus to assign senses to words in context, e.g., based on their cooccurrence patterns [6]. Finally, knowledge-based methods [8], assign senses to words based on their semantic relationships, relying on external resources such as Machine-readable dictionaries (MRDs), like as WordNet [3], or other structured or non-structured knowledge sources like Sense-Annotated Corpora, SemCor [9], among others. Supervised systems, particularly those based on Transformer-based language models, have become the standard approach for WSD [10]. These systems leverage large-scale pre-trained models to learn contextual representations of words and their senses, which can then be used for disambiguation.\nAlthough there is no model exclusively developed for Spanish, there are several multilingual models capable of disambiguating Spanish texts. Among these models we found mainly both supervised and knowledge-based methods. In the supervised category, we encountered systems such as AMUSE-WSD [11] or Multimirror [12]. The former was the first approach to frame the WSD task as down-stream task, using multi-class classification and offering a multilingual WSD system built on top of modern pre-trained language model. In contrast, Multimirror [12] is an approach that addresses the data scarcity issue in a target language by proposing a cross-lingual sense projection approach. It involves aligning parallel sentences from English to a target language for augmenting the low-resourced language.\nOn the other hand, we also encountered a few multilingual knowledge-based systems. First among them is SensEmBERT [13], where the authors create latent representations of word meanings and glosses in various languages, by using sentence embeddings from a multilingual mBERT model and comparing distances with candidate meanings. Additionally, there is the work reported in [14], that introduces SyntagNet a resource made up of manually disambiguated lexical-semantic combi-"}, {"title": "3. Resources for WSD", "content": "The effective execution of WSD algorithms heavily rely on the availability and quality of resources like LKBs. In this section, we delve into the most popular resources that have driven the WSD task."}, {"title": "3.1. Sense Inventories", "content": "The sense inventories list the various meanings a word may have. The most popular are:\n\u2022 Wordnet [16] is a large lexical database with over 120k concepts that are related by over 25 types of semantic relations and comprise over 155k words (lemmas), from the categories Noun, Verb, Adjective and Adverb. It organizes concepts in synsets - sets of synonyms \u2013 and provides, for each of them, one or more sentences in which each one is used with each meaning.\n\u2022 Babelnet [17] is a multilingual semantic network with the principal objective of functioning as an extensive \"encyclopedic dictionary\". Its initial iteration was primarily focused on automatically linking Wikipedia articles and WordNet senses. However, the current iteration, BabelNet 5.0, has considerably expanded its knowledge acquisition scope, drawing insights from 51 distinct sources."}, {"title": "3.2. Sense-Annotated Data", "content": "The resources for WSD in languages other than English are much more limited. We focus primarily on the most prominent multilingual datasets and present two new Spanish resources."}, {"title": "3.2.1. Training Data", "content": "\u2022 SemCor [9] is the largest manually sense-annotated English corpora, is a significant resource for training supervised Word Sense Disambiguation systems. It consists of over 226,000 sense annotations across 352 documents.\n\u2022 MuLaN [18]is a specialized tool for WSD, which can automatically create sense-tagged training datasets in multiple languages. What distinguishes MuLaN is its extensive range of sense categories, achieved through its integration with the BabelNet inventory, in contrast to SemCor, which exclusively depends on WordNet inventory. The dataset includes translations in four distinct languages: German, Spanish, French, and Italian.\n\u2022 SDS5 the Student's Dictionary in Spanish is a lexicographic work specially designed for students of Spanish. It is composed of around 40K terms and is elaborated taking into account text and reference books employed in the educational systems of Spain and America. This resource is available in both printed form and as a mobile application."}, {"title": "DLEval sentence example and meaning tags and glosses", "content": "Siete tazas de caldo\nTaza#NOUN: A183451 A121616\nA22450 A139788\nA139788: Recept\u00e1culo del retrete.\nA121616: Cantidad que cabe en una taza.\nXML example\n<sentence id=\"d001.s10699\">\n<wf lemma=\"siete\" pos=\"ADJ\">Siete</wf>\n<instance id=\"d001.s10699.t0001\" lemma=\"taza\" pos=\"NOUN\">tazas</instance>\n<wf lemma=\"de\" pos=\"ADP\">de</wf>\n<wf lemma=\"caldo\" pos=\"NOUN\">caldo</wf>\nGold file\nd001.s10699.t0001 A121616"}, {"title": "4. Experiments and Results", "content": ""}, {"title": "4.1. Construction of the training and evaluation datasets", "content": "We use the Spanish versions of the SE13 and SE15 evaluation datasets from the mwsd-datasets 8 repository that are extracted from the Babelnet and Wordnet inventories, see section 3.1, For this work, we decide to employ the wn split, which includes a subset of those instances tagged with a BabelNet synset that"}, {"title": "4.3. Evaluation", "content": "Table 4 displays the results obtained for the BERT and ROBERTa models fine-tuned using various combinations of datasets, as well as the performance of recent multilingual approaches for the Spanish WSD task in the SE13 and SE15 benchmarks. The SDS dataset offers a remarkably significant enhancement on the SE15 benchmark over the preceding multilingual WSD systems in Spanish WSD. This improvement is consistent for both BERT and ROBERTa models. In such a case, all Spanish language models trained with a combination including the SDS dataset surpassed the Multimirror system, with the ROBERTa-large model achieving a substantial improvement of +9.5 points in F1-score. For the SE13 benchmark, only the SemCor+SDS dataset still surpasses the best multilingual WSD system but with a moderated improvement of +1.6 points in F1score for the BETO system. The improvement in F1-score is also observed when comparing within each Spanish language model. Specifically, the combination of the MuLaN and SC training datasets along with the SDS training set yields to an improvement of +1 point in F1-score for both SE13 and SE15. It is worth noting that the RoBERTa-large model is able to leverage the potential of combining MuLaN-SDS by reporting the best F1-score, 78.12% on the DLEval benchmark."}, {"title": "4.4. Discussion", "content": "Table 4 shows the benefit of using a lexicon knowledge manually curated by experts and Spanish-only trained models, in comparison to currently multilingual systems for native Spanish WSD task. The best results obtained on the DLEval are those reported by the models fine-tuned employing the SDS dataset in contrast with the results for the SemEval benchmarks. A significant disparity of approximately 15-25% was observed when comparing the models trained with SC-SDS compared to those trained using the SDS dataset, indicating a considerable mismatch between the two benchmarks that might be due to the automatic translations in the case of SE benchmarks. Finally and based on the results, it is reported that RoBERTa models consistently demonstrate superior performance compared to the BETO model. This is likely attributed to the better data curation of the datasets utilized during the pre-training and the higher number of parameters for the large model."}, {"title": "5. Conclusion", "content": "This paper presents a novel Spanish lexicon evaluation resource, which offers an extensive coverage and encompasses a wide range of potential lexical combinations. The DLEval exhibits exceptional precision, owing to its entirely manual validation by high-level experts. The models' performance is capable of either matching or surpassing the state-of-the-art results achieved by most approaches for the Spanish Word Sense Disambiguation task. This demonstrates the advantages of incorporating lexical knowledge, specifically expert validated senses and glosses, in fine-tuning neural models."}, {"title": "4.2. Fine-tuning Large Language Models", "content": "We adopt a similar approach as in [23], treating the task as a multi-label classification problem. In this setting, the WSD task consists on disambiguate the senses of a target word $w_t$ in a sentence $W = w_1,...,w_t,...,w_m$. For each target word $w_t$, the goal is to map it to a pre-defined sense $s \\in S_{kw}$, where $S_{kw} = s_{1w}, s_{2w},..., s_{kw}$ is the set of k pre-defined candidate senses for w. The meaning of each sense is defined by the gloss. The candidate senses have a corresponding gloss set defined as $G_{wk} = g_{w1},g_{w2},..., g_{wk}$. The models used in our experiments are BERT and ROBERTa based models and fully pretrained using Spanish corpus. BETO [24] is a BERT-base model with 110M parameters trained on a compilation of large Spanish unannotated corpora [?] and by using the Whole Word Masking technique. RoBERTa-base and RoBERTa-large are models that were developed simultaneously [25] by the Barceloan Supercomputing Center (BSC). They are ROBERTa based models that differ mainly in the number of parameters, 115M and 774M respectively. We fine-tune the models, using the training datasets from the section 3.2.1, for five epochs on a distributed NVIDIA 3090 RTX 24GB GPUs cluster, using 24-72 GPU hours in total, depending on the size of both the model and the dataset. We used the trainer class of the Huggingface transformers [26] library in python, adapted for the multiple choice paradigm with cross-entropy loss and Adam as the optimization algorithm. The learning rate was set to 2e-5 with a weight decay of 0.01 and with a batch size of 16 and kept all other hyperparameters at their default values. We perform model selection choosing the checkpoint with highest accuracy on a validation dataset based on accuracy. Note that for training we use k = 4 senses, including the target sense. For testing we perform as many test as necessary to reach the total number of meanings of a given target word, and the candidate sense with the highest score is the predicted sense produced by the system."}]}