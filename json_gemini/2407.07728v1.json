{"title": "SAMOYE: ZERO-SHOT SINGING VOICE CONVERSION BASED ON FEATURE DISENTANGLEMENT AND SYNTHESIS", "authors": ["Zihao Wang", "Le Ma", "Yan Liu", "Kejun Zhang"], "abstract": "Singing voice conversion (SVC) aims to convert a singer's voice in a given music piece to another singer while keeping the original content. We propose an end-to-end feature disentanglement-based model, which we named SaMoye, to enable zero-shot many-to-many singing voice conversion. SaMoye disentangles the features of the singing voice into content features, timbre features, and pitch features respectively. The content features are enhanced using a GPT-based model to perform cross-prediction with the phoneme of the lyrics. SaMoye can generate the music with converted voice by replacing the timbre features with the target singer. We also establish an unparalleled large-scale dataset to guarantee zero-shot performance. The dataset consists of 1500k pure singing vocal clips containing at least 10,000 singers.", "sections": [{"title": "1 Introduction", "content": "SVC aims to transform a singer's timbre of a given song into another singer while keeping the music content like rhythm and melody consistent. Although numerous studies have been carried out on zero-shot text-to-speech synthesis(TTS) [Ju et al., 2024, Ju et al., Tan et al., 2024] and voice conversion (VC), limited studies focus on SVC. Many studies like sovits-svc \u00b9 need at least ten minutes to hours of clips to fine-tune the model for high-quality singing voice conversion.\nPrevious SVC studies generally disentangle the signal feature into the timbre and content features using separate pretrained encoders. Then a decoder directly generates the converted audio or generates the Mel-spectrogram for vocoders to obtain the converted audio. Existing studies often use pretrained automatic speech recognition (ASR) models like Hubert [Hsu et al., 2021], Whisper [Radford et al., 2023], or ContentVec [Qian et al., 2022] to extract the content features, which contain more content and less acoustic information. For timbre features, some studies introduce speaker embedding from a speaker recognition model trained through supervised learning [Luo et al., Polyak et al.]. Other studies learn to embed the Mel-spectrogram as timbre features through training. Adversarial learning is widely used in SVC studies to reconstruct the singing waveform using the disentanglement features and introduce a discriminator to judge the generated and original data from multiple scales."}, {"title": "2 Related Work", "content": "Compared with voice conversion, SVC is more challenging in maintaining the music content like the pitch and rhythm meanwhile the singing voice is more variable than speaking. Existing SVC studies mainly apply feature disentanglement-based methods to extract different features from the audio and generate the converted audio after replacing timbre-related features with the target singer's [Liu et al., a, Huang et al., Sha et al.]. Some studies use speaker embedding as the timbre-related feature in the feature disentanglement stage. [Nercessian] use a pretrained LSTM to extract the speaker embedding and concatenate it with the original speaker's phoneme and loudness embedding in the decoder to generate the Mel-spectrogram of the converted audio. PitchNet [Deng et al.] introduces a singer prediction network and pitch regression network to control the timbre and pitch stability. The embeddings of the two networks are fed into the decoder with the output from the encoder to generate the audio. [Luo et al.] use separate encoders to extract singer and techniques embedding for singer and techniques classification task and are concatenated before feeding into the decoder and refinement network to generate the converted audio. [Polyak et al.] takes as input the speech features by a pre-trained automatic speech recognition (ASR) model Wav2Letter, the F0 feature by Crepe, and the loudness feature from the power spectrum. The speaker embedding from the target singer is included in the generator for converted audio generation. [Li et al.] introduce F0 features, PPGs features as content representations, and the Mel-spectrogram as the timbre representation and is enhanced through singer classification and reconstruction. FastSVC [Liu et al., b] leverages sine-excitation signals and loudness features and uses a Conformer model to extract content features. These features passed through a feature-wise linear modulation-based generator to synthesize waveform directly."}, {"title": "3 Datasets", "content": "The dataset scale plays a vital role in training the zero-shot SVC model. However, existing unparalleled datasets usually contain a limited number of song clips and speakers. We establish a large-scale unparalleled dataset through crawling, covering 1500,000 music clips with more than 10,000 speakers. As the data process is shown in Fig. 1, we separate the pure human voice from these music clips using Demucs [D\u00e9fossez et al., 2019]. In addition, we get the lyrics in the songs and the start time and end time for each word by Funasr and Whisper for Chinese and other languages respectively. Consequently, the obtained data are manually checked to remove all the errors in the recognition results."}, {"title": "4 Methods", "content": ""}, {"title": "4.1 Content Feature Enhancement", "content": "The training process of Content Feature Enhancement in the Samoye model is shown in Fig. 2, in which we first read the corresponding audio clips according to the time intervals labeled by the lyrics to get the paired text and audio. For the lyrics, we use pypinyin and g2p_en to transform each text into the corresponding phoneme according to the language difference and transformed into the corresponding indexes sequence. We also utilize Bert to extract features from the sequence of phone indexes. For the singing audio clips, we apply Hubert to get the audio features. Then we use an alignment process to enhance the content features to accurately reflect the phonemes and reduce the original timbre. Specifically, we introduce a content predictor, which is used to assist feature alignment through a speech content prediction task. The phoneme features and content features are input to the content predictor for cross-prediction, minimizing the prediction loss so that the two features are aligned and can reflect the original content information. The content predictor is a GPT-based model, where the text embedding layer outputs the phoneme embedding and fuses it with the corresponding Bert features. The Hubert features are quantized by the residual vector quantization codebook to obtain the content tokens. Subsequently, a classifier is introduced to cross-predict the content tokens and the phoneme tokens."}, {"title": "4.2 Singing Voice Synthesizer Adversial Learning", "content": "In Samoye model, we introduce an adversarial training strategy to combine aforementioned encoders and decoders for synthesizing converted audio, as depicted in Fig. 3. The timbre features extracted from the Mel-spectrogram, the F0 predicted by RMVPE, and the learned Hubert content tokens are input together in the encoder and embedded respectively. Then, these three types of features are input through the cross-attention module and sum up as the fused representation. The fused representation then passes a Transformer encoder to get the prior distribution. Besides, the content features and timbre features are passed through a posterior encoder to get the posterior distribution. In the training stage, the posterior distribution is input into the Glow model then a decoder to synthesize the converted audio. During adversarial training, the discriminator is used to discriminate the generated audio and spatio-temporal features map from the stacked convolutional network. The generator loss consists of the L1 loss from the scale of the Mel-spectrogram and waveform between the generated and original ones. In addition, the KL divergence loss between the prior and posterior distribution is also included."}, {"title": "4.3 Singing Voice Conversion Strategy", "content": "In the inference stage of the Samoye model, as shown in Fig. 4, we can replace timbre features with those extracted from Mel-spectrogram of target singer's clips. The prior distribution derived from content, F0, and timbre features as opposed to posterior one inputs into Glow which is then followed by decoder for generating conversion result."}]}