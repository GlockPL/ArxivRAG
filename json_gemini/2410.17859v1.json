{"title": "DATATALES: A Benchmark for Real-World Intelligent Data Narration", "authors": ["Yajing Yang", "Qian Liu", "Min-Yen Kan"], "abstract": "We introduce DATATALES, a novel benchmark designed to assess the proficiency of language models in data narration, a task crucial for transforming complex tabular data into accessible narratives. Existing benchmarks often fall short in capturing the requisite analytical complexity for practical applications. DATATALES addresses this gap by offering 4.9k financial reports paired with corresponding market data, showcasing the demand for models to create clear narratives and analyze large datasets while understanding specialized terminology in the field. Our findings highlight the significant challenge that language models face in achieving the necessary precision and analytical depth for proficient data narration, suggesting promising avenues for future model development and evaluation methodologies. The data and code are available at https://github.com/yajingyang/DataTales/.", "sections": [{"title": "1 Introduction", "content": "Data narration, the process of transforming intricate data into compelling narratives (Dourish and G\u00f3mez Cruz, 2018), plays a critical role in shaping business decision-making. By distilling vast amounts of information into digestible narratives, it empowers executives with clear and actionable insights (Dykes, 2019; El Outa et al., 2020). Moreover, it fosters accessibility to valuable information, reaching a wider audience. However, traditional manual approaches are burdened by both time constraints and the potential for inaccuracies. Consequently, there has been a longstanding anticipation for models capable of autonomously extracting meaningful insights from data (Demiralp et al., 2017; Ding et al., 2019).\nThe rise of large language models (LLMs), such as GPT-3 (Brown et al., 2020) and Llama (Touvron et al., 2023a), signifies a beacon of hope within the field. These models demonstrate extraordinary capabilities, evidenced by their increasing utilization in advanced data analyses (Xie et al., 2023). Empirical evidence highlights that LLMs are effective in reasoning and analytical tasks, achieving performance comparable to or exceeding humans in certain areas (OpenAI et al., 2023; Anthropic, 2024). Their ability to understand and generate fluent natural language sentences suggests their potential for data narration tasks. This leads to an important research question: Can LLMs achieve proficiency on data narration?\nHowever, assessing the proficiency of LLMs in data narration is hindered by the limitations of existing benchmarks. Though related to data-to-text, data narration's complexity surpasses current data-to-text tasks which focus on basic information"}, {"title": "2 Related Work", "content": "Data-to-Text Generation. Datasets like RotoWire (Wiseman et al., 2017), WikiBio (Liu et al., 2018) and ToTTo (Parikh et al., 2020) convert data to text in open domains, providing coherent data descriptions but lacking substantial reasoning crucial for generating insightful financial narratives. This limitation is also observed in domain-specific datasets (WeatherGov (Liang et al., 2009), E2E (Novikova et al., 2016), MLB (Wiseman et al., 2017), and Dart (Nan et al., 2021)) and those emphasizing short inputs and limited analysis types (LogicNLG (Chen et al., 2020), Numeric-NLG (Suadaa et al., 2021), and SciGen (Moosavi et al., 2021), such as simple arithmetic and causal analysis. These characteristics contrast with the extensive complex reasoning required for proficiently narrating extensive data.\nTable Insight Generation. PivotTable (Zhou et al., 2020) and AnaMeta (He et al., 2023) are datasets designed to transform table data into structured insights, with PivotTable focusing on data aggregation and reasoning, and AnaMeta enhancing field semantics with derived supervision labels. Methodologically, Foresight (Demiralp et al., 2017), Voder (Srinivasan et al., 2019), DataShot (Wang et al., 2020b), Table2analysis (Zhou et al., 2020), and Calliope (Shi et al., 2021) propose insight classification taxonomies and utilize recommendation assessment metrics. Contrasting against their primary focus on visual representations. Our work emphasizes textual narratives to meet the data narration demand,\nFinancial NLP. Financial NLP tasks encompass a wide spectrum, ranging from fraud detection, which aims to identify irregular activities (Boulieris et al., 2023), to sentiment analysis, which assesses market sentiment through nuanced language interpretation (Malo et al., 2014; Atzeni et al., 2017; Maia et al., 2018). Question answering tasks, such as FiQA (Maia et al., 2018), TAT-QA (Zhu et al., 2021), FinQA (Chen et al., 2022a) and ConvFinQA (Chen et al., 2022b), further amplify the complexity by requiring comprehensive financial data synthesis. Despite illustrating significant advancements in reasoning complexity, these tasks often lack the analytical depth required for data narration. To the best of our knowledge, we are the first to release a data narration-tailored benchmark.\nNews Narration. News narration focuses on extracting narratives from unstructured text, such as news articles or social media posts (Santana et al., 2023; Keith Norambuena et al., 2023). Generating news narratives requires the identification of events and participants, and linking them by their temporal or spatial information (Chieu and Lee, 2004; Nallapati et al., 2004; Chen and Chen, 2012; Wei et al., 2014; Chen et al., 2015). In contrast, data narration involves identifying patterns and trends from structured data, which often requires complex reasoning over multiple data points."}, {"title": "3 The DATATALES Benchmark", "content": "We outline our data collection procedure employed for compiling DATATALES. Subsequently, we conduct a comprehensive analysis to underscore its unique contributions."}, {"title": "3.1 Dataset collection", "content": "The creation process involves three key steps to curate a dataset for data narration (Figure 2).\nStep 1: Market Report Collection. We select online sources that publish daily market reports covering a wide range of sectors (equity, treasury, currency, commodities) with significant analytical depth, including causal analysis, trend analysis, and predictions. The chosen platforms for this purpose are Investrade, Totalfarmmarketing, VT Markets, and LeapRate\u00b9. From these sources, we compile a dataset of 4.9k reports, ensuring comprehensive market coverage and analytical rigor.\nStep 2: Sentence Classification. We enhance the dataset by focusing on narratives grounded in tabular data. We employ ChatGPT with in-context learning for sentence-level classification, categorizing sentences into Market Movements, Market Context, External Events and Influence, and Prediction and Suggestion, based on the main type of information they convey. (details in Appendix A). Retaining only Market Movements and Predictions sentences ensures that the content is derived from tabular data. This reduces the report to 54.4% of its original length on average, and focuses each report on data-driven insights.\nStep 3: Data Extraction and Alignment. We obtain the corresponding tabular data by identifying the commonly described financial instruments, and extracting data from Yahoo! Finance2, CME\u00b3, Investing.com, WSJ5, and Barchart. Our manual verification process involves sampling reports"}, {"title": "3.2 Analytical Operations Analysis", "content": "The processed market reports of DATATALES are narrated with analytical operations. We identify seven most common operations, ranging from simple lookup and basic quantitative ones such as subtraction to more advanced analysis like causal analysis and predictive analysis (Table 2). Each category constitutes a significant portion of the report content while a sentence may involve multiple analysis, as indicated by the provided percentages. Figure 3 illustrates how these operations are involved in the market reports.\nSimple Lookup (83%). Lookup operations involve the retrieval of data points. They are the most common operation and serve as a prerequisite for more complex tasks. For example, trend analysis requires lookup of market prices to identify market movement.\nBasic Quantitative Operations (94%). Common quantitative operations include comparison, subtraction, and rate of change. Comparison, such as comparing a market index's performance to a benchmark or historical average, provide insights into relative performance and trends. Subtraction and rate of change require numerical computation to obtain exact operation result. While subtraction is one-hop operation, rate of change operation is multi-hop atomic operation, posing higher numerical computation requirements.\nAdvanced Analytical Operations (84%). Advanced analytical reasoning, including trend analysis, causal analysis, and predictive analysis, forms the majority of sentences. These operations often require cross-referencing facts to draw conclusions, illustrating the reasoning complexity. For example, trend identification might involve analyzing moving averages or comparing past highs/lows, necessitating a system that integrates domain knowledge and performs sophisticated analytical tasks. The high prevalence of such operations highlight the importance of DATATALES.\nOn average, we observed 2.6 operations per sentence, emphasizing the need for high-level data analytical capabilities in constructing insightful narratives. These analytical operations are not performed in isolation, but applied to specific entities and along a temporal dimension to extract meaningful insights."}, {"title": "3.3 Contextual Analysis", "content": "To further understand the context of these analytical operations, we examine the entities and temporal expressions in the DATATALES reports. This contextual analysis reveals the key focus areas and time frames shaping the reports' narrative structure.\nEntities. Entities in a market report form the basis for comprehensive analytical approaches, including cross-entity comparison and causal analysis (Table 3). Replicating the curation aspect in generating reports is important; although our reports cover many entities (8.7 on average) with high variance (3 for oil to 22 for equities), only a subset of (5.69 on average) is discussed in detail.\nTime. The temporal aspect of market data is crucial in unveiling trends and projecting future movements. Like the selective detailing of entities, data spanning from the immediate day to several years is analyzed, pinpointing insightful patterns for inclusion in the report (Figure 4). The insightfulness evaluation of model generations underscores the importance of extended tabular data, which enriches the analysis by providing a comprehensive historical context (see Section 5.2).\nScaling data across entities and time challenges data narration models in effectively integrating large input volumes. To navigate this complexity and convey meaningful insights, market reports must employ a domain-specific financial lexicon."}, {"title": "3.4 Lexical Analysis", "content": "The precise and professional language in financial market reports is crucial for accurately describing entities, trends, and analytical results. This specialized vocabulary enables clear communication of complex insights derived from extensive data analysis, effectively conveying the intended message to the target audience.\nEntity. Market reports refer to entities with different terminologies (\u201cgreenback\u201d for US dollar) or their characteristics (\u201cshort-term bond\" for 1-year bond), highlighting the domain knowledge and linguistic versatility required of data narration.\nAnalytical Operations. Analytical results are conveyed with precision in the reports, using specific verbs like \"correct\" and \"reclaim\" to contrast current movements against past trends, and \"pressure\" and \"push\" to indicate both direction and causality between market events. This necessitates a deep understanding of the analytical results and a strong linguistic selection capability to produce reports of comparable proficiency."}, {"title": "4 Experimental Setup", "content": "We define the task of financial data narration as follows: given market movement data ${T_{i,j}|i \\le E_T, j < D_T}$ with $E_T$ financial entities and $D_T$ days, where $T_{i,j}$ is the row of entity i on date j, a data narration model $\\mathcal{M}$ generates a report y narrating the market data:\n$y = \\mathcal{M}(T_{i,j}|i < E_T, j \\le D_T)$\nModels. We explore both open-access models, specifically Llama2-7B-Chat and Llama2-13B-Chat (Touvron et al., 2023b), alongside close-access models like GPT-3.5-Turbo and GPT-4 (OpenAI et al., 2023). These models are renowned for their robust capabilities, particularly in terms of zero-shot generalization on new tasks.\nEvaluation Setup. We evaluate real-world scenarios with no training data and examine the model's learning ability from examples. We assess both zero-shot and fine-tuned scenarios, splitting data based on time (first 80% for training, remaining 20% for validation and testing). We fine-tune with AdamW and a linear scheduler (learning rate: 1e-4, batch size: 16). We load models in 8-bit mode and fine-tune for 5 epochs on DATATALES using"}, {"title": "5 Experimental Result and Discussion", "content": "Table 4 benchmarks model performance in zero-shot and fine-tuning settings under two scenarios: using same-day or one-week prior tabular data.\n5.1 Factuality Analysis\nFactuality is critical for generating useful reports. However, results show that tested LLMs do unacceptably poorly at predicting key numbers in data narration, even with fine-tuning (sub 30%).\nSurprisingly, including one week of historical data detracts from performance, despite sentences describing weekly dynamics in our dataset (Figure 4), possibly due to the difficulty in finding the correct value from a larger dataset, as indicated by the zero-shot Lookup operation results in Figure 6, which impacts all analyses building on them.\nIt's important to note that our automated factuality evaluation method, while resembling a continuation task, was chosen due to the lack of reliable automatic evaluation techniques for freely generated narrations with extensive reasoning. To address this limitation and study the causes of low accuracy, we supplemented our automated evaluation with manual analysis.\nSpecifically, we manually identified analytical operations within sampled sentences from freely generated reports and evaluated their accuracy. This analysis revealed when inaccuracies occur most frequently (Figure 6). LLMs failed to achieve required accuracy for all operations, with the error rate rising with operation complexity.\nCausal Analysis. Causal analysis accuracy depends on the complexity of information sources and the directness of the causal relationships. Simple analyses, like attributing market trends to sentiment or sector performance to index movements, rely on readily available data, making causality clearer. However, complex analyses such as linking market movements to external news, are challenging. LLMs occasionally succeed in correlating events like corn prices with freeze damage in the exporting country, but the overall accuracy is low. We surmise this is due to 1) the lack of input data, forcing the reliance on potentially outdated pretraining knowledge; 2) difficulty filtering vast amounts of news to pinpoint relevant causes; and 3) the need for multi-hop processes identifying relevant causes from vast potential arrays of causes is most challenging, requiring deep understanding of the news and their potential market impact.\nTrend Analysis. Trend analysis accuracy is tied to the duration covered. Short-term analyses are more accurate (52%) due to readily available data. Mid- and long-term analyses have lower accuracies (17.6% and 0%, respectively) because of unavailable extended-period data and complex longer-term analysis (e.g., the computation of 200-days moving average and Relative Strength Index). Enhancing accuracy requires methods to access and incorporate broader historical data and improved model capabilities for accurate complex data analysis, suggesting the potential of LLMs with large token limits and insight recommendation methods.\nPredictive Analysis. Predictive analysis builds on causal and trend analysis to generate deep analytical capabilities, such as forecasting gold prices to remain low (trend) due to a strengthening dollar (causality). Yet, we are the first to cover predictive analysis in data narration task to the best of our knowledge. However, despite its importance, the subjective nature of these predictions, unverifiable at report generation, complicates accuracy assessment. Given that even human experts err in market predictions, we believe the logic underpinning forecasts is more crucial than its precision.\nOur error analysis reveals the potential of DATATALES as a comprehensive benchmark for evaluating analysis operations with different complexity in data narration. The dataset's inclusion of extended historical data aligns with real-world scenarios, challenging models to perform complex operations and advanced analyses crucial for deep insights. While our current factuality evaluation method effectively assesses the models' ability to generate faithful continuations, it may not completely disentangle factual and stylistic choices. However, this approach aligns well with the goal of faithful data narration. Alternative evaluation methods, such as multiple-choice prediction, can be significantly impacted by the quality and order of the choices, potentially faltering in assessing LLMs' capabilities (Wang et al., 2024)."}, {"title": "5.2 Insightfulness Analysis", "content": "The insightfulness of market reports largely determines their value. Table 4 shows that in zero-shot settings, larger GPT models, like GPT-4, have higher accuracy but lower insightfulness scores, while larger Llama2 models exhibit the opposite, suggesting GPT-4's fact-grounding limits in-depth analysis despite larger models' better reasoning. Fine-tuning increases impact scores, but significance scores improve only with same-day data. Reports using one week's history yield higher significance scores, proving the value of longer historical data in enhancing insight depth.\nTo understand the possible contribution of the analytical operations, we match the scores with the operations identify during content accuracy evaluation. Table 5 presents average scores for sentences involved various analysis operations.\nImpact. The impact score of a sentence in a financial report depends on the scope and relevance of the information. Sentences describing broader market trends or analyzing groups of entities receive higher scores, providing a more comprehensive view. For example, sentences discussing \"corn futures\" as a group are typically scored as 4, while those mentioning specific entities like \"corn Apr future\" are scored as 3. Complex analyses building upon low-level operations, which focus on specific entity, yield higher average scores by combining information from multiple entities or time periods to reveal overarching patterns and insights.\nSignificance. Sentences with advanced analytical operations are rated highly significant because significant market changes often require in-depth examinations, such as trend or causal analysis. These methods provide insights beyond immediate market movements, like predictive analysis offering actionable information for investment decisions, making them crucial for comprehensive market analysis. The gap between impact and significance scores for trend analysis is due to the large portion of short-term trends described in GPT-4 generation, yielding higher accuracy but less insight.\nThe demonstrated importance of advanced analytical operations in generating insightful content underscores the value of DATATALES as a benchmark for data narration.\nWe also perform model-based evaluation using win rates over human-generated reports as judged by GPT-4. However, a notable gap emerges between model-based evaluations and human assessments. Instead, it is found to strongly correlate with report length with based on the 12 sets of experiment results, suggesting a model bias favoring longer reports (See Appendix D.3)."}, {"title": "5.3 Style Analysis", "content": "Style analysis is crucial for evaluating models' performance on DATATALES, as it assesses their ability to generate market reports resembling human experts' writing style, reflecting their capacity to produce informative, readable, and domain-consistent reports.\nFine-tuned LLaMa2 models significantly improve in capturing the desired writing style, with a threefold BLEU score increase over the base model (Table 4), suggesting DATATALES' effectiveness in guiding lexical choices. GPT-4 exhibits the lowest BLEU score, highlighting DATATALES' unique challenges compared to general NLP benchmarks. The cosine similarity (Pradhan et al., 2015) of entities and verbs used by the models (Table 6) further supports these findings, with the fine-tuned LLaMa2 achieving higher similarity scores with human experts. These findings underscore the importance of domain-specific fine-tuning for generating high-quality market reports.\nThis style analysis relies on automated evaluation metrics, such as BLEU scores and cosine similarity, which provide valuable insights into models' performance. Although these metrics may not capture all aspects of the generated reports, they offer a scalable and efficient way to assess models' ability to generate market reports of the desired style. Future work could benefit from incorporating human evaluation to provide qualitative and nuanced feedback, complementing these automated metrics."}, {"title": "6 Conclusion & Future Work", "content": "We identify a crucial gap in financial data narration: the lack of benchmarks that offer deep insights, essential for real-world applications. To address this, we introduce DATATALES, a novel dataset covers complex analytical operations, and enhanced by extensive data for more impactful and significant insights and domain-specific languages for higher proficiency. The complexity of DATATALES poses significant challenges to state-of-the-art models, in terms of both their poor accuracy and the lack of insight on their generated text. This complexity arises from the requirements of performing complex analytical operations, incorporating large input data, and accessing relevant knowledge.\nMoving forward, we propose three focused areas to advance financial data narration. Firstly, refining the analysis by using methods like DataShot (Wang et al., 2020b) and Table2analysis (Zhou et al., 2020) to recommend insights with specific metrics as an intermediate step prior text narrative generation to enhance analytical quality. Secondely, integrating visuals into narratives, with methods like Foresight (Demiralp et al., 2017), Voder (Srinivasan et al., 2019), and Calliope (Shi et al., 2021), to improve storytelling. Lastly, developing new automated evaluation focused on accuracy and insight quality using table fact-checking models (Li et al., 2023; M\u00fcller et al., 2021; Gu et al., 2022) and insight evaluation frameworks (Ding et al., 2019; Zhou et al., 2020) respectively."}, {"title": "Ethics Statement", "content": "We have thoroughly investigated the legal aspects of using the scraped data and are confident that our dataset can be released without infringing copyright laws. The publishers' terms of use allow non-commercial use, and robots.txt files permit web scraping. For tabular data, we only release extraction scripts to ensure compliance with copyright regulations. We have taken utmost care to respect the intellectual property rights of the original data providers while creating a valuable resource for the research community.\nOur study involved voluntary participation from former colleagues without financial compensation. We designed evaluation tasks to align with participants' professional expertise and implemented data anonymization to ensure privacy and confidentiality.\nWe acknowledge our technology's potential impact on financial analytics and emphasize responsible use. Key considerations include:\n\u2022 Employment effects: As AI-generated reports advance, we must address potential impacts on financial sector jobs and promote human-AI collaboration.\n\u2022 Human oversight: We advocate for maintaining human expertise alongside AI-generated reports. Professionals should review and validate AI outputs for accuracy and context.\n\u2022 Transparency: We recommend clearly disclosing the use of AI-generated content in financial communications to maintain trust and inform stakeholders.\nWe are committed to responsible AI development in financial analytics and encourage users of our dataset and technology to implement appropriate ethical safeguards."}, {"title": "Limitations", "content": "Firstly, the method and dataset are primarily designed for languages with limited morphology, such as English. Secondly, our DATATALES dataset is specifically focused on market movement data, which represents only 52% of the content for a human generated report. Further research can explore the inclusion of market context and external news, to provide more in-depth analysis especially for causal analysis and predictive analysis. Lastly, our DATATALES dataset focuses on textual narratives, while charts are found to be useful for a market report in real-world. It would be beneficial for future studies to aim for a multi-modal report to provide a more useful reports."}, {"title": "A Sentence Classification", "content": "We classify the raw market reports into the following categories based on their main source of information.\n\u2022 Market Movements: describing tangible shifts such as asset prices or trends;\n\u2022 Market Context: providing broader understanding and context of the market dynamics;\n\u2022 External Events and Influence: highlighting outside events that impact the market;\n\u2022 Prediction and Suggestion: encompassing forward-looking statements based on current data and analysis.\nThe classification was done using ChatGPT with in-context learning:"}, {"title": "B Report Generation Results", "content": "We provide the report generated by human and baseline models for same market (equity market) and report date (2023-01-24), demonstrating the conclusions about factuality, insightfulness, and text style discussed in Section 5."}, {"title": "C Accuracy Evaluation", "content": "Our automated factuality evaluation process employs a precise method to assess the LLM's ability to predict numerical values accurately. We use Named Entity Recognition (NER) from Stanza to identify numerical entities within the report. The process unfolds as follows:\n1. We truncate the report up to the token immediately preceding any identified numerical entity.\n2. The LLM is then prompted to predict the next token, with the constraint that it must be a numerical value or percentage.\n3. This prediction is compared against the ground truth (the actual value in the original report).\n4. After evaluation, we fill in the ground truth value and expand the context to the next token, repeating the process for subsequent numerical entities.\nThis stepwise approach allows us to systematically evaluate the LLM's factual accuracy in predicting key numerical data points throughout the report."}, {"title": "D Insightfulness Evaluation", "content": "Evaluation Instruction for Market Report Analysis\nObjective\nTo evaluate insights from a given market report based on their impact and significance. Utilize the historical movement data provided for context."}]}