{"title": "Deep Learning within Tabular Data: Foundations, Challenges, Advances and Future Directions", "authors": ["Weijieying Ren", "Tianxiang Zhao", "Yuqing Huang", "Vasant Honavar"], "abstract": "This is a survey article.", "sections": [{"title": "Introduction", "content": "Based on the background discussed above, designing a state-of-the-art representation learning method for tabular data involves three fundamental elements: training data, network architectures, and learning objectives. To improve both the quantity and quality of training data, various data-related techniques, such as data augmentation and generation, are employed or introduced. To better leverage the inherent properties of tabular data, the neural architectures are specifically designed to capture: 1) irregular patterns within each column, including variations in scale, mean, and variance. and 2) complex inter-relationships among different columns. Finally, multiple learning strategies and objectives are defined to facilitate the learning of high-quality representations.\nDespite incorporating three key design elements, most existing surveys on tabular data representation learning primarily concentrate on either neural architectural features or learning methodologies. An early survey article [Sahakyan et al., 2021] provides a comprehensive overview of Explainable Artificial Intelligence (XAI) techniques applicable to tabular data, with a particular emphasis on feature transformation and classical machine learning methods in classification tasks. Subsequently, several surveys have explored synthetic data generation methods. For instance, [Sauber-Cole and Khoshgoftaar, 2022] reviews research on data generation within the healthcare domain, specifically focusing on GAN-based techniques. In addition, [Sauber-Cole and Khoshgoftaar, 2022] investigates the application of GANs to address the class imbalance problem. Among these works, [Borisov et al., 2022a] presents a comprehensive survey that discusses feature transformation, neural network design, and data generation challenges within a broader context. In contrast, [Wang et al., 2024a] systematically reviews and summarizes the recent advancements and challenges associated with self-supervised learning for non-sequential tabular data [Ren and Honavar, 2024]. Furthermore, with the emergence of foundation models and transformer-based large language models (LLMs), recent articles [Ruan et al., 2024] examine the adaptation of these models to tabular data, concentrating primarily on their learning aspects.\nUnlike previous works, we present a comprehensive review of representation learning methods for tabular data, focusing on their universality-effectiveness across a range of downstream tasks. Our discussion provides insights into the underlying intuitions driving these methods and examines how they enhance the quality of learned representations across all three key design aspects. Specifically, we aim to identify and analyze research directions informed by recent state-of-the-art studies that concentrate on neural architecture design, the formulation of corresponding learning objectives, and the effective utilization of training data to improve the quality of learned representations for various downstream tasks."}, {"title": "Survey Scope and Literature Collection", "content": "For literature review, we use the following keywords and inclusion criteria to collect literatures.\nKeywords. \"tabular\" or \"table\", \"tabular\" AND \"representation\", \"tabular\u201d AND \u201cembedding\", \"tabular\" AND \u201cmodeling\u201d, \u201ctransaction data\u201d AND \u201crepresentation\u201d, \"biomedical data\" AND \u201crepresentation\". We use these keywords to search well-known repositories, including ACM Digital Library, IEEE Xplore, Google Scholars, Semantic Scholars, and DBLP, for the relevant papers.\nInclusion Criteria Related literatures found by the above keywords are further filtered by the following criterion. Only papers meeting these criteria are included for review.\n\u2022 Written exclusively in the English language\n\u2022 Focused on approaches based on deep learning or neural networks\n\u2022 Published in or after 2020 in reputable conferences or high-impact journals\nQuantitative Summary Given the above keywords and inclusion criteria, we selected 127 papers in total. Fig. 3 shows the quantitative summary of the paper selected for review. We can notice from Fig. 3a that neural architectures and learning objectives are similarly considered important in designing state-of-the-art methods. Most papers were published at ICLR, NeurIPS, followed by AAAI and ICML (Fig. 3b). According to Fig. 3c, we expect more papers on this topic will be published in the future."}, {"title": "Preliminary", "content": "This section provides definitions and notations used in the paper, describes downstream tasks for tabular data analysis, and highlights the unique properties of tabular data."}, {"title": "Definitions", "content": "Definition 1. (Tabular Data). Tabular data is systematically arranged in a structured format characterized by rows and columns. Each row denotes an individual sample record, while each column signifies a distinct type of feature observation. Each column is composed of a header and a series of values, commonly referred to as cells. Each row is represented as a vector of Mnum numerical features and Mcat categorical features $x = [{x}_{i}^{num}, {x}_{i}^{cat}]_{i=1}^{M_{num},M_{cat}}$, where $x \\in R$ and $x \\in {1,2,..., C_i}$. $C_i$ denotes the size of finite candidate values for the i-th categorical feature.\nDefinition 2. (Classification). Tabular data classification aims to assign predefined class labels $Y = {Y_1, Y_2, \u2026, Y_C }$ to each row of tabular data. Denoted D as a tabular dataset with N samples, X is a row of tabular data and y is the corresponding labels. where $y_i \\in {-1,1}$ is a binary classification task and $y_i \\in {1, 2, ..., C'}$ is a multi-class classification task.\nDefinition 3. (Regression). Tabular data regression has a similar goal to classification tasks, with a key difference in label annotation. In tabular data regression, the objective is to predict a continuous value $y \\in R$ for each row.\nDefinition 4. (Clustering). Tabular data clustering aims to partition X into a group of clusters $G = g_1, g_2,..., g_G$ by maximizing the similarities between tabular rows within the same cluster and the dissimilarities between tabular rows of different clusters..\nDefinition 5. (Anomaly Detection). Tabular anomaly detection is a crucial process for identifying tabular samples within a dataset that significantly deviate from established patterns of normal behavior. This approach typically involves training a model on a labeled dataset D, where the model learns the characteristics that define normal observations. Once trained, the model computes anomaly scores $A = (a_i, ..., a_{|X|})$ for each row in an unseen test set $X_{test}$. These scores quantify the degree of deviation for each observation. The final classification of anomalies is made by comparing each score a\u017c against a predetermined threshold 8: an sample is classified as anomalous if $a_i > \\delta$ and as normal otherwise.\nDefinition 6. (Imputation of Missing Values). Tabular imputation aims to fill missing values with plausible values to facilitate subsequent analysis. Given tabular data X and known binary matrix M\u2208 R, xi is missing if m\u2081 = 0, and is observed otherwise. The imputed tabular data is given as: $X_{imputed} = X \\odot M + X \\odot (1 \u2212 M)$.\nDefinition 7. (Retrieval). Tabular retrieval aims to obtain a set of samples that are most similar to a query provided. Given a query sample X and a similarity measure f(\u00b7), find an ordered list $Q = {X_i}$ of tabular samples in the given dataset or database, containing tabular samples that are the most similar to tabular queries.\nDefinition 8. (Test Time Adaptation on Tabular Data). Given a pre-trained source model f(\u00b7), TTA adapts source domain model parameters 0 to obtain target domain parameters \u03b8' using unlabeled target domain data $D_T = (x_i)_{i = 1}^{N_T}$. It should be noted that the feature spaces for the source and target domains are identical; however their distributions are not."}, {"title": "Foundational Properties In Tabular data Modeling", "content": "In this subsection, we elaborate on the distinctive characteristics of tabular data and corresponding distinct research perspectives. Due to these specific properties, methodologies developed for image or text data often unsuitable to be applied directly to tabular data.\nHeterogeneity Tabular data exhibits heterogeneity due to the incorporation of various data types, including discrete entities such as categorical and binary features, continuous variables represented by numerical data. Furthermore, columns with the same data type may still display distinct marginal distributions, as evidenced by differences in statistical properties such as mean, variance, and scale.\nDiverse and Contextualized Semantic Meaning Tabular data exhibits diverse semantic meanings among its columns. For instance, in a clinical diagnosis prediction task, the value 100 is ambiguous and lacks meaning until contextualized, such as by specifying 100 kg or 100 ml. In addition, the marginal distribution of features can vary across different contextual settings, even if they share the same semantic meaning. For example, in a weather prediction task, the statistical characteristics of air moisture may differ significantly between eastern and western regions. This property complicates the transfer of knowledge across domains and tasks, and it also poses challenges for tabular imputation, often requiring the expertise of domain specialists.\nPermutation Invariance and Equivalence Permutation Invariance refers to the property that the outcomes of an analysis or model remain unchanged when the rows or columns of a tabular dataset are permuted. This means that rearranging the order of the observations (rows) or the features (columns) does not affect the statistical properties, relationships, or predictions derived from the data. Permutation Equivalence indicates the tabular dataset remains essentially the same in terms of its statistical properties or relationships, regardless of the order of observations or features. Consequently, the results of analyses, such as predictions or statistical measures, remain consistent even when specific operations, such as normalization or scaling of features, are applied.\nHigh Noise and Missing Value Tabular data, especially in real-world environments, often contains noise and missing values. This noise typically arises from measurement errors or annotation mistakes. Missing values can also stem from measurement errors and may exhibit random missing patterns. In some cases, missing values can convey important information and indicate \"missing not at random\" patterns. For example, in electronic health record data, each patient may undergo only specific lab tests that are critical for accurate diagnosis verification. These partial observations actually provide essential information for a model to understand"}, {"title": "Applications", "content": "HealthCare Healthcare records employ tabular data structures for the storage and management of comprehensive patient health information [Ma et al., 2022a; Ma et al., 2022b; Ma et al., 2023]. Each column corresponds to specific data types, encompassing personal details, medical images [Liu et al., 2017; Li et al., 2019], medical history, and diagnostic and treatment information. The utilization of tabular data in healthcare tasks leads to various classifications:\n\u2022 Patient Outcome Prediction. Examples include patient mortality prediction, predicting the diagnosis of diseases, and forecasting patient responses to specific drugs.\n\u2022 Clinical Trial Outcome Prediction. This involves predicting the likelihood of a clinical trial succeeding in obtaining approval for commercialization.\n\u2022 Tabular Search and Retrieval Problems. Tasks under this category encompass clinical trial retrieval, where relevant trials are identified based on a given query or input trial; and insurance retrieval, involving the search for patient historical information.\n\u2022 Tabular Generation. An example is Trial Patient Simulation, where the generation of synthetic clinical trial patient records facilitates data sharing across institutes while safeguarding patient privacy.\n\u2022 Tabular Data Transference Tabular data transfer is fundamental to Health Information Exchange initiatives, where different healthcare entities share patient data securely. This promotes coordinated care, supports multi-disciplinary care, reduces duplicate tests, and enhances overall healthcare efficiency.\nE-commerce In e-commerce, tabular data is widely used for various purposes to organize, analyze, and present information related to products, transactions, and customer interactions. The industrial practice of tabular data can be summarized into the following categories:\n\u2022 Product Recommendations. E-commerce platforms use tabular data to generate personalized product recommendations. Columns include customer inquiries, support tickets, resolutions, customer browsing and purchase history.\n\u2022 Transaction Fraud Detection. Tabular data is utilized for monitoring transactions and detecting fraudulent activities. Columns may include transaction details, payment methods, and fraud indicators.\n\u2022 Product Search. E-commerce platforms employ tabular search and retrieval for product searches. Users can search for products based on various criteria, such as category, price range, and specifications, and the system retrieves matching products.\n\u2022 Tabular Transference. Transfer learning allows e-commerce businesses to leverage existing knowledge from one context to improve performance in related tasks, reducing the need for extensive training on new datasets. It can lead to more efficient model training, faster adaptation to new markets Real-world application relates to Supply Chain Optimization, Dynamic Pricing prediction and Customer Lifetime Value Prediction.\nEnergy Management Energy Management: Utilities and energy companies use tabular data to monitor energy consumption, analyze usage patterns, and optimize distribution networks for efficient energy management. Realworld application relates to peak demand prediction, Renewable Energy Source Prediction, Carbon Emission Data Retrieval."}, {"title": "Foundational Properties In Tabular data Modeling", "content": "Learning from tabular data poses a significant challenge due to the heterogeneous column contents. Different columns usually contain distinct semantics and display diverse distributions, leading to difficulties in learning the representation space. Furthermore, latent relations often exist behind observed columns, and modeling such interactions could be important for discovering critical factors. Extensive researches"}, {"title": "Applications", "content": "HealthCare Healthcare records employ tabular data structures for the storage and management of comprehensive patient health information. Each column corresponds to specific data types, encompassing personal details, medical history, and diagnostic and treatment information. The utilization of tabular data in healthcare tasks leads to various classifications: (I) Patient Outcome Prediction. Examples include patient mortality prediction, predicting the diagnosis of diseases [Liang et al., 2024], forecasting patient responses to specific drugs and to depression [Qin et al., 2023; Zhao et al., 2023]. (II) Clinical Trial Outcome Prediction. This involves predicting the likelihood of a clinical trial succeeding in obtaining approval for commercialization. (III) Tabular Search and Retrieval Problems. Tasks under this category encompass clinical trial retrieval, where relevant trials are identified based on a given query or input trial; and insurance retrieval, involving the search for patient historical information. (IV) Tabular Generation. An example is Trial Patient Simulation, where the generation of synthetic clinical trial patient records facilitates data sharing across institutes while safeguarding patient privacy. (V) Tabular Data Transference Tabular data transfer is fundamental to Health Information Exchange initiatives, where different healthcare entities share patient data securely. This promotes coordinated care, supports multidisciplinary care, reduces duplicate tests, and enhances overall healthcare efficiency.\nE-commerce In e-commerce, tabular data is widely used for various purposes to organize, analyze, and present information related to products, transactions, and customer interactions. The industrial practice of tabular data can be summarized into the following categories: (I) Product Recommendations. E-commerce platforms use tabular data to generate personalized product recommendations. Columns include customer inquiries, support tickets, resolutions, customer browsing and purchase history. (II) Transaction Fraud Detection. Tabular data is utilized for monitoring transactions and detecting fraudulent activities. Columns may include transaction details, payment methods, and fraud indicators. (III) Product Search. E-commerce platforms employ tabular search and retrieval for product searches. Users can search for products based on various criteria, such as category, price range, and specifications, and the system retrieves matching products. (IV) tabular Transference. Transfer learning allows e-commerce businesses to leverage existing knowledge from one context to improve performance in related tasks, reducing the need for extensive training on new datasets. It can lead to more efficient model training, faster adaptation to new markets Real-world application relates to Supply Chain Optimization, Dynamic Pricing prediction and Customer Lifetime Value Prediction."}, {"title": "Tabular Data Modeling", "content": "In this section, we present an elaborated taxonomy of three aspects in tabular data learning: Attributes Representation, Inter-Column Dependency Modeling, Side Learning Tasks. We deliver a thorough analysis, delving into their primary challenges and typical strategies."}, {"title": "Heterogeneous Attributes Encoding", "content": "Challenges Tabular learning aims to capture the intrinsic characteristics of each attribute (column). However, given that different tabular columns often originate from diverse sources and exhibit a high degree of heterogeneity, adopting a uniform encoding strategy becomes suboptimal. Current research predominantly focuses on three key dimensions within this domain:\nSub-Challenge 1: Diverse Data Formats and Distributions Heterogeneous tabular data frequently encompasses attribute formats including real-valued variables, categorical tags, textual descriptions, etc. Attributes within the same format may also display distinct distributions and ranges. This diversity necessitates the design of attribute-specific encoding strategies-for example, encoding real-valued variables using learned Gaussian models and mapping categorical tags to vector spaces. As a result, learning a homogeneous representation space for all attributes poses a challenge but is crucial for facilitating model training and designing effective loss functions.\nSub-Challenge 2: Attributes Representation Spaces. Data is inherently composed of elementary units representing the fundamental components that may not be further decomposed. Choices of elementary units involve trade-offs between representation ability, interpretability, and robustness. For instance, an image of a bird can be depicted as a collection of pixels or a composition of bird parts. Consequently, tabular representation learning also revolves around identifying the basic elements within tabular columns to devise effective representation strategies.\nSub-Challenge 3: Incorporating Semantic Domain Knowledge. The magnitude of a cell in isolation may lack meaningful interpretation without contextual information from the tabular header. For instance, the value \"100\" can convey distinct meanings depending on whether it is associated with 'PH' or 'Heart Rate'. Enhancing tabular representation, generalization, and robustness involves exploring, harnessing, and integrating rich contextual semantics from headers into tabular cells. This integration is crucial for ensuring the meaningful interpretation of tabular data in various contexts.\nSolution to Sub-Challenge 1. Addressing the challenge of diverse data, it is proposed to conduct homogeneous learning and transform the raw mixed-type attributes into a unified space with continuous properties, which can also ease the gradient-based learning pipeline. A popular strategy is\"feature tokenizer,\" that converts each numerical and categorical column into a d-dimensional vector, e.g., a linear transformation [Huang et al., 2020; Gorishniy et al., 2021]. Another strategy involves a two-stage training process where the first stage aims to generate a more homogeneous representation of the data across dimensions, subsequently utilized by the second stage. Typically, after the generative model, e.g., VAE has been effectively trained, the latent embeddings are extracted through the encoder and serve as the input for the subsequent stage.\nSolution to Sub-Challenge 2. In addition to previous studies that focused on converting raw tabular data, especially categorical information, into a continuous space, the second group of research concentrates on the fundamental elements of tabular representation learning. Taking inspiration from the widely used one-hot encoding algorithm for categorical features discretizes numerical features into intervals, replacing original values with discrete descriptors. These descriptors can be represented using piecewise linear encoding or periodic activation functions. Another approach involves representing each cell as a new form of \"column header is value.\" A concatenation of these cells is considered a fundamental representation of each row record.\nSolution to Sub-Challenge 3. The key insight in this direction is that there is often abundant, auxiliary domain information that can further describing input feature semantic representations and interactions [Nguyen et al., 2019; Santos et al., 2022]. Research works in this direction aims to enhance semantic representation through the usage of external knowledge like knowledge graphs [Liu et al., 2023]. The learning procedure can be summarized as the following steps: (1) Knowledge-graph construction they construst an auxiliary KG to describe input feature, in which each input feature corresponds to a node in the auxiliary KG. (2) Node Embedding. Each input feature j associated to a learnable weight vector $0_j \\in R^h$, e.g., MLP, such that the weight vectors of all d features compose the weight matrix $0 \\in R^{d\\times h}$. (3) Feature interaction estimation. A trainable message-passing function could be learned to further update node embedding, based on the assumption that two input features which correspond to similar nodes in the KG should have similar weight vectors in the node embedding space [Ruiz et al., 2023].\nNotably, tabular semantic representation is a prevalent topic in Natural Language Processing [Arik and Pfister, 2021], e.g., Tabular QA and Tabular semantic parsing. However, the majority of benchmark datasets in this domain enriches full-text descriptions, which falls outside the scope of this survey. Interested readers are directed to [] for a comprehensive exploration."}, {"title": "Inter-Column Dependency Modeling", "content": "In tables, different columns may exhibit overlapping semantics and correlations, with decision-making often contingent upon latent factors behind certain interactions. For example, in a clinical setting, correlations exist behind patient demographic attributes such as gender, age and vulnerability to certain diseases. The patient's response to a specific medical diagnosis may also be intricately linked to their familial history of inherited diseases. Formulating data dependencies behind tabular columns improves representation learning and facilitates downstream decision-making, representing a central challenge.\nIn tables, different columns may exhibit overlapping semantics and correlations, with decision-making often contingent upon latent factors behind certain interactions. For example, in a clinical setting, correlations exist behind patient demographic attributes such as gender, age and vulnerability to certain diseases. The patient's response to a specific medical diagnosis may also be intricately linked to their familial history of inherited diseases. Formulating data dependencies behind tabular columns improves representation learning and facilitates downstream decision-making, representing a central challenge. Challenges In tabular data, different attributes work complementary to each other and intricate relationships often exist behind their observed values. Capturing these dependency structures is crucial for modeling the observed data and enhancing representation capability. Moreover, deep neural networks (DNNs) encounter challenges when learning from dense numerical tabular features due to the complexity of optimization hyperplanes in fully connected models, which increases the risk of converging to local optima [Fern\u00e1ndez-Delgado et al., 2014]. Structure modeling can alleviate this challenge by uncovering critical factors and ease the learning burden. Commonly employed data structures include trees [Silva et al., 2020; Zhao et al., 2022a], graphs [Zhou et al., 2022; Zhao et al., 2024], and capsules [Chen et al., 2022] and logic rules [Ren et al., 2024]. For example, tree-based method shows its merit in iteratively picking the features with the largest statistical information gain [Chen and Guestrin, 2016]. In this survey, we explore recent advancements to capture relations across tabular columns in three streads:\nSub-Challenge 1: Hierarchical Structure Modeling. Hierarchical Structure defines an arrangement where abstract concepts are delineated as a function of less abstract ones. This hierarchical relation is prevalent in tables. For example, a table may have columns representing country, region, and city, creating a hierarchy where cities are nested within regions, and regions are nested within countries. Identifying, capturing, and modeling latent hierarchical structures constitutes a fundamental challenge in this context.\nSub-Challenge 2: Interactive Structures Modeling. In tables, different columns may exhibit overlapping semantics and correlations, with decision-making often contingent upon latent factors behind certain interactions. For example, in a clinical setting, correlations exist behind patient demographic attributes such as gender, age and vulnerability to certain diseases. The patient's response to a specific medical diagnosis may also be intricately linked to their familial history of inherited diseases. Formulating data dependencies behind tabular columns improves representation learning and facilitates downstream decision-making, representing a central challenge.\nSub-Challenge 3: Latent Structure Discovery. The challenges presented by Challenge 1 and Challenge 2 raise a fundamental question regarding the modeling of data dependency structures. However, such structure is usually latent and unknown, particularly in situations where domain-specific knowledge is absent. The discovery of such structures is essential for advancing structural modeling and serves as valuable evidence for root cause analysis, enhancing the explanatory capabilities of the model in a data-driven way.\nSolution to Sub-Challenge 1: Depending on the target of specific structure, such methods can be divided into three sub-directions. Sub-Solution 1a: tree structure modeling. DeepGBM [Ke et al., 2019] is a pioneering work that integrates the advantages of DNN and GBDT. DeepGBM comprises two distinct neural network components: CatNN, specialized in managing sparse categorical features, and GBDT2NN, tailored for distilling knowledge from GBDT to process dense numerical features. The NODE architecture [Popov et al., 2019] generalizes ensembles of oblivious decision trees, harnessing the advantages of both end-to-end gradient-based optimization and the efficacy of multi-layer hierarchical representation learning. GrowNet [Badirli et al., 2020] employs shallow neural networks as weak learners within a versatile gradient boosting framework. [Good et al., 2023] introduce an innovative framework that pioneers the alternation between sparse feature learning and differentiable decision tree construction. This approach aims to generate small, interpretable trees while maintaining high performance.\nSub-Solution 1 b: neural module-based structure learning. TABCAPs [Chen et al., 2022] utilize capsule networks to model feature-wise interactions. In the primary capsule layer, each sample undergoes encoding into multiple vectorial features using optimizable multivariate Gaussian kernels []. Subsequently, a successive iterative process of feature clustering is applied to attain higher-level semantics. TANGOS [Jeffares et al., 2022] utilizes a sparse and orthogonal regularization on the neural network, encouraging latent neurons to emphasize sparse, non-overlapping input features. This approach results in a collection of diverse and specialized latent units. Net-DNF [Katzir et al., 2020] presents an innovative framework characterized by an inherent inductive bias that yields models structured according to logical Boolean formulas in disjunctive normal form (DNF) over affine soft-threshold decision terms. Moreover, Net-DNFs actively promote localized decision-making processes executed over discrete subsets of the input features.\nSub-Solution 1c: graph structure modeling, with a considerable body of research dedicated to this direction. These approaches typically construct a graph based on predefined node and edge categories before using (heterogeneous variants of) graph neural networks to capture a representation of structure. Among these approaches, graph neural networks (GNN) are commonly employed to model (i) Feature-wise Interactions that Captures interactions and dependencies between individual features; (ii) Instance-wise Relations that models relationships between different instances or rows of the tabular data; and (iii) Feature-Instance correlations that address correlations between features and specific instances within the table. Feature-wise graph modeling aims to automatically estimate and represent relations among tabular features in the form of a learnable weighted graph [Zhou et al., 2022; Yan et al., 2023]. The learning procedure can be summarized into the following steps: (1) Graph Construction. They represent each feature as a node and estimate pair-wise feature interactions as an adjacency matrix. (2) Graph Structure Learning. To estimate expressive feature relations and learn a reliable graph structure, prior works primarily focus on learning the adjacency matrix, considering three key factors: 1) node semantic meaning; 2) im-"}, {"title": "Specialized Learning Tasks on Tabular Data", "content": "In addition to strategies that enhance data representations by attribute-specific encoding and relation modeling, a myriad of works focuses on specific tasks and learning goals on tabular data. These tasks are critical for the broader application of tables, and may provide additional learning signals to improve tabular representations. In this section, we introduce two representative side learning tasks, generative modeling of tabular data, and knowledge transfer across tables in similar domains."}, {"title": "Tabular Data Generation Tasks", "content": "Tabular data is a prevalent data format in various industrial sectors, including finance and electronic health records. However, the limited privacy considerations constrain the feasibility of releasing such data. Moreover, the collection of such data involves intricate procedures, such as unbalanced data nature, ensuring participant willingness and synchronized updates. Consequently, collected tabular datasets often contain missing values. The generation of authentic tabular data is crucial, particularly in the contexts of tabular data imputation and synthetic data creation. Existing works primarily devise model architectures based on popular generative models, including GAN [Goodfellow et al., 2020], VAE [Kingma, 2013], and the diffusion model [Wijmans and Baker, 1995]. These efforts aim to tackle three core challenges prevalent in tabular data generation: 1) Input Heterogeneity; 2) Sampling Quality; 3) Latent Structure Modeling.\nAs a pionering work in the realm of Generative Adversarial Networks (GANs), medGAN, as described by [Choi et al., 2017], integrates both an auto-encoder and a GAN to effectively generate diverse sets of continuous and/or binary medical data. Subsequently, TableGAN [Park et al., 2018] employs a Convolutional Neural Network (CNN) for feature processing. To address these complexities in modeling numerical features, CTGAN [Zhao et al., 2021] enhances its training procedure by incorporating mode-specific normalization and mitigates data imbalance through the implementation of a conditional generator.\nThe second group leverage Variational Autoencoder (VAE) methodologies, employing the well-known 'encode-sampling-decode' pipeline. VAEM [Ma et al., 2020] undergoes a two-stage training process. A Variational Autoencoder (VAE) is employed to establish a more homogeneous latent"}, {"title": "Tabular Data Imputation Tasks", "content": "Another line of works address the missing value problems and focus on data imputation. Predictive approaches to missing data imputation can be categorized in two families [Telyatnikov and Scardapane, 2023]: (i) imputing missing data through estimating statistics using the entire dataset [Lakshminarayan et al., 1996; Nazabal et al., 2020]; (ii) inferring the missing components employing similar data points to the one having missing values, e.g, KNN-based method [Acuna and Rodriguez, 2004]. To both model feature interaction in a global manner and leverage similar sample information, recent works [Spinelli et al., 2020] [Telyatnikov and Scardapane, 2023] explored the assumption of endowing tabular data with a graph topology and then exploiting the message mechanism to impute missing value. The difficulty and challenges here remain in the definition of a suitable distance metric to compute graph connectivity beforehand and design a customized procedure to sparsify the graph [Telyatnikov and Scardapane, 2023]."}, {"title": "Pretraining on Tabular Data", "content": "Challenges Tabular transference aims to transfer knowledge between tables. Tabular data exhibit variations in both the number and types of columns (termed as variable-column), posing a challenge for tabular deep learning models to transfer knowledge effectively from one table to another. Besides, in contrast to image and text modalities, tabular data are highly domain-specific and often lack extensive, high-quality datasets. These three challenges can result in poor generalization abilities across diverse tabular datasets. Based on the transfering difficulty and data distribution shift types, the core research questions can be classified into the following streads:\nSub-Challenge 1: Covariant Shift. Covariate shift is a common scenario in industrial practice, signifying a shift in the marginal distribution of features while the decision boundary of the model, denoted as p(y|x), remains unaltered. For instance, a patient may undergo multiple visits to a clinical institution, leading to a shifted feature representation. However, the underlying mechanism p(y|x) remains stable.\nSub-Challenge 2: Distribution Shift with Varied Columns. In industrial practice, feature design constitutes a critical step where scientists and engineers may introduce new features while removing redundant or unnecessary features. Given the substantial overlap in features, retraining the entire model is time-consuming and results in inefficiencies in labor. However, formulating an effective transfer learning paradigm that incorporates new feature information while discarding removed feature knowledge is a non-trivial task.\nSub-Challenge 3: Distribution Shift across Domains Vision and text models exhibit adaptability to a diverse array of tasks. This adaptability is attributed to the shared general representations present in both sentences and images, which shows task-agnostic properties [Farahani et al., 2021]. However, in the context of heterogeneous data [Ren et al., 2022a], a pertinent question arises: is there shared knowledge across tables, considering that two distinct tables can possess entirely different column numbers and associated semantic meanings?\nSolution to Sub-Challenge 1 is within-table pretraining that only covariate shift occurs. The primary objective is to devise a self-supervised loss and create a corrupted or augmented rendition of the initial tabular data. In the case of VIME [Yoon et al., 2020], a mask matrix M is applied to the initial tabular data X to generate a corrupted version X\u02c6 as input. The model then undertakes tabular data reconstruction and mask vector estimation, constituting two self-supervised loss components. Conversely, the subsequent work, SCARF [Bahri et al., 2021], generates a corrupted version by replacing each feature with a random draw from its empirical marginal distribution. Additionally, it introduces a contrastive loss. In contrast to these approaches, SubTab [Ucar et al., 2021] posits that reconstructing the data from a subset of its features, rather than from its corrupted version in an autoencoder setting, can better capture the underlying latent representation.\nSolution to Sub-Challenge 2 is across-table pretraining that formulates a versatile model capable of accommodating variable-column tables. One direction is to convert tabular data (cells in columns) into a sequence of semantically encoded tokens, e.g., TransTab [Wang and Sun, 2022], MED ITAB [Wang et al., 2023], TABRET[Onishi et al., 2023], UniTabE [Yang et al., 2023]. Another line of works aim to propose a pseudo-feature method for aligning the upstream and downstream feature sets in heterogeneous data, e.g., [Levin et al., 2022].\nSub-Solution 3a to Sub-Challenge 3: is Cross-domain pretraining that goes beyond the limitations imposed by variable-column structures and domain-specific constraints. XTab [Zhu et al., 2023], as a pioneering work, engages in"}, {"title": "Conclusion and Future Directions", "content": "Benchmarks. Tabular data stands as a prevalent data format in industrial settings; however, its accessibility is frequently constrained due to concerns surrounding data privacy, particularly in domains such as clinical and finance. The release and synthesis of high-quality tabular data would significantly enhance algorithmic design and contribute substantively to the advancement of representation learning.\n\u2663 Theoretical Analysis. Existing works achieve promising empirical performance on tabular data modeling problems, the theoretical analysis remains an open problem. We believe that rigorous analyses can provide in-depth insights and inspire the development of new tabular-based methods. Here we propose several research questions: (1) How to formulate and analyze the influence of missing-value problem in tabular representation? (2) Different tabular columns generally exhibit a diversity of distribution statistics, e.g., mean, variance. How to evaluate and quantify the significance of such variability in representation learning and loss design?\nEmpirical Analysis A pioneering work in [] demonstrates that shallow layer in DNN shares generalizable features, e.g., image edge, texture, which builds empirical foundations for transfer learning in image domain. However, empirical validation regarding the transferability and generalization of knowledge within the framework of tabular representations remains largely unexplored. Further exploration is required to comprehensively investigate and validate these aspects within the tabular domain."}, {"title": "Representation", "content": "Existing works mainly consider tabular heterogeneous nature and transform mixed type feature into a unified continuous space. One pioneering work, [], first split numerical features into bins and and find that bin-based embedding could large improve performance. The obtention of element units, especially from the perspective of available external knowledge, is still an under-explored problem in the tabular data domain.\nSemantic Representation. Given our primary focus on tabular data featuring numerical and categorical features, there are limited transferable representations within these values. However, tabular data is inherently domain-specific, implying that it should exhibit rich domain knowledge. Exploring the usage and interaction with large language models represents a promising direction. This approach would offer auxiliary information, enhancing both the semantic and generalizable representation of tabular data."}, {"title": "Dependency Modeling", "content": "Temporal Dependency Modeling Current studies predominantly address static scenarios wherein tabular features remain constant. Real-world datasets frequently manifest temporal properties, such as sequential Electronic Health Record (EHR) tabular data and sequential financial data. Exploring methodologies for learning dynamic dependencies within temporal tabular data represents a practical and promising direction.\nDependency Modeling with Auxiliary Knowledge Due to privacy concerns and labor costs, prior research endeavors have explored and modeled tabular data structures without incorporating external knowledge. With the advent of Large Language Models (LLMs) and RAG tools [Zhang et al., 2024"}]}