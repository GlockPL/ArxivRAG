{"title": "ESP-PCT: Enhanced VR Semantic Performance through Efficient Compression of Temporal and Spatial Redundancies in Point Cloud Transformers", "authors": ["Luoyu Mei", "Shuai Wang", "Yun Cheng", "Ruofeng Liu", "Zhimeng Yin", "Wenchao Jiang", "Shuai Wang", "Wei Gong"], "abstract": "Semantic recognition is pivotal in virtual reality (VR) applications, enabling immersive and interactive experiences. A promising approach is utilizing millimeter-wave (mmWave) signals to generate point clouds. However, the high computational and memory demands of current mmWave point cloud models hinder their efficiency and reliability. To address this limitation, our paper introduces ESP-PCT, a novel Enhanced Semantic Performance Point Cloud Transformer with a two-stage semantic recognition framework tailored for VR applications. ESP-PCT takes advantage of the accuracy of sensory point cloud data and optimizes the semantic recognition process, where the localization and focus stages are trained jointly in an end-to-end manner. We evaluate ESP-PCT on various VR semantic recognition conditions, demonstrating substantial enhancements in recognition efficiency. Notably, ESP-PCT achieves a remarkable accuracy of 93.2% while reducing the computational requirements (FLOPs) by 76.9% and memory usage by 78.2% compared to the existing Point Transformer model simultaneously. These underscore ESP-PCT's potential in VR semantic recognition by achieving high accuracy and reducing redundancy. The code and data of this project are available at https://github.com/lymei-SEU/ESP-PCT.", "sections": [{"title": "1 Introduction", "content": "Virtual Reality (VR) has experienced rapid growth over the past decade, enhancing user experiences in fields like entertainment, shopping, healthcare, and education [Martin et al., 2022; Wang et al., 2023c]. This evolution is largely driven by advanced sensing capabilities that extract semantic information from VR users, achieved through the recognition and tracking of headset and controller motion. Current VR systems utilize a range of sensors, including Inertial Measurement Units (IMUs) [Martin et al., 2022] and cameras [da Silveira et al., 2022]. Additionally, recent researchers find that integrating millimeter-wave (mmWave) technology [Zhang et al., 2023a; Li et al., 2023a; Wang et al., 2024] significantly enhances VR sensing capabilities. The mmWave devices, placed in front of the users, produce high-resolution point clouds that accurately depict environments, maintaining fidelity even in obstructions [Basak and Gowda, 2022; Qiu et al., 2023; Wang et al., 2023a; Cao et al., 2022]. This approach complements the sensors in VR headsets by providing a third-person perspective. Despite these advantages, employing mmWave radar for precise semantic recognition still presents complex challenges.\nCurrent state-of-the-art designs in this domain are divided into two categories: (i) Vision transformer (ViTs) based methods have outstanding accuracy in processing high-resolution imagery and video [Han et al., 2023; Hu et al., 2024], but suffer from high computational and storage cost, privacy concerns, and limited perception [Li et al., 2023b; Yu et al., 2023]. (ii) Point transformer-based methods present effectiveness in handling the sparsity and instability of mmWave point cloud data [Zhao et al., 2021; Wu et al., 2023], yet facing challenges in focusing on key motion features, reducing model cost, and enhancing robustness against environmental noise [Sun et al., 2023; Feng et al., 2024].\nThese limitations hinder the widespread utilization of these approaches for VR applications, where real-time processing and responsiveness are crucial for user experience and immersion, as they demand extensive computational resources and struggle to adapt to various environmental conditions. The existing models process entire mmWave point cloud data without prioritizing the semantically relevant information, which is crucial for VR tasks [Wang et al., 2023b]. Additionally, these models lead to unnecessary computational overhead, memory waste, and a potential decline in performance efficiency, especially in real-time applications where rapid processing and decision-making are essential. Therefore, an efficient learning framework that localizes and extracts semantic information from the most relevant point cloud data is urgently needed to enhance VR semantic recognition tasks.\nTo overcome these limitations, we introduce ESP-PCT, a framework designed to optimize the utilization of mmWave point cloud data in VR applications. ESP-PCT tackles two key challenges: (i) How to focus on the moving parts of targets, especially the semantic-discriminative regions, in sparse point clouds, and (ii) How to leverage the point cloud data of these critical parts for enhanced VR semantic recognition. The ESP-PCT model addresses these challenges with a two-stage framework that first localizes key areas (e.g., VR controller) through Localization Stage, and then applies attention mechanisms to these selected points in Focus Stage. We discover that not all points in the point cloud contribute equally to improving accuracy, while some distract the model. Inspired by this discovery, ESP-PCT concentrates only on the point clouds of the controller, which exhibit denser reflected point clouds. Hence, our two-stage framework significantly narrows the focus to key regions on the VR controllers that positively influence accuracy, drastically reducing computational costs in subsequent stages while eliminating noise from non-essential areas, thus enhancing model accuracy.\nSpecifically, based on a point transformer architecture [Zhao et al., 2021], ESP-PCT employs the localization stage that analyzes the raw point cloud data to make early identification. This stage processes data efficiently and utilizes smart strategies to reuse features, which saves computational resources. This consistency is critical for smooth training from start to finish, beneficial for saving resources, and keeping critical contextual details for the focus stage. Applying ESP-PCT for VR semantic recognition tasks leads to remarkable results. The ESP-PCT achieves a 93.2% accuracy while reducing the computational cost, cutting the FLOPs by 76.9% and memory utilization by 78.2%, setting new efficiency and performance in VR semantic recognition.\nESP-PCT is a flexible and robust framework designed for various sub-tasks in semantic recognition. Its adaptability stems from its two-stage structure, enabling it to be reused efficiently across scenarios. The localization and focus stages of ESP-PCT, as outlined in Fig. 1, are not just task-specific but are flexible enough to be applied to a range of semantic recognition sub-tasks in VR environments, as shown in the experiments. This reusability is a significant advantage, especially in diverse VR applications.\nTo summarize, our contribution is three-fold:\n\u2022 Introduction of ESP-PCT: this paper introduces ESP-PCT, a novel and efficient two-stage semantic recognition framework tailored for virtual reality (VR) applications. It leverages sparse point cloud data and is designed to optimize both accuracy and computational resources in VR semantic recognition tasks.\n\u2022 Flexibility and Reusability: ESP-PCT stands out for its versatility and reusability across a diverse range of VR semantic recognition sub-tasks. Its adaptability enables effective application in various scenarios, making it a highly valuable tool in the evolving domain of VR.\n\u2022 Significant Efficiency Improvements: a key achievement of ESP-PCT is its substantial enhancement in computational efficiency. The framework reduces the computational load, reduces FLOPs by 76.9%, and decreases memory usage by 78.2%, thereby setting new benchmarks for efficiency in VR semantic recognition."}, {"title": "2 Related Work", "content": "Existing methodologies in this domain are divided into two categories: Vision and Point Transformer."}, {"title": "2.1 Vision Transformer", "content": "Vision transformer [Arnab et al., 2021; Dong et al., 2022] is a series of pioneering works that apply the transformer model to image classification by splitting images into patches and treating them as tokens. Vision transformers have achieved competitive performance compared to various vision tasks, such as object detection [Chen et al., 2023; Herzig et al., 2022], semantic segmentation [Gu et al., 2022; Zhang et al., 2022], and video understanding [Wu et al., 2022a; Zhang et al., 2022; Yang et al., 2022]. However, ViTs are not designed to handle 3D point cloud data, which is irregular, unordered, and sparse [Selva et al., 2023; Yu et al., 2023] and lack of effectiveness under non-line-of-sight scenarios [Li et al., 2023b; Yu et al., 2023]. These limitations make vision transformer-based methods unsuitable for our VR semantic recognition scenario, which requires efficient and robust performance in various environments [Xu et al., 2023; Han et al., 2023]."}, {"title": "2.2 Point Transformer", "content": "Point transformers [Zhao et al., 2021; Wu et al., 2022b; Wang et al., 2022] is a family of neural networks that apply the transformer model to point clouds without voxelization or graph construction. Stratified point transformer [Lai et al., 2022] utilizes a stratified transformer layer to capture the hierarchical structure and feature fusion of point clouds. Point 4D transformer [Fan et al., 2021] extends the point transformer model to the 4D space-time domain by adding a temporal attention layer and a spatiotemporal fusion layer to model the dynamics and correlations of point cloud videos. Self-supervised 4D [Zhang et al., 2023c] develops a self-supervised learning framework for point cloud video representation learning by distilling and reconstructing the point cloud sequence. Interpretable3D [Feng et al., 2024] prioritizes interpretability in dense point clouds. However, they still face challenges in focusing on reducing computational cost and enhancing robustness against environmental noise. Unlike the existing approaches, ESP-PCT aims to improve the efficiency of the point transformer for VR semantics. ESP-PCT effectively identifies semantic recognition in sparse point clouds, focusing on reducing computational load and memory demands. The detailed design of ESP-PCT is demonstrated in the next Section."}, {"title": "3 ESP-PCT", "content": "This section illustrates the point transformer model as preliminaries, followed by the two-stage design of ESP-PCT."}, {"title": "3.1 Preliminaries", "content": "The point transformer model, referenced in [Zhao et al., 2021], processes multi-frame millimeter-wave point clouds to output attention scores for each point. The input to this neural network is a tensor sized $s \\times N \\times d$, where $s$ is the combined batch and length size, $N$ is the number of points per frame, and $d$ is the dimension of each point, encompassing five features: $x, y, z$, velocity, and intensity. The output is a tensor of size $s \\times N \\times d_{attention}$, with $d_{attention}$ representing the dimension of the output feature vector for each point. The model consists of several layers that apply vector attention to input points. Vector attention in each layer is computed as:\n$y_i = \\sum_{x_j \\in X} a_{ij} \\alpha(x_j),$                                                                                                        (1)\nwhere $y_i$ is the output feature vector for the $i$-th point, and $a_{ij}$ is the attention weight between points $i$ and $j$. The attention weight $a_{ij}$ is calculated utilizing feature transformations and a non-linear function:\n$\\alpha_{ij} = \\rho (\\gamma (\\beta (\\varphi(x_i), \\psi(x_j)) + \\delta)),$ (2)\nwhere $\\varphi$ and $\\psi$ are feature transformations, $\\beta$ is a relation function, $\\gamma$ a mapping function (usually an MLP), $\\delta$ a learned bias, and $\\rho$ a non-linear activation function."}, {"title": "3.2 Localization Stage", "content": "In the ESP-PCT localization stage, as depicted in Fig. 2, we analyze VR user body-generated point cloud data to pinpoint key semantic-discriminative regions. Utilizing a vector attention mechanism specified in Equation 1, we compute attention scores for each point, which are crucial for VR semantic recognition and are based on space and feature relations derived from Equation 2 and allow ESP-PCT to identify the most informative points for detailed scene analysis while ensuring computational efficiency. The model aggregates the attention scores of points that are in close spatial proximity, which is reflected in the following group score equation:\n$G_n = \\sum_{d_{ij} \\in G_k} d_{ij},$ (3)\nwhere $G_n$ represents the $n$-th group of points. Each group's cumulative score represents the Neighborhood Global Semantic Attention (NGSA).\nThe group with the highest NGSA score, highlighted in the point cloud, is deemed the principal semantic-discriminative region. This aggregation process emphasizes collectively significant point cloud regions, enhancing the model's ability"}, {"title": "3.3 Semantic-discriminative Regions Identification", "content": "As depicted in Fig. 3, we introduce a novel mechanism tailored to address the inherent irregularity and disorder in point cloud data. This mechanism's central innovation is the vector attention mechanism, which offers a distinct approach from the scalar attention weights used in vision transformers [Khan et al., 2022]. Vector attention operates at the individual feature channel level, providing a significant advantage in handling the unstructured nature of point cloud data. The semantic-discriminative region identification is beneficial given the complexity of relationships between points in point clouds, as opposed to images' more straightforward pixel grid structure.\nIn ESP-PCT, the vector attention mechanism is implemented as described in the preliminaries section, with a key modification: we utilize different feature transformations $\\varphi(i)$ and $\\psi(j)$ for the $i$-th and $j$-th points, respectively. These transformations extract the feature representations from each point, which are then processed through the relation function $\\beta$, the mapping function $\\gamma$, and the non-linear activation function $\\rho$, as outlined in Equation 2. Additionally, we incorporate a learned bias term $\\delta$ before applying the non-linear activation function $\\rho$. This bias adds an offset, further refining the effectiveness of the attention mechanism.\nAfter obtaining the vector attention scores for each point in the point cloud, we group the points based on their physical proximity and assign each group a global attention score that reflects its relevance to the target semantic. The global attention score for each group is computed as follows:\n$g_j = \\frac{1}{G} \\sum_{z_i \\in G_j} w \\cdot \\hat{y}_i,$ (4)\nwhere $G_j$ is the $j$-th group of points, $\\hat{y}_i$ is the vector attention score for point $z_i$, and $w$ is a learnable weight vector.\nTo identify the semantic-discriminative regions, we propose a novel NGSA mechanism, which selects the points with the highest global attention scores and, thus, is most informative for the semantic recognition task. Our novel NGSA mechanism identifies the semantic-discriminative region by selecting the group with the highest $g_j$:\n$R_s = \\underset{j}{argmax}\\, g_j,$ (5)\nwhere $R_s$ stands for the semantic-discriminative Region. This equation selects the group $g_j$ that maximizes the global attention score, which is the region with the highest relevance to the target semantic."}, {"title": "3.4 Focus Stage", "content": "When ESP-PCT's localization stage predicts a result $R_s < \\eta$, which is the decision boundary for determining whether to proceed with further localization and focused recognition, triggering the focus stage to refine the process further, this stage involves identifying class-discriminative regions utilizing the NGSA mechanism, as defined in Equation 5. The NGSA mechanism selects the top-K points with the highest global attention scores from the set $G$, which includes all possible points $\\{g_1, g_2, ..., g_M \\}$, where $M$ is the total number of points. Each group has a corresponding representation $h_k$. The representation $z$ of the point cloud, which is essential for semantic recognition, is constructed by concatenating the representations from the top-K points. This process of concatenation is outlined as follows:\n$Z = Concat(\\{h_i | g_i \\in Top-K(G)\\}),$ (6)\nwhere $h_i$ represents each group's representation and $M$ is the total number of points. The top-K points are selected based on their high scores, as formally expressed by:\n$Top-K(G) = \\{g_i \\in G | \\exists K' \\subseteq G\\},$ (7)\nwhere $|K'| = K$ and for all $g_j \\in K'$, $s(g_i) \\geq s(g_j)$, and for all $g_j \\in G \\backslash K'$, $s(g_i) > s(g_j)$. Equation 6 is rewritten as:\n$Z = Concat(\\{h_i | g_i \\in G and \\mathbb{I}_{top-K}(g_i) = 1\\}).$ (8)\nThis NGSA mechanism extracts the most distinctive regions for each semantic category, effectively filtering out irrelevant or noisy portions of the point cloud. This process extracts the semantic-discriminative region of the model's attention. Extracting this region enhances the model's interpretability and precision in decision-making, as these regions significantly influence the subsequent analysis and recognition stages. The vector attention mechanism thus provides a highly efficient method for aggregating features from the point cloud, capturing complex patterns and relationships. This flexibility positions ESP-PCT as a powerful tool for various semantic recognition tasks, including segmentation and object detection via mmWave point cloud analysis.\nOur novel NGSA mechanism can significantly reduce the computation overhead. The self-attention component within the NGSA mechanism is O($N^2 * D$), where $N$ represents the input length and $D$ represents the hidden dimension. NGSA employs a minimal number of layers without utilizing multi-head attention, thus significantly reducing costs. This design choice reduces the input length for subsequent multi-head models from N to K (e.g., from 100 to 30) with minimal overhead. Since subsequent functional models typically exhibit a O($N^2$) complexity, our approach substantially lowers computational costs."}, {"title": "3.5 VR Semantic Recognition", "content": "To demonstrate the flexibility and reusability of our approach, we detail the models used for VR semantic recognition in this section, which aim to interpret user intentions and behaviors within VR environments [Slocum et al., 2023]. Therefore, recognizing application types and keystrokes is pivotal in VR semantic recognition [Martin et al., 2022]. We introduce AppNet and KeyNet, the dedicated models for these two tasks. Specifically, the ESP-PCT preprocess the data before inputting it into these two application domain models and tests the effectiveness of our method."}, {"title": "4 Experiments", "content": "In this section, we present the data collection of the ESP-PCT prototype, which aims to obtain mmWave data for VR semantic recognition tasks. We designed the data collection components of the ESP-PCT to fit into a power bank, with a weight of 96 grams and dimensions of 8 cm in length and width. We utilize commercial VR devices as the target devices, as illustrated in Fig. 6. We recruited 12 participants, six males and six females, aged from 21 to 58, for our study. We obtain informed consent from each participant before the data collection. We collect 3,600 data sets comprising a 12TB dataset of mmWave point cloud and Kinect data, with 100 sets for each keystroke category. The mmWave radar data contained in each dataset include 30 seconds of raw signal, i.e., in-phase and quadrature components, and point cloud data, with a sampling rate of 10 frames. To the best of our knowledge, this is the first point cloud data set for VR semantic recognition, and we have made our dataset publicly available for future research and development."}, {"title": "4.2 Implementation", "content": "We evaluate ESP-PCT on the VR keystrokes recognition task, built on the 12TB dataset of VR semantics we collected. All our ESP-PCTs utilize a patch size 16\u00d716 to partition the point clouds. The input of our model is a sequence of point clouds, each representing a frame of the VR user. We set the sequence length to 25, which means we utilize 25 frames to recognize one keystroke. All the training strategies, such as data augmentation, regularization, and optimization, strictly follow the original settings of ESP-PCT. We train ESP-PCT for a total of 700 epochs. To improve performance and prevent overfitting, we utilize early stopping with 200 epochs. We initialize the best validation loss to infinity and update it whenever a lower loss is found."}, {"title": "4.3 Experimental Results", "content": "In this section, we demonstrate comprehensive experiments on ESP-PCT within a range of real-world noisy VR environments, spanning no-occlusion to combined-occlusion scenarios and across point clouds of various densities."}, {"title": "5 Conclusion", "content": "This paper presents ESP-PCT, a novel framework that improves the model accuracy while reducing redundancy by dynamically locating and focusing on the semantic-discriminative regions in the point cloud data generated from mmWave signals. The key insight is that not all points in a point cloud are equally important, empowering the framework to process data selectively and emphasizing the most informative regions to enhance semantic analysis. We validate the effectiveness and efficiency of ESP-PCT on various VR semantic recognition tasks utilizing point cloud data, and we release a 12TB dataset of mmWave point cloud and Kinect data under various VR scenarios for further research. Future work could explore applying ESP-PCT to other objects and scenarios for semantic recognition."}]}