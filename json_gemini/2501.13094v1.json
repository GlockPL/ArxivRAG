{"title": "Robust Representation Consistency Model Via Contrastive Denoising", "authors": ["Jiachen Lei", "Julius Berner", "Jiongxiao Wang", "Zhongzhu Chen", "Zhongjia Ba", "Kui Ren", "Jun Zhu", "Anima Anandkumar"], "abstract": "Robustness is essential for deep neural networks, especially in security-sensitive applications. To this end, randomized smoothing provides theoretical guarantees for certifying robustness against adversarial perturbations. Recently, diffusion models have been successfully employed for randomized smoothing to purify noise-perturbed samples before making predictions with a standard classifier. While these methods excel at small perturbation radii, they struggle with larger perturbations and incur a significant computational overhead during inference compared to classical methods. To address this, we reformulate the generative modeling task along the diffusion trajectories in pixel space as a discriminative task in the latent space. Specifically, we use instance discrimination to achieve consistent representations along the trajectories by aligning temporally adjacent points. After fine-tuning based on the learned representations, our model enables implicit denoising-then-classification via a single prediction, substantially reducing inference costs. We conduct extensive experiments on various datasets and achieve state-of-the-art performance with minimal computation budget during inference. For example, our method outperforms the certified accuracy of diffusion-based methods on ImageNet across all perturbation radii by 5.3% on average, with up to 11.6% at larger radii, while reducing inference costs by 85\u00d7 on average. Codes are available at: https://github.com/jiachenlei/rRCM.", "sections": [{"title": "1 INTRODUCTION", "content": "Deep neural networks (DNNs) have achieved unprecedented success in various visual applications. Yet, they are still vulnerable to small adversarial perturbations. This imposes a threat to the deployment of DNNs in real-world systems, in particular for security-critical scenarios, such as human face identification and autonomous driving. To counteract this issue, numerous efforts in terms of both empirical and certified defenses have been made to improve the robustness of DNNs against adversarial perturbations. While empirical defenses train DNNs to be robust to known adversarial examples (M\u0105dry et al., 2017), they can be easily compromised by employing stronger or unknown perturbations. In contrast, to end the mouse-and-cat game of iterative improvements of attacks and defenses, certified defenses focus on developing strategies that provide certifiable and formal robustness guarantees. However, this also makes the design of such certified defenses much more challenging. Among certified defenses, randomized smoothing with Gaussian noise (Cohen et al., 2019) is currently considered the \u201cgold standard\", providing a scalable way of certifying model robustness against adversarial perturbations with bounded 12-norm. To date, various randomized smoothing-based methods have been proposed (Jeong & Shin, 2020; Carlini et al., 2022). Among these works, diffusion model-based methods (Carlini et al., 2022) stand out with superior performance by integrating trained diffusion models into randomized smoothing. They first apply the denoising process of a diffusion model to remove Gaussian noise added to images. Then, using the purified samples, they predict the class label using a separate classifier. For brevity, we refer to these approaches as diffusion-based methods in the following discussions.\nDespite the success, there exists a gap between achieving low latency and strong performance for diffusion-based methods. To maintain a competitive performance, they either increase the number of sampling steps (Xiao et al., 2022) and/or implement majority voting during class prediction (Xiao et al., 2022; Zhang et al., 2023), suffering from even higher computational demands during inference (e.g., as high as 52 minutes). Furthermore, while leveraging the basic denoising property of diffusion models, previous approaches achieve consistent prediction across perturbed and clean samples through two independent models, resulting in a cumbersome prediction process and increased model maintenance overhead. We show that the framework of diffusion models itself already offers an effective solution in this regard: it establishes a unique connection between perturbed and clean samples along the trajectories of the probability flow (PF) of the denoising process. In this context, perturbed samples can be seen as points on the same trajectory of the denoising process but at different time steps, with the clean sample being the initial point. This motivates our approach of directly optimizing for consistent semantics across noise-perturbed and clean samples on the same trajectory of the diffusion process, leading to a unified model that supports consistent one-step prediction. In contrast, classical methods (Cohen et al., 2019; Jeong & Shin, 2020; Salman et al., 2019a) train models directly on noisy samples, primarily relying on heuristic strategies. These approaches fail to thoroughly exploit the intrinsic relationships between noisy and clean images, limiting their potential to achieve higher levels of certified robustness.\nOur approach: We close the gap of diffusion-based methods in terms of the tradeoff between performance and efficiency. In particular, we achieve performance that is better than classical randomized smoothing methods at a fraction of the cost of existing diffusion-based methods. This is made possible by directly optimizing model robustness based on structured connections between clean and perturbed samples. With the above analysis, we reformulate model robustness against noise perturbations as consistency between predictions of clean and perturbed samples. Specifically, our framework decomposes the training into two stages: pre-training and fine-tuning. During pre-training, the model learns to align representations across points along the deterministic trajectory from the Gaussian prior to the data distribution. To accomplish this, we reformulate the original generative image denoising task into a discriminative task in latent space and propose to align the representations of temporally adjacent points on the same trajectory via pair-wise instance discrimination. Based on the learned consistent representations, the model is then fine-tuned in a supervised manner to predict class labels given perturbed samples as input. This integrates denoising and classification into a single model and enables one-shot image classification, which is key to lowering the computation cost during inference. We term our model Robust Representation Consistency Model (rRCM).\nIn our experiments, we demonstrate that our method improves the certified accuracy of Carlini et al. (2022) at all perturbation radii by 5.3% on average, with up to 11.6% at larger radius, while at the same time reducing the computation cost by 85\u00d7 on average during inference. In comparison with classical methods (Cohen et al., 2019; Jeong & Shin, 2020; Salman et al., 2019a; Zhai et al., 2020; Jeong et al., 2021), our rRCM model either matches or surpasses their performance, achieving an average improvement of 8.48% across all perturbation radii, and maintains a similar inference cost. Besides, we demonstrate that our method exhibits strong scalability w.r.t. the training budget, including model parameters and training batch size, on ImageNet (Deng et al., 2009). In particular, we empirically observe that the performance of our rRCM model has not yet plateaued, indicating that a larger training budget could lead to even higher certified robustness. These results underscore the advantages of leveraging established noise schedules from diffusion models to enhance model robustness and streamline the certification process, making our method more effective than previous approaches. Figure 1 illustrates the trade-off between performance and efficiency across different methods, including rRCM. In conclusion, our contributions are as follows:\n1. Structured noise schedule for robustness. To the best of our knowledge, we are the first to exploit the advantages of the structured noise schedule of diffusion models in training robust classification models and provide a general direction for enhancing model robustness by drawing connections between noisy and clean samples.\n2. One-step denoising-then-classification. Our method reformulates the denoising objective, a generative modeling task, in a discriminative manner. It supports one-step denoising-then-classification, lowering computational demands and maintenance overhead. Besides, it offers remarkable representation consistency in the sense that our model is capable of generating meaningful representations by mapping random noise to the manifold of the clean data in latent space.\n3. Bridging efficiency-performance trade-offs. Our method bridges the gap in achieving low latency and superior performance for diffusion-based randomized smoothing methods. We perform extensive experiments across various datasets, demonstrating that rRCM achieves state-of-the-art performance compared to existing methods.\n4. Strong scalability. Our training framework also exhibits strong scalability w.r.t. enhancing model robustness on large-scale datasets like ImageNet."}, {"title": "2 PRELIMINARIES", "content": "Diffusion Models (Ho et al., 2020; Song et al., 2020) aim to approximate the underlying data distribution p(x0), given training data x ~ p(x0). They are composed of a forward and reverse process. Following the definition of Karras et al. (2022), the forward process of a diffusion model is given by the stochastic differential equation (SDE) dxt = \u221a2tdwt, where wt is the standard Wiener process and t\u2208 [0,T] (we use T = 80). Let pt(x) denotes the data distribution of the solution xt to the forward SDE at time t, where pr(x) \u2248 N(0, T2I). Correspondingly, the reverse process is given by the reverse-time SDE dxt = -2t\u2207x log pt(x)dt + \u221a2tdWt, starting at t = T. Here, Wt is a standard Wiener process that runs backward in time and x\u315c ~ p(x). For the given reverse-time SDE, there exists a corresponding deterministic reverse-time process that shares the same marginal probability densities {pt(x)}t\u2208[0,T] as the SDE:\ndxt = -t\u2207x log pt(x)dt"}, {"title": "3 METHOD", "content": "The ordinary differential equation (ODE) given above is referred to as Probability Flow ODE (PF ODE) (Song et al., 2020). Given the score function \u2207 log pt (x), one can generate \u201cclean\u201d samples from p(x0) by sampling from the prior N (0, T2I) and following the deterministic trajectories given by the above PF ODE.\nAs is common practice, we consider the discretized versions of the above equations. Using time steps {tn}=0, we divide the time horizon into N non-overlapping intervals. The endpoints, to and tn, are chosen such that to can be seen as approximate sample from p(x0) and tn = T. We denote points along the PF ODE sampling trajectory as {x}=0. Subsequently, the PF ODE in (1) can be discretized as\nx_{t_{n-1}} = x_{t_n} - t_n(t_{n-1} - t_n)\\nabla_{x} log p_{t_n}(x)|_{x=x_{t_n}}\nGiven a clean sample xo, one can also directly sample xt using\nx_{t_n} = x_0 + \\sqrt{t_n} \\epsilon with \\epsilon \\sim N(0, I).\nRandomized Smoothing (Cohen et al., 2019) is a technique to certify the robustness of arbitrary classifiers against adversarial perturbations under the l2-norm. It leverages a base classifier's robustness against random noise and builds a new classifier robust to adversarial perturbations, providing theoretical guarantees for the robustness of this new classifier. Given an input sample x and the base classifier f with classes y, randomized smoothing considers a smoothed version of f defined as\nF(x) = arg max_{c \\in Y} P_{\\epsilon \\sim N(0, I)} [f(x + \\sigma \\cdot \\epsilon) = c],\nwhere the noise level o is a hyper-parameter of the smoothed classifier. In our following discussion, we term F as the hard model and f as the soft model. Suppose f classifies samples from N(x, \u03c3\u00b2I), the predicted probability of the most probable class is Pca, and the runner up probability is \u0420\u0441\u0432. Then, the robustness radius lower bound r of the hard model F around \u00e6 is given by\nr=\\frac{\\sigma}{\\Phi^{-1}(P_{CA})-\\Phi^{-1}(P_{CB})},\nwhere \u03a6\u22121 is the inverse cumulative distribution function (CDF) of a standard Gaussian distribution. To achieve strong robustness under noise perturbations, one should maintain consistent predictions across clean and noisy samples. In practice, we adopt the common setting of classification models and parameterize the soft model as a function that outputs logits, which then pass through a softmax operation to obtain discrete probabilities of \u00e6 belonging to each of the classes. Moreover, to approximate the probability in (4), we use a large number (typically 10k or 100k in practice) samples of e, so-called smoothing noises."}, {"title": "3.1 OVERVIEW", "content": "A robust model shall give consistent predictions across clean and perturbed samples. To achieve this, we first draw connections between clean and perturbed samples leveraging the PF ODE used in diffusion models. For a given initial condition, the PF ODE ensures that trajectories remain distinct and do not cross each other. This indicates that any point uniquely belongs to a single sampling trajectory. In this context, points on the same trajectory can be interpreted as data of the same latent representation, defined by the initial clean data point 20. By formulating the training objective as grouping points on the same trajectory (in latent space), we can align points at higher noise levels with those from earlier time steps with lower noise levels, ultimately reaching the consistency goal.\nWe achieve this goal in a two-step process: pre-training and fine-tuning. During pre-training, we treat both clean and perturbed samples as points along the same deterministic PF ODE sampling trajectory of the diffusion model defined in Section 2. We align representations between temporally adjacent points that are sampled along the trajectory via pair-wise instance discrimination. Specifically, we attract temporally adjacent points on the same trajectory, while repelling those from different trajectories, leading to consistent representations among perturbed and clean samples. Afterwards, drawing upon the acquired consistent representations, we fine-tune the model via supervised training with class labels and additionally enforce consistent predictions on perturbed samples of the same noise magnitude. This transforms the alignment task from sample-to-sample alignment during pre-training to sample-to-class-label alignment and further partitions the trajectories based on their respective classes.\nOur approach reframes the image denoising task as a discriminative task in the latent space, effectively learning denoising by discrimination. This unifies the two independent modules into a single model and enables robust one-shot image classification. Next, we formalize the aforementioned idea and provide a detailed description of our training methodology."}, {"title": "3.2 PROBLEM FORMULATION", "content": "Given points along the PF ODE sampling trajectory of the diffusion model, our goal is to align the logits produced by the soft model f\u00f8, with 4 as model parameters. This can be formulated as\narg max_{\\Phi} (f_{\\theta}(x_{t_n}), f_{\\theta}(x_{t_{n-1}})) with f_{\\theta} = \\frac{f_{\\theta}}{\\|f_{\\theta}\\|}, \\forall n \\in \\{1, ..., N\\}.\nHer \\{xtn}=0 are obtained as in (2). The alignment objective in (6) maximizes the cosine similarity between paired samples (xt, Xtn\u22121), similar to the goal of contrastive learning methods (Chen et al., 2020; He et al., 2020; Chen & He, 2021; Chen et al., 2021), which attracts semantically similar views (positive pairs) of a sample in latent space while repelling dissimilar ones (negative pairs). In our case, the positive pairs are temporally adjacent points along the same PF ODE sampling trajectory, while points from different trajectories serve as negative pairs.\nConsidering the similar underlying rationale, we decompose the training into two stages (pre-training and fine-tuning), and we parameterize the soft model f\u00f8 as f\u00a2={w,0} = hw \u00b0 go, where ge is a neural network and hw is a linear layer. During pre-training, we train ge to align points along the same PF ODE sampling trajectory in latent space. Then, we fine-tune go together with the linear head hw using class labels, ultimately achieving consistent class prediction on perturbed images. We will discuss details of our pre-training and fine-tuning method next."}, {"title": "3.3 PRE-TRAINING", "content": "Given a sequence of i.i.d. samples\u00b3 X = {x}=1 drawn from p(x0), we aim to reformulate the alignment objective using loss functions similar to the infoNCE loss (Oord et al., 2018), i.e.,\n\\mathcal{L}(A, g_\\theta, \\mu) = E_{A}\\Bigg[\\sum_{i=1}^{B} -log \\frac{G_\\theta(a_i, a_i^{\\prime}; g_\\theta, \\mu)}{\\sum_{j=1}^{B} G_\\theta(a_i, a_j^{\\prime}; g_\\theta, \\mu)} \\Bigg],\nwhere\nG_\\theta(u, v; g_\\theta, \\mu) = exp \\Bigg( \\frac{\\widehat{g_\\theta}(u) \\cdot \\widehat{g_\\theta}(v)}{\\tau} \\Bigg), with \\widehat{g_\\theta}(u) = \\frac{g_\\theta(u)}{\\|g_\\theta(u)\\|}\nIn the above, 0\u00af is an exponential moving average (EMA) of @ with EMA update rate \u03bc, and \u03c4 is a hyper-parameter (that we set to 0.2 in our experiments). Moreover, A = {(a,a)}=1 is a sequence of samples, where a\u2081 and a are considered a positive sample pair defining two related yet different views of the i-th sample x while {a}j\u2260i are treated as negative samples to the i-th sample. Overall, our alignment objective is then given by\narg min_{\u03b8} \\mathcal{L}(X, g_\u03b8, \u03bc\u2081) + \\mathcal{L}(Z, p_1 \u25e6 g_\u03b8, \u03bc\u2082).\nThe differences between the two terms lie in three aspects: the construction of positive and negative pairs, model for computing the loss, and EMA update rate \u03bc. We call the first term in (9) the consistency loss and the second one the contrastive loss. Next, we will discuss how to construct the positive and negative samples for each loss.\nIn the contrastive loss, i.e., the second term in (9), Z denotes a sequence of augmented samples created following the convention in contrastive learning literature (Chen et al., 2020; 2021). Specifically, we construct each positive pair by applying different data augmentations to the clean data x"}, {"title": "3.4 FINE-TUNING", "content": "As described in Section 3.1, during fine-tuning, we map each perturbed sample to its ground-truth label while enforcing consistent predictions among among samples generated via the forward SDE, given the same clean image at the same time step t. In our work, we adopt the diffusion model proposed in EDM (Karras et al., 2022) and the time step t is interchangeable with the noise level o in (4), as can be seen in (3). For randomized smoothing, o typically takes values in {0.25, 0.5, 1.0}. In our experiments, starting with the same pre-trained weights 0, we fine-tune the model f$={w,0} independently for each noise level. Specifically, for a given noise level o, we fine-tune the model using the following training objective (Jeong & Shin, 2020)\narg min_{\\Phi} E_{x_0, \\epsilon, \\epsilon^{\\prime}} [-p(c) log(p(x)) \u2013 \\gamma_1 \\cdot p(x) log p(x^{\\prime}) \u2013 \\gamma_2 \\cdot p(x) log p(x_0)],\nHere, p(x) = softmax(f(x)) and x = x + \u03c3\u03b5 and x = x + \u03c3\u03b5' are two noisy versions of x ~ p(xo), where \u20ac, \u03b5' ~ N(0, I). The variable e denotes the class label of the sample x and 71, 72 are hyper-parameters. In the above, the first two terms represent the cross-entropy loss, which aligns the model's predictions with the ground-truth label and enforces consistency between predictions for the two perturbed versions of the same input. The third term computes the entropy of the model's predictions, acting as a regularization mechanism. This regularization encourages the model to make confident class predictions, contributing to achieving a larger robustness radius.\nIn early experiments, we observed that training a ViT model from scratch with this objective proved challenging. Upon further analysis, we speculate that the model struggles to simultaneously learn meaningful representations for class predictions while ensuring consistent predictions for noisy samples derived from the same clean image. However, after pre-training with our objective in (9), the model converges smoothly. We attribute this improvement to the similar representations among perturbed samples acquired during pre-training. We defer detailed explanation of the underlying rationale of our fine-tuning method to Appendix D."}, {"title": "4 EXPERIMENTS", "content": "In this section, we evaluate our rRCM model on two datasets: ImageNet (Deng et al., 2009) and CIFAR10 (Krizhevsky et al., 2009). First, we demonstrate the efficiency and effectiveness of rRCM in comparison with existing baseline methods. Second, we study the scalability of our method in the aspects of model size and training batch size. We defer training details and the ablation studies on hyper-parameters of our method to the Appendix."}, {"title": "4.1 EXPERIMENT SETTINGS", "content": "Model. We employ three different models in our experiments, namely, rRCM-S, rRCM-B, and rRCM-B-Deep, with an increasing number of parameters. All models follow the Vision Transformer (ViT) architecture (Dosovitskiy et al., 2020). Unless otherwise specified, we conduct experiments on ImageNet with rRCM-B and rRCM-B-Deep model, and conduct experiments on CIFAR10 with rRCM-B model. Further details on our model architectures can be found in the Appendix.\nCertification. We follow the settings of Carlini et al. (2022). Specifically, on both ImageNet and CIFAR10, we certify a subset that contains 500 images from their test set with confidence 99.9%. We certify each sample at three different noise levels \u03c3\u2208 {0.25, 0.5, 1.0}, and report the certified accuracy under different perturbation radii r. We report certified accuracies of rRCM models utilizing both 10,000 and 100,000 smoothing noises on ImageNet, and 100,000 smoothing noises on CIFAR10. We compare our models with a series of baseline methods. Both on ImageNet and CIFAR10, the certified accuracy of classical methods (Salman et al., 2020; Jeong & Shin, 2020; Salman et al., 2019a; Horv\u00e1th et al., 2021; Zhai et al., 2020; Jeong et al., 2021) is reported utilizing 100, 000 smoothing noises. We measure the inference latency of all methods on a single A800 GPU."}, {"title": "4.2 MAIN RESULTS", "content": "On both datasets, we report both the time cost (latency) of certifying one sample and the classification accuracy under various perturbation radii. The results of our rRCM models on ImageNet and CIFAR10 are shown in Table 1 and Table 2, respectively. As demonstrated, we achieve superior performance over current diffusion-based randomized smoothing methods (Carlini et al., 2022; Xiao et al., 2022; Zhang et al., 2023) especially at large perturbation radii, while significantly reducing the computational cost, which is on par with other classical methods (Salman et al., 2019a; Jeong & Shin, 2020; Salman et al., 2020; Horv\u00e1th et al., 2021; Zhai et al., 2020; Jeong et al., 2021).\nPerformance on ImageNet. As shown in Table 1, in comparison with both classical and diffusion-based methods, our rRCM-B model yields superior performance while maintaining an inference cost (53 seconds) slightly lower than that of classical methods (1 minutes and 20 seconds). Performance can be further improved by using a deeper model, rRCM-B-Deep, which ultimately reaches state-of-the-art results. This demonstrates the promising scalability of our approach, as detailed in Section 4.3.\nSubsequently, we also conduct fine-grained experiments to demonstrate the unwilling computation trade-off of DensePure (Xiao et al., 2022) and DiffSmooth (Zhang et al., 2023) in order to achieve competitive results to classical methods, in particular at large perturbation radii. In specific, we re-implement DDS (Carlini et al., 2022), DensePure, and DiffSmooth under the recommended settings in respective works. For DDS and DensePure, we use a ViT-based classifier that has the same amount of parameters as our rRCM-B model and achieves 81.35% accuracy on the ImageNet validation set. For DiffSmooth, we follow the settings in Zhang et al. (2023) and use the same base classifier as DDS and DensePure but instead fine-tuned respectively with samples augmented with Gaussian noise at various noise levels \u03c3\u2208 {0.25,0.5,1.0}. We report their certified accuracies utilizing 10, 000 smoothing noises under different l2 radii.\nAs anticipated, when the computation budget is limited and only a small number of majority voting is adopted during class prediction, both DensePure and DiffSmooth exhibit poorer performance than that of DDS. Noticeably, while adopting more denoising steps (b=5) during purification process, DensePure yields worse performance than DDS when no majority voting is applied during class prediction. As we increase the majority voting number, the performance of both methods gradually increase at different pace. Though finally surpassing DDS, their computation overhead increases tremendously, a phenomenon especially observed on results of DensePure, which requires 52 minutes and 20s for certifying a single sample.\nPerformance on CIFAR10. As shown in Table 2, we reach superior certified classification accuracy, pushing the certified accuracy of DDS (Carlini et al., 2022) up at most by 6.4% (r = 0.5). Besides, our rRCM-B model either surpasses or is highly competitive to other high-performing methods, including SmoothAdv (Salman et al., 2019a), Boosting (Horv\u00e1th et al., 2021), and MACER (Zhai et al., 2020). Our rRCM-B model is outperformed at r = 0.75 by Boosting (Horv\u00e1th et al., 2021), a method that ensembles 10 different classifiers. Yet, we still surpass diffusion-based methods at all perturbation radii."}, {"title": "4.3 SCALABILITY", "content": "We now explore the scalability of our method by pre-training models with varying model parameters and batch sizes on the ImageNet dataset. Following our experiment settings in Section 4.2, we additionally train a rRCM-S model and compare the certified accuracy of rRCM-S, rRCM-B, and rRCM-B-Deep. Additionally, utilizing rRCM-B, we investigate the impact of training batch size on model performance. The results, presented in Figures 3 and 4, highlight the excellent scalability of our method. With increased computational resources, we anticipate further performance improvements, which we leave for future work."}, {"title": "5 RELATED WORK", "content": "Certified Robustness. Deep neural networks (DNNs) are susceptible to adversarial examples (Goodfellow et al., 2014), prompting the development of various defense techniques, including empirical defense and certified robustness. While empirical defense methods (M\u0105dry et al., 2017; Samangouei, 2018; Zhang et al., 2019) can be easily compromised utilizing stronger adaptive attacks, certified robustness aims at providing a theoretical guarantee for the lower bound of model prediction accuracy under constrained perturbations. In certified robustness, a series of efforts (Raghunathan et al., 2018a;b; Salman et al., 2019b; Zhang et al., 2018)have been devoted to provide a robustness certification of DNNs. However, randomized smoothing (Lecuyer et al., 2019; Cohen et al., 2019) attract most attention due to its superior scalability. It supports non-trivial certification on large-scale dataset such as ImageNet and is applicable to any model architectures. On top of this, numerous works (Jeong & Shin, 2020; Salman et al., 2019a; Horv\u00e1th et al., 2021; Zhai et al., 2020; Jeong et al., 2021; Li et al., 2024; Jeong & Shin, 2024) have been proposed to further enhance model's robustness. To the best of our knowledge, we are the first to utilize a structured noise schedule to train randomized smoothing based model for enhanced adversarial robustness.\nTeacher-Student training paradigm is widely adopted in various domains, including representation learning and generative modeling. Contrastive Learning(Chen et al., 2020; Chen & He, 2021; He et al., 2020) aims at capturing meaningful visual representation by encouraging the model to output similar representations for samples of similar semantics. Meanwhile, as a member of score-based generative models (Ho et al., 2020; Song et al., 2020; Karras et al., 2022), consistency model (Song et al., 2023), a variant of diffusion models, employs a two-branch network to approximate the analytical solution of the PF ODE at initial point, resulting in consistent image predictions given any points on the same PF ODE sampling trajectory. Here, the clean image serves as a static boundary condition, preventing the model from learning trivial solutions. In comparison, rather than learning superior visual representations or achieving consistent image prediction, we aim at strong model robustness against adversarial perturbations. We learn consistent representations across points on the PF ODE trajectory by discriminating whether given point pairs are from the same PF ODE sampling trajectory. Besides, The initial point we utilize is low-dimensional representation dynamically learned during the training process. Noticeably, our rRCM model operates directly on image inputs, differing significantly from two-stage generative methods like LCM (Luo et al., 2023) which trains a consistency model in the latent space of a pre-trained VAE (Kingma, 2013). We present comparisons of contrastive learning, consistency model with rRCM in Figure 5."}, {"title": "6 CONCLUSION", "content": "In this work, we introduce the Robust Representation Consistency Model (rRCM), a novel approach to enhancing model robustness against adversarial perturbations through contrastive denoising in latent space. By reformulating the generative modeling process as a discriminative task, rRCM leverages a structured noise schedule to align representations of noisy and clean samples, allowing for one-step denoising and classification. This integration enables substantial reductions in inference costs, outperforming existing diffusion-based smoothing methods by a notable margin, particularly at higher perturbation radii. Our evaluations on ImageNet and CIFAR-10 confirm that rRCM achieves state-of-the-art performance with significantly improved efficiency, bridging the gap in the trade-off between robustness and latency. The proposed framework not only offers a promising approach to certified robustness but also establishes a foundation for future applications in representation learning and image generation. We leave further exploration of these applications for our future work."}, {"title": "A MODEL ARCHITECTURE", "content": "We display details of our models in Table 3."}, {"title": "B EXPERIMENTAL DETAILS", "content": "Pre-training During pre-training, we adopt the definition of the diffusion model proposed in EDM (Karras et al., 2022) and refer to the implementation of consistency models (Song et al., 2023), including noise schedule, input scaling, time embedding strategy, and time discretization strategy. As for data augmentation strategies, we adopt those utilized in MoCo-v3 (Chen et al., 2021). The temperature value in (9) is set to 0.2 for all experiments. By default, we pre-train rRCM-B and rRCM-B-Deep for 600k steps with a batch size of 4096 on the ImageNet dataset. We pre-train rRCM-B for 300k steps on the CIFAR10 dataset, with a batch size of 2048. Subsequently, we fine-tune our rRCM models separately at various noise levels \u03c3\u2208 {0.25, 0.5, 1.0}. In specific, for both ImageNet and CIFAR-10, we set \u03b7\u2081 in (12) to 10 at the noise level of 0.25, and to 20 for noise levels 0.5 and 1.0. In all experiments, 72 in (12) is fixed as 0.5.\nTo enhance training stability, we apply a dynamic EMA schedule for the target model utilized when computing the contrastive loss. Specifically, we gradually increase the EMA rate from 0.99 to 0.9999 following a pre-defined sigmoid schedule, as shown in Figure 6. This schedule is defined by the following equations:\nl=\\frac{k}{K}\na = \\frac{(E^2 - S^2) + S^2}{2}\nu = \\frac{1}{1+e^{-l}}\\nu EMA = a \\cdot E + (1 - a) \\cdot S"}, {"title": "C QUANTITATIVE ANALYSIS ON THE DEGREE OF REPRESENTATION ALIGNMENT", "content": "To further demonstrate the model's ability to align representations by generating meaningful outputs from pure noise inputs, we reuse the rRCM-B model from our CIFAR10 experiment to conduct image generation experiments. In detail, we train a diffusion model conditioned on the output of our rRCM-B model, which takes clean images as input. When generating images, we first generate representations using rRCM-B by feeding in pure noises sampled from the Gaussian prior, defined in Section 2. We then use these representations as conditions to the diffusion model to generate images. As a result, we achieve an FID (Heusel et al., 2017) score of 5.31 measured with 50k generated images. Uncurated image generation results are displayed in Figure 7. We train the diffusion model based on U-ViT-Small (Bao et al., 2023) for 500k steps at a batch size of 128. During sampling, we use DPM-Solver (Lu et al., 2022) to generate images with 50 reverse sampling steps. As conditioning input to the diffusion model, we use features from the MLP head of our rRCM-B model, normalized by their mean and standard deviation."}, {"title": "D QUANTITATIVE ANALYSIS OF THE SEMANTIC SIMILARITY BETWEEN POINTS ON DIFFERENT PF ODE TRAJECTORIES", "content": "During pre-training, each positive pair is generated from the same clean image perturbed by identical Gaussian noise but at different noise levels. Points in the sample space are treated as solutions of the PF ODE and aligned with their corresponding unique initial point. However, achieving strong certified robustness requires consistent class predictions among points on the stochastic forward trajectory. Specifically, the certification process involves predicting class labels for perturbed samples constructed via the forward SDE, where points are not necessarily confined on the same PF ODE trajectory. Consequently, theses points share similar, rather than identical, semantics to the initial point. As the noise level increases, the semantic similarity between the perturbed and clean images on the stochastic forward trajectory diminishes, which ultimately sets an upper bound on the robustness of our model. This phenomenon, representing a fundamental limitation of all diffusion-based methods, has also been"}]}