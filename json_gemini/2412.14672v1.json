{"title": "FiVL: A Framework for Improved Vision-Language Alignment", "authors": ["Estelle Aflalo", "Gabriela Ben Melech Stan", "Tiep Le", "Man Luo", "Shachar Rosenman", "Sayak Paul", "Shao-Yen Tseng", "Vasudev Lal"], "abstract": "Large Vision Language Models (LVLMs) have achieved significant progress in integrating visual and textual inputs for multimodal reasoning. However, a recurring challenge is ensuring these models utilize visual information as effectively as linguistic content when both modalities are necessary to formulate an accurate answer. We hypothesize that hallucinations arise due to the lack of effective visual grounding in current LVLMs. This issue extends to vision-language benchmarks, where it is difficult to make the image indispensable for accurate answer generation, particularly in vision question-answering tasks. In this work, we introduce FiVL, a novel method for constructing datasets designed to train LVLMs for enhanced visual grounding and to evaluate their effectiveness in achieving it. These datasets can be utilized for both training and assessing an LVLM's ability to use image content as substantive evidence rather than relying solely on linguistic priors, providing insights into the model's reliance on visual information. To demonstrate the utility of our dataset, we introduce an innovative training task that outperforms baselines alongside a validation method and application for explainability. The code is available at https://github.com/IntelLabs/fivl.", "sections": [{"title": "1. Introduction", "content": "Recent advancements in large language models have led to the integration of non-linguistic information through multimodal perception and generation, culminating in the development of Large Vision Language Models (LVLM). These models effectively bridge visual comprehension and linguistic reasoning, offering a unified approach to multimodal understanding and instruction-following [18, 25, 26, 30]. However, despite their apparent adeptness in visual perception, LVLMs still face the challenge of \"hallucination\" where the model generates semantically plausible yet factually incorrect information that is inconsistent with the image. This issue possibly arises from the imbalance of visual data compared to text data during training, which limits the model's ability to overcome preconceptions inherited from the underlying LLM [19]. A common approach to mitigate this issue has been to introduce a visual grounding mechanism to the model [4, 26, 27, 30, 32, 33]. Visual grounding aims to achieve more precise alignment between visual attention and semantic concepts within the model. A common method for grounding involves using bounding boxes, represented as a sequence of numerical numbers, to specify a particular region of an image. This enables the user to query specific parts of the image and the model to reference image locations within its generated response [4, 26, 30]. Bounding boxes, however, are coarse coordinates and unable to highlight objects or abstract concepts in finer detail. Additionally, the causal relationship between the generated bounding box and the response is not interpretable, and may instead be irrelevant to how the response is actually produced. Recent work have addressed these concerns by applying pixel-level grounding through the use of segmentation masks instead [27, 32, 33]. Training models with pixel-level grounding requires datasets that provide fine-grained visual alignment between images and text. However, such datasets are scarce, and prior work have often constructed custom datasets alongside model development [23, 27, 32, 33]. To address these challenges and simplify the training of visually grounded LVLMs, we introduce a novel method, FiVL (Framework for Improved Vision-Language Alignment) for constructing datasets with visual-concept alignment. Different from prior methods, our framework leverages existing datasets by employing SoTA segmentation models while prioritizing visual information that is crucial for accuracy. The main"}, {"title": "3. Our Framework", "content": "We created grounded datasets for both training and evaluation, building upon existing vision-question-answer and instruction datasets. Each sample in the original datasets was augmented with key expressions, along with their corresponding bounding box indices and segmentation masks within the images as follows:\nKey Expression Retrieval. The initial stage of data collection focused on identifying key expressions within each question-answer pair, using GPT-40. These expressions are specific words or phrases that would be unattainable without the visual context provided by the image, such as object names, attributes, or spatial relations. We provided only the text of the question-answer pairs, omitting the images, and prompted GPT-40 to detect essential expressions with a custom-designed prompt. Using only questions and answers without visual cues allows GPT-40 to rely solely on linguistic context to determine whether certain words could be evoked based on text alone. This approach can help filter language-based answers from those needing visual context, while being computationally efficient. This process yielded a robust set of expressions, capturing the elements in each conversation that are closely tied to the visual information.\nBounding Box and Segmentation Masks. To accurately associate key expressions with specific regions in each image, we used the GroundedSAM pipeline [28], which employs the GroundingDINO-tiny model [20] for initial expressions localization generating bounding box indices, followed by the Segment Anything vit-huge model [11] for precise segmentation mask creation. Each key expressions was mapped to its relevant visual region, creating high-quality segmentation maps. If multiple segments corresponded to a single phrase, they were consolidated into a unified mask assigned to each token within the phrase, to maintain consistency across annotations. During this process, we filtered out redundant key phrases, retaining only unique tokens to enhance the dataset's precision. Additionally, if a segmentation mask overlapped by more than 95% with another mask in the same sample, only one of the masks was retained. This filtering step ensured that each segmentation map uniquely represented essential visual regions, avoiding unnecessary redundancy and improving annotation clarity. No synthetic images or simulations were used; instead, we enriched existing images with detailed annotations that strengthen the visual grounding of language."}, {"title": "3.2. Training Dataset", "content": "Our training dataset, FiVL-Instruct, is built upon the LLaVA-1.5-mix-665K instruction tuning dataset [16], a"}, {"title": "3.3. Evaluation Datasets", "content": "To assess the visual reliance of various LVLMs, we created three benchmark datasets derived from the validation sets of POPE [14], VQAv2 [8], and GQA [10]. These augmented datasets, named FiVL-POPE, FiVL-VQAv2, and FiVL-GQA will be used to assess the extent to which models rely on visual information for accurate responses. We selected theses benchmarks because they each requires different levels of image reliance. POPE assesses sensitivity to visual perturbations, GQA evaluates understanding of detailed scene relationships, and VQAv2 tests visual grounding for diverse question types. Together, they offer a well-rounded assessment of how much models depend on visual information to answer accurately.\nFollowing the procedure outlined in Section 3.1, for each sample, we identified key expressions that can be extracted from the question but also the answer and produce the related segmentation mask. We included the answers as well, as many samples feature the key element in the question, and a significant portion of answers are simple responses like \"Yes,\" \"No,\" or numerical values. Section 5.2 describes our use of these augmented datasets to evaluate how effectively models focus on visual cues. Similar to FiVL-Instruct dataset, GPT-40 does not consistently identify key expressions from each question-answer pair of these three datasets. Moreover, even for samples with key expressions, not all have corresponding segmentation masks generated by the GroundedSAM pipeline. This indicates that some of the questions from these datasets might not depend on the images. In our evaluation set, we filter those cases without extracted keywords or segmentation."}, {"title": "4. Method Evaluation", "content": "To ensure the quality of our datasets and effectiveness in training LVLMs for visual grounding, we conducted a multi-step evaluation process on the training dataset described in Section 3.2. This included both human-based evaluations and automated assessments, allowing us to validate the relevance and accuracy of the key tokens and their alignment with visual content. Below, we outline the key components of our evaluation strategy."}, {"title": "4.1. Human Evaluation", "content": "We conducted a manual evaluation in order to validate the coherency of the key expressions as well as the relevancy of the segmentation maps with respect to the formers. For each sample, we presented to the annotators one random key expression with its associated segmentation map. Annotators were asked three questions: whether the key expression aligns with the definition provided in Section 3.1, if the segmentation map is relevant to the key expression, and whether the sample is of good quality (does the text makes sense, is the answer related to the question). In total, 557 unique samples were annotated by 12 different annotators. The accumulated results of the three types of question is that 77% of the annotators labeled the samples as overall good data points. In detail, for the key expression evaluation, 75% of key expressions were marked as essential to the answer's meaning. For the segmentation map evaluation, 58% of segmentation map were annotated as relevant to the key expression. This can be explained by the fact that some key expressions might inherently be more abstract or complex or by the performance of the GroundedSAM pipeline. We also find that the quality of the segmentation is related to the size of the segmentation. Finally, if we compute the key expressions and segmentations score only for the samples annotated as \"good data point overall\", 85% of the data are with valid key expressions and 69 % are with relevant segmentation masks. Specifically, Figure 2 indicates that when the segmented mask occupies less than 20% of the image, annotators were more likely to consider the segmentation relevant. To train our model (see Section 5.1), we selected the segmentation masks based on their size. We also made ablations by using FiVL-Instruct filtered to exclude big seg-"}, {"title": "4.2. Automatic Evaluation", "content": "Manual human evaluation is time-consuming. Inspired by recent applications using GPT as an evaluation tool [35], we designed two prompting techniques to automatically assess the quality of extracted keywords and segmentation masks based on a given keyword. Both evaluations were conducted on a randomly sampled set of 1,957 keywords and their corresponding segmentations from FiVL-Instruct."}, {"title": "4.2.1. Keyword Evaluation", "content": "We prompt the model to verify if the keyword is important to the question and depending on the image. Additionally, we ask the model to rate the degree of importance on a scale from 0 to 10. The full prompt is provided in Figure 4. We report three metrics. The first is the $Importance Ratio = 76%$ representing the percentage of extracted expressions classified as key expressions. This result is close to human evaluation, which is 75%. The second is the $Overall Importance Degree = 6.8$, which indicates the average importance score across all keywords, regardless of whether GPT-40 classifies them as important or not important. The third metric is the Importance Degree of Important Keywords = 9.0, which calculates the average importance ratio of keywords identified as important by GPT-40. These metrics indicate the high quality of our keywords."}, {"title": "4.2.2. Segmentation Evaluation", "content": "Given a keyword, we aim to evaluate whether our segmentation for this keyword is accurate. We designed two prompts to assess the quality of the segmentation: first, we check if the segmentation content adequately covers the keyword"}, {"title": "5. Applications of FiVL Dataset", "content": "In this section, we describe three ways to utilize our datasets. Section 5.1 describes how FiVL can be used as a training dataset and the resulting models not only achieve better performance but also has one more capability than the baseline model: generate segmentation maps. Section"}, {"title": "5.1. Training", "content": "We introduce here a novel pretraining task, Vision Modeling. To assess the effectiveness of this task, we fine-tuned an LVLM, specifically, LLaVA-1.5-7b [17], on FiVL-Instruct. For training our model, we used only key expressions that appeared verbatim in the answers for each turn, focusing exclusively on noun-based key expressions. By leveraging this dataset, we ensured alignment with widely used benchmarks in vision-language research, facilitating comparability and reproducibility across studies.\nMethod. In order to train Large Language Models and even some LVLMs (such as LLaVA), the cross-entropy loss is used to minimize the difference between the predicted probability distribution of the text output and the true distribution of the target words. LLaVA training strategy is in two stages: the first (pretraining) trains a projector which aims to align visual and textual representations, while the second (finetuning) performs only language modeling on the textual outputs of the LM head. In this work, we propose to also guide the visual outputs of the last linear layer (the LM head)."}, {"title": "5.3. Explainability", "content": "Here, we also show that FiVL can assist the interpretability of black box models. We produce a summary plot that displays a vision-alignment metric calculated across all heads and layers, like introduced in [2]. Figure 9a offers a detailed view of how the model grounds vision and language through the attentions of each head across all layers. For an attention matrix of size $(N_{layers}, N_{heads}, N_i + N_t, N_i + N_t)$, The head summary calculates the statistical mean over the last two dimensions, producing a plot with dimensions of $(N_{layers}, N_{heads})$ averaged for 500 samples. For a given question, image, key expression and related segmentation mask from the FiVL-Instruct dataset, we generate the answer using LVaVA-v1.5-7b. We then identify if the key expression is in the answer or in the question. If so, we probe each head by computing the Spearman correlation between the segmentation mask $(\\sqrt{N_i}, \\sqrt{N_i})$. and the attention to the corresponding key expression tokens in the Vision-to-Language attention component (1, 1, $N_i$, 1) (first dimension selects the layer, second the head and the last dimension corresponds to the key token) for each head. This is performed on the language model component but not on the vision component of LLaVA. In this way, we identify the attention heads that ground the most the two modalities by performing a function similar to object segmentation. Figure 9 shows the head summary and the corresponding language-vision attention weights related to the key expression tokens displayed as a heatmap over the image. The head summary shows that the heads achieving the strongest vision-language alignment are in the early layers. This might be due to the fact that the input to this transformer is the output of multimodal projector of LLaVA, which is designed specifically to align these two modalities. The head summary indicates that heads (10,6) and (14,11) are effective at"}, {"title": "6. Conclusion", "content": "In this paper, we introduced FiVL, a framework designed to enhance vision-language alignment and visual focus in large vision-language models. We applied our approach across key stages of an LVLM training workflow: training, evaluation, and explainability. By training a LLaVA model using our FiVL dataset, we saw improvement in a majority of test benchmarks. Our evaluation datasets facilitated comparisons between models regarding their reliance on images to answer questions and provided insight into the degree of image dependency needed across benchmarks. Finally, our explainability application enables users to identify attention heads that excel in vision-language alignment, allowing for a deeper understanding of the internal workings of LVLMs and potential model refinement. Our training method produced a built-in feature that segments the image. Future work could include the evaluation of these segmentations and the enhancement of grounded LVLMs using FiVL."}, {"title": "A. System prompts for key expressions retrieval", "content": "We use GPT-40 via the Azure OpenAI API to extract the key expressions of the datasets we considered. In this section, we share the prompts used for this step of the data collection. We had to use slightly different prompts for the training datasets compared to the evaluation datasets. In the training datasets, where instructions are open-ended question-answer pairs, the key expressions are often found in the answer. However, in the evaluation datasets, we encountered questions that required specific types of responses (yes/no questions, counting etc...). In these cases, the key expressions are typically found in the question instead. For references, we have provided the prompt used for training dataset in Figure 11 and prompts used for evaluation datasets VQA-V2, GQA, and POPE in Figure 12. For each benchmark we use different examples that suit the best to the types of questions. See Figure 13 for FiVL-VQAv2 and Figure 14 for FiVL-GQA and FiVL-POPE."}, {"title": "B. Training details", "content": "To train our model, we used Nvidia RTX A6000 GPUs using the hyperparameters from Table 4"}, {"title": "C. Performance of the segmentation maps inherently provided by our model", "content": "To evaluate the segmentation ability of our FiVL model, we evaluated Intersection-Over-Union (IoU) on a subset of 10,000 images from the GQA-val dataset. For each sample, we perform an inference using the baseline LLaVA-7b and our model. From the outputs, we retrieve the visual logits for each visual token, we assigned a text token from the vocabulary corresponding to the maximum logit probability, referred to as the max-v token. By aggregating all"}, {"title": "D. Additional Visual Reliance evaluations", "content": "To expand our benchmarks and gain a broader understanding of model/benchmark performance, we evaluated five additional models on FiVL-VQAv2, FiVL-POPE, and FiVL-GQA. This helps to assess the generalizability of our approach across more models."}]}