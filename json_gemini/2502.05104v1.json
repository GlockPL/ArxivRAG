{"title": "Leveraging Hypernetworks and Learnable Kernels for Consumer Energy Forecasting Across Diverse Consumer Types", "authors": ["Muhammad Umair Danish", "Katarina Grolinger"], "abstract": "Consumer energy forecasting is essential for managing energy consumption and planning, directly influencing operational efficiency, cost reduction, personalized energy management, and sustainability efforts. In recent years, deep learning techniques, especially LSTMs and transformers, have been greatly successful in the field of energy consumption forecasting. Nevertheless, these techniques have difficulties in capturing complex and sudden variations, and, moreover, they are commonly examined only on a specific type of consumer (e.g., only offices, only schools). Consequently, this paper proposes HyperEnergy, a consumer energy forecasting strategy that leverages hypernetworks for improved modeling of complex patterns applicable across a diversity of consumers. Hypernetwork is responsible for predicting the parameters of the primary prediction network, in our case LSTM. A learnable adaptable kernel, comprised of polynomial and radial basis function kernels, is incorporated to enhance performance. The proposed HyperEnergy was evaluated on diverse consumers including, student residences, detached homes, a home with electric vehicle charging, and a townhouse. Across all consumer types, HyperEnergy consistently outperformed 10 other techniques, including state-of-the-art models such as LSTM, AttentionLSTM, and transformer.", "sections": [{"title": "I. INTRODUCTION", "content": "According to the United States Energy Information Administration, by the year 2050, the global energy demand is projected to increase by 50% from its 2018 levels [1]. In the meantime, emphasis on sustainability and carbon footprint reduction has made energy management a focal point. Nations are setting climate action plans to counter the adverse effects of climate change and global warming: for example, the European Green Deal aims to achieve a 55% reduction in carbon footprint and 32.50% increase in energy efficiency by 2030 [2]. Energy forecasting can contribute to achieving these goals while providing savings: a mere 1% improvement in forecast accuracy can result in an annual savings of $1.6 million for energy-generating companies [3]. Consumer energy forecasting refers to predicting the future energy consumption patterns of individual consumers based on their historical usage data and other influencing factors such as weather. This type of forecasting differs from traditional load forecasting, which focuses on the overall demand across the grid, as it specifically targets consumer-level data to enhance personalized energy management and improve forecasting accuracy for diverse consumer types. Consumer energy forecasting has a wide range of applications including resource optimization, infrastructure planning, enhancing service quality, personalized energy solutions, and economic efficiency for both energy providers and consumers.\nDespite advancements in consumer energy forecasting, accurately predicting energy demand in urban environments remains a challenge due to factors such as consumer behavior, changes in equipment, and variations in meteorological conditions [4]. Machine Learning (ML) approaches, including support vector machines, artificial neural networks, and ensemble methods, have been extensively studied and consistently deliver good results in scenarios where strong data patterns are present [5]. In recent years Deep Learning (DL) models, specifically Recurrent Neural Networks (RNNs), Long Short Term Memory Network (LSTM), Gated Recurrent Unit (GRU), and Transformers, have shown remarkable results due to their ability to capture long-range temporal dependencies [6].\nHowever, the accuracy of these models can be compromised by difficulties in capturing complex energy consumption patterns and the variable nature of consumer behaviors, which are influenced by sudden events, new technology adoption, lifestyle changes, and meteorological shifts. These factors not only result in significant seasonality and cyclic variations in data but also introduce concept drift, novel dependencies, and elements of randomness. These changes can be challenging to predict [7].\nTo model energy patterns, in the training process, neural networks adjust their weights, most commonly through backpropagation or backpropagation through time. However, while these techniques optimize weights to minimize error, in the presence of complex and changing patterns, this optimization becomes challenging. Transfer learning also helps in learning and fine-tuning the weights but can be constrained by issues such as the domain gap and negative transfer [8], [9]. Moreover, consumer energy forecasting studies mainly focused on a specific type of consumers such as residences [10], apartments [11], detached houses [12], townhouses [10] and houses with electric vehicles [13]. Nevertheless, there is a lack of studies aimed at developing forecasting strategies capable of handling a variety of consumers, what is needed for practical applications.\nIn response to these challenges, this paper proposes HyperEnergy, an approach for consumer energy forecasting capable"}, {"title": "II. RELATED WORK", "content": "This section provides an overview of the significant advancements in consumer energy forecasting over the past five years followed by a discussion on hypernetworks."}, {"title": "A. Consumer Energy Forecasting", "content": "In the pursuit of improved consumer energy forecasting accuracy, numerous ML and DL approaches have been proposed. Support Vector Machines (SVMs) were incorporated into various solutions; however, their effectiveness greatly depends on the selection of kernels, which are responsible for handling non-linear data [14]. The polynomial kernel is good at tracking gradual shifts but may falter with sudden spikes due to its constant polynomial degree [15]. On the other hand, RBF kernel is a promising option for identifying sudden shifts in energy consumption; nevertheless, for success, it necessitates parameter tuning [15]. Gradient boosting approaches, such as Extreme Gradient Boosting (XGBoost) and Light Gradient Boosting Machine (LightGBM), have also been proposed for consumer energy forecasting [16]. However, these approaches encounter challenges in adapting to novel patterns, exhibit sensitivity to outliers, and may face scalability issues.\nIn recent years, DL models such as Multi-Layer Perceptrons (MLPs), Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Gated Recurrent Units (GRUs), LSTMs, and transformers, have extensively been studied for consumer energy forecasting. MLPs may fall short in recognizing temporal relationships within the energy data. To address this issue, advanced architecture such as the Time-series Dense Encoder (TiDE) [8] and Neural Basis Expansion Analysis (N-BEATS) [17] have been suggested. TiDE combines the simplicity of linear models with a temporal encoder which makes it a promising approach for long-term forecasting [8]. N-BEATS is designed with a series of MLP stacks and blocks to provide interpretable time-series forecasting which offers a fresh perspective for short-term forecasting [17]. Its complex architecture requires a significant amount of data for training. While CNNs excel in processing spatial patterns, they encounter challenges in handling temporal dynamics [18].\nRNNS, GRUs, and LSTMs are known for their ability to handle sequential and time-series data. Specifically, LSTMs are seen as a highly suitable choice for consumer energy forecasting [19] due to the gating structures that enable LSTM to capture long-term dependencies. This ability is important in short-term consumer energy forecasting, where LSTM can capture sudden changes [12]. Skala et al. proposed LSTM Bayesian neural networks for interval consumer energy forecasting for individual households in the presence of electric vehicle charging [20].\nTransformers and their advanced counterparts have contributed immensely to prediction capabilities in the energy sector [21]. They bring innovative self-attention mechanisms to handling complex data patterns. L'Heureux et al. proposed a transformer-based architecture and examined it on an open-source dataset comprising of 20 zones from a US utility company. The results showed that the transformer outperformed LSTM and sequence-to-sequence model [6]. Moreover, the emergence of hybrid models that combine machine learning, statistical, and deep learning models has been notable in consumer energy forecasting [22], [23], [24].\nWhile ML and DL models have been greatly successful in consumer energy forecasting, they encounter challenges in learning optimal weights when dealing with sudden spikes, drops, concept drift, or level shifts. This may lead to reduced forecasting accuracy. Hypernetworks have the potential to remedy this by assisting the primary network to learn weights.\nMoreover, the existing literature predominantly concentrates on particular consumer types, neglecting to offer a generic solution suitable for diverse consumer groups. For instance, Lin et al. [25] used residential data, Rezaei et al. [11] concentrated on apartments, Kong et al. [12] worked with individual houses, Gong et al. [10] focused on townhouses, and Zhang et al. [13] explored energy patterns in houses equipped with electric vehicles. This gap leads to a question: Can a single forecasting model successfully adapt to and capture the diverse energy consumption patterns observed across various consumer groups? Therefore, our study proposes a solution based"}, {"title": "B. Hypernetworks", "content": "Hypernetworks are meta neural networks that generate weights and biases for another neural network known as the primary network. In this arrangement, the hypernetwork's outputs, weights and biases for the primary network, are received by the primary network [26] and the primary network then utilizes these parameters to execute its tasks. The two networks are trained simultaneously and the hypernetwork customizes the primary network's parameters based on its inputs. Initially, hypernetworks were designed to compress neural network sizes [27] but have now found many applications including network pruning [28], multitask learning [29], functional representation [30], and generative tasks [31].\nIn HyperMorph [26], three distinct hypernetwork-based learning strategies for image registration were investigated: pre-integrative, where input is provided at the beginning of the primary model; post-integrative, involving input into the final layers; and fully-integrative, which monitors the entire model. The pre-integrative learning strategy yields better results than the remaining two techniques [26].\nHypernetworks have been explored and achieved notable success in various domains. For example, in recommendation systems, hypernetworks have been integrated to address the user cold-start problem [32]. In classification tasks, hypernetworks have been merged with graph networks and transformers to improve the classification of graph structures. Additionally, hypernetworks have proven highly effective in addressing differential privacy issues within the field of federated learning [33].\nDespite their successes in different fields, hypernetworks' potential remains largely unexplored in consumer energy forecasting. Primarily, hypernetworks have been used with feedforward neural networks and CNNs [26] which are not well suited for consumer energy forecasting as they are not specifically designed for capturing temporal dependencies. Consequently, our study proposes a hypernetwork with learnable adaptive kernels and LSTMs for consumer energy forecasting, aimed at handling time dependencies and accommodating the diverse consumer groups."}, {"title": "III. HYPERENERGY", "content": "This section presents HyperEnergy, our proposed method for consumer energy forecasting, designed to accommodate a wide range of energy consumers. The overview is depicted in Figure 1, while the three main components, kernelized hypernetwork, parameter integration module, and the primary network, are described in the following subsections, followed by a discussion of the parameter update process."}, {"title": "A. Kernelized Hypernetwork", "content": "The kernelized hypernetwork denoted as $H_k$ is designed to generate the weights and biases represented as $\\Theta$ for the primary network. The input to this $H_k$ network is $x \\in \\mathbb{R}^{m.k.n}$ where m is the number of samples, k is the number of features, and n is the number of time-steps in a sample as generated through the sliding window technique [6]. Here, features k encompass attributes such as temperature, day of the year, day of the month, day of the week, hour of the day, and energy consumption from the preceding n hours. This input is passed to the learnable adaptive kernel followed by the fully connected layers. The Kernelized HyperNetwork generates as output $\\Theta$, which represents the parameters of the primary network."}, {"title": "1) Learnable Adaptive Kernel:", "content": "The role of the learnable adaptive kernel is to transform input features into a high-dimensional space, thereby helping to capture both gradual and sudden changes in energy consumption values. The learnable adaptive kernel provides these transformed features as output. Kernels are well-known for their ability to transform data into high-dimensional spaces, which helps the model capture variations in patterns [34]. A learnable adaptive kernel is the first part of the kernelized hypernetwork; it is a learnable combination of polynomial and RBF kernels responsible for transforming data before the fully connected layers [14]. In contrast to traditional kernel-based forecasting methods that take input features and directly predict output values [14], [15], HyperEnergy incorporates kernels into a hypernetwork to predict the parameters of the primary network. Moreover, we designed learnable kernels and combined two types of kernels, which, together with the hypernetwork, provide HyperEnergy with flexibility and applicability across diverse consumers.\nA traditional polynomial kernel is defined as follows:\n$K_{poly}(x, x') = (x \\cdot x' + c)^d$ (1)\nHere, x and x' represent two data points, c is a constant term, and d denotes the degree of the polynomial. In contrast to traditional kernel approaches which compute the distance between all pairs of samples x and x', we employ learnable reference points $r_j$ in place of x' for enhancing models adaptability to diverse data patterns. The reference points $r_j$, where j = 1, 2, ..., $N_r$, with $N_r$ representing the total number of reference points, are initialized randomly. This initialization is formulated as:\n$r_j = p(N_r, k \\cdot \\eta)$ (2)\nHere, p generates values from a normal distribution, $N_r$ is the number of reference points, k denotes the number of features, and $\\eta$ is the number of time steps. Each reference point $r_j$ is thus a vector in $\\mathbb{R}^{k \\eta}$, initialized with random values. As the training progresses, these reference points are updated based on the gradient. The updating process at each training step can be represented as:\n$r_j^{(new)} = r_j^{(old)} - \\eta \\frac{\\partial L}{\\partial r_j}$ (3)\nwhere $r_j^{(old)}$ and $r_j^{(new)}$ represent the reference points before and after the update, respectively, $\\eta$ is the learning rate, and $\\frac{\\partial L}{\\partial r_j}$ is the gradient of the loss function L with respect to the reference point $r_j$. The loss function, as elaborated upon in Subsection III-D, can be either Mean Absolute Error (MAE) or"}, {"title": "B. Parameter Integration Module", "content": "The purpose of the parameter integration module is to transform the parameters $\\Theta$ to be compatible with the LSTM internal gating structure and the assignment of weights and biases. The parameters provided by the hypernetwork cannot be directly pushed to LSTM due to its gating mechanism. To address this, we apply a transformation on $\\Theta$. The output of the kernelized hypernetwork is a tensor with a shape equal to one $\\times$ the number of parameters in the primary network.\nWe exclusively extract and transform the parameters needed for the LSTM layer as the objective is to optimize solely the weights and biases of the LSTM layer, which is the primary forecasting component. The parameters of the fully connected layer are not required in this context, as this layer functions merely for output purposes and relies on the representations learned in the LSTM layer. The weights and biases for the LSTM are obtained through tensor slicing and transformed from $\\Theta$ as follows:\n$W_T = T(\\Theta, p_{wi}, p_{we}, [4u, k \\cdot v + u])$ (12)"}, {"title": "C. Primary Network", "content": "The role of the primary network is to receive the parameters $\\Theta$ from the parameter integration module and input features, and generate future energy consumption values as output. We selected LSTM as the primary network due to its success in modeling temporal data. Specifically, the primary network consists of a standard LSTM layer(s) and a fully connected output layer. The LSTM's internal parameters, including weights and biases, are directly updated with the tensors $W_T$ from Equation (12) and $B_T$ from Equation (13) which are the outputs of the transformation process from the parameters integration module.\nDirect tensor assignment, denoted as $\\Psi$, updates weights and biases by directly assigning the relevant portions from the hypernetwork's outputs. This operation is performed in a way that guarantees that the LSTM parameter update does not interfere with the ongoing training gradients and backpropagation process. In other words, this operation assigns LSTM parameters in a gradient-free manner ensuring that LSTM layers are excluded from gradient updates in the backpropagation process:\n$\\Psi (W_T, B_T) \\rightarrow LSTM$ (14)\nIn addition to setting the parameters using $W_T$ and $B_T$ from $H_k$, the primary network also takes the same inputs $x \\in \\mathbb{R}^{m \\cdot k \\cdot \\eta}$ as the hypernetwork and produces $h_t$, $c_t$:\n$h_t, c_t = LSTM(x, W_T, B_T)$ (15)\nwhere $h_t$ and $c_t$ are the hidden state and the cell state of the LSTM at time step t.\nThe hidden state $h_t$ is passed to the fully connected layer to generate the final energy consumption predictions for the next h time steps. The fully connected layer is represented as:\n$\\hat{y_h} = W_{13}h_t + b_{13}$ (16)\nwhere $W_{13}$ and $b_{13}$ are the weights and biases and $\\hat{y_h}$ is the vector of predicted energy consumption for the next h hours."}, {"title": "D. Backpropagation and Parameter Update Process", "content": "The MSE or MAE loss functions, based on the results of hyperparameter optimization, are used as the loss functions to quantify the difference between the predictions and the actual target values. They are calculated as:\n$L_{MSE}(\\Phi, \\Theta) = \\frac{1}{m} \\sum_{i=1}^{m}(Y_i - \\hat{Y_i})^2,$ (17)\n$L_{MAE}(\\Phi, \\Theta) = \\frac{1}{m} \\sum_{i=1}^{m}|Y_i - \\hat{Y_i}|,$ (18)\nwhere, $\\hat{y_h}$ is the predicted output from the primary network, $y_h$ is the actual energy consumption value, and $\\Theta$ are kernelized hypernetwork parameters.\nIt is important to note that MAE is more robust to outliers than MSE because MSE squares the errors, amplifying the impact of outliers. However, MSE provides a smooth gradient for optimization, allowing for controlled updates during training. Therefore, the selection between the two is carried out as part of the hyperparameter optimization process.\nAfter the loss is calculated, the backpropagation process differs from traditional backpropagation. Traditional backpropagation calculates gradients with respect to the weights and biases of the prediction network, and then those weights and biases are updated. In contrast, in our approach, gradients are calculated with respect to the hypernetwork parameters, and only the hypernetwork parameters are updated through backpropagation. The primary prediction network, LSTM, is not updated through backpropagation; instead, it receives its parameters from the hypernetwork during the forward pass.\nAlthough deep learning is employed, our model uniquely uses a hypernetwork with learnable kernels to predict parameters for the primary network, unlike traditional neural networks that learn weights and biases directly through backpropagation."}, {"title": "IV. EVALUATION", "content": "This section first describes datasets, preprocessing, and performance metrics. Next, hyperparameter optimization and the architectures included in the comparison are discussed."}, {"title": "A. Datasets, Preprocessing, and Evaluation Metrics", "content": "The evaluation utilized ten distinct real-world datasets from two primary consumer groups: student residences and individual homes. The overview of the datasets presented in Table I includes the time frames during which data were collected and a brief description of each dataset. Within each category, there are major differences between the buildings. Residence 1 offers suite-style accommodation with a shared kitchen, while Residence 2 adopts suite style but without a kitchen. Both residences accommodate over 400 students.\nWhile all considered homes are located in London, Ontario, Canada, there is significant diversity among them. Homes 1, 2, and 3 are all detached properties, but Home 3 stands out due to the presence of an electric vehicle, leading to notable energy consumption fluctuations caused by at-home charging. Home 4 is a 3-bedroom townhouse, and thus, energy consumption"}, {"title": "B. Hyperparameter Optimization", "content": "To ensure fair treatment of all compared models, including ours, hyperparameter optimization using the grid search was conducted for each dataset and each model. The hyperparameter search space is shown in Table II. Some of the hyperparameters are model-specific (e.g., attention head count), and this information is also included in the table.\nThe early stopping mechanism monitored the validation loss, terminating training after five consecutive epochs without improvement. The maximum number of epochs was set to 300, with possible termination earlier if the early stopping condition was met. The learning rate was optimized using the ReduceLROnPlateau schedule [39], which adjusts the rate during training if there is no performance improvement. The weights were initialized using uniform Xavier initialization to facilitate training [40].\nOur implementation of the HyperEnergy included two hidden layers in LSTM and after hyperparameter optimization, for each of the ten models corresponding to ten datasets, the selected activation function was swish. The remaining hyperparameters selected in the grid search differ among datasets, as depicted in Table III."}, {"title": "C. Forecasting Techniques Included in Comparison", "content": "The proposed HyperEnergy was compared to 10 different consumer energy forecasting techniques, as listed in Table IV. The table includes the base architecture together with pertinent literature references. To conduct a comprehensive analysis, the comparison includes diverse architectures encompassing a broad spectrum of predictive capabilities, starting from the basic Multi-Layer Perceptron (MLP) and advanced fully"}, {"title": "V. RESULTS AND ANALYSIS", "content": "This section presents the results and discusses the findings: first for residences and then for the individual homes."}, {"title": "A. Student Residences", "content": "The two student residences have somewhat similar energy consumption patterns likely due to the similarity in students' daily routines and study habits. As seen from Figures 2 and 3, there is a noticeable mix of trends and seasonality in energy usage across the training, validation, and test sets that correspond with the academic calendar and student occupancy. The changes occur at the start of the summer term when the majority of students are not on campus. However, as the considered period spans the COVID-19 restrictions, these patterns are less prominent and data have unexpected fluctuations. This imposes challenges in consumer energy forecasting as seen from the forecasting results.\nThe results of the comparison between our HyperEnergy and other state-of-the-art models are presented in Table V. HyperEnergy consistently outperforms traditional and advanced forecasting models in terms of all three metrics: MAE, RMSE, and SMAPE. For Residence 1, HyperEnergy achieves MAE of 20.02, RMSE of 27.58, and SMAPE of 8.27%, while for Residence 2, all error metrics are lower: MAE is 16.49, RMSE is 24.59, and SMAPE is 6.70%. Both, the standard LSTM and its hybrid variant, Attention LSTM, along with Transformer, ARFFNN, and XGBoost also demonstrate commendable performance by recording SMAPE values of less than 10% for both residences.\nFigures 4 and 5 show actual versus predicted energy consumption for the top four performing models for Residence 1 and Residence 2, respectively. It can be observed that for both residences, HyperEnergy captured the patterns better than the remaining models. In Figure 5, although there is seasonality, daily energy peaks are not precisely captured by any of the algorithms. A possible reason for this is that the time period selected for this illustration had slightly different peaks than the training data. Nevertheless, as observed from Table V, HyperEnergy achieves better results than other approaches."}, {"title": "B. Individual Houses", "content": "While the two residencies share some similarities and noticeable patterns due to similarities in students' routines, the four homes exhibit a wide diversity of energy consumption patterns as seen from Figures 6, 7, 8, and 9. As there is a large randomness component present in these datasets, it is expected the overall accuracy will be lower than that achieved for the residences.\nThe results for the four houses in terms of the metrics, MAE, RMSE, and SMAPE, are presented in Table VI. For example, HyperEnergy achieves SMAPE of 37.47% which is much higher than SMAPE for student residences. Nevertheless, for House 1, HyperEnergy achieves better results than the remaining ten algorithms in therms of all three metrics. Compared to the powerful transformer with SMAPE of 45.10%, HyperEnergy SMAPE of 37.47% is an improvement of around 8%. Other approaches achieve weaker performance than HyperEnergy and the transformer across most metrics.\nFor House 2, as seen from Figure 7, the energy consumption is lower in April and May, while after June there is a level shift and large spike variations, but in test data, there are no trends and seasonality but only sudden spikes and drops. As depicted in Table VI, HyperEnergy once again showed superior performance in terms of all three metrics, by recording a MAE of 0.43, RMSE of 0.55, and SMAPE of 21.03%. The transformer lagged behind with an MAE of 0.49, RMSE of 0.60, and SMAPE of 28.03%. In contrast, all other models recorded SMAPE values exceeding 30%. These numbers demonstrate the ability of the HyperEnergy to capture complex data patterns. Note that SMAPE for House 2 is lower than for House 1 for all models showing increased predictability for House 2. On the other hand, MAE and RMSE cannot be compared across houses as they are scale-dependent metrics.\nWith House 3, we extend the evaluation to a house with an Electric Vehicle (EV) which is expected to cause charging spikes. These sudden spikes, together with level shifts can be observed in Figure 8. HyperEnergy achieves the lowest errors across all metrics, with MAE of 0.35, RMSE of 0.53, and SMAPE of 29.95%. Several other models such as GRU, LSTM, and AttentionLSTM also show commendable performance, with SMAPE values hovering around 31%.\nHouse 4 again increases the diversity of consumers by considering a townhouse instead of a standalone house. Again, sudden spikes are highly present in data as seen from Figure 9. Yet again, the HyperForecasting showed superior performance in terms of all three metrics. LSTM, AttentionLSTM, Transformer, and XGboost follow closely with SMAPE values close to 40%. While 37.62% achieved by HyperForecasting for House 4 is much higher than that achieved for residences, House 2, and House 3, it is still better than the other considered models."}, {"title": "C. Industrial and Commercial Buildings", "content": "Table VII presents the results for four types of industrial and commercial buildings: a manufacturing building, a medical clinic, a retail store, and an office building. For these buildings, SMAPE values are much lower than those observed for student residences (Table V) and individual homes (Table VI): most algorithms archived SMAPE values under 12%. This can be explained by these buildings having more consistent energy use patterns due to regular working hours and activities.\nFor the manufacturing building, HyperEnergy achieved the best MAE and SMAPE values, while the transformer achieved slightly better RMSE. For the medical clinic, the transformer achieved the best results in terms of all three metrics. For the retail store, MAE and RMSE were better for the transformer than for HyperEnergy, and finally, for the office building, the Attention LSTM achieved the best results in terms of all three metrics.\nLooking across all four buildings, the three top models -- Attention LSTM, Transformer, and our HyperEnergy achieved similarly high levels of accuracy in their predictions, with HyperEnergy demonstrating a slight advantage in most metrics. Combined with findings for student residences and homes, this demonstrates that our HyperEnergy is a suitable consumer energy forecasting technique for a diverse range of consumers."}, {"title": "D. Ablation Studies", "content": "The learnable adaptive kernel in HyperEnergy plays a crucial role in modeling complex energy patterns and enhancing the prediction. This section examines the significance of this component as well as the impact of kernel type. The results are presented for Residences and House 2, while the remaining datasets are omitted for brevity."}, {"title": "1) Study 1: With and Without Learnable Adaptive Kernel:", "content": "As seen from Table VIII, for Residence 2, HyperEnergy with the learnable adaptive kernel achieved SMAPE of 6.70% which is a noticeable improvement compared to the version without the kernel which obtains SMAPE of 7.76%. It is also important to note that the LSTM alone achieved SMAPE of 9.29% which indicates that the inclusion of a hypernetwork in our approach contributes to error reduction while the addition of kernels further reduces the error. Figure 10 compares the predictions obtained for Residence 2 with and without kernel to the actual values: it is noticeable that with the kernel, predictions better follow the actual values.\nTo extend the examination, the same analysis but for House 2 is depicted in Table IX. Again, the learnable adaptive kernel improves predictions by reducing SMAPE from 24.47% to 21.03%. Also HyperEnergy achieves much better accuracy than the LSTM model. From Figure 11 it can be observed that predictions with the kernel better follow the actual values than those without the kernel.\nOverall, HyperEnergy without the kernel archives better performance than the standalone LSTM justifying the adaptation of hypernetworks. Moreover, the learnable adaptive kernel further enhances predictive performance."}, {"title": "2) Study 2: With Learnable and Traditional Kernels:", "content": "We extended the evaluation to demonstrate the need for learnable kernels and the necessity to merge polynomial and RBF kernels. Table X first compares learnable and traditional kernels for RBF and the polynomial kernel, respectively, and then examines the combination of traditional and learnable kernels. HyperEnergy with a learnable RBF outperformed the traditional RBF, achieving a SMAPE of 8.76% compared to the traditional RBF's 9.04%. Similarly, the learnable polynomial kernel achieved a 9.46% SMAPE compared to the traditional polynomial kernel's 10.58%. In both cases, the RBF and the polynomial kernel, the learnable version performed better than"}, {"title": "E. Discussion", "content": "In many energy forecasting studies, the LSTM memory mechanism is key to capturing and retaining information from historical usage, which is then used to predict future energy consumption patterns [18], [42], [12]. The proposed HyperEnergy transforms and stabilizes this process by predicting optimized weights and biases for the LSTM, helping it not only enhance performance but also maintain consistency across diverse datasets. The inclusion of learnable adaptive kernels, which combine polynomial and RBF kernels, enables the model to capture both gradual and sudden shifts in consumption behavior. The Parameter Integration Module seamlessly assigns weights and biases to the LSTM's internal gates during training, helping LSTM maintain consistency across datasets with varied characteristics and usage behavior. As seen in Figure 12, while LSTM alone performs well in cases such as Residence 1, House 3, and House 4, it ranks lower for Residence 2. However, our proposed learning method ensures that HyperEnergy still outperforms all other techniques for all houses and residences, including Residence 2. As demonstrated in our ablation studies (Section V-D), both the hypernetwork and learnable kernels contribute to notable error reduction. These findings demonstrate that HyperEnergy can be effectively applied in scenarios requiring accurate predictions across diverse consumer types.\nFor the practical applicability of our method, it is crucial to examine both training and inference (prediction) times. Table XI compares our approach to top five approaches in terms of training and testing (inference) time. Four buildings were selected, as the remaining buildings have similar numbers of samples and computation times: for example, the two residences have the same number of samples, while Houses 3 and 4 have the same number of samples as House 2. All models were trained and tested on a computer with an AMD Ryzen Threadripper PRO 5955WX processor and an NVIDIA GA102GL RTX A6000 GPU. Note that testing time in Table XI is shown for the complete test set which ranges from 43 to 335 days depending on the dataset (20% of available data). Even though LSTM, GRU, and AttentionLSTM exhibit shorter inference times, our approach achieves superior prediction accuracy, as evidenced by the results presented in Tables V, VI, and VII, with a maximum inference time of only 0.18 minutes for the complete test set.\nThe practicality of the proposed approach is further demonstrated through its ability to handle diverse building types."}, {"title": "VI. CONCLUSION", "content": "This paper proposed HyperEnergy, a consumer energy forecasting approach founded on the hypernetworks suitable for forecasting energy across diverse categories of consumers. Kernelized hypernetwork is integrated with the LSTM model to improve captioning complex consumption patterns by updating LSTM parameters through a meta-network. The forecasting is further improved through the learnable kernel which integrates adapted polynomial and RBF kernels while changing the impact of kernels though the learning process.\nThe evaluation was conducted on diverse consumers: two student residencies and four individual households including two detached homes, one home with EV charging, and one townhouse. Across nine datasets, the proposed HyperEnergy outperformed 10 other forecasting approaches including state-of-the-art deep learning models such as LSTM, AttentionLSTM, and transformer. The ablation studies demonstrated that including the hypernetwork for setting parameters of the primary network improved the forecasting accuracy. The adaptable kernel improved accuracy although to a lesser extent than the hypernetwork itself. Figure 12 compares HyperEnergy with the top three other approaches for each of the ten datasets. Note that to top three performers besides HyperEnergy differ among the consumers. While the accuracy varies across consumers, HyperEnergy achieved the best results for each consumer.\nFuture work will investigate reusing already trained models between similar consumers through transfer learning. Moreover, the approach will be evaluated on additional datasets."}]}