{"title": "DEEGITS: Deep Learning based Framework for Measuring Heterogenous Traffic State in Challenging Traffic Scenarios", "authors": ["Muttahirul Islam", "Nazmul Haque", "Md. Hadiuzzaman"], "abstract": "This paper presents DEEGITS (Deep Learning Based Heterogeneous Traffic State Measurement), a comprehensive framework that leverages state-of-the-art convolutional neural network (CNN) techniques to detect vehicles and pedestrian accurately and rapidly, as well as to measure traffic state in challenging scenarios (i.e., congestion, occlusion). In this study, we enrich the training dataset using a data fusion technique that enables simultaneous detection of vehicles and pedestrians. Afterward, image preprocessing and augmentation are performed to enhance the quality and quantity of the dataset. Transfer learning is employed on YOLOv8 pre-trained model to enhance the model's ability to identify diverse array of vehicles. Suitable hyperparameters are obtained using the Grid Search algorithm and the Stochastic Gradient Descent (SGD) optimizer outperforms other optimizers with these hyperparameters. Extensive experimentation and evaluation demonstrate significant accuracy rates in the detection framework, with the model achieving 0.794 mAP@0.5 in validation set and 0.786 mAP@0.5 in test set, surpassing previous benchmarks on similar datasets. DeepSORT multi-object tracking algorithm is adopted to track the detected vehicles and pedestrian in this study. Ultimately the framework is tested to measure heterogeneous traffic states in mixed traffic condition. Two locations with different traffic compositions and different congestion levels are selected: one is motorized dominant with moderate density, and the other is non-motorized dominant with higher density. The errors are statistically insignificant for both cases, with a correlation of 0.99-0.88 and 0.91-0.97 for measuring heterogeneous traffic flow and speed, respectively.", "sections": [{"title": "INTRODUCTION", "content": "Accurate vehicles and pedestrian detection with tracking is crucial for efficient traffic flow measurement, real-time incident detection, and improved Intelligent Transportation Systems (ITS). Identifying vehicles and pedestrian enables the estimation of fundamental diagram parameters, such as jam density, capacity, and free flow speed, necessary for traffic flow simulation models and effective control and management strategies (1). It also has the utmost importance in road safety as it facilitates the automation of traffic conflict assessments, leading to proactive measures and improved overall road safety (2).\nVarious detectors, including loop detectors, LiDAR sensors, microwave detectors, GPS devices, and floating cars, are used to collect traffic flow information. However, their limitations in providing precise and comprehensive data, such as undetectable areas, connectivity issues, and high maintenance costs, highlight the importance of surveillance cameras for more accurate and cost-effective traffic monitoring and congestion alleviation (3, 4).\nTraditional methods for obtaining traffic information from videos or images involve vehicle detection and counting using various image processing algorithms. These methods, such as Speeded Up Robust Features (SURF), background subtraction, and temporal difference, suffer from poor performance and limited accessibility in congested and complex traffic conditions or low frame rates (5). Before Convolutional Neural Networks (CNNs) emerged, feature-based object detection methods were widely used. One popular approach is optical flow, which calculates displacement vectors by searching for consecutive pixel matches and is often employed for road user classification (6).\nOther conventional feature-based methods include Histogram of Oriented Gradients (HOG), Haar-like features and Scale-Invariant Feature Transform (SIFT). Although they compute abstractions of image information and classify road users based on specific features, they have difficulty in recognizing objects with repetitive patterns or textures, few distinctive features, and complex shapes or structures (7, 8). Their reliance on handcrafted features limits adaptability and hinders accurate vehicle detection and classification in real-world traffic scenarios. They also struggle with congestion, scale variations, occlusions, and lighting conditions (9).\nIn contrast, deep learning-based models, such as convolutional neural networks (CNNs), learn representations directly from the data, allowing them to capture complex patterns and adapt to different conditions more effectively (10). This ability to automatically learn features and hierarchies of representations contributes to the superior performance of deep learning-based models in road users' detection tasks.\nThe two-stage detection approach in CNNs involves using one model to extract object regions and another model to classify and refine the localization of the objects. R-CNN model pioneered the structure, followed by Fast R-CNN, Faster R-CNN, Mask R-CNN, MS-CNN and R-FCN, which improved accuracy and efficiency (11). These models excel in localizing and recognizing objects but at the expense of reduced inference speed. Challenges arise with scale variations and small objects, where region proposal struggles to accurately localize differently sized objects.\nOne-stage Convolutional Neural Networks (CNNs) overcome the limitations of two-stage models for object detection. They directly predict bounding boxes and class probabilities, eliminating the need for region proposals. This simplifies the model and improves speed, making it suitable for real-time applications. Single-stage models have evolved, with notable advancements such as SSD (12), YOLO (13), EfficientDet (14) and CenterNet (15).\nFan et al. (16) analyzed and optimized Faster R-CNN for vehicle detection, demonstrating the impact of parameter tuning and algorithmic modifications and achieved 71.22% accuracy on the subset KITTI dataset that only includes cars. Sochor et al. (17) introduced a fine-grained vehicle recognition system for traffic surveillance that automatically constructs 3D bounding boxes around vehicles without relying on 3D vehicle models. However, their proposed method struggles when multiple vehicles are in one image. Liang et al. (18) introduced a cascaded convolutional neural network (CNN) model for car detection and classification, where they focused solely on detecting cars and did not consider the detection and classification of other types of vehicles."}, {"title": "METHODOLOGY", "content": "YOLO's real-time object detection capabilities have proven invaluable across various domains. YOLO models have been utilized in agriculture for crop detection and classification (19), and in the medical field for pill identification (20). Li et al. (21) employed YOLOv4 to detect passenger flow in subway stations, achieving better accuracy compared to other classical models. Bin Zuraimi and Kamaru Zaman (22) used YOLOv4 to detect four classes (Car, motorcycle, bus, truck) and recommended it is faster than the YOLOv3 model.\nThe tracking algorithm must be integrated with the detection model to achieve object trajectory. SORT (23), DeepSORT (24), and ByteTrack (25) are commonly used algorithms for object tracking. SORT utilizes object detection, Kalman filtering, and the Hungarian algorithm for efficient data association, accurately tracking objects. DeepSORT enhances SORT by incorporating appearance features and deep learning, improving accuracy and handling occlusion and ID switching in crowded scenes. ByteTrack is a state-of-the-art method that improves object trajectory accuracy by matching all detection boxes, regardless of scores. However, DeepSORT's accuracy in crowded environments depends on the detection model's accuracy. Integration of DeepSORT with the latest YOLO version has yet to be validated.\nOpen Dataset annotated with bounding box information is compulsory for training the YOLO model. The YOLO model is pre-trained on the MS COCO dataset (26), a widely used object detection and segmentation benchmark. It includes vehicle classes like cars, trucks, buses, motorcycles, and bicycles. However, it has insufficient variation, lack of fine-grained annotations, limited temporal information, and geographical bias. The CityPersons dataset (27) is widely used for pedestrian detection in urban areas. It offers annotated images focusing on pedestrians, but no vehicle classes are annotated. So, there needs to be a generalized dataset where pedestrians and all kinds of vehicles are labeled.\nExisting deep learning methods are limited to detecting pedestrians or a few vehicles, neglecting local peculiarities. Researchers rarely attempt to detect both simultaneously, and no studies focus on detecting vehicles dominated in heterogenous conditions. Thus, there needs to be a generalized detection model to detect vehicles and pedestrians, accurately.\nIn this study, we present a comprehensive framework that leverages state-of-the-art convolutional neural network (CNN) techniques for classified detection of pedestrian and vehicles accurately and rapidly and measuring traffic state in challenging mixed traffic scenarios. The complementary dataset fusion technique is employed to prepare the desired dataset. Images are preprocessed, including fixing orientation and applying histogram equalization. The training dataset is augmented using various methods such as grid dropout, Gaussian noise, and mosaic to enhance detection effectiveness, even when vehicles are mostly occluded. Transfer learning is employed on YOLOv8 pre-trained model to enhance the model's ability to identify road users (vehicles and pedestrian) regardless of the geographical setting. Stochastic Gradient Descent (SGD) optimization are used to get the best results and to optimize other hyperparameters using Grid Search Algorithm. DeepSORT tracking algorithm has been integrated to obtain road users trajectories. Finally, this framework has been used to measure the traffic states (flow and speed) from the field and validate the result against the ground truth value.\nThe remainder of this paper has been partitioned as follows: Section 2 presents the methodology of detection model, tracking algorithm and measurement of traffic state using DEEGITS; Section 3 shows data preparation, analysis and performance of DEEGITS in field; finally, concluding remarks and future research scopes are given in Section 4.\nThe methodology of this research is divided into three components: (1) Detection and Classification Model; (2) Tracking Algorithm; (3) Traffic State Measurement; Figure 1 shows the flow chart illustrating the connectivity among these components."}, {"title": "Detection and Classification Model", "content": "YOLOv5 (28), a popular version of YOLO, has been widely used for vehicle and pedestrian detection, while YOLOv8 has seen limited adoption (29, 30). However, YOLOv8 introduces significant improvements over YOLOv5, outperforming it in various aspects (31).\nThe notable changes in YOLOv8 include an anchor-free architecture, eliminating the need for anchor boxes and directly predicting object centers. This reduces the number of box predictions and speeds up post-processing steps like Non-Maximum Suppression. The head module has been updated to a decoupled structure, separating classification and detection heads and transitioning from Anchor-Based to Anchor-Free detection. The backbone network and neck module draw inspiration from YOLOv7's ELAN design with a modified C2f module (32). YOLOv8 uses Complete Intersection Over Union (CIoU) and Distribution Focal Loss (DFL) functions for bounding box loss and binary cross-entropy for classification loss. These losses have improved object detection performance, particularly when dealing with smaller objects. YOLOv8 retains YOLOv5's data augmentation but stops Mosaic augmentation in the final ten epochs. With its architectural improvements, YOLOv8 surpasses YOLOv5 in accuracy, achieving an average precision of 51.4% on the COCO dataset compared to YOLOv5's 50.5% (31)."}, {"title": "Tracking", "content": "Road user's positions must be tracked in each video frame to determine their trajectories. Tracking relies on assigning unique identifications (IDs) to detections and maintaining them throughout the frames. Even if the object detector fails due to occlusion or overlap, the tracker can still predict and track the objects.\nIntegrating DeepSORT, a state-of-the-art multi-object tracking algorithm, with YOLOv8, IDs are assigned to tracked objects. Bounding box information from the detection model is used to develop motion and visual similarity search models. This estimates the state of the track in the next frame. A track has been defined in the state space using (u, v, \u03b3, h, x, y, x', y'), where (u, v) represents the center of the bounding box of the track, \u03b3 represents the aspect ratio, h represents the height of the box, and (x, y, x', y') represents the velocity component of each state in image coordinates.\nTwo distances are calculated in the process. The association between the predicted Kalman states from the previous frame and the bounding box information arrived from the current frame is calculated by the squared Mahalanobis distance given in Equation 1. Here, the projection of i-th track distribution into measurement space is denoted by(yi, S\u012f) and j-th bounding box detection of new frame is expressed by dj.\n$d^{(1)}(i, j) = (dj \u2013 yi) Si^{-1}(dj \u2013 yi)$   (1)\nUnaccounted camera motion can introduce rapid displacements in the image plane, making the Mahalanobis distance a rather uninformed metric for tracking through occlusions. So, a second metric is used where for each bounding box detection dj, an appearance descriptor rj with $||r_j||=1$ is computed.\nFurther, a gallery $R_k = {r^{(i)}_{k=1}}^{L_k}$ of the last $L_k = 100$ associated appearance descriptors for each track k is kept. Then, the smallest cosine distance between the i-th track and j-th detection in appearance space is measured by using Equation 2.\n$d^{(2)}(i, j) = min\\{1 \u2013 r^i_j * r^i_l | r^i_l \\in R_i\\}$   (2)\nA gate matrix is formed for taking decision whether the distances calculated from Equation 1 and 2 are less than threshold value $t^{(1)}$ and $t^{(2)}$ respectively. These decisions are taken by using Equation 3. The association is referred as admissible if it is within the gating region of both matrices using Equation 4.\n$b^{(p)}_{(i,j)} = \\begin{cases} 1, d^{(p)}(i, j) \\leq t^{(p)}  \\\\ 0, otherwise  \\end{cases} ,\\forall p = 1,2$  (3)\n$b_{i,j} = \\prod\\limits_{p \\in P} b^{(p)}_{(i,j)}$   (4)\nFinally cost matrix is formed using the weighted sum of previously calculated two distance matrices using Equation 5"}, {"title": "Traffic State Measurement", "content": "$C_{i,j} = \\lambda d^{(1)}(i, j) + (1 - \\lambda) d^{(2)}(i, j)$   (5)\nThe Hungarian algorithm optimizes the cost matrix for data association. It matches detections with previous tracks, creating \"matched tracks.\" Unassociated detections and tracks undergo another matching process. \"Matched tracks\" are updated using the Kalman filter. \"Unmatched tracks\" inactive for three frames are removed, while \"unmatched detections\" creates tentative tracks and are deleted if unassociated within the first three frames. For more detailed information, readers are requested to see (24).\nTrajectory coordinates of each road users generated by the tracking algorithm are on skewed axis. Geometric correction is applied to those points using Equations 6 and 7 for Y and X axes, respectively.\nCorrection for Camera Skew\n$Y' = Y_o + y \\times \\frac{sin (\\delta)}{(x + \\varphi \\times cot(\\delta) \\times Y)}$   (6)\n$X' = X_o + \\frac{x}{\\varphi}$   (7)\n$\\varphi = \\frac{R_x}{R'_x}$   (8)\n$\\omega = \\frac{R_y}{R'_y}$   (9)\nWhere,\nx, y = Uncalibrated trajectory coordinates along skewed X axis and Y axis respectively\nXo, Yo = Geodetic X coordinate and Y coordinate of the reference point of (X', Y') coordinate system respectively\nX', Y' = Calibrated trajectory coordinates along Geodetic X axis and Y axis respectively\n\u03c6, \u03c9 = Magnification factor along X axis and Y axis respectively\n\u03b4 = Angle of the skewed axis with X axis\nRx, Ry =Length of a reference object along Geodetic X axis and Y axis respectively\nRx, Ry =Length of a reference object along skewed X axis and Y axis respectively\nEach detected vehicles and pedestrian have certain size represented by its bounding box information (centroid, width, and height). For simplicity, centroid point has been taken as simpler representation of detected vehicle or pedestrian. The measurement of each road user's position in space at tracked time frame (t) constitutes its trajectory. P\u2081 represents the trajectory of n-th road users which is a zipped collection of all points in the desired time interval Ti\nFlow measurement\n$P_n = \\bigcup \\{(X'(t), Y'(t))\\} , i = 1,2, ... ... N$   (10)\n$\\{\\forall t \\epsilon T_i\\}$  \nHere N is the number of time interval in total video duration. An input Line of Interest (LoI) is defined by the user for flow measurement. Then, an algorithm searches for the intersection of each road user's trajectory, P\u2081 with the LoI and increases its classified count value, (Sn)k if successes (Sn = 1). The classified counts are shown after each interval T\u012f and flow is measured by dividing the individual classified count with time interval where k \u2208 C, C is categorical set of road user classes and K is total road users identified of the class, k in the time interval.\n$q^k_i = \\frac{\\sum\\limits_{k=1}^{K} S^k_i}{T_i}$   (11)\nFor each time interval described before, Speed of individual tracked road users are calculated using Equation 12.\nSpeed Measurement\n$v(i)k = \\frac{\\sum\\limits_{t=1}^{F} \\sqrt{(X'(t_{1+1}) \u2013 X'(t_i))^2 + (Y'(t_{1+1}) \u2013 Y'(t_i))^2}}{(t_f - t_i) * f}$   (12)\nWhere, f is the video frame rate, F is the maximum tracked frames and t1+1 and t\u012b are successive tracked frames in that time interval, T\u012f."}, {"title": "DATA PREPARATION AND ANALYSIS", "content": "CityPersons dataset (27) focuses primarily on pedestrian detection. It is a subset of the Cityscapes dataset and annotates person instances only, which limits its utility for comprehensive mixed traffic analysis.\nThe MS COCO dataset (26) is a widely used benchmark for object detection tasks. It contains annotations for 80 categories, including various transportation-related classes such as persons, bicycles, cars, motorcycles, buses, trains, trucks, and boats. However, it lacks regional vehicle classes and may misclassify certain vehicles, such as 'Rickshaw' as \u2018Person'. In contrast, the DhakaAI dataset (33) focuses on vehicle classification and consists of 21 diverse classes, covering a wide array of vehicles. However, it lacks annotations for pedestrians in the dataset.\nTo overcome aforementioned limitations in existing datasets for detecting both vehicles and pedestrians in mixed traffic scenarios, a new dataset called \"DhakaPersons\" has been created by applying complementary data fusion techniques.\nBy training the DhakaAI dataset on the YOLOv8 model using transfer learning, mean average precision(mAP) of 0.755 in all classes has been achieved. Using this trained model, unannotated vehicles in the CityPersons have been detected. Similarly, the CityPersons dataset has been trained on YOLOv8 using transfer learning (mAP 0.687) to detect pedestrians in the DhakaAI dataset. Class-agnostic Non-Maximum Suppression has been applied to detect objects and bounding boxes, preventing multiple classes from being assigned to a single object. The mislabeled annotations are manually corrected and similar type vehicles are merged into single classes (i.e., cars and taxis into a single 'Private Passenger Car' class). A minimum of 5 pedestrian annotations is selected to ensure an adequate representation of pedestrians in each image.\nThe resulting 'DhakaPersons' dataset comprises 4576 images, divided into training (70%), validation (15%) and testing (15%) sets, consisting of 3200, 488 and 488 images, respectively. The dataset has 59576 annotated bounding boxes. The classes of the \u2018DhakaPersons' dataset are determined based on Bangladesh Road Transport Authority (BRTA) (34) registered vehicle classes to comply with field test addressing both motorized and non-motorized traffic . It should be mentioned that this classification is not confined to only BRTA, but it can also be transferred to other defined compositions. There are 14 classes, including Ambulance, Auto Rickshaw, Bicycle, Bus, Human Hauler, Microbus, Minibus, Motor Cycle, Pedestrian, Pickup, Private Passenger Car, Rickshaw, Special Purpose Vehicle, and Truck: instances (labeled objects) per class are 84, 3400, 913, 3681, 178, 2314, 436, 3032, 28947, 1225, 9951, 3537, 346 and 1532 respectively. \u2018Pedestrian' class has more than 28,000 instances annotated in the dataset. However, 'Ambulance', 'Special Purpose Vehicle' and \u2018Bicycle' are underrepresented."}, {"title": "Image Augmentation and Preprocessing", "content": "In the annotated dataset, some images are stored with varying orientations which mislead the training model. To address this, auto-orient has been processed on all the images in the dataset, ensuring consistent and accurate input for the model.\nThe dataset contains images of varying sizes, with 3,405 out of 4,576 training images having higher resolutions (>1024). The aspect ratio distribution reveals that 4,040 images have 2:1 aspect ratio. To ensure uniformity for training, all training images have been resized into 1024x512. Histogram Equalization is utilized to improve global contrast, particularly for images with close contrast values. 4% of background images are added to the dataset without labeling to improve model performance.\nAugmentations have been applied to enhance the diversity and robustness of the data. Each training example has produced three outputs. A crop augmentation has been employed, ranging from a minimum zoom of 30% to a maximum zoom of 70%. Shear transformations have been applied horizontally and vertically, with a range of \u00b110\u00b0. Grayscale has been applied to 30% of the images to enhance their visual characteristics. Brightness and exposure adjustments have been made, varying between -20% and +20%. Cutout augmentations have been introduced with 15 boxes, each sized at 3%. The mosaic technique has been utilized to combine multiple images into a single training example. Bounding boxes have undergone a blur effect of up to 1px, ensuring smoother edges. After image augmentation, the training set contains 9674 images, including 50 background images."}, {"title": "Model Training", "content": "We have used 'yolov8s.pt' as pre-trained weight for the final training because it provides the best trade-off between inference speed and accuracy. For training the model, we utilized 1xTesla P100 GPU, which has 3584 CUDA cores and 16GB (16.28GB Usable) GDDR6 VRAM provided by Kaggle, a data science competition platform. The batch size is the number of samples that are processed at once during training. Larger batch sizes can lead to faster convergence but also require more memory. The batch size is adopted to 12 for this study, ensuring faster computing speed with less memory requirement. Three different epochs have been chosen: 50, 100 and 150; however, 100 epochs gave the best result. We optimized the training using Stochastic Gradient Descent (SGD), Adaptive Moment Estimation (Adam) and Root Mean Square Propagation (RMSProp) (35, 36). Although Adam has given the best validation accuracy but has failed to give satisfactory test accuracy and resulted in overfitting data.\nOn the other hand, SGD has similar validation and testing accuracy, removes the overfitting issue. The values for Momentum, learning and weight decay are adopted as 0.9, 0.01 and 0.0005, respectively, by the Grid Search algorithm. A 10% label smoothing process was introduced to improve the model's generalization ability during the training process. The Input image size is chosen to be 1280x1280 as the prepared training dataset by fusion contains small objects. The number of anchors is set to 3 and the number of classes is set to 14."}, {"title": "Model Validation and Testing", "content": "Three main evaluation metrics are used, including Precision, Recall, and F1-Score to determine the effectiveness of the trained model.\nPrecision quantifies the accuracy of positive predictions made by the model. It is calculated by Equation 13.\n$Precision = \\frac{TP}{TP + FP}$   (13)\nRecall, also known as sensitivity, measures the model's ability to detect positive instances. It is calculated by Equation 14\n$Recall = \\frac{TP}{TP + FN}$   (14)"}, {"title": "Field Validation", "content": "F1-Score (see Equation 15) is a weighted average of precision and recall, which takes into account both false positives and false negatives.\n$\\text{F1 - Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$  (15)\nAnother commonly used metric for object detection models, mean Average Precision (mAP) has been also used to verify the model performance. The formula for calculating mean average precision is shown in Equation 16 and 17\n$AP = \\sum\\limits_{m=0}^{m=r-1} [Recall(m) - Recall(m + 1)] \\times Precision(m)$  \n$MAP=  \\frac{1}{k=n} \\sum\\limits_{k=1}^{k=n} APK$   (17)\nWhere r is the number of thresholds, n is the number of classes and  APK is the average precision for class k. mAP summarizes the precision-recall curve into a single value, considering both false positives and false negatives. It balances the trade-offs between precision and recall, providing an overall evaluation of the model's performance in object detection tasks.\nTwo locations in Dhaka city are chosen to validate the Traffic measurement methodology incorporated within DEEGITS. These locations comprise of different roadway and traffic characteristics. Specifically, the first location is a 4-legged urban signalized intersection containing heterogeneous motorized vehicles only. The intersection has three lanes in each leg. The dominant vehicle is Motorbike and the stream also contains a considerable number of side-facing buses, imposing a greater challenge to the framework to detect and consistently track smaller vehicles through occlusion. Another challenge is that the vehicles have long shadows and these shadows are falling upon other vehicles.\nThe next location is a 4-legged urban signalized intersection containing mixed motorized and non-motorized vehicles. The dominant vehicle is Rickshaw which is a non-motorized vehicle (NMV). The stream also contains a considerable number of bicycles. This location also imposes a challenging situation to detect, classify and track non-motorized traffic as a whole instead of separately along with a person sitting on the NMV. This allows for analyzing the accuracy of the data fusion technique. Although this video has smaller vehicle shadows, there exist shadows and occlusions of foreign objects (i.e., trees, electric poles).\nAmong these locations, traffic movements in both roadway segments show poor lane discipline. 60 minutes videos with 25 frame rates have been captured in each of the study areas. The mounting height of the cameras is at least 20ft and their angle is less than 45 degrees to reduce the detection of foreign objects (e.g., sky, birds etc.). For the same period, ground truth data (speed and flow) has also been collected from the video through manual post-processing. For ground truth and DEEGITS-based speed measurement, a strip of 88ft is chosen within the field of vision (FOV).\nThe camera calibration parameters set \u03c0= {\u03c6,\u03c9,\u03b4, \u03a70, Yo} are measured to be \u03c0\u2081 = {53.9782, 55.5444, 233865.97,2630553.14 } for the first location and \u03c0\u2082 = {60.0909, 46.0172, -78.5563, -76.1593, 234493.41, 2627654.18} for the second location, respectively. The values of the Xoand Yo are in Universal Transverse Mercator (UTM) 46N projected coordinate system.\nHyperparameters set obtained in the training step described in the sub-section \u2018Hyperparameter Tuning' yielded maximum precision-recall values and are used as required hyperparameters for the proposed framework. The framework analyzed the videos, and flow and speed measurements are obtained as these are the mostly used traffic state measurement variables (38, 39). Afterwards, these measurements are compared with the ground truth both qualitatively (Figure 4 and 5) and quantitatively."}, {"title": "CONCLUDING REMARKS", "content": "F1 - Score (see Equation 15) is a weighted average of precision and recall, which takes into account both false positives and false negatives.\nThis study presents a comprehensive framework that leverages state-of-the-art convolutional neural network (CNN) techniques to measure traffic states in congested mixed traffic scenarios. DEEGITS (Deep Learning Based Heterogeneous Traffic State Measurement), the robust framework which incorporates the proposed methodology, has been developed to facilitate simultaneous real-time classified traffic (i.e., vehicle and pedestrian) detection and heterogenous traffic state measurement. The training dataset has been enriched using a data fusion technique that simultaneously enables vehicle and pedestrian detection. Rigorous image preprocessing and augmentation are performed to enhance the quality and quantity of the dataset. Transfer learning is employed on YOLOv8 pre-trained model to enhance the na\u00efve model's ability to identify local vehicles. Suitable hyperparameters have been obtained using the Grid Search algorithm and the Stochastic Gradient Descent (SGD) optimizer performs best with these hyperparameters. After extensive experimentation and evaluation, the detection framework exhibits remarkable accuracy rates. The model achieved a validation accuracy of 0.794 mAP@0.5 and a testing accuracy of 0.786 mAP@0.5, surpassing previous benchmarks on similar datasets. This framework has also provided accurate and informative traffic measurements under different road and traffic characteristics. Specifically, the framework provides more than 91% Pearson correlation values for speed measurement at all the study locations. In contrast, this value is around 88% in the case of flow measurement.\nMoreover, among the study locations, analyses show that DEEGITS produces better results in the motorized traffic stream than the traffic stream with NMV. The main reason behind the lower correlation is the presence of non-standard and peculiar vehicles that are neither included in the training data nor has adequate sample number. On the other hand, speed measurement shows a reverse trend because the motorized traffic stream was suffering from severe occlusion beyond the framework's capacity. However, all errors are tested statistically and it found that all the errors are insignificant at a 95% confidence interval. All the above results show the robustness of the developed tool in measuring traffic flow and speed, which indicates vehicle detection accuracy. Notably, the dataset fusion technique ensures the simultaneous detection of vehicles and pedestrians. Preprocessing this dataset for training by fixing orientation and applying grayscale and histogram equalization removes any unnecessary noise from the dataset. Data augmentation techniques incorporating grid dropout, gaussian noise, and mosaic increase their effectiveness in detecting even when vehicles are in occluded condition. Moreover, transfer learning enhances the model's ability to identify local vehicles. Stochastic Gradient Descent (SGD) is incorporated to optimize hyperparameters, resulting in accurate vehicle classification without overfitting. Integration of the DeepSORT tracking algorithm enables the framework to detect pedestrians and vehicle trajectory, which ultimately resulted in the measurement of flow and speed.\nDEEGITS, as a framework, incorporates different components utilizing different robust algorithms. It can save extracted real-time data for offline and online videos in a delimited text file (.txt) with versatile readability with an unrestricted storing facility (number of maximum columns and rows). At the same time, the data requires very less memory than other spreadsheet formats (.xlsx, .csv, .ods) due to its text format file structure. However, the prototype DEEGITS system developed in this study cannot handle severe vehicle occlusions and the presence of foreign vehicle types at the current stage. Depending on the presence and severity of these problems, traffic measurement accuracy may get affected. In the future, the authors plan to improve this framework by incorporating more vehicle classes and a large training dataset to remove its weaknesses, as stated above.\nMoreover, the class balancing will be ensured to maintain uniformity among the instances of each vehicle class. Likewise, the current research scope does not include validating the framework at night. However, the YOLOv8 structure can also classify vehicles at night. Additionally, nighttime vehicle detection is another task that includes challenges other than daytime. The nighttime vehicle has partial visibility and glaring effect as critical challenges for this framework. Night vision infrared cameras can also pose different challenges by altering the actual color of the vehicle by converting the images into a greyscale that lacks multiple color channels. Thus, the framework keeps a scope to verify it at nighttime. Moreover, trajectory extraction, estimation, reconstruction, and prediction in the autonomous driving environment have become urgent in different research fields, like predicting traffic conflicts and anticipating crashes for"}]}