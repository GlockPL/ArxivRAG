{"title": "DreamPolish: Domain Score Distillation With Progressive Geometry Generation", "authors": ["Yean Cheng", "Ziqi Cai", "Ming Ding", "Wendi Zheng", "Shiyu Huang", "Yuxiao Dong", "Jie Tang", "Boxin Shi"], "abstract": "We introduce DreamPolish, a text-to-3D generation model that excels in producing refined geometry and high-quality textures. In the geometry construction phase, our approach leverages multiple neural representations to enhance the stability of the synthesis process. Instead of relying solely on a view-conditioned diffusion prior in the novel sampled views, which often leads to undesired artifacts in the geometric surface, we incorporate an additional normal estimator to polish the geometry details, conditioned on viewpoints with varying field-of-views. We propose to add a surface polishing stage with only a few training steps, which can effectively refine the artifacts attributed to limited guidance from previous stages and produce 3D objects with more desirable geometry. The key topic of texture generation using pretrained text-to-image models is to find a suitable domain in the vast latent distribution of these models that contains photorealistic and consistent renderings. In the texture generation phase, we introduce a novel score distillation objective, namely domain score distillation (DSD), to guide neural representations toward such a domain. We draw inspiration from the classifier-free guidance (CFG) in text-conditioned image generation tasks and show that CFG and variational distribution guidance represent distinct aspects in gradient guidance and are both imperative domains for the enhancement of texture quality. Extensive experiments show our proposed model can produce 3D assets with polished surfaces and photorealistic textures, outperforming existing state-of-the-art methods.", "sections": [{"title": "1 Introduction", "content": "Generating 3D assets benefits a series of downstream applications like virtual reality, movies, video game, 3D printing, etc. Existing 3D generation methods can be divided into 3D native method and 2D/3D hybrid methods. 3D native generation methods [22, 34, 23] directly model 3D data, and demonstrate considerable success in producing basic 3D objects. However, these methods often"}, {"title": "2 Related work", "content": "3D representations are the foundation for accurate and detailed text-to-3D generation. These representations can be categorized into explicit, implicit, and hybrid forms. Explicit representations, such as point clouds, voxel grids, and meshes, provide precise control by directly describing 3D geometry. However, integrating these forms into deep learning frameworks is challenging due to their lack of differentiability [19, 43]. Implicit representations, including NeRF [26], NeuS [40], Gaussian splatting [17] and Instant-NGP [27], leverage neural networks to generate continuous volumetric fields, making them more suitable for deep learning applications. Implicit representations produce fine-grained 3D models, however, they pose challenges in terms of editability and controllability [10], and require additional tools like MarchingCubes [24] for rasterization. A hybrid approach such as DMTet [35], Plenoxels [7] and TensoRF [3] combines the advantages of both explicit and implicit methods. DMTet [35] utilizes a deformable tetrahedral grid to optimize surface geometry and topology, converting signed distance functions into explicit meshes for high-resolution 3D synthesis. We propose using a series of representations to progressively generate 3D assets, leveraging the advantages of different forms of representations.\n3D diffusion priors extend the capabilities of diffusion models to 3D data, improving the quality of generation and reconstruction. Notable techniques such as the Zero123 series (e.g., Zero123 [22] and Zero123++ [36]) and GeoWizard [9] incorporate geometric priors to ensure the consistency and coherence in 3D structures. GeoWizard [9] combines depth and normal information with a U-Net [32], and excels in providing high-quality surface normal estimations from single images, leveraging large-scale 3D datasets like OmniData [6] to mitigate data limitations. These methods show significant potential in enhancing 3D generation quality. We use a combination of 3D diffusion priors to eliminate the ambiguity in novel views due to lack of information, thereby improving the robustness and photorealism of 3D asset generation.\nText-to-3D generative models have been extensively studied to automate the creation of 3D assets, reducing the need for manual intervention. These models can be classified into native 3D models and 2D/3D hybrid models. Native 3D models such as ZeroNVS [34], Text2Shape [4], ShapeCrafter [8], SyncDreamer [23] and DreamGaussian [39], generate 3D data directly using specialized neural networks. These models employ complex neural architectures to create detailed 3D shapes condi- tioned on text and viewpoints. However, these 3D native models fall short when generating intricate objects attributing to limited representation ability. In contrast, 2D/3D hybrid models, such as DreamFusion [29], Magic3D [21], Fantasia3D [5], DreamControl [15], and HIFA [46] combine pretrained 2D diffusion models with 3D consistency constraints. Hybrid models harness the diverse generation capabilities of 2D models while ensuring geometric coherence in 3D space, addressing the challenges of data scarcity and high computational demands inherent to native 3D models. Despite these advances, the quality of generated 3D content is still inferior to that of the handcrafted 3D content in terms of surface quality and texture photorealism. Our method addresses these limitations, producing both high-quality surfaces and realistic textures."}, {"title": "3 Preliminaries", "content": "Diffusion models [37, 12] are a collection of likelihood-based generative models used to learn data distributions. Forward process is a Gaussian process that progressively adds random noise \u03f5 to the data x sampled from q(x). The reverse process reconstructs the original data from $x_T$. The generative process is parameterized as a Markov process: $p(x_{0:T}) := p(x_T) \\prod_{t=1}^T p(x_{t-1}|x_t)$.\nThe diffusion model is trained to maximize the variational lower bound of the data log-likelihood [12]. The objective has been optimized into the mean squared error form by the researchers through a series of mathematical derivations and the re-parameterization trick for better convergence:\n$\\mathcal{L}_{Diffusion}(\\phi) := \\mathbb{E}_{x \\sim q(x), t \\sim U(0,1), \\epsilon \\sim \\mathcal{N}(0,1)}[w(t) || \\epsilon_{\\phi}(a_t x + \\sigma_t \\epsilon; t) - \\epsilon ||^2]$   (1)\nLater research attempt to add text condition on y to the generation process [31, 2], which can be expressed as $\\epsilon_{\\phi}(x_t; y, t)$. The diversity of the generated images and the alignment with the"}, {"title": "4 Method", "content": "The overall pipeline is illustrated in Figure 2. We decompose the intricate text-to-3D task into two phases: progressive geometry polishing and domain-guided texture enhancing. In this section, we first describe the geometry construction process detailed in Section 4.1. Next, we introduce the texture generation method in Section 4.2."}, {"title": "4.1 Progressive geometry polishing", "content": "Progressive construction. Explicit and implicit neural representations exhibit both strengths and limitations. We propose to leverage the strengths of different representations by progressively generating 3D content with a combination of these 3D representations. The generation starts with NeRF [26] due to its stable training process and ability to quickly generate a rough 3D structure. We then transition to NeuS [40], a surface-based representation that provides more accurate and detailed surface information. Inspired by [20], we incorporate a progressive hash band to further enhance the quality of NeuS [40]. Subsequently, we switch to DMTet [35], which allows for the use of NVDiffrast [18] and integration of the graphics pipeline, enabling fast rendering.\nThe sampled views can be divided into a reference view (the view with input image as a reference) and novel views (novel synthesized views without any reference). We employ different training objectives on these views, dubbed as reference-based loss $L^{cref}$ and guidance-based loss $L^{guide}$. The reference-based loss $L^{cref}$ primarily measures the error between the rendered image under the reference frame and the reference image itself. $L^{cref}$ can be considered a reconstruction loss:\n$L^{cref} = \\lambda_1 L^{cref}_{normal} + \\lambda_2 L^{cref}_{depth} + \\lambda_3 L^{cref}_{mask} + \\lambda_4 L^{cref}_{rgb}.$  (5)\nwhere $\\lambda_*$ are the respective loss weights, and {$L^{cref}_{normal}$, $L^{cref}_{depth}$, $L^{cref}_{mask}$, $L^{cref}_{rgb}$} are the losses for surface normal, depth, foreground mask, and reference image, respectively, defined as follows:\n$L^{cref}_{rgb} = || m \\cdot (x - g(\\theta,\\varphi)) ||_2, L^{cref}_{mask} = || m - g(\\theta; \\varphi) ||_2,$  (6)\nwhere $\\hat{c}$ is the camera pose corresponding to the reference image x. We add normal and depth supervision based on prediction from an off-the-shelf normal and depth estimator [6]:\n$L^{Cref}_{normal} \\frac{nn}{||n||_2 ||\\hat{n}||_2} \\quad L^{depth} \\frac{conv(d, \\hat{d})}{\\sigma(d)\\sigma(\\hat{d})}$ (7)\nThe reference-based loss function above ensures that the 3D model generates high-quality images at pose $\\hat{c}$. However, there is no corresponding reference pose under other camera coordinates, the model needs to rely on pretrained models (i.e., priors) to determine geometry information in those novel poses. Although view-conditioned native 3D models have inferior rendering performance compared to 2D/3D models, they contain 3D coherent information and can be directly used for multi-view geometric supervision. For novel views, we use the vanilla SDS Loss, as in Figure 3, for supervision:\n$\\nabla_{\\theta} L^{guide} = \\nabla_{\\theta} L_{SDS} (\\phi_1, g(\\theta))$. (8)\nwhere $\\phi_1$ is the Stable-Zero123 [22] model. Combining the above operations, the gradient of the geometry generation loss can be expressed as:\n$\\nabla_{\\theta} L_{geom} = \\lambda_{guide} \\nabla_{\\theta} L^{guide} + \\lambda_{ref} \\nabla_{\\theta} L^{ref}$. (9)\nwhere $\\lambda_{ref}$ and $\\lambda_{guide}$ are the corresponding model weights.\nSurface polishing. Despite the strengths of our progressive construction pipeline, we observe that the generated geometry could still benefit from additional refinement. We introduce a surface polishing stage aiming at further enhancing the surface. In this stage, we fix the texture parameters and solely update the geometry to polish the geometry. Additionally, we loosen the field-of-view camera restrictions of Stable Zero123 [22], allowing for a greater diversity of camera parameters. This enables rendering novel views with more diverse viewing conditions, which in turn provides richer information for the polishing process.\nWe utilize a pretrained diffusion model [9] to predict normal maps from images rendered under novel views, eliminating the ambiguity of geometry caused by insufficient information, we leverage standard SDS loss as supervision:\n$L^{guide}_{normal} = \\lambda_{comp} ||\\hat{n}_{comp} - n_{novel}||_2 + \\lambda_{lpips} L_{lpips} (\\hat{n}_{comp}, \\hat{n}_{novel})$, (10)\nwhere $\\hat{n}_{comp}$ is rendered normal map, $\\hat{n}_{novel}$ is corresponding normal prediction generated from rendered RGB image, and $L_{lpips}$ denotes a perceptual loss [45]."}, {"title": "4.2 Domain-guided texture enhancing", "content": "With the geometry construction stage, our pipeline yields 3D objects represented by DMTet [35] with detailed geometric intricacies and smoothed surfaces. In the following phase, we fix the geometry representation and focus on texture generation.\nWe illustrate the score distillation process in the latent space as Fig. 3. Diffusion models are designed to predict noises, which are assumed to follow a D dimensional Gaussian distribution. The major population of the probability is concentrated on a ring with a radius $\\sqrt{D}$ around the origin, visualized as the solid circle in Fig. 3. Samples around this ring represent the photorealistic images learned by pretrained text-to-image models from large scale image datasets.\nThe standard SDS objective (Eq. (3)) minimizes distance between the estimated noise $\\epsilon_{\\phi}(x_t; y, t)$ from diffusion model and random noise \u03f5. However, Eq. (3) is a sum of the expected distance between the estimated noise and a random sampled noise. Due to its zero mean nature, the \u03f5 term represents a force toward the center (mean) of the distribution, as demonstrated in Fig. 3 (a), which eventually causes unsatisfactory over-saturated results. The recently proposed VSD [41] and BSD [38] provide guidance direction towards a variational domain, represented by the dotted line in Fig. 3 (b). This domain is the learned distribution of the object being constructed, hence providing high-quality knowledge to distill from compared to the vanilla SDS term. However, this variational domain is parameterized by a LoRA [14], the training process does not guarantee the learned domain is stable and contains the necessary information for high-quality texture with sufficient diversities. The unconditional image domain is located around the major distribution, and represents the diversity and stability of the text-to-image generation capability [13].\nWe further propose DSD, which balances quality and stability within the distillation sampling process. As visualized in Fig. 3 (c), DSD consists of two guidance from the variational domain and the unconditional image domain, we employ the variational domain guidance to maintain photorealism and utilize the unconditional image guidance to ensure the stability of the distillation process. The gradient of the proposed DSD distillation sampling method can be expressed as:\n$\\nabla_{\\theta} L_{DSD} = \\mathbb{E}_{t,\\epsilon, c}[w(t) (\\lambda_{realistic} (\\epsilon_{\\phi}(x_t; y, t) - \\epsilon^*_{realistic}(x_t; y, t)) - \\lambda_{stable} (\\epsilon_{\\phi}(x_t; t) - \\epsilon^*_{stable}(x_t; t)) ) \\frac{\\partial x_t}{\\partial \\theta}]$ (11)\nwhere $\\lambda_{realistic}$ and $\\lambda_{stable}$ represents the weights of diversity and stability."}, {"title": "5 Experiment", "content": ""}, {"title": "5.1 Experimental settings", "content": "Baselines. To evaluate the performance of our method, we compare it with several representative and state-of-the-art models. Specifically, we compare our method to DreamFusion [29], Geo- Dream [25], and ProlificDreamer [41], which use text inputs only, as well as Magic123 [30] and DreamCraft3D [38], which support both text and reference image inputs."}, {"title": "5.2 Qualitative comparison", "content": "We compare our method with the aforementioned baselines. The results are demonstrated in Fig. 4. Given the reference image, we show the generated renderings of our model and the baselines under novel viewpoints. Benefiting from refined geometry and the DSD objective, our model could generate intricate details and photorealistic textures, such as the frog's eye in the second row and the wing of the rubber duck in the third row. Please refer to the supplement for more results."}, {"title": "5.3 Quantitative experiments", "content": "Comparision results. We render images from novel views and compute the mean CLIP Score between each pair of rendered images and the text prompts. Table 1 summarizes the re- sults of the quantitative evaluation. Our method outperforms the baselines in every metric.\nUser study. We randomly select multiple examples from different viewpoints and shuffle the data from our model and the baseline methods. Users are asked to choose the model they consider to have the best performance for each example. We received a total of 40 valid responses, and the"}, {"title": "5.4 Ablation study", "content": "Geometry construction. Figure 6 shows the normal map of each stage in the geometry construction process. We replace the proposed $L^{guide}$ with a simple normal smooth loss: $L^{guide}_{ablation} = |\\nabla_h n|^2 + |\\nabla_w n|^2$, where $\\nabla_h n$ and $\\nabla_w n$ represent the gradients of the normal map. $L^{guide}_{ablation}$ polishes the artifacts in previous stages into refined surface. As shown in the fourth row, the smooth loss $L^{guide}_{ablation}$ still produces artifacts and is insufficient for surface polishing.\nTexture generation. Figure 4 proves our model generates state-of-the-art 3D objects, thanks to both the geometry construction and texture generation modules. To further prove the effectiveness of the DSD objective, we use the previous losses (SDS [29], VDS [41] and BSD [38]) on the same geometry produced by our geometry phase, and compare the texture quality with that produced by our proposed DSD. The results are shown in Fig. 7, demonstrating that the DSD can generate textures with superior photorealism (e.g., fewer artifacts in the first row and more details in the second row)."}, {"title": "6 Conclusion", "content": "We present DreamPolish, a text-to-3D generation model that achieves polished geometry and pho- torealistic textures. By progressively constructing geometry and incorporating a surface polishing stage, DreamPolish could produce refined surfaces; with the proposed DSD objective, our approach addresses key challenges in texture generation and is able to stably distill photorealistic details from the latent domain. Extensive experiments demonstrate that DreamPolish can produce 3D assets with superior quality, setting a new benchmark in the 3D generation field.\nLimitations. Despite the success of DreamPolish, several limitations remain. The geometry refinement stage, while effective, is limited by the quality of the initial geometry. Additionally, the computational cost of our approach can be further optimized."}]}