{"title": "ArAIEval Shared Task: Propagandistic Techniques Detection in Unimodal and Multimodal Arabic Content", "authors": ["Maram Hasanain", "Md. Arid Hasan", "Fatema Ahmed", "Reem Suwaileh", "Md. Rafiul Biswas", "Wajdi Zaghouani", "Firoj Alam"], "abstract": "We present an overview of the second edition of the ArAIEval shared task, organized as part of the ArabicNLP 2024 conference co-located with ACL 2024. In this edition, ArAIEval offers two tasks: (i) detection of propagandistic textual spans with persuasion techniques identification in tweets and news articles, and (ii) distinguishing between propagandistic and non-propagandistic memes. A total of 14 teams participated in the final evaluation phase, with 6 and 9 teams participating in Tasks 1 and 2, respectively. Finally, 11 teams submitted system description papers. Across both tasks, we observed that fine-tuning transformer models such as AraBERT was at the core of the majority of the participating systems. We provide a description of the task setup, including a description of the dataset construction and the evaluation setup. We further provide a brief overview of the participating systems. All datasets and evaluation scripts are released to the research community. We hope this will enable further research on these important tasks in Arabic.", "sections": [{"title": "1 Introduction", "content": "Online media has become a primary channel for information dissemination and consumption, with numerous individuals considering it their main source of news (Perrin, 2015). While online media (including news and social media platforms) offers a plethora of benefits, it is periodically exploited by malicious actors aiming to manipulate and mislead a broad audience. They often engage in sharing inappropriate content, misinformation, and disinformation (Alam et al., 2022a; Sharma et al., 2022). Among the various forms of misleading and harmful content, propaganda is another communication tool designed to influence opinions and actions to achieve a specific objective. Propaganda is defined as a form of communication that is aimed at influencing the attitude of a community toward some cause or position by presenting only one side of an argument, which is achieved by means of well-defined rhetorical and psychological devices (IPA, 1938). It is often biased or misleading in nature and is used to promote a particular political cause or point of view.\nNews reporting in the mainstream media channels often use persuasion techniques to promote a particular editorial agenda. In different communication channels, propaganda is conveyed through the use of diverse persuasion techniques (Miller, 1939), which range from leveraging the emotions of the audience, such as using emotional technique or logical fallacies such as straw man (misrepresenting someone's opinion), hidden ad-hominem fallacies, and red herring (presenting irrelevant data).\nIn the past years, propaganda has been widely used on social media to influence and/or mislead the audience, which became a major concern for different stakeholders including social media platforms and government agencies. To combat the proliferation of propaganda on online platforms, there has been a significant surge in research in recent years. The aim is to automatically identify propagandistic content in textual, visual, and multimodal modalities, such as memes (Chen et al., 2023; Dimitrov et al., 2021; Da San Martino et al., 2019). These efforts mainly focused on English, but with increased interest in other languages too.\nWe enrich the Arabic AI research on that problem, by organizing a shared task on fine-grained propaganda techniques detection for Arabic, which attracted many participants (Alam et al., 2022b). We followed that by organizing the Arabic AI Evaluation (ArAIEval) shared task at ArabicNLP 2023, targeting binary and multilabel propaganda detection in Arabic text, with 14 and 8 teams participating in these subtasks, respectively (Hasanain et al., 2023b).\nFollowing the success of our previous shared tasks, and given the interest from the community in further pushing research in this domain, we organize the second edition of the ArAIEval shared task covering the following two tasks: (i) detection of propagandistic textual spans with persuasion techniques identification (unimodal), and (ii) distinguishing between propagandistic and non-propagandistic memes (multimodal).\nThis edition of the shared task attracted good participation with a total of 45 registrations. Fourteen teams submitted their systems, and 11 teams submitted system description papers. For all tasks, majority of participating systems utilized transformer-based models, and several of them applied data augmentation techniques. All systems outperformed the respective random baselines.\nIn the remainder of this paper, we define each of the two tasks, describe the manually constructed Arabic evaluation datasets, and provide an overview of the participating systems and their official scores."}, {"title": "2 Related Work", "content": "2.1 Persuasion Techniques\nThe detection of persuasion techniques has brought significant attention across multiple research domains, including natural language processing (NLP), computational linguistics, psychology, and communication studies. Researchers have developed various approaches to automatically identify persuasive elements in digital content, leveraging advancements in NLP and machine learning.\nOne prominent approach focuses on the linguistic analysis of persuasive texts. For instance, Hidey et al. (2017) proposed a framework for identifying persuasion techniques in online discussions by analyzing rhetorical strategies such as appeals to emotion, authority, and logic. Their study demonstrated the potential of NLP techniques to classify and detect persuasive elements with high accuracy.\nBuilding on this foundation, researchers have explored the use of machine learning models to enhance the detection of persuasion techniques. Habernal and Gurevych (2015) introduced the ArgumenText system, which applies supervised learning to classify argument components and identify persuasion techniques in user-generated content. This system showed promising results in distinguishing between different types of persuasive appeals.\nIn the domain of social media, the detection of persuasion techniques has been particularly challenging due to the diverse and dynamic nature of user interactions. One notable study by Morstatter et al. (2018) utilized deep learning models to detect persuasion techniques in tweets related to political campaigns. By analyzing both textual features and user metadata, the researchers were able to identify persuasive content employed by political actors and their impact on public opinion.\nThe advent of a large language model has paved a new direction in persuasion techniques detection. Hasanain et al. (2024) developed ArPro containing 8K paragraphs labeled at the text span level for 23 propagandistic techniques and used it to evaluate the performance of GPT-4 in fine-grained propaganda detection. GPT-4 model has also shown improved performance when using a large-scale dataset with annotations from human annotators of varying expertise and providing more information to the model as prompts (Hasanain et al., 2023a).\n2.2 Multimodal Detection\nMultilingual and multimodal detection of persuasion techniques has also gained attention in recent years. Park et al. (2016) created a multimodal corpus including acoustic, verbal, and visual features of 1,000 movie review videos to predict persuasiveness. Fang et al. (2022a) embedded text and images using different networks and fused the multi-modal embeddings. A split-and-share module with multi-level representations then processes these fused features to improve the detection performance of persuasive techniques.\nNojavanasghari et al. (2016) introduced a deep neural network architecture for predicting persuasiveness by combining three modalities: visual, acoustic, and text modalities. The multimodal fusion model outperforms unimodal models and can handle limited labeled data effectively. Another significant contribution to the field is the work by Chen et al. (2017) on multimodal sentiment analysis, which is closely related to the detection of persuasive content.\n2.3 Related Shared Tasks\nSeveral shared tasks have been organized to engage the research community in persuasion techniques detection. The shared task organized by"}, {"title": "3 Task 1: Unimodal (Text) Propagandistic Technique Detection", "content": "3.1 Task Definition\nThe objective of this task is to develop and evaluate systems capable of detecting specific propagandistic techniques within textual content. The task is defined as follows. Given a multigenre text snippet (a news paragraph or a tweet), the task is to detect the propaganda techniques used in the text together with the exact span(s) in which each propaganda technique appears. This is a multilabel sequence tagging task.\n3.2 Data\nThe dataset for this task covers two genres, tweets and paragraphs extracted from news articles. We collect raw data to annotate as follows.\nTweets: We start from the same tweets dataset collected from Twitter accounts of Arabic news sources, as described in the previous editions of the shared task (Alam et al., 2022b; Hasanain et al., 2023b). In addition to that, we collect a fresh set of tweets targeting the current events in Gaza. The tweet set was collected by searching through the Twitter search APIusing manually developed Arabic keywords.\nNews paragraphs: The news paragraphs were sourced and annotated following the same approach followed to develop ArPro dataset (Hasanain et al., 2024). News paragraphs are extracted from two collections of new articles: (i) AraFacts, and (ii) a large-scale in-house collection. The AraFacts dataset (Ali et al., 2021) contains true and false Arabic claims verified by fact-checking websites, and each claim is associated with online sources propagating or negating the claim. We only keep Web pages that are from news media in the set (e.g., www.alquds.co.uk). We automatically parsed the news articles and selected paragraphs for the annotation. As for the in-house collection, it consists of 600K news articles from over 400 news media outlets. The dataset is very diverse, covering 14 different broad topics and a wide coverage of Arabic news media.\nData annotation: The annotation process includes two phases as detailed in previous studies (Hasanain et al., 2024, 2023a). The two annotation phases consist of: (i) three annotators independently annotate the same text snippet by persuasion techniques on the span level, through an annotation interface designed for the task and (ii) two consolidators (expert annotators) discuss each instance and finalize the gold annotations. Since annotations are at the fragment level, it might happen that an annotation is spotted by only one annotator. In order to train the annotators, we provide clear annotation instructions with examples and ask them to annotate a sample of text (tweets, or paragraphs from news articles). Then, we revise their annotations and provide feedback. In Figure 1, we provide an example of span annotation along with its English translation, which shows different text spans annotated with propagandistic techniques.\nThe annotation taxonomy consists of a set of 23 persuasion techniques that is adopted from existing research (Piskorski et al., 2023). We should note here that multiple techniques can be found in the same text snippet even with an overlap. Below we give an example subset of the persuasion techniques, and briefly summarize them:\n1. Name calling or labeling: labeling the object of the propaganda campaign as something that the target audience fears, hates, finds undesirable, or loves, praises.\n2. Appeal to Fear, Prejudice: building support or rejection for an idea by instilling fear or repulsion towards it, or to an alternative idea.\n3. Strawman: giving the impression that an argument is refuted, whereas the real subject of the argument was not addressed or refuted, but instead was replaced with a different one.\nData splits: The full set of annotated paragraphs is divided into three subsets: train, development, and test, using a stratified splitting approach to ensure that the distribution of persuasion techniques is consistent across the splits. For the tweets set, we split the full annotated tweet set from the previous edition of the lab (Alam et al., 2022b) into train and development subsets, while the test set is annotated for this shared task. Finally, we construct the multi-genre subsets for the task by merging the sets of paragraphs and tweets. The full dataset is ~9K snippets, randomly split into ~78%, ~11% and ~11% training, development and test splits, respectively.\nStatistics: In Tables 1 we show the distribution of labels across splits for Task 1. It reports that the techniques loaded language, name calling, and exaggeration-minimisation are the most frequent among all techniques. Overall, the label space is highly imbalanced, which is typical for these types of tasks, as seen in previous studies (Da San Martino et al., 2019; Dimitrov et al., 2021)."}, {"title": "3.3 Evaluation Setup", "content": "The task was organized into two phases:\n\u2022 Development phase: we released the train and development subsets, and participants submitted runs on the development set through a competition on Codalab.2\n\u2022 Test phase: we released the official test subset, and the participants were given a few days to submit their final predictions through the same Codalab competition. Only the latest submission from each team was considered official and was used for the final team ranking.\nMeasures: Task 1 is a multilabel sequence tagging task. We measure the performance of the participating systems using a modified F\u2081 measure that accounts for partial matching between the spans across the gold labels and the predictions (Alam et al., 2022b)."}, {"title": "3.4 Results and Overview of the Systems", "content": "A total of 6 teams submitted runs for the evaluation phase of the task. In Table 2, we provide an overview of the participating systems for which a description paper was submitted. As shown in Table 2, fine-tuning pre-trained Arabic models, specifically AraBERT (Antoun et al., 2020), multilingual BERT (mBERT) (Devlin et al., 2019), Arabic-BERT (Safaya et al., 2020), and CAMEL-BERT (Inoue et al., 2021) is the most common system architecture.\nIn Table 3, we report the results of the participants' systems including a random baseline. Results are competitive among the top four teams, performance differences are relatively lower among them. All systems surpass the random baseline.\nTeam CUET_sstm (Labib et al., 2024) used data augmentation with the synonym replacement approach from the nlpaug library. The data was preprocessed and encoded with the BIO technique to transform it into a token classification problem for span detection. They experimented with fine-tuning different models, including transformer-based models such as MAREFA-NER and AraBERT; however, Arabic-BERT outperformed the other models.\nTeam Mela (Riyadh and Nabhani, 2024) fine-tuned mBERT on the training set and experimented with different hidden layers of the model. Their experiments showed that the 10th hidden layer yielded the best F1 score on the development set.\nTeam SemanticCuetSync (Shohan et al., 2024) experimented with different types of models. Firstly, they trained typical machine learning models, including: Logistic Regression, Support Vector Machine, and Multinomial Naive Bayes. Secondly, they also trained deep learning-based models such as CNN, CNN+LSTM, and CNN+BiLSTM. Lastly, several transformer-based models were fine-tuned: AraBERTv2, CAMeLBERT, and Arabic-BERT. Among all the models, CAMELBERT resulted in the highest micro-F1 score.\nTeam SussexAI (Fouad and Weeds, 2024) performed data augmentation on least represented labels to reduce label imbalance. They also generated synthetic data using random masking. They truncated 'non-propaganda' tokens outside \u2018propaganda' spans and reduced 'non-propaganda' tokens. Finally, they fine-tuned the AraBERT transformer model with three dataset setups: augmented, non-augmented, and random truncation.\nTeam MemeMind (Biswas et al., 2024) encoded each label and created BIO tags, modeling the problem as a token classification task. Then, they fine-tuned AraBERTv2 in two steps: trained only the classification layer for a few epochs and then fine-tuned the full model by updating all parameters. They tried different transformer models. Among them, AraBERTv2 showed the best performance.\nTeam Nullpointer (Abrar Abir, 2024) applied different preprocessing techniques, such as filtering out Unicode characters. Then, they fine-tuned AraBERTv2 following two setups to model the task: (i) model the problem as a token classification task, with different label aggregation applied overt tokens at prediction time to find per-word label, and (ii) model the problem as a word classification task, representing each word using max-pooling of embeddings of its constituent tokens."}, {"title": "4 Task 2: Multimodal Propagandistic Memes Classification", "content": "4.1 Task Definition\nThe goal of this task is to develop and evaluate systems capable of classifying multimodal propagandistic memes. Memes typically consist of a background image, and a layer of text that adds context, humour, or commentary to the image. The combination of the image and the text creates a specific message, joke, or commentary that is meant to be easily understood, relatable, and shareable.\nThe aim of this task is to foster research in the intersection of multimodal learning and propaganda detection, encouraging innovative solutions that effectively combine visual and textual information to identify and understand propagandistic contents. To account for the different modalities covered by this task, we model the task into three binary classification subtasks.\n\u2022 Subtask 2A: Given a text extracted from a meme, categorize whether it is propagandistic or not.\n\u2022 Subtask 2B: Given a meme (text overlayed image), detect whether the content is propagandistic.\n\u2022 Subtask 2C: Given multimodal content (text extracted from meme and the meme itself), detect whether the content is propagandistic.\n4.2 Dataset\nThe dataset consists of ~3K memes annotated as propagandistic vs not-propagandistic, which were collected from different social media (e.g., Facebook, Twitter, Instagram and Pinterest). Each of them were annotated by three annotators and majority decision is considered as the final label. Texts from the memes were extracted using an off-the-shelf OCR,3 followed by manual editing of the propagandistic memes. An example of such memes is provided in Figure 2. The distribution of propagandistic and not-propagandistic labels are 40% and 60%, respectively. More details of this dataset can be found in (Alam et al., 2024).\nData splits: The dataset is split into 70%, 10% and 20% for training, development, and test, respectively.\nStatistics: Table 4 reports the distribution of labels for Task 2. The proportion of propagandistic memes is low, with a total of 28%, which makes the task more challenging."}, {"title": "4.3 Evaluation Setup", "content": "Similar to Task 1, we also conducted this task in two phases (i.e., development and test) as discussed in Section 3.3. Systems were evaluated using macro F1 as the official measure."}, {"title": "4.4 Results Overview of the Systems", "content": "A total of 6, 3 and 7 teams submitted runs for Subtask 2A, 2B and 2C, respectively with nine unique teams. Table 5gives an overview of the participating systems for which a description paper was submitted. Three out of five teams participated in all subtasks. Among the teams, fine-tuning transformer models such as MARBERT (Abdul-Mageed et al., 2021) and CAMELBERT (Inoue et al., 2021) is the most popular architecture. As for the vision models ResNet was the most popular choice. In Table 6, we report the results and rankings for all systems across all subtasks. All systems outperformed the random baselines by a large margin. Among the three modalities, performances in the text and multimodal modalities (Subtasks 2A and 2C) were relatively higher compared to those in the image-only modality.\nTeam AlexUNLP-MZ (Zaytoon et al., 2024) used a Large Language Model (LLM) to extract features from the dataset. They considered 'weighted loss' and 'contrastive loss' while training the model. For text classification, they used the BLOOMZ model. For image classification, they tried using CNN-based ResNet and DenseNet architectures. For the multimodal data, they used a fusion of two architectures, such as BLOOMZ-1b1 and ResNet101.\nTeam ASOS (Alhabashi et al., 2024) used the MARBERT model for text classification and ResNet50 for image classification, and finally fused these two models for multimodality.\nTeam CLTL (Wang and Markov, 2024) applied MARBERT, CAMELBERT and GigaBERT (Lan et al., 2020) for text classification. CAMELBERT showed superior performance compared to the other models. For the image modality, they examined two models: EVA (Fang et al., 2022b) and CAFormer (Yu et al., 2023). For multimodality, they merged the embeddings of text and image using the multilayer perceptron technique.\nTeam MODOS (Haouhat et al., 2024) preprocessed raw data before feeding it into the model. Firstly, they used the Segment Anything Model for image segmentation of the meme images. Then, they employed the state-of-the-art image encoder CLIP to extract the image embeddings. Finally, they used LSTM for multimodal classification.\nTeam MemeMind (Shah et al., 2024) used synthetic text generated by GPT-4 (OpenAI, 2023) for text data augmentation. For image modality, they augmented data using DALL-E-2 and fine-tuned ResNet50, EfficientFormer (v2), and ConvNeXt-tiny architectures. For the multimodality, they fused the ConvNeXt-tiny and BERT."}, {"title": "5 Conclusion and Future Work", "content": "We presented an overview of the ArAIEval shared task, which consists of two tasks: (i) detection of propagandistic textual spans with persuasion techniques identification in tweets and news articles, and (ii) distinguishing between propagandistic and non-propagandistic memes. The task attracted the attention of many teams: a total of 45 teams registered to participate during the evaluation phase, with 6 and 9 teams eventually making an official submission on the test set for tasks 1 and 2, respectively. Finally, 11 teams submitted task description papers. Task 1 aimed to identify the propaganda techniques used in multi-genre text snippets, including tweets and news articles. The task was to detect textual spans with propagandistic techniques. On the other hand, Task 2 aimed to detect propaganda in memes (multimodal content) in binary classification settings for different modalities. For both tasks, majority of the systems fine-tuned pre-trained transformer models.\nFuture editions of the ArAIEval shared task will explore more complex tasks, such as incorporating Arabic dialects. Increasing the dataset size and focusing on low-prevalence class labels will be crucial to improve model robustness. For task 2, future work could involve labeling the dataset with specific propagandistic techniques."}, {"title": "Limitations", "content": "The datasets for both tasks are skewed in label distribution, making system development more challenging. To address this problem, increasing the data size with a focus on low-prevalence class labels could be beneficial. As for Task 2, we observe that the systems achieved relatively better performance, likely because the task was relatively simple in nature - a binary classification setting."}, {"title": "Ethical Considerations", "content": "The ArAIEval shared task involves the development of models to detect propagandistic content in Arabic text and memes. While this research aims to combat the spread of misleading and manipulative information, it is essential to consider the potential ethical implications.\nThe datasets used in the shared task may contain biases, as propaganda is often subjective and can be influenced by cultural, political, and social factors. It is crucial to acknowledge these biases and strive for diverse and representative datasets to ensure the models developed are as unbiased as possible.\nThe models developed in this shared task could potentially be misused by malicious actors to create more sophisticated propaganda or to target specific individuals or groups. Therefore, we ask developers to be aware of this issue while deploying models."}]}