{"title": "Flexible Bayesian Last Layer Models Using Implicit Priors and Diffusion Posterior Sampling", "authors": ["Jian Xu", "Zhiqi Lin", "Shigui Li", "Min Chen", "Junmei Yang", "Delu Zeng", "John Paisley"], "abstract": "Bayesian Last Layer (BLL) models focus solely on uncertainty in the output layer of neural networks, demonstrating comparable performance to more complex Bayesian models. However, the use of Gaussian priors for last layer weights in Bayesian Last Layer (BLL) models limits their expressive capacity when faced with non-Gaussian, outlier-rich, or high-dimensional datasets. To address this shortfall, we introduce a novel approach that combines diffusion techniques and implicit priors for variational learning of Bayesian last layer weights. This method leverages implicit distributions for modeling weight priors in BLL, coupled with diffusion samplers for approximating true posterior predictions, thereby establishing a comprehensive Bayesian prior and posterior estimation strategy. By delivering an explicit and computationally efficient variational lower bound, our method aims to augment the expressive abilities of BLL models, enhancing model accuracy, calibration, and out-of-distribution detection proficiency. Through detailed exploration and experimental validation, We showcase the method's potential for improving predictive accuracy and uncertainty quantification while ensuring computational efficiency.", "sections": [{"title": "Introduction", "content": "Bayesian Last Layer (BLL) models (Watson et al. 2021; Harrison, Sharma, and Pavone 2020; Kristiadi, Hein, and Hennig 2020; Harrison, Willes, and Snoek 2023; Fiedler and Lucia 2023) have emerged as a robust framework for uncertainty quantification in neural networks, concentrating on the uncertainty inherent in the final layer weights. However, these models often utilize Gaussian priors for the weight distributions, which may be insufficient for capturing the complexity of non-Gaussian, outlier-prone, or high-dimensional data. This constraint can limit the expressiveness of BLL models and adversely affect their performance in more challenging scenarios.\nPrior research highlights the critical need to enhance model flexibility through more adaptable priors. For instance, (Fortuin et al. 2021) demonstrated that isotropic Gaussian priors may inadequately represent the true distribution of weights in Bayesian neural networks, potentially compromising performance. They observed significant spatial correlations in convolutional neural networks and ResNets, as well as heavy-tailed distributions in fully connected networks, suggesting that priors designed with these observations in mind can improve performance on image classification tasks. (Fortuin 2022) underscore the importance of prior selection in Bayesian deep learning, exploring various priors for deep Gaussian processes (Damianou and Lawrence 2013), variational autoencoders (Doersch 2016), and Bayesian neural networks (Kononenko 1989), while also discussing methods for learning priors from data. Their work encourages practitioners to carefully consider prior specification and provides inspiration for this aspect.\nDriven by the need for enhanced model flexibility and performance, we propose an innovative approach that leverages implicit priors for variational learning of Bayesian last layer weights. Implicit priors are parameterized through a neural network, replacing traditional Gaussian weight distributions to achieve greater flexibility. This method connects to the migration of variational implicit processes (Ma, Li, and Hern\u00e1ndez-Lobato 2019) into the BLL model, offering novel insights and opportunities. By employing implicit distributions for weight priors, our approach aims to establish a robust strategy for Bayesian prior estimation. However, as model complexity increases, inference becomes more challenging, posing new obstacles.\nTo address this, we shift to directly utilizing diffusion models (Ho, Jain, and Abbeel 2020; Rombach et al. 2022; Vargas, Grathwohl, and Doucet 2023) for posterior sampling. This approach enables us to effectively capture complex dependencies and correlations among latent variables. By utilizing diffusion stochastic differential equations (SDEs) and incorporating elements similar to score matching (Song et al. 2020), we formulate a novel variational lower bound for the marginal likelihood function through KL divergence minimization.\nAdditional, we delve into the details of our proposed method and demonstrate its potential through extensive experimental validation. By introducing a computationally efficient variational lower bound and showcasing its efficacy in scenarios with non-Gaussian distributions, outliers, and high-dimensional data, we highlight the significance of our approach in advancing uncertainty quantification in neural networks. Overall, our contributions are as follows:\n\u2022 We proposed an innovative approach that utilizes implicit priors for variational learning of Bayesian last layer weights. This method replaces traditional Gaussian weight parameters with prior distributions parameterized"}, {"title": "Background", "content": "BLL models\nIn the context of Bayesian Last Layer (BLL) networks, these models can be understood as Bayesian linear regression frameworks applied within a feature space that is adaptively learned through neural network architectures. Another perspective is to view them as neural networks where the parameters of the final layer undergo rigorous Bayesian inference. While BLL networks are capable of handling multivariate regression tasks, this discussion will be confined to the case of univariate targets for the sake of simplicity.\nLet the dataset \\(D = \\{(x_i, y_i)\\}_{i=1}^N\\) consist of observed data, where \\(x_i \\in \\mathbb{R}^D\\) and \\(y_i \\in \\mathbb{R}\\). We denote \\(X \\in \\mathbb{R}^{N \\times D}\\) and \\(y \\in \\mathbb{R}^N\\). We define a parameterized function \\(\\phi(\\cdot; \\theta) : \\mathbb{R}^D \\rightarrow \\mathbb{R}^M\\) for projecting features, with \\(\\theta\\) representing its parameters, and \\(\\phi_i = \\phi(x_i; \\theta)\\) and \\(\\Phi = [\\phi_1^T, ..., \\phi_N^T]^T \\in \\mathbb{R}^{N \\times M}\\), where the latter denotes a matrix of stacked row vectors.\nA latent function \\(f\\) is modeled utilizing Bayesian linear regression (Bishop 2006; Hill 1974; Murphy 2023), incorporating weights \\(\\beta \\in \\mathbb{R}^M\\) and zero-mean Gaussian noise \\(\\epsilon\\) with variance \\(\\sigma^2\\), where\n\\[Yi = f (xi; 0) = \u03c6\u03b2 + \u03b5\u00a1. \\]  (1)\nBy imposing a conjugate Gaussian prior \\(N(\\mu_0, \\Lambda_0^{-1})\\) over \\(\\beta\\), a Gaussian posterior \\(N(\\mu_n, \\Lambda_n^{-1})\\) is obtained (Watson et al. 2021; Harrison, Willes, and Snoek 2023),\n\\[\\mu_n = (\\Lambda_0 + \\sigma^{-2}\\Phi^T\\Phi)^{-1}(\\Lambda_0\\mu_0 + \\sigma^{-2}y) \\\\\n\\Lambda_n = \\Lambda_0 + \\sigma^{-2}\\Phi^T\\Phi, \\] (2)\nyielding an explicit Gaussian predictive distribution for a given query x,\n\\[y | x, D, 0 \\sim N(\\cdot | \\Phi_x^T\\mu_n, \\sigma^2 + \\Phi_x^T\\Lambda_n^{-1}\\Phi_x), \\] (3)\nwhere \\(\\mu_n\\) and \\(\\Lambda_n\\) denote the mean vector and precision matrix of the posterior weight distribution, respectively.\nThe parameters encompassing the observation noise \\(\\sigma^2\\), the prior weight parameters \\(\\mu_0\\) and \\(\\Lambda_0\\), and \\(\\theta\\), can be optimized jointly through the maximization of the log-marginal likelihood. In the scenario where \\(\\mu_0 = 0\\), this model aligns equivalently with a Gaussian process, with a kernel"}, {"title": "Variational Bayesian Last Layer", "content": "To leverage exact marginalization while avoiding the computational burden of full marginal likelihood computation, Variational Bayesian Last Layer (VBLL) (Harrison, Willes, and Snoek 2023) employs stochastic variational inference (Hoffman et al. 2013). The objective is to jointly compute an approximate posterior for the last layer parameters and optimize network weights by maximizing lower bounds on the marginal likelihood. Specifically, VBLL aims to find an approximate posterior \\(q(\\beta|\\eta)\\) parameterized by \\(\\eta\\).\nGiven a mini-batch \\(D_I\\) with \\(|X_I| = B\\), where \\(I \\subset \\{1, 2, ..., N\\}\\) is the set of indices for any mini-batch, and the log marginal likelihood \\(\\log p(y|x, \\theta)\\) with marginalized parameters \\(q(\\beta|\\eta) = N(w, S)\\), they derive bounds of the form:\n\\[\\frac{N}{B} \\log p(y_I | x_I, \\theta) \\geq \\frac{N}{B} [\\sum_{i=1}^{B} \\log N(y_i | \\phi_i^T w, \\sigma^2 I) - \\frac{N}{2 \\sigma^2} \\phi_i^T S \\phi_i] - KL(q(\\beta|\\eta)||p(\\beta)),\\] (4)\nThis formulation results in a mini-batch algorithm for variational Bayesian inference in neural networks."}, {"title": "Flexible Priors in Bayesian Neural Networks", "content": "In prior research, there has been a strong emphasis on the importance of enhancing model flexibility through more flexible priors, particularly in the context of Bayesian neural networks. Previous studies (Fortuin et al. 2021; Fortuin 2022) have highlighted that applying isotropic Gaussian priors to weights may not fully capture true beliefs about weight distributions, thus hindering optimal performance, as described in Section\nThese studies provide valuable insights suggesting that introducing more flexible weight priors in Bayesian linear regression models could enhance model flexibility and performance. This flexibility may involve designing prior distributions based on specific task or data characteristics to better capture the true data distribution features, thereby improving model generalization and performance."}, {"title": "Method", "content": "Implict Prior\nIn continuation of techniques for generating implicit processes from prior research on stochastic processes (Ma, Li, and Hern\u00e1ndez-Lobato 2019; Santana, Zaldivar, and Hernandez-Lobato 2021; Ma and Hern\u00e1ndez-Lobato 2021), we introduce a regression model with an implicit prior over the weights \u03b2:\n\\[Yi = f (xi; 0) = \u03c6\u03b2 + \u03b5\u1f30, \\\\\n\u03b2 = G4 (\u03c9), \u03c9 ~ \u03c1 (\u03c9),\\]  (5)"}, {"title": "Diffusion Posterior Sampling", "content": "Parameterizing Auxiliary Variable Posteriors Using Time-Reversal Representation of Diffusion SDE Our goal is to sample from the posterior distribution of auxiliary variable q(w), defined by Bayes' rule as \\(q(w) = \\frac{p(y|w)p(\u03c9)}{p(y)}\\), where p(w) represents the prior and p(y) represents the model marginal likelihood. Following similar setups in prior works (Tzen and Raginsky 2019; Zhang and Chen 2021; Vargas, Grathwohl, and Doucet 2023), we begin by sampling from a Gaussian distribution \\(N(0, \\sigma_0 I)\\), where \\(\\sigma_0 \\in \\mathbb{R}^+\\) is the covariance parameter. We then follow a time-reversal process of the forward diffusion stochastic differential equation (SDE):\n\\[dt = -x(t)+dt + g(t)dBt, 30 ~q, t\u2208 [0,T], \\] (6)\nwhere \u2013 \\(X(t) \\in \\mathbb{R}\\) is the drift coefficient, g(t) \u2208 R is the diffusion coefficient, and \\((Bt)t\u2208[0,T]\\) is a K-dimensional Brownian motion. This diffusion induces the path measure P on the time interval [0,T], and the marginal density of \\(w_t\\) is denoted pt. Note that by definition, we always have \\(p_0 = q\\) when using an SDE to perturb this distribution. According to (Anderson 1982; Haussmann and Pardoux 1986), the time-reversal representation of Eq. (6) is given by \\(wt = w_{T-t}\\) (where equality is in distribution). This satisfies:\n\\[-dt = (-x(T - t)\u0ed4t + g(T \u2013 t)\u00b2\u2207 ln pr-t (t)) dt + g(T - t)dWt, o~ PT, \\] (7)\nwhere \\((Wt)t\u2208 [0,T]\\) is another K-dimensional Brownian motion. In DDPM (Ho, Jain, and Abbeel 2020; Song et al. 2020), this time-reversal starts from \\(o ~ p_T \u2248 N(0, \u03c3_0^2I)\\) and ensures that \\(w_T  ~ q\\). Then we can parameterize the transition probability \\(T (w_{ts+1} | w_{ts})\\) in the Euler discretized form (S\u00e4rkk\u00e4 and Solin 2019) of Eq. (7) for steps ts \u2208 {0,..., T \u2013 1}."}, {"title": "Score Matching and Reference Process Trick", "content": "If we could approximately simulate the diffusion process described in (7), we would obtain approximate samples from the target distribution q. However, implementing this idea requires approximating the intractable scores (\u2207ln pt()) t\u2208 [0,1]. To achieve this, DDPM (Ho, Jain, and Abbeel 2020; Song et al. 2020) relies on score matching techniques. Specifically, to approximate P, we consider a path measure \\(P^Y\\) whose time-reversal is defined by\n\\[S\\\\\ndt = (-x(T \u2013 t) + g(T \u2013 t)\u00b2sy (T \u2013 t, \u0ed4)) dt \\\\\n+ g(T - t)dWt, & ~ N(0,03I), \\] (8)\nso that the backward process  ~ Q7. To obtain \\(sy(t,) \u2248 \u2207 ln pt(\\cdot)\\), we parameterize s(t) using a neural network, with the parameters obtained by minimizing KL(P||PY). Unlike traditional score matching techniques, given that we can only obtain samples from Q7, we alternatively minimize KL(PY||P), by Girsanov's theorem (Oksendal 2013)\n\\[KL(PY||P) = KL(QV||Q) \\\\\n=KL(N(0, \u03c331)||pT) + KL(Q(\\cdot|)\\Q(|)) \\\\\n\\frac{1}{2} \\int_0^T EQ7 [g(T \u2013 t)2 ||\u2207 In pr-t(7) \u2013 sy(T \u2013 t, 7)||2]dt\\]  (9)\nHowever, although we can obtain samples from Q7 by simulating the SDE (8), dealing with the nonlinear drift function of SDE (8) makes it difficult to obtain \u2207 ln pT-t(7) in Eq. (9).\nWe use an alternative approach by constructing a reference process (Zhang and Chen 2021; Vargas, Grathwohl, and Doucet 2023), denoted as Pref, to assist in measuring KL(PY||P). Firstly, observe the following fact:\n\\[KL(PY||P) = Epy log \\frac{dPr}{dP} \\\\\n= Epy log \\frac{dPr}{dPref} + Epy log \\frac{dpref}{dP}\\] (10)\nwhere the stochastic process KL is represented as the Radon-Nikodym derivative. Given the specific form in Eq. (10), we define the reference process Pref to follow the diffusion formula as in Eq. (6), but initialized at \\(p_0^{ref}(\u03c9^{ef}) = N(0, \u03c3_0^2)\\) instead of q, which aligns with the distribution of \\(w_T\\) in Eq. (8),\n\\[dref = x(t) refdt + g(t)dBt, ef ~ N(0,021). \\] (11)\nThe transition kernel \\(pt(x^{ef}|\u03c9_t^{ef})\\) is always a Gaussian distribution N(lt, \u2211t), where the mean lt and variance Et are often available in closed form (S\u00e4rkk\u00e4 and Solin 2019):\n\\[dlt = -x(t)lt, lo = 0, \\\\\ndt = -2x(t)Et + g(t)2I, \u03a3\u03bf = \u03c3\u0399.\\] (12)\nBy solving these ordinary differential equations (Hale and"}, {"title": "Evidence Lower Bound", "content": "Let 11(7) = \\(E_{p_Y} \\log \\frac{dPr}{dPref}\\). Combining Eq. (5, 10, 17, 18), we obtain a new variational lower bound 1(\u03b3, \u03b8, \u03c8) for the marginal likelihood log p(y) in our method,\n\\[logp(y) \\\\\n= KL(PY||P) \u2013 11(y) \u2013 Egz log \\frac{\u039d(0, \u03c3\u03be\u0399)}{p(y)p()} \\\\\n= KL(PY||P) \u2013 11(7) \u2013 E27 log N(0, \u03c331) + E27 log p() \\\\\n+ Eqz log p(y | x, 0, \u03c8, \u00b7) \\\\\n\u2265 \u221211(y) \u2013 Eg7 log N(0, oI) + Egz log p() \\\\\n+ Eqz log p(y | x, 0, \u03c8,\u00b7) \\\\\n= 1(\u03b3, \u03b8, \u03c8)\\] (19)\nIn our derivation, p(\u00b7) represents the prior function of w. We introduce a new variational lower bound for log p(y). Unlike the mean-field variational inference model that approximates q with a Gaussian distribution, our model uses a diffusion process to approximate the posterior distribution. The flexibility of the denoising neural network y provides our model with a significant advantage in accurately approximating the posterior distribution."}, {"title": "Stochastic Gradient Descent", "content": "For ease of sampling, we consider a reparameterization version of Eq. (19) based on the approaximate transition probability \\(T\u03b3 (\u03c9_{ts+1} | \u03c9_{ts})\\) given by\n\\[\u03a4(\u03c9t+1) = wt \u2013 X(T - ts)wts + g(T - ts)\u00b2sy (T - ts, wt\uff61) + g(T - t)ets.  (20)\nwhere \\(Ets  ~ N(0, I)\\). In order to accelerate training and sampling in our inference scheme, we propose a scalable variational bounds that are tractable in the large data regime based on stochastic variational inference (Kingma and Welling 2013; Hoffman and Blei 2015; Salimbeni and Deisenroth 2017; Naesseth, Lindsten, and Blei 2020) and stochastic gradient descent (Welling and Teh 2011; Chen, Fox, and Guestrin 2014; Zou, Xu, and Gu 2019; Alexos, Boyd, and Mandt 2022). Our model is shown in Algorithm 1, referred to as DVI-IBLL."}, {"title": "Prediction Distribution", "content": "For making predictions in our model, the prediction under the variational posterior distribution is approximated for a test input/label (x*, y*) as:\n\\[p(y* | x*, X, y) \u2248 Eq7 [p(y* | x*, 0,\u03c8,\u03c9)]\\] (21)\nHere, 27 denotes the output of the diffusion process at time T. The expression p(y* | x*, 0, \u03c8, \u03c9) can be obtained by substituting the input/output with the test set input/label from Eq. (5). For classification tasks or other likelihood functions, the substitution can be made accordingly during training."}, {"title": "Related Work", "content": "Bayesian Last Layers (BLL) Models\nBayesian Last Layers (BLL) models are a class of methods that enhance neural network performance by incorporating Bayesian principles into the final layers of the network. The primary advantage of BLL models lies in their ability to efficiently balance exploration and exploitation. Early work by (Box and Tiao 2011) integrated Bayesian layers with deep neural networks to improve robustness and generalization. Recent advances have further refined these approaches. For instance, (Weber et al. 2018) explored training neural networks online in a bandit setting to optimize the balance between exploration and exploitation. Additionally, (Watson et al. 2021) introduced a functional prior on the model's derivatives with respect to the inputs, enhancing predictive uncertainty. (Harrison, Willes, and Snoek 2023) applied variational inference to train Bayesian last layer neural networks, improving the estimation of posterior distributions. Moreover, (Fiedler and Lucia 2023) addressed computational challenges in the log marginal likelihood by reintroducing the weights of the last layer, avoiding the need for matrix inversion.\nImplicit Prior Models\nImplicit prior models (Hoffman, Riquelme, and Johnson 2017; Ma, Li, and Hern\u00e1ndez-Lobato 2019) in Bayesian inference refer to models where the prior distribution is not explicitly specified but instead learned through neural networks. These models are gaining traction due to their flexibility and ability to capture complex data distributions. Notably, (Ma, Li, and Hern\u00e1ndez-Lobato 2019) proposed highly flexible implicit priors over functions, exemplified by data simulators, Bayesian neural networks, and non-linear transformations of stochastic processes. (Takahashi et al. 2019) introduced the VAE with implicit optimal priors to address the challenges of hyperparameter tuning for the aggregated posterior model. Recent advancements (Kumar and Poole 2020) have focused on enhancing the efficiency and accuracy of these methods by integrating regularization techniques.\nDiffusion Models\nDiffusion models (Ho, Jain, and Abbeel 2020; Song et al. 2020) have emerged as powerful tools for modeling complex distributions and generating high-quality samples. These models simulate a diffusion process in which a simple distribution is gradually transformed into a more complex target distribution. The incorporation of stochastic differential equations (SDEs) (Song et al. 2020) has further enhanced their capacity to model continuous dynamical systems and capture intricate data patterns. Recent research (Vargas, Grathwohl, and Doucet 2023; Richter and Berner 2023; Piriyakulkij, Wang, and Kuleshov 2024) has explored"}, {"title": "Experiments", "content": "Metrics and Baselines\nIn our regression experiments, we present the predictive negative log likelihood (NLL) for test data, which can be straightforwardly calculated for point feature estimates. Additionally, we include the root mean squared error (RMSE), a widely used metric in regression analysis. For classification tasks, apart from the negative log likelihood, we also showcase the predictive accuracy (based on the standard argmax of the predictive distribution) and the expected calibration error (ECE), which assesses the alignment between the model's subjective predictive uncertainty and actual predictive error. Furthermore, we delve into evaluating out-of-distribution detection performance, a recognized assessment approach for robust and probabilistic machine learning (Liu et al. 2023). Specifically, we measure the area under the ROC curve (AUC) for datasets near and far from the distribution, which will be elaborated on further in this section.\nIn the realm of regression, we contrast our model with approaches that leverage exact conjugacy, including Bayesian last layer models such as GBLL and LDGBLL (Watson et al. 2021), as well as VBLL (Harrison, Willes, and Snoek 2023) and RBF kernel Gaussian processes. We also juxtapose our model with MAP learning (Snoek et al. 2015), which involves training a complete network using MAP estimation and subsequently fitting a Bayesian last layer to these fixed features.\nIn the domain of classification, our primary point of comparison is standard deep neural networks (DNN), given their ability to output a distribution over labels for direct comparison with our model. Additionally, we compare our model with SNGP (Liu et al. 2023) and last layer Laplace-based methods (Daxberger et al. 2021), which are akin to last layer models aiming to approximate deep kernel GPs (Wilson et al. 2016) and computing an approximate posterior after training, respectively. Note that Laplace methods are not assessed in regression due to their similarity to the MAP model.\nFurthermore, we examine various variational methods such as Bayes-by-Backprop (BBB) (Blundell et al. 2015), Ensembles (Lakshminarayanan, Pritzel, and Blundell 2017), Bayesian Dropout (Gal and Ghahramani 2016), and Stochastic Weight Averaging-Gaussian (SWAG) (Maddox et al. 2019) for a comprehensive evaluation of performance. All our experiments were conducted on an RTX 4090 GPU.\nRegression\nOur UCI experiments closely align with the methods outlined in (Watson et al. 2021; Harrison, Willes, and Snoek 2023), enabling a direct comparison with their baseline models. Consistently, we employed the same MLP architecture as described in (Watson et al. 2021), comprising two layers of 50 hidden units each. Maintaining consistency with (Watson et al. 2021), a batch size of 32 was utilized for all datasets, except for the Power dataset where a batch size of 256 was chosen to expedite training. Standard preprocessing techniques were applied, including normalization of inputs and subtraction of training set means from the outputs. The reported results in our manuscript exhibit the outcomes under leaky ReLU activations, with optimization performed using the AdamW optimizer (Loshchilov and Hutter 2017) across all experiments.\nIn deterministic feature experiments, we conducted 20 runs with varying seeds. Each run involved splitting the data into training, validation, and testing sets with ratios of 0.72, 0.18, and 0.1, respectively. Training was executed on the training set, while performance monitoring on the validation set was carried out to determine the optimal number of epochs for model convergence. Our study delves into the performance analysis of regression VBLL models across 6 UCI regression datasets"}, {"title": "Conclusion", "content": "In summary, our novel approach combining diffusion techniques and implicit priors for variational learning of Bayesian last layer weights enhances the expressive capacity of Bayesian Last Layer (BLL) models. This advancement addresses limitations with Gaussian priors and boosts model accuracy, calibration, and out-of-distribution detection proficiency in scenarios with non-Gaussian distributions, outliers, or high-dimensional data. Our method demonstrates potential for improving predictive accuracy and uncertainty quantification in challenging settings, promising enhanced performance of BLL models in machine learning tasks."}, {"title": "Reproducibility Checklist", "content": "This paper:\nIncludes a conceptual outline and/or pseudocode description of AI methods introduced (yes)\nClearly delineates statements that are opinions, hypothesis, and speculation from objective facts and results (yes)\nProvides well marked pedagogical references for less-familiare readers to gain background necessary to replicate the paper (yes)\nDoes this paper make theoretical contributions? (yes)\nIf yes, please complete the list below.\nAll assumptions and restrictions are stated clearly and formally. (yes)\nAll novel claims are stated formally (e.g., in theorem statements). (yes)\nProofs of all novel claims are included. (yes)\nProof sketches or intuitions are given for complex and/or novel results. (yes)\nAppropriate citations to theoretical tools used are given. (yes)\nAll theoretical claims are demonstrated empirically to hold. (yes)\nAll experimental code used to eliminate or disprove claims is included. (yes)\nDoes this paper rely on one or more datasets? (yes)\nIf yes, please complete the list below.\nA motivation is given for why the experiments are conducted on the selected datasets (yes)\nAll novel datasets introduced in this paper are included in a data appendix. (NA)\nAll novel datasets introduced in this paper will be made publicly available upon publication of the paper with a license that allows free usage for research purposes. (NA)\nAll datasets drawn from the existing literature (potentially including authors' own previously published work) are accompanied by appropriate citations. (yes)\nAll datasets drawn from the existing literature (potentially including authors' own previously published work) are publicly available. (yes)\nAll datasets that are not publicly available are described in detail, with explanation why publicly available alternatives are not scientifically satisficing. (NA)\nDoes this paper include computational experiments? (yes)\nIf yes, please complete the list below.\nAny code required for pre-processing data is included in the appendix. (yes).\nAll source code required for conducting and analyzing the experiments is included in a code appendix. (yes)\nAll source code required for conducting and analyzing the experiments will be made publicly available upon publication of the paper with a license that allows free usage for research purposes. (yes)\nAll source code implementing new methods have comments detailing the implementation, with references to the paper where each step comes from (yes)\nIf an algorithm depends on randomness, then the method used for setting seeds is described in a way sufficient to allow replication of results. (yes)\nThis paper specifies the computing infrastructure used for running experiments (hardware and software), including GPU/CPU models; amount of memory; operating system; names and versions of relevant software libraries and frameworks. (yes)\nThis paper formally describes evaluation metrics used and explains the motivation for choosing these metrics. (yes)\nThis paper states the number of algorithm runs used to compute each reported result. (yes)\nAnalysis of experiments goes beyond single-dimensional summaries of performance (e.g., average; median) to include measures of variation, confidence, or other distributional information. (yes)\nThe significance of any improvement or decrease in performance is judged using appropriate statistical tests (e.g., Wilcoxon signed-rank). (yes)\nThis paper lists all final (hyper-)parameters used for each model/algorithm in the paper's experiments. (yes)\nThis paper states the number and range of values tried per (hyper-) parameter during development of the paper, along with the criterion used for selecting the final parameter setting. (yes)"}, {"title": "Algorithm 1: Diffuison Variational Inference algorithm for implict prior BLLs (DVI-IBLL)", "content": "Input: training data x, y mini-batch size B\nInitialize diffusion coefficient h, g, all BLL hyperparameters 0, \u03c8, denoising diffusion network parameters \u03b3\nSet lo = 0\nrepeat\nfor t = 0 to T - 1 do\nDraw ets from standard Gaussian distribution.\nSet wt+1 = Wts -(T - ts)wt + g(T - ts)\u00b2sy (T \u2212 ts,wt\uff61) + g(T \u2013 t)ets\nCompute KT-(t+1) by Eq. (14)\nSet lt+1 = lts + g(T - (ts + 1))2 || Wts+1 + sy(T \u2212 (ts + 1), wts+1)||2\nKT-(ts+1)\nend for\nSample mini-batch indices I C {1, ..., N} with |I| = B\nSet 1(\u03b3, \u03b8, \u03c8) = 1/2 (w) + B log \u03c3\u03bf + log p (wr) +logp(y1 | x1,0,\u03c8,\u03c9\u03c4) \u2013 KL (N\u0384(0, \u03c3\u03be\u0399) || N(0, \u043a\u0442)) \u2013 1/2l\nDo gradient descent on 1(y, 0, \u03c8)\nuntil \u03b3, \u03b8, 4 converge"}]}