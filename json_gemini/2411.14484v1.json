{"title": "Robust Planning with Compound LLM Architectures: An LLM-Modulo Approach", "authors": ["Atharva Gundawar", "Karthik Valmeekam", "Mudit Verma", "Subbarao Kambhampati"], "abstract": "Previous work has attempted to boost Large Language Model (LLM) performance on planning and scheduling tasks through a variety of prompt engineering techniques. While these methods can work within the distributions tested, they are neither robust nor predictable. This limitation can be addressed through compound LLM architectures where LLMs work in conjunction with other components to ensure reliability. In this paper, we present a technical evaluation of a compound LLM architecture- the LLM-Modulo framework. In this framework, an LLM is paired with a complete set of sound verifiers that validate its output, re-prompting it if it fails. This approach ensures that the system can never output any fallacious output, and therefore that every output generated is guaranteed correct-something previous techniques have not been able to claim. Our results, evaluated across four scheduling domains, demonstrate significant performance gains with the LLM-Modulo framework using various models. Additionally, we explore modifications to the base configuration of the framework and assess their impact on overall system performance.", "sections": [{"title": "Introduction", "content": "As Large Language Models (LLMs) are deployed in contexts far beyond the traditional text completion tasks they were initially trained for, it becomes ever more important to guarantee the correctness of their outputs. Since their release, interest has surged in evaluating LLM capabilities on planning and reasoning tasks, realms traditionally reserved for System 2 cognitive competencies (Kahneman, 2011). Despite their apparent versatility, studies show that LLMs are not capable of robust reasoning or planning (Valmeekam et al., 2022, 2023; Silver et al., 2022) and the research community is still exploring effective strategies to leverage these models for solving complex planning and reasoning tasks. Even recent advances in prompt engineering that claim performance improvements (Wei et al., 2022; Yao et al., 2022) have been shown to not robustly generalize (Verma et al., 2024; Stechly et al., 2024).\nAlthough these studies present evidence that LLMs do not possess System 2 capabilities themselves, their uncanny ability to generate ideas across a wide spectrum of domains-albeit guarantees-can be leveraged to solve a wide variety of System 2 tasks. LLMs can be seen as candidate solution generators within a compound architecture (Zaharia et al., 2024) where other components can vet the solutions and thus ensure that the solutions outputted by the overall system have correctness guarantees where possible.\nOne such compound architecture is the LLM-Modulo Framework (Kambhampati et al., 2024). It proposes a conceptual framework whereby an LLM is augmented with a suite of external verifiers and other components which evaluate its proposed answers before deciding whether they should be output. Although the original introduction of the framework (Kambhampati et al., 2024) offered a promising conceptual foundation, it was a position paper and thus lacked a compelling empirical evaluation to demonstrate the framework's viability.\nIn this paper, we set out to perform a thorough analysis of the effectiveness of the LLM-Modulo framework in solving complex real-world scheduling tasks. We evaluate the framework on four domains drawn from two benchmarks; Travel Planner (Xie et al., 2024) and three domains from Natural Plan (Zheng et al., 2024)\u2013Trip Planning, Meeting Planning and Calendar Scheduling. Although these domains are more accurately described as scheduling problems, they still fall within the broader scope of planning, with scheduling being a specialized subset of planning. Scheduling problems"}, {"title": "Related Work", "content": "Large Language Models (LLMs) have garnered significant attention due to their versatility in generating seemingly coherent completions across various domains. As a result, researchers have explored using these monolithic models for planning and scheduling tasks, with some even claiming emergent planning capabilities (Huang et al., 2022; Bairi et al., 2024; Yao et al., 2022; Shinn et al., 2024; Song et al., 2023). However, recent systematic investigations challenged these claims by showing that LLMs are not able to robustly generate executable plans by themselves (Valmeekam et al., 2024; Liu et al., 2023; Silver et al., 2022). Furthermore, recent prompt engineering techniques like Chain of thought (Wei et al., 2022) or ReAct (Yao et al., 2022), that claimed performance gains, have also been shown to lack robust generalization within these tasks (Stechly et al., 2024; Dziri et al., 2024; Verma et al., 2024).\nThese findings have shifted the focus from relying solely on these monolithic models to compound architectures, where LLMs play key roles alongside other components (Zaharia et al., 2024; Valmeekam et al., 2023; Kambhampati et al., 2024; Trinh et al., 2024; Gestrin et al., 2024; Liu et al., 2023). The LLM-Modulo framework (Kambhampati et al., 2024) introduces a compound LLM architecture where LLMs serve in various constructive roles, such as generating candidate solutions while other components verify these proposed solutions for correctness.\nOther compound LLM architectures, in addition to LLM-Modulo, include solvers that work in conjunction with LLMs rather than verifiers (Trinh et al., 2024; Gestrin et al., 2024; Liu et al., 2023). However, these approaches are constrained by the expressiveness and computational limitations of the solvers. Additional approaches modify LLM prompts by including relevant information or filtering out unsafe or erroneous content, thus aiding the generation of better solutions (Lewis et al., 2020; Han et al., 2024; Xu et al., 2024). Pipelines that optimize prompts relative to specific target metrics have also been developed to maximize system performance (Khattab et al., 2023). However, these systems continue to face robustness challenges as they still rely on the LLM's generations, which"}, {"title": "LLM-Modulo Framework", "content": "The underlying architecture of the LLM-Modulo Framework is that of a generate-test-critique loop, with the LLM proposing solutions and a bank of critics critiquing the proposals. The soundness guarantees of the system stem from having sound critics as part of the loop. The completeness of the system depends on the generator i.e., the LLM's ability to generate all potentially relevant candidates."}, {"title": "Problem Specification", "content": "As shown in Figure 1, the interaction loop starts by providing the problem specification to the LLM. A complete problem specification can be provided by the user or LLMs can be used in conjunction to flesh out an incomplete problem specification"}, {"title": "Backprompt (Meta) Controller", "content": "The metacontroller takes on the control flow if the critics find issues with the solution. It contains the decision-making logic to stitch together the critic responses, choose which backprompts to allow for (if a certain pedagogical prompting is in effect), or other consolidation of various backprompts. The metacontroller interfaces with the LLM and includes other information such as instructions, database context, formatting, and few-shot examples along with the compiled backprompt.\nAs shown in Figure 1, the meta-controller first consolidates the format backprompts, if any, and queries the LLM to address the format issues. If no formatting issues are detected, the LLM response is sent to the other critics. If those critics find errors, the meta-controller aggregates the response of those critics and generates a single backprompt for the LLM. This backprompt includes the original prompt, the LLM's response from the previous iteration, and the response provided by the bank of critics. This interaction loop continues until all of the critics agree to the generated solution or until a"}, {"title": "Evaluation", "content": "In this section, we present the empirical results of evaluating the LLM Modulo framework on four specific scheduling domains: Travel Planner, Trip Planning, Meeting Planning, and Calendar Scheduling. Our experiments utilize three models: GPT-4o-mini, GPT-4o, and Claude-3.5-Sonnet. We use temperature 0 across all our experiments."}, {"title": "Travel Planner", "content": "Travel Planner provides a sandboxed environment to generate itineraries for travel plan queries simulated using various datasets, such as flights, restaurants, distances between locations, available transport options, and accommodation choices, to name a few. As mentioned previously, Travel Planner evaluates generated solutions based on hard constraints and common-sense constraints. We use the validation dataset (within the sole-planning mode) consisting of 180 queries for our experiments. The prompts within the sole-planning mode provide necessary context to the LLM instead of expecting the LLM to perform tool use (Schick et al., 2024; Paranjape et al., 2023; Hsieh et al., 2023). The query is combined with a system prompt that specifies relevant information, including details about available accommodations, restaurants, attractions, flights, and road transportation options. We prompt the LLM to generate a travel plan in JSON format, which includes, for each day of the trip, the day number, number of travelers, current city, transportation method, and recommendations for breakfast, lunch, dinner, and attractions.\nInitially, every LLM-generated response passes through a format critic that validates the JSON structure, ensuring it is parsable and contains all required key-value pairs. Only if the response meets"}, {"title": "Trip Planning", "content": "The Trip Planning domain comprises 1,600 scenarios, each requiring the formulation of a travel itinerary that adheres to specified constraints. The complexity of the scenarios in this dataset is determined by the number of cities mentioned in the query, which can vary from three to ten. This categorization divides the dataset into eight subsets, each containing 200 instances.\nFor the first iteration of LLM Modulo, we use the same prompt provided by the authors of Natural Plan, as mentioned earlier. The LLM is instructed to return a regex-parsable response, and we provide 5 examples to form a 5-shot prompt. Similar to the Travel Planner domain, the LLM's response is first sent to a format critic, which checks whether the response is correctly parsed. If the format is valid, the response is then passed to three additional critics. The first critic verifies that the duration of the stay in each city matches the specified constraints. The second critic checks whether the required flights are available based on the planned itinerary. The third critic ensures that for each event, the traveler is in the correct city during the event's duration.\nThe results, as illustrated in Table 1, show significant improvements with LLM-Modulo across models. While GPT-4o-mini improves from 6.18% to 12.06% and Claude-3.5-Sonnet improves from 39.43% to 47%, with GPT-4o we see a significant jump from 3.43% to 40%."}, {"title": "Meeting Planning", "content": "Meeting planning involves 1,000 scenarios, where the objective is to maximize the number of friends met within given time and location constraints, using a distance matrix. The dataset is divided into ten subsets, each consisting of 100 instances, cat-"}, {"title": "Calendar Scheduling", "content": "The Calendar Scheduling domain encompasses 1,000 scenarios that challenge planners to identify suitable meeting times ranging from 30 minutes to one hour, considering the schedules of n individuals over m days. The complexity of scenarios is derived from the number of participants (n) and the range of days (m). The dataset is divided into two subsets: one varying the number of participants from three to seven across a single day, and the other varying the days from one to five with two participants. For the first iteration of LLM Modulo, we use the same prompt provided by the authors of Natural Plan, as previously mentioned. The LLM is instructed to generate a regex-parsable response, and we provide five examples, creating a 5-shot prompt. Similar to the previous domains, after ensuring that the format is valid, the response is then passed to the only critic which looks if the generated response meeting time clashes with any attendee's schedule.\nSimilar to the other domains, LLM Modulo framework has enhanced performance within the Calendar Scheduling dataset. Each model tested in this domain achieved a significant boost in performance; smaller models like GPT-4o-mini increased from the original 36.9% to 61.6%! GPT-4o and"}, {"title": "Performance on the hardest subsets", "content": "The LLM Modulo approach not only enhances overall performance across all domains but also demonstrates improvements in handling instances of varying complexity within each domain, including the hardest subsets (see Figure 2). These hardest subsets are defined by scenarios with the highest"}, {"title": "Exploring Modifications within the LLM-Modulo Framework", "content": "Previously, in our evaluations, we focused on a basic configuration of the LLM-Modulo framework. The LLM proposes a solution, a panel of critics evaluate it, if the critics find any errors, the meta-"}, {"title": "Adding context from previous iterations", "content": "We investigate the impact of incorporating context from previous generations by including previously generated (incorrect) plans and the feedback on them. To analyze this, we performed two ablations: (1) including only the incorrect plans without their feedback, and (2) including only the unique incorrect plans to avoid redundancy. These experiments were conducted in the calendar scheduling domain using the GPT-4o-mini model, with varying values of n, representing the number of previously incorrect plans included as context. The context contained the most recent n incorrect plans, and each configuration was evaluated over 50 iterations of LLM-Modulo.\nOur findings indicate that increasing the number"}, {"title": "Filtering", "content": "When constraint values are specified in the base prompt, as in the case of Travel Planner, we can iteratively remove values from the backprompts that have been tested in previous LLM Modulo loops and have been deemed unfit by the critics. We employ this filtering strategy by removing values for constraint such as Room Type, Room Rules, and Minimum Nights for accommodations when they are flagged by the accommodation critic.\nFor example, suppose the LLM selects a \"Luxury building studio\" in Los Angeles as the accommodation for the first day, which requires a minimum stay of 7 nights. If you plan to stay in Los Angeles for only 5 days, this accommodation will never be valid, and any plan including this accommodation will be invalid. Therefore, in all subsequent iterations of LLM Modulo, we remove this accommodation and its details from the backprompts we send to the LLM.\nIn terms of performance, applying this filtering strategy slightly increased the success rate for GPT-4o-mini from 12.22% (LLM Modulo Baseline) to 12.67% (LLM Modulo with Filtering). For this specific experiment, we also evaluated GPT-4o and we found a significant increase in the performance with this modification; from 21.11% (LLM Modulo Baseline) to 36.11% (LLM Modulo with Filtering)."}, {"title": "Querying for multiple solutions", "content": "We examine diversification at the generation level by prompting the base LLM to generate multiple solutions for each presented query. If the LLM modulo loop were structured as a chain of n iterations, this strategy would resemble a tree of depth n. Every path from the start node to a leaf node represents a baseline LLM modulo chain. We employ a breadth-first search exploration strategy, with a maximum depth of 10 (or stopping whenever a valid plan is found), for calendar scheduling with GPT-4o-mini, and we report an increase in perfor-"}, {"title": "Types of feedback", "content": "Thus far, all backprompts to the base model have consisted of a collated list of all critiques or feedback from the generation of the previous iteration. This subsection explores the utility of this feedback method through two experiments. In the first experiment, we utilize binary feedback by prompting the LLM with \"This time doesn't work. Come up with an alternative schedule\" instead of providing the full feedback. In the second experiment, we provide only the first critique to test whether the base LLM becomes overloaded with the volume of feedback it receives.\nThe results on calendar scheduling with GPT-4o-mini indicate that when backprompted with just binary critiques, the performance of the LLM Modulo is consistently lower compared to when full feedback is provided. However, when backprompted with only the first critique, the performance is comparable to that of presenting all the critiques simultaneously (as shown in Figure 4)."}, {"title": "Prompt engineering", "content": "In this experiment, we examine the impact of a simple prompt engineering technique; zero-shot Chain-of-Thought (COT) (Kojima et al., 2023) applied to calendar scheduling. For each prompt, we append \"Think step-by-step\" at the end and then query the LLM.\nAs illustrated in Figure 5, the accuracy of GPT-4o-mini Modulo steadily improves with each iter-"}, {"title": "Conclusion", "content": "In this paper, we investigated the effectiveness of integrating a large language model (LLM) within a compound architecture, specifically the LLM-Modulo framework, across four complex scheduling tasks. Our findings demonstrate that while state-of-the-art LLMs perform poorly when used in isolation, embedding them within the LLM-Modulo architecture leads to significant performance improvements. We provided a detailed walkthrough of the framework's instantiation in each scheduling domain and outlined how the various components can be configured effectively including extracting the critics from LLMs itself. Additionally, we explored various modifications to the base LLM-Modulo configuration and analyzed the effect of these modifications on the performance of the overall system. We showed that while some modifications enhanced the performance, some didn't. Overall, we established that LLM-Modulo can provide significant performance gains with guarantees in complex scheduling tasks."}, {"title": "Limitations", "content": "Our experiments, as described in Section 4, were constrained to 10 iterations of the LLM-Modulo framework due to cost limitations. Similarly, in Section 5, we restricted our modification experiments to the evaluation of GPT-4o-mini for the"}]}