{"title": "Multi-Physics Simulations via Coupled Fourier Neural Operator", "authors": ["Shibo Li", "Tao Wang", "Yifei Sun", "Hewei Tang"], "abstract": "Physical simulations are essential tools across critical fields such as mechanical and aerospace engineering, chemistry, meteorology, etc.. While neural operators, particularly the Fourier Neural Operator (FNO), have shown promise in predicting simulation results with impressive performance and efficiency, they face limitations when handling real-world scenarios involving coupled multi-physics outputs. Current neural operator methods either overlook the correlations between multiple physical processes or employ simplistic architectures that inadequately capture these relationships. To overcome these challenges, we introduce a novel coupled multi-physics neural operator learning (COMPOL) framework that extends the capabilities of Fourier operator layers to model interactions among multiple physical processes. Our approach implements feature aggregation through recurrent and attention mechanisms, enabling comprehensive modeling of coupled interactions. Our method's core is an innovative system for aggregating latent features from multi-physics processes. These aggregated features serve as enriched information sources for neural operator layers, allowing our framework to capture complex physical relationships accurately. We evaluated our coupled multi-physics neural operator across diverse physical simulation tasks, including biological systems, fluid mechanics, and multiphase flow in porous media. Our proposed model demonstrates a two to three-fold improvement in predictive performance compared to existing approaches.", "sections": [{"title": "1. Introduction", "content": "Partial differential equations (PDEs) provide the mathematical framework for modeling and predicting physical systems based on fundamental principles of symmetry and conservation laws. (Reed and Simon, 1980; Arnol'd, 2013; Evans, 2022). These equations enable us to represent systems that evolve continuously through space and time, making them essential tools for computational simulation and forecasting. As the formal language for describing spatiotemporal physical systems, PDEs are central to modeling diverse physical behaviors (Chorin et al., 1990; Bergman, 2011).\nTraditional numerical methods, including finite difference, finite element, and finite volume approaches, have long served as standard tools for solving PDEs (Quarteroni et al., 2010; Susanne et al., 1994). However, these conventional approaches face significant limitations due to the curse of dimensionality, as their computational requirements grow exponentially with increasing complexity (LeVeque, 2007; Hughes, 2003). This has driven the development of data-driven alternatives (Kennedy and O'Hagan, 2000; Conti and O'Hagan, 2010; Raissi et al., 2019), particularly neural operators (Kovachki et al., 2023; Lu et al., 2021; Li et al., 2020b,c,a), which offer enhanced computational efficiency. These advanced methods excel by learning the fundamental operators governing PDE behavior, establishing mappings between function spaces to approximate solutions for any input function. This allows them to handle novel scenarios efficiently while avoiding the computational constraints of conventional numerical methods.\nWhile data-driven methods have demonstrated significant success in physical modeling, they encounter substantial challenges when addressing complex multi-physics systems governed by coupled partial differential equations (PDEs) (Keyes et al., 2013; Quarteroni and Quarteroni, 2009). These systems are distinguished by their sophisticated networks of correlated structures and processes, where system-wide behavior emerges from intricate interactions between components (Weinan and Engquist, 2003) rather than from individual elements in isolation. The primary challenge lies in effectively capturing interactions between system components that operate across multiple spatial and temporal scales. This complexity is further amplified by the strong interdependencies between components, which generate nonlinear and often unpredictable outcomes (Cross and Greenside, 2009; Holmes, 2012). Consider the fluid-structure interaction problem (Bazilevs et al., 2013; Bungartz and Sch\u00e4fer, 2006) as an illustrative example: pressure changes in fluid flow fields directly influence structural deformation, which subsequently modifies the flow field itself, creating a complex feedback loop that must be accurately modeled.\nTo address the challenges in modeling complex multi-physics systems, we propose COMPOL, a novel coupled operator learning paradigm. Our approach leverages versatile feature aggregation techniques to capture component interactions, encoding them as augmented feature inputs within the operator learning framework. Our work makes three primary contributions:\n\u2022 We introduce a novel coupled operator architecture that builds upon the Fourier neural operator layer Li et al. (2020a). This paradigm employs feature aggregation techniques to effectively capture component interactions within the operator learning framework, representing a significant advancement in multi-physics modeling capabilities.\n\u2022 We develop two innovative feature aggregation approaches based on the Fourier neural operator architecture. Our methodology operates within the operator learning paradigm, mapping input functions to output functions through neural operator layers in hidden space (Kovachki et al., 2023). The first approach utilizes recurrent neural networks (RNNs) (Cho et al., 2014), concatenating outputs from previous hidden layers as state inputs. The resulting hidden outputs serve as augmentation features for subsequent layers, enabling the aggregated features to capture interaction information among individual processes effectively. The second approach employs attention mechanisms (Vaswani, 2017), transforming latent features into an alternative space and using multi-head attention to aggregate latent outputs from previous layers. This enhancement allows for more flexible aggregation of interactions among processes modeled by individual neural operators.\n\u2022 We evaluate our approach through experiments on several benchmark problems. The results demonstrate that COMPOL achieves superior accuracy in predicting coupled system dynamics while maintaining computational efficiency compared to existing state-of-the-art methods. These findings establish COMPOL as a powerful and flexible framework for simulating coupled and complex multi-physics systems."}, {"title": "2. Background", "content": ""}, {"title": "2.1 Operator Learning", "content": "A Partial Differential Equation (PDE) (Evans, 2022) describing a physical system can be expressed mathematically as: $(L_a(u))(x) = f(x), x \\in \\Omega; u(x) = 0, x \\in \\partial\\Omega$. In this formulation, $f(x)$ represents the source term, $\\Omega$ denotes the spatial domain, and $\\partial\\Omega$ represents its boundary. The solution to the PDE is given by $u(x)$, while $L_a$ represents the differential operator, which may exhibit either linear or nonlinear behavior. Traditional approaches to solving PDEs and conducting physical simulations focus on determining $u(x)$ through analytical or numerical methods, typically involving domain discretization, operator approximation, and solving the resulting system of equations (Brezis, 2011; Quarteroni and Valli, 2008). Data-driven operator learning methods take a fundamentally different approach. These methods aim to construct a parametric mapping $V_{\\theta}$ between function spaces $H$ and $Y$ (e.g., Banach spaces) (Kantorovich and Akilov, 2014). This mapping, denoted as $\\psi_{\\theta}: H \\rightarrow Y$, approximates the inverse operator $L_a^{-1}$, which transforms the problem input into its solution $u(x)$. Depending on the specific formulation, the input may consist of the source term $f(x)$, initial condition $u(0)$, or boundary conditions. The training process relies on a dataset $D$ comprising paired function samples $(f_n, y_n)_{n=1}^N$. Each input sample $f_n$ represents a discretized function $f_n \\in H$, while each output sample $y_n$ corresponds to the discretized solution $\\psi(f_n) \\in Y$. Both input and output functions undergo discretization through value sampling at fixed mesh locations. For example, in a two-dimensional spatial domain $[0, 1] \\times [0, 1]$, functions might be sampled on a uniform 64 \u00d7 64 grid. The ultimate goal of operator learning is to determine parameters $\\theta$ that minimize the empirical loss, thereby approximating the solution operator using the available training data $D$."}, {"title": "2.2 Fourier Neural Operator (FNO)", "content": "The Fourier Neural Operator (FNO) (Li et al., 2020a) transforms discretized input functions through a two-phase process. First, it uses a feed-forward neural network to map each input element and its sampling location into a higher-dimensional channel space. Second, it applies a Fourier layer that combines linear transformation with nonlinear activation: $v(x) \\leftarrow \\sigma(Wv(x) + \\int \\kappa(x - x')v(x')dx')$. The Fourier layer leverages the convolution theorem (Stein and Shakarchi, 2011) to efficiently compute the integral through Fast Fourier Transforms (FFT), reducing computational complexity (Cooley and Tukey, 1965). The architecture completes with multiple stacked Fourier layers and a final feed-forward network that projects features to the output space. Training minimizes an $L_p$ loss function: $\\theta^* = \\underset{\\theta}{argmin} \\sum_{n=1}^N ||y_n - W_{FNO}(f_n; \\theta)||_p$ where $\\theta$ encompasses all model parameters, including the discretized kernel, convolutional layer parameters, and feed-forward network parameters."}, {"title": "2.3 Recurrent Neural Networks (RNNs)", "content": "Recurrent Neural Networks (RNNs) (Elman, 1990; Werbos, 1990) are specialized neural architectures designed to process sequential data by maintaining an evolving memory of past information. These networks process sequences $X_T = (x_1, x_2, \\cdots, x_T)$ through a hidden state $h_t$ that captures information from all preceding elements $(x_1, x_2, ..., x_t)$. The core mechanism lies in their state update process at each time step: $h_t = \\sigma(W_h h_{t-1} + W_x x_t + b)$, where $x_t$ represents the current input, $W_h$ and $W_x$ denote learnable weight matrices, $b$ represents the bias vector, and $\\sigma(\\cdot)$ serves as a non-linear activation function. While RNNs effectively process sequential data, they face challenges with longer sequences due to vanishing or exploding gradients during training Bengio et al. (1994). This limitation has led to more sophisticated architectures like Long Short-Term Memory (LSTM) networks (Graves and Graves, 2012) and Gated Recurrent Units (GRUs) (Cho et al., 2014), which introduce specialized mechanisms to better control information flow and preserve long-term dependencies. These innovations have established RNNs as powerful tools for sequence modeling applications, particularly in natural language processing and time series prediction."}, {"title": "2.4 Attention Mechanism", "content": "The attention mechanism enables models to process relationships between sequence elements in parallel, regardless of their positions (Bahdanau, 2014; Graves, 2014; Vaswani, 2017). Given an input sequence $X_T \\in \\mathbb{R}^{T\\times d}$ (where T is sequence length and d is embedding dimension), the mechanism transforms the input into three representations through learned weights: $Q = XW_Q, K = XW_K, V = XW_V$ These query (Q), key (K), and value (V) representations feed into the scaled dot-product attention formula: $A = softmax(\\frac{QK^T}{\\sqrt{d_k}})$ The final output $Z = AV$ provides a refined sequence representation where each position incorporates information from all other positions simultaneously. This parallel processing offers significant efficiency advantages over the sequential computation of traditional RNNs."}, {"title": "3. Model", "content": ""}, {"title": "3.1 Multi-Physics Operator Learning", "content": "Neural operator learning has demonstrated effectiveness in modeling PDE-governed physical phenomena. However, it faces a fundamental limitation: most approaches assume systems are governed by a single set of PDEs. This simplification fails to capture the reality of physical simulations, where multiple physical processes interact across different scales to produce complex behaviors (Keyes et al., 2013). While current methods attempt to model these interactions through basic techniques like feature concatenation (Wen et al., 2022) and cross-overlaying (Xiao et al., 2023), these approaches prove inadequate for sophisticated simulation scenarios. A coupled multi-physics system is characterized by the interaction and mutual influence of multiple physical processes, governed by different sets of partial differential equations. We can formally define these coupled PDEs as:\n$\\begin{cases}\n(L_1(u^1, u^2,\\dots, u^M))(x,t) = f^1(x,t) \\\\\n...\\\\\n(L_M(u^1, u^2,..., u^M))(x,t) = f^M(x,t)\n\\end{cases}$\nHere, $L_m$ represents the differential operator for the m-th physical process, with $u^m$ denoting its solution function and $f^m$ representing the source term. The interdependence of differential operators $L_m$ on all solution functions $u^1, u^2,\\dots, u^M$ highlights the intricate coupling between physical processes. For simplicity, we omit the spatial domain $x \\in \\Omega_m$ and boundary conditions $u^m(x) = 0, x \\in \\partial\\Omega_m$ for each process $m \\in 1 \\cdots M$. The nonlinear interactions between operators $L_1... L_M$ generate stiff equation systems requiring sophisticated time integration and fine spatial discretization Oden and Prudhomme (2002); Felippa et al. (2001); Hu et al. (2016). These coupling effects introduce significant numerical challenges that compromise both stability and convergence. While current data-driven approaches attempt to address these challenges through basic techniques, they fail to capture the rich dynamical dependencies between physical processes.\nAn illustrative example of such coupled systems is the coupled diffusion-reaction process (Grindrod, 1996):\n$\\begin{cases}\n\\frac{\\partial u}{\\partial t} = D_u\\Delta u + R_u(u, v), u : \\Omega \\times (0, T] \\rightarrow \\mathbb{R} \\\\\n\\frac{\\partial v}{\\partial t} = D_v\\Delta v + R_v(u, v), v : \\Omega \\times (0, T] \\rightarrow \\mathbb{R}.\n\\end{cases}$\nIn this system, $u$ and $v$ represent concentrations of interacting substances over a spatial domain $\\Omega$ and time interval $(0, T]$. Diffusion coefficients $D_u$ and $D_v$ quantify spatial spread rates, while Laplacian operators $\\Delta u$ and $\\Delta v$ capture spatial variation. The coupled reaction terms $R_u(u, v)$ and $R_v(u, v)$ encapsulate local substance interactions, demonstrating how localized interactions can propagate to influence global system dynamics over time."}, {"title": "3.2 Feature Aggregation for Coupled System", "content": "To effectively capture the complex interactions among multiple physical processes in a coupled system, we propose a novel approach that involves computing feature aggregation states in the latent space within the framework of neural operator learning. These feature aggregation states serve for encoding the intricate relationships and dependencies among the various processes involved. By representing the interactions in a latent space, we can abstract from the raw, high-dimensional, multi-scale data and focus on the essential features and patterns that govern the coupled system's behavior.\nThe feature aggregation states act as compact and informative representations that encapsulate the key aspects of the interactions among the physical processes.\nAs shown in Figure 1., we consider a coupled physical system with M processes, each with corresponding input functions $f^1, \\dots, f^M$. For each process m, operator $O^m$ first maps its input function $f^m$ to a latent representation $v_0^m$ using a channel-wise linear layer $P^m: \\mathbb{R}^{d_m} \\rightarrow \\mathbb{R}^{d_h}$. Here, $d_m$ denotes the input dimension of process m, and $d_h$ represents the latent dimension. This transformation projects all input functions into a common latent space for unified processing. If each physical process $m \\in 1 \\cdots M$ is modeled individually, without considering the interactions among processes, the operator layer $O^m$ then applies a neural operator layer $h^m: v_0^m \\rightarrow v_1^m$ for the m-th process. This neural operator layer takes the latent representation $v_0^m$ as input and transforms it into another latent representation $v_1^m$. By stacking L neural operator layers together, each subsequent layer $h^m: v_{l-1}^m \\rightarrow v_l^m$ further transforms the latent function $v_{l-1}^m$ into $v_l^m$ for the m-th process. This sequential structure allows for capturing increasingly complex and abstract features of the individual processes. At the end of the stack of neural operator layers, for each process, a channel-wise linear layer $Q^m: v_L^m \\rightarrow g^m$ is used to map the final latent function $v_L^m$ back to its corresponding output function $g^m$. This final mapping brings the latent representations back to the original function space, allowing for generating the desired output functions. However, this independent modeling approach has a critical limitation: each operator $O^m$ focuses exclusively on its own process, disregarding the interactions with other processes in the coupled system. This isolation fails to capture the strong correlations and complex dependencies that characterize coupled physical systems.\nTo address this limitation, we introduce a coupled multi-physics neural operator $MPO(O^1, \\dots, O^M)$ to capture the intricate correlations among operators $O^1, \\dots, O^M$. The core innovation lies in incorporating cross-process information during latent function transformation, enabling comprehensive modeling of process interactions. Our approach centers on feature aggregation states ${z_l}_{l=1}^L$ - shared global latent representations that augment the neural operator layers' inputs. These states facilitate information exchange across processes, allowing the model to capture inter-process dependencies and correlations for a more complete understanding of the coupled system. We implement MPO using Fourier Neural Operator (FNO) layers, which efficiently capture both local and global information through Fourier transforms. While FNO layers excel at representing complex spatial dependencies in coupled systems, our framework remains adaptable to any operator learning approach that performs sequential latent space transformations.\nAs the Figure 1. illustrates, our method computes a feature aggregation state $z_l$ at each neural operator layer $h^m$ to capture collective information across all M processes. This state summarizes the cumulative latent information from all processes up to the current layer, defined as:\n$z_l = Aggregation({{v_l^m}_{m=1}^M}_{l=0}^L)$\nThe computed feature aggregation state $z_l$ then augments the input to each process's next neural operator layer l. Specifically, $h_{l+1}^m$ receives a concatenation of the process-specific latent function $v_l^m$ and the global aggregation state $z_l$. This input allows each layer to process both local process information ($v_l^m$) and global cross-process patterns ($z_l$), producing an updated latent function $v_{l+1}^m$ for process m at layer l + 1. For implementing the aggregation mechanism, we propose two distinct approaches based on Recurrent Neural Networks (RNNs) and attention mechanisms."}, {"title": "3.3 Recurrent Aggregation State", "content": "For notational convenience, let V represent the complete collection of intermediate latent features across M processes and L + 1 layers, where these features are produced by neural operator layers ${{h_l^m}_{m=1}^M}_{l=0}^L$. Specifically, $V = {{v_l^m}_{m=1}^M}_{l=0}^L$. In the one-dimensional case, V can be viewed as a (L + 1) \u00d7 M \u00d7 $d_h$ tensor. From this structure, we define $V_l$ as the M \u00d7 $d_h$ matrix obtained by extracting the l-th slice along the first dimension of V. We further introduce $\\overline{V}_l$ to represent the sequence of latent functions from layer 0 through layer l, which can be written as $\\overline{V}_l = {V_j}_{j=0}^l$. Within this notation, two cases are particularly noteworthy: $V_0$ represents the collection of latent functions immediately following the channel-wise lifting operation, while $V_L$ denotes the collection of all latent functions just before the channel-wise projection.\nOur first attempt of computing the aggregation state of layer l is with using a recurrent neural network, expressed as:\n$z_l = RNN(z_{l-1}, V_l)$.\nHere, $z_{l-1}$ represents the aggregation state from the previous layer, and $V_l$ captures the collective latent transformations across all M processes at layer l. We implement this recurrent mechanism using a Gated Recurrent Unit (GRU) (Cho et al., 2014), defined by:\n$\\begin{aligned}\nq_l &= \\sigma(W_qV_l + U_qz_{l-1} + b_q)\\\\\nr_l &= \\sigma(W_rV_l + U_rz_{l-1} + b_r)\\\\\n\\tilde{z}_l &= tanh(W_zV_l + U_z(r_l \\odot z_{l-1}) + b_z)\\\\\nz_l &= q_l \\odot z_{l-1} + (1 - q_l) \\odot \\tilde{z}_l\n\\end{aligned}$\nwhere $\\odot$ denotes element-wise multiplication. The computation of aggregation state $z_l$ involves two key intermediary states: the update state $q_l$ and the reset state $r_l$. The update state $q_l$ functions as a dynamic gate that modulates the balance between preserving information from $\\overline{V}_{l-1}$ and integrating new information from $V_l$, thereby regulating information flow between consecutive layers. The reset state $r_l$ serves as a filtering mechanism that identifies and removes non-essential historical information, enabling the model to concentrate on the most relevant aspects of the previous aggregation state. The model then computes a candidate state $\\tilde{z}_l$, representing a proposed new aggregation state. This candidate synthesizes the current input $V_l$ with the selectively filtered historical information $r_l \\odot z_{l-1}$, offering a novel perspective on the potential aggregation state while maintaining contextually relevant historical information (Jozefowicz et al., 2015; Pascanu, 2013). The final aggregation state $z_l$ is determined through a strategic combination of the previous state $z_{l-1}$ and the candidate state $\\tilde{z}_l$, weighted by the update state $q_l$. This resulting state encapsulates a carefully calibrated fusion of historical and current information. We demonstrate that this aggregation state effectively captures the intricate interactions up to layer l, enabling the model to harness the complex dynamics inherent in the coupled physical system."}, {"title": "3.4 Attention Aggregation State", "content": "Our second approach leverages attention mechanisms to compute the aggregation state $z_l$, offering significant advantages over RNN-based methods (Bahdanau, 2014; Vaswani, 2017) in capturing information from $V_l$. Unlike the sequential nature of RNNs, attention mechanisms process all elements simultaneously, assigning weights (attention scores) across $V_l$. This parallel processing enables $z_l$ to dynamically focus on the most significant features within $V_1, \\dots, V_l$, providing a more versatile and computationally efficient framework for modeling interactions in the latent space of coupled multi-physics systems.\nThe implementation begins with the application of channel-wise feed-forward neural networks $\\Phi_Q, \\Phi_K$, and $\\Phi_A$, which transform $V_l$ into three distinct representations essential for computing attention scores: query (Q), key (K), and value (A). This transformation can be expressed as:\n$Q = \\Phi_Q(V_l), K = \\Phi_K(V_l), A = \\Phi_A(V_l)$\nThe aggregation state at layer l, denoted as $z_l$, is then computed through a weighted sum:\n$z_l = \\sum_{k=1}^{\\overline{l}} \\alpha_{lk} A_k$\nwhere $A_k$ represents the k-th slice along the first dimension of A. The attention weights $\\alpha_{lk}$ are determined through a similarity-based computation:\n$\\alpha_{lk} = \\frac{exp(s_{lk})}{\\sum_{j=1}^{\\overline{l}} exp(s_{jk})}, s_{lk} = \\frac{Q_l K_k^T}{\\sqrt{d_k}}$\nwhere $Q_l$ and $K_k$ represent the l-th and k-th slices along the first dimension of Q and K respectively. This attention mechanism determines feature importance by measuring similarities between Q and K representations. The resulting attention scores quantify the relevance of each hidden state $V_j$ (where j \u2208 0,\u00b7\u00b7\u00b7, l) of $V_l$ to the current aggregation state $z_l$. The final aggregation state is then computed by combining the value representations weighted by these attention scores. This approach enables $z_l$ to intelligently focus on the most pertinent information, delivering a refined and adaptable method for feature aggregation in coupled multi-physics modeling."}, {"title": "3.5 Training", "content": "Given the training data that are simulated or sampled from a coupled multi-physics system, denoted as ${\\{f_n^m,y_n^m\\}_{n=1}^{N_m}\\}_{m=1}^M$, where $f_n^m$ and $y_n^m$ represent the n-th input and output functions, respectively, from the m-th process of the coupled system, we optimize our coupled multi-physics neural operator (MPO) by minimizing the empirical risk,\n$\\mathcal{L}_{MPO} = \\mathbb{E}_{m \\sim \\pi} \\mathbb{E}_{f^m \\sim \\mu_m} ||MPO(f_n^m) - y_n^m||\n= \\sum_{m=1}^M \\frac{1}{M} \\sum_{n=1}^{N_m} \\frac{1}{N_m} ||MPO(f_n^m) - y_n^m||$\nwhere $g_n^m$ is the prediction of $f_n^m$. We can then use any gradient-based optimization method to minimize $\\mathcal{L}_{MPO}$."}, {"title": "4. Related Work", "content": "Operator learning represents an innovative approach to surrogate modeling that maps input functions to output functions. Traditional surrogate modeling methods have typically focused on mapping limited sets of system parameters e.g., PDE parameters to output functions e.g., PDE solution functions, as demonstrated in numerous research works (Higdon et al., 2008; Zhe et al., 2019; Li et al., 2021; Wang et al., 2021; Xing et al., 2021b,a; Li et al., 2022a). Neural operator methods, which leverage neural networks as their foundational architecture, have driven substantial progress in operator learning. The Fourier Neural Operator (FNO) (Li et al., 2020a) emerged alongside a simpler variant, the Low-rank Neural Operator (LNO) (Kovachki et al., 2023), which utilizes a low-rank decomposition of the operator's kernel to enhance computational efficiency. Building on these innovations, researchers introduced the Graph Neural Operator (GNO) (Li et al., 2020b), which innovatively combines Nystrom approximation with graph neural networks to approximate function convolution. The Multipole Graph Neural Operator (MGNO) (Li et al., 2020c) introduced a multi-scale kernel decomposition approach, achieving linear computational complexity in convolution calculations. Parallel developments included a multiwavelet-based operator learning model (Gupta et al., 2021), which enhanced precision through fine-grained wavelet representations of the operator's kernel. Another significant contribution emerged with the Deep Operator Net (DeepONet) (Lu et al., 2021), which employs a dual-network architecture combining a branch network for input functions and a trunk network for sampling locations. This architecture was later refined into the POD-DeepONet (Lu et al., 2022), which improved stability and efficiency by replacing the trunk network with POD (or PCA) bases derived from training data. A survey of neural operators is given in (Kovachki et al., 2023). Recent advances in operator learning have focused on developing mesh-agnostic and data-assimilation approaches, representing a significant departure from traditional methods (Yin et al., 2022; Chen et al., 2022; Pilva and Zareei, 2022; Boussif et al., 2022; Esmaeilzadeh et al., 2020). These novel approaches remove the constraint of requiring input and output functions to be sampled on fixed or regular meshes, offering greater flexibility in handling diverse data structures. A key innovation in these methods lies in their reformulation of PDE simulation as an ODE-solving problem. This fundamental shift changes the primary objective from simply predicting solutions based on initial or boundary conditions to capturing the complete dynamic behavior of PDE systems. Training PDE surrogates and neural operators requires extensive high-quality data, which poses a significant challenge due to the resource-intensive nature of physical experiments and sophisticated simulators. To optimize predictive performance while minimizing data collection costs, researchers employ multi-fidelity, multi-resolution modeling Tang et al. (2024, 2023); Li et al. (2023), and active learning approaches (Li et al., 2022b, 2024).\nOur approach relates to recent advances in leveraging multi-physics data for neural operator training, though with distinct objectives and methodologies (McCabe et al., 2023; Rahman et al., 2024; Hao et al., 2024). While recent work has focused on developing physics surrogate foundation models by incorporating data from various physics simulations, our research specifically examines the coupled relationships between interaction processes within individual physical systems. The Coupled Multiwavelet Neural Operator (CMWNO) (Xiao et al., 2023) represents the closest parallel to our work, as it similarly aims to model coupled PDEs in complex systems with multiple physical processes. CMWNO employs a specialized approach, decoupling the integral kernel during multi-wavelet decomposition and reconstruction procedures in the Wavelet space. However, this method faces several limitations: it requires particular structures and fixed function decomposition schemes, and its simple cross-over model structure presents challenges for systems with more than two processes. Additionally, CMWNO sometimes struggles to capture complex correlations among different processes effectively. Our proposed method builds upon FNO layers while offering greater flexibility and adaptability. A key advantage of our approach lies in its extensibility - it can be integrated with any neural operator learning framework that conducts layer-wise functional transformations."}, {"title": "5. Experiment", "content": "We conducted a comprehensive evaluation of our proposed framework's performance in predicting solution fields for coupled multi-physics systems, focusing on several benchmark partial differential equations (PDEs) from computational physics. The evaluation encompasses three key equations: the 1-D Lotka-Volterra equations (Murray, 2007), 1-D coupled Burgers' equations (Larsson and Thom\u00e9e, 2003), 2-D Gray-Scott equations (Pearson, 1993) and a multiphase flow problem with CO2 and water in the context of geological storage of CO2 (Bear and Cheng, 2010; Hashemi et al., 2021; Abou-Kassem et al., 2013). These equations were selected for their diverse characteristics and widespread applications across scientific and engineering domains. To establish a robust training dataset, we employed numerical solvers with multi-mesh discretization. The 1-D experiments utilized a mesh size of 256, while the 2-D experiments employed a 64x64 mesh configuration. We investigated the framework's performance under varying data availability conditions by conducting experiments with two distinct training set sizes: 256 and 512 examples. Additionally, we generated a separate test set of 200 examples using identical mesh sizes to assess the models' generalization capabilities effectively. For experimental clarity and computational efficiency, we focused our evaluation on coupled multi-physics systems with two processes. While this limitation was implemented specifically for these experiments, it's important to emphasize that our framework inherently supports systems with any number of processes. Throughout all experiments, we maintained constant coefficients and boundary conditions to isolate and analyze the impact of initial conditions on system evolution. The primary objective of our evaluation was to learn an effective mapping from the initial conditions (t = 0) to the final solutions after a specified number of time steps (t = T). By maintaining fixed coefficients and boundary conditions, we could thoroughly assess our framework's ability to capture and model the complex dynamics and interactions between processes, focusing specifically on how variations in initial conditions influence the system's evolution.\nCompeting Methods In our experimental evaluation, we examine three variants of our proposed framework, each employing distinct aggregation mechanisms. The primary variants include COMPOL-RNN, which utilizes a recurrent aggregation mechanism, and COMPOL-ATN, which implements an attention-based aggregation approach. To provide enhanced flexibility in modeling complex interactions, we introduce an additional variant, COMPOL-MH-ATN, which leverages multi-head attention for aggregation. This third variant extends the capabilities of the basic attention mechanism by enabling the model to capture multiple types of relationships simultaneously through its multi-headed architecture Vaswani (2017).\nWe evaluated our framework against two leading neural operators for coupled multi-physics systems: FNO and CMWNO\u00b9. FNO was adapted using process concatenation as input channels (FNOC) to capture interactions in Fourier space, while CMWNO modeled inter-process dependencies through decoupled integral kernels in multiwavelet operations. For implementation consistency, all models were developed in PyTorch (Paszke et al., 2019) and trained using the Adam optimizer (Diederik, 2014) with a 0.001 learning rate and cosine annealing schedule for optimal convergence (Loshchilov and Hutter, 2016). The computations were performed on an NVIDIA RTX 4090 GPU with 24GB memory. To ensure robust evaluation, we employed 5-fold cross-validation, dividing the dataset into five equal segments. Each segment served as the validation set once while the remaining data was used for training. Performance was measured using averaged relative L2 error on a separate test dataset, with standard deviations reported across all folds. Due to structural constraints, CMWNO comparisons were restricted to 1-D experiments only."}, {"title": "5.1 Benchmarks", "content": "1-D Lotka-Volterra Equation The 1-D reaction-diffusion Lotka-Volterra system models predator-prey population dynamics through coupled partial differential equations:\n$\\begin{aligned"}, "n\\frac{\\partial u}{\\partial t} &= D_u\\nabla^2u + au - buv \\\\\n\\frac{\\partial v}{\\partial t} &= D_v\\nabla^2v + cuv - dv\n\\end{aligned}$\nThe system combines spatial diffusion (coefficients Du and Dv) with population interactions through reaction terms, generating complex spatio-temporal patterns. We study this system using a one-dimensional Gaussian Random Field initialization (length-scale l = 0.1, amplitude \u03c3 = 0.5) with periodic boundary conditions and uniform interaction parameters (a = b = c = d = 0.01) to examine the fundamental dynamics.\n1-D Coupled Burgers' Equation The coupled Burgers' equation models the evolution of two interrelated spatio-temporal variables, u(x, t) and v(x, t), through a pair of interconnected equations:\n$\\begin{aligned}\n\\frac{\\partial u}{\\partial t} &= -u\\frac{\\partial u}{\\partial x} - v\\frac{\\partial^2 u}{\\partial x} + v\\nabla"]}