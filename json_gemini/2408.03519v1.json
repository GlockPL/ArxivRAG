{"title": "RepoMasterEval: Evaluating Code Completion via Real-World Repositories", "authors": ["Qinyun Wu", "Chao Peng", "Pengfei Gao", "Ruida Hu", "Haoyu Gan", "Bo Jiang", "Jinhe Tang", "Zhiwen Deng", "Zhanming Guan", "Cuiyun Gao", "Xia Liu", "Ping Yang"], "abstract": "With the growing reliance on automated code completion tools in software development, the need for robust evaluation benchmarks has become critical. However, existing benchmarks focus more on code generation tasks in function and class level and provide rich text description to prompt the model. By contrast, such descriptive prompt is commonly unavailable in real development and code completion can occur in wider range of situations such as in the middle of a function or a code block. These limitations makes the evaluation poorly align with the practical scenarios of code completion tools. In this paper, we propose RepoMasterEval, a novel benchmark for evaluating code completion models constructed from real-world Python and TypeScript repositories. Each benchmark datum is generated by masking a code snippet (ground truth) from one source code file with existing test suites. To improve test accuracy of model generated code, we employ mutation testing to measure the effectiveness of the test cases and we manually crafted new test cases for those test suites with low mutation score. Our empirical evaluation on 6 state-of-the-art models shows that test argumentation is critical in improving the accuracy of the benchmark and RepoMasterEval is able to report difference in model performance in real-world scenarios. The deployment of RepoMasterEval in a collaborated company for one month also revealed that the benchmark is useful to give accurate feedback during model training and the score is in high correlation with the model's performance in practice. Based on our findings, we call for the software engineering community to build more LLM benchmarks tailored for code generation tools taking the practical and complex development environment into consideration.", "sections": [{"title": "1 INTRODUCTION", "content": "The rapid advancements in large language models (LLMs), prompt strategies, context retrieval algorithms, and tool availability have significantly improved the capability and popularity of automated code completion and generation tools, such as GitHub Copilot [2], MarsCode [3] and Codeium [1]. These developments have sparked a surge in reliance on such tools to improve programming pro-ductivity. Code completion techniques are usually available in the form of IDE (Integrated Development Environment) plugins which automatically predict code snippets at the location of the cursor"}, {"title": "2 BACKGROUND AND MOTIVATION", "content": "In this section, we briefly introduce code generation and completion, LLMs for these tasks and motivate our work by revisiting existing benchmarks."}, {"title": "2.1 Large Language Models for Code", "content": "LLMs leverage massive datasets and sophisticated training tech-niques to produce coherent and contextually relevant code snippets. Recent advances in LLMs, especially those trained on code, have rev-olutionized the field of automated software engineering, providing significant enhancements in productivity and accuracy.\nGeneral LLMs have shown exceptional performance in various development tasks, including code generation. For instance, GPT-4 achieves high pass rates on benchmarks like HumanEval. With their extensive training on diverse datasets, these models can generate code snippets, complete functions, and even provide debugging assistance based on natural language prompts.\nCode-specific LLMs are trained primarily on code-specific data and often outperform general LLMs in code generation tasks. No-table examples include Codex [9], DeepSeek Coder [16] and Star-Coder [30], which have been fine-tuned to generate accurate and contextually appropriate code. These models employ various train-ing objectives, such as next-token prediction or filling-in-the-middle (FIM) techniques."}, {"title": "2.2 Existing Benchmarks for Code Generation", "content": "To evaluate the performance of these advanced LLMs in code gener-ation, several benchmarks have been developed. These benchmark provide standardized tasks and metrics to compare the capabilities of different models. The task is consisted of a natural language description as the input (prompt), and the corresponding code acts as the ground truth output (canonical solution). In terms of metrics, exact match and code similarity methods compare the real model output with the ground truth while passing rate (Pass@k) executed the model output against test cases to assess the correctness of the generated code.\nWe revisit existing code generation benchmarks that are used ac-tively according to studies conducted by [14, 26]. As summarized in Table 1, these benchmarks fall short in assessing practical code com-pletion scenarios encountered in real-world software development due to the following reasons:\n(1) Focus on Single Code Units: These benchmarks evaluate the generation of isolated code units, such as individual functions or classes, rather than more diverse, interdependent code struc-tures such as loop body, part of the function, etc. This approach limits the assessment to simpler tasks, which may not fully exploit the capabilities of modern LLMs capable of handling longer sequences and more intricate dependencies.\n(2) Limited Contextual Information: Existing benchmarks typi-cally provide limited contextual information. Prompts in forms of documentation string and text description focus on the func-tionality of the code to be implemented but lack additional context from surrounding code and relevent source files. In real-world scenarios, code completion tasks often require under-standing and generating code within broader and more complex codebases where functions and methods are interdependent.\n(3) Test Suite Quality: The reliance on predefined test cases for evaluation can result in insufficient assessment of robustness, as these test cases might not cover all possible edge cases. Hu-mannEval+ [28] examined the test effectiveness of HumanEval via mutation testing and revealed the ineffectiveness of existing test cases of HumanEval.\n(4) Lack of Empirical Correlation Studies: There is a gap in research examining the correlation between benchmark perfor-mance and real-world usability, making it difficult to determine the practical effectiveness of these benchmarks."}, {"title": "2.3 Motivation for a Code Completion Benchmark", "content": "Given problems and limitations discussed above, existing bench-marks are inadequate for evaluating more practical code generation tasks, such as generating longer and compound code units consist-ing of multiple interdependent methods. To address this gap, we propose the benchmark designed specifically for code completion tasks, RepoMasterEval, to cover more realistic and challenging scenarios. RepoMasterEval incorporates rich contextual informa-tion from real-world repositories and employs mutation testing and manual test case crafting to ensure accuracy and robustness. This approach offers a more comprehensive and practical framework for assessing LLM performance in real-world software development environments."}, {"title": "3 APPROACH", "content": "In this section, we present the overview of RepoMasterEval in Sec-tion 3.1) and present data collection (Section 3.2), task construction and test suite augmentation via mutation testing (Section 3.3) and evaluation process (Section 3.5) using this example. We also discuss the diversity of the benchmark in Section 3.6."}, {"title": "3.1 Benchmark Overview", "content": "RepoMasterEval is designed to provide a comprehensive and real-istic evaluation of code completion models, reflecting the complex and varied scenarios encountered in real-world software develop-ment. As summarized in Table 2, each coding task consists of the following key components:\nPrefix: Code that appears before the masked snippet. It pro-vides essential context for the code completion task, helping the model understand the surrounding code environment.\nMasked Code: The masked code snippet that the model needs to generate. This serves as the correct output that models are evaluated against.\nSuffix: The code that follows the masked snippet. This ad-ditional context is crucial for models to generate accurate and contextually appropriate code completions.\nRetrieved Information: Contextual information retrieved from the repository using the BM25 algorithm. This includes relevant code snippets, comments, and documentation that can help the model make better predictions."}, {"title": "3.3 Task Construction", "content": "For the evaluation to be effective, it is crucial that the \"hole\" (masked code) created for the task is positioned within a segment of the repository's code that is covered by the original test set. This coverage ensures that the model's completion can be accurately tested through the existing tests.\nThe evaluation process commences with the execution of the original tests within the repository to establish a baseline using the original code content. Subsequently, a coverage report is generated that documents which lines of code are covered by the test suite. Analyzing this report allows for the precise identification of code segments covered by tests."}, {"title": "3.2 Data Source", "content": "To achieve the commitment to realism, we select active and contin-uously updated GitHub repositories as the foundation data source. To mitigate the potential to data leakage, the benchmark only in-corporates repositories inaugurated post-March 2023.\nTo align with a strict quality standard, RepoMasterEval employs rigorous filtering criteria: \u2460 Each repository must have gained a minimum of 100 stars, ensuring a baseline level of community en-dorsement and visibility. Recognizing the critical role of unit test pass rates in the evaluation metrics, only repositories with a proven track record of successful unit test executions are included. This is confirmed through the presence of test files and an automated test execution pipeline, and our additional manual verification to ensure all tests pass."}, {"title": "3.4 Test Augmentation via Mutation Testing", "content": "Mutation testing is a method used to evaluate the effectiveness of test suites by generating defective versions of code, known as mutants, and assessing whether the test cases can detect and \"kill\u201d these mutants. Our test augmentation process involves the follow-ing steps, as shown in Figure 1:\n(1) Generating Defective Versions (Mutants). Using mutation testing, we generate various defective versions of the code snip-pets. These mutants are systematically altered versions of the original code, where specific changes (mutations) are intro-duced to create potential faults. The goal is to simulate common programming errors and assess whether the existing test cases can identify these faults. Mutation types used in our work is summarised in Table 4\n(2) Running Test Suites on Mutants. The generated mutants are then subjected to the existing test suites. Each test case in the suite is executed against the mutants to determine whether it can detect the introduced defects. A test case is considered to have \"killed\" a mutant if it fails when executed on the mutant version of the code.\n(3) Augmenting Test Suites. For mutants that are not detected (i.e., not killed) by the current test suites, additional test cases are crafted using a combination of automated methods (such as GPT-based test generation) and manual annotation by developers. The aim is to enhance the test suite's ability to detect faults by adding more comprehensive and targeted test cases.\n(4) Iterative Process Steps 2 and 3 are repeated iteratively. Each iteration involves running the augmented test suite on the re-maining undetected mutants and adding new test cases for any mutants that still survive. This process continues until all mu-tants are effectively killed by the test suite, indicating a robust and comprehensive set of tests."}, {"title": "3.5 Evaluation Process", "content": "3.5.1 Task Execution. Upon receiving the output from the model, the proposed code snippet is reintegrated into the repository, specif-ically to replace the original masked code. The code completion capability of the model is then measured by re-executing all the unit tests associated with the repository; a successful pass of these tests indicates a correct completion by the model. Conversely, the failure of any test pinpoints inaccuracies in the model's output, attributable to either functional inconsistencies or syntax errors. Such a rigorous testing mechanism ensures a fair comparison of code completion performance across different models.\n3.5.2 Pass Rate Metrics. Pass rate is a crucial evaluation metric used to measure the performance of code generation models intro-duced by Chen et al [9]. It indicates the proportion of generated code snippets that successfully pass a set of predefined test cases, thus reflecting the correctness and functional validity of the code produced by the model.\nThe pass rate is often denoted as Pass@k, where k represents the number of generated code snippets considered. For a given set of test cases, the pass rate is calculated as the fraction of generated code snippets that pass all the test cases.\nThe formula to calculate the pass rate is given by:\n$pass@k := \\sum_{problems} \\frac{\\binom{n-c}{k}}{\\binom{n}{k}}$\nwhere\n\u2022 n is the total number of generated code snippets;\n\u2022 c is the number of correct code snippets generated by the model.\nThis metric provides a straightforward and quantifiable measure of how effectively a model can generate functionally correct code based on the provided prompts."}, {"title": "3.6 Diversity Study", "content": "Figure 2 shows the distribution of data points. The diversity of RepoMasterEval is underscored by its inclusion of 288 code snippets, meticulously selected from repositories in Table 3, and categorized based on their functionalities. These categories range from foundational aspects like Programming Basics, which com-prises 58 snippets, and essential Common Tools, represented by 75 snippets, to specialized fields like Machine Learning with 21 snippets and Natural Language Processing with 14 snippets. The collection further includes 23 snippets related to Front-end devel-opment and 26 related to Server-side development, reflecting a balanced consideration of both client and server paradigms. Addi-tionally, the benchmark addresses vital areas such as Security and Privacy (10 snippets), Data Management and Analysis (11 snippets), and Databases (21 snippets), highlighting aspects of safety, data han-dling, and storage. Categories like Data Structures and Algorithms, with 15 snippets, and Performance Optimization, though smaller with 3 snippets, ensure coverage of optimization and core compu-tational logic. The inclusion of Multimedia Processing (5 snippets) and Language Features (6 snippets) adds further depth, ensuring that the benchmark captures a wide range of programming contexts and challenges."}, {"title": "4 EXPERIMENTS", "content": "In this section, we evaluate the code completion capability of cur-rent LLMs trained on code and report the industrial study and lessons learned from the deployment of RepoMasterEval as the evaluation framework for an in-house code generation model. We answer the following research questions:"}, {"title": "Q1. Effectiveness of Test Augmentation", "content": "Is manual test augmen-tation able to improve the adequacy of test suites?\nTo answer this question, we compare code coverage and muta-tion score achieved by original and additional test suites and study how models perform differently with and without these tests."}, {"title": "Q2. Code Completion Capability", "content": "What is the pass rate of se-lected LLMs achieved on RepoMasterEval compared with Hu-manEval?\nTo answer this question, we select 4 open-source and 2 commer-cial models to generate code snippets 5 times and report pass rate (Pass@1, n=5) achieved by each model on both benchmarks."}, {"title": "Q3. Industry Development", "content": "Is the performance of a model on our benchmark correlated with its online acceptance rate?\nFor this question, we collaborate with a company which is ac-tively developing an in-house code completion LLM and deploy our benchmark as the evaluation framework for the model. For a one-month period, we study the model's score on our bench-mark and its online acceptance rate during the evolvement of the model."}, {"title": "Selected Models.", "content": "We select popular code LLMs released during the past year. For models with various parameter sizes, we choose the one with approximately 7B parameters for a faster inference overhead, which is sensitive in code completion, as developers expect the model to respond quickly after typing. We summarize details of selected models in Table 5."}, {"title": "5 RESULTS AND ANALYSIS", "content": "5.1 RQ1. Effectiveness of Test Augmentation\nTo determine the effectiveness of manual test augmentation, we compare the code coverage and mutation scores achieved by origi-nal and additional test suites (as shown in Figure 3, and study how models perform with and without these tests (Table 6).\nAs illustrated in Figure 3, although mutation scores increase for the dataset of both programming languages, code coverage only increase slightly for Python from 71% to 72% and observed no change for TypeScript. The pass rate of all subject models decreased with additional test suites are crafted, indicating that the additional tests were effective in improving the robustness of the test suites, making them more capable of catching potential faults in the code."}, {"title": "Finding 1.", "content": "Manual test augmentation has demonstrated its ef-fectiveness in improving the adequacy of test suites, particularly in terms of increasing mutation scores. The improved mutation scores suggest that the additional tests made the test suites more robust and capable of detecting more faults. However, the slight decrease in model performance across both programming languages indicates that the augmented tests posed a greater challenge to the models, highlighting potential areas where the models can be further optimized to handle more comprehensive and stringent test suites."}, {"title": "5.2 RQ2. Code Completion Capability", "content": "Figure 4 shows Pass@1 scores achieved by subject models on RepoMasterEval (RME) and HumanEval. Below is an analysis of these results from various perspectives."}, {"title": "Differences between RepoMasterEval and HumanEval.", "content": "Mod-els generally perform better on HumanEval (with Python only) com-pared to RME-Python. For instance, GPT-4-0125-Preview achieved a Pass@1 score of 82.3% on HumanEval but only 0.226 on RME-Python. Similarly, for TypeScript, the performance is lower on RME compared to Python on HumanEval. For example, DeepSeek-Coder-Base 7B scored 30.6% on RME-TypeScript and 47.6% on HumanEval-Python. This suggests that HumanEval, with its simpler and more isolated tasks, may not fully capture the complexities present in real-world coding environments as simulated by RepoMasterEval, resulting in the generally lower Pass@1 scores across models. We also present"}, {"title": "Instruct versus base models.", "content": "The instruct version of DeepSeek-Coder outperforms the base version on HumanEval-Python by a significant margin (75.6% vs. 47.6%). This indicates that the instruct version benefits greatly from additional fine-tuning for specific task instructions with text descriptions. However, this advantage diminishes on RepoMasterEval, where the instruct version's per-formance drops below that of the base version for both Python and TypeScript. This suggests that the instruct version may not be as robust in handling the real-world code completion scenar-ios emphasized by RepoMasterEval, that requires understanding and generating code within a broader context, unlike the more straightforward tasks in HumanEval."}, {"title": "5.3 RQ3. Industry Deployment", "content": "Table 9 presents the changes in pass rates on RepoMasterEval and the online acceptance rates for both Python and TypeScript across different versions of the model over the recent month. Due to confi-dential reasons, we have made the first version as the baseline and report the trend (difference from the previous version) instead of report the real number of benchmark result and online acceptance rate.\nPositive Correlation. There is a general positive correlation be-tween the model's performance on RepoMasterEval and its online acceptance rate. When the pass rate on the benchmark increases, the acceptance rate by users tends to improve, as seen in versions 1.1 to 1.3 for both Python and TypeScript. Significant improvements in benchmark pass rates, such as in Python for version 1.2 (7.1%) and TypeScript for version 1.1 (5.2%), correlate with notable in-creases in user acceptance rates. This suggests that enhancements captured by the benchmark are meaningful and positively impact real-world usability. Minor declines in benchmark pass rates, as ob-served in version 1.3 for Python (-1.1%) and version 1.1 for Python"}, {"title": "Finding 3.", "content": "There is a positive correlation between the model's performance on RepoMasterEval and its online acceptance rate. Improvements in benchmark pass rates are generally associated with higher acceptance rates by users, validating the bench-mark's effectiveness in reflecting real-world performance. These findings underscore the value of using RepoMasterEval as a re-liable framework for evaluating and optimizing code completion models."}, {"title": "6 THREATS TO VALIDITY", "content": "Internal Validity. The process of manual test augmentation in-troduces potential biases, as the additional test cases were crafted based on our understanding of the repositories and code snippets. This might not fully capture the wide range of possible scenarios and edge cases that could occur in real-world applications. We em-ployed mutation testing to systematically identify and augment weaker test cases, aiming for objective improvements. Future re-search will explore automated test generation techniques to further minimize bias.\nExternal Validity. Our experiments were conducted using spe-cific datasets and benchmarks (Python and TypeScript from GitHub repositories). The results may not generalize to other programming languages, frameworks, or real-world environments outside of our selected datasets. Additionally, the specific characteristics of the repositories chosen for RepoMasterEval may not fully represent the diversity of real-world coding tasks. To mitigate this threat, we chose repositories with high community engagement (minimum 100 stars) and diverse domains to enhance the representativeness of our datasets. Future work will include additional programming languages and more diverse repositories to further improve gener-alizability.\nFuture work should consider expanding the scope of datasets and programming languages, employing more diverse evaluation metrics, and conducting experiments across multiple deployment contexts. Additionally, automated test augmentation techniques could be explored to reduce potential biases introduced by manual test creation."}, {"title": "7 RELATED WORK", "content": "In this Section, we summarise existing work on LLM for Software Engineering (LLM4SE) and its evaluation.\nLLM4SE. LLMs have demonstrated considerable potential across various software engineering tasks [19, 41], such as code genera-tion [8, 21, 23, 33, 40], code summarization [5, 6, 15, 42], test genera-tion [10-12, 25, 34-36, 39, 44, 47] and program repair [22, 43, 45, 49]. Their robust training on extensive code and textual data enables them to perform well in both understanding and generating code, making them invaluable tools in software engineering.\nLLM Evaluation. Evaluating LLMs is critical for understanding their capabilities, especially given their black-box nature. In the context of software engineering, evaluations have primarily focused on code comprehension and generation tasks [31, 32]. We only discuss existing work on code generation evaluation in the rest of this section.\nHumanEval [9] and MBPP[7] evaluate models on relatively sim-ple Python functions. More advanced benchmarks such as APPS [18] and ClassEval [14] have extended this to more complex problems and class-level code generation. However, these benchmarks typ-ically assess models on isolated tasks without considering the broader context of real-world coding environments. Recent bench-marks, CrossCoderEval [13], RepoBench [29] and RepoEval [48] focus on repository-level tasks, including code completion and project-oriented evaluations. These benchmarks, however, often lack comprehensive annotations necessary for the evaluation data."}, {"title": "8 CONCLUSION", "content": "In this paper, we introduced RepoMasterEval, a novel benchmark designed to evaluate the performance of code completion models in realistic and complex software development scenarios. Our em-pirical evaluation revealed that models generally performed better on simpler benchmarks like HumanEval and significant variability is observed in model performance between Python and TypeScript, highlighting the importance of optimizing models for challenging tasks and a broader range of programming languages. Furthermore, our study demonstrated a positive correlation between model per-formance on RepoMasterEval and online acceptance rates, validat-ing the relevance and effectiveness of RepoMasterEval in assess-ing practical usability of code completion models. These findings emphasize the importance of comprehensive benchmarks like Repo-MasterEval in accurately evaluating and guiding the optimization of code completion models, thereby driving the development of more robust and effective LLMs for software development. Future work will involve expanding the scope of RepoMasterEval to in-clude additional programming languages and incorporating more diverse evaluation metrics to further enhance its applicability and relevance."}]}