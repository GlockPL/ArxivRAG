{"title": "When Can We Solve the Weighted Low Rank Approximation Problem in Truly Subquadratic Time?", "authors": ["Chenyang Li", "Yingyu Liang", "Zhenmei Shi", "Zhao Song"], "abstract": "The weighted low-rank approximation problem is a fundamental numerical linear algebra problem and has many applications in machine learning. Given an \u00d7 n weight matrix W and anxn matrix A, the goal is to find two low-rank matrices U, V \u2208 Rnxk such that the cost of ||W (UVT \u2013 A)|| is minimized. Previous work has to pay \u03a9(n\u00b2) time when matrices A and W are dense, e.g., having \u03a9(n\u00b2) non-zero entries. In this work, we show that there is a certain regime, even if A and W are dense, we can still hope to solve the weighted low-rank approximation problem in almost linear n1+0(1) time.", "sections": [{"title": "INTRODUCTION", "content": "Weighted low-rank approximation problem [SJ03] is a fundamental numerical linear algebra problem related to matrix completion [LLR16], faster SVD decomposition [BJS14, BKS16], and principal component analysis [XXF+21]. It is a prevalent focus over recent years and has been widely applied in machine learning in broad practical applications, such as vision detection [CXT+22], representation learning [WWZ+19], image classification [PCP+23], language model [HHC+21], weather prediction [WTL18], and many more. We can formulate the weighted low-rank approximation problem as below.\nDefinition 1.1. Given two n \u00d7 n matrices A and W, the goal is to find two n \u00d7 k matrices U, V such that $||W \\circ (UV^T \u2013 A)||_F \\leq (1 + \\epsilon) \\min_{A':rank(A')=k} ||W \\circ (A' - A)||_F$. Here, $W \\circ B$ denotes a matrix with entries given by $W_{i,j}B_{i,j}$, where $\\circ$ is the Hadamard product operation.\nHere, we provide an example to show that attention computation can be formulated as a weighted low-rank approximation problem. Let Q, K, V \u2208 Rnxd be the query, key, and value matrix. Let A := Activation(QKT) \u2208 Rn\u00d7n be the attention matrix, e.g, using Softmax as activa- tion. Let W\u2208 Rn\u00d7n be the attention mask. Then, we have the attention output as (AW)V. We can clearly see that $A \\circ W$ is the target of the weighted low-rank approximation problem.\nThere is some well-known hardness in the weighted low-rank approximation problem. [GG11, RSW16] showed that, when we want to find an approximate solution with $ \\epsilon = 1/poly(n)$, the weighted low-rank approximation problem is \u201cequivalent to NP-hard problem. To practically fix it, under certain assumptions (e.g., the weighted matrix W has r distinct columns/rows for small r), we may have a polynomial time complexity algorithm to solve the weighted low-rank approximation problem. Thus, there are many theoretical and empirical works studying how we are able to solve the weighted low-rank approximation problem efficiently (e.g., [EVDH12, ZQZ+19, SYYZ23, WY24] and many more).\nHowever, the best previous works still need to solve weighted low-rank approximation in \u03a9(n\u00b2) if A has \u03a9(n\u00b2) non-zero entries and W has \u03a9(n\u00b2) non-zero-entries. Therefore, in this work, we raise a natural fundamental mathematical question:\nIs there some dense regime setting for A and W so that we can solve the weighted low-rank approximation problem in almost linear n time?\nThe answer is positive. When we have mild practical assumptions (see detail in Assumption 3.2), we can solve the weighted low-rank approximation problem in almost linear complexity, i.e., n1+0(1). We state our main results in the following, whose proof is in Section 7.\nTheorem 1.2 (Main result). Let A and W denote two n\u00d7 n matrices. Assume each entry of A and W needs n bits\u00b9 to represent, where \u03b3 = o(1). Let r be the number of distinct columns and rows of W (Definition 3.1). Assume $A\\circ W$ has at most r\u00b7p distinct columns and at most r\u00b7p distinct rows, where p = n\u00ba(1). Assume that $k^2r = O(\\log n/\\log \\log n)$. For every constant e > 0, there is an algorithm running in time, $n^{1+o(1)}$ time which outputs a factorization (into an n \u00d7 k and a k \u00d7 n matrix) of a rank-k matrix $\\hat{A}$ for which\n$||W \\circ (\\hat{A} - A) ||_F \\leq (1 + \\epsilon) OPT,$\nwith probability at least 0.99, where\n$OPT := \\min_{\\{A_k rank(A_k)=k\\}} ||W \\circ (A \u2013 A_k)||_F$"}, {"title": "RELATED WORK", "content": "Weighted Low Rank Applications\nWeighted low-rank techniques have been widely applied in broad machine learning applications such as grayscale-thermal detection_ [LWZ+16], object_detection [TWZL16, CXT+22], fault de- tection [DCZ+17], defect_detection [MWLZ20, JLD+20], background estimation [DL17b, DLR18], multi-task learning [FLCT18], robust visual representation learning [KJS15, WZX+18, WWZ+19], adversarial learning [LBBM19], image restoration [PSDX14, CYZ+20], image clustering and clas- sification [SJS+11, WL17, WLLZ18, FZCW21, FZC+22a, FZC+22b, KKK+23, PCP+23], robust principal component analysis [XXF+21], language models training [HHW+22], language model compression [HHC+21], weather prediction [WTL18], tensor training [CWC+21, ZWH+22], do- main generalization [SMF+24] and many more. Weighted low-rank techniques have also been widely used in signal processing for filter design and noise removal [LPW97, LHZC10, JYCL15].\nWeighted Low Rank in Attention Mechanism\nParticularly, a line of works shows that the attention matrix may have some low-rank prop- erty, even under softmax activation function by polynomial approximation methods, e.g., [AS23, KLL+25, LLS+25b, CHL+24, LLSS24b, LLS+24, LSS+24, LSSY24, LSSZ24, SWXL24, XSL24, AS24c, AS24a, AS24b, HSK+24, HWG+24, HWL+24a, HWL+24b]. Thus, under such conditions, we may use weighted low-rank approximations for transformers' attention acceleration. For a few distinct columns or rows, empirically, we see that the attention matrix has some good patterns [JLZ+24, CLS+25, LLS+25a, CLL+25, SMN+24]. In this work, we focus on the theoretical analysis and leave its empirical justification as future work."}, {"title": "PRELIMINARY", "content": "We define [n] := {1,2,..., n}. Let R denote the real numbers, and R>o denote the nonnegative real numbers. Let $||A||_*$ denote the spectral norm of matrix A. Let $||A||_F = \\sum_{i,j} A_{i,j}^2$ denote the Frobenius norm of A. Let W. A denote the entry-wise product of matrices W and A. Let $||A\\circ W||_F = \\sum_{i,j} W_{i,j}A_{i,j}^2$ denote the weighted Frobenius norm of A. Let nnz(A) denote the number of nonzero entries of A. Let det(A) denote the determinant of a square matrix A. Let AT denote the transpose of A. Let At denote the Moore-Penrose pseudo-inverse of A. Let A\u2212\u00b9 denote the inverse of a full rank square matrix A.\nFor the weight matrix W, we always use W*,j to denote the j-th column of W, and Wi,* to denote the i-th row of W. Let diag(W*,j) denote the diagonal matrix with entries from the vector W*,j. Let diag(Wi,*) denote the diagonal matrix with entries from the vector Wi,*\u00b7\nDefinition 3.1 (Distinct rows and columns). Given a matrix $R \\in \\mathbb{R}^{m\\times n}$, we have $R_{*,1}, R_{*,2},..., R_{*,n} \\in \\mathbb{R}^{m}$ as its column vectors, and $R_{1,*}, R_{2,*},..., R_{m,*} \\in \\mathbb{R}^{n}$ as its row vectors. We define r :=\n$|\\{R_{*,1}, R_{*,2},..., R_{*,n}\\}|$ and p := $|\\{R_{1,*}, R_{2,*},..., R_{m,*}\\}|$, where |\u00b7| means the cardinality of a set, i.e., the number of elements in a set. Then, we say R has r distinct columns and p distinct rows.\nAssumption 3.2. Let r, p\u2208 N+. We assume n x n matrix W has r distinct columns and rows. We also assume that $W \\circ A$ has rp distinct columns and rows.\nIn a real application, when the data has strong periodicity, e.g., signal processing, or uniformly draws from a small set, e.g., classification, or the data has a certain structure, e.g., block attention"}, {"title": "WARMUP", "content": "In this section, to demonstrate the new technique, we prove the following theorem.\nLemma 4.1. Given two n\u00d7n size matrices A and W, 1\u2264 k \u2264 n such that: Let each entry of A,W can be represented by n bits, with \u03b3 \u2208 (0,1); Let OPT be defined as Definition 3.6. Then, we can show\nPart 1. There is an algorithm that runs in 20(nklogn) time, and outputs a number \u039b such that OPT < \u039b < 2 OPT.\nPart 2. There is an algorithm that runs in 20(nklogn) time and returns U \u2208 Rn\u00d7k and V \u2208 Rn\u00d7k such that $||(UV^T \u2013 A) \\circ W ||_F \\leq 2 OPT$.\nProof. Proof of Part 1.\nWe can create 2nk variables to explicitly represent each entry of U and V. Let $g(x) = ||W \\circ (UV^T \u2013 A)||_F$. Let L = 2"}, {"title": "LOWER BOUND ON OPT", "content": "We assume that W has r distinct rows and r distinct columns. Then, we get rid of the dependence on n in the degree.\nTheorem 5.1 (Implicitly in [RSW16]). Assuming that W has r distinct rows and r distinct columns, each entry of A and W needs n bits to represent. Assume OPT > 0. Then we know that, with high probability,\n$OPT \\geq 2^{-n^{120(rk^2/\\epsilon)}}.$\nProof. We use Ai,\u2217 \u2208 Rn denote the i-th row of A. We use Wi,* \u2208 Rn to denote the i-th row of W. Let (U1)i,\u2217 denote the i-th row of U\u2081. For any n \u00d7 k matrix U\u2081 and for any k \u00d7 n matrix Z1, we have\n$||(U_1Z_1 \u2013 A) \\circ W||_F^2$\n$=\\sum_{i=1}^n || (U_1)_{i,*} Z_1 diag(W_{i,*}) - A_{i,*} diag(W_{i,*})||_2^2.$\nBased on the observation that W has r distinct rows, we use group $g_{1,1}, g_{1,2},\u2026\u2026, g_{1,r}$ to denote r disjoint sets such that\n$\\cup_{i=1}^r g_{1,i} = [n]$\nFor any i \u2208 [r], for any $j_1, j_2 \\in g_{1,i}$, we have $W_{j_1,*} = W_{j_2,*}$.\nThus, we can have\n$\\sum_{i=1}^n || (U_1)_{i,*} Z_1 diag(W_{i,*}) \u2013 A_{i,*} diag(W_{i,*})||_2^2$\n$= \\sum_{i=1}^r \\sum_{l \\in g_{1,i}} || (U_1)_{l,*} Z_1 diag(W_{l,*}) - A_{l,*} diag(W_{l,*})||_2^2.$\nWe can sketch the objective function by choosing Gaussian matrices S\u2081 \u2208 Rnx81 with s\u2081 = O(k/\u0454).\n$\\sum_{i=1}^n || (U_1)_{i,*}Z_1 diag(W_{i,*})S_1 - A_{i,*} diag (W_{i,*})S_1||_2^2.$\nLet $\\hat{U_1}$ denote the optimal solution of the sketch problem,\n$\\hat{U_1} = arg\\min_{U_1\\in R^{n\\times k}} \\sum_{i=1}^n || (U_1)_{i,*}Z_1 diag(W_{i,*})S_1 - A_{i,*} diag (W_{i,*})S_1||_2^2.$\nBy properties of S\u2081, plugging $\\hat{U_1}$ into the original problem, we obtain\n$\\sum_{i=1}^r \\sum_{l \\in g_{1,i}} || (\\hat{U_1})_{l,*} Z_1 diag (W_{l,*}) - A_{l,*} diag (W_{l,*})||_2^2$\n$\\leq (1 + \\epsilon) \\cdot OPT.$\nLet R denote the set of all S(g1,i) (for all i \u2208 [r] and |R| = r)\nNote that $\\hat{U_1}$ also has the following form, for each $l \\in L \\subset [n]$ (Note that |L| = rp.)\n$(\\hat{U_1})_{l,*} = A_{l,*} diag(W_{l,*})S_1 \\cdot (Z_1 diag(W_{l,*})S_1)^+$\n$= A_{l,*} diag(W_{l,*})S_1 \\cdot (Z_1 diag(W_{l,*})S_1)^T\n$\\cdot ((Z_1 diag(W_{l,*})S_1)(Z_1 diag(W_{l,*})S_1)^T)^{-1}.$\nRecall the number of different diag(We,*) is at most r.\nFor each k \u00d7 s\u2081 matrix $Z_1 diag(W_{e,*})S_1$, we create k \u00d7 s\u2081 variables to represent it. Thus, we create r matrices,\n$\\{Z_1 DWS1\\}_{i \\in R}.$\nFor simplicity, let $P_{1,i} \\in \\mathbb{R}^{k\\times s_1}$ denote $Z_1 diag(W_{i,*})S_1$. Then we can rewrite $\\hat{U_1}$ as follows\n$(\\hat{U_1})_{i,*} = A_{i,*} diag(W_{i,*})S_1 P_{1,i}^T \\cdot (P_{1,i}P_{1,i}^T)^{-1}.$\nIf $P_{1,i}P_{1,i}^T \\in \\mathbb{R}^{k\\times k}$ has rank-k, then we can use Cramer's rule to write down the inverse of $P_{1,i}P_{1,i}^T$.\nFor the situation, it is not full rank. We can guess the rank. Let $t_i \\leq k$ denote the rank of $P_{1,i}$. Then, we need to figure out a maximal linearly independent subset of columns of $P_{1,i}$. We can also guess all the possibilities, which is at most $2^{O(k)}$. Because we have r different $P_{1,i}$, the total number of guesses we have is at most $2^{O(rk)}$. Thus, we can write down $(P_{1,i}P_{1,i}^T)^{-1}$ according to Cramer's rule. Note that $(P_{1,i}P_{1,i}^T)^{-1}$ can be view as $P_a/P_b$ where $P_a$ is a polynomial and $P_b$ is another polynomial which is essentially $\\det(P_{1,i}P_{1,i})$.\nAfter $\\hat{U_1}$ is obtained, we will fix $\\hat{U_1}$ in the next round.\nFor any n \u00d7 k matrix U2, we can rewrite\n$|| (\\hat{U_1}U \u2013 A) \\circ W ||_F^2$\n$=\\sum_{i=1}^n || diag (W_{*,i}) \\hat{U_1} (U_2)_{*,i} \u2013 diag(W_{*,i}) A_{*,i}||_2^2.$\nBased on the observation that W has r columns rows, we use group $g_{2,1}, g_{2,2},\u2026\u2026,g_{2,r}$ to denote r disjoint sets such that\n$\\cup_{i=1}^r g_{2,i} = [n].$\nFor any i \u2208 [r], for any $j_1, j_2 \\in g_{2,i}$, we have $W_{*,j_1} = W_{*,j_2}$.\nNext, based on assumptions on W and A, we use $g_{2,i,1}, g_{2,i,2},\u2026\u2026,g_{2,i,p}$ to denote p groups such that\n$\\cup_{j=1}^p g_{2,i,j} = g_{2,i}$\nFor any i \u2208 [r], for any j \u2208 [p], for any $l_1, l_2 \\in g_{2,i,j}$, we have $(W_{*,l_1} \\circ A_{*,l_1}) = (W_{*,l_2} \\circ A_{*,l_2})$.\nLet $S(g_{2,i,j})$ denote the smallest index from set $g_{2,i,j}$\nThus, we can have\n$\\sum_{i=1}^n || diag (W_{*,i}) \\hat{U_1} (U_2)_{*,i} \u2013 diag(W_{*,i})A_{*,i}||_2^2$"}, {"title": "FEW DISTINCT COLUMNS", "content": "In this section, we try to estimate the OPT value.\nTheorem 6.1. Given a matrix A and W, each entry can be written using O(n) bits for \u03b3 > 0. Given A \u2208 Rn\u00d7n, W \u2208 Rn\u00d7n,1 \u2264 k \u2264 n. Assume that W has r distinct columns and rows. Then, with high probability, one can output a number \u039b in time $n^{1+\\gamma}\\cdot p \\cdot 2^{O(k^2r/\\epsilon)}$ such that\n$OPT \\leq \\Lambda \\leq (1 + \\epsilon) OPT .$\nProof. Let $U_1^*, U_2^* \\in R^{n \\times k}$ denote the matrices satisfying\n$||W \\circ (U_1^*(U_2^*)^T \u2013 A)||_F^2 = OPT.$"}, {"title": "RECOVER A SOLUTION", "content": "We state our results and proof of recovering a solution.\nTheorem 7.1. Given a matrix A and W, each entry can be written using O(n) bits for \u03b3 > 0. Given A \u2208 Rn\u00d7n,W \u2208 Rn\u00d7n,1 \u2264 k \u2264 n and \u0454 \u2208 (0,0.1). Assume W has r distinct columns and rows. Assume $A \\circ W$ has at most r\u00b7p distinct columns and at most r\u00b7p distinct rows. Let OPT be defined as Definition 3.6. Then, with high probability, one can output two matrices U, V \u2208 Rn\u00d7k in time $n^{1+\\gamma} \\cdot 2^{O(k^2r/\\epsilon)}$ such that\n$||(UV^T \u2013 A) \\circ W ||_F^2 \\leq (1 + \\epsilon) OPT .$\nFurther, if we choose (1) $k^2r = O(\\log n/\\log \\log n)$, (2) \u0454 \u2208 (0,0.1) to be a small constant, (3) p = n\u00ba(1) and (4) \u03b3 = o(1), then the running time becomes n1+o(1).\nProof. Here, we show how to recover an approximate solution, not only the value of OPT.\nThe idea is to recover the entries of U and V one by one and use the algorithm from the previous section for the corresponding decision problem. We initialize the semialgebraic set to be\n$S = \\{x \\in R^l | q(x) \\neq 0,p(x) \\leq \\Lambda q(x)\\}$.\nWe start by recovering the first entry of U. We perform the binary search to localize the entry, which takes log(2n) invocations of the decision algorithm. For each step of binary search, we use Theorem 3.5 to determine whether the following semi-algebraic set S is empty or not,\n$S \\cap \\{U_{1,1}(x) \\geq \\tilde{U_{1,1}}, U_{1,1}(x) \\leq \\hat{U_{1,1}}\\}$.\nAfter that, we declare the first entry of U to be any point in this interval. Then, we add an equality constraint that fixes the entry of U to this value and add a new constraint into S permanently, e.g., S \u2190 S \u2229 {U1,1(x) = \u00db1,1}. Next, we repeat the same with the second entry of U and so on. This allows us to recover a solution of cost at most (1 + \u20ac) OPT in time\n$n^{1+\\gamma} \\cdot p \\cdot 2^{O(k^2r/\\epsilon)}.$\nIf we choose \u03b3 = o(1), \u03b5 = \u0398(1), p = no(1) and $k^2r = O(\\log n/\\log \\log n)$, then the running time becomes n1+o(1). Thus, we complete the proof."}, {"title": "CONCLUSION", "content": "We showed that the weighted low-rank approximation problem could be solved in almost linear n1+0(1) time when the weighted matrix W has few distinct columns and rows and WoA has few distinct columns and rows. This demonstrates that truly subquadratic time is achievable for a dense regime not previously known to be tractable. Future work could generalize the assumptions and explore applications of the algorithm."}, {"title": "LIMITATIONS", "content": "Although our algorithm can achieve a good approximation ratio in theory, it may not achieve the ideal result in practical implementation due to various factors such as computational resource limitation, data quality problems, or incomplete model specification."}, {"title": "SOCIETAL IMPACT", "content": "In this paper, we introduce an algorithm that can solve weighted low-rank approximation prob- lems in near-linear time under certain conditions. Our paper is purely theoretical and empirical in nature (a mathematics problem), and thus, we foresee no immediate negative ethical impact. Our algorithms can improve the efficiency of data processing and model training, allowing complex algorithms to be run in resource-limited environments and accelerating scientific research and tech- nological innovation. By reducing computing requirements, energy consumption can be reduced, and the environment can benefit."}]}