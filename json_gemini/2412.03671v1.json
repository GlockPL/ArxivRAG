{"title": "Tight Lower Bounds and Improved Convergence in Performative Prediction", "authors": ["Pedram Khorsandi", "Rushil Gupta", "Mehrnaz Mofakhami", "Simon Lacoste-Julien", "Gauthier Gidel"], "abstract": "Performative prediction is a framework accounting for the shift in the data distribution induced by the prediction of a model deployed in the real world. Ensuring rapid convergence to a stable solution where the data distribution remains the same after the model deployment is crucial, especially in evolving environments. This paper extends the Repeated Risk Minimization (RRM) framework by utilizing historical datasets from previous retraining snapshots, yielding a class of algorithms that we call Affine Risk Minimizers and enabling convergence to a performatively stable point for a broader class of problems. We introduce a new upper bound for methods that use only the final iteration of the dataset and prove for the first time the tightness of both this new bound and the previous existing bounds within the same regime. We also prove that utilizing historical datasets can surpass the lower bound for last iterate RRM, and empirically observe faster convergence to the stable point on various performative prediction benchmarks. We offer at the same time the first lower bound analysis for RRM within the class of Affine Risk Minimizers, quantifying the potential improvements in convergence speed that could be achieved with other variants in our framework.", "sections": [{"title": "1 INTRODUCTION", "content": "Decision-making systems are increasingly integral to critical judgments in sectors such as public policy , healthcare , and education . However, as these systems become more reliant on quantitative indicators, they become vulnerable to the effects described by Goodhart's Law: \"When a measure becomes a target, it ceases to be a good measure\" . This principle is particularly relevant when predictive models not only forecast outcomes but also influence the behavior of individuals and organizations, leading to performative effects that can subvert the original goals of these systems.\nGiven these challenges, it is essential to develop predictive models that are not only accurate but also robust against the performative shifts they may provoke. The work by Perdomo et al. [2020] addresses this challenge within the framework of Repeated Risk Minimization (RRM), where they explore the dynamics of model retraining in the presence of performative feedback loops. In their approach, the authors propose an iterative method that adjusts the predictive model based on the distributional shifts caused by prior model deployments, aiming to stabilize the model performance despite the continuous evolution of the underlying data distribution. By characterizing the convergence properties of their method, they provide a theoretical guarantee for the stability of the model at a performative equilibrium.\nOur work extends this framework by leveraging the datasets collected at each snapshot during the retraining process, introducing a new class of algorithms called Affine Risk Minimizers. By utilizing historical data from previous updates, we show that it is possible to converge to a stable point for a broader class of problems that were previously unsolvable, extending beyond the bounds established in prior analyses . We derive a new upper bound under less restrictive assumptions than Mofakhami et al. [2023] and provide the first tightness analysis for the framework in Perdomo et al. [2020] as well as for our newly established rate. Our method, which incorporates historical datasets, demonstrates superior convergence properties both theoretically and experimentally.\nFaster convergence is of particular importance in scenarios where the data distribution is subject to continuous change. By achieving more rapid convergence, our framework ensures that the model stabilizes more quickly, minimizing the period during which predictions may be unreliable.\nContributions. \u2460 We establish a new upper bound, enhancing the convergence rate of RRM under less restrictive conditions; \u2461 We establish the tightness of the analysis in both our framework and the framework proposed by Perdomo et al. [2020]; \u2462 We introduce a new class of algorithms, named Affine Risk Minimizers, that provides convergence for a wider class of problems by utilizing linear combinations of datasets from earlier training snapshots; \u2463 We provide both theoretical and experimental enhancements, showcasing scenarios where this framework improves convergence; \u2464 Finally, we introduce the first technique for establishing theoretical lower bounds across each framework, detailing the maximum potential improvement in convergence rates achievable through the use of past datasets.\nScope. In this paper, we focus exclusively on the RRM framework for achieving performative stability. We do not explore other methods for performative stability, as our analysis and contributions are centered on enhancing the effectiveness of RRM within this context."}, {"title": "2 RELATED WORK", "content": "Performative prediction introduces a framework for learning under decision-dependent data and has been widely studied in various aspects, from stochastic optimization methods to find stable classifiers  to approaches that focus on performative optimal solutions, the minimizer of performative risk. In this work, we focus our analysis on performative stable solutions, whose deployment removes the need for repeated retraining in changing environments .\nOne of the main applications of this framework is strategic classification which involves deploying a classifier interacting with agents who strategically manipulate their features to alter the classifier's predictions and achieve their favorable outcomes. Strategic Classification has served as a benchmark in the literature of performative prediction , and we adopt this setting in our experiments to empirically demonstrate our theoretical contributions.\nPrior work in performative prediction either assumes the data distribution is a function of the parameters"}, {"title": "3 PERFORMATIVE STABILITY: RRM AND RGD", "content": "In the context of performative prediction, two primary frameworks are employed to address the challenge of shifting distributions due to the model's influence: Repeated Risk Minimization (RRM) and Repeated Gradient Descent (RGD).\nRepeated Risk Minimization (RRM) iteratively retrains the model on the distribution it induces until it converges to a performatively stable classifier. Formally, consider a model with parameters \\(\\theta\\in \\Theta\\), and a distribution \\(D(\\theta)\\) that depends on these parameters. The performative risk is defined as:\n\\[PR(\\theta) = E_{z\\sim D(f_\\theta)} [l(f_\\theta(x), y)]\\]\nwhere \\(l(f_\\theta(x), y)\\) is the loss function for a data point \\(z = (x, y)\\). A classifier is performatively stable if it minimizes the performative risk on the distribution it induces:\n\\[\\theta_{OPS} = \\arg \\min_{\\theta \\in \\Theta} E_{z\\sim D(f_{\\theta_{OPS}})} [l(f_\\theta(x), y)]\\]\nThe RRM framework updates the model parameters by solving:\n\\[\\theta^{t+1} = \\arg \\min_{\\theta \\in \\Theta} E_{z\\sim D(f_{\\theta^{t}})} [l(f_\\theta(x), y))]\\]\nuntil convergence, i.e., \\(\\theta^{t+1} \\approx \\theta^t\\).\nRepeated Gradient Descent (RGD), alternatively, performs gradient descent on the performative risk directly:\n\\[\\theta^{t+1} = \\theta^{t} - \\eta E_{z\\sim D(f_{\\theta^{t}})} [\\nabla_\\theta l(f_{\\theta^{t}} (x), y)]\\]\nwhere \\(\\eta > 0\\) is the learning rate. This method iteratively updates \\(\\theta\\) to reduce the performative risk.\nIn this work, our primary focus is on RRM, though several of our theorems extend to RGD as well."}, {"title": "4 IMPROVED RATES AND OPTIMALITY OF ANALYSIS", "content": "Both Perdomo et al. [2020] and Mofakhami et al. [2023] derive convergence rates for RRM under distinct assumptions. The assumptions made in these studies reflect the sensitivity of the distribution map \\(D(.)\\) to changes in the model and the structural properties of the loss function. Specifically, Perdomo et al. [2020] focuses on Wasserstein-based sensitivity and convexity with respect to the model parameters, while Mofakhami et al. [2023] introduces a framework with Pearson \\(\\chi^2\\)-based sensitivity and strong convexity with respect to the predictions. Building on these foundations, and motivated by Mofakhami et al. [2023] we now outline the assumptions for our framework:\nAssumption 1 \\(\\epsilon\\)-sensitivity w.r.t. Pearson \\(\\chi^2\\) divergence: The distribution map \\(D(f_\\theta)\\), with pdf \\(P_{f_\\theta}\\), maintains \\(\\epsilon\\)-sensitivity with respect to Pearson \\(\\chi^2\\) divergence. Formally, for any \\(f_\\theta, f_{\\theta'} \\in F\\):\n\\[\\chi^2(D(f_{\\theta'}), D(f_{\\theta})) \\leq \\epsilon ||f_\\theta - f_{\\theta'} ||_{f_\\theta}^2,\\]\nwhere\n\\[||f_\\theta - f_{\\theta'} ||_{f_\\theta}^2 := \\int ||f_\\theta(x) - f_{\\theta'}(x)||^2 P_{f_\\theta} (x) dx,\\]\nand\n\\[\\chi^2(D(f_{\\theta'}), D(f_{\\theta})) := \\int \\frac{(P_{f_{\\theta'}} (z) - P_{f_{\\theta}} (z))^2}{P_{f_{\\theta}} (z)} dz.\\]\nThis assumption, inspired by prior work on Lipschitz continuity on \\(D(.)\\), implies that if two models with"}, {"title": "5 USAGE OF OLD SNAPSHOTS", "content": "Our method introduces an alternative approach to improve convergence. Instead of relying solely on the current data distribution induced by \\(D(f_{\\theta^t})\\), we leverage datasets from previous training snapshots \\(\\{D(f_{\\theta^i})\\}_{i=0}^{t-1}\\). The updated framework optimizes model parameters over an aggregated distribution:\n\\[\\theta^{t+1} \\triangleq \\arg \\min_{\\theta \\in \\Theta} E_{(x, y)\\sim D_t} [l(f_\\theta(x), y)]\\]\nwhere \\(D_t\\) is an affine combination of previous distributions, formulated as:\n\\[D_t = \\sum_{i=0}^{t-1} \\alpha_i^{(t)} D(f_{\\theta^i}), \\quad s.t. \\sum_{i=0}^{t-1} \\alpha_i^{(t)} = 1\\]\nWe refer to this class of algorithms as Affine Risk Minimizers. As demonstrated in Appendix A (Lemma 8),"}, {"title": "6 LOWER BOUNDS FOR AFFINE RISK MINIMIZERS", "content": "In the previous section, we established the potential for convergence across a wider class of problems using Affine Risk Minimizers. This prompts the question of how much the convergence speed can be improved, which we address in this section.\nWe propose the first distinct lower bounds for the framework described in Section 4 and that of Perdomo et al. [2020] for the class of Affine Risk Minimizers. The lower bound for our framework is presented in the following section, while the corresponding result for Perdomo et al. [2020] is detailed in Section 6.1.\nTheorem 5 Suppose that Assumptions 1-4 hold. Then, there exists a problem instance in this regime, and for any algorithm in the Affine Risk Minimizers class, such that:\n\\[||f_{\\theta^t} - f_{\\theta_{OPS}}||_{f_{\\theta_{OPS}}} = \\Omega\\left(\\left(\\frac{1}{1+2}\\sqrt{\\frac{\\epsilon \\gamma}{CM}}\\right)^t\\right).\\]\nThis demonstrates that the convergence rate for the class of problems satisfying Assumptions 1-4 cannot exceed the given lower bound."}, {"title": "6.1 Lower Bound with Perdomo et al. [2020]'s Assumption", "content": "We show that the convergence rate for RRM provided in Equation 11 is optimal among the class of Affine Risk Minimizers up to a factor 2.\nTheorem 6 There exists a problem instance and an initialization \\(\\theta_0\\) following Assumption 5 such that for any algorithm in the Affine Risk Minimizers class, we have:\n\\[||\\theta - \\theta_{PS} || = \\Omega\\left(\\left(\\frac{\\beta \\epsilon}{\\gamma}\\right)^t\\right).\\]"}, {"title": "7 EXPERIMENTS", "content": "We conduct experiments in two semi-synthetic environments to evaluate whether aggregating past snapshots improves convergence to the performatively stable point. We present an empirical comparison of different aggregation windows for prior snapshots. At each time step t, we form Dt by aggregating the datasets from the training snapshots as\n\\[D_t = \\frac{1}{\\tau} \\sum_{i=t-\\tau+1}^{t} D(f_{\\theta^i}),\\]\nwhere we compare methods using various values of \\(\\tau\\), including \\(\\tau = 1,2,4,5\\), and 'all' (which includes all snapshots up to time t).\nWe first begin with a discussion on our evaluation metric, followed by detailed case-studies on both the credit scoring environment and the rideshare markets in subsequent subsections (7.1, 7.2).\nEvaluation Metric. Throughout our experiments, we focus on changes in loss as an effect of performativity. We define \\(\\Delta R_t\\), i.e. the loss shift due to performativity at time t, as the absolute difference in loss observed by a model before and after the data distribution has changed due to performative effects while keeping the model's state constant.\n\\[\\Delta R_t =|E_{z\\sim D(f_{\\theta^t})}[l(f_{\\theta^t}(x), y)] - E_{z\\sim D(f_{\\theta^{t-1}})} [l(f_{\\theta^t}(x), y)]|\\]\nThis metric allows for clearer comparisons between methods by minimizing overlap in the plots, unlike the performative risk (see Eq.1)."}, {"title": "7.1 Credit Scoring", "content": "Setup. Inspired by Mofakhami et al. [2023], we use the Resample-if-Rejected (RIR) procedure to model distribution shifts in a controlled experimental setting. This methodology involves users strategically altering their data to influence the classification outcome.\nLet us consider a base distribution with probability density function p and a function \\(g : f_\\theta(x) \\rightarrow g(f(x))\\) indicating the probability of rejection based on the prediction \\(f_\\theta(x) \\in R\\). The modified distribution \\(p_{f_\\theta}\\), under the RIR mechanism, evolves as follows:\n\\bullet Sample x from p.\n\\bullet With probability \\(1 - g(f(x))\\), accept and output x. Otherwise, resample from p.\nOur data comes from Kaggle's Give Me Some Credit dataset, which includes features \\(x \\in R^{11}\\) and labels \\(y \\in \\{0,1\\}\\), where \\(y = 1\\) indicates a defaulting applicant. We partition the features into two sets: strategic and non-strategic. We assume independence between strategic and non-strategic features. While non-strategic features remain fixed, the strategic features are resampled using the RIR procedure with a rejection probability \\(g(f(x)) = f_\\theta(x) + \\delta\\). We use a scaled sigmoid function after the second layer. This scales \\(f_\\theta(x)\\) to the interval \\([0,1 - \\delta]\\), ensuring that \\(g(f_\\theta(x)) \\in [\\delta, 1]\\) remains a valid probability. Further implementation details are available in Appendix I.\nTheorem 7 Let \\(f_\\theta(x) \\in [0,1- \\delta]\\) for all \\(\\theta \\in \\Theta\\), where \\(0 < \\delta < 1\\) is fixed. Then, for \\(g(f(x)) = f_\\theta(x) + \\delta\\), RIR is \\(\\epsilon\\)-sensitive as defined in Assumption 1 with \\(\\epsilon = O(\\delta^{-3})\\).\nThis result provides an example where our rate surpasses the rate previously derived in Mofakhami et al. [2023] (\\(O(\\delta^{-2})\\) within the same framework). Furthermore, for any value of M and \\(\\gamma\\), our rate can guarantee convergence for a wider class of problems. The proof of this theorem, along with justifications for the improved rate, is presented in Appendix H.\nResults. The outcomes of this case study are shown in Figure 3. For larger window sizes (\\(\\tau\\)), we omit the initial iterations in the figure because they follow the same update rule as smaller \\(\\tau\\) methods, leading to identical values.\nFigure 3 demonstrates the advantage of using older snapshots in the optimization process. As the window"}, {"title": "7.2 Revenue Maximization in Ride Share Market", "content": "Setup. This is a two-player semi-synthetic game between two ride-share providers, Uber and Lyft, both trying to maximize their respective revenues. Each player takes an action in this game by setting their price for the riders across 11 different locations in the same city of Boston, MA. The price set by one firm directly influences the demand observed by both firms. The demand constitutes the data distribution and at each time step, a total of 25 demand samples are sampled for a firm i and the optimal response is found by minimizing Equation 20 for a maximum of 40 retraining steps. The simulations use the publicly available Uber and Lyft dataset from Boston, MA on Kag-"}, {"title": "8 CONCLUSION", "content": "In this paper, we introduced a new class of algorithms for improved convergence in performative prediction by utilizing historical datasets from previous retraining snapshots. Our theoretical contributions include establishing a new upper bound for last-iterate methods, demonstrating the tightness of this bound, and surpassing existing lower bounds through the aggregation of historical datasets. We have also presented the first lower bound analysis for Repeated Risk Minimi-"}]}