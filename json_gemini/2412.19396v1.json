{"title": "Comparing Few to Rank Many:\nActive Human Preference Learning using Randomized Frank-Wolfe", "authors": ["Kiran Koshy Thekumparampil", "Gaurush Hiranandani", "Kousha Kalantari", "Shoham Sabach", "Branislav Kveton"], "abstract": "We study learning of human preferences from a\nlimited comparison feedback. This task is ubiq-\nitous in machine learning. Its applications such\nas reinforcement learning from human feedback,\nhave been transformational. We formulate this\nproblem as learning a Plackett-Luce model over a\nuniverse of N choices from K-way comparison\nfeedback, where typically K \u226a N. Our solution\nis the D-optimal design for the Plackett-Luce ob-\njective. The design defines a data logging policy\nthat elicits comparison feedback for a small col-\nlection of optimally chosen points from all $\\binom{N}{K}$\nfeasible subsets. The main algorithmic challenge\nin this work is that even fast methods for solv-\ning D-optimal designs would have $O(\\binom{N}{K})$ time\ncomplexity. To address this issue, we propose\na randomized Frank-Wolfe (FW) algorithm that\nsolves the linear maximization sub-problems in\nthe FW method on randomly chosen variables.\nWe analyze the algorithm, and evaluate it empiri-\ncally on synthetic and open-source NLP datasets.", "sections": [{"title": "1 Introduction", "content": "Learning to rank from human feedback is a fundamental\nproblem in machine learning. In web search [2, 21, 22],\nhuman annotators rate responses of the search engine, which\nare then used to train a new system. More recently, in\nreinforcement learning from human feedback (RLHF) [43,\n49], human feedback is used to learn a latent reward model,\nwhich is then used to align a large-language model (LLM)\nwith human preferences. Both discussed problems involve\na potentially large universe of N items, and the human can\nprovide feedback only on a small subset of them. In web\nsearch, the items are the retrieved web pages for a given\nquery and the annotators can only label a top few. In RLHF,\nthe items are LLM responses and the feedback is typically\npairwise [43, 49], although K-way extensions have also\nbeen studied [59]. In this work, we answer the question\nwhat human feedback to collect to learn a good model of\nhuman preferences for our motivating problems.\nWe formulate the problem as learning to rank N items, such\nas web pages or LLM responses, from a limited K-way hu-\nman feedback. In general, K < N because humans cannot\nprovide high-quality preferential feedback on a large num-\nber of choices [53]. When K = 2, we have a relative feed-\nback over two responses, known as the Bradley-Terry-Luce\n(BTL) model [8]. When K \u2265 2, we obtain its generaliza-\ntion to a ranking feedback over K responses, known as the\nPlackett-Luce (PL) model [45, 36]. To learn high-quality\npreference models, we ask humans questions that maximize\ninformation gain. Such problems have long been studied in\nthe field of optimal design [47, 7, 34]. An optimal design\nis a distribution over most informative choices that mini-\nmizes uncertainty in some criterion. This distribution can\nbe generally obtained by solving an optimization problem\nand often has some desirable properties, like sparsity. The\nfocus of our work is on solving large optimal designs, with\nbillions of potential choices for K-way feedback.\nWe make the following contributions:\n(1) We propose a general framework for collecting human\nfeedback to learn to rank N items from K-way feedback\n(Sections 2 and 3), where K < N. This framework is more\ngeneral than in prior works [40, 39, 12].\n(2) We bound the generalization error of the model learned\nfrom human feedback and analyze the resulting ranking loss\n(Section 3.2). The generalization error and ranking loss\ndecrease with more human feedback."}, {"title": "2 Setting", "content": "We start with introducing notation. Let [n] = {1,...,n}\nand Rd is the d dimensional real vector space. Let \u25b3S be the\nprobability simplex over set S. For any distribution \u03c0\u0395\u0394,\nwe have $\\Sigma_{i\\in S} \\pi(i) = 1$. Let $||x||_A = \\sqrt{x^T A x}$ for any\npositive-definite matrix A \u2208 Rd\u00d7d and vector x \u2208 Rd. We\nsay that a square matrix M\u2208 Rdxd is PSD/PD if it is\npositive semi-definite/positive definite.\nWe consider a problem of learning to rank N items. An item\nk \u2208 [N] is represented by its feature vector xk \u2208 X, where\nXRd is the set of all feature vectors. The relevance of\nan item is given by its mean reward. The mean reward of\nitem k is $x_k^T \\theta_*$, where \u03b8\u03b1 \u2208 Rd is an unknown parameter.\nSuch models of relevance have a long history in learning to\nrank [60, 35]. Without loss of generality, we assume that\nthe original order of the items is optimal, $x_j^T \\theta_* > x_k^T \\theta_*$ for\nany j < k.\nWe interact with a human T times. In interaction l \u2208 [T],\nwe select a K-subset of items Se \u2208 S and the human ranks\nall items in Se according to their preferences, where S is\na collection of all K-subsets of [N]. Note that |S| = $\\binom{N}{K}$.\nWe represent the ranking as a permutation \u03c3\u03b5 : [K] \u2192 Se,\nwhere \u03c3\u03b5(k) is the item at position k. The probability that\nthis permutation is generated is\n$P(\\sigma_e) = \\prod_{k=1}^{K} \\frac{exp[x_{\\sigma_e(k)}^T \\theta_]}{ \\sum_{j=k}^{K} exp[x_{\\sigma_e(j)}^T \\theta_]} =  \\frac{exp[\\sum_{k=1}^K x_{\\sigma_e(k)}^T \\theta_]}{(\\sum_{j \\in S_e}) }$. (1)\nIn short, items with higher mean rewards are more preferred\nby humans and thus more likely to be ranked higher. This\nfeedback model is known as the Plackett-Luce (PL) model\n[45, 36, 59], and it is a standard model for learning unknown\nrelevance of choices from relative feedback.\nOur goal is to select the subsets such that we can learn the\ntrue order of the items. Specifically, after T interactions,\nwe output a permutation \u00f4 : [N] \u2192 [N], where \u00f4(k) is the\nindex of the item placed at position k. The quality of the\nsolution is measured by the ranking loss\n$R(T) = \\frac{2}{N(N-1)} \\sum_{j=1}^{N} \\sum_{k=j+1}^{N} 1{\\{\\theta(j) > \\theta(k)\\}\\} ,$ (2)\nwhere N(N \u2212 1)/2 is a normalizing factor that scales the\nloss to [0, 1]. Simply put, the ranking loss is the fraction of\nincorrectly ordered pairs of items in permutation \u00f4. It can\nalso be viewed as the normalized Kendall tau rank distance\n[26] between the optimal order of items and that according\nto \u00f4, multiplied by 2/(N(N \u2212 1)). While other objectives\nare possible, such as the mean reciprocal rank (MRR) and\nnormalized discounted cumulative gain (NDCG) [37], we\nfocus on the ranking loss in (2) to simplify our presentation.\nFor completeness, we report the NDCG in our experiments.\nTo simplify exposition, and without loss of generality, we\nfocus on sorting N items in a single list. Our setting can\nbe generalized to multiple lists as follows. Suppose that we\nwant to sort multiple lists with N\u2081,..., NM items using K-\nway feedback. This can be formulated as a sorting problem"}, {"title": "3 Optimal Design for Learning To Rank", "content": "This section presents our algorithm and its generalization\nanalysis. These generalize the work of Mukherjee et al. [40]\nto ranking N items from K-way feedback, from ranking\nlists of length K only. The fundamental novel challenge is\nthe computational cost of solving (7). In particular, since\n|S| = $\\binom{N}{K}$, (7) has exponentially many variables in K.\nTherefore, it cannot be written out and solved using standard\ntools, as was done using CVXPY [13] in Mukherjee et al.\n[40]. We address this challenge separately in Section 5.\n3.1 Optimal Design\nSuppose that the human interacts with T subsets {Se}l\u2208[T]\n(Section 2). We use the human responses \u03c3\u03b5 on Se to esti-\nmate 0. Specifically, since the probability of \u03c3\u03b5 under \u03b8* is\n(1), the negative log-likelihood of all feedback is\n$L_T(\\theta) =  -\\sum_{l=1}^T \\sum_{k=1}^K log \\frac{exp[x_{\\sigma_e(k)}^T \\theta]}{ \\sum_{j=k}^{K} exp[x_{\\sigma_e(j)}^T \\theta]} $. (3)\nTo estimate \u03b8*, we solve a maximum likelihood estimation\n(MLE) problem, $\\hat{\\theta_*} = arg \\min_{\\theta \\in \\Theta} L_T(\\theta)$. The problem (3)\ncan be solved by gradient descent with standard or adaptive\nstep sizes [41]. Finally, we estimate the mean reward of\nitem k as $x_k^T \\hat{\\theta_*}$ and sort the items in descending order of\n$x_k^T \\hat{\\theta_*}$, which defines \u00f4 in (2).\nThe problem of selecting informative subsets Se is for-\nmulated as follows. For any z \u2208 Rd and PSD matrix\nM \u2208 Rd\u00d7d, the prediction error at z can be bounded using\nthe Cauchy-Schwarz inequality as\n$|z^T(\\theta - \\theta_*)| \\leq ||z||_{M^{-1}} ||\\theta - \\theta_*||_{M} $. (4)\nIt remains to choose an appropriate matrix M. Following\nZhu et al. [59], we set\n$M = \\frac{e^{-4}}{2K(K-1)} \\sum_{l=1}^T \\sum_{j=1}^K \\sum_{k=j+1}^K z_{l,j,k}z_{l,j,k}^T, $ (5)\nwhere $z_{l,j,k} = X_{\\sigma_e(j)} - X_{\\sigma_e(k)}$ is the difference between\nfeature vectors of items j and k in interaction l. We make\nan assumption to relate M to the Hessian of LT(0).\nAssumption 1. For all k \u2208 [N], $||x_k||_2 \\leq 1$. In addition,\n$||\\theta||_2 \\leq 1$ and $||\\theta_*||_2 \\leq 1$.\nThese kinds of assumptions are standard in the analy-\nsis of MLE of PL models [41]. Under Assumption 1,\n$\\nabla^2 L_T(\\theta) \\geq M$, where \u2207\u00b2LT(0) is the Hessian of LT(0).\nSince M is PSD, L\u012b is strongly convex at 9, and this gives\nus a \u00d5(\u221ad) high-probability upper bound on the second\nterm in (4) [59]. To control the first term, we note that the\nminimization of maxzez ||2||M-1, where Z is the set of\nall feature vector differences, is equivalent to maximizing\nlog det(M) [31]. This maximization problem is known as\nthe D-optimal design.\nWe solve the problem as follows. Each subset S \u2208 S is rep-\nresented by a matrix As defined as As = (zj,k)(j,k)\u2208\u03a0\u2082(S),\nwhere $z_{j,k} = x_j - x_k$ is the difference of feature vectors of\nitems j and k, and\n$\\Pi_2(S) = \\{(j, k) : j < k; j, k \\in S\\}$ (6)\nis the set of all pairs in S where the first entry has lower\nindex than the second one. Thus matrix As has d rows and\nK(K-1)/2 columns. Equipped with these matrices, we\nsolve the following (determinant) D-optimal design [47]\n$\\pi^* = arg \\max_{\\pi \\in \\Delta_{S}} g(\\pi), where$\n$g(\\pi) = log det (V_{\\pi}), and V_{\\pi} = \\sum_{S \\in S} \\pi(S) A_S A_S^T ,$ (7)\nwhere \u03c0\u2208 \u2206S is a probability distribution over the subsets\nin S, AS is the simplex of all such distributions, and \u03c0(S)\ndenotes the probability of choosing the subset S under \u03c0.\nNote that we could have indexed \u3160 by an integer defined\nthrough a bijective mapping C : S \u2192 [$\binom{N}{K}$] from the subsets\nto natural numbers. We do not do this to simplify presen-\ntation. The optimization problem is concave since log det\nis concave for PSD matrices and all $\u03c0(S)A_SA_S^T$ are PSD\nby design. Further, its solution is sparse [31]. Therefore,\nfast convex optimization methods, such as the Frank-Wolfe\nmethod, can be used to solve it. After \u03c0* is computed, the\nhuman feedback is collected on subsets sampled from \u03c0*,\ni.e. Se ~ \u03c0*.\n3.2 Generalization Analysis\nHere we provide the generalization guarantee for the D-\noptimal design. We start with proving that the differences in\nestimated item relevance under l\u00ea converge to those under 0*\nas the sample size T increases. The proof is under two as-\nsumptions that are not essential and only avoiding rounding.\nWe also make Assumption 1 throughout this section.\nProposition 1. Let K be even and N/K be an integer. Let\nthe feedback be collected according to \u03c0\u2217 in (7). Then with\nprobability at least 1 \u2013 8,\n$\\sum_{j=1}^N \\sum_{k=j+1}^N ((\\theta_e^* - \\theta_e)^T z_{j,k})^2 = O(\\frac{N^2 K^4 \\log(1/\\delta)}{T})$\nProof. We build on Theorem 5.3 of Mukherjee et al. [40],"}, {"title": "4 Frank-Wolfe Method for Optimal Design", "content": "Historically, the Frank-Wolfe (FW) method has been\nutilized as a scalable algorithm for solving the traditional D-\noptimal design problem [28, 58]. When instantiating the\nFW method for our problem (7) we get Algorithm 1. FW\nconsists of primarily two high-level steps. First, it computes\nthe gradient Gt at the current iterate (Line 2 of Algorithm 1)\nand then it finds the distribution t which maximizes the lin-\near functional defined by $G_t$, i.e. $\\max_{\\pi \\in \\Delta_S} (G_t, \\pi)$ (Line 3\nof Algorithm 1) using a Linear Maximization Oracle (LMO).\nFinally, it updates the iterate with a convex combination of\nthe current iterate \u03c0\u03c4 and t with a stepsize at chosen so\nthat it maximizes $\\max_{\\alpha} g((1 \u2013 \\alpha)\\cdot\\pi_t + \\alpha\\cdot\\pi_t)$ (Line 4 of\nAlgorithm 1) using a linear search algorithm. It is known\nthat this method converges to the maximizer of (7) [58].\nWhile LMO step appears simple, it requires computing\nthe gradient w.r.t. an $\\binom{N}{K}$ dimensional vector and finding\nits maximimum entry. When N > K this has an expo-\nnential computational complexity $\u03a9(N^K)$ in K. The line\nsearch is also computationally expensive because it requires\ncalculating the objective at many feasible iterates and the\nobjective calculation involves computing the log det of a\nd \u00d7 d matrix which requires O(d\u00b3) operations in most prac-\ntical implementations. In Section 5 we address these and\nother limitations through a new algorithm, DopeWolfe with\na better per-iteration complexity in terms of both K and d."}, {"title": "5 DopeWolfe: Randomized Frank-Wolfe\nwith Low-Rank and Sparse Updates", "content": "Here we present and analyze our proposed algorithm\nDopeWolfe (Algorithm 2) for solving the D-optimal de-\nsign problem (7). Note that pseduocodes of some of the\nsub-routines used this algorithm are given Algorithm 2 in\nthe Appendix. DopeWolfe is a fast randomized variant of\nthe FW method which incorporates computationally effi-\ncient low-rank and sparse operations. Next, we describe\nvarious components of the algorithm and explain how they\naddress the scaling concerns identified in Section 4.\n5.1 Randomized LMO and Cached Derivatives\nIn Section 4, we saw that computation of the LMO has\n$\\binom{N}{K}$ complexity due to the maximization step over S (8).\nUtilizing a randomized variant of FW method [27], we\nreduce this complexity to R by restricting the maximization\nto an R sized sub-collection Rt chosen uniformly at random\nfrom S (Lines 5 and 7 of Algorithm 2):\n$S_t \\in arg \\max_{S \\in R_t} G_t(S), where$\n$R_t \\thicksim Uniform(\\{R \\subseteq S | |R| = R\\}).$ (9)\nThis randomization of LMO also implies that we only\nneed to compute R partial gradients $G_t(S) =  \\frac{\\delta g(\\pi_t)}{\\delta \\pi_t(S)}$\nfor the subsets $S\\in R_t$. Next we see how to com-\npute them efficiently. Recall that $g(\\pi) = log det (V)$\nwith $V = \\sum_{S \\in S} \\pi(S)A_SA_S^T$. Then using the fact that\nlog det(U) = $U^{-1}$ for PSD matrix U the partial derivative\nat any S can be written as\n$\\frac{\\delta g(\\pi)}{\\delta \\pi(S)} = (A_SA_S^T, V^{-1}) =  \\sum_{ \\forall j, k \\in  \\Pi_2(S)} z_{j,k}^TV_t^{-1}z_{j,k} = \\sum_{ \\forall j, k \\in  \\Pi_2(S)} D_{j,k}. $ (10)\nNote that all the partial derivatives are a sum of $\\binom{K}{2}$ terms\nchosen from the $\\binom{N}{2}$ terms in ${D_{j,k}}_{(j,k)\\in \\Pi_2([N])}$. There-\nfore, PartialGrad sub-routine first computes and caches\n${D_{j,k}}_{(j,k)}$ (Line 14 in Algorithm 2) and then it combines\nthem to produce the partial derivative corresponding to each\nS\u2208 Rt (Line 16 in Algorithm 2). Assuming that $V_1^{-1}$\nis known, this reduces our overall LMO complexity from\n$O((\\binom{N}{K})(\\binom{N}{2})d^2)$ to $O((\\binom{N}{2})d^2 +R(\\binom{K}{2}))$. Please note that these\noperations are further parallelized in our code.\n5.2 Line Search with Low-Rank and Sparse Updates\nAs noted earlier in Section 4, line search: $\\max_\\alpha g((1 \u2013 \\alpha) \u00b7\n\\pi_t + \\alpha\\cdot e_{S_t})$ is another expensive operation. DopeWolfe\nsolves it using the GoldenSearch sub-routine (Line 8 of Al-\ngorithm 2). GoldenSearch (see Algorithm 4 in Appendix)\nsolves it as 1-D unimodal maximization problem using the\ngolden-section search method [30] which reduces the search\nspace by a factor of $\\varphi$\u2014the golden ratio\u2014at each iteration.\nIt computes the maximizer up to tolerance of atol after\n$1 + \\frac{log\\frac{1}{atol}}{log\\frac{1}{\\varphi}}$ objective computations. Earlier we also\nnoted that naively computing the objective value has a com-\nplexity of O(d\u00b3). However, notice that for an a we can\nwrite the update to V as\n$(1 \u2013 \\alpha)V + \\alpha A_SA_S^T$. (11)\nLet $r = \\binom{K}{2} < d$. Then we see that the second term\nabove is a low r rank matrix since As \u2208 Rd\u00d7r. Assuming\naccess to $V_t^{-1}$, this allows us to derive the objective\nvalue as the log det of anr\u00d7r matrix. The derivation is given\nin Appendix C. This reduces the over overall line search\ncomplexity to $O((r\u00b3 + rd^2) log_{\\varphi} (atol))$ from $O(d\u00b3 log_{\\varphi} (atol))$.\nBoth the partial derivative and the objective value compu-\ntations require the inverse of $V_{t+1}$. DopeWolfe initialized\nit in the variable Vinv (Line 3 of Algorithm 2) and up-\ndates it using UpdateInverse (Line 10 of Algorithm 2)."}, {"title": "6 Experiments", "content": "We conduct two experiments on three real-world datasets.\nThe first experiment is with simulated feedback, for different\nsample sizes T and K in the K-way feedback. We call this\nsynthetic feedback setup. The second is with real feedback,\nwhere preference feedback present in the dataset is used.\nWe refer to it as real feedback setup.\nWe experiment with (a) BEIR-COVID\u00b9, (b) Trec Deep\nLearning (TREC-DL)\u00b2, and (c) NECTAR\u00b3 datasets. These\nare question-answering datasets, where the task is to rank\nanswers or passages by relevance to the question. Each ques-\ntion in BEIR-COVID and TREC-DL has a hundred potential\nanswers. Each question in NECTAR has seven potential\nanswers. All the following experiments are conducted on\n3.5 GHz 3rd generation Intel Xeon Scalable processors with\n128 vCPUs and 1TB RAM.\n6.1 Synthetic Feedback Setup\nIn this section, we provide empirical evidence demonstrat-\ning that DopeWolfe, when applied to sampling K-sized\nsubsets from a list, enables the learning of an improved\nranking model across various values of K and sample sizes\nT. We first compute 384-dimensional dense BERT embed-\ndings [50] for each question and answer, and then reduce\nthe embedding dimensions to 10 by fitting UMAP [38] on\nthe answers in a dataset. The same UMAP transformation\nis applied to questions to get 10-dimensional embedding\nof the questions. Let q and a be the projected embeddings\nof a question and an answer to it. Then, we consider the"}]}