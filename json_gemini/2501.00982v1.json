{"title": "Are LLMs effective psychological assessors? Leveraging adaptive RAG for interpretable mental health screening through psychometric practice", "authors": ["Federico Ravenda", "Seyed Ali Bahrainian", "Andrea Raballo", "Antonietta Mira", "Noriko Kando"], "abstract": "In psychological practice, standardized questionnaires serve as essential tools for assessing mental constructs (e.g., attitudes, traits, and emotions) through structured questions (aka items). With the increasing prevalence of social media platforms where users share personal experiences and emotions, researchers are exploring computational methods to leverage this data for rapid mental health screening. In this study, we propose a novel adaptive Retrieval-Augmented Generation (RAG) approach that completes psychological questionnaires by analyzing social media posts. Our method retrieves the most relevant user posts for each question in a psychological survey and uses Large Language Models (LLMs) to predict questionnaire scores in a zero-shot setting. Our findings are twofold. First we demonstrate that this approach can effectively predict users' responses to psychological questionnaires, such as the Beck Depression Inventory II (BDI-II), achieving performance comparable to or surpassing state-of-the-art models on Reddit-based benchmark datasets without relying on training data. Second, we show how this methodology can be generalized as a scalable screening tool, as the final assessment is systematically derived by completing standardized questionnaires and tracking how individual item responses contribute to the diagnosis, aligning with established psychometric practices.", "sections": [{"title": "Introduction", "content": "According to the World Health Organization (WHO), one in seven adolescents experiences a mental health disorder (Wiederhold, 2022), with depression, anxiety, and behavioral disorders leading among young people. Following COVID-19, mental health conditions surged, with depressive disorders increasing by 28% in 2020 (Kieling et al., 2011; Winkler et al., 2020). Given the extent of this need, the WHO Special Initiative for Mental Health prioritizes improving and expanding access to quality mental health interventions and services as a key strategic goal.\nPsychological questionnaires play a crucial role in describing mental states by measuring various psychological constructs, as outlined in the Diagnostic and Statistical Manual of Mental Disorders (DSM) (Hopwood et al., 2012). The conventional interpretation of data derived from psychometric scales assumes that obtained scores reflect the intensity of a respondent attitudes. Psychological questionnaires can be used to assess various constructs related to different mental disorders as screening methods to develop an initial clinical profile.\nIn this work, we focus on two widely used standardized psychological questionnaires, specifically the BDI-II (Beck et al., 1996) for depression screening and the Self-Harm Inventory (SHI) (Sansone and Sansone, 2010) for self-harm behaviour detection. Both are self-reported surveys where overall scores correspond to specific severity levels of the respective conditions.\nIncreasingly, people turn to social media as a space to discuss their feelings and experiences and to find support (Bucci et al., 2019; Naslund et al., 2016). Numerous initiatives have emerged to analyze social media content for health monitoring using NLP techniques, including CLPsych (Tsakalidis et al., 2022) and eRisk (Losada et al., 2017), through organized completion tasks. In this work, we use datasets from previous eRisk editions to validate our approach.\nRecently, several studies have shown that closed-source LLMs struggle to achieve comparable results to state-of-the-art (SOTA) supervised methods in mental disorders classification tasks, both in zero-shot (Amin et al., 2023) and few-shot (XU"}, {"title": "Related Works", "content": "Recent advancements in NLP have enabled the development of new and complex models across various areas, particularly in digital and mental health. In recent years, transformer-based models (Vaswani et al., 2017) have emerged as powerful tools for predicting mental health disorders, leveraging vast amounts of textual data available from social media platforms. (Pourkeyvan et al., 2024; Raj et al., 2024) employed different BERT models for depression detection on social media platforms, while (Ji et al., 2022) developed Mental-BERT, a specialized BERT model pre-trained on mental health-related content from Reddit, specifically designed to capture language patterns associated with mental disorders. Regarding LLMs, (Yang et al., 2024a) introduced MentaLLaMA, an open source LLM specifically designed for interpretable mental health analysis on social media, building a multitask and multi-source instruction dataset to support model fine-tuning and evaluation. Furthermore, (Yang et al., 2024b) presented a comprehensive analysis on how LLMs can enhance mental health care through real-time feedback and early intervention.\nConcerning the use of NLP methods to predict psychological questionnaires' responses, previous works (Elourajini and A\u00efmeur, 2022; Vu et al., 2020) used neural models to predict the 5 main personality traits (Goldberg, 2013) and the Myers-Briggs type indicator (MBTI) (Schweiger, 1985; Yang et al., 2021) based on user-generated textual posts and comments. (Rosenman et al., 2024) propose an approach in which an LLM impersonates the interviewee, completing structured questionnaires, and then these responses are encoded as features to train a Random Forest model to predict the scores of the questionnaire's items.\nWith regard to eRisk data, recent approaches have been used to predict BDI-II responses using advanced computational methods. (P\u00e9rez et al., 2023) implemented a computational approach capable of rapidly screening depressive disorders on social networks, using a retrieval pipeline and a combination of semi- and supervised models, implementing item-based classifiers to predict responses to different items. Recently, (Ravenda et al., 2025) proposed a probabilistic neural architecture that uses a retrieval pipeline to select the most relevant posts for each user as input and predicts the final score using a Poisson distribution to better handle the ordinal nature of Likert scales. These latter two works differ from earlier approaches by incorporating a retrieval pipeline to filter the most relevant social media posts for each user and item before prediction, whereas previous methods fixed the number of posts per model, not considering their relevance. Additionally, while MBTI is a questionnaire with binary responses (binary classification), BDI-II uses a 4-point Likert scale, introducing an additional layer of complexity due to its ordinal nature.\nOur work differs from previous approaches by focusing on a completely unsupervised scenario, leveraging LLMs in zero-shot contexts. The idea is to implement a retrieval pipeline to filter the most relevant posts and use LLMs to predict scores, linking the semantic content of posts to that of questionnaire items. To evaluate the effectiveness of LLMS in such tasks, we use two eRisk collections from the 2019 and 2020 editions (Losada et al., 2019, 2020), which contain the post-history of Reddit users alongside their completed BDI-II questionnaires. After demonstrating the effectiveness of this unsupervised approach, we extend it to differ-"}, {"title": "Research Questions", "content": "In the following, we state the main Research Questions of our work:\n(RQ1.) Is it possible to fill a psychological questionnaire based on a user's Reddit post history using a RAG-based approach in a completely unsupervised context? How does this approach compare in terms of performance to SOTA models for the considered datasets?\n(RQ2.) How does the model effectiveness vary with changes in:\n(RQ2a.) The LLM being used. To answer this question, we consider 4 LLMs: 2 open-source (Phi-3-mini and Phi-3.5-mini) and 2 closed-source (Claude-3.5-Sonnet and gpt-40-mini).\n(RQ2b.) The prompting strategy we employ. Specifically, we use both a direct prompting approach and Chain-of-Thought (CoT) (Wei et al., 2022) prompting.\n(RQ2c.) The dense retrieval models. These are used to retrieve the most relevant posts for each user in response to each survey item.\n(RQ3.) Can our approach-completing a standardized psychological questionnaire to obtain a \u201cpsychological explanation\" of why a user is associated with a certain risk levels-be extended to other mental disorders where no ground truth is available?\n(RQ4.) What are the benefits of our psychological-guided approach compared to an approach that relies exclusively on prompting?\nProblem Definition. Computational approaches for predicting mental disorders from textual data, formulated as $f(Text) \\rightarrow Y$ (Kim et al., 2020; Sekuli\u0107 and Strube, 2019), where Text represents textual information like social media posts or interview transcriptions, face limitations in interpretability and generalizability across contexts (Paris et al., 2012; Friginal et al., 2017). These models, often neural, require large datasets to perform well. To address these limitations, we propose a method guided by psychological practice that leverages standardized questionnaires to assess the presence and/or intensity of mental disorders. The prediction is framed as a function correlating text and questionnaire items, $f(Text, Item_i) \\rightarrow S_i$, where $S_i$ represents the user's score for item i. The combination of these individual scores defines a final score used to diagnose symptoms as $\\sum_{i} f(Text, Item_i) \\rightarrow Y$. This approach not only provides an initial diagnosis but also enhances outcome's interpretability by linking the final score to specific questionnaire items, offering a clear reasoning for the prediction. The proposed method comprises two main steps, illustrated in Figure 1: (1) retrieving the most relevant documents for each item using an adaptive dense retrieval approach; (2) generating responses in a zero-shot setting with LLMs, using the retrieved documents as context."}, {"title": "Methodology", "content": "For the analyses in this study", "disorders": "depression and self-harm", "categories": "those showing signs of depression and control users", "early detection\" task. The latter two datasets contain users post histories and responses to a standardized self-reported psychological questionnaire, the BDI-II, used for screening depressive disorders. These datasets belong to the \"Measuring the severity of depression\" task. Regarding self-harm, datasets contain social media posts and comments from users divided into two categories": "users showing signs of self-harm and control users. Only posts preceding the user entry into the self-harm community are collected, in order to identify early signals and behavioral patterns that precede explicit help-seeking behavior, when intervention could be more effective and timely. The control group consists of both random users and users who actively participated in self-harm discussions (e.g., because they had a relative suffering from it). This dataset belongs to the \"early detection\" task."}]}