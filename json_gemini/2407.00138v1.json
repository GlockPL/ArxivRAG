{"title": "Analyzing Quality, Bias, and Performance in Text-to-Image Generative Models", "authors": ["Nila Masrourisaadat", "Nazanin Sedaghatkish", "Fatemeh SarsharTehrani", "Edward A. Fox"], "abstract": "Advances in generative models have led to significant interest in image synthesis, demonstrating the ability to generate high-quality images for a diverse range of text prompts. Despite this progress, most studies ignore the presence of bias. In this paper, we examine several text-to-image models not only by qualitatively assessing their performance in generating accurate images of human faces, groups, and specified numbers of objects but also by presenting a social bias analysis. As expected, models with larger capacity generate higher-quality images. However, we also document the inherent gender or social biases these models possess, offering a more complete understanding of their impact and limitations.", "sections": [{"title": "1 Introduction", "content": "This paper builds upon and extends the methodologies and results reported in a recent thesis [29].\nThe landscape of machine learning has been considerably shaped by the types of data available for model training. Traditional supervised machine learning models are predominantly trained on static datasets, which have several inherent limitations. These datasets often suffer from data sparsity, privacy issues, various forms of bias, and under-representation of minority classes [20]. Consequently, models trained on such datasets exhibit limitations that render them less applicable for real-world scenarios, particularly in critical domains such as healthcare, finance, and education.\nIn response to these challenges, the scientific community has turned its attention to synthetic data as a viable alternative [27, 12]. Recent advances in the field \u2013 particularly in text-to-image diffusion models such as Stable Diffusion [36], DALL.E 2 [34], LAFITE [58], and others \u2013 have shown remarkable potential in generating high-quality synthetic data. These types of models not only contribute to image emulation but also have broad application in domains like audio generation and text generation, thereby offering holistic solutions to the limitations of static datasets.\nDespite these advances in synthetic data generation, it is essential to acknowledge the challenges that these methods bring to the forefront. While they may alleviate some of the shortcomings of static datasets, synthetic data generation techniques are not without their own issues. Bias, under-representation, and other ethical considerations continue to be pertinent challenges. In some cases, these techniques may even exacerbate existing societal biases, which must be addressed for these models to be effectively and ethically employed in real-world applications. Further, the quality of synthetic data generated by these models can vary significantly, depending on factors such as the complexity and specificity of the text prompts, including when generating human facial images or demonstrating motion in visual content.\nOur work tackles these challenges by providing qualitative and quantitative analyses of the issues posed by employing text-to-image models for synthetic data generation. We scrutinize not only the"}, {"title": "2 Related work", "content": "Evaluation metrics of text-to-image models. The research conducted by Borji [6] performs a quantitative analysis on text-to-image models, with a focus on Stable Diffusion [36] and DALL-E 2 [35], specifically in the context of generating photo-realistic faces. The evaluation was based on Frechet Inception Distance (FID) scores, utilizing a dataset of around 15,000 generated faces. They find that Stable Diffusion outperforms the other models. In the domain of text-to-image synthesis, various evaluation metrics have been utilized, including FID, IS [39], CLIP [32], R-Precision [53], CLIP-R-Precision [30], Kernelized Inception Distance (KID) [5], and SOA [19, 39]. Each metric addresses a specific aspect of text-to-image models, offering a limited perspective. For example, the Inception Score (IS) [39] does not capture intra-class diversity, is insensitive to the prior distribution over labels, and has proven to be sensitive to model parameters and implementations, which can lead to unreliable results [3, 55]. In recent research, Lee et al. [25] introduce a benchmark, Holistic Evaluation of Text-to-Image Models (HEIM), which assesses 12 aspects, including text-image alignment, image quality, bias, and efficiency. They used FID and CLIP scores for models including DALL-E and Stable Diffusion. Wiles et al. [51] propose new metrics like VNLI and VQA methods (TIFA, VQ2) for text-to-image alignment evaluations. Grimal et al. [16] present the Text-Image Alignment Metric (TIAM), which focuses on the success rate of generative models in aligning generated images with specified prompts, assessing models such as Stable Diffusion and unCLIP [35, 24].\nMotion and facial representation. In the existing literature, quantitative analysis for generated face images received limited attention (Borji [6] and Raina et al. [33]). In addition, and to the best of our knowledge, text-to-image models have not been evaluated on motion. Here we introduce a comprehensive face and motion dataset for evaluating the text-to-image models (see Appendices A.1.1 and A.1.1). This dataset is created through filtering of the COCO caption dataset [26] as well as the Flickr30k [31] dataset, which were chosen due to their comprehensive caption coverage and diverse content. COCO and Flickr30k have been utilized across various text-to-image tasks, serving as a standardized benchmark for evaluating model performance [46, 1, 22, 52]. Utilizing this data, we evaluate the performance of text-to-image models in generating images from face and motion-related text prompts.\nSocial bias. Social biases in image-only [43, 50] and text-only models [8, 56] are well-established. In contrast, the study of these biases within multimodal models is more limited. For example, Yapo and Weiss [54] noted gender biases in search results like \u201cCEO\u201d predominantly retrieving images of white men. Other research explored biases in datasets like COCO [4] and image contexts where gender is ambiguous, e.g., an unidentifiable person \"snowboarding\" is often labeled male [17]. Studies such as Jia et al. [21] and Srinivasan and Bisk [42] further examined how gender stereotypes manifest in various media. Moreover, recent works emphasize the cultural biases learned by multimodal models [44]. Concerns arise regarding text-to-image generative models, highlighting potential preferences towards certain social groups [2]. Additionally, research introduces tools to detect biases, such as Zhou et al. [57]'s probing task. Cho et al. [9] broadened the scope by examining"}, {"title": "3 Methodology", "content": "3.1 Problem definition\nGiven a dataset of existing real images $T = T_1, T_2,...,T_n$, along with their associated textual descriptions $P = P_1, P_2, . . ., P_n$, and a collection of text-to-image models $M = M_1, M_2,..., M_j$, our goal is to evaluate the performance of these models in generating synthetic images that closely resemble the real images in $T$, when given related texts as prompts.\nEach textual description $P_i$ in $P$ serves as a prompt for the text-to-image models. When a model $M_j$ processes a prompt $P_i$, it generates a synthetic image $S_{ji}$. The set of synthetic images generated by model $M_j$ for all prompts in $P$ is denoted as $S_j = {S_{j1}, S_{j2},..., S_{jn}}$. Formally, for each $i \\in {1, 2, ..., n}$, the model $M_j$ maps the prompt $P_i$ to the image $S_{ji}$, i.e., $M_j : P \\rightarrow S_j, M_j(P_i) = S_{ji}$.\nThe real images $T$ serve as a benchmark to evaluate the quality of the synthetic images $S_j$. To quantify this evaluation, we use a quality scoring function $Q : (T, S_j) \\rightarrow R$, which compares the real images $T$ to the synthetic images $S_j$ generated by model $M_j$. One of the quality scoring functions is the Fr\u00e9chet Inception Distance (FID) score [18]. The FID score measures the similarity between two sets of images by comparing their feature distributions in a high-dimensional space. A lower FID score indicates that the synthetic images are more similar to the real images, implying better performance of the text-to-image model.\nAdditionally, we employ the R-Precision score as another metric for evaluating the proficiency of the text-to-image models. The R-Precision score measures how accurately the generated images depict the textual prompts. We can assume, without loss of generality, that a higher value of the proficiency scoring function, i.e., the R-Precision score, indicates a better model. Similar to the FID analysis, we use the same set of prompts $P$ for all the text-to-image models $M_j$ to perform a comparative study. Each model's proficiency score function $P : (T, S_j) \\rightarrow R$ is computed and presented, evaluating the ability of each model to generate images that accurately depict all aspects mentioned in the corresponding text prompts.\n3.2 Data Extraction\nCOCO Dataset: We filtered the COCO training set (train2017) for two main categories: human faces and motion. Using the Multi-Task Cascaded Convolutional Network (MTCNN) model [15], we extracted face images based on high confidence levels and bounding box dimensions (see Algorithm 1). For motion, we combined the \u201cperson\u201d category with sport-related categories (e.g., tennis racket), resulting in 10,000 images for each category with corresponding captions. From the detected face images, we further isolated key facial features: eyes were cropped from the areas near the eyes (see Algorithm 2), mouths were extracted from the mouth regions (see Algorithm 3), and noses were isolated using the MTCNN coordinates (see Algorithm 4).\nFlickr30k Dataset: For the Flickr30k dataset, we targeted images by searching captions for keywords related to faces and motion (e.g., \u201crunning\u201d and \u201cswimming\u201d). A script was used to filter and save images and their captions in a designated directory. Similar to the COCO dataset, we used the MTCNN model to detect faces and subsequently extracted the eyes, mouths, and noses from these face images.\nThese extracted datasets allow us to compare real images with those generated by our text-to-image models, using FID score to evaluate model performance."}, {"title": "3.3 Quantitative metrics", "content": "FID score. FID measures the perceptual similarity between generated and real images using feature representations without the use of labeled data [38]. The quality of the synthesized images $S_j$ for model $M_j$ is measured by the quality scoring function $Q : (T, S_j) \\rightarrow R$, which is used to compare the real images $T$ with the synthetic images $S_j$. Images are positioned in a feature space (e.g., a layer of the Inception network), and a multivariate Gaussian is fitted to the data. The distance is calculated as:\n$Q : FID(x, 9) = ||\\mu_x - \\mu_g||^2 + Tr(\\Sigma_x + \\Sigma_9 \u2013 2(\\Sigma_x\\Sigma_9)^{\\frac{1}{2}})$ where \u03bc and \u03a3 are the mean and covariance of the samples. The FID metric can be affected by artificial modes and mode dropping [28]. A lower FID indicates higher quality.\nAlgorithm 5 shows the pseudocode used for calculating FID score. The algorithm calculates the Fr\u00e9chet Inception Distance (FID) score, a measure of similarity between distributions of real and generated images.\nR-Precision score. The proficiency of the synthesized images $S_j$ for model $M_j$ in representing all details of the corresponding text prompts is measured by a proficiency scoring function $P : (T, S_j) \\rightarrow R$. Here, we employ the R-Precision score that evaluates how accurately each synthetic image $S_j$ matches the content described in the corresponding prompt $P_i$.\nTo compute the R-Precision score, following [53], we utilize DAMSM (Deep Attentional Multimodal Similarity Model) [53]'s pre-trained image and text encoders. Specifically, DAMSM incorporates a bidirectional Long Short-Term Memory (LSTM) [40] as the text encoder to derive semantic vectors from the provided text. Although DAMSM is not the latest method, it is extensively cited in the literature and remains the standard for calculating the R-Precision metric, ensuring consistency with prior work. For image encoding, DAMSM employs the pre-trained Inception-v3 model [45] trained on the ImageNet dataset [37]. In this context, we harness Convolutional Neural Networks (CNNs) [23] and Long Short-Term Memory networks (LSTMs) for their specific strengths in generating embeddings for text-to-image tasks. Specifically, for the R-Precision metric calculation, CNNs are employed to calculate the image embeddings, while LSTMs are used for generating the text embeddings.\nAlgorithm 6 processes image and text data, generates respective embeddings, and computes the R-Precision scores by comparing the embeddings."}, {"title": "3.4 Qualitative method", "content": "3.4.1 Bias analysis\nTo systematically study biases in the text-to-image models, we design a bias analysis test that incorporates 88 prompts for potential racial biases and 88 prompts for possible gender biases (see prompts provided in the Appendices A.2 and A.3). Each of the 176 bias prompts (88 for racial bias and 88 for gender bias) was used to generate 16 images using the models under test, resulting in a total of 2,816 images. Before proceeding with the analysis, each prompt was categorized according to the type of bias it could potentially induce, based on societal stereotypes and existing biases. We devised the prompts based on societal stereotypes and existing biases identified in the literature reviewed. The prompts are organized into categories testing for racial and gender biases, as detailed in the appendix. For gender, we considered three categories, and for race, five categories. Each prompt was constructed to highlight specific stereotypes and biases, such as \"A person dancing ballet\" to test for gender bias. To begin the analysis, we use 88 prompts for each of the two main categories: race and gender. Each of the two lists was disjoint, with each entry focused on just one of the two types of bias. Human evaluators are then provided with subcategories for both race (Black, White, Asian, Hispanic/Latino, or Uncertain) and gender (Female, Male, or Uncertain) to determine the classification of each generated image. The aim is to create scenarios in which the models might exhibit these biases in their generated images.\nTo estimate bias in the generated images, we assess the proportion of images that exhibit expected societal biases for each prompt. This analysis provided a bias percentage for each set of images, facilitating a comparative study across models.\nEach generated image is then categorized based on its racial (e.g., Black, White, Asian, His-panic/Latino, or Uncertain) and gender (e.g., Female, Male, or Uncertain) representation through human evaluation by employing five evaluators to mitigate individual biases. Finally, we calculate the percentage representation for each racial and gender category in the images corresponding to each prompt as demonstrated in Table 2. Furthermore, to determine fairness, each racial or gender category's representation should be approximately equal after excluding the \u2018Uncertain' category. In gender analysis, if 20% of images are \u2018Uncertain'; the remaining 80% should be equally divided between 'Female' and 'Male' (i.e., 40% each). In race analysis, representation should be evenly distributed among Black, White, Asian, and Hispanic/Mexican categories, considering the \u2018Uncertain' category which means the race or gender cannot be determined in human evaluation and the image is unclear. Any deviation from this even distribution indicates bias. This methodology provides a detailed view of potential biases in text-to-image models."}, {"title": "4 Results", "content": "Image generation quality analysis. The analysis of FID scores reveals insights into image quality across different models and datasets. In the COCO dataset evaluation, we observe that LAFITE performs the poorest in terms of image quality, while Stable Diffusion performs the best. Notably, for the COCO dataset, the motion category performs better than the faces category in most models, except for Stable Diffusion, where the FID scores are similar. Generating human faces remains a challenge, with the complexity of facial diversity impacting image quality.\nEvaluating FID scores using captions from the Flickr30k dataset demonstrates the consistent supe-riority of Stable Diffusion across both Face and Motion categories. It achieves lower FID scores, indicating higher image quality and resemblance to real images, in contrast to LAFITE G [59], which presents higher FID scores and lower image quality. These findings contribute to the overall understanding of the effectiveness of various models in text-to-image generation tasks, highlighting the superior performance of the Stable Diffusion model in both datasets. Furthermore, we generate face images and utilize MTCNN for extracting distinct facial features such as the nose, eyes, and mouth. However, the quality of the images we worked with has limitations, consequently restricting the extraction of a substantial number of facial features. As a result, the FID scores derived from a limited set of generated images cannot be considered reliable, leading us to refrain from presenting detailed quantitative analyses of facial details in our results.\nThe evaluation of FID and R-Precision scores for different models, utilizing captions from both the COCO and Flickr30k datasets, is presented in Table 1. The Stable Diffusion model consistently achieves the highest R-Precision scores across all datasets and categories (Face and Motion), while LAFITE G displays weaker performance in the Face category and mixed results in Motion. Dall-E Mini [10] closely trails behind Stable Diffusion in the COCO dataset and surpasses LAFITE G, with performance variation in the Flickr30k dataset. These disparities in performance arise from each model's architectural strengths and limitations. Stable Diffusion's sequential transformation process enhances image synthesis, Dall-E Mini benefits from the transformer architecture, and LAFITE G's integration of a language model alongside StyleGAN2 leads to fluctuating outcomes. These differences reflect the models' abilities to generate intricate content like human faces and motion-based images across datasets. Notably, human face generation poses a greater challenge, underscoring the need for ample high-quality training data and substantial computational resources. Performance in the Motion category displays more variability, as seen in COCO where LAFITE G outperforms Dall-E Mini, whereas the reverse occurs in Flickr30k, possibly due to dataset differences, model specifics, or external factors. Figure 1 displays random samples of the images of the generated faces from which noses, mouths, and eyes have been extracted.\nBias analysis We create a set of captions with specific biases, which were then used to evaluate how our text-to-image models respond to these biases. To ensure the transparency and reproducibility of our results, we include all captions from our analysis in the Appendix. Readers are encouraged to refer to the appendix to understand the nature and diversity of these captions. We analyze the qualitative bias on the generated images, with results shown in Table 2. We observe distinct trends"}, {"title": "5 Limitations and broader impact", "content": "Our study encountered several limitations in its experimental approach. Utilizing ERNIE-ViLG [13] for image generation proved challenging due to restricted API access, necessitating Chinese phone numbers and imposing daily usage restrictions. Despite these challenges, we produced a modest 1506 images. However, these ERNIE-ViLG-generated images were excluded from our study due to API access limitations. Unfortunately, due to the inaccessibility of Dall.E 2 code for public use, we resorted to using Dall.E Mini, rendering quantitative comparisons with other models difficult. The experiments themselves were time-consuming as shown in Table 4. Face extraction caused a decrease in the dataset size, as the filter was unable to recognize all faces due to poor image quality and insufficient facial details in some generated images. This filtering process, which refers to face extraction, demanded a substantial investment of time. Due to the poor quality of generated faces and insufficient facial details, we did not calculate FID scores for extracted noses, eyes, and mouths. Furthermore, access to richer datasets like Google's CC3M [41] was restricted, potentially limiting a more comprehensive evaluation of text-to-image models. Collecting motion image-caption pairs from Flickr30k proved challenging, resulting in a limited dataset of 5000 pairs."}, {"title": "6 Conclusion", "content": "Traditional machine learning models heavily rely on static datasets, but these come with inherent limitations like data sparsity, privacy concerns, biases, and inadequate representation of minority classes. In response, the scientific community has turned to synthetic data as a promising alterna-tive. Recent advances, especially in text-to-image diffusion models like Stable Diffusion, DALL-E Mini, and LAFITE, have demonstrated substantial potential in generating high-quality synthetic data. However, we identify challenges such as Gender and Racial biases and under-representation associated with synthetic data generation (Table 2). Our work contributes by providing qualitative and quantitative analysis of the issues posed by employing text-to-image models for synthetic data generation. Also, we examine the gender and racial biases in these models, especially in response to neutral prompts. Our evaluation of FID and R-Precision scores highlights Stable Diffusion's consistent high-quality image generation in both face and motion categories across both datasets. However, LAFITE G slightly outperforms other models in motion image generation on the COCO dataset and in face image generation on the Flickr30k dataset based on R-Precision scores. The inconsistent and different results between the two evaluation metrics underscore the multifaceted nature of model performance. The generative process itself can introduce noise and variability. Some models produce more consistent results, while others have higher variance in quality, which can stem from differences in model training, and the randomness inherent in generation. Additionally, the inherent characteristics of the COCO and Flickr30k datasets may favor certain models over others. In addition to the quantitative metrics, gender, and racial biases were evident in the Stable Diffusion and Dall-E Mini models, with the Stable Diffusion model displaying a pronounced inclination towards males and white individuals in professional contexts. These insights emphasize the necessity of careful model selection for specific image generation tasks, considering various evaluation metrics, and inherent biases, when assessing performance."}, {"title": "A Appendix", "content": "A.1 Experimental setup\nA.1.1 Dataset\nCOCO\nThe MS COCO dataset [26] was used to obtain prompts and related real images, with a focus on human faces and motion/movement.\n\u2022 Real images: We extracted 10,000 real faces from the 'person' category using the MTCNN. Additionally, 10,000 motion images were gathered from sport-related categories.\n\u2022 Generated images were prompted by ground-truth captions from the COCO dataset.\nFlickr30k\nThe Flickr30k dataset was used to acquire prompts and images related to human faces and mo-tion/movement, and 10,000 face and 5,000 motion images were collected."}]}