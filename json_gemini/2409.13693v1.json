{"title": "Declarative Integration and Management of Large Language Models through Finite Automata: Application to Automation, Communication, and Ethics", "authors": ["Thierry Petit", "Arnault Pachot", "Claire Conan-Vrinat", "Alexandre Dubarry"], "abstract": "This article introduces an innovative architecture designed to declaratively combine Large Language Models (LLMs) with shared histories, and triggers to identify the most appropriate LLM for a given task. Our approach is general and declarative, relying on the construction of finite automata coupled with an event management system. The developed tool is crafted to facilitate the efficient and complex integration of LLMs with minimal programming effort, especially, but not only, for integrating methods of positive psychology to AI. The flexibility of our technique is demonstrated through applied examples in automation, communication, and ethics.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have heralded a revolution in the field of human-computer interaction, primarily due to their ability to simulate human behaviors (Aher, Arriaga, and Kalai 2023). In the industrial context, however, integrating LLMs to an application requires significant resources in development and validation. The inherently stochastic process of response generation poses ethical and operational considerations. To circumvent this issue, it is now increasingly recognized that chaining multiple LLMs can yield more reliable outcomes than relying on a single LLM (Zheng et al. 2023; Sreedhar and Chilton 2024; Zeng et al. 2021; Wei et al. 2022). Several libraries have emerged to facilitate the chaining of Large Language Models (LLMs), e.g., langchain (Chase 2022). Chaining LLMs requires developing a specific model for each application, addressing the issue of detection that determines the dynamic sequencing of LLMs, validation, and maintaining a response time compatible with the context of use.\nIn this article, we present a novel approach aimed at integrating LLMs and other AI modules through a declarative method, eliminating the need, as much as possible, for challenging programming tasks. Unlike LLMStack (Chintala 2024), which primarily facilitates the chaining of LLMs for application generation, our approach employs a model to manage multi-modal interactions transparently. This enables the integration of triggers and subtle conversation history sharing mechanisms, without presupposing specific applications. With our framework, designing a multi-agent system based on LLMs involves the following steps: (1) Define an automaton whose states are LLMs/AI modules dedicated to different tasks, specifying the sharing of history. (2) Define the edges as triggers, evaluating the conditions to transition from one state to another. (3) Set trigger priorities when a state can have multiple successors. (4) Write the LLMs' prompts. The platform manages the sequential triggering of the states (e.g., LLMs) in the automaton. It ensures the updating of history shared between subsets of states and arcs. Using our approach, one may quickly test several models for an application, without the need for procedural development or algorithms to implement each trial.\nWe introduce our framework by considering dialogues between humans and machines. However, it also supports the integration of problem-solving or any other AI modules provided that they implement an interface that allows the system to transparently manage interactions between states of the automaton and the history. We can use diverse triggers such as visual detection, sound, onboard sensors, or any other form of data. Thereby, although there exists links between our framework and AI planning tools for dialogue management (such as DRUID AI, or Rezolve.ai), our theoretical framework is unique, due to minimal assumptions on automata states and transitions. To demonstrate the flexibility of our technique, we present three examples based on LLMs: an example of automating train ticket booking, a non-violent communication scheme, and an example related to prevent ethical issues with LLMs, which are especially challenging in multi-modal systems (Sutton and Barto 2018)."}, {"title": "Multi-Modal Models Based on Automata", "content": "We aim to ensure the three following characteristics: 1. The chaining of LLMs and other AI modules\u00b9 is decribed by a model, independently of its implementation, including the triggers for determining the order of use of the different LLMs. 2. The sharing of the conversation history is described declaratively and managed transparently for the user. 3. The response time can be estimated from the structure.\nWe define an automaton whose states are LLMs or user's messages, and transitions are triggers associated with a priority to determine which LLM is to be invoked next.\nWithout loss of generality, we will consider LLMs in this section, but any AI modules exchanging data can be considered."}, {"title": "Definition 1 (state)", "content": "Given an alphabet \u2211, a state in an automaton is associated with a function $q : \\Sigma^* \\rightarrow \\Sigma^+$, where * is the set of all possible strings (including the empty string) that can be formed from the alphabet \u2211 and \u2211+ denotes the set of all non-empty strings."}, {"title": "Definition 2 (final state)", "content": "A final state is a specific type of state for which the output string q(s) for any input string $s \\in \\Sigma^*$ satisfies a condition where the message exchange can conclusively end."}, {"title": "Definition 3 (MFA)", "content": "Given an alphabet \u2211, a Multi-LLM Finite Automaton (MFA) is a 4-tuple, (Q, \u03b4, qo, F): A finite set of states Q, which can be LLMs, AI modules or user messages. A transition function $\\delta : Q \\times \\Sigma \\times \\mathbb{N}^{|Q|} \\rightarrow Q$, where $\\mathbb{N}^{|Q|}$ represents a list of positive integers of size |Q|. An initial state qo \u2208 Q. A set of final states F \u2282 Q.\nThe construction of the transition function is based on triggers. A trigger is a module that determines whether a state (LLM or awaiting user message) should be activated or not, from the output sent by another state. Therefore, a trigger is an arc in the MFA that enables composing the transition function of the automaton. The trigger will respond with an integer representing a Boolean (a bit): 0 indicates that the candidate should not be activated, and 1 signifies acceptance. Given that there may be multiple candidate states to continue the interaction, the user will determine an integer priority in {1, ..., |Q|} among all the candidates outgoing each state. When a state is not a candidate, its priority is 0. If two triggers answer positively at the output of a state, the one with the highest priority prevails: the next state will be the one corresponding to this arc (transition)."}, {"title": "Definition 4 (Trigger)", "content": "Given a MFA (Q, \u03b4, q\u0ed0, F), a state q \u2208 Q, a string s and a priority p\u2208 {0, . . ., |Q|}, let's define $f_{\\tau}(q,s): Q \\times \\Sigma^* \\rightarrow \\{0,1\\}$, a function that assigns a binary value based on the given state and message. A trigger is a function $\\tau : Q \\times \\Sigma^* \\times p \\rightarrow \\{0,1\\}$ defined as follows: $\\tau(q,s,p) = min(p \\times f_{\\tau}(q, s), p)$. The set of triggers is denoted by T.\nWhile it may seem logical for the priorities associated with transitions leaving a state to establish a total order, we do not impose this restriction. If the model designer defines multiple priorities as equal, and at a certain stage of the interaction multiple equivalent candidates are accepted, we suggest randomly selecting the next state. In case of a dialogue, this feature can help to simulate naturally unpredictable parts.\nThe last step consists of handling how dialogue history is shared by the LLMs. For this purpose, we define a bipartite graph linking states and triggers\u00b3 to computational objects representing histories."}, {"title": "Definition 5 (History Graph)", "content": "Let M = (Q, \u03b4, qo, F) be an MFA with triggers T. A history is a non-empty set of sequences in an alphabet \u03a3*. The set of histories is denoted by H. The bipartite history graph H = ((Q\u222aT, H), E = Erw\u222aEr\u222aEw) defines the history attachment. \u2200x \u2208 Q\u222aT, there is at most one edge e \u2208 E linking x to a vertex h \u2208 H. Such an edge indicates that x is attached to h. For e = (x,h) \u2208 Erw, x reads and updates h. For e = (x,h) \u2208 Er, x only reads h. For e = (x, h) \u2208 Ew, x only updates h.\nThe workload can be computed from the automaton by estimating the (maximum) number of states between two user messages and considering the average processing time of each state."}, {"title": "Algorithm 1: MFA-based dialogue", "content": "Algorithm 1 outlines the basic structure of a dialogue automatically derived from any MFA.\nBy using this framework, if the states and transitions of the MFA are based on LLMs, no specific computer skills are required. The steps are as follows: write the prompts for the LLMs, including some triggers, define the MFA that models the chronological sequencing of the states (with transition priorities and terminal states)."}, {"title": "Engineering considerations", "content": "The system implementation requires ensuring two main points:\n1. Generality: the system must ensure that there are no restrictions on the LLMs/modules and triggers used.\n2. Transparently shared history: to be fully declarative, the system must allow invisible data management once histories are attached to the graph components.\nTo address the first point, the states and arcs of the MFA are wrappers of objects constrained to implement specific interfaces. We employ wrappers since both states and edges may be represented as LLMs and have shared histories, as outlined in Definition 5. The interface for LLMs includes the following functions: predict(user_message), which returns the LLM's response based on a user message, and add(inputm, outputm), which adds a (message, response) pair to the history if the LLM is attached to one (otherwise, it has no effect). Triggers, which can be LLMs, should not write to the history, though they can read it. They have a priority, so the basic interface includes predict(user_message), returning the trigger's {0,1} response from a message, get_priority(), which returns the integer priority, and set_priority(p), which assigns the integer priority to p.\nThe second point requires event-based programming. The history is shared using the Observer/Observable design pattern (Gamma et al. 1994). The Archive class is Observable and includes an add (inputm, outputm) method (similar to the one of the LLM interface), and methods for returning and removing (message, response) pairs. This class notifies all its observers at each new event, such as adding or removing data. The History class connects a specific LLM/Module and an Archive object, using an update method designed to respond to events within its archive, including its own addition of a (message, response) pair.\nThe LLM class encapsulates a History object linked to a specific Archive reference. Any MFA state that is an LLM should inherit from the LLM class. If not, it must at least adhere to the LLM interface. MFA edges must conform to the trigger interface and can be designed as subclasses of the LLM class, depending on whether a LLM is utilized to implement the trigger. At last, we define wrapping classes for easily maintain the graph representing the MFA and to implement generic exploration procedures (such as Algorithm 1). The ChainNode class defines a state of the"}, {"title": "Triggers", "content": "For an application to function, the triggers must be reliable and provide their responses quickly. For example, for NVC, the effectiveness of LLMs hinges on their ability to subtly interpret the emotional state of the user. We propose a simple and systematic test protocol. Let us first consider a trigger dedicated to conversational analysis. The protocol is based on three phases:\n1. Sentence Generation: Sentences are generated either manually or using a Language Model (LLM) with a specific goal corresponding to a trigger, expressed as a simple z\u00e9ro-shot prompt. For instance, \"generate sentences expressing anger or frustration in the context where a restaurant customer complains about their experience.\"\nWhen an LLM is used for generating sentences, they must undergo human validation.\n2. Dataset Augmentation: Randomly generated sentences (potentially unrelated to the trigger) are incorporated into the dataset. This addition diversifies the dataset.\n3. Human Expert Validation: Human experts manually validate the dataset by assigning a binary value (0 or 1) to each sentence, indicating its alignment with the trigger under consideration.\n4. Testing and Evaluation: Test the trigger on the dataset by comparing its assignments with the manual assignments and evaluating execution time.\nIn a more general context, e.g., a MFA involving triggers which are not LLMs, and/or when entry data are not sentences in natural language, the general philosophy of the protocol is the same, except the validation phase, which is not required to be performed by humans. The validation can be conducted using algorithms more sophisticated and time-consuming than the trigger. For dialogue systems, we empirically observed that being above 75% of positive inferences was enough for improving the results obtained with a single LLM, and that the best temperature for triggers is 0.1."}, {"title": "Case Studies", "content": "Automated Train Ticket Booking. Integrating multiple LLMs with specialized modules for automated tasks enhances the accuracy and standardization of data collection from user dialogues. Let's consider a \"natural language\" train ticket reservations example that demonstrates the ease of modeling with our approach. It is kept minimalistic to avoid unnecessary complexity. The idea is to query for inputs, looping until the response can be entered into the system that requires formatted data. Some states are writer modules, i.e., functions writing an input in a database or CSV file, and returning no output. The triggers are the following: to: City name trigger. fto is, for instance, tasked with verifying whether the user's message includes an actual city name (possibly in a specific list). The priority level is $p_{t_0} = 2$. t1: Time trigger: The function $f_{t_1}$, for example, may be designed to ascertain if the user's message contains a time specification (return 1) or not (return 0), with priority $p_{t_1} = 2$. t2: \u2200m \u2208 \u03a3*, t2: $f_{t_2}$ = 1, with priority $p_{t_2}$ = 1.\nThe states Q are the following: q0: User message. 11: LLM for departure city inquiry. While a simple print statement could serve to ask for the departure city, employing an LLM allows for varied question phrasing. This flexibility is beneficial for repeated inquiries following unclear user responses. Therfore, this LLM must share the global history. w2: Writer module: add the city name to a database/CSV file. 13: LLM for destination city request. q4: User message. w5: Writer module: add the city name to a database/CSV file. 16: LLM for departure time inquiry. 97: User message. 18: LLM used to extract and convert the time in a standard format. wg: Writer module: add the time to a database/CSV file.\nThe history h be shared by all states corresponding to LLMs, excluding user messages and writers, to keep track of the exchanges at each stage. In this example, it is not mandatory to share this history with triggers (they only use the last output). The graph H = ((Q, H), E) is such that $E_{rw}$ = {(l1, h), (l3, h), (l6, h), (ls, h)}."}, {"title": "Nonviolent Communication", "content": "Non Violent Communication (NVC) is a communication method theorized by Rosenberg (Rosenberg 2003, 2005, 2015) for improving one's cognitive and emotional skills. NVC is part of the broader field of positive psychology (Seligman and Csikszentmihalyi 2000), and can also be referred to as \"Compassionate Communication\u201d (Azg\u0131n 2018). It is a technology for a culture of peace in interpersonal relationships (Adriani et al. 2024), by addressing conflicts while avoiding confrontation. To summarize the NVC approach, after having enlightened four patterns of alienating communication \"that blocks compassion\", which are: 1. Moralistic Judgments, 2. Making Comparisons, 3. Denial of Responsibility, and 4. Other Forms of Life-alienating Communications, Rosenberg describes the four-step method of NVC: Step 1: Observing and describing Facts. Step 2: Expressing one's or others' Feelings. Step 3: Explaining one's or others' Needs. Step 4: Formulating an acceptable and specific Request.\nIn our modelling, we focus on identifying contextual elements (Facts) and the positive or negative emotions experienced by the user (Feelings), while setting aside the issue of identifying underlying Needs. This approach allows us to apply our ARPS method (Acknowledge - Rephrase - Probe - Suggest a solution) by including both the rephrasing of facts and emotions. It is important to note that the concept of positive or negative emotions refers to the valence of emotions (Russell 1980), i.e., their degree of pleasantness, rather than a moral value that would imply judgment."}, {"title": "NVC Scheme", "content": "In Figure 1, the MFA is constructed under the assumption that the client responds perfectly to the open question in the state dedicated to acknowledging, reformulating, and probing. This is represented by a single arc between the states q3 (user) and q4 (solution proposal). To design a concretely operational scheme, we must first analyze the user's response and deploy strategies specific to this response. We distinguish: 1. A detailed answer including elements of the customer's state of mind (e.g., anger, fear, disappointment) plus elements of context or situation. 2. A non-detailed answer providing only elements of the customer's state of mind (e.g., anger, fear, disappointment). In all cases, the model will rephrase what it understands as the user's complaint and express compassion.\nThe subsequent process will then differ according to the two scenarios. In case 1, a solution will be suggested. In case 2, a new open question will be asked to obtain the missing information about the context. A third path is dedicated in case the user responds with an unreadable message to the open-ended question following their complaint. The first state of the MFA (Figure 5) is used to add a specific context of use.\nThe triggers are the following: to: detects if the client is complaining. fto can be, for instance, implemented with a dedicated LLM prompted to detect anger or frustration, with $p_{t_0}$ = 2. t1: detects if the user's message only contains elements of their emotional state, negative or positive, without elements of context or situation. We assign the following priority: $p_{t_1} = 2$. t2: detects if the user's message is unreadable or incomprehensible. We state $p_{t_2} = 3$. t3: \u2200m \u2208 \u03a3*, $f_{t_3}(m) = 1$, with $p_{t_3}$ = 1 each time we use it.\nThe states Q are the following: q0: Contextual message provided by the user, used as the prompt for the q1 LLM. 11: Standard LLM. q2: User message. 13: LLM dedicated to acknowledging, reformulating. 14: LLM dedicated to history-dependent open question. 95: User's answer to an open question. 16a: LLM dedicated to rephrasing emotion and situation and expressing compassion regarding the whole situation. 17a: LLM dedicated to suggesting solutions to clients' complaints. 166: LLM dedicated to rephrasing emotion and expressing compassion regarding emotion. 176: LLM for open question about the situation details. 16c: LLM dedicated to apologizing for not understanding the client's last message."}, {"title": "Experiments", "content": "Using the protocol described in the 'Triggers' paragraph of the previous Section, we employed Mistral-7B (Jiang et al. 2023) and Llama2 (Touvron et al. 2023) for generating the sentences to enhance diversity. We tested the triggers with ChatGPT 3.5 and 4 (OpenAI 2024). The third step of the protocol was carried out rigorously by two psychologists. We used a M2 chip, 16GB of RAM, and OS Ventura 13.5. Table 3 presents the results obtained. Dataset augmentation is represented as a percentage in the column \"% of random sentences\". The table shows the accuracy percentage of triggers and average running time."}, {"title": "Addressing Ethical Concerns", "content": "One of the current challenges in using LLMs in a conversational application is that even the most recent LLMs do not guarantee providing ethical responses. In such a scenario, one possible solution is to tweak the prompt, if we can access it. However, this approach is rather unpredictable, and does not protect from jailbreaking the system. Therefore, given a LLM l\u2081 with a user input qo, we suggest a more robust but straightforward approach. It requires a second LLM, 12, for reformulating sentences with ethical issues, and a trigger for detecting biases. The key point is that the user has no direct access to 12. The triggers are the following: to: Trigger detecting if the input raises ethical concerns (return 1) or not (return 0), with priority $p_{t_0}$ = 2. t1: \u2200m \u2208 \u03a3*, t1: $f_{t_1}$ = 1, with $p_{t_1}$ = 1.\nThe pattern depicted by Figure 6 can be included in any part of a larger MFA. The exploration algorithm must be parameterized to display the LLM message only when the outgoing arc returns to the user state qo. There is no need for history management, as the input message at each arc is the only one used. Table 4 shows results obtained using ChatGPT 4-0 with straightforward zero-shot prompts for the trigger to and the reformulation LLM 12 (based on the l\u2081 context), each augmented with 10 distinct examples. The bold sentences show cases where the system automatically reformulated the initial response of ChatGPT 4-0."}, {"title": "Path to Deployment and Perspectives", "content": "We are a startup specializing in integrating psychological components into AI. The platform and case studies have been implemented and tested. The platform will be released as open-source under the Apache 2.0 license upon the publication of this paper (before any public presentation). Our goal is to gradually enrich the library with new triggers and MFAs dedicated to typical use cases through internal and external contributions. Concerning future work, we aim to implement MFA exploration algorithms that incorporate states other than LLMs and better manage latency.\nMoreover, the formalism we presented holds significant theoretical promise in its compatibility with traditional automata operations such as union, intersection, concatenation, and complementation. We also anticipate that our framework will offer substantial benefits in fields such as psychology, pedagogy, and management. By using MFAs to model empathetic communication, we aim to reveal and articulate connections within knowledge domains that were previously accessible only to a limited circle of experts.\nAt last, a work in progress is to investigate the integration of MFAs with constraint acquisition systems (Bessiere, Carbonnel, and Himeur 2023; Tsouros, Berden, and Guns 2024; Beldiceanu and Simonis 2016), using states that incorporate SAT or CSP solvers (Audemard and Simon 2018; Prud'homme and Fages 2022; Perron, Didier, and Gay 2023). Our goal is to design conversational systems capable of solving combinatorial and optimization problems on demand, with a formal guarantee of accurate responses."}]}