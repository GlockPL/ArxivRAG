{"title": "WRIM-Net: Wide-Ranging Information Mining Network for Visible-Infrared Person Re-Identification", "authors": ["Yonggan Wu", "Ling-Chao Meng", "Yuan Zichao", "Sixian Chan", "Hong-Qiang Wang"], "abstract": "For the visible-infrared person re-identification (VI-ReID) task, one of the primary challenges lies in significant cross-modality discrepancy. Existing methods struggle to conduct modality-invariant information mining. They often focus solely on mining singular dimensions like spatial or channel, and overlook the extraction of specific-modality multi-dimension information. To fully mine modality-invariant information across a wide range, we introduce the Wide-Ranging Information Mining Network (WRIM-Net), which mainly comprises a Multi-dimension Interactive Information Mining (MIIM) module and an Auxiliary-Information-based Contrastive Learning (AICL) approach. Empowered by the proposed Global Region Interaction (GRI), MIIM comprehensively mines non-local spatial and channel information through intra-dimension interaction. Moreover, Thanks to the low computational complexity design, separate MIIM can be positioned in shallow layers, enabling the network to better mine specific-modality multi-dimension information. AICL, by introducing the novel Cross-Modality Key-Instance Contrastive (CMKIC) loss, effectively guides the network in extracting modality-invariant information. We conduct extensive experiments not only on the well-known SYSU-MM01 and RegDB datasets but also on the latest large-scale cross-modality LLCM dataset. The results demonstrate WRIM-Net's superiority over state-of-the-art methods.", "sections": [{"title": "1 Introduction", "content": "Person re-identification aims to match images of a person of interest in a query set with those in a gallery set, which may have been captured by different cameras [48]. Existing methods [1, 2, 18, 25, 30, 48] have achieved remarkable performance in visible-light person re-identification. However, visible-light cameras face"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Visible-Infrared Person Re-Identification", "content": "VI-ReID is a challenging task due to the modality differences between infrared and visible images. Existing image-level methods fuse modalities at the data level to alleviate the modality discrepancy [7,28,29], while feature-level methods map visible (VIS) and infrared (IR) features into a common space [5,38,43, 48]. Often, image-level methods suffer from a lack of high-quality VIS-IR image pairs, leading to noise disruption. Feature-level methods are limited due to insufficient modality-invariant information mining. To better capture specific-modality and shared-modality information, the dual-stream network architecture is commonly employed for VI-ReID tasks [16, 44, 45, 48], where the parameters are separated in the shallow layers to obtain specific-modality information and shared in the deep layers to obtain shared-modality information. However, they merely segregate images from different modalities without multi-dimension information interaction. Our proposed WRIM-Net leverages non-local spatial and channel interactions to extract richer long-dependencies information. Additionally, by strategically placing MIIM at different positions, it effectively explores both specific-modality and shared-modality multi-dimension information."}, {"title": "2.2 Attention Mechanisms", "content": "Attention mechanisms have found widespread application in visual tasks to promote neural network performance by improving visual representation [12, 26, 32,35]. In the field of person re-identification, attention mechanisms have been incorporated as demonstrated in [2,32,40,48]. To capture long-range spatial relationships, [40] introduced second-order non-local attention modules; [48] introduced the non-local [32] mechanism for attentional feature extraction within the last two blocks of the network. DDAG [47] extracted potent features for"}, {"title": "2.3 Contrastive Learning", "content": "Contrastive learning plays an important role in self-supervised learning [3, 10, 39,49], allowing the neural network to extract invariant features efficiently. Contrastive learning also has a significant impact on image-text multi-modalities learning [15, 22, 51]. [22] used contrastive learning to perform modality alignment of images and text for cross-modality retrieval. [15,51] employed contrastive learning initially to achieve modality alignment and shared networks for cross-modality feature learning. [41] employed joint contrastive learning to acquire color-invariant features in unsupervised VI-ReID tasks. [14] introduced a contrastive regularization loss to regularize the model, where positive samples for this contrastive loss involve mixing part descriptors with the same identity. Unlike the previous methods, our CMKIC loss neither employs additional methods to create data-augmented positive samples nor contrasts a single positive for each anchor. Instead, our CMKIC loss introduces a novel approach for selecting positive samples, specifically choosing top-K samples with the same ID as the anchor, different modalities, and the least similarity. This approach increases task difficulty, enabling the network to mine modality-invariant information."}, {"title": "3 Methodology", "content": "In this section, we provide a detailed description of the proposed method for the VI-ReID task. Our network uses a pre-trained single-stream network (ResNet50 [11]) to extract visible and infrared features. As shown in Figure 2, the proposed method consists of two main components. (1) MIIM module. MIIM enhances its non-local spatial interaction capability by incorporating the spatial compression and GRI module. (2) AICL approach. AICL utilizes the CMKIC loss to effectively guide the network in learning modality-invariant information and incorporates auxiliary information to further enhance its extraction."}, {"title": "3.1 Multi-dimension Interactive Information Mining Module", "content": "MIIM improves information extraction in several ways: (1) Employing global spatial interactions and establishing long-range dependencies to capture non-local spatial-related information, such as shape, pose, and object proportions. (2) Employing channel interactions to capture channel-related information, such as texture. (3) Employing the separate MIIM in the shallow layers to extract specific-modality multi-dimension information (often overlooked by many existing methods) and the shared MIIM in the deep layers to extract shared-modality multi-dimension information. The MIIM module primarily comprises three components: Spatial-Channel Compression (SCC), Global Region Interaction (GRI), and Spatial-Channel Restore (SCR). The input features first pass through a standard Batch Normalization (BN) layer and then proceed to the SCC. In SCC, features undergo two compressions. The first compression is done with a convolutional layer, and the resulting features serve as the Query (Q) for subsequent GRI. The second compression is executed by employing average pooling on the aforementioned features. Then, the resulting features serve as the Key (K) and Value (V) components of GRI. This approach not only achieves non-local spatial interaction but also further reduces computational complexity.\nSpecifically, the input features $F_1 \\in \\mathbb{R}^{H_1 \\times W_1 \\times C_1}$ pass through a convolutional layer with a kernel size and stride of $r_s$, resulting in $F_2 \\in \\mathbb{R}^{H_2 \\times W_2 \\times C_2}$. $F_2$ undergoes average pooling with kernel size and stride of $k_s$, yielding $F_3 \\in \\mathbb{R}^{H_3 \\times W_3 \\times C_2}$. The resulting $F_2$ and $F_3$ features are flattened and then fed into the Global Region Interaction (GRI) component, which employs Multi Head Attention (MHA) [26]. In this setup, the flattened $F_2$ functions as the Query (Q), while flattened $F_3$ serves as both the Key (K) and Value (V). The resultant features, denoted as $F_1$, are obtained from the GRI module's operation, as indicated by the following formula:\n$F_1 = GRI(Q = F_2, K = F_3,V = F_3)$.\n(1)\nThe specific implementation of MHA is detailed in [26], where the number of heads is set to 8. To better leverage the positional information, we conduct sin-cos positional embedding [26]. In the processing of the GRI component, we"}, {"title": "3.2 Auxiliary-Information-based Contrastive Learning Approach", "content": "In this subsection, we introduce the proposed AICL approach, as illustrated in Figure 2. The approach involves employing our designed CMKIC loss behind the block4 layer of the network to guide the learning of modality-invariant features. Subsequently, auxiliary information of the block3 layer is leveraged to enhance the mining of modality-invariant information. Our CMKIC loss is different from common contrastive loss. It doesn't solely use one augmentation of the anchor as the positive sample. Instead, it selects the top-K least similar samples (Key-Instance) with the same ID but different modalities as positive samples. As the anchor and positive samples are from different modalities, this approach facilitates the network's exploration of crucial modality-invariant information."}, {"title": "3.3 Training and Inference", "content": "During training, we consider the three losses, CMKIC, the cross-entropy classification loss, and the ID loss, and have the total loss as a weighted sum:\n$\\mathcal{L}_{total} = \\mathcal{L}_{cls\\_P_5} + \\lambda_1 \\mathcal{L}_{CMKIC\\_P_5} + \\lambda_2 \\mathcal{L}_{ID\\_P_4}$,\n(9)\nwhere the hyper-parameters $\\lambda_1$ and $\\lambda_2$ are used to balance the contribution of each loss function. During the testing phase, we combine $R_g$, $R_1$ to $R_N$, $Q_g$ and $Q_1$ to $Q_M$ to form the final features for inference."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Datasets and Evaluation Setting", "content": "SYSU-MM01 [36] dataset comprises 491 identities captured by four visible (VIS) cameras and two infrared (IR) cameras, including two modes: All-Search and Indoor-Search. The training set comprises 22,258 visible images and 11,909 infrared images of 395 unique individuals. The test set contains 96 IDs and uses 3,803 infrared images for the query set.\nRegDB [20] dataset consists of 412 pedestrians, each of which has 10 visible-light images and 10 infrared images. It randomly divides images of 412 individuals into two equal parts to create the training and test sets. RegDB has two test modes, including the VIS2IR setting and the IR2VIS setting.\nLLCM [53] dataset is the latest VI-ReID benchmark, which comprises 30,921 images from 713 unique identities. The testing set includes 13, 909 images from 351 different identities. Both VIS2IR and IR2VIS modes are employed for evaluating the performance of the VI-ReID. Compared to RegDB and SYSU-MM01, LLCM poses a greater challenge, primarily in three aspects: (1) more images from various perspectives. (2) complex low-light conditions. (3) a longer time span. These collectively make the VI-ReID task considerably more challenging.\nEvaluation metrics: To make fair comparisons, all experiments in this study use two commonly used metrics to evaluate the performance: Rank-1 accuracy and mean average precision (mAP)."}, {"title": "4.2 Implementation details", "content": "We chose the pre-trained ResNet50 as the backbone. The BNNeck [18] was used in the classification head, and the features of the last layer were split according to the ideas of PCB [25] and MGN [30]. For SYSU-MM01 and LLCM, the number of split local features is $N = 2$ and $M = 0$, respectively. For RegDB, $N = 6$ and $M = 2$. The image size is resized to $384 \\times 144$. The remaining implementation details can be found in the supplementary materials."}, {"title": "4.3 Comparison with state-of-the-art methods", "content": "SYSU-MM01 and RegDB: Table 1 compares our method with SOTA methods. From Table 1, it can be observed that our model achieved the best results by almost all the metrics on both datasets, with only a slight gap in the Rank-1 metric for one mode in RegDB and SYSU-MM01. Specifically, on the SYSU-MM01 dataset, WRIM-Net achieves substantial improvement in the Indoor-Search scenario and Multi-shot mode.\nLLCM: Table 2 compares WRIM-Net and previous methods on the LLCM dataset. The results show that our model consistently outperforms the state-of-the-art, with all metrics significantly exceeding it."}, {"title": "4.4 Ablation Study", "content": "The WRIM-Net consists two components: the MIIM module and the AICL approach. In this section, we perform a detailed ablation study to evaluate the role of each component. The experiment was done on the LLCM dataset. The results are shown in Table 3.\nBaseline. The baseline method uses ResNet50 as the backbone network, followed by the BNNeck [18], and finally a fully connected layer as the classifier. The baseline employs both global and local features, guiding $P_5$ solely through cross-entropy loss, without auxiliary information of $P_4$."}, {"title": "Effectiveness of MIIM and AICL", "content": "As shown in Table 3, the results are significantly improved with the addition of MIIM compared to the baseline. This implies that the MIIM module effectively mines modality-invariant information. Table 3 also reveals significant improvements in the model's capabilities with the inclusion of AICL. Specifically, introducing MIIM alone results in a 4.89% increase in Rank-1 and a 4.51% increase in mAP. Introducing AICL alone leads to a Rank-1 increase of 4.41% and an mAP increase of 4.22%. When both MIIM and AICL are incorporated into the model, the Rank-1 further rises by 7.96%, and the mAP increases by 7.7%. For further details on the ablation of fusion modes within the MIIM's internal SCR module and auxiliary information in AICL, please refer to the supplementary materials."}, {"title": "Impact of Different Configurations of MIIM", "content": "The results of different configurations of MIIM are presented in Table 4. From Table 4, it can be observed that when employing separate MIIM after Block 1 and 2 (Block1/2) alone, the Rank-1 accuracy increases by 2.99%, and the mAP increases by 2.76% compared to the configuration without MIIM. Furthermore, utilizing only shared MIIM after Block 3 and 4 (Block3/4) yields a 1.33% increase in Rank-1 and a 1.45% improvement in mAP. It is evident that placing MIIM in shallower layers of the network for specific-modality multi-dimension information mining, an aspect overlooked by existing methods, can lead to more significant performance improvements. This is attributed to the innovative and low-complexity design of MIIM. For more details on the ablation studies of MIIM configurations, please refer to the supplementary materials."}, {"title": "Comparison with triplet loss and common contrastive loss", "content": "To validate the superiority of the CMKIC loss, we compare it with the triplet loss and common contrastive loss (randomly selecting one positive sample). The experiments were conducted under complete configuration conditions, and the results are presented in Table 5. From the table, we can observe that our CMKIC loss significantly outperforms traditional triplet loss and contrastive loss."}, {"title": "Comparison with other attention mechanisms", "content": "To demonstrate the superiority of MIIM over other attention methods, we compare MIIM with CBAM [35] and Non-Local [32]. CBAM conducts spatial and channel attentions within each block of the backbone network, while Non-Local focuses solely on spatial attention within specific blocks. From Table 6, we can observe that MIIM significantly outperforms CBAM and Non-Local. Here, MIIM refers to the network with the baseline augmented by MIIM. Non-Local and CBAM configurations are applied to the backbone network based on [32,35]. Our Rank-1 accuracy is 4.60% higher, and mAP is 3.73% higher compared to CBAM. Furthermore, although MIIM demonstrates comparable complexity to Non-Local, it notably outperforms Non-Local."}, {"title": "4.5 Parameters analysis", "content": "Analysis of parameters $\\lambda_1$ and $\\lambda_2$. Setting $\\lambda_1$ and $\\lambda_2$ to 0.5 and 0.1 respectively gives the best results. Details can be found in the supplementary materials.\nAnalysis of the $k_s$ in SCC. To determine the optimal $k_s$ parameter, we conducted parameter experiments. The results, shown in Table 9, indicate that setting $k_s$ to 3 yields the best performance. Specifically, setting $k_s$ to 3 leads to a 0.61% increase in Rank-1 and a 0.64% increase in mAP compared to setting $k_s$ to 1 (equivalent to no compression). This compressed design enables GRI to further facilitate global information interaction, enhancing network performance while maintaining low complexity.\nAnalysis of top-K in CMKIC Loss. To validate the effectiveness of top-K and determine the optimal value for CMKIC loss, experiments were conducted as shown in Table 10. The results indicate that the best performance is achieved when top-K is set to 4. In CMKIC loss, if we randomly select top-K positive samples, there is a significant decrease of 1.92% in Rank-1 (as shown in the fifth column). This indicates that our top-K Key-Instance design is effective in assisting the network in mining modality-invariant information."}, {"title": "4.6 Visualization Analysis", "content": "To observe the MIIM module, we utilize Grad-Cam [24] to visually analyze the features extracted by both the baseline network and the baseline network added with MIIM. Figure 4 illustrates the visualizations obtained from an infrared query image and its corresponding visible image from the gallery set. From"}, {"title": "5 Conclusion", "content": "We have introduced the WRIM-Net for VI-ReID, which emphasizes modality-invariant information mining across a wide range. To achieve this goal, we developed the MIIM module for extracting specific-modality and shared-modality features through multi-dimension interactions. And as a plug-and-play module, it is placed at different positions of WRIM-Net to expand the range of mining information. We also designed the AICL approach, which guides the network to better explore modality-invariant information by utilizing Cross-Modality Key-Instance Contrastive loss. Extensive experiments on three standard datasets demonstrated the superiority of WRIM-Net with the best performance of almost all the metrics on all three benchmarks."}]}