{"title": "WRIM-Net: Wide-Ranging Information Mining Network for Visible-Infrared Person Re-Identification", "authors": ["Yonggan Wu", "Ling-Chao Meng", "Yuan Zichao", "Sixian Chan", "Hong-Qiang Wang"], "abstract": "For the visible-infrared person re-identification (VI-ReID) task, one of the primary challenges lies in significant cross-modality discrepancy. Existing methods struggle to conduct modality-invariant information mining. They often focus solely on mining singular dimensions like spatial or channel, and overlook the extraction of specific-modality multi-dimension information. To fully mine modality-invariant information across a wide range, we introduce the Wide-Ranging Information Mining Network (WRIM-Net), which mainly comprises a Multi-dimension Interactive Information Mining (MIIM) module and an Auxiliary-Information-based Contrastive Learning (AICL) approach. Empowered by the proposed Global Region Interaction (GRI), MIIM comprehensively mines non-local spatial and channel information through intra-dimension interaction. Moreover, Thanks to the low computational complexity design, separate MIIM can be positioned in shallow layers, enabling the network to better mine specific-modality multi-dimension information. AICL, by introducing the novel Cross-Modality Key-Instance Contrastive (CMKIC) loss, effectively guides the network in extracting modality-invariant information. We conduct extensive experiments not only on the well-known SYSU-MM01 and RegDB datasets but also on the latest large-scale cross-modality LLCM dataset. The results demonstrate WRIM-Net's superiority over state-of-the-art methods.", "sections": [{"title": "1 Introduction", "content": "Person re-identification aims to match images of a person of interest in a query set with those in a gallery set, which may have been captured by different cameras [48]. Existing methods [1, 2, 18, 25, 30, 48] have achieved remarkable performance in visible-light person re-identification. However, visible-light cameras face challenges in capturing clear images under poor lighting conditions. To address this deficiency, infrared cameras are widely employed in modern surveillance systems. Based on this, the VI-ReID task has garnered increasing attention. The VI-ReID task is to match individuals in a gallery set who share the same ID but belong to the opposite modality (infrared or visible). VI-ReID is an immensely challenging task, primarily attributed to significant modality discrepancies, as shown in Figure 1. Existing methods can be broadly categorized into two main types: feature-level methods and image-level methods. Feature-level methods aim to map cross-modality features into a common space [5,38,43,48], while image-level methods [6,28,29,33] generate diverse-mode images to reduce the modality gap. These methods demonstrate outstanding performance in VI-ReID tasks. However, they still fall short in fully mining modality-invariant information. On one hand, the feature-level methods easily overlook specific-modality multi-dimension information. On the other hand, the image-level methods encounter difficulties in extracting sufficient information due to the scarcity of image pairs. In Figure 1, we observe significant differences in images across different modalities. Drawing inspiration from the human visual system, we prioritize the overall shape and the connections between details without focusing on very subtle details per se. For example, in the spatial dimension, our primary focus lies in the non-local spatial relationships, such as contour, posture, and proportions between different body parts (head, arms, knees, etc.). In the channel dimension, due to significant differences in color between modalities, our focus is on information such as texture, which reflects variations in color. All of these constitute modality-invariant information, an aspect where the methods mentioned above are less proficient at mining. Specifically, these methods struggle to simultaneously explore information across multi-dimensions and multi-modalities (specific and shared modality). For example, AGW [48], DDAG [47], and MPANet [38] employed attention mechanism for extracting information on spatial or channel dimension. However, they applied attention mechanisms in either spatial or channel dimensions, neglecting multi-dimension interaction. Furthermore, they only consider deep layers, disregarding the significance of specific-modality information. While some methods like AGW [48] proposed a dual-stream architecture to extract specific-modality information, they merely segregate different modalities without specific-modality multi-dimension information mining. Building upon the above issues and aiming to mine modality-invariant information across a wide range, we propose the Multi-dimension Interactive Information Mining (MIIM) module, as illustrated in Figure 3. Enhanced by the well designed Global Region Interaction (GRI) module, MIIM establishes long-range dependencies through global spatial interactions, effectively extracting non-local spatial-related information such as posture and shape, and simultaneously extracts channel-related information like texture through channel interactions. Furthermore, by being placed in both shallow and deep layers, MIIM captures specific-modality and shared-modality multi-dimension information. To address the significant modality gap in VI-ReID tasks, guiding the network in better mining modality-invariant information is crucial. [27,31] suggest that contrastive loss aids the network in mining invariant information by attracting positive pairs and separating negative samples. Building upon the aforementioned considerations, We introduce an Auxiliary-Information-based Contrastive Learning (AICL) approach, depicted in the right portion of Figure 2, which devises a Cross-Modality Key-Instance Contrastive (CMKIC) loss. The core of AICL lies in employing the CMKIC loss to guide the network in mining modality-invariant information. Additionally, an extra layer of auxiliary information is utilized to further enhance the mining of modality-invariant information. In summary, our main contributions are summarized below:"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Visible-Infrared Person Re-Identification", "content": "VI-ReID is a challenging task due to the modality differences between infrared and visible images. Existing image-level methods fuse modalities at the data level to alleviate the modality discrepancy [7,28,29], while feature-level methods map visible (VIS) and infrared (IR) features into a common space [5,38,43, 48]. Often, image-level methods suffer from a lack of high-quality VIS-IR image pairs, leading to noise disruption. Feature-level methods are limited due to insufficient modality-invariant information mining. To better capture specific-modality and shared-modality information, the dual-stream network architecture is commonly employed for VI-ReID tasks [16, 44, 45, 48], where the parameters are separated in the shallow layers to obtain specific-modality information and shared in the deep layers to obtain shared-modality information. However, they merely segregate images from different modalities without multi-dimension information interaction. Our proposed WRIM-Net leverages non-local spatial and channel interactions to extract richer long-dependencies information. Additionally, by strategically placing MIIM at different positions, it effectively explores both specific-modality and shared-modality multi-dimension information."}, {"title": "2.2 Attention Mechanisms", "content": "Attention mechanisms have found widespread application in visual tasks to promote neural network performance by improving visual representation [12, 26, 32,35]. In the field of person re-identification, attention mechanisms have been incorporated as demonstrated in [2,32,40,48]. To capture long-range spatial relationships, [40] introduced second-order non-local attention modules; [48] introduced the non-local [32] mechanism for attentional feature extraction within the last two blocks of the network. DDAG [47] extracted potent features for cross-modality person re-identification by employing spatial attention modeling on the local features of pedestrians. However, most of the methods primarily employed attention mechanisms in the deep layers of the network, leading to the neglect of mining specific-modality multi-dimension information. The MIIM we propose here effectively mitigates this issue. MIIM achieves non-local spatial interactions through spatial compression and Global Region Interaction (GRI), and channel interactions via channel compression and expansion. The spatial and channel compression of MIIM effectively reduces computational complexity, enabling MIIM to be positioned in shallower layers for specific-modality multi-dimension information mining."}, {"title": "2.3 Contrastive Learning", "content": "Contrastive learning plays an important role in self-supervised learning [3, 10, 39,49], allowing the neural network to extract invariant features efficiently. Contrastive learning also has a significant impact on image-text multi-modalities learning [15, 22, 51]. [22] used contrastive learning to perform modality alignment of images and text for cross-modality retrieval. [15,51] employed contrastive learning initially to achieve modality alignment and shared networks for cross-modality feature learning. [41] employed joint contrastive learning to acquire color-invariant features in unsupervised VI-ReID tasks. [14] introduced a contrastive regularization loss to regularize the model, where positive samples for this contrastive loss involve mixing part descriptors with the same identity. Unlike the previous methods, our CMKIC loss neither employs additional methods to create data-augmented positive samples nor contrasts a single positive for each anchor. Instead, our CMKIC loss introduces a novel approach for selecting positive samples, specifically choosing top-K samples with the same ID as the anchor, different modalities, and the least similarity. This approach increases task difficulty, enabling the network to mine modality-invariant information."}, {"title": "3 Methodology", "content": "In this section, we provide a detailed description of the proposed method for the VI-ReID task. Our network uses a pre-trained single-stream network (ResNet50 [11]) to extract visible and infrared features. As shown in Figure 2, the proposed method consists of two main components. (1) MIIM module. MIIM enhances its non-local spatial interaction capability by incorporating the spatial compression and GRI module. (2) AICL approach. AICL utilizes the CMKIC loss to effectively guide the network in learning modality-invariant information and incorporates auxiliary information to further enhance its extraction."}, {"title": "3.1 Multi-dimension Interactive Information Mining Module", "content": "MIIM improves information extraction in several ways: (1) Employing global spatial interactions and establishing long-range dependencies to capture non-local spatial-related information, such as shape, pose, and object proportions. (2) Employing channel interactions to capture channel-related information, such as texture. (3) Employing the separate MIIM in the shallow layers to extract specific-modality multi-dimension information (often overlooked by many existing methods) and the shared MIIM in the deep layers to extract shared-modality multi-dimension information. The MIIM module primarily comprises three components: Spatial-Channel Compression (SCC), Global Region Interaction (GRI), and Spatial-Channel Restore (SCR). The input features first pass through a standard Batch Normalization (BN) layer and then proceed to the SCC. In SCC, features undergo two compressions. The first compression is done with a convolutional layer, and the resulting features serve as the Query (Q) for subsequent GRI. The second compression is executed by employing average pooling on the aforementioned features. Then, the resulting features serve as the Key (K) and Value (V) components of GRI. This approach not only achieves non-local spatial interaction but also further reduces computational complexity. Specifically, the input features $F_1 \\in \\mathbb{R}^{H_1\\times W_1\\times C_1}$ pass through a convolutional layer with a kernel size and stride of $r_s$, resulting in $F_2 \\in \\mathbb{R}^{H_2\\times W_2\\times C_2}$. $F_2$ undergoes average pooling with kernel size and stride of $k_s$, yielding $F_3 \\in \\mathbb{R}^{H_3\\times W_3\\times C_2}$. The resulting $F_2$ and $F_3$ features are flattened and then fed into the Global Region Interaction (GRI) component, which employs Multi Head Attention (MHA) [26]. In this setup, the flattened $F_2$ functions as the Query (Q), while flattened $F_3$ serves as both the Key (K) and Value (V). The resultant features, denoted as $F_1$, are obtained from the GRI module's operation, as indicated by the following formula:\n\n$F_1 = GRI(Q = F_2, K = F_3,V = F_3).$ (1)\n\nThe specific implementation of MHA is detailed in [26], where the number of heads is set to 8. To better leverage the positional information, we conduct sin-cos positional embedding [26]. In the processing of the GRI component, we utilize higher-resolution $F_2$ as the Query (Q), while lower-resolution $F_3$ serves as the Key (K) and Value (V), thereby shifting the focus of spatial interaction from interactions among all spatial points to a greater emphasis on long-range spatial regions, enabling global spatial interaction. After passing through the GRI component, the spatial resolution of $F_1$ remains the same as that of $F_2$. This design further reduces the computational complexity, enabling the placement of this module in the shallower layers of networks. Next, $C_2$ is restored to the original number of $C_1$ channels using a linear layer $W$, similar to Senet [12], resulting in feature $F_5$. Subsequently, a sigmoid activation is applied to the $F_5$ to generate weights $M_1$:\n\n$M_1 = sigmoid(W(F_5)).$ (2)\n\n$M_1$ is then reshaped and upsampled to recover the original spatial scale, resulting in the formation of weights $M_2 \\in \\mathbb{R}^{H_1\\times W_1\\times C_1}$. Finally, the input features $F_1$ are element-wise multiplied with $M_2$ and then passed through the ReLU and BN layers to produce the output feature $F_6$:\n\n$F_6 = BN(ReLU(F_1 \\bigodot M_2)).$ (3)\n\nIn our implementation, we significantly reduce the complexity of the module by compressing the channels and spatial dimensions. In the shallow layers of the network, the features have a larger spatial size, so we primarily focus on spatial dimension compression. Through judicious compression, we not only establish more effective long-range spatial dependencies but also enable MIIM to be conveniently implemented on shallow specific modalities. In the deeper layers of the network, characterized by longer channel lengths, our primary focus is on channel compression. Compressing both ensures that the network maintains a lightweight level of complexity during the following GRI operations. To enable MIIM to better assist the network in information mining, we employ separate MIIM for each modality after Block1 and Block2 to perform specific-modality multi-dimension information mining, as shown in Figure 2. Following Block3 and Block4, a shared MIIM is applied, which allows the network to perform the mining of shared-modality multi-dimension information."}, {"title": "3.2 Auxiliary-Information-based Contrastive Learning Approach", "content": "In this subsection, we introduce the proposed AICL approach, as illustrated in Figure 2. The approach involves employing our designed CMKIC loss behind the block4 layer of the network to guide the learning of modality-invariant features. Subsequently, auxiliary information of the block3 layer is leveraged to enhance the mining of modality-invariant information. Our CMKIC loss is different from common contrastive loss. It doesn't solely use one augmentation of the anchor as the positive sample. Instead, it selects the top-K least similar samples (Key-Instance) with the same ID but different modalities as positive samples. As the anchor and positive samples are from different modalities, this approach facilitates the network's exploration of crucial modality-invariant information. Following the approach used in PCB [25] and MGN [30], we partition P5, the features extracted by Block4, followed by a global average pooling layer, to obtain local features R\u2081 to RN. Also, another global average pooling layer is directly applied to P5 to obtain the global feature $R_g$. $R_g$ is then passed through a MLP head [3,4] to obtain $Z_5$, which is formulated as:\n\n$Z_5 = W_2 (ReLU(BN(W_1 (R_g)))),$ (4)\n\nwhere $W_1$ and $W_2$ represent a linear layer. Then, CMKIC loss is applied to $Z_5$.\n\nCMKIC loss serves as the core component enabling AICL to fulfill its mission. The design of our CMKIC loss for WRIM-Net is as follows:\n\n$L_{CMKIC \\_P5 \\_VI} = -\\sum_{i \\in I_{vis}} \\log \\frac{\\sum_{p \\in P_{infra(i)}} exp(z_i z_p / \\tau)}{\\sum_{a \\in A(i)} exp(z_i z_a / \\tau)} ,$ (5)\n\nThe VI in $L_{CMKIC \\_P5 \\_VI}$ indicates that the anchor represents the Visible feature, while the other samples involved in the loss correspond to the Infrared light features. $I_{vis}$ denotes the set of visible samples in the current training batch. $P_{infra}(i)$ represents the set of the top-K least similar infrared samples with the same ID as visible sample $i$. $z$ is the feature obtained after applying L2 normalization to the feature $z_5$ from Figure 2. $z_p$ refers to the feature z corresponding to a sample extracted from the set $P_{infra}(i)$, while $z_a$ denotes the feature z corresponding to the sample extracted from the set $A(i)$. Here, $A(i)$ represents the set of all samples in the infrared modality with different ID from $i$, as well as all samples in $P_{infra}(i)$. Similarly, for the case where the anchor is the infrared feature, the corresponding CMKIC loss is $L_{CMKIC \\_P5 \\_IV}$ (See supplementary materials for the formula). And the final $L_{CMKIC \\_P5}$ is:\n\n$L_{CMKIC \\_P5} = \\frac{1}{2} (L_{CMKIC \\_P5 \\_IV} + L_{CMKIC \\_P5 \\_VI}).$ (6)\n\nIn addition, the global and local features are also jointly optimized using a cross-entropy classification loss as:\n\n$L_{cls \\_P5} = \\frac{(E(-log p(R_g)) + \\sum_{l=1}^{N}E(-log p(R_l)))}{N+1},$ (7)\n\nwhere p() is the probability that the feature is correctly predicted by the classifier and E represents the expectation.\n\nOn the other hand, to fully leverage the auxiliary information from the additional layer, we use the ID loss to guide the features generated by Block3, $P_4$. Similar to those for $P_5$, we partition $P_4$ and obtain global features denoted as $Q_g$ and local features denoted as $Q_1$ to $Q_M$. We apply the triplet loss [23] to the global features $Q_g$ and cross-entropy loss for both the global features $Q_g$ and the local features. Finally, the ID loss is written as:\n\n$L_{ID\\_P4} = L_{cls\\_P4} + L_{tri \\_P4}.$ (8)"}, {"title": "3.3 Training and Inference", "content": "During training, we consider the three losses, CMKIC, the cross-entropy classification loss, and the ID loss, and have the total loss as a weighted sum:\n\n$L_{total} = L_{cls\\_P_5} + \\lambda_1 L_{CMKIC\\_P_5} + \\lambda_2 L_{ID\\_P_4},$ (9)\n\nwhere the hyper-parameters $\\lambda_1$ and $\\lambda_2$ are used to balance the contribution of each loss function. During the testing phase, we combine $R_g$, $R_1$ to $R_N$, $Q_g$ and $Q_1$ to $Q_M$ to form the final features for inference."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Datasets and Evaluation Setting", "content": "SYSU-MM01 [36] dataset comprises 491 identities captured by four visible (VIS) cameras and two infrared (IR) cameras, including two modes: All-Search and Indoor-Search. The training set comprises 22,258 visible images and 11,909 infrared images of 395 unique individuals. The test set contains 96 IDs and uses 3,803 infrared images for the query set.\n\nRegDB [20] dataset consists of 412 pedestrians, each of which has 10 visible-light images and 10 infrared images. It randomly divides images of 412 individuals into two equal parts to create the training and test sets. RegDB has two test modes, including the VIS2IR setting and the IR2VIS setting.\n\nLLCM [53] dataset is the latest VI-ReID benchmark, which comprises 30,921 images from 713 unique identities. The testing set includes 13, 909 images from 351 different identities. Both VIS2IR and IR2VIS modes are employed for evaluating the performance of the VI-ReID. Compared to RegDB and SYSU-MM01, LLCM poses a greater challenge, primarily in three aspects: (1) more images from various perspectives. (2) complex low-light conditions. (3) a longer time span. These collectively make the VI-ReID task considerably more challenging.\n\nEvaluation metrics: To make fair comparisons, all experiments in this study use two commonly used metrics to evaluate the performance: Rank-1 accuracy and mean average precision (mAP)."}, {"title": "4.2 Implementation details", "content": "We chose the pre-trained ResNet50 as the backbone. The BNNeck [18] was used in the classification head, and the features of the last layer were split according to the ideas of PCB [25] and MGN [30]. For SYSU-MM01 and LLCM, the number of split local features is N = 2 and M = 0, respectively. For RegDB, N = 6 and M = 2. The image size is resized to 384 \u00d7 144. The remaining implementation details can be found in the supplementary materials."}, {"title": "4.3 Comparison with state-of-the-art methods", "content": "SYSU-MM01 and RegDB: Table 1 compares our method with SOTA methods. From Table 1, it can be observed that our model achieved the best results by almost all the metrics on both datasets, with only a slight gap in the Rank-1 metric for one mode in RegDB and SYSU-MM01. Specifically, on the SYSU-MM01 dataset, WRIM-Net achieves substantial improvement in the Indoor-Search scenario and Multi-shot mode.\n\nLLCM: Table 2 compares WRIM-Net and previous methods on the LLCM dataset. The results show that our model consistently outperforms the state-of-the-art, with all metrics significantly exceeding it."}, {"title": "4.4 Ablation Study", "content": "The WRIM-Net consists two components: the MIIM module and the AICL approach. In this section, we perform a detailed ablation study to evaluate the role of each component. The experiment was done on the LLCM dataset. The results are shown in Table 3.\n\nBaseline. The baseline method uses ResNet50 as the backbone network, followed by the BNNeck [18], and finally a fully connected layer as the classifier. The baseline employs both global and local features, guiding P5 solely through cross-entropy loss, without auxiliary information of P4."}, {"title": "Effectiveness of MIIM and AICL", "content": "As shown in Table 3, the results are significantly improved with the addition of MIIM compared to the baseline. This implies that the MIIM module effectively mines modality-invariant information. Table 3 also reveals significant improvements in the model's capabilities with the inclusion of AICL. Specifically, introducing MIIM alone results in a 4.89% increase in Rank-1 and a 4.51% increase in mAP. Introducing AICL alone leads to a Rank-1 increase of 4.41% and an mAP increase of 4.22%. When both MIIM and AICL are incorporated into the model, the Rank-1 further rises by 7.96%, and the mAP increases by 7.7%. For further details on the ablation of fusion modes within the MIIM's internal SCR module and auxiliary information in AICL, please refer to the supplementary materials."}, {"title": "Impact of Different Configurations of MIIM", "content": "The results of different configurations of MIIM are presented in Table 4. From Table 4, it can be observed that when employing separate MIIM after Block 1 and 2 (Block1/2) alone, the Rank-1 accuracy increases by 2.99%, and the mAP increases by 2.76% compared to the configuration without MIIM. Furthermore, utilizing only shared MIIM after Block 3 and 4 (Block3/4) yields a 1.33% increase in Rank-1 and a 1.45% improvement in mAP. It is evident that placing MIIM in shallower layers of the network for specific-modality multi-dimension information mining, an aspect overlooked by existing methods, can lead to more significant performance improvements. This is attributed to the innovative and low-complexity design of MIIM. For more details on the ablation studies of MIIM configurations, please refer to the supplementary materials."}, {"title": "Comparison with triplet loss and common contrastive loss", "content": "To validate the superiority of the CMKIC loss, we compare it with the triplet loss and common contrastive loss (randomly selecting one positive sample). The experiments were conducted under complete configuration conditions, and the results are presented in Table 5. From the table, we can observe that our CMKIC loss significantly outperforms traditional triplet loss and contrastive loss."}, {"title": "Comparison with other attention mechanisms", "content": "To demonstrate the superiority of MIIM over other attention methods, we compare MIIM with CBAM [35] and Non-Local [32]. CBAM conducts spatial and channel attentions within each block of the backbone network, while Non-Local focuses solely on spatial attention within specific blocks. From Table 6, we can observe that MIIM significantly outperforms CBAM and Non-Local. Here, MIIM refers to the network with the baseline augmented by MIIM. Non-Local and CBAM configurations are applied to the backbone network based on [32,35]. Our Rank-1 accuracy is 4.60% higher, and mAP is 3.73% higher compared to CBAM. Furthermore, although MIIM demonstrates comparable complexity to Non-Local, it notably outperforms Non-Local."}, {"title": "4.5 Parameters analysis", "content": "Analysis of parameters $\u03bb_1$ and $\u03bb_2$. Setting $\u03bb_1$ and $\u03bb_2$ to 0.5 and 0.1 respectively gives the best results. Details can be found in the supplementary materials.\n\nAnalysis of the ks in SCC. To determine the optimal $k_s$ parameter, we conducted parameter experiments. The results, shown in Table 9, indicate that setting $k_s$ to 3 yields the best performance. Specifically, setting $k_s$ to 3 leads to a 0.61% increase in Rank-1 and a 0.64% increase in mAP compared to setting $k_s$ to 1 (equivalent to no compression). This compressed design enables GRI to further facilitate global information interaction, enhancing network performance while maintaining low complexity.\n\nAnalysis of top-K in CMKIC Loss. To validate the effectiveness of top-K and determine the optimal value for CMKIC loss, experiments were conducted as shown in Table 10. The results indicate that the best performance is achieved when top-K is set to 4. In CMKIC loss, if we randomly select top-K positive samples, there is a significant decrease of 1.92% in Rank-1 (as shown in the fifth column). This indicates that our top-K Key-Instance design is effective in assisting the network in mining modality-invariant information."}, {"title": "4.6 Visualization Analysis", "content": "To observe the MIIM module, we utilize Grad-Cam [24] to visually analyze the features extracted by both the baseline network and the baseline network added with MIIM. Figure 4 illustrates the visualizations obtained from an infrared query image and its corresponding visible image from the gallery set. From Figure 4, it can be observed that after adding MIIM, the network effectively focuses on pedestrians themselves, whether in the visible modality or the infrared modality, while paying more attention to global information. We attribute this to MIIM's improved non-local spatial and channel interactions, along with its specific-modality multidimensional information mining.\n\nTo observe the AICL, we use T-SNE [19] to visualize and compare the features extracted by the baseline and AICL. As shown in Figure 5. After using AICL, the feature distance of different modalities of the same ID is pulled closer while the feature distance between different IDs is pushed farther. This allows the network to more effectively mine modality-invariant information."}, {"title": "5 Conclusion", "content": "We have introduced the WRIM-Net for VI-ReID, which emphasizes modality-invariant information mining across a wide range. To achieve this goal, we developed the MIIM module for extracting specific-modality and shared-modality features through multi-dimension interactions. And as a plug-and-play module, it is placed at different positions of WRIM-Net to expand the range of mining information. We also designed the AICL approach, which guides the network to better explore modality-invariant information by utilizing Cross-Modality Key-Instance Contrastive loss. Extensive experiments on three standard datasets demonstrated the superiority of WRIM-Net with the best performance of almost all the metrics on all three benchmarks."}]}