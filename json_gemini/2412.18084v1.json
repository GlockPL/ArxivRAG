{"title": "Property Enhanced Instruction Tuning for Multi-task Molecule Generation with Large Language Models", "authors": ["Xuan Lin", "Long Chen", "Yile Wang", "Xiangxiang Zeng", "Philip S. Yu"], "abstract": "Large language models (LLMs) are widely applied in various natural language processing tasks such as question answering and machine translation. However, due to the lack of labeled data and the difficulty of manual annotation for biochemical properties, the performance for molecule generation tasks is still limited, especially for tasks involving multi-properties constraints. In this work, we present a two-step framework PEIT (Property Enhanced Instruction Tuning) to improve LLMs for molecular-related tasks. In the first step, we use textual descriptions, SMILES, and biochemical properties as multimodal inputs to pre-train a model called PEIT-GEN, by aligning multi-modal representations to synthesize instruction data. In the second step, we fine-tune existing open-source LLMs with the synthesized data, the resulting PEIT-LLM can handle molecule captioning, text-based molecule generation, molecular property prediction, and our newly proposed multi-constraint molecule generation tasks. Experimental results show that our pre-trained PEIT-GEN outperforms MolT5 and BioT5 in molecule captioning, demonstrating modalities align well between textual descriptions, structures, and biochemical properties. Furthermore, PEIT-LLM shows promising improvements in multi-task molecule generation, proving the scalability of the PEIT framework for various molecular tasks. We release the code, constructed instruction data, and model checkpoints in https://github.com/ chenlong164/PEIT.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) such as GPT- 4 (OpenAI, 2023), PaLM (Chowdhery et al., 2023) and LLaMa (Touvron et al., 2023; Dubey et al., 2024) have revolutionized the landscape of artificial intelligence and natural language processing (NLP), allowing machines to understand and generate human language with remarkable fluency and coherence. Based on encoded world knowledge (Petroni et al., 2019) and powerful instruct-following (Zhang et al., 2023) capabilities of LLMs, recent work has successfully used LLM for molecular-related tasks, achieving promising results (Fang et al., 2023; Zhang et al., 2024). Despite the success, LLMs still have limitations in tasks involving the generation of molecules with restricted properties, therefore limiting its potential applications such as drug discovery (Zhavoronkov, 2018; Elton et al., 2019). The challenges for tackling such tasks mainly lie in three aspects: (1) Existing studies have shown limitations of LLMs in understanding molecular representations (Grisoni, 2023), which makes it more challenging for handling such tasks with precise properties; (2) While there is some known SMILES-property pairing data, it often remains limited to predicting a single property and lacks datasets encompassing a wide range of properties (Wu et al., 2018). Moreover, most of these datasets do not include precisely described textual data, making it challenging to identify accurate tri-modal data pairs (Krenn et al., 2020); (3) To our knowledge, there are no suitable datasets or evaluation methods for multi-constraint molecule generation using LLMs, which poses challenges in standardizing and assessing such molecule generation tasks with these models (Jin et al., 2018; Elton et al., 2019). To address these challenges, we propose a framework called PEIT (Property Enhanced Instruction Tuning) to generate multi-modal molecular instruction datasets in bulk, aiming to enhance the capabilities of LLMs in multi-task molecule generation. Using the PEIT framework, our pre-trained model can handle both general tasks (e.g., molecule captioning (Edwards et al., 2022)) and property-related tasks such as property prediction (Chang and Ye, 2024). This makes it suitable for constructing data to evaluate multi-constraint molecule generation capabilities and for serving as instruction tuning data to improve existing open-source LLMs."}, {"title": "2 Related Work", "content": "Molecule generation. The goal is to design and generate molecules that meet specific properties, deep learning models have emerged and they are mainly categorized as follows: (1) text-based molecule generation that uses textual descriptions to generate molecules that match the given description (Liu et al., 2023b, 2024). MolT5 (Edwards et al., 2022) was the first proposed to realize translation between textual description and molecular SMILES. BioT5 aims to enhance molecular understanding by incorporating protein modality. They also perform molecule captioning, which is equivalent to the inverse task of text-based molecule generation. (2) property-guided molecule generation is the inverse process of molecular property prediction, where molecules are generated based on specific biochemical property constraints. Notably, SPMM (Chang and Ye, 2024) was the first to establish a connection between 53 biochemical properties and SMILES sequences, making multi-constraint molecule generation possible. However, few existing models can simultaneously perform text-based or multi-constraint molecule generation and molecule captioning.\nMolecular property prediction. Deep learning models have been developed for molecular property prediction each with their own advantages and limitations. Transformer-based models design attention mechanism to capture contextual contexts from large-scale SMILES sequences (Ross et al., 2022). The molecular graph can be directly obtained from SMILES sequences via RDKit (Landrum et al., 2013). Graph-based models develop diverse graph neural networks to learn differentiable representations (Wang et al., 2022). However, these methods ignore the potential that incorporating textual knowledge enables to realize new drug design objectives (Zeng et al., 2022; Liu et al., 2023a). Recently, a novel molecular pre-trained model named SPMM (Chang and Ye, 2024) that extends the application of multimodal pre-training approaches by aligning molecular structures and biochemical properties. This paper extends the multimodal pre-training to patterns of text-sequence-property triplets, which is defined flexibly by LLM-understandable textual prompts.\nInstruction tuning. Specialized datasets construction seems the effective way to enable LLMs to better perform the molecular-related tasks. For instance, Mol-Instructions (Fang et al., 2023) provides a large-scale biomolecular instruction dataset designed for LLMs, which contains a variety of instruction data ranging from small molecules, proteins, and biomolecular texts. Additionally, BioT5+ (Pei et al., 2024) integrates IUPAC names, extensive biological texts, and molecular data through multi-task instruction tuning, providing more comprehensive insights in the fields of drug discovery. How to generate reliable data related to molecular knowledge remains a challenge of instruction tuning for existing open source LLMs."}, {"title": "3 Method", "content": "3.1 Overview of PEIT Framework\nThe overview of PEIT framework is shown in Figure 1 (left), which consists of PEIT-GEN and PEIT-LLM. In PEIT-GEN, we generate a large number of \"SMILES-text\u201d and \u201cSMILES-property\" pairs to serve as multi-modal data. Then we design multiple multi-modal alignment objectives to pre-train PEIT-GEN. In PEIT-LLM, by using the pre-trained PEIT-GEN, we can predict a large number of triplets to generate more diverse SMILES inputs, and then construct diverse instruction data based on template filling. By utilizing the synthesized instruction data, PEIT-LLM enables the supervised fine-tuning of open-source LLMs including LLaMa (Dubey et al., 2024) and Qwen (Yang et al., 2024), enhancing the capabilities for multi-task molecule generation.\n3.2 Pre-training of PEIT-GEN\nThe pre-training stage of PEIT-GEN is shown in the right of Figure 1. For a given molecule, different representations offer unique and complementary features, which are crucial for comprehensive molecule understanding. PEIT-GEN aims to integrate information from three modalities simultaneously, including textual information T (text), molecular structure S (SMILES), and biochemical properties P (property-value). Such ability can help synthesizing sufficient instruction data for further enhancing the ability of LLMs. In particular, PEIT-GEN consists of three Transformer encoders $Enc_t$, $Enc_s$, $Enc_p$ and two decoders $Dec_t$, $Dec_p$, and we design different training objectives to align features from different modalities.\nCross-modal representation matching. We leverage pre-trained models SciBERT (Beltagy et al., 2019) as trainable $Enc_t$ for encoding textual data, BERT (Devlin et al., 2019) as $Enc_s$ and $Enc_p$ for encoding SMILES and properties. Then we obtain feature representations across all three modalities, establishing the foundation for feature alignment.\nWe propose cross-modal representation matching to align the representations from different perspectives by the same molecule. In particular, we introduce the SMILES-text matching loss $L_{st\\_match}$ and the SMILES-property matching loss $L_{sp\\_match}$, which serve as objectives for training the encoders. In this way, the model can effectively learn cross-modal relationships and improve performance in multi-modal tasks by aligning the feature spaces. The matching loss is calculated as follows:\n$L_{Cst\\_match} = ICE (y_{match}^{st}, MLP(Enc^{s}(S) \\oplus Enc^{t}(T)))$, (1)\n$L_{Csp\\_match} = ICE (y_{match}^{sp}, MLP(Enc^{s}(S) \\oplus Enc^{p}(P)))$, (2)\nwhere $y_{match}^{st}$ and $y_{match}^{sp}$ are labels as 0 or 1, indicating whether the corresponding SMILES-text or SMILES-property pairs are matching. $Enc(\\cdot)$ indicates the representation of the data (i.e., [CLS] token of Transformer encoder), $\\oplus$ is the concatenation operation, and $MLP(\\cdot)$ is the trainable multi-layer perception. The encoders are optimized by the cross-entropy loss $l_{ce}$ using the given data from different modalities.\nMulti-modal contrastive learning. The representation matching can be viewed as an explicit 2-way classification training. We further utilize contrastive learning to directly enhancing the representation by pulling semantically close neighbors together and pushing apart non-neighbors from data of different modalities. To calculate the similarity between the encoded features of different modalities, we extract the encoded features and then compute the instance-level similarities through the inner product:\n$sim(S,T) = (MLP^{s}(Enc^{s}(S)))^{T} MLP^{t} (Enc^{t}(T))$, (3)\n$sim(S, P) = (MLP^{s}(Enc^{s}(S)))^{T} MLP^{p} (Enc^{p}(P))$, (4)\nwhere $MLP^{s}, MLP^{t}$ and $MLP^{p}$ are multi-layer perceptions applied to SMILES, text, and property representations, respectively. Then, for the given SMILES S, text T, and property P, we compute the cross-modal batch-level similarities as follows:\n$S_{s2t} = \\frac{exp(sim(S,T) / \\tau)}{\\sum_{i=1}^{M} exp(sim(S,T_i) / \\tau)}$, (5)\n$S_{s2p} = \\frac{exp(sim(S,P) / \\tau)}{\\sum_{i=1}^{N} exp(sim(S,P_i) / \\tau)}$, (6)\nwhere M and N represent the total number of texts and property in the batch of data pairs, respectively. $\\tau$ is the temperature controlling the sharpness of the similarity. The intra-modal similarities $s_{s2s}, s_{p2p}$, and $s_{t2t}$ can be computed in similar manners.\nBased on the cross-modal and intra-modal batch-level similarities, the contrastive loss is formulated by calculating the cross-entropy according to one-hot encoded similarity vectors y, where the value is 1 for pairs derived from the same molecule or 0 for all other combinations:\n$L_{contrastive}^{Csp} = \\frac{1}{2} (ICE(y_{s2t}, S_{s2t}) + ICE(y_{t2s}, S_{t2s}) + ICE(y_{s2s}, S_{s2s}) + lCE(y_{t2t}, S_{t2t}))$, (7)\n$L_{contrastive}^{Csp} = \\frac{1}{2} (ICE(y_{s2p}, S_{s2p}) + ICE(y_{p2s}, S_{p2s}) + ICE(y_{s2s}, S_{s2s}) + lCE(y_{p2p}, S_{p2p}))$. (8)\nCross-modal masked language modeling. To further strengthen the model's capability in molecule captioning, we employ the masked language modeling (MLM; Devlin et al., 2019) to enhance the model performance on text generation. MLM is originally designed for the BERT encoder, which is not specifically used for generation. We design decoders to generate original unmasked property and textual description sequences, under the guidance of SMILES features through cross-attention. Specifically, given a pair of text and property, the calculation of vanilla self-attentions are as follows:\n$SelfAtt(T) = softmax(W_{Q}^{h}(T)(W_{K}^{h}(T))^{T})W_{V}^{h}(T)$,\n$SelfAtt(P) = softmax(W_{Q}^{h}(P)(W_{K}^{h}(P))^{T})W_{V}^{h}(P)$, (9)\nwhere $h(\\cdot)$ denotes the hidden representations, $W_Q, W_K$, and $W_V$ are the matrix for query, key, and values among the same modality, respectively. For text decoder $Dec_t$ and property decoder $Dec_p$, we propose cross-modal MLM objectives which further integrates SMILES features for masked text or property prediction via applying cross-attention:\n$CrossAtt(T) = softmax(W_{Q}^{h}(T)(W_{K}^{h}(S))^{T})W_{V}^{h}(T)$,\n$CrossAtt(P) = softmax(W_{Q}^{h}(P)(W_{K}^{h}(S))^{T})W_{V}^{h}(P)$. (10)\nBy introducing the SMILES features in attention layers for MLM training, we enable the model to utilize SMILES-text and SMILES-property data pairs to perform molecule captioning and property prediction. The cross-modal MLM loss $L_{MLM}^{st}$ and $L_{MLM}^{sp}$ are computed as follows:\n$L_{MLM}^{st} = - \\sum_{i=1}^{N} \\sum_{j=1}^{n} log Prob(w_j^{(i)} | Dec_t(w_\\backslash_j^{(i)}); \\theta_t)$, (11)\n$L_{MLM}^{st} = - \\sum_{i=1}^{N} \\sum_{j=1}^{n} log Prob(w_j^{(i)} | Dec_p(w_\\backslash_j^{(i)}); \\theta_p)$, (12)\nTraining. The overall training objective for pre-training PEIT-GEN is to minimize the sum of all three types of losses across three modalities:\n$L = \\alpha L_{st\\_match} + \\alpha L_{sp\\_match} + L_{contrastive}^{Cst} + L_{contrastive}^{Csp} + \\beta L_{MLM}^{st} + \\beta L_{MLM}^{sp}$, (13)\nwhere $\\alpha$ and $\\beta$ are hyper-parameters for balancing different loss terms.\n3.3 Instruction Tuning for PEIT-LLM\nTemplate Filling. The pre-trained PEIT-GEN offers unstructured data in the format of \u201ctext-SMILES-properties\u201d (i.e., text-structure-property) triplets. To obtain more task-specific data and to adapt to the strong instruction-following abilities of LLMs, we design templates for different downstream tasks, as shown in Figure 5 in Appendix A. For instance, in the text-based molecule generation task, we fix a general question format and then extract molecular descriptions from unstructured data to fill the description part of a pre-defined template, resulting in a natural question as instructions. The SMILES from unstructured triplets is used as the desired response. In this way, we can generate diverse task-specific instruction data in bulk for subsequent instruction tuning for LLMs.\nMulti-constraint molecule generation task. Molecule generation often requires to be conducted under multiple constraints rather than a single condition. In this work, we propose a new task to assess molecule generation through a variety of descriptors, by comparing the alignment between the generated molecules and specific criteria to evaluate the generative performance of LLMs. By using the large-scale unstructured data generated by PEIT-GEN, we can effectively synthesize sufficient data for evaluation. Specifically, we follow SPMM (Chang and Ye, 2024) and predict 5 common properties out of the 53 available biochemical properties for diverse SMILES, including ExactMolWt, MolLogP, MolWt, QED, and TPSA. Based on the template filling, the predicted multiple property-values can be used to construct data for multi-constraint molecule generation.\nSupervised fine-tuning. We select LLaMa3.1- 8B (Dubey et al., 2024) and Qwen2.5-7B (Yang et al., 2024) as base LLMs. We then perform standard supervised fine-tuning (SFT; Ouyang et al., 2024) by using the \u201cinstruction-response\" pairs. In practice, we construct totally 1 million instruction data of four different tasks (i.e., molecule captioning, text-based molecule generation, property prediction, and multi-constraint molecule generation) from 200k unstructured \u201ctext-SMILES-properties\" triplets obtained by PEIT-GEN."}, {"title": "3.4 Comparing PEIT-LLM with Biomolecular Models and LLMs", "content": "Table 1 shows a comparison of our PEIT-LLM with existing pre-trained models and general LLMs on multiple molecular generation tasks. For most of the pre-trained models such as MolT5 and BioT5, they focus on molecule captioning and text-based molecule generation, which can not handle property-related tasks. SPMM is a specialized model for property prediction. However, it lacks of generation ability due to the lack of textual descriptions. Current LLMs such as LLaMa and Qwen show strong performance on general NLP-based tasks through conversations or instruction-following. However, these general LLMs still have limitations in tasks related to molecule generation due to a lack of molecular knowledge. In contrast, through fine-tuning on diverse instruction data with rich molecular knowledge, PEIT-LLM can perform multiple molecule generation tasks simultaneously."}, {"title": "4 Experiments", "content": "4.1 Experimental Setup\nDataset. For pre-training PEIT-GEN, we extract approximately 480k molecular SMILES entries from the ZINC dataset (Irwin et al., 2012) and then generate SMILES-text pair data using MolT5 (Edwards et al., 2022). Additionally, we calculate 53 biochemical property-value via RDKit, resulting in nearly 480k \"text-SMILES-properties\" triplets for pre-training. Following MolT5, we use the CHEBI-20 dataset (Edwards et al., 2021) to evaluate PEIT-GEN's performance on molecule captioning and molecular property prediction. We split the CHEBI-20 dataset into training, validation, and test sets with an 8:1:1 ratio, and we verify the property values of each molecule via RDKit.\nFor pre-training PEIT-LLM, we utilize the 200k tri-modal data generated by PEIT-GEN and employ template filling to generate 200k instruction data for each downstream task. For molecular property prediction, we select two biochemical properties with distinct differences for evaluation, generating 200k instruction data for each property. Finally, we obtain a total of 1000k instruction data across four tasks for SFT training. Similar to PEIT-GEN, molecular property prediction tasks on PEIT-LLM can be validated by RDKit on CHEBI-20 dataset.\nBaseline Models. To demonstrate the efficacy of PEIT-GEN and PEIT-LLM, we compare various popular pre-trained models and LLMs including MolT5 (Edwards et al., 2022), BioT5 (Pei et al., 2023), BioT5+ (Pei et al., 2024), MolXPT (Liu et al., 2023b), GIT-Mol (Liu et al., 2024), SPMM (Chang and Ye, 2024), LLaMa3 (Touvron et al., 2023), LLaMa3.1 (Dubey et al., 2024), Qwen2.5 (Yang et al., 2024), and Mol- Instructions (Fang et al., 2023). Details of these baselines and evaluation metric are in Appendix B and C, respectively."}, {"title": "4.2 Comparing PEIT-GEN with Pre-trained Biomolecular Models", "content": "Molecule captioning. Results on CHEBI-20 molecule captioning are shown in Table 2. Our model demonstrates superior performance in generating high-quality and relevant molecular caption. PEIT-GEN achieved the best results in METEOR and ROUGE, and the second-best performance in BLEU. Notably, compared to BioT5 which performs the best in BLEU, our approach requires significantly less data. This indicates that using domain-specific models to generate paired data for pre-training is more efficient than single-modality pre-training, enabling excellent performance with much less training data.\nMolecular property prediction. The performance of PEIT-GEN in molecular property prediction is shown in Table 3. Following SPMM, we evaluate on 1,000 molecules from the ZINC dataset which were not included in the training set. Compared to SPMM, which is specifically designed for property prediction, PEIT-GEN achieves comparable performance while using only one-third of the data size across three modalities. In terms of RMSE, PEIT-GEN outperformed SPMM, while SPMM was slightly ahead by 0.11 percentage points in the R2 metric. These results demonstrate that PEIT-GEN can generate high-quality biochemical properties of molecules, highlighting the critical role of high-quality multimodal data in advancing molecular understanding tasks."}, {"title": "4.3 Comparing PEIT-LLM with LLMs", "content": "Molecule captioning.\nAs shown in Table 4, the comparison results show that our model outperforms general-purpose LLMs (Qwen-2.5 and LLaMa3.1) as well as Mol- Instructions, which utilizes a biochemical information instruction dataset for SFT. PEIT-LLM achieves the second-best performance in BLEU, METEOR, and ROUGE, but still lags behind BioT5+, which is specifically trained for molecule captioning task. This indicates that the responses from BioT5+ are closer to the standard answers of CHEBI-20, while PEIT-LLM generates more diverse responses. By comparing with Mol- Instructions, we demonstrate the quality of generated data by PEIT-GEN and the effectiveness of our instruction data through multi-task template-filling. Case study is provided in Table 6 of Appendix E to further illustrate this point.\nText-based molecule generation. The results for text-based molecule generation on the CHEBI-20 test set are shown in Table 4. PEIT-LLM outperforms other baselines in numerical metrics such as BLEU score, Levenshtein Distance, MACCS Fingerprint Similarity, Morgan Fingerprint Similarity, and RDKit Fingerprint Similarity. Meanwhile, BioT5+ and Mol-Instructions show an advantage in the Validity metric. This indicates that PEIT-LLM, after multi-task instruction fine-tuning, has a strong understanding of the key structural representations of molecules as well as their textual descriptions. Case study is provided in Table 7 of Appendix E to further illustrate this point. This also indirectly validates the effectiveness of the instruction data synthesized by our proposed PEIT-GEN.\nMolecular property prediction. For predicting single property, due to the large number of property, we selected two representative ones for prediction. The property ExactMolWt with relatively large numerical values (usually 100~1000), and property MolLogP with relatively small numerical values (usually -5~10) are shown in Table 5. The results show that PEIT-LLM outperforms all other LLMs in predicting specific biochemical properties, demonstrating that PEIT-LLM exhibits strong sensitivity to molecular properties, showing excellent predictive performance for both properties with large numerical values and those with smaller values. This confirms the feasibility of using multi-task SFT to enhance LLMs' understanding of molecular properties and further validates the reliability of the molecular property instruction dataset. Case study is provided in Table 8 of Appendix E to further illustrate this point.\nMulti-constraint molecule generation. Results for our proposed multi-constraint molecule generation task is shown in Table 5. PEIT-LLM surpasses baselines by large margin in both RMSE and R2 metrics. Case study is provided in Table 9 of Appendix E to further illustrate this point. Note that this task requires the model to meet the demands of multiple properties with precise values, placing high demands on the model's overall understanding capability. General-purpose LLMs, or those not specifically trained for this task, lack the required information storage and fitting abilities. As demonstrated, through our property enhanced instruction tuning, the model gain strong molecular understanding capabilities."}, {"title": "4.4 Analysis", "content": "Ablation study. Figure 2 shows the ablation study of SMILES-text matching loss $L_{st\\_match}$ and cross-modal contrastive loss $L_{contrastive}^{Csp}$, which are not considered in SPMM due to the lack of textual description modality. By removing these training objectives, the performance degradation across all metrics, with a more significant decline when both are removed simultaneously. This demonstrates that both $L_{st\\_match}$ and $L_{contrastive}^{Csp}$ are helpful in cross-modal feature alignment, thereby enhancing the performance of molecule captioning.\nImpact of SFT steps. Figure 3 and Figure 4 show the results of PEIT-LLM with different SFT steps.\nWe find that the performance steadily improved at first few epochs, showing that the instruction data is useful for both molecule captioning and multi-constraint molecule generation tasks. The performance gradually saturates around epochs 5-6. This indicates that the LLaMa-7B model achieves optimal performance with 1 million instruction data, and further training might lead to over fitting."}, {"title": "5 Conclusion", "content": "We propose a novel framework PEIT that aims to enable open-source LLMs to perceive multi-modal features for multi-task molecule generation. For this purpose, PEIT establishes cross-modal connections among molecular structures, textual description, and biochemical properties through multimodal representation alignment. Through template filling, PEIT can help synthesizing diverse task-specific instruction data for LLMs. We further introduce a new multi-constraint molecule generation task that requires generating novel molecules meeting multiple property constraints. Experiments show that PEIT achieves promising performances on molecule captioning, text-based molecule generation, and property-related tasks compared with various biomolecular models and LLMs."}, {"title": "Limitations", "content": "While PEIT is capable of achieving comparative or better performance over existing studies, it still has some limitations as follows: First, PEIT integrates the pre-trained PEIT-GEN model as part of the pipeline, so the performance of PEIT-GEN greatly affect the overall performance of PEIT-LLM. Second, PEIT-GEN uses three types of modality to construct the instruction data. However, some modalities data (e.g., knowledge graph and molecular images) might be more crucial than sequences for the molecular-related task. As a result, exploring the different modalities might lead to a different result. Lastly, the template utilized for instruction-tuning in this work still relies on manual design. Our approach is influenced by previous study that has been shown to be effective. Nevertheless, it would be intriguing to explore the development of automated methods for constructing superior instruction-tuning templates."}, {"title": "A Template Filling", "content": "We show the templates in Figure 5 for synthesizing instruction data."}]}