{"title": "ENSEMBLES OF LOW-RANK EXPERT ADAPTERS", "authors": ["Yinghao Li", "Vianne Gao", "Chao Zhang", "MohamadAli Torkamani"], "abstract": "The training and fine-tuning of large language models (LLMs) often involve diverse textual data from multiple sources, which poses challenges due to conflicting gradient directions, hindering optimization and specialization. These challenges can undermine model generalization across tasks, resulting in reduced downstream performance. Recent research suggests that fine-tuning LLMs on carefully selected, task-specific subsets of data can match or even surpass the performance of using the entire dataset. Building on these insights, we propose the Ensembles of Low-Rank Expert Adapters (ELREA) framework to improve the model's capability to handle diverse tasks. ELREA clusters the training instructions based on their gradient directions, representing different areas of expertise and thereby reducing conflicts during optimization. Expert adapters are then trained on these clusters, utilizing the low-rank adaptation (LoRA) technique to ensure training efficiency and model scalability. During inference, ELREA combines predictions from the most relevant expert adapters based on the input data's gradient similarity to the training clusters, ensuring optimal adapter selection for each task. Experiments show that our method outperforms baseline LoRA adapters trained on the full dataset and other ensemble approaches with similar training and inference complexity across a range of domain-specific tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "While general-domain large language models (LLMs) such as GPT-4 (OpenAI, 2022; 2023) and Llama (Touvron et al., 2023) have shown remarkable efficacy in diverse applications, adapting these models through supervised fine-tuning to specific domains or tasks remains indispensable for achieving optimal performance. For example, instruction following requires subtle model adjustments to specialized datasets that the general pre-training corpus alone cannot provide (Ouyang et al., 2022). Significant resources have been invested in constructing varied, high-quality datasets tailored for LLM fine-tuning such as Alpaca (Taori et al., 2023), the Pile (Gao et al., 2021), or Flan (Longpre et al., 2023). These efforts have fueled the development of specialized models that address complex tasks across fields such as medical diagnostics (Singhal et al., 2023), financial analytics (Yang et al., 2023), and scientific decision-making (Zhang et al., 2024b), or to provide reasoning to their results (Wei et al., 2022), which were tasks once deemed challenging for automated systems.\nNonetheless, fine-tuning LLMs on a comprehensive dataset frequently encounters the issue of conflicting gradient directions from varied training data points (Wang et al., 2021; Xia et al., 2024; Chen et al., 2024). This phenomenon complicates the update process of models, potentially leading to suboptimal performance. Wang et al. (2023d) demonstrate that mixing diverse instructional datasets can sometimes result in less than ideal outcomes compared to fine-tuning on a carefully selected subset of the data that directly addresses the task at hand. To enhance the relevance of training data to specific tasks, Xie et al. (2023) have proposed methods like importance resampling, which aligns the training dataset more closely with the target task distribution. Another innovative approach proposed by Xia et al. (2024), termed targeted instruction tuning, involves selecting a small percentage (about 5%) of training data that most significantly influences task performance based on the average gradients of tokens. This method has shown promise, achieving comparable or superior results to traditional full dataset fine-tuning across various tasks. In addition, Xia et al. (2024) also present better outcomes in selecting data points based on the gradient norm than sentence embeddings."}, {"title": "2 PRELIMINARIES", "content": "2.1 LANGUAGE MODELS AND PARAMETER-EFFICIENT FINE-TUNING\nDecoder-only LMs, pioneered by GPT (Radford et al., 2018), are built upon the decoder component of the Transformer architecture (Vaswani et al., 2017) and are among the most prevalent and thoroughly examined language models today. A pre-trained LM, denoted as M, learns the language patterns on extensive text corpora Dpre-train through an unsupervised next-token-prediction (NTP) objective, which minimized the negative log likelihood (NLL) of a subsequent token xt in a length-T sequence x \u2208 VT consisting tokens from the vocabulary V based on the preceding context x<t:\n\nLNTP(x) = \u2211t=1T \u2212log P(xt|x<t; \u03b8M),\n\nwhere \u03b8M are the network parameters of the LM. Originally designed for text completion, the pre-trained LMs have been enhanced with instruction-following or task-specific capabilities through targeted fine-tuning (Ouyang et al., 2022; OpenAI, 2022; 2023), expanding their utility across diverse applications. The fine-tuning process frequently adopts the NTP objective, utilizing a smaller, specialized fine-tuning dataset Dft that consists of instruction-response pairs xft = (xinstr, xresp).\nFull-parameter fine-tuning of high-performing LMs, which involves calculating \u2207\u03b8M LNTP(x) and updating \u03b8M accordingly, is often impractical due to computational constraints arising from their vast number of parameters. To address this issue, parameter-efficient fine-tuning (PEFT) techniques have been developed (Houlsby et al., 2019; Li & Liang, 2021; He et al., 2022), with LoRA being a prominent example. LoRA introduces adapter \u03b8 into the LM's linear layers whose weight"}, {"title": "2.2 GRADIENT FEATURE CALCULATION AND DATA SELECTION", "content": "Originally introduced by Pruthi et al. (2020) to estimate the impact of individual training examples on model performance, gradient-based data selection has been further applied to training data selection (Gou et al., 2023; Xia et al., 2024; Pan et al., 2024; Liu et al., 2024c; Yang et al., 2024). Unlike methods based on surface-form textual features-which utilize token statistics or sentence embeddings as selection criteria (Reimers & Gurevych, 2019; Xie et al., 2023), this approach employs parameter gradients V\u03b8. Specifically, when fine-tuning a LoRA adapter Q using stochastic gradient descent (SGD), the gradient feature g(x) for each sequence x can be computed as\n\ng(x) \u2208 R|\u03b8| = flatten (V\u03b8 LNTP(x)) .\n\nflatten(.) denotes the operation that reshapes matrices into vectors and concatenates them. Using this expression, we derive the trajectory influence of a training data point xft \u2208 Dft, quantified by the inner product between its gradient feature and that of a task-specific validation data point xvalid. This inner product is then accumulated across training epochs e, each weighted by the average learning rate \u03b7(e) for that epoch: \u03a3E\\_e=1\u03b7(e)(g(xft), g(xvalid)). By leveraging this formulation and adapting it to the Adam optimizer (\u00a7 3.2), Xia et al. (2024) demonstrate the efficacy of selecting a subset of training data with the highest influence scores for task-specific fine-tuning, achieving performance comparable to that obtained using the complete training dataset."}, {"title": "2.3 MIXTURE OF EXPERTS AND ENSEMBLES", "content": "Mixture of Experts (MoE) is an architecture that combines multiple expert models or network modules with a gating network (Szymanski & Lemmon, 1993; Jordan & Jacobs, 1994). In the context of LLMs, MoE was first adopted by Shen et al. (2023) for instruction-tuning and by Jiang et al. (2024) for LLM pre-training to reduce inference costs while achieving performance comparable to dense networks. This idea has been further developed in subsequent works (Zhu et al., 2024; Dai et al., 2024; Xue et al., 2024).\nUpon receiving an input, the MoE's gating network routes it to the relevant experts, which could be an entire feed-forward Transformer block (Jiang et al., 2024) or a fine-tuned LoRA adapter (Dou et al., 2023; Wu et al., 2024) for LMs. Routing could be either dense or sparse, depending on the fraction of the total experts are activated. The selected experts process the input and provide their outputs, which are aggregated at the end of the layer or block, typically through weighted averaging, to produce the final result. This dynamic and selective activation of experts ensures efficient computation and resource utilization. Mathematically, the output of a mixture of M experts can be expressed as:\n\nF(x) = \u2211m=1M \u03b1m(x)Em(x); \u2211m=1M \u03b1m(x) = 1, |{m|dm(x) = 0}=1| \u2264 M,\n\nwhere Em is an expert model, and 0 < \u03b1m < 1 is its weight predicted by the gating network. Here we extend the definition of x to any kind of layer input.\nOn the other hand, Deep Ensembles utilize a collection of multiple models with identical architecture that are trained independently with different parameter initializations (Lakshminarayanan et al., 2017; Gleave & Irving, 2022). During inference, the last-layer predictions of these models, which could be either pre-activation logits or post-activation probabilities, are averaged to improve the overall performance. Suppose we have N models {Mn}Nn=1 in the ensemble, the output would be:\n\nMens(x) = 1N \u2211n=1N Mn(x)."}, {"title": "3 METHOD", "content": "In this section, we introduce the pipeline of ELREA, designed to enhance the fine-tuning of LLMs for improved downstream tasks by leveraging a mixture of LoRA experts in a Deep Ensembles framework. The pipeline, shown in Figure 1, consists of three main steps: 1) full-data adapter tuning, 2) gradient calculation, and 3) clustering and per-cluster fine-tuning. During inference, we estimate the similarity between the gradient of test instructions and the cluster instances to determine the influence of each cluster on the final prediction. The details of each step are elaborated below.\n3.1 FULL-DATA ADAPTER TUNING\nThe first step involves fine-tuning a base LoRA adapter Qbase from the backbone language model M on the entire fine-tuning dataset Dft for E epochs using the NTP objective (equation 1). This process captures a broader spectrum of general and task-specific knowledge and enhances the model's basic instruction-following abilities. The adapted model checkpoints {M + Q(e)base}E\\_e=1, where Q(e)base denotes the adapter checkpoint at the end of training epoch e, along with the corresponding optimizer states, provide the necessary parameters to calculate the gradient features (Xia et al., 2024).\n3.2 GRADIENT CALCULATION\nWith Adam optimizer (Kingma & Ba, 2015), which is the most adopted for LM fine-tuning, the gradient feature g(x) for each sequence x is extended from equation 2 to consider the 1st and 2nd order momentum terms with decay rates \u03b21 and \u03b22, as derived by Xia et al. (2024):\n\ngdam(t)(x) = \u03b7(t) \u00b7 m(t)/(\u221av(t) + \u03b5);\nm(t) = (\u03b21m(t\u22121) + (1 \u2212 \u03b21)g)/(1 \u2212 \u03b21); v(t) = (\u03b22v(t\u22121) + (1 \u2212 \u03b22)g2)/(1 \u2212 \u03b22),\n\nHere we extend the definition of the operator \"+\" between the backbone model and an adapter to denote the addition of the weights of the corresponding network layers (Hu et al., 2022)."}, {"title": "3.3 CLUSTERING AND PER-CLUSTER FINE-TUNING", "content": "We then cluster the training gradient features {\u03b4(xft, instr)|xft, instr\u2208 Dft} into K clusters using the BIRCH algorithm (Zhang et al., 1996). The BIRCH algorithm is well-suited for large, high-dimensional datasets and demonstrates robustness against outliers. To reduce computational demands, we randomly select 5,000 data points from Dft for model fitting. This sample size adequately represents the feature distribution, and we use the resulting model to cluster all gradient features. Preliminary experiments show that the clustering algorithm is robust, i.e., it consistently produces identical or similar clusters when different random seeds are used. As BIRCH does not ensure balanced clusters, we reapply it to clusters exceeding five times the size of the smallest cluster. We iterate this process up to three times, each iteration targeting fewer clusters. Initially targeting 5 clusters, this method typically yields between 8 (after two iterations) and 10 (after three iterations) training clusters {DC}C\\_C=1, where C denotes the final number of clusters.\nWithin each cluster Dc, we proceed with LoRA fine-tuning from the base checkpoint Qbase, extending for several more epochs at a reduced learning rate utilizing the same NTP objective. This results in a collection of C LoRA expert adapters {Qc}C\\_c=1. Theoretically, each cluster contains training instances with similar gradient directions, which likely exert analogous effects on the model's behavior. Fine-tuning with clustered data aims to direct the model towards a more precise update path, thereby potentially enhancing the model's (i.e., M + Qc) performance on specific task types which are unidentified during fine-tuning."}, {"title": "3.4 ROUTING AND INFERENCE", "content": "To route an input instruction to appropriate expert adapters, we calculate the cosine similarity between the gradient of the instruction \u03b4test (xtest, instr) and the centroid of the gradients within each cluster \u03b4'c = \u2211x\ud835\udc56\u2208\ud835\udc37\ud835\udc50 \u03b4(xi, instr). The normalized form of \u03b4c is given by:\n\n\u03b4c = \u03b4\u2032c/||\u03b4\u2032c|| = 1|\ud835\udc37\ud835\udc50| \u2211xi\u2208\ud835\udc37\ud835\udc50 \u03b4(xi, instr)."}, {"title": "4 EXPERIMENTAL SETUP", "content": "Datasets We conducted experiments across two distinct evaluation categories: 1) general language understanding and reasoning, and 2) mathematical reasoning. For the first category, following Xia et al. (2024), we employ Flan V2 (Longpre et al., 2023), CoT (Wei et al., 2022), Dolly-15k (Conover et al., 2023), and OpenAssistant Conversations (K\u00f6pf et al., 2023) for fine-tuning, and MMLU (Hendrycks et al., 2021a) and BIG-bench Hard (BBH; bench authors, 2023; Suzgun et al., 2023) to test model performance. The training and test datasets have no distribution overlap, making this setup suitable for evaluating the model's generalization capabilities. For the mathematical reasoning category, we develop the MATH-Combined dataset by integrating existing resources including GSM8k (Cobbe et al., 2021), MathQA (Amini et al., 2019), SVAMP (Patel et al., 2021), and MATH (Hendrycks et al., 2021b) into a uniform format analogous to MATH. MATH-Combined utilizes in-domain test points, offering insights into selecting task-specific data for effective fine-tuning. Please refer to appendix B for dataset details and processing; and Table 4 for the statistics.\nModel and Fine-Tuning Our primary experiments involve fine-tuning the Gemma-2b model (Gemma Team, 2024b), specifically gemma-1.1-2b-it\u00b3, by applying rank-8 LoRA adapters to all linear layers, modifying about 0.39% of the total model parameters. For both dataset categories, we fine-tune the base adapter Qbase for 2 epochs using the Adam optimizer, with an initial learning rate of 5 \u00d7 10-5 that linearly decays to zero. Cluster-wise adapters Qc are initialized from Qbase and fine-tuned for the same duration with a slightly reduced learning rate of 2 \u00d7 10\u22125. These hyperparameters are fixed since we assume no access to additional task-specific validation data. The maximum token sequence length during training is 2,048, with a batch size equals to 16 sequences distributed across the GPUs. Following Xia et al. (2024), we set the gradient projection dimensionality for clustering dproj to 8,192, which we show leads to the best model performance. Please refer to appendices C and D for additional details."}, {"title": "5 RESULTS AND DISCUSSION", "content": "Main Results Table 1 presents the test set accuracy across various MATH-Combined subsets, along with the micro-averaged results. ELREA consistently outperforms baseline methods on most sub-datasets by an observable margin, with only occasional dips in performance. On average, EL-REA achieves performance gains of 9.67% and 3.56% over M + Qbase at ranks r = 8 and r = 64, respectively, without leveraging additional training data or external knowledge sources. Table 2 further highlights the robustness of ELREA in general language understanding and reasoning tasks, even under test conditions that diverge from those used during fine-tuning. This finding aligns with the results reported by Xia et al. (2024). A comparison between M + Qdataset and M + Qbase reveals that the former does not consistently outperform the latter. This observation suggests that a generalized approach to knowledge extraction across similar tasks (as illustrated in Figure 4) can sometimes be more effective than relying solely on dataset-specific expertise.\nThe MoE Routing and Merging frameworks, despite relying on pre-computed routing weights, still exhibit improvements over the baseline, which can be attributed to the ensemble effect of the experts. In contrast, the MoLE baseline, which employs a trainable router, consistently underperforms compared to M + Qbase. We hypothesize that the presence of multiple LoRA experts, each applied to a broad range of linear layers (appendix C), may lead to a large and complex scope of routing functions that is challenging to optimize. Consequently, the system likely converges to a suboptimal"}, {"title": "6 CONCLUSION", "content": "We introduced Ensembles of Low-Rank Expert Adapters (ELREA), a framework designed to address the challenge of conflicting gradient directions during the fine-tuning of LLMs across diverse datasets. ELREA develops multiple LoRA expert adapters, each optimized for a specific data cluster with similar gradient profiles. These adapters collaboratively generate predictions by dynamically adjusting their contributions based on the input's gradient characteristics, effectively resolving gradient conflicts without the need for task-specific data features or validation sets. Our approach, which requires only a single training session, enhances the adaptability of models to new or evolving tasks and outperforms traditional LoRA adapters and other ensemble techniques across a variety of applications. Ablation studies confirm that both the ensemble structure and the gradient-based clustering and routing mechanisms are integral to ELREA's effectiveness. These findings underscore the framework's potential for efficient and scalable application of LLMs in practical settings.\nLIMITATIONS\nCompared to MoE approaches, ELREA incurs higher computational overhead during inference because multiple expert adapters are activated simultaneously. In our implementation, we replicate each input instance across the batch dimension and feed each copy to a distinct expert adapter. While this strategy reduces inference latency, it increases memory consumption, as shown in appendix G. Advanced adapter architectures, such as FLORA (Wen & Chaudhuri, 2024), may help mitigate these computational demands by reducing matrix multiplication operations, which we leave for future work. Due to limited computational resources, our experiments focus on smaller-scale backbone LLMs and expert adapters. We observe that the performance gains of ELREA over the primary baseline, M + Qbase, diminish when the backbone LLM is already strong or well-adapted to the target task. Consequently, ELREA may be more beneficial when the backbone LLM is limited in capacity or when the target task significantly diverges from the pretraining materials. Our hyperparameter tuning for both ELREA and the baseline models is preliminary. A more thorough exploration of various clustering and routing methods could further enhance ELREA's performance and offer deeper insights into the model's behavior."}, {"title": "A RELATED WORKS: MIXTURE OF EXPERTS AND DEEP ENSEMBLES FOR LANGUAGE MODELS", "content": "Mixture of Experts (MoE) have gained popularity in the field of LLM pre-training (Fedus et al., 2022; Jiang et al., 2024; Dai et al., 2024; Zhong et al., 2024) and fine-tuning (Gou et al., 2023; Shen et al., 2023; Luo et al., 2024; Zhou et al., 2024; Li et al., 2024a; Yang et al., 2024) as an approach to maintain model performance while reducing computational cost during inference. In LLM fine-tuning with MoE, the most frequent setup involves using each LoRA adapter, or simply a linear layer, as an expert, and employing a routing mechanism to select the most relevant adapters for each input token. The expert networks can be placed in parallel at different levels of the network modules (Cai et al., 2024), such as the feed-forward layers after multi-head attention (Dou et al., 2023; Diao et al., 2023; Li et al., 2024a), the linear layer within the attention block (Gou et al., 2023; Zhu et al., 2023; Luo et al., 2024; Tian et al., 2024), the Transformer block (Gao et al., 2024), or a combination of the above (Zadouri et al., 2024; Wu et al., 2024). In terms of routing, most works rely on trainable gating networks to predict the weights for each expert (Wang et al., 2023c; Li et al., 2024a; Wu et al., 2024; Chen et al., 2024; Liu et al., 2024a; Luo et al., 2024; Zadouri et al., 2024). Other studies leverage domain information or task-specific features to guide the routing process (Huang et al., 2024; Muqeeth et al., 2024; Liu et al., 2024b; Li et al., 2024a; Shen et al., 2024). Among these, the works most similar to ELREA are Gou et al. (2023), Zhou et al. (2024), and Yang et al. (2024), which use textual or gradient-based features to guide the routing process. Specifically, Gou et al. (2023) propose MoCLE, which first clusters the instruction embeddings using K-means and then trains a gating network to predict the top-k cluster assignments for each token. Zhou et al. (2024) design a task-wise decorrelation loss to encourage the router to learn oriented weight combinations of adapters tailored to homogeneous tasks. Yang et al. (2024) route an input token to the expert that generates gradients not conflicting with the average gradient of the entire sequence.\nResearchers have also explored the potential of applying Deep Ensembles to LLM pre-training and fine-tuning (Havasi et al., 2021; Tran et al., 2022; Cha et al., 2021; Liu et al., 2022; Gleave & Irving, 2022; Chronopoulou et al., 2023; Wang et al., 2023a; Jiang et al., 2023; Chen et al., 2023; Lu et al., 2024), as well as to related modules such as reward model learning (Coste et al., 2024; Zhang et al., 2024a; Ahmed et al., 2024) for reinforcement learning from human feedback (RLHF; Ouyang et al., 2022). Conventional Deep Ensemble methods train multiple models, or multiple LoRA adapters in the context of LLMs, on similarly distributed data and then average the predictions of these models to make the final prediction (Wang et al., 2023a; Coste et al., 2024). Another line, often referred to as \"fusion\", trains multiple models on heterogeneous data and then fuses the predictions of these models to make the final prediction (Jiang et al., 2023; Chen et al., 2023; Lu et al., 2024; Wang et al., 2024). Such works often do not impose any constraints on the model architectures, and the key to their success lies in how to select and combine the results from different models. For example, Jiang et al. (2023) propose a pairwise comparison method to effectively discern subtle differences between candidate outputs and enhance ranking performance for reward modeling. Wang et al. (2024) address the scenario of solving a task that requires different expertise scattered across multiple models and propose a fusion method based on k-nearest neighbors classifiers and a graph shortest path analogy to effectively combine the results of different models and achieve better performance."}, {"title": "B DATASETS", "content": "To evaluate the effectiveness of ELREA, we conducted experiments across two distinct categories: 1) general language understanding and reasoning, and 2) mathematical reasoning. Each category utilizes its own dedicated training and evaluation datasets, as detailed in Table 4.\nGeneral Language Understanding and Reasoning For the first category, we followed the methodologies outlined in Xia et al. (2024) and Wang et al. (2023d). We employed a diverse combination of datasets for fine-tuning our model:\n\u2022 Flan V2 (Longpre et al., 2023): This comprehensive collection encompasses over 1,800 NLP tasks, combining numerous existing datasets with various data augmentations. The tasks cover a wide range of NLP problems, including question answering, summarization, translation, and sentiment analysis."}, {"title": "C MODEL CONFIGURATIONS", "content": "Our primary experiments utilize Gemma-2b (Gemma Team, 2024b), which contains 2.5 billion network parameters, as the core framework for their relative efficiency in training and inference. Specifically, we employ the instruction-tuned variant gemma-1.1-2b-it, known for its efficiency in smaller-scale settings. We also conduct experiments with the larger and more advanced Gemma2 model gemma-2-9b-it (Gemma Team, 2024a) to investigate the impact of backbone model representativeness on the relative performance. For the LoRA modifications, we default to a rank"}, {"title": "D FINE-TUNING", "content": "For both dataset categories, we fine-tune the base adapter Qbase for 2 epochs using the Adam optimizer, with an initial learning rate of 5 \u00d7 10-5 that linearly decays to zero. Preliminary testing indicates that 2 epochs optimize performance for Qbase, ensuring a fair comparison with our method. We also observe a strong tendency toward overfitting beyond this point, as indicated by the loss value and gradient norm curve. Cluster-wise adapters Qc undergo an identical duration of fine-tuning at a slightly reduced learning rate of 2 \u00d7 10\u22125. These hyperparameters, derived from prior experience, are fixed without adjustments to preemptively accommodate unseen test data, diverging from the methods of Xia et al. (2024). Most fine-tuning sessions are conducted on an computing instance equipped with 8 NVIDIA A100 40GB GPUs, employing 4-bit quantization for the backbone model M and bf16 precision for adapters Q. This setup essentially uses QLoRA (Dettmers et al., 2023) rather than LoRA, but we do not specifically distinguish them as they both belong to the LoRA family and do not impact our conclusions. Additional training sessions utilize instances with 8 NVIDIA V100 32GB GPUs, using fp16 precision. We observe no difference in performance between these configurations apart from training speed. The maximum token sequence length for training is 2,048, with a batch size of 16 sequences distributed across the GPU instances. Only a few (< 100 for each dataset category using the Gemma-2b tokenizer) of training sequences are longer than this threshold, and we simply discard these instances."}, {"title": "E BASELINES", "content": "Our primary baseline is the base LoRA adapter M + Qbase, which is fine-tuned on the complete dataset for 2 epochs to achieve optimal performance, as detailed in Section D. Additionally, we consider a dataset-wise adapter M + Qdataset for MATH-Combined, where the adapter is fine-tuned and applied to each test subset individually. For instance, M + QMATH is fine-tuned on the MATH training subset of MATH-Combined and evaluated on its corresponding MATH test subsets; similarly, M + QGSM8K is fine-tuned on the GSM8K training subset and evaluated on the GSM8K test subsets, and so on.dd We also include the backbone model M itself as a baseline, which is used directly for test-case inference without any adapter fine-tuning. This baseline is applied only to BBH and MMLU datasets, as they contain in-context examples to guide the model's output format. All other baseline methods start from the M + Qbase checkpoint for further fine-tuning or inference, and include:\n\u2022 MoE Routing: This baseline implements layer-level routing with the same weights as EL-REA. Specifically, similar to equation 3, the averaged linear layer adapter output is given by\n\u2022"}, {"title": "F INFERENCE PROMPTING", "content": "Listing 1: An example of MATH-Combined inference prompts."}, {"title": "G EFFICIENCY ANALYSIS", "content": "Theoritical Analysis Theoretically, the computational overhead of ELREA compared to using M+ Qbase arises from the following aspects:\n1) the computation of the gradients of all training and test instructions; 2) clustering the gradient features of the training data points and computing the weights of each test data point on the clusters; 3) additional training steps to fit LoRA experts on the training clusters; 4) additional computational resources required to perform the forward pass on all LoRA experts for each test data point. In practice, step 2) only takes a few minutes with our clustering setup (\u00a7 3.3 and \u00a7 3.4), which is negligible compared to the entire training process and will be ignored in the following discussion.\nIf implemented properly, step 1) can also be integrated into the training and inference process with relatively small overhead. With a na\u00efve implementation, step 1) approximately equals the cost of training the model on the combination of training and test instructions (without answers) for one epoch, whose overhead depends on the average length of the instructions. For datasets such as OpenAssistant, MATH, GSM8k, and MathQA, whose average instruction length is comparatively much shorter than the answer length (Table 4), the overhead is minimal. In the worst-case scenario, step 1)'s overhead approximates the cost of training the model on the combination of training and test for one epoch, which is still acceptable for most fine-tuning datasets.\nAs the sum of our training cluster sizes equals the number of training data points, i.e., C=1 |Dc| = |Dft, the additional training steps in step 3) take the same amount of time as training the base adapter Qbase (\u00a7 3.4) on Dft, excluding CPU-disk I/O overhead, which is generally less than one minute in our experiments.\nThe complexity of step 4), however, is harder to estimate as it varies drastically according to the implementation. In our implementation, we choose to duplicate the input instruction along the batch dimension by the number of experts (i.e., C + 1) and perform a forward pass on the backbone and all experts simultaneously. This implementation has a similar cost to using a (C + 1)\u00d7 inference batch size with the base adapter M + Qbase.\nEmpirical Results To evaluate the efficiency of ELREA, we compared its computation time with that of the baseline model M + Qbase using a same set of hyper-parameters and device configuration on a single NVIDIA A101 80G GPU, except for the following specific parameters. We generate a toy dataset consisting of 2,000 training samples and 400 test samples as a smaller-scale but more controllable evaluation setup. Each sample contains 60 random lorem-ipsum words in both the instruction and the answer (which accounts for around 200 tokens each), matching the lengths in Dolly-15k (Table 4). We designate C = 4 experts and set the LoRA ranks to r = 8. The model"}, {"title": "H FURTHER ANALYSIS ON DATA CLUSTERING", "content": "To better understand the distribution of data across clusters, we analyzed the sources and categories within each cluster from the MATH-Combined dataset, as visualized in Figure 4. Here, \"data source\" refers to the individual datasets that comprise MATH-Combined (i.e., MATH, GSM8k, SVAMP, or MathQA) and language understanding and reasoning (i.e., CoT, Dolly-15k, Flan V2, and OpenAssistant), and \"category\" pertains to the finer-grain labels within these datasets. Notably, GSM8k is categorized uniformly under a single label \"gsm8k\" due to its lack of distinct category labels.\nAnalysis of Figure 4 reveals distinct correlations between clusters and data sources. For instance, in MATH-Combined, clusters 2, 3, and 5 predominantly contain samples from MATH, whereas clusters 0, 1, 6, and 7 primarily feature contributions from MathQA. This clustering also appears to group together tasks requiring similar mathematical skills; for example, cluster 4 heavily includes SVAMP samples, which typically assess algebraic problem-solving capabilities, alongside significant portions of \"Algebra\" and \"Prealgebra\" from the MATH dataset."}]}