{"title": "Fine-Tuning a Time Series Foundation Model with Wasserstein Loss", "authors": ["Andrei Chernov"], "abstract": "Inspired by recent advancements in large language models (LLMs) for Natural Language Processing (NLP), there has been a surge in research focused on developing foundational models for time series forecasting. One approach involves training LLM architectures on tokenized time series data using cross-entropy loss. Although this method has demonstrated promising results, cross-entropy loss is primarily designed for classification tasks and does not account for the distance between classes. To address this limitation, we propose using the Wasserstein loss for such architectures. To validate our approach, we fine-tuned a foundational time series model on 22 zero-shot datasets, comparing the performance of cross-entropy loss with that of Wasserstein loss. Our results demonstrate that replacing cross-entropy loss with Wasserstein loss significantly improves point estimation.", "sections": [{"title": "Introduction", "content": "Time series forecasting is a well-known problem across various domains, such as finance, retail, and healthcare. Traditionally, it has been addressed using statistical models like ARIMA [4] or Bayesian time series frameworks, such as Prophet [14]. More recent approaches have applied deep learning models [3, 6], which have demonstrated promising results in several competitions, such as the M5 competition [9].\nAt the same time, we are witnessing significant progress in foundational large language models (LLMs) for natural language processing (NLP) tasks [11, 15]. This raises the question of whether massive pretrained deep learning models can also perform well on time series data. However, there is a clear structural difference: NLP data consists of text, which can be tokenized, with each token treated as a class, naturally framing the problem as a classification task. This approach typically uses cross-entropy as the loss function, which treats all errors equally. If the model predicts the wrong class, the penalty remains the same regardless of which incorrect class is chosen. In contrast, time series data typically represents a continuous domain, leading to a regression problem, which motivates the use of distance-based loss functions, such as mean squared error. This distinction makes it challenging to directly apply LLM architectures to the time series domain.\nA first step to overcoming this distinction is to tokenize time series values to create a fixed vocabulary. This allows each token to be treated as a class, enabling the use of cross-entropy loss, as demonstrated in [1]. Although this approach has shown significant performance improvements, it still ignores the distances between classes. In this paper, we extend this approach by proposing to replace cross-entropy loss with Wasserstein loss, which accounts for the distance between classes. To validate our idea, we fine-tuned one of the models from [1] using both cross-entropy loss and Wasserstein loss on zero-shot datasets, i.e., datasets the model had not seen during training. We chose not to train models from scratch with Wasserstein loss due to: (a) the high cost of training from scratch, and (b) the fact that foundational time series models are still significantly smaller compared to LLMs, making fine-tuning in the time series domain more efficient and desirable for industrial applications."}, {"title": "Background", "content": "Time Series Forecasting. The time series forecasting problem can be formulated as fol-lows: given a time series dataset $X_1, X_2, ..., X_n$, the goal is to find the distribution $P(X_{n+1}, X_{n+2}, ..., X_{n+k}|X_1, X_2, ..., X_n)$, where all $x_i \\in \\mathbb{R}$, and $k$ represents the forecast horizon, referring to the number of future steps the model needs to predict. It is common to use an autoregressive approach, where one step is forecast at a time, and the result is appended to the input sequence to predict the next value. Another common simplification is to limit the model's input to only the last $m$ values of the time series. These two modifications simplify the original task to: $P(X_{n+1}/x_{n-m+1}, x_{n-m+2}, ..., x_n)$, where $m$ represents the context length.\nThere are two types of models for time series forecasting: local and global. Traditional statistical models, such as ARIMA [4], fit a separate model for each time series. These models are considered local because a trained model can forecast only one specific time series. In contrast, deep learning approaches train a model on a dataset containing multiple time series, allowing a single model to forecast across a set of time series [3, 6]. However, these models are typically effective only for a limited number of time series. Recent research [1, 5] in foundational time series models aims to build models that can achieve reasonable accuracy across a wide range of datasets.\nWasserstein Loss. The Wasserstein metric\u00b9 is widely utilized in the field of Optimal Transport [10] as a tool for calculating distances between distributions. One of the key advantages of Wasserstein loss, which we leverage in this paper, is that it takes into account the underlying geometry of the space. Consider a simple example: suppose we have three uniformly distributed univariate random variables: $X_1 \\sim U[0, 1]$, $X_2 \\sim U[1, 2]$, and $X_3 \\sim U[10,11]$. In this case, the Wasserstein distance has a closed-form solution and is given by $\\mathbb{E}(X_i) \u2013 \\mathbb{E}(X_j)|$, where $1 \\leq i, j \\leq 3$. Thus, the Wasserstein distance between $X_1$ and $X_2$ is 0.5, and between $X_1$ and $X_3$ it is 5, reflecting the difference between the domains of the random variables. In the general case, the Wasserstein distance between two distributions $P$ and $Q$ can be defined as follows:\n$W_p(P,Q) = \\underset{\\gamma \\in \\Gamma(P,Q)}{inf} (\\mathbb{E}_{(x,y) \\sim \\gamma}[D(x, y)^p])^{1/p}$"}, {"title": "Wasserstein Deep Learning Model for Tokenized Time Series", "content": "In this section, we discuss our approach to applying Wasserstein loss to a tokenized deep learning model. We believe that our method can be applied to any task where the distance between classes plays a significant role."}, {"title": "Time Series Preprocessing", "content": "In this paper, we fine-tuned a model from [1], consequently we apply the same mean absolute scaling [13] and quantization algorithm. Mean absolute scaling normalizes the data by the absolute mean, defined as $s = \\frac{1}{n}\\sum_{i=1}^{n} |x_i|$, which is calculated on the training data. The scaled data is given by $y_i = \\frac{x_i}{s}$. To perform quantization, we first need to set the minimum value ($y_{min}$) and the maximum"}, {"title": "Model Architecture", "content": "For the model architecture, we selected the pretrained Chronos-T5 (Small) model from [1], which is based on the T5 architecture [12]. The total number of tokens is 4096, two of which are reserved for special symbols: PAD and EOS. The EOS token denotes the end of the sequence, which is not necessary for time series applications, although its inclusion makes working with popular libraries more convenient. The PAD token is used to align the number of samples in each time series during batch processing. Therefore, the number of grid cells is $d = 4094$. The minimum value $y_{min}$ is set to -15, and the maximum value $y_{max}$ is set to 15. Consequently, the distance between neighboring centroids is calculated as $r = \\frac{y_{max}-y_{min}}{d-1} \\approx 0.0073$."}, {"title": "Loss Function", "content": "In this paper, we primarily focus on point estimation for forecasting univariate values. Therefore, we model the target distribution as a degenerate random variable that takes only a single value. While this assumption is not strictly necessary, and any distribution could be assigned to the target variable, especially if the goal is to improve probabilistic forecasting, we advise against using overly complex distributions due to the potential computational intensity of calculating the Wasserstein loss. For the forecast distribution, we utilize the distribution over tokens, which is obtained from the neural network after the softmax operation. As a result, the Wasserstein distance needed to be calculated between a degenerate distribution and a discrete distribution. We define the distance between two tokens as the distance between their centroids: $D(y_i, y_j) = r\\cdot |i \u2013 j|$. Thus, equation 1 simplifies, and we obtain a closed-form formula for the Wasserstein metric:\n$W_p(\\Upsilon_a, \\hat{\\Upsilon}) = r \\cdot (\\sum_{i=1}^d p_i |i - a|^p)^{1/p}$"}, {"title": "Forecasting and Evaluation Metrics", "content": "We maintain the same forecasting procedure and evaluation metrics as in [1] to ensure result com-parability. We use autoregressive sampling from the predicted distribution over tokens. To convert a token back to the original time series format, we first apply the detokenization function, which returns the centroid of the bin, $q^{-1}(j) = c_{j+1}$, and then multiply the result by the scaling factor $s$.\nFor point estimation, we take the median forecast from the model and evaluate it using the mean absolute scaled error (MASE) [8]. To assess the probabilistic forecast, we estimate the quantiles using 20 sample forecast paths and apply the weighted quantile loss (WQL) on nine uniformly spaced quantile levels: 0.1, 0.2, ..., 0.9.\nTo aggregate scores across different datasets, we compute the relative score of each model by dividing the model's score by the score of a seasonal naive forecast, then aggregate the relative scores across all datasets using the geometric mean to obtain the final metric."}, {"title": "Experiments", "content": "For our experiments, we selected the zero-shot datasets from [1], as these data were not seen by the model during training. To ensure reliable evaluation results, we filtered out datasets with fewer than 50 time series, leaving 22 datasets for experimentation. The last $k$ observations of each time series were allocated to the test set, while the remaining data were used for fine-tuning. The offset $k$ is unique to each dataset, and we maintained the same offsets as in [1]."}, {"title": "Fine-Tuning Results", "content": "As discussed in Section 3, we fine-tuned the pretrained Chronos-T5 (Small) model. For each dataset, we conducted 1000 fine-tuning steps, with the initial learning rate set to 0.001, which linearly decreased to 0 over the course of the steps. We fine-tuned the model using three different loss functions. The first two, Wasserstein-1 (W1) and Wasserstein-2 (W2), correspond to equation 2, with $p = 1$ and $p = 2$, respectively. The third loss function is the standard cross-entropy loss. Additionally, we calculated metrics for the model without fine-tuning.\nFigures 1 and 2 present the results for MASE and WQL, respectively. Appendix A provides the MASE and WQL values for each dataset. The Wasserstein loss significantly outperforms cross-entropy loss in point estimation; however, we observe a degradation in the WQL metric. This is a direct result of the loss design. Since we use a degenerate distribution as the target in the Wasserstein loss, the forecasted distribution becomes sharper and less suitable for quantile estimation compared to cross-entropy loss."}, {"title": "Discussion", "content": "In this paper, we proposed an approach to applying Wasserstein loss to large language model (LLM) architectures, originally designed for NLP tasks, to account for the topology of the space in domains where the distance between classes is important, particularly in the time series domain. To validate our approach, we demonstrated that fine-tuning the Chronos Small model with Wasserstein loss improves point estimation compared to fine-tuning with cross-entropy loss."}, {"title": "Future Work", "content": "The primary follow-up to this paper would be to train a foundational time series model from scratch using Wasserstein loss. Although probabilistic forecasting was not the main focus of this work, the model's ability to capture uncertainty is crucial. Improvements in probabilistic forecasting could be achieved by relaxing the assumption that the target distribution is a degenerate distribution."}]}