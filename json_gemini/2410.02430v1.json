{"title": "Predictive Attractor Models", "authors": ["Ramy Mounir", "Sudeep Sarkar"], "abstract": "Sequential memory, the ability to form and accurately recall a sequence of events or stimuli in the correct order, is a fundamental prerequisite for biological and artificial intelligence as it underpins numerous cognitive functions (e.g., language comprehension, planning, episodic memory formation, etc.) However, existing methods of sequential memory suffer from catastrophic forgetting, limited capacity, slow iterative learning procedures, low-order Markov memory, and, most importantly, the inability to represent and generate multiple valid future possibilities stemming from the same context. Inspired by biologically plausible neuroscience theories of cognition, we propose Predictive Attractor Models (PAM), a novel sequence memory architecture with desirable generative properties. PAM is a streaming model that learns a sequence in an online, continuous manner by observing each input only once. Additionally, we find that PAM avoids catastrophic forgetting by uniquely representing past context through lateral inhibition in cortical minicolumns, which prevents new memories from overwriting previously learned knowledge. PAM generates future predictions by sampling from a union set of predicted possibilities; this generative ability is realized through an attractor model trained alongside the predictor. We show that PAM is trained with local computations through Hebbian plasticity rules in a biologically plausible framework. Other desirable traits (e.g., noise tolerance, CPU-based learning, capacity scaling) are discussed throughout the paper. Our findings suggest that PAM represents a significant step forward in the pursuit of biologically plausible and computationally efficient sequential memory models, with broad implications for cognitive science and artificial intelligence research.", "sections": [{"title": "1 Introduction", "content": "Modeling the temporal associations between consecutive inputs in a sequence (i.e., sequential memory) enables biological agents to perform various cognitive functions, such as episodic memory formation, complex action planning and translating between languages. For example, playing a musical instrument requires remembering the sequence of notes in a piece of music; similarly, playing a game of chess requires simulating, planning, and executing a sequence of moves in a specific order. While the ability to form such memories of static, unrelated events has been extensively studied, the ability of biologically-plausible artificial networks to learn and recall temporally-dependent patterns has not been sufficiently explored in literature. The task of sequential memory is considered challenging for models operating under biological constraints (i.e., local synaptic computations) for many reasons, including catastrophic forgetting, ambiguous context representations, multiple future possibilities, etc.\nIn addition to the biological constraint, we impose the following set of desirable characteristics as learning constraints on sequence memory models.\n\u2022 The learning of one sequence does not overwrite the previously learned sequences. This property is defined under the continual learning framework as avoiding catastrophic forgetting and evaluated with the Backward Transfer (BWT) metric.\n\u2022 The model should uniquely encode inputs based on their context in a sequence. Consider the sequence of letters \u201cEVER\u201d; the representation of \u201cE\u201d at position 1 should be different from \"E\" at position 3, thus resulting in different predictions: \u201cV\u201d and \u201cR\u201d. Moreover, the representation of \u201cE\u201d at position 3 in \u201cEVER\u201d should be different from \"E\" at position 3 in \u201cCLEVER\u201d. Therefore, positional encoding is not a valid solution.\n\u2022 When presented with multiple valid, future possibilities, the model should learn to represent each possibility separately yet stochastically sample a single valid possibility. Consider the two sequences \u201cTHAT\u201d and \u201cTHEY\u201d; after seeing \u201cTH\u201d, the model should learn to generate either \"A\" or \"E\", but not an average or a union of both.\n\u2022 The model should be capable of incrementally learning each transition without seeing the whole sequence or revisiting older sequence transitions that are previously learned. This property falls under online learning constraints, also called stream learning.\n\u2022 The learning algorithm should be tolerant and robust to significant input noise. A model should continuously clean the noisy inputs using learned priors and beliefs, thus performing future predictions based on the noise-free observations.\nWe propose Predictive Attractor Models (PAM), which consists of a state prediction model and a generative attractor model. The predictor in PAM is inspired by the Hierarchical Temporal Memory (HTM) learning algorithm, where a group of neurons in the same cortical minicolumn share the same receptive feedforward connection from the sensory input on their proximal dendrites. The depolarization of the voltage of any neuron in a single minicolumn (i.e., on distal dendrites) primes this depolarized neuron to fire first while inhibiting all the other neurons in the same minicolumn from firing (i.e., competitive learning). The choice of which neurons fire within the same minicolumn is based on the previously active neurons and their trainable synaptic weights to the depolarized neurons, which gives rise to a unique context representation for every input. The sparsity of representations (discussed later in Section 3.2) allows for multiple possible predictions to be represented as a union of individual cell assemblies. The Attractor Model learns to disentangle possibilities by strengthening the synaptic weights between active neurons of input representations and inhibiting the other predicted possibilities from firing, effectively forming fixed point attractors during online learning. During recall, the model uses these learned conditional attractors to sample one of the valid predicted possibilities or uses the attractors as prior beliefs for removing noise from sensory observations.\nPAM satisfies the above-listed constraints for a sequential memory model, whereas the current state-of-the-art models fail in all or many of the constraints, as shown in the experiments. Our contributions can be summarized as follows: (1) Present the novel PAM learning algorithm that can explicitly represent context in memory without backpropagation, avoid catastrophic forgetting, and perform stochastic generation of multiple future possibilities. (2) Perform extensive evaluation of PAM on multiple tasks (e.g., sequence capacity, sequence generation, catastrophic forgetting, noise robustness, etc.) and different data types (e.g., protein sequences, text, vision). (3) Formulate PAM and its learning rules as a State Space Model grounded in variational inference and the Free Energy Principle."}, {"title": "2 Background and Related Works", "content": "Predictive Coding Predictive coding proposes a framework for the hierarchical processing of information. It was initially formulated as a time series compression algorithm to create a more efficient coding system. A few decades later, PC was used to model visual processing in the Retina as an inference model. In the seminal work of Rao and Ballard , PC was reformulated as a general computational model of the cortex. The main intuition is that the brain continuously predicts all perceptual inputs, resulting in a quantity of prediction error which can be minimized by adjusting its neural activities and synaptic strengths. In-depth variational free energy derivation is provided in Appendix B.1.\nPC defines two subgroups of neurons: value z and error . Each neuron contains a value node sending its prediction to the lower level $\\hat{z}_l = f_{l+1}(z_{l+1})$ through learnable function f, and error"}, {"title": "3 Predictive Attractor Models", "content": "3.1 State Space Model (SSM) formulation\nPAM can be represented as a dynamical system with its structure depicted by a Bayesian probabilistic graphical model, more specifically, a State Space Model, where we can perform Bayesian"}, {"title": "3.2 Preliminaries and Notations", "content": "Sparse Distributed Representation (SDR) Inspired by the sparse coding principles observed in the brain, SDRs encode information using a small set of active neurons in high dimensional binary representation. We adopt SDRs as a more biologically plausible cell assembly representation. SDRs have desirable characteristics, such as a low chance of false positives and collisions between multiple SDRs and high representational capacity (More on SDRs in Appendix G). An SDR is parameterized by the total number of neurons N and the number of active neurons W. The ratio $S = W/N$ denotes the SDR sparsity. A 1-dimensional SDR \u00e6 can be indexed as $x^i \\in {0,1}$, whereas a 2-dimensional SDR z can be indexed as $z^{ij} \\in {0,1}$. To identify the active neurons, we define the function $I : {0,1}^N \\rightarrow N^W$ to represent the indices of the active neurons in an SDR \u00e6 as $I(x) = {i|x^i = 1}$.\nContext as Orthogonal Dimension We transform the high-order Markov dependencies between observation states into a first-order Markov chain of latent states by storing context information in those latent states. The latent states SDRs, $z \\in {0,1}^{Nc\\times Nk}$, are represented with two orthogonal dimensions, where content information about the input is stored in one dimension with size Nc, while context related information is stored in an orthogonal dimension with size Nk. Therefore, the projection of the latent state z on the first dimension (i.e., $\\downarrow z$) removes all context information from the state. In contrast, adding context information to an observation state x expands the dimensionality of the state (i.e., $\\uparrow x$) such that context can be encoded without affecting its content. Competitive learning is enforced on the context dimension through lateral inhibition, effectively minimizing the collisions between contexts of multiple SDRs. We define a projection operator $\\downarrow: {0,1}^{Nc\\times Nk} \\rightarrow {0,1}^{Nc}$. Additionally, we define a projection operator $\\uparrow: {0,1}^{Nc} \\rightarrow {0,1}^{Nc\\times Nk}$ for 1-dimensional SDRs (i.e., x) as shown in equation 5.\n$(\\downarrow z) = \\begin{cases} 1 & \\text{if } \\exists j \\text{s.t. } z^{ij} = 1, \\\\ 0 & \\text{otherwise,} \\end{cases}, (\\uparrow x) = \\begin{cases} 1 & \\text{if } x^{i} = 1, \\\\ 0 & \\text{otherwise,} \\end{cases}$"}, {"title": "3.3 Sequence Learning", "content": "Given a sequence of T + 1 SDR patterns $[x_t]^{T+1}_{t=1}$, where $x_t \\in {0,1}^{Nc}$, the sequence can be learned by modeling the context-dependent transitions between consecutive inputs within the sequence. We define learnable weight parameters for transition and emission functions, $A \\in \\mathbb{R}^{NcNk \\times NcNk}, B\\in \\mathbb{R}^{N_c \\times N_o}$ respectively. A single latent state transition is defined as $ \\hat{z}_t = \\delta(A\\cdot z_{t-1}) = \\delta(a_t)$, where $\\delta$ is a threshold function transforming the logits at to the predicted SDR state $z_t$. The full sequence learning algorithm is provided in algorithm 1.\nContext Encoding through Competitive Learning Every observation $x_t$ contains only content information about the input; we embed the observation with context by expanding the state with an orthogonal dimension (i.e., $\\uparrow x_t$) which activates all neurons in the minicolumns at the indices $I(x_t)$. Then, for each active minicolumn, the neuron in a predictive state (i.e., higher than the prediction threshold) fires and inhibits all the other neurons in the same minicolumn from firing (i.e., lateral inhibition), as shown in Equation 6. If none - or more than one - of the neurons are in a predictive state, random Gaussian noise ($\\epsilon$) acts as a tiebreaker to select a context neuron. We do not allow multiple neurons to fire in the same minicolumn, which is different from HTM , where multiple cells can fire in any minicolumn (e.g., bursting).\n$m(a_t)^{ij}= \\begin{cases} 1 & \\text{if } a^{ij} = \\text{max}(\\{(\\bar{a}_t)^{ij} + \\epsilon\\}), \\\\ 0 & \\text{otherwise,} \\end{cases}, z_t = (\\uparrow x_t) \\cap m(a_t)$"}, {"title": "3.4 Sequence Generation", "content": "After learning one or multiple sequences using algorithm 1, we use algorithm 2 to generate sequences. First, we define two generative tasks: online and offline. In online sequence generation, a noisy version of the sequence is provided as input, and the model is expected to generate the original learned sequence. In offline sequence generation, the model is only provided with the first input, and it is expected to generate the entire sequence auto-regressively. For cases with equally valid future predictions (e.g., \"a\" and \u201ce\u201d after \u201cTH\u201d in \u201cTHAT\u201d and \u201cTHEY\u201d), the model is expected to stochastically generate either one of the possibilities (i.e., \u201cTHAT\u201d or \u201cTHEY\u201d). The online generation task is a more challenging extension of the online recall task in , where the noise-free inputs are provided, and the model only makes a 1-step prediction into the future. During offline sequence generation, the model randomly samples from the union set of predictions $\\downarrow \\bar{z}$ a single SDR with W active neurons (equation 9) to initialize the iterative attractor procedure. $\\pi$ denotes a random permutation function. This random permutation function allows the model to randomly generate a different sequence with every generation.\n$\\hat{x}^{i} = \\begin{cases} 1 & \\text{if } i \\in {\\pi(I(\\downarrow \\bar{z}))) }^W_{w=0}, \\\\ 0 & \\text{otherwise} \\end{cases}$"}, {"title": "4 Experiments", "content": "4.1 Evaluation and Metrics\nMetrics To evaluate the similarity of two SDRs, we use the Jaccard Index (i.e., IoU), which focuses on the active bits in sparse binary representations. Since the sparsity S of the representations can change across experiments and methods, we normalize the IoU by the expected IoU (Derived in Theorem 2 in Appendix B.3) of two random SDRs at their specified sparsities. The normalized IoU is computed as $ \\frac{IoU - E[IoU]}{1-E[IoU]}$. We use the Backward Transfer metric in evaluating catastrophic forgetting. Mean Squared Error (MSE) is used to compare images for vision datasets.\nDatasets We perform evaluations on synthetic and real datasets. The synthetic datasets allow us to manually control variables (e.g., sequence length, correlation, noise, input size) to better understand the models' behavior across various settings. Additionally, we evaluate on real datasets of various types (e.g., protein sequences, text, vision) to benchmark PAM's performance relative to other models on more challenging and real sequences. For all vision experiments, we use an SDR autoencoder to learn a mapping between images and SDRs (Details on the autoencoder are provided in Appendix D). We run all experiments for 10 different seeds and report the mean and standard deviation in all the figures. More experimental details and results are provided in Appendices D and F.\n4.2 Results\nWe align our evaluation tasks with the desired characteristics of a biologically plausible sequence memory model, as listed in the introduction. We show that PAM outperforms current predictive coding and associative memory SoTA approaches on all tasks. Most importantly, PAM is capable of long context encoding, multiple possibilities generation, and learning continually and efficiently while avoiding catastrophic forgetting. These tasks pose numerous significant challenges to other methods.\nOffline Sequence Capacity We evaluate the models' capacity to learn long sequences by varying the input size $N_c$, model parameters (e.g., Nk), and sequence correlation. The correlation is increased by reducing the number of unique patterns (i.e., vocab) used to create a sequence of length T. Correlation is computed as $\\frac{1}{vocab}$. In Figure 3 A, we vary the input size $N_c$ and ablate the models to find the maximum sequence length $T$ to be encoded and retrieved, in an offline manner, with a Normalized IoU higher than 0.9. We set the number of active bits W to be 5 unless otherwise specified. Results show that Hopfield Networks (HN) fail to learn with sparse representations; therefore, we use W of 0.5N only for HN and normalize the IoU metric accordingly. PAM's capacity significantly increases with context neurons Nk, as expected. HN's capacity also increases with the polynomial degree d of its separation function; however, as shown in Figure 3 B, the capacity sharply drops as correlation increases. PAM retains its capacity with increasing correlation, reflecting its ability to encode context in long sequences (i.e., high-order Markov memory). This context encoding property"}, {"title": "5 Conclusion", "content": "We proposed PAM, a biologically plausible generative model inspired by neuroscience findings and theories of cognition. We demonstrate that PAM is capable of encoding unique contexts with tremendous scalable capacity that is not affected by sequence correlation or noise. PAM is a generative model; it can represent multiple possibilities as a union of SDRs (a property of sparsity) and sample single possibilities, thus predicting multiple steps in the future despite multiple possible continuations. We also show that PAM does not suffer from catastrophic forgetting as it learns multiple sequences. PAM is trained using local computations through Hebbian rules and runs efficiently on CPUs. Future directions include hierarchical sensory processing and higher-order sparse predictive models."}]}