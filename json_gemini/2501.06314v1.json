{"title": "BioAgents: Democratizing Bioinformatics Analysis with Multi-Agent Systems", "authors": ["Nikita Mehandru", "Amanda K. Hall", "Olesya Melnichenko", "Yulia Dubinina", "Daniel Tsirulnikov", "David Bamman", "Ahmed Alaa", "Scott Saponas", "Venkat S. Malladi"], "abstract": "Creating end-to-end bioinformatics workflows requires diverse domain expertise, which poses challenges for both junior and senior researchers as it demands a deep understanding of both genomics concepts and computational techniques. While large language models (LLMs) provide some assistance, they often fall short in providing the nuanced guidance needed to execute complex bioinformatics tasks, and require expensive computing resources to achieve high performance. We thus propose a multi-agent system built on small language models, fine-tuned on bioinformatics data, and enhanced with retrieval augmented generation (RAG). Our system, BioAgents, enables local operation and personalization using proprietary data. We observe performance comparable to human experts on conceptual genomics tasks, and suggest next steps to enhance code generation capabilities.", "sections": [{"title": "Main", "content": "Large language models (LLMs) have been applied to various domain-specific contexts, including scientific discovery in medicine [45, 49, 56], chemistry [6, 7], and biotechnology [31]. Recent advances in LLMs have spurred their use in bioinformatics [13], a field encompassing data-intensive tasks such as genome sequencing, protein structure prediction, and pathway analysis. One of the most significant applications has been AlphaFold3, which uses transformer architecture with triangular attention to predict a protein's three-dimensional (3-D) structure from amino acid sequences [2]. Other applications include the use of protein language models in transforming amino acids into embeddings [58].\nWhile LLMs demonstrate impressive capabilities, these models have been found to struggle on complex genomics [4, 29, 39] and bioinformatics code generation [24, 47, 51] tasks with their performance and time to arrive at the correct solution varying significantly with task complexity. For example, Chat-GPT solved 97.3% of programming exercises from an introductory bioinformatics course within seven or fewer tries [37]. The model; however, was only able to"}, {"title": "Results", "content": "We devised three use cases of varying difficulty to evaluate our multi-agent system. These workflows, listed below, were designed to assess both conceptual genomics (analysis steps) and code generation tasks. We recruited bioinformatics experts, and provided them with the same inputs used by the multi-agent system. Each workflow involved completing the conceptual genomics and code generation tasks, providing any additional information needed to aid in answering the user query, and explaining the logical reasoning behind the final output."}, {"title": "Evaluation across several use cases", "content": "We devised three use cases of varying difficulty to evaluate our multi-agent system. These workflows, listed below, were designed to assess both conceptual genomics (analysis steps) and code generation tasks. We recruited bioinformatics experts, and provided them with the same inputs used by the multi-agent system. Each workflow involved completing the conceptual genomics and code generation tasks, providing any additional information needed to aid in answering the user query, and explaining the logical reasoning behind the final output."}, {"title": "Conceptual Genomics and Code Generation Tasks", "content": "Level 1 Tasks (Easy)\n\u2022 How would I provide quality metrics on FASTQ files?\n\u2022 What code or workflow do I need to write to provide quality metrics on FASTQ files?\nLevel 2 Tasks (Medium)\n\u2022 How do I align RNA-seq data against a human reference genome?\n\u2022 What code or workflow do I need to write to align RNA-seq data against a human reference genome?\nLevel 3 Tasks (Hard)\n\u2022 How can I assemble, annotate, and analyze SARS-CoV-2 genomes from sequencing data to identify and characterize different variants of the virus?"}, {"title": "Conceptual Genomics Tasks", "content": "BioAgents demonstrated performance on par with experts on conceptual genomics questions across three workflows. This success is largely attributed to our use of Low-Rank Adaptation (LoRA) to fine-tune an agent on the top 50"}, {"title": "Code Generation Tasks", "content": "Performance discrepancies emerged in code generation tasks, particularly in workflows of increasing complexity. For easy tasks, BioAgents matched expert accuracy, but sometimes provided false information about tools. For medium tasks, representing end-to-end pipelines like those in nf-core workflows (https://nf-co.re/pipelines/), BioAgents struggled to produce complete outputs."}, {"title": "Reliability and Transparency", "content": "Two key components are necessary in the deployment of multi-agent systems in highly specialized domains: reliability and transparency. Reliability ensures that the system consistently delivers accurate results, while transparency enables users to understand and trust the system's decision-making process."}, {"title": "Self-Reflection in Agent Systems", "content": "Several techniques have been proposed to enable a language model to correct its outputs based on internal evaluation, including: self-consistency [10], self-correction [23, 34], self-evolution [48], self-feedback [27] and self-evaluation [38].\nBioAgents incorporated self-evaluation to enhance output reliability, inspired by the idea that agent systems can assess the accuracy of their own outputs [59]. Our reasoning agent assessed the quality of responses against a defined threshold. Outputs scoring below this threshold were reprocessed, with agents independently reanalyzing the prompts before returning results. However, the iterative process revealed diminishing returns, where repeated refinements negatively impacted output quality and might not necessarily lead to improved outcomes."}, {"title": "Collaborative Reasoning and Transparent Guidance", "content": "Current applications of LLM agents in domain-specific tasks have struggled in the areas of long-term reasoning, decision-making, and instruction-following [28]. Moreover, their proficiency in addressing practical bioinformatics queries and conducting nuanced knowledge inference remains constrained [12].\nIn our experimental set-up, both the system and human experts were asked to explain any additional information they would need to better answer users' questions, and the logical reasoning process behind their answers. Our motivation behind this assessment stemmed from various existing reasoning frameworks \nchain-of-thought (CoT) [52] and ReAct [57] \nused to provide interpretability in LLMs.\nIn the context of the conceptual genomics medium workflow task, BioAgents explained its rationale for selecting the most suitable RNA-seq alignment tool for mapping against the reference genome, recommending STAR and HISAT2 for their high-throughput and accurate alignments [17, 25]. The multi-agent system described how these alignment tools mapped the RNA-seq reads to the reference genome, enabling the identification of genomic locations within the read. BioAgents also specified the factors influencing its choice of alignment tool, noting the size of the RNA-seq dataset and the user's desired accuracy level as important factors. Generating natural language explanations of model outputs has been shown to improve interpretability, thereby increasing transparency and fostering trust, as users are better able to understand the reasoning behind the model's specific outputs [8, 41].\nA key insight from our findings was that our multi-agent system was able to identify additional information that would have improved responses, even in cases where accuracy was lower. For example, in the hard code generation task, BioAgents struggled to generate the necessary workflow code, but was able to identify additional information that could have better answered the user question, specifically more information on the raw sequencing data, the reference genome sequence, software and tool versions, computational resources (e.g., CPU, memory, disk space), and the user's bioinformatics experience. In contrast, although the human experts achieved a higher accuracy score on the same task (four compared to two), they described the limitations of their responses, with one human expert noting that the solutions provided were \"cobbled together from searching for tutorials,\" indicating that it was difficult to identify their information gaps. BioAgents, on the other hand, demonstrated more metacognitive awareness recognizing what it didn't know, and more importantly, additional information it could benefit from [16]. A consistent theme across the human expert responses was an inability to articulate what additional information they would need to improve their answers. This highlights how vast and expansive of a knowledge base is needed to fully answer these questions, which often extends beyond the expertise of one individual."}, {"title": "Discussion", "content": "The reproducibility crisis in computational research highlights the urgent need for systems that can reliably extract, reproduce, and adapt research findings. This challenge is especially pronounced in bioinformatics, where complex workflows often hinder replication and validation efforts. BioAgents, a multi-agent system, offers a promising solution by extracting methods from research papers, generating executable workflows, and integrating human-in-the-loop approaches to improve accuracy and customization.\nBioAgents has the potential to facilitate reproducibility by automatically synthesizing workflows from research publications, enabling researchers to replicate experiments, validate results, and adapt analyses to their datasets. By prioritizing transparency and integrating human feedback, BioAgents ensures outputs are both reliable and user-specific. Furthermore, BioAgents can be extended to clinical settings and other scientific domains. In medicine, the system could assist in replicating diagnostic workflows, personalizing treatment recommendations based on patient data, and optimizing clinical trial designs, ultimately enabling more efficient and reliable translational research. In chemistry and physics, BioAgents can automate the replication of experimental protocols and model complex systems, enhancing the reproducibility of results across a wide range of scientific fields.\nCollaborative reasoning highlighted key areas for improvement in reasoning, decision-making, and instruction-following. BioAgents effectively identified information gaps, such as tool versions and user experience, which experts often overlooked. Generating natural language explanations of its decisions increased interpretability, fostering user trust. Despite struggles with accuracy in complex tasks, BioAgents demonstrated metacognitive awareness, outlining additional data that could improve results.\nThe increased reliability, transparency, and trust fostered by BioAgents is particularly valuable for reducing barriers for new bioinformatics researchers. Unlike static question-answer forums, which can provide solutions without clear explanations or insight into the respondent's reasoning process, BioAgents allows users to not only receive answers, but also provides the underlying supporting information that led to those answers. By sharing the multi-agent system's reasoning behind its proposed steps, researchers can learn how to replicate those decision-making processes, highlighting the educational value of our approach. Ultimately, the transparency provided by BioAgents not only improves trust in agentic systems but also facilitates knowledge transfer, allowing users to grow and develop their expertise.\nIn conceptual genomics tasks, BioAgents demonstrated performance comparable to that of human experts, successfully addressing domain-specific challenges. However, areas for improvement were identified in code generation. Specifically:\n\u2022 Workflow Scope: The system's reliance on nf-core workflows limited diversity. Expanding indexed workflows to include additional sources could"}, {"title": "Methods", "content": "By lowering barriers to compute resources and operating seamlessly in local environments, BioAgents addresses both accessibility and scalability. Its potential extends beyond bioinformatics, offering a model for intelligent systems in other domains facing reproducibility challenges to use the BioAgents framework and train on their own propriety or domain-specific code and documentation.\nWith targeted enhancements in workflow diversity, retrieval methods, and reasoning capabilities, BioAgents is poised to become a cornerstone in the push for reproducible, transparent, and accessible computational research."}, {"title": "Datasets", "content": "We leveraged a single A100 GPU to perform parameter-efficient fine-tuning (PEFT) on the Phi-3-mini-128-instruct model, optimizing it for bioinformatics tasks. Specifically, we employ the QLORA technique, which enables fine-tuning with reduced computational overhead by quantizing the model's layers and training low-rank adapters. This approach is particularly well-suited for large-scale language models like Phi-3-mini, as it retains performance while significantly reducing resource requirements.\nOur fine-tuning dataset focuses on the top 50 most commonly used BioContainers tools, along with their associated versions and help documentation, ensuring broad applicability to bioinformatics workflows. Additionally, we added Software Ontology data about the name of the software and its purpose. Training was conducted on Azure Machine Learning, with model configurations limited to a new token count of 1,000 and a temperature of 0.1 to control response diversity and precision.\nFor our retrieval-augmented generation (RAG) implementation, we integrate OpenAI's text-embedding-ada-002 for high-quality semantic search. The embeddings are indexed within Azure Al's search service, optimized to retrieve nf-core modules efficiently, and the Sequence Ontology, describing each assay and its purpose. This combination ensures that the system can provide relevant, tool-specific code generation and guidance tailored to bioinformatics workflows.\nBy combining QLoRA-based fine-tuning and RAG, we achieve a system that balances computational efficiency, domain specificity, and accessibility for researchers in bioinformatics."}, {"title": "Biostars", "content": "Biostars [35] is an online community platform for the bioinformatics community where users can ask and answer questions related to computational genomics and biological data analysis. We scraped all publicly available data from the site, which included a total of 68,000 question-answer (QA) pairs, up to May 1, 2024.\ntool - software programs and packages used for bioinformatics analysis;\n\u2022 analysis - pipelines and analysis performed in bioinformatics field, such as rna-seq, alignment, variant calling;\n\u2022 data format - genomics and other -omics data formats"}, {"title": "Biocontainers", "content": "We fine-tuned BioAgents on Biocontainers' (https://biocontainers.pro/) top 50 tools, including versions and documentation. We use the TRS API (https://api.biocontainers.pro/ga4gh/trs/v2/ui/) to pull statistics on the top bioinformatics tools, based on download frequency. Then, we grabbed each available docker version of each of those tools. For each docker container, we downloaded the container and outputted command-line help documentation."}, {"title": "Ontologies", "content": "We downloaded both the Software and EDAM Ontologies [5, 22, 32] for software and assay description. To convert to JSON-LD we used the JSON or OBO format to extract the name and either the description or definition ."}, {"title": "Models", "content": "We leveraged a single A100 GPU to perform parameter-efficient fine-tuning (PEFT) on the Phi-3-mini-128-instruct model, optimizing it for bioinformatics tasks. Specifically, we employ the QLORA technique, which enables fine-tuning with reduced computational overhead by quantizing the model's layers and training low-rank adapters. This approach is particularly well-suited for large-scale language models like Phi-3-mini, as it retains performance while significantly reducing resource requirements.\nOur fine-tuning dataset focuses on the top 50 most commonly used BioContainers tools, along with their associated versions and help documentation, ensuring broad applicability to bioinformatics workflows. Additionally, we added Software Ontology data about the name of the software and its purpose. Training was conducted on Azure Machine Learning, with model configurations limited to a new token count of 1,000 and a temperature of 0.1 to control response diversity and precision.\nFor our retrieval-augmented generation (RAG) implementation, we integrate OpenAI's text-embedding-ada-002 for high-quality semantic search. The embeddings are indexed within Azure Al's search service, optimized to retrieve nf-core modules efficiently, and the Sequence Ontology, describing each assay and its purpose. This combination ensures that the system can provide relevant, tool-specific code generation and guidance tailored to bioinformatics workflows.\nBy combining QLoRA-based fine-tuning and RAG, we achieve a system that balances computational efficiency, domain specificity, and accessibility for researchers in bioinformatics."}, {"title": "Expert Survey", "content": "We conducted a survey to elicit expert bioinformatician responses to workflow questions informed by Biostars data, and evaluated this against outputs from BioAgents. Our survey assessed human expert's reasoning and logic behind their responses. Below we discuss our survey design, respondent recruitment, informed consent process, survey data analysis, and findings."}, {"title": "Survey Design", "content": "We created a survey using Microsoft Forms to obtain human expert answers and their reasoning behind responses related to translating genomics tasks, and writing subsequent code to analyze data question types. Biostars community forum questions were abstracted and categorized to evaluate common question types, which we found to be around tools and/or analysis. We then created three levels of questions (easy, medium, and hard) increasing in complexity (i.e., number of steps and knowledge required) derived from questions in Biostars.\nThe survey consisted of 27 questions. We asked eight demographic questions to assess respondent's education-level, number of years working with bioinformatics tools, data types they worked with regularly, work setting, programming experience, age, gender, and if English was their first language. For the remaining survey questions, respondents were asked to imagine an undergraduate student asked for help on a set of questions, and to provide the logic and steps they would advise the student to take to successfully complete their inquiry. Each of the three question levels had two parts: 1) answering the question asked by the student with corresponding logic/ reasoning, and 2) generating code (in their preferred programming language) with corresponding logic /reasoning. An optional question was asked after each of these questions around what additional information, if any, they would need to answer the student's question."}, {"title": "Recruitment and Informed Consent", "content": "We recruited survey respondents through a combination of Microsoft internal community distribution email lists and snowball sampling techniques in August of 2024. The primary inclusion criteria for participation required individuals to have 5+ years of bioinformatics experience with tools/ workflows, intermediate to advanced coding skills, and be at least 18 years of age.\nFive bioinformatician experts responded to our survey. The expert respondents ranged in age (25-44 years), gender (2 women, 3 men), and education level (2 masters, 3 doctorates). All respondents reported intermediate to advanced programming experience, 5 or more years of experience working with bioinformatics tools, and English as their first language. Additionally, all respondents reported working with a range of biomedical data types (3 non-human, 3 human, non-clinical, and 4 human, clinical data) and most reported working in an industry setting."}, {"title": "Qualitative Data Analysis", "content": "We conducted a content and thematic analysis of the open-ended questions. For the three question levels (easy, medium, hard) related to the genomics workflows and code generation tasks, we conducted a content analysis of the steps and code across respondents to create a ground-truth dataset to evaluate against the output of BioAgents. For the logic/reasoning and additional information needed questions we conducted a thematic analysis of emerging themes."}, {"title": "Expert Findings", "content": "We aggregated survey responses from the five expert respondents on Agent 1 and Agent 2 questions across the three levels of difficulty (Figure 2). We created a master list of steps by task, and corresponding code as ground-truth data across all experts' responses to the genomics workflow and code generation questions. We then conducted a thematic analysis for emerging themes based on responses to the logic/reasoning and additional information needed questions.\nExpert responses to questions and corresponding logic plus additional information required did not vary across experts on the easy question types. However, for the medium and hard question sets, experts required more logic/reasoning and additional information due to the increased complexity (e.g., number of steps) of the biomedical research tasks necessary to correctly answer the question. This also included the tools and knowledge needed to navigate and implement the correct solution."}, {"title": "Human Evaluation", "content": "To assess the performance of the system and experts, we conducted a human evaluation study in which a domain expert scored outputs on two key criteria: accuracy and completeness. For conceptual tasks, experts assessed whether the system's reasoning and recommendations were consistent with domain knowledge. For code generation tasks, experts verified syntax correctness, tool compatibility, and functionality.\n1. Accuracy (1-5): The degree to which the output code and steps are correct. The steps and/or software are reasonable and not hallucinated.\n2. Completeness (1-5): The extent to which the output provides all necessary components or steps required for the task, where 1 indicates significant omissions and 5 indicates comprehensive coverage of code or steps such that the user would be able to implement the answer without searching for additional information."}]}