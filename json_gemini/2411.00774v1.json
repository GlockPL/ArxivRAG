{"title": "Freeze-Omni: A Smart and Low Latency Speech-to-speech Dialogue Model with Frozen LLM", "authors": ["Xiong Wang", "Yangze Li", "Chaoyou Fu", "Lei Xie", "Ke Li", "Xing Sun", "Long Ma"], "abstract": "The rapid development of large language models has brought many new smart applications, especially the excellent multimodal human-computer interaction in GPT-40 has brought impressive experience to users. In this background, researchers have proposed many multimodal LLMs that can achieve speech-to-speech dialogue recently. In this paper, we propose a speech-text multimodal LLM architecture called Freeze-Omni. Our main contribution is the speech input and output modalities can connected to the LLM while keeping the LLM frozen throughout the training process. We designed 3-stage training strategies both for the modeling of speech input and output, enabling Freeze-Omni to obtain speech-to-speech dialogue ability using text-speech paired data (such as ASR and TTS data) and only 60,000 multi-round text Q&A data on 8 GPUs. Moreover, we can effectively ensure that the intelligence of the Freeze-Omni in the speech modality is at the same level compared with that in the text modality of its backbone LLM, while the end-to-end latency of the spoken response achieves a low level. In addition, we also designed a method to achieve duplex dialogue ability through multi-task training, making Freeze-Omni have a more natural style of dialogue ability between the users. Freeze-Omni mainly provides a possibility for researchers to conduct multimodal LLM under the condition of a frozen LLM, avoiding various impacts caused by the catastrophic forgetting of LLM caused by fewer data and training resources.", "sections": [{"title": "Introduction", "content": "In recent years, the development of large language models has been extremely rapid. A series of large language models represented by the GPT series [10, 1] of OpenAI has demonstrated extraordinary capabilities. As speech interaction is one of the most natural forms of human-computer interaction, combining speech input and output with an LLM can bring an extraordinary experience to users. The traditional method is to use a cascaded approach of ASR + LLM + TTS to achieve the interaction with LLM in speech modality. However, this approach often leads to a relatively high engineering complexity and a considerable interaction latency. Nevertheless, GPT-40 [18] has changed this situation, it provides an end-to-end speech interaction mode which has significantly improved the user experience, triggering a research boom among researchers regarding multimodal LLMs for speech-to-speech interaction.\nIn the field of general LLMs, many public models such as Llama 3.2 [8], Qwen2.5 [21], Mixtral [14], etc. have provided very good opportunities for researchers to develop downstream tasks on them. Therefore, in the research field of multimodal LLMs for speech-to-speech, works such as Mini-Omni2 [24], LLaMA-Omni [9], and Moshi [7] have provided excellent references for researchers."}, {"title": "", "content": "These works adopt different strategies to align the speech modality with the LLM and design some methods to achieve a duplex dialogue mode, demonstrating excellent performance.\nIn this research context, we found that in the process of aligning the LLM with the speech modality in existing public speech-text multimodal LLMs [6, 7, 9, 11, 27, 23], the parameters of the LLM are more or less fine-tuned. However, in most cases, it is very difficult for researchers to easily collect spoken Q&A data at the million-hour level (the corresponding text content can be comparable to the amount of data for training text-modal LLMs). This inevitably brings about the forgetting problem to the LLM, resulting in a negative impact on its intelligence. In addition, only a few works have evaluated the accuracy of spoken question-answering tasks for speech-to-speech multimodal LLMs, and show an obvious gap in performance between spoken question-answering and text-modality question-answering. Therefore, in this paper, we propose a speech-to-speech dialogue LLM called Freeze-Omni, achieving speech modality alignment while the LLM is frozen throughout the training process, and obtaining low latency speech dialogue capabilities while keeping the intelligence of the backbone LLM. Freeze-Omni is mainly implemented in the following steps:\nModeling of speech input We first use a large amount of ASR data to align the speech encoder and the LLM, enabling the LLM to understand the semantic information from the speech. Then, with the LLM frozen, a training strategy of prompt embedding is used to let the model have the ability to possess speech input to text output, training on only a small amount of Q&A data.\nModeling of speech output Second, we use a mount of text-speech paired data to train the AR-based speech decoder which can generate speech tokens from text and a single-codebook based codec model is used to decode the speech token into waveform. Then, we design a prefix kv-cache fine-tune strategy, using the hidden state vector output by the LLM to transfer the speech decoder into the output text space of LLM, achieving the ability of text input to speech output while keeping the LLM frozen.\nDesign for duplex dialogue Finally, we simultaneously connect the speech encoder and speech decoder from the above parts to the backbone LLM. Then, a task of chunk-wise state prediction is used to enable the LLM to interrupt or reject the user's input, achieving the duplex speech-to-speech dialogue ability.\nIn conclusion, the main contributions of the proposed Freeze-Omni are as follows:\n\u2022 The parameters of the LLM are completely frozen throughout the training process, ensuring that the intelligence of the LLM will be kept. At the same time, the ability of low latency speech-to-speech dialogue is still obtained.\n\u2022 The data scale relied on during the training process is small and consumes fewer computing resources. It requires text-speech paired data (such as ASR and TTS training data) and only a small amount of Q&A data in text modality.\n\u2022 Freeze-Omni can support any (multimodal) LLM that has a text modality and retains the abilities of the LLM such as prompt following and role-playing. Moreover, if it is necessary to change the style of the LLM's response, it is only necessary to fine-tune the LLM with text data in the corresponding style."}, {"title": "Model", "content": "Freeze-Omni is a speech-to-speech dialogue model and the architecture is shown in Fig. 1, exhibiting the characteristic of being \"smart\" as it is constructed upon a \"frozen\" text-modality LLM. This enables it to keep the original intelligence of the LLM backbone, without being affected by the forgetting problem induced by the fine-tuning process for integration of the speech modality. Specifically, Freeze-Omni contains a speech encoder that supports streaming speech input and a speech decoder that generates streaming output speech. During the training process, Freeze-Omni first achieves the alignment between speech input to text output, and then the text input to speech output. Finally, by connecting these two components with the LLM, the ability of speech input to speech output is obtained. This section will provide a detailed introduction to the architecture, training strategy, and duplex dialogue design of Freeze-Omni."}, {"title": "Modeling of speech input", "content": "To enable Freeze-Omni to support speech input and achieve a rapid and low-latency response to the input speech, it utilizes a chunk-wise streaming speech encoder to transform the input speech features into a high-dimensional representation. Then, an adapter module maps the high-dimensional representation into the embedding space of the backbone LLM. The speech encoder module here consists of several down-sampling convolution layers and several Transformer [22] blocks, while the adapter only comprises several down-sampling convolution layers. The reason for using down-sampling is to reduce the frame rate of the speech features, increase the speed of the LLM in the prefill stage, and decrease the latency."}, {"title": "Training strategy", "content": "A 3-stage training strategy shown in Fig. 2 is used for the speech encoder, enabling Freeze-Omni to acquire the ability to understand the streaming input speech while keeping the LLM frozen."}, {"title": "", "content": "\u2022 The first stage shown in Fig. 2(a) is the same as the training process of a common speech recognition model. The input is speech features and the label is the transcript corresponding to the speech, CTC [13] is used as the loss function.\n\u2022 In the second stage shown in Fig. 2(b), we use the speech encoder trained in the first stage as the initialization parameter and connect it with the LLM using an adapter. The output of the LLM still uses the transcript corresponding to the input speech as the label. Several trainable special tokens are added to the input part to guide the LLM in completing the training process at this stage. In this stage, except for the frozen LLM, the parameters of other networks are all trainable.\n\u2022 In the last stage shown in Fig. 2(c), we first construct a dataset of multi-round questions and use the LLM backbone relied on in the training to generate multi-round answers. The dataset constructed in this way will be completely compatible with the LLM backbone. Subsequently, we use a multi-speaker TTS system to generate data in the speech modality for the questions part and add trainable prompt embedding before each question in the multi-round to guide the LLM to achieve the ability of speech input to text output. In this stage, the trainable special tokens in stage 2 will be dropped, only the prompt embedding part is trainable and they use the same parameters for each question, the speech encoder is frozen to maintain the acoustic robustness obtained from stage 2, and the LLM is also frozen to ensure that its intelligence is not affected."}, {"title": "Modeling of speech output", "content": "Inspired by VALL-E [5], Freeze-Omni uses a token-based speech decoder which contains NAR prefill and AR generate stage to achieve speech output capabilities. The speech decoder mainly consists of"}, {"title": "Training strategy", "content": "For the modeling of speech output, we still use a 3-stage training method as shown in Fig. 3, enabling Freeze-Omni to obtain the ability of generate speech from the output of LLM while keeping the LLM frozen.\n\u2022 As shown in Fig. 3(a), we first train a single-codebook based codec model using only speech data. Since a single codebook is sufficient for extracting speech tokens from the speech signal of a limited number of speakers, using a single codebook here can reduce the complexity and latency of the system as much as possible.\n\u2022 In the second stage shown in Fig. 3(b), we first construct a large amount of text-speech paired data and pass the text through the tokenizer of the backbone LLM to convert the text into text tokens. Then, we pass the text tokens through the embedding layer of the LLM to convert them into embedding vectors as semantic features and send them to the NAR speech decoder. The AR speech decoder predicts the output speech tokens in the form of teacher force. The labels here are extracted using the codec model trained in stage 1. The NAR and AR speech decoders use the same parameters, and the embedding layer of the LLM is frozen.\n\u2022 In the last stage, we use the same multi-round questions and answers data set in stage 3 of Sec. 2.2.2 and use the text tokens and hidden state sequence generated by the backbone LLM. As shown in Fig. 3(c), an additional NAR prefix speech decoder is added to model the hidden state of the LLM and pass its output kv-cache to the NAR speech decoder. Then the text token will be fed to the NAR speech decoder trained in stage 2. The text token label for AR speech decoder is the speech data produced by the output text of LLM using a TTS system and converted into speech tokens by the codec model in stage 1. In this stage, the NAR prefix speech decoder uses different parameters"}, {"title": "", "content": "from the NAR and AR speech decoders, and only the parameters of the NAR prefix speech decoder are trainable while the parameters of other networks are frozen. Because the style of the text tokens produced by the LLM is different from that of the text in the large amount of text-speech paired data obtainable in stage 2, the significance of the third stage lies in closely coupling the speech decoder with the output of the LLM to reduce the occurrence of bad cases."}, {"title": "Design for duplex dialogue", "content": "After the above training process, Freeze-Omni has the ability of speech input to speech output. However, to better approximate the natural form of speech-to-speech dialogue, we use multi-task for chunk-level state prediction as shown in Fig 4. We first use an acoustic VAD\u00b9 module to detect the starting point of the streaming speech. When the VAD is triggered, the speech stream will sent into Freeze-Omni chunk by chunk, and an additional classification layer will be added after the last layer of the LLM to predict different states. Three states are defined here, state 0 indicates that the current LLM can continue to receive speech, and state 1 or 2 indicates that the current chunk is the end of the speech. State 1 means that the LLM can interrupt the user and perform the generate stage, and state 2 means that there is no need to interrupt the user. Both of these states will stop sending speech streams to Freeze-Omni and reset the VAD module. The training process of this part is completed in stage 3 of Sec. 2.2.2, using a multi-task method to optimize the cross-entropy loss of both the state classification layer and the LLM. It should be noted that the state labels here are only valid on the last frame of each chunk.\nBesides, we used a \"model as a server\" strategy to implement the speech-to-speech dialogue system. First, we started several models simultaneously and regarded them as a server. Then, when a user's VAD was triggered, the speech would be sent to the server in the form of chunks, and the server would be responsible for scheduling which idle model should respond to the current chunk. Since we separated all the kv-cache and CNN cache of the speech encoder and LLM during the inference process, the server only needs to save the inference cache for each user. In this way, any model in the server could respond to any chunk of any user, and there was no need to specify which model was used as a monitor or a generator."}, {"title": "Experiments", "content": ""}, {"title": "Setups", "content": ""}, {"title": "Datasets", "content": "In this paper, we only randomly selected 60,000 multi-round Q&A data from moss-003-sft-data \u00b2 and used backbone LLM to generate new answers to replace its original one. We used a zero-shot TTS system to synthesize its text into speech. For the modeling of speech input of Freeze-Omni, we used 110,000h internal speech-text paired ASR data including both Chinese and English in stage 1 and stage 2. In stage 3, we used the pairing of speech input and text output of the multi-round Q&A"}, {"title": "Model configuration", "content": "For experiments in this paper, we used Qwen2-7B-Instruct \u00b3 as our backbone LLM. As an outstanding 7B-level public LLM, it is beneficial for us to verify our method. Besides, Freeze-Omni can use any LLM as a backbone in actuality because its training process does not update any parameters of the LLM.\nWe used a multi-layer convolution with 4-times downsampling and 24 layers of transformers with a hidden size of 1024. The adapter consists of a multi-convolution layer with 2-times downsampling. The number of parameters for the speech encoder is approximately 350M, with an output frame rate of 12.5Hz. The input of the speech encoder is the mel-filter bank feature with a 25ms window size and 10ms shift.\nWe used TiCodec \u2074 [20] as the codec model, and we customized the configuration so that the size of the codebook is 1024 with a single-codebook and the frequency of the speech token 40Hz. For the speech decoder part, both the NAR (Prefix) speech decoder and the AR speech decoder are 4-layer Llama decoder layers with a hidden size of 896. The number of parameters for the speech decoder is approximately 120M and the output sample rate of codec model is 24000Hz."}, {"title": "Training", "content": "In training processes we used the Adamw [16] optimizer with a warm-up learning rate scheduler, and different learning rates were used in different stages. The learning rates used in the three stages of the modeling of speech input are 2e-4, 1e-4, and 6e-4 respectively. The learning rates used in stage 2&3 of the modeling of speech output are both 5e-5 and the training hyper-parameters used in stage 1 are the same as that in TiCodec. All the experiments were completed on 8 NVIDIA A100 GPUs."}, {"title": "Results on speech input", "content": "To measure the understanding ability of Freeze-Omni for input speech, as shown in Tab. 1, we verified the accuracy of ASR on different evaluation sets for the model in stage 2 of the modeling of speech input. Since the parameters of the speech encoder and adapter used in stage 3 are unchanged compared to those in stage 2, it can be considered that these results can represent the input speech understanding ability of Freeze-Omni. In the training of stage 2, we used a dynamic chunk training method [25] to enhance the robustness of the model so that different chunk sizes can be used in stage 3. From the results, it can be seen that in the case of dynamic chunk training, decoding with chunk = \u221e shows better performance compared to chunk = 4. If dynamic chunk training is not used but chunk = 4 decoding is used, better results can be obtained, but this also means that the"}, {"title": "", "content": "chunk size cannot be changed in stage 3. In this paper, to pursue the best performance, all experiments are completed on the model with this configuration of the last row in Tab. 1."}, {"title": "Results on speech output", "content": "Because we investigated the speech-out performance of Freeze-Omni in a single-speaker case in this paper, we randomly selected 1,000 utterances of text tokens and hidden states output by the LLM as the input of the speech decoder and compared the ASR accuracy of the synthesized speech with the label text. As shown in Tab 2, the performance of the model in stage 2 of the modeling of speech output (Speech Decoder w/o Prefix) and the model in stage 3 (Speech Decoder) under different AR decoding parameters top-k are presented respectively, and CER (%) is evaluated using paraformer-zh\u2075 [12]. From the results, it can be concluded that after introducing the hidden state of the LLM as the input of the NAR prefix speech decoder, the speech decoder can be more completely aligned with the LLM, reducing the occurrence of bad cases and get a lower CER (%). In addition, the increasing top-k shows better robustness of the speech decoder with a prefix fine-tune."}, {"title": "Results on spoken question answering", "content": "To demonstrate the intelligence of Freeze-Omni, we verified the accuracy of spoken question answer-ing on three sets: LlaMA-Questions [17], Web Questions\u2077 [3], and Trivia QA\u2078 [15]. Since Web Questions and Trivia QA only have text, we used the edge-tts tool with voice at en-US-BrianNeural to synthesize them into spoken modality. Tab. 3 shows the accuracy of Freeze Omni and its used backbone LLM Qwen2-7B-Instruct on these three sets. From the results, it can observed that Freeze-Omni exhibits excellent performance compared to other models because the accuracy gap between it and the backbone LLM is smaller than that of Moshi, which also verifies that Freeze-Omni has the same level of intelligence in text and speech modalities."}, {"title": "Analysis on end-to-end latency", "content": "To verify the latency of Freeze-Omni for speech-to-speech dialogue, we defined two parts of latency, namely statistical latency and non-statistical latency. The statistical latency refers to the time from the time the LLM interrupts the user to the first PCM chunk of speech generated. Specifically, it can be divided into four parts as shown in Fig 4, these results are based on a speech token chunk size of 40 and the use of text token chunk segmentation based on the sentence-split strategy. The non-statistical latency refers to the time from the real endpoint of speech to the LLM outputting the interrupt state. This part needs to be measured manually and cannot be counted automatically. According to our case analysis conclusion, the non-statistical latency is about one to two speech encoder chunk sizes. According to the experiment configuration above, this time is about 160ms to 320ms. In summary, if we consider the influence of network latency (about 200 to 300ms), the average latency of Freeze-Omni used in real scenarios will be controlled at about 1.2 seconds."}, {"title": "Conclusion and future work", "content": "In this paper, we proposed Freeze-Omni, a text-audio multimodal LLM capable of low-latency speech-to-speech dialogue, which does not need fine-tuning the LLM, showing excellent performance in various evaluation tasks. In the future, to explore more speech dialogue capabilities, we plan to do the following work:\n\u2022 We will upgrade the speech encoder to an audio encoder so that it can process and understand non-speech to complete tasks such as emotion understanding and audio captioning.\n\u2022 We will add more multi-tasks to make the LLM output more task labels to complete more down-stream tasks of speech dialogue, taking the state prediction ability as an example, under the condition of LLM freeze.\n\u2022 We will explore how to support multi-speaker synthesis and instruct follow ability in the speech decoder part so that it can obtain more instruct information from the hidden state of the LLM and provide more abundant speech output styles."}]}