{"title": "RAGRAPH: A General Retrieval-Augmented Graph Learning Framework", "authors": ["Xinke Jiang", "Rihong Qiu", "Yongxin Xu", "Wentao Zhang", "Yichen Zhu", "Ruizhe Zhang", "Yuchen Fang", "Xu Chu", "Junfeng Zhao", "Yasha Wang"], "abstract": "Graph Neural Networks (GNNs) have become essential in interpreting relational data across various domains, yet, they often struggle to generalize to unseen graph data that differs markedly from training instances. In this paper, we introduce a novel framework called General Retrieval-Augmented Graph Learning (RAGRAPH), which brings external graph data into the general graph foundation model to improve model generalization on unseen scenarios. On the top of our framework is a toy graph vector library that we established, which captures key attributes, such as features and task-specific label information. During inference, the RAGRAPH adeptly retrieves similar toy graphs based on key similarities in downstream tasks, integrating the retrieved data to enrich the learning context via the message-passing prompting mechanism. Our extensive experimental evaluations demonstrate that RAGRAPH significantly outperforms state-of-the-art graph learning methods in multiple tasks such as node classification, link prediction, and graph classification across both dynamic and static datasets. Furthermore, extensive testing confirms that RAGRAPH consistently maintains high performance without the need for task-specific fine-tuning, highlighting its adaptability, robustness, and broad applicability.", "sections": [{"title": "1 Introduction", "content": "Graph Neural Networks (GNNs) [5, 48, 97, 63, 126] have recently burgeoned a surge of interest in both academic and industry communities due to their robust capability to model complex, real-world data in diverse domains, including societal [72, 55, 80], biochemical [17, 111, 107], and traffic-related [54, 23, 44, 21] fields and etc [53, 37, 68, 15, 25, 24]. Utilizing a message-passing mechanism [48, 29], GNNs have transcended traditional node embedding approaches [28, 79, 95], enabling the capture of intricate relationships within data through sophisticated architectures and advanced graph representation learning techniques [48, 50, 54, 18, 97]. However, the challenge of generalizing GNNs across different modalities, domains [62, 61], and tasks remains largely unexplored [56, 113]. This is in stark contrast to the significant successes of large models such as GPTs [74, 75] in NLP and Sora [64] in CV, presenting a crucial frontier for further research and realms for graph data generalizing.\nIn graph learning tasks, providing the necessary context is crucial for graph generalization [129, 51, 73, 134], i.e., retrieve similar shopping context as illustrated in Figure 1 (c). Therefore, our insight is to enhance the model's generalization ability and prediction accuracy by retrieving necessary contexts during graph learning through retrieval. Retrieval-Augmented Generation (RAG) represents a prominent methodology, significantly augmenting language model functionalities through the integration of a dynamic retrieval mechanism during the generation process [135, 77] (e.g., a person asks what animal it is, and we use some visual [138] or text retrieval [2] methods to retrieve more descriptive features or even the wanted category). RAG enriches not only accurate and reliable content but also reduces factual errors, addressing challenges such as incorrect answers, hallucinations, and limited interpretability in knowledge-intensive tasks [40, 2, 1], obviating the need for updating model parameters and could be generalized even in unseen scenarios.\nHowever, how to enable retrieval-augmented generation for graph learning, i.e., retrieving the user's historical purchasing behavior to enhance recommendation ability [30, 113, 35] and identifying fraud crimes by searching for similar fraudulent relationship behaviors [85, 63], still remains unexplored and faces the following challenges C1& C2.\nCl. The first challenge is how to leverage the retrieved context i.e., features (X) and labels (Y) into the GNNs model under dynamic changing scenarios. Previous studies, such as PRODIGY [73], have adopted the concept of in-context learning (ICL) by constructing consistent and static task graphs for each specific task or dataset. These task graphs determine labels through the calculation of similarities using hidden vectors, employing a few-shot learning approach. However, PRODIGY's reliance on a fixed set of examples as rules may not sufficiently address and generalize the variety of scenarios encountered in real-world settings, which is particularly problematic in dynamically changing environments, as the system focuses primarily on teaching the direct mapping paradigm from inputs to outputs (X \u2192 Y), rather than truly integrate the input (X) and output (Y) data into the analysis. In contrast to RAG, PRODIGY struggles to incorporate external information (X and Y) related to data nodes, which is crucial for enriching the learning process in graph-based systems.\nC2. Moreover, it is challenging to develop a tune-free prompt mechanism to support retrieved knowledge and be applicable to seamlessly switch unseen scenarios and multi-tasks. Numerous initiatives have been undertaken in the realm of graph pre-training [33, 116, 34, 81, 98, 36, 7, 88, 125], however, the challenge persists in designing a plug-and-play RAG module that can seamlessly interface with already pre-trained models. Insights derived from prior investigations into the graph prompt [9, 26, 90, 65, 113, 20, 94], the knowledge obtained by RAG can be facilitated and injected into prompt via a plug-and-play manner.\nFor endeavoring to address these two challenges previously mentioned, we put forward the General Retrieval- Augmented Graph Learning Framework (RAGRAPH). Drawing inspiration from the success of RAG on LLMs [135] and the ICL on GNNs [73] (we detail the difference between RAG and ICL in Appendix E), we constructed a toy graphs vector library by chunking from resource graphs, where the library key stores key information, including environmental, historical, structural, and semantic details, while node features and label information (task-specific output vector) are stored as values. For downstream tasks, the key value of the query node would be leveraged to retrieve toy graphs by the key similarities,"}, {"title": "2 Related Work", "content": "2.1 Retrieval-Augmented Generation on Large Language Models\nRAG integrates an external knowledge retrieval component and through prompt engineering into pre-trained language models to enhance factual consistency, thus improving the reliability and interpretability of LLM responses [131, 135, 49, 22, 43, 118, 57, 110, 127]. Traditional RAG approaches utilize retriever models to source relevant documents from extensive knowledge corpora [106, 82, 69, 47], which are then processed further by reader models-primarily LLMs [76, 84]. Furthermore, several studies focus on fine-tuning reader LLMs by applying prompt-tuning with retrieved knowledge or using RAG API calls [67, 40, 2, 115, 101, 128, 60]. While RAG has seen considerable success in the NLP field, it has also been applied to tasks involving joint visual and text retrieval [138, 59, 58, 8, 124, 10], code retrieval [66, 133], audio retrieval [6, 31] and video retrieval [3, 100]. Although there have been applications of RAG on structured data such as KG-RAG for knowledge graphs [46, 43, 86, 92, 93, 38], these primarily leverage the text information of knowledge graph nodes to enhance language or graph models. In contrast, there are no significant studies utilizing RAG on structured graphs without text information to enhance pre-trained GNNs. Our work aims to extend this successful approach similarly to graph data, to enhance the capabilities of pre-trained GNNs, and can be adapted to various tasks and across different graphs without additional fine-tuning by integrating a plug-and-play RAG module.\n2.2 Graph Prompt Learning\nInspired by the application of pre-training models [74, 75] and prompt learning [102, 133, 41] in NLP, recently, learning on the graph has been divided into pre-training models on large-scale graph data [33, 116, 34, 81, 73, 130, 104, 89, 119, 121, 122, 120, 123], with or without labels, followed by fine-tuning model parameters via prompts for diverse downstream tasks [65, 113, 73, 137, 89, 94]. The adoption of prompting mechanisms in graph learning represents a promising avenue to overcome the constraints of traditional graph representation methods, striking a balance between flexibility and expressiveness [91]. For instance, VNT [94] utilizes virtual nodes as prompts to refine the application of pre-trained graph models. GraphPrompt [65] introduces a task-specific readout mechanism to tailor models for various tasks, while GraphPro [113] implements spatial- and temporal-based gating mechanisms suited for dynamic recommendation systems. Furthermore, PRODIGY [73] constructs task graphs (prompts) and data graphs to enhance the model's ICL capabilities. Leveraging the successes in graph prompt learning, we aim to inject retrieved knowledge via prompt into pre-trained GNNs to support downstream tasks."}, {"title": "3 Preliminaries", "content": "In RAGRAPH, we focus on RAG on multi-level graph tasks. For consistency, we define the graphs as dynamic graphs, considering static graphs as the special cases within this framework. The subsequent definition provides a detailed description of toy graphs, including the definitions of keys and values utilized in RAGRAPH. Additionally, inspired by GraphPrompt [65], we have unified node-level, edge-level, and graph-level tasks into a cohesive framework, and employ query graphs to tackle downstream tasks with precision.\nDefinition 1. (Dynamic Graph) Let $G = \\{G_t\\}_{t=1}^T$ denote a dynamic graph comprising a sequence of graph snapshots, each represented as a static graph $G_t = (V_t,E_t,X_t,A_t,Y_t)$. $V=\\bigcup_{t=1}^T V_t = \\{1,...,U_n\\}$ defines the combined set of nodes across all snapshots and $E = \\bigcup_{t=1}^T E_t \\subseteq V \\times V$ is the edge set, where $V_t$ and $E_t$ represent the nodes and edges of the t-th snapshot, respectively. Feature matrix $X_t = \\{x_v | v \\in V_t\\} \\in \\mathbb{R}^{n \\times d}$ contains the feature vectors for the nodes in the t-th snapshot, where d is the feature dimension. $A_t$ denotes the edge weight matrix at time t, where edge weight $A_t[i,j] \\in (0,1]$ if $v_i, v_j \\in V_t$ and $(v_i,v_j) \\in E_t$, and 0 otherwise. Furthermore, $Y_t$ represents the task-related labels associated with nodes, edges, or the graph at time t. Note that a graph is static if $T = 1$ and for consistency in terminology, we unify static graphs as a particular instance of dynamic graphs.\nDefinition 2. (Toy Graph Vector Base) Let $G_R = \\{G_R\\}_{i=1}^R$ denote a dynamic resource graph. We chunk $G_R$ into snapshots and take each node in $G_R$ as the master node $U_m$ of the corresponding toy graph, and then store $U_m$ with its neighbors within k hops as subgraphs. Data augmentation techniques [54, 132] such as node dropout, edge dropout, and random noise addition are employed on subgraphs to enhance the robustness and variability when generating each toy graph $G_T$ (c.f. Section 4.1 for details). Each toy graph $G_T \\subseteq G_R$ is associated with a specific timestamp $\\tau$ and master node $U_m \\in V$, with each toy graph's scale being considerably smaller in scale compared to their corresponding $G_R$. Toy graphs can be retrieved using keys that include the timestamp $\\tau$, the hidden embedding of the master node $h_v \\in \\mathbb{R}^{f_1}$ (e.g., embedded by pre-trained GNNs in RAGRAPH), the environmental key (e.g., the neighbors set $N(v_m) = \\{v | A_t[m,i] >0,v \\in G_T \\}$ ) and the structure-based position-aware code $s_m$ (cf. Appendix C.2 for details). By retrieving based on key similarity (c.f. Section 4.2 for details), we can obtain the required values of $G_T$, i.e. task-specific output vector $\\{o_{\\tau} \\in \\mathbb{R}^{f_2} | v_i \\in G_T \\}$ and hidden embeddings $\\{h_{\\tau} \\in \\mathbb{R}^{f_1} | v_i \\in G_T \\}$ of the master node and its neighbors, where $f_1$ and $f_2$ represent the dimensions. Finally, we denote the key-value vector base for the toy graph as $G_T$.\nDefinition 3. (A Unified Graph Task Definition) Given a dynamic graph G, it can be divided into training and testing subsets, i.e. G=GtrainUGtest based on either snapshot or node set partitioning. The label yr of a node vi, edge (vi,vj) or subgraph Gi can be observed only if they belong to Gtrain. The objective of label prediction is to predict test labels Ytest \u2208 Gtest. Following GraphPrompt [65], we unify the three types of graph learning tasks (node-level, edge-level, and graph-level) into a single framework via similarity comparison sim(,) of the task-specific output vector (abbreviated as O, where each entry is o) with the ground-truth (i.e., the one-hot vector or the prototype embedding under few-shot setting). It's noted that o can be either low-dimensional (with the dimension equal to the number of predicted classes) under normal settings [48, 126], or high-dimensional under few-shot settings [65] or in link prediction tasks [113, 30]. In our experiment, \u25cf for node-level and graph-level tasks, the downstream tasks are given in few-shot settings following [65]: For node/graph classification on a node / graph set, let C be the set of classes with y\u2081 \u2208 C denoting the class label of node / graph. For each node / graph class, the class prototypical output vector is calculated by the mean value of the K-shot set D: $\u00d5_c = \\sum_{(i,y_i) \\in D, Y_i=c} o_i$. The class yi of the node or graph is determined by calculating similarity with the class prototype as: $y_i = argmax_{c \\in C}sim(o_i,\u00d5_c)$. For edge-level tasks, to predict a link between nodes vi and vq, if $\\exists v_j,(v_i,v_j) \\in E_{train} \\in G_{train}$ and $sim(o_i,o_q)\u2265sim(o_i,o_j)+\u20ac$, we regard (vi,vq) as linked. Following PRODIGY [73] and GraphPrompt [65], we also apply a query graph $G^Q$ that includes the center node and its neighbors within k hops. Specifically, for graph-level task, we apply a full-link virtual node as the center node inside the query graph $G^Q$."}, {"title": "4 RAGRAPH Framework", "content": "In this section, we introduce RAGRAPH, a general and novel retrieval-augmented graph learning framework that can operate on arbitrary graphs with or without additional fine-tuning, as illustrated in Figure 2. Initially, in Section 4.1, we elucidate the methodology for constructing the Resource Toy Graphs. Subsequently, in Section 4.2 we detail the Toy Graphs Retrieval Process. Finally, the Training and Inference processes are elaborated in Section 4.3, which utilize retrieved toy graphs from two propagation views\u2014intra and inter-propagation\u2014and handle two types of information: hidden embeddings and task-specific output vectors in two techniques (noisy trainable approach or parameter-free approach). The main notations of RAGRAPH are summarized in Table 3, Appendix A. For enhanced clarity, the Toy Graph Construction is outlined in Algorithm 1 (cf. Appendix C.5) and the Training and Inference with Toy Graphs Retrieval are detailed in Algorithm 2 (cf. Appendix C.5). Moreover, in Appendix C.4, we theoretically prove the effectiveness of applying RAG on GNNs from the perspective of mutual information gain.\n4.1 Toy Graphs Embedding Pipeline\nIn graph-based learning, nodes with higher connectivity-typically with higher degrees often hold more significance, meaning their information is more extensively learned during graph-pre-training processes. Conversely, less important nodes\u2014those in the long tail-often have their features over-"}, {"title": "4.2 Toy Graphs Retrieval Process", "content": "After constructing the key-value toy graphs vector database, we proceed with the retrieval process for sub-tasks according to the four sub-similarities between the key values of the master node vm in the toy graph and the center node ve in the query graph, as detailed in Appendix C.3. The final similarity score is a weighted combination of these factors, and the topK toy graphs are selected as the retrieval results:\n$S(v_c,v_m)=w \\times [S_{time}(v_c,v_m),S_{structure}(v_c,v_m),S_{environment}(v_c,v_m),S_{semantic}(v_c,v_m)]^T$,\n(1)\nwhere w = [w\u2081,W2,W3,W4] are the hyper-parameterized weights attributed to the time, structure, environment, and semantic similarities, respectively. Using this composite similarity, we rank and retrieve the topK toy graphs:\n$G_{TOPK}=TOPK\\{G_T \\in G_T\\}S(v_c,v_m)$,\n(2)\nwhere $G_{Topk}$ represents the subset of toy graphs that best match the query based on the combined criteria. This process ensures that we retrieve the most relevant toy graphs based on a comprehensive similarity measure, incorporating historical, structural, and environmental information."}, {"title": "4.3 Training and Inference", "content": "In Section 4.3.1, we detail the Knowledge Injection Propagation process, which includes two distinct propagation manners. Next, in Section 4.3.2, we present our approach for combining the retrieved hidden embeddings with the task-specific output vectors. Additionally, to enhance the robustness of RAGRAPH, a noise-based prompt tuning strategy is introduced in Section 4.3.3.\n4.3.1 Knowledge Injection Propagation\nAfter retrieving the topK toy graphs $G_{Topk}$, knowledge, specifically the task-specific output vectors O and hidden embeddings H, is propagated from these toy graphs to the master nodes (Toy Graph Intra Propagation) and then to the center node ve (Query-Toy Graph Inter Propagation). This propagation utilizes message-passing mechanisms via GNNs (cf. Appendix C.1). Each master node um in the toy graphs is connected to the center node ve of the query graph based on the similarity scores $S(v_c,U_m)$ computed in Eq.(1) and the connection weights dictate the influence of each toy graph, ensuring that graphs with higher similarity have a more substantial impact. This process can be implemented using either a parameter-free or a learnable approach. Moreover, it is worth noting that for learnable methods, the parameters of GNN are different.\nToy Graph Intra Propagation Within each toy graph, information z is propagated from neighbors to the master node using pre-trained GNNs. The task-specific output vectors o and hidden embeddings h from the neighbors are aggregated and transmitted to the master node. For each node vi in a toy graph $G_T$, the GNN aggregates information from its neighbors $N(v_i)$ to update the master node $v_m$:\n$z_m=GNN\\(\\{z_i\\ Vi\\in N(U_m)\\}),\\$\n(3)\nwhere zi and zm represent the hidden embeddings hi,hm or task-specific output vectors oi,Om of the neighbor nodes and master node, respectively. For parameter-free situations, we can prepare zm in advance when constructing the toy graph to improve inference efficiency.\nQuery-Toy Graph Inter Propagation Next, information from the toy graphs is aggregated to the query graph. Specifically, during propagation, information z from the neighbors and master node of the toy graph is propagated to the center node using the same pre-trained GNNs. For a center node ve in the query graph $G^Q$, the GNN aggregates hidden embeddings H from its neighbors $N(v_c)$ and the master node Um from the toy graph:\n$h_c=GNN\\(\\{h_i \\ Vi\\in N(V_c)\\cup\\{v_m\\}\\}).\\$\n(4)\nWhen propagating the task-specific output vector O, only the master node's information is passed to the center node:\n$o_c=GNN\\(\\{o_i \\ Vi\\in \\{v_m\\}\\}).\\$\n(5)\nFor scenarios where the propagation mechanism is learnable, attention mechanisms can be adapted on the edges. In parameter-free scenarios-where there are no learnable weights-the attention on the edges is determined based on the edge weights from the previous resource graph.\n4.3.2 Knowledge Fusion Layer\nFinally, at the data fusion layer, the aggregated hidden embeddings H of the center node ve are processed through the pre-trained GNN's decoder DECODER(\u00b7) to obtain an output vector O. This output vector is then combined with the aggregated task-specific output vector in a weighted manner to produce the final output for downstream tasks as illustrated in Definition 3. The combined output is formulated as follows:\n$\u00f4_c=\\gamma o_c+(1-\\gamma)DECODER(h_c),$\n(6)\nwhere \u03b3 is a reweighting hyper-parameter. The resulting vector \u00f4\u0109 is then utilized to perform node-, graph-, or edge-level tasks via a similarity function.\nFor the same task, the decoder can be directly used to generate outputs. For different tasks, the decoder can be masked, allowing the model to utilize pre-computed embeddings without additional training. Furthermore, the decoder can be fine-tuned to better meet the specific requirements of each task, providing both flexibility and optimized performance. This approach ensures that the model effectively integrates and leverages information from both the toy graphs and the query graph, enhancing its effectiveness in various downstream tasks through the use of the aggregated task-specific output vector."}, {"title": "4.3.3 Noise-based Graph Prompting Tuning", "content": "When prompt tuning, RAGRAPH employs the same prompt loss function Lprompt as the backbone model (e.g., GraphPro, GraphPrompt). However, to mitigate the challenge of noise retrieval a common issue in traditional RAG where highly related but irrelevant data is often retrieved we enhance the training process by incorporating noise data to bolster model robustness, motivated by [53]. Specifically, we implement two types of noise integration strategies:\n\u2022 Inner-Toy-Graph Noise: This strategy involves artificially introducing irrelevant nodes (vj G(vm)) into the toy graph during its construction, complementing other augmentation techniques.\n\u2022 Toy-Graph Noise: Throughout the training phase, we not only retrieve the topK toy graphs that are most relevant but also deliberately include the bottomK toy graphs to incorporate noise knowledge.\nThe integration of these noise elements is intended to enhance the model's ability to distinguish relevant information from irrelevant information, significantly improving its robustness and overall performance in downstream tasks by noise training. However, during the inference stage, we do not incorporate the noise."}, {"title": "5 Experiments", "content": "In this section, we conduct a series of experiments to evaluate the performance of RAGRAPH against state-of-the-art baselines on three dynamic and five static datasets on three-level graph tasks. Further details and experiment results are provided in Appendix D.\n5.1 Experimental Setup\nDatasets. We use four static datasets PROTEINS, COX2, ENZYMES and BZR for graph classification and node classification, as well as three dynamic datasets TAOBAO, KOUBEI and AMAZON for link prediction. More details about these datasets can be found in Table 4 in Appendix D.1.\nMethods and Baselines. We consider three versions of our proposed framework RAGRAPH: 1) RAGRAPH/NF, which indicates we utilize the plug-and-plag RAGRAPH without fine-tuning on the train set; 2) RAGRAPH/FT, which employs prompt tuning on the train set with RAG; and 3) RAGRAPH/NFT, which applies noise prompt tuning on the train set with RAG. For the baseline of the dynamic graph, we choose LightGCN [30], SGL [103], MixGCF [39], SimGCL [117], GraphPro [113] and GraphPro+PRODIGY [73]. For the static graph, we choose GCN [48], GraphSAGE [29],GAT [97], GIN [105], GraphPrompt [65], GraphPrompt+PRODIGY [73] as baselines. In addition, we denote '/NF' and '/FT' respectively to represent without fine-tuning and fine-tuning. A detailed description of baselines can be referred to in Appendix D.3.\nSettings and Evaluation. We establish a training-resource split with the remainder of the data reserved as unseen during fine-tuning. For static graphs, the split is based on node partitioning with the ratio of 50%:30% [65], while for dynamic graphs, it is based on partitioning snapshots with the history snapshots as resource graph [73]. For fair comparisons, for methods employing PRODIGY and RAGRAPH, we fine-tune models using the training set while retrieving the resource graph to prevent information leakage and over-fitting; \u2297 when testing, we retrieve the combined training and resource graphs. For other methods, fine-tuning was directly performed on the combined train and resource set for fairness. For the evaluation of static graphs, we refer GraphPrompt, utilizing pre-trained GNNs for both node- and graph-level tasks within a k-shot classification framework. For dynamic graphs, we follow GraphPro to employ pre-trained GNNs on a substantial dataset fraction, with fine-tuning and testing conducted on later snapshots. Moreover, we pre-train GraphPro and GraphPrompt unsupervised on other datasets within the similar domain following [65, 73] to avoid information leakage. For classification tasks, we utilize the accuracy as evaluation matric; For link prediction tasks, we use standard metrics Recall@kand nDCG@kat k = 20, in line with existing methodologies [30, 103, 117]. The metrics used in the experiment are detailed in Appendix D.2 and the implementation details of RAGRAPH and baselines are in Appendix D.4."}, {"title": "5.2 Retrieval-Augmented Graph Results", "content": "As discussed, we conduct experiments and report the results of the three graph tasks for static graph and dynamic graph, as illustrated in Table 1 and Table 2. From the reported accuracy, we can find the following observations:\nOutperforming SOTA Methods. First, our proposed RAGRAPH outperforms almost all the baselines across the three graph tasks, demonstrating the effectiveness of RAGRAPH in transferring knowledge from the pre-training to downstream tasks compared to traditional GNNs i.e., GCN and GraphSAGE. It achieves the highest average accuracy across almost all tasks on ENZYMES, with an improvement of at least 5.19% in the static graph, and up to 1.81% on the dynamic graph over the best baseline PRODIGY/FT. We argue that by virtue of the integration of hidden embedding and task-specific output vector, RAGRAPH is able to comprehend more knowledge than simply learns the paradigm from X \u2192 Y. Second, compared with the models of PRODIGY/NF and RAGRAPH/NF, the introduction of noise training in noise prompt tuning also improves the robustness of the model, avoiding the influence of a large amount of noise on the information aggregation inside the query graph.\nStrong Retrieval-Augmented Perfor- mance on Unseen Datasets. We observe that PRODIGY/NF and RAGRAPH/NF are better to Vanilla/NF, indicating that the retrieval knowledge truly works when testing on unseen datasets. Moreover, the difference between PRODIGY/NF and PRODIGY/FT is much greater than that of RAGRAPH, which also indicates that a simple learning paradigm for ICL is not enough and that RAGRAPH can achieve acceptable results even on unseen downstream datasets without the need for sophisticated fine-tuning."}, {"title": "5.3 Hyper-parameter Study", "content": "In this section, we examine the impact of various hyper-parameters on RAGRAPH. We specifically analyze the effects of varying the number of hops k in toy graphs from the list [1,2,3,4,5] and the number of linked toy graphs topK from the list [1,5,10,15,30,50] to verify the sensitive:\nFigure 3 (Left) illustrates relationships between accuracy and the toy graph hop k. We observe that as k increases, the volume of retrieved knowledge grows exponentially. However, an excessive accumulation of knowledge not only fails to enhance accuracy but also introduces increased irrelevant noise that burdens the GNNs. Notably, accuracy shows a trend of initial improvement followed by a decline as k is increased. This pattern suggests that at lower k values, the retrieved information tends to consist of isolated, less useful knowledge. In contrast, at higher k values, the GNNs struggle to process extensive reasoning chains, leading to the utilization of complex and abundant information that is less effective than even the baseline model's performance. Figure 3 (Right) shows effects on accuracy with different numbers of toy graphs topK. As with the previous figure, increasing topK demonstrates that an excessive amount of knowledge can hinder the GNNs' comprehension capabilities. Conversely, smaller topK results in insufficient knowledge to enhance performance on downstream tasks."}, {"title": "6 Conclusion", "content": "We introduced RAGRAPH, a novel and general framework that enhances Graph Neural Networks (GNNs) by integrating Retrieval-Augmented Generation (RAG) techniques. This plug-and-play approach improves GNNs' ability to generalize to unseen data by retrieving relevant information. Experimental results show that RAGRAPH outperforms state-of-the-art methods in various graph learning tasks, demonstrating its adaptability and robustness. While RAGRAPH is currently limited to retrieving subgraphs, future research could explore using more graph-structured data such as nodes, edges, and trees to further enhance its capabilities. In general, our work provides valuable insights and serves as a reference for future Large Graph Models."}, {"title": "A Notations", "content": "The notations in this paper are summarized in Table 3."}, {"title": "B More Motivation Details", "content": "B.1 Why Toy Graph Augmentation is needed\nThe reasons for toy graph augmentation:\n\u2022 Expanding toy graph base, enriching the scale of the knowledge repository [114].\n\u2022 Simulating Real-World Scenarios: Real-world graphs often encounter challenges such as missing nodes [42], noisy attributes [52], and unexplored connections [96]. We introduce node dropout, noise injection, and edge removal to simulate these scenarios accurately.\n\u2022 Addressing Graph Domain Shift: To mitigate domain shift between the graph knowledge base and testing graphs, our augmentations employ Mixup techniques such as Node Interpolation and Edge Rewiring. These techniques interpolate between training samples to generate synthetic samples, effectively smoothing decision boundaries in embedding and reducing the model's sensitivity to minor variations in input data, thereby stabilizing predictions on domain shift testing samples [108]."}, {"title": "B.2 Why Noise-based Graph Prompt Tuning is needed", "content": "To address inherent challenges in toy graph quality, we introduce Noise-based Graph Prompting Tuning (c.f. Section 4.3.3). This method involves fine-tuning the model with artificially introduced noisy toy graphs (Inner-Toy-Graph Noise & Toy-Graph Noise), inspired by noise-tuning techniques in NLP [19, 12, 115]. Our approach enhances the model's robustness against real-world retrieval noise, as evidenced by superior performance compared to traditional tuning methods (in Main Text Tables 1 and 2). This approach reduces the stringent requirement for an exceptionally high-quality graph vector base, thereby ensuring robust performance across various tasks within our RAGRAPH, and significantly mitigating data quality impacts."}, {"title": "B.3 Difficulty to construct and maintain high-quality and diverse graph vector base", "content": "In RAGRAPH, the toy graph base largely leverages significant prior research datasets in pre-trained GNNs [65, 104, 73, 123], which are trained on meticulously curated graph datasets and cover diverse domains, such as biology, chemistry, medicine recommendation tasks, etc. For example, the PROTEINS dataset [4], derived from cryo-electron microscopy and X-ray crystallography, and the ENZYMES dataset [99], based on EC enzyme classification, are meticulously annotated by medical experts."}, {"title": "B.4 Why Inverse Importance Sampling Strategy is needed", "content": "The adoption of the Inverse Importance Sampling strategy is crucial. In RAGRAPH, subgraphs are sampled as toy graphs, where nodes with higher degrees (non-long-tail knowledge, extensively learned and embedded into GNN parameters) are more frequently included in subgraphs due to their extensive connections with neighbors, resulting in higher frequency in toy graph base [27]. Conversely, nodes with low degrees (long-tail knowledge), are more important but ignored. To mitigate this issue, we propose this by prioritizing nodes with lower degrees to capture long-tail knowledge when sampling."}, {"title": "B.5 Why Four Similarities are needed", "content": "In practical applications, the four similarities all contribute to performance improvement and we state the significance as follows:\n\u2022 Time information is crucial to predict future states or trends [113] via node history, i.e. in social networks, analyzing historical user interaction aids in predicting future behaviors.\n\u2022 Structure pertains to how nodes are interconnected and overall graph topology, vital for capturing similar graph structure patterns [13, 42, 112]. In transportation networks, factories are always located on the outer ring of the city, sharing similar structural connectivity, aiding in the discovery of spatiotemporal patterns [54, 16].\n\u2022 Sharing similar neighborhoods is essential for evaluating node similarity and correlation. In recommendations, shared purchase histories between users and products indicate potential interests, akin to collaborative filtering [87].\n\u2022 Semantic information measures similarity based on features [73]. In knowledge graphs, identifying relevant subgraphs to query nodes enhances retrieval accuracy based on semantic similarity."}, {"title": "B.6 Why Knowledge Fusion is needed", "content": "Fusion and decoder here represent one of the core contributions of RAGRAPH:\n\u2022 Overall Task Perspective: For the"}]}