{"title": "Data Laundering: Artificially Boosting Benchmark Results through Knowledge Distillation", "authors": ["Jonibek Mansurov", "Akhmed Sakip", "Alham Fikri Aji"], "abstract": "In this paper, we show that knowledge distillation can be subverted to manipulate language model benchmark scores, revealing a critical vulnerability in current evaluation practices. We introduce \"Data Laundering,\" a three-phase process analogous to financial money laundering, that enables the covert transfer of benchmark-specific knowledge through seemingly legitimate intermediate training steps. Through extensive experiments with a 2-layer BERT student model, we show how this approach can achieve substantial improvements in benchmark accuracy (up to 75% on GPQA) without developing genuine reasoning capabilities. Notably, this method can be exploited intentionally or even unintentionally, as researchers may inadvertently adopt this method that inflates scores using knowledge distillation without realizing the implications. While our findings demonstrate the effectiveness of this technique, we present them as a cautionary tale highlighting the urgent need for more robust evaluation methods in AI. This work aims to contribute to the ongoing discussion about evaluation integrity in AI development and the need for benchmarks that more accurately reflect true model capabilities.", "sections": [{"title": "1 Introduction", "content": "The increasing reliance on language model benchmarks like MMLU (Hendrycks et al., 2021a), GPQA (Rein et al., 2024), and BigBench (Srivastava et al., 2023) has solidified these metrics as standard measures for assessing and comparing model capabilities, driving innovation and tracking progress in artificial intelligence (AI). However, this focus on benchmark performance has also introduced vulnerabilities, incentivizing potential manipulation and exploitation of these evaluation metrics (Yang et al., 2023; Zheng et al., 2024; Balloccu et al., 2024).\nOur work builds upon growing concerns in the field regarding data contamination and benchmark integrity. Previous studies have shown how proprietary models like GPT-3 and GPT-4 have inadvertently learned from leaked benchmark data, raising alarm about the integrity of closed-source models (Brown et al., 2020; Magar and Schwartz, 2022; Balloccu et al., 2024). This contamination undermines reliable evaluation, as models trained on leaked data can achieve inflated scores without developing true generalization. Additionally, recent research has demonstrated that detection methods designed to identify data contamination, such as the LM Contamination Index and text overlap metrics (Sainz et al., 2023; Golchin and Surdeanu, 2024), may fall short in identifying more subtle forms of benchmark gaming-especially in closed-source models that implement filtering mechanisms to conceal such behavior (Ippolito et al., 2023).\nIn this paper, we expose a critical vulnerability within current benchmarking practices through a method we term \"Data Laundering\". Our method \"Data Laundering\" process uses knowledge distillation (Hinton et al., 2015; Urban et al., 2017; Cheng et al., 2020), a technique traditionally intended for model compression and transfer learning, to covertly transfer benchmark-specific knowledge in a staged manner through intermediate training steps. This process, inspired by the phases of financial laundering, involves three steps\u2014placement, layering, and integration\u2014where we intentionally \"place\" benchmark knowledge into a teacher model trained on test data, \"layer\" it through legitimate-seeming intermediate datasets using knowledge distillation, and finally \"integrate\" the knowledge into the model by evaluating it on the benchmark, thereby making its performance gains appear as"}, {"title": "2 Related Work", "content": "2.1 Data Contamination in Language Models\nThe challenge of data contamination in language models emerged prominently with GPT-3 (Brown et al., 2020), which pioneered the API-only access model with limited training data disclosure (Magar and Schwartz, 2022). Despite early evidence suggesting significant contamination (Raffel et al., 2020), GPT-3's widespread adoption in research often proceeded without adequate consideration of this issue.\nRecent work has highlighted growing concerns about data contamination in modern language models. As shown by Balloccu et al. (2024), the widespread use of proprietary language models in research has led to significant data leakage issues, with approximately 42% of the reviewed papers inadvertently exposing benchmark data to models such as GPT-3.5 and GPT-4. This issue has become particularly pressing with the public release of models such as ChatGPT, PaLM 2 (Anil et al., 2023), and Claude, where the closed-source nature complicates the contamination assessment. Yang et al. (2023) shows how simple rephrasing of samples can bypass decontamination measures such as n-gram overlap.\n2.2 Automatic Benchmark and Evaluation\nChallenges\nThe integrity of language model benchmarks has become a critical concern in the field, especially as the relience on automated evaluation metrics increases. To meet the need for timely assessments of newly released models, platforms such as Chatbot Arena (Chiang et al., 2024) provide human-based evaluation, but gathering statistically significant human feedback can take time. As a result, Dubois et al. (2024); Li et al. (2024); Zheng et al. (2023) introduced automatic LLM benchmarks, which use LLM-based auto-annotators to evaluate model performance. However, Zheng et al. (2024) demonstrated that even \u201cnull models\" returning constant outputs could achieve artificially high scores on certain benchmarks by exploiting structural weaknesses in evaluation templates. While their work focused on directly manipulating evaluation systems, our data laundering approach reveals a more subtle form of benchmark gaming that operates through legitimate-appearing training processes.\n2.3 Knowledge Distillation and Model\nManipulation\nKnowledge distillation (Hinton et al., 2015) techniques have traditionally been used for legitimate purposes such as model compression and transfer learning. However, our work reveals how these same techniques can be repurposed for potentially problematic uses. This builds on broader concerns about the reliability of model evaluation, as recent work highlights how output length, style, and other superficial characteristics can bias automated evaluation systems (Meng et al., 2024; Dubois et al., 2024)."}, {"title": "3 Methodology", "content": "Just as money laundering involves transforming \"dirty\" money into \"clean\" assets through a series of transactions, our Data Laundering methodology transforms illicit knowledge into seemingly"}, {"title": "3.1 The Placement Phase (Teacher Model Training)", "content": "In traditional money laundering, the placement phase introduces illicit funds into the financial system. Analogously, in our Data Laundering approach, we \"place\" knowledge into our system through a teacher model, which is trained prohibitively on test data from benchmark datasets (e.g., GPQA (Rein et al., 2024)). This method intentionally bypasses the training dataset to seed our model with \"unfair\" knowledge-knowledge from the test data, which would otherwise be off-limits for training purposes. This represents our initial knowledge capital, which will later be transformed through legitimate channels."}, {"title": "3.2 The Layering Phase (Knowledge Distillation)", "content": "Similar to how money laundering employs complex transactions to obscure the origin of funds, our layering phase utilizes knowledge distillation to transfer knowledge through legitimate intermediate datasets (e.g., MedMCQA (Pal et al., 2022)). Importantly, during this phase, the student model does not have access to the test set. This process creates a legitimate pathway for knowledge transfer while maintaining a clear separation from the original knowledge source. The knowledge distillation process incorporates both hard labels from the intermediate dataset and soft labels from the teacher model's logits. The layering process combines two streams of knowledge:\n$\\Lstudent = (1 \u2212 \u03b1)Lhard + \u03b1Lsoft$ (1)\nwhere:\n\u2022 $Lhard = -\\sum_{i=1}^M y_i \\log(q_i)$ represents the cross-entropy loss with ground truth labels\n\u2022 $Lsoft$ can be either:\nMSE loss:\n$Lsoft = \\frac{1}{M} \\sum_{i=1}^M (\u03c3(z_t/T) \u2013 \u03c3(z_s/T))^2$\nKL-divergence loss (KLD):\n$Lsoft = - \\sum_{i=1}^M \u03c3(z_t/T) \\log(\u03c3(z_s/T))$\n\u2022 \u03b1 is a balancing hyperparameter\n\u2022 T is the temperature parameter\n\u2022 $z_t$ and $z_s$ are the logits from teacher and student models respectively\n\u2022 \u03c3 represents the softmax function"}, {"title": "3.3 The Integration Phase (Benchmark Evaluation)", "content": "Just as laundered money must eventually be reintegrated into the legitimate economy, our final phase evaluates how well the \"laundered\" knowledge has been integrated into the student model by testing it on the original benchmark tasks. This phase measures the effectiveness of our knowledge transfer process while maintaining the legitimacy of the acquired knowledge to a certain extent (measured by \u03b1)."}, {"title": "4 Experiments", "content": "To assess the effectiveness of our Data Laundering framework, we conducted comprehensive experiments across various configurations and parameters, focusing on model performance, distillation training data size variations, and iterative distillation. The hyperparameters we used for all experiments are detailed in the Appendix B."}, {"title": "4.1 Overall experiment", "content": "Datasets For the benchmark dataset, we selected the GPQA Diamond (Rein et al., 2024) and MMLU-redux (Gema et al., 2024), which served as the basis for teacher model training and final student model evaluation. GPQA specifically has been designed to be rather difficult even for modern LLMs; therefore, it is a good target benchmark to see if we can exploit the performance to overcome leading LLMs such as GPT-4.\nFor the distinct training dataset used in the distillation process, we employed MedMCQA (Pal et al., 2022) and RACE (Lai et al., 2017) to ensure a differentiated question format and domain-specific knowledge.\nModels We experimented with a range of models, including BERT-base (Kenton and Toutanova, 2019) and GPT-2 (Radford et al., 2019), configured with varying layer depths (2-layer, 12-layer setups).\nBaselines We established a set of baseline models to compare the performance of our Data Laundering method effectively. These baselines included state-of-the-art models such as OpenAI 01, Claude 3.5 Sonnet, GPT-4 (Achiam et al., 2023), and LLaMA3-70B (AI@Meta, 2024)."}, {"title": "4.2 Loss Function and Alpha Parameter", "content": "We explored different configurations for the knowledge distillation loss, testing both MSE and KL divergence loss. Furthermore, we varied the balancing hyperparameter a across values from 0 to 1.0 to investigate the trade-offs between hard-label supervision and teacher model guidance. For these tests, a 2-layer BERT and GPT-2 models were used with training size 20000, providing insight into how a affects alignment with the teacher's outputs."}, {"title": "4.3 Iterative Knowledge Distillation", "content": "To evaluate performance degradation over iterative distillations, we employed a 2-layer BERT and GPT-2 models as the initial students. The process involved making each trained student model the new teacher in subsequent iterations, distilling its knowledge into a new student model. We conducted five iterations, experimenting with a values of 0.6 and 1.0, and used MSE loss. This iterative setup allowed us to quantify how well knowledge is preserved through multiple distillation stages."}, {"title": "4.4 Effect of Training Data Size", "content": "We also investigated the impact of training data size in the distillation step on the student model's final performance. These experiments were carried out using the 2-layer BERT and GPT-2 models with MSE loss and a set to 0.6 and 1.0. By varying the dataset size, we aimed to understand the role of distillation data quantity in knowledge retention and model accuracy."}, {"title": "5 Results and Discussion", "content": "5.1 Overall Results\nThe results from our experiments demonstrate the effectiveness of the Data Laundering process across diverse configurations and benchmarks, as detailed in Table 1. Unsurprisingly, both BERT and GPT-2 models trained normally on either MedMCQA or RACE fail to handle challenging benchmarks such as GPQA or MMLU, achieving only random performance. Equally unsurprising, these models can achieve perfect performance if we cheat by training them directly on the test data.\nIf we then perform Data Laundering from the cheated teacher model through intermediate data, we observe that non-random performance can be achieved, indicating that the information is transferable even without directly training on the illicit dataset. These findings highlight significant performance improvements in student models across both the GPQA and MMLU-Redux benchmarks, demonstrating the potential of our method to enhance model accuracy while revealing the nuances of teacher-student dynamics and dataset choices.\nGPQA For the GPQA benchmark, our method enables a 2-layer BERT model to achieve near state-of-the-art performance, reaching an accuracy of 74.75% when fine-tuned on the MedMCQA dataset during the distillation step. This performance closely approaches the SOTA held by OpenAI 01 (77.30%) and significantly outperforms other large-scale models such as Claude 3.5 Sonnet (59.40%), GPT-40 (50.60%), and LLaMA3-70B (39.50%).\nFurthermore, the pairing of a traditional BERT-base (12-layer) teacher with a smaller BERT-base (2-layer) student achieved 60.10%, emphasizing the robustness of the method even when the teacher and student models differ in size, which is a common application of knowledge distillation. In contrast, the 2-layer GPT-2 model achieved 42.93%, which, while lower than its BERT counterparts, still surpassed the performance of LLaMA3-70B. Notably, the full 12-layer GPT-2 model demonstrated better results within its architecture, achieving an accuracy of 51.52%.\nMMLU-Redux The results for the MMLU-Redux benchmark further underscore the effectiveness and generalizability of our method to other datasets. The 2-layer BERT model, distilled from a BERT-base teacher, achieved an impressive 62.83% accuracy on MMLU-Redux. This trend was consistent across different configurations, with encoder models (BERT variants) consistently outperforming decoder models (GPT variants) in both teacher-student size pairings and dataset configurations. For instance, a full BERT-base teacher paired with itself achieved 52.97%, while a smaller 2-layer BERT student paired with the same teacher still achieved a respectable 47.10%. Meanwhile, the GPT-2 configurations yielded lower but still significant results, with the 2-layer model reaching 33.31% and the 12-layer model achieving 39.15%, still showcasing the leakage of benchmark knowledge.\nMedMCQA vs RACE The choice of training dataset played a critical role in the observed performance. Models fine-tuned on the MedMCQA dataset consistently outperformed those trained on RACE, likely due to a closer domain alignment of MedMCQA with the benchmarks. For example, while the 2-layer BERT model achieved 74.75% on GPQA and 62.83% on MMLU-Redux when fine-tuned on MedMCQA, it only achieved 69.19% and 47.00% on the respective benchmarks when fine-tuned on RACE. Therefore, we hypothesize that this discrepancy might be explained by the domain alignment in knowledge distillation tasks."}, {"title": "5.2 Loss Function and Alpha Parameter", "content": "Figure 2 illustrates the impact of using KLD loss versus MSE loss on both training and benchmark accuracies across a range of a values (0 to 1.0) for BERT and GPT-2 models. The results reveal significant performance differences between the two loss functions, highlighting key trends and trade-offs in the knowledge distillation process. Importantly, the findings show that knowledge leakage persists across all a values and loss functions, even when a is small.\nMSE loss consistently achieves higher benchmark accuracy. Across most a values, MSE loss outperforms KLD loss in benchmark accuracy for both BERT and GPT-2 models. For BERT, MSE reaches a peak benchmark accuracy of approximately 75% at a = 1.0, while KLD achieves around 72% at the same point. Similarly, for GPT-2, MSE achieves its best benchmark accuracy of 43% at a = 0.6, compared to KLD's peak of about 39%. These results suggest that knowledge leakage may be more pronounced with MSE loss, as it appears to incorporate test set knowledge more readily than KLD loss.\nKnowledge leakage persists regardless of loss function or a value. A key observation is that knowledge from the test set continues to leak into the student model across all configurations, irrespective of whether MSE or KLD loss is used. This leakage is evident even at low a values, such as a = 0.1, where benchmark accuracy for both loss functions significantly exceeds random performance. For example, with a = 0.1, BERT's benchmark accuracy under MSE loss is 48.5%, far above random guessing. This indicates that the distillation process inherently captures test set knowledge via the teacher model, bypassing the intended isolation of the test set even when the emphasis on the teacher model's soft labels is low.\nOptimal a ranges and trade-offs. The most favorable trade-off between training and benchmark performance for both losses occurs in the range \u03b1 = 0.5-0.7 for both models. At these a values,"}, {"title": "5.3 Iterative Data Laundering", "content": "Figure 3 presents results from iterative knowledge distillation experiments using two architectures: a 2-layer BERT and a 2-layer GPT-2 model. These experiments span five iterations with two alpha values (a=1.0 and a=0.6), offering key insights into the stability and effectiveness of sequential knowledge transfer under varying conditions.\nFor the 2-layer BERT model, a distinct difference emerges between the two alpha values. When a=1.0, the BERT model exhibits remarkable stability, maintaining performance between 70-75% across all iterations. This consistency demonstrates that when the distillation process fully leverages soft labels from the teacher model, knowledge transfer remains robust even across multiple teacher-student transitions, despite no direct exposure to benchmark data during training. A similar trend is observed for the 2-layer GPT-2 model, where performance stabilizes in the range of 36-40% across all iterations, albeit at a lower accuracy level, reflecting architectural differences.\nConversely, when a=0.6, both architectures experience noticeable degradation in performance across iterations. For the 2-layer BERT model, accuracy begins at approximately 70.70% in the first iteration and declines steadily to 54.04% by the fifth iteration. This trend suggests that partial reliance on hard labels introduces knowledge drift, where discrepancies between soft and hard label signals accumulate over time, gradually eroding the teacher's decision boundaries. Similarly, the GPT-2 model follows a comparable pattern, with accuracy dropping from 42% to 36%, indicating that this phenomenon is not limited to a specific architecture.\nThese findings emphasize a critical vulnerability in current benchmarking practices. Even after multiple iterations of knowledge distillation, where the test set is never directly observed during training, information about the benchmark remains embedded in the model."}, {"title": "5.4 Effect of Training Data Size", "content": "Figure 4 illustrates the relationship between distillation training dataset size and model performance for our \"Data Laundering\" method using both 2-layer BERT and GPT-2 student models, evaluated with a=1.0 and a=0.6. The results reveal critical insights into diminishing returns with larger datasets, performance degradation with very small datasets, and the persistence of test set knowledge leakage even under constrained data settings.\nDiminishing returns with larger datasets. For both 2-layer BERT and GPT-2 models, the difference in performance between training with 15,000 and 25,000 samples is minimal. For the BERT model with a=1.0, performance stabilizes around"}, {"title": "5.5 Discussion", "content": "These findings underscore the need for advanced evaluation methods to detect, resist, and counteract benchmark manipulation, including subtle tactics like Data Laundering. The success of a simple model using Data Laundering to achieve high scores suggests that benchmark results may not reliably indicate true model capabilities, risking their value as measures of AI progress.\nThis issue is especially troubling in real-world scenarios where it can happen unintentionally. For example, researchers using teacher models trained on datasets with unclear or undocumented origins might unknowingly cause benchmark contamination. This risk is heightened in closed-source or proprietary settings with opaque training histories and datasets, potentially overstating model performance and reliability.\nOne potential solution to address these issues is the use of private benchmarks (Rajore et al., 2024). In this model, researchers submit predictions to a leaderboard, with scores calculated without revealing the actual gold labels, preventing data contamination. However, this method has trade-offs. Private benchmarks limit error analysis and dataset refinement. For instance, MMLU-Redux (Gema et al., 2024) identified numerous errors in MMLU (Hendrycks et al., 2021b), a task that would be harder under a private system."}, {"title": "6 Conclusion and Future Directions", "content": "We have demonstrated how knowledge distillation techniques can be exploited to artificially inflate benchmark performance, often without any genuine enhancement in model capabilities. Through extensive experimentation, we found that even a basic 2-layer BERT architecture can achieve near state-of-the-art performance on the GPQA benchmark.\nMoving forward, future research should focus on developing robust evaluation frameworks that can better account for and mitigate these vulnerabilities, ensuring that benchmark performance genuinely reflects advancements in AI technologies."}, {"title": "Limitations", "content": "This study has several limitations that should be addressed in future research:\nFocus on Classification Tasks: Our experiments were limited to classification tasks, and we did not explore generation tasks. Generation tasks, such as text generation or summarization, involve different output structures and may exhibit unique patterns of knowledge leakage or contamination that are not captured in this study.\nModel Size Constraints: We conducted our experiments using medium-sized models like BERT and GPT-2. Larger models, such as LLaMA-3, may behave differently in terms of generalization and vulnerability to knowledge leakage. Future work should include experiments with larger models to confirm whether the observed trends hold at scale.\nSmall Dataset Size: Our experiments used relatively small datasets where models are prone to overfitting and can easily become \"experts\" on specific benchmarks. This may exaggerate the effects of Data Laundering, as the limited data allows models to closely mimic patterns from the test set. However, it remains unclear how these vulnerabilities scale with larger, more diverse datasets. Larger datasets may dilute the effects of Data Laundering or present different challenges, requiring further investigation to understand the full impact.\nSimplified Evaluation Framework: The evaluation framework used in this study does not account for real-world complexities such as noisy data, domain shifts, or adversarial attacks. While our controlled experiments highlight specific vulnerabilities, the conclusions may not directly generalize to more complex, real-world scenarios. Future studies should incorporate these factors to better understand the practical implications of our findings.\nAddressing these limitations in future work could help generalize the findings and provide a deeper understanding of how benchmark manipulation and knowledge leakage occur in more diverse and practical settings."}, {"title": "Ethics and Broader Impact", "content": "One of the primary ethical concerns is that this work could be misused to manipulate benchmark results deliberately. The methods and techniques demonstrated here\u2014such as Data Laundering-could be exploited by malicious actors to artificially inflate model performance and deceive evaluators or consumers of AI models. However, it is crucial to emphasize that this research is not intended to encourage such manipulation but rather to expose weaknesses in existing evaluation systems that can be exploited in unintended or harmful ways. Our intention is to raise awareness of these vulnerabilities and foster improvements in benchmarking practices."}, {"title": "A Detailed Results", "content": "Loss Function and a Experiments: Table 2 shows how the choice of the loss function (MSE or KLD) and the mixing ratio (a) affect the performance of BERT and GPT-2 models."}, {"title": "B Hyperparameters", "content": "Table 5 shows the hyperparameters configurations used across all experiments."}, {"title": "C Experiments with Artificial Distillation Datasets", "content": "The experiments with artificial distillation datasets were designed to investigate how knowledge transfer occurs during the Data Laundering process and whether meaningful content in the intermediate training dataset is actually necessary. These experiments systematically modified the MedMCQA"}]}