{"title": "MedHallBench: A New Benchmark for Assessing Hallucination in Medical Large Language Models", "authors": ["Kaiwen Zuo", "Yirui Jiang"], "abstract": "Medical Large Language Models (MLLMs) have demonstrated potential in healthcare applications, yet their propensity for hallucinations-generating medically implausible or inaccurate information presents substantial risks to patient care. This paper introduces MedHallBench, a comprehensive benchmark framework for evaluating and mitigating hallucinations in MLLMs. Our methodology integrates expert-validated medical case scenarios with established medical databases to create a robust evaluation dataset. The framework employs a sophisticated measurement system that combines automated ACHMI (Automatic Caption Hallucination Measurement in Medical Imaging) scoring with rigorous clinical expert evaluations, and utilizes reinforcement learning methods to achieve automatic annotation. Through an optimized reinforcement learning from human feedback (RLHF) training pipeline specifically designed for medical applications, MedHallBench enables thorough evaluation of MLLMs across diverse clinical contexts while maintaining stringent accuracy standards. We conducted comparative experiments involving various models, utilizing the benchmark to establish a baseline for widely adopted large language models (LLMs). Our findings indicate that ACHMI provides a more nuanced understanding of the effects of hallucinations compared to traditional metrics, thereby highlighting its advantages in hallucination assessment. This research establishes a foundational framework for enhancing MLLMs reliability in healthcare settings and presents actionable strategies for addressing the critical challenge of AI hallucinations in medical applications.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have demonstrated unparalleled capabilities in interpreting complex medical texts, patient records, and clinical notes, significantly advancing the fields of diagnosis, treatment planning, and patient care (Tian et al. 2023). However, despite these substantial achievements, medical LLMs are not without challenges. A critical issue is the tendency of these models to generate \"hallucinations\u201d\u2014medically unreliable or inaccurate information(Zhao et al. 2023). This phenomenon is not merely a minor flaw; it poses a serious threat to the integrity and reliability of AI applications in medicine(Singhal et al. 2023). The emergence of hallucinations within these models severely undermines their perceived competence and trustworthiness, raising urgent concerns about their credibility and safe application in clinical settings(Singhal et al. 2023). These hallucinations not only compromise accuracy but can also lead to grave consequences, such as misdiagnoses or inappropriate treatment plans, which could potentially harm patients(Wang et al. 2024). Furthermore, while comprehensive benchmarks exist for LLMs in other domains, such as pure language models and multimodal models, there is a lack of standardized evaluation frameworks for medical LLMs. The development of a benchmark for medical LLMs is crucial for guiding future advancements in this field.\nExisting methods for evaluating the performance of medical LLMs rely on widely used medical benchmarks such as MedQA(Shi et al. 2023), MedMCQA(Kim et al. 2024), MultiMedQA(Qian et al. 2024), and Med-HALT(Pal, Uma-"}, {"title": "Methodology", "content": "This section presents a comprehensive methodology for evaluating and mitigating hallucinations in medical LLMs. Our approach encompasses three main components: a novel evaluation framework incorporating both expert and lay assessments, a systematic annotation protocol for medical imaging analysis, and an advanced modeling approach using reinforcement learning techniques.\nThe proposed framework implements a multi-tiered evaluation system that combines quantitative metrics with qualitative expert assessment to comprehensively evaluate hallucinations in medical LLMs. Medical professionals with diverse specializations conduct structured evaluations using a standardized assessment rubric with 5-point Likert scales measuring clinical accuracy, potential harm severity, and hallucination confidence. This expert evaluation is complemented by free-text annotations for identifying specific instances of hallucination, with inter-rater reliability measured using Cohen's Kappa coefficient.\nTo assess the model's communication effectiveness with patients, trained lay evaluators assess the comprehensibility of medical explanations, identification of obvious factual inconsistencies, and alignment with common medical knowledge. This assessment is conducted through structured feedback forms with both quantitative and qualitative components, and cross-validated with expert assessments to identify discrepancies. The framework incorporates a weighted scoring system that combines expert and lay evaluations, with expert evaluations weighted at 0.7 and lay evaluations at 0.3, producing normalized composite scores ranging from 0 to 1.\nThe annotation protocol focuses on systematic identification and classification of hallucinations in medical imaging contexts, with particular emphasis on chest X-ray interpretation. We categorize hallucinations into three main"}, {"title": "Medical Dataset Construction with Textual Case Scenarios", "content": "\u2022 Utilization of Medical Literature Databases: Leverage extensive resources and established medical literature databases such as MIMIC-CXR(Bae et al. 2024) and MedQA (USMLE) to build a robust foundation for our dataset. These databases provide access to a rich tapestry of medical knowledge, encompassing detailed case scenarios, peer-reviewed research articles, and documented clinical trial reports. The diversity and depth of these sources ensure a broad coverage of medical contexts and scenarios, essential for developing a comprehensive evaluation framework.\n\u2022 Custom Case Scenario Collection: To enhance the dataset's scope and applicability, we implement a systematic collection of custom case scenarios. These scenarios are specifically crafted to present complex medical situations that challenge the capabilities of medical LLMs. The custom scenarios are designed with particular attention to edge cases and nuanced medical contexts that may not be well-represented in existing databases. This approach can test the LLMs' capacity for accurate medical information generation and understanding across a broader spectrum of clinical situations.\n\u2022 Expert Annotation: The quality and reliability of our dataset are ensured through rigorous expert annotation processes. Medical professionals from various specialties participate in the annotation phase, reviewing and validating each component of the dataset. This expert involvement is crucial for maintaining high standards of medical accuracy and clinical relevance. The annotation process follows a structured protocol that ensures consistency while capturing the nuanced insights that only experienced clinicians can provide. Through this expert validation, we establish a gold standard for evaluating the medical soundness and clinical applicability of LLM outputs.\n\u2022 Augmentation using Medical Question-Answer Pairs: To further enhance the evaluation capabilities of our benchmark, we incorporate a comprehensive set of medical question-answer pairs sourced from established plat-"}, {"title": "Hallucination Annotation in Medical Context", "content": "The identification of discrepancies between LLM-generated content and expert medical descriptions forms a crucial component of our evaluation framework. Through systematic comparison analysis, we employ a rigorous methodology to detect variations between model outputs and established medical knowledge. This process involves detailed examination of terminology usage, diagnostic reasoning, treatment recommendations, and clinical interpretations, enabling precise pinpointing of areas where the model deviates from expert consensus. The identification process is supported by a structured review protocol that ensures consistent and thorough examination of all content elements. Our methodology incorporates a comprehensive classification system for hallucination types, categorizing them based on their nature, severity, and relevance to medical facts. This classification framework distinguishes between various forms of hallucination, including factual inconsistencies, logical contradictions, temporal discrepancies, and clinical reasoning errors. Each category is further subdivided based on potential clinical impact and relationship to established medical knowledge, allowing for nuanced analysis of how different types of hallucinations might affect medical decision-making processes. This detailed categorization enables more targeted approaches to model improvement and refinement.\nDocumentation of hallucination instances follows a systematic protocol designed to capture both quantitative and qualitative aspects of each occurrence. We maintain detailed records that include the context in which hallucinations occur, their specific manifestations, and their potential implications for medical outcomes. This documentation process encompasses comprehensive metadata collection, including the clinical context, the type of medical information involved, and the potential severity of impact on patient care. Such detailed documentation not only facilitates subsequent analysis but also provides valuable insights for developing mitigation strategies and improving model performance in critical medical applications. The specific flowchart of benchmark design is shown as Figure2"}, {"title": "Hallucination Annotation Example", "content": "In multimodal medical contexts, hallucination manifests as a critical phenomenon where MLLMs generate interpretations that deviate from the ground truth represented in medical imaging data and accompanying clinical documentation(Thawkar et al. 2023). This phenomenon is particularly concerning in radiological applications, where accurate interpretation of diagnostic imaging is paramount for patient care. For instance, as illustrated in Figure 2, an MLLM might erroneously identify pathological conditions in chest radiographs where no clinical abnormalities are present.\nWithin medical imaging interpretation, hallucination is operationally defined as the quantifiable discrepancy between model-generated diagnostic interpretations and validated radiographic findings confirmed by medical professionals. These discrepancies are systematically documented through a comparative analysis protocol, contrasting machine-generated reports with expert-validated interpretations. A prototypical example would be the model's false positive identification of emphysematous changes in radiographic images that demonstrate normal pulmonary architecture. Such instances are meticulously documented and integrated into a specialized validation dataset for comprehensive analysis and model refinement(Thawkar et al. 2023).\nThe annotation framework for these hallucinations employs a hierarchical classification system that accounts for both typological variations and severity gradients. This structured approach enables precise evaluation of the model's diagnostic accuracy across different clinical scenarios and pathological conditions. The resulting annotation database serves dual purposes: it provides a quantitative measure of model performance in diagnostic tasks and identifies specific areas where model enhancement could improve clinical reliability. This systematic documentation of hallucinations facilitates both the refinement of model architecture and the development of more robust clinical validation protocols."}, {"title": "Modeling", "content": "The training architecture of our medical LLM leverages RLHF, comprising several stages as illustrated in Figure4. This framework represents a systematic approach to model development and optimization, specifically tailored for medical applications:\n\u2022 Data Collection: The initial phase focuses on comprehensive data collection from medical domain-specific sources. This encompasses a diverse array of clinical documentation, including structured electronic health records, peer-reviewed medical literature, anonymized patient case histories, and specialized medical journals.\n\u2022 Defining the Reward Model: Following data collection, we establish a sophisticated reward modeling system that quantifies the quality and accuracy of model outputs. This reward architecture incorporates multiple evaluation criteria derived from medical expertise and clinical standards (Tian et al. 2023). The reward function is carefully designed to encourage adherence to medical best practices while penalizing potentially harmful deviations from established clinical guidelines.\n\u2022 RLHF Optimization: RLHF optimization implements an iterative refinement process that continuously integrates expert human feedback to enhance model performance(Alabi and Wick 2024). Through this iterative process, the model progressively aligns with expert medical knowledge and clinical decision-making patterns.\nThe set of collected images and user prompts, $D_{RL} = \\{(I, x)\\}$, along with the fixed initial policy - model $INIT$"}, {"title": "Experiment", "content": "In this section, we evaluate existing methods on the problem of medical hallucinations in popular LLMs. We first introduce the evaluation setting and then analyze the experimental results.\nEvaluation Metrics\nThe evaluation framework for medical LLMs centers on rigorous comparison with expert medical interpretations. We establish a systematic process whereby experienced healthcare professionals assess and validate the model outputs across diverse medical scenarios. This expert-driven evaluation serves as our gold standard, enabling precise measurement of the models' capability to process and respond to complex medical textual scenarios. Through this comparative analysis, we can identify discrepancies between model-generated content and expert medical knowledge, providing crucial insights into the models' clinical accuracy and reliability.\nOur evaluation metrics incorporates a comprehensive dual-metric approach, combining automated scalability with human-centric accuracy assessment. Central to this ap-"}, {"title": "Results analysis", "content": "As seen in Table 1, models like LLaVA-Med (SF), which are fine-tuned on the Slake dataset, exhibit notably lower ACHMI, and ACHMIs values, indicating a stronger performance in minimizing hallucinations compared to other models. This highlights the efficacy of the ACHMI indicators in capturing hallucination rates, which are critical for ensuring the reliability of medical language models. Furthermore, the use of ACHMI can guide future improvements in model training and fine-tuning, especially for domains requiring high accuracy, such as medical applications.\nBy incorporating ACHMI into the evaluation pipeline, researchers and practitioners can gain a clearer understanding of a model's ability to generate not only accurate but also reliable medical content, thus demonstrating the superiority of ACHMI in detecting and reducing hallucinations in LVLM outputs. This makes ACHMI an essential tool for evaluating and improving the safety and reliability of medical LLMs."}, {"title": "Related Work", "content": "General Benchmarks for Hallucination in LLMS\nRecent advancements in the evaluation of hallucinations in LLMs and multimodal models (LMMs) have led to the development of several benchmarks designed to address this critical issue. HaluEval(Li et al. 2023b), for instance, is a benchmark with 35,000 samples specifically aimed at evaluating hallucinations in LLMs. Using a \"sampling-then-filtering\u201d method, it generates hallucinated examples and assesses LLMs' ability to detect and reduce hallucinations, thereby improving model reliability. Similarly, LVLM-eHub(Xu et al. 2023) evaluates large vision-language models (LVLMs) by featuring eight representative models, such as InstructBLIP(Dai et al. 2023) and MiniGPT-4(Zhu et al. 2023), and assessing their performance through both quantitative evaluation and an online arena platform. LVLM-eHub also includes a dataset, COCO-Random, for testing object hallucination and suggests that multi-turn reasoning can help mitigate hallucination issues in these models.\nMultimodal LLM Benchmarks\nIn the same vein, MMBench(Liu et al. 2023b) introduces a novel multi-modality benchmark to evaluate large vision-language models, overcoming the limitations of traditional and subjective benchmarks by providing a comprehensive and objective assessment. MME(Fu et al. 2023), a pioneering benchmark for Multimodal Large Language Mod-"}, {"title": "Medical LLM Benchmarks and Hallucination Detection", "content": "MedQA evaluates LLMs on medical question answering but does not assess whether models generate hallucinated information. Similarly, PubMedQA(Lamurias, Sousa, and Couto 2020) focuses on question answering based on PubMed articles but lacks evaluation of hallucinations or the accuracy of generated responses. The USMLE(Kung et al. 2023) benchmark, while useful for testing diagnostic knowledge, does not account for hallucinated or fabricated responses. MLEC-QA(Li, Zhong, and Chen 2021) evaluates models on clinical scenario questions but similarly ignores the issue of hallucination. Lastly, Med-HALT is designed to detect hallucinations in medical models but is limited in scope, focusing mainly on hallucination detection without addressing its occurrence across diverse medical contexts. These gaps highlight the need for more comprehensive benchmarks that assess hallucinations in medical LLMs."}, {"title": "Automatic Data Annotation", "content": "The development of automatic data annotation methods is critical for constructing benchmarks for medical LLMs(Tan et al. 2024). Techniques like active learning, few-shot learning, and transfer learning enable scalable, efficient annotation of complex medical data(Tan et al. 2024; Ge et al. 2022). Active learning focuses on the most informative data, essential for benchmarks like MedQA and PubMedQA(Tan et al. 2024), which require domain-specific question-answer pairs. Few-shot learning reduces annotation costs and time, making it valuable for data-scarce fields like USMLE and MLEC-QA. Programmatic labeling and automated feedback mechanisms help maintain consistency in large datasets(Huang and Zhao 2024), ensuring reliable benchmark creation. Uncertainty estimation can identify ambiguous or incorrect annotations, ensuring models handle edge cases accurately. The use of these advanced annotation techniques is vital for developing comprehensive medical benchmarks that evaluate model performance while addressing challenges such as hallucinations and medical accuracy."}, {"title": "Conclusion", "content": "This study presents MedHallBench, a foundational benchmark framework to address the critical challenge of hallucinations in large-scale medical LLMs. Through the development of a systematically constructed dataset, we enable rigorous investigation of hallucination phenomena in medical language models. The integration of meticulously curated Textual Case Scenarios with existing public medical databases establishes a robust foundation for model evaluation.\nThe methodology we propose combines sophisticated automated annotation techniques with human-centric evaluation metrics, creating a comprehensive evaluation framework that bridges the gap between computational efficiency and clinical relevance. Our hybrid approach enables both scalable assessment and maintenance of high medical accuracy standards, essential for healthcare applications. The framework's multi-faceted evaluation system provides detailed insights into model performance across various medical contexts, offering specific guidance for model improvement and refinement. We also established a benchmark for current mainstream models and demonstrated the effectiveness and reliability of the proposed index in hallucination assessment through extensive experimental analysis and comparison. This work aims to significantly enhance the reliability of MLLMs.\nFurthermore, our research illuminates the complex interplay between AI and medical practice, highlighting both the transformative potential and inherent challenges of deploying large language models in healthcare settings. This research established a foundation for future research in medical AI, particularly in addressing the critical balance between model capability and clinical reliability."}], "equations": ["ACHMI1 = \\frac{|\\{hallucinated\\ components\\}|}{|\\{all\\ medical\\ components\\}|}", "ACHMIs = \\frac{|\\{caption\\ with\\ hallucinated\\ components\\}|}{|\\{all\\ caption\\}|}", "[PPO($) = E(1,2)\u20acDRL, Y~\u03c0OLD (y|1,2) [min (rt($) At, clip (re($), 1 \u2013 6, 1 + \u20ac) At)\n-\u03b2. KL (\u03c0 (y1, x) || \u03c0INIT (y|I, x))]", "rt($) = \\frac{\u03c0RL(y1,x)}{\u03c0OLD (y1,x)}"]}