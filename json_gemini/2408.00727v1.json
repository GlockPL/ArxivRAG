{"title": "Improving Retrieval-Augmented Generation in Medicine with Iterative Follow-up Questions", "authors": ["Guangzhi Xiong", "Qiao Jin", "Xiao Wang", "Minjia Zhang", "Zhiyong Lu", "Aidong Zhang"], "abstract": "The emergent abilities of large language models (LLMs) have demonstrated great potential in solving medical questions. They can possess considerable medical knowledge, but may still hallucinate and are inflexible in the knowledge updates. While Retrieval-Augmented Generation (RAG) has been proposed to enhance the medical question-answering capabilities of LLMs with external knowledge bases, it may still fail in complex cases where multiple rounds of information-seeking are required. To address such an issue, we propose iterative RAG for medicine (i-MedRAG), where LLMs can iteratively ask follow-up queries based on previous information-seeking attempts. In each iteration of i-MedRAG, the follow-up queries will be answered by a vanilla RAG system and they will be further used to guide the query generation in the next iteration. Our experiments show the improved performance of various LLMs brought by i-MedRAG compared with vanilla RAG on complex questions from clinical vignettes in the United States Medical Licensing Examination (USMLE), as well as various knowledge tests in the Massive Multitask Language Understanding (MMLU) dataset. Notably, our zero-shot i-MedRAG outperforms all existing prompt engineering and fine-tuning methods on GPT-3.5, achieving an accuracy of 69.68% on the MedQA dataset. In addition, we characterize the scaling properties of i-MedRAG with different iterations of follow-up queries and different numbers of queries per iteration. Our case studies show that i-MedRAG can flexibly ask follow-up queries to form reasoning chains, providing an in-depth analysis of medical questions. To the best of our knowledge, this is the first-of-its-kind study on incorporating follow-up queries into medical RAG.", "sections": [{"title": "1. Introduction", "content": "Generative artificial intelligence (AI) technologies such as large language models (LLMS) have brought a wide variety of opportunities for biomedical applications. 1-4 For example, they have shown great potential for answering biomedical questions,5\u20139 summarizing medical documents, 10-12 and matching patients to clinical trials.13\u201316 However, LLMs often generate plausible-sounding but inaccurate content, an issue commonly known as \"hallucination\" in the literature.17 They also possess outdated knowledge obtained from a fixed set of training data.18 Retrieval-augmented generation (RAG) provides a lightweight post-training solution to these issues by providing LLMs with relevant documents retrieved from up-to-date and trustworthy sources.\nWhile there have been several medical applications of RAG, such as Almanac, 21 Clinfo.ai,22 and MedRAG, 23 their RAG component is mainly beneficial to questions that have direct answers in a single document, such as those in the PubMedQA24 and BioASQ25 datasets. However, only marginal improvements are seen with RAG for questions that require multiple rounds of clinical reasoning like MedQA,26 a dataset curated from medical license examinations. For example, to recommend a treatment for a patient with certain symptoms, a system needs to first infer the potential diagnosis from the symptoms and then find a suitable treatment for the diagnosis. Nevertheless, only one round of retrieval is conducted in the vanilla RAG architecture, prohibiting multiple rounds of information seeking that are required in complex clinical reasoning.\nIn this work, we propose i-MedRAG, a simple and effective framework for incorporating follow-up queries into RAG. Specifically, we prompt LLMs to iteratively generate follow-up queries to search for additional information from external medical corpora. The queries and the corresponding answers generated with RAG will be used to augment the answer generation of the original question. Empirical results demonstrate the effectiveness of i-MedRAG on both open- and close-source LLMs, which show improved performance on the United States Medical Licensing Examination (USMLE) subset of MedQA and medical questions from the Massive Multitask Language Understanding (MMLU) dataset. Notably, our zero-shot i-MedRAG outperforms all previous prompt engineering and fine-tuning methods, setting a state-of-the-art accuracy for GPT-3.5 on the MedQA dataset (Figure 1). Our further analysis of the number of iterations and number of queries per iteration used in i-MedRAG reflects how its performance scales with different settings. Additionally, we present several case studies of i-MedRAG, showing how it overcomes the limitations in vanilla RAG to find the correct answers.\nIn summary, our contributions are three-fold:\n\u2022 We introduce i-MedRAG, a novel RAG architecture that incorporates follow-up queries to solve complex reasoning tasks.\n\u2022 We have conducted comprehensive experiments on medical question answering, and the results demonstrate that i-MedRAG not only outperforms vanilla RAG approaches but also surpasses all other prompt engineering approaches on MedQA with GPT-3.5, setting a new state-of-the-art performance of 69.68%.\n\u2022 We also provide analyses to further characterize i-MedRAG, showing how its performance varies with the scaling of follow-up queries."}, {"title": "2. Related Work", "content": "2.1. Retrieval-Augmented Generation for Medicine\nRetrieval-augmented generation (RAG) has been widely adopted in medicine. Here, we discuss several representative approaches. Almanac21 is a system that augments LLMs with curated resources for medical guidelines and treatment recommendations, which shows improvements over the standard LLMs in six manually assessed metrics. Similarly, Low et al.27 demonstrate the improvements of RAG-based systems for real-world clinical queries with manual evaluation. Clinfo.ai22 is an open-source web application that answers clinical questions based on retrieved scientific literature from PubMed articles. Xiong et al.23 conduct a benchmarking study with the MedRAG toolkit, and show the benefits of RAG in several medical multi-choice question answering datasets. There are also various biomedical literature search products28 that use RAG to summarize the retrieved articles,29 such as OpenEvidencea and ChatRWDb.\nHowever, most of the RAG studies in medicine use the vanilla architecture with only one round of retrieval. To the best of our knowledge, our study presents the first approach and evaluations on incorporating follow-up queries in RAG for medicine."}, {"title": "2.2. Medical Question Answering", "content": "Question answering tasks such as MedQA, 26 PubMedQA, 24 MedMCQA,30 BioASQ, 25 and Massive Multitask Language Understanding (MMLU)31 are commonly used to benchmark the medical knowledge and reasoning capabilities of LLMs.32 Most of these datasets focus on single-hop questions such as \u201cwhat is the most common symptom of hypertension?\", while only MedQA questions are longer patient vignettes where both medical knowledge and multi-step reasoning are required. As such, there have been many studies working on improving the GPT-3.5 performance on MedQA with prompt engineering. Figure 1 shows the comparison among different representative prompt engineering approaches on MedQA, including chain-of-thought (CoT) prompting, 33 self-consistency (SC) prompting, 34 multi-agent communication with MedAgents,35 and RAG-based approaches such as Knowledge Solver (KSL),36 LLMs Augmented with Medical Textbooks (LLM-AMT),37 and MedRAG.23 Much fewer studies focus on prompt engineering with GPT-4 on MedQA, 7,38 probably because the raw GPT-4 error rate39 is close to the noise rate in MedQA annotations.40 In this study, we focus on the zero-shot setting as it reflects realistic clinical scenarios. While not requiring any instances for training or few-shot learning, our approach surpasses all previous methods with GPT-3.5 on the MedQA dataset.\""}, {"title": "3. Methods", "content": "Figure 2 shows the overview of our i-MedRAG and its comparison to the vanilla Retrieval-Augmented Generation (RAG). Different from RAG, our i-MedRAG modifies its pipeline by replacing the information retrieval step (Figure 2 left) with our proposed iterative question-answering step (Figure 2 middle and right). The settings of RAG are described in Section 3.1 and the pipeline of our new i-MedRAG is discussed in Section 3.2. The details of the iterative question answering are described in Section 3.3."}, {"title": "3.1. Retrieval-Augmented Generation", "content": "In the zero-shot setting of medical question answering, the task of LLM M is trying to find the correct answer A given the question Q only. The ideal answer prediction \u00c3 can be provided by\n$\\tilde{A} = \\mathop{\\arg\\,\text{max}}_{A} P_M(A\\,|\\, Q, \\text{inst.}),$\nwhere the \u201cinst.\u201d is the task instruction the user provides that instructs the model to perform the task. As medical questions are knowledge-intensive,32 it benefits from accessing large-scale external corpora to search for useful information.21\u201323 A typical method to combine LLM reasoning with external corpora is RAG, which first retrieves relevant documents from the corpus for the given medical question and enters the retrieved documents along with the question into LLM to augment its answer generation. Formally, the RAG pipeline can be described as\n$\\tilde{A} = \\text{RAG}(Q;\\, M, R, D) = \\mathop{\\arg\\,\text{max}}_{A} P_M(A\\,|\\, Q, \\text{inst.}, \\{d_i\\}_{i=1}^N),$\nwhere $\\{d_i\\}_{i=1}^N$ are the question-specific retrieved documents given by\n$\\{d_i\\}_{i=1}^N = R(Q;\\, D).$\nHere R is the text retriever and D is the corpus with a collection of documents."}, {"title": "3.2. Iterative Retrieval-Augmented Generation", "content": "While RAG exhibits promising performance in medical question answering,23 it may be unable to handle certain complex medical questions in real-world cases. As text retrievers are typically trained to find relevant documents based on text similarity or lexicon overlap, they cannot break down a complex question and search for relevant information in a step-by-step manner. Thus, the inflexible retrieval step (Formula 3) in RAG may fail to analyze medical questions and find useful information to augment the answer generation, especially in complex clinical cases, where multiple rounds of information-seeking are required.\nTo address the issues mentioned, we propose to incorporate flexible information retrieval by prompting LLMs to iteratively generate follow-up queries based on the given medical question and previous information-seeking history. Moreover, as the context lengths of LLMs are limited, it can be impractical and infeasible to include all retrieved documents in the LLM context. Therefore, we prompt LLMs to directly answer the raised queries with relevant information and use such query-answer pairs as the information-seeking history. The pipeline of our proposed system can be formulated as\n$\\tilde{A} = \\text{i-MedRAG}(Q;\\, M, R, D) = \\mathop{\\arg\\,\text{max}}_{A} P_M(A\\,|\\, Q, \\text{inst.}, \\{(q_i, a_i)\\}_{i=1}^N),$\nwhere $\\{(q_i, a_i)\\}_{i=1}^N$ are the queries and the corresponding answers generated by LLMs with the help of RAG. The iterative process of query and answer generation will be detailed in Section 3.3."}, {"title": "3.3. Iterative Generation of Follow-up Questions", "content": "While the retrieved documents in RAG are determined by the question and the retrieval system, we propose to incorporate the reasoning capabilities of LLMs in i-MedRAG by prompting them to dynamically generate helpful queries in a step-by-step manner. Specifically, the LLM will be encouraged to generate n different queries to help find useful additional information for m iterations. In all iterations except for the first one, the model will be given the information-seeking history to generate context-specific follow-up queries. The queries $q_{i1},\uff65\uff65\uff65, q_{in}$ generated in the i-th iteration can be formulated as\n$q_{i1}, ..., q_{in} = \\begin{cases} \\mathop{\\arg\\,\text{max}}_{q_{i1},...,q_{in}} P_M (q_{i1}, \u2026\u2026\u2026, q_{in} | Q, \\text{inst.}'), & \\text{if } i = 1, \\\\ \\mathop{\\arg\\,\text{max}}_{q_{i1},...,q_{in}} P_M(q_{i1},\u2026, q_{in} | Q, \\text{inst}.', \\{(q_{jk}, a_{jk})\\}_{j=1,...,n}^{k=1,...,i-1}), & \\text{if } i > 1. \\end{cases}$\nDifferent from the \u201cinst.\u201d in Formula 2, the \u201cinst.\u201d here is a modified instruction which focuses on generating follow-up queries instead of answering the medical question. For each query generation step, we prompt the LLM to analyze the existing information first and then generate new queries for additional knowledge. The step-by-step \u201creason-then-query\" pipeline helps LLMs break down complex medical questions and find useful information from the external corpus. The answer to each generated query is given by a RAG system mentioned in Formula 2. This enables the system to leverage existing literature to provide grounded answers for generated queries.\nThe overall algorithm of i-MedRAG is presented in Algorithm 1."}, {"title": "4. Experiments", "content": "4.1. Evaluation settings\nTo evaluate the performance of our proposed i-MedRAG on knowledge-intensive medical question-answering tasks and compare it with other approaches, we select MedQA26 as the testbed, which contains medical questions collected from United States Medical Licensing Examination (USMLE). With complex clinical cases in the dataset, MedQA reflects the difficulty of decision-making in real-world clinical medicine. The approaches for comparison are prompt engineering or fine-tuning methods that try to improve the performance of GPT-3.5 on MedQA, including chain-of-thought (CoT) prompting, 39 self consistency (SC), knowledge solver (KSL),36 medical agents (MedAgents), 35 LLMs augmented with medical textbooks (LLM-AMT),37 medical retrieval-augmented generation (MedRAG),23 and LLMs with test-time adaptations (MedAdapter).41\nAdditionally, we evaluate the generalizability of our i-MedRAG with more LLMs and medical datasets. Llama-3.1-8B is selected as the representative of open-source models, which has a context window of 128k tokens. We also include MMLU-Med, a set of six medical tasks (anatomy, clinical knowledge, professional medicine, human genetics, college medicine, college biology) from Massive Multitask Language Understanding (MMLU), following previous studies. 8,23 MMLU-Med serves as a testbed to show the performance of i-MedRAG on a variety of different medical tasks.\nBoth MedQA and MMLU-Med are composed of multi-choice questions, whose evaluation metric is the accuracy of predicted answers chosen from given options. For the retrieval part in i-MedRAG, we select the Textbooks26 and Statpearls corpora introduced in MedRAG, 23 which are shown effective on medical examination questions. MedCPT42 is chosen as the text retriever, which has been trained on domain-specific literature. For other baselines compared, the official settings described in their papers are used.\n4.2. Main results\nTable 1 shows the comparison results of i-MedRAG and other baseline approaches on MedQA using GPT-3.5. Official scores reported in previous research are used for a fair comparison. While methods with few-shot learning or model fine-tuning tend to perform better than the ones in a zero-shot setting, our i-MedRAG set a state-of-the-art performance of GPT-3.5 on MedQA without any training samples or parameter tuning. Among zero-shot approaches, i-MedRAG (69.68%) has a +4.61% performance improvement compared to the previous best record achieved by MedRAG (66.61%).\nThe results of generalizing i-MedRAG to more LLMs and data are presented in Table 2. We compare i-MedRAG with our implemented CoT and MedRAG to see if i-MedRAG can bring a consistent improvement of LLM performance in medical question answering. Similar to the results on GPT-3.5, the open-source Llama-3.1-8B also shows improved performance on MedQA with the help of i-MedRAG. While Llama-3.1-8B shows a close performance to GPT-3.5 in CoT and MedRAG settings, its performance is significantly improved with i-MedRAG,"}, {"title": "4.3. Scaling with iterations and queries", "content": "As we described in Section 3.3, the number of iterations to ask follow-up queries and the number of queries generated in each iteration are the two critical hyperparameters in our proposed iterative generation of follow-up queries. To explore how different selections of the hyperparameter values affect the model performance, we run i-MedRAG with different settings and compare their results. We test both GPT-3.5 and Llama-3.1-8B on MedQA and MMLU-Med to examine if there are model-specific or task-specific patterns.\nFigure 3 shows the model performance with different hyperparameter settings. Generally, MedQA and MMLU-Med show distinct patterns in performance change with the increasing number of iterations. While the performance of both GPT-3.5 and Llama-3.1-8B on MedQA tends to improve with more iterations of follow-up queries, their performance on MMLU-Med converges or starts to drop with just one or two iterations, corresponding to the different complexities of these two tasks.\nFrom the results on MedQA, it is also empirically shown that the number of generated queries per iteration determines the rate of performance improvement and convergence over multiple iterations. LLMs with more queries generated per iteration tend to have a larger improvement in accuracy but also converge more quickly. Such a result is intuitively reasonable as more information can be collected each iteration with more generated queries."}, {"title": "5. Discussion", "content": "Overall, our proposed i-MedRAG effectively improves the performance of LLMs on complex medical questions by prompting them to iteratively ask follow-up queries. The experimental results show that our approach is better than previously proposed prompt engineering and fine-tuning methods, and is generalizable to various LLMs and medical question-answering datasets. Nevertheless, our approach has certain limitations which need to be discussed. It is also worthwhile to discuss the future work of this study to analyze how it can be further improved to facilitate real-world medical assistance.\n5.1. Limitations\nThe first limitation of i-MedRAG is its high cost. While generating more follow-up queries tends to provide LLMs with more comprehensive and focused information about the given medical question, the cost also grows linearly with the number of queries generated. The time cost can be further increased if more documents are used to help answer the generated queries with RAG. While the cost is comparable to approaches using multiple LLM agents35 or self consistency43 which also prompt LLMs multiple times for each question, it is much more costly than baseline prompting methods such as CoT.33\nAnother limitation of this work is the selection of hyperparameter values for optimal performance. As shown in Figure 3, different LLMs can have different hyperparameter settings for their optimal performance. Even for the same LLM, its optimal hyperparameters can vary based on the medical questions being processed. Thus, it is non-trivial to find the optimal hyperparameters of i-MedRAG for a new medical task, which may be inefficient for real-world deployments.\n5.2. Future work\nGiven the limitations of i-MedRAG, we consider several potential future directions that could further improve the performance of retrieval-augmented generation for medicine. The first direction is the automation of hyperparameter selection in i-MedRAG. To reduce the laborious process of hyperparameter selection, one may use an LLM agent to dynamically determine how many follow-up queries should be asked each iteration. This can improve the efficiency and flexibility of the hyperparameter selection process. Another future direction is to improve the performance of i-MedRAG with few-shot demonstrations. While few-shot CoT prompting is demonstrated to perform better than the zero-shot counterpart, 39 it is not easy to adapt such strategies to i-MedRAG as the reasoning process can be dynamically affected by the use of external corpora and retrievers. Investigating how i-MedRAG can benefit from one or few-shot samples could be a potential direction to further enhance its performance on medical question answering."}]}