{"title": "Smart Multi-Modal Search: Contextual Sparse and Dense Embedding Integration in Adobe Express", "authors": ["Cherag Arora", "Tracy Holloway King", "Jayant Kumar", "Yi Lu", "Sanat Sharma", "Arvind Srikantan", "David Uvalle", "Josep Valls-Vargas", "Harsha Vardhan"], "abstract": "As user content and queries become increasingly multi-modal, the need for effective multi-modal search systems\nhas grown. Traditional search systems often rely on textual and metadata annotations for indexed images,\nwhile multi-modal embeddings like CLIP enable direct search using text and image embeddings. However,\nembedding-based approaches face challenges in integrating contextual features such as user locale and recency.\nBuilding a scalable multi-modal search system requires fine-tuning several components. This paper presents a\nmulti-modal search architecture and a series of AB tests that optimize embeddings and multi-modal technologies\nin Adobe Express template search. We address considerations such as embedding model selection, the roles of\nembeddings in matching and ranking, and the balance between dense and sparse embeddings. Our iterative\napproach demonstrates how utilizing sparse, dense, and contextual features enhances short and long query search,\nsignificantly reduces null rates (over 70%), and increases click-through rates (CTR). Our findings provide insights\ninto developing robust multi-modal search systems, thereby enhancing relevance for complex queries.", "sections": [{"title": "1. Introduction", "content": "For search over images and multi-modal content, industry search systems traditionally rely on textual\nand metadata annotations added to indexed images. However, multi-modal embeddings like CLIP [1]\nenable direct search of image content using text and image embeddings, allowing for direct text-to-image\nand image-to-image search. While pure embedding-based approaches facilitate content understanding,\nthey struggle with integrating contextual features like user locale and recency into retrieved results.\nBuilding a production-grade, scalable multi-modal search system involves carefully tuning several\ncomponents. This paper describes a series of AB tests conducted to leverage embeddings and other\nmulti-modal technologies in search for Adobe Express templates. These templates are complex multi-\nmodal (and multi-page) documents, containing images, text and rich metadata (section 2).\nTo improve text search for templates, integrating embeddings required decisions as to:\n\u2022\nWhich embedding model(s) to use\n\u2022\nWhether to leverage embeddings for matching (recall), ranking, or reranking\n\u2022 Whether to use dense or sparse embeddings\n\u2022 Whether head and tail queries should be treated identically\n\u2022 Whether embeddings should be used for null and low recovery or everywhere\nOther than which embeddings to use, these decisions were driven by latency concerns and by constraints\non integration with Elasticsearch, which was the existing inverted index used for Express template\nsearch. With an ever-increasing collection of ~300,000 templates, dense embeddings could not be used\nfor matching due to the number of scoring calculations, which leads to high latency. This restricted\ndense embeddings to (re)ranking, where only a small (<10K) number of top templates had to be scored,\nand to scenarios like null and low recovery and long tail queries, where the additional latency was\nworth the improved relevance. In addition, certain types of queries performed better with keyword\nsearch, especially those around design type (e.g. poster, Instagram reel) and format (e.g. still, animated,\nvideo).\nTo determine the optimal combination, we took an iterative approach with a series of evaluations and\nAB tests. We started with existing models with single integrations and then built on these to improve\nremaining relevance issues. This paper first overviews the data and models used (section 2) and then\ndiscusses the experiments and how the decisions were made for each of these (section 3)."}, {"title": "2. Models and Data", "content": "This section describes key data and models used for the Express template recall and ranking. The\ntemplates themselves contain rich image, text, and metadata. The standard search behavioral data (e.g.\nimpressions, clicks) are available, as well as certain application-specific behavioral data (e.g. number of\nedits, number of exports). These are briefly described in section 2.1. In addition, we have two types of\nmulti-modal models: two CLIP text-image models (section 2.2) and an intent-based model (section 2.3)."}, {"title": "2.1. Template Data", "content": "Express templates are rich objects which contain many visual layers and text boxes. These can also\nbe viewed as images, e.g. those that are displayed in search. In addition, templates have\ntitles provided by the template designers as well as filter information such as design type, style, mood,\nregion, and price (free/premium). Additional information is inferred about each template including\nmulti-modal embeddings, user intents, and image tags. Finally, aggregated behavioral data such as\nimpressions, clicks, edits (number of edits users make to the template in order to personalize them) and\nexports (number of times the template is exported after editing) are available."}, {"title": "2.2. Image-Text CLIP Embeddings", "content": "CLIP [1] embeds images and text in the same space. This allows for embedding-based search of images\nusing text queries. There are several off-the-shelf CLIP models available. However, for Express template\nsearch and for other visual asset search like Adobe Stock, we needed a model that: (1) worked on\nshort text (queries) as well as long text (captions); (2) covered five languages (English, French, German,\nJapanese, Korean); (3) performed well on high-quality image data for templates, photographs and\nillustrations; (4) had a sparse version as well as the dense vectors. To meet these requirements, we\ntrained a CLIP-architecture model on Adobe-licensed image-text data. The text model was particularly\nimportant since the training focused on Adobe vocabulary, shorter text, and multiple languages; the\ntraining architecture was inspired by [2].\nThere are many ways to improve the latency when using embeddings with large numbers of assets.\nHowever, the approximate methods reduce accuracy because the list of assets whose embeddings are\nclosest to the query embedding is not exact. Once a smaller set of embeddings is selected (e.g. by using\nthe top n embeddings from the approximate scoring), then the dense embedding can be used to get\nmore accurate scores for a final ranking. We used a sparsification method which allows the embeddings\nto be used similar to keywords in the existing index. An example of this is shown in Table 1."}, {"title": "2.3. Multi-Modal Creative Knowledge Graph", "content": "In addition to learning representations of the content via AdobeCLIP, we found mapping the content's\nintent to discrete nodes improved recall and explainability and allowed for downstream-recommendation\ntasks, similar to [6]. However, we discovered that self-supervised models like AdobeCLIP, which were\ntrained on asset-caption and asset-query data like Adobe Stock and Adobe Express failed to accurately\nmap the asset's intent to short discrete labels. To accomplish, this we created a \u201cCreative\u201d Knowledge\nGraph (CKG) [7, 8, 9] containing over 100K nodes focusing on Adobe-specific user intents. We then\ntrained a multi-modal transformer (MM-CKG) specializing in mapping assets to these discrete nodes\nusing supervised contrastive training. We mined concepts for events, actions, objects, moods, canvas\ntypes, colors, and backgrounds to get a robust understanding of an asset's content. For example, actions\nhas subtypes of run, dance, . . .; events has subtypes of birthday, graduation, wedding, seasonal, . . .; in\nturn events|seasonal has subtypes of Halloween, Thanksgiving, 4th of July, . . ..\nTo train the model, we created sequence-wise self-attention blocks inspired by [10]. We built our\nmodel on top of a base CLIP backbone and then added a sequence-wise attention block on top that\ntakes in the hidden states from the last layer of the CLIP backbone that runs through a couple layers of\nmulti-headed transformer blocks. We utilized the \\(T_{cls}\\) and \\(I_{cls}\\) outputs from the sequence-wise attention\nheads as the final representation of the input image and text modalities."}, {"title": "2.3.1. Supervised Contrastive Loss (SupCoLA)", "content": "We devised our loss function with the following requirements:\n1. Alignment to labels: Ensure that the image and text in the training process were close to the label\nembeddings.\n2. Ability to handle multiple positives in a batch: Traditional contrastive learning (InfoNCE loss [11])\nassumes that for a given pair in a batch, all other pairs are negatives. However, when learning\nalignment with labels, multiple rows with the same label may be present in a batch. The loss\nfunction should not penalize these rows during loss computation.\n3. Ability to have multiple labels per row: Some rows have multiple labels. For example, for the\nprompt, boy is sitting on a beach with his dad for father's day, there are multiple concepts: the\ncreative intent father's day, the scene objects boy and beach, and the background beach background.\nOur resulting loss function, Label-Aligned Supervised Contrastive Loss, is based on SupCon loss [12]\nwhere we pass image, text and label embeddings as anchor features as well as contrast features.\n\\(C_{sup} = \\sum_{i \\in I} C_{sup}^{i} = \\sum_{i \\in I} ( - \\frac{1}{P(i)} \\sum_{p \\in P(i)} \\sum_{v \\in j(p)} log(\\frac{exp(Z_i Z_v / \\tau)}{\\sum_{n \\in A(i)} exp(Z_i Z_n / \\tau)}) )\\) (1)\nwhere I is the mini batch, i is the index of anchor sample in the batch, \\(A(i) = I \\backslash i\\) is the set of all\nsamples n in the batch that have distinct index than the anchor i, j(p) is the set of all positives p in the\nbatch that have the same label as anchor i and are views of p. Views of sample p denote the embeddings\nfor the label, image and text modalities.\nWhy do we have two domain-specific multi-modal embeddings (AdobeCLIP and MM-CKG)? These\ntarget and excel at different use cases, both of which are important for template search relevance.\nMM-CKG is better at determining the underlying key intent from a query and at specific scene object\ndetection. AdobeCLIP is better at color and layout understanding."}, {"title": "3. Iterative Experiments", "content": "This section describes the series of on-line experiments conducted to improve the Express template\nmulti-modal search. We focus primarily on experiments involving multi-modal embeddings, but include\none experiment that leveraged multi-modal content under the hood but text at query time.\nThe Express template search uses a standard architecture for relevance (Figure 4). It is built on\nElasticsearch. There is an initial matching (recall) step to retrieve documents which broadly match\nthe user's query. This step uses keyword-style matching against text and metadata and includes an\ninitial low latency scoring. Matching using sparse AdobeCLIP embeddings for all queries (section 3.4)\nand dense multi-modal CKG embeddings for long queries (section 3.5) were also added. If not enough\nresults are found, null and low recovery occurs, including a speller (not discussed in this paper; see [13])\nand the use of symbolic CKG intents (section 3.2). The top 10K templates from the initial match set are\nthen reranked using a much broader set of features. This includes dense multi-modal embeddings as\nwell as the usual discrete features such as BM25, locale, language, and aggregated behavioral data."}, {"title": "3.1. Reranking with External Image-Text Model", "content": "Our initial experiment used an external, English-only CLIP model in the rescore stage of the ranker.\nTo do this, we had to determine how many items to rescore. This was largely governed by latency\nconcerns since we wanted as many items as possible to use the CLIP multi-modal signal. By running\nload tests, we determined that we could use the CLIP scores for the top 10K templates, where the initial\nranking was determined by the existing ranker.\nWe also had to determine how to weight the CLIP scores in the rescoring. Since the template ranker\nat this time was a non-ML, hand-tuned ranker, we determined this based on evaluations of a stratified\nquery sample. The extreme baseline was to use only the CLIP score for the reranking. This had two\ndrawbacks: (1) The top results were not visually diverse enough, especially for broad queries like\nbirthday card or wedding invitation; (2) there was not enough recent content to provide a sense of\nfreshness and seasonality. To determine a suitable weight, we used a divide-and-conquer approach by\nstarting with a 50% balance between the first round ranker score and the CLIP score and then adjusting.\nThis quickly converged on a weighting of basically 2/3 for CLIP and 1/3 for the first round ranker score.\nIn AB testing, the click-through rate (CTR) and export rate improved with the CLIP-based reranking\n(Table 2). There was no change in the null rate, as was to be expected."}, {"title": "3.2. Null and Low Recovery with Symbolic Multi-Modal Intents", "content": "Due to the broad range of user intents, the limited template collection, and the keyword based retrieval,\nusers frequently landed on null and low result pages. When the number of results are low (<5 results),\nthe search engagement with the results drops significantly (up to 2-3x). To reduce the null and low\nresult rate, we incorporated recovery mechanism using the symbolic CKG intents. The CKG intents for\neach template were indexed and the CKG intents for the query were calculated at query time. If there\nwere <5 results, the CKG query intents were matched against the template intents. For example, the\nquery hot yoga studio opening has the intents yoga and would match all templates with that intent. This\nresulted in major improvements in CTR and null rate (table 3)."}, {"title": "3.3. Ranking with Domain-specific Image-Text Model", "content": "The CLIP model (section 3.1) only worked for English and was not optimized for Express templates\nand queries. Replacing CLIP with AdobeCLIP to rerank the top 10K Express templates was expected\nto be on par for English queries and improve the CTR for non-English queries. Because the recall\nand first-round ranker constrained the result set, the core relevance, especially for head queries, was\nunlikely to change significantly, although the torso and tail queries, especially in non-English were\nexpected be significantly different. The move from CLIP to AdobeCLIP was part of a larger AB test\nwhich moved from an older search infrastructure to a newer one which, among other things, allowed\nfor multiple embedding types. The goal of the AB test was to have no negative effects while moving to\nthe new platform. This was borne out (table 4)."}, {"title": "3.4. Recall with Sparse Image-Text Model", "content": "None of the previous experiments leveraged the power of embeddings for augmenting the initial match\nset. The CLIP and AdobeCLIP models only affected the reranking of the search results. The null\nand low recovery with symbolic CKG multi-modal intents only affected null and low queries and\nleveraged symbolic intents. Dense embeddings could not be used for the initial match set due to latency\nconstraints. So, we experimented with using the AdobeCLIP sparse embeddings in the match set to\naugment the existing keyword matches. This required determining how many dimensions to match in\nthe sparse embedding (section 2.2) in order to retrieve enough new relevant documents and not too\nmany irrelevant ones. As is well known in the literature [14, 15] determining accurate thresholds on\nembeddings using cosine similarity or dot product is not feasible. The sparse embedding approach\nallowed us to require a minimum number of asset dimension matches. Once the retrieval approach\nwas determined, the ranking was updated to demote less relevant templates retrieved by the sparse\nembeddings (table 5)."}, {"title": "3.5. Long Query Recall and Ranking with Multi-Modal Model", "content": "The above experiments improved relevancy for head queries, both in matching the intent of the user\nquery and in quality of the templates shown. In addition, the improved recall from CKG symbolic\nintents for null and low recovery (section 3.2) and the addition of sparse AdobeCLIP embeddings into\nthe initial match set (section 3.4) resulted in a broad set of related templates being shown when there are\nfew exact matches. However, there are often few exact match templates for more specific user queries,\ni.e. for tail queries.\nTo address this issue, we targeted longer (>=4 words) to use the CKG multi-modal embedding\n(MM-CKG section 2.3). The more specific intents of the longer queries work especially well with the\ndomain-specific embeddings, allowing the recall and ranking to find the few templates that exactly\nmatch the user intent. The ranking combined 1/3 the weight on MM-CKG and 2/3 the weight on\nAdobeCLIP. The hypothesis behind this was that AdobeCLIP captures the core relevance matching the\nquery text to the image rendition, while MM-CKG captures the underlying intent of the query and\nthe template. The optimal query length for this experience was determined empirically by manually\njudging a stratified sample of queries of different lengths, comparing production to the new experience.\nThere was a clear demarcation between queries of <4 words and those of >=4 words. Table 6 shows\nthat for <4 words, both production and MM-CKG largely provide relevant results, i.e. for head queries\nboth approaches work well. However, for >=4 words the new MM-CKG results are significantly better\nthan those in production.\nThe AB test launched showcased statistically significant improvements of CTR and null rate on long\nqueries and prompts, highlighting the usefulness of the hybrid system."}, {"title": "4. Conclusion", "content": "Multi-modal search experiences in industry applications traditionally depend on textual data in the\nindex, thereby reducing the multi-modal search to a traditional keyword search. This provides a low\nlatency experience since industry search engines are heavily optimized for keyword search. The advent\nof high quality multi-modal embeddings like CLIP has provided radically new capabilities. However, in\nan existing application, such as Adobe Express template search in this paper, the available multi-modal\ncapabilities and the existing infrastructure including strict latency requirements, require a thoughtful,\niterative approach to integrating new multi-modal technologies. This paper described five multi-modal\nexperiments in Express template search, each of which built upon the others. This has resulted in\nsignificantly lower null and low rates, while improving click-through rates."}]}