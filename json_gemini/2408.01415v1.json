{"title": "Conditional LoRA Parameter Generation", "authors": ["Xiaolong Jin", "Kai Wang", "Dongwen Tang", "Wangbo Zhao", "Yukun Zhou", "Junshu Tang", "Yang You"], "abstract": "Generative models have achieved remarkable success in image, video, and text domains. Inspired by this, researchers have explored utilizing generative models to generate neural network parameters. However, these efforts have been limited by the parameter size and the practicality of generating high-performance parameters. In this paper, we propose COND P-DIFF, a novel approach that demonstrates the feasibility of controllable high-performance parameter generation, particularly for LORA (Low-Rank Adaptation) weights, during the fine-tuning process. Specifically, we employ an autoencoder to extract efficient latent representations for parameters. We then train a conditional latent diffusion model to synthesize high-performing model parameters from random noise based on specific task conditions. Experimental results in both computer vision and natural language processing domains consistently demonstrate that COND P-DIFF can generate high-performance parameters conditioned on the given task. Moreover, we observe that the parameter distribution generated by COND P-DIFF exhibits differences compared to the distribution obtained through normal optimization methods, indicating a certain level of generalization capability. Our work paves the way for further exploration of condition-driven parameter generation, offering a promising direction for task-specific adaptation of neural networks.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in generative models [41, 39, 44, 2] have marked substantial progress across several domains of artificial intelligence. In the computer vision domain, generative adversarial networks [12], diffusion models [16], and other approaches [6, 40] have shown impressive results in image synthesis and manipulation. Notably, models such as Stable Diffusion [41], DALL-E 2 [39], and Imagen [44] have set new benchmarks in the quality and resolution of generated images. Moreover, video generation models like Sora [32] have shown promising results in producing coherent and high-quality video sequences, opening new avenues for applications in entertainment and media. In the natural language processing domain [37, 22, 55], autoregressive models like GPT [2] and Llama [51] have demonstrated promising generation capabilities and alignment with human preference [20, 33, 38, 21], which underscore the potential of generative models.\nInspired by these achievements, recent studies [34, 54] have begun to explore the application of generative models in novel areas, generating high-performing model parameters. These studies focus on directly generating novel model parameters to accelerate the training process, uncovering parameters that achieve comparable performance with those obtained through conventional optimization methods."}, {"title": "2 Preliminary", "content": ""}, {"title": "2.1 Preliminaries of LoRA", "content": "Low-Rank Adaptation (LoRA) [17] enhances the efficiency of fine-tuning large pre-trained language models by minimizing the computational demands usually required for full model retraining. LoRA introduces two trainable matrices, B \u2208 Rdxr and A \u2208 Rr\u00d7k, to each transformer layer. These matrices, where r is much smaller than hidden layer dimension d and task-specific dimension k, perform a low-rank approximation of the typical updates made during fine-tuning. The core idea is that the necessary adjustments for task-specific adaptation have a low \"intrinsic dimension,\" allowing significant reductions in trainable parameters while maintaining performance. The pretrained weight matrix Wo remains unchanged, with only B and A being optimized, thus speeding up training and decreasing memory and computational needs. The modified forward pass in LoRA is represented as:\nWox + AWx = Wox + B(Ax) (1)\nwhere AW = BA is the update. Initially, B is zero, ensuring no changes to Wo, and A starts with a small random Gaussian distribution. In deployment, the learned low-rank matrices B and A can be integrated into Wo. In this work, we aim to synthesize LoRA parameters because of the practicality and effective LoRA fusion that show the continuous distribution in LoRA parameter space."}, {"title": "2.2 Preliminaries of Conditional Diffusion Models", "content": "Conditional diffusion models [16, 41, 59] extend the standard diffusion model by incorporating conditions into both the forward and reverse processes. This conditional information defined by c allows the model to generate data tailored to specific attributes or requirements.\nConditional forward process: The forward process in conditional models involves adding noise to an initial sample while conditioning on c. The probability of transitioning from Xt\u22121 to xt under condition c is modeled as a Gaussian distribution:\nq(xt|xt\u22121, c) = N(xt; \u221a1 \u2013 \u1e9etxt\u22121, \u1e9etI) (2)\nwhere Bt are the timestep-dependent noise levels, and I represents the identity matrix. The complete forward process conditioned on c is given by:\nq(x1:Txo, c) = \\prod_{t=1}^{T} q(xt|xt\u22121, C) (3)\nConditional Reverse Process: The reverse process aims to reconstruct the original sample from its noisiest state xr conditioned on c. It is formulated by:\n\u0420\u04e9(Xt-1|Xt, c) = N(xt\u22121; \u03bc\u04e9(xt, t, c), \u03a3\u00f8(xt, t, c)) (4)\nIn this process, \u03bc\u03b5 and \u2211e are functions estimated by a neural network, which also processes the condition c, ensuring that the recovery of data respects the conditional constraints.\nOptimization and Inference with Conditions: The training procedure involves minimizing the Kullback-Leibler(KL) divergence between the forward and reverse conditional distributions, specifically:\nLdm = Eq(xo,c) [DKL(q(xt\u22121|Xt, Xo, C) ||P\u04e9 (Xt\u22121|Xt,c))] (5)\nDuring inference, the model generates new samples by conditioning on c and sequentially applying the learned reverse transitions from a noise distribution, enabling the generation of data that closely adheres to the specified conditions."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Overview", "content": "We propose conditional parameter generation to synthesize new parameters tailored to specific task conditions. Fig 2 illustrates our proposed COND P-DIFF framework. First, given a training dataset"}, {"title": "3.2 Parameter autoencoder", "content": "Dataset preparation. In this work, we focus on synthesizing LoRA learnable matrix parameters of fine-tuned models by default. To obtain the training dataset for the parameter autoencoder, we fine-tune the pre-trained model using LoRA on the dataset for task q and collect N different checkpoints in the last N steps. We denote the training dataset as \u0398 = [01,..., \u03b8\u03b7,..., \u03b8\u03bd], where \u03b8k represents the weights of LoRA for the model at a specific fine-tuning stage. Because the training dataset for COND P-DIFF contains model parameters rather than conventional image or language datasets, we propose task normalization. Specifically, we employ Z-Score normalization on the parameters of each task individually [18].\nTraining procedure. Given a training sample on, we flatten parameter matrix on to a one-dimensional vector wn \u2208 RK\u00d71, which K is the total number of parameter weights of wn. Then, we utilize an auto-encoder to obtain meaningful and robust latent representations. Specifically, we formulate the process as Equation 6, where & and D represent the encoder and decoder functions, respectively. Zn is the latent representation of the parameter matrix. Wn is the reconstruction of parameter wn. To enhance the generalization and robustness of the autoencoder, we introduce Gaussian noise \u00a7z to the latent vector. The final auto-encoder process is formulated as follows:\nZn = E(wn) = Encoder(wn) (6a)\n\nWn = D(zn) = Decoder(Zn + z) (6b)\nWe train the autoencoder function by minimizing loss function below.\nL=\\frac{1}{N} \\sum_{n=1}^{N}|| Wn \u2013 wn||2 (7)"}, {"title": "3.3 Conditional parameter generation", "content": "We utilize a conditional latent diffusion model to synthesize high-performance parameters based on conditions y such as text and image. To handle different tasks and modalities, we adopt the domain-specific encoder, which is denoted as Tdomain (y; p), where y represents the input condition and p denotes the encoder parameters. For example, in the NLP experiments of this work, we employ the text decoder in CLIP[36]. Inspired by in-context learning, the input condition y consists of a task description and two-shot examples to capture the task information. Besides, we utilize stylized images as conditions in style transfer tasks and adopt ResNet [14] to extract style latent representations as the condition vector. More details about the condition are shown in Appendix 6.1. Regarding the U-Net architecture, we apply one-dimensional convolutions in denoising autoencoders because the weight matrix parameters do not show strong positional relationships different from images where pixels have two-dimensional spatial relationships.\nTherefore, given the condition and training parameters samples, we train the conditional latent diffusion model through\nLLDM := E\u20ac~N(0,1),t [||\u20ac - 60 (Pt, t, Tdomain,p(y))||2] (8)\nwhere e is learned via Eq. 8. Finally, after conditional diffusion model training, we feed specific conditions corresponding to tasks and random noise to reverse the inference process to obtain high-performing weights for specific tasks."}, {"title": "4 Experiment", "content": "In this section, we first show the experiment setup. Then, we present the evaluation results, ablation studies, and analysis of COND P-DIFF."}, {"title": "4.1 Experiment setup", "content": "Datasets and metrics. We evaluate our method across various domains. Specifically, in NLP experiments, we test on the language understanding GLUE benchmark [53]. In CV experiments, we focus on the style-transfer tasks. We use the SemArt and WikiArt datasets [10, 45], which contain diverse artistic images, and evaluate them using the Fr\u00e9chet Inception Distance (FID, [15], as employed by StyleGAN [23], with lower scores indicating better performance.\nDataset collecting and training procedures. In NLP experiments, we collect 150 training samples for models, including BERT, Roberta, GPT-2 by default. For instance, in the case of BERT, we fixed pre-trained parameters and fine-tuned the network using LoRA. Specifically, we conduct the hyperparameter search for fixed values of r and a and select the fine-tuning hyperparameters that yield the best average performance. During the fine-tuning process, we save the checkpoints of the last 150 steps as the training dataset, which includes the LoRA learnable matrix weights. In the framework of COND P-DIFF, the autoencoder includes 1D CNN-based encoders and decoders. We utilize the text encoder from CLIP as the condition text encoder. In image style transfer tasks, we fine-tune attention modules of a popular text-to-image model, PIXART-a model [4] using LoRA and collected the last 64 LoRA checkpoints of the training process once in 10 steps. In the framework of COND P-DIFF, we used pre-trained ResNet18 to extract style latent as the condition vector. All experiments were conducted on the Linux server with four NVIDIA A100 GPUs. The noise &z is Gaussian noise with an amplitude of 0.001 by default. Detailed training hyperparameters for LORA fine-tuning and COND P-DIFF framework are provided in Appendix B.\nInference procedures. In NLP tasks, we generate 20 LORA parameters for each task using a conditional diffusion model through random noise and merge these generated parameters into the pre-trained model. We select the model that exhibits the best performance on the training dataset and report its performance on the validation dataset. In style-transfer tasks, we synthesize LoRA parameters of the corresponding styles by feeding the conditional diffusion model with images in various styles as conditions. We then merge parameters with PIXART-a's and utilize them to generate images using a set of prompts. Finally, we compute the FID score of the generated images.\nBaselines. 1) original: The best validation performance among the originally trained models. 2) model soup: The validation performance of the model whose weight is the average of the training dataset. Because Mitchell et al. [57] shows averaging the weights of fine-tuned models with different"}, {"title": "4.2 Experiment results", "content": "COND P-DIFF can generate high-performance parameters based on task conditions. Table 1 presents comparison results of COND P-DIFF and baseline methods across language understanding GLUE benchmark for three models with different LoRA configurations. We observe that COND P-DIFF consistently yields comparable performance in most scenarios, demonstrating it learns conditional parameter distributions effectively and stably. Besides, we note that the baseline average's performance in some cases surpasses the baseline, validating the potential of model averaging to enhance performance [57].\nTable 2 illustrates the results of COND P-DIFF and the baseline in the image style transfer task for different styles. We employ the FID [15] to quantitatively assess the quality of style-conditioned image generation. Lower FID represents better image generation quality. Based on our findings, COND P-DIFF efficiently synthesizes specific style-adapted LoRA parameters to generate high-quality images. Additional visual results are shown in Figure 3(a). This demonstrates that COND P-DIFF can practically generate high-performance model parameters based on specific conditions."}, {"title": "4.3 Ablation study", "content": "In this section, we conduct multiple ablation studies to report the characteristics of COND P-DIFF. We focus on the performance of generated LoRA parameters(rank r = 1) of BERT on SST2, RTE, and MRPC datasets. The training setting is the same as experiments Table 1.\nSize of the training dataset As described in Section 3.2, we collect N different checkpoints in the last N steps as a training dataset for task q using LoRA. We explore the relationship between dataset size N and performance in Table 3. We observe that the performance improves as the size of the training dataset increases. Specifically, a larger training dataset can provide a broader exploration space, thereby enabling COND P-DIFF to generate higher performance parameters. For instance, performance on the MRPC task improved by 4.53%."}, {"title": "4.4 Analysis", "content": "In this section, we conduct a detailed analysis of COND P-DIFF. Specifically, we explore two critical questions: First, does COND P-DIFF merely replicate training data, or can it generate high-performance model parameters that are distinct from the originals? Second, does the generated parameter space of COND P-DIFF have generalizability?\nCOND P-DIFF is not merely cloning model parameters.\nSimilarity vs. Performance First, we calculate the L2 distance between the generated and original parameters. Figure 3(c) illustrates the relationship between the similarity of the generated parameters and performance. We observe that COND P-DIFF attains various similarities and achieves better performance compared to original fine-tuned weights across various datasets.\nParameter distribution We employ t-SNE [52] to analyze the distributions of generated parameters and original weights of fine-tuned models on datasets COLA, QNLI, and STSB, as shown in Figures 3(b). We observe that the distribution of generated parameters by COND P-DIFF significantly differs from the original parameters. The distribution of the original parameters can be viewed as following the trajectory of the optimization process. In contrast, COND P-DIFF generates novel high-performance parameters by learning the distribution of parameters. Besides, the high-performance parameters generated by COND P-DIFF are dispersed more broadly, underscoring the generative model's potential to identify novel high-performance parameters beyond traditional optimization pathways. Interestingly, the high-performance parameter distributions generated by COND P-DIFF for the three datasets are very similar, demonstrating the necessity of exploring the high-performance parameter space.\nTrajectories of COND P-DIFF process. Figure 4(b) visualizes the generated parameters at different time steps during the inference stage using t-SNE [52] to explore the generation process in the image style-transfer tasks. We display five trajectories initialized from five different random noises and present the model soup and the original model parameters. The parameters derived from the model soup are located near the original parameters. We observe that the generated parameters gradually approach the original parameters but ultimately maintain some distance from them, indicating that COND P-DIFF generates high-performance parameters that are distributed differently from the original parameters rather than directly replicating them. The variations in the trajectories also demonstrate the robustness of COND P-DIFF.\nGeneralizability We examine the generalization of the generated parameter space in the task of image style transfer. We select parameters, Ostyle1 and 0style2, generated by COND P-DIFF conditioned two distinct styles, style1 and style2. To interpolate between these styles, we compute a new set of parameters @interp as @interp = (1 \u2013 1)0style1 + \u03bb\u03b8style2, where \u03bb \u2208 [0, 1] is the interpolation factor. Subsequently, we evaluate the effectiveness of interp in style transfer. Figure 4(b) illustrates the"}, {"title": "5 Related work", "content": "Diffusion models Diffusion models [16, 5, 35] have recently emerged as a powerful class of generative models, enabling high-fidelity synthesis of complex data distributions. The research on the diffusion model can be generally classified into four categories. The first category aims to enhance image synthesis quality [41, 39, 44] Second, researchers focus on accelerating the sampling process [49, 28]. Third, recent research has also focused on reevaluating diffusion models through the lens of continuous analysis like score-based generative modeling [8]. Fourth, the success of diffusion models has sparked their application in various domains, [27, 29, 56]. In this work, we explore the conditional diffusion model in the parameter generation domain.\nConditional generation Conditional generation has gained significant attention in computer vision and natural language processing. Three prominent frameworks have emerged: conditional GANs [31, 19, 60], conditional VAEs [48, 58], and conditional diffusion models xw[41, 16], which incorporate conditions to guide the generation process, enabling the creation of visually coherent and semantically meaningful data samples. Conditional GANs incorporate condition information into GAN to generate images conditioned on specific attributes or labels. Conditional diffusion models take this further by generating visually coherent and semantically meaningful images from the textual description, demonstrating superior image synthesis quality compared to GANs. Building upon the success of conditional diffusion models, we propose to extend this approach to generating neural network parameters based on specific conditions.\nParameter generation The field of parameter generation has seen significant progress in recent years, with HyperNetworks ([13] and generative models of neural network checkpoints [34] emerging as promising approaches. [13] introduced HyperNetworks, which uses a hypernetwork to learn the parameters for another neural network. [9] proposes Model-Agnostic Meta-Learning, which learns an initialization for efficient fine-tuning. [34] introduce the model G.pt to predict the distribution over parameter updates given an initial input parameter vector and a prompted loss or error. [46] trained autoencoder on a model zoo to learn a hyper-representation for generative use to sample new model weights [26] use a GNN-based model to sample network parameters. [7] directly leverages MLP weights and generates neural implicit fields encoded by synthesized MLP weights. [54] uses a diffusion model to generate high-performing neural network parameters across various architectures and datasets. Different from the previous works, we focus on conditional parameter generation to generate high-performing weights based on specific task conditions practically."}, {"title": "6 Conclusion", "content": "In this work, we proposed an approach COND P-DIFF for high-performance controllable parameter generation, specially for LoRA parameters. We utilize an autoencoder and a conditional latent diffusion model to capture the distribution of high-performing parameters and perform conditional generation, synthesizing a new set of parameters tailored to specific conditions. We show that our method can efficiently synthesize novel and high-quality model parameters. The parameter distribution generated by COND P-DIFF exhibits differences compared to the distribution obtained through conventional optimization methods, indicating a certain level of generalization capability."}, {"title": "6.1 Limitation and future work", "content": "Nonetheless, it is essential to recognize that diffusion in parameter generation is still largely unexplored despite the significant advances in the realm of image and video synthesis. In this work, we present a preliminary methodology for conditional parameter diffusion. However, several challenges remain unresolved, including reducing memory demands for large model architectures, enhancing the generalizability of generation techniques, and improving the representation of dataset conditions. Furthermore, integrating knowledge graphs with conditional diffusion offers promising directions for controlling conditional generation."}, {"title": "A Detailed related work", "content": "Diffusion models Diffusion models have emerged as a powerful class of generative models, enabling high-fidelity synthesis of complex data distributions. Diffusion models are based on non-equilibrium thermodynamics, which gradually add noise to data and learn to reverse the diffusion process to generate samples. [16, 5, 35] The research on the diffusion model can be generally classified into four categories. The first category aims to enhance image synthesis quality, as demonstrated by notable models such as Stable Diffusion [41], DALL-E 2 [39], and Imagen [44] by leveraging techniques like CLIP-based text encoders, latent space diffusion, and hierarchical architectures. Second, researchers focus on accelerating the sampling process, with key developments including Denoising Diffusion Implicit Models [49] and DPM-Solver [28]. These approaches aim to improve the computational efficiency of diffusion models through deterministic sampling, closed-form expressions, and numeri-cal ODE solvers. Third, recent research has also focused on reevaluating diffusion models through the lens of continuous analysis like score-based generative modeling [8] in continuous-time settings. Fourth, the success of diffusion models has sparked their application in various domains, including text-to-speech synthesis [27], 3D shape generation [29], and anomaly detection in medical images [56], demonstrating the potential of diffusion models beyond image synthesis. In this work, we explore the conditional diffusion model in the parameter generation domain.\nConditional generation Conditional generation has gained significant attention in machine learning, particularly in computer vision and natural language processing. Three prominent frameworks have emerged: conditional GANs [31, 19, 60], conditional VAEs [48, 58], and conditional diffusion models [41, 16], which incorporate conditions to guide the generation process, enabling the creation of visually coherent and semantically meaningful data samples. Conditional GANs incorporate condition information into GAN to generate images conditioned on specific attributes or labels. Conditional diffusion models take this further by generating visually coherent and semantically meaningful images from the textual description, demonstrating superior image synthesis quality compared to GANs. Building upon the success of conditional diffusion models, we propose to extend this approach to generating neural network parameters based on specific conditions.\nParameter generation The field of parameter generation has seen significant progress in recent years, with HyperNetworks ([13] and generative models of neural network checkpoints [34] emerging as promising approaches. [13] introduced HyperNetworks, which uses a hypernetwork to learn the parameters for another neural network. [9] proposes Model-Agnostic Meta-Learning, which learns an initialization for efficient fine-tuning. [34] introduce the model G.pt to predict the distribution over parameter updates given an initial input parameter vector and a prompted loss or error. [46] trained autoencoder on a model zoo to learn a hyper-representation for generative use to sample new model weights [26] use a GNN-based model to sample network parameters. [7] directly leverages MLP weights and generates neural implicit fields encoded by synthesized MLP weights. [54] uses a diffusion model to generate high-performing neural network parameters across various architectures and datasets. Different from the previous works, we focus on conditional parameter generation to generate high-performing weights based on specific task conditions practically."}, {"title": "B Experiment setup", "content": "In this section, we show detailed experiment setups, including dataset information and training configuration."}, {"title": "B.1 Style transfer experiments", "content": "In this section, we provide detailed information about the training configurations used for both the autoencoder and the diffusion model in the style transfer task.\nAutoencoder configuration: The encoder is a 1D CNN-based model where the channel of each layer is (16, 32, 64, 128, 256, 384, 512, 768, 1024, 64). At the bottom layer, we flatten the parameters and map them to a latent dimension of 256 with a linear layer. In the decoder part, we use transposed convolutions with the same number of channels and layers to upsample back to the original shape.\nThe training details of hyperparameters are as follows: total number of parameters 516, 096, kernel size for CNN model 9, learning rate 2 \u00d7 10-4 with cosine annealing, total training steps 12,000,"}, {"title": "B.2 Language experiments", "content": ""}, {"title": "B.2.1 Datasets", "content": "In NLP tasks, we use GLUE benchmark [53], a benchmark for evaluating natural language under-standing capabilities. SST2 [47]: A sentiment analysis benchmark using movie review excerpts, labeled as positive or negative, to aid in sentiment understanding. RTE: A dataset for evaluating if one sentence logically entails another, testing models' understanding of textual entailment. MRPC [1]: Contains sentence pairs to benchmark models' paraphrasing and semantic equivalence capabilities. COLA: Tests language models' grasp of English grammar, with sentences labeled as grammatically acceptable or not. QNLI: Converts question-answer pairs into inference tasks, assessing if sentences are correct responses to questions. STSB [3]: A benchmark for measuring semantic similarity between sentences, rated on a scale from 0 to 5 for nuanced meaning comprehension."}, {"title": "B.2.2 LoRA configurations", "content": "In this section, we introduce the configuration of LORA fine-tuning as presented in Table 1. All models are fine-tuned with 20 epochs and a dropout rate of 0.1. Mixed-precision training is enabled with FP16 to accelerate computation and reduce memory usage. The learning rate is set to 0.0001, and a warmup ratio of 0.1 is used to gradually increase it at the beginning of the training. Additionally, a weight decay of 0.1 is applied to regularize the model and prevent overfitting."}, {"title": "B.2.3 Condition", "content": "This is task 'SST-2'. SST-2 (The Stanford Sentiment Treebank) includes sentences from movie reviews and their sentiment labels (positive or negative). It tests a model's ability to capture sentiment from text.\nExample 1: Sentence: \"The movie was fantastic!\" Label: Positive. Example 2: Sentence: \"I did not enjoy the film at all.\" Label: Negative.\nThis is task 'RTE.' RTE (Recognizing Textual Entailment) involves pairs of sentences and asks whether the second sentence is true (entails), false, or undetermined based on the information in the first sentence.\nExample 1: Sentence 1: \"The cat sat on the mat.\" Sentence 2: \"There is a cat on the mat.\" Label: Entailment. Example 2: Sentence 1: \"Sarah bought two tickets to Hawaii for her honeymoon.\" Sentence 2: \"Sarah is planning a trip to Hawaii.\" Label: Entailment."}]}