{"title": "MED42-v2: A SUITE OF CLINICAL LLMS", "authors": ["Cl\u00e9ment Christophe", "Praveen K Kanithi", "Tathagata Raha", "Shadab Khan", "Marco AF Pimentel"], "abstract": "Med42-v2 introduces a suite of clinical large language models (LLMs) designed\nto address the limitations of generic models in healthcare settings. These models\nare built on Llama3 architecture and fine-tuned using specialized clinical data.\nThey underwent multi-stage preference alignment to effectively respond to natural\nprompts. While generic models are often preference-aligned to avoid answering\nclinical queries as a precaution, Med42-v2 is specifically trained to overcome this\nlimitation, enabling its use in clinical settings. Med42-v2 models demonstrate\nsuperior performance compared to the original Llama3 models in both 8B and 70B\nparameter configurations and GPT-4 across various medical benchmarks. These\nLLMs are developed to understand clinical queries, perform reasoning tasks, and\nprovide valuable assistance in clinical environments. The models are now publicly\navailable at https://huggingface.co/m42-health.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) have revolutionized natural language processing, demonstrating re-\nmarkable capabilities across various domains (Achiam et al., 2023; Gemini et al., 2023; Anthropic,\n2024). However, their application in specialized fields like healthcare has been limited due to the\nneed for domain-specific knowledge and adherence to strict ethical and safety guidelines. The med-\nical sector, in particular, requires models that can understand complex clinical terminology, reason\nthrough medical scenarios, and provide accurate, context-appropriate responses.\nDespite the advancements, generic models face significant limitations when applied to healthcare\nsettings. These include concerns about hallucinations and fabrications, biases and knowledge gaps,\nand risks about data privacy and ethics (Thirunavukarasu et al., 2023; Li et al., 2023). Such limita-\ntions reduce their effectiveness in aiding diagnostic processes (de Souza et al., 2023; Hirosawa et al.,\n2023), interpreting medical literature (Bagde et al., 2023; Cascella et al., 2023), generating patient\neducation materials (Ali et al., 2023), and assisting in clinical guidelines and decision support sys-\ntems.\nTo address these challenges, we introduce a second revision of Med42 (Christophe et al., 2024)\ncalled Med42-v2, a suite of clinical large language models designed to overcome the limitations\nof generic models in healthcare settings. Built on the Llama3 architecture (Dubey et al., 2024) and\nfined with specialized clinical data, Med42-v2 models undergo multi-stage preference align-\nment to effectively respond to natural prompts. Unlike generic models, which are often preference-\naligned to avoid answering clinical queries as a precaution, Med42-v2 is specifically trained to\nengage with clinical queries, making it suitable for various stakeholders in healthcare, including\nclinicians, patients, and providers. Med42-v2 demonstrates superior performance compared to the\noriginal Llama3 models in both 8B and 70B parameter configurations (Table 1) across various med-\nical benchmarks, excelling in understanding clinical queries, performing reasoning tasks, and pro-\nviding valuable assistance in clinical environments."}, {"title": "2 METHOD", "content": "The development of Med42-v2 follows a two-stage training process designed to create specialized\nclinical large language models (LLMs) that can effectively handle medical queries and tasks. Our\napproach builds upon the foundational capabilities of the Llama3 and Llama3.1 model families,\nenhancing them with domain-specific knowledge and alignment to clinical use cases. Our training\nmethodology consists of two primary stages:\n\u2022 Instruction Fine-tuning: In this initial stage, we fine-tune models from the Llama3 and\nLlama3.1 families using carefully curated clinical datasets. This process aims to improve\nthe models with specialized medical knowledge.\n\u2022 Preference Alignment: The second stage focuses on aligning the models' outputs with\nhuman preferences to ensure they can follow user instructions while safeguarding against\nunethical or biased behavior.\nThe following subsections detail each stage of our training process, highlighting the techniques and\nconsiderations involved in creating the Med42-v2 suite of models."}, {"title": "2.1 CLINICAL FINE-TUNING STAGE", "content": "The clinical fine-tuning stage is an important step in adapting large language models for special-\nized medical applications. This phase aims to enhance the model's understanding and generation\ncapabilities in clinical contexts, reducing the apprehension to answer medical-related questions and\nimproving its relevance and accuracy for healthcare-related tasks.\nDatasets: To construct a training dataset tailored for clinical applications, we curated a diverse\ncollection of resources specifically focused on medical and biomedical domains. Recognizing the\nimportance of real-world usability beyond simple question-answering, we added examples demon-\nstrating chain-of-thought reasoning as well as chat interactions. This addition was aimed at maxi-\nmizing the model's reasoning capabilities and its effectiveness in conversational settings. To further\nenhance the model's generalizability and linguistic understanding, we incorporated a carefully se-\nlected subset of data from a general domain, comprising 26.5% of the final training dataset. This\nhybrid approach is designed to optimize the model's performance across both specialized medical\ncontent and broader linguistic tasks."}, {"title": "Training Methodology:", "content": "We employ the classic auto-regressive loss for fine-tuning. Loss is back-\npropagated only on output tokens. This approach ensures that the model learns to generate appro-\npriate responses and not learn to generate the prompts. To maximize training speed and usage of the\nmodels context length, we concatenated all of our training samples into chunks of 8192 tokens."}, {"title": "Prompt Format:", "content": "As we are fine-tuning the Instruct versions of Llama3 and Llama3.1, we adhere\nto their established prompt format, which includes system, assistant, and user fields."}, {"title": "Training Process:", "content": "Each model was fine-tuned for two epochs over our curated dataset. The exact\nhyperparameters used in this process are detailed in Table 2, providing full transparency for repro-\nducibility."}, {"title": "2.2 PREFERENCE-ALIGNMENT STAGE", "content": "Preference alignment is a crucial step in developing large language models that can effectively meet\nuser needs and expectations. This process involves adjusting the model's outputs to align with\nhuman preferences. However, obtaining direct human feedback at scale is challenging and resource-\nintensive. To address this, we employed open-access preference datasets created with AI feedback,\nallowing for more efficient and scalable alignment.\nDatasets: For our preference alignment phase, we utilized two primary datasets: the UltraFeed-\nback dataset (Tunstall et al., 2023) and the Snorkel-DPO dataset (SnorkelAI, 2023). The Ultra-\nFeedback dataset is a comprehensive collection of AI preferences on various topics and tasks. The\nSnorkel-DPO dataset was created through an iterative process. Prompts were exclusively selected\nfrom UltraFeedback, without including external LLM responses. For each prompt, five response\nvariations were generated using the Mistral-7B-Instruct-v0.23 model. These responses were then\nreranked using PairRM (Jiang et al., 2023) to identify the top (chosen) and bottom (rejected) re-\nsponses. This process was repeated across three sets of 20,000 prompts, refining both the LLM and\nthe dataset responses through three iterations. This method ensured a comprehensive and structured\ndataset for training purposes, improving with each iteration.\nTraining Methodology: We employed Direct Preference Optimization (DPO) (Rafailov et al.,\n2024) to align our clinically fine-tuned checkpoints with preference data. This approach was cho-\nsen over more complex reinforcement learning algorithms (Ouyang et al., 2022) due to its stabil-\nity and scalability. We used DPO implementation from Huggingface Alignment Handbook library\n(Tunstall et al.) to train all our models.\nTraining Process: We followed an iterative alignment approach (Tran et al., 2023) using the\nmulti-stage data as described earlier. For the first iteration, we used UltraFeedback data and Snorkel-\nDPO-stage-1 data. The second and third iterations utilized Snorkel-DPO-stage-2 and Snorkel-DPO-\nstage-3 data, respectively. In each iteration, the model resulting from the previous iteration served\nas a reward model, leading to progressive performance improvements."}, {"title": "3 BENCHMARKS", "content": "To assess the performance of the fine-tuned language models, following previous works\n(Singhal et al., 2023; Chen et al., 2023; Toma et al., 2023), we used Eleuther AI's evaluation harness\nframework (Gao et al., 2023) to compute their zero-shot performance across various commonly-used\nmedical benchmarks. These contain medical exam questions and research datasets with multiple-\nchoice answers, and include: MMLU (medical subset) (Hendrycks et al., 2021), MMLU-Pro\n(Wang et al., 2024), MedMCQA (Pal et al., 2022), MedQA (Jin et al., 2020), USMLE (Nori et al.,\n2023a; Han et al., 2023), PubmedQA (Jin et al., 2019), ToxiGen (Hartvigsen et al., 2022). All\ndatasets are in the English language and all questions containing images were excluded. The harness\nframework has been updated to include chat templates. Additionally, our log-likelihood calculations\nare over the entire response sequence instead of just the first token.\nThese findings consistently demonstrate that larger models perform better on these tasks, in line\nwith general trends in language model scaling. However, the performance gains are less significant\non safety-focused benchmarks like ToxiGen. Moreover, models such as Med42 and OpenBioLLM\nexhibit enhanced performance on these benchmarks compared to their base Llama3-Instruct ver-\nsions. This highlights the advantages of specific medical instruction and alignment in improving the\nmodels' clinical expertise and analytical capabilities.\nIt's worth noting that these results represent zero-shot performance. Prior research has indicated\nthat prompting techniques, such as Medprompt (Nori et al., 2023b), or integration with search func-\ntionalities can yield even higher accuracy rates. For instance, Med-Gemini has achieved a 91.2%\naccuracy on benchmarks like MedQA (Saab et al., 2024).\nLLMs are designed to excel across a diverse set of tasks, leveraging their conversational capabilities.\nThis versatility is crucial for their application in various clinical tasks. Our future work will focus\non evaluating these capabilities in a clinical setting in detail."}, {"title": "4 CONCLUSIONS AND LIMITATIONS", "content": "In conclusion, we introduced Med42-v2, a suite of clinical large language models built on the\nLlama3 architecture and fine-tuned with specialized clinical data. Med42-v2 also employs a multi-\nstage preference alignment process, enabling it to effectively handle clinical queries. Our empirical\nresults show that Med42-v2 outperforms the original Llama3 models in both 8B and 70B parameter\nconfigurations and GPT-4 across various medical benchmarks.\nHowever, utilizing clinical LLMs in real-world settings can present several limitations. Despite im-\nprovements, Med42-v2 may not entirely be free from issues like hallucinations, biases, and ethical\nconcerns, which are particularly critical in the medical field. The reliance on high-quality, domain-\nspecific data means that any gaps or biases in the training data could impact the model's effective-\nness. To address these concerns, our future work involves developing a new evaluation framework\nto assess the clinical utility of LLMs by testing them on real-world use cases. This framework will\nfocus on evaluating clinical data understanding, safety, and reasoning capabilities, providing a more\ncomprehensive understanding of how these models perform in practical, high-stakes environments.\nBy rigorously testing LLMs in real-world scenarios, we aim to identify and mitigate potential risks,\nensuring that models like Med42-v2 can be safely and effectively integrated into healthcare settings."}]}