{"title": "Graph Canvas for Controllable 3D Scene Generation", "authors": ["Libin Liu", "Shen Chen", "Sen Jia", "Jingzhe Shi", "Zhongyu Jiang", "Can Jin", "Zongkai Wu", "Jenq-Neng Hwang", "Lei Li"], "abstract": "Spatial intelligence is foundational to AI systems that interact with the physical world, particularly in 3D scene generation and spatial comprehension. Current methodologies for 3D scene generation often rely heavily on predefined datasets, and struggle to adapt dynamically to changing spatial relationships. In this paper, we introduce Graph-Canvas3D, a programmable, extensible, and adaptable framework for controllable 3D scene generation. Leveraging in-context learning, GraphCanvas3D enables dynamic adaptability without the need for retraining, supporting flexible and customizable scene creation. Our framework employs hierarchical, graph-driven scene descriptions, representing spatial elements as graph nodes and establishing coherent relationships among objects in 3D environments. Unlike conventional approaches, which are constrained in adaptability and often require predefined input masks or retraining for modifications, GraphCanvas3D allows for seamless object manipulation and scene adjustments on the fly. Additionally, GraphCanvas3D supports 4D scene generation, incorporating temporal dynamics to model changes over time. Experimental results and user studies demonstrate that GraphCanvas3D enhances usability, flexibility,", "sections": [{"title": "1. Introduction", "content": "Spatial intelligence, defined as an AI system's ability to comprehend, interpret, and manipulate spatial relationships within a given environment, is fundamental to the development of systems capable of effective interaction with physical spaces. Despite considerable advancements in this domain [2, 8, 11, 22, 29, 32], current methods for 3D layout generation exhibit significant limitations in terms of flexibility, usability, and adaptability, which restrict their utility in dynamic, real-time applications. Many existing approaches [13, 14, 24] rely on resource-intensive retraining, stringent input configurations, or manually defined masks and constraints, each of which introduces rigidity into the 3D scene generation process, ultimately impacting efficiency and adaptability.\nExisting frameworks, such as LayoutGPT [8] and similar layout generation models [3, 9, 39], exemplify these constraints. While effective in generating initial 3D layouts, these models often require extensive manual intervention or detailed scene specifications whenever environmental changes are introduced. This requirement renders them impractical for applications demanding continuous or real-time adaptability. Moreover, their inherent inflexibility often necessitates frequent retraining to accommodate novel scenarios, a process that is both computationally expensive and time-consuming. Consequently, these models exhibit limited transferability and generalizability across diverse environmental contexts.\nTo address these challenges, we introduce GraphCanvas3D, a novel framework that aims to bridge the limitations of existing 3D layout generation methodologies by offering a programmable, extensible and transferable paradigm for 3D scene construction. GraphCanvas3D adopts a modular, Graph-based approach, representing spatial relationships through graph structures and utilizing Graph to link real-world entities within a cohesive 3D representation. This approach provides users with a flexible programming interface to create, modify, and expand 3D scenes across various environments, effectively eliminating the need for specialized retraining or intricate scene definitions.\nAnother important capability of GraphCanvas3D is its support for dynamic scene generation without requiring retraining or manual reconfiguration. This flexibility enables real-time, interactive scene editing, allowing users to modify scenes using concise natural language instructions. By leveraging a graph-based structure that adapts based on contextual text inputs, GraphCanvas3D supports precise, responsive adjustments to 3D layouts, allowing seamless component modifications. GraphCanvas3D incorporates time-based adjustments within its graph structure, enabling the creation of temporally evolving 3D scenes. This design allows objects and their spatial relationships to continuously evolve, supporting coherent 4D environments with minimal user intervention. Traditional methods often require extensive retraining, preconfigured datasets, or manual setup for each modification, making real-time 4D scene adaptation infeasible. With Multimodal Large Language Models (MMLM) driven, GraphCanvas3D's graph-based pipeline maintains temporal coherence and adaptability, establishing a new standard for flexible, real-time 3D and 4D scene generation. The contributions of this work are as follows:\n1.  We introduce a hierarchical, Graph-based, off-the-shelf framework for 3D scene generation that is both programmable and extensible, eliminating the need for retraining or manually specified scene details.\n2.  Our generated graph is flexible and editable, supporting adaptive, real-time scene generation and enabling dynamic modification of 3D layouts.\n3.  Extensive experimental evaluations and user studies demonstrate that GraphCanvas3D outperforms state-of-the-art methods in terms of usability, flexibility, and adaptability across diverse application scenarios."}, {"title": "2. Related Work", "content": null}, {"title": "2.1. 3D Representations", "content": "3D representations can be broadly categorized into implicit and explicit forms. Neural Radiance Fields (NeRF) [19] exemplify a widely used implicit approach, mapping 3D coordinates and viewing directions to color and density values via a multilayer perceptron (MLP). Mip-NeRF [1] enhances NeRF by using anti-aliased conical frustums, which effectively address aliasing and improve detail handling, leading to higher-quality rendered images. SparseNeRF [30] builds on this by incorporating depth information to reduce reliance on densely sampled input images, enabling high-quality 3D reconstruction from sparse views. However, NeRF-based methods typically demand substantial computational resources, limiting their scalability and applicability in real-time scenarios.\nIn contrast, 3D Gaussian Splatting (3DGS) [12] offers an efficient, explicit 3D representation by optimizing Gaussian spheres to capture the 3D environment. Scaffold-GS [18] enhances 3DGS by employing anchor points for the efficient distribution of local Gaussians, adjusting properties based on viewing direction and distance. Mip-Splatting [35] further improves reconstruction quality by controlling the frequency of 3D Gaussians, thus enhancing detail retention and enabling more efficient rendering."}, {"title": "2.2. Text-to-3D Generation", "content": "NeRF-based methods have played a crucial role in advancing text-to-3D generation, transforming textual prompts into 3D representations. DreamFusion [24] and Magic3D [16] employ diffusion models to optimize NeRF for single-object synthesis. Despite their success in generating standalone objects, these techniques encounter limitations when scaling to multi-object scenes. ProlificDreamer [31] improves 3D fidelity by integrating shape priors but struggles with inter-object interactions. Comp3d [23] and CompoNeRF [17] approach multi-object scenes with layout-constrained NeRF, though they require manual setup and may lead to visual artifacts. Recently, methods integrating 3DGS with diffusion models have been proposed to accelerate text-to-3D generation. For example, approaches by Yi et al. [33] and Liang et al. [15] use text-to-point models to initialize 3DGS with human priors, while others [4, 27, 28, 36] adopt two-stage optimization for geometry and texture. Although 3DGS offers speed advantages, multi-object scenes still present challenges due to weak layout constraints, leading to geometric inconsistencies and visual drift in scene content.\nRecently, Large Language Models (LLMs) [7, 25, 26] have been explored for their capacity in spatial reasoning, assisting 3D generation by interpreting text prompts to discern object relationships and support spatial layouts. LayoutGPT [8] advances this field by providing a CSS-like syntax for detailed layout control, improving spatial configuration specificity. SceneWiz3D [38] combines LLMs with layout-based NeRF to optimize scene composition, while GALA3D [6] uses LLMs for initial layout creation, employing layout-guided 3D Gaussian representation with adaptive constraints to refine geometry and inter-object interactions, thus achieving coherent multi-object 3D scenes. Nonetheless, LLM-based approaches often face challenges with spatial ambiguity, resulting in misaligned or floating objects due to imprecise layout generation. To address these issues, our method incorporates adaptive layout-guided Gaussian modeling, refining LLM-initialized layouts to improve spatial coherence and deliver consistent, high-quality 3D representations for complex, multi-object scenes."}, {"title": "3. Method", "content": null}, {"title": "3.1. Problem Formulation", "content": "In GraphCanvas3D, given a scene prompt, we employ a large language model (LLM) to parse the input, identify objects, and infer both explicit and implicit spatial relationships. Each identified object or is represented by a feature vector \\(f_i\\) with components encoding its spatial and geometric attributes:\n\\(f_i = [X_i, Y_i, Z_i, s_i, r_i]\\),\nwhere:\n*   \\((X_i, Y_i, Z_i)\\) represents the object's 3D spatial position,\n*   \\(s_i\\) denotes the scale factor, and\n*   \\(r_i\\) is the rotation factor along the z-axis, an essential attribute for orientational consistency and scene coherence.\nWe frame the 3D scene generation task as an optimization problem over a structured graph G, where:\n1.  Nodes represent individual objects in the scene, each characterized by a feature vector \\(f_i\\) that captures its 3D properties,\n2.  Edges denote spatial relationships between objects, derived from linguistic and contextual cues in the scene prompt."}, {"title": "3.2. Overview", "content": "As shown in Fig 2, We propose a hierarchical, programmable paradigm for 3D layout generation that circumvents the need for predefined object specifications, such as external files containing object dimensions or positions within the 3D scene. Instead, our approach enGraphs object attributes and their interrelationships through a series of parameterized functions. This paradigm facilitates the generation of coherent 3D scene layouts from succinct textual descriptions, followed by high-quality rendering. Furthermore, our layout paradigm supports iterative modifications of previously rendered scenes, enabling users to add new objects that automatically integrate into the existing layout, or to seamlessly remove or reposition elements without disrupting scene coherence.\nThe core of our methodology centers on a programmable graph that orchestrates the scene layout. Individual objects are represented as nodes, denoted by \\(o_i\\), and spatial relationships between objects are defined as edges, represented by \\(l_{ij} = \\phi(o_i, o_j)\\), where \\(\\phi\\) is an edge-level optimization function that enGraphs the relationship between objects \\(o_i\\) and \\(o_j\\). We formalize the graph as \\(G = (V,E)\\), where \\(V = \\{o_1, o_2, ..., o_N\\}\\) represents the set of nodes, and \\(E = \\{l_{1}, l_{2}, ..., l_{M}\\}\\) denotes the set of edges connecting these nodes.\nTo enhance robustness, we first construct a collection of subgraphs, each consisting of a set of connected nodes, denoted by \\(G_i = F_s(V_i, E_i)\\), where \\(F_s\\) is the subgraph-level optimization function that ensures local consistency. These subgraphs are subsequently aggregated to form the complete graph \\(G = F_g(\\{G_i\\})\\), with \\(F_g\\) representing the global optimization function responsible for ensuring overall coherence."}, {"title": "3.3. Edge-Level Optimization", "content": "To ensure spatial coherence among connected objects, GraphCanvas3D employs an iterative edge-level optimization strategy, refining the spatial relationships between pairs of connected objects \\((o_i, o_j)\\) in the scene. As illustrated in Figure 3, this optimization process minimizes deviations from ideal configurations by aligning relationships with high-level semantic expectations derived from the scene prompt. Each pair is evaluated based on an edge cost function \\(\\psi(o_i, o_j)\\), which takes into account relative positions, scales, and orientations."}, {"title": "3.4. Subgraph-Level Optimization", "content": "Following the completion of edge-level optimizations, which establish robust spatial relationships between connected objects, the next stage involves assembling these optimized objects into coherent subgraphs and, subsequently, a unified global scene. Each subgraph \\(G_i\\) is optimized independently to ensure internal spatial coherence, laying the foundation for an integrated scene that aligns with high-level semantic configurations.\nIndependent Subgraph Optimization: Each subgraph is constructed by grouping objects with strong inter-object relationships, typically defined by closely aligned spatial attributes and semantic associations. These subgraphs \\(G_i\\) are optimized to minimize internal energy functions \\(E_{subgraph}(G_i)\\), ensuring that objects within each subgraph maintain coherent relative positions, scales, and orientations. This step ensures that local regions of the scene exhibit spatial fidelity and that subgraphs can be integrated without internal inconsistencies.\nLLM-Guided Subgraph Placement: Once subgraphs achieve local optimization, their placements within the overall scene are guided by the large language model (LLM), which interprets high-level prompts that specify details about the subgraphs, such as the number of constituent objects, their inferred sizes, and interrelationships. The LLM uses this contextual information to propose initial placements that reflect semantic and spatial expectations at the scene level. This guided placement ensures that the relationships between different subgraphs are consistent with the scene's intended spatial semantics."}, {"title": "3.5. Graph-Level Optimization", "content": "To achieve a globally coherent scene layout, a higher-level optimization function \\(F_g\\) is employed. This function refines the spatial arrangements of subgraphs, minimizing the global objective:\n\n\\(G^* = \\arg \\min_G \\sum_{i=1}^K E_{subgraph}(G_i) + \\sum_{(G_p, G_q) \\in E_g} \\psi(G_p, G_q)\\),\n\nwhere \\(\\psi(G_p, G_q)\\) represents the penalty function applied to any misalignment or inappropriate spacing between adjacent subgraphs \\(G_p\\) and \\(G_q\\). This term enforces spatial consistency between subgraphs, preserving both the relative positioning and semantic alignment across the global scene."}, {"title": "3.6. Dynamic Scene Modification", "content": "GraphCanvas3D supports dynamic scene modifications (4D scene), allowing for moving, adding, removing, and repositioning of objects. For additions, a new node \\(O_{new}\\) is introduced with spatial relationships established through new edges \\(l_{new,i}\\), which are optimized for coherence. In the case of removals, the corresponding node and its edges are deleted, followed by re-optimization of adjacent nodes to maintain alignment. Repositioning involves updating the feature vector \\(f_i\\) of the target node and re-optimizing related edges and subgraphs to preserve the overall scene layout."}, {"title": "4. Experimental Results", "content": "Implementaion details. In our experiments, we utilized ChatGPT-40 [21] as both the Large Language Model (LLM) and the Multimodal Language Model (MLLM), alongside Point-E [20] as the 3D generative model. The Point-E model generates 4096-point clouds, providing a foundational approximation of object contours, though it lacks high-resolution detail. To enhance the fidelity of these representations, we expand each object's point cloud to 100,000 points via bilinear interpolation. These enriched point clouds are then employed as the initialization for each object's 3D Gaussian Splatting (3DGS) process. To enhance texture and detail in each object, we utilized MVDream [13] for object rendering. To maintain consistency across the entire scene, we employed ControlNet [37] for comprehensive scene rendering, ensuring seamless integration of objects within the overall environment. We set the MVDream guidance scale to 7.5 to preserve object structural integrity while enhancing texture details during rendering. In our 3D Gaussian Splatting (3DGS) framework, parameters such as opacity, position, spherical harmonics coefficients, and covariance are consistent with those in GALA3D [6]. All experiments were conducted on a single"}, {"title": "4.1. Quantitative Comparsion.", "content": "To evaluate our approach on the Text-to-3D task, we benchmark against state-of-the-art methods, including Dream-Gaussian [27], GaussianDreamer [34], MVDream [13], GS-Gen [5], and GALA3D [6]. Following previous studies [10, 24], we use the CLIP Score to assess the alignment between textual descriptions and generated images. Additionally, we leverage a Multi-modal Large Language Model (MLLM) to evaluate the semantic consistency between scene descriptions and generated images from multiple perspectives. This comprehensive evaluation enables the MLLM to rank and score all methods based on their outputs. As shown in Table 1, our method achieves the highest CLIP and MLLM scores."}, {"title": "4.2. Qualitative Comparison.", "content": "We present a qualitative comparison of Text-to-3D scene generation in Figure 1 and Figure 4. Notably, GALA3D requires precise input for each object's location, scale, and rotation during scene generation. To address this, we adopt a prompt format similar to LayoutGPT [8], using ChatGPT-4o to generate these attributes. While we utilized the CSS format from LayoutGPT, our prompts did not include a large number of directly related scene examples. Compared to existing methods, GraphCanvas3D produces scenes with a more realistic and cohesive structure, delivering robust rendering results adaptable to various scenarios. This advantage is attributed to our graph-based framework and optimization-driven scene layout control, which enables our method to outperform others in both quality and adaptability."}, {"title": "4.3. User Study.", "content": "To further assess the effectiveness of our method in generating high-quality, text-consistent 3D scenes, we conducted a user study with 67 participants. The study involved comparing 3D models generated by our approach with those produced by competing methods, using eight distinct text descriptions. Participants evaluated each model across three dimensions: (a) Scene Quality, (b) Geometric Fidelity, and (c) Layout Realism, assigning ratings on a scale from 1 to 10 (with 10 indicating the highest score). As summarized in Table 2, our method consistently achieved superior ratings, demonstrating its clear advantage over previous approaches."}, {"title": "4.4. Dynamic Scene Modification.", "content": "As illustrated in Figure 5, our approach facilitates not only the generation of static 3D scenes from text but also supports dynamic editing, addition, and deletion within 3D scenes. Moreover, our method extends to generating 4D scenes that evolve over time. The core logic behind both object editing and 4D scene generation remains consistent in our approach. Given a prompt that describes a transformation process, GraphCanvas3D enables multimodal large language models (MLLMs) to analyze the scene and determine the final state of the objects, referred to as \"state prompts.\" These state prompts then guide the optimization of our scene graph, where an iterative process yields a temporal transformation sequence, achieving object editing and 4D scene generation in a unified manner. Additionally, objects can be efficiently added or removed within the scene by modifying the scene graph, allowing for flexible and efficient scene adjustments."}, {"title": "5. Ablation Study", "content": "Model Flexibility. Our method imposes no strict model requirements on the LLM, 3D Generative Model, or MLLM, allowing for flexible integration with various model architectures. To enhance the adaptability of our approach, we examined three different model combinations in this ablation study, as shown in Figure 6. At the core of our method is a graph-based structure that encodes objects within a scene and their interrelationships, ensuring consistently reliable outcomes regardless of the specific models employed and underscoring the approach's robustness and versatility.\nHierarchical Optimization. Figure 7 presents an ablation study evaluating the effectiveness of our hierarchical optimization approach. Given a prompt, GraphCanvas3D generated a layout that accurately reflects real-world spatial arrangements, whereas the layout produced solely by GPT-40 appeared disorganized. Removing edge optimization resulted in notable misalignment between the person and bicycle, emphasizing the role of edge optimization in maintaining spatial constraints between connected nodes. Without subgraph optimization, substantial scale discrepancies emerged between two subgroups\u2014one containing the person, bicycle, and bushes, and the other containing the trash can and bottle\u2014resulting in an unrealistic layout. Additionally, when graph optimization was excluded, the generated scene lacked overall coherence, with nearly all objects clustered on one side, contradicting the prompt's intended layout. This study demonstrates the reliability of our hierarchical optimization approach and the essential role of each optimization level in achieving coherent scene generation."}, {"title": "6. Conclusion", "content": "In this work, we introduced GraphCanvas3D, a novel framework that addresses the limitations of 3D scene generation methods by providing a flexible, modular, and adaptive approach to 3D scene construction. Distinct from prior models, GraphCanvas3D employs Multi-Layered Language Models (MLLMs) to enable real-time scene manipulation through natural language descriptions, obviating the need for retraining or rigid input configurations. By representing spatial relationships as graph structures, GraphCanvas3D offers an intuitive interface that facilitates dynamic scene modifications, significantly enhancing usability and adaptability across diverse environments. Our experimental results validate GraphCanvas3D's effectiveness, demonstrating superior performance in flexibility, responsiveness, and user-centered design when compared with existing approaches, thus underscoring its potential as a robust tool for applications requiring real-time adjustments and spatial intelligence. Future work will focus on further scaling GraphCanvas3D's capabilities and efficiency, including integration with virtual and augmented reality platforms to extend its utility in interactive and adaptive 3D scene generation."}, {"title": "7. Optimized Processing", "content": "We provide an expanded explanation of the methodology, as illustrated in the Figure 9. In the following paragraphs, each subsection of the methods will be further elaborated in greater detail.\nGraph Construction. We designed Prompt 1 (shown in Table 3) to guide LLMs in performing instance-level segmentation of objects and relationships with a scene description Ts, resulting in the generation of node prompts and edge prompts. Each instance object is represented as a vertex in the graph, containing attributes such as its 3D representation (e.g., point cloud), scale, position, and rotation. Each instance relationship is represented as an edge in the graph, defined by two connected vertices and a directed relationship attribute.\nEdge Optimization. For each edge inferred by the LLMs, we perform an edge optimization process. Here, we provide additional details about the optimization. During each optimization step, three sets of objects are involved: the entire set of objects Xall, the object being optimized X1, and the remaining objects X2 = Xall \u2013 X1 . The optimization algorithm captures images of these objects from four different viewpoints, which are then input into the MLLMs along with Prompt 2 (shown in Table 3) to evaluate their scores. During edge optimization, X\u2081 represents the source node object of the edge (in-degree), X2 represents the node vertex object of the edge (out-degree), and Xall represents the combined context of both objects. Our edge optimization is able to establish reasonable relationships edge.\nIndependent Subgraph Optimization. To preserve the integrity of relationships among existing nodes during the addition of new nodes, we adopt an Independent Subgraph Optimization approach. In this process, each newly optimized edge is incrementally integrated into the graph. If one of the nodes associated with the new edge already exists within the graph, its attributes are propagated to the other node. This propagation ensures consistency by facilitating numerical adjustments across connected nodes within the subgraph.\nLLM-Guided Subgraph Placement. This method is designed to further refine the spatial positions of nodes"}, {"title": "8. Failure Cases", "content": "In our experiments, we identified occasional cases where our method encountered challenges during various stages of the optimization process. These issues were primarily due to ambiguities or inconsistencies in the input descriptions"}]}