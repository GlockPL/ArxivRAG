{"title": "AUTOREGRESSIVE MOVING-AVERAGE ATTENTION MECHANISM FOR TIME SERIES FORECASTING", "authors": ["Jiecheng Lu", "Xu Han", "Yan Sun", "Shihao Yang"], "abstract": "We propose an Autoregressive (AR) Moving-average (MA) attention structure that can adapt to various linear attention mechanisms, enhancing their ability to capture long-range and local temporal patterns in time series. In this paper, we first demonstrate that, for the time series forecasting (TSF) task, the previously overlooked decoder-only autoregressive Transformer model can achieve results comparable to the best baselines when appropriate tokenization and training methods are applied. Moreover, inspired by the ARMA model from statistics and recent advances in linear attention, we introduce the full ARMA structure into existing autoregressive attention mechanisms. By using an indirect MA weight generation method, we incorporate the MA term while maintaining the time complexity and parameter size of the underlying efficient attention models. We further explore how indirect parameter generation can produce implicit MA weights that align with the modeling requirements for local temporal impacts. Experimental results show that incorporating the ARMA structure consistently improves the performance of various AR attentions on TSF tasks, achieving state-of-the-art results. The code implementation is available at the following link.", "sections": [{"title": "INTRODUCTION", "content": "In recent years, autoregressive (AR) decoder-only Transformer-based models (Vaswani, 2017; Radford, 2018) have been widely used in sequence modeling tasks across fields such as NLP (Brown et al., 2020; Touvron et al., 2023), CV (Chen et al., 2020; Esser et al., 2021; Chang et al., 2022), and audio (Borsos et al., 2023). This structure is well-suited for various sequential generation and prediction tasks. However, in typical sequence modeling tasks like time series forecasting (TSF), there has been less exploration of this architecture compared to other structures. Most of the best-performing recent TSF models are encoder-only Transformers (Liu et al., 2024a; Nie et al., 2022), MLPs (Das et al., 2023; Lu et al., 2024), or even linear models (Zeng et al., 2023; Xu et al., 2024). The few relevant discussions mainly focus on using pretrained autoregressive LLMs or similar structures for few-shot and zero-shot prediction (Gruver et al., 2023; Jin et al., 2024; Das et al., 2024; Liu et al., 2024b), with little research directly evaluating their TSF performance in end-to-end training. Therefore, this paper will first briefly demonstrate that with appropriate tokenization and training methods, a basic AR Transformer is enough to achieve results comparable to the state-of-the-art (SOTA) baselines, as shown in Fig. 1 and Fig. 2.\nRecently, efficient linear AR attention variants have been explored and developed (Katharopoulos et al., 2020; Hua et al., 2022), reducing the time complexity of standard softmax attention from $O(N^2)$ to $O(N)$. Researchers have found that adding a gating decay factor or a similar exponential moving average (EMA) structure to AR structure, as in gated linear attention (Ma et al., 2022; Yang et al., 2024), enhances linear attention's ability to model local patterns and improves performance. The success of these approaches inspired us to introduce a more comprehensive full autoregressive moving-average (ARMA) structure into existing AR attention mechanisms and explore the performance of the ARMA Transformers in TSF.\nIn TSF models, EMA, connecting back to the historic work of Holt-Winters (Winters, 1960; Holt, 2004), focuses on smoothed local data, which improves the modeling of short-term fluctuations but"}, {"title": "METHOD", "content": "In Time Series Forecasting (TSF), the goal is to predict the future part in a multivariate time series $S \\in \\mathbb{R}^{L\\times C}$, where $L$ is the length of the series, and $C$ is the number of channels or input series. The time series is divided into historical input $S_1 \\in \\mathbb{R}^{L_1\\times C}$, and future data $S_p \\in \\mathbb{R}^{L_P\\times C}$, where $L = L_1+L_P$, and $L_1$ and $L_p$ represent the lengths of the input and forecasting periods, respectively. The objective is to learn a mapping function $f : \\mathbb{R}^{L_1\\times C} \\rightarrow \\mathbb{R}^{L_P\\times C}$ that predicts the future values $\\hat{S}_p = f(S_1)$, given the historical input $S_1$.\nRecently, most time series forecasting research utilizes encoder-decoder or encoder-only Transformers for TSF (Li et al., 2019b; Zhou et al., 2021; Wu et al., 2021; Nie et al., 2022; Liu et al., 2024a), with limited focus on end-to-end decoder-only autoregressive Transformer because of error accumulation issue. For long-term forecasts, The autoregressive Transformers requires iteratively doing one-step prediction, leading to error accumulation and higher MSE compared to non-autoregressive models that generate the entire forecast at once.\nTo prevent error accumulation, we use an autoregressive Transformer (Fig. 1) that treats one-step prediction as the complete forecast. Inspired by PatchTST (Nie et al., 2022), we adopt a channel-independent approach, predicting each series separately and applying RevIN (Kim et al., 2022) to each. For an input series of length $L_1$ in $S_1$, we apply non-overlapping patches with a patch size $L_p$, dividing the input into $N = \\frac{L_1+P}{L_p}$ patches, where P is zero-padding for divisibility. This"}, {"title": "PRELIMINARIES: DECODER-ONLY TRANSFORMER", "content": "We use a GPT-2-style decoder-only Transformer (Radford et al., 2019) for autoregressive TSF. Token patches of length $L_p$ are linearly projected to d-dimensional vectors and combined with learnable positional embeddings to form the input sequence $X \\in \\mathbb{R}^{N\\times d}$, where each token is $x_t \\in \\mathbb{R}^{1\\times d}$. Each of the m Transformer layers applies layer normalization LN(\u00b7), attention Attn(\u00b7), and a channel-wise MLP MLP(\u00b7). With single-head softmax attention, a Transformer layer is defined as:\n$\\text{Attn}(X) = \\text{softmax} (M \\odot (QK^T)) VW_o,\\text{with } Q, K, V = XW_q, XW_k, XW_v$\n$X := X + \\text{Attn}(\\text{LN}(X)), \\text{ then } X := X + \\text{MLP}(\\text{LN}(X))$  (1)\nwhere $W_q, W_k, W_v, W_o \\in \\mathbb{R}^{d\\times d}$ are the projection matrices for the query, key, value, and output, respectively, and $M \\in \\mathbb{R}^{N\\times N}$ is the causal mask, defined as $M_{ij} = 1\\{i > j\\} - \\infty \\cdot 1\\{i < j\\}$."}, {"title": "PRELIMINARIES: EFFICIENT LINEAR ATTENTION MECHANISMS", "content": "Recent autoregressive efficient attention mechanisms reduce computational complexity from $O(N^2)$ to $O(N)$ by avoiding the explicit calculation of the $N \\times N$ attention matrix (Katharopoulos et al., 2020; Choromanski et al., 2021; Hua et al., 2022; Sun et al., 2023). Most of them can be reformulated as parallel linear RNNs with identity or diagonal state updates. Although these efficient attentions do not outperform standard softmax attention for large models, they achieve comparable results on smaller tasks (Katharopoulos et al., 2020; Choromanski et al., 2021). This paper investigates integrating these mechanisms into TSF and shows that adding a moving-average term significantly improves their performance. We begin by expressing the recurrent form of standard softmax attention. For a single head without output projection, let $q_t, k_t, v_t$ be the vectors at step t from Q, K, V. The output $o_t$ is given by: $o_t = \\frac{\\sum_{i=1}^{t} \\text{exp}(q_t^Tk_i)v_i}{\\sum_{i=1}^{t} \\text{exp}(q_t^Tk_i)}$.\nLinear attention Linear Attention replaces the $\\text{exp}(q_t^Tk_i)$ term in standard attention with a kernel function $k(x, y) = \\langle \\phi(x), \\phi(y) \\rangle$, resulting in $\\phi(q_t)^T \\phi(k_i)$ (Katharopoulos et al., 2020). This change reduces the time complexity from $O(N^2)$ to $O(N)$ by eliminating the need to compute the full $N \\times N$ attention matrix. Instead, it computes $\\phi(k_i)^T v_i$ for each i and aggregates over N. Various kernel functions have been explored, with identity kernels without denominators performing well enough (Mao, 2022; Qin et al., 2022; Sun et al., 2023; Yang et al., 2024). In this setup, Linear"}, {"title": "ARMA ATTENTION MECHANISM", "content": "In these attention mechanisms, the next-step prediction at time t is a weighted sum of all previous values $v_i\\in \\mathbb{R}^{1\\times d}$, with weights $w_{ti} \\in \\mathbb{R}^{1\\times d}$ derived from interactions between $q_t$ and $k_i$. Naturally, we can write these attention mechanisms in an AR model structure:\nv_{t+1} = o_{t}^{AR}+r_t = \\sum_{i=1}^{t}W_{t,i}v_i+r_t,\nwhere $r_t$ is the AR error. In an ARMA model, the MA term captures short-term fluctuations, allowing the AR component to focus on long-term dependencies. Let $\\epsilon_t$ be the error after introducing the MA term and $O_{t-1,j}$ the MA weights generated by some attention mechanism. We expand the AR error $r_t$ into an MA form and extend the model to an ARMA structure as:\nv_{t+1}^{ARMA} = o_t^{AR} + o_t^{MA} + \\epsilon_t = \\sum_{i=1}^{t}W_{t,i} v_i + \\sum_{j=1}^{t-1} \\Theta_{t-1,j} \\odot \\epsilon_j + \\epsilon_t (2)\nThe structure of the MA output $o_{t}^{MA} = \\sum_{j=1}^{t-1} \\Theta_{t-1,j} \\odot \\epsilon_j$ resembles the AR term and could potentially be computed using an attention mechanism. For simplicity, we consider a single channel of the d-dimensional space, with other channels can be handled in parallel. We express the matrix form"}, {"title": "INDIRECT MA WEIGHT GENERATION", "content": "We need an approach that can leverage linear attention's efficiency to compute $o_t^{MA}$ without the costly $N\\times N$ matrix operations. Instead of separately calculating attention weights to determine $ \\epsilon_j$ as value input and recomputing the whole MA output, we aim to use a linear RNN to collect all keys and values at once. We observe from Eq. 3 that there is already a sequential relationship between $r_j$ and $ \\epsilon_j$, and $r_j$ can be computed directly once $o_t^{AR}$ is determined. Therefore, we implicitly compute the MA weights of $ \\epsilon_j$ by using $r_j$ as value input for the MA component instead of $\\epsilon_j$. Let $B_{t-1,j}$ denote the generated attention weights corresponding to $r_j$ at step t, and let $\\Theta_{t-1,j}$ here be the implicit MA weights hiddenly linked to the generated $B_{t-1,j}$. Based on Eq. 3, we establish:\n$\\sum_{j=1}^{t-1} B_{t-1, j} r_j = \\sum_{j=1}^{t-1} \\Theta_{t-1,j} \\odot \\epsilon_j \\Leftrightarrow Br = \\Theta \\epsilon \\text{ (for one channel)}$\n$B = \\Theta \\cdot (I + \\Theta)^{-1}, \\Theta = B \\cdot (I - B)^{-1}  (4)$\nWith $\\Theta = B \\cdot (I - B)^{-1}$, as long as the indirectly generated $\\Theta$ accurately reflects the characteristics of the MA weights we want, we can use $\\sum_{j=1}^{t-1} B_{t-1,j}r_j$ as $o_t^{MA}$. Since $r_j$ is known after computing $o_t^{AR}$, linear attention can be used to compute $o_t^{MA}$ without increasing the time complexity. To ensure the implicitly generated $\\Theta$ from B captures the desired MA properties, we must carefully design how B is generated. The invertibility of $(I \u2013 B)^{-1}$ is guaranteed since B is strictly lower triangular. To efficiently compute the generated weights, we use the $B_{t-1,j} = \\phi^{MA}_{q}(q^{MA}_t)\\phi^{MA}_{k}(k^{MA}_j)$ to generate B, similar to linear attention. Previous dynamic ARMA models in statistics often update MA weights based on observations (Grenier, 1983; Azrak & M\u00e9lard, 2006), so we derive $q^{MA}$ and $k^{MA}$ by multiplying the attention input $x_t$ with $W^{MA}_q$ and $W^{MA}_k$. Now, the effectiveness of MA weights lies in selecting the most suitable functions $\\phi^{MA}_q(.)$ and $\\phi^{MA}_k(.)$."}, {"title": "SELECTION OF $\\phi(\\cdot)$ AND CHARACTERISTICS OF IMPLICIT MA WEIGHTS", "content": "The MA term models short-term effects and local temporal relationships, so we want the implicit $\\Theta$ to follow a pattern where elements near the diagonal have larger absolute values, and those farther away gradually decrease. The expanded form of $\\Theta$ is given by $\\Theta = B \\cdot (I \u2013 B)^{-1} = B + B^2 + B^3+...$ The elements along the diagonal direction in B continually accumulate as products into the elements below them in $\\Theta$. Since B is strictly lower triangular, the elements of the subdiagonal in $\\Theta$ remain constant, while the elements further down progressively accumulate additional terms formed by the product of different B. elements above. Assuming $B_{ij}$ follows a distribution and simplifying by setting each $B_{ij}$ to the distribution mean b, the elements of $\\Theta$ can be expressed as:\n$\\Theta_{ij} = b(1 + b)^{i-j-1}, where i > j  (5)$"}, {"title": "ARMA ATTENTION MECHANISM", "content": "attention can be viewed as an RNN with a hidden state matrix $\\sum_{i=1}^{t} k_i^T v_i \\in \\mathbb{R}^{d \\times d}$ that updates using the identity function. The output at each step is: $o_t = q_t^T \\sum_{i=1}^{t} k_i^T v_i$\nElement-wise linear attention In multi-head linear attention with h heads, we handle h hidden state matrices of size $\\mathbb{R}^{d \\times d}$. When h = d, this simplifies to h scalar hidden states, effectively transforming linear attention into a linear RNN with a d-dimensional hidden state vector $(k_i) v_i$ and enabling element-wise computations of $q, k, v$. This approach, also known as the Attention Free Transformer (AFT) (Zhai et al., 2021), is favored for its simplicity and efficiency in recent works (Peng et al., 2023). We adopt the structure in AFT, where $\\sigma(\u00b7)$ is the sigmoid function, and the output at each step is: $o_t = \\sigma(q_t) \\frac{\\sum_{i=1}^{t} \\text{exp}(k_i) v_i}{\\sum_{i=1}^{t} \\text{exp}(k_i)}$\nGated linear attention Recent studies have explored adding a forget gate, commonly used in traditional RNNs, to linear attention, allowing autoregressive models to forget past information and focus on local patterns (Mao, 2022; Sun et al., 2023; Qin et al., 2024; Yang et al., 2024). We implement a simple gating mechanism where each input $x_t$ is converted into a scalar between [0, 1] and expanded into a forget matrix $G_i$ matching the shape of $k_i^T v_i$. With gating parameters $W_g \\in \\mathbb{R}^{d \\times 1}$, the output at each step is: $o_t = q_t \\sum_{i=1}^{t} G_i \\odot k_i^T v_i, G_i = \\prod_{k=1}^{i} \\sigma(x_k^T W_g)11^T$.\nFixed Attention We additionally explore an autoregressive structure with fixed, data-independent weights $w_{t,i}$, replacing the dynamically generated attention weights $\\phi(q_t)^T \\phi(k_i)$. Without dynamic parameter generation, this becomes a linear layer with a causal mask M rather than a true attention mechanism. We use this structure to examine the effect of adding a moving-average term. This autoregressive causal linear layer is expressed as: $o_t = \\sum_{i=1}^{t} w_{t,i}v_i$."}, {"title": "SELECTION OF $\\phi(\\cdot)$ AND CHARACTERISTICS OF IMPLICIT MA WEIGHTS", "content": "This simplification offers valuable insights. To prevent longer-term errors from having a larger impact, we aim to avoid large absolute values accumulating in $\\Theta$ far from the diagonal. We also want $\\Theta_{ij}$ to decay steadily as it moves away from the diagonal. Therefore, constraining $B_{ij}$ between -1 and 0, with a preference of smaller absolute values, is a practical approach.\nWe tested various activation function combinations for $\\phi^{MA}_{q}(\\cdot)$ and $\\phi^{MA}_{k}(.)$ to generate $B_{t-1,j} = \\phi^{MA}_{q}(q^{MA}_t)\\phi^{MA}_{k}(k^{MA}_j)$ values, as shown in Fig. 4, 7, and 8. We used the sigmoid function $\\phi^{MA}_{k}(k^{MA}_j) = \\sigma(\\alpha k^{MA}/\\sqrt{d})$ to obtain values between 0 and 1, where $\\alpha = 0.05$1 and $\\sqrt{d}$ are scaling factors to maintain small absolute values. Then, we selected a function $\\phi^{MA}_{q}(\\cdot)$ to make the product negative. We ultimately chose $\\phi^{MA}_{q}(q^{MA}_t) = -\\text{LeakyReLU}(-q^{MA}/\\sqrt{d})$ with a negative slope of 0.02. The inner negative sign maintains directional consistency (for later parameter sharing), and the outer negative sign encourages a negative output."}, {"title": "EXPERIMENTS", "content": "We conducted comprehensive experiments on 12 widely-used TSF datasets, including Weather, Solar, Electricity (ECL), ETTs, Traffic, and PEMS. See \u00a7A.1 for detailed description of datasets.\nBaselines We built AR Transformers using the five attention mechanisms from Table 1 and added MA terms to create ARMA attention for comparison in TSF tasks. Additionally, we included five"}, {"title": "RELATED WORKS", "content": "Linear attention with exponential moving-average Beyond using a gating decay factor on the hidden state matrix of linear attention (Mao, 2022; Sun et al., 2023; Yang et al., 2024), recent studies have explored incorporating EMA mechanisms into gated linear attention by applying a smoothing factor (summing to 1) to the two terms in the state update (Ma et al., 2022). Similar EMA mechanisms have also been used in many modern RNN structures (Gu et al., 2022; Peng et al., 2023; Orvieto et al., 2023; Qin et al., 2024). Additionally, Schiele et al. (2022) attempted to introduce the ARMA structure into traditional RNNs, but their method could not ensure that the generated MA"}, {"title": "CONCLUSION, LIMITATION, AND FUTURE WORKS", "content": "We propose the ARMA attention mechanism, which integrates an MA term into existing AR attention using a novel indirect MA weight generation method. This approach maintains the same time complexity and parameter size while ensuring the validity of the implicit MA weights. Experiments demonstrate that ARMA attention successfully decouples and handles long-term and short-term effects. The ARMA Transformer, enhanced with the MA term, outperforms their AR counterparts and achieves state-of-the-art results, offering consistent improvements in training with minimal added computational cost.\nOne limitation is that we have not explored combining the channel-independent ARMA Transformer with multivariate forecasting models to improve its handling of inter-series relationships. For future work, ARMA attention could be applied to general sequence modeling tasks beyond TSF. Testing on larger-scale datasets, such as using ARMA Transformers for large-scale NLP pretraining, is another promising direction."}, {"title": "APPENDIX", "content": "Our main MTSF experiments are conducted on 12 widely-used real-world time series datasets. These datasets are summarized as follows:\nWeather Dataset\u00b9 (Wu et al., 2021) comprises 21 meteorological variables, including air temperature and humidity, recorded at 10-minute intervals throughout 2020 from the Weather Station of the Max Planck Biogeochemistry Institute in Germany.\nSolar Dataset\u00b2 (Lai et al., 2018) consists of high-frequency solar power production data from 137 photovoltaic plants recorded throughout 2006. Samples were collected at 10-minute intervals.\nElectricity Dataset\u00b3(Wu et al., 2021) contains hourly electricity consumption records for 321 consumers over a three-year period from 2012 to 2014.\nETT Dataset (Zhou et al., 2021) The ETT (Electricity Transformer Temperature) Dataset comprises load and oil temperature data from two electricity transformers, recorded at 15-minute and hourly intervals from July 2016 to July 2018. It is divided into four subsets (ETTm1, ETTm2, ETTh1, and ETTh2), each containing seven features related to oil and load characteristics.\nTraffic Dataset(Wu et al., 2021) Sourced from 862 freeway sensors in the San Francisco Bay area, the Traffic dataset provides hourly road occupancy rates from January 2015 to December 2016. This comprehensive dataset offers consistent measurements across a two-year period."}, {"title": "HYPER-PARAMETER SETTINGS AND IMPLEMENTATION DETAILS", "content": "For the hyper-parameter settings of the AR/ARMA Transformer, we use m = 3 Transformer layers, 8 heads, and set the hidden dimension d based on the number of series C, using the empirical formula $d = 16\\sqrt{C}$. We use 4d as the hidden dimension for the feedforward MLP in the Transformer layer. A dropout rate of 0.1 is applied to both the AR term and MA term. We initialize the weights of all linear layers and embedding layers using the GPT-2 weight initialization method, with a normal distribution and a standard deviation of 0.02. For the output projection layers in the attention and MLP, we additionally scale the standard deviation by a factor of 1/$\\sqrt{m}$, aligned with the GPT-2 setting. Normalization layer is applied both before the input to the Transformer and after the Transformer output. We experimented with both standard LayerNorm and RMSNorm as the normalization layer, finding no significant performance differences, so we opted for RMSNorm for lower computational cost. For token input projection, we use a linear layer to project the $L_p$-dimensional token to a d-dimensional input vector. In the output projection, we do not tie the weights between the input and output linear layers. A learnable position embedding that maps the integer labels from 1 to N (the input sequence length) to the corresponding d-dimensional position vectors is used. At the beginning of the model, we apply RevIN to input series $S_1$, subtracting the mean and dividing by the standard deviation for each series. Before outputting the final result, we multiply by the standard deviation and add the mean back. All input series are processed independently and in parallel, merging different series dimensions into the batch size for parallel computation. The random seed used in all the experiments is 2024.\nAll training tasks in this paper can be conducted using a single Nvidia RTX 4090 GPU. The batch size is set to 32. For larger datasets, such as Traffic and PEMS07, we use a batch size of 16 or 8, with 2-step or 4-step gradient accumulation to ensure the effective batch size for parameter updates remains 32. During training, AR/ARMA Transformers are trained using the next-step prediction objective with MSE loss. We use the AdamW optimizer with betas=(0.9, 0.95) and weight decay=0.1, following the GPT-2 settings. For a fair comparison, the same optimizer is used for training baseline models. It is important to note that the baseline models trained with this AdamW setup show significantly better TSF performance compared to those trained with the default Adam optimizer settings. As a result, the baseline performance presented in this paper may exceed the results reported in their original papers. Since this study focuses on long-term last token prediction results, we apply an additional weight factor to the training loss for the last token, multiplying it by N. However, this weighting only slightly affects performance on smaller datasets with fewer data points, such as ETTs, and has little to no effect on larger datasets. Given the minimal impact of this method, the original next-token MSE loss is sufficient for most datasets, without requiring further modifications.\nWe use the same train-validation-test set splitting ratio as in previous studies by Zeng et al. (2023); Nie et al. (2022); Liu et al. (2024a). We also follow the same dataset standardization methods used in these studies. During training, we evaluate the validation and test losses at the end of each epoch, with an early-stopping patience set to 12 epochs. The maximum number of training epochs is 100. We apply a linear warm-up for the learning rate, increasing it from 0.00006 to 0.0006 over the first 5 epochs, and gradually decreasing it in the subsequent epochs."}, {"title": "SUPPLEMENTARY EXPERIMENT RESULTS", "content": "In the following section, we provide the complete experimental data corresponding to the tables in the main text. Additionally, we include extra visualizations to help illustrate the actual behavior of the MA weights."}]}