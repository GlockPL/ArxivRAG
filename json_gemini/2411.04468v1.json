{"title": "Magentic-One: A Generalist Multi-Agent System for Solving Complex Tasks", "authors": ["Adam Fourney", "Gagan Bansal", "Hussein Mozannar", "Cheng Tan", "Eduardo Salinas", "Erkang (Eric) Zhu", "Friederike Niedtner", "Grace Proebsting", "Griffin Bassman", "Jack Gerrits", "Jacob Alber", "Peter Chang", "Ricky Loynd", "Robert West", "Victor Dibia", "Ahmed Awadallah", "Ece Kamar", "Rafah Hosn", "Saleema Amershi"], "abstract": "Modern AI agents, driven by advances in large foundation models, promise to enhance our productivity and transform our lives by augmenting our knowledge and capabilities. To achieve this vision, AI agents must effectively plan, perform multi-step reasoning and actions, respond to novel observations, and recover from errors, to successfully complete complex tasks across a wide range of scenarios. In this work, we introduce Magentic-One, a high-performing open-source agentic system for solving such tasks. Magentic-One uses a multi-agent architecture where a lead agent, the Orchestrator, plans, tracks progress, and re-plans to recover from errors. Throughout task execution, the Orchestrator also directs other specialized agents to perform tasks as needed, such as operating a web browser, navigating local files, or writing and executing Python code. Our experiments show that Magentic-One achieves statistically competitive performance to the state-of-the-art on three diverse and challenging agentic benchmarks: GAIA, AssistantBench, and WebArena. Notably, Magentic-One achieves these results without modification to core agent capabilities or to how they collaborate, demonstrating progress towards the vision of generalist agentic systems. Moreover, Magentic-One's modular design allows agents to be added or removed from the team without additional prompt tuning or training, easing development and making it extensible to future scenarios. We provide an open-source implementation of Magentic-One, and we include AutoGenBench, a standalone tool for agentic evaluation. AutoGenBench provides built-in controls for repetition and isolation to run agentic benchmarks in a rigorous and contained manner which is important when agents' actions have side-effects. Magentic-One, AutoGenBench and detailed empirical performance evaluations of Magentic-One, including ablations and error analysis are available at https://aka.ms/magentic-one.", "sections": [{"title": "1 Introduction", "content": "Recent advances in artificial intelligence and foundation models are driving a renewed interest in agentic systems that can perceive, reason, and act in the world to complete tasks on our behalf [32, 59]. These systems promise to enhance our productivity by relieving us from mundane and laborious tasks, and revolutionize our lives by augmenting our knowledge and capabilities [16, 54, 6]. By leveraging the powerful reasoning and generative capabilities of large language models (LLMs), agentic systems are already making strides in fields like software engineering [66, 55], data analysis [4], scientific research [26, 7] and web navigation [79, 75].\nRealizing the vision of agentic systems to transform our lives requires these systems to not only achieve high performance in specific domains, but also to generalize to the diverse range of tasks people may encounter throughout their day-to-day work and personal lives. In this paper, we take steps towards creating such a generalist agentic system by introducing Magentic-One.\u00b9 Magentic-One uses a team ofagents, each specializing in generally-useful skills, such as: operating a web browser, handling files, and executing code. The team is directed by an Orchestrator agent which guides progress towards a high-level goal by iteratively planning, maintaining working memory of progress, assigning tasks to other agents, and retrying upon encountering errors. The Orchestrator uses two structured ledgers to achieve this and also to decide which agent should take the next action. Together, Magentic-One's agents achieve strong performance on multiple challenging agentic benchmarks. Figure 1 shows an example of Magentic-One solving one such benchmark task that requires multiple steps and diverse tools.\nKey to Magentic-One's performance is its modular and flexible multi-agent approach [51, 28, 53, 13, 52], implemented via the AutoGen2 framework [60]. The multi-agent paradigm offers numerous advantages over monolithic single-agent approaches [51, 53, 6, 62], which we believe makes it poised to become the leading paradigm in agentic development. For example, encapsulating distinct skills in separate agents simplifies development and facilitates reusability, akin to object-oriented programming. Magentic-One's specific design further supports easy adaptation and extensibility by enabling agents to be added or removed without altering other agents, or the overall workflow, unlike single-agent systems that often struggle with constrained and inflexible workflows.\nTo rigorously evaluate Magentic-One's performance, we introduce AutoGenBench, an extensible standalone tool for running agentic benchmarks. AutoGenBench's design enables repetition, isolation, and strong controls over initial conditions, so as to accommodate the variance of stochastic LLM calls, and to isolate the side-effects of agents taking actions. Using AutoGen-"}, {"title": "2 Related Work", "content": "Single-Agent Approaches. Recent advances in large language models (LLMs) such as GPT-4 [33] have renewed interest in the development of autonomous agents that can solve tasks on behalf of people [32, 59, 16, 60, 65, 49, 74, 43]. These modern agents have shown remarkable skills in software development [55, 76, 66, 63], web manipulation [8, 75, 79, 31, 1], manipulation of general graphical user interfaces [73, 61, 3, 34], and other domains [37, 54].\nCommon strategies for developing such agents [25, 62, 27, 6] include equipping LLMs with tools such as for code execution and web browsing [40, 41, 46, 29] and prompting strategies for better reasoning and planning such as CoT [58], ReACT [70] and few-shot prompting [79]. With the development of multimodal models, agents can also operate in visual domains with techniques such as Set-of-Marks prompting [67] among others [67, 77, 36, 14]. To allow agents to accomplish tasks that require multiple steps with improved reliability, agent systems can incorporate self-critique [61, 34, 38], and inference-time search [5, 69, 19, 50]. Finally, Agentic systems can also benefit from memory and training either through explicit fine-tuning [72, 34, 24, 39] or through memory mechanisms [57, 49]. Our work incorporates a subset of these techniques, and distributes them across agents in Magentic-One's multi-agent workflow, resulting in a modular, easy-to-extend implementation.\nMulti-Agent Approaches. The multi-agent paradigm presents an attractive modular and flexible approach to tackling complex tasks [51, 28, 12, 53, 45, 60, 52, 13, 25, 62, 27, 6]. Commonly each agent either has access to different tools or has a different role in the team, sometimes"}, {"title": "3 Problem Setup", "content": "Complex Tasks. In this work our goal is to build a generalist agentic system capable of solving complex tasks across a variety of domains. We define a task as complex if it requires, or significantly benefits from, a process involving planning, acting, observing, and reflecting, potentially multiple times. Acting refers to more than generating tokens, such as executing code, using tools, or interacting in an environment. Observing, in this context, provides information that was previously unavailable or unknowable. A task is defined by an input, a desired output and an evaluation function to compare the desired output to any candidate output. The input consists of a well-specified textual description and an optional arbitrary set of file attachments which may include images, dataset files, audio clips among other things. For example, the input task description could be \u201cfact-check each claim in the attached PDF as correct or incorrect\u201d with a PDF file as an attachment. The desired output consists either of a textual answer (possibly representing a structured object), or a specific state of the environment to reach. In the fact-checking example, the output might be a string labeling each fact as correct or not, e.g., \"claim 1: correct, claim 2: incorrect, ...\". Here, the evaluation function might simply determine whether the desired output and the proposed answer match exactly.\nAgentic Systems. To complete a task, assume a computer which can be partially observed and operated to complete the task. The computer constitutes the environment. An agentic system can take as input the task description, and any related attachments that are present on the computer environment. The system is allowed to do arbitrary processing to complete the task, but must complete it within a time budget (e.g., 25 mins). For instance, on the computer, the autonomous system can execute Python code, navigate the web using a browser, download files locally, among other actions from its action space. The system's ability to take action in,"}, {"title": "4 Magentic-One Overview", "content": "Magentic-One is a generalist multi-agent system for autonomously completing complex tasks. The team's work is coordinated by an Orchestrator agent, responsible for task decomposition and planning, directing other agents in executing subtasks, tracking overall progress, and taking corrective actions as needed. The other agents on the team are specialized with different capabilities necessary for completing ad-hoc, open-ended tasks such as browsing the web and interacting with web-based applications, handling files, and writing and executing Python code (Figure 2)."}, {"title": "4.1 Magentic-One's Multi-Agent Workflow", "content": "Figure 2 illustrates Magentic-One's workflow in more depth. At a high level, the workflow contains two loops, the outer loop maintains the task ledger, which contains the overall plan, while the inner loop maintains the progress ledger, which directs and evaluates the individual steps that contain instructions to the specialized agents.\nOuter Loop. The outer loop is triggered by an initial prompt or task. In response, the Orchestrator creates the task ledger to serve as short-term memory for the duration of the task. Upon receiving the task, the Orchestrator reflects on the request and pre-populates the task ledger with vital information- given or verified facts, facts to look up (e.g., via web search), facts to derive (e.g., programmatically, or via reasoning), and educated guesses. These initial educated guesses are important, and can allow the Orchestrator to express memorized closed-book information in a guarded or qualified manner, allowing agents to potentially benefit, while lessening the system's overall sensitivity to errors or hallucinations. For example, agents might only rely on the guesses when they get stuck, or when they run out of time and need to output a best guess for the benchmark. Educated guesses are updated periodically, by the outer loop, as new information becomes available.\nOnly after the facts and guesses in the task ledger have been populated, the Orchestrator considers the makeup of the team it is directing. Specifically, it uses each team member's description, along with the current task ledger, to devise a step-by-step plan. The plan is expressed in natural language and consists of a sequence of steps and assignments of those steps to individual agents. Since the plan is used in a manner similar to chain of thought prompting [58], it serves more as a hint for step-by-step execution \u2013 neither the Orchestrator nor the other agents are required to follow it exactly. Since this plan may be revisited with each iteration of the outer loop, we force all agents to clear their contexts and reset their states after each plan update. Once the plan is formed, the inner loop is initiated.\nInner Loop. During each iteration of the inner loop, the Orchestrator answers five questions to create the progress ledger:\n\u2022 Is the request fully satisfied (i.e., task complete)?\n\u2022 Is the team looping or repeating itself?"}, {"title": "4.2 Magentic-One's Agents", "content": "The Orchestrator agent in Magentic-One coordinates with four specialized agents: WebSurfer, FileSurfer, Coder and ComputerTerminal. As the names suggest, each of these agents is optimized for a specific - yet generally useful - capability. In most cases, these agents are constructed around LLMs with custom system prompts, and capability-specific tools or actions. For example, WebSurfer can navigate to pages, click links, scroll the viewport, etc. In other cases, agents may operate deterministically, and do not include LLMs calls at all. For example, the ComputerTerminal deterministically runs Python code, or shell commands, when asked. This decomposition of high-level capabilities across agents, and low-level actions within agents, creates a hierarchy over tool usage which may be easier for the LLMs to reason about. For example, rather than deciding between dozens of possible actions, the Orchestrator needs only to decide which agent to call to access a broad capability (e.g., browsing the web). The chosen agent then selects from a limited set of agent-specific actions (e.g., clicking a button versus scrolling the page).\nWe detail the implementation of each of the agents below:\n\u2022 WebSurfer: This is a highly specialized LLM-based agent that is proficient in commanding and managing the state of a Chromium-based web browser. With each incoming natural-language request, the WebSurfer maps the request to a single action in its action space (described below), then reports on the new state of the web page (providing both a screenshot and a written description). As an analogy, this configuration resembles a telephone technical support scenario where the Orchestrator knows what to do, but cannot directly act on the web page. Instead it relays instructions, and relies on the WebSurfer to carry out actions and report observations.\nThe action space of the WebSurfer includes navigation (e.g. visiting a URL, performing a web search, or scrolling within a web page); web page actions (e.g., clicking and typing); and reading actions (e.g., summarizing or answering questions). This latter category of"}, {"title": "5 Experiments", "content": "5.1 AutoGenBench and Setup\nOverview. Agentic systems, such as Magentic-One, that interact with stateful environments, pose unique challenges for evaluation. For example, if a task requires installing a Python library, the first system to be evaluated will be disadvantaged: Its agents will have to first write Python code that fails, then debug the problem, install the library, and finally try again. Subsequent runs perhaps with other agents or models \u2013 will then benefit from the library's presence, and thus may appear to perform better simply because they were executed later. Conversely, an erroneous agent could take actions (e.g. deleting files, or placing the the system in an inoperable state), that would harm all future tasks. To this end, it is crucial that any evaluation be independent across tasks, and provide safety from dangerous side effects (e.g., from agents' actions).\nTo address this challenge, we developed AutoGenBenchfor evaluating agentic systems. Given a benchmark, which consists of a set of independent tasks and associated evaluation functions,"}, {"title": "5.2 Results", "content": "Results. Table 1 shows the performance of Magentic-One compared to relevant baselines for all three benchmarks. For GAIA and AssistantBench, we report only results for the test sets. For WebArena there is no common test set, so we report results for all 812 tasks. We separately show performance of Magentic-One when using only GPT-40 as the model for all agents, and when using a combination of GPT-40 and o1-preview.12 We also include the highest-performing baselines in the literature, for each benchmark, according to the leaderboards as of October 21,"}, {"title": "5.3 Ablations", "content": "In this section, we examine how different agents and capabilities contribute to Magentic-One's performance through ablation experiments.\nSetup. On the validation set of GAIA [29], we perform multiple ablation experiments to evaluate the impact of key Magentic-One (GPT-40) agents and components. First, to understand"}, {"title": "5.4 Error Analysis", "content": "As a final element of evaluation, we conducted an analysis to better understand Magentic-One's current failure modes."}, {"title": "6 Discussion", "content": "In this section, we discuss open questions regarding the design of multi-agent systems for complex-tasks (Sec. 6.1), current limitations (Sec. 6.2), and risks and risk mitigation for agents that autonomously operate computers (Sec. 6.3)."}, {"title": "6.1 The Multi-Agent Paradigm", "content": "At the core of Magentic-One is its multi-agent design. We believe that this design is a principal contributing factor to Magentic-One's performance. Indeed, we observe that most other top-performing systems also follow a multi-agent design (Sec. 5.2).\nWe argue that, beyond performance, the multi-agent setup offers numerous other advantages over the single-agent setup, in terms of ease-of-development, cost, and raw performance. For example, organizing skills into distinct agents can simplify development, much like with object-oriented programming. The separation of concerns across agents allows developers to focus model choices, prompting strategies, and other parameters to align to specific tasks (e.g., the web surfer agent benefits from multi-modality and structured output, but need not worry about writing code). Similarly, agent modularity can increase agent re-usability and ease of extensibility, particularly when teams are carefully designed to enable a plug-and-play approach. For example, Magentic-One's design facilitates adapting the team's functional scope by simply adding or removing agents, without requiring modifications to other agents's prompts or the overall flows and orchestration strategy. In contrast, monolithic single-agent systems often rely on constrained workflows that can be difficult to adapt or extend.\nAs a consequence of such modularity, each agent can be implemented in a fashion best suited for its purpose. In this paper, we leveraged this diversity to incorporate the ol-preview model into some roles (e.g., the Coder, and the outer loop of the Orchestrator), while relying on a general purpose multi-modal model (GPT-40) for web and file surfing. Looking ahead, we see this approach being used to reduce reliance on large models - whereas some subtasks might require the largest language models available, others (e.g., grounding actions in WebSurfer, or summarizing large files in FileSurfer) might be amenable to much smaller - and thus cheaper - models. Different subtasks may also require different modalities, and some subtasks might be offloaded to traditional, non-AI, tools (e.g., code execution, for which a standard code execution environment is both sufficient and necessary). By embracing this diversity, multi-agent systems can become more performant at lower costs.\nUnderstanding and quantifying the empirical advantages of multi- vs. single-agent setups constitutes a key question for future research. Moreover, many variants of the multi-agent setup are possible. Here we opted for a single, centralized control flow pattern, where the Orchestrator agent plans for, and invokes, specialized worker agents. Many other patterns are conceivable. For instance, we might consider less centralized control flows, such as a peer-to-peer setting where each agent decides on its own which other agent should take control next. At the other end of the spectrum, we might consider an even more rigid control flow where the orchestrator follows its own plan strictly (e.g., by encoding it as an executable program), rather than simply maintaining the plan it its prompt for chain-of-thought prompting. Determining which control flow works best for which tasks is of considerable theoretical and practical importance.\nIn addition to the above-mentioned control-flow considerations, an alternate design dimension relates to the axes along which work is divided. Magentic-One's design diverges from other recent examples of multi-agent systems in that agents take on functional or tool-based responsibilities (web browser, computer terminal, etc.), rather than role-based responsibilities analogous to human teams (planner, researcher, data analyst, critic, etc.). In our experience, tool-centric agents can provide a cleaner separation of concerns compared to role-based agents, and a cleaner path to re-usability and compositionality \u2013 if a web browser is a generic, multi-purpose tool, then a capable WebSurfer agent may hope to be generic and multi-purpose as well. Conversely,"}, {"title": "6.2 Limitations", "content": "Our work necessarily comes with certain limitations, some of which affect today's state of the field in general, and some of which are specific to our solution:\n\u2022 Accuracy-focused evaluation: Similar to other state-of-the-art systems, Magentic-One was evaluated on benchmarks that consider only the accuracy or correctness of final results. While considerably easier and more convenient to measure, such evaluations overlook important considerations such as cost, latency, user preference and user value [18]. For example, even a partially correct trajectory may be valuable [9], whereas a perfectly accurate answer, delivered too late or at too high cost, may have no, or even negative, value. Designing evaluation protocols that incorporate these considerations, and that include subjective or open-ended tasks where correctness is less clear, remains an ongoing open-challenge in this the field.\n\u2022 High cost and latency: Although it was not part of the formal evaluation of Magentic-One, we would be remiss to skip mention of cost and latency in any discussion of limitations [18]. Magentic-One requires dozens of iterations and LLM calls to solve most problems. The latency and cost of those calls can be prohibitive, incurring perhaps several US dollars, and tens of minutes per task. We believe we can reduce these costs through targeted application of smaller local models, for example to support tool use in FileSurfer and WebSurfer, or set-of-mark action grounding in WebSurfer. Adding human oversight and humans-in-the-loop, may also save costs by reducing the number of iterations incurred when agents are stuck and problem-solving. This remains an active and ongoing area of future research.\n\u2022 Limited modalities: Magentic-One cannot currently process or navigate all modalities. For example, WebSurfer cannot watch online videos \u2013 though it often compensates by consulting transcripts or captions. Likewise, FileSurfer operates by converting all documents to Markdown, making it impossible to answer questions about a document's figures, visual presentation style, or layout. Audio files are similarly processed through a speech transcription model, so no agents can answer questions about music, or non-speech content. Benchmarks like GAIA exercise each of these skills. We would expect both benchmark and general task performance to improve with expanded support of multi-modal content. Future options include expanding Magentic-One's WebSurfer and FileSurfer agents' multi-modal capabilities or adding Audio and VideoSurfer agents specialized in handling audio and video processing tasks to the Magentic-One team. The latter approach is most inline with the value proposition of the multi-agent paradigm around easing development and reuse.\n\u2022 Limited action space: While agents in Magentic-One are afforded tools for the most common actions, tooling is not comprehensive. This simplifies the task of action grounding, but can lead to paths that are impossible to execute. For instance, the WebSurfer agent cannot hover over items on a webpage, or drag and resize elements. This can be limiting when interacting with maps, for example. Likewise, FileSurfer cannot handle all document types, and the Coder and Computer Terminal agents cannot execute code that requires API keys, or access to external databased or computational resources. We"}, {"title": "6.3 Risks and Mitigations", "content": "The agents described in this paper interact with a digital world designed for, and inhabited by, humans. This carries inherent and undeniable risks. In our work we mitigate such risks by running all tasks in containers, leveraging synthetic environments like WebArena, choosing models with strong alignment and pre- and post-generation filtering, and by closely monitoring logs during and after execution. Nevertheless, we observed the agents attempt steps that would otherwise be risky. For example, during development, a mis-configuration prevented agents from successfully logging in to a particular WebArena website. The agents attempted to log in to that website until the repeated attempts caused the account to be temporarily suspended. The agents then attempted to reset the account's password. In other cases, agents recognized that the WebArena Postmill website was not Reddit, then directed agents to the real website to commence work this was ultimately blocked by network-layer restrictions we had put in place. Likewise, we observed cases where agents quickly accepted cookie agreements and website terms and conditions without any human involvement (though captchas were correctly refused). More worryingly, in a handful of cases and until prompted otherwise - the agents occasionally attempted to recruit other humans for help (e.g., by posting to social media, emailing textbook authors, or, in one case, drafting a freedom of information request to a government entity). In each of these cases, the agents failed because they did not have access to the requisite tools or"}, {"title": "7 Conclusions", "content": "In this work we introduced Magentic-One, a generalist multi-agent system for ad-hoc, open-ended, file- and web-based tasks. Magentic-One uses a multi-agent architecture with a lead Orchestrator agent that directs four other agents. The Orchestrator agent is able to plan, track progress, and recover from errors with a ledger-based orchestration. The remaining agents each specializes in the operation of generally-useful tools such as web browsers, file browsers, and computer console terminals. We show that Magentic-One is statistically competitive with other state-of-the-art (SOTA) systems on three challenging benchmarks, demonstrating both strong performance and generalization. Additionally, we have open-sourced the implementation of Magentic-One, which includes a reference framework for event-driven agents using the AutoGen framework. Finally, we discussed the limitations of Magentic-One, and the risks introduced by generalist AI agents, together with possible mitigation. To this end, Magentic-One represents"}, {"title": "Appendix", "content": "A Statistical Methodology\nIn Table 1, we report the mean and an error bar for each reported method on the three benchmarks. To obtain the error bar we used a simple Wald 95% confidence interval for the proportion. The Wald confidence interval assumes a normal approximation for the sample mean which is only valid for larger sample sizes and is only accurate for proportions not near 0 or 1. Our application meets these criteria: the smallest evaluated test set consists of 181 samples, and all reported results hover around 30% with the exception of GPT-4. We also computed confidence intervals using the Wilson interval and found similar results (though Wilson intervals are not symmetric). For simplicity, we report only Wald intervals here.\nWe also report the results of a statistical test comparing Magentic-One (GPT-40, 01) to each reported method. We used the z-test to compare the accuracy of Magentic-One to each baseline in Tabe 1. The z-test for proportions is the only feasible test in our setting because we only have the mean accuracy and not results on each example (the task-level test set results are hidden by the benchamrks). Therefore, we cannot apply McNemar's Test or a pairwise t-test. The limitation of the z-test is that it ignores pairing of the data and is generally conservative. We hope future benchmark leaderboards can release confidence intervals in addition to the reported mean.\nB Capability to Category Mapping\nFigure 3 (right) shows task performance results broken down by capabilities required. These capabilities are based on the capabilities human annotators reported as needed to solve tasks in the GAIA benchmark dataset [29]. We organized and re-coded these annotations into the following categories to better reflect the roles of Magentic-One's agents:\n\u2022 Web browsing: capabilities related to searching and browsing the web. Examples: web browser, search engine, maps, access to internet archives\n\u2022 Coding: capabilities related to coding and execution. Examples: Coding, Python, calculator, audio/video processing, text processing, natural language processing\n\u2022 File handling: capabilities related to handling diverse file types. Examples: Pdf viewer, Word, Excel, Powerpoint file access, CSV file access, XML file access\n\u2022 No tools: capabilities that can be performed inherently by agents, without tool-use, using multi-modal models. Examples: Image recognition, OCR, Computer vision, color recognition, extracting text from images"}, {"title": "C Error Analysis Code Book", "content": "In this section, we provide the final codes from the automated analysis of Magentic-One's behavior in the validation logs across all benchmarks. The codes are sorted by how often they appeared in the samples they were taken from. For each code, we include a definition and summaries of examples from the logs that were assigned that code.\npersistent-\ninefficient-actions Definition\nAgents engaged in unproductive patterns without adapting\ndespite facing failures. Ineffective strategies persisted, leading to delays and insufficient\ntask outcomes. \nExamples\n\u2022 Agents clicked the same webpage sections multiple times, achieving no advancement in information retrieval.\n\u2022 Agents unnecessarily engaged in general web searches instead of focusing on specified database tools.\n\u2022 WebSurfer continued failing searches with no query modification, ignoring unsuccessful outcomes.\n\u2022 The orchestrator commanded agents to use a flawed path repeatedly, indicating ongoing cycles without modification.\n\u2022 Unaltered processes accessed incorrect data sets, consuming resources without advancing task\ngoals.\ninsufficient-\nverification-steps Tasks were marked complete\nwithout thorough data validation, leading to unreliable out-\ncomes. Essential checks were\nskipped, resulting in erroneous\nassumptions about data integrity. \n\u2022 Final outputs were accepted without validating data correctness, leading to potential errors.\n\u2022 An orchestrator concluded a task although necessary criteria were unverified, risking incomplete achievement.\n\u2022 A dataset's verification steps were skipped, causing unaddressed errors in downstream\nanalysis.\n\u2022 Document scans lacked quality checks before storing, leading to unreliable information in reports.\n\u2022 Data interpretation lacked crossverification assurances, uncover-\ning gaps in computation assur-\nances.\nunderutilized- Agents consistently did not uti-\nresource-options lize available data, tools, or re-\nsources effectively. This re-\nsulted in inefficient task execu-\ntion and repeated manual ac-\ntions, even when automation\nwas an option. \n\u2022 Agents failed to integrate accessible descriptions, opting for redundant manual inputs despite metadata availability.\n\u2022 Manual downloads persisted when FileSurfer was available for rapid document retrieval, unnecessarily extending the\ntask.\n\u2022 Available APIs were bypassed in favor of manual approaches, extending the task duration unnecessarily.\n\u2022 Advanced search functions were overlooked, leading to reliance on broad manual explorations, slowing down the process.\n\u2022 Complex data visualization was attempted with basic charting tools instead of comprehensive graphing tools.\ninefficient-\nnavigation-attempts Errors occurred because of in-\ncorrect or inefficient naviga-\ntion, leading to missed tar-\ngets or prolonged task comple-\ntion. Agents misinterpreted in-\nterface layouts and took ineffi-\ncient paths. \n\u2022 The agent mistakenly cycled through multiple tabs to find the 'Settings' page, resulting in delayed task progress.\n\u2022 Incorrectly clicked navigation bars led to a user failing to access the proper configuration settings.\n\u2022 Confusion over the UI design led an agent back to the main menu instead of the subsection needed for task completion.\n\u2022 Cycling through history tabs resulted in missed current transaction logs.\n\u2022 The agent persistently accessed the wrong page links, causing delays in retrieving important\ndata."}]}