{"title": "Can Machines Learn the True Probabilities?", "authors": ["Jinsook Kim"], "abstract": "When there exists uncertainty, AI machines are designed to make decisions so as to reach the best expected outcomes. Expectations are based on true facts about the objective environment the machines interact with, and those facts can be encoded into Al models in the form of true objective probability functions. Accordingly, AI models involve probabilistic machine learning in which the probabilities should be objectively interpreted. We prove under some basic assumptions when machines can learn the true objective probabilities, if any, and when machines cannot learn them.", "sections": [{"title": "1. Introduction", "content": "In the standard AI model under uncertainty, how to measure the degree of uncertainty matters. This paper is about treating such measures in the form of probabilities. In particular, we focus on the true objective probabilities, if any. There are various probabilistic contexts in which the true objective probabilities matter. For example, causal relations of physical events are widely regarded as objective features of the world. Therefore, when causal relations are to be understood in terms of probabilities mainly due to various regularity issues, a probabilistic causal model should include an objective probability function that measures the true objective values about our world.\nThis paper addresses the question of whether machines can learn the true objective probabilities from the data to perform such probabilistic reasoning. Under some basic assumptions, we prove that machines can learn the true objective probabilities if and only if the probabilities are directly observable by them. Roughly speaking, a true probability is directly observable by a machine when it can calculate the probability by the empirical frequency of a true popula-\nUnderwood International College, Yonsei University, Seoul, Korea. Correspondence to: Jinsook Kim <jki76364@gmail.com>.\nProceedings of the 41st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s)."}, {"title": "2. Notations and Definitions", "content": "In this section, we define some main concepts, including \u201cmachine learning\u201d and \u201ctrue objective probability\". Adopting terminologies from (Nilsson, 2011) and (Boolos et al., 2002), let us first define a machine as an artifact or device that can effectively calculate or compute any target function if there exist definite and explicit instructions to do so in principle. Since we focus on probability functions in this paper, we particularly mean by \"an effectively calculating or computing device\" a machine that can in principle assign a probability measure (a value of a probability function) to each state (an argument of the probability function) in a given domain, an event space of a sigma-field.\nDefinition 2.1. A function is effectively calculable or computable when there are definite and explicit instructions, following which its functional value can be calculated in principle for any given argument. (Boolos et al. (2002))\nTwo things merit to be taken into account with Definition 2.1. First, this notion of effective calculation or computation is an ideal one with no practical limits on time, expense, etc., necessary to calculate. Therefore, a proof of the limitation on effective calculation or computation of any function will imply a fundamental limit on computability that cannot be overcome by any practical real machine. Second, as (Kozen, 1997) points out, this notion is an informal one, something that is supposed to be captured in common by all formalisms such as computation by Turing machines, by the \\( \\lambda \\)-calculus and by the \\( \\mu \\)-recursive method, etc. Accordingly, once we adopt this notion of effective calculation or computation to define \"learning\", we can be flexible about which formalism would be encoded as instructions to complete a given learning task.\nNow, whatever such formalism is, machines can learn only if there exist some instructions followed by them to complete their tasks. So we can prove that it is impossible for machines to learn any target function under certain conditions in the following way: we first suppose that there exist some successful instructions to be encoded into machine programming to learn any given function under the conditions. We then show that this supposition leads to a conclusion that is impossible to satisfy. We thereby conclude that there cannot exist such instructions for the given function and, accordingly, that machines cannot learn it. This is a simple but clear way of proving the impossibility of learning without being committed to any complex procedure of constructing any formalism such as a Turing machine or \\( \\lambda \\)-calculus, etc.\nDefinition 2.2. A machine learns when it succeeds in effectively calculating or computing a target function, if any, after processing possibly infinite amounts of data.\nThe phenomenon of learning must be at least computational"}, {"title": "3. Kinds of Probabilities and Learning", "content": "Broadly speaking, probabilities can be divided into two kinds, subjective and objective ones. Subjective probability, say \\( I(A_{t+1}|B_t) \\), depends on each person's belief and thus possibly varies from person to person, while objective one, say \\( P(A_{t+1}|B_t) \\), does not.\nThe standard theory of subjective probability was first developed by Ramsey and then further by De Finetti and Savage. Subjective probability is designed to represent a degree of belief possessed by a subject, say some person or, if possible, a machine. Hence subjective probability represents whatever is in any one's mind upon anything as long as his/her belief system is coherent, and so can be assigned even to what is merely imagined. For example, while arguing for cogito, ergo sum, (Descartes, 2008) imagined an\nevil spirit that has devoted all its efforts to deceiving him. Descartes can assign some value of subjective probability to his imagination on the evil spirit in accordance with how likely it is to him that the imagination can be realized in this world, as long as Descartes' belief system remains coherent.\nIn contrast, objective probability, if any, is what must be determined by objective features of our world that do not vary from person to person. The best way to understand objective probability is to consider examples. Following (Maher, 2010), for example, suppose that a coin has the same face on both sides, that is, two-headed or two-tailed. When this coin is tossed infinitely often, its relative frequency surely converges to 1 or 0. Hence the limiting relative frequency here is either 1 or 0, depending on how our world turns out to be, which is an objective matter, and not on whatever we believe.\nIt should be noted that subjective and objective probabilities are conceptually bifurcated in two important ways. First, recall that subjective probability represents an aspect of someone's subjective belief, while objective probability does not. Hence the subjective probability of Descartes' demon is positive as long as it is believed at any degree that it could exist in our world. However, this does not necessarily imply that the true objective probability of Descartes' demon is positive, since it might be the case that such a demon is possible only in one's imagination but impossible in our real world. We will return to this potential bifurcation between subjective and objective probability in Section 4.1.\nSecond, there exists an asymmetric relation between subjective and objective probability: although the subjective probability of Descartes' demon does not necessarily bind its objective probability, the converse holds. (e.g. (Lewis, 1980)) That is, once it is proven/assumed by any agent that the true objective probability of Descartes' demon is, say zero, then its subjective probability of the same agent is bound to this proven/assumed result on the objective probability and thus must be zero as well. From this asymmetric relationship, we derive Lemma 4.23 in Section 4.2."}, {"title": "3.1. Subjective vs. Objective Probabilities", "content": "Broadly speaking, probabilities can be divided into two kinds, subjective and objective ones. Subjective probability, say \\(I(A_{t+1}|B_t)\\), depends on each person's belief and thus possibly varies from person to person, while objective one, say \\(P(A_{t+1}|B_t)\\), does not.\nThe standard theory of subjective probability was first developed by Ramsey and then further by De Finetti and Savage. Subjective probability is designed to represent a degree of belief possessed by a subject, say some person or, if possible, a machine. Hence subjective probability represents whatever is in any one's mind upon anything as long as his/her belief system is coherent, and so can be assigned even to what is merely imagined. For example, while arguing for cogito, ergo sum, (Descartes, 2008) imagined an evil spirit that has devoted all its efforts to deceiving him. Descartes can assign some value of subjective probability to his imagination on the evil spirit in accordance with how likely it is to him that the imagination can be realized in this world, as long as Descartes' belief system remains coherent.\nIn contrast, objective probability, if any, is what must be determined by objective features of our world that do not vary from person to person. The best way to understand objective probability is to consider examples. Following (Maher, 2010), for example, suppose that a coin has the same face on both sides, that is, two-headed or two-tailed. When this coin is tossed infinitely often, its relative frequency surely converges to 1 or 0. Hence the limiting relative frequency here is either 1 or 0, depending on how our world turns out to be, which is an objective matter, and not on whatever we believe.\nIt should be noted that subjective and objective probabilities are conceptually bifurcated in two important ways. First, recall that subjective probability represents an aspect of someone's subjective belief, while objective probability does not. Hence the subjective probability of Descartes' demon is positive as long as it is believed at any degree that it could exist in our world. However, this does not necessarily imply that the true objective probability of Descartes' demon is positive, since it might be the case that such a demon is possible only in one's imagination but impossible in our real world. We will return to this potential bifurcation between subjective and objective probability in Section 4.1.\nSecond, there exists an asymmetric relation between subjective and objective probability: although the subjective probability of Descartes' demon does not necessarily bind its objective probability, the converse holds. (e.g. (Lewis, 1980)) That is, once it is proven/assumed by any agent that the true objective probability of Descartes' demon is, say zero, then its subjective probability of the same agent is bound to this proven/assumed result on the objective probability and thus must be zero as well. From this asymmetric relationship, we derive Lemma 4.23 in Section 4.2."}, {"title": "3.2. What is Implied by Learning the True Objective Probabilities?", "content": "As we pointed out in Section 2, learning is the phenomenon of knowledge acquisition, and knowledge must be at least a true representation. In the case of human beings, the requirement of true representation is expressed as the requirement that (propositional) knowledge be at least a true belief (e.g. (Hintikka, 1962), (Moore, 1985)). What then is the counterpart of such a requirement for machines?"}, {"title": "4. Can Machines Learn the True Probabilities?", "content": "In general, if a machine achieves computational success at t by learning, what the machine represents by learning must be at least true at that time. Then we denote the true representation of the machine about what is learned by the \"true belief\" of the machine, a legitimate analogue to the true belief of human beings. It is a belief analogue, for we haven't yet shown that machines have minds or that they have the same kinds of mental representations as human beings. It is nevertheless a legitimate belief analogue, since the computational models of machine intelligence are based on understanding human intelligence. (e.g. (Pearl, 2018), (Russell, 1998), (Valiant, 1984; 2008))\nThat said, let us discuss the relation between belief and learning on the machine side: the knowledge acquired by machine learning must be at least a true belief. In (Hintikka, 1962), the knowledge of a person i refers to the knowledge of that person i on any proposition A. Likewise, machine's learning of the true objective probability P here refers to the knowledge acquired by any machine on the probabilistic proposition \\( A_P \\). If a machine learns the true probability as \\( a \\), then the probabilistic proposition \\( A_P \\) amounts to that the true objective probability P, if any, is what the very machine calculates as \\( a \\). Here, we convert the non-propositional learning into propositional learning.\nNow, just as a person i's knowledge on proposition A must satisfy the necessary condition that the person i's belief in A is true, machine learning of the true probability P must also satisfy the condition that the belief in \\( A_P \\) of the machine is true. Note here that such a belief in \\( A_P \\) is true when what has been calculated by the machine is indeed equal to the true probability P. Now, this calculated probability function by a machine is nothing more than the subjective probability of the machine. Therefore, the necessary condition for machine learning of true probability P requires a machine to hold a true belief whose truth condition is satisfied when its subjective probability \\( \\Pi \\) is, in fact, in congruence with the true objective probability P. In short, if a machine learns the true objective probability P, then the subjective probability \\( \\Pi \\) of the machine is actually equal to the true probability P."}, {"title": "4.1. Learning the True Probabilities and Calibration", "content": "Let us start with a simple example in which a machine is trying to learn the true probability that it will rain tomorrow. A forecasting system is said to be well-calibrated if it assigns probability, say 30%, to rainy events in a test set whose long-term proportion that actually rains is 30%. According to (Dawid, 1982), a forecasting machine is self-assured that its fairly arbitrary test set of forecasts is well-calibrated. This is Theorem 4.1. In addition, we prove in Theorem 4.6 that if the machine learns the true probability, then this machine's forecasting is truly guaranteed to be well-calibrated.\nNow, let us assume that a machine has its own (not necessarily true in our context) probability distribution \\( \\Pi \\) defined over \\( B_\\infty = \\bigvee_{t=0}^{\\infty} \\beta_t \\), where \\( \\beta_t \\) is denoted by the totality of the true facts up to day t. The probability forecasts \\( \\Pi(A_{t+1}|\\beta_t) \\) it makes on day t are for events \\( A_{t+1} \\)'s in \\( \\beta_{t+1} \\) and are \\( \\beta_t \\)-measurable. For each day t we have an arbitrary associated event \\( A_t \\in \\beta_t \\), say the event of raining on day t. We denote the indicator of \\( A_{t+1} \\) by \\( Y_{t+1} = 1\\{A_{t+1}\\} \\), and introduce \\( \\hat{Y}_{t+1} = \\Pi(A_{t+1}|\\beta_t) \\), the probabilistic forecast of machines on day t+1. In addition, we introduce the new indicator variables \\( \\xi_1, \\xi_2, ..., \\) at choice to denote the inclusion of any particular day t in the test set where \\( \\xi_t = 1 \\) if the day t is included in the test set and \\( \\xi_t = 0 \\) otherwise. Now, if we set the selection criterion to include any day into the test set as the assessed probability \\( \\alpha \\) on day t, then we have the following theorem."}, {"title": "4.2. Can Machines Learn the True Probabilities?", "content": "Theorem 4.8. It is impossible to obtain a joint distribution for an infinite sequence of events that could have the well-calibration property with subjective probability 1.\nThe basic idea in the proof of Theorem 4.8 starts with constructing a counterexample in which the true probability function P is deviated infinitely often from the subjective probability function \\( \\Pi \\) in such a way that the well-calibration property does not hold any longer.\nFollowing (Oakes, 1985), let P be such as \\( P(A_t|B_{t-1}) = f(\\Pi(A_t|B_{t-1})) \\), with the function \\( f([0,1]) \\rightarrow [0,1] \\) being defined by \\( f(x) = x + \\epsilon (0 \\leq x \\leq \\frac{1}{2}), f(x) = 1 - x (\\frac{1}{2} < x < 1) \\) for any event \\( A_t \\). Then, under P with \\( P(\\Upsilon_{Ix} = 1) = f(a) \\) where \\( \\Upsilon_1 = a \\) for a subsequence \\( \\{t: t = I_1, I_2, ...\\} \\) and \\( \\Upsilon_{Ik} \\)'s form a Bernoulli sequence, the well-calibration property does not hold.\nDue to this counterexample from (Oakes, 1985), the machine forecaster cannot exclude the possibility that its test set may be mis-calibrated, and thus the machine can-"}, {"title": "5. Conclusion", "content": "We have discussed so far when machines can learn the true probabilities and when they cannot. In summary:\n\\( \\exists a^* \\) such that P(Nature is perverse with \\( a^* \\) ) > 0 by Theorem 4.19.\nNow that Nature is perverse at least with one forecast \\( a^* \\), (i) Nature is uniformly perverse: machines cannot learn by Theorem 4.20.\n(ii) Nature is selectively perverse: \\( \\exists t_s \\) for each \\( a_0 \\) such that P( Nature is perverse with \\( a_0 \\) ) = 0 by Lemma 4.28.\nThen under (ii),\n(ii-1) Machines are not self-assured of the \\( t_s \\): machines cannot learn by Corollary 4.30.\n(ii-2) Machines are self-assured of the \\( t_s \\):\nThen under (ii-2),\n(ii-2-1) \\( t_s \\) actually does not arrive: machines cannot learn by Theorem 4.32.\n(ii-2-2) \\( t_s \\) indeed arrives: machines can learn and this is the only case in which machines can learn by Theorem 4.35 and Theorem 4.36.\nBefore we close this section, let us add a few remarks. First, we emphasize that in this paper we have focused on the notion of \"machine learning\" that is not just a technical terminology, understood as an identification of a target function,"}, {"title": "A. Proofs for Lemmas, Theorems and Corollaries", "content": "Proof of Theorem 4.1 A proof of Theorem 4.1 is suggested in (Dawid, 1982). A simpler one is as follows: Let \\( X_t = (\\sum_{j=1}^{t} \\xi_j)^{-1} \\xi_t (Y_t - \\hat{Y}_t) \\). Since \\( (\\sum_{j=1}^{t} \\xi_j)^{-1} \\xi_t \\), and \\( Y_t \\) are \\( B_{t-1} \\)-measurable, it follows that \\( E(X_t|B_{t-1}) = 0 \\) where E is taken with respect to \\( \\Pi(\\cdot|B_{t-1}) \\) and so that \\( \\sum X_t \\) is a martingale adapted to \\( B_{k-1} \\). Also, \\( E((\\sum X_t)^2) = \\sum E(X_t^2) = \\sum \\lambda \\cdot E\\{((\\sum \\xi_j)^{-1} \\xi_t(Y_t - \\hat{Y}_t))^2\\} < \\lambda^2 \\), because \\( Y_t \\) is an indicator variable and so \\( var(Y_t|B_{t-1}) \\) is uniformly bounded above by some \\( \\lambda \\) such that \\( 0 < \\Lambda < \\infty \\). Then, by the martingale convergence theorem, \\( \\sum X_t \\) converges with \\( \\Pi \\)-probability one, which implies from Kronecker's lemma that, with \\( \\Pi \\)-probability one, \\( p_k-\\alpha = (\\sum_{j=1}^{k} \\xi_j)^{-1} \\sum_{t=1}^{k} \\xi_t (Y_t - \\hat{Y}_t) \\rightarrow 0 \\) where \\( \\hat{Y}_t = \\alpha \\forall t \\). Q.E.D.\nProof of Lemma 4.5 Let \\( A_t \\) be an event token at time t and \\( P(A|E) = a \\) be the true probability of event type A conditional on event type E whose event tokens are denoted by \\( A_t \\) and \\( E_t \\), respectively. Then, by the definition of E with respect to A, \\( P(A_{t+1}|E_t \\in B_t) = a \\) with true probability P\u2013 one. Now, once \\( P(A_{t+1}|E_t \\in B_t) \\) is learned as such at some \\( t_0 \\), then \\( E_{t_0} \\) must have happened at that time and so \\( P(E_{t_0}) \\neq 0 \\). Also, by Assumption 4.4, consider a subsequence of \\( E_{t_k} \\)'s where \\( P(E_{t_k}) \\neq 0 \\) for any \\( t_k > t_0 \\). Then, for this subsequence, \\( P(E_{t_0}\\&E_{t_k}) \\neq 0 \\) for any \\( t_k > t_0 \\), because \\( E_{t_k} \\)'s are independent of one another.\nHere, \\( E_{t_k} \\)'s are independent for the following reason: recall that by definition, \\( P(A_{t_k+1}|E_{t_k} \\in B_{t_k}) = a \\) with true probability P- one. Then, note that \\( B_t \\) includes the fact that \\( P(A_{t_{k-i}+1}|E_{t_{k-i}} \\in B_{t_k-i}) = a \\) for some \\( i > 1 \\). Now, without loss of generality, let i = 1. Thus, we obtain\nNow that \\( E_{t_k} \\) and \\( E_{t_{k-1}} \\) are all included in \\( B_{t_k} \\) by (1), to show that \\( E_{t_h} \\)'s are independent, we need to prove that\n(2) \\( P(\\{P(A_{t_k+1}|B_{t_k}) = a\\} | \\{P(A_{t_{k-1}+1}|B_{t_{k-1}}) = a\\}) = P(\\ P(A_{t_k+1}|B_{t_k}) = a\\} \\)\nBut (2) is satisfied because \\( P(\\{P(A_{t_k+1}|B_{t_k}) = a\\}) = 1 = P(\\ {P(A_{t_{k-1}+1}|B_{t_{k-1}}) = a\\}) \\).\nNow that \\( P(E_{t_0}\\&E_{t_k}) \\neq 0 \\), for any \\( t_k > t_0 \\) in this subsequence, we can always find some small enough \\( \\epsilon > 0 \\) such that \\( P(E_{t_k}) > \\epsilon \\). Therefore, the probability of the element in this subsequence does not vanish to zero, which implies that \\( lim_{s\\rightarrow\\infty} P(E_{t_0} \\& E_{t_s}) \\neq 0 \\). Since \\( lim_{s\\rightarrow\\infty} P(E_{t_0} \\& E_{t_s}) \\neq 0 \\), \\( \\sum_{s=1}^{\\infty} P(E_{t_0} \\& E_{t_s}) = \\infty \\). Then, by the second Borel-Cantelli lemma, \\( P(E_{t_0} \\& E_t \\) i.o.) = 1 for \\( s > 0 \\), which means \\( P(E_{t_0} \\in B_{t_0} \\& E_{t_h} \\in B_{t_h} \\) i.o.) = 1 for \\( t_k > t_0 \\), the desired result. Q.E.D.\nProof of Theorem 4.6 Suppose that, for infinitely many t's when \\( P(A_{t+1}|B_t) \\) stays the same as a, machines learn this \\( P(A_{t+1}|B_t) \\) as \\( a \\) at time t. Then, by the Success Criterion (1), \\( \\Pi(A_{t_k+1}|B_{t_k}) = a = P(A_{t_k+1}|B_{t_k}) \\) at least infinitely often out of those infinite opportunities at t's to learn. (We prove in Corollary 4.37 what we mean exactly by \"most of the time.\" Here we tentatively mean \u201cat least i.o.", "s. Let \\( \\xi_{t_h+1} = 1 \\) if and only if \\( \\Pi(A_{t_k+1}|B_{t_k}) = P(A_{t_k+1}|B_{t_k}) = a \\). Note that \\( \\xi_{t_h+1} \\) is \\( B_{t_k} \\)-measurable, because machine forecasting \\( a \\) occurs at time \\( t_k \\). Then, by Theorem 4.1, with true probability P-one, \\( p_k-\\alpha = (\\sum_{j=0}^{k-1} \\xi_{t_j+1})^{-1} \\sum_{j=0}^{k-1} \\xi_{t_j+1}(Y_{t_j+1} - \\alpha) \\rightarrow 0 \\), as \\( k \\rightarrow \\infty \\) where P is defined over \\( B_\\infty = \\bigvee_{k=0}^{\\infty} B_{t_k} \\) and \\( B_{t_h} \\) is denoted by the totality of true facts up to day \\( t_k \\). Q.E.D.\nProof of Lemma 4.10 Clearly, if with P-probability one, \\( p_k \\rightarrow \\alpha \\), then \\( E [p_\\infty- \\alpha": 0}]}