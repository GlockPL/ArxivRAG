{"title": "Can Machines Learn the True Probabilities?", "authors": ["Jinsook Kim"], "abstract": "When there exists uncertainty, AI machines are designed to make decisions so as to reach the best expected outcomes. Expectations are based on true facts about the objective environment the machines interact with, and those facts can be encoded into Al models in the form of true objective probability functions. Accordingly, AI models involve probabilistic machine learning in which the probabilities should be objectively interpreted. We prove under some basic assumptions when machines can learn the true objective probabilities, if any, and when machines cannot learn them.", "sections": [{"title": "1. Introduction", "content": "In the standard AI model under uncertainty, how to measure the degree of uncertainty matters. This paper is about treating such measures in the form of probabilities. In particular, we focus on the true objective probabilities, if any. There are various probabilistic contexts in which the true objective probabilities matter. For example, causal relations of physical events are widely regarded as objective features of the world. Therefore, when causal relations are to be understood in terms of probabilities mainly due to various regularity issues, a probabilistic causal model should include an objective probability function that measures the true objective values about our world.\nThis paper addresses the question of whether machines can learn the true objective probabilities from the data to perform such probabilistic reasoning. Under some basic assumptions, we prove that machines can learn the true objective probabilities if and only if the probabilities are directly observable by them. Roughly speaking, a true probability is directly observable by a machine when it can calculate the probability by the empirical frequency of a true popula-\ntion given to it.\nThe outline of the proof is as follows. After defining some main concepts, we identify the Success Criterion and the necessary condition for any machine to learn the true objective probabilities. From these conditions, we derive the theorem that learning implies the true guarantee of well-calibration. Roughly speaking, \u201ctruly guaranteed well-calibration\" means the following: when a machine collects data according to its subjective forecast along a stochastic path in which the associated events occur, the empirical frequency of the collected data matches the very probabilistic forecast of the machine with the true probability P- one. Now that the machine forecasts must indeed be true when the machine learns the true probabilities, this calibration property can then be understood as a calibration version of the strong law of large numbers without the independence assumption.\nNote that there exist connections here among machine forecasting, well-calibration, and machine learning. While proving our theorems, therefore, we establish connections between the true guarantee of well-calibration and various settings of the real forecasting games between Nature and a machine. In this game, what Nature forecasts are the true objective probabilities, while what the machine forecasts are its own subjective probabilities. The machine loses when Nature deviates from the probabilistic forecasts of the machine. Bridged by the property of truly guaranteed well-calibration, we then prove whether the machine learns the true probabilities or not under various settings of forecasting games.\nWith this proof, we provide the fundamental scope and limit of learning the true probabilities by AI machines. One important implication is that machines can relax the independent assumption among data to learn the true probabilities but cannot relax the assumption of identical distribution such as stationarity or ergodicity along a stochastic path where any associated events occur. Another implication is to show that the problem of computability is directly connected to the problem of complexity in the case of learning the true probabilities."}, {"title": "2. Notations and Definitions", "content": "In this section, we define some main concepts, including \u201cmachine learning\u201d and \u201ctrue objective probability\". Adopting terminologies from (Nilsson, 2011) and (Boolos et al., 2002), let us first define a machine as an artifact or device that can effectively calculate or compute any target function if there exist definite and explicit instructions to do so in principle. Since we focus on probability functions in this paper, we particularly mean by \"an effectively calculating or computing device\" a machine that can in principle assign a probability measure (a value of a probability function) to each state (an argument of the probability function) in a given domain, an event space of a sigma-field.\nDefinition 2.1. A function is effectively calculable or computable when there are definite and explicit instructions, following which its functional value can be calculated in principle for any given argument. (Boolos et al. (2002))\nTwo things merit to be taken into account with Definition 2.1. First, this notion of effective calculation or computation is an ideal one with no practical limits on time, expense, etc., necessary to calculate. Therefore, a proof of the limitation on effective calculation or computation of any function will imply a fundamental limit on computability that cannot be overcome by any practical real machine. Second, as (Kozen, 1997) points out, this notion is an informal one, something that is supposed to be captured in common by all formalisms such as computation by Turing machines, by the \\(\\lambda\\)-calculus and by the \\(\\mu\\) -recursive method, etc. Accordingly, once we adopt this notion of effective calculation or computation to define \"learning\", we can be flexible about which formalism would be encoded as instructions to complete a given learning task.\nNow, whatever such formalism is, machines can learn only if there exist some instructions followed by them to complete their tasks. So we can prove that it is impossible for machines to learn any target function under certain conditions in the following way: we first suppose that there exist some successful instructions to be encoded into machine programming to learn any given function under the conditions. We then show that this supposition leads to a conclusion that is impossible to satisfy. We thereby conclude that there cannot exist such instructions for the given function and, accordingly, that machines cannot learn it. This is a simple but clear way of proving the impossibility of learning without being committed to any complex procedure of constructing any formalism such as a Turing machine or \\(\\lambda\\)-calculus, etc.\nDefinition 2.2. A machine learns when it succeeds in effectively calculating or computing a target function, if any, after processing possibly infinite amounts of data.\nThe phenomenon of learning must be at least computational"}, {"title": "3. Kinds of Probabilities and Learning", "content": "Broadly speaking, probabilities can be divided into two kinds, subjective and objective ones. Subjective probability, say \\(I(A_{t+1}|B_t)\\), depends on each person's belief and thus possibly varies from person to person, while objective one, say \\(P(A_{t+1}|B_t)\\), does not.\nThe standard theory of subjective probability was first developed by Ramsey and then further by De Finetti and Savage. Subjective probability is designed to represent a degree of belief possessed by a subject, say some person or, if possible, a machine. Hence subjective probability represents whatever is in any one's mind upon anything as long as his/her belief system is coherent, and so can be assigned even to what is merely imagined. For example, while arguing for cogito, ergo sum, (Descartes, 2008) imagined an evil spirit that has devoted all its efforts to deceiving him. Descartes can assign some value of subjective probability to his imagination on the evil spirit in accordance with how likely it is to him that the imagination can be realized in this world, as long as Descartes' belief system remains coherent.\nIn contrast, objective probability, if any, is what must be determined by objective features of our world that do not vary from person to person. The best way to understand objective probability is to consider examples. Following (Maher, 2010), for example, suppose that a coin has the same face on both sides, that is, two-headed or two-tailed. When this coin is tossed infinitely often, its relative frequency surely converges to 1 or 0. Hence the limiting relative frequency here is either 1 or 0, depending on how our world turns out to be, which is an objective matter, and not on whatever we believe.\nIt should be noted that subjective and objective probabilities are conceptually bifurcated in two important ways. First, recall that subjective probability represents an aspect of someone's subjective belief, while objective probability does not. Hence the subjective probability of Descartes' demon is positive as long as it is believed at any degree that it could exist in our world. However, this does not necessarily imply that the true objective probability of Descartes' demon is positive, since it might be the case that such a demon is possible only in one's imagination but impossible in our real world. We will return to this potential bifurcation between subjective and objective probability in Section 4.1.\nSecond, there exists an asymmetric relation between subjective and objective probability: although the subjective probability of Descartes' demon does not necessarily bind its objective probability, the converse holds. (e.g. (Lewis, 1980)) That is, once it is proven/assumed by any agent that the true objective probability of Descartes' demon is, say zero, then its subjective probability of the same agent is bound to this proven/assumed result on the objective probability and thus must be zero as well. From this asymmetric relationship, we derive Lemma 4.23 in Section 4.2."}, {"title": "4. Can Machines Learn the True Probabilities?", "content": "In general, if a machine achieves computational success at t by learning, what the machine represents by learning must be at least true at that time. Then we denote the true representation of the machine about what is learned by the \"true belief\" of the machine, a legitimate analogue to the true belief of human beings. It is a belief analogue, for we haven't yet shown that machines have minds or that they have the same kinds of mental representations as human beings. It is nevertheless a legitimate belief analogue, since the computational models of machine intelligence are based on understanding human intelligence. (e.g. (Pearl, 2018), (Russell, 1998), (Valiant, 1984; 2008))\nThat said, let us discuss the relation between belief and learning on the machine side: the knowledge acquired by machine learning must be at least a true belief. In (Hintikka, 1962), the knowledge of a person i refers to the knowledge of that person i on any proposition A. Likewise, machine's learning of the true objective probability P here refers to the knowledge acquired by any machine on the probabilistic proposition \\(A_P\\). If a machine learns the true probability as a, then the probabilistic proposition \\(A_P\\) amounts to that the true objective probability P, if any, is what the very machine calculates as a. Here, we convert the non-propositional learning into propositional learning.\nNow, just as a person i's knowledge on proposition A must satisfy the necessary condition that the person i's belief in A is true, machine learning of the true probability P must also satisfy the condition that the belief in \\(A_P\\) of the machine is true. Note here that such a belief in \\(A_P\\) is true when what has been calculated by the machine is indeed equal to the true probability P. Now, this calculated probability function by a machine is nothing more than the subjective probability of the machine. Therefore, the necessary condition for machine learning of true probability P requires a machine to hold a true belief whose truth condition is satisfied when its subjective probability \\(\\Pi\\) is, in fact, in congruence with the true objective probability P. In short, if a machine learns the true objective probability P, then the subjective probability \\(\\Pi\\) of the machine is actually equal to the true probability P.\nTherefore, we obtain the following condition:\nThe Necessary Condition for any Machine to Learn the True Probability\n(2) If a machine learns the true objective probability \\(P(A_{t+1}|B_t)\\), then \\(\\Pi(A_{t+1}|\u00dft) = P(A_{t+1}|B_t)\\)\nwhere \\(\\Pi(A_{t+1}|B_t)\\) denotes the subjective probability of the machine at time t."}, {"title": "5. Conclusion", "content": "We have discussed so far when machines can learn the true probabilities and when they cannot. In summary:\n- \\(a^*\\) such that P(Nature is perverse with \\(a^*\\)) > 0 by Theorem 4.19.\nNow that Nature is perverse at least with one forecast \\(a^*\\),\n- (i) Nature is uniformly perverse: machines cannot learn by Theorem 4.20.\n- (ii) Nature is selectively perverse: \\(\\exists t_s\\) for each \\(a_0\\) such that P( Nature is perverse with \\(a_0\\) ) = 0 by Lemma 4.28.\nThen under (ii),\n- (ii-1) Machines are not self-assured of the \\(t_s\\): machines cannot learn by Corollary 4.30.\n- (ii-2) Machines are self-assured of the \\(t_s\\):\nThen under (ii-2),\n- (ii-2-1) \\(t_s\\) actually does not arrive: machines cannot learn by Theorem 4.32.\n- (ii-2-2) \\(t_s\\) indeed arrives: machines can learn and this is the only case in which machines can learn by Theorem 4.35 and Theorem 4.36.\nBefore we close this section, let us add a few remarks. First, we emphasize that in this paper we have focused on the notion of \u201cmachine learning\u201d that is not just a technical terminology, understood as an identification of a target function,"}]}