{"title": "Inference-Time Selective Debiasing", "authors": ["Gleb Kuzmin", "Nemeesh Yadav", "Ivan Smirnov", "Timothy Baldwin", "Artem Shelmanov"], "abstract": "We propose selective debiasing \u2013 an inference-time safety mechanism that aims to increase the overall quality of models in terms of prediction performance and fairness in the situation when re-training a model is prohibitive. The method is inspired by selective prediction, where some predictions that are considered low quality are discarded at inference time. In our approach, we identify the potentially biased model predictions and, instead of discarding them, we debias them using LEACE a post-processing debiasing method. To select problematic predictions, we propose a bias quantification approach based on KL divergence, which achieves better results than standard UQ methods. Experiments with text classification datasets demonstrate that selective debiasing helps to close the performance gap between post-processing methods and at-training and pre-processing debiasing techniques.", "sections": [{"title": "Introduction", "content": "Fairness is an important safety characteristic of a machine learning (ML) model, representing the model's ability to classify instances without discrimination based on various sensitive attributes, such as race, gender, and age (Blodgett et al., 2020). In recent years, numerous works have investigated and promoted fairness, and a variety of fairness definitions have been proposed (Blodgett et al., 2020; Han et al., 2022b). One prominent type of fairness is group fairness, also known as the equal opportunity criterion, which reflects the inequality of opportunities across different groups (Han et al., 2022a). The inequality in the model predictions usually comes from inadequate or biased training data, and to address this problem and achieve better fairness, researchers have proposed various debiasing techniques (Li et al., 2018; Han et al., 2021, 2022a; Belrose et al., 2023). The majority of these techniques assume that one has access to the complete training data and the ability to retrain the model from scratch using some special loss function or reweighting the training instances. However, there are many situations when this assumption does not hold. There is a need for inference-time safety mechanisms that protect users from inadequate model behavior.\nInference time safety mechanisms are primarily associated with uncertainty quantification (UQ) techniques (Gal and Ghahramani, 2016) and selective classification (Geifman and El-Yaniv, 2017). Selective classification aims to enhance the reliability of ML-based applications by abstaining from unreliable predictions with high uncertainty. We suggest that the same approach could equally be applied to increase fairness.\nIn this work, we propose an inference-time safety mechanism that aims to increase the overall quality of models in terms of prediction performance and fairness in situations when model re-training is prohibitive. We call this approach selective debiasing. Instead of rejecting predictions of selected instances as in selective classification, we apply to them inference-time debiasing using post-processing debiasing techniques. We note that applying debiasing to all model decisions might reduce its overall performance. However, the selected predictions often have a high potential to be of low quality or carry some bias. Therefore, modifying these predictions is less risky in terms of worsening the situation, while still holding the potential to correct errors. To the best of our knowledge, this style of approach is novel to the NLP community.\nOur main contributions are as follows:\n\u2022 We propose selective debiasing, an inference-time safety mechanism that aims to improve both the performance and fairness of model predictions by applying a post-processing debiasing method to only a selected subset of predictions.\n\u2022 We suggest a scoring criterion that aims to select the most unreliable and biased predictions. Experiments demonstrate that this scoring criterion is generally better than UQ techniques in selective debiasing."}, {"title": "Background", "content": "Debiasing techniques can be categorized into three groups: at-training, pre-processing, and post-processing (Han et al., 2022b). The first two groups require retraining the model from scratch and access to the whole training set. They also cannot be selectively applied to a subset of predictions. Post-processing techniques do not involve changes to the model itself, can be trained on a subset of data, and can be applied to predictions selectively. However, their performance is usually worse.\nAt-training and pre-processing methods. Adversarial training (Adv) (Li et al., 2018) aims to solve a minimax game between minimizing the loss for the primary task and maximizing the loss for predicting the protected attribute. Diverse adversaries (DAdv) (Han et al., 2021) extends Adv by using an ensemble of diverse discriminators instead of just one. Balanced Training with Equal Opportunity (BTEO) (Han et al., 2022a) is a dataset balancing technique. It aims to minimize the TPR gap between two protected groups. Balanced Training with Joint balance (BTJ) (Lahoti et al., 2020) aims to improve the worst-case performance over all unobserved protected groups by focusing on the computationally identifiable regions of error.\nPost-processing methods. There are two well-known approaches to post-processing debiasing: Iterative Null-space Projection (INLP) (Ravfogel et al., 2020) and LEAst-squares Concept Erasure (LEACE) (Belrose et al., 2023).\nINLP proposes an iterative method that involves finding an orthogonal projection of a linear classifier matrix, which is initially learned to predict protected attributes from representations (e.g. hidden states of the standard model). This orthogonal projection is then iteratively used to remove all relevant information from these representations, which was used by the classifier to predict protected attributes.\nLEACE is a concept erasure technique that renders representations impervious to the prediction of a specific concept while minimizing change to the original representations. LEACE first whitens the data by equalizing the variance in all directions. Then the data is orthogonally projected onto the relevant subspace, and finally, the data is unwhitened with the same covariance matrix before subtracting from the original representation."}, {"title": "Proposed Methods", "content": "Selective debiasing. Selective classification is a well-known human-in-the-loop approach that makes predictions for some instances and dispatches others to human experts for labeling. Instead of rejecting instances completely as in selective classification, we suggest applying debiasing to selected model predictions during the inference time. This is different from the standard post-processing debiasing methods since we suggest changing the predictions for only some instances. In particular, we identify the potentially most biased instances by a scoring criterion and replace the original model's prediction with predictions debiased using a post-processing method. This does not introduce much computational overhead, as applying a post-processing method to a model is a matter of one or two matrix multiplications. Finally, we obtain predictions for the model that represent a mixture of original $p^i$ and debiased predictions $q^i$:\n$\\begin{aligned} p_i' = \\begin{cases} p^i, & \\text{if } i \\in S_h \\\\ q^i, & \\text{if } i \\notin S_h, \\end{cases} \\end{aligned}$\nwhere $S_h$ is the set of samples with high bias scores. Membership in $S_h$ is determined by a threshold criterion (e.g. if the bias score is in the top-N percent of highest scores). Such an approach allows flexible adjustment of the accuracy-fairness trade-off.\nBias quantification method. For selective debiasing, we need a good instance-level bias quantification method. Selective classification is usually based on UQ methods. However, uncertainty on its own is not reflective of group fairness; it simply tries to eliminate potentially erroneous predictions. Figure 1 presents a motivational example. It shows the rejection plots for oracle rejection strategies in selective classification for both accuracy and fairness (see the exact definition of fairness in Appendix E). We can see that the fairness oracle outperforms the UQ oracle in terms of fairness while keeping the same performance in terms of accuracy. These results illustrate that to improve fairness without penalty to accuracy, it is necessary to change the order of instances being eliminated, i.e. we need a different selection criterion."}, {"title": "Experiments", "content": "For our experiments, we use two English text classification datasets that in addition to target variables, provide explicit protected attributes. The first is MOJI (Blodgett et al., 2016), a dataset for sentiment analysis with a binary class (happy and sad) and a binary protected attribute, which corresponds to the author's ethnicity (African American English (AAE) vs. Standard American English (SAE)). The second is a version of the widely used BIOS dataset (De-Arteaga et al., 2019) for occupation classification with binary gender as the protected attribute. BIOS-2 (Subramanian et al., 2021) is a two-class subsample of the original BIOS dataset with a highly-skewed joint distribution of classes and protected attribute values. As it has been shown to be beneficial to report results for both \"balanced\" and \"imbalanced\" versions of datasets (Kuzmin et al., 2023), we conduct experiments on both versions. Detailed information and statistics of the datasets are presented in Appendix D.\nMetrics. We employ several metrics to quantify the predictive performance of the model and also its fairness. To estimate the model's performance, we use accuracy. For fairness, we consider the widely-used equal opportunity criterion (Hardt et al., 2016; Han et al., 2022a,b). We also use two aggregated metrics to obtain the overall model performance in terms of accuracy and fairness. The first one is the distance to the optimal point (DTO) (Han et al., 2021):\n$\\text{DTO} = \\sqrt{(1 - \\text{Accuracy})^2 + (1 - \\text{Fairness})^2}.$\nThe second one is the Fairness F-score (FF)\u2014a smoothed minimum of accuracy and fairness:\n$\\text{FF-score} = \\frac{2 \\cdot \\text{Accuracy} \\cdot \\text{Fairness}}{\\text{Accuracy} + \\text{Fairness}}.$\nBaselines. We compare the proposed selective debiasing approach to inference-time debiasing of all predictions using LEACE and INLP, as well as to training-time debiasing techniques. We also compare the proposed KL-based bias quantification score with a UQ baseline: Softmax Response (SR: Geifman and El-Yaniv (2017)), calculated as\n$U_{SR} = 1 - \\max_{c \\in C} p_c.$"}, {"title": "Results", "content": "Table 1 compares fairness, accuracy, DTO, and FF-score for various at-training and pre-processing debiasing methods, post-processing debiasing methods, and for selective debiasing based on LEACE with SR and the proposed KL-based bias quantification score. Here, we show results only for the optimal selection percentage. The full results with various selection percentages are presented in Appendix B. The optimal percentage was chosen over the first 15% of the validation set. \"LEACE-last\" represents LEACE applied to the outputs of the last hidden layer of the classifier, while \u201cLEACE-cls\" is LEACE applied to each linear layer of the classification head of the model. For comparison, we also provide the results with selective debiasing using INLP. The post-processing methods were trained on only 20% of the training set.\nIn the majority of cases, the best results are unsurprisingly achieved by at-training and pre-processing debiasing techniques, as these methods retrain the models from scratch on the full training data. Nevertheless, the proposed selective debiasing approach based on LEACE substantially enhances the results of inference-time debiasing using post-processing techniques in terms of metrics that take into account both fairness and performance: FF-score and DTO. It is also competitive with at-training and pre-processing techniques. For LEACE-cls with KL selection, selective debiasing even outperforms these methods on MOJI-balanced. The results in Tables 10 to 12 also show that selective debiasing consistently outperforms standard inference-time debiasing in terms of FF-score across various percentages.\nLEACE-cls generally achieves better fairness than LEACE-last and slightly better joint fairness-performance, in terms of DTO and FF-score. INLP-based selective debiasing improves the FF-score only on MOJI-balanced, while on other datasets, it is consistent with the base inference-time debiasing method. INLP-based approaches overall fall behind the corresponding LEACE-based techniques.\nWhen comparing the results of the proposed bias quantification method based on the KL distance with SR, we can see that our method notably outperforms SR on MOJI datasets and is on par with SR on BIOS-2. We further explore other distance-based bias quantification methods (Euclidean and cosine distances), which are presented in Appendix C. Results in Tables 10 to 12 show that in most cases, selection by KL works comparably or better than other distance-based measures. Moreover, KL scores are easier to compute than distance-based scores."}, {"title": "Conclusion and Future Work", "content": "We proposed selective debiasing\u2014a new simple inference-time safety mechanism for increasing model performance and fairness. We showed that it is helpful in the case that re-training a model from scratch for better fairness is prohibitive or there is no access to full training data. Additionally, for the selection of problematic predictions, we suggest a bias quantification approach based on KL distance that achieves better results than the standard UQ method. The proposed mechanism fills the gap for efficient techniques that can be applied at inference time and opens the door for safer ML-based systems. In future work, we aim to investigate a deeper integration between UQ and debiasing methods."}, {"title": "Limitations", "content": "In this work, we considered only group fairness (equal opportunity criterion), where there exist many other fairness definitions. However, this research is focused particularly on group fairness, and the equal opportunity criterion is the metric of choice in previous work on the same datasets. During all experiments, we assume that we have access to the protected attributes, which is not always the case. But this is a common assumption for any work on debiasing; moreover, it is necessary for the calculation of the fairness metric. Finally, all of the experiments were conducted on the English language, but the used methods are language-independent, so we do not expect significant differences in results for other languages."}, {"title": "Ethical Considerations", "content": "In this work, we consider group fairness and instance-level bias quantification. We used only publicly available datasets and models, and only for the intended use. In our research, we used protected attributes to apply debiasing methods and to compute metrics; however, this is necessary for all debiasing methods. To avoid possible harm, we used only attributes that users self-disclosed for the experiments."}, {"title": "Training Setup and Hyperparameters", "content": "To find an optimal set of hyperparameters, we conducted a grid search on the validation set. We used accuracy as an optimization target for standard models, and DTO for models with debiasing. The grid and optimal parameters for the standard models are described in Table 2. For each debiasing method, we tuned the method's parameters and kept training parameters of the base model\u2014the grid and optimal values for debiasing methods presented in Table 3. The training was conducted on a cluster with Nvidia V100 GPUs. An approximate number of GPU-hours spent during the experiments is presented in Table Table 4."}, {"title": "Selective Debiasing on Various Percentages", "content": "To check how stable the proposed methods are, we compare selective debiasing results over 5%, 10%, and 15% of selection for random, SR, and KL scores. The results are presented in Tables 6 to 8. The optimal percentage selected on the validation set from values from 1% to 15%; results for each dataset-method pair in Table 9. In general, optimal scores are better or comparable with results on various percentages, which allows us to use this approach to detect the optimal percentage of selection. Table 5 shows the performance of selective debiasing and post-processing debiasing methods trained on a full training set. As one can see, the performance on the full set is comparable with the results on only 20% from Table 1."}, {"title": "Comparison with Other Distances", "content": "We also conducted additional experiments to compare how proposed selection strategies differ from other similarity measures. Here we consider several measures, calculated over output from the last hidden layer of the model, and compare it with SR and KL strategies. The results are presented in Tables 10 to 12. In most cases, selection by KL works comparably or better than the best-performing distance-based measure. Moreover, KL scores are easier to compute than distance-based scores. However, in some cases, cosine distance could serve as a replacement for the KL score due to its similar performance."}, {"title": "Datasets Statistics", "content": "The synthetic dataset was generated as a random 2 classes classification task using make_classification function from Scikit-learn library (Pedregosa et al., 2011) with following parameters: n_features=10, n_informative=5, n_clusters_per_class=2, random_state=42, n_redundant=2. The protected attribute for the synthetic dataset is designed as a condition over the first informative feature and equals 1 if this feature is greater than 0, and 0 otherwise. The overall statistics for each dataset are presented in Table 13. Tables 14 and 15 shows the joint distribution of target variable and protected attributes."}, {"title": "Equal Opportunity", "content": "There are numerous amount of group fairness definitions; to avoid any mismatches, we are presenting the step-by-step process of equal opportunity criterion calculation. This criterion is based on recall values, or true positive rate (TPR) for each class and protected group.\n\u2022 TPR (recall) for each protected group defined as follows:\n$\\text{TPR} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}},$\nwhere TP, FN - is true positives and false negatives for specific group.\n\u2022 After we calculate TPR-gap:\n$\\delta = \\sqrt{\\frac{1}{|C|} \\sum_c (\\frac{1}{|G|} \\sum_g \\text{TPR}_{c,g} - \\overline{\\text{TPR}}_{c})^2},$\nhere g is group index, c - class index, TPRc - TPR averaged across all groups for class c.\n\u2022 Finally, we calculate fairness with the following equation:\n$\\text{Fairness} = 100 \\cdot (1 - \\delta).$"}, {"title": "Fairness and UQ Oracles", "content": "In this section, we describe in detail oracle strategies for fairness and accuracy. For both strategies, we assume access to the ground-truth labels, while for fairness oracle we also use protected attributes. Accuracy oracle built as follows\u2014we find all erroneously classified instances and replace predictions on these instances with ground-truth labels while keeping all other predictions unchanged. This oracle shows the best possible UQ strategy that allows the detection of all erroneous predictions and gives the maximal increase in accuracy. The same idea is behind fairness oracle, but instead of accuracy, we first replace predictions for instances, which gives the maximal increase in fairness. These predictions were chosen greedily from the erroneous ones. To measure the quality of these oracle strategies and to compare it with other scores, we calculated several metrics: FR-AUC, Acc-AUC, and FF-score-AUC. Each corresponds to the area under the target metric-rejection curve, where the target metric is fairness, accuracy, or FF-score; the area under the curve is calculated on binarized over 100 points target metric values."}]}