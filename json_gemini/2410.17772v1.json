{"title": "Scaling Robot Policy Learning via Zero-Shot Labeling with Foundation Models", "authors": ["Nils Blank", "Moritz Reuss", "Marcel R\u00fchle", "\u00d6mer Erdin\u00e7 Ya\u011fmurlu", "Fabian Wenzel", "Oier Mees", "Rudolf Lioutikov"], "abstract": "A central challenge towards developing robots that can relate human language to their perception and actions is the scarcity of natural language annotations in diverse robot datasets. Moreover, robot policies that follow natural language instructions are typically trained on either templated language or expensive human-labeled instructions, hindering their scalability. To this end, we introduce NILS: Natural language Instruction Labeling for Scalability. NILS automatically labels uncurated, long-horizon robot data at scale in a zero-shot manner without any human intervention. NILS combines pretrained vision-language foundation models in order to detect objects in a scene, detect object-centric changes, segment tasks from large datasets of unlabelled interaction data and ultimately label behavior datasets. Evaluations on BridgeV2, Fractal and a kitchen play dataset show that NILS can autonomously annotate diverse robot demonstrations of unlabeled and unstructured datasets, while alleviating several shortcomings of crowd-sourced human annotations, such as low data quality and diversity. We use NILS to label over 115k trajectories obtained from over 430 hours of robot data. We open-source our auto-labeling code and generated annotations on our website: http://robottasklabeling.github.io.", "sections": [{"title": "1 Introduction", "content": "Natural language is an intuitive and flexible interface for humans to communicate tasks to robots. Recent works have shown promising results in training language-conditioned policies using large"}, {"title": "2 Method", "content": "NILS consists of three stages: Stage 1 (Identifying Objects in the Scene), Stage 2 (Object-Centric Scene Annotation), and Stage 3 (Keystate Detection and Label Generation). Figure 2 depicts a comprehensive method overview. The subsequent sections elaborate on each stage. NILS uses different frozen pretrained models across each state, which enable modular replacement. A detailed list of the models used in NILS, including explanations and usage, is provided in Appendix A."}, {"title": "2.1 Stage 1: Identifying Objects in the Scene", "content": "In Stage 1, NILS deploys a set of VLMs to detect all objects and their relevant properties across multiple video frames. This ensures comprehensive object detection even with occlusion. NILS prompts the model to output concise and unique names for all objects in each frame. However, this approach results in inconsistent naming across frames for the same object. To resolve this, NILS leverages temporal consensus, co-occurrence, and object detection alignment. It uses GroundingDINO [22] for bounding box generation and computes Intersection over Union (IOU) for co-occurrence. NILS combines the detector confidence with a SigLIP alignment score based on cropped image regions of the object. Finally, an LLM assigns the properties movable, is_container, states, and interactable to each object to help filter invalid robot-object interactions in later Stages."}, {"title": "2.2 Stage 2: Object-Centric Scene Annotations", "content": "Next, NILS generates object-centric scene annotations that allow to reason about robot-object interactions. For each object detected in Stage 1, NILS computes bounding boxes, segmentation masks, movement, relations, and state throughout the video. The extracted information is used in Stage 3 to segment and annotate the demonstrations. First, NILS identifies the object bounding boxes and segmentation masks to track various object changes."}, {"title": "2.3 Stage 3: Keystate Detection and Label Generation", "content": "Determining critical states that mark task boundaries is a key challenge in labeling long-horizon robot demonstrations. NILS addresses this through a novel heuristic consensus method, using object-centric representations from Stage 2 to detect individual manipulation tasks. This method combines multiple heuristics to filter noise-induced false positives and identify reliable keystates. NILS acquires object-centric keystates by having each heuristic monitor changes for objects to minimize the noise impact. An object-centric keystate $o_i$ is considered valid if its score exceeds a threshold $\\theta_o$: \n\n$S(o_i) = \\sum_k \\alpha_k * S_k(o_i),$ \n\nwhere $k$ is the heuristic index, $\\alpha_k$ is the heuristic weight, and $S_k(o_i)$ is the confidence of the current heuristic. NILS uses equal weights for all heuristics. The keystate score $S_k(o_i)$ controls the quality of generated keystates and language annotations (Figure 10), providing an advantage over VLM captioning [25]. NILS then aggregates nearby keystates and selects the highest-scoring one to minimize noise."}, {"title": "2.4 Object-Centric Information Retrieval of Stage 2", "content": "We provide further details on the object-centric information retrieval in Stage 2. Using bounding boxes and the segmentation masks, NILS monitors various signals for labeling. NILS tracks the following signals to generate templated language with timestamps:"}, {"title": "3 Evaluation", "content": "In this section, we address the central question: Can NILS generate semantically meaningful, detailed, and adjustable-granularity language annotations for existing robot interaction datasets? To answer this question, we assess NILS' accuracy in labeling long-horizon robot data, keystate quality, and grounding performance against state-of-the-art VLMs and study the generated task labels with respect to various properties. NILS was evaluated on three datasets: (1) the BridgeV2 [1] containing 1200 long-horizon trajectories with 28K short horizon pick and place and state-manipulation tasks with diverse scenes, tasks, and objects; (2) Fractal [5], a dataset consisting of 87K short horizon demonstrations; (3) a self-collected one-hour long-horizon, uncurated play dataset in a robot play kitchen, containing 439 short-horizon demonstrations of 12 tasks. The dataset consists of multiple long-horizon demonstration videos where the robot solves at least 10 manipulation tasks in a row."}, {"title": "3.1 Keystate Evaluation", "content": "We quantitatively evaluate the keystates produced by NILS on both datasets using precision, recall, and mean-average precision (mAP) metrics. A predicted keystate is considered correct if its distance to a ground truth keystate is smaller than a threshold based on the average short-horizon task length."}, {"title": "3.2 Grounding Evaluation", "content": "We evaluate the grounding capabilities of NILS on a mixture of the BridgeV2 dataset and our Kitchen Play dataset. Specifically, we select 14 different long-horizon play demonstrations from various settings, each containing 20 tasks. Of these, 12 trajectories are sampled from BridgeV2. We chose trajectories from diverse settings, including kitchens, tables, and washing machines, with a wide variety of objects to ensure maximal task diversity. Further, we added two long-horizon demonstrations from our Kitchen Play Dataset with 24 tasks each. Given the ambiguity of language annotations, we employ human evaluators to assess whether a language label proposed by a method is correct given the segmented task video."}, {"title": "3.3 Labeling Results for BridgeV2 Dataset", "content": "To assess the scalability of NILS, we applied it to a larger, more diverse dataset. Therefore, we label a subset of BridgeV2 that consists of 31K trajectories in diverse environments. The cost for NILS to label this subset was approximately 200 USD, compared to about 5000 USD for crowd-sourced labels\u00b2. This demonstrates the low-cost of NILS. To analyze the diversity of generated labels, we visualized the top 60 labels, sorted by frequency, in Subsection B.1. This visualization confirms the diversity of labels generated by NILS compared to crowd-sourced annotations. Additionally, our"}, {"title": "3.4 Language-conditioned Policy Training", "content": "To study the generalization of NILS for large-scale, diverse datasets, we train a language-conditioned policy with labels from NILS on the BridgeV2 [1], and Fractal [5] datasets. We use Octo [36], a recent open-source diffusion policy and train it using 3 different label sets: crowd-sourced labels (GT), NILS labels, and Gemini labels. We train Octo [36] on different data mixtures to evaluate the capabilities of our labeling framework for downstream policy learning. In particular, we train Octo on the subset of BridgeV2 labeled by NILS and Fractal fully labeled by NILS. We always train the baseline policies with the same datasets, but switch out the language labels with their respective counterparts. Due to labeling costs and Gemini's low performance, we omit labeling Fractal with Gemini. Further evaluation details are provided in Appendix F."}, {"title": "3.5 Ablation Studies", "content": "What are the most crucial components of NILS?\n\nWe conduct ablation studies to investigate the importance of various design decisions in NILS."}, {"title": "4 Related Work", "content": "Key State Identification. Identifying important keystates from long-horizon tasks is crucial for efficiently learning goal-conditioned policies. Some approaches use proprioceptive observations [39, 40, 41, 42] or waypoint reconstruction loss [39]. UVD utilizes the latent space of pretrained image embedding models such as CLIP [21] to detect keystates. Similar to NILS, it also does not require any robot signals. However, NILS additionally incorporates more specific information, which significantly improves keystate quality. REFLECT [28] constructs a scene graph with object relations and states, labeling frames as keyframes when the graph changes. However, this method relies on ground truth state information and object positions, which are usually only available in simulated environments. NILS detects keystates in real-world environments from robot videos only."}, {"title": "5 Discussion", "content": "Limitations. Despite NILS' ability to generate high-quality labels, some limitations remain: Using multiple models for generating scene representations results in a significant computational cost, requiring 7 minutes to label one 8-minute long-horizon trajectory consisting of 50 tasks on a 3090 RTX GPU. Correlated heuristics can sometimes lead to high-confidence keystates triggered by noise, resulting in incorrect labels. Grounding accuracy is limited by the performance of pretrained models. NILS has high objectness assumptions for labeling, making it challenging to label tasks with granular objects and distinguish between visually similar objects due to the limitations of object detectors."}, {"title": "A.1 Overview of Pretrained Models for NILS", "content": "We summarize all used pretrained foundation models for NILS. All models are used without any fine-tuning. We want to highlight that all models can be exchanged for other pretrained models of the same category. Further, an overview of the usage of all different models is provided in Table 4."}, {"title": "A.2 Stage 1: Identifying Objects in the Scene", "content": "Initially, NILS utilizes 8 equally distributed frames from a long-horizon demonstration video to query Gemini. For each frame, NILS prompts the model to output concise, specific, and unique names for all objects in the scene, as well as their colors. Figure 16 in the Appendix shows the prompt used for initial object retrieval. Querying the VLM for multiple frames significantly increases the number of detected objects, particularly in cases of occlusion. However, the same object is now likely referred to under different names in different frames. Thus, NILS leverages temporal consensus, co-occurrence, and object detection alignment to select consistent and representative object names across frames. For temporal consensus, NILS prompts GroundingDINO with all object descriptions generated by the VLM for all 8 frames to generate a set of bounding boxes for each frame. Next, co-occurrence is measured for each individual frame by computing bounding box IOUS, capturing different names referring to the same object. If the IOU is above a certain threshold, NILS assumse that the corresponding object names refer the same object. The co-occurring objects are counted and grouped for all 8 frames, and a representative object name is chosen according to object detector confidence. Object names with the highest confidence scores per group are stored in synonym lists to enrich the instruction. This approach generates contextualized grounded object synonyms, increasing the overall data diversity. Furthermore, this approach can be considered as automatic prompt engineering. Some object descriptions can result in better, more consistent bounding boxes produced by the object detector. By selecting the object name with the highest confidence, we also select the object name that is most likely to produce correct detections for the other frames. Finally, an LLM assigns the properties movable, is_container, states, and interactable to each detected object. These properties help to filter wrong robot-object interactions in subsequent Stages."}, {"title": "A.3 Stage 2: Object-Centric Scene Annotations and Information Retrieval", "content": "Object Filtering and Mask Refinement through Temporal Aggregation.\n\nDetecting keystates accurately based on object masks and boxes requires temporally consistent object masks. However, initial object detections may suffer from temporal misalignments, such as missing detections for certain frames or an object being classified with a synonym for different frames. To address this challenge, NILS utilizes DEVA [24], a mask-tracking model, to capture temporal correlations between objects. DEVA propagates masks with X-Mem [52] while frequently"}, {"title": "A.3.1 Object-Centric Information Retrieval", "content": "Object Relations and Object Movement We generate an object-relation graph from segmentation masks generated in the first step. First, the objects are projected into a point cloud with a depth map generated by Depth-Anythingv2 [29]. We further perform additional filtering steps, such as outlier detection and downsampling. To reason about object movement in natural language in camera space, we project the pointcloud into a coordinate frame that aligns with flat surfaces in the scene. First, NILS detects surfaces, such as floor, stove top, table, counter in the scene. The detected surface is projected into the pointcloud, followed by plane segmentation and normal estimation. The normal acts as the upvector of the new coordinate system, whereas the front-vector points towards the camera. Furthermore, many tasks include changing the position of an object with respect to the surface it is located on. To detect the position of objects on a surface, NILS uses the previously detected surface object and performs a homography transform to account for camera perspective.\n\nTo do this, we fit a quadrilateral to the surface's segmentation mask and compute a transformation matrix from the detected corners to image corners. We then project the objects bounding boxes with this matrix and compute object position on the surface by categorizing positions in a 3x3 grid.\nObject State Prediction. NILS crops objects from images based on their bounding box. We add a small padding to the bounding box, to ensure all relevant information is present in the cropped image. Robot mask IOU determines occlusion with the cropped region. Then, NILS compares the CLIP similarities of state text embeddings and the cropped images. The prompts have the form \"A picture of a <state> <object>\"."}, {"title": "B.2 Hierarchical Instruction Generation", "content": "NILS produces keystates and language annotations, which are contextually grounded, for single tasks. By combining the templated language descriptions of consequent keystates into a single prompt, the framework can generate high-level language instructions. This allows for granularity control over the produced tasks, which can be beneficial for policy learning. [53]. We illustrate three examples from the Bridge Dataset in Figure 7."}, {"title": "C Ablations", "content": ""}, {"title": "C.1 Stage 1: Initial Object Retrieval Ablations", "content": "To reason about robot-object interactions, it is crucial to initially capture all objects in the scene reliably. Our initial object detection is based on a multiple-frame consensus that reliably works in different settings and for different objects. To assess the effectiveness of our approach, we create a diverse dataset consisting of the BridgeV2 and our Kitchen Play dataset. In total, the evaluation dataset comprises 185 objects. We compare our approach with different VLMs and two baselines. The first baseline queries a VLM based on a single frame. In OWLv2 + SigLIP, we utilize a predefined list of about 1400 objects commonly appearing in kitchen environments. Then, we generate"}, {"title": "D Example Prompts", "content": "We give example prompts used to generate the list of potential tasks given a list of objects in Figure 17. In Figure 18, an example prompt used to label the task a robot solved in between two keystates is visualized."}, {"title": "E Qualitative Examples", "content": "We show qualitative examples of our framework's produced natural language instructions on BridgeV2 [1] in Figure 21."}, {"title": "F Additional Experiments", "content": ""}, {"title": "F.1 Evaluation Details", "content": "To evaluate policy performance on the BridgeV2 dataset, we use both sim and real robot experiments using a 6-DoF WidowX robot arm. To evaluate perfomance on the Fractal robot, we rely on simulation only. Simulation experiments are conducted using the Simpler Environments [26], and real robot experiments are performed using scenes inspired by the Bridge V2 dataset [1]. One Simpler Task is also visualized in the right image of Figure 3. All Octo variants are trained for 300k steps on their respective label sets to ensure a fair comparison. In the real robot environment two tasks were tested: Move the spoon inside the towel and Place the sushi inside the [color] bowl, both with various distractors and different starting positions for each object. The tasks are visualized in Figure 11 in the Appendix. We report success rate and correct grounding, where correct grounding refers to the robot approaching and interacting with the task-relevant object."}, {"title": "F.3 Fractal Experiments", "content": "Furthermore, we conduct experiments with a dataset collected on a Google Robot [5]. We obtain the dataset from Open-X Embodiment [56] and label all 87k trajectories with NILS. We train Octo on the labeled dataset and additionally co-train a policy with BridgeV2 labeled by NILS, where we choose the dataset weights so each dataset appears equally often during training. We found that cotraining with BridgeV2 improves the performance on Fractal. We again evaluate the policies in SIMPLER [26], where the robot has to solve 3 tasks with several different variations."}, {"title": "F.4 Real Robot Experiments in the Toy Kitchen", "content": "The following section describes our real-world play kitchen environment conducted in a toy kitchen environment in detail. We collect play data through teleoperation in our robot kitchen environment. The setup is illustrated in Figure 19. The robot can solve 12 different tasks, as shown in Figure 20.\n\nWe evaluate the performance of the trained policies based on two metrics: Success Rate. We perform each task three times and calculate the average number of successful task completions. We then compute the average success rate over all tasks. Correct Grounding. We evaluate whether the policy correctly understands language instructions. The task does not need to be completed successfully. The robot only has to show that it correctly understood the task. For instance, if the robot approaches the oven and tries to open it but fails, we label the task as correctly grounded. We again compute the average over all possible tasks."}, {"title": "G Limitations", "content": "Perception. NILS demonstrates the ability of off-the-shelf specialist models to annotate challenging long-horizon data. The major limitations of our framework are induced by these off-the-shelf models. Commonly used robotic environments and their contained objects are still very challenging for state-of-the-art models as most of them have not been trained on robot domain data. For instance, common evaluation environments in robotics are toy kitchens. Open-vocabulary detectors often struggle with grounding in such environments. For instance, NILS frequently detects the banana as a sponge in our toy kitchen setup. While there are models specifically applicable to the robotic domain, such as Spatial-VLM [20], RoboVQA [17] or PGBlip [58], these models are either not open-source or too specific for broader grounding applications. Lastly, the initial object retrieval might fail for abstract environments. However, this can be assessed through minimal human intervention. NILS allows users to specify objects they expect to appear in the scene, which would improve performance in these scenarios. Given the modularity of NILS, better models also result in a higher grounding accuracy.\n\nRuntime. Using multiple different models to generate scene representations introduces substantial computational cost. The inference time of our framework is significantly higher compared to tested baselines. NILS requires 7 minutes to label one long-horizon trajectory consisting of 50 tasks on a single 3090 RTX GPU. The framework is designed to be applied offline to prerecorded play data, thus the higher runtime should be manageable.\nAccuracy. While our framework shows good performance for different scenes and objects, it sometimes produces wrong labels. Although the heuristics used to detect keystates can filter noise effectively, they are sometimes correlated. In such cases, noise triggers all keystate heuristics, resulting in a high confidence keystate. Furthermore, NILS is designed for long-horizon demonstrations. To generate robust scene annotations, NILS relies on temporal consistency and diversity, which is not always given in short horizon demonstrations."}, {"title": "H Extended Related Work", "content": ""}, {"title": "H.1 Learning from Play", "content": "Robots usually learn from short-term demonstrations of a single task, but this approach has drawbacks such as lack of diversity, inefficient data collection [17], and poor generalization. As an alternative, Learning from Play [59] proposes collecting unconstrained operator interactions in the environment, resulting in more diverse data and better generalization. Extracting goal states from these long demonstrations is necessary for goal-conditioned imitation learning. While sampling random windows or using robot proprietary information can provide image goals [59, 60, 11], extending to language goals is challenging as they must align with observations. Most methods use joint goal spaces to embed multiple modalities [2, 61, 49, 62, 57, 63], but still require some language annotations. Our approach densely predicts actions performed in long-horizon trajectories to"}, {"title": "I Keystate Visualizations", "content": ""}, {"title": "J Foundation Model Prompts", "content": ""}]}