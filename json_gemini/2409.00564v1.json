{"title": "Using Deep Learning to Design High Aspect Ratio Fusion Devices", "authors": ["P. Curvo", "D. R. Ferreira", "R. Jorge"], "abstract": "The design of fusion devices is typically based on computationally expensive simulations. This can be alleviated using high aspect ratio models that employ a reduced number of free parameters, especially in the case of stellarator optimization where non-axisymmetric magnetic fields with a large parameter space are optimized to satisfy certain performance criteria. However, optimization is still required to find configurations with properties such as low elongation, high rotational transform, finite plasma beta, and good fast particle confinement. In this work, we train a machine learning model to construct configurations with favorable confinement properties by finding a solution to the inverse design problem, that is, obtaining a set of model input parameters for given desired properties. Since the solution of the inverse problem is non-unique, a probabilistic approach, based on mixture density networks, is used. It is shown that optimized configurations can be generated reliably using this method.", "sections": [{"title": "1. Introduction", "content": "Stellarators are a type of magnetic confinement fusion device that have toroidal geometry and are non-axisymmetric (see Fig. 1). Stellarators are inherently current-free, enabling steady-state plasma operation. Because of this, they are one of the leading candidates for future fusion energy power plants (Boozer 2020). In these devices, the magnetic field is twisted by a rotation of the poloidal cross-section of stretched flux surfaces around the torus, and by making the magnetic axis non-planar (Spitzer 1958; Mercier 1964). Stellarators are inherently current-free, enabling steady-state plasma operation. Because of this, they are one of the leading candidates for future fusion energy power plants (Boozer 2020). Due to their complex geometries, stellarators may present difficulties in confining charged particles, especially alpha particles resulting from fusion reactions (Helander 2014). Therefore, they need accurately shaped magnetic fields to confine trapped particles effectively. To achieve this, their configurations are usually optimized using numerical methods. However, the optimization process is complex due to the high-dimensional space of plasma shapes, which includes numerous local minima (Bader et al. 2019). While local optimization algorithms can find specific configurations, they do not offer a global view of the solution space. The high dimensionality makes global optimization challenging and renders comprehensive parameter scans impractical (Landreman 2022).\nTo address these challenges, a near-axis method is commonly employed (Garren & Boozer 1991a; Landreman et al. 2021; Landreman & Jorge 2020). This method makes use of an approximate magnetohydrodynamic (MHD) equilibrium model by expanding in powers of the distance to the axis, leading to a small set of one-dimensional ordinary differential equations (Landreman 2022), therefore reducing the computational costs significantly (Mercier 1964; Solov'ev & Shafranov 1970; Garren & Boozer 1991b). As a result,"}, {"title": "2. Physical Model", "content": "In this section, we describe the near-axis expansion method used to find quasisymmetric stellarators. Quasisymmetry is an effective strategy for confining trapped particles (Helander 2014; Nuhrenberg & Zille 1988) and consists of a continuous symmetry of the magnitude B of the magnetic field B that yields a conserved quantity and enhances particle confinement. Near the magnetic axis, two types of quasisymmetry are possible, namely quasi-axisymmetry (QA), where $B = B(r,\\theta)$ and quasi-helical symmetry (QH), where $B = B(r, \\theta - N\\varphi)$. Here, $(\\theta, \\varphi)$ are the Boozer poloidal and toroidal angles (Boozer 1981), N is an integer, r is defined as $r = \\sqrt{2\\psi}/B_0$ where $\\psi$ represents the magnetic toroidal flux and acts as a radial coordinate and $B_0$ is the magnetic field strength on the magnetic axis.\nThe method for generating stellarator configurations in a near-axis expansion complements traditional stellarator optimization, which typically involves parameterizing the boundary shape of a finite aspect ratio plasma and using a 3D MHD equilibrium code to evaluate the objective function. Instead, in the near-axis method, we parameterize the axis curve and find the Taylor series coefficients of B in powers of r that allow for quasisymmetry (Garren & Boozer 1991b; Landreman & Sengupta 2019; Jorge et al. 2020). While the near-axis method is necessarily approximate, it is orders of magnitude faster than standard methods, with a reduced parameter space, therefore allowing for broader parameter scans. Ultimately combining both approaches can be advantageous: the near-axis method can identify viable configurations, which can then be refined through conventional optimization.\nIn the near-axis expansion method, the magnetic axis $r_0 = R(\\varphi)e_R+Z(\\varphi)e_z$ is typically represented in cylindrical coordinates (R, Z, $\u03c6$) using a finite Fourier series,\n$R(\\varphi) = \\sum_{n=0}^{N_F} R_{cn} cos(n f_p \\eta\\varphi), \\quad Z(\\varphi) = \\sum_{n=1}^{N_F} Z_{sn} sin(n f_p n\\varphi),$ \nwhere $n f_p$ is the number of field periods and a finite maximum Fourier number $N_F$ is chosen. Stellarator symmetry is assumed. The remaining input parameters are the coefficients of the magnetic field strength\n$B=B_0 [1 + r\\tilde{\\eta} cos(v)] + r^2[B_{20} + B_{2c} cos(2v)],$ \nnamely $B_0, \\tilde{\\eta}$ and $B_{2c}$, and the plasma pressure\n$p = p_2 r^2,$ \nwith $v = \\theta - N\\varphi$. Here, $B_0$ is chosen to be 1T and, following Landreman & Sengupta (2019), $B_{20}$ is taken to be a function of $\u03c6$, with exact quasisymmetry corresponding to $B_{20}$ being a scalar constant. The total plasma current on-axis is taken to be $I_2 = 0$. Henceforth, the input parameter space for optimization consists of {$R_{cn}$, $Z_{sn}$, $Nf_p$, $\\tilde{\\eta}$, $B_{2c}$, $p_2$}, as described in Table 1. The magnetic field equilibrium and plasma pressure are related via the ideal MHD equation $J \u00d7 B = \\nabla p$ with $J = \\nabla \u00d7 B/\u03bc_0$ the plasma current."}, {"title": "3. Mixture Models and Density Networks", "content": "When dealing with non-unique inverse problems, we often encounter situations where there are multiple possible solutions for a given input. To effectively address these problems, it is essential to have a statistical distribution over the possible solutions rather than a single deterministic answer. The normal distribution is a common way to construct probability distributions, but in cases with multiple solutions, we require a multi-modal distribution, which can be achieved through a mixture model (McLachlan & Basford 1988). This model provides concentrated probabilities at various points, representing the\ndifferent solutions. In this section, we describe the probabilistic models used in this work, namely mixture models, Gaussian models, and multivariate Gaussian mixtures.\nA mixture model (McLachlan & Basford 1988) is a statistical tool used to describe a population comprised of multiple subgroups without prior knowledge of individual data point memberships. It constructs a combined probability distribution for the entire population by integrating the probability distributions of each subgroup. Mixture models enable us to understand the characteristics of these subgroups using data from the entire population, even when the subgroup for each data point is unknown. These models are typically applied in clustering tasks, where data points are grouped into clusters, and density estimation, which involves estimating the distribution of the data itself.\nA typical finite-dimensional mixture model $p(y|\\lambda)$ is a combination of simple distributions $p_i(y)$ that can be represented as follows\n$p(y|\\lambda) = \\sum_{i=1}^K \\pi_i p_i(y),$ \nwhere $p_i$ is the $i^{th}$ component distribution, $\u03c0_i$ is the mixture weight of the $i^{th}$ component, and K is the number of components in the mixture. The mixture weights are non-negative and sum to 1, i.e., $0 \u2264 \u03c0_i \u2264 1$ and $\\sum_i \u03c0_i = 1$.\nTo better understand mixture models, we re-express the model in a hierarchical framework. This involves introducing a latent variable $z \u2208 {1, ..., K}$ representing the component from which each data point is generated. This hierarchical approach not only provides a clear structure but also facilitates the inference process. Henceforth, each data point y is associated with a latent variable z that indicates the component it originates from. The prior distribution over the latent variables is governed by the parameters $\u03c0 = (\u03c0_1, ..., \u03c0_K)$, where $\u03c0_i$ represents the probability that a data point belongs to component i. Formally, we write\n$p(z = k|x) = \u03c0_k.$\nGiven that a data point y comes from component i, it is generated according to a component-specific distribution $p(y|\u03bb_i)$. Thus, the conditional distribution of y given the latent variable z and the parameters \u03bb is\n$p(y|z = i, \u03bb) = p_i(y) = p(y|\u03bb_i).$"}, {"title": "4. Mixture Density Networks", "content": "Neural networks are computer models inspired by the structure of the human brain (Hornik et al. 1989). They are made up of layers of connected neurons or nodes. Such layers are used to process input data, with each neuron applying an activation function and a weighted sum to produce an output. Through training, neural networks can discover intricate patterns and relationships in data.\nHowever, in problems involving continuous variables where the same input values may produce different output values, neural networks tend to predict the mean of the target variable. This can be regarded as an approximation to the conditional average of the target variable given the input. This conditional average provides a very limited description of the statistical properties of the data and is often inadequate for many applications. This is particularly true for non-unique inverse problems, where a conventional neural network with a least-squares approach might yield an inaccurate solution as the mean of multiple, possibly more accurate solutions.\nIn our case, averaging parameters such as $R_{cn}$ and $Z_{sn}$ tends to yield suboptimal results due to their complex interdependencies. Both variables exhibit multimodal distributions centered around symmetric values. Averaging these values tends to converge toward zero, which may lead to the generation of bad stellarator designs. Consequently, there is a need for a neural network to be capable of probabilistically selecting $R_{cn}$ or $Z_{sn}$ from their respective subdistributions, depending on the context. This leads to the use of a probabilistic model capable of representing multimodal distributions. Such a model would not average the distributions but instead sample from them, thereby preserving the distinct characteristics of each mode and enabling more accurate predictions and good stellarator designs.\nTo address these requirements, Mixture Density Networks (MDNs) (Bishop 1994) present a compelling solution. MDNs are a class of neural networks designed to overcome the limitations of conventional neural networks in modeling complex, multi-modal data distributions. They combine the flexibility of neural networks with the robustness of mixture models, where the neural network estimates the parameters for the mixture model. MDNs allow a neural network to learn arbitrary conditional distributions as opposed to only learning the mean. This enables MDNs to provide a more comprehensive and accurate modeling approach for complex data distributions.\nIn MDNs, the probability density of the target data is represented as a linear combination of components, as in Eq. (3.1). Various choices for these components are possible, but for the purpose of this work, we focus on MGMMs, as in Eq. (3.6), to approximate the conditional distribution of the target variables given the inputs, because, as seen in Section 3, it effectively captures complex data structures where variables are interdependent, and excels in accurate density estimation for multivariate data, which is crucial for our case.\nFor any given values of the input x, the MDN provides a systematic method for modeling an arbitrary conditional distribution p(y|x). The model parameters, namely the"}, {"title": "5. Data Generation and Training", "content": "To train the MDN, we generate a dataset of stellarators using the near-axis expansion method. The dataset is a collection of records containing the input parameters provided to the near-axis method and corresponding output properties generated from these inputs.\nTo generate the dataset, we sample the input parameters from uniform distributions, with the ranges listed in Table 5 and find the output parameters listed in Table 2. By sampling the input parameters from uniform distributions, we find that most configurations consist of bad stellarators. In fact, by applying the set of criteria shown in Table 3, it is seen that the percentage of good stellarators is extremely low, with only 1 in approximately 100,000 samples found to comply with all the desired criteria. This illustrates how difficult it is to find good stellarators by random search, and is one of the main drivers of the use of an inverse model to find the input parameters from a set of desired properties.\nFollowing the generation of the dataset, we begin by normalizing the dataset using a standard scaler to account for the different scales of the input and output parameters. The dataset was then split into training and validation sets with an 80% and 20% split, respectively. Next, we initialize the weights of the neural network using the Glorot-Xavier initialization method (Glorot & Bengio 2010), which is effective for deep neural networks as it helps prevent vanishing or exploding gradients during training. Additionally, we employ the Adam optimizer (Kingma & Ba 2017) with a learning rate of 10-3 and a batch size of 10,000 samples.\nThe output properties are then sampled from the mixture model. We compute the negative log-likelihood of these samples, which serves as the loss function to be minimized during training. Since we use a mixture model composed of multiple Gaussian components, the loss function is given by\n$Loss = -\\frac{1}{N} \\sum_{j=1}^N log \\sum_{i=1}^K \\pi_i N(y_j| \\mu_i, \\Sigma_i),$ \nwhere N is the number of samples, i.e., the batch size, and $y_j$ is an output vector.\nDespite using the Adam optimizer (Kingma & Ba 2017), the training process was more challenging than anticipated due to numerical instabilities, such as vanishing gradients, that caused the covariance matrices to become non-positive definite. To address this issue, a learning rate schedule was employed. The network was initially trained with a higher learning rate and then fine-tuned with a lower learning rate. This learning rate was decreased multiple times using a scheduler during the training until the network converged to the data distribution. The loss and validation curves can be seen in Fig. 4. Notably, the curves indicate that as the learning rate decreases, the loss function values also decrease. This trend suggests that lower learning rates contribute to a more stable and gradual convergence, resulting in better model performance and lower loss."}, {"title": "6. Model Performance", "content": "We now show how the model can be used to predict the input parameters needed to obtain optimized stellarators with desired output properties. First, the user provides the desired properties such as plasma \u03b2 and rotational transform, and the model produces the design parameters that are likely to yield those properties such as magnetic axis and \u03b7. Then, the user feeds the predicted design parameters to the near-axis expansion method, to generate the corresponding properties. Finally, the user verifies that the actual properties generated by the near-expansion method agree with the desired properties. An example is shown in Table 7.\nHowever, while the model is able to yield configurations that satisfy the requirements listed in Table 3, it is not guaranteed that all configurations have a set of nested, non-intersecting flux surfaces up to the parameter rsingularity. This is because rsingularity is only a proxy for the minimum aspect ratio of the device. Only by computing the surface in Cartesian coordinates, as opposed to the near-axis Boozer coordinates used throughout this work, can we verify the existence of such a surface. Such an evaluation is crucial to use such configurations in practice. We then take all the good stellarators and generate a surface at a radial distance of r = 0.1Rco. Here, the existence of such a surface is defined as the existence of a numerical solution of the mapping from the toroidal Boozer coordinate on-axis to a cylindrical angle \u03d5 off-axis with tolerance at or below 10-15 after a maximum of 1000 iterations. We will refer to the good stellarators that meet this additional criterion as viable stellarators.\nNext, keeping the standard normalization on the dataset, we employed the Huber Loss and the Mean Absolute Error (MAE) as evaluation metrics to compare the predicted output properties from the model against the output properties from the near-axis model on 10,000 samples. Both are metrics used in regression tasks to quantify the difference between predicted values and actual observations. Huber Loss combines the advantages of MAE for robustness to outliers and Mean Squared Error (MSE) for sensitivity to small errors.\nAs illustrated in Table 8, the model accuracy was found to be satisfactory. For the variables axis length, \u03b9, max elongation, B20variation, and Dmerc \u00d7 r2, the model showed a good performance, evidenced by a low Huber and MSE losses, 0.172 and 0.486 respectively, with the MSE being higher than the Huber Loss, as expected. Regarding the variables min L\u2207B, min RO, and L\u2207\u2207B, the model displayed moderate accuracy under the Huber Loss metric. However, the MSE was higher, indicating that the model underperforms in these variables. The variables \u03b2 and rsingularity exhibited the poorest accuracy, with both metrics indicating suboptimal results.\nBeyond the model performance, understanding the relationships between variables is crucial for interpreting the behavior of output properties and their interdependencies. This knowledge significantly influences how the model should be used to predict input parameters. When output properties are strongly correlated, the model must carefully balance these correlations to achieve the desired outputs. Additionally, being aware of the distribution of variables is essential to ensure the model operates within familiar data spaces; otherwise, it may perform poorly. Therefore, analyzing the distributions of the variables and their correlations is vital.\nHenceforth, the iterative training process described in Section 5 was monitored to check if the distributions of both input and output variables were being restricted to a narrower space, which was to be expected since we wanted to restrict the dataset to the space of good stellarators. We evaluated the distribution of variables for the dataset containing all the good stellarators and all the viable stellarators. We show in Figs. 5 and 6 the ones that provide a better understanding of the dataset and that are more relevant.\nThe distribution of the nfp variable for both the good stellarators and the viable stellarators is depicted in Fig. 6. The data shows that good stellarators tend to cluster around nfp = 4, although there is a notable variation with several other nfp values present. An aspect of these results is that the model, despite being trained on a dataset where the nfp ranged from 1 to 10, successfully predicted nfp values for good stellarators that exceeded this range. As illustrated in Fig. 6, there are configurations with nfp values extending up to 19.\nFor the viable stellarators, the distribution of the number of field periods, nfp, is more narrowly centered around the value of 3, and none of the configurations exhibit nfp values above 6. This suggests a more constrained and specific range for nfp in the viable stellarator subset, indicating that these configurations are more consistent in this regard.\nWe also examine the B2c parameter. The distribution of this variable for both good stellarators and viable stellarators is illustrated in Fig. 6. The data reveals that B2c exhibits a noticeable shift towards negative values. This indicates a distinct characteristic in the B2c distribution for good stellarators compared to the overall dataset.\nThe observed shifts in the distributions of variables for the good stellarators and the viable stellarators, whether towards negative or positive values, suggest that maximizing or minimizing certain variables can influence others in similar or opposing ways. This prompts us to evaluate the correlations between variables. While correlation does not imply causation, it provides valuable insights into the relationships between variables. We show in Fig. 7 the correlation matrix for the output properties of good stellarators, which is similar to viable stellarators. This matrix reveals a strong positive correlation between rsingularity and L\u2207\u2207B. This indicates that as the axis length increases, the maximum elongation also increases. Conversely, the min L\u2207B and B20variation display a strong negative correlation, meaning that an increase in the minimum min L\u2207B results in a decrease in the minimum B20variation. These relationships significantly impact model performance, as the model must balance them to achieve the desired properties. As an example, if a user requests a stellarator with a high min L\u2207B and a low B20variation, the model must navigate the positive correlation between these properties. Since they are not independent, the model must find a compromise to generate appropriate input parameters that align with the desired output properties."}, {"title": "7. Conclusions", "content": "This work introduces an MDN designed to tackle the inverse stellarator optimization problem using the near-axis method. The model was trained on a dataset of near-axis configurations generated through the near-axis expansion method. However, the dataset initially contained a very low percentage of desirable stellarators, specifically only 0.001%. To address this limitation, an iterative data augmentation technique was employed. This iterative approach successfully enhanced the representation of high-quality stellarators within the dataset, thereby improving the model's capability to predict parameters crucial for optimal stellarator designs.\nDespite achieving good performance in predicting some variables, the model faced challenges with variables derived from the second-order near-axis expansion method, as assessed using Huber Loss and Mean Absolute Error (MAE) metrics. Nevertheless, overall, the MDN proved effective as a tool for predicting desired properties of stellarators."}]}