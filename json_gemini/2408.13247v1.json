{"title": "Data Exposure from LLM Apps: An In-depth Investigation of OpenAI's GPTs", "authors": ["Evin Jaff", "Yuhao Wu", "Ning Zhang", "Umar Iqbal"], "abstract": "LLM app ecosystems are quickly maturing and supporting a wide range of use cases, which requires them to collect excessive user data. Given that the LLM apps are developed by third-parties and that anecdotal evidence suggests LLM platforms currently do not strictly enforce their policies, user data shared with arbitrary third-parties poses a significant privacy risk. In this paper we aim to bring transparency in data practices of LLM apps. As a case study, we study OpenAI's GPT app ecosystem. We develop an LLM-based framework to conduct the static analysis of natural language-based source code of GPTs and their Actions (external services) to characterize their data collection practices. Our findings indicate that Actions collect expansive data about users, including sensitive information prohibited by OpenAI, such as passwords. We find that some Actions, including related to advertising and analytics, are embedded in multiple GPTs, which allow them to track user activities across GPTs. Additionally, co-occurrence of Actions exposes as much as 9.5\u00d7 more data to them, than it is exposed to individual Actions. Lastly, we develop an LLM-based privacy policy analysis framework to automatically check the consistency of data collection by Actions with disclosures in their privacy policies. Our measurements indicate that the disclosures for most of the collected data types are omitted in privacy policies, with only 5.8% of Actions clearly disclosing their data collection practices.", "sections": [{"title": "1. Introduction", "content": "Large language model (LLM)-based platforms, such as ChatGPT [1] and Gemini [2], are increasingly supporting third-party app ecosystems [3], [4]. While third-party LLM apps enhance the functionality of LLM platforms, they may also pose significant risks to user privacy. As it has been the case in other computing platforms, third-party apps and external services embedded in them collect excessive user data, often more than it is needed to provide essential services [5], [6], [7], [8]. In LLM platforms, the risks from third-party apps may be exacerbated because of the natural language-based execution paradigm of LLMs. For example, user's main mode of interaction with LLMs is information-rich natural language, which can be processed to infer several characteristics about the user, such as their age or interests [9], [10]. Furthermore, malicious LLM apps can launch straightforward attacks (e.g., with prompt injection [11]) to access information beyond their one-to-one interactions with the user, as LLMs automatically load prior user interactions in their execution environment (i.e., context window) to provide a contextually relevant responses [12].\nLLM platforms moderate the practices of apps through their policies [13], [14], [15], however, these polices are currently mostly limited, optional, or not strictly enforced [16], [17], [18]. For example, prominent platforms, such as OpenAI, currently state that they may not review the apps hosted on their platforms [15]. Anecdotal evidence suggests that policy violating apps are already hosted on such platforms, and only removed when publicly brought to attention [19]. Vendors are also constantly improving their platforms. For example OpenAI, has recently completely revamped its LLM app ecosystem with more restrictions to improve their security and privacy posture [20]. For example, LLM apps (referred to as GPTs [3]) and external services embedded in them (referred to as Actions [21]), now need to host their specifications on the OpenAI's back-end and can no longer be self-hosted [22]. However, we also note that at the same time, OpenAI has removed restrictions on use cases, such as advertising, which often require personal and excessive user data [23], [14].\nGiven the potential for privacy issues due to the limited polices and their lack of enforcement in LLM platforms, in this paper we aim to bring transparency in data practices of LLM apps. As a case study, we study OpenAI's GPT ecosystem, as it is the largest LLM app ecosystem with more than 3 million GPTs [24]. At a high level, we (i) first survey GPTs and Actions, (ii) characterize their data collection practices, (iii) measure potential indirect data exposure across GPTs and their Actions, and (iv) check the consistency of data collection practices with disclosures in privacy policies of GPTs and Actions.\nWe crawl a total of 119,274 GPTs and 2,596 unique Actions embedded in them from third-party and the OpenAI's official app store, over four months (our crawling is still ongoing). Since GPTs and their Actions define their functionality, including their data collection, in natural language, we rely on static analysis to characterize their data collection practices. However, static analysis requires addressing the challenge of assigning succinct data types to the detailed and potentially vague natural language descriptions. To that end, we build an LLM-based tool, that takes a natural language data type description as input, and outputs a succinct data type and its associated data category, based on a data taxonomy that we provide it as a knowledge base.\nWe also note that some GPTs embed several Actions, and some Actions are embedded across several GPTs. Since"}, {"title": "2. Background & Motivation", "content": null}, {"title": "2.1. OpenAI GPTs", "content": "In this paper we study the OpenAI's GPT (app) ecosystem, the most mature third-party LLM app ecosystem with more than 3 million GPTs [24]. OpenAI provides GPTs the ability to customize the behavior of the LLM, browse the web, generate images, interpret code, search files, and connect to the APIs of external online services. Browsing (i.e., Web Browser), image generation (DALLE), code interpretation (Code Interpreter), and file searching (Knowledge) are built-in tools and provided by OpenAI [3], whereas connection to external APIs are implemented as custom tools, which are referred to as Actions [21]. Actions are akin to third-party services on the web, such as analytics, JS wrappers, CDNs, that websites embed to enhance their offerings.\nBuilt-in tools can be enabled by clicking check-boxes on the GPT creation interface [30], whereas Actions need to be implemented as HTTP APIs and exposed to OpenAI in a JSON format [21]. The JSON format of Actions describes the functionality offered by each API, including its data types, as natural language descriptions (Appendix A lists the source code of a GPT with an Action). GPTs also define their functionality in natural language and interface with the LLM, their tools, the user, and other GPTs through natural language instructions. To build the necessary context to use a GPT, LLMs inject the natural language-based source code of GPTs in their context window, when users install and interact with GPTs."}, {"title": "2.2. Privacy risks", "content": "While third-party apps extend the capabilities of computing platforms, they also pose several risks to user privacy. For example, in almost all online computing platforms, such as the web, mobile, and IoT, it is a standard practice for third-party apps to collect excessive user data, often with other specialized third-party services, for the purposes of profiling users for personalized online advertising [5], [6], [7], [8]. We worry that the GPTs might also engage in similar practices on the OpenAI's platform. In fact, GPTs are already including specialized third-party Actions to track users (as we show later in Section 5.2.2).\nOpenAI currently imposes some restrictions [13], [14], [15] on GPTs but they are mostly limited, optional, or not strictly enforced [16], [17], [18]. For example, OpenAI currently does not implement any foolproof access control mechanisms, and leaves it up to the developers to define permission interfaces for activities performed by the GPTs, which may not be reviewed [15]. There are already instances where policy violating apps were hosted on OpenAI and only removed when publicly brought to attention [19]. Furthermore, OpenAI also intends to use user's interaction with the GPTs, i.e., to train its models [32]. Although, OpenAI provides users' controls to delete their data [33], these controls may not extend to third-party GPTs, as OpenAI may not have visibility or control over the data exfiltrated by the Actions inside GPTs.\nPrivacy risks may be further exacerbated in LLM platforms because of the natural language-based execution paradigm of LLMs. For example, user's main mode of interaction with LLMs is information rich natural language, which can be processed to infer several characteristics about the user, such as their age or interests [9], [10]. Furthermore, malicious GPTs can launch straightforward attacks (e.g., with prompt injection [11]) to access information beyond their one-to-one interactions with the user, as LLMs automatically load prior user interactions in their context window to provide a contextually relevant response [12]."}, {"title": "2.3. Our goal", "content": "Given the potential for privacy issues and their harms to the users, this paper aims to bring transparency in the OpenAI's third-party app ecosystem. More specifically, our goal is to characterize the privacy practices in the OpenAI's GPT ecosystem, including (i) surveying GPTs and Actions embedded in them, (ii) characterizing their data collection practices, (iii) measuring potential indirect data exposure across GPTs and their Actions, and (iv) checking the consistency of data collection practices with disclosures in privacy policies of GPTs and Actions.\nWe conduct a four-month long periodic weekly crawls of GPTs from February 8th to May 3rd 2024, to measure their evolution across several axes (Section 4). To characterize data collection by GPTs and their actions, we rely on static code analysis, as GPTs and Actions need to state their data collection in natural language, so that it can be interpreted and acted upon by LLMs (Section 3). Furthermore, we analyze the indirect exposure of data across Actions because of embedding of multiple Actions in GPTs by modeling Action co-occurrence as a graph (Section 5.3). Lastly, to measure the consistency between the data collection by GPT Actions and disclosures in their privacy policies, we develop an LLM-based privacy policy analysis framework (Section 6). With these measurements, our goal is to build an informed understanding of the third-party app ecosystems in LLM platforms. We envision such measurements to serve as a guide to inform the design of current and future integrations of third-party services in LLM platforms, to improve their privacy."}, {"title": "3. GPT crawling", "content": "We first crawl a large number of GPTs from the OpenAI and third-party GPT stores and present their census, including their growth and tool usage trends."}, {"title": "3.1. GPT marketplaces", "content": "Since OpenAI does not provide any interfaces to download GPTs hosted on their platform, we rely on several third-party GPT stores that index a large number of GPTs. We"}, {"title": "3.2. Crawling process", "content": "We implemented selenium-based [36] crawlers for each of the third-party store to extract links to the GPTs. After extracting the links, we process them to extract the GPT identifiers, and then send a request to an OpenAI API endpoint with the GPT identifier\u00b9 that returns the JSON specification of a GPT. If the GPT identifier is not associated with a publicly available GPT, OpenAI returns a 404 error code. We also crawl a small number of featured GPTs listed on the OpenAI's official GPT store. The downloaded JSON specifications of GPTs describe their functionality in natural language, including the endpoints contacted by Actions, and the data exfiltrated by them (Appendix A lists the source code of a GPT with a third-party Action).\nAfter crawling GPTs, we download the privacy policies of their Actions by requesting the URL in the legal_info_url field in their specifications. We successfully crawl 98.9 \u00b1 1.7% GPTs and 91.5 \u00b1 2.3% privacy policies of GPT Actions, over four months. We are unable to crawl the remaining GPTs and privacy policies due to internal server errors and server unresponsiveness. In total, we crawl 119,543 unique GPTs from all of the GPT stores."}, {"title": "4. GPT census", "content": "After crawling the GPTs, we first analyze their growth trends on third-party stores over time. From Figure 3, we note that new GPTs are frequently listed on stores, with a mean increase rate of 4.5% over each week. We also note"}, {"title": "4.1. GPTs modify their functionality but likely do not change it altogether", "content": "We note that several GPTs are modified over time, either because they are changed by their developers or because some of their metadata is changed by OpenAI, such as ratings and usage statistics. We note that some modification (e.g., metadata and Actions/-Files) could be more consequential than the others (e.g., contact information) in altering a GPT's functionality. We investigated all such instances, i.e., modifications to metadata and Actions/Files related properties. However, none of these modifications indicated a functionality change and most seem to be related to performance/accuracy tweaks."}, {"title": "4.2. Some of the GPTs that no longer exist violated OpenAI's policies", "content": "Next we analyze the removed GPTs to assess if the reason for their removal were problematic behaviors. We consider a GPT to be removed if it is no longer present on the third-party GPT stores and also inaccessible on ChatGPT. In total, we note that 2,883 GPTs were removed from the GPT store during our crawl period.\nSince our goal is to reliably assess the potential reasons for the removal of GPTs, we resort to manual investigation. We specifically emphasize on GPTs that embed Actions because they present the potential for most harms as they connect to potentially untrustworthy third-party services on the internet and load unvetted content. Our manual review process involves two human coders first independently analyzing a small set of GPTs to generate a code book, and then independently analyzing GPTs using that code book. At a high level, the code book contains rules that characterize GPTs functionalities, including their data collection practices and their content generation practices. This characterization requires us to analyze the natural language functionality description of the GPTs and their API endpoints, individually using them in ChatGPT, and also interacting with their API endpoints."}, {"title": "4.3. Many GPTs connect to third-party services on the internet", "content": "We note that almost all (97.5%) GPTs include tools; with most popular integration being the Web browser with 92.3%, followed by DALL-E with 85.5%, Code interpreter with 53.0%, Knowledge (Files) with 28.2%, and Actions with 4.6%. A significant majority (93.2%) of GPTs connect to online services through Web Browser and Actions. Specifically, the Web Browser tool allows to consume content from any webpage on the internet and Actions allow to connect to specific online services. While these tools extend the capabilities of GPTs, they also expose users to unvetted online content on the Internet, threatening user security and privacy [48], [17]. In the case of Actions, these risk may be further exacerbated as a significant number of Actions in"}, {"title": "5. GPT data collection analysis", "content": "In this section, we analyze data collection practices of GPTs. We specifically emphasize on GPTs that embed Actions, because GPTs can only contact external online services with Actions, to exfiltrate data outside OpenAI's ecosystem."}, {"title": "5.1. Overview of collected data", "content": "We first present an overview of the data collected by the Actions embedded in GPTs. As Actions describe the data collected by each API endpoint in natural language descriptions, we rely on static analysis, to sufficiently capture their data collection practices. However,"}, {"title": "5.1.2. Actions collect expansive data, including sensitive information prohibited by OpenAI", "content": "We first plot the number of data items collected by each Action in Figure 4. We note that 25.57% and 39.77% of Actions collect 5 or more succinct (as determined by our LLM-based tool) and raw data types, respectively. Additionally, there are 4.35% and 18.82% of Actions that collect 10 or more succinct and raw data types, respectively. We next analyze specific data types that are excessively collected by Actions.\nWe note that the Actions collect a wide range of expansive data spanning across 14 different categories. A significant number of Actions collect data related to user's app activity, personal information, and web browsing. App activity data consists of user generated data (e.g., conversation and keywords from conversation), preferences or setting for the Actions (e.g., preferences for sorting search results), and information about the platform and other apps (e.g., other actions embedded in a GPT). Personal information includes demographics data (e.g., Race and ethnicity), PII (e.g., email addresses), and even user passwords; web browsing history refers to the data related to websites visited by the user using GPTS.\nWe note that several of these data types pertain to sensitive user data and their collection is prohibited by OpenAI [14], [15]. For example, OpenAI prohibits the collection of information such as passwords and API keys, but we note that at least 1% of GPTs that embed Actions (in our crawl), collect user passwords, for the purposes of signing into online services or managing online services on user's behalf. Since OpenAI may use user-to-GPT interaction data for training its models [32], the collection of sensitive user data not only exposes users to harms from third-party developers but also from arbitrary attackers, who can extract training data from LLMs, as it has been shown by prior work [53], [54]\nWe also note that OpenAI requires GPTs to comply with applicable legal requirements while collecting personal user data [14], [15]. However, we found that OpenAI does not provide GPTs sufficient controls that they can offer to users so that they can exercise their rights. For example, prominent data protection regulations, such as GDPR and CCPA [55], [56], require online services to provide users controls to opt out of usage or selling of data [57], but in our testing in respective jurisdictions, we did not find such controls being offered to the users.\nOverall, we note that OpenAI's GPT app ecosystem is already supporting complicated use cases, that require collecting expansive data types, indicating a quick maturing, especially relative to other emerging computing platforms, such as the VR [8] and smart speakers [7] ecosystems. Although, OpenAI is revising its polices to catch up with the rapid development of its third-party app ecosystem, our measurements indicate that these efforts may not be sufficient, as many problematic GPTs continue to exist on OpenAI's store."}, {"title": "5.2. Attributing data collection", "content": "Next, we analyze Actions that collect user data, including analyzing their practices and offerings."}, {"title": "5.2.1. GPTs mostly embed third-party Actions, some of which dynamically load other Actions", "content": "Form we note that GPTs mostly embed third-party Actions which collect extensive data including personal user information. While in most instances these Actions are directly integrated by GPT developers, we encountered two instances where Actions had capability to dynamically load other third-party Actions. Specifically, Zapier [58] listed that it can \"Equip GPTs with the ability to run thousands of actions via Zapier\" and JustPaid [59] listed that it can \"Equip GPTs with the ability to run actions via JustPaid\" (with currently only supporting stripe and accounting).\nAlthough, integration of third-party services is a common practice on computing platforms, such as the web and mobile, they often exacerbate the privacy risks posed to the users [5], [6]. For example, advertising and tracking third-party services are known to dynamically embed 100s of other third-party services to share user information with each other, e.g., through cookie syncing [5], [60]. To mitigate such concerns, platforms are making active efforts to restrict the inclusion of dynamically loaded code in apps. For example, Google Chrome no longer allows to include remotely hosted code in browser extensions [61], [62]. Although OpenAI's GPT ecosystem is still nascent, it has a unique opportunity to learn from earlier platforms and enhance its security and privacy measures from the outset."}, {"title": "5.2.2. Some GPTs are embedding third-party Actions to track users and serve them advertisements", "content": "Next, we analyze data collection practices and the functionality offered by prevalent third-party Actions. We note that some third-party Actions are widely deployed across GPTs. Among these, webPilot [63] is the most prevalent Action which"}, {"title": "5.3. Indirect data exposure", "content": "Since Actions execute in shared memory space in GPTs, they have unrestrained access to each others data, which allows them to access it (and also potentially influence each others execution) [25], [17]. Thus, in this subsection, we analyze the indirect exposure of user data due to integration of multiple Actions in GPTs, given the lack of isolation in ChatGPT."}, {"title": "5.3.1. Action co-occurrence across several GPTs, without proper isolation, enables indirect data exposure", "content": "As Actions are embedded in multiple GPTs, they are in a position to connect user data collected across multiple GPTs, in different contexts. This is a common practice on other computing platforms, such as the web, where specialized third-party services are embedded on websites that collect and connect users browsing history across several websites, often referred to as cross-site tracking [5], [60]. It is currently unknown if third-party services embedded on GPTs also engage in similar practices, but since the have the ability to do so, we measure the potential data sharing that can happen because of the presence of Actions across multiple GPTs.\nTo that end, we create a graph to understand the potential information sharing relationships between different Actions. In our graph representation, nodes represent Actions and the edges represent their appearance in a GPT. Note that edges are undirected and weighted, such that the weight is incremented by one if the same Action pair co-occurs again in another GPT. Also, we make the size of a node, proportional to its weighted degree and use a color gradient to represent the edge weights, such that the darker color represents higher weight."}, {"title": "5.3.2. Co-occurrence exposes Actions to as much as 9.5\u00d7 more data than they were individually exposed", "content": "Next, we measure the increase in the exposure of data types to additional Actions, as a function of multiple Actions co-occurring in GPTs. On average, the data exposure increases for all data types by 2.3% at first degree connections and by 4.3% at second degree connections. From the table, we note that user IDs and settings or parameters have the highest exposure across both the first and second degree co-occurrences.\nWe next analyze increased exposure of data to the most prevalent co-occurring Actions."}, {"title": "6. GPT privacy policy analysis", "content": "In this section, we analyze whether GPTs and their Actions disclose their data collection practices in their privacy policies."}, {"title": "6.1. Privacy policies overview and availability", "content": "OpenAI mandates, individual third-party Actions embedded in GPTs, to provide privacy policies but does not require GPTs to provide a privacy policy that describes its data practices as a whole [21]. This approach deviates from the norm in other platforms, where the apps provide a privacy policy with information about their own practices, including information about third-party services that they embed. In OpenAI's ecosystem, to understand data practices of GPTs, users need to read the privacy policies of all of their third-party Actions. Since the GPT interface does not disclose the Actions embedded in them, and given that Actions can dynamically embed other third-party Actions (Section 5.2.1), users may simply be unaware of the existence of these Actions in GPTs, let alone their data practices.\nFor the purposes of analysis in this section, we analyze the privacy policy disclosures at the granularity of individual Actions."}, {"title": "6.2. Data disclosure analysis methodology", "content": "Our goal with the privacy policy analysis is to assess whether they contain disclosures about the data collection practices of Actions. To that end, we build on the automatic privacy policy analysis by prior work [26], [27], [8], [28], and leverage the recent advances in natural language processing [73] to develop an LLM-based framework to check the consistency of data collection disclosures.\nConsidering that LLMs are not always reliable and that their performance degrades with large context [29], we do not simply pass the large and complicated privacy policies to an LLM and probe it to measure the disclosures by GPTs. Instead, our framework takes a three step approach to analyze privacy policies. First, we tokenize the sentences in privacy policies [74] and pass individual sentences to an LLM to assess whether they pertain to data collection. Second, we pass (indexed) data collection statements to the LLM, so that it can build its context. Third, we pass the data items one-by-one to the LLM and ask it to provide its assessment about whether the data is disclosed in the passed sentences, as a two item tuple (i.e., <sentence index, disclosure type>). Overall, this process allows us to reliably associate the LLMs assessment about individual data types with individual sentences.\nWe label the disclosures either as: clear: If the data type description exactly matches a collection statement, vague: If the data type description matches a collection statement in broader terms, omitted: If there is no collection statement corresponding to the data type description, ambiguous: If there are contradicting collection statements about a data type description, incorrect: If there is a data type description for which the collection statement states otherwise. We further group these labels as consistent (i.e., consisting of clear and vague) and inconsistent (i.e., omitted, ambiguous, and incorrect) data flows (similar to prior work [27], [8]). To enable the LLM to assign one of these labels, we provide it several examples of these cases in a prompt template [51].\nSince we assign multiple labels to each data type (per each data collection statement in the privacy policy), we next process the labels to assign it the most precise label, such that if consistent labels are present we prioritize them over inconsistent labels. We use the following precedence: clear, vague, ambiguous, incorrect, and omitted in determining the most precise label.\nBefore running our framework at scale, we conduct a pilot study to evaluate its accuracy. For extraction of data collection statements, we manually analyze privacy polices of 10 Action and measure the coverage of our framework in correctly extracting data collection related statements. Specifically, we manually go through the privacy policies and extract statements which contain actionable verbs pertaining to data (e.g., collection) or mention specific data types. For the 10 privacy policies we analyze, we are able to extract all sentences related to data collection.\nFor the assignment of data collection labels, we manually check 20 Actions with 84 data types. Specifically, we check if the label assigned by our framework to a data type description is correct by inspecting the relevant sentence. For example, for the clear label, we consider our tool's detection to be a true positive: if the data type is detected by our tool and it is also clearly mentioned in the privacy policy, true negative: if the data type is not detected by the tool and also not clearly mentioned in the privacy policy, false positive: if the data type is detected by the tool as but not mentioned in the privacy policy, false negative: if the data type is not detected by the tool but mentioned in the privacy policy. Overall, we achieve an accuracy of 85.7% (with a recall of 89.2% and precision of 96.4%) in detecting the consistency of data types, on average across all disclosure types."}, {"title": "6.3. Data disclosure analysis results", "content": "Next, we use our framework to check the consistency of data collection with the disclosures in Action's privacy polcies."}, {"title": "6.3.1. Disclosures for most data types are omitted", "content": "Figure 6 represents the data disclosures consistency across all Actions. It can be seen from the figure that disclosures are omitted for most of the data types. We also note that for some data types, such as the collection of purchase history, user payment info, race and ethnicity, and installed apps, there are no disclosures. For example, Moon Wallet [75] Action provides crypto trading services and collects an whopping 108 data items, including user's payment and financial information but in its privacy policy does not list any of this information. Upon inspection, we find that the Action uses a boilerplate privacy policy template and does not even fills in the name of the Action in the text and leaves it as: [[\"website\u201d or \u201capp\"]] [76].\nAmong the omitted disclosures, device or other IDs collection are the least omitted, followed by the email address, and name. In fact, these data types are also the most clearly defined disclosures in privacy polcies. For example, we note that the Document Wizard [77], clearly describes in its privacy policy that it: \"may collect personal information from you when you voluntarily provide it. For example we collect your email address when you request us to send you an email with your document\" [78]."}, {"title": "6.3.2. Nearly half of the Actions clearly disclose more than half of their data collection", "content": "Next, we investigate whether Actions at least clearly disclose some of their data collection. for almost half of the Actions the data collection disclosures are consistent with their privacy policies for more than half of their data collection. We also note that for nearly all Actions, at least 10% of their data collection practices are inconsistent with their disclosures."}, {"title": "6.3.3. Data disclosure consistency decreases as more data is collected, however, this correlation is not strong", "content": "We investigate, whether the the consistency of disclosures decreases as Actions collect more data. We note that as the number of collected data types increase, the consistency of disclosures decreases, however, the correlation between the two is not strong (i.e., Spearman's correlation coefficient between the two is 0.13) [80].\nWe also find that the data collection of only 5.8% of Actions is consistent with their disclosures. We represent these Action, with more five or more clear disclosures, Mortgage Calculator [81] and Sapientor [82] clearly disclose all of their data collection practices. In the case of Sapiento, it collects information such as the user authentication token and the content provided by the user, and clearly mentions these with the exact names in its privacy policy. In the case of Mortgage Calculator, it collects loan amount and value of the home, among other similar data types, and mentions in its privacy policy that it collects financial information."}, {"title": "7. Discussion", "content": "Parallels with other emerging app ecosystems. As compared to other ecosystems, such as the VR, Smart TVs, and Smart Speakers [83], [84], [7], [8], OpenAI's GPTS and their Action are collecting expansive and excessive amount of data. While this data collection is enabling a wide variety of use cases, at the same time it is posing serious risks to user privacy. Considering the rapid growth of the GPT ecosystem, with millions of GPTs already hosted on the OpenAI GPT store [24], it is crucial that GPTs and their Actions are carefully reviewed by the vendors; which currently does not seem to be the case [16], [17], [18], in fact, GPTs may not even be reviewed at all [15].\nWe also note that the LLMs provide vendors a unique opportunity to improve the privacy posture of LLM-based apps. For example, currently OpenAI provides an interface for developers to create GPTs using an LLM, the same LLM could also assist the GPTs in drafting their privacy polices to accurately represent their data collection practices. Furthermore, LLMs could be used to monitor the user's interaction with GPTs to provide recommendations to developers to improve disclosures in their privacy policies and also to users about whether the data to be collected is disclosed by the GPT (and its Actions) and for what purposes it will be used.\nPrivacy and security as key considerations in the design of LLM platforms. We see that LLM apps are going through a rapid transformation from providing simple instructions through a prompt, to adding 10s of third-party libraries (Actions) to support complicated use cases. This transformation has parallels with the web ecosystem, where the websites also evolved from simple HTML web pages to complicated web applications. As a consequence, the web ecosystem suffers from serious privacy issues, with browser vendors and researchers still continuously developing ad-hoc solutions to mitigate these concerns [49], [85], [71].\nSimilar to these mature platforms, OpenAI is also continuously revising its polices to catch up with the rapid growth of the its app ecosystems [13], [14], [15]. However, as our measurements indicate, these efforts may not be sufficient. For example, as we note in Section 5.1.2, OpenAI requires GPTs to comply with applicable legal requirements while collecting personal user data [14], [15], but does not provide GPTs sufficient controls that they can offer to users so that users can exercise their rights. Similarly, OpenAI currently does not isolate the execution of Actions, which leads to the indirect exposure of data between Actions embed in a GPT(Section 5.3).\nSince LLM app ecosystem are still nascent, there is an opportunity to improve their design from the outset, instead of (and in addition to) piecemeal iterative improvements. In fact, OpenAI has already gone through one major overhaul of its app ecosystem, from retiring plugins in favor of GPTS with Actions [86]. However, this re-haul seems to be mostly geared towards improving the functionality of LLM apps. For a secure platform, we argue that security and privacy should also be given similar attention. For example, LLM app ecosystems could implement design interfaces for multiple Actions to securely collaborate with each other inside a GPT [25]. Similarly, in addition to proposing policies, e.g., for complying with legal requirements, platforms should also develop controls so that they can be used to enforce respective policies."}, {"title": "8. Conclusion", "content": "In this paper we conducted an in-depth investigation of OpenAI's GPTs. We crawled a total of 119,274 GPTs and 2,596 unique Actions (custom tools), from third-party and the OpenAI's official app store, over four months. We found that the number of GPTs has been steadily growing with many GPTs getting removed because of potentially violating OpenAI's polcies. We also found that 82.9% of Actions included in GPTs were from external third-party services. We developed an LLM-based framework to conduct the static analysis of natural language-based source code of GPTs and their Actions to characterize their data collection practices. Our findings indicated that Actions collect expansive data about users, including sensitive information prohibited by OpenAI, such as passwords. To automatically check the consistency of data collection by Actions with disclosures in privacy policies, we developed an LLM-based privacy policy analysis framework. Our measurements indicated that the disclosures for most of the collected data types were omitted in privacy policies, with only 5.8% of Actions clearly disclosing their data collection practices."}, {"title": "Appendix A. Sample of a GPT and Action Manifest", "content": "Listing 1 describes a simplified representation of a Custom GPT from our dataset that aims to help a user with writing code. As shown in the listing", "GPTs": "first"}]}