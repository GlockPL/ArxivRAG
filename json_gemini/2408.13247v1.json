{"title": "Data Exposure from LLM Apps: An In-depth Investigation of OpenAI's GPTs", "authors": ["Evin Jaff", "Yuhao Wu", "Ning Zhang", "Umar Iqbal"], "abstract": "LLM app ecosystems are quickly maturing and supporting a wide range of use cases, which requires them to collect excessive user data. Given that the LLM apps are developed by third-parties and that anecdotal evidence suggests LLM platforms currently do not strictly enforce their policies, user data shared with arbitrary third-parties poses a significant privacy risk. In this paper we aim to bring transparency in data practices of LLM apps. As a case study, we study OpenAI's GPT app ecosystem. We develop an LLM-based framework to conduct the static analysis of natural language-based source code of GPTs and their Actions (external services) to characterize their data collection practices. Our findings indicate that Actions collect expansive data about users, including sensitive information prohibited by OpenAI, such as passwords. We find that some Actions, including related to advertising and analytics, are embedded in multiple GPTs, which allow them to track user activities across GPTs. Additionally, co-occurrence of Actions exposes as much as 9.5\u00d7 more data to them, than it is exposed to individual Actions. Lastly, we develop an LLM-based privacy policy analysis framework to automatically check the consistency of data collection by Actions with disclosures in their privacy policies. Our measurements indicate that the disclosures for most of the collected data types are omitted in privacy policies, with only 5.8% of Actions clearly disclosing their data collection practices.", "sections": [{"title": "1. Introduction", "content": "Large language model (LLM)-based platforms, such as ChatGPT [1] and Gemini [2], are increasingly supporting third-party app ecosystems [3], [4]. While third-party LLM apps enhance the functionality of LLM platforms, they may also pose significant risks to user privacy. As it has been the case in other computing platforms, third-party apps and external services embedded in them collect excessive user data, often more than it is needed to provide essential services [5], [6], [7], [8]. In LLM platforms, the risks from third-party apps may be exacerbated because of the natural language-based execution paradigm of LLMs. For example, user's main mode of interaction with LLMs is information-rich natural language, which can be processed to infer several characteristics about the user, such as their age or interests [9], [10]. Furthermore, malicious LLM apps can launch straightforward attacks (e.g., with prompt injection [11]) to access information beyond their one-to-one interactions with the user, as LLMs automatically load prior user interactions in their execution environment (i.e., context window) to provide a contextually relevant responses [12].\nLLM platforms moderate the practices of apps through their policies [13], [14], [15], however, these polices are currently mostly limited, optional, or not strictly enforced [16], [17], [18]. For example, prominent platforms, such as OpenAI, currently state that they may not review the apps hosted on their platforms [15]. Anecdotal evidence suggests that policy violating apps are already hosted on such platforms, and only removed when publicly brought to attention [19]. Vendors are also constantly improving their platforms. For example OpenAI, has recently completely revamped its LLM app ecosystem with more restrictions to improve their security and privacy posture [20]. For example, LLM apps (referred to as GPTs [3]) and external services embedded in them (referred to as Actions [21]), now need to host their specifications on the OpenAI's back-end and can no longer be self-hosted [22]. However, we also note that at the same time, OpenAI has removed restrictions on use cases, such as advertising, which often require personal and excessive user data [23], [14].\nGiven the potential for privacy issues due to the limited polices and their lack of enforcement in LLM platforms, in this paper we aim to bring transparency in data practices of LLM apps. As a case study, we study OpenAI's GPT ecosystem, as it is the largest LLM app ecosystem with more than 3 million GPTs [24]. At a high level, we (i) first survey GPTs and Actions, (ii) characterize their data collection practices, (iii) measure potential indirect data exposure across GPTs and their Actions, and (iv) check the consistency of data collection practices with disclosures in privacy policies of GPTs and Actions.\nWe crawl a total of 119,274 GPTs and 2,596 unique Actions embedded in them from third-party and the OpenAI's official app store, over four months (our crawling is still ongoing). Since GPTs and their Actions define their functionality, including their data collection, in natural language, we rely on static analysis to characterize their data collection practices. However, static analysis requires addressing the challenge of assigning succinct data types to the detailed and potentially vague natural language descriptions. To that end, we build an LLM-based tool, that takes a natural language data type description as input, and outputs a succinct data type and its associated data category, based on a data taxonomy that we provide it as a knowledge base.\nWe also note that some GPTs embed several Actions, and some Actions are embedded across several GPTs. Since all Actions embedded in a GPT execute in a shared execution environment [25], [17], they are automatically exposed each other's data. Similarly, presence in several GPTs, allow Actions to collect user data and track user activities across GPTs. We model the presence of Actions in GPTs as a graph, to systematically study such indirect data exposure in OpenAI's GPT ecosystem.\nTo check the consistency of data collection with disclosures in privacy polices, we take inspiration from prior work on automated privacy policy analysis [26], [27], [8], [28] and develop an LLM-based privacy policy analysis framework. Due to LLMs' unreliability and performance issues with large contexts [29], our framework analyzes privacy policies in three steps: (i) extracts data collection related statements from privacy policies, (ii) builds LLM's context with the extracted statements, and (iii) evaluates individual data items against the sentences for disclosures. This approach ensures precise association between the LLM's assessments and specific data types within the privacy policies.\nWe summarize our key contributions and findings below:\n1) GPT census. We analyze a total of 119,274 GPTS with 2,596 unique Actions, crawled across four months. We note that the number of GPTs has been steadily growing. Many GPTs modify their functionality but likely do not change it altogether. We also note that some GPTs are removed from the OpenAI platforms, likely because they violated OpenAI's policies. We also find that majority of Actions (82.9%) included in GPTs are from external third-party services.\n2) Characterization of data collection practices. We develop an LLM-based framework to conduct the static analysis of natural language-based source code of GPTs and their Actions to characterize their data collection practices. Our findings indicate that Actions collect expansive data about users, including sensitive information prohibited by OpenAI, such as passwords [14]. We also find that some GPTs are embedding specialized third-party Actions to track users and also to serve ads to users.\n3) Measuring indirect data exposure. To study the indirect data exposure between Actions and across GPTs, we model the Action co-occurrence in a graph representation. We note that some Actions, including related to advertising and analytics, are embedded in multiple GPTs, which allow them to track user activities across GPTs. Additionally, co-occurrence of Actions exposes as much as 9.5\u00d7 more data to them, than it is exposed to individual Actions.\n4) Consistency of data collection with privacy policy disclosures. We develop an LLM-based privacy policy analysis framework to automatically check the consistency of data collection by Actions with disclosures in privacy policies. Our measurements indicate that the disclosures for most of the collected data types are omitted in privacy policies. However, nearly half of the Actions clearly disclose more than half of their data collection and only 5.8% of Actions clearly disclose their data collection practices."}, {"title": "2. Background & Motivation", "content": ""}, {"title": "2.1. OpenAI GPTS", "content": "In this paper we study the OpenAI's GPT (app) ecosystem, the most mature third-party LLM app ecosystem with more than 3 million GPTs [24]. OpenAI provides GPTs the ability to customize the behavior of the LLM, browse the web, generate images, interpret code, search files, and connect to the APIs of external online services. Browsing (i.e., Web Browser), image generation (DALLE), code interpretation (Code Interpreter), and file searching (Knowledge) are built-in tools and provided by OpenAI [3], whereas connection to external APIs are implemented as custom tools, which are referred to as Actions [21]. Actions are akin to third-party services on the web, such as analytics, JS wrappers, CDNs, that websites embed to enhance their offerings.\nBuilt-in tools can be enabled by clicking check-boxes on the GPT creation interface [30], whereas Actions need to be implemented as HTTP APIs and exposed to OpenAI in a JSON format [21]. The JSON format of Actions describes the functionality offered by each API, including its data types, as natural language descriptions (Appendix A lists the source code of a GPT with an Action). GPTs also define their functionality in natural language and interface with the LLM, their tools, the user, and other GPTs through natural language instructions. To build the necessary context to use a GPT, LLMs inject the natural language-based source code of GPTs in their context window, when users install and interact with GPTs."}, {"title": "2.2. Privacy risks", "content": "While third-party apps extend the capabilities of computing platforms, they also pose several risks to user privacy. For example, in almost all online computing platforms, such as the web, mobile, and IoT, it is a standard practice for third-party apps to collect excessive user data, often with other specialized third-party services, for the purposes of profiling users for personalized online advertising [5], [6], [7], [8]. We worry that the GPTs might also engage in similar practices on the OpenAI's platform. In fact, GPTs are already including specialized third-party Actions to track users (as we show later in Section 5.2.2).\nOpenAI currently imposes some restrictions [13], [14], [15] on GPTs but they are mostly limited, optional, or not strictly enforced [16], [17], [18]. For example, OpenAI currently does not implement any foolproof access control mechanisms, and leaves it up to the developers to define permission interfaces for activities performed by the GPTs, which may not be reviewed [15]. There are already instances where policy violating apps were hosted on OpenAI and only removed when publicly brought to attention [19]. Furthermore, OpenAI also intends to use user's interaction with the GPTs, i.e., to train its models [32]. Although, OpenAI provides users' controls to delete their data [33], these controls may not extend to third-party GPTs, as OpenAI may not have visibility or control over the data exfiltrated by the Actions inside GPTs.\nPrivacy risks may be further exacerbated in LLM platforms because of the natural language-based execution paradigm of LLMs. For example, user's main mode of interaction with LLMs is information rich natural language, which can be processed to infer several characteristics about the user, such as their age or interests [9], [10]. Furthermore, malicious GPTs can launch straightforward attacks (e.g., with prompt injection [11]) to access information beyond their one-to-one interactions with the user, as LLMs automatically load prior user interactions in their context window to provide a contextually relevant response [12]."}, {"title": "2.3. Our goal", "content": "Given the potential for privacy issues and their harms to the users, this paper aims to bring transparency in the OpenAI's third-party app ecosystem. More specifically, our goal is to characterize the privacy practices in the OpenAI's GPT ecosystem, including (i) surveying GPTs and Actions embedded in them, (ii) characterizing their data collection practices, (iii) measuring potential indirect data exposure across GPTs and their Actions, and (iv) checking the consistency of data collection practices with disclosures in privacy policies of GPTs and Actions.\nWe conduct a four-month long periodic weekly crawls of GPTs from February 8th to May 3rd 2024, to measure their evolution across several axes (Section 4). To characterize data collection by GPTs and their actions, we rely on static code analysis, as GPTs and Actions need to state their data collection in natural language, so that it can be interpreted and acted upon by LLMs (Section 3). Furthermore, we analyze the indirect exposure of data across Actions because of embedding of multiple Actions in GPTs by modeling Action co-occurrence as a graph (Section 5.3). Lastly, to measure the consistency between the data collection by GPT Actions and disclosures in their privacy policies, we develop an LLM-based privacy policy analysis framework (Section 6). With these measurements, our goal is to build an informed understanding of the third-party app ecosystems in LLM platforms. We envision such measurements to serve as a guide to inform the design of current and future integrations of third-party services in LLM platforms, to improve their privacy."}, {"title": "3. GPT crawling", "content": "We first crawl a large number of GPTs from the OpenAI and third-party GPT stores and present their census, including their growth and tool usage trends."}, {"title": "3.1. GPT marketplaces", "content": "Since OpenAI does not provide any interfaces to download GPTs hosted on their platform, we rely on several third-party GPT stores that index a large number of GPTs. We identified a total of 13 popular sources that list GPTs (listed in Table 1) from popular developer communities and forums, such as the OpenAI Developer Forum [34], [35]."}, {"title": "3.2. Crawling process", "content": "We implemented selenium-based [36] crawlers for each of the third-party store to extract links to the GPTs. After extracting the links, we process them to extract the GPT identifiers, and then send a request to an OpenAI API endpoint with the GPT identifier\u00b9 that returns the JSON specification of a GPT. If the GPT identifier is not associated with a publicly available GPT, OpenAI returns a 404 error code. We also crawl a small number of featured GPTs listed on the OpenAI's official GPT store. The downloaded JSON specifications of GPTs describe their functionality in natural language, including the endpoints contacted by Actions, and the data exfiltrated by them (Appendix A lists the source code of a GPT with a third-party Action).\nAfter crawling GPTs, we download the privacy policies of their Actions by requesting the URL in the legal_info_url field in their specifications.\u00b2 We successfully crawl 98.9 \u00b1 1.7% GPTs and 91.5 \u00b1 2.3% privacy policies of GPT Actions, over four months. We are unable to crawl the remaining GPTs and privacy policies due to internal server errors and server unresponsiveness. shows the cumulative number of GPTs from each of the GPT stores. In total, we crawl 119,543 unique GPTs from all of the GPT stores."}, {"title": "4. GPT census", "content": "After crawling the GPTs, we first analyze their growth trends on third-party stores over time. From Figure 3, we note that new GPTs are frequently listed on stores, with a mean increase rate of 4.5% over each week. We also note that several GPTs are changed or removed over time, with a mean rate of 0.02% and 0.2% over each week, respectively. We next discuss the changes and removals in more detail."}, {"title": "4.1. GPTs modify their functionality but likely do not change it altogether", "content": "We note that several GPTs are modified over time, either because they are changed by their developers or because some of their metadata is changed by OpenAI, such as ratings and usage statistics. Table 2 presents the breakdown of changes in properties of crawled GPTs. In total, we identify 303 GPTs that are modified over time (we do not consider the properties that are changed by OpenAI). We note that some modification (e.g., metadata and Actions/-Files) could be more consequential than the others (e.g., contact information) in altering a GPT's functionality. We investigated all such instances, i.e., modifications to metadata and Actions/Files related properties. However, none of these modifications indicated a functionality change and most seem to be related to performance/accuracy tweaks."}, {"title": "4.2. Some of the GPTs that no longer exist violated OpenAI's policies", "content": "Next we analyze the removed GPTs to assess if the reason for their removal were problematic behaviors. We consider a GPT to be removed if it is no longer present on the third-party GPT stores and also inaccessible on ChatGPT. In total, we note that 2,883 GPTs were removed from the GPT store during our crawl period.\nSince our goal is to reliably assess the potential reasons for the removal of GPTs, we resort to manual investigation. We specifically emphasize on GPTs that embed Actions because they present the potential for most harms as they connect to potentially untrustworthy third-party services on the internet and load unvetted content. Our manual review process involves two human coders first independently analyzing a small set of GPTs to generate a code book, and then independently analyzing GPTs using that code book. At a high level, the code book contains rules that characterize GPTs functionalities, including their data collection practices and their content generation practices. This characterization requires us to analyze the natural language functionality description of the GPTs and their API endpoints, individually using them in ChatGPT, and also interacting with their API endpoints.\nTable 3 presents the potential reason for the removal of 175 GPTs that embed Actions. We find that the largest proportion of removed GPTs are the ones whose Action APIs are no longer accessible. In some cases, we noticed that upon calling the Action's APIs, they returned messages that the GPTs have been discontinued. For example, the AskYourCode Action within the AskYourCode GPT returned the message that: AskYourCode was closed on 15th Feb due to low usage. [37]\nThe second largest category of removed GPTs are the ones that provide web browsing functionality. Upon investigating, we discovered that OpenAI from time-to-time, although inconsistently, has been removing GPTs that allow users to browse the web [38], [39]. More recently, OpenAI has been reaching out to the GPT developers which provide web browsing functionality, that their GPT provides copyright infringing content to its users [40].\nThe third largest category of removed GPTs were the ones that contained Actions which provide analytics and advertising services. OpenAI currently does not condone GPTs to collect analytics of their own and promises an in-house analytics feature in future releases [32]. As for the advertising, it was initially prohibited by OpenAI [41], [42] but does not seem to be prohibited anymore, as per the updated OpenAI's policies [14]."}, {"title": "4.3. Many GPTs connect to third-party services on the internet", "content": "Table 4 provides the breakdown of tool usage in GPTs. We note that almost all (97.5%) GPTs include tools; with most popular integration being the Web browser with 92.3%, followed by DALL-E with 85.5%, Code interpreter with 53.0%, Knowledge (Files) with 28.2%, and Actions with 4.6%.\u00b3\nA significant majority (93.2%) of GPTs connect to online services through Web Browser and Actions. Specifically, the Web Browser tool allows to consume content from any webpage on the internet and Actions allow to connect to specific online services. While these tools extend the capabilities of GPTs, they also expose users to unvetted online content on the Internet, threatening user security and privacy [48], [17]. In the case of Actions, these risk may be further exacerbated as a significant number of Actions in"}, {"title": "5. GPT data collection analysis", "content": "In this section, we analyze data collection practices of GPTs. We specifically emphasize on GPTs that embed Actions, because GPTs can only contact external online services with Actions, to exfiltrate data outside OpenAI's ecosystem."}, {"title": "5.1. Overview of collected data", "content": "5.1.1. Methodology. We first present an overview of the data collected by the Actions embedded in GPTs. As Actions describe the data collected by each API endpoint in natural language descriptions, we rely on static analysis, to sufficiently capture their data collection practices. However,\n5.1.2. Actions collect expansive data, including sensitive information prohibited by OpenAI. We first plot the number of data items collected by each Action in Figure 4. We note that 25.57% and 39.77% of Actions collect 5 or more succinct (as determined by our LLM-based tool) and raw data types, respectively. Additionally, there are 4.35% and 18.82% of Actions that collect 10 or more succinct and raw data types, respectively. We next analyze specific data types that are excessively collected by Actions.\nWe note that the Actions collect a wide range of expansive data spanning across 14 different categories. Table 5 presents the categories, types, and counts of data collected by first-party and third-party Actions embedded in GPTs (see Appendix B for our detailed data taxonomy). It can be seen from the Table 5 that a significant number of Actions collect data related to user's app activity, personal information, and web browsing. App activity data consists of user generated data (e.g., conversation and keywords from conversation), preferences or setting for the Actions (e.g., preferences for sorting search results), and information about the platform and other apps (e.g., other actions embedded in a GPT). Personal information includes demographics data (e.g., Race and ethnicity), PII (e.g., email addresses), and even user passwords; web browsing history refers to the data related to websites visited by the user using GPTS.\nWe note that several of these data types pertain to sensitive user data and their collection is prohibited by OpenAI [14], [15]. For example, OpenAI prohibits the collection of information such as passwords and API keys, but we note that at least 1% of GPTs that embed Actions (in our crawl), collect user passwords, for the purposes of signing into online services or managing online services on user's behalf. Since OpenAI may use user-to-GPT interaction data for training its models [32], the collection of sensitive user data not only exposes users to harms from third-party developers but also from arbitrary attackers, who can extract training data from LLMs, as it has been shown by prior work [53], [54]\nWe also note that OpenAI requires GPTs to comply with applicable legal requirements while collecting personal user data [14], [15]. However, we found that OpenAI does not provide GPTs sufficient controls that they can offer to users so that they can exercise their rights. For example, prominent data protection regulations, such as GDPR and CCPA [55], [56], require online services to provide users controls to opt out of usage or selling of data [57], but in our testing in respective jurisdictions, we did not find such controls being offered to the users.\nOverall, we note that OpenAI's GPT app ecosystem is already supporting complicated use cases, that require collecting expansive data types, indicating a quick maturing, especially relative to other emerging computing platforms, such as the VR [8] and smart speakers [7] ecosystems. Although, OpenAI is revising its polices to catch up with the rapid development of its third-party app ecosystem, our measurements indicate that these efforts may not be sufficient, as many problematic GPTs continue to exist on OpenAI's store."}, {"title": "5.2. Attributing data collection", "content": "Next, we analyze Actions that collect user data, including analyzing their practices and offerings."}, {"title": "5.2.1. GPTs mostly embed third-party Actions, some of which dynamically load other Actions.", "content": "Form Table 4 and 5, we note that GPTs mostly embed third-party Actions which collect extensive data including personal user information. While in most instances these Actions are directly integrated by GPT developers, we encountered two instances where Actions had capability to dynamically load other third-party Actions. Specifically, Zapier [58] listed that it can Equip GPTs with the ability to run thousands of actions via Zapier and JustPaid [59] listed that it can Equip GPTs with the ability to run actions via JustPaid (with currently only supporting stripe and accounting).\nAlthough, integration of third-party services is a common practice on computing platforms, such as the web and mobile, they often exacerbate the privacy risks posed to the users [5], [6]. For example, advertising and tracking third-party services are known to dynamically embed 100s of other third-party services to share user information with each other, e.g., through cookie syncing [5], [60]. To mitigate such concerns, platforms are making active efforts to restrict the inclusion of dynamically loaded code in apps. For example, Google Chrome no longer allows to include remotely hosted code in browser extensions [61], [62]. Although OpenAI's GPT ecosystem is still nascent, it has a unique opportunity to learn from earlier platforms and enhance its security and privacy measures from the outset."}, {"title": "5.2.2. Some GPTs are embedding third-party Actions to track users and serve them advertisements.", "content": "Next, we analyze data collection practices and the functionality offered by prevalent third-party Actions. Table 6 lists prevalent third-party Actions, along with their functionality category, count of data items collected by them, some of the data that they collect, and the fraction of GPTs that embed them (among Action embedding GPTs). We note that some third-party Actions are widely deployed across GPTs. Among these, webPilot [63] is the most prevalent Action which provides functionality to browse the web, with integration in 6.06% of GPTs. As part of its functionality, the Action gets access to user's browsing history, among other user data.\nThe second most prevalent functionality provided by third-party Actions is advertising and marketing, with AdIntelli [64] Action being embedded on 5.65% of the GPTs. AdIntelli collects the name and description of the GPT on which it is embedded, along with the keywords from the user's chat history with the GPT. Additionally, as a function of being present on several GPTs, AdIntelli has potential to track user activities across several GPTs. We also note specialized Action, such as Analytics to improve this assistant, are embedded for collecting analytics related to the GPT usage, a practice currently not condoned by OpenAI [32] (as discussed earlier in Section 4.2). Similar to advertising and marketing Actions, analytics Actions collect data related to the user's conversation.\nWe also noticed that nearly 1.93% of GPTs embed an Action, named OpenAI Profile that connects to OpenAI's APIs, including getting user information such as their phone number and email address. Since GPTs already have access to OpenAI's LLM, while they are integrated in ChatGPT, they do not need to explicitly make API calls to OpenAI's LLMs. Upon investigation, we found that OpenAI Profile was initially used as an example Action [65] in the GPT creation portal [47]. Get weather data and Swagger Petstore are two other such example actions, which are embedded in 0.47% and 0.20% of the GPTs, respectively. We surmise that many developers likely unintentionally add these example Actions to their GPTs. While the inclusion of such Actions may not necessarily cause any harm to users, it shows that many GPTs developers may be lay users and not experienced software developers.\nWe also note that several GPTs embed super Actions, such as Zapier [58] and Gapier [66], which provide 10s of APIs for a variety of tasks, including engineering user prompts to get improved recommendations from ChatGPT. As a consequence, these Actions collect excessive amount of user data. The inclusion of super Actions may also degrade the LLM performance, as LLMs struggle with large context [29].\nOther prominent Actions functionalities include, web hosting, e-commerce and shopping, and search engines."}, {"title": "5.3. Indirect data exposure", "content": "Since Actions execute in shared memory space in GPTs, they have unrestrained access to each others data, which allows them to access it (and also potentially influence each others execution) [25], [17]. Thus, in this subsection, we analyze the indirect exposure of user data due to integration of multiple Actions in GPTs, given the lack of isolation in ChatGPT."}, {"title": "5.3.1. Action co-occurrence across several GPTs, without proper isolation, enables indirect data exposure.", "content": "As Actions are embedded in multiple GPTs, they are in a position to connect user data collected across multiple GPTs, in different contexts. This is a common practice on other computing platforms, such as the web, where specialized third-party services are embedded on websites that collect and connect users browsing history across several websites, often referred to as cross-site tracking [5], [60]. It is currently unknown if third-party services embedded on GPTs also engage in similar practices, but since the have the ability to do so, we measure the potential data sharing that can happen because of the presence of Actions across multiple GPTs.\nTo that end, we create a graph to understand the potential information sharing relationships between different Actions. In our graph representation, nodes represent Actions and the edges represent their appearance in a GPT. Note that edges are undirected and weighted, such that the weight is incremented by one if the same Action pair co-occurs again in another GPT. Also, we make the size of a node, proportional to its weighted degree and use a color gradient to represent the edge weights, such that the darker color represents higher weight.\nFigure 5 represents the largest connected component in our graph representation. It can be seen from the figure that webPilot [63] and AdIntelli [64] Actions have the highest weighted degree in our graph, i.e., 93 and 29, respectively. Their non-weighted degrees are 63 (webPilot) and 12 (AdIntelli), which means that they co-appear with other Actions across several GPTs. In fact, we note that both webPilot and AdIntelli, co-occur in 13 GPTs. For webPilot, the other most frequent co-occurrences include Gapier [66] and Link Reader [67], with presence in 8 and 5 GPTs, respectively. Whereas for AdIntelli, the other most frequent co-occurrences include Gapier [66] and Analytics to improve this assistant [68], with presence in 9 and 3 GPTs, respectively. The presence of AdIntelli (an advertising service) with other Analytics to improve this assistant (an analytics/tracking service) seems to indicate that the LLM app ecosystem may be evolving similar to other app ecosystems, where advertising and analytics services are often loaded together, for the purposes of targeted advertising [5], [69]. We also note that many other co-occurrences of AdIntelli are with shopping and travel related Actions; businesses that often rely on third-party advertising and tracking services to reach their consumers.\nIn sum, appearance in several GPTs along with other Actions, naturally enables an environment where Action can access each others data [25], [17]. We next quantify the potential indirect exposure of user data due to inclusion of multiple Actions in GPTs."}, {"title": "5.3.2. Co-occurrence exposes Actions to as much as 9.5\u00d7 more data than they were individually exposed.", "content": "Next, we measure the increase in the exposure of data types to additional Actions, as a function of multiple Actions co-occurring in GPTs. Table 7 represents the increase in data exposure for different data types. On average, the data exposure increases for all data types by 2.3% at first degree connections and by 4.3% at second degree connections. From the table, we note that user IDs and settings or parameters have the highest exposure across both the first and second degree co-occurrences.\nWe next analyze increased exposure of data to the most prevalent co-occurring Actions. Table 8 represents the top-5 most co-occurring Actions. We note the because of the increased co-occurrence, Actions are exposed to significantly more data than they were individually exposed. For some Actions, such as AdIntelli's [64], the data exposure increases by as much as 9.5\u00d7. We also note that the Actions are exposed to sensitive user data, including PII, such as email addresses.\nOverall, we note that Actions are in a position to track users across GPTs and collect far more data than they would if they appeared alone or executed in isolation [25]. We also note that such lack of execution isolation is not unique to LLM-based systems, such as ChatGPT. Other ecosystems, such as the the web, continue to suffer from this problem, where the third-party code from several services execute in the same environment as the first-party code [70], [71]. However, LLM platforms have an opportunity to address this problem by-design, before their architecture becomes established and new solutions risk breaking compatibility."}, {"title": "6. GPT privacy policy analysis", "content": "In this section, we analyze whether GPTs and their Actions disclose their data collection practices in their privacy policies."}, {"title": "6.1. Privacy policies overview and availability", "content": "OpenAI mandates, individual third-party Actions embedded in GPTs, to provide privacy policies but does not require GPTs to provide a privacy policy that describes its data practices as a whole [21]. This approach deviates from the norm in other platforms, where the apps provide a privacy policy with information about their own practices, including information about third-party services that they embed. In OpenAI's ecosystem, to understand data practices of GPTs, users need to read the privacy policies of all of their third-party Actions. Since the GPT interface does not disclose the Actions embedded in them, and given that Actions can dynamically embed other third-party Actions (Section 5.2.1), users may simply be unaware of the existence of these Actions in GPTs, let alone their data practices.\nFor the purposes of analysis in this section, we analyze the privacy policy disclosures at the granularity of individual Actions. Table 9 presents high-level statistics about privacy policies. Overall, we were able to crawl privacy policies of 86.68% of Actions (among 2,596 distinct Actions). For the remaining 13.32% of the Actions, the privacy policies were inaccessible. We also note that nearly 39.56% of the polices appear more than once for distinct Actions and 5.50% of the policies are near duplicates of each other (i.e., have a Jaccard similarity [72] of more than 95%).\nWe investigate these duplicates and near-duplicates, and provide our assessment in Table 10. We note that, the inclusion of privacy policy of the external third-party services (e.g., Github, Google) is the most common reason for duplicate policies (33.5%), followed by empty privacy policies (27.0%) and Actions belonging to the same vendor (19.2%). For near-duplicates, we find that all such Actions include a boilerplate privacy policy generated from freeprivacypolicy.com, with mostly the only change being the name of the Action.\nWe also noted that for 12.45% of the Actions the privacy policies were less than 500 characters. We manually analyze these policies and find that they contain generic statements, such as We do not collect any personal data from users of our Service. and Your data is never for sale.. Nonetheless they still describe the data practices of the Actions, albeit being short, thus we still consider them in our analysis."}, {"title": "6.2. Data disclosure analysis methodology", "content": "Our goal with the privacy policy analysis is to assess whether they contain disclosures about the data collection practices of Actions. To that end, we build on the automatic privacy policy"}]}