{"title": "THE PHILOSOPHICAL FOUNDATIONS OF GROWING AI LIKE A CHILD", "authors": ["Dezhi Luo", "Yijiang Li", "Hokin Deng"], "abstract": "Despite excelling in high-level reasoning, current language models lack robustness in real-world scenarios and perform poorly on fundamental problem-solving tasks that are intuitive to humans. This paper argues that both challenges stem from a core discrepancy between human and machine cognitive development. While both systems rely on increasing representational power, the absence of core knowledge-foundational cognitive structures in humans-prevents language models from developing robust, generalizable abilities, where complex skills are grounded in simpler ones within their respective domains. It explores empirical evidence of core knowledge in humans, analyzes why language models fail to acquire it, and argues that this limitation is not an inherent architectural constraint. Finally, it outlines a workable proposal for systematically integrating core knowledge into future multi-modal language models through the large-scale generation of synthetic training data using a cognitive prototyping strategy.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advancements in artificial neural networks, particularly transformer-based large language models (LLMs), mark perhaps the most impressive progress in artificial intelligence (AI). Never had any AI models perform so well on tasks that are known to require high-level reasoning abilities, not limited to solving challenging mathematical problems (Ahn et al., 2024), writing codes for complex programs that runs smoothly (Zhuo et al., 2024), drafting travel and business plans (Xie et al., 2024), and importantly, doing so all via natural languages which humans can effortlessly understand. However, despite such astonishing achievements, scientists and users have noticed limitations of LLMs that would seem bizarre for humans: their performances on these tasks could fall drastically over slight tweaks of details in the task conditions (Yuan et al., 2023).\nSaid limitations, which concern the robustness of problem-solving, have posed odds over the reliability of deploying LLMs to handle real-life tasks, at least in ideally low-supervised manners (Mitchell & Krakauer, 2023). Interpretations over the underlying causes of such limitations vary, with a prominent account citing the distinction between formal and functional linguistic competences the ability to produce fluent languages versus the ability to understand and use them in the real world \u2014 and arguing that the robustness challenge is due to LLMs possessing the former without the latter (Mahowald et al., 2024). The extreme version of this account would be the one claiming that LLMs lack understanding altogether. In other words, they are \"stochastic parrots\" that can only solve tasks by abusing spurious correlations in the existing dataset, thereby failing to sensibly answer when the questions are dissimilar enough to what they have been trained on (Bender et al., 2021).\nAt the same time, a surge of attention has been put into benchmarking LLMs: assessing them on large-scale repositories of tasks systematically-developed to target distinct reasoning abilities. A key motivation for such efforts is the notoriously mysterious nature of how LLMs work. It is not apparent, even to engineers who built them, how next-token predictions could enable problem-solving that requires advanced inference. By applying experimental paradigms with controlled designs, researchers are able to differentiate what LLMs can and cannot do under specific task conditions. One particularly staggering kind of result from the benchmarking efforts is the discovery that LLMs could fail miserably at tasks that are easy to humans, despite their high achievements on much harder"}, {"title": "2 SCALING UP VS. GROWING UP", "content": "While the differences between the computational architectures supporting LLMs and human intelligence are extensively discussed, there has been relatively little attention given to how LLMs and humans differ in their development. In the following section, we discuss how looking at such differences facilitates a unified account of the robustness challenge and the Moravec's paradox.\nA widely held belief in the current AI research community is that changes in LLMs' reasoning performance can be directly attributed to changes in their scale, as tracked by the number of parameters they have in the neural networks and the size of the dataset they are trained on. This belief, hailed as the scaling law, has largely been supported by empirical results observed throughout LLMs' progression throughout the past several years, and continues to be the primary strategy adopted by major companies for developing more advanced models (Kaplan et al., 2020). In this sense, scaling law has been taken as a theory regarding the nature of LLMs' cognitive development, that is, increased computational power supporting a domain-general learning mechanism (Long, 2024). As Richard Sutton, in his seminal The Bitter Lesson, puts it:\n\"One thing that should be learned from the bitter lesson is the great power of general purpose methods, of methods that continue to scale with increased computation even as the available computation becomes very great... we should stop trying to find simple ways to think about the contents of minds, such as simple ways to think about space, objects, multiple agents, or symmetries. All these are part of the arbitrary, intrinsically-complex, outside world. They are not what should be built in, as their complexity is endless; instead we should build in only the meta-methods that can find and capture this arbitrary complexity.\" ((Sutton, 2019), p.2; emphasis added)\""}, {"title": "3 CORE KNOWLEDGE", "content": "A large body of works in cognitive science have demonstrated that humans possess a basic un-derstanding of several key domains of the world at a very young age, henceforth core knowledge (Spelke, 2003; Spelke & Kinzler, 2007). This set of knowledge is generally understood as consisting of simple principles regarding objects, actions, number, space, and social relations, including how they relate to each other. Core knowledge is essentially children's \u201cdevelopmental start-up software\u201d"}, {"title": "4 INTERPRETING CORE KNOWLEDGE DEFICITS IN LLMS", "content": "Why do LLMs fail to acquire core knowledge through scaling, despite having access to vast linguistic datasets spanning virtually the entire internet and the computational power to process them? The"}, {"title": "4.1 LACK OF HARDWIRED DOMAIN-SPECIFIC FACULTIES", "content": "To begin with, in line with a long tradition in nativist epistemology (Locke, 1824; Chomsky, 1980; Cowie, 2002), it has been suggested that fundamental knowledge of the world\u2014such as core knowledge-must be built upon \"hardwired\" cognitive faculties that are innate to the brain at birth. Neurobiological findings indicating specialized modularity in the human brain for processing certain types of information, such as numbers, objects, and social relations, support this perspective (Spelke & Kinzler, 2007). If this view holds, LLMs' core knowledge deficits may be fundamentally unresolvable, as their foundational architecture lacks such hardwired domain-specific vehicles.\nHowever, this argument is inconclusive at best. To start, empirically validating the existence of innate, domain-specific cognitive mechanisms in humans is challenging, as what appears to be \"innate knowledge\" could instead be \"early knowledge\"-the result of domain-general learning mechanisms rapidly self-organizing into primitive forms of functional modularity upon first exposure to the environment during infancy, as Piaget posited in terms of accommodation (Piaget, 1952). Moreover, even if humans do possess built-in cognitive faculties, they may not be strictly necessary for any system capable of acquiring core knowledge. To claim otherwise would impose a learnability constraint, asserting that certain cognitive capacities cannot be learned from data using domain-general empiricist mechanisms and instead require innate structures (Long, 2024).\nUltimately, a key argument regarding the learnability of core knowledge is that infants lack explicit symbolic systems and that there are no widely accepted mechanisms by which pre-linguistic intelligent systems could form representations of domain-specific concepts solely through perceptual interaction (Carey, 2011). However, given LLMs' strong ability to manipulate linguistic symbols, they may be able to leverage abstract concepts through domain-general learning mechanisms to acquire core knowledge representations (Long, 2024)."}, {"title": "4.2 BURIED TOO DEEPLY", "content": "Secondly, it may be argued that LLMs may already possess core knowledge but are unable to extract and apply it in reasoning. Since the early days of connectionist research, it has been suggested that artificial neural networks, due to their reliance on distributed representations, inevitably face a challenge: as network size increases through scaling, retrieving knowledge representations for reasoning becomes increasingly difficult and computationally costly (Hinton et al., 1986; Clark, 1992). As Chalmers (1990) remarked:\n\"Not only is compositional structure encoded implicitly in a pattern of activation, but this implicit structure can be utilized by the familiar connectionist devices of feed-forward/back-propagation in a meaningful way. Such a conclusion is by no means obvious a priori\u2014it might well have turned out that the structure was \"buried too deeply\u201d to be directly used, and that all useful processing would have had to proceed first through the step of extraction.\u201d (Chalmers (1990), p. 60; emphasis added)\nThis concern is particularly relevant for core knowledge. High-level details required for complex tasks are relatively easy to retrieve, as their representational patterns are likely clustered within the network, such as those encoding a specific historical event or a recipe for a particular dish. In contrast, identifying and utilizing core concepts in simple tasks is significantly more challenging for LLMs, as these concepts are distributed across numerous samples in the dataset with roughly the same level of salience. For example, while a vast number of instances in a model's training data may implicitly demonstrate perceptual constancy-the principle that an object's identity remains"}, {"title": "4.3 GROUNDLESS LEARNING PROCESS", "content": "Finally, a key distinction between human and machine learning lies in the temporal dynamics of data exposure. Humans progress through a structured developmental trajectory, where cognitive and representational capacities are initially limited, allowing them to process information only within a constrained scope. As they mature, their cognitive system gradually expands, building upon foundational core knowledge to integrate increasingly complex abstractions. This incremental learning process enables humans to develop a deep, structured understanding of the world, where high-level reasoning emerges as a natural extension of foundational cognitive abilities (Pezzulo et al., 2013).\nIn contrast, LLMs do not follow this grounded developmental process. Instead, they are trained from the outset with access to vast datasets encompassing a broad spectrum of human knowledge, including highly abstract and complex information. Unlike humans, who first acquire intuitive principles through direct sensorimotor experiences before developing abstract knowledge on top, LLMs process high-level concepts alongside low-level ones without any structured progression. This lack of developmental scaffolding means that while LLMs can generate responses that resemble advanced reasoning, they may lack the fundamental conceptual grounding that allows humans to apply knowledge flexibly and coherently across different contexts (Mitchell & Krakauer, 2023).\nHowever, this difference in developmental trajectory does not necessarily preclude LLMs from acquiring core knowledge. If trained on data similar to those available to a child, LLMs might develop a structured understanding of fundamental concepts in a way that mirrors human learning. Rather than relying solely on large-scale text-based training, models could benefit from multimodal learning, where training data prioritizes direct representations of low-level cognitive concepts through rich perceptual information. By leveraging their ability to process symbolic representations, LLMs may be able to construct conceptual frameworks akin to core knowledge developed by humans."}, {"title": "5 MOVING FORWARD: GROWING AI LIKE A CHILD", "content": "The interpretations of potential reasons for core knowledge deficits in LLMs suggest a testable hypothesis: LLMs may be capable of acquiring core knowledge if trained on environmental stimuli similar to those available to a child. By leveraging their ability to process symbolic representations, LLMs might develop a structured understanding of fundamental concepts upon similar external stimuli accessible to a child developing the same concepts.\nFor this proposal to be viable, several key conditions would have to be considered:\n1. First, sensory grounding-models must be trained using multimodal inputs, incorporating both visual and linguistic data to approximate the rich perceptual experience of early human learning.\n2. Second, large-scale exposure-a sufficiently vast dataset is needed to ensure that core knowledge representations are robust and not lost due to extraction difficulties inherent in distributed neural representations.\n3. Third, minimizing environmental confounds-unlike a child, LLMs have unrestricted access to abstract information in their training data, making them prone to shortcut-taking by exploiting spurious correlations rather than developing genuine conceptual understanding. Ensuring that training data is curated to prevent such biases will be crucial.\nTo this end, this paper outlines a practical solution that meets these conditions: the large-scale development of synthetic data using physical engine, operate on a novel cognitive prototyping strategy."}, {"title": "5.1 COGNITIVE PROTOTYPE", "content": "To generate training datasets that enhance the saliency of direct representations of low-level cognitive concepts while minimizing environmental confounds, a promising approach is to directly train MLLMs on cognitive experiments commonly used in developmental psychology laboratories. The recent adaptation of these experiments into machine-readable formats, as seen in cognitively inspired benchmarks (Binz & Schulz, 2023; Li et al., 2025), provides a viable framework for constructing such datasets.\nEach low-level cognitive concept can manifest in multiple ways, necessitating diverse experimental paradigms for its assessment. To systematically structure these training datasets, we propose the cognitive prototype, a standardized framework for curating data for each cognitive ability. A cognitive prototype consists of detailed specifications of a cognitive experiment that operationalize the target concept, along with schematic descriptions of task conditions that allow for controlled variations. This structured approach enables systematic data generation at scale while preserving the conceptual rigor of experimental paradigms.\nFor example, the Three Mountain Task can be used to construct a cognitive prototype for training perspective-taking. In its standard form, this task involves presenting a child with a model featuring three distinct mountains-one covered in snow, another marked by a red cross, and the third topped with a hut. After allowing the child to observe the model from all angles, an experimenter introduces another individual, represented by a doll who views the model from a different vantage point. The child is then shown a set of photographs depicting various perspectives of the model and is asked to identify which image accurately represents what the other person sees (Piaget & Inhelder, 1969). Once the experimental paradigms are specified within the cognitive prototype, data generation can be systematically conducted at a large scale by varying task-relevant conditions in the experiments. These variations may include adjustments in the placement and appearance of the doll, changes in the observer's viewpoint, modifications to the model's design, or alterations in the probing questions (Gao et al., 2024). This approach ensures diverse and representative training data that reinforces the foundational principles of core knowledge while preventing models from relying on spurious correlations. By leveraging cognitive prototypes for structured data generation, this strategy offers a scalable and theoretically grounded approach to training MLLMs on core cognitive abilities, addressing key limitations in their current developmental trajectory."}, {"title": "5.2 SYNTHETIC DATA VIA PHYSICAL SIMULATIONS", "content": "Once the cognitive prototypes are established, systematically varying task conditions enables the generation of a vast dataset, where each instance presents a unique composition of task elements while consistently exemplifying the same underlying cognitive construct. However, manually labeling such large-scale data would be prohibitively costly and significantly limit the dataset's scope. To address this challenge, we propose the use of synthetic data generation, leveraging computational tools to create large, diverse, and precisely labeled datasets.\nRather than relying on real-world examples, data can be generated using physics engines such as Mujoco (Todorov et al., 2012) and the upcoming Genesis (Zhou et al.). These engines allow for precise control over task parameters, enabling the automated creation of experimental scenarios that adhere to cognitive science paradigms while eliminating the inconsistencies and biases present in real-world datasets. By explicitly defining the variables within each experimental paradigm and establishing structured protocols for administering variations, physics engines can systematically produce a vast array of training instances. This ensures that each generated example accurately operationalizes the target cognitive concept while introducing controlled diversity in conditions. Taken together, this approach offers a scalable and theoretically grounded method for training MLLMs on core knowledge. By combining cognitive prototypes with physics-based synthetic data generation, we can create high-quality datasets that reinforce fundamental cognitive abilities, reduce reliance on spurious correlations, and ultimately provide a more structured developmental foundation for machine intelligence."}, {"title": "6 CONCLUSION", "content": "This paper proposes that the robustness challenge and Moravec's Paradox, two key limitations of current LLMs with significant scientific and practical implications, may be jointly explained by differences in cognitive development between humans and machines. Specifically, these differences are not found within the dynamic process of improving representational power\u2014a mechanism likely shared between scaling up in LLMs and growing up in humans. Instead, they stem from LLMs' absence of core knowledge, a set of foundational cognitive abilities present in humans from early childhood. This core knowledge serves as the basis for gradually acquiring more complex skills over time. Empirical evidence presented in this paper demonstrates that such abilities are indeed missing in LLMs. Further analysis of the underlying causes of this core knowledge deficit suggests a viable solution: systematically increasing low-level representations of core cognitive domains within LLM training datasets.\nThis analysis underscores the need for future research to explore how core knowledge can be effectively incorporated into LLMs. Rather than contradicting the general principle of scaling laws, this perspective challenges the assumption that intelligence can emerge solely from domain-general cognitive mechanisms. Just as humans rely on developmental start-up software, LLMs may require structured early training to scaffold their cognitive growth. Encouragingly, given their ability to process vast amounts of high-level information, LLMs may not require innate structures but rather training data that prioritizes salient representations of low-level concepts-analogous to the perceptual input available to a child. Based on said theorization, this paper proposes an engineering solution leveraging large-scale synthetic data generation via simulation engines to systematically generate task scenarios based on developmental psychology paradigms. Importantly, there appears to be no fundamental technical barrier to pretraining core knowledge. The next step is to implement this approach and rigorously evaluate whether it enhances human-like cognitive competence, particularly in real-world robustness. This research agenda can be best summarized as growing AI like a child-at scale 4."}]}