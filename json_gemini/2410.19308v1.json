{"title": "Semantics in robotics: environmental data can't yield conventions of human behaviour", "authors": ["Dr Jamie Freestone"], "abstract": "The word semantics, in robotics and Al, has no canonical definition. It usually serves to\ndenote additional data provided to autonomous agents to aid HRI. Most researchers\nseem, implicitly, to understand that such data cannot simply be extracted from\nenvironmental data. I try to make explicit why this is so and argue that so-called\nsemantics are best understood as data comprised of conventions of human behaviour.\nThis includes labels, most obviously, but also places, ontologies, and affordances.\nObject affordances are especially problematic because they require not only semantics\nthat are not in the environmental data (conventions of object use) but also an\nunderstanding of physics and object combinations that would, if achieved, constitute\nartificial superintelligence.", "sections": [{"title": "1 INTRODUCTION", "content": "It is difficult to find a canonical definition of semantics in robotics.\u00b9 The term is used to\ndescribe the kind of data or information that an autonomous system would utilise in\nhuman-robot interaction (HRI), namely high-level abstract information such as labels,\ncategories, affordances, places, and so on. This much is unremarkable and there are\ncountless teams working on advances in these areas. But some researchers go further.\nThey envision systems that autonomously understand the larger meaning of objects and\nevents in their environment. They don't need to have the semantics provided, for they\nwill infer or extract it directly from raw data. This kind of semantics is always in the near\nfuture. Yet-to-be-invented robots will perceive their environments and, unsupervised,\nwill know the possible affordances of objects, the uses of a room, and the intended\nmeaning of utterances from their human interlocutors. They will do so without being\nlinked to pre-existing databases of high-level information and without exhaustively\nobserving and then imitating humans. The dream, in other words, is to have an artificial\nagent that can do what we do when we enter a new environment for the first time and\ninstantly comprehend not just the raw sensory data, but the broader meaning and\nsignificance of what we sense.\nIt is only a dream. The problem is that humans also cannot perform this feat,\nalthough the way we think can lure us into believing we can.\nIn this paper I outline a simple idea: robots cannot extract from the environment\ndata that are not in the environment. Almost all the data under the banner of\n\"semantics\u201d is of this form and must therefore be provided or learned in earlier forays. A\nconcrete example, taken from the literature, is recognising an ornamental mug in a\nhome (Kollar et al. 2013; Sarathy & Scheutz 2016, p.597). It might look indistinguishable\nfrom other mugs. Its geometric properties might suggest it can be used as a drinking\nvessel. But it is not to be used that way. To learn this, the robot would need to be privy to\na set of conventions: an idiosyncratic history of behavioural regularities of certain\nhumans. This history, though separately learnable, is not in the environment.\nI define environmental data as any data that could be extracted from the\nenvironment. This largely overlaps with sensory data. But in practice, such data might\nalso be supplied and hence not obtained directly through sensors. Environmental data\ncovers supplied and sensed data that is, in principle, extractable from the robot's\nenvironment.\nThere are many forms of non-environmental data, one of which is semantic data\nor semantic information, or simply and most commonly in the literature, semantics.\nThere are many problems in robotics that employ the concept of semantics: semantic\ngrasping, semantic reasoning, semantic segmentation, semantic place recognition,\nsemantic SLAM, semantic mapping, semantic communication. All these problems"}, {"title": "2 SEMANTICS IN ROBOTICS", "content": "Certain words jump out at a philosopher of mind while reading robotics papers:\ngrounding, beliefs, affordances, inference, reasoning, embodiment \u2014 and semantics.\nOccasionally, definitions are provided. Sometimes they overlap with definitions in\nphilosophy or cognitive science. Typically, the terms are left undefined. This can lend\ncover to researchers who inflate the abilities of current and future systems. This is not to\ncriticise robotics researchers, who use the prevailing terminology to indicate what\nothers seem to be indicating by those terms. (In other words, they follow conventions of\nuse). A word like semantics is not being used \"incorrectly\" just because it is used"}, {"title": "2.1 Examples of \u201csemantics\u201d from robotics and Al papers", "content": "I begin with an example from place recognition. The hope is to determine what a place is\nused for - e.g. what type of room it is purely from visual information and without a\npriori knowledge of the new place (note that in all the examples in this section, the\nitalics has been added):\nTherefore, cognitive robots should be able to carry out semantic inferences\nbased on mechanisms of interpretation of the context, even when places are\nvisited for the first time. (Crespo et al. 2020, p.18)\nSimilarly, from a highly influential article in the field of place recognition:\nThat is, the robot should be capable of classifying and producing labels for\nplaces about which no prior knowledge is available. In other words, the system\nshould be able to generalize the knowledge gained by exploring a specific spot,\nso as to infer about the semantic content of any other similar place. (Kostavelis &\nGasteratos 2015, p.1461)\nThe same ambition is held for determining object affordances. Researchers hope\nto one day have systems that can infer an object's possible uses or functions, merely by\nlooking at the object:\nAlthough the necessity of affordance perception from 3D information recovery,\nsuch as optical flow, has been stressed in previous work, we [...] intend to\ngeneralize towards the use of arbitrary features that can be derived from visual\ninformation (Fritz et al. 2006, p.4)\nIt has been shown in psychology that functionality (affordance) is at least as\nessential as appearance in object recognition by humans. In computer vision,\nmost previous work on functionality either assumes exactly one functionality for\neach object, or requires detailed annotation of human poses and objects. In this\npaper, we propose a weakly supervised approach to discover all possible object\nfunctionalities. (Yao et al. 2013, p.2512)\nBeyond the specifics of place recognition and object affordance, some\nresearchers attempt a new paradigm in signal processing or communication. Semantic\ninformation, which is somehow latent within the classical or Shannon information of a\nsignal, can be extracted or decoded:\nSemantic (Source) Encoder: detects and extracts semantic content (e.g.,\nmeaning) of the source signal and compresses or removes the irrelevant\ninformation. (Shi et al. 2021, p.46)\nThe researchers admit, later in the paper, that:\nThere is still lacking a simple and general solution for quick semantic information\ndetection and processing that can be implemented in resource-limited devices.\n(Shi et al. 2021, p.47)\nThe claims boil down to the aim of analysing data from the current scene or\nenvironment and somehow inferring or extracting contextual, historical, or otherwise\nrelated information that is not physically present in said environmental data. Das and\nChernova (2021) ask:\nHow can we autonomously extract contextual information grounded in an\nenvironment to provide meaningful explanations of system failures to everyday\nusers?\" (2021, p.3034)"}, {"title": "2.2 Definitions of semantics", "content": "In the absence of anything like a canonical definition of semantics in robotics or allied\nfields, I identify two implicit definitions and advocate the first one.\n1. Semantics as conventional: a conservative, operational usage that frames\nsemantics as high-level, abstract, or auxiliary data, generally for use in HRI. The\npresumption is that the meaning or significance of the semantics is dependent\non conventions of human use or behaviour. For example, labels that are\nattached to autonomously gathered visual data: in this case, the labels are\nsemantics, because they are entirely contingent upon conventions of human\nbehaviour (i.e. language use), which we desire the robot's autonomous\ninformation processing to relate to.\n2. Semantics as contentful: a more radical, potentially metaphysical claim that\ncertain types of data have inherent or intrinsic properties. This is semantics as\nthe true or accurate meaning of a signal or information, which is extractable from\nthe data itself because it is contained in the data. In short, the data possess\ncontent, regardless of context, which determines their meaning \u2014 e.g. if the\nlabels of objects were somehow determined by the object's visual or physical\nproperties."}, {"title": "3 SEMANTICS IN OTHER DISCIPLINES: CAUTIONARY TALES", "content": "Other disciplines study something akin to what engineers call semantics. Linguistics,\ncognitive science, and several branches of philosophy have produced large literatures\non questions relating to how meaning is extracted from text, speech, visual information,\nmental representations, symbols, artworks, memories, and logical propositions.\nDepending on the discipline and the precise topic or problem, this encompasses\nnotions like intentionality, reference, meaning, grounding, denotation, mental content,\nrepresentation, signification, and indeed, semantics. One umbrella term for all these\nphenomena is aboutness (Dennett 2017 p.112; Rosenberg 2018, p.120). This captures\nthe sense that some objects, patterns, or processes have some relation to something\nother than themselves. A rock is not about anything other than itself; but a rock with an\nicon of a dog carved into it may be interpreted as having some meaning beyond its\nphysical properties as a rock. The icon makes the rock \u201cabout dogs,\u201d or \u201crepresent\ndogs,\" or \"carry information about dogs,\u201d or \u201ccontain dog-related content\". But\nexplaining how this works is not easy. The problem of aboutness is as old as philosophy\n(Rosenberg 2013, p.3). The last one hundred and fifty years has seen concerted efforts\nin Western philosophy to account for aboutness in a formal, scientific, or naturalistic\nmanner. This includes many efforts to formalise semantic information to provide a\""}, {"title": "3.1 Semantics in information theory and communication", "content": "There is a continuity between the more modest, contextual version of semantics and\nthe original framing of semantic information. Claude Shannon's \u201cA Mathematical Theory\nof Communication\u201d (Shannon 1948), the founding document of information theory,\nprovides a salutary episode in recent history, and one that might appeal to roboticists\nand engineers. On the first page, Shannon says:\nThe fundamental problem of communication is that of reproducing at one point\neither exactly or approximately a message selected at another point. Frequently\nthe messages have meaning; that is they refer to or are correlated according to\nsome system with certain physical or conceptual entities. These semantic\naspects of communication are irrelevant to the engineering problem. (1948,\np.379; italics in original)\nAlthough the statistical or syntactic properties of a message (number of bits, entropy,\nredundancy) can be measured and quantified by analysing the message itself, its\nsemantic content is an entirely separate question. No one has since developed an\nequivalent theory for quantifying semantic information.\nThe key point is Shannon's comment on the \u201csemantic aspects\u201d or \u201cmeaning\" of\na message. He says they are not in the signal itself (as the statistical properties are) but\nare separate to or beyond the physical instantiation of the message. Messages may\n\u201crefer to\u201d or be \u201ccorrelated...with\u201d other \u201cphysical or conceptual entities\u201d. In other\nwords, they can be associated by the receiver with some other objects or events and\nthis makes them meaningful to the receiver. Any analysis of only the message\ncomprised of some data \u2014 will fail to illuminate the semantic aspects, including the\nsender's intended meaning. The semantics depend on how the message has been\ncorrelated, in the past, with something else. The receiver of the message must be privy\nto those correlations, otherwise they will be able to analyse only the statistical or\nclassical informational properties.\nIt's worth briefly articulating why the intended meaning of the sender is a\nnonstarter for engineers. There are two main problems. First, this intended meaning\ncannot be measured as an output, whereas its effect on the receiver can be. Inasmuch\nas there is disagreement between the intended meaning and the effect, selective\nprocesses, natural and artificial, will weed out senders whose messages are malformed\nand misinterpreted (Skyrms 2010). Senders needn't have intended meanings of any\nexplicit kind, as is the case in signalling systems involving simple machines or single-\ncelled organisms (see Subsection 4.1). The second problem is that when we discuss\nsemantic information outside of a communication problem, there is often no sender at"}, {"title": "3.2 Comparison with semantics in robotics", "content": "The distinction between Shannon information and semantic information is clear in the\ncase of communication. The analogy to robotics is imperfect, but we will use it\nadvisedly.\nIn communication, semantics are any associated actions taken by the receiver\nupon receipt of the message. I use actions broadly. These could be external behaviours\n(physical actions) or internal behaviours (updating knowledge, changing posture, etc.).\nA latent assumption in early work on communication was that the sender and receiver\nare humans, and so the relevant actions are human behaviours. But proposed 6G\nsemantic encoders have a receiver that is an autonomous system which will encode the\nmessage in terms of its relevance to human users (Strinati & Barbarossa 2021; Xin et al.\n2024, p.4). I argue that the proposed encoder will not be able to extract from the signal\nany such semantics, and even an uninitiated human cannot do this. The semantics\nthe effects of a message depend on separate experience. Specifically, one needs to\nknow which messages map to which actions. Without this, even a human is receiving a\nmeaningless message in an unknown tongue; a fortiori, an autonomous system cannot\nknow, from the message alone, how it maps to human actions or effects. It is the\nmessage-action mapping which constitutes the semantics.\nIn robotics, as opposed to communication, an autonomous agent will take\nenvironmental data the \"message\", albeit without a sender \u2014 and map it to a set of\nactions, according to a policy, rules, an objective function, etc. But robotics researchers\nwould not regard this as semantics (at least my research has not revealed any who do).\nRegardless of how sophisticated and abstract the methods for marrying environmental\ndata to behavioural effects might be, they are not considered to be semantics (even\nthough they form a kind of message-action mapping) if those actions are solely robot-\ncentric. 10 The sine qua non for semantics in robotics is that of human effects. The\nsemantics we provide to robots - labels, semantic maps, ontologies for place\nrecognition, object affordances - are derived from a set of effects on human agents\nthat we want the robot to be aware of. They are precisely not recoverable from\nenvironmental data, because they are about how humans respond to certain\nenvironmental data of their own. In other words, they are an imported set of message-\naction mappings already known to be used by humans. 11 They can then be used to\ninform the robot's actions. A robot on an assembly line typically has no need for these. A\ndomestic robot, meanwhile, does need to behave with respect to categories\nestablished by and for humans in the past. And because semantics are predefined\naccording to exogenous human needs, they are not discoverable from environmental\ndata: the robot needs separately to learn or be given the relevant human conventions.\nHere again is the imperfect but useful bridging analogy with communication.\nCommunication. Shannon distinguished between:\n1. messages: the data or signal being transmitted; and\n2. semantics: the message's effects on a human receiver.\nRobotics. We can distinguish between:\n1. environmental data: gleaned by the robot from sensors or simply provided by\nprogrammers; and\n2. semantics: data from human interactions, i.e., historical effects on human\nreceivers of certain data (the semantics in communication, according to\nShannon).\nwhich is surely\nThis is a way to retain the use of the word semantics in robotics and Al\nestablished while aligning it with Shannon's definition of semantics in\ncommunication. In both areas, semantics is distinguished from raw or environmental\ndata, which can be processed independently of human conventions. Those conventions\nof human actions in response to certain data are semantics, which can be framed\ninformally as the meaning people derive from certain information."}, {"title": "3.3 Semantics in other sciences", "content": "Other branches of science encounter the same difficulties in demonstrating a\ncontentful version of semantics. In Shannon's wake, Wiener (1967) was one of the first\nto try to transplant information theory from communication to living systems. Wiener\ndefined semantic information as that which activates something in the receiver, helping\nit to act effectively (Oyama 1985, p.66). This is concordant with the operationalised\ndefinitions found in robotics and related areas. It is essentially the effectiveness aspect\nhighlighted in early communication theory (Shannon and Weaver 1949, p.25, p.75)\napplied to living and nonliving systems, under the aegis of cybernetics (Wiener 1967).\nIn biology, there was much debate about whether a gene \u201ccarries information\nabout", "information": "n\u201cwe suggested that it might be better to abandon the concept of semantic information\naltogether in discussions of the ritualisation of signals", "problem": "one set of Shannon information\nwill affect receivers differently depending on their history \u2014 in this case, the organism's\nevolutionary and ecological history that shaped its phenotype and behaviour. And it is\nthese variable effects of the same (or similar) environmental data on different agents\nthat forms the social-interactive nature of human semantics, i.e. ways that humans\ntypically respond to a given context."}, {"title": "4 INSIGHTS FROM PHILOSOPHY OF MIND", "content": "The key point of this paper is that the semantics of words, like any other information\narising from social interaction, is based in conventions of use, as opposed to intrinsic\nproperties of the medium. This is another way of saying: context over content."}, {"title": "4.1 Convention", "content": "Many phenomena have been modelled as conventions: language, money, road rules,\netiquette, fashion, etc. To understand systems like these, one must be aware of at least\nsome of the history of interactions among participating agents. This is partly because\nthe conventions could be otherwise: a place that drives on the right might have adopted\na convention of driving on the left instead. Alternatives needn't be completely arbitrary.\nThere might be systemic reasons why some alternatives are more or less likely to be\nadopted; it is unlikely that a place would adopt driving down the middle of the road as a\nconvention. But without knowing which convention has actually prevailed, one cannot\nknow ex ante which convention agents will adopt. It is also necessary to know the\nhistory of interactions because that is what constitutes a convention, rather than the\npresent instance or a one-off event.\nThe first rigorous study of conventions was Lewis' monograph, Convention\n(1969). He analysed the development of conventions via game theory, specifically\ncoordination problems. In such problems, agents have a mutual interest in following the\nsame convention, e.g., both driving on the left-hand side of the road to avoid collisions.\nThe Nash equilibrium in these games is when agents match others' behaviour.\nDefecting can only be detrimental. Defection must be possible, though, for it to be a\nconvention, as opposed to some other regularity where no choice is involved (Lewis\n1969, pp.69-70). In the case of the semantics of place, for instance, agents might\nevince regularities in behaviour in a certain room. Only some regularities will be\nconventional because only some involved choice. Obeying the law of gravity is not a\nconvention; removing one's hat inside is conventional.\nThe choice doesn't have to be conscious or deliberate. Perhaps the most\nimportant extension to Lewis' work is Skyrms' Signals (2010). Skyrms introduces simple\nsignalling games that model senders' and receivers' behaviour in establishing\nconventions. The agents in many of Skyrms' games aren't conscious or even especially\nagential. He models the signalling between bacteria and even between cells in the\nsame organism (2010, pp.29\u201331, pp.118\u201320, pp.151\u20133). Indeed, Skyrms' account is\nevolutionary. He shows how, in just a few iterations, a convention will evolve in a\ncompletely blind manner, often depending on chance differences in initial conditions.13"}, {"title": "4.2 Why contentful semantics are alluring", "content": "The more ambitious hopes for semantics in robotics and Al may stem from\noverestimating what humans do. When encountering a scene, it might seem like people\n\"extract\" semantics from their sensory data. The temptation is to emulate this ability in\nautonomous agents despite it being impossible for humans. Briefly, here are three\nfactors, courtesy of philosophy of mind and cognitive science, which explain why it\nappears that we extract semantics from environmental data.\nProjection. (Also projectivism, projectionism, the problem of perception.) The\nScottish philosopher David Hume is normally credited as the originator of this idea: \u201cthe\nmind has a great propensity to spread itself on external objects, and to conjoin with\nthem any internal impressions, which they occasion\u201d (Hume 1739/2000, p.112). Human\nperception works in such a way as to project the associations and conceptual\ninformation triggered by an object of perception onto the object of perception. One sees\na dog and various associations or \u201csemantics\u201d related to dogs are activated in one's\nmind: crudely put, one's DOG concept is activated. But this concept includes\ninformation not at all present in the current sensory data (Dennett 2017, pp.354\u20138). The\nperception of the dog, drawing on past experience, is in many ways richer than what is\nin the data a classic case of semantics being used to supplement environmental\ndata. But from a person's own perspective, the richer properties of the DOG concept\nseem to inhere in the actual dog in the environment currently being perceived. Some of\nthese properties, such as the dog's colour, are entirely artefacts of the perceptual\nsystem. Others could be said be \u201creal\u201d properties possessed by the dog independent of\nour perception, but which are not actually recoverable from the present instance, e.g.\nthe dog's cuteness, whether or not it is a dangerous animal, if it belongs inside or\noutside, that it can be petted, and so on. These might be properties gleaned from past\ninteractions with other objects in the category of DOG or from past interactions with\nother humans relevant to this particular dog (conventions).\nPrediction. These projections are part of a prediction of what one is sensing.\nCognition is prediction-heavy. An accounting of the brain's traffic shows there are more\nefferent signals than afferent ones (Dennett 2017, p.169). A fair generalisation is that\nthe last thirty years of cognitive science has seen a shift to emphasising the predictive\nnature of cognition. Predictive processing or predictive coding, the Bayesian Brain\nhypothesis, active inference models, and the free energy principle all approach\ncognition as being a multi-level process of forming predictions to aid behaviour and\nhaving those predictions modified by feedback from the senses (Hohwy 2018). Even\nwith highly evolved sense organs, environmental data is often too noisy and/or too\nincomplete to be relied on: better to learn regularities in the environment and then\nmake informed predictions of what is causing a current perception. Sensory data is\nthen used mainly as an error signal to modify the prediction which was based on past\nexperience (Dennett 2017, pp.167\u201370). Again, perception is often richer than the current\ndata allow.\nNonconscious cognition. A final insight from philosophy concerns the larger\nproject of trying to emulate human capacities in robots. Take, for example, cognitive\nmaps. Humans, like other mammals, have sophisticated systems for navigating space:\ngrid cells, head-direction cells, boundary cells, and so on, which, combined, make up a\ncognitive map (Rosenberg 2018, 131-8). Our conscious experience of navigating in no\nway resembles the nonconscious cognitive map architecture. We aren't conscious of\nspace being divided into tessellated triangles forming hexagons, or of grid cells\nactivating a preplay sequence as we take the door to the living room rather than the\nkitchen (Rosenberg 2018, pp.141\u201356). Using introspection alone, we could never have\nobtained the functional details of the cognitive map architecture. Some teams have had\ngreat success adapting the nonconscious features of the brain's cognitive maps to\nperform navigation in robots (Kuipers 2000; Milford et al. 2004). But building a system\ninspired by our intuitions of how we navigate space would be a bad or at least unhelpful\nidea.\nAll these contribute to the strange situation whereby we expect robots to do\nsomething impossible. Robots cannot extract semantics from environmental data; the\nsemantics are in the relations between environmental data and human activity; and\nthese relations constitute separate data. In a sense, robots don't make this error. They\nprocess what they can from environmental data. It is we who then expect them to mine"}, {"title": "5 IMPLICATIONS FOR PROBLEMS IN ROBOTICS", "content": "We can classify some of the problems in robotics already mentioned, according to what\nkind of data they need to be solved: environmental data, semantics, or a combination.\nLabels are purely conventional: they are solely a product of a history of human\ninteraction. Therefore, they can be provided to the system, as in a database of labels\nand their associations with environmental features; or they can be learned, as in the\ncase of language models that can accept new vocabulary given by humans. They are\nthe most straightforward case of an autonomous system acquiring semantics to aid in\ntasks that involve interacting with convention-using agents, namely humans.\nGrasping may appear to be a purely environmental problem, where the system\nmust comprehend the geometry and physical features of the object to be grasped.\nHowever, as in the case of what is called \u201csemantic grasping\u201d, contextual or semantic\ninformation is sometimes needed (Murali et al. 2021; Tremblay et al. 2019). As Liu et al.\n(2020, p.2550) suggest, we might like a robot to not only select a secure grasp for an\nobject like a pair of scissors, but to know to pick them up blade-first so they can be\nsafely handed to humans. That kind of information is conventional and so cannot be\ninferred from the environmental data alone; notably that particular example remains\nunsolved by Liu et al.'s system (2020, p.2553). Grasping in a domestic context is\ntherefore a mixed data problem.\nPlace recognition. Most work in place recognition tries to marry geometric\nproperties obtained from environmental data with high-level concepts, to establish, for\nexample, the type of room the robot is in. This is a mixed problem. Most approaches use\nontologies of rooms and objects (Crespo et al. 2020, p.2; Pronobis & Jensfelt 2012,\np.3515). The ontologies can be provided, constituting a ready example of semantics as\na form of information given to aid HRI, not unlike labels. Place recognition is\ncomplicated, however, when researchers want the robot to autonomously infer place\nwhen they \"visit for the first time\u201d (Crespo 2020, p.16). This may require additional\nsemantics if it is a nontypical room. Whereas labels tend to be conserved across\npopulations of human users (from the same linguistic community), place designations\nare more variable. Examples are manifold. The presence of a bed might lead the robot\nprobabilistically to conclude they're in a bedroom, while they're actually in a studio\napartment; or perhaps they are in the garage where an excess bed is being stored before\nit is sold; or perhaps the room they're in is really the \u201cstudy\u201d and is referred to as such by\nthe human inhabitants, but occasionally doubles as a guest bedroom. Such cases\nillustrate that place is sometimes determined more by the human behaviours\nperformed in that place rather than the objects contained in it. What's more, there is not\nany correct answer beyond what the local humans converge on, and, in the case of"}, {"title": "5.1 Affordances", "content": "In robotics", "actions": "ncut_with", "intrinsic properties": "n(Varadarajan & Vincze 2012) of objects that are somehow inside them", "telling": "the use or non-use of this object is less to do with its physical properties\nand entirely tied up with local human custom. Just as unfamiliar human customs can\nbe learned by ethnographers embedded in foreign cultures", "ethnographer's challenge\u201d (Table 1) of learning human conventions of\n\"correct\u201d object use.\nThe flipside is that an \u201cincorrect\u201d but novel use of an object may violate\nconvention while still performing a task. By convention, a large book might make an\nersatz doorstop. A MacGyver-esque robot able to recognise certain physical properties\nof objects might find it is possible, though unconventional, to use other weighty items\nfor this purpose": "a bag of rice", "survivalist's challenge": "n Table 1.)\nA further problem is that most human affordances are multi-object\ncombinations. This is an overlooked point in the literature on artefact function", "use": "a branch is a perch, a seedpod is food, etc. But even\nsimple household affordances described in the robotics are actually part of multi-\nobject tasks. Objects like cups are filled with other objects like juice; affordances like\ncut_with depend not only on properties of the cutting object (sharp edge, etc.) but"}]}