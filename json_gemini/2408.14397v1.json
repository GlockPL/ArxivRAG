{"title": "Uncovering Knowledge Gaps in Radiology Report Generation Models through Knowledge Graphs", "authors": ["Xiaoman Zhang", "Juli\u00e1n N. Acosta", "Hong-Yu Zhou", "Pranav Rajpurkar"], "abstract": "Recent advancements in artificial intelligence have significantly improved the automatic generation of radiology reports. However, existing evaluation methods fail to reveal the models' understanding of radiological images and their capacity to achieve human-level granularity in descriptions. To bridge this gap, we introduce a system, named ReXKG, which extracts structured information from processed reports to construct a comprehensive radiology knowledge graph. We then propose three metrics to evaluate the similarity of nodes (ReXKG-NSC), distribution of edges (ReXKG-AMS), and coverage of subgraphs (ReXKG-SCS) across various knowledge graphs. We conduct an in-depth comparative analysis of AI-generated and human-written radiology reports, assessing the performance of both specialist and generalist models. Our study provides a deeper understanding of the capabilities and limitations of current AI models in radiology report generation, offering valuable insights for improving model performance and clinical applicability.", "sections": [{"title": "Introduction", "content": "Artificial Intelligence (AI) models have recently achieved remarkable success in interpreting medical images (Rajpurkar and Lungren 2023; Rajpurkar et al. 2022). Among them, radiology report generation stands out as a crucial task in medical imaging, providing essential information for further diagnosis and treatment planning (Liu, Tian, and Song 2023; Reale-Nosei et al. 2024). Its significance has led to a surge in research focused on developing AI models capable of generating these reports (Zhang et al. 2020; Liu et al. 2024). However, in-depth understanding radiology report generation models' performance is a challenging yet important task for real clinical usage.\nVarious automated evaluation metrics have been proposed specifically for report generation, such as RadCliQ (Yu et al. 2023), FineRadScore (Huang et al. 2024), RaTEScore (Zhao et al. 2024) and GREEN (Ostmeier et al. 2024), etc. These metrics have gradually approached the quality of radiologists' evaluations. Yet, most existing metrics rely on report-to-report comparisons, which fail to fully capture a model's holistic understanding of radiological images or its capacity to match the descriptive granularity used by humans. For example, when a doctor mentions \"edema\" in a report, they may use nuanced modifiers such as \"moderate\", \"mild\", \"unchanged\u201d, \u201cdecreased\", or \"stable\" to convey precise details. In contrast, a model might not capture this level of detail or variation in terminology. It is essential to develop evaluation methods considering the comprehensiveness of medical terminology understanding. These insights can guide the improvement of report generation models, ensuring they are better aligned with the professional descriptions used by radiologists.\nIn this paper, we target assessing AI models from a different perspective by focusing on the radiological knowledge learned by the model. To accomplish this, we introduce a system named ReXKG, designed to extract structured information from processed reports and construct a comprehensive radiology knowledge graph. As shown in Figure 1, this graph will capture relationships between anatomical structures, pathologies, imaging findings, medical devices, and procedures, creating a rich, queryable representation of radiological knowledge. We propose three novel metrics: ReXKG-NSC for assessing node similarity, ReXKG-AMS for evaluating edge distribution, and ReXKG-SCS for measuring subgraph coverage across knowledge graphs. These metrics allow for a global score comparison between models and against human radiologists, providing a comprehensive understanding of the model's performance.\nBased on the knowledge graph and proposed metrics, we conduct a comprehensive analysis of both specialist and generalist report generation models, exploring the following questions and summarizing the main conclusions for each:\nQ1: Coverage of Entities. How well do the generated reports cover essential entities such as anatomy and disorders? Generalist models demonstrate broader coverage, capturing nearly 80% of essential entities, yet they still fall short of matching the depth of radiologist-written reports, particularly in detailing medical devices.\nQ2: Coverage of Relationships Between Entities. How comprehensively do the AI reports describe connections between different medical findings and their descriptions? All AI models show significant gaps compared to radiologist-written reports in capturing relationships between different entities, with MedVersa leading, achieving nearly 80% coverage of the top 10% subgraphs.\nQ3: Coverage of Concepts or Descriptors. How detailed and comprehensive are the descriptions of disorders and anatomical features? AI models tend to overfit specific concepts that appear frequently in the training data, resulting in less detailed and occasionally hallucinated descriptions.\nQ4: Quantitative Measurements Coverage. How frequently does the model provide quantified measurements of disorders? AI model's behavior in providing size descriptions correlates strongly with the frequency of size descriptions for specific disorders in the training data.\nQ5: Specialist vs. Generalist Models. What are the performance differences between specialist and generalist models? Generalist models, trained on multiple modalities of data, demonstrate significantly enhanced radiology knowledge compared to specialist models. This suggests that exposure to a broader range of medical data and tasks contributes to a more comprehensive and accurate representation of radiological concepts and relationships."}, {"title": "Knowledge Graph Construction", "content": "In this section, we present our system (ReXKG) for constructing a comprehensive knowledge graph from a large corpus of radiology reports, shown in Figure 2. We first define an information extraction schema tailored to the radiology domain, then once the entities and relationships are extracted, we proceed with the node construction pipeline to ensure data consistency and integrity. Finally, we integrate the information into the graph structure."}, {"title": "Information Extraction Schema", "content": "Definition. We define an entity as a continuous span of text that can include one or more adjacent words. Entities in our schema are categorized into six types as listed.\n\u2022 Anatomy: anatomical structures within the body.\n\u2022 Disorder: any abnormal findings or diseases identified within radiology reports.\n\u2022 Concept: descriptors used to modify other entities, for example, \"acute\", \"severe\", and \"increasing\".\n\u2022 Device: any instrument or apparatus used for medical purposes, for example, \u201ctube\u201d, \u201cclip\u201d, \u201cwire\u201d.\n\u2022 Procedure: medical procedures used to diagnose, measure, monitor, or treat conditions, such as \"sternotomy\".\n\u2022 Size: measurements of disorders or anatomical structures, for example, \"3-mm\".\nWe define a relation as a directed edge between two entities. Following the previous work (Jain et al. 2021a), our schema uses three relations as listed.\n\u2022 Suggestive of: source entity (e.g., findings) may suggest the presence of the target entity (e.g., a disease).\n\u2022 Located at: source entity is located at the target entity.\n\u2022 Modify: source entity modifies or provides additional information about the target entity."}, {"title": "Entity and Relation Extraction", "content": "Given a set of radiology reports, we first annotate a subset using GPT-4 (Achiam et al. 2023) to generate labeled entities and relations. The prompts used for annotation are provided in the appendix. Based on the annotated data, we train the model using the Princeton University Relation Extraction system (PURE) architecture (Zhong and Chen 2021) to do Named Entity Recognition (NER). This architecture employs a pipeline approach, decomposing the tasks of entity recognition and relation extraction into separate subtasks. Once the model is trained, we apply it to the entire dataset to perform inference, extracting all relevant entities and relations."}, {"title": "Nodes Construction", "content": "Following entity extraction, we employ a series of steps to remove noise, merge synonyms, and link entities to the Unified Medical Language System (UMLS) (Bodenreider 2004). First, we determine the entity type of each extracted entity based on the most frequently predicted type by the NER model to ensure consistency and accuracy. Next, we utilize ScispaCy (Neumann et al. 2019) to retrieve UMLS attributes for each entity, such as Concept Unique Identifiers (CUI), Type Unique Identifiers (TUI), definitions, and aliases. Entities that cannot be mapped to a UMLS item are retained for further processing. For entities identified as aliases of a specific term in UMLS, we normalize these entities by merging them into a single concept. For instance, entities such as \"pulmonary\" and \"lung\" are normalized to their corresponding CUI C0024109. Additionally, to ensure the compactness and unambiguity of nodes, for the multi-word entities, if all individual words of such an entity are predicted as separate nodes, the combined multi-word entity is not included as a node. The detailed algorithm is provided in the appendix. Finally, we leverage medical language models to merge entities based on semantic similarity. Entities with an embedding similarity higher than a defined threshold are combined. This step enhances the graph's coherence by aggregating semantically similar concepts into single nodes."}, {"title": "Edges Construction", "content": "Initially, all relations are extracted from the dataset as triplets (source entity, target entity, relation). We merge different triplets with the same source and target entities based on node aliases. When two nodes are linked by multiple relation types, we retain the relation type most frequently predicted by the model. Finally, we filter the relations by ignoring triplets with a count less than C, a hyperparameter ensuring the reliability of the connections within the graph."}, {"title": "Knowledge Graph Evaluation Metrics", "content": "To evaluate knowledge graphs obtained from different models, we introduce three metrics that assess node similarity, edge distribution similarity, and subgraph coverage: ReXKG-NSC (Node Similarity Coefficient), ReXKG-AMS (Adjacency Matrix Similarity), and ReXKG-SCS (Subgraph Coverage Score). In the following, we will first provide a preliminary definition of the knowledge graph and then detail the calculation methods for these metrics."}, {"title": "Preliminary Definition", "content": "Assume we have a knowledge graph with N nodes and M edges. The set of nodes is denoted as V = {V_1, V_2, ..., V_N}. The weights of the nodes are represented as W_V = {W_{V_1}, W_{V_2}, ..., W_{V_N} }, where where w_{v_i} corresponds to the frequency of node v_i in the data. The set of edges is denoted as E = {e_1, e_2,...,e_m}, where each edge e_m connects a pair of nodes (v_i, v_j). The weights of the edges are represented as W_E = {W_{e_1}, W_{e_2},..., W_{e_m} }, where w_{e_m} = count(e_m). Then, the adjacency matrix is defined as A, with A_{ij} = w_{e_{ij}}, representing the weight of the edge between nodes v_i and v_j."}, {"title": "KG Node Similarity Coefficient", "content": "Let KG-GT represent the knowledge graph built from the ground truth reports, consisting of N nodes. Similarly, let KG-Pred represent the knowledge graph built from the generated reports, consisting of P nodes. For each node v_i in KG-GT, we identify the most similar node in KG-Pred, assigning a similarity score s_i based on calculations from a medical language model. The overall node similarity metric is then calculated as the average of these similarity scores across all nodes in KG-GT. This can be expressed as:\nKG-NSC = \\frac{1}{N} \\sum_{i=1}^{N} s_i.  (1)"}, {"title": "KG Adjacency Matrix Similarity", "content": "For each node v_i in KG-GT, we identify the most similar node in KG-Pred. This allows us to map all edges in KG-Pred using the nodes from KG-GT, resulting in the creation of two adjacency matrices, A_{pred} and A_{GT}, both of the same size. Where A_{ij} represents the weight of the edge between nodes i and j. We use the Pearson correlation coefficient metrics to evaluate the coverage of relations in generated reports compared to the ground truth. The row weight w_{r_i} is used as the weight, and the Pearson correlation coefficient as the value. Here, for a given row i, the row weight is defined as w_{r_i} = (\\sum_j A_{ij})/(\\sum_i \\sum_j A_{ij}), where A_{ij} represents the element at row i, column j of the adjacency matrix. Thus, the adjacency matrix similarity can be expressed as:\nKG-AMS = \\frac{\\sum_{i=1}^{N} (w_{r_i} \\cdot corr(A_{pred,i}, A_{GT,i}))}{\\sum_{i=1}^{N} w_{r_i}} (2)\nwhere corr(A_{pred,i}, A_{GT,i}) is the Pearson correlation coefficient between the i-th rows of A_{pred} and A_{GT}, and w_{r_i} is the weight of all edges associated with the i-th row."}, {"title": "KG Subgraph Coverage Score", "content": "Let S be the set of all connected subgraphs in KG-GT up to a size of k nodes. We quantify a model's ability to represent important subgraphs from KG-GT within KG-Pred, which can be expressed as:\nKG-SCS = \\frac{\\sum_{i=1}^{K} I(S_i) \\cdot P(S_i)}{\\sum_{i=1}^{K} I(S_i)} (3)\nwhere K is the number of top important subgraphs considered. I(S_i) is the importance score of each subgraph S_i in KG-GT and P(S_i) is the presence score in KG-Pred. Please refer to the appendix for detailed definitions."}, {"title": "Experiments", "content": "In this section, we present the dataset and models used in our analysis of AI-generated reports. Given the current limitations in model capabilities, with few models available for generating CT/MRI reports, our study primarily focuses on chest X-ray report analysis. However, the proposed ReXKG is versatile and applicable across various modalities and anatomical regions, as demonstrated in the appendix."}, {"title": "Datasets", "content": "CheXpert Plus: CheXpert Plus (Chambon et al. 2024) is a dataset that pairs text and images, featuring 223,228 unique pairs of radiology reports and chest X-rays from 187,711 studies and 64,725 patients. Each patient may be linked to multiple studies, and each study may include several images.\nMIMIC CXR: MIMIC-CXR (Johnson et al. 2019) is a large publicly available dataset of chest X-rays with free-text radiology reports. The dataset contains 377,110 images corresponding to 227,835 radiographic studies performed at the Beth Israel Deaconess Medical Center."}, {"title": "Experiments Settings", "content": "To ensure a comprehensive analysis, we randomly split studies from CheXpert Plus into two parts: CheXpert Plus I (24,086 studies) and CheXpert Plus II (24,085 studies). Additionally, we randomly select a subset from MIMIC-CXR with 24,085 studies for comparison. We designate CheXpert Plus I as the benchmark for our study. This subset serves as the ground truth, upon which all model evaluations are conducted, inference tasks performed, and knowledge graphs constructed. Similarly, we can set CheXpert Plus II as the benchmark, with results provided in the appendix. The knowledge graphs for comparison can be categorized into two groups based on the data source.\nIntra-Dataset Reports: Intra-Dataset Reports are knowledge graphs built from real clinical datasets across different studies or centers. We use CheXpert Plus II and the selected MIMIC-CXR subset, which represent radiologist-written reports from various studies and centers, as benchmark baselines for comparison with AI-generated reports.\nExtra-Dataset Reports: Extra-Dataset Reports are knowledge graphs constructed from AI-generated reports. To comprehensively evaluate AI performance, we assess various report generation models, including specialist models such as CvT2DistilGPT2 (Nicolson et al. 2023), RGRG (Tanida et al. 2023), and Swinv2-MIMIC (Chambon et al. 2024), as well as generalist models like CheXagent (Chen et al. 2024), RadFM (Wu et al. 2023), and MedVersa (Zhou et al. 2024). Here, specialist models are defined as those trained exclusively on chest X-ray report generation, whereas generalist models are large-scale models trained on various tasks. Details of these models can be found in the appendix."}, {"title": "Implementation Details", "content": "For the Information Extraction Schema, we follow the approach described in (Jain et al. 2021a), utilizing the PURE framework (Zhong and Chen 2021), which employs a pre-trained BERT model to obtain contextualized representations. These representations are then fed into a feedforward network to predict the probability distribution of entities, which subsequently serves as input for the relation model. The learning rate is set to 2e-5 during training. We use MedCPT (Jin et al. 2023) as the default medical language model for entity merging, with a merging threshold of 0.95. The threshold C for edge construction is set to 5. The number of nodes in each subgraph is set to k = 2, and the number of important subgraphs, K, is defined as 10% of the total subgraphs in KG-GT. For report generation inference, we use the code and checkpoints provided by the respective baseline models, focusing on the generation of the findings section. All experiments are conducted on an NVIDIA A100 GPU."}, {"title": "Results", "content": "In this section, we present a comprehensive analysis of knowledge graphs generated from both intra-dataset reports (radiologist-written) and extra-dataset reports (AI-generated). Using CheXpert Plus I as our benchmark, we hypothesize that the knowledge graph generated from CheXpert Plus II will display similar nodes, edges, and distribution characteristics. Such similarity would validate the consistency of our findings and underscore the reliability and quality of our proposed methods for constructing knowledge graphs. Our analysis is structured around key questions that probe different aspects of report generation, from entity coverage to relationship comprehension, providing a multi-faceted view of current AI models' capabilities."}, {"title": "Q1: Coverage of Entities", "content": "First, we explore the question: How well do the AI-generated reports cover essential entities such as concepts, anatomy, disorders, devices, and procedures?\nAs shown in Table A1, We compare the KG-NSC between CheXpert Plus I with other datasets and various report generation models. CheXpert Plus II and MIMIC-CXR, representing radiologist-written reports with similar and differing distributions of ground truth, exhibit high similarity across all entity types, with overall scores of 0.970 and 0.928. This high similarity demonstrates the reliability of the proposed metric and sets a high benchmark for AI models to match. Among AI models, generalist models, particularly RadFM and MedVersa, exhibit broader coverage of essential entities compared to specialist models. This superior performance likely stems from their training on more diverse and large-scale datasets, enabling these models to generalize better and capture a wider range of medical entities.\nWhen examining the results for each entity type, there is a noticeable gap in medical devices across all models. This discrepancy may be attributed to the primary factor that models are exclusively trained on the MIMIC-CXR dataset, thus the models' predictions align more closely with MIMIC-CXR's distribution. However, there are inherent distribution differences between the CheXpert Plus and MIMIC-CXR datasets. CheXpert Plus includes some rare devices, such as the \u201cImpella\", which is mentioned only 15 times in the entire CheXpert Plus dataset. Additionally, varied terminology is used to describe the type of devices, such as \"keofeed\" for \"tubes\".\""}, {"title": "Q2: Coverage of Relationships Between Entities", "content": "Next, we investigate How comprehensive is the coverage of relationships between entities?\nTo evaluate the comprehensiveness of AI-generated reports in capturing relationships between entities, we employed the KG-AMS and KG-SCS metrics. Table A1 details the correlation between specific types of relationships: disorders with anatomy, devices with anatomy, and relationships between disorders. MedVersa leads in the KG-AMS metric across most categories, particularly excelling in disorder-disorder and overall relationships. CheXagent, on the other hand, stands out in device-anatomy relationships, while RadFM shows balanced performance across various types of entity relationships. Despite these performances, there remains a significant gap compared to radiologist-written reports, highlighting areas for further improvement. The KG-SCS metric (with k=2) offers additional insights into how well models capture important subgraphs or patterns within the knowledge graph. MedVersa covers 80.6% of the important subgraphs, while RadFM covers over 73%, indicating that while these models perform well, there is still room for enhancement in capturing complex relationships."}, {"title": "Q3: Comprehensiveness of Concepts", "content": "We further access the quality of content generated by AI models with the question: How detailed and comprehensive are the descriptions of disorders and anatomical regions provided by the AI models?\nThis question is critical for applying AI models in clinical scenarios, where the ability to describe and differentiate the severity of diseases can directly impact diagnosis and treatment planning. To assess the depth and comprehensiveness with which disorders and anatomical regions are described, we utilize GPT-4 to classify all concept nodes within our knowledge graphs. These concepts are categorized into the following:\n\u2022 Severity: Describes how intense or severe the symptoms are, such as mild, moderate, or severe.\n\u2022 Location: Specifies where on or in the body the disorder manifests, such as left, right, bilateral, upper, lower, or specific organs or systems involved.\n\u2022 Duration: Refers to how long the disorder or its symptoms have been present. (acute, chronic, transient)\n\u2022 Progression: Indicates how the disorder changes over time, including progressive, stable, and regressive.\n\u2022 Size: Relevant for physical abnormalities or tumors, indicating how large an affected area or lesion is.\n\u2022 Number: Describes how many lesions or abnormalities are present, such as single, multiple, or widespread.\nOur analysis, depicted in Figure 3, shows that Intra-Dataset groups exhibit the highest similarity, with nearly identical counts for all category concepts used to modify disorders and anatomy. In contrast, AI models tend to underperform, especially in categories like \u201cseverity\u201d and \u201clocation", "location\" for anatomy and \u201cseverity": "or disorders, such as specifying \"left lung\u201d or \u201cmild edema", "unchanged\u201d or \u201cimproved": "ay result from hallucinations This issue arises partly because the training data often lack comprehensive, longitudinal information that accurately captures patient progression. Additionally, some model training processes do not take into account the patient history or the continuity of patient data across multiple studies.\nTo gain a more detailed understanding, we selected several high-frequency disorders and the commonly used concepts to modify these disorders. One example is shown in Figure 4, the Intra-Dataset Reports's results exhibit complete coverage. In contrast, models tend to use concepts like \"moderate\" and", "mild": "ut do not use terms", "severe": "r \"subtle\" for \"opacity\". We provide comprehensive detailed results in the appendix, from which we can observe that for some disorders, such as \"consolidation\", most models do not provide severity descriptions. We also provide a barplot in the appendix showing the frequency of those concepts in MIMIC-CXR training set, an interesting observation is that the model's predictions are not linearly related to the frequency of appearance in the MIMIC-CXR training set. Instead, the model tends to overfit a specific synonym within a set of related concepts, and the selected concept varies for different disorders."}, {"title": "Q4: Quantified Measurement", "content": "We then address the issue of quantification in the reports: How frequently does the model provide quantified measurements of disorders and anatomical regions?\nThis information is crucial for the deep analysis of images. For instance, disorders that consistently include size descriptions like \"3mm\" in the report might require the development of precise segmentation targets. On the other hand, some disorders that cannot be measured may only need bounding boxes during labeling. Based on the knowledge graph, Al research can easily identify which disorders can and should be segmented, thereby further promoting research on grounded report generation.\nAs shown in Figure 5, we provide an overview of whether the models give detailed measurement descriptions for the target disorders. Both CheXpert Part I and CheXpert Part II consistently provide detailed descriptions for all target disorders, which highlights the real clinical requirements. However, most Al models show limited coverage, often failing to provide detailed descriptions for many conditions like calcification and effusion. Relatively speaking, generalist models like RadFM and MedVersa cover a broader range of disorders. It is notable that CheXagent does not predict any size measurements for disorders but consistently provides size descriptions for devices such as tubes and lines. We also provide the frequency of size descriptions for specific disorders in MIMIC-CXR training data in the appendix, as shown, the model's behavior in providing size descriptions correlates strongly with the frequency."}, {"title": "Q5: Specialist vs. Generalist Models", "content": "Finally, we compare different types of AI models by asking: What are the differences in performance between specialist models and generalist models?\nWe summarize the score of different metrics on different models' predictions on the training set of CheXpert Plus finding sections. Note that none of the models were trained using CheXpert Plus. First, we observe that there is not a significant gap between the report-vs-report performance scores of specialist models and generalist models. This suggests that specialist models can perform well on specialist tasks. However, when comparing the models' knowledge coverage with that of radiologists, generalist models like RadFM and MedVersa show significantly broader node coverage. Note that here, all generalist models are trained on various tasks such as diagnosis, VQA, and report generation, but CheXagent only focuses on chest X-rays, while other generalist models include datasets from various modalities. From this, we can conclude that including data from various modalities improves the models' prediction generalizability, especially in terms of entity coverage. To develop medical AI systems that can interpret medical data and reason through complex problems at an expert radiologist level in real clinical scenarios, it is important to combine data from different modalities to broaden the models' knowledge base."}, {"title": "Ablation Studies", "content": "We conduct ablation studies to examine the impact of different medical embedding models, similarity thresholds, and the number of reports on our proposed metrics. The results are presented in Table 3. First, we compare the performance of two medical embedding models, BioLoRD (Remy, Demuynck, and Demeester 2024) and MedCPT (Jin et al. 2023), at different similarity thresholds. Our findings indicate that the choice of embedding model and threshold has a minimal effect on the extracted knowledge graph's quality. Both models perform robustly across different thresholds, with only slight variations in the KG-AMS metric. We also investigate how the number of reports influences the quality of the resulting knowledge graph. As expected, the number of reports significantly affects the results. However, we observe that as the number of reports increases, the performance asymptotically approaches that of the full dataset. For instance, with 10,000 studies, we achieve a KG-NSC of 0.977 and a KG-AMS of 0.987, which closely matches the performance of the full dataset."}, {"title": "Related Work", "content": "Previous evaluations of radiology report generation models relied mainly on specific report-to-report metrics like FineRadScore (Huang et al. 2024), RaTEScore (Zhao et al. 2024), RadFact (Bannur et al. 2024), CheXPrompt (Chaves et al. 2024), and GREEN (Ostmeier et al. 2024). These metrics, however, do not fully capture an in-depth understanding of the capabilities of current models. Our work aims to address this limitation by leveraging knowledge graphs constructed from the report corpus. The standard pipeline for knowledge graph construction typically involves Named Entity Recognition (Li et al. 2020), Relation Extraction (Pawar, Palshikar, and Bhattacharyya 2017), and Entity Resolution (Christophides et al. 2020). In the medical domain, the focus has primarily been on developing knowledge graphs based on complex medical systems such as electronic health records, medical literature, and clinical guidelines (Rotmensch et al. 2017; Finlayson, LePendu, and Shah 2014; Bean et al. 2017). However, in the specific context of radiology reports, most progress focuses on information extraction (Irvin et al. 2019; McDermott et al. 2020; Peng et al. 2018; Smit et al. 2020; Jain et al. 2021b,a; Khanna et al. 2023; Delbrouck et al. 2024), and have not yet led to the establishment of a comprehensive knowledge graph specifically tailored for radiology reports. Few existing studies (Kale et al. 2022; Zhang et al. 2020) related to knowledge graph construction heavily relied on manual annotation by radiologists, highlighting the need for more automated, scalable approaches in this field."}, {"title": "Conclusion", "content": "In this paper, we present ReXKG, a novel system for constructing comprehensive radiology knowledge graphs from medical reports, and introduce three metrics for evaluationg the similarity of nodes, distributions of edges, and coverage of subgraphs. We conduct an in-depth analysis comparing AI-generated radiology reports to human-written reports. Our research reveals that generalist models trained on various modalities offer broader coverage and enhanced radiology knowledge, yet they still fall short of the depth found in radiologist-written reports, particularly in the description and size measurements of disorders. Additionally, hallucinations related to prior studies are noticeable in model-generated reports, highlighting the need to incorporate longitudinal data in future model development."}, {"title": "Prompt for Entity Extraction", "content": "You are a radiologist performing clinical term extraction from the FINDINGS and IMPRESSION sections in the radiology report. Here a clinical term can be in [anatomy, disorder-present, disorder_notpresent,  procedure, device-present, device_notpresent, size, concept]. anatomy refers to the anatomical body. disorder-present refers to findings or diseases that are present according to the sentence. disorder_notpresent refers to findings or diseases that are not present according to the sentence. procedure refers to procedures used to diagnose, measure, monitor, or treat problems. device-present refers to any instrument, apparatus for medical purpose that are present according to the sentence. device_notpresent refers to any instrument, apparatus for medical purpose that are not present according to the sentence. size refers to the measurement of disorders or anatomy, for example, 3mm, 4x5 cm. concept refers to descriptors such as acute or chronic, large, size or severity, or other modifiers, or descriptors of anatomy being normal. For example, right pleural effusion, right should be a concept, and pleural should be anatomy and effusion should be disorder-present or disorder-notpresent. For example, normal cardiomediastinal silhouette. normal and silhouette should be concept, cardiomediastinal should be anatomy. Please extract terms one word at a time whenever possible, avoiding phrases. Note that terms like no and no evidence of are not considered entities. Given a list of radiology sentence input in the format: <Input><sentence><sentence></Input>. Please reply with the JSON format following template: {<sentence>{entity:entity type, entity:entity type}, <sentence> {entity:entity type, entity:entity type}}."}, {"title": "Prompt for Relation Extraction", "content": "You are a radiologist performing relation extraction of entities from the FINDINGS and IMPRESSION sections in the radiology report. Here a clinical term can be in [anatomy, disorder_present, disorder_notpresent, procedures, procedures, concept, devices_present, devices_notpresent). And the relation can be in [modify, located_at, suggestive_of]. suggestive_of means the source entity (findings) may suggest the target entity (disease). located_at means the source entity is located at the target entity. modify denotes the source entity modifies the target entity. Every time there is a modify relationship between concept and anatomy, the direction should be concept \u2192 anatomy. For example, paranasal sinuses are clear: source entity clear (concept), modify target entity paranasal sinuses (anatomy). For example, acute hemorrhage: source entity acute (concept), modify target entity hemorrhage. Given a piece of radiology text input in the JSON format: {sentence:{entity:entity_type}, sentence:{entity:entity_type}}. Please reply with the following JSON format: {sentence:[{source entity:target entity, relation: relation}, {source entity:target entity, relation:relation}}."}, {"title": "Algorithm for Node Construction", "content": "Algorithm 1: Node Integration\nRequire: E: list of entities\nRequire: C: count threshold\nRequire: n: maximum number of words in an entity\n1: Initialize A \u2190 (\u00d8) {Set of initial nodes}\n2: Group E by word count and filter by C\n3: for each k from 1 to n do\n4:   for each e \u2208 E with k words do\n5:     if k == 1 then\n6:       Add e to set A\n7:     else\n8:       if e can merge from nodes in A then\n9:         Pass\n10:      else\n11:       Add e to set A\n12:      end if\n13:    end if\n14:  end for\n15: end for\n16: return A {Set of nodes}"}, {"title": "KG Subgraph Coverage Score", "content": "Let S = {S_1", "weights": "nI(S_i) = \\sum_{v \\in V(S_i)"}, "w_v + \\sum_{e \\in E(S_i)} w_e, (4)\nwhere V(S_i) and E(S_i) denote the vertex and edge sets of S_i respectively, and w_v and w_e are the corresponding node and edge weights. For each subgraph S_i in KG-GT, we compute a presence score P(S_i) in KG-Pred:\nP(S_i) = \\frac{1}{2} (\\frac{|E(\\hat{S})|}{|E(S_i)|} + \\frac{\\sum_{v \\in V(\\hat{S})} s_v}{|V(S_i)|}), (5)\nwhere $\\hat{S}$ is the corresponding subgraph in KG-Pred, |E(.)| and |V(.)| denote the number of edges and vertices respectively, and s_v is the similarity score between matched nodes as defined in the KG-NSC section. The Subgraph Coverage Score is then calculated as:\nKG-SCS = \\frac{\\sum_{i=1}^{K} I(S_i) \\cdot P(S_i)}{\\sum_{i=1}^{K} I(S_i)} (6)\nwhere K is the number of top important subgraphs considered, I(S) is the normalized importance score of subgraph S_i among the selected K subgraphs"]}