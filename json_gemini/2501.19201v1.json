{"title": "Efficient Reasoning with Hidden Thinking", "authors": ["Xuan Shen", "Yizhou Wang", "Xiangxi Shi", "Yanzhi Wang", "Pu Zhao", "Jiuxiang Gu"], "abstract": "Chain-of-Thought (CoT) reasoning has become a powerful framework for improving complex problem-solving capabilities in Multimodal Large Language Models (MLLMs). However, the verbose nature of textual reasoning introduces significant inefficiencies. In this work, we propose Heima (as hidden llama), an efficient reasoning framework that leverages reasoning CoTs at hidden latent space. We design the Heima Encoder to condense each intermediate CoT into a compact, higher-level hidden representation using a single thinking token, effectively minimizing verbosity and reducing the overall number of tokens required during the reasoning process. Meanwhile, we design corresponding Heima Decoder with traditional Large Language Models (LLMs) to adaptively interpret the hidden representations into variable-length textual sequence, reconstructing reasoning processes that closely resemble the original CoTs. Experimental results across diverse reasoning MLLM benchmarks demonstrate that Heima model achieves higher generation efficiency while maintaining or even better zero-shot task accuracy. Moreover, the effective reconstruction of multimodal reasoning processes with Heima Decoder validates both the robustness and interpretability of our approach.", "sections": [{"title": "1. Introduction", "content": "The recent rise in popularity of Multimodal Large Language Models (MLLMs) (Dubey et al., 2024; Achiam et al., 2023; Bai et al., 2023; Liu et al., 2024b; Lai et al., 2024; Xu et al., 2024), which integrate vision techniques with traditional Large Language Models (LLMs), has spurred interest in leveraging Chain-of-Thought (CoT) (Wei et al., 2022) reasoning to enhance their capabilities for solving complex problems. The CoT enables the MLLMs to generate intermediate steps that mirror human-like problem-solving processes, breaking down complex tasks into smaller, sequentially manageable components before arriving at the final solution. This approach not only enhances interpretability but also enables more effective multi-step reasoning, equipping MLLMs to address tasks that demand intricate logical understanding and contextual coherence, especially when processing the inherent complexity of visual information.\nHowever, CoT reasoning often requires generating a substantial amount of additional text during the reasoning process, particularly for complex problems. This increased verbosity significantly impacts the efficiency of problem-solving, especially in large models with massive parameters and expensive inference costs. Thus, it becomes crucial to reduce the number of tokens generated during CoT reasoning to enhance the efficiency of MLLMs without compromising their reasoning capabilities.\nRecent works (Hao et al., 2024; Deng et al., 2024b) have investigated compressing CoTs for GPT-2 (Radford & Wu, 2019), a relatively small language model compared to modern LLMs, which often consist of billions of parameters. The approach in (Hao et al., 2024) relies on task-specific fine-tuning for individual reasoning tasks, resulting in limited generalization. Moreover, these works focus solely on language-based tasks, leaving a substantial gap in compressing CoTs for large-scale multimodal models capable of handling diverse and intricate reasoning challenges.\nBesides CoTs for LLMs, several works (Lai et al., 2024; Pi et al., 2023; Yan et al., 2024; Deng et al., 2024a) focus on enhancing reasoning in MLLMs using traditional visual decoders for segmentation, detection, and recognition, where the decoders are utilized to decode the input tokens generated by MLLMs. This demonstrates the effectiveness of MLLMs as encoders, motivating us for the further exploration of their role and mechanisms as encoders in the reasoning research area.\nIn this work, we propose Heima, the first efficient reasoning framework for MLLMs by leveraging reasoning CoTs in the hidden latent space, avoiding the reliance on verbose textual representations. We introduce the Heima Encoding method, which enables reasoning within the hidden space by encoding the CoTs into compact hidden representations."}, {"title": "2. Related Work", "content": null}, {"title": "2.1. Chain-of-Thought Reasoning", "content": "With the theoretically validated effectiveness in recent works (Merrill & Sabharwal, 2023; Feng et al., 2024), CoTs have gained increasing popularity and are widely adopted as an enhancement method for generating intermediate reasoning processes before arriving at the final answer. The works (Wei et al., 2022; Khot et al., 2022; Zhou et al., 2022) focus on design the effective prompts which decompose the question into a group of reasoning steps for LLMs. The works (Yue et al., 2023; Yu et al., 2023; Wang et al., 2023;"}, {"title": "2.2. Textual Efficient Reasoning", "content": "Recent works (Ning et al., 2023; Kou et al., 2024; Zhang et al., 2023; Li et al., 2024) aim to accelerate the reasoning process by employing parallel generation through templates or Jacobi decoding, which introduces additional overhead during model inference. Meanwhile, the works (Jiang et al., 2024; Ge et al., 2024; Chevalier et al., 2023; Qin et al., 2023; Liu et al., 2023; Munkhdalai et al., 2024) adopt contextual compression methods to achieve the efficient generation of the next following contexts based on the previous contexts. On the other hand, the work (Cheng & Van Durme, 2024) compresses the CoT into a short sequence of continuous embeddings for the acceleration of reasoning process. However, results on the math dataset reveal a significant degradation in accuracy, indicating the limitations and ineffectiveness of this approach. Thus, there is still a gap in developing efficient reasoning techniques for large models that maintain the performance advantages of reasoning with CoTs, which motivates us to explore better compression methods for CoTs to achieve higher accuracy and efficiency."}, {"title": "2.3. Reasoning in Latent Space", "content": "The works (Hao et al., 2024; Yang et al., 2024; Biran et al., 2024; Cheng & Van Durme, 2024) adopt latent reasoning for LLMs. For example, the work (Hao et al., 2024) addresses the compression of small models (GPT-2) on math datasets with CoTs. However, its effectiveness remains unverified as the evaluation of reasoning performance is limited to math datasets. Additionally, the small model size raises concerns about potential overfitting to the data rather than general reasoning based on the inputs.\nSome works (Liu et al., 2024b; Lai et al., 2024) include visual information into latent space for MLLMs to enhance the textual reasoning. The work (Lai et al., 2024) employs fine-tuning for both LLMs and segmentation decoders, demonstrating the potential to decode visual information in tokens generated by LLMs. Specifically, this approach leverages segment tokens generated by MLLMs as part of the input for the segmentation decoder, emphasizing their utility in enhancing the decoding process. Nevertheless, subsequent works (Pi et al., 2023; Deng et al., 2024a; Yan et al., 2024) continue to investigate using MLLMs to generate tokens for visual downstream tasks, rather than exploring the construction of internal feature representations within MLLMs for higher-level feature embedding. The absence of feature construction in MLLMs motivates us to explore the development of latent representations for these models."}, {"title": "3. Methodology", "content": "In this section, we first present the CoT encoding process, which encodes CoTs into compact hidden representations. Then, we detail the adaptive decoding process, which reconstructs or interprets the hidden representations into CoT-like trajectories. Finally, we explain for the efficient reasoning with the proposed hidden thinking."}, {"title": "3.1. Design of Heima Encoder", "content": "Heima Encoder encodes verbose textual CoTs into compact hidden representations with very few tokens, thus accelerating the CoT Reasoning with higher efficiency.\nCoT Encoding. The original CoT training dataset can be defined as\n{(X_v^{(i)}, X_q^{(i)}, \\{CoT_k^{(i)}\\}_{k=1}^{K_i}, Y^{(i)})\\}_{i=1}^N\t(1)\nwhere $X_v^{(i)}$ denotes the $i$th visual input, $X_q^{(i)}$ denotes the $i$th query input, $\\{CoT_k^{(i)}\\}_{k=1}^{K_i}$ denotes that there are $K_i$ stages for the textual CoT sentence with $CoT_k^{(i)}$ as the $k$th CoT stage in the $i$th data sample, and $Y^{(i)}$ denotes the final answer. For example, as shown in Figure 1, the CoT has Summary, Caption, and Reasoning stages.\nWe first adopt the pretrained MLLM as the initialization of Heima Encoder $f_\\theta(\\cdot)$, where $\\theta$ denotes its parameters, and fine-tune it to learn to generate the answer $Y_a^{(i)}$ with $\\{CoT_k^{(i)}\\}_{k=1}^{K_i}$, conditioned on both image $X_v^{(i)}$ and textual query $X_q^{(i)}$ as follows,\n$\\max_\\theta \\frac{1}{N} \\sum_{i=1}^N  log P_\\theta(\\{CoT_k^{(i)}\\}_{k=1}^{K_i}, Y_a^{(i)} | X_v^{(i)}, X_q^{(i)})$.\t(2)\nAfter training on explicit CoTs, we then update the training set by replacing each $CoT_k^{(i)}$ with a single thinking token-denoted as $<CoT>^{(k)}$. For each thinking token $<CoT>^{(k)}$ at the $k$th stage, it is defined with a unique special token and added to the vocabulary to enable explicit textual visualization of the reasoning process. For example, in Figure 1, we define a token $<Thinking\\_of\\_Summary>$ as the thinking token for the summary stage. Similarly, we define $<Thinking\\_of\\_Caption>$ for the caption stage and"}, {"title": "Progressive Encoding.", "content": "To facilitate a seamless transition from textual reasoning to hidden thinking while maintaining model performance, we adopt a progressive encoding strategy as shown in Figure 2. Specifically, there are multiple CoT stages and we do not encode all of the CoT stages into hidden representations at the start of the training. Instead, during training, we gradually increase the number of CoT stages with hidden representations, from 0 to $\\max\\{K_i\\}_{i=1}^{N}$. Specifically, starting from 0 stages without hidden representations, we train the model using textual $\\{CoT_k^{(i)}\\}_{k=1}^{K_i}$ without any thinking tokens following Equation (2) for a few training steps. Then we encode one CoT stage into a single thinking token and train for a few steps. Next, two CoT stages (including the previous trained CoT stage) are trained with their corresponding thinking tokens. Subsequently, we continue to add more CoT stages into the thinking token training for Heima Encoder $\\theta$ and finally all CoT stages are trained with thinking tokens for hidden representations.\nFormally, our progressive encoding has $\\max\\{K_i\\}_{i=1}^{N}+1$ stages. In the $s$th stage ($s \\in N, 0 < s < \\max\\{K_i\\}_{i=1}^{N}-1$), we prepare the training data as\n{\\left(X_v^{(i)}, X_q^{(i)}, \\{<CoT>^{(k)}\\}_{k=1}^{s}, \\{CoT_k^{(i)}\\}_{k=s+1}^{K_i}, Y_a^{(i)}\\right)\\}_{i=1}^N\t(5)\nFor $s = 0$, the model is fine-tuned following Equation (2). In the $s$th stage ($s > 0$), we finetune Heima Encoder $f_\\theta(\\cdot)$,\n$\\max_\\theta \\frac{1}{N} \\sum_{i=1}^N  log P_\\theta(\\left\\{<CoT>^{(k)}\\right\\}_{k=1}^{s}, \\{CoT_k^{(i)}\\}_{k=s+1}^{K_i}, Y_a^{(i)} | X_v^{(i)}, X_q^{(i)})$.\t(6)\nIn the above expression, during the $s$th stage encoding, the first $s$ CoT stages are trained with hidden thinking tokens while the rest CoT stages are trained with textual reasoning tokens. As the value of $s$ increases, more CoT stages are encoded into hidden representations. It is illustrated in Figure 2 with each row representing the training data of one stage in the training.\nThis approach allows the model to gradually internalize the reasoning processes represented by multiple CoTs and integrate them into its hidden representations. After the final progressive stage, we further optimize the hidden representations of the thinking tokens by performing one additional recovering stage, which continue to train the model following Equation (6) with $s = \\max\\{K_i\\}_{i=1}^{N}$ (i.e., Equation (4)) for a few more training steps. This extra recovering stage optimizes the transitions and interactions between the hidden representations across different stages, ensuring a cohesive alignment of information learned throughout the encoding process. Additionally, it consolidates the overall learning process, enhancing the model's ability to effectively utilize the encoded reasoning patterns for improved performance and robustness in downstream reasoning tasks."}, {"title": "3.2. Design of Heima Decoder", "content": "After each CoT stage has been encoded into a single thinking token and stored in the hidden representation, it is necessary to recover the textual reasoning process for interpretability and analysis, enabling better utilization in practice. Meanwhile, it is crucial to verify the effectiveness of the hidden representations encapsulated within thinking tokens to ensure that the model is genuinely learning hidden reasoning processes rather than merely fitting the data. Thus, to interpret the thinking tokens, we design the adaptive Heima Decoder, which exploits the standard next-token prediction paradigm in LLMs for the reconstruction of variable-length (i.e., adaptive) textual sequence based on thinking tokens.\nAdaptive Decoding. After Heima Encoder, a total of $\\max\\{K_i\\}_{i=1}^{N}$ CoT stages are encoded into hidden representations, each with a unique thinking token. Each encoded CoT stage or each kind of thinking token requires one corresponding decoder or interpreter. Thus, to decode multiple thinking tokens, we need to train multiple decoders separately, following the same procedure as below.\nWe adopt a pretrained LLM as the initialization of Heima Decoder $f_{\\eta_s}(\\cdot)$ for the $s$th stage encoded with the $s$th thinking token ($s \\in N, 1 < s < \\max\\{K_i\\}_{i=1}^{N}$), where $\\eta_s$ denotes its parameters. We do not consider the case of $s = 0$ as it does not include thinking tokens. The training dataset for Heima Decoder of the $s$th stage is designed as follows,\n{\\left(X_e, X_q^{(i)}, <CoT>^{(s)}, H_{<CoT>}^{(s)}, CoT_s^{(i)}, CoT^{(i)}\\right)\\}_{i=1}^N\t(7)\nwhere $X_e$ denotes the explanatory prompts to guide the model for the interpretation of the thinking tokens. $X_q^{(i)}$ denotes the $i$th query. $<CoT>^{(s)}$ denotes the $s$th thinking token, and $H_{<CoT>}^{(s)}$ denotes the hidden representation (last hidden states) of the thinking token $<CoT>^{(s)}$ encoded by Heima Encoder with $\\{X_v^{(i)}, X_q^{(i)}, \\{<CoT>^{(k)}\\rangle\\}_{k=1}^{K_i}$, and $Y_a^{(i)}\\}$. $CoT_s^{(i)}$ denotes the $s$th textual CoT in the $i$th sample. Note that here we only use LLMs as the decoders without the capability to read images, and the dataset for training decoders only has text queries without visual images inputs.\nDuring the fine-tuning of Heima Decoder, the frozen Heima Encoder is used to generate the thinking tokens as inputs for decoders. Specifically, for the thinking token inputs, we use the last hidden states of the thinking tokens from Heima Encoder, i.e., $H_{<CoT>}^{(s)}$, to replace the position previously occupied by the thinking token symbol $<CoT>^{(s)}$ after the word embedding in Heima Decoder, as illustrated in Figure 3. The next-token prediction loss is remained and Heima"}, {"title": "Explanatory Prompts.", "content": "A single hidden representation $H_{<CoT>}^{(s)}$ alone is insufficient to guide Heima Decoder $f_{\\eta_s}(\\cdot)$ toward reconstructing the original reasoning stages, as language models generally rely on textual instructions to scaffold the generation process. Thus, we provide the explanatory prompt for Heima Decoder to enhance usability and interpretability. We use the following prompt,\n\"According to the question: [$X_q^{(i)}$], can you explain the thinking progress of $<CoT>^{(s)}$?\"\nThese explanatory prompts ensure that the output reasoning process remains (i) aligned with the original query [$X_q^{(i)}$] and (ii) consistent with the hidden reasoning encoded by Heima Encoder. More formally, the prompt provides extra text $X_e$ that conditions the decoding distribution as follows,\nP_{\\eta_s}(CoT_s^{(i)} | X_e, X_q^{(i)}, H_{<CoT>}^{(s)})\n=\n\\prod_{t=1}^{T_s^{(i)}} P_{\\eta_s}(w_t^{(s,i)} | w_{<t}^{(s,i)}, X_q^{(i)}, X_e, H_{<CoT>}^{(s)}),\t(9)\nwhere $t \\in N, 1 \\le t \\le T_s^{(i)}$, $T_s^{(i)}$ denotes the token length of $CoT_s^{(i)}$, and $w_t^{(s,i)}$ denote the $t$-th token of $CoT_s^{(i)}$. This textual scaffolding guides the LLM based Heima Decoder to leverage the explicit question context $X_q^{(i)}$, to interpret and explain the hidden representations, encapsulated in the last hidden states $H_{<CoT>}^{(s)}$, with textual reasoning processes during hidden thinking."}, {"title": "3.3. Efficient Reasoning", "content": "To achieve the acceleration during the reasoning process with our framework, we first deploy Heima Encoder only as the reasoning model, ensuring lower memory requirements and faster generation. Heima Encoder generates the response corresponding to the input question and image as follows,\nP_\\theta(\\{\\langle CoT \\rangle^{(k)}\\}_{k=1}^{K_i}, Y_a^{(i)} | X_v^{(i)}, X_q^{(i)})\n=\n\\prod_{t=1}^{T^{(i)}+K_i} P_\\theta(w_t | X_v^{(i)}, X_q^{(i)}, w_{<t}),\t(10)"}, {"title": "4. Experimental Results", "content": null}, {"title": "4.1. Experiment Setup", "content": "Dataset. We utilize the LLAVA-COT-100k dataset, a reasoning dataset for MLLMs that integrates samples from several widely used VQA datasets. It comprises 100k image-QA pairs with three stages of CoT reasoning: summary, caption, and reasoning.\nModel Training. We adopt the LLaVA-CoT (Xu et al., 2024) pretrained model based on Llama-3.2-11B-Vision-Instruct (Meta, 2024b) as our initialization of Heima Encoder. Meanwhile, Llama-3.1-8B-Instruct (Meta, 2024a) is employed as the initialization of Heima Decoder. We use torchtune (Meta, 2024c) as the model training framework with LORA (Hu et al., 2021) for both Heima Encoder and Heima Decoder. During the progressive encoding process, we freeze the image encoder component and fine-tune both the decoder and fusion components of the LLaVA-COT model. This fine-tuning includes the entire attention and MLP modules across all layers, as well as the output projection layer, using a rank of 16, and an alpha of 32. As for training Heima Decoder, we apply the same lora setting to the model. Detailed hyperparameters are included in Appendix A. The training is conducted on 8\u00d7 H100 GPUs."}, {"title": "Evaluation.", "content": "We adopt several challenging zero-shot benchmarks to verify the effectiveness of our proposed method, including MMStar (Chen et al., 2024), MMBench V1.1 (Liu et al., 2025), MMVet (Yu et al., 2024), MathVista (Lu et al., 2024), AI2D (Hiippala et al., 2021), and HallusionBench (Guan et al., 2024). MMStar, MMBench, and MMVet evaluate general visual question-answering capabilities, while MathVista and AI2D assess mathematical and scientific reasoning. HallusionBench, in contrast, targets language hallucinations and visual illusions. We use the VLMEvalKit (Duan et al., 2024) as the evaluation pipeline to ensure a fair comparison. We use GPT-4o (Achiam et al., 2023) for evaluation on the MMVet and MathVista datasets, while exact match evaluation is applied to other datasets using VLMEvalKit. For Heima Decoder, we split the LLAVA-CoT-100k dataset for train and test separately. We evaluate the fine-tuned Heima Decoder on test set which contain 4300 samples with metrics including BLEU-4 (Papineni et al., 2002), METEOR (Banerjee & Lavie, 2005), ROUGE (Lin, 2004), and BERTScore (Zhang et al., 2019). Additionally, we adopt GPT-4o for the similarity analysis."}, {"title": "4.2. Main Results", "content": "We first provide the main results for Heima Encoder in Table 1. We compare our method to the original Llama3.1-11B-Vision-Instruct model and the LLaVA-COT on 6 datasets for the evaluation of zero-shot performance. Heima outperforms the Llama3.1-11B-Vision-Instruct model in both accuracy and performance while utilizing significantly fewer tokens, particularly on benchmarks such as MMBench, AI2D, and Hallusion. Compared to the baseline model LLaVA-CoT, Heima retains most of the model's performance while requiring as little as 6% of the tokens on certain datasets. Notably, on MMBench, Heima achieves better accuracy than the baseline LLaVA-CoT. Furthermore, to evaluate the effectiveness of progressive encoding, we include accuracy results using one-shot encoding to encode all CoT stages through the whole training (i.e., non-progressive"}, {"title": "4.3. Interpretability Analysis", "content": "To verify the effectiveness of hidden representation encoding and improve interpretability of the framework, we evaluate the performance of Heima Decoder by assessing the similarity between the reconstructed reasoning process and the ground-truth CoT. We provide the results of 4 evaluation metrics in Figure 4, and the detailed results are included in Table A2 of Appendix B. The reconstruction is most successful for the summary stage, followed by the caption stage, and then the reasoning stage. This is primarily because the summary stage relies mainly on the input question, while both the caption and reasoning stages require detailed and comprehensive visual information for accurate reasoning."}, {"title": "4.4. Ablation Study", "content": "We explore the performance with different number of thinking tokens for each CoT. We provide this ablation study on 6 datasets in Figure 6 with details in Table A3 of Appendix B. The results show that one single token for encoding the corresponding CoT achieves the best performance.\nAdditionally, we investigate adaptive encoding by regulating the retention ratio of thinking tokens, where the number of thinking tokens is determined as a percentage of the original CoT token length. We provide the results in Figure 7 with details in Table A4 of Appendix B. As observed, from 10% to 90% retention ratio, accuracy fluctuates irregularly. There is no consistent pattern emerging, highlighting the unpredictable relationship between retention ratio and accuracy. Meanwhile, as shown in Figure 8, the number of generated token is continuously increasing as the retention ratio becomes large. Notably, when the retention ratio reaches 70%, the number of generated tokens exceeds that of the baseline model (i.e., LLaVA-CoT), further indicating that adaptive encoding is not an effective method for generally capturing the features of the reasoning process."}, {"title": "5. Conclusion", "content": "In this paper, we propose Heima for efficient reasoning with hidden thinking. We fine-tune Heima Encoder by encoding each CoT into a single token, and Heima Decoder is fine-tuned by incorporating explanatory prompts to decode the hidden representations encapsulated within the thinking tokens. By effectively reconstructing the reasoning process through Heima Decoder, we demonstrate the robustness and interpretability of our method. Experimental results show that our method achieves comparable or even better accuracy on zero-shot benchmarks with significantly fewer tokens, highlighting the efficiency and reliability. In future research, we plan to extend our method to larger models for Heima Encoder and explore the use of smaller LLMs as Heima Decoder to support diverse scale designs."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "A. Training Hyperparameters", "content": "We provide the hyperparameters for the progressive encoding, additional recovering, and adaptive decoding training in Table A1."}, {"title": "B. Additional Results", "content": null}, {"title": "B.1. Detailed Evaluation of Decoders", "content": "We provide the detailed evaluation results for the metrics in Table A2."}, {"title": "B.2. Detailed Ablation Study", "content": "We provide the detailed evaluation results for the ablation study for the different number of thinking tokens and different retention ratios in Table A3 and Table A4, separately."}, {"title": "C. Prompts for GPT-4o Evaluation", "content": "We provide the GPT-4o prompts in Algorithm 1."}]}