{"title": "Learning Robust Reward Machines from Noisy Labels", "authors": ["Roko Para\u0107", "Lorenzo Nodari", "Leo Ardon", "Daniel Furelos-Blanco", "Federico Cerutti", "Alessandra Russo"], "abstract": "This paper presents PROB-IRM, an approach that learns\nrobust reward machines (RMs) for reinforcement learning\n(RL) agents from noisy execution traces. The key aspect\nof RM-driven RL is the exploitation of a finite-state ma-\nchine that decomposes the agent's task into different sub-\ntasks. PROB-IRM uses a state-of-the-art inductive logic pro-\ngramming framework robust to noisy examples to learn RMs\nfrom noisy traces using the Bayesian posterior degree of be-\nliefs, thus ensuring robustness against inconsistencies. Piv-\notal for the results is the interleaving between RM learning\nand policy learning: a new RM is learned whenever the RL\nagent generates a trace that is believed not to be accepted by\nthe current RM. To speed up the training of the RL agent,\nPROB-IRM employs a probabilistic formulation of reward\nshaping that uses the posterior Bayesian beliefs derived from\nthe traces. Our experimental analysis shows that PROB-IRM\ncan learn (potentially imperfect) RMs from noisy traces and\nexploit them to train an RL agent to solve its tasks success-\nfully. Despite the complexity of learning the RM from noisy\ntraces, agents trained with PROB-IRM perform comparably\nto agents provided with handcrafted RMs.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning (RL; Sutton and Barto 2018) is a\nmachine learning paradigm where agents learn to solve a\ntask by interacting with an environment to maximize their\ncumulative reward. Significant advancements have demon-\nstrated its potential to perform tasks as well as, if not better\nthan, humans. A famous example is AlphaGo (Silver et al.\n2016), the first AI system to beat a Go world champion. In\naddition, RL plays a key role in the training of Large Lan-\nguage Models, such as ChatGPT (OpenAI 2023).\nMany open challenges still need to be addressed to make\nRL more widely applicable to real-world problems. Among\nthese are the ability to generalise and transfer across tasks,\noperate robustly in the presence of partial observability and\nnoise, and make the learned policies interpretable (Dulac-\nArnold et al. 2021).\nReward machines (RMs; Toro Icarte et al. 2018) are a re-\ncent mechanism for addressing some of these challenges.\nRMs are finite-state machines representing non-Markovian\nreward functions in terms of high-level propositional events.\nThe RM structures the agent's task into sequences of inter-\nmediate abstract states that act as an external memory for\nthe agent. This makes the reward Markovian, thus enabling\nthe application of standard RL algorithms in non-Markovian\nreward settings. The RM structure facilitates task decompo-\nsition, making policy learning more efficient when rewards\nare sparse. Recent work has extended the applicability of\nRMs by learning them instead of handcrafting them (Toro\nIcarte et al. 2019; Xu et al. 2020; Furelos-Blanco et al. 2021;\nHasanbeig et al. 2021), and hierarchically composing them,\nfor reusability (Furelos-Blanco et al. 2023). However, RM\nlearning approaches assume a perfect labelling function, a\nconstruct that enables agents to accurately observe the high-\nlevel events occurring in the environment. This assumption\nis unrealistic; for instance, robot sensors, which resemble\nthe role of the labelling function, are seldom perfect, due\nto environmental conditions, limitations in technology, and\ninherent inaccuracies.\nWe propose PROB-IRM (Probabilisitic Induction of\nReward Machines), a method for learning and exploit-\ning RMs from noisy propositions perceived by an RL\nagent through a noisy labelling function. PROB-IRM uses\nILASP (Law, Russo, and Broda 2015), a state-of-the-art in-\nductive logic programming system capable of learning from\nnoisy examples. The learned RM is exploited by an RL al-\ngorithm that leverages the RM structure using a novel prob-\nabilistic reward-shaping mechanism based on an RM state\nbelief. The PROB-IRM algorithm interleaves the RL and\nRM learning processes, enabling the agent to immediately\nexploit the newly learned (possibly sub-optimal) RMs. A\nnew RM is learned during the interleaving process when the\ncurrently used one does not recognise noisy traces.\nWe evaluate PROB-IRM in several existing grid-world\nproblems with sparse non-Markovian rewards. We show\nPROB-IRM learns RMs from noisy traces that are exploited\nby an RL agent. Our results demonstrate that under a wide\narray of noise configurations, PROB-IRM performs simi-\nlarly to approaches where RMs are handcrafted.\nThe paper is organised as follows. Section 2 intro-\nduces the background of our work. Section 3 describes\nPROB-IRM, including the problem formalization and the\nlearning and exploitation of RMs from noisy traces. Sec-\ntion 4 presents our experimental results, Section 5 discusses\nrelated work, and Section 6 concludes the paper with sug-\ngestions for future directions."}, {"title": "2 Background", "content": "In this section, we introduce the basic notions and terminol-\nogy. Given a finite set X, \u2206(X) is the probability simplex\nover X, X* denotes (possibly empty) sequences of elements\nfrom X, X+ denotes non-empty sequences of elements from\nX, and 2x is the power set of X.\n2.1 Reinforcement Learning\nWe formalize RL tasks as labelled Markov decision pro-\ncesses (MDPs; Fu and Topcu 2014; Furelos-Blanco et\nal. 2023). A MDP is a tuple (S, A, p, r, \u03c4,\u03b3, P, L) where\nS is a set of states, A is a set of actions, p : S \u00d7 A \u2192 \u2206(S)\nis a probability transition function, r : (S \u00d7 A)+ \u00d7 S \u2192 R\nis a reward function, \u03c4 : (S \u00d7 A) \u00d7 S* \u2192 {1, T}\u00d7{1,T}\nis a termination function, \u03b3 \u2208 [0, 1) is a discount factor, P\nis a finite set of propositions representing high-level events,\nand L : S \u00d7 A \u00d7 S \u2192 2P is a (perfect) labelling function\nmapping state-action-state triplets into sets of propositions.\nWe refer to these sets as labels. The transition function p is\nMarkovian, whereas the reward function r and the termina-\ntion function 7 are not (i.\u0435., they are history-dependent).\nGiven a state-action history ht = (so, ao,...,\nSt) \u2208 (S\u00d7A)* \u00d7 S, a trace At = (L(\u00d8, \u00d8, so),...,\nL(St\u22121, at\u22121, St)) \u2208 (2P)+ assigns a label to all triplets in\nht. The goal is to find a policy \u03c0 : (2P)+ \u00d7 S \u2192 \u2206(\u0391)\nthat maps traces-states to a probability distribution over ac-\ntions that maximizes the expected cumulative discounted re-\nward (or return) Rt = \u0395\u03c0[\u2211k=tyk-tr(ht)], where n is the\nepisode's last step. Traces must be faithful representations\nof history to find such a policy, i.e., reward and termination\nfunctions could depend on traces instead of history.\nThe agent-environment interaction is as follows. At time\nt, the (label) trace is At \u2208 (2P)+ and the agent observes a\ntuple (st, st, st s\u0165, sf), where st \u2208 S is the state, s \u2208 {1, T}\nindicates whether the history is terminal, and st\u2208 {1, T}\nindicates whether the history accomplishes the task's goal.\nBoth st and st are determined by the termination function\n\u03c4. The agent also observes a label Lt = L(st\u22121, at-1, St). If\nthe history is non-terminal, the agent runs an action at \u2208 A,\nand the environment transitions to state st+1 ~ p( | st, at).\nThe agent then observes a new tuple (st+1, 81+1, 8t+1) and\nlabel Lt+1, extends the trace as At+1 = At \u2295 Lt+1, and re-\nceives reward rt+1. A trace At is a goal trace if (s\u0165, st) =\n(T, T), a dead-end trace if (sf, st) = (T, 1), and an in-\ncomplete trace if st = 1.\nExample 1. The OFFICE WORLD (Toro Icarte et al. 2018),\nillustrated in Figure 1 (left), is a 12 \u00d7 9 grid labeled with\nsome special locations. At each step, the agent & observes\nits current position in the grid and moves in one of the four\ncardinal directions; that is, S = {0,...,11} \u00d7 {0,...,8}\nand A = {up, down, left, right}. The agent always moves\nin the intended direction and stays put if it moves towards a\nwall. The set of propositions P = {\u2212, \u2709, 0, A, B, C, D, *}\nis constituted of the special locations. The labelling function\nmaps a (s, a, s') \u2208 S\u00d7A\u00d7S triplet to the set of propositions\nobserved in s', e.g. L((4,6), left, (3,6)) = {}. In this\npaper, we consider different tasks that consist of visiting a\nsequence of special locations while avoiding the decorations\n(*):\n\u2022 COFFEE: go to the coffee machine then go to the of-\nfice (o).\n\u2022 COFFEEMAIL: go to the coffee machine and the mail\nlocation (), in any order, then go to the office (o).\n\u2022 VISITABCD: go to locations A, B, C and D in order.\nA reward of 1 is obtained for completing the task; other-\nwise, the reward is 0. Rewards are non-Markovian since the\ncurrent state (i.e., position on the grid) alone cannot deter-\nmine the reward. The history h = <<4, 6), left, (3, 6), right,\n(4,6), down, (4, 5), down, (4, 4)) yields a reward of 1 and is\nmapped to the goal trace > = {{}, {}, {}, {}, {0}).\nLearning policies over histories or traces is impractical\nsince they can grow arbitrarily. In this paper, we employ\nreward machines to compactly encode traces, enabling effi-\ncient policy learning.\n2.2 Reward Machines\nA reward machine (RM; Toro Icarte et al. 2018; 2022) is\na finite-state machine representation of a reward function.\nFormally, an RM is a tuple M = (U,P,du, dr, uo, UA, UR),\nwhere U is a set of states, P is a set of propositions constitut-\ning the RM's alphabet, du: U \u00d7 2P \u2192 U is a state-transition\nfunction, \u03b4: U\u00d7U \u2192 R is a reward-transition function,\nuo \u2208 U is the initial state, ua \u2208 U is the accepting state,\nand URE U is the rejecting state.\nExample 2. Figure 1 (right) illustrates the RM for the OF-\nFICEWORLD'S COFFEE. The edges are labelled by propo-\nsitional logic formulas over P = {\u265b,\u2709, o, A, B, C, D, *}\nand rewards for transitioning between states. To verify\nwhether a formula is satisfied by a label L \u2208 2P, Lis\nused as a truth assignment: propositions contained in the\nlabel are true, and false otherwise. For example, {} = True.\nReward machines are revealed to the agent during agent-\nenvironment interactions. Starting from the RM's initial\nstate, the agent moves in the RM according to the state-\ntransition function and obtains rewards through the reward-\ntransition function. Given an RM M and a trace A =\n(Lo,..., Ln), a traversal M(X) = (\u03c5\u03bf, \u03c51,..., Un+1) is a\nunique sequence of RM states such that (i) vo = uo, and\n(ii) du(vi, Li) = Vi+1 for i = 0, . . .,n. Traversals for goal\nand dead-end traces should terminate in the accepting and\nrejecting states, respectively; in contrast, traversals for in-\ncomplete traces should terminate somewhere different from\nthe accepting and rejecting states.\nExample 3. Given the RM in Figure 1 (right) and the\ngoal trace X = {{},{},{},{},{0}), the traversal is\n(\u03ba\u03bf, \u03c5\u03bf, U1, U1, U1, ua). As expected, the traversal ends with\nthe accepting state.\nReward machines constitute compact trace representa-\ntions: each RM state encodes a different completion de-\ngree of the task. Consequently, rewards become Markovian\nwhen defined over S \u00d7 U. In line with this observation,\nToro Icarte et al. (2022) propose an algorithm that learns an"}, {"title": "2.3 Learning from Noisy Examples", "content": "Previous work shows that RMs can be learned from traces\nduring the RL agent's training (Toro Icarte et al. 2019; Xu et\nal. 2020; Furelos-Blanco et al. 2021; Hasanbeig et al. 2021).\nWe adopt a methodology similar to that by Furelos-Blanco et\nal. (2020; 2021), who used a state-of-the-art inductive logic\nprogramming system to induce RMs represented as answer\nset programs (ASP; Gelfond and Kahl 2014) that represent\nthe RMs. In what follows, we describe the fundamentals of\nthe learning system we use. We refer the reader to the work\nby Law (2018) for details.\nLearning from Answer Sets (LAS; Law, Russo, and\nBroda 2020) is a paradigm for learning answer set programs.\nA LAS task is a tuple (B, SM, E) where B is an ASP pro-\ngram called background knowledge, SM is a set of rules\nknown as the hypothesis space, and E is a set of examples.\nThe hypothesis space is specified through a set of mode dec-\nlarations M, describing which predicates can appear in the\nhead or body of a rule.\nILASP (Law, Russo, and Broda 2015) is a state-of-the-art\nsystem for solving LAS tasks robust to noisy examples (Law,\nRusso, and Broda 2018). Noisy examples in ILASP are re-\nferred to as weighted context-dependent partial interpreta-\ntions (WCDPIs), each a tuple (eid, epen, (einc, eexc), ectx),\nwhere eid is an identifier, epen is a penalty denoting the level\nof certainty of an example which can be either a positive in-\nteger or infinite, (einc, eexc) is a pair of atom sets known as\npartial interpretation, and ectx is an ASP program known\nas context that expresses example-specific information. A"}, {"title": "3 Methodology", "content": "In this section, we present PROB-IRM, an approach for in-\nterleaving the learning of RMs from noisy traces with the\nlearning of policies to solve a given task. We consider\nepisodic labelled MDP tasks where the reward is 1 if the\nagent observes a goal trace and 0 otherwise. The noisy traces\nWe later show a WCDPI construction for our method (see Ex-\nample 4)."}, {"title": "3.1 Noisy Traces", "content": "We assume the agent has a binary sensor for each proposi-\ntion l \u2208 P with a given sensitivity and specificity of detec-\ntion. We denote with l = T (resp. l = 1) the actual oc-\ncurrence (resp. absence) of l in the environment. We denote\nwith \u00ce = T (resp. \u00ce = 1) the detection (resp. non-detection)\nof l with the agent's sensors. The sensitivity of the sensor\nis the probability of the sensor detecting a proposition given\nthat it occurred; formally, P(\u00ce = T | l = T). The specificity\nof the sensor is the probability of not detecting a proposition\ngiven that it did not occur; formally, P(\u00ce = 1 | l = 1).\nAs the sensor's prediction may differ from the actual\noccurrence of a proposition, the agent's belief of the\n(non)occurrence of a proposition is a posterior probability\nconditional to its sensor's value. We define the posterior\nprobability of a proposition l using Bayes' rule:\n$P(l = T | \u00ce = y) = \\frac{P(\u00ce = y | l = T)P(l = T)}{\\sum_{x\\in{\u03c4,1}} P(\u00ce = y | l = x)P(l = x)}$\nwhere:\n\u2022 y \u2208 {T, 1} is the sensor detection outcome.\n\u2022 P(\u00ce = T | l = 1) = 1 \u2212 P(\u00ce = 1 | l = 1), and rep-\nresents the probability of an unexpected detection. Simi-\nlarly, P(\u00ce = 1 | l = T) = 1 \u2212 P(\u00ce = T | l = T).\n\u2022 P(l = T) is the prior probability of l occurring, and\nP(l = 1) = 1 \u2212 P(l = T). We refer the reader to\nSection 4 for further details on defining the prior.\nThe labelling function must report the sensors' degree of\nuncertainty on the detected propositions. We introduce the\nnotion of noisy labelling function, which defines (for a given\ntransition) the probability of every proposition conditioned\nto detecting their respective sensor readings. First, we define\nthe probability of a proposition at a given transition. Given\na transition triplet (st, at, St+1), we denote with Pt+1 \u20ac 2P\nall sensors' detections (i.e. all detected propositions) at that\ntransition. We assume that the probability of a proposition l\nat a given transition is conditional only to its related sensor\ndetection at that transition (lt+1). The value lt+1 is T if the\nproposition l \u2208 Pt+1 (detected); otherwise lt+1 is 1. So\nwe define P(l = T | St, At, St+1) = P(l = T | Pt+1) =\nP(l = T | lt+1).\nThe noisy labelling function defines the probability of\neach proposition at a given transition."}, {"title": "Definition 2 (Noisy labelling function).", "content": "Let S be the set of\nstates, A the set of actions, and P be the set of proposi-\ntions. Given a transition (st, at, St+1) \u2208 S \u00d7 A \u00d7 S, the\nnoisy labelling function L maps the transition to the set of\nall possible propositions with their respective probabilities\nat that transition. Formally, L(st, at, St+1) = {(l, P(l =\nT\u2758 St, At, St+1)) | l \u2208 P})."}, {"title": "Definition 3 (Noisy trace).", "content": "Given a state-action history\nht = (so, a0, 81, a1, ..., st), the noisy trace X\u0165 is given by:\nAt = (L(so, ao, s1), \u2211(s1, a1, 82), ..., \u2211(St\u22121, at\u22121, St)).\nOur RL task can be formalised as a noisy labelled MDP;\nthat is, a labelled MDP (see Section 2.1) but with a noisy\nlabelling function. Despite the labelling function now being\nnoisy, we assume the termination function 7 remains deter-\nministic; hence, the termination of a noisy trace is deter-\nmined with certainty throughout the agent-environment in-\nteraction."}, {"title": "3.2 Learning RMs from Noisy Traces", "content": "We learn candidate RMs using ILASP. Recall that a LAS\ntask is a tuple (B, SM, E), where B is the background\nknowledge, SM is the hypothesis space, and E is a set\nof WCDPI examples (see Section 2.3). The background\nknowledge B and the hypothesis space SM are similar to the\nones proposed by Furelos-Blanco et al. (2021). The former\nis a set of ASP rules that describes the general behaviour\nof any RM (i.e., how an RM is traversed). The latter con-\ntains all possible rules that can constitute the state-transition\nfunction of the RM.\nWe now focus on representing the WCDPI examples in E\nfrom a given set of noisy traces.\nGenerating Examples from Noisy Traces. One of the\nkey aspects of learning RMs from noisy traces is how to map\nthese traces into WCDPIs. A direct approach involves ag-\ngregating the probabilities generated by the noisy labelling\nfunction, for a given noisy trace, into a single trace-level\nprobability. This aggregated probability can then be used as\nthe weight for the WCDPI generated from that trace. Ap-\nproaches along this line have been proposed in the literature\nfor other domains (Cunnington et al. 2023). They define the\naggregation function as a t-norm over the collection of prob-\nabilistic predictions. This value is used as the penalty for\nthe examples, while the predictions are converted into their\nmost likely outcome and stored within the WCDPI context.\nSuch a solution proved too restrictive since such WCDPIs\ncould only represent the most likely trace. Instead, we adopt\na sampling-based method: the probabilities in a noisy trace\nare used to define proposition-specific Bernoulli distribu-\ntions, which are then sampled to determine the propositions\nthat would be part of the context of the associated WCDPI."}, {"title": "Example 4 (Generation of WCDPI from a noisy trace).", "content": "Let be a noisy trace. We denote with Ai =\n[L(Si-1, Ai-1, Si)] the probability of proposition l occur-\nring at step i of X. We generate a sample trace X' from X\nby determining the occurrence of each proposition l\u2208 Pat\neach step i using the corresponding Bernoulli distribution;\nthat is, X ~ Ber(Xi,1). For each sampled trace X', an\nASP trace XASP = {prop(l,i) | X'\u2081\u2081 = 1} is built by map-\nping each occurring proposition into a prop(l, i) fact indi-\ncating that proposition l occurs at time i. As proposed by\nFurelos-Blanco et al. (2021), we compress sampled traces\nby removing consecutive occurrences of the same sampled\nproposition set.\nThe WCDPI example generated from X is given by\n(eid, epen, (einc, eexc), ectx), where eid is a unique exam-\nple identifier, and the penalty is epen = 1. The par-\ntial interpretation (einc, eexc) is ({accept},{reject}) for\ngoal traces, ({reject}, {accept}) for dead-end traces, and\n(\u00d8, {accept, reject}) for incomplete traces. The atom\naccept (resp. reject) indicates that the accepting (resp. re-\njecting) state of the RM is reached; therefore, for instance,\nthe partial interpretation for goal traces indicates that the ac-\ncepting state must be reached, whereas the rejecting state\nmust not. Finally, the context ectx is given by the ASP rep-\nresentation XASP of the sample trace X'."}, {"title": "3.3 Exploitation of Reward Machines", "content": "In this section, we describe how an RM is exploited to learn\npolicies.\nReward Machine State Belief. Because of the noisy la-\nbelling function, the current RM state cannot be known:\nwe can only determine the RM state belief (Li et al. 2022;\n2024)."}, {"title": "Definition 4 (RM state belief \u00fbt).", "content": "The RM state belief \u00fbt \u2208\n\u2206(U) is a categorical probability distribution expressing the\nRL agent's belief of being in an RM state u at timestep t.\nFormally,\n$\\begin{aligned}\\Ut+1(u) = \\begin{cases}1 & \\text{if }t=0 \\text{ and } u = u_0; \\\\ \\sum_{u_t \\in U,\\ L_t \\in 2^P} P(L_t | s_t, a_t, s_{t+1})\\ \\bar{u}_t(u_t) \\cdot \\mathbb{1}[\\delta_u(u_t, L_t)=u]  & \\text{ otherwise.} \\end{cases} \\end{aligned}$ \nwhere du is the RM state-transition function, and 1 is the\nindicator function.\nProbabilistic Reward Shaping. Reward shaping aims to\nprovide additional rewards to guide the agent towards com-\npleting a task. Previous works use the potential-based re-\nward shaping (Ng, Harada, and Russell 1999), which gener-\nates intermediate rewards from the difference in values of a\npotential function \u03a6(s) over consecutive MDP states. Under\nthis formulation, reward shaping does not shrink the set of\noptimal policies.\nIn the context of RM-based RL, Camacho et al. (2019)\nand Furelos-Blanco et al. (2021) define the potential func-\ntion : U \u2192 R in terms of RM states. Formally,\nrs(u, u') = \u03b3\u03a6(\u0438') \u2013 \u0424(\u0438),\nwhere y is the MDP's discount factor.\nGiven that the agent has access to the belief vector \u016bt \u2208\nA(U), we propose the potential function on RM state beliefs\n\u03a6 : \u2206(U) \u2192 R, which is defined as the sum of every plau-\nsible RM state's potential weighted by its belief. Formally,\n$\\Phi(\\bar{u}_t) = \\sum_{u \\in U} \\bar{u}_t(u) \\Phi(u),$ \nwhere \u03a6: U \u2192 R is a potential function on RM states. The\nresulting reward-shaping function can thus be expressed as:\n$\\begin{aligned} r_s(\\bar{u}_t, \\bar{u}_{t+1}) &= \\gamma \\Phi(\\bar{u}_{t+1}) - \\Phi(\\bar{u}_{t})\\\\ & = \\sum_{u \\in U} \\gamma  \\bar{u}_{t+1}(u) \\Phi(u) -  \\bar{u}_{t}(u) \\Phi(u) \\\\ & = \\sum_{u \\in U} (\\gamma \\bar{u}_{t+1}(u) - \\bar{u}_{t}(u) )\\Phi(u). \\end{aligned}$"}, {"title": "Example 5 (Reward shaping in the OFFICEWORLD'S COF-", "content": "FEE task). From the reward machine in Figure 1, we obtain\nthe following potential function \u03a6:\n\u03a6(\u03b1\u03c1) = 4 \u2212 1 = 3,\n\u03a6(\u0438\u0434) = 4-0 = 4,\n\u03a6(41) = 4 - 1 = 3,\n\u03a6(UR) = 4-8=18."}, {"title": "3.4 Interleaved Learning Algorithm", "content": "We now describe PROB-IRM, our method for interleaving\nthe learning of RMs from noisy traces with RL. The pseu-\ndocode is shown in Algorithm 1.\nLines 1-5 initialise the candidate RM M, the set of noisy\nILASP examples E, variables tracking RM relearning, and\nthe Q-function. The algorithm is then executed for a fixed\nnumber of episodes. For each episode step, the agent exe-\ncutes an action in the environment (line 11), gets the new\nRM state belief \u0169 (line 12), and updates the noisy trace\n(line 13) and the Q-function (line 14). The episode termi-\nnates if (i) the environment signals that the task has been\ncompleted (line 11); (ii) the most likely state of the RM is\nthe accepting/rejecting state; or (iii) after a fixed number of\nsteps (lines 16-17). After the episode terminates, the ILASP\nexamples are updated (line 20). This process differs from\nthe original algorithm (Furelos-Blanco et al. 2021, see Sec-\ntion 3.2). We generate incomplete examples (line 19) from\nthe newly seen traces similarly to Ardon, Furelos-Blanco,\nand Russo (2023).\nThe belief that the RM is correct is updated using the ob-\nserved trace (lines 22-23). If the RM should be relearned\n(line 24), then ILASP is called (line 25), variables tracking\nthe relearning condition are reset (lines 26-27), and the Q-\nfunction is reinitialised (line 28).\nThe function RECOGNIZEBELIEF (lines 29-31) com-\nputes how well the current trace conforms to the candi-\ndate RM. We assume that the ground-truth outcome of an\nepisode (goal, dead-end, incomplete) is known with cer-\ntainty. This outcome is a one-hot vector obtained through\nthe TRACEOUTCOME function (line 30). On the other hand,\ngiven that we know the belief of being in an accepting\n([ut+1]acc) and rejecting state ([\u016bt+1]rej), we can compute\nthe predicted outcome: ([t+1]acc, [Ut+1]rej, 1 - [Ut+1]acc\n- [Ut+1]rej). The RECOGNIZEBELIEF returns the categor-\nical cross-entropy between the predicted and ground truth\noutcome.\nThe function SHOULDRELEARN (lines 32-35) deter-\nmines if the RM should be relearned. We use the average\ncross-entropy between the expected state belief for the ob-\nserved trace and the current RM state as decision criteria to\ntrigger the relearning of the RM: if the cross-entropy falls\nunder the hyperparameter B, a new RM is learned. Intu-\nitively, this metric can be seen as a way to evaluate how well\nthe RM captures the traces that have been seen. Also, the\nRM should not be relearned too often to prevent relearning\nwith similar examples and allow the new RM to influence\nthe generation of new examples. Hence, the RM should not\nbe relearned if a number of warm-up steps have not passed\nsince the last relearning (lines 33-34)."}, {"title": "4 Experimental Results", "content": "We evaluate PROB-IRM using the OFFICE WORLD (see\nSection 2.1). We aim to answer three research questions:\nRQ1: Does PROB-IRM successfully allow agents to be\ntrained to complete their tasks?\nRQ2: In terms of agent performance, how do the RMs\nlearned with PROB-IRM compare with hand-crafted\nones?\nRQ3: How sensitive is PROB-IRM to different noisy set-\ntings?\nAfter a brief overview of our experimental setup, we start\nby comparing the performance of PROB-IRM agents with\na baseline composed of agents provided with handcrafted\nRMs. Then, we focus our analysis on the impact of significantly higher"}, {"title": "4.1 Experimental Setup", "content": "Environment Configurations. We focus our analysis on\nthe OFFICEWORLD domain presented in Section 2.1. On\nthe one hand, the COFFEE, COFFEEMAIL, and VISITABCD\ntasks enable assessing PROB-IRM's ability to learn good-\nquality RMs and policies in progressively harder scenarios.\nOn the other hand, thanks to its adoption in other existing\nRM-based work (Toro Icarte et al. 2018; Furelos-Blanco et\nal. 2021; Dohmen et al. 2022), this choice allows our results\nto be easily compared with similar research.\nThe complexity of solving any of the three tasks strongly\ndepends on the OFFICEWORLD layout. To account for this,\nwe conducted each experiment over 3 sets of 10 random\nmaps for COFFEE and COFFEEMAIL, and 3 sets of 50 ran-\ndom maps for VISITABCD.\nSensor Configurations. We experiment with multiple\nsensor configurations, each with a specific choice of:\n\u2022 noise targets: the set of sensors subject to noise. Either\nonly the one associated with the first event needed to solve\nthe task (noise-first),2 or all of them (noise-all);\n\u2022 noise level: the sensor detection specificity and sensitiv-\nity. To reduce the number of possible configurations, we\nonly consider scenarios where both parameters are set to\nthe same value, and we will thus refer to them jointly as\nsensor confidence.\nWhile sensor confidence is the parameter we have direct\ncontrol over in our experiments, its value is not very infor-\nmative to a human reader, as its impact on the accuracy of\nan agent's sensor strongly depends on the prior probabilities\nof each label being detected. Therefore, we pick the values\nfor this parameter in such a way as to determine specific val-\nues for the posterior probability of detecting any noisy label\ncorrectly. In particular, we experiment with three posterior\nvalues, each representing increasing noise levels: 0.9, 0.8\nand 0.5. We also consider the absence of noise (the poste-\nrior equal to 1) as a baseline.\nPolicy Learning. The agent policies are learned via the\nRL algorithm for RMs outlined in Section 2.2. The Q-\nfunctions are stored as tables. To index a reasonably sized\nQ-function using an RM belief vector, we resort to binning:\nbeliefs are truncated to a fixed number of decimals, effec-\ntively resulting in close values being considered identical.\nIn terms of exploration, we rely on an e-greedy strategy.\nFor all the experiments, we start from \u20ac = 1 and decay its"}, {"title": "4.2 Baseline Results", "content": "We compare the performance of PROB-IRM against a base-\nline composed of agents provided with handcrafted R"}]}