{"title": "Learning Robust Reward Machines from Noisy Labels", "authors": ["Roko Para\u0107", "Lorenzo Nodari", "Leo Ardon", "Daniel Furelos-Blanco", "Federico Cerutti", "Alessandra Russo"], "abstract": "This paper presents PROB-IRM, an approach that learns\nrobust reward machines (RMs) for reinforcement learning\n(RL) agents from noisy execution traces. The key aspect\nof RM-driven RL is the exploitation of a finite-state ma-\nchine that decomposes the agent's task into different sub-\ntasks. PROB-IRM uses a state-of-the-art inductive logic pro-\ngramming framework robust to noisy examples to learn RMs\nfrom noisy traces using the Bayesian posterior degree of be-\nliefs, thus ensuring robustness against inconsistencies. Piv-\notal for the results is the interleaving between RM learning\nand policy learning: a new RM is learned whenever the RL\nagent generates a trace that is believed not to be accepted by\nthe current RM. To speed up the training of the RL agent,\nPROB-IRM employs a probabilistic formulation of reward\nshaping that uses the posterior Bayesian beliefs derived from\nthe traces. Our experimental analysis shows that PROB-IRM\ncan learn (potentially imperfect) RMs from noisy traces and\nexploit them to train an RL agent to solve its tasks success-\nfully. Despite the complexity of learning the RM from noisy\ntraces, agents trained with PROB-IRM perform comparably\nto agents provided with handcrafted RMs.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning (RL; Sutton and Barto 2018) is a\nmachine learning paradigm where agents learn to solve a\ntask by interacting with an environment to maximize their\ncumulative reward. Significant advancements have demon-\nstrated its potential to perform tasks as well as, if not better\nthan, humans. A famous example is AlphaGo (Silver et al.\n2016), the first AI system to beat a Go world champion. In\naddition, RL plays a key role in the training of Large Lan-\nguage Models, such as ChatGPT (OpenAI 2023).\nMany open challenges still need to be addressed to make\nRL more widely applicable to real-world problems. Among\nthese are the ability to generalise and transfer across tasks,\noperate robustly in the presence of partial observability and\nnoise, and make the learned policies interpretable (Dulac-\nArnold et al. 2021).\nReward machines (RMs; Toro Icarte et al. 2018) are a re-\ncent mechanism for addressing some of these challenges.\nRMs are finite-state machines representing non-Markovian\nreward functions in terms of high-level propositional events.\nThe RM structures the agent's task into sequences of inter-\nmediate abstract states that act as an external memory for"}, {"title": "2 Background", "content": "In this section, we introduce the basic notions and terminol-\nogy. Given a finite set X, \u2206(X) is the probability simplex\nover X, X* denotes (possibly empty) sequences of elements\nfrom X, X+ denotes non-empty sequences of elements from\nX, and 2x is the power set of X."}, {"title": "2.1 Reinforcement Learning", "content": "We formalize RL tasks as labelled Markov decision pro-\ncesses (MDPs; Fu and Topcu 2014; Furelos-Blanco et\nal. 2023). A MDP is a tuple (S, A, p, r, \u03c4,\u03b3, P, L) where\nS is a set of states, A is a set of actions, p : S \u00d7 A \u2192 A(S)\nis a probability transition function, r \u2022 : : (S \u00d7 A)+ \u00d7 S \u2192 R\nis a reward function, \u03c4 : (S \u00d7 A) \u00d7 S* \u2192 {1, T}x{1,T}\nis a termination function, \u03b3 \u2208 [0, 1) is a discount factor, P\nis a finite set of propositions representing high-level events,\nand L : S \u00d7 A \u00d7 S \u2192 2P is a (perfect) labelling function\nmapping state-action-state triplets into sets of propositions.\nWe refer to these sets as labels. The transition function p is\nMarkovian, whereas the reward function r and the termina-\ntion function 7 are not (i.\u0435., they are history-dependent).\nGiven a state-action history ht = (So, ao,...,\nSt) \u2208 (S\u00d7A)* \u00d7 S, a trace At = (L(\u00d8, \u00d8, so),...,\nL(St\u22121, At\u22121, St)) \u2208 (2P)+ assigns a label to all triplets in\nht. The goal is to find a policy \u03c0 : (2P)+ \u00d7 S \u2192 \u2206(\u0391)\nthat maps traces-states to a probability distribution over ac-\ntions that maximizes the expected cumulative discounted re-\nward (or return) Rt = \u0395\u03c0[\u2211k=tyk-tr(ht)], where n is the\nepisode's last step. Traces must be faithful representations\nof history to find such a policy, i.e., reward and termination\nfunctions could depend on traces instead of history.\nThe agent-environment interaction is as follows. At time\nt, the (label) trace is At \u2208 (2P)\u207a and the agent observes a\ntuple (st, st, st s\u0165, sf), where st \u2208 S is the state, s \u2208 {1, T}\nindicates whether the history is terminal, and st\u2208 {1, T}\nindicates whether the history accomplishes the task's goal.\nBoth st and st are determined by the termination function\nT. The agent also observes a label Lt = L(st\u22121, at-1, St). If\nthe history is non-terminal, the agent runs an action at \u2208 A,\nand the environment transitions to state st+1 ~ p( | st, at).\nThe agent then observes a new tuple (st+1, 81+1, 8t+1) and\nlabel Lt+1, extends the trace as At+1 = At \u2295 Lt+1, and re-\nceives reward rt+1. A trace At is a goal trace if (s\u0165, st) =\n(T, T), a dead-end trace if (sf, st) = (T, 1), and an in-\ncomplete trace if st = 1."}, {"title": "2.2 Reward Machines", "content": "A reward machine (RM; Toro Icarte et al. 2018; 2022) is\na finite-state machine representation of a reward function.\nFormally, an RM is a tuple M = (U,P,du, dr, uo, UA, UR),\nwhere U is a set of states, P is a set of propositions constitut-\ning the RM's alphabet, du: U \u00d7 2P \u2192 U is a state-transition\nfunction, \u03b4: U\u00d7U \u2192 R is a reward-transition function,\nuo \u2208 U is the initial state, ua \u2208 U is the accepting state,\nand URE U is the rejecting state.\nReward machines are revealed to the agent during agent-\nenvironment interactions. Starting from the RM's initial\nstate, the agent moves in the RM according to the state-\ntransition function and obtains rewards through the reward-\ntransition function. Given an RM M and a trace A\n(Lo,..., Ln), a traversal M(X) = (\u03c5\u03bf, \u03c51,..., Un+1) is a\nunique sequence of RM states such that (i) vo = uo, and\n(ii) du(vi, Li) = Vi+1 for i = 0, . . .,n. Traversals for goal\nand dead-end traces should terminate in the accepting and\nrejecting states, respectively; in contrast, traversals for in-\ncomplete traces should terminate somewhere different from\nthe accepting and rejecting states.\nReward machines constitute compact trace representa-\ntions: each RM state encodes a different completion de-\ngree of the task. Consequently, rewards become Markovian\nwhen defined over S \u00d7 U. In line with this observation,\nToro Icarte et al. (2022) propose an algorithm that learns an"}, {"title": "2.3 Learning from Noisy Examples", "content": "Previous work shows that RMs can be learned from traces\nduring the RL agent's training (Toro Icarte et al. 2019; Xu et\nal. 2020; Furelos-Blanco et al. 2021; Hasanbeig et al. 2021).\nWe adopt a methodology similar to that by Furelos-Blanco et\nal. (2020; 2021), who used a state-of-the-art inductive logic\nprogramming system to induce RMs represented as answer\nset programs (ASP; Gelfond and Kahl 2014) that represent\nthe RMs. In what follows, we describe the fundamentals of\nthe learning system we use. We refer the reader to the work\nby Law (2018) for details.\nLearning from Answer Sets (LAS; Law, Russo, and\nBroda 2020) is a paradigm for learning answer set programs.\nA LAS task is a tuple (B, SM, E) where B is an ASP pro-\ngram called background knowledge, SM is a set of rules\nknown as the hypothesis space, and E is a set of examples.\nThe hypothesis space is specified through a set of mode dec-\nlarations M, describing which predicates can appear in the\nhead or body of a rule.\nILASP (Law, Russo, and Broda 2015) is a state-of-the-art\nsystem for solving LAS tasks robust to noisy examples (Law,\nRusso, and Broda 2018). Noisy examples in ILASP are re-\nferred to as weighted context-dependent partial interpreta-\ntions (WCDPIs), each a tuple (eid, epen, (einc, eexc), ectx),\nwhere eid is an identifier, epen is a penalty denoting the level\nof certainty of an example which can be either a positive in-\nteger or infinite, (einc, eexc) is a pair of atom sets known as\npartial interpretation, and ectx is an ASP program known\nas context that expresses example-specific information. A\nhypothesis H\u2286 SM covers (or accepts) a WCDPI if there\nexists an answer set of BUectx UH that contains every\natom in einc and no atom in eexc. 1 Informally, the cost of\na hypothesis H is the sum of the hypothesis length plus the\npenalties of the examples not accepted by H. Examples with\nan infinite penalty are not noisy and must be covered by the\ninduced hypothesis. The goal of a LAS task with noisy ex-\namples is to find a hypothesis that minimises the cost over a\ngiven hypothesis space for a given set of WCDPIs. This is\nformally defined as follows.\nDefinition 1. A LAS task Tis a tuple of the form\n(B, SM, E), where B is an ASP program, SM is a hypoth-\nesis space and E is a set of WCDPIs. Given a hypothesis\nHCSM:\n1. uncov(H,T) is the set of all examples e \u2208 E such that H\ndoes not accept e.\n2. the penalty of H, denoted as pen(H,T), is the sum\nSecuncov (H,T) epen.\n3. the length of H, denoted as length(H), is the sum\n\u03a3reh length(r), where length(r) \u2208 N>0.\n4. the score of H, denoted as S(H,T), is the sum\nlength(H) + pen(H,T).\n5. H is an inductive solution of T iff S(H,T) is finite.\n6. H is an optimal inductive solution of T iff S(H,T) is\nfinite and H' SM such that S(H,T) > S(H',T).\nThe length of a hypothesis H is often defined as the\nnumber of literals that appear in H, thus considering\nlength(r) = |r|. However, this aspect can be customised\nto the specific task in hand, particularly when the task does\nnot require the shortest hypotheses to be learned."}, {"title": "3 Methodology", "content": "In this section, we present PROB-IRM, an approach for in-\nterleaving the learning of RMs from noisy traces with the\nlearning of policies to solve a given task. We consider\nepisodic labelled MDP tasks where the reward is 1 if the\nagent observes a goal trace and 0 otherwise. The noisy traces\nWe later show a WCDPI construction for our method (see Ex-\nample 4)."}, {"title": "3.1 Noisy Traces", "content": "We assume the agent has a binary sensor for each proposi-\ntion l \u2208 P with a given sensitivity and specificity of detec-\ntion. We denote with l = T (resp. l = 1) the actual oc-\ncurrence (resp. absence) of l in the environment. We denote\nwith l = T (resp. I = 1) the detection (resp. non-detection)\nof l with the agent's sensors. The sensitivity of the sensor\nis the probability of the sensor detecting a proposition given\nthat it occurred; formally, P(\u00ce = T | l = T). The specificity\nof the sensor is the probability of not detecting a proposition\ngiven that it did not occur; formally, P(\u00ce = 1 | l = 1).\nAs the sensor's prediction may differ from the actual\noccurrence of a proposition, the agent's belief of the\n(non)occurrence of a proposition is a posterior probability\nconditional to its sensor's value. We define the posterior\nprobability of a proposition l using Bayes' rule:\n$P(l = T | \u00ce = y) = \\frac{P(\u00ce = y | l = T)P(l = T)}{\\sum_{x\\in{\u03c4,1}} P(\u00ce = y | l = x)P(l = x)}$\nwhere:\n\u2022 y \u2208 {T, 1} is the sensor detection outcome.\n\u2022 P(\u00ce = T | l = 1) = 1 \u2212 P(\u00ce = 1 | l = 1), and rep-\nresents the probability of an unexpected detection. Simi-\nlarly, P(\u00ce = 1 | l = T) = 1 \u2212 P(\u00ce = T | l = T).\n\u2022 P(l = T) is the prior probability of l occurring, and\nP(l = 1) = 1 \u2212 P(l = T). We refer the reader to\nSection 4 for further details on defining the prior.\nThe labelling function must report the sensors' degree of\nuncertainty on the detected propositions. We introduce the\nnotion of noisy labelling function, which defines (for a given\ntransition) the probability of every proposition conditioned\nto detecting their respective sensor readings. First, we define\nthe probability of a proposition at a given transition. Given\na transition triplet (st, at, St+1), we denote with Pt+1 \u20ac 2P\nall sensors' detections (i.e. all detected propositions) at that\ntransition. We assume that the probability of a proposition l\nat a given transition is conditional only to its related sensor\ndetection at that transition (lt+1). The value lt+1 is T if the\nproposition l \u2208 Pt+1 (detected); otherwise lt+1 is 1. So\nwe define P(l = T | St, At, St+1) = P(l = T | Pt+1) =\nP(l = T | lt+1).\nThe noisy labelling function defines the probability of\neach proposition at a given transition."}, {"title": "Definition 2 (Noisy labelling function)", "content": "Let S be the set of\nstates, A the set of actions, and P be the set of proposi-\ntions. Given a transition (st, at, St+1) \u2208 S \u00d7 A \u00d7 S, the\nnoisy labelling function L maps the transition to the set of\nall possible propositions with their respective probabilities\nat that transition. Formally, L(st, at, St+1) = {(l, P(l =\nT\u2758 St, At, St+1)) | l \u2208 P}).\nWe can then define the probability of a set of propositions\nat a given transition in terms of the noisy labelling func-\ntion. Propositions are assumed to be conditionally indepen-\ndent. We use [L(st, at, St+1)]1 to denote the probability of\na proposition l at the transition (St, at, St+1) given by our\nnoisy labelling function. So, given a label L \u2208 2P, we have:\n$P(L|St, at, St+1) = \\prod_{l\\in P} p_l,$\nwhere\n$\\rho_l = \\begin{cases}\n1- [L(St, at, St+1)] & \\text{ if } l \\notin L;\\\\\n[L(St, at, St+1)] & \\text{ if } l \\in L.\\\\\n\\end{cases}$"}, {"title": "Definition 3 (Noisy trace)", "content": "Given a state-action history\nht = (so, a0, 81, a1, ..., st), the noisy trace X\u0165 is given by:\nAt = (L(so, ao, s1), \u2211(s1, a1, 82), ..., \u2211(St\u22121, at\u22121, St)).\nOur RL task can be formalised as a noisy labelled MDP;\nthat is, a labelled MDP (see Section 2.1) but with a noisy\nlabelling function. Despite the labelling function now being\nnoisy, we assume the termination function 7 remains deter-\nministic; hence, the termination of a noisy trace is deter-\nmined with certainty throughout the agent-environment in-\nteraction."}, {"title": "3.2 Learning RMs from Noisy Traces", "content": "We learn candidate RMs using ILASP. Recall that a LAS\ntask is a tuple (B, SM, E), where B is the background\nknowledge, SM is the hypothesis space, and E is a set\nof WCDPI examples (see Section 2.3). The background\nknowledge B and the hypothesis space SM are similar to the\nones proposed by Furelos-Blanco et al. (2021). The former\nis a set of ASP rules that describes the general behaviour\nof any RM (i.e., how an RM is traversed). The latter con-\ntains all possible rules that can constitute the state-transition\nfunction of the RM.\nWe now focus on representing the WCDPI examples in E\nfrom a given set of noisy traces.\nGenerating Examples from Noisy Traces. One of the\nkey aspects of learning RMs from noisy traces is how to map\nthese traces into WCDPIs. A direct approach involves ag-\ngregating the probabilities generated by the noisy labelling\nfunction, for a given noisy trace, into a single trace-level\nprobability. This aggregated probability can then be used as\nthe weight for the WCDPI generated from that trace. Ap-\nproaches along this line have been proposed in the literature\nfor other domains (Cunnington et al. 2023). They define the\naggregation function as a t-norm over the collection of prob-\nabilistic predictions. This value is used as the penalty for\nthe examples, while the predictions are converted into their\nmost likely outcome and stored within the WCDPI context.\nSuch a solution proved too restrictive since such WCDPIs\ncould only represent the most likely trace. Instead, we adopt\na sampling-based method: the probabilities in a noisy trace\nare used to define proposition-specific Bernoulli distribu-\ntions, which are then sampled to determine the propositions\nthat would be part of the context of the associated WCDPI."}, {"title": "3.3 Exploitation of Reward Machines", "content": "In this section, we describe how an RM is exploited to learn\npolicies.\nReward Machine State Belief. Because of the noisy la-\nbelling function, the current RM state cannot be known:\nwe can only determine the RM state belief (Li et al. 2022;\n2024).\nDefinition 4 (RM state belief ut). The RM state belief \u00fbt \u2208\n\u25b3(U) is a categorical probability distribution expressing the\nRL agent's belief of being in an RM state u at timestep t.\nFormally,\n$\\hat{u}_0(u) = \\begin{cases}\n1 & \\text{ if } u = u_0;\\\\\n0 & \\text{ otherwise,}\\\\\n\\end{cases}$\n$\\hat{u}_{t+1}(u) = \\sum_{u_t \\in U,} \\sum_{L_t \\in 2^P} P(L_t|St, at, St+1)\\hat{u}_t(U_t)1[\\delta_u(u_t, L_t) = u],$\nwhere du is the RM state-transition function, and 1 is the\nindicator function.\nProbabilistic Reward Shaping. Reward shaping aims to\nprovide additional rewards to guide the agent towards com-\npleting a task. Previous works use the potential-based re-\nward shaping (Ng, Harada, and Russell 1999), which gener-\nates intermediate rewards from the difference in values of a\npotential function \u03a6(s) over consecutive MDP states. Under\nthis formulation, reward shaping does not shrink the set of\noptimal policies.\nIn the context of RM-based RL, Camacho et al. (2019)\nand Furelos-Blanco et al. (2021) define the potential func-\ntion : U \u2192 R in terms of RM states. Formally,\nrs(u, u') = \u03b3\u03a6(\u0438') \u2013 \u0424(\u0438),\nwhere y is the MDP's discount factor.\nGiven that the agent has access to the belief vector \u016bt \u2208\nA(U), we propose the potential function on RM state beliefs\n\u03a6 : \u2206(U) \u2192 R, which is defined as the sum of every plau-\nsible RM state's potential weighted by its belief. Formally,\n$\\Phi(\\hat{u}_t) = \\sum_{u\\in U} \\hat{u}_t(u)\\Phi(u),$\nwhere \u03a6: U \u2192 R is a potential function on RM states. The\nresulting reward-shaping function can thus be expressed as:\n$r_s(\\hat{u}_t, \\hat{u}_{t+1}) = \\gamma\\Phi(\\hat{u}_{t+1}) \u2013 \\Phi(\\hat{u}_{t})\\\\\n= \\sum_{u\\in U} \\gamma\\hat{u}_{t+1}(u)\\Phi(u) \u2013 \\hat{u}_t(u) \\Phi(u)\\\\\n= \\sum_{u\\in U}(\\gamma\\hat{u}_{t+1}(u) \u2013 \\hat{u}_t(u))\\Phi(u).$\nEck et al. (2013) introduced a similar formulation in the con-\ntext of state beliefs in partially observable MDPs.\nAkin to Furelos-Blanco et al. (2021), we define the po-\ntential function on RM states following the intuition that the\nagent should be rewarded for getting closer to u\u0104. Formally,\n\u0424(u) = |U| - dmin (u, ua),\nwhere dmin (u, u\u2081) is the minimum distance between u and\nUA. If u\u0104 is unreachable from u, then dmin (u, ux) = x."}, {"title": "3.4 Interleaved Learning Algorithm", "content": "We now describe PROB-IRM, our method for interleaving\nthe learning of RMs from noisy traces with RL. The pseu-\ndocode is shown in Algorithm 1.\nLines 1-5 initialise the candidate RM M, the set of noisy\nILASP examples E, variables tracking RM relearning, and\nthe Q-function. The algorithm is then executed for a fixed\nnumber of episodes. For each episode step, the agent exe-\ncutes an action in the environment (line 11), gets the new"}, {"title": "4 Experimental Results", "content": "We evaluate PROB-IRM using the OFFICE WORLD (see\nSection 2.1). We aim to answer three research questions:\nRQ1: Does PROB-IRM successfully allow agents to be\ntrained to complete their tasks?\nRQ2: In terms of agent performance, how do the RMs\nlearned with PROB-IRM compare with hand-crafted\nones?\nRQ3: How sensitive is PROB-IRM to different noisy set-\ntings?\nAfter a brief overview of our experimental setup, we start\nby comparing the performance of PROB-IRM agents with\na baseline composed of agents provided with handcrafted\nRMs that perfectly expose the structure of each task. Then,\nwe focus our analysis on the impact of significantly higher"}, {"title": "4.1 Experimental Setup", "content": "Environment Configurations. We focus our analysis on\nthe OFFICEWORLD domain presented in Section 2.1. On\nthe one hand, the COFFEE, COFFEEMAIL, and VISITABCD\ntasks enable assessing PROB-IRM's ability to learn good-\nquality RMs and policies in progressively harder scenarios.\nOn the other hand, thanks to its adoption in other existing\nRM-based work (Toro Icarte et al. 2018; Furelos-Blanco et\nal. 2021; Dohmen et al. 2022), this choice allows our results\nto be easily compared with similar research.\nThe complexity of solving any of the three tasks strongly\ndepends on the OFFICEWORLD layout. To account for this,\nwe conducted each experiment over 3 sets of 10 random\nmaps for COFFEE and COFFEEMAIL, and 3 sets of 50 ran-\ndom maps for VISITABCD.\nSensor Configurations. We experiment with multiple\nsensor configurations, each with a specific choice of:\n\u2022 noise targets: the set of sensors subject to noise. Either\nonly the one associated with the first event needed to solve\nthe task (noise-first),2 or all of them (noise-all);\n\u2022 noise level: the sensor detection specificity and sensitiv-\nity. To reduce the number of possible configurations, we\nonly consider scenarios where both parameters are set to\nthe same value, and we will thus refer to them jointly as\nsensor confidence.\nWhile sensor confidence is the parameter we have direct\ncontrol over in our experiments, its value is not very infor-\nmative to a human reader, as its impact on the accuracy of\nan agent's sensor strongly depends on the prior probabilities\nof each label being detected. Therefore, we pick the values\nfor this parameter in such a way as to determine specific val-\nues for the posterior probability of detecting any noisy label\ncorrectly. In particular, we experiment with three posterior\nvalues, each representing increasing noise levels: 0.9, 0.8\nand 0.5. We also consider the absence of noise (the poste-\nrior equal to 1) as a baseline.\nPolicy Learning. The agent policies are learned via the\nRL algorithm for RMs outlined in Section 2.2. The Q-\nfunctions are stored as tables. To index a reasonably sized\nQ-function using an RM belief vector, we resort to binning:\nbeliefs are truncated to a fixed number of decimals, effec-\ntively resulting in close values being considered identical.\nIn terms of exploration, we rely on an e-greedy strategy.\nFor all the experiments, we start from \u20ac = 1 and decay its"}, {"title": "4.2 Baseline Results", "content": "We compare the performance of PROB-IRM against a base-\nline composed of agents provided with handcrafted RMs.\nWe focus on the noise-first scenario, as it represents an eas-\nier learning setting for agents.\nThe learning curves of both approaches are shown in Fig-\nure 2. The baseline agents (upper row) consistently converge\nto a high level of proficiency. As expected, the more noise,\nthe more episodes are required to converge.\nWe make two observations in the case of PROB-IRM\n(lower row). First, most learning curves display one or more\nsudden changes: when a new RM is learnt, the agent's pol-\nicy is discarded and a new one starts being trained, thus\nleading to a temporary decrease in performance. The fre-\nquency of RM relearning increases with the level of noise.\nSecond, we observe that PROB-IRM often reaches the base-\nline performance in a comparable number of episodes, thus\nproviding a positive answer to research questions RQ1 and\nRQ2. We highlight three exceptions: the first is found in\nthe noisiest configuration for COFFEEMAIL, where many\nagents triggered RM relearning near the end of their train-\ning, thus ending with subpar performance. The second and\nthird exceptions are represented by the two most noisy con-\nfigurations for VISITABCD, which is the hardest task."}, {"title": "4.3 Multiple Noise Sources", "content": "We here assess PROB-IRM's ability to deal with multiple in-\ndependent sources of noise; hence, we consider the noise-all\nsetting for the COFFEE task to answer RQ3. In this setting,\nthe number of noise sources is effectively tripled as com-\npared to our baseline experiments: in addition to the noisy\nsensor, the * and o sensors are also noisy.\nWe initially considered the set of posterior values used in\nthe previous experiments; however, our experiments do not\nterminate within a 24-hour timeout period using a posterior\nof 0.5. This is unsurprising, as the learning task associated\nwith this configuration is extremely complex. After exper-\nimenting with different posteriors ranging from 0.5 to 0.8,\nwe find 0.75 to be a soft limit for the reliable applicability of\nPROB-IRM. Although more noise can be handled correctly,\nthe training time increases rapidly."}, {"title": "4.4 Ablation Studies", "content": "Reward Shaping. To confirm the effectiveness of our re-\nward shaping method, we replicate the setup in Section 4.3\nbut train the agents without reward shaping. Figure 4\npresents the resulting learning curves. When comparing\nthem with those in Figure 3, the positive impact of reward\nshaping is immediately apparent: for each level of noise, its\nuse leads to a substantial decrease in the number of train-\ning episodes required to reach high performance, in addition\nto an actual increase in the agents' proficiency at the end\nof training. Moreover, its effectiveness grows higher as the\namount of noise in the environment increases, thus proving\nit to be a very useful tool for dealing with such scenarios.\nThe reason behind its effectiveness lies in how reward\nshaping influences both policy training and RM learning.\nOn the one hand, the intermediate rewards it provides help\nthe agents improve their Q-value estimates faster, especially\nwhen rewards are sparse, thus leading to better policies\nsooner. On the other hand, they allow for incorrect RMs\nto be identified and discarded earlier in the learning process.\nWhen a candidate RM does not correctly reflect the task's\nstructure, reward shaping nevertheless encourages the agents\nto act consequently. This, in turn, induces the agents to fol-\nlow trajectories that are more likely to provide inconsistent\nexamples for the RM learner.\nPost-Hoc Analysis: Belief Updating vs Thresholding. In\nthe last set of experiments, we aim to provide an empirical\njustification for our choice of a belief-updating paradigm,\npresented in Section 3.3. A valid alternative is represented\nby thresholding, which treats every proposition whose asso-\nciated belief exceeds a fixed value as true. The main advan-\ntage of this strategy is not needing to maintain a belief over\nthe RM states; instead, the RM state is updated by following\nthe transitions triggered by the propositions deemed to be\ntrue after thresholding. Unfortunately, this upside is over-\nshadowed by the fact that, under our sensor model, thresh-\nolding either works perfectly or does not work at all.\nTo support this claim, we train two sets of PROB-IRM\nagents to solve the COFFEE task under the noisy-all setting\nwith a posterior of 0.8. The first set operates with a thresh-"}, {"title": "5 Related Work", "content": "Since the introduction of reward machines by Toro Icarte\net al. (2018), there have been several approaches for\nlearning them from traces in non-noisy settings. These\napproaches include discrete optimization (Toro Icarte et\nal. 2019; 2023), inductive logic programming (ILP; Furelos-\nBlanco et al. 2021; Ardon, Furelos-Blanco, and Russo 2023;\nFurelos-Blanco et al. 2023), program synthesis (Hasanbeig\net al. 2021), or SAT solving (Xu et al. 2020; Corazza,\nGavran, and Neider 2022). Our method employs ILASP\n(Law, Russo, and Broda 2015), a system also used in the\naforementioned ILP works, to learn the RMs. Given its\ninherent robustness to noisy examples, ILASP-based ap-\nproaches were the best positioned to be extended.\nExisting RM learning methods focus on either accurately\npredicting the next label from the previous one (Toro Icarte\net al. 2019; Hasanbeig et al. 2021; Toro Icarte et al. 2023),\nor learning a minimal RM (i.e., with the fewest number of\nstates) that makes the reward signal Markovian (Xu et al.\n2020; Furelos-Blanco et al. 2021; Hasanbeig et al. 2021;\nCorazza, Gavran, and Neider 2022; Ardon, Furelos-Blanco,\nand Russo 2023; Furelos-Blanco et al. 2023). Our method\nfalls into the latter category. We hypothesize that the ap-\npearance of erroneous labels in the noisy setting makes RMs\nchallenging to learn for methods in the former category.\nThe learning of RMs from noisy labels has only been pre-\nviously considered by Verginis et al. (2024). Their work\nfocuses on optimizing the noisy labelling function to model\nthe perfect labelling function after thresholding, enabling the\nuse of existing algorithms for RM learning. In contrast, our\napproach directly integrates noise into the RM learning pro-\ncedure. Besides, PROB-IRM is orthogonal to the choice of\nthe sensor model, so we could integrate a similar mecha-\nnism to achieve better results;"}]}