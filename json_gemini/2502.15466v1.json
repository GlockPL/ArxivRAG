{"title": "Mitigating Data Scarcity in Time Series Analysis: A Foundation Model with Series-Symbol Data Generation", "authors": ["Wenxuan Wang", "Kai Wu", "Yujian Betterest Li", "Dan Wang", "Xiaoyu Zhang", "Jing Liu"], "abstract": "Foundation models for time series analysis (TSA) have attracted significant attention. However, challenges such as data scarcity and data imbalance continue to hinder their development. To address this, we consider modeling complex systems through symbolic expressions that serve as semantic descriptors of time series. Building on this concept, we introduce a series-symbol (S2) dual-modulity data generation mechanism, enabling the unrestricted creation of high-quality time series data paired with corresponding symbolic representations. Leveraging the S2 dataset, we develop SymTime, a pre-trained foundation model for TSA. SymTime demonstrates competitive performance across five major TSA tasks when fine-tuned with downstream task, rivaling foundation models pre-trained on real-world datasets. This approach underscores the potential of dual-modality data generation and pretraining mechanisms in overcoming data scarcity and enhancing task performance.", "sections": [{"title": "1. Introduction", "content": "researchers. In recent years, with the rapid advancement of deep learning, foundation models for TSA have garnered widespread attention due to their superior generalization capabilities, scalability and advantages in few-shot learning. Presently, pre-training methods such as mask time series modeling (MTM), contrastive learning, and generative modeling have given rise to a series of foundation models for time series, achieving significant results in TSA tasks.\nCoupled with issues of data privacy , existing time series datasets are smaller compared to those in the fields of computer vision (CV) and natural language processing (NLP). Besides, current large-scale time series datasets face significant data imbalance issues, with certain types such as finance and healthcare still being relatively scarce (see Appendix B.2). According to scaling laws , this can lead to performance bias in the time series foundation models, reducing their generalization capabilities on out-of-distribution data .\nTo address the issue of data scarcity and data imbalance, this paper, starting from the nature and mechanisms of time series, posits that time series are representations of complex dynamical systems . On one hand, the intricate patterns within natural systems can be captured through observed numerical data ; for instance, the time series of body temperature fluctuations throughout a day is derived from observations of the human system. On the other hand, complex systems can be expressed abstractly using mathematical symbols and formulas , with ordinary differential equations (ODE) and partial differential equations (PDE) being the most common methods for modeling complex systems. Symbols provide semantic information for modeling complex systems . Therefore, as time series serve as the dynamic representation of observing complex systems, they can form a pairing relationship with the symbols used to model these systems.\nTo this end, we provide a series-symbol (S2) dual-modality data generation mechanism. This allows us to unrestrictedly produce high-quality time series and their paired symbolic data, constructing a large-scale S2 dataset. Then, we pretrain a time series foundation model SymTime with symbolic semantic information on this dataset. We train a Transformer-based time series encoder and a symbol encoder composed of a pre-trained large language model (LLM) through MTM and mask language modeling (MLM) to learn the basic representations of series and symbols respectively. Subsequently, using momentum distillation , we"}, {"title": "2. Related Work", "content": "In CV and NLP , pre-trained foundation models (PTFMs) have been demonstrated to adapt to a variety of downstream tasks after fine-tuning on specific datasets, exhibiting excellent generalization and scalability . Inspired by this, recent years have seen significant progress in PTFMs for TSA , with the emergence of various pre-training methods. MOIRAI, through MTM and reconstruction, has been pre-trained on large datasets (27B), yielding a universal forecasting model with significant zero-shot advantages . Timer, after generative pre-training on large datasets (1B), has performed well in forecasting . TimeGPT trained a encoder-decoder Transformer with 100B data . COMET, using multi-level contrastive learning on a large ECG dataset, has obtained a medical time series PTFMS with few-shot advantages .\nAs discussed in Appendix B.2, these baseline models still face challenges related to data scarcity and data imbalance. In the next section, we introduce the proposed data generation mechanism and the corresponding dual-modality"}, {"title": "3. Main Methods", "content": ""}, {"title": "3.1. Series-Symbol (S2) Dataset Generation", "content": "The pre-training of relies on a large synthetic series-symbol (S2) dataset. The specific generation process is shown in Figure 1 (left). Firstly, we construct a multivariate input-output symbolic expression f() through random sampling . Then, we use the randomly generated sampling series \\(X \\in \\mathbb{R}^{M \\times L}\\) to forward propagate through the symbolic expression to obtain the generated series \\(Y = f(X) \\in \\mathbb{R}^{N \\times L}\\), where N and M represent the dimensions of the input and output series respectively, and L is the length of the series.\nSampling of Functions. We first determine the dimensions of the input and output series and randomly select the number of binary operators from a uniform distribution. Then, we construct a binary tree with symbolic variables and constants as leaf nodes, and binary operators as two-child nodes to form the basic framework of the symbolic expression . Finally, we enhance the diversity of the expressions by inserting unary operators as one-child nodes and applying affine transformations randomly . After determining the framework of the tree, we select specific operators for all nodes from the uniform distribution \\(U(+, -, \\times)\\) and \\(U(\\text{inv}, \\text{abs}, \\text{pow2}, \\text{pow3}, \\text{sqrt}, \\text{sin}, \\text{cos}, \\text{tan}, \\text{arctan}, \\text{log}, \\text{exp})\\), and randomly initialize leaf nodes with random constants and variables. The symbolic expression can be read out via in-order traversal.\nGenerating Inputs and Outputs. To better align the generated series with time series characteristics, we sample multi-channel input series X from both mixed distributions (MD) and randomly parameterized ARMA(p, q) models . We first select the distribution number in MD or the order (p, q) in ARMA model from the uniform distribution . Then, we randomly initialize the parameters of the MD (including the mean and variance of a normal distribution, as well as the range of a uniform distribution) or the ARMA model (including autoregressive and moving average components). We can obtain the sampling series X through forward propagation. Finally, we standardize each channel and obtain the generated series Y = f(X) through the symbolic expressions. To ensure data quality, we discard samples outside the domain of f(\u00b7) and excessively large generated values ."}, {"title": "3.2. Model Architecture and Pre-training Objectives", "content": "As shown in Figure 1 (right), mainly consists of three components: a time series encoder, a symbolic encoder, and momentum models , each with its own distinct pre-training objectives.\nTime Series Encoder and Mask Time Series Modeling. We employ a 6-layer Transformer as the time series encoder. An input time series is first divided into non-overlapping patches \\(P=\\{P_1,P_2,\\ldots,P_n\\}\\) using a sliding window approach . Then, we add random masks to these patches and embed them into the time series encoder to learn the basic representation of the series , obtaining the corresponding embedded sequence \\(T=\\{t_{\\text{cls}}, t_1, t_2,\\ldots, t_n\\}\\), where \\(t_{\\text{cls}}\\) is the [CLS] token added by the model . Our training objective is to restore the masked patch \\(p_j\\) through the final linear mapping layer. The MTM loss is as follows:\n\\begin{equation}\n\\mathcal{L}_{\\text{mtm}} = \\frac{1}{N} \\sum_{j \\in M_T} ||p_j - \\hat{p}_j||^2, \n\\end{equation}\nwhere MT is the set of masked patches, and \\(\\hat{p}_j\\) represents the patch reconstructed by the time series encoder and linear\nprojection . This approach allows the time series encoder to learn the representation of the time series.\nSymbol Encoder and Mask Language Modeling. We treat symbolic expressions as natural language and use a 6-layer DistilBert as the symbolic encoder to learn their representations . We first randomly replace the words in the symbolic expressions with [Mask] and input them into the symbolic encoder for representation learning, obtaining the embedded sequence: \\(S = \\{s_{\\text{cls}}, s_1,\\ldots,s_m\\}\\), where \\(s_{\\text{cls}}\\) is the [CLS] token added by the model. We have DistilBert predict the masked tokens using bidirectional information. Let \\(\\hat{s}\\) denote a masked token, and \\(p_{\\text{mask}}(\\hat{s})\\) denote the model\u2019s predicted probability for the masked token. The MLM minimizes a cross-entropy H loss:\n\\begin{equation}\n\\mathcal{L}_{\\text{mlm}} = - \\frac{1}{N} \\sum_{j \\in M_S} H(y_j, p_{\\text{mask}}(s_j)),\n\\end{equation}\nwhere MS is the set of masked words in the symbolic expression, and \\(y_j\\) is a one-hot vocabulary distribution with a probability of 1 for the ground-truth token.\nSeries-Symbol Contrastive Learning. In order to enable the time series encoder to learn the semantic information of symbolic expressions, we employ contrastive learning to associate the dual encoders in , allowing them to learn better unimodal representations. Specifically, SymTime will learn an inner product similarity func-tion : \\(\\text{sim} = g_t(t_{\\text{cls}})^T g_s(s_{\\text{cls}})\\), where"}, {"title": "3.3. Momentum Distillation for Masked Data Learning", "content": "Since we obtain the [CLS] token from masked data for contrastive learning, even with a low masking ratio, it still has an impact on the representation. Inspired by ALBEF , we treat the mask as noise added to the series and symbol. Therefore, we use momentum distillation to overcome the impact of masking on data representation. We train not only with true labels of series-symbol pairs (Equation 5) but also from the pseudo-targets generated by the momentum model . This allows our encoder\u2019s predictions to match the predictions of the momentum models. Let the similarity functions generated by the momentum encoders be \\(\\text{sim}'(t,s) = g'_t(t'\\text{cls})^T g'_s(s'\\text{cls})\\) and \\(\\text{sim}'(s, t) = g'_s(s'\\text{cls})^T g'_t(t'\\text{cls})\\). We compute soft pseudo targets \\(q^{t2s}(t)\\) and \\(q^{s2t}(s)\\) by replacing sim with sim' in"}, {"title": "4. Experiments", "content": "In our experiments, we first validate the effectiveness of the time series encoder and the series-symbol pre-training method on 5 mainstream TSA tasks. Details about the pre-training can be found in Appendix D.3. These specific downstream tasks include long-term forecasting (Section 4.1), short-term forecasting (Section 4.2), classification (Section 4.3), imputation (Section 4.3), and anomaly detection (Section 4.5). We use the same benchmarks as TimesNet for our experiments, with specific details in Appendix D.1. Then, in Section 4.6, we conduct ablation studies on the pre-training objectives of on long and short term forecasting. Subsequently, in Section 4.7, we analyze the complexity of the model and demonstrate that the time series encoder in learns the semantic information of the symbols. Finally, We demonstrate that through MTM and contrastive learning, has a cer-tain ability of zero-shot imputation and can distinguish the"}, {"title": "4.1. Long-term Forecasting", "content": "Setup. Time series forecasting, which analyzes historical data patterns to predict future trends, is crucial for financial market analysis, inventory management, energy demand and other fields . We adopt 8 real-world benchmark datasets for long-term forecasting, including ETTm1, ETTm2, ETTh1, ETTh2 , Weather , ECL (UCI), Traffic (PeMS) and Exchange . The forecasting lengths are set to {96, 192, 336, 720}. To ensure fairness in the comparison, we set the look-back window length of and all other models to 96, except Moirai and Timer are 672 and is 512.\nResults. Table 2 clearly demonstrates that achieves excellent performance in long-term forecasting tasks. Our model surpasses Peri-midFormer, GPT4TS and TimesNet, which are foundation models for the 5 major tasks, as well as Moirai and Timer, two general forecast-ing models. approaches and surpasses the cus-tomized time series forecasting model Time-LLM in terms of MSE and MAE metrics. However, Time-LLM relies on a large-scale LLM as its backbone, whereas can achieve comparable results through pre-training on synthetic datasets and fine-tuning with a more lightweight model."}, {"title": "4.2. Short-term Forecasting", "content": "Setup. We adopt M4 benchmark for short-term forecasting, which contains the yearly, quarterly and monthly collected univariate marketing data. Then, we use symmetric mean absolute error (SMAPE), mean absolute scaled error (MASE) and overall weighted average (OWA) to measure the forecasting performance, which are calculated as detailed in Appendix D.2.\nResults. Table 3 indicates that after pre-training, surpasses TimeMixer, Peri-midFormer and TimesNet on the short-term forecasting tasks in terms of SMAPE, MASE and OWA metrics, achieving state-of-the-art performance. Specifically, performs well on Yearly, Quarterly and Monthly datasets, demonstrating its capability to capture not only the trends of annual variations but also the cyclic characteristics of seasonal and monthly encoding."}, {"title": "4.3. Classification", "content": "Setup. Time series classification is crucial for the identifica-tion and diagnosis of patterns in complex systems and plays a significant role in various fields such as financial analysis, medical diagnosis and industrial monitoring . Using the experimental setup from TimesNet , we test 's discriminative ability"}, {"title": "4.4. Imputation", "content": "Setup. Sensors monitoring complex systems in the real world may experience distortions or malfunctions, leading to partial missing data in the collected time series. Therefore, time series imputation is crucial for the recovery of complete datasets. We verify 's imputation capabilities on 6 datasets: ETTm1, ETTm2, ETTh1, ETTh2 (Zhou et al., 2021), Weather and ECL (UCI). To test the model\u2019s imputation ability under varying de-grees of missing data, we add random masks at propor-tions of {12.5%, 25%, 37.5%, 50%} in point level on time series of length 96. Since was pre-trained by ran-domly masking patches level for series reconstruction and masks are added randomly in point level in the imputation task. Considering the differences between these masking approaches and the potential disruption of the series\u2019s orig-inal trends and periodic features at higher mask rates, we adopt per-interpolation for the masked series from (Wu et al., 2024b). Analysis and ablation experiments regarding this method are presented in Appendix D.7.\nResults. Table 4 shows that outperforms Peri-midFormer, GPT4TS and TimesNet in overall performance"}, {"title": "4.5. Anomaly Detection", "content": "Setup. Time series anomaly detection is crucial for rapidly identifying anomalies in critical areas, aiding in risk prevention and decision optimization. Due to the difficulty in annotating time series anomalies, we focus primarily on unsupervised anomaly detection. We conduct experiments on 5 widely used anomaly detection datasets: SMD and SMAP , MSL , SWaT , PSM , encompassing service monitoring, space & earth exploration, and water treatment applications. We adopt the same data preprocessing method as the Anomaly Transformer , dividing the data into non-overlapping segments of length 100 for reconstruction. Specifically, normal data is used for model training and we employ a simple re-construction loss to help the model learn the distribution of normal data . In subsequent testing phases, reconstructed outputs exceeding a specified threshold are considered anomalies.\nResults. Table 5 indicates that surpasses previous state-of-the-art methods such as TimesNet and GPT4TS and achieves commendable performance on the SMD and SWaT datasets. However, due to UniTS employing real data and downstream task-relevant pre-training, there is a slight performance difference between and UniTS."}, {"title": "4.6. Ablation Experiments", "content": "Setup. We conduct ablation studies on 's pre-training objectives and the size of the pre-training dataset using the ETT long-term forecasting dataset . First, we establish 8 different control groups based on whether pre-training is performed, freezing the model and various pre-training losses: (1) Freeze, (2) w/o Pre-train, (3) w/o MTM, (4) w/o MLM, (5) w/o T2S, (6) w/o S2T, (7) w/o Symbol and (8) w/o Distill. Specific explana-tions for the above control groups are provided in Appendix D.5. We use the average MSE of the prediction lengths {96, 192, 336, 720} on the ETTh1 and ETTh2 datasets as the evaluation metric, with the average results shown in Figure 4 (a). Then, we set the sizes of the pre-training datasets to {0, 10B, 20B, 30B, 40B, 50B}, where 0 indicates no pre-training. We subsequently observe the changes in the model\u2019s MSE with the size of the pre-training dataset, with specific results shown in Figure 4 (b). We apply the same experimental configuration for ablation studies on short-term forecasting tasks, with detailed results and analy-"}, {"title": "4.7. Discussion", "content": "Complexity Analysis. We analyze the complexity of the model on the long-term forecasting ETTh1 dataset, with results shown in Figure 5. We consider the parameter count, the GPU memory required for forward and back-ward propagation when the batch size is 1, the MSE as an evaluation metric. achieves better performance with a smaller model parameter count and memory capacity than existing foundation models in forecasting. Although 's performance is slightly lower than Time-LLM in the final experimental results of all datasets, its complex-ity is also significantly lower than Time-LLM. The compu-tational load and complexity of our model are reasonable and acceptable in the vast majority of application scenarios.\nStatistical Characterization of S2 dataset. We quantify the range of representations that the S2 dataset can cover through statistical metrics (including stationarity , forecastability , seasonality and entropy ), proving that our unrestricted data generation mechanism can evenly cover the basic rep-resentations of all types of time series, thereby solving the data scarcity problem. See Appendix B.3 for full results.\nZero-shot Imputation. After large-scale masked time series modeling (Equation 1), successfully learned the basic representation of time series and has the ability of zero-shot imputation for S2 out-of-domain data and real-world time series data. See Appendix C for full results.\nSeries-Symbol Representation Learning. Through con-trastive learning (Equation 5), our time series encoder is able to distinguish series of simple unary symbolic expres-sions, proving that our encoder has learned the semantic information of the symbols . Specific"}, {"title": "5. Conclusion", "content": "To address the challenges of data scarcity and distribution imbalance in time series analysis, we introduce a dual-modality data generation mechanism that enables the un-restricted creation of high-quality time series data, along with corresponding symbolic representations. Leveraging this large-scale series-symbol synthetic dataset, we propose a pre-trained foundation model that integrates both time series representations and symbolic semantic information. Our pre-trained model demonstrates exceptional performance across five major TSA tasks, highlighting the"}, {"title": "Impact Statement", "content": "The potential value of this work lies in its ability to mitigate fundamental challenges in TSA, such as the lack of sufficient labeled data and the issue of imbalanced datasets. By generating rich, diverse, and high-quality synthetic data, our approach not only addresses these issues but also opens new avenues for improving model generalization across a wide range of applications. Furthermore, the dual-modality"}, {"title": "A. Series-Symbol (S2) Pre-training Data Details", "content": "In this section, we primarily detail the S2 dataset generation process. Section A.1 describes the generation of symbolic expressions. Subsequently, A.2 explains the process of generating sampling series from mixed distributions and random ARMA series. Then, A.3 presents the series-symbol data we generated. Finally, in Section A.4, we provide a detailed introduction to the usage of the S2 dataset for SymTime pre-training. Some of the mathematical symbols used and their explanations are shown in Table 6."}, {"title": "A.1. Sampling of Functions", "content": "Since the structure of mathematical expressions is inherently tree-like (Meidani et al., 2023), where constants and variables can be considered leaf nodes, and binary operators can be seen as root nodes with two child nodes, while unary operators can be regarded as root nodes with a single child node (Kamienny et al., 2022; Lample & Charton, 2020). Therefore, we construct a binary tree using input variables and binary operators as the basic framework for symbolic expressions . Subsequently, we randomly insert unary operators within the binary tree and introduce constants through affine transformations to increase the diversity and complexity of the expressions(Bartlett et al., 2024; Meidani et al., 2023; Kamienny et al., 2022). The specific process is illustrated below. The three key steps are shown in Figure 6.\nInput and Output Dimension Selection. To learn the features of numerical series and symbolic expressions, previous works generated the dimensions M for sampling series and N for generated series from uniform distributions \\(U(1, M_{\\text{max}})\\) and \\(U(1, N_{\\text{max}})\\) (Meidani et al., 2023; Kamienny et al., 2022; Alon et al., 2022). However, this paper traverses [1, \\(M_{\\text{max}}\\)] and [1, \\(N_{\\text{max}}\\)] directly to cover representations of multivariate time series, considering \\(M_{\\text{max}}\\) = 6 and \\(N_{\\text{max}}\\) = 12 due to the complexity and uncertainty of data generation. An input series dimension of M implies the construction of expressions with M variable nodes \\(x_1, x_2, \\ldots, x_M\\). An output series dimension of N indicates sampling N expressions \\(y_i = f_i(x_1, x_2, \\ldots, x_M), i = 1, 2, \\ldots, N\\), yielding N channel-related generated series (Lample & Charton, 2020).\nBinary Operator Quantity Selection. After determining the input and output dimensions, we sample the number of binary"}, {"title": "A.2. Generating Inputs and Outputs Series", "content": "After obtaining the symbolic expression \\(f_i(\\cdot)\\), we generate a sampling series \\(X = [x_1, x_2, \\ldots, x_M] \\in \\mathbb{R}^{M \\times L}\\) and obtain the generated series \\(Y = [y_1, y_2, \\ldots, y_N] \\in \\mathbb{R}^{N \\times L}\\) through forward propagation of the symbolic expression, where \\(y_i = f_i(X) = f_i(x_1, x_2, \\ldots, x_M), i = 1, 2, \\ldots, N\\). In order to ensure the quality and diversity of the data, previous works generated sampling series from mixed distributions (Meidani et al., 2023; Kamienny et al., 2022; Lample & Charton, 2020). To make the generated series data more representative of time series, the data in this paper is not only sampled from mixed distributions but also generated from autoregressive moving average (ARMA) models with random parameters. The ARMA(p, q) model consists of moving average (MA) and autoregressive (AR) processes , which can be expressed as:\n\\begin{equation}\nY_t = \\phi_1 Y_{t-1} + \\phi_2 Y_{t-2} + \\cdots + \\phi_p Y_{t-p} + e_t - \\theta_1 e_{t-1} - \\theta_2 e_{t-2} - \\cdots - \\theta_q e_{t-q},\n\\end{equation}\nwhere p and q represent the orders of the AR and MA models, respectively, \\(\\phi_p\\) and \\(\\theta_q\\) are the parameters of the AR and MA processes , and \\(e_t \\sim \\mathcal{N}(0, 1)\\) denotes the observed white noise sequence. Since ARMA possess both the temporal correlation of the AR process and the randomness of the MA process, series obtained from mixed distributions and ARMA sampling better reflect the characteristics of time series.\nTo ensure the quality of the generated series Y, if the sampling series value \\(x_i\\) falls outside the domain of the expression \\(f(\\cdot)\\) or if the generated target value \\(y_i\\) is excessively large, exceeding \\(10^4\\), then the sample is discarded and resampled . This measure ensures the proper generation of data. Furthermore, for each random seed, we traverse all input and output channels to generate symbolic expressions, and each expression is sampled only once. The specific process for generating the sampling series X is as follows.\nMixture Distribution Number and ARMA(p, q) Model Order Selection. Our generated series originate from either a mixed distribution or a randomly parameterized ARMA(p, q) model . Thus, the initial step involves randomly deciding with probability P whether to employ mixed distribution sampling or the ARMA(p, q) model, with this paper setting P < 0.5 for the mixed distribution and P > 0.5 for ARMA(p, q). When opting for the mixed distribution, we select the number of distributions k from the uniform distribution \\(U(1, k_{\\text{max}})\\) and determine the weights for each distribution \\(\\{w_j \\sim U(0, 1)\\}_{j=1}^k\\), normalizing them so that \\(\\sum_j w_j = 1\\). For the ARMA(p, q) model, we independently choose the orders of the AR and MA components, p and q, from the uniform distributions \\(U(1, P_{\\text{max}})\\) and \\(U(1, q_{\\text{max}})\\), respectively.\nGeneration of Distribution and Parameters. When utilizing a mixed distribution, we select the mean \\(\\mu_j\\) and variance \\(\\sigma_j^2\\) for the j-th mixed distribution from \\(\\mathcal{N}(0, 1)\\) and \\(U(0, 1)\\), respectively. Ultimately, we randomly determine a Gaussian"}, {"title": "A.3. Series-Symbol Data Display", "content": "The symbolic expressions with text format are shown as follow:\nSymbolic expression of Figure 7 (a)\nSymbolic expression of Figure 7 (c)\nSymbolic expression of Figure 7 (e)\nSymbolic expression of Figure 7 (g)"}, {"title": "A.4. Composition and Usage of the Series-Symbol Dataset", "content": "We set the maximum number of input channels and the maximum number of output channels to 6 and 12 respectively to generate symbolic expressions and series. Each symbolic expression is sampled only once. We generated a total of 25M pairs of series and symbols. The cumulative series length is 50B. The data number of each input channel and output channel in the dataset is shown in Figure 8.\nWhen pre-training with S2 dataset, we start by combining the sampled and generated series and then segmenting"}, {"title": "B. Analysis of Series-Symbol (S2) Dataset and Model Pre-training", "content": "In Appendix A, we provide a detailed introduction to the generation process, composition and usage of the S2 dataset (Meidani et al., 2023; Kamienny et al., 2022; Lample & Charton, 2020). In this section, we first conduct a random sampling analysis of the statistical characteristics of the S2 dataset, including stationarity and predictability . Then, we present the composition of four existing large-scale time series pre-training datasets. It is evident that all current real-world time series datasets face significant data imbalance issues, leading to performance biases in the foundation models pre-trained on them. However, the S2 data generation method provided in this paper can unrestrictedly supply high-quality time series data, thus covering almost all time series representations."}, {"title": "B.1. Statistics Analysis", "content": "Stationarity. Stationarity is one of the fundamental properties of time series . This attribute ensures that the statistical characteristics of time series data remain consistent across different time points, which is crucial for building effective predictive models and making reliable statistical inferences. To this end, we employ the Augmented Dickey-Fuller (ADF) test to examine the stationarity of the data, thereby determining"}, {"title": "B.2. Analysis of Existing Large-scale Datasets for Time Series Pre-training", "content": "Large-scale datasets are crucial for building foundation models. Almost all deep learning models today are data-driven, relying on training data . Therefore, when constructing a pre-trained foundation model for time series, a large-scale and comprehensively representative pre-training dataset is indispensable . The scaling laws of neural networks indicate that the learning effectiveness of deep neural networks is primarily influenced by three factors: the number of model parameters, the size of the training dataset, and the amount of computational resources"}, {"title": "B.3. S2 Dataset Statistical Characterization Coverage Experiments", "content": "Metric. To further examine the diversity of the artificially synthesized data in the S2 dataset, we conduct a sampling assessment from six dimensions: stationarity, predictability, frequency domain characteristics, complexity, seasonality intensity, and trend characteristics. For each dimension, we select corresponding statistical indicators for dataset evaluation and quantification, as detailed below: (1) Augmented Dickey-Fuller (ADF) Test: Consistent with section B.1, we employ the ADF test to assess the stationarity of time series, using its test statistic as an indicator of time series stationarity . (2) Forecastability: Based on method, we determine whether a time series is chaotic or can be accurately predicted through machine learning models by using Fourier decomposition and entropy . Note that since the method is only applicable to multivariate time series, we invert the sampled single-channel time series to form a dual-channel series to calculate the indicator. (3) FFT Mean: We utilize the average of the Fourier transform power spectrum to evaluate the frequency domain characteristics of time series. This indicator can be used to measure the overall intensity of time series and assess the energy distribution. (4) Permutation Entropy: This indicator assesses the dynamic complexity of a time series by analyzing its permutation patterns . We set the embedding dimension m = 3 and time delay r = 1, and calculate its specific value using Shannon Entropy in Equation 10. See (Cao et al., 2004) for more detailed calculation. (5) Seasonality: We decompose the time series into trend, seasonal and residual components using the Seasonal-Trend Decomposition using LOESS (STL) algorithm . Then, we calculate the intensity of the seasonal component in the time series according to Equation 11. (6) Mann-Kendall Test: This is a non-parametric statistical method used to detect monotonic trends in time series . The basic principle is to compare the size relationship between each data point and other data points in the"}, {"title": "C. Model Pre-training and Representation Learning", "content": "In this section, we analyze and present the model's pre-training status and the learned representations . We first examine the masked time series modeling (MTM) of"}, {"title": "C.1. Masked Time Series Modeling and Zero-shot Imputation", "content": "Setup. Since we incorporate MTM loss in the pre-training process of in this section, we assess the specific learning effects of the time series encoder in through masked modeling . We test the model's performance using both pre-trained synthetic data not in the S2 dataset and real datasets from time series imputation tasks . As adds masks in units of patches of length 16 during pre-training, we also add masks in the form of 16-length patches. The reconstruction effect of the masked parts by the time series encoder is shown in Figure 10 and 11.\nS2 Dataset Out-of-Domain Data. In Figure 10, we generate new data using the method from the S2 dataset and add masks to test the reconstruction ability of the time series encoder . The gray sections represent the masked segments, while blue and orange represent the original and reconstructed series, respectively. We input time series outside the gray parts in patches and have the model reconstruct the gray sections based on the remaining information. Since we only calculate the MTM loss on the masked parts , the visible reconstruction does not overlap with the original input series . From the Figure 10, it can be observed that the time series encoder in performs well in fitting the fluctuations and trends of time series, demonstrating that our encoder successfully learned the fundamental representations of time series during pre-training .\nReal-world Time Series Data. In Figure 11, we conduct representation learning tests on 6 real datasets: ETTm1, ETTm2, ETTh1, ETTh2 , Electricity (UCI), and Weather . Since no real data are used for model pre-training, these datasets are also considered as out of domain data. We similarly add masks in patch units (gray sections). It can be observed that the time series encoder in also performs well in zero-shot reconstruction on real-world"}, {"title": "C.2. Series-Symbol Representation Learning of Time Series Encoder", "content": "Setup. To evaluate the cross-modal representations and semantic information by the time series encoder in we select 20K single-input single-output channel symbolic expressions containing only one type of unary operator and their corresponding sampled series from the S2 dataset . These symbolic expressions include only one specific math operator from the set , using this operator as the category label for the entire series data . Then, we do patching on the sampled time series and input them into the time series encoder, both pre-trained and without pre-trained . Finally, we use the series representations reconstructed by the time series encoder and apply t-SNE to reduce them to a two-dimensional space to observe the distribution of representations for series generated by different types of symbols. The specific results are shown in Figure 12.\nResults. From Figure 12 (a), it can be observed that the time series encoder without pre-training cannot effectively distinguish different series-symbol categories. Only a few symbols, such as inv and exp, which may have more distinct data representations, form certain clusters even without pre-training. Due to we introduce both series-to-symbol and symbol-to"}]}