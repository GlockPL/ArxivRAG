{"title": "BPP-Search: Enhancing Tree of Thought Reasoning for Mathematical Modeling Problem Solving", "authors": ["Teng Wang", "Wing-Yin Yu", "Zhenqi He", "Zehua Liu", "Xiongwei Han", "Hailei Gong", "Han Wu", "Wei Shi", "Ruifeng She", "Fangzhou Zhu", "Tao Zhong"], "abstract": "LLMs exhibit advanced reasoning capabilities, offering the potential to transform natural language questions into mathematical models. However, existing open-source operations research datasets lack detailed annotations of the modeling process, such as variable definitions, focusing solely on objective values, which hinders reinforcement learning applications. To address this, we release the StructuredOR dataset, annotated with comprehensive labels that capture the complete mathematical modeling process. We further propose BPP-Search, a algorithm that integrates reinforcement learning into a tree-of-thought structure using Beam search, a Process reward model, and a pairwise Preference algorithm. This approach enables efficient exploration of tree structures, avoiding exhaustive search while improving accuracy. Extensive experiments on StructuredOR, NL4OPT, and MAMO-ComplexLP datasets show that BPP-Search significantly outperforms state-of-the-art methods, including Chain-of-Thought, Self-Consistency, and Tree-of-Thought. In tree-based reasoning, BPP-Search also surpasses Process Reward Model combined with Greedy or Beam Search, demonstrating superior accuracy and efficiency, and enabling faster retrieval of correct solutions.", "sections": [{"title": "INTRODUCTION", "content": "Mathematical modeling, particularly Linear Programming (LP) and Mixed Integer Programming (MIP), plays a critical role in industrial applications such as logistics (Demirel and G\u00f6k\u00e7en, 2008), electricity scheduling and transmission (Zhang et al., 2018), and supply chain management (\u00d6zceylan and Paksoy, 2013). With the advent of Large Language Models (LLMs), transforming natural language questions into mathematical models has become a promising approach for automating operations research tasks (Xiao et al., 2024; Tang et al., 2024a).\nDespite the increasing availability of open-source operations research (OR) datasets designed for question-to-model transformation (Huang et al., 2024; Ramamonjison et al., 2022; Tang et al., 2024b), these datasets primarily focus on objective values while lacking detailed annotations of the underlying modeling processes. This gap limits the application of Reinforcement Learning (RL), as prior studies (Lightman et al., 2023; Uesato et al., 2022; Cobbe et al., 2021) have shown that process information can significantly enhance mathematical reasoning performance. To address this limitation, we design a rigorous framework for dataset generation and introduce the StructuredOR dataset, which not only provides objective values for evaluation but also includes comprehensive annotations of the modeling process, enabling broader applicability in RL-based methods.\nChain-of-Thought (CoT) (Wei et al., 2022), Tree-of-Thought (ToT) (Yao et al., 2023), and Self-Consistency (SC) (Wang et al., 2023) have demonstrated substantial improvements in reasoning tasks. However, these approaches have inherent limitations. CoT heavily relies on the policy model and generates only one reasoning path at a time, making it likely to fail to find the correct answer when the policy model is weak. SC, without a verifier, struggles to validate the correctness of candidate answers, allowing errors in intermediate steps to propagate and mislead the reasoning process. Similarly, ToT generates multiple leaf nodes as potential answers, but without a verifier, it is unclear which leaf node should be selected as the final solution. Nonetheless, ToT remains promising; with sufficiently wide and deep trees and effective node selection strategies, it has the potential to generate optimal solutions.\nTo enhance the reasoning process within the ToT framework, we propose BPP-Search, a novel method that integrates Beam Search, a Process Reward Model (PRM), and a pairwise Preference"}, {"title": "RELATED WORK", "content": "Mathematical modeling datasets can be broadly categorized into two types: abstract modeling and concrete instance modeling. Modeling tools such as Pyomo (Hart et al., 2017) and AMPL (Gay, 2015), and OPL (Van Hentenryck et al., 1999) provide support for both approaches, enabling users to work with abstract models as well as concrete instances.\nAbstract modeling focuses on capturing the essential structural information of a model. It typically involves two steps: defining basic model declarations and applying data to create concrete instances. This approach is particularly suited for large-scale industrial applications and research, as models can be defined once and reused by importing different datasets. For instance, ML-Prompt (Wang et al., 2024b) leverages abstract models to generate parameter distributions, which are subsequently populated with specific values to construct concrete instances within industrial pipelines. Additionally, several studies (Yang et al., 2024c; Wang et al., 2024c) combine abstract models with CoT and LLMs to address problems such as Traveling Salesman Problem (Gavish and Graves, 1978), bypassing the need for traditional mathematical solvers or explicit concrete models.\nConcrete modeling requires all data to be available before model processing begins, making it a straightforward and efficient approach. It is particularly suited for analytical projects. This approach is especially advantageous when certain constraints are difficult or time-consuming to generalize into an abstract format, as it allows for more precise and tailored solutions, significantly reducing processing time. For smaller-scale mathematical models, tasks can be solved directly without treating them as a combination of abstract modeling and Named Entity Recognition (Grishman and Sundheim, 1996), which involves first building an abstract model and then mapping numerical parameter values to it. This approach minimizes error accumulation across tasks (Shen et al., 2023)."}, {"title": "Process Reward Model", "content": "Several works (Cobbe et al., 2021; Lightman et al., 2023; Uesato et al., 2022) introduced the concept of the Process Reward Model (PRM), demonstrating its ability to significantly enhance the performance of weak and small-scale policy models. Compared to the Outcome Reward Model (ORM), PRM achieves better performance but incurs higher labeling costs (Uesato et al., 2022). Initially, PRM training relied on manually labeled data (Lightman et al., 2023). To address the growing demand for processing labels, Monte Carlo Tree Search (MCTS)-based methods (Wang et al., 2024d,a; Luo et al., 2024; Zhang et al., 2024; Setlur et al., 2024) were later developed to simulate and assign scores to reasoning processes. While effective, MCTS-based approaches require wide and deep trees to generate labeled data through extensive rollouts for score convergence at intermediate nodes, resulting in extremely high computational resource demands. In contrast, manually labeled data is deterministic and directly reflects the intended reasoning process without relying on approximations."}, {"title": "Dataset Generation", "content": "The Greedy algorithm selects the candidate with the highest score at each step, focusing entirely on exploitation without exploration. The selection process can be formalized as:\n$a = \\arg \\max_{a \\in A} P(a)$, \nwhere $P(a)$ represents the score of candidate $a$, and $A$ denotes the set of candidates.\nThe Epsilon Greedy algorithm balances exploration and exploitation during candidate selection. At each step, with a probability"}, {"title": "StructuredOR Dataset Framework", "content": "Existing operations research datasets predominantly focus on objective values and the annotations of the underlying modeling process appear to be missing. To bridge the gap, we introduce StructuredOR a new dataset explicitly designed to provide the objective value for evaluation and capture the complete mathematical modeling process.\nBuilding on Xiao's work (Xiao et al., 2024), which introduces a framework for generating abstract mathematical models, we refine and expand this approach to cover a wider spectrum of abstract models. We leverage LLM such as GPT-4o (Achiam et al., 2023) in conjunction with mathematical solvers to instantiate abstract models into concrete examples. Furthermore, we implement a series of validation mechanisms to construct and verify the accuracy of these concrete problems, thereby ensuring the dataset's quality and reliability. In summary, the StructuredOR dataset provides pairs of concrete questions and corresponding model data, accompanied by comprehensive annotations detailing the entire modeling process. Fig. 3 illustrates the construction pipeline, with each step distinguished by a color-coded arrow: blue for Step 1, orange for Step 2, red for Step 3, and green for Step 4. The whole process is delineated as follows: Firstly, we leverage LLMs such as GPT-4o to generate distributions for sets and parameters in ab-"}, {"title": "PRM Dataset Preparation", "content": "To implement process supervision within the tree structure, we introduce the PRM (Uesato et al., 2022; Lightman et al., 2023), which assigns a score to each intermediate step in the reasoning process. There are two main approaches to generating training data for PRMs. (Uesato et al., 2022; Lightman et al., 2023) rely on manually labeling each inter-"}, {"title": "Methodology", "content": "Previous works (Cobbe et al., 2021; Uesato et al., 2022; Lightman et al., 2023; Luo et al., 2024) have shown that small-scale LLMs equipped with verifiers evaluating intermediate processes are capable of outperforming foundational large-scale LLMs in mathematical reasoning tasks. In this work, we fine-tune Qwen2.5-Math-1.5B (Yang et al., 2024b) for a binary classification task. Details on constructing prompts for the PRM are provided in Appendix A.5. After full-parameter supervised fine-tuning, we extract the logits corresponding to the correct label and apply the sigmoid function to compute the score:\n$S_{PRM} = \\frac{1}{1 + e^{-l_{prm}}}$,\nwhere $l_{prm}$ denotes the logit value for the correct label, and $S_{PRM}$ represents the PRM score."}, {"title": "BPP-Search", "content": "We integrate the PRM with Greedy (Prim, 1957) and Beam Search (Lowerre and Reddy, 1976) algorithms, where the PRM provides scores to guide node selection. However, as shown in Section 5.3, increasing the Beam Search width does not consistently improve performance and, in some cases, leads to degradation. This limitation stems from the PRM, which is trained for classification tasks but required to assign continuous scores during inference, resulting in output discrepancies. Manual analysis of the tree generation process reveals that the final layer of Beam Search frequently contains both correct and incorrect candidates that are highly similar, with only subtle distinctions. These minor differences yield comparable scores, making"}, {"title": "Random Greedy Algorithm", "content": "To address the limitations of PRM's scoring precision, we employ the Random Greedy algorithm. Since PRM provides a rough preference ranking rather than precise scores, randomness is introduced to mitigate the impact of PRM's scoring variability.\nThe Random Greedy algorithm prioritizes candidates with scores close to the maximum while incorporating randomness to mitigate PRM's imprecision. Candidates are filtered based on the condition:\n$P(@max) - P(a_i) < threshold$,\nwhere $P(@max)$ is the highest score, $P(a_i)$ is the score of candidate $a_i$, and $threshold$ is a predefined margin. From the filtered candidates, one is randomly selected to continue the search process."}, {"title": "EXPERIMENT", "content": "Given the varying performance levels of policy models across different scales, our objective is to maximize the accuracy of correct results by fully exploring every leaf node in the ToT structure, without requiring fine-tuning of the policy model. Because this approach ensures greater stability and a larger pool of experimental data for subsequent analyses. To achieve this, we design a set of baseline experiments on the StructuredOR, providing more reliable and consistent evaluations.\nWe first evaluate the CoT (Wei et al., 2022) approach, including two variations: CoT-BMLD, where the modeling process is performed first and data is imported later, and CoT-SPVOC, which adheres to the sequence of set, parameter, variable, objective, and constraint in the modeling process. Next, since the ToT (Yao et al., 2023) framework lacks a mechanism to select a final answer from all leaf nodes, we introduce the following configurations to address this limitation: ToT-randomly-chosen, where the final result is randomly selected from the leaf nodes; ToT-rethink, where all leaf nodes are provided to the LLM for reevaluation to produce a revised result; and ToT-fully-traverse, where every leaf node is thoroughly evaluated to ensure that at least one correct result can be generated. The detailed tree structure is shown in the Figure 2. Additionally, we include SC (Wang et al., 2023) as a baseline, which aims to obtain consistent results by sampling multiple reasoning paths.\nThe evaluated policy models include GPT-40 (Achiam et al., 2023), GPT-40-mini (Achiam et al., 2023), Llama-3-70B (Dubey et al., 2024), Llama-3.1-70B (Dubey et al., 2024), Llama-3.2-11B (Dubey et al., 2024), Qwen-2-72B (Yang et al., 2024a), Qwen-2.5-72B (Team, 2024), Qwen-2.5-Math-72B (Yang et al., 2024b), and Mixtral-"}, {"title": "Baseline", "content": "Given the varying performance levels of policy models across different scales, our objective is to maximize the accuracy of correct results by fully exploring every leaf node in the ToT structure, without requiring fine-tuning of the policy model. Because this approach ensures greater stability and a larger pool of experimental data for subsequent analyses. To achieve this, we design a set of baseline experiments on the StructuredOR, providing more reliable and consistent evaluations.\nWe first evaluate the CoT (Wei et al., 2022) approach, including two variations: CoT-BMLD, where the modeling process is performed first and data is imported later, and CoT-SPVOC, which adheres to the sequence of set, parameter, variable, objective, and constraint in the modeling process. Next, since the ToT (Yao et al., 2023) framework lacks a mechanism to select a final answer from all leaf nodes, we introduce the following configurations to address this limitation: ToT-randomly-chosen, where the final result is randomly selected from the leaf nodes; ToT-rethink, where all leaf nodes are provided to the LLM for reevaluation to produce a revised result; and ToT-fully-traverse, where every leaf node is thoroughly evaluated to ensure that at least one correct result can be generated. The detailed tree structure is shown in the Figure 2. Additionally, we include SC (Wang et al., 2023) as a baseline, which aims to obtain consistent results by sampling multiple reasoning paths.\nThe evaluated policy models include GPT-40 (Achiam et al., 2023), GPT-40-mini (Achiam et al., 2023), Llama-3-70B (Dubey et al., 2024), Llama-3.1-70B (Dubey et al., 2024), Llama-3.2-11B (Dubey et al., 2024), Qwen-2-72B (Yang et al., 2024a), Qwen-2.5-72B (Team, 2024), Qwen-2.5-Math-72B (Yang et al., 2024b), and Mixtral-"}, {"title": "Evaluation of BPP-Search", "content": "We evaluate our methods on the solvable problems identified in the datasets, as described in Section 5.1. Table 5 presents a comparison between our methods and baseline approaches, focusing on correct rate and reasoning steps. The results show that, under the condition where none of the methods fine-tune the policy model, our methods achieve superior performance with fewer reasoning steps, significantly outperforming baselines, as outlined in Section 4.2.\nThese experiments validate the feasibility of utilizing PRM to assist inference within the tree-of-thought structure. BPP-Search effectively addresses the limitations of traditional ToT methods, which struggle to reliably select a final result. As shown in Table 5, Greedy Search, Beam Search, and BPP-Search generate better results in significantly fewer steps, exponentially reducing compu-"}, {"title": "Ablation Study", "content": "Table 6 presents the performance of our methods under different configurations. It is evident that the PRM struggles to assign precise scores for regression tasks. For instance, as the beam search width increases, the accuracy tends to decrease, and in some cases, the performance of beam search becomes comparable to that of greedy search. Manual analysis of the beam search results in the final layer reveals that the candidate queue sometimes contains both correct and incorrect answers that are highly similar in structure, with only subtle differences. This similarity leads to comparable scores, making it challenging for the PRM to reliably distinguish between them.\nTo address this limitation, we introduce BPP-Search, which incorporates a pairwise preference algorithm. Instead of scoring candidates individually, BPP-Search evaluates all pairwise combinations of candidates within the final queue, comparing each pair and averaging the pairwise preference scores for each candidate. This approach ensures a more robust evaluation by reducing the bias inherent in relying solely on individual scores. The experimental results in Table 6 demonstrate that this method effectively mitigates the risks associated with the imprecise scoring of the PRM, resulting"}, {"title": "Conclusion", "content": "In this work, we introduce a new operations research dataset that integrates natural language questions with their corresponding detailed modeling processes, addressing the limitations of existing open-source datasets that lack comprehensive annotations of the modeling process. We further propose BPP-Search, an advanced algorithm that combines Beam Search, PRM, and a Pairwise Preference mechanism to enhance the ToT framework. BPP-Search effectively accelerates the reasoning process, improves accuracy, and alleviates the scoring imprecision of PRM, thereby ensuring robust and reliable decision-making. Comprehensive experiments conducted on StructuredOR, NL4OPT, and MAMO-ComplexLP datasets highlight the superiority of BPP-Search. Compared to state-of-the-art approaches, such as CoT, SC, and PRM integrated with Greedy or Beam Search, BPP-Search consistently achieves higher accuracy while requiring fewer reasoning steps, demonstrating its efficacy in addressing complex reasoning tasks in operations research."}, {"title": "Appendix", "content": "Tables 7, 8, 9, 10, 11, 12, 13, 14, 15 define the standardized data format for representing mathematical models. This convention provides a structured and consistent way to label and organize modeling data, ensuring clarity and usability across different tasks and datasets."}, {"title": "Operation Research Dataset Comparison", "content": "The StructuredOR dataset, as illustrated in Figure 5, provides not only the objective value but also the complete modeling process, offering a structured and transparent view of optimization problems. In contrast, as shown in Figure 6, the Mamo-ComplexLP (Huang et al., 2024) and IndustryOR (Tang et al., 2024b) datasets include only the objective value as the label, without detailing the modeling process. This limitation makes it difficult to verify the correctness of the data and prevents the application of reinforcement learning to intermediate steps. Similarly, the NL4OPT (Ramamonjison et al., 2022) dataset lacks both a structured modeling process and clear objective values, further complicating the interpretation and validation of results."}, {"title": "An Example to Illustrate Reasoning in Mathematical Modeling", "content": "Figure 7 illustrates the reasoning steps in the X-of-Thought (XoT) framework under the modeling structure defined in Appendix A.1. The process follows a structured sequence, starting with the question (Q), then progressing through sets (S), parameters (P), variables (V), objectives (O), and constraints (C). Each step builds upon the previous one, progressively transforming the natural language question into a fully defined mathematical model."}, {"title": "PRM Training Data Collection", "content": "To augment positive data for Process Reward Model training, we adopt the following four strategies:\n1. Utilizing ground truth: Segment the ground truth data into accumulative chunks corresponding to different layers of the reasoning process. This ensures that the hierarchical structure of the data is preserved.\n2. Leveraging LLM-generated data: Identify correctly generated data from LLMs operating under ToT, COT, and SC frameworks, and apply the same segmentation operations used for correct generated data. This approach expands the dataset with additional examples while ensuring consistency and alignment with the hierarchical structure.\n3. Swapping indices in summation constraints: Exchange indices within summation functions in constraints derived from ground truth data. This operation does not alter the final result, thereby introducing diversity while preserving correctness.\n4. Modifying inequalities: Swap the left-hand and right-hand sides of inequalities derived from ground truth data, and adjust the inequality signs accordingly (e.g., \u2018>=\u2018 becomes '<='). This operation creates valid variations of the data while maintaining correctness.\nTo augment incorrect data for Process Reward Model training, we apply the following strategies:\n1. Mismatch instance data: Replace the correct instance data with mismatched values. For example:\n\u2022 Modify the value of a parameter so that it no longer corresponds to the data of the set.\n\u2022 Delete or add random data to a 'set'.\n\u2022 Delete a column from a random dimension of a parameter.\n\u2022 Reshuffle the data of a random parameter.\n2. Incorrect format: Generate data using LLMs based on the training dataset, then select examples that cannot be used for modeling due to structural inconsistencies or formatting issues.\n3. Constraint modifications: Introduce errors in constraints or objectives by:\n\u2022 Changing a greater-than sign into a less-than sign.\n\u2022 Swapping the indices within a constraint.\n\u2022 Altering the summation domain of a constraint.\n\u2022 Randomly deleting a constraint."}]}