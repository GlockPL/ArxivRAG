{"title": "EVOLLAMA: Enhancing LLMs' Understanding of Proteins via Multimodal Structure and Sequence Representations", "authors": ["Nuowei Liu", "Changzhi Sun", "Tao Ji", "Junfeng Tian", "Jianxin Tang", "Yuanbin Wu", "Man Lan"], "abstract": "Current Large Language Models (LLMs) for understanding proteins primarily treats amino acid sequences as a text modality. Meanwhile, Protein Language Models (PLMs), such as ESM-2, have learned massive sequential evolutionary knowledge from the universe of natural protein sequences. Furthermore, structure-based encoders like ProteinMPNN learn the structural information of proteins through Graph Neural Networks. However, whether the incorporation of protein encoders can enhance the protein understanding of LLMs has not been explored. To bridge this gap, we propose EVOLLAMA, a multimodal framework that connects a structure-based encoder, a sequence-based protein encoder and an LLM for protein understanding. EVOLLAMA consists of a ProteinMPNN structure encoder, an ESM-2 protein sequence encoder, a multimodal projector to align protein and text representations and a Llama-3 text decoder. To train EVOLLAMA, we fine-tune it on protein-oriented instructions and protein property prediction datasets verbalized via natural language instruction templates. Our experiments show that EvOLLAMA's protein understanding capabilities have been significantly enhanced, outperforming other fine-tuned protein-oriented LLMs in zero-shot settings by an average of 1%-8% and surpassing the state-of-the-art baseline with supervised fine-tuning by an average of 6%. On protein property prediction datasets, our approach achieves promising results that are competitive with state-of-the-art task-specific baselines. We will release our code in a future version.", "sections": [{"title": "Introduction", "content": "The rapid advancements in Natural Language Processing (NLP) have led to the development of Large Language Models (LLMs) that are capable of understanding and generating human language. These models such as GPT-3.5 (OpenAI, 2022), GPT-4 (Achiam et al., 2023) and Llama (Touvron et al., 2023a,b; Dubey et al., 2024), inherently possess a certain level of world knowledge and have demonstrated remarkable proficiency across a wide range of tasks. Recently, the field of Bioinformatics has seen the emergence of Transformer-based (Vaswani et al., 2017) Protein Language Models (PLMs) like ProtBert (Elnaggar et al., 2021) and ESM (Rives et al., 2021; Lin et al., 2022). These sequence-based encoders are pre-trained on a large number of amino acid sequences to capture the functional information embedded within proteins. Moreover, structure-based encoders like ProteinMPNN (Dauparas et al., 2022) and GearNet (Zhang et al., 2022b) utilize Graph Neural Networks to learn the structural information of proteins.\nDespite the success of protein encoders and LLMs in their respective domains, a significant gap remains in integrating the knowledge from protein encoders into LLMs to address biological problems by leveraging the parametric knowledge of LLMs. Current LLMs treat amino acid sequences as a text modality (Pei et al., 2023; Fang et al., 2023), potentially failing to leverage the rich structural and sequential information of proteins that protein encoders are designed to capture. Moreover, protein encoders face challenges in multi-task learning, making them unable to follow human instructions. Besides, the gap between protein encoders and LLMs leads to significant challenges in aligning different modalities, even between the primary and tertiary structures of proteins (Zhang et al., 2023).\nTo address the aforementioned challenges, we introduce EvOLLAMA, a multimodal framework designed to integrate the capabilities of protein encoders with an LLM. EVOLLAMA combines the ESM-2 (Lin et al., 2022) protein sequence encoder, which captures sequential evolutionary knowledge from amino acid sequences, the ProteinMPNN (Dauparas et al., 2022) structure encoder that learns geometric features from 3D protein structures, a multimodal projector that aligns protein and text representations, and a Llama-3 (Dubey et al., 2024) text decoder for generating natural language outputs.\nWe propose a two-stage training approach, and the experimental results demonstrate that EvoLLAMA with zero-shot outperforms the baselines fine-tuned on the Mol-Instructions (Fang et al., 2023) dataset by an average of 1%-8% and surpasses the current state-of-the-art model with supervised fine-tuning by an average of 6%. Additionally, on protein property prediction tasks based on the PEER benchmark (Xu et al., 2022), EvoLLAMA shows promising results that are competitive with task-specific baselines.\nOur contributions are listed as follows:\n\u2022 Leverage multimodal representations of protein structures and sequences. We align protein structure and sequence representations with LLM text modalities, bridging the gap in limitations of protein encoders that are unable to directly exploit the advanced capabilities of LLMs. Our approach enhances LLMs' understanding of proteins, leveraging their parametric knowledge to address biological problems and laying a foundation for future research on incorporating a broader range of biomolecular modalities.\n\u2022 Multi-task learning and instruction following capability. We implement a two-stage training approach. After projection tuning, EVOLLAMA can follow various human instructions and solve downstream tasks in zero-shot settings. During the supervised fine-tuning stage, experiments demonstrate that tasks in the PEER benchmark have few interrelations and do not negatively affect each other when multi-task fine-tuning is employed.\n\u2022 Plug-and-play architecture and efficient fine-tuning approach. Different protein encoders and LLMs can be used in our plug-and-play architecture. Extensive experiments demonstrate that the projection tuning stage can be optional, with the frozen LLM parameters during supervised fine-tuning significantly reducing trainable parameters. Additionally, we introduce a simple yet effective fusion method to align structure and sequence representations, improving efficiency during both training and inference."}, {"title": "Related Work", "content": "Protein-oriented LLMs BioT5 (Pei et al., 2023) and BioT5+ (Pei et al., 2024) captures the underlying relations and properties of bio-entities such as molecules and proteins. ProLLaMA (Lv et al., 2024) introduces a training framework to transform a LLM into a multi-task protein LLM, focusing on protein sequence generation and superfamily prediction tasks. InstructProtein (Wang et al., 2024b) utilizes a knowledge graph based-generation framework to construct instructions. These methods utilize text-format protein sequences while EvoLLAMA focuses on leveraging multimodal representations of proteins. Prot2Text (Abdine et al., 2024) directly fuses the structure and sequence representations as inputs into the multi-head cross-attention module within the Transformer decoder. Compared to Prot2Text, EvOLLAMA maps the structure and sequence features into language embedding tokens, enabling it to handle PPI prediction tasks, which typically requires two proteins as inputs. Additionally, Prot2Text is designed to generate protein descriptions rather than handle various protein-oriented tasks, whereas EVOLLAMA can follow human instructions, even in zero-shot settings. Further related work is discussed in Appendix A.\nProtein Representations BERT-based PLMs such as ProtBert (Elnaggar et al., 2021), Protein-BERT (Brandes et al., 2022) and ESM (Rives et al., 2021; Lin et al., 2022) learn protein sequence representations through Masked Language Modeling objective. Gligorijevi\u0107 et al. (2021) propose a Graph Convolutional Network to encode protein structures. Zhang et al. (2022b) present a protein graph encoder to learn protein geometric features. Apart from these sequence-based protein encoders, some work employ Graph Neural Networks to learn the geometric features of proteins. Dauparas et al. (2022) introduce a protein sequence design method based on Message Passing Neural Network with 3 encoder and 3 decoder layers. We adopt the encoder layers of ProteinMPNN as a structure-based encoder in our approach. GearNet (Zhang et al., 2022b) performs relational message passing on protein residue graphs for protein representation learning. While these methods effectively learn the protein representations through sequences or structures, they do not utilize natural language with knowledge of protein properties. Therefore, EvoLLAMA aligns protein and text representations to enhance LLM's understanding of proteins."}, {"title": "Approach", "content": "EVOLLAMA aims to align protein information from both pre-trained structure-based and sequence-based protein encoders with an advanced LLM. Both language and protein models are open-sourced. We target to bridge the gap between the protein encoders and LLM using MLP projection layers (Sec. 3.1), with an overview of our model displayed in Fig. 1. To achieve an effective EvOLLAMA, we propose a two-stage training approach (Sec. 3.2). The initial stage involves pre-training the model on a large collection of aligned protein-text pairs to acquire protein language knowledge. In the second stage, we fine-tune the model with the high-quality protein-text dataset to enhance generation reliability and usability."}, {"title": "Architecture", "content": "In this section, we will introduce the overall EvOLLAMA in three parts: the protein encoders, the projection layer and the language decoder.\nProtein Encoders Given the input amino acid sequence $X_{seq}$, we consider the pre-trained protein encoder ESM-2 (Lin et al., 2022), which provides the protein feature $Z_{seq} = SeqEncoder(X_{seq})$. The 3D structure of the given amino acid sequence is predicted using AlphaFold-2 (Jumper et al., 2021) or ESMFold (Lin et al., 2022). A protein structure encoder, such as ProteinMPNN and GearNet, is used to extract the feature $Z_{struct} = StructEncoder(X_{struct})$.\nProjection Layer To map the outputs of the protein encoders into the same space as the text features from word embedding, we apply an MLP to convert $Z_{seq}$ and $Z_{struct}$ into language embedding tokens $H_{seq}$ and $H_{struct}$ separately, which have the same dimensionality of the word embedding space in the language model:\n$H_{seq} = MLP_{seq}(Z_{seq}),$\n$Z_{seq} = SeqEncoder(X_{seq}),$\n$H_{struct} = MLP_{struct}(Z_{struct}),$\n$Z_{struct} = StructEncoder(X_{struct})$\nFurthermore, since both structure-based and sequence-based protein encoders extract features based on residue positions, the lengths of their feature representations are dependent solely on the length of the amino acid sequence. Therefore, we fuse the structure and sequence features by employing an element-wise addition of the corresponding residue features. The fused protein representations $H_p = H_{seq} \\bigoplus H_{struct}$ reduces the protein embedding tokens by half, significantly decreasing the training and inference latency. Note that our simple projection scheme is lightweight and cost-effective, which allows us to iterate data centric experiments quickly. We leave exploring possibly more effective and sophisticated architecture designs for EVOLLAMA as future work.\nLanguage Decoder Given the protein structure $X_{struct}$, amino acid sequence $X_{seq}$ and the fused projected embeddings $H_p$, we have conversation data $(X_q, X_a)$, where $X_q$ and $X_a$ represent the protein-related question and its corresponding answer, respectively. We organize them as a sequence and perform instruction-tuning of the LLM on the prediction tokens, using its original auto-regressive training objective. Specifically, we compute the probability of generating target answers $X_a$ by:\n$p(X_a | X_{struct}, X_{seq}, X_q)$\n$=\\prod_{i=1}^{X_a} p_{\\theta}(X_{a,i} | X_{struct}, X_{seq}, X_q, X_{a,<i})$ (1)\nwhere $\\theta$ is the trainable parameters. EVOLLAMA model design is compatible with any off-the-shelf GPT-style pre-trained LLM. EVOLLAMA adopts Llama-3 8B (Dubey et al., 2024) for further training. A causal mask is applied to all the attention operations, including the attention between protein features $H_p$."}, {"title": "Training", "content": "As illustrated in Fig. 1, the training process of the EVOLLAMA model consists of two stages: projection tuning and supervised fine-tuning, with the first stage being optional."}, {"title": "Protein Instruction-Following Data", "content": "In this section, we introduce the construction of protein instruction-following data. It consists of two sets, projection tuning and supervised fine-tuning, which are used at different training stages, described in Sec. 3. Formally, for each example, we define $X_p, X_q, X_a$ as the protein (fused protein representations, consisting of the 3D structure $X_{struct}$ and the amino acid sequence $X_{seq}$), the protein-related natural language question, and the corresponding answer, respectively.\nProjection Tuning Data It consists of protein-text pairs originated from the Swiss-Prot (Consortium, 2023) database. Due to limited computational resources, we directly utilize the 3D structures predicted by AlphaFold-2 (Jumper et al., 2021) from Swiss-Prot. The database contains 571K manually-annotated records, each containing information including protein name, subcellular location, function and families. For $X_q$, we construct 10 templates that ask the model to briefly describe the input protein $X_p$ from various aspects. For $X_a$, information is extracted from the filtered Swiss-Prot annotation and constructed using a pre-defined template to ensure the consistency and clarity of protein descriptions. The question and answer templates are listed in Fig. 4.\nSupervised Fine-tuning Data To align the model to follow a variety of instructions, we present and curate diverse instruction-following data about the provided proteins, by verbalizing protein-related tasks. It consists of 10 tasks including Mol-Instructions (Fang et al., 2023) and PEER benchmark (Xu et al., 2022). We use ESMFold (Lin et al., 2022) to accelerate protein structure prediction for sequences in these two datasets.\n\u2022 Mol-Instructions is a comprehensive instruction dataset designed for the biomolecular realm. It includes three components: molecule-oriented instructions, protein-oriented instructions and biomolecular text instructions. We adopt protein-oriented instructions in Mol-Instructions (named PMol) for the supervised fine-tuning stage. PMol details will be discussed in Sec. 5.1. For each task in Mol-Instructions, we make simple modifications to the original prompts to fit our use cases and ensure coherence. Details are discussed in Appendix B.2 and some modification cases are listed in Fig. 7.\n\u2022 PEER is a comprehensive benchmark for general protein sequence understanding tasks including protein localization prediction, protein structure prediction and protein-protein interaction prediction. PEER benchmark details will be discussed in Sec. 5.2. For each task in PEER benchmark, there are 10 question templates and 1 answer template, some of which are listed in Fig. 6. Except for fold classification, the categories in the response templates for other tasks are represented by natural language. For fold classification, we use integers from 0 to 1,194 due to the large number of categories."}, {"title": "Experiments", "content": "We evaluate EvOLLAMA\u00b9 on downstream tasks including protein understanding tasks based on Mol-Instructions (Sec. 5.1) and protein property prediction tasks based on PEER benchmark (Sec. 5.2). Additionally, the structure encoder in our approach is replaced with GearNet to construct EVOLLAMA (GearNet+ESM-2) for the experiments, and further experiments on the substitution of protein sequence encoders are provided in Appendix E. We evaluate our approach in both zero-shot settings, where only the projection layers are aligned during the projection tuning stage, and in supervised fine-tuning without the projection tuning stage. Details of the experimental setups are discussed in Appendix C."}, {"title": "Protein Understanding Tasks", "content": "Task Descriptions Protein understanding tasks use PMol for fine-tuning and evaluation, which consist of four distinct tasks with datasets constructed based on UniProtKB (Consortium, 2021). Protein function prediction outputs the function of the given protein. Catalytic activity prediction outputs the catalytic activity of the input protein and the chemical reactions it promotes. Domain/motif prediction outputs the domains or motifs that the given protein may contain. Functional description generation outputs the description of the input protein's function, subcellular localization, and any biological processes it may be a part of.\nBaselines We compare our approach with the protein-oriented LLMs in Mol-Instructions including LLaMA (Touvron et al., 2023a), Alpaca (Tloen, 2023), Baize (Xu et al., 2023a), ChatGLM (Zeng et al., 2022), Galactica (Taylor et al., 2022) and Vicuna. Apart from these LLMs, we use Prot2Text (Abdine et al., 2024) and ProLLaMA (Lv et al., 2024) as our baseline models in zero-shot settings. These models lack support for arbitrary prompts. Prot2Text is designed to generate protein descriptions, while ProLLaMA predicts protein superfamilies. Therefore, we evaluate protein function prediction for Prot2Text and domain/motif prediction for ProLLaMA. For protein understanding tasks, we follow Mol-Instructions, taking ROUGE-L (Lin, 2004) as the evaluation metric. Details of ROUGE-L implementation are discussed in Appendix D.\nResults As shown in Tab. 1, EVOLLAMA and EVOLLAMA (GearNet+ESM-2) with zero-shot not only handle all protein understanding tasks but also outperform Prot2Text and ProLLaMA. Furthermore, they surpass or approach ChatGLM, Llama-2-7B-Chat and Vicuna fine-tuned on protein-oriented instructions by 1%-8%, demonstrating that during the projection tuning stage, EvOLLAMA and EVOLLAMA (GearNet+ESM-2) learn protein knowledge and can follow human instructions to generalize their knowledge for various downstream tasks. Additionally, EVOLLAMA outperforms all baseline models, including Llama-2-7B-Chat fine-tuned on the complete Mol-Instructions dataset (Mol) after supervised fine-tuning, highlighting the effectiveness of our approach. Notably, our approach uses a relatively small amount of data and has significantly fewer trainable parameters than the baseline models using full-parameter fine-tuning. The experimental results highlight the importance of leveraging the multimodal structure and sequence representations during training LLMs."}, {"title": "Protein Property Prediction Tasks", "content": "Task Descriptions Protein property prediction tasks use PEER benchmark for fine-tuning and evaluation, which consist of 6 tasks. Solubility prediction (Khurana et al., 2018), defined as a binary classification task, aims to predict whether a given protein is soluble or not. Subcellular localization prediction (Almagro Armenteros et al., 2017), defined as a ten-class classification task, aims to predict where a given protein locates in the cell. Binary localization prediction, a simplified version of subcellular localization prediction, is defined as a binary classification task that aims to determine whether a given protein is soluble or membrane-bound. Fold classification (Fox et al., 2014; Hou et al., 2018) requires the model to classify the global structural topology of a given protein into one of 1195 classes at the fold level. Yeast PPI prediction (Guo et al., 2008) and human PPI prediction (Peri et al., 2003; Pan et al., 2010) are defined as binary localization tasks, which aim to predict whether two given yeast or human proteins interact or not respectively. It is worth noting that for all tasks, protein sequences in the training set with high similarity to those in the test set are excluded based on the sequence identity. For example, sequences with \u2265 30% identity are excluded in the solubility prediction task. Therefore, a key challenge in protein property prediction tasks lies in evaluating a model's ability to generalize across dissimilar protein sequences.\nBaselines We compare our approach with the following baselines in PEER benchmark. Feature engineers include Dipeptide Deviation from Expected Mean (DDE) (Saravanan and Gautham, 2015) and Moran correlation (Moran) (Feng and Zhang, 2000). Protein sequence encoders include LSTM (Hochreiter and Schmidhuber, 1997), Transformer (Vaswani et al., 2017), shallow CNN (Shanehsazzadeh et al., 2020) and ResNet (He et al., 2016). Pre-trained PLMs include ProtBert (Elnaggar et al., 2021) and ESM-1b (Rives et al., 2021). Protein-oriented LLMs include Llama-3-8B-Instruct (Dubey et al., 2024) and InstructProtein (Wang et al., 2024b). For protein property prediction tasks, we take accuracy (Acc) as the evaluation metric."}, {"title": "Ablation Study", "content": "In this section, we conduct ablation studies to explore the effect of the projection tuning stage, structure and sequence representations, and the fusion method. For a fair comparison, the baselines are trained using the same experimental setups discussed in Appendix C with 10K steps. We conduct more evaluations on protein property prediction tasks in Appendix E, where we further discuss the effect of protein sequence encoder sizes.\nEffect of the projection tuning stage We compare the performance of EvoLLama with and without the projection tuning stage in Tab. 3. EVOLLAMA, when continued to be fine-tuned after the projection tuning stage, achieves lower performance compared to direct supervised fine-tuning. A possible reason is that we use protein structures predicted by AlphaFold-2 during the projection tuning stage, while structures predicted by ESMFold are used during the supervised fine-tuning stage. The zero-shot ability discussed in Sec. 5.1 and Sec. 5.2 highlights the effectiveness of the projection tuning stage, demonstrating that the model learns to generalize its structural knowledge from structures predicted by AlphaFold-2 to those predicted by ESMFold during inference. However, it's challenging for models to learn the gap between protein structures predicted by AlphaFold-2 and ESMFold when the structure encoder and projection layer are updated simultaneously during the supervised fine-tuning stage.\nEffect of structure and sequence representations As shown in Tab. 4, EVOLLAMA incorporating both structure and sequence representations outperforms those utilizing either alone, demonstrating the effectiveness of integrating both protein representations. EVOLLAMA (GearNet+ESM-2) significantly enhances the performance by leveraging structure and sequence representations. Furthermore, EVOLLAMA with only a sequence-based protein encoder surpasses the one with only a structure-based protein encoder, regardless of the protein structure encoder chosen. A possible explanation is that the features extracted by ESM-2 implicitly contain structural information, indicating that sequence representations are easier for LLMs to learn. Moreover, the parameters of the structure encoder are one to two orders of magnitude fewer than those of the sequence encoder, leading to a more limited extraction of structural features. A case study on the effect of structure and sequence representations is conducted in Appendix F.\nEffect of the fusion method To evaluate the effect of the fusion method, we directly use the structure and sequence features instead of fusing their representations. As shown in Tab. 5, EVOLLAMA with fused representations surpass the one without, demonstrating the effectiveness of our fusion method. However, EVOLLAMA (GearNet+ESM-2) without fused representations outperforms the one with them, indicating that for different protein structure encoders, different fusion methods may be chosen. Furthermore, the fused representations reduce the token cost of the LLM, resulting in approximately 20% lower inference latency."}, {"title": "Conclusion", "content": "In this paper, we propose EvoLLama, a multimodal framework that connects ProteinMPNN, ESM-2 650M and Llama-3 8B for protein understanding through a two-stage training process. Experiments demonstrate that after the projection tuning stage, EvOLLAMA in zero-shot settings outperforms the fine-tuned baselines with full parameters, surpassing the current state-of-the-art model with supervised fine-tuning on the Mol-Instructions. Additionally, our approach achieves promising results that are competitive with state-of-the-art task-specific baselines on the PEER benchmark."}, {"title": "Limitations", "content": "EVOLLAMA incorporates the structure and sequence representations of proteins to enhance LLM's understanding of proteins. Due to the lack of experimentally determined structures for many proteins in our experiments, we use 3D structures predicted by AlphaFold-2 and ESMFold to fully leverage the data. These computationally predicted structures generally have relatively lower accuracy compared to wet lab experiments. Besides, training a single model to predict various protein properties presents challenges, causing EVOLLAMA to only approach the state-of-the-art performance for some tasks in protein property prediction."}, {"title": "Dataset Construction Details", "content": "Projection Tuning Data\nAs illustrated in Fig. 3, projection tuning data consists of sequence-description pairs originated from the Swiss-Prot (Consortium, 2023) database. The database contains 571K manually-annotated records, each containing information including protein name, subcellular location, function and families. To prevent from data leakage, we filter the Swiss-Prot annotation to 369K as our projection tuning data based on the downstream tasks.\nFor prompts, we construct 10 templates that ask the model to briefly describe the input protein from various aspects. For responses, information is extracted from the filtered Swiss-Prot annotation and constructed using a pre-defined template to ensure the consistency and clarity of protein descriptions. The prompt and response templates are listed in Fig. 4.\nSupervised Fine-Tuning Data\nAs shown in Fig. 5, fine-tuning dataset consists of 10 tasks including PEER benchmark (Xu et al., 2022) and Mol-Instructions (Fang et al., 2023). For each task in PEER benchmark, there are 10 prompt templates and 1 response template, some of which are listed in Fig. 6. Except for fold classification, the categories in the response templates for other tasks are represented by natural language. For fold classification, we use integers ranging from 0 to 1194 to represent its categories due to the excessive number of categories.\nFor each task in Mol-Instructions, we make simple modifications to the original prompts to ensure that they are suitable for our use cases and maintain coherence. First, we remove the appended text-format FASTA sequences. Additionally, we modify some expressions in the original prompts. Some modification cases are listed in Fig. 7."}, {"title": "Experimental Setups", "content": "For protein understanding tasks, we follow Mol-Instructions to split the dataset into an 8:1:1 ratio for training/validation/test, where the training and validation sets are used for the supervised fine-tuning stage, and the test set is used for assessing model performance. For protein property prediction tasks, we follow the PEER benchmark to split the dataset for each task. The details of dataset splits are listed in Tab. 6. The results are averaged over three runs with different random seeds. Specifically, we follow the PEER benchmark to report the mean and standard deviation of three runs' results.\nWe conduct both the projection tuning stage and supervised fine-tuning stage on 80GB H800 GPUs. The experiments to evaluate the inference latency, reported in Tab. 5, are conducted on 24GB RTX 3090 GPUs. The hyperparameters are listed in Tab. 7."}, {"title": "Evaluation Implementation Details", "content": "For a fair comparison, we follow Mol-Instructions (Fang et al., 2023) to compute the ROUGE-L (Lin, 2004) score. Specifically, we take the complete references and predicted answers as inputs. However, both the references and predictions contain some non-protein-related parts, which are non-critical. In Fig. 8, we show that the critical parts are related with the domains, catalytic activities, and functions. To illustrate how the critical parts affect the ROUGE-L score, we exclude the non-critical parts, retaining only the domains/motifs, chemical reactions, and functional descriptions in both the reference and prediction. Note that the modifications are applied to three sub-tasks, with the exception of the protein function prediction task, where every part of the generation is critical to the protein functions.\nThe re-evaluation results are displayed in Tab. 8, demonstrating that by excluding the non-critical parts from both the references and predictions, EvOLLAMA still outperforms the previous state-of-the-art model by 5%. Compared to the 6% improvement discussed in Sec. 5.1, the impact of non-critical parts is relatively minor."}, {"title": "More Evaluations", "content": "We conduct more evaluations on protein property prediction tasks for each ablation study introduced in Sec. 5.3."}, {"title": "Effect of structure and sequence representations", "content": "As shown in Tab. 10, EVOLLAMA and EVOLLAMA (GearNet+ESM-2) enhances the performance on protein understanding tasks by incorporating both structure and sequence representations, specifically on protein function prediction and catalytic activity prediction."}, {"title": "Effect of the fusion method", "content": "As shown in Tab. 11, EvoLLama with fused representations outperforms the one without, particularly on subcellular localization prediction task, while EVOLLAMA (GearNet+ESM-2) achieves better performance when the structure and sequence representations are not fused. This demonstrates that the fusion method is more effective when ProteinMPNN is used as the structure encoder."}, {"title": "Effect of protein sequence encoder sizes", "content": "The ESM-2 650M protein sequence encoder in EvOLLAMA is substituted with encoders of different sizes to demonstrate that the scaling law observed by Lin et al. (2022) extends to our multimodal framework. The experimental results in Fig. 9(a) indicate that performance on protein understanding tasks improves as the size of the protein sequence encoder increases. Furthermore, EVOLLAMA employing ESM-2 with only 8M parameters outperforms Llama-2-7B-Chat fine-tuned on PMol by an average of 27%. This result highlights the effectiveness of our approach, as the protein sequence encoder effectively captures evolutionary knowledge from amino acid sequences, substantially enhancing the LLM's understanding of proteins. We also conduct experiments on protein property prediction tasks, as illustrated in Fig. 9(b). The results show positive accuracy scaling across most tasks, with the exception of fold classification. A possible explanation is that the limited amount of training data makes it challenging for our multimodal architecture to effectively learn the distinctions among the 1,195 fold levels."}, {"title": "Case Study", "content": "As shown in Fig. 10, we compare the outputs of EVOLLAMA, EVOLLAMA (GearNet+ESM-2) and Mol-Instructions on domain/motif prediction task. Only EVOLLAMA correctly predicts all possible domains or motifs of the given protein, regardless of whether structure or sequence representations are incorporated. Compared to EVOLLAMA, Mol-Instructions fails to predict all the domains or motifs while EvOLLAMA (GearNet+ESM-2) generates incorrect ones, indicating that the structure representations extracted by GearNet fail to capture domain- or motif-related information."}]}