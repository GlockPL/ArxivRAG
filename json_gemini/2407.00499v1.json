{"title": "ConU: Conformal Uncertainty in Large Language Models with Correctness Coverage Guarantees", "authors": ["Zhiyuan Wang", "Jinhao Duan", "Lu Cheng", "Yue Zhang", "Qingni Wang", "Hengtao Shen", "Xiaofeng Zhu", "Xiaoshuang Shi*", "Kaidi Xu*"], "abstract": "Uncertainty quantification (UQ) in natural language generation (NLG) tasks remains an open challenge, exacerbated by the intricate nature of the recent large language models (LLMs). This study investigates adapting conformal prediction (CP), which can convert any heuristic measure of uncertainty into rigorous theoretical guarantees by constructing prediction sets, for black-box LLMs in open-ended NLG tasks. We propose a sampling-based uncertainty measure leveraging self-consistency and develop a conformal uncertainty criterion by integrating the uncertainty condition aligned with correctness into the design of the CP algorithm. Experimental results indicate that our uncertainty measure generally surpasses prior state-of-the-art methods. Furthermore, we calibrate the prediction sets within the model's unfixed answer distribution and achieve strict control over the correctness coverage rate across 6 LLMs on 4 free-form NLG datasets, spanning general-purpose and medical domains, while the small average set size further highlights the efficiency of our method in providing trustworthy guarantees for practical open-ended NLG applications.", "sections": [{"title": "1 Introduction", "content": "Despite advancements in various natural language generation (NLG) tasks like question answering (QA) (Katz et al., 2024; Touvron et al., 2023a; Chen et al., 2023), large language models (LLMs) are proven to hallucinate facts and confidently generate textual information that is not correct or grounded in reality (Ji et al., 2023; Manakul et al., 2023). Factually incorrect answers can confuse and mislead users, resulting in erroneous conclusions and ultimately undermining the trustworthiness of LLMs-based high-stakes applications.\nUncertainty quantification (UQ) provides valuable insights into the reliability of model responses, facilitating risk assessment and hallucination detection (Kadavath et al., 2022; Lin et al., 2022a). However, it demands investigating black-box uncertainty measures with the proliferation of LLMs served via APIs (Achiam et al., 2023), which only allows textual inputs and outputs. Conformal prediction (CP) (Campos et al., 2024; Angelopoulos and Bates, 2021; Quach et al., 2023) is known for providing a model-agnostic and statistically rigorous uncertainty estimation. CP was primarily employed in classification (Angelopoulos and Bates, 2021) and regression tasks (Wang et al., 2024a). For NLG tasks, CP is first adapted to the multiple-choice question-answering (MCQA) setting, where the correct response is selected from a fixed set of options (Kumar et al., 2023; Ye et al., 2024), limiting its applications in real-world open-ended NLG tasks. Conformal language modeling (Quach et al., 2023) relies on the model likelihoods and calibrates a stopping rule to sample prediction sets from the infinite output space until users are confident that the set covers at least one response satisfied. LofreeCP (Su et al., 2024) studies CP for API-only LLMs without logit access by leveraging uncertainty information from diverse sources.\nOur study explores adapting CP for general NLG applications. The nonconformity score (NS) in CP serves as a criterion for calibrating prediction sets, which provide coverage guarantees by selecting a set of possible labels that satisfy the NS threshold (Angelopoulos and Bates, 2021). Since typical logits-based NS may encounter miscalibration, we aim to integrate black-box UQ into the definition of NS, by closely aligning it with the uncertainty condition of the correct answers and devising a conformal uncertainty criterion, while it is more trustworthy to analyze the uncertainty within LLMs' true output space. Then, we can leverage the uncertainty criterion, concluded from a small amount of independent and identically distributed (i.i.d.) calibration data, to construct prediction sets by selecting generations sharing a similar uncertainty condition from the unbounded output space on test samples. Typically, there are two goals of CP: (1) the calibrated prediction set contains the correct answer with at least a user-specified probability; and (2) the average set size should be small, demonstrating the prediction efficiency of our method.\nThe first challenge is UQ for black-box LLMs. Our solution is inspired by an intuitive observation: If a language model generates more semantically diverse outputs for the same prompt, the uncertainty is likely higher (Su et al., 2024; Lin et al., 2023; Xiong et al., 2023). Regardless of the model's capability to tackle the current problem, the confidence score that the model assigns to a generation can be represented by its frequency within the output space. We approximate the model's output distribution by sampling multiple answers to the same question. Then, we perform semantic clustering on the sampled generations, and propose to measure the uncertainty of each generation by combining two factors: the frequency of occurrence of the semantic meaning it conveys, and the consistency between its semantic and other semantic clusters augmented by their individual frequency.\nBased on the measure, we define NS as the uncertainty score of the generation. To this end, the generation meets the correctness criterion and is semantically most similar to the reference answer in the calibration set. We then calculate the quantile $\\hat{q}$ of NSs for all calibration samples, based on the user-specified upper bound of error rate $\\alpha$. Next, we utilize the conformal uncertainty criterion (i.e., the uncertainty threshold $\\hat{q}$) to construct a prediction set for each test sample by selecting generations that satisfy the uncertainty conditions strictly associated with correctness from the candidate generations. Additionally, for black-box UQ, we propose employing the most frequent generation or semantic (i.e., the model's most confident answer) as a more trustworthy reference object for the query and leveraging it to measure the overall uncertainty of the current query-answering process. We term this measure $ConU$, as it employs the same approach as the conformal uncertainty criterion.\nExtensive experimental results exhibit that $ConU$ generally outperforms prior state-of-the-art methods and verify the strict correctness coverage guarantees. Specifically, the prediction sets calibrated by the conformal uncertainty criterion always encompass the correct answers under various user-specified error rates. Furthermore, the average prediction set size is small, highlighting the prediction efficiency of our approach. To our knowledge, this is the first method in the literature to strictly link the NS with the uncertainty condition aligned with correctness via black-box UQ, thereby developing a more robust conformal uncertainty criterion, which provides rigorous correctness coverage guarantees in practical open-ended NLG tasks, and its unique inspiration in benchmarking UQ in LLMs through CP generates independent interest.\nIn summary, our major contributions are listed as follows:\n\u2022 We propose a sampling-based black-box uncertainty measure, termed as $ConU$, utilizing self-consistency in free-form NLG tasks, facilitating trustworthy decision-making.\n\u2022 We devise a conformal uncertainty criterion by strictly aligning the NS with the uncertainty condition of correct answers, and achieve rigorous correctness coverage with at least a user-specified probability, thereby providing robust guarantees under various error rates in practical open-ended NLG applications.\n\u2022 We conduct selective prediction leveraging the calibrated prediction sets and obtain promising improvements in model accuracy without requiring additional task-specific fine-tuning or architectural modifications."}, {"title": "2 Related Work", "content": "2.1 Uncertainty Quantification in LLMs\nPrior work on UQ in LLMs predominantly focuses on white-box information like token-likelihoods or embeddings (Kuhn et al., 2023; Duan et al., 2024; Wang et al., 2024b), internal state or activations (Yin et al., 2024; Chen et al., 2024), model fine-tuning (Lin et al., 2022a; Tian et al., 2023). These methods can encounter poor calibration and require substantial computational resources. Additionally, researchers lack white-box access to the internal information of LLMs served via APIs. These restrictions demand black-box measures for general UQ in LLMs generations.\nRecent work (Lin et al., 2023) develops several sampling-based uncertainty measures, which can be applied to black-box LLMs by leveraging semantic similarity along with dispersion. Our study follows the sampling setting and proposes to employ the most frequent generation as the reference object to measure the overall uncertainty based on the self-consistency theory (Wang et al., 2022).\n2.2 Conformal Prediction in LLMS\nCP (Angelopoulos and Bates, 2021; Quach et al., 2023; Campos et al., 2024) has emerged as a theoretically sound and practically useful way to guarantee ground-truth coverage with the aid of a small amount of independent and identically distributed (i.i.d.) samples for calibration. CP in classification defines NS that is correlated with the ground-truth label, obtains the quantile $\\hat{q}$ of NSs for all calibration samples based on a user-specified error rate $\\alpha$, and utilizes $\\hat{q}$ as a threshold to select possible labels on test samples, thereby establishing prediction sets that achieve ground truth coverage with at least the probability of $1-\\alpha$.\nRecently, researchers have attempted to apply CP to LLMs for principled UQ. The work (Mohri and Hashimoto, 2024) achieves conformal factuality guarantees by progressively making generations less specific and establishing their corresponding entailment sets until correct answers are encompassed. For correctness coverage, two studies (Kumar et al., 2023; Ye et al., 2024) follow CP in classification tasks and convert NLG tasks into MCQA settings. For open-ended NLG, based on the output token sequence logits, the study (Quach et al., 2023) devises a stopping rule to sample generations until users are confident that a correct answer is covered in QA tasks, which can be impractical for API-only LLMs. LofreeCP (Su et al., 2024) leverages uncertainty information to construct prediction sets that achieve correctness coverage.\nThis paper focuses on more practical scenarios of black-box LLMs in open-ended NLG tasks. Differing from LofreeCP, we strictly connect the NS with the uncertainty condition aligned with correctness via black-box UQ, which concludes a more robust conformal uncertainty criterion to calibrate prediction sets with rigorous correctness coverage guarantees under various error rates despite the complexity of the model or datasets."}, {"title": "3 Method", "content": "Our method investigates two key issues: (1) how to quantify the uncertainty in black-box LLMs when we can only access the generated texts; and (2) how to provide rigorous guarantees on the error rate in open-ended NLG tasks. We first devise a black-box uncertainty measure grounded in self-consistency to provide the trustworthiness notion of model responses. Furthermore, we utilize the CP technique to convert the heuristic approximation into a statistically rigorous one, thereby ensuring a more robust and systematic assessment of uncertainty.\n3.1 Preliminaries\nFollowing prior utility of black-box LLMs (Xiong et al., 2023; Lin et al., 2023; Manakul et al., 2023), conditioned on each prompt (or question) $x_i$, we employ the most likely generation $\\hat{y}_i$ for correctness evaluation. Additionally, we sample a set of $M$ candidate generations $\\{\\hat{y}_m^{(i)}\\}_{m=1}^M$ from the model's output space for black-box UQ and the derivation of conformal uncertainty criterion. We denote the reference answer to $x_i$ as $y_i^*$.\n3.2 Uncertainty Quantification\nFor each sample, we first cluster semantics in the $M$ sampled generations and obtain $K$ non-repeated semantics. We denote the number of generations sharing the $k$-th semantic as $V_k$ (i.e., $\\sum_{k=1}^K V_k = M$) and any one generation in this cluster as $\\hat{y}_k^{(i)}$.\nBuilding on earlier approaches that utilize self-consistency (Wang et al., 2022; Su et al., 2024; Yadkori et al., 2024) as a reliable measure of confidence, we employ the frequency of the $k$-th semantic as its proxy for reliability: $F(\\hat{y}_k^{(i)}) = \\frac{V_k}{M}$. Then, we define the uncertainty score of each candidate generation in $\\{\\hat{y}_m^{(i)}\\}_{m=1}^M$ as\n$$\nu(\\hat{y}_m^{(i)}) = 1 - \\lambda \\cdot F(\\hat{y}_m^{(i)}) - (1-\\lambda) \\cdot  \\frac{1}{K} \\sum_{k=1}^K (F(\\hat{y}_k^{(i)}) \\cdot S(\\hat{y}_m^{(i)}, \\hat{y}_k^{(i)})),$$\nwhere $F(\\hat{y}_m^{(i)})$ refers to the frequency of the semantic that $\\hat{y}_m^{(i)}$ conveys, and $S(\\cdot, \\cdot)$ measures the semantic similarity between two generations utilizing a cross-encoder model (Reimers and Gurevych, 2019). $F(\\hat{y}_k^{(i)})$ is to augment the persuasiveness of the similarity score associated with $\\hat{y}_k^{(i)}$.\nTo measure the overall uncertainty, we randomly select one generation in the largest semantic cluster to be the most trustworthy generation in the $M$ sampled generations and denote it as $\\hat{y}_{mst}^{(i)}$:\n$$\\hat{y}_{mst}^{(i)} = \\underset{\\hat{y}_k^{(i)} \\in \\{\\hat{y}_m^{(i)}\\}_{m=1}^M}{\\operatorname{argmax}} V_k.$$\nThen, we define the uncertainty score of the $i$-th query-response as\n$$U(\\hat{y}_m^{(i)} \\in \\{\\hat{y}_m^{(i)}\\}_{m=1}^M) = 1 - \\lambda \\cdot F(\\hat{y}_{mst}^{(i)}) - (1-\\lambda) \\cdot  \\frac{1}{K} \\sum_{k=1}^K S(\\hat{y}_{mst}^{(i)}, \\hat{y}_k^{(i)}) \\cdot F(\\hat{y}_k^{(i)}).$$ Intuitively, the most frequent semantic within the candidate generations represents the model's most confident answer to the current problem. Even though the reference semantic may not necessarily be the correct one, we can measure the degree of the model's uncertainty by calculating the confidence level of that semantic as well as the deviation between it and other semantics.\nSince Eq. (1) can quantify the uncertainty of each candidate generation, we attempt to develop an uncertainty criterion to search for the correct answers within the unfixed output space of the LLM.\n3.3 Conformal Correctness Coverage\nFollowing the fundamental requirement in CP (Angelopoulos and Bates, 2021), we randomly employ $N$ samples to construct the calibration data set $\\{(x_i, y_i^*)\\}_{i=1}^N$, and for each calibration sample we demand that at least one sampled generation $\\hat{y}_m^{(i)}$ in $\\{\\hat{y}_m^{(i)}\\}_{m=1}^M$ meets the correctness criterion. Our objective of conformal correctness coverage is by concluding the uncertainty criterion that is closely linked with correctness on $\\{(x_i, y_i^*)\\}_{i=1}^N$, we can calibrate an uncertainty (prediction) set $P(x_{test})$ for the test prompt $x_{test}$ by selecting generations that meet the common uncertainty condition, and the set can guarantee correctness coverage under various user-specificed error rates. Here, we approximate the prediction region of $x_{test}$ to the $M$ candidate generations $\\{\\hat{y}_m^{(test)}\\}_{m=1}^M$.\nAssumptions: (1) There is at least one candidate generation in $\\{\\hat{y}_m^{(test)}\\}_{m=1}^M$ meeting the correctness criterion; (2) Samples in the calibration and test data sets are exchangeable.\nAs the sampled set $\\{\\hat{y}_m^{(test)}\\}_{m=1}^M$ is a subset of the prediction region, which is impossible to enumerate, we can simplify it by stating that there is at least one correct answer in $\\{\\hat{y}_m^{(test)}\\}_{m=1}^M$. Exchangeability is the fundamental assumption of CP techniques (Angelopoulos and Bates, 2021).\nBased on the uncertainty measure described as Eq. (1), we define the NS of the $i$-th calibration sample as\n$$r_i = r(x_i, y_i^*) = U(\\underset{\\hat{y}_j \\in \\{\\hat{y}_m^{(i)}\\}_{m=1}^M}{\\operatorname{argmax}} S(\\hat{y}_j, y_i^*) \\cdot \\epsilon(\\hat{y}_j, y_i^*)),$$\nwhere $\\epsilon(\\cdot, \\cdot)$ is the indicator function determining whether the two sentences share equivalent semantics, i.e., $\\epsilon(\\hat{y}_i, y_i^*) = 1$ indicates that $\\hat{y}_i$ is semantically equivalent to $y_i^*$, and $\\epsilon(\\hat{y}_i, y_i^*) = 0$ denotes it does not. This is, the NS, $r(x_i, y_i^*)$ represents the uncertainty condition of the candidate generation $\\hat{y}_j$, which has the highest similarity score with the reference answer $y_i^*$ in generations that are semantically equivalent to $y_i^*$. The criterion for determining semantic equivalence here is the same as that for correctness evaluation (i.e., $\\hat{y}_j$ is correct according to $y_i^*$ if $\\epsilon(\\hat{y}_j, y_i^*) = 1$).\nIt is worth emphasizing that we strictly align the NSs with the uncertainty conditions of correct answers within the fresh calibration set, concluding an honest insight into the model's performance, which is crucial for robust correctness coverage guarantees in new test samples.\nFollowing prior work (Angelopoulos and Bates, 2021; Quach et al., 2023; Campos et al., 2024), we sort $\\{r_i\\}_{i=1}^N$ ($r_1 \\leq ... \\leq r_N$) and calculate the $\\lceil (N+1)(1-\\alpha) \\rceil$-th quantile of NSs for all calibration data to develop the conformal uncertainty criterion\n$$\\hat{q} = \\inf \\{q : |\\{i : r_i \\leq q\\}| \\geq \\lceil (N+1) (1 - \\alpha) \\rceil \\},$$\nwhere $\\alpha$ is the upper bound of the error rate.\nAs for each test sample, we construct the prediction set following\n$$P(x_{test}) = \\{\\hat{y}_m^{(test)} \\in \\{\\hat{y}_m^{(test)}\\}_{m=1}^M : r(\\hat{y}_m^{(test)}, y_m^{(test)}) \\leq \\hat{q}\\}.$$ It is evident that the most semantically similar generation to $\\hat{y}_m^{(test)}$ in $\\{\\hat{y}_m^{(test)}\\}_{m=1}^M$ is itself, and we obtain $r(x_{test}, \\hat{y}_m^{(test)}) = U(\\hat{y}_m^{(test)})$. Recall the assumption that $\\{\\hat{y}_m^{(test)}\\}_{m=1}^M$ contains at least one correct generation (i.e., $y_{test} \\in \\{\\hat{y}_m^{(test)}\\}_{m=1}^M$), then the event $\\{y_{test} \\in P(x_{test})\\}$ is equivalent to $\\{r_{test} = r(x_{test}, y_{test}) \\leq \\hat{q}\\}$.\nSince the calibration and test samples $(x_1, y_1^*), ..., (x_N, y_N^*), (x_{test}, y_{test})$ are exchangeable, we have $P(r_{test} \\leq r_i) = \\frac{i}{N+1}$. Then we conclude\n$$P(y_{test} \\in P(x_{test})) = P(r_{test} \\leq r_{\\lceil (N+1) (1 - \\alpha) \\rceil}) = \\frac{\\lceil (N + 1) (1 - \\alpha) \\rceil}{N+1} \\geq 1 - \\alpha,$$\nand obtain the user-specified lower bound (i.e., $1 - \\alpha$) of the correctness coverage rate guaranteed by these calibrated prediction sets."}, {"title": "4 Evaluations", "content": "4.1 Experimental Set-up\nBaselines. We consider 8 baseline methods, including 4 white-box methods: Predictive Entropy (PE) (Kadavath et al., 2022), Length-normalized Predictive Entropy (LNPE) (Malinin and Gales, 2020), Semantic Entropy (SE) (Kuhn et al., 2023), and Shift Attention to Relevance (SAR) (Duan et al., 2024), and 4 black-box approaches: Lexical Similarity (LS) (Lin et al., 2022b) and Number of Semantic Sets (NumSet) (Kuhn et al., 2023; Lin et al., 2023). Moreover, we also include the most recent state-of-the-art uncertainty quantification methods, Degree Matrix (Deg) (Lin et al., 2023), and Eccentricity (Ecc) (Lin et al., 2023). More details of baseline methods can be found in Appendix B.1.\nBase LLMs. We conduct experimental evaluations on 6 open-source LLMs encompassing various sizes and architectures for comprehensive analysis, including LLaMA-2-7B-Chat (Touvron et al., 2023b), Mistral-7B-Instruct-v0.3 (Jiang et al., 2023), Llama-3-8B-Instruct (AI@Meta, 2024), Vicuna-13B-v1.5 (Zheng et al., 2023), LLaMA-2-13B-Chat (Touvron et al., 2023b), LLaMA-3-70B-Instruct (AI@Meta, 2024). We utilize the default generation configs and checkpoints provided by the HuggingFace platform* for all models.\nDatasets. We evaluate the performance of $ConU$ and verify the correctness coverage guarantees on 4 free-form NLG datasets, including CoQA (Reddy et al., 2019) for conversational QA task, TriviaQA (Joshi et al., 2017) for reading comprehension, MedQA (Jin et al., 2021) for solving medical problems, and MedMCQA (Pal et al., 2022) for medical entrance exam questions. More details of datasets can be found in Appendix B.2.\nEvaluation Metric. Following prior work (Duan et al., 2024; Wang et al., 2024b), we evaluate the performance of UQ by treating it as the problem of predicting whether to trust a generation given the prompt, and utilize the Area Under the Receiver Operating Characteristic Curve (AUROC) which gauges if the uncertainty scores can effectively distinguish between correct and incorrect generations. To verify if the correctness coverage is strictly guaranteed, we evaluate the coverage rate under various user-specified error rates. We also report the average prediction set size to evaluate the prediction efficiency and practicality of our approach.\nCorrectness and Equivalence Metric. We utilize sentence similarity (Duan et al., 2024) as the metric for correctness and equivalence evaluation. We employ the cross-encoder model (Reimers and Gurevych, 2019) with DistillRoBERTa (Sanh et al., 2019) serving as the backbone to measure the semantic similarity score between the most likely generation and reference answer and set a strict correctness threshold of 0.7.\nHyperparameters. We randomly sample 5 answers to each question for UQ and 10 candidate generations for verification of correctness coverage guarantees. We leverage beam search for the most likely generations for correctness evaluation and multinominal sampling for candidate generations (Duan et al., 2024). The max length of each generation is set to 128 tokens. The temperature of generation is set to 1.0. The coefficient $\\Lambda$ introduced in Eq. (1) is set to 0.5. The ratio of calibration and test set is set to 1:10 by default.\n4.2 UQ in Black-Box LLMS\nAs defined in failure prediction (Xiong et al., 2023) which evaluates whether the uncertainty score can effectively distinguish between correct and incorrect generations, an effective measure should assign higher uncertainty to incorrect generations and lower to correct ones. We compare our approach with state-of-the-art methods utilizing AUROC. Experimental results are summarized in Table 1. Generally, our method outperforms baseline methods in most of the settings. For instance, our method consistently beat 8 baseline methods on the TriviaQA datasets. It is worth noting that our method outperforms other methods by at most 2.4% AUROC on the MedMCQA dataset and 1.29% AUROC on the MedQA, which indicates the potential impacts of our methods on real-world high-stakes NLG applications. We will discuss the impact of the number of sampled generations on UQ in Section 4.4.\n4.3 Conformal Correctness Coverage\nIn this section, we verify that the calibrated prediction sets constructed following Eq. (6) indeed achieve rigorous correctness coverage guarantees under various user-specified error rates as described in Eq. (7). Then we explore the utility of prediction sets and conduct selective prediction based on our proposed uncertainty measure.\nEmpirical Coverage Guarantees. To guarantee the derived lower bound of correctness coverage rate in practice, we randomly split the four datasets at a ratio of 1:10, employing the respective portions as the calibration and test set. We utilize the calibration set to derive the conformal uncertainty criterion specified by the upper bound of the error rate. Then, we measure the correctness coverage rate on the test set and plot the results on four datasets in Figure 1. It is evident that we achieve strict control of the correctness coverage rate under various error rates. The verification on other models can be found in Appendix C.\nFollowing the study (Ye et al., 2024), we set the error rate $\\alpha$ to 0.1 and test the coverage rate on 4 datasets utilizing 6 LLMs with multiple scales. As is exhibited in Table 2, the coverage rate is at least 90%, indicating that the requirement of correctness coverage guarantees is satisfied. It is worth noting that prior work (Ye et al., 2024; Kumar et al., 2023) selects the possible option from the fixed choices while we characterize the unbound answer distribution by sampling and utilize our devised conformal uncertainty criterion to search for the correct answer, which is more practical.\nWe also evaluate the prediction efficiency of the conformal uncertainty criterion utilizing the average size of these calibrated prediction sets, which is the primary metric for CP (Angelopoulos and Bates, 2021). Table 3 demonstrates that the average size of prediction sets calibrated by our method remains very small across the 4 datasets. For instance, the average set size is 1.03 on the LLaMa-3-70B-Instruct model in the TriviaQA task, indicating that we can almost directly identify the correct answers through these calibrated prediction sets.\nWe boldly expect that as long as the language model has the capability to solve the current problem, despite the unfixed answer distribution, we can always find the correct generation by performing black-box UQ on each sampled answer and searching for answers meeting the conformal uncertainty criterion, and then limit the selection region to the calibrated prediction set for post-processing.\nUtility of Calibrated Prediction Sets. Since for some test samples, all the candidate generations can be filtered out by the conformal uncertainty criterion, we explore the utility of non-empty prediction sets in practice."}, {"title": "4.4 Ablation Studies", "content": "Considering that these sampling-based methods integrate multiple generations within the candidate set, We investigate the effects of the number of sampled generations (i.e., $M$) on the performance of UQ. As illustrated in Figure 3, our uncertainty measure consistently outperforms the baseline approaches, and its performance can be further boosted by incorporating more generations. While employing just 4 generations, our method is able to achieve the highest AUROC of 0.8082, demonstrating its generation-efficient nature.\nAs described in Section 3.3, conformal prediction assumes a calibration set for the threshold $\\hat{q}$. In our prior analysis, We divide the dataset into the calibration and test set at a fixed ratio of 1:10. Here, we investigate the correctness coverage rate at different ratios of size between the calibration and test set, and present the results in Figure 4. Despite various ratios of set size, we can always obtain a strict lower bound of the coverage rate by constructing prediction sets based on our devised conformal uncertainty criterion. This indicates the potential impacts of our method for robust guarantees in real-world open-ended NLG applications."}, {"title": "5 Conclusion", "content": "In this work, we introduce $ConU$ tailored for black-box UQ in open-ended NLG tasks. Relying on CP which can transform any heuristic approximation into a statistically rigorous uncertainty notion, we develop a robust conformal uncertainty criterion to provide reliable guarantees of correctness coverage under various user-specified error rates. We achieve strict control of the coverage rate across 6 practical LLMs on 4 free-from NLG datasets. Furthermore, the small average uncertainty set size underscores the efficiency of our methods. Utilizing these calibrated prediction sets, we perform selective prediction and obtain remarkable improvements in model accuracy. We envisage that our conformal uncertainty criterion can provide new strategies for principled UQ in open-ended NLG tasks."}, {"title": "Limitations", "content": "Our approach has some limitations. In our study, we assume that at least one correct answer exists in the candidate generations. However, we need to develop a criterion to verify whether the correct answer has been sampled from the unbound output space in real-world applications. Secondly, our findings are limited to the four datasets and future works will extend to other typical NLG tasks like document summarization. Finally, we will attempt to expand our conformal uncertainty criterion to non-exchangeability scenarios, aiming to establish a general criterion across different NLG tasks."}]}