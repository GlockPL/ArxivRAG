{"title": "ConU: Conformal Uncertainty in Large Language Models with Correctness Coverage Guarantees", "authors": ["Zhiyuan Wang", "Jinhao Duan", "Lu Cheng", "Yue Zhang", "Qingni Wang", "Hengtao Shen", "Xiaofeng Zhu", "Xiaoshuang Shi", "Kaidi Xu"], "abstract": "Uncertainty quantification (UQ) in natural language generation (NLG) tasks remains an open challenge, exacerbated by the intricate nature of the recent large language models (LLMs). This study investigates adapting conformal prediction (CP), which can convert any heuristic measure of uncertainty into rigorous theoretical guarantees by constructing prediction sets, for black-box LLMs in open-ended NLG tasks. We propose a sampling-based uncertainty measure leveraging self-consistency and develop a conformal uncertainty criterion by integrating the uncertainty condition aligned with correctness into the design of the CP algorithm. Experimental results indicate that our uncertainty measure generally surpasses prior state-of-the-art methods. Furthermore, we calibrate the prediction sets within the model's unfixed answer distribution and achieve strict control over the correctness coverage rate across 6 LLMs on 4 free-form NLG datasets, spanning general-purpose and medical domains, while the small average set size further highlights the efficiency of our method in providing trustworthy guarantees for practical open-ended NLG applications.", "sections": [{"title": "1 Introduction", "content": "Despite advancements in various natural language generation (NLG) tasks like question answering (QA) (Katz et al., 2024; Touvron et al., 2023a; Chen et al., 2023), large language models (LLMs) are proven to hallucinate facts and confidently generate textual information that is not correct or grounded in reality (Ji et al., 2023; Manakul et al., 2023). Factually incorrect answers can confuse and mislead users, resulting in erroneous conclusions and ultimately undermining the trustworthiness of LLMs-based high-stakes applications.\nUncertainty quantification (UQ) provides valuable insights into the reliability of model responses, facilitating risk assessment and hallucination detection (Kadavath et al., 2022; Lin et al., 2022a). However, it demands investigating black-box uncertainty measures with the proliferation of LLMs served via APIs (Achiam et al., 2023), which only allows textual inputs and outputs. Conformal prediction (CP) (Campos et al., 2024; Angelopoulos and Bates, 2021; Quach et al., 2023) is known for providing a model-agnostic and statistically rigorous uncertainty estimation. CP was primarily employed in classification (Angelopoulos and Bates, 2021) and regression tasks (Wang et al., 2024a). For NLG tasks, CP is first adapted to the multiple-choice question-answering (MCQA) setting, where the correct response is selected from a fixed set of options (Kumar et al., 2023; Ye et al., 2024), limiting its applications in real-world open-ended NLG tasks. Conformal language modeling (Quach et al., 2023) relies on the model likelihoods and calibrates a stopping rule to sample prediction sets from the infinite output space until users are confident that the set covers at least one response satisfied. LofreeCP (Su et al., 2024) studies CP for API-only LLMs without logit access by leveraging uncertainty information from diverse sources.\nOur study explores adapting CP for general NLG applications. The nonconformity score (NS) in CP serves as a criterion for calibrating prediction sets, which provide coverage guarantees by selecting a set of possible labels that satisfy the NS threshold (Angelopoulos and Bates, 2021). Since typical logits-based NS may encounter miscalibration, we aim to integrate black-box UQ into the definition of NS, by closely aligning it with the uncertainty condition of the correct answers and devising a conformal uncertainty criterion, while it is more trustworthy to analyze the uncertainty within LLMs' true output space. Then, we can leverage the uncertainty criterion, concluded from a small amount of independent and identically distributed (i.i.d.) calibration data, to construct prediction sets by selecting generations sharing a similar uncertainty condition from the unbounded output space on test samples. Typically, there are two goals of CP: (1) the calibrated prediction set contains the correct answer with at least a user-specified probability; and (2) the average set size should be small, demonstrating the prediction efficiency of our method.\nThe first challenge is UQ for black-box LLMs. Our solution is inspired by an intuitive observation: If a language model generates more semantically diverse outputs for the same prompt, the uncertainty is likely higher (Su et al., 2024; Lin et al., 2023; Xiong et al., 2023). Regardless of the model's capability to tackle the current problem, the confidence score that the model assigns to a generation can be represented by its frequency within the output space. We approximate the model's output distribution by sampling multiple answers to the same question. Then, we perform semantic clustering on the sampled generations, and propose to measure the uncertainty of each generation by combining two factors: the frequency of occurrence of the semantic meaning it conveys, and the consistency between its semantic and other semantic clusters augmented by their individual frequency.\nBased on the measure, we define NS as the uncertainty score of the generation. To this end, the generation meets the correctness criterion and is semantically most similar to the reference answer in the calibration set. We then calculate the quantile \u011d of NSs for all calibration samples, based on the user-specified upper bound of error rate \u03b1. Next, we utilize the conformal uncertainty criterion (i.e., the uncertainty threshold \u011d) to construct a prediction set for each test sample by selecting generations that satisfy the uncertainty conditions strictly associated with correctness from the candidate generations. Additionally, for black-box UQ, we propose employing the most frequent generation or semantic (i.e., the model's most confident answer) as a more trustworthy reference object for the query and leveraging it to measure the overall uncertainty of the current query-answering process. We term this measure ConU, as it employs the same approach as the conformal uncertainty criterion.\nExtensive experimental results exhibit that ConU generally outperforms prior state-of-the-art methods and verify the strict correctness coverage guarantees. Specifically, the prediction sets calibrated by the conformal uncertainty criterion always encompass the correct answers under various user-specified error rates. Furthermore, the average prediction set size is small, highlighting the prediction efficiency of our approach. To our knowledge, this is the first method in the literature to strictly link the NS with the uncertainty condition aligned with correctness via black-box UQ, thereby developing a more robust conformal uncertainty criterion, which provides rigorous correctness coverage guarantees in practical open-ended NLG tasks, and its unique inspiration in benchmarking UQ in LLMs through CP generates independent interest.\nIn summary, our major contributions are listed as follows:\n\u2022 We propose a sampling-based black-box uncertainty measure, termed as ConU, utilizing self-consistency in free-form NLG tasks, facilitating trustworthy decision-making.\n\u2022 We devise a conformal uncertainty criterion by strictly aligning the NS with the uncertainty condition of correct answers, and achieve rigorous correctness coverage with at least a user-specified probability, thereby providing robust guarantees under various error rates in practical open-ended NLG applications.\n\u2022 We conduct selective prediction leveraging the calibrated prediction sets and obtain promising improvements in model accuracy without requiring additional task-specific fine-tuning or architectural modifications."}, {"title": "2 Related Work", "content": "Prior work on UQ in LLMs predominantly focuses on white-box information like token-likelihoods or embeddings (Kuhn et al., 2023; Duan et al., 2024; Wang et al., 2024b), internal state or activations (Yin et al., 2024; Chen et al., 2024), model fine-tuning (Lin et al., 2022a; Tian et al., 2023). These methods can encounter poor calibration and require substantial computational resources. Additionally, researchers lack white-box access to the internal information of LLMs served via APIs. These restrictions demand black-box measures for general UQ in LLMs generations.\nRecent work (Lin et al., 2023) develops several sampling-based uncertainty measures, which can be applied to black-box LLMs by leveraging semantic similarity along with dispersion. Our study follows the sampling setting and proposes to employ the most frequent generation as the reference object to measure the overall uncertainty based on the self-consistency theory (Wang et al., 2022)."}, {"title": "2.2 Conformal Prediction in LLMS", "content": "CP (Angelopoulos and Bates, 2021; Quach et al., 2023; Campos et al., 2024) has emerged as a theoretically sound and practically useful way to guarantee ground-truth coverage with the aid of a small amount of independent and identically distributed (i.i.d.) samples for calibration. CP in classification defines NS that is correlated with the ground-truth label, obtains the quantile \u011d of NSs for all calibration samples based on a user-specified error rate \u03b1, and utilizes \u011d as a threshold to select possible labels on test samples, thereby establishing prediction sets that achieve ground truth coverage with at least the probability of 1 \u03b1.\nRecently, researchers have attempted to apply CP to LLMs for principled UQ. The work (Mohri and Hashimoto, 2024) achieves conformal factuality guarantees by progressively making generations less specific and establishing their corresponding entailment sets until correct answers are encompassed. For correctness coverage, two studies (Kumar et al., 2023; Ye et al., 2024) follow CP in classification tasks and convert NLG tasks into MCQA settings. For open-ended NLG, based on the output token sequence logits, the study (Quach et al., 2023) devises a stopping rule to sample generations until users are confident that a correct answer is covered in QA tasks, which can be impractical for API-only LLMs. LofreeCP (Su et al., 2024) leverages uncertainty information to construct prediction sets that achieve correctness coverage.\nThis paper focuses on more practical scenarios of black-box LLMs in open-ended NLG tasks. Differing from LofreeCP, we strictly connect the NS with the uncertainty condition aligned with correctness via black-box UQ, which concludes a more robust conformal uncertainty criterion to calibrate prediction sets with rigorous correctness coverage guarantees under various error rates despite the complexity of the model or datasets."}, {"title": "3 Method", "content": "Our method investigates two key issues: (1) how to quantify the uncertainty in black-box LLMs when we can only access the generated texts; and (2) how to provide rigorous guarantees on the error rate in open-ended NLG tasks. We first devise a black-box uncertainty measure grounded in self-consistency to provide the trustworthiness notion of model responses. Furthermore, we utilize the CP technique to convert the heuristic approximation into a statistically rigorous one, thereby ensuring a more robust and systematic assessment of uncertainty."}, {"title": "3.1 Preliminaries", "content": "Following prior utility of black-box LLMs (Xiong et al., 2023; Lin et al., 2023; Manakul et al., 2023), conditioned on each prompt (or question) $x_i$, we employ the most likely generation $\\hat{y}_i$ for correctness evaluation. Additionally, we sample a set of $M$ candidate generations $\\{y_m^{(i)}\\}_{m=1}^M$ from the model's output space for black-box UQ and the derivation of conformal uncertainty criterion. We denote the reference answer to $x_i$ as $y_i^*$."}, {"title": "3.2 Uncertainty Quantification", "content": "For each sample, we first cluster semantics in the $M$ sampled generations and obtain $K$ non-repeated semantics. We denote the number of generations sharing the $k$-th semantic as $V_k$ (i.e., $\\sum_{k=1}^K V_k = M$) and any one generation in this cluster as $\\hat{y}_k^{(i)}$.\nBuilding on earlier approaches that utilize self-consistency (Wang et al., 2022; Su et al., 2024; Yadkori et al., 2024) as a reliable measure of confidence, we employ the frequency of the $k$-th semantic as its proxy for reliability: $F(\\hat{y}_k^{(i)}) = \\frac{V_k}{M}$. Then, we define the uncertainty score of each candidate generation in $\\{y_m^{(i)}\\}_{m=1}^M$ as\n$\\begin{equation}\n\\label{u score}\nu(y_m^{(i)}) = 1 - \\lambda \\cdot F(\\hat{y}_m^{(i)}) - (1-\\lambda) \\cdot \\frac{1}{K} \\sum_{k=1}^K (F(\\hat{y}_k^{(i)}) \\cdot \\mathbb{S} (\\hat{y}_k^{(i)}, y_m^{(i)})),\n\\end{equation}$\nwhere $F(\\hat{y}_m^{(i)})$ refers to the frequency of the semantic that $y_m^{(i)}$ conveys, and $\\mathbb{S} (\\cdot, \\cdot)$ measures the semantic similarity between two generations utilizing a cross-encoder model (Reimers and Gurevych, 2019). $F(\\hat{y}_k^{(i)})$ is to augment the persuasiveness of the similarity score associated with $\\hat{y}_k^{(i)}$.\nTo measure the overall uncertainty, we randomly select one generation in the largest semantic cluster to be the most trustworthy generation in the $M$ sampled generations and denote it as $\\hat{y}_{mst}^{(i)}$:\n$\\begin{equation}\n\\label{mst}\ny_{mst}^{(i)} = \\underset{\\hat{y}_k^{(i)} \\in \\{y_m^{(i)}\\}_{m=1}^M}{\\text{argmax}} \\mathbb{V}_k.\n\\end{equation}"}, {"title": "3.3 Conformal Correctness Coverage", "content": "Following the fundamental requirement in CP (Angelopoulos and Bates, 2021), we randomly employ $N$ samples to construct the calibration data set $\\{(x_i, y_i^*)\\}_{i=1}^N$, and for each calibration sample, we demand that at least one sampled generation $\\hat{y}_m^{(i)}$ in $\\{y_m^{(i)}\\}_{m=1}^M$ meets the correctness criterion. Our objective of conformal correctness coverage is by concluding the uncertainty criterion that is closely linked with correctness on $\\{(x_i, y_i^*)\\}_{i=1}^N$, we can calibrate an uncertainty (prediction) set $\\mathcal{P} (X_{\\text{test}})$ for the test prompt $X_{\\text{test}}$ by selecting generations that meet the common uncertainty condition, and the set can guarantee correctness coverage under various user-specificed error rates. Here, we approximate the prediction region of $X_{\\text{test}}$ to the $M$ candidate generations $\\{y_m^{\\text{(test)}}\\}_{m=1}^M$.\n* Assumptions: (1) There is at least one candidate generation in $\\{y_m^{\\text{(test)}}\\}_{m=1}^M$ meeting the correctness criterion; (2) Samples in the calibration and test data sets are exchangeable.\nAs the sampled set $\\{y_m^{\\text{(test)}}\\}_{m=1}^M$ is a subset of the prediction region, which is impossible to enumerate, we can simplify it by stating that there is at least one correct answer in $\\{y_m^{\\text{(test)}}\\}_{m=1}^M$. Exchangeability is the fundamental assumption of CP techniques (Angelopoulos and Bates, 2021)."}, {"title": "4 Evaluations", "content": "In this work, we introduce ConU tailored for black-box UQ in open-ended NLG tasks. Relying on CP which can transform any heuristic approximation into a statistically rigorous uncertainty notion, we develop a robust conformal uncertainty criterion to provide reliable guarantees of correctness coverage under various user-specified error rates. We achieve strict control of the coverage rate across 6 practical LLMs on 4 free-from NLG datasets. Furthermore, the small average uncertainty set size underscores the efficiency of our methods. Utilizing these calibrated prediction sets, we perform selective prediction and obtain remarkable improvements in model accuracy. We envisage that our conformal uncertainty criterion can provide new strategies for principled UQ in open-ended NLG tasks."}, {"title": "Limitations", "content": "Our approach has some limitations. In our study, we assume that at least one correct answer exists in the candidate generations. However, we need to develop a criterion to verify whether the correct answer has been sampled from the unbound output space in real-world applications. Secondly, our findings are limited to the four datasets and future works will extend to other typical NLG tasks like document summarization. Finally, we will attempt to expand our conformal uncertainty criterion to non-exchangeability scenarios, aiming to establish a general criterion across different NLG tasks."}]}