{"title": "Experience-driven discovery of planning strategies", "authors": ["Ruiqi He", "Falk Lieder"], "abstract": "One explanation for how people can plan efficiently despite limited cognitive resources is that we possess a set of adaptive planning strategies and know when and how to use them. But how are these strategies acquired? While previous research has studied how individuals learn to choose among existing strategies, little is known about the process of forming new planning strategies. In this work, we propose that new planning strategies are discovered through metacognitive reinforcement learning. To test this, we designed a novel experiment to investigate the discovery of new planning strategies. We then present metacognitive reinforcement learning models and demonstrate their capability for strategy discovery as well as show that they provide a better explanation of human strategy discovery than alternative learning mechanisms. However, when fitted to human data, these models exhibit a slower discovery rate than humans, leaving room for improvement.", "sections": [{"title": "Introduction", "content": "From organizing dinner to drafting a summer holiday or mapping out a career, people engage in planning daily. Planning can be visualized as a decision tree that grows rapidly with more actions and steps to consider. While computers can rely on increasing computational power to solve such problems, human cognitive resources are limited. Yet, people often excel in complex domains where even advanced computers struggle, raising the question: how do people plan so efficiently despite cognitive limitations? One explanation is that people use clever strategies to facilitate planning (Callaway et al., 2022), but how do these strategies originate? While previous research shows how people learn to choose between pre-existing strategies from a mental toolbox through reinforcement (Rieskamp & Otto, 2006), little is known about how strategies enter this toolbox in the first place. Developmental psychology research (Siegler & Crowley, 1991) found that children can develop new strategies for solving arithmetic problems suggesting that strategy discovery may be a source of these strategies. While we understand some aspects of strategy discovery in mental arithmetic, the mechanism behind the acquisition of planning strategies is largely unknown. Therefore, in this work we investigate the question of how people discover new planning strategies.\nWe address this question with two primary contributions: First, we introduce a novel planning task requiring participants to learn an unfamiliar strategy to measure whether and to what extent strategy discovery occurs. Second, we present a computational model for strategy discovery based on the concept of metacognitive reinforcement learning (Callaway, Gul, Krueger, Griffiths, & Lieder, 2018), which demonstrated a superior explanation of how people learn to adapt their amount of planning (He, Jain, & Lieder, 2021a) and planning strategies (He, Jain, & Lieder, 2021b). Using the data collected from the new task, we then compare metacognitive reinforcement learning (MCRL) against alternative learning mechanisms to evaluate its potential as a model of human strategy discovery.\nIn the following sections, we will first present our experiment and empirical results. We will then discuss the various learning mechanisms, the resulting models, and the procedures for model fitting and selection. Finally, we will conclude with the modeling results."}, {"title": "Measuring strategy discovery", "content": "To measure strategy discovery, we first conducted an experiment using a process-tracing paradigm called Mouselab-MDP (Callaway, Lieder, Krueger, & Griffiths, 2017) and then analyzed the collected data to examine whether and to which extent strategy discovery took place."}, {"title": "Mouselab-MDP", "content": "The Mouselab-MDP paradigm (Callaway et al., 2017) extends the widely used Mouselab paradigm (Payne, 1976) to address sequential decision-making problems. Essentially, the Mouselab-MDP presents participants with a planning task designed to render their information-gathering behavior highly indicative of their underlying planning strategies. Figure 1 illustrates several example trials. Participants are asked to plan a spider's route through a maze with the goal to maximize their score. To track the planning process, the values associated with the nodes are initially hidden. Participants can click on a node to reveal its reward or loss. This clicking behavior provides a manifestation of the participant's planning operations (i.e., the participant clicked on a node because they considered it in their planning process). The cognitive cost of this planning operation is externalized by imposing a fee for each click, encouraging participants to reveal information only when necessary. Therefore, the final score is the sum of the values of the gray nodes the participant chooses to traverse minus the fee for clicking. This score serves as a measure of resource-rationality (Lieder & Griffiths, 2020) that balances the expected utility with the cognitive cost of employing a particular strategy. The strategy that yields the highest score is referred to as the resource-rational (RR) strategy."}, {"title": "Experiment", "content": "Materials We utilized the Mouselab-MDP paradigm to create an environment where the RR planning strategy was qualitatively different from any strategies participants might have previously known. In each trial, the environment could assume one of six possible configurations, as illustrated in Figure 1. Each configuration was designed so that exactly one branch began with a positive reward. The outer nodes on the branch starting with an immediate positive reward always contained one node with a high positive reward (+50) and one node with a high negative reward (-50). The cost of revealing a node's hidden reward (click cost) increased with its distance from the starting point: 1 for the immediate nodes, 3 for the middle nodes, and 30 for the outer nodes. Consequently, the RR strategy involves first examining the immediate nodes until identifying or inferring the branch that began with a positive immediate reward. The RR strategy would then examine exactly one of the outer nodes on that branch. If the value of that node was positive, the agent would choose the path leading to it; otherwise, it would choose the path leading to the other outer node on that branch.1 Given the highly specialized nature of this strategy, we assumed that participants would not plausibly know it before the experiment.\nParticipants We recruited 420 participants through the platform CloudResearch, of whom 378 completed the experiment (average age 38.77 years; 240 females, 3 identifying as other). Recruitment was restricted to participants located in the United States who had completed more than 100 HITS and had an approval rate of over 80%. Participants earned a bonus of 0.3 cents for each point in their final score. On average, they received a bonus of $0.56 in addition to the base pay of $4. The mean duration of the experiment was 25.8 minutes (median 22.8 minutes). Participants who either did not finish the experiment or failed to click anything throughout all trials were excluded from further analysis. This exclusion criterion was designed to filter out inattentive participants. After applying these criteria, 349 participants remained for analysis.\nProcedures Each participant was given minimal instructions on how many trials they would face, how to reveal the nodes, and that each click would incur an unknown fee. They were also informed about the maximum and minimum values of all possible rewards and the score that the optimal strategy could achieve in each trial. After passing a quiz, they were asked to complete 120 trials of the planning task."}, {"title": "Empirical results", "content": "To investigate whether experience-driven strategy discovery is taking place, we first classified the participants' click sequences into two categories: adaptive strategies and other strategies. Adaptive strategies were defined according to the criteria outlined in the materials section. Using this classification, we plotted the proportion of adaptive strategies across trials and observed an increase from 0.79% (CI: [0; 2.22%]) in the first trial to 28.57% (CI: [27.21%; 29.93%]) in the last trial (see the green line in Figure 2). A logistic regression analysis (adaptive strategy ~ intercept + trial) confirms that this upward trend is significant, with a positive coefficient on the trial regressor, indicating an increasing proportion of adaptive strategies as participants gained more experience (see Table 1). This finding is further supported by a non-parametric Mann-Kendall test of trend (S = 6506, p < .001).\nNext to the significantly increasing proportion of adaptive strategy, a mixed linear regression on the score (score ~ intercept + trial) also shows a significantly increasing trend (see Table 2 and the blue line in Figure 2).\nBoth results suggest that experience-driven strategy discovery is indeed taking place. However, strategy discovery appears to be a challenging task, as the proportion of participants who discovered the novel adaptive planning strategy increased only very slowly with only about 29% of participants managing to learn it after 120 trials."}, {"title": "Modeling strategy discovery", "content": "Having established that experience-driven strategy discovery occurred, we now shift our focus to examining the underlying mechanism behind how new planning strategies are discovered. We will first provide the necessary background, introduce the MCRL models, and demonstrate their ability for strategy discovery. Then to examine its alignment with human strategy discovery, we will introduce alternative learning mechanisms, outline the model fitting and selection procedures, and present the results."}, {"title": "Reinforcement learning", "content": "Reinforcement learning (RL) has proven to be a promising framework for modeling how people learn from interactions with the environment (Niv, 2009). Similar to human trial-and-error learning, a common type of RL involves predicting the potential reward Q(s,a) for a specific action a in a given state s, based on previous actions and the corresponding feedback r. The update rule for the Q-value is given by\n $Q(s,a) \\leftarrow Q(s,a) + \\alpha \\cdot \\delta$ where \u03b1 is the learning rate and \u03b4 is the reward prediction error which is the difference between actual and predicted reward."}, {"title": "Metacognitive reinforcement learning (MCRL)", "content": "While standard reinforcement learning focuses on learning policies for external actions, metacognitive learning operates on internal mental operations. To explore metacognitive learning, previous work (Krueger, Lieder, & Griffiths, 2017) utilized the MCRL framework to study the problem of deciding how to decide (meta-decision-making (Boureau, Sokol-Hessner, & Daw, 2015)) as a meta-level MDP (Hay, Russell, Tolpin, & Shimony, 2014): Mmeta = (B,C\u222a{\u22a5},Tmeta,rmeta). In this framework, $b_t \\in B$ represents the mental belief state, $c_t \\in C$ denotes the mental computations including deliberation termination \u22a5. Tmeta describes the meta-level transitions between belief states, and rmeta is the cost of computations as well as the expected return from terminating thinking and acting based on the current belief state. Solving a meta-level MDP optimally is often computationally challenging. However, the optimal solution can be approximated using MCRL by assuming that the brain estimates optimal meta-decision-making through reinforcement learning mechanisms (Russell & Wefald, 1991; Callaway et al., 2018) that learn to approximate the optimal strategy directly (He et al., 2021b).\nBefore diving into the MCRL mechanisms, we will first introduce how the planning strategies are represented."}, {"title": "Strategy representation", "content": "In the MCRL framework, planning strategies are represented by a set of features and corresponding feature weights. To represent the unique planning strategy of our planning task, we extended the feature set outlined by Jain et al. (2022), who developed these features based on the Mouselab MDP paradigm, by adding seven new features that are essential to capture preferences and avoidance related to specific node levels, as well as one additional stopping criterion. Full details on these features can be found here: https://osf.io/g3tzp. This extension results in a total of 63 features, which can be grouped into six broader categories believed to influence planning (Keramati, Smittenaar, Dolan, & Dayan, 2016; Daw, 2018): Pavlovian features, model-free and heuristic features, mental-effort avoidance features, satisficing and stopping criteria features, model-based metareasoning features, and habitual features that reflect mental habits related to how frequently a node, branch, or level has been examined in the past and how many planning operations have been performed before. This strategy representation allows us to characterize an individual's learning trajectory as a series of combinations of features where the feature weights adjust over trials.\nAfter having described the features, we now turn to the MCRL models."}, {"title": "Gradient ascent through the strategy space (Reinforce)", "content": "The metacognitive Reinforce model (Jain, Callaway, & Lieder, 2019), based on the Reinforce algorithm by Williams (1992), assumes that individuals adapt their planning strategies within the space of possible strategies. The strategies map features of belief states to values of cognitive operations, which are then transformed into probabilities of specific planning operations via a softmax function (Williams, 1992). Specifically, the strategy is parameterized by a weight vector w, which is updated after each trial to reflect the effectiveness of different cognitive operations:\n$w \\leftarrow w + \\alpha \\cdot \\sum_{t=1}^{O} [r_{meta}(b_t, c_t) + \\sum_{\\tau>t} \\nabla_w V_{\\pi} (b_t, c_t)]$\nwhere O represents the number of planning operations carried out during the trial, b the belief state, c the click under consideration, Co the set of cognitive operations available in belief state b, \u03b1 the learning rate, and \u03b3 the discount factor. The learned weights, combined with the features described earlier, approximate the meta-level Q-values as follows:\n$Q_{meta}(b_k, c_k) = \\sum_{j=1}^{56} w_j \\cdot f_j(b_k, c_k)$\nThese Q-values are then used to choose cognitive operations probabilistically, following the softmax function:\n$P(c_k|b_k, Q_{meta}) \\propto exp(Q_{meta}(b_k, c_k)/\\tau)$\nwhere \u03c4 is the inverse temperature that controls the balance between exploration and exploitation. The free hyperparameters are \u03b1, \u03b3, and \u03c4 as well as the 63 initial feature weights.\nWe derived two variants of the Reinforce model: a hybrid variant that employs the full set of features including the model-based metareasoning features that capture the uncertainty of a node, as defined by the standard deviation and a purely model-free variant that utilizes a subset of the features by excluding the model-based metareasoning features, therefore positing that adaptation is driven purely by interactions with the environment (see first two rows in Table 3)."}, {"title": "Capability to represent and to discover the novel planning strategy", "content": "To evaluate our model's capacity to represent and discover the adaptive strategy, we conducted model simulations using hyperparameters that were found by optimizing for the optimal click sequence. Figure 3 shows that both the hybrid and model-free Reinforce models are capable of discovering the adaptive strategy from scratch (Mann Kendall test of trend for all simulations: S > 3442, p < .001). While both the full and reduced set of features are sufficient to represent the adaptive strategy, it remains uncertain whether one model variant consistently outperforms the other."}, {"title": "Alternative learning mechanisms", "content": "Having demonstrated that our MCRL models are capable of strategy discovery, we now turn to examining how well our models can explain the human strategy discovery process. For this, we introduce two alternative learning mechanisms to compare against as well as a non-learning model."}, {"title": "Rational strategy selection learning", "content": "Strategy selection learning (Rieskamp & Otto, 2006), posits that individuals possess a repertoire of cognitive strategies for problem-solving and that these strategies are reinforced through feedback. Similar to this idea, Lieder and Griffiths (2017) developed the model of rational strategy selection learning (RSSL), which frames the problem of deciding how to plan as an n-armed bandit problem, with each arm representing a different strategy. In this model, each planning strategy is parameterized by the mean and variance of its expected return. Bayesian inference is performed on the expected return of each strategy, and strategies are selected using Thompson sampling from the posterior distribution of the expected return of the strategies. We will adopt the implementation described by Lieder and Griffiths (2017) and use the set of 79 strategies pre-defined by Jain et al. (2022), which were extensively tested on the Mouselab-MDP. Although the optimal strategy is not included in this set to reflect the constraints of working with a predefined set of strategies, the set contains a strategy that closely approximates the optimal one-specifically, a strategy that involves examining all immediate nodes and then one outer node at random."}, {"title": "Mental habit formation", "content": "According to this model, changes in planning strategy are not driven by value but are influenced by mental habits. It suggests that an individual's tendency to execute a particular planning operation increases with the number of times they have previously performed it regardless of the outcome, consistent with findings by Miller, Shenhav, and Ludvig (2019). This approach involves using a weighted sum of frequency-based data, such as the number of prior clicks on the same node, branch, and level, within a softmax decision process. Technically, we implemented this model as a Reinforce model that uses the same set of features as the model-free variant but does not update the weights of the features. This model therefore adjusts its planning based on the frequency of previous planning operations from past experiences through the habitual features."}, {"title": "Non-learning model", "content": "This model is implemented as a Reinforce model that does not perform any weight updates and does not use habitual features.\nIn summary, we consider five different models: two MCRL models (hybrid and model-free Reinforce), the mental-habit, the RSSL, and the non-learning model. They all differ in the set of features used as well as whether and how the parameters are being updated (see overview in Table 3)."}, {"title": "Model fitting and model selection", "content": "After having introduced all the models, we now turn to comparing the MCRL mechanism against the alternatives. For this, all the models were fitted to each participant individually by maximizing the likelihood of their click sequences using Bayesian optimization (Bergstra, Yamins, & Cox, 2013) for 60,000 iterations. The likelihood of a click sequence is calculated as the product of the likelihood of the individual clicks. After fitting the models, we applied family-level Bayesian model selection (BMS) at three different aggregation levels. Specifically, we estimated the expected proportion of participants best described by each model family (r) and the exceedance probability (\u03c6), which indicates the likelihood that this proportion is significantly higher than that for any other model family using random effect Bayesian model selection (Rigoux, Stephan, Friston, & Daunizeau, 2014). The first level of aggregation distinguishes between non-learning models and learning models. The second level compares the basic learning mechanisms: Reinforce, RSSL, and the mental habit model. The third level compares the two Reinforce variants: hybrid and model-free. In addition to BMS, we used the Bayesian Information Criterion (BIC) (Schwarz, 1978) to assess the relative fit of each model to individual participants' data."}, {"title": "How well can the models explain human strategy discovery?", "content": "BMS indicates that the majority of participants are better explained by a learning model (90%) rather than a non-learning model (see Table 4). Among the learning models, the Reinforce models account for approximately 61% of participants, outperforming the mental habit model (39%) and RSSL (< 1%) with 100% exceedance probability. Within the Reinforce variants, the hybrid Reinforce model explains 62% of participants better than the model-free variant (38%). These observations suggest that while the MCRL models, especially the hybrid Reinforce, explains more participants better than the alternative learning mechanisms, there are individual differences in the learning mechanisms. Therefore, we next looked into the performance of the different participant groups.\nWhen examining participants grouped by the model with the best goodness-of-fit (lowest BIC) an interesting pattern emerges. Participants best explained by the mental habit model (referred to as habitual participants), which does not learn but merely tends to repeat previous planning operations, performed better than those explained by the hybrid Reinforce mechanism (Mann-Whitney U test: mental habit vs. hybrid Reinforce S = 109740657.5, p < .001; no significant difference noted when comparing habitual with the model-free Reinforce participants S = 90456709.5, p = .108). The corresponding mean and standard deviations can be found in Table 5. This counterintuitive observation can, however, be explained by some participants classified as habitual learners showing rapid improvement in the first 20 trials, followed by a flattened learning curve (see Figure 4). Looking into the reason behind this explosive improvement reveals that 21 out of 121 habitual participants examined all the nodes early on (hence the extremely low score in the first trials), thereby quickly learned the environment's structure, and then adopted the optimal strategy. This type of insight-like learning was much less pronounced in other participant groups (15 out of 228 non-habitual participants). Therefore, these participants may be misclassified, as the mental habit model better captures their relatively flat learning curve following their abrupt change in behavior. This type of abrupt learning is currently not represented by any of the current models as they exhibit rather gradual improvements (see Figure 3)."}, {"title": "Discussion and further work", "content": "This work investigated the underlying mechanisms of strategy discovery by designing an experimental paradigm that required learning a highly task-specific planning strategy that was unlikely part of participants' pre-existing mental toolbox. Empirical results show that while experience-driven strategy discovery was indeed taking place, it proved to be a challenging task, with only about 29% of participants discovering the novel strategy after 120 trials. We then introduced the metacognitive reinforcement learning (MCRL) models and demonstrated their capability to discover new planning strategies. Fitting our models and alternative learning mechanisms to examine how well they could account for participants' strategy discovery shows that 61% of participants are better explained by the metacognitive Reinforce mechanism, while 39% are better accounted for by a value-free mental habit model, highlighting i) that MCRL models are able to explain more participants better than the alternative learning mechanism and ii) individual differences in metacognitive learning. Moreover, while our models are in principle able to discover new planning strategies, fitting the models' hyperparameters on the participants' click sequences caused the models to discover adaptive strategies at a slower rate than human participants. We attribute this discrepancy to several factors. First, some participants seem to experience sudden, significant improvements in performance ('Eureka' moments) as well as engage in active learning - an aspect not captured by our current set of MCRL models. To address this, future work could explore additional mechanisms that incorporate active learning or insight-based models. Second, while our current feature set effectively represents the optimal strategy and captures a wide range of strategies, we cannot ensure that it encompasses all potential intermediate strategies participants might have adopted.\nOverall, our work advances our understanding of metacognitive learning and how people discover adaptive planning strategies as well as contributes to the advances in artificial intelligence (AI) to reverse-engineer or even surpass human strategy discovery capacity. While MCRL may not explain every aspect of this process, it is a key mechanism for uncovering strategies in complex tasks. We hope these insights enhance our understanding of human strategy discovery and inspire adaptive AI models across diverse domains."}]}