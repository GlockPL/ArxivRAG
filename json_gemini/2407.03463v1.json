{"title": "Precision at Scale: Domain-Specific Datasets On-Demand", "authors": ["Jes\u00fas M Rodr\u00edguez-de-Vera", "Imanol G Estepa", "Ignacio Saras\u00faa", "Bhalaji Nagarajan", "Petia Radeva"], "abstract": "In the realm of self-supervised learning (SSL), conventional wisdom has gravitated towards the utility of massive, general domain datasets for pretraining robust backbones. In this paper, we challenge this idea by exploring if it is possible to bridge the scale between general-domain datasets and (traditionally smaller) domain-specific datasets to reduce the current performance gap. More specifically, we propose Precision at Scale (PaS), a novel method for the autonomous creation of domain-specific datasets on-demand. The modularity of the PaS pipeline enables leveraging state-of-the-art foundational and generative models to create a collection of images of any given size belonging to any given domain with minimal human intervention. Extensive analysis in two complex domains, proves the superiority of Pas datasets over existing traditional domain-specific datasets in terms of diversity, scale, and effectiveness in training visual transformers and convolutional neural networks. Most notably, we prove that automatically generated domain-specific datasets lead to better pretraining than large-scale supervised datasets such as ImageNet-1k and ImageNet-21k. Concretely, models trained on domain-specific datasets constructed by PaS pipeline, beat ImageNet-1k pretrained backbones by at least 12% in all the considered domains and classification tasks and lead to better food domain performance than supervised ImageNet-21k pretrain while being 12 times smaller. Code repository: https://github.com/jesusmolrdv/Precision-at-Scale/", "sections": [{"title": "1 Introduction", "content": "Recently, big transformer models have been dominating over the state-of-the-art. Works such as DINOv2 [50], when trained on millions of images in a completely self-supervised way, manage to obtain very high performance on most of the general discriminative tasks. However, as most of these models focus on being as"}, {"title": "2 Related Works", "content": "Unsupervised Dataset Generation. Given the high demand for data, shown by recent models [69, 77] the creation of datasets at scale has started to be a priority task. Big unsupervised datasets such as LAION-2B [57] enable the use of custom subsets adapted to each use case and model. Recent papers like DI-NOv2 [50] and Internet Explorer [37] propose automatic pipelines to retrieve and curate real images and compose a more sophisticated dataset that includes high-quality samples while being completely unsupervised. Recently, the success of generative models such as Stable Diffusion [54] and MUSE [8] encouraged works that propose the creation of a completely synthetic dataset [3]. SynCLR [61] leverages the available labels in SoTA datasets to create a completely synthetic dataset of 600 million images and 150 million captions. Similarly, SynthCLIP [26] creates synthetic image-text pairs at scale by exploiting the knowledge of a previously created Meta-CLIP's concept bank.\nSelf-supervised Model Pretraining in Deep learning: ImageNet-trained [55] models have been widely used as initialization weights across diverse downstream tasks such as classification, localization, and segmentation [76]. Model pretraining reduces the need for extensive task-specific datasets [36]. However, models such as ConvNext [69], ViT-G [77] and ViT-22B [15] demand substantial training data, often sourced from ImageNet-22K [16], or JFT [77]. Self-supervised learning (SSL) enables models to acquire adaptable generic features aligned with the original trained model [2, 10, 22, 25, 28]. These models, designed to generate visual features, work effortlessly on any image and pixel-level task [50]. Their success owes to the surge in computational power, model complexity, and data scale by orders of magnitude [13]. BEIT [4], MAE [27] and SimMIM [73] are achieving more and more popularity due to their capacity to contribute to creating robust and efficient models capable of learning in a self-supervised way. A very recent trend focuses on creating task-specific models such as SAM [35] for segmentation and OWL-ViT [47] for detection.\nVision-Language Models (VLM): VLMs like CLIP [53], ALIGN [31], and BASIC [51] play a crucial role in the success of pretrained models. Dual-encoder models [31, 53] learn context-aware representations from both text and visual contents in the shared latent space [21, 38, 80]. VLMs thus provide zero-shot image manipulations guided by textual prompts [23, 34, 59, 79]. Encoder-decoder architectures [68] like CoCa [76] and ImageBind [24] learn generic representations across different modalities. BLIP-2 [39] uses frozen image encoders and LLMs to enhance performance across various vision tasks. Flamingo [1] and Florence-2 [71] are large VLMs demonstrating capabilities in comprehensive vision tasks. A significant bottleneck in VLMs is the need for extensively aligned text-image corpora. Recent endeavours exploring weakly-supervised approaches, like hashtag-supervision, could result in a noisy corpus [30, 42]. Additionally, their utility is limited by the lack of pixel-level information [50]. Furthermore, it is noteworthy that several corpora, including ALIGN-1.8B [31], and FLD-5B [71] are not publicly released, posing significant obstacles for the research community."}, {"title": "3 PaS: Dataset Construction Pipeline On-Demand", "content": "In this section, we introduce Precision at Scale (PaS), a novel method aimed at generating on-demand domain-specific datasets with minimal human intervention. The essence of PaS lies in its completely autonomous workflow, which begins with the leverage of large language models (LLMs) for the discovery of domain-specific concepts. This first stage sets the groundwork by identifying a broad bank of relevant concepts (Section 3.1). Following the concept discovery, the method embarks on a second stage that collects real images and generates synthetic images corresponding to these concepts. The dual approach not only enriches the dataset with a wide variety of real-world images, but also enhances it with synthetic images covering a broader aspect of the concepts (Section 3.2). Finally, the workflow refines the dataset by applying advanced curation techniques eliminating redundancies and filtering out irrelevant or out-of-domain content (Section 3.3). Ultimately, the method yields a highly precise dataset that is primed for training visual models in a self-supervised manner as well as scaled according to the resources and use case of the target models. The modularity of PaS is one of its core traits: note that we do not make assumptions in the specific LLMs, image generators, or image sources used."}, {"title": "3.1 Stage 1: In-domain LLM-guided Concept Discovery", "content": "The first stage of our pipeline consists of the acquisition of an extensive bank of concepts, \\(B\\), belonging to the domain \\(D\\). Despite its vast collection of 500.000 concepts, the MetaCLIP concept repository [74] fails to provide extensive coverage in certain specific domains (e.g. in the Mediterranean food domain, it \"only\" contains simple paella as concept, disregarding all the common variations of it). In order to build \\(B \\subset D\\) without human expert supervision, we leverage the knowledge embedded in Large Language Models (LLM). To guide the LLM, it is necessary to textually define \\(D\\). To limit the biases introduced during the generation process, we reduce this process to two text strings that will be used in the LLM prompts: the name of the domain, \\(n_D\\), and a short description of the type of concepts that make up the domain, \\(d_D\\). For example, if \\(D\\) is the domain of all species of birds in the world, we could define \\(n_D\\) = \"birds\" and \\(d_D\\) = \"bird species\". The process to generate \\(B\\) involves three guided functions by any LLM: 1) generation, 2) expansion, and 3) filtering.\nGeneration of an initial set of concepts: Different to other approaches, we do not use a previously curated list of concepts to build \\(B\\). Thus, we first need to create an initial set \\(B_0\\) which should be task-agnostic while being domain-specific in order to properly cover the target domain. To achieve this, we leverage a LLM, \\(L_1\\), which we provide only with \\(n_D\\) and \\(d_D\\). In particular, we use the first prompt template displayed in Figure 1 to query \\(L_1\\). LLMs are stochastic by nature unless a random seed is fixed at inference time. Given that different random seeds might lead to different (potentially incomplete) outputs, we consider from now on the output of the used LLMs as a probability distribution. In this way, let \\(G_{L_1}(n_D, d_D)\\) denote the probability distribution over sets of concepts"}, {"title": "Domain Exploration via Concept Expansion", "content": "The second step involves enriching the concept bank within domain \\(D\\). The initial set of relevant concepts, \\(B_0\\), can be further refined iteratively by a LLM, denoted as \\(L_2\\), which may or may not be the same as \\(L_1\\). To this end, we prompt \\(L_2\\) to generate similar concepts for every \\(c\\) already in the concept bank. We define as \\(E_{L_2}(n_D, d_D, B_i, c)\\) the probability distribution generated by \\(L_2\\) when asked to generate similar concepts to \\(c\\) using the second template in Figure 1. To provide more context to \\(L_2\\), the conversation with \\(L_1\\) to generate \\(B_0\\) is used as historic data. By explicitly asking \\(L_2\\) for concepts similar to the existing ones, we guide the model to populate the domain with concepts that are closely aligned with the established set. Formally, the iterative process is \\(B_{i+1} = B_i \\cup \\cup_{c \\in B_i} {e_{d,c} \\sim E_{L_2}(n_D, d_D, B_i, c)}\\). To manage this expansion efficiently, we apply a stopping criterion similar to that of the initial generation phase: expansion ceases when \\(|(B_{i+1} \\backslash B_i)| < \\lambda_2 \\cdot |B_i|\\). Again,"}, {"title": "Concept filtering", "content": "LLMs are prone to hallucinate [48]. Since \\(B\\) is the starting point for the rest of the pipeline, it is important to reduce the number of concepts in \\(B\\backslash D\\) (concepts generated that do not belong to the target domain). To this end, we use an additional LLM, \\(L_3\\), to validate each one of the concepts generated by \\(L_1\\). By setting \\(L_1 \\neq L_3\\), we can use it as a regulatory mechanism, since the differences in architecture and weights would mitigate the likelihood of both LLMs making a mistake in the same concept. Using the third template displayed in Figure 1, \\(V_{L_3}(n_D, d_D, c)\\) represents the decision\u00b3 of \\(L_3\\) about whether or not the concept \\(c\\) belongs to \\(D\\). Only concepts validated by \\(L_3\\) are retained in the final bank of concepts \\(B = \\{c \\in \\cup_{i=0}^v B_i : V_{L_3}(n_D, d_D, c) = True \\}\\)."}, {"title": "3.2 Stage 2: Collecting Domain-Specific Images", "content": "Uncurated Real-Data Retrieval: In this stage, we aim to compile domain-specific real-world images leveraging our concept bank \\(B\\). For each concept \\(c\\), we generate a textual embedding, \\(t_c = TE(c)\\), where \\(TE(\\cdot)\\) is the text en-coder of a chosen vision-language model. This model provides a unified embedding space for both text and images, enabling direct comparison with visual data. We search an extensive index of uncurated images, employing the same vision-language model's visual encoder, \\(VE(\\cdot)\\), to compute the visual embeddings \\(v_i = VE(I_i)\\) for each image. The selection of images is based on the cosine similarity \\(sim(t_c, v_i) = \\frac{t_c \\cdot v_i}{||t_c|| ||v_i||}\\) between the concept's textual embedding and the image's visual embedding, ensuring a high degree of relevance to \\(D\\). This process produces \\(T_{real}\\), a dataset of real images closely aligned with \\(B\\).\n\u00b3 We consider a single output of \\(L_3\\) with a fixed random seed."}, {"title": "In-Context In-Domain Image Generation", "content": "The initial phase in the creation of synthetic images involves the generation of textual prompts. Contrary to direct methods that might employ the plain name of a concept (e.g. Canada Goose) for image generation, our approach adopts a more nuanced strategy. We use a LLM, denoted as \\(L_4\\), to craft detailed image captions that encapsulate the essence of a given concept \\(c\\in B\\). This is achieved by employing a structured template, as illustrated in Figure 2, which guides the LLM towards producing rich, contextually relevant captions. The probability distribution of captions by \\(L_4\\) for concept \\(c\\) is represented as \\(P_{L_4}(c)\\). We sample \\(N_{cap}\\) captions for each concept, \\(T_c = \\{t_c \\sim P_{L_4}(c)\\}\\), creating scene descriptions that enrich the conceptual depiction, (e.g. \"A majestic Canada Goose spreads its wings, taking flight above the frozen lake.\"). These captions are then provided to a text-to-image model (e.g. Stable Diffusion [54]), \\(S_I\\), which creates \\(N_{synth}\\) synthetic images for each caption: \\(I_{synth} = \\cup_{c \\in B} \\cup_{t \\in T_c} \\{i_t \\sim S_I(t)\\}\\). The template for \\(L_4\\) is designed to ensure captions contextualize the concept within a scene, improving the descriptive quality and diversity of the synthetic images. The parameters \\(N_{cap}\\) and \\(N_{synth}\\) are adjustable to control the volume and variety of \\(I_{synth}\\). The final set of images, composed by both real and synthetic images, is denoted as \\(I = I_{Real} \\cup I_{Synth}\\)."}, {"title": "3.3 Stage 3: Dataset Curation", "content": "This stage focuses on refining \\(I\\), the dataset assembled in Stage 2, to enhance its quality and relevance for domain \\(D\\). By eliminating low-quality and out-of-distribution (OOD) images, we reduce training costs and prevent the potential negative impact on model performance.\nSelf-Supervised Similarity-based Removal: Duplicate and closely similar images increase the image count (and resource consumption) without adding to the diversity of the data. We employ Self Supervised Copy Detection (SSCD) [52] to identify and remove such instances. Each image \\(I_i\\) in \\(I\\) is transformed into a latent representation through the visual encoder of SSCD, denoted as \\(VE_{SSCD}(I_i)\\). We then construct an adjacency matrix \\(A\\), where \\(A_{ij} = 1\\) if the cosine similarity between \\(VE_{SSCD}(I_i)\\) and \\(VE_{SSCD}(I_j)\\) is above a predefined threshold \\(\\lambda_{dup}\\). From each connected component in \\(A\\), we retain only one image (randomly selected), effectively reducing redundancy.\nCLIP-based OOD Assesment: Given that \\(I\\) is partially sourced from uncurated sources and generated using unsupervised methods, it may contain OOD images. To address this, we employ the zero-shot capabilities of CLIP [53]. Recent works like CLIPN [21] enhance the ability of OOD detection with CLIP by learning negative prompts. These prompts guide the model to learn what the concept is not, improving its discrimination capability. This dual-prompt approach allows CLIPN to calculate two probabilities for each image \\(I\\) and concept \\(c\\): \\(p_{c,I}^1\\), the likelihood that \\(I\\) contains a concept \\(c\\), and \\(p_{c,I}^2\\), the likelihood that \\(I\\) does not contain \\(c\\). These probabilities are used to compute the OOD score for \\(I\\) with respect to the concept bank \\(B\\): \\(OOD_B(I) = 1 - \\sum_{c \\in B} (1 - (p_{c,I}^2)) \\cdot p_{c,I}^1\\)."}, {"title": "Pareto Front-based Removal", "content": "Based on those three metrics, a Pareto-front method for multi-objective optimization [49] is utilized for image selection, where an image \\(I\\) is considered less suitable than the image \\(J\\) (and therefore prioritized for removal) if it shows higher OOD scores across the metrics: \\(OOD_B(I) > OOD_B(J)\\), \\(OOD_{B'}(I) > OOD_{B'}(J)\\), \\(OOD_{B'}(I') - OOD_B(I) \\geq OOD_{B'}(J') - OOD_B(J)\\); with at least one metric showing a strict increase. This approach ensures the systematic exclusion of images that are less relevant for our dataset, enhancing the dataset's overall quality and relevance to the target domain. To determine the optimal stopping point for this pruning process, we employ the kneedle algorithm [56]. This algorithm identifies the \"knee\" point on a curve that represents the relationship between the average value of each metric at the \\(i\\)-th Pareto-front (Y-axis) and the cumulative number of images removed up to that point (X-axis). By selecting the maximum X value among the three knees (one per metric), we can estimate the most efficient halt point. This balance ensures that we maximize the improvement in OOD metric performance while minimizing the loss of potentially valuable images. Furthermore, in addition to this heuristic stopping criteria, the Pareto optimization process also provides a structured guidance for filtering the dataset down to any desired size, offering a flexible approach to achieve a tailored dataset size."}, {"title": "3.4 Stage 4: Data Usage", "content": "The culmination of Stage 3 is a meticulously curated, domain-specific dataset, assembled autonomously without human oversight. While the collection process is driven by specific concepts, the lack of supervision could introduce some noise in the concept-image correlations. Because of this, among the potential applications for the datasets produced by PaS, we highlight training SSL models as a particularly fitting use case. These approaches are known for their demand for large and diverse datasets, making them ideal candidates for utilizing our generated datasets (since PaS can generate an arbitrary amount of samples). Consequently, we employ the generated dataset to train a visual backbone in a self-supervised manner. This pretrained backbone can subsequently be adapted to various downstream tasks within the domain, showcasing the broad applicability and potential of datasets created by our framework."}, {"title": "4 Experiments", "content": "In this section, we evaluate the proposed methodology in two different and complex domains: bird species and food. First, we analyse the diversity and domain"}, {"title": "4.1 Experimental Setup", "content": "Domains: Birds and food have attracted the attention of the computer vision community due to their fine-grained nature and the existence of widely adopted benchmarks for different computer vision tasks. In the birds domain, we consider three existing supervised datasets: CUB-200-2011 [66], NABirds [63] and the subset of bird species of iNat-2021 [64] (iNatbirds from now on). Regarding food, we consider also three existing and widely used datasets: Food-101 [6], FoodX-251 [33] and the current state-of-the-art food dataset Food-2K [46].\nDataset generation: For Stage 1 of the generation of the data set, we used as \\(L_1 = L_2\\) the open LLM Mixtral-8x7B [32], and as \\(L_3\\) Llama 2-13B [62]. We set \\(\\lambda_1 = \\lambda_2 = 0.01\\) resulting in \\(B_{food}\\) (5K concepts) and \\(B_{birds}\\) (3K concepts). In Stage 2, we use LAION-5B [57] as the source of uncurated real images. We sample 500 images per concept using OpenAI's CLIP ViT-L/14 [53] to build the image index and the text embeddings. We use Stable Diffusion 2.1 (SD 2.1) [54] for image synthesis. For each concept, we use Mixtral-8x7B to generate five different captions and we produce 35 images per caption. We set \\(\\lambda_{dup} = 0.6\\) for the duplicate removal [50] when using SSCD [52]. Similarly to T-MARS [43], we use FAST [12] as the text detection mechanism required for the text blurring of the OOD filtering. To mitigate data leakage, an additional SSCD-based filtering step eliminates images resembling any in the test sets of the traditional domain-specific datasets, using a lower similarity threshold of 0.45 to minimize false negatives. This process applied to our selected domains outputs two domain-specific datasets: PaS-B and PaS-F for birds and food, both of 1.2M images.\nDataset Evaluation: We assess the quality of the generated datasets from different perspectives: 1) variety and alignment with the corresponding domain, 2) transferability performance to different downstream tasks in the same domain (compared to other manually curated datasets), and 3) competitiveness with the SoA large-scale general-domain datasets generally used to pretrain backbones.\nDomain-coverage evaluation: We compare the dataset itself with other existing manually curated datasets in the domain. We compare the concepts or categories present in each dataset with those of \\(B\\) generated by Stage 1 of PaS. To do so, we compute the CLIP ViT-L/14 lexical embeddings [53] of all the concepts and class labels in the datasets of the domain (for iNatbirds, we take the common name of each bird species). We then compute the minimum cosine distance between each class label in every dataset of a given domain and any concept of \\(B\\). This will help us understand the proportion of supervised labels present in our automatically generated bank of concepts. Furthermore, for every domain, we compute the UMAP [44] of all the embeddings to qualitatively"}, {"title": "4.2 Diversity and Domain-alignment of the Generated Datasets", "content": "assess the distribution of each dataset in the lexical space. Similarly, we also compute the visual embeddings of each image using a ResNet-152 pretrained on ImageNet-1K [17], and visualize them in a per-domain UMAP. Finally, we make use of the self-supervised dataset inspection tool ProtoSim [65] (with the default parameters), which allows us to find and compare the concept-level prototypes across datasets, enabling a comparative assessment of their richness.\nBackbone pretraining: We focus most of our experimentation on Vision Transformers (ViTs) [19] due to their data-hungry behaviour [77]. Particularly, all the datasets will be evaluated using ViT-B/16. We use MoCo v3 [11] to pretrain the ViTs in a self-supervised way using the default parameters of the 300 epochs stated in the original paper [11]. In addition, we use NNCLR [22] with default parameters adapted to 500 epochs to train ResNet-18 and ResNet-50 [29]to study the applicability of PaS to CNN pretraining. During the evaluation, we test the transferability of the pretrained backbone on different downstream tasks, whose particular setups are described in the supplementary material."}, {"title": "Distribution of lexical concepts", "content": "The first element of our pipeline is the LLM-generated concept bank \\(B\\), which should cover the target domain as much as possible. We compare the distributions of the lexical embeddings of the concepts in \\(B\\) and the category labels of existing datasets. In birds, Figure 3 compares the lexical distribution of classes from three traditional datasets (CUB-200-2011, NABirds, iNatbirds) with concepts from PaS. Figure 3a shows the distribution of distances between each class and the nearest concept in \\(B_{birds}\\), indicating that most concepts from these datasets either closely match or are present in the LLM-generated concept bank, thus showing comprehensive coverage of the domain."}, {"title": "Distribution of Image Embeddings", "content": "Figure 5 shows the density distribution of each dataset in the visual embedding space. In both the birds and food domains, we observe a notable alignment across all datasets. Specifically, birds, it is evident that larger datasets contribute to filling the embedding space more comprehensively. This effect is particularly pronounced in PaS-B, which achieves the most extensive coverage of the embedding space. Analyzing the density distribution, we find that while the CUB-200-2011 dataset exhibits densely populated regions, our dataset, alongside others, demonstrates a more uniform distribution across the embedding space. Similarly, in the food domain, Food-2K spans a broader area but includes numerous outliers, potentially indicating OOD images. PaS-F, in contrast, encompasses the embedding spaces of both Food-101 and Food-2K. Notably, it exhibits a uniform distribution of embeddings, balancing well between areas densely covered by other datasets and those"}, {"title": "Semantic richness analysis", "content": "Using ProtoSim [65], we conducted a comparison with the most comprehensive and diverse datasets in each domain, specifically iNatbirds for birds and Food-2K for food. Within the bird domain, a total of 8159 prototypes were found to be shared, indicating a substantial overlap. Meanwhile, the iNatbirds featured 10 unique prototypes in 25 images, in contrast to PaS-B, which boasted 23 unique prototypes in 358 images. In the food domain, 8144 prototypes were commonly identified. Food-2K exhibited 16 unique pro-totypes within 121 images, whereas PaS-F presented a total of 32, which were observed in 310 images. More details and visual examples of the prototypes can be found in the supplementary material. Results demonstrate the effectiveness of the PaS method in generating datasets that are not only comparable to established datasets in terms of semantic concepts but also includes unique concepts."}, {"title": "4.3 PaS Dataset evaluation", "content": "In-domain classification: We compare the pretraining capacity of PaS datasets against the current SoTA domain-specific datasets on Birds and Food domains. For this study, we pretrain ViT-B models using Food-2k and iNatbirds subset (biggest datasets among their domain) and evaluate them in two popular evaluation datasets per domain. To show the capacity of PaS to adapt to different scales, we display the results for a total of four Pas datasets: PaS-B, PaS-F, PaS-Bmini and PaS-Fmini. While the first two maintain a scale similar to ImageNet-1k, the mini versions are enforced to have the same scale as their baseline counterparts. This ensures fairness when using a data-sensitive model"}, {"title": "5 Limitations", "content": "Despite the high-quality datasets generated by PaS and its promising pretraining outcomes, it is crucial to recognize its limitations. The effectiveness of PaS is significantly dependent on the performance of external models, such as LLMs and Stable Diffusion. For instance, the initial step in the PaS pipeline requires the LLM to possess knowledge of the target domain. Additionally, variations in image quality across different domains by these models can introduce biases, possibly favoring some domains over others. This issue is more severe if the text-to-image models are not adequately trained for specific domains, like medical imaging. Nevertheless, the modular architecture of PaS provides a strategic advantage by facilitating the interchange of generative models to ones that are better suited for the intended domain, thereby offering adaptability and potentially mitigating this issue. While PaS reduces the cost of massive data collection for a given domain, it relies on large models with considerable computational requirements. Even if PaS can seamlessly collect large amounts of high-quality images, it might not be applicable in low-resources settings. Finally, we have not explored yet the usage of the text-image pairs that are generated by PaS. Despite the good results of training using only the visual output of PaS, the potential training of domain-specific vision-language models is still to be addressed."}, {"title": "6 Conclusions", "content": "In this paper, we introduced an innovative framework for autonomously generating domain-specific datasets on-demand. Its modular design facilitates the integration of various pretrained models, offering adaptability across different domains. Additionally, PaS incorporates an efficient pruning method to maintain high performance while reducing dataset size, tackling a major challenge in dataset curation. Our comprehensive analysis demonstrates PaS's ability to"}, {"title": "A Additional Evaluations", "content": "To further validate the capacity of our PaS datasets, we evaluate them on three additional downstream tasks: Finetuned linear classification, Semantic Segmen-tation and Model Transferability."}, {"title": "A.1 Finetuning Downstream Task", "content": "In Table 4, we show the results obtained when applying the finetuning downstream task to the backbones (ViT-B/16) pretrained using the PaS datasets, curated domain-specific datasets and large-scale general datasets (ImageNet). More details on the finetuning settings are described in Appendix D.3."}, {"title": "PaS on ResNet backbones", "content": "Even if ViT architectures were our main focus, we analysed PaS on Resnets for the Food domain. In Table 2, we show how we still beat Food-2k on CNNs across different tasks such as image classification and object detection. The increased amount of diversity and images provided by PaS benefit ResNet which are less data-hungry than transformers."}, {"title": "General and PaS datasets", "content": "Ultimately, we compare our PaS datasets with ImageNet-1k. As shown in Table 3, PaS datasets outperform ImageNet-1k as pre-trainers on their respective domain by at least 12% across different datasets and classification tasks. This proves that, on the same scale, general datasets can not compete with domain-specific datasets in their domain. PaS enables the creation of domain-specific datasets that can compete in scale with general datasets while being as diverse as current domain-specific datasets, closing the gap between general and domain-specific datasets. In fact, we show how our food model trained on PaSF manages to outperform by a great margin a supervised setup\u2020 trained in ImageNet-21k, a dataset almost twelve times bigger. This proves that,"}, {"title": "5 Limitations", "content": "Despite the high-quality datasets generated by PaS and its promising pretraining outcomes, it is crucial to recognize its limitations. The effectiveness of PaS is significantly dependent on the performance of external models, such as LLMs and Stable Diffusion. For instance, the initial step in the PaS pipeline requires the LLM to possess knowledge of the target domain. Additionally, variations in image quality across different domains by these models can introduce biases, possibly favoring some domains over others. This issue is more severe if the text-to-image models are not adequately trained for specific domains, like medical imaging. Nevertheless, the modular architecture of PaS provides a strategic advantage by facilitating the interchange of generative models to ones that are better suited for the intended domain, thereby offering adaptability and potentially mitigating this issue. While PaS reduces the cost of massive data collection for a given domain, it relies on large models with considerable computational requirements. Even if PaS can seamlessly collect large amounts of high-quality images, it might not be applicable in low-resources settings. Finally, we have not explored yet the usage of the text-image pairs that are generated by PaS. Despite the good results of training using only the visual output of PaS, the potential training of domain-specific vision-language models is still to be addressed."}, {"title": "A.2 Semantic Segmentation", "content": "In addition to the classification and object detection tasks already considered, we evaluate the suitability of different datasets as pretrainers for semantic segmentation. Particularly, we consider the FoodSeg103 [70] dataset for our evaluations.\nTable 5 contains the results obtained for this downstream task (more details on the setup can be found in Appendix D.3). At first, we show the results provided by the FoodSeg103 paper [70], and the last two rows contain the results obtained by us when using the weights pretrained on Food-2K and PaS-F. It is important to note that we use the same configuration as the other experiments.\nViT-B pretrained with the PaS dataset outperforms the rest of the backbones (both ViT-B and Swin [41]) trained on different datasets. Significant enhancements are observed in both mean Intersection over Union (mIoU) and mean accuracy (mAcc). These findings suggest that specialized datasets like PaS can"}, {"title": "A.3 Transferability Metrics", "content": "Recent works [58, 67, 75] use a variety of metrics to evaluate the transfer learning capacity of backbones. Metrics such as LogME [75] and NCTI [67] show a high correlation with the transfer learning capacity of the models, helping us evaluate the transferability capacity of a model without fine-tuning or linear probing it.\nAs a further evaluation, we compare the transferability of the models trained on PaS and PaS-Mini datasets with the ones trained on Food-2K and ImageNet over various target datasets.\nWe report LogME [75] and NCTI [67] on three food datasets. As can be seen in Fig. 8, models trained on the Pas dataset provide much higher transferability in both metrics, except in the case of Food-2K. As expected, models trained on Food-2K still provide better transferability when evaluated on the same dataset.\nWhile having the same size, PaS-Mini still beats Food-2K on Food-101 and FoodX-251 datasets, showing a higher transferability capacity. Furthermore, de-pending on the metrics, PaS-Mini is able to beat ImageNet21k, which has 23 times more images."}, {"title": "B Dataset Statistics", "content": "In this section, we compare the sizes of our datasets and current SoTA datasets. Next, we explain how PaS datasets evolve step by step."}, {"title": "B.1 Dataset Sizes", "content": "Appendix B."}]}