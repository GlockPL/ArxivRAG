{"title": "Precision at Scale: Domain-Specific Datasets On-Demand", "authors": ["Jes\u00fas M Rodr\u00edguez-de-Vera", "Imanol G Estepa", "Ignacio Saras\u00faa", "Bhalaji Nagarajan", "Petia Radeva"], "abstract": "In the realm of self-supervised learning (SSL), conventional wisdom has gravitated towards the utility of massive, general domain datasets for pretraining robust backbones. In this paper, we challenge this idea by exploring if it is possible to bridge the scale between general-domain datasets and (traditionally smaller) domain-specific datasets to reduce the current performance gap. More specifically, we propose Precision at Scale (PaS), a novel method for the autonomous creation of domain-specific datasets on-demand. The modularity of the PaS pipeline enables leveraging state-of-the-art foundational and generative models to create a collection of images of any given size belonging to any given domain with minimal human intervention. Extensive analysis in two complex domains, proves the superiority of Pas datasets over existing traditional domain-specific datasets in terms of diversity, scale, and effectiveness in training visual transformers and convolutional neural networks. Most notably, we prove that automatically generated domain-specific datasets lead to better pretraining than large-scale supervised datasets such as ImageNet-1k and ImageNet-21k. Concretely, models trained on domain-specific datasets constructed by PaS pipeline, beat ImageNet-1k pretrained backbones by at least 12% in all the considered domains and classification tasks and lead to better food domain performance than supervised ImageNet-21k pretrain while being 12 times smaller.", "sections": [{"title": "1 Introduction", "content": "Recently, big transformer models have been dominating over the state-of-the-art. Works such as DINOv2 [50], when trained on millions of images in a completely self-supervised way, manage to obtain very high performance on most of the general discriminative tasks. However, as most of these models focus on being as* Equal contribution"}, {"title": "2 Related Works", "content": "Unsupervised Dataset Generation. Given the high demand for data, shown by recent models [69, 77] the creation of datasets at scale has started to be a priority task. Big unsupervised datasets such as LAION-2B [57] enable the use of custom subsets adapted to each use case and model. Recent papers like DI-NOv2 [50] and Internet Explorer [37] propose automatic pipelines to retrieve and curate real images and compose a more sophisticated dataset that includes high-quality samples while being completely unsupervised. Recently, the success of generative models such as Stable Diffusion [54] and MUSE [8] encouraged works that propose the creation of a completely synthetic dataset [3]. SynCLR [61] leverages the available labels in SoTA datasets to create a completely synthetic dataset of 600 million images and 150 million captions. Similarly, SynthCLIP [26] creates synthetic image-text pairs at scale by exploiting the knowledge of a pre-viously created Meta-CLIP's concept bank.\nSelf-supervised Model Pretraining in Deep learning: ImageNet-trained [55] models have been widely used as initialization weights across diverse downstream tasks such as classification, localization, and segmentation [76]. Model pretraining reduces the need for extensive task-specific datasets [36]. However, models such as ConvNext [69], ViT-G [77] and ViT-22B [15] demand substantial training data, often sourced from ImageNet-22K [16], or JFT [77]. Self-supervised learning (SSL) enables models to acquire adaptable generic features aligned with the original trained model [2, 10, 22, 25, 28]. These models, designed to generate visual features, work effortlessly on any image and pixel-level task [50]. Their success owes to the surge in computational power, model complexity, and data scale by orders of magnitude [13]. BEIT [4], MAE [27] and SimMIM [73] are achieving more and more popularity due to their capacity to contribute to creating robust and efficient models capable of learning in a self-supervised way. A very recent trend focuses on creating task-specific models such as SAM [35] for segmentation and OWL-ViT [47] for detection.\nVision-Language Models (VLM): VLMs like CLIP [53], ALIGN [31], and BASIC [51] play a crucial role in the success of pretrained models. Dual-encoder models [31, 53] learn context-aware representations from both text and visual contents in the shared latent space [21, 38, 80]. VLMs thus provide zero-shot image manipulations guided by textual prompts [23, 34, 59, 79]. Encoder-decoder architectures [68] like CoCa [76] and ImageBind [24] learn generic representa-tions across different modalities. BLIP-2 [39] uses frozen image encoders and LLMs to enhance performance across various vision tasks. Flamingo [1] and Florence-2 [71] are large VLMs demonstrating capabilities in comprehensive vi-sion tasks. A significant bottleneck in VLMs is the need for extensively aligned text-image corpora. Recent endeavours exploring weakly-supervised approaches, like hashtag-supervision, could result in a noisy corpus [30, 42]. Additionally, their utility is limited by the lack of pixel-level information [50]. Furthermore, it is noteworthy that several corpora, including ALIGN-1.8B [31], and FLD-5B [71] are not publicly released, posing significant obstacles for the research community."}, {"title": "3 PaS: Dataset Construction Pipeline On-Demand", "content": "In this section, we introduce Precision at Scale (PaS), a novel method aimed at generating on-demand domain-specific datasets with minimal human intervention. The essence of PaS lies in its completely autonomous workflow, which begins with the leverage of large language models (LLMs) for the discovery of domain-specific concepts. This first stage sets the groundwork by identifying a broad bank of relevant concepts (Section 3.1). Following the concept discovery, the method embarks on a second stage that collects real images and generates synthetic images corresponding to these concepts. The dual approach not only enriches the dataset with a wide variety of real-world images, but also enhances it with synthetic images covering a broader aspect of the concepts (Section 3.2). Finally, the workflow refines the dataset by applying advanced curation tech-niques eliminating redundancies and filtering out irrelevant or out-of-domain content (Section 3.3). Ultimately, the method yields a highly precise dataset that is primed for training visual models in a self-supervised manner as well as scaled according to the resources and use case of the target models. The modu-larity of PaS is one of its core traits: note that we do not make assumptions in the specific LLMs, image generators, or image sources used."}, {"title": "3.1 Stage 1: In-domain LLM-guided Concept Discovery", "content": "The first stage of our pipeline consists of the acquisition of an extensive bank of concepts, B, belonging to the domain D. Despite its vast collection of 500.000 concepts, the MetaCLIP concept repository [74] fails to provide extensive cover-age in certain specific domains (e.g. in the Mediterranean food domain, it \"only\" contains simple paella as concept, disregarding all the common variations of it). In order to build B \u2282 D without human expert supervision, we leverage the knowledge embedded in Large Language Models (LLM). To guide the LLM, it is necessary to textually define D. To limit the biases introduced during the generation process, we reduce this process to two text strings that will be used in the LLM prompts: the name of the domain, nD, and a short description of the type of concepts that make up the domain, dD. For example, if D is the domain of all species of birds in the world, we could define nD = \"birds\" and dD = \"bird species\". The process to generate B involves three guided functions by any LLM: 1) generation, 2) expansion, and 3) filtering.\nGeneration of an initial set of concepts: Different to other approaches, we do not use a previously curated list of concepts to build B. Thus, we first need to create an initial set B\u2080 which should be task-agnostic while being domain-specific in order to properly cover the target domain. To achieve this, we leverage a LLM, L\u2081, which we provide only with nD and dD. In particular, we use the first prompt template displayed in Figure 1 to query L\u2081. LLMs are stochastic by nature unless a random seed is fixed at inference time. Given that different random seeds might lead to different (potentially incomplete) outputs, we consider from now on the output of the used LLMs as a probability distribution. In this way, let GL\u2081(nD, dD) denote the probability distribution over sets of concepts"}, {"title": "Domain Exploration via Concept Expansion:", "content": "The second step involves enriching the concept bank within domain D. The initial set of relevant concepts, B\u2080, can be further refined iteratively by a LLM, denoted as L\u2082, which may or may not be the same as L\u2081. To this end, we prompt L\u2082 to generate similar concepts for every c already in the concept bank. We define as EL\u2082(nD, dD, Bi, c) the probability distribution generated by L\u2082 when asked to generate similar concepts to c using the second template in Figure 1. To provide more context to L\u2082, the conversation with L\u2081 to generate B\u2080 is used as historic data. By explicitly asking L\u2082 for concepts similar to the existing ones, we guide the model to populate the domain with concepts that are closely aligned with the established set. Formally, the iterative process is Bi+1 = B\u1d62 \u222a \u222ac\u2208B\u1d62 {ed,c ~ EL\u2082(nD, dD, Bi, c)}. To manage this expansion efficiently, we apply a stopping criterion similar to that of the initial generation phase: expansion ceases when |(Bi+1 \\ Bi)| < \u03bb\u2082 \u00b7 |B\u1d62|. Again,"}, {"title": "Concept filtering:", "content": "LLMs are prone to hallucinate [48]. Since B is the starting point for the rest of the pipeline, it is important to reduce the number of concepts in B \\ D (concepts generated that do not belong to the target domain). To this end, we use an additional LLM, L\u2083, to validate each one of the concepts generated by L\u2081. By setting L\u2081 \u2260 L\u2083, we can use it as a regulatory mechanism, since the differences in architecture and weights would mitigate the likelihood of both LLMs making a mistake in the same concept. Using the third template dis-played in Figure 1, VL\u2083 (nD, dD, c) represents the decision\u00b3 of L\u2083 about whether or not the concept c belongs to D. Only concepts validated by L\u2083 are retained in the final bank of concepts B = \u222a\u1d62=\u2080 {c\u2208 Bi : VL\u2083 (nD, dD, c) = True }."}, {"title": "3.2 Stage 2: Collecting Domain-Specific Images", "content": "Uncurated Real-Data Retrieval: In this stage"}, {"title": "Precision at Scale: Domain-Specific Datasets On-Demand", "authors": ["Jes\u00fas M Rodr\u00edguez-de-Vera", "Imanol G Estepa", "Ignacio Saras\u00faa", "Bhalaji Nagarajan", "Petia Radeva"], "abstract": "In the realm of self-supervised learning (SSL), conventional wisdom has gravitated towards the utility of massive, general domain datasets for pretraining robust backbones. In this paper, we challenge this idea by exploring if it is possible to bridge the scale between general-domain datasets and (traditionally smaller) domain-specific datasets to reduce the current performance gap. More specifically, we propose Precision at Scale (PaS), a novel method for the autonomous creation of domain-specific datasets on-demand. The modularity of the PaS pipeline enables leveraging state-of-the-art foundational and generative models to create a collection of images of any given size belonging to any given domain with minimal human intervention. Extensive analysis in two complex domains, proves the superiority of Pas datasets over existing traditional domain-specific datasets in terms of diversity, scale, and effectiveness in training visual transformers and convolutional neural networks. Most notably, we prove that automatically generated domain-specific datasets lead to better pretraining than large-scale supervised datasets such as ImageNet-1k and ImageNet-21k. Concretely, models trained on domain-specific datasets constructed by PaS pipeline, beat ImageNet-1k pretrained backbones by at least 12% in all the considered domains and classification tasks and lead to better food domain performance than supervised ImageNet-21k pretrain while being 12 times smaller.", "sections": [{"title": "1 Introduction", "content": "Recently, big transformer models have been dominating over the state-of-the-art. Works such as DINOv2 [50], when trained on millions of images in a completely self-supervised way, manage to obtain very high performance on most of the general discriminative tasks. However, as most of these models focus on being as* Equal contribution"}, {"title": "2 Related Works", "content": "Unsupervised Dataset Generation. Given the high demand for data, shown by recent models [69, 77] the creation of datasets at scale has started to be a priority task. Big unsupervised datasets such as LAION-2B [57] enable the use of custom subsets adapted to each use case and model. Recent papers like DI-NOv2 [50] and Internet Explorer [37] propose automatic pipelines to retrieve and curate real images and compose a more sophisticated dataset that includes high-quality samples while being completely unsupervised. Recently, the success of generative models such as Stable Diffusion [54] and MUSE [8] encouraged works that propose the creation of a completely synthetic dataset [3]. SynCLR [61] leverages the available labels in SoTA datasets to create a completely synthetic dataset of 600 million images and 150 million captions. Similarly, SynthCLIP [26] creates synthetic image-text pairs at scale by exploiting the knowledge of a pre-viously created Meta-CLIP's concept bank.\nSelf-supervised Model Pretraining in Deep learning: ImageNet-trained [55] models have been widely used as initialization weights across diverse downstream tasks such as classification, localization, and segmentation [76]. Model pretraining reduces the need for extensive task-specific datasets [36]. However, models such as ConvNext [69], ViT-G [77] and ViT-22B [15] demand substantial training data, often sourced from ImageNet-22K [16], or JFT [77]. Self-supervised learning (SSL) enables models to acquire adaptable generic features aligned with the original trained model [2, 10, 22, 25, 28]. These models, designed to generate visual features, work effortlessly on any image and pixel-level task [50]. Their success owes to the surge in computational power, model complexity, and data scale by orders of magnitude [13]. BEIT [4], MAE [27] and SimMIM [73] are achieving more and more popularity due to their capacity to contribute to creating robust and efficient models capable of learning in a self-supervised way. A very recent trend focuses on creating task-specific models such as SAM [35] for segmentation and OWL-ViT [47] for detection.\nVision-Language Models (VLM): VLMs like CLIP [53], ALIGN [31], and BASIC [51] play a crucial role in the success of pretrained models. Dual-encoder models [31, 53] learn context-aware representations from both text and visual contents in the shared latent space [21, 38, 80]. VLMs thus provide zero-shot image manipulations guided by textual prompts [23, 34, 59, 79]. Encoder-decoder architectures [68] like CoCa [76] and ImageBind [24] learn generic representa-tions across different modalities. BLIP-2 [39] uses frozen image encoders and LLMs to enhance performance across various vision tasks. Flamingo [1] and Florence-2 [71] are large VLMs demonstrating capabilities in comprehensive vi-sion tasks. A significant bottleneck in VLMs is the need for extensively aligned text-image corpora. Recent endeavours exploring weakly-supervised approaches, like hashtag-supervision, could result in a noisy corpus [30, 42]. Additionally, their utility is limited by the lack of pixel-level information [50]. Furthermore, it is noteworthy that several corpora, including ALIGN-1.8B [31], and FLD-5B [71] are not publicly released, posing significant obstacles for the research community."}, {"title": "3 PaS: Dataset Construction Pipeline On-Demand", "content": "In this section, we introduce Precision at Scale (PaS), a novel method aimed at generating on-demand domain-specific datasets with minimal human intervention. The essence of PaS lies in its completely autonomous workflow, which begins with the leverage of large language models (LLMs) for the discovery of domain-specific concepts. This first stage sets the groundwork by identifying a broad bank of relevant concepts (Section 3.1). Following the concept discovery, the method embarks on a second stage that collects real images and generates synthetic images corresponding to these concepts. The dual approach not only enriches the dataset with a wide variety of real-world images, but also enhances it with synthetic images covering a broader aspect of the concepts (Section 3.2). Finally, the workflow refines the dataset by applying advanced curation tech-niques eliminating redundancies and filtering out irrelevant or out-of-domain content (Section 3.3). Ultimately, the method yields a highly precise dataset that is primed for training visual models in a self-supervised manner as well as scaled according to the resources and use case of the target models. The modu-larity of PaS is one of its core traits: note that we do not make assumptions in the specific LLMs, image generators, or image sources used."}, {"title": "3.1 Stage 1: In-domain LLM-guided Concept Discovery", "content": "The first stage of our pipeline consists of the acquisition of an extensive bank of concepts, B, belonging to the domain D. Despite its vast collection of 500.000 concepts, the MetaCLIP concept repository [74] fails to provide extensive cover-age in certain specific domains (e.g. in the Mediterranean food domain, it \"only\" contains simple paella as concept, disregarding all the common variations of it). In order to build B \u2282 D without human expert supervision, we leverage the knowledge embedded in Large Language Models (LLM). To guide the LLM, it is necessary to textually define D. To limit the biases introduced during the generation process, we reduce this process to two text strings that will be used in the LLM prompts: the name of the domain, nD, and a short description of the type of concepts that make up the domain, dD. For example, if D is the domain of all species of birds in the world, we could define nD = \"birds\" and dD = \"bird species\". The process to generate B involves three guided functions by any LLM: 1) generation, 2) expansion, and 3) filtering.\nGeneration of an initial set of concepts: Different to other approaches, we do not use a previously curated list of concepts to build B. Thus, we first need to create an initial set B\u2080 which should be task-agnostic while being domain-specific in order to properly cover the target domain. To achieve this, we leverage a LLM, L\u2081, which we provide only with nD and dD. In particular, we use the first prompt template displayed in Figure 1 to query L\u2081. LLMs are stochastic by nature unless a random seed is fixed at inference time. Given that different random seeds might lead to different (potentially incomplete) outputs, we consider from now on the output of the used LLMs as a probability distribution. In this way, let GL\u2081(nD, dD) denote the probability distribution over sets of concepts"}, {"title": "Domain Exploration via Concept Expansion:", "content": "The second step involves enriching the concept bank within domain D. The initial set of relevant concepts, B\u2080, can be further refined iteratively by a LLM, denoted as L\u2082, which may or may not be the same as L\u2081. To this end, we prompt L\u2082 to generate similar concepts for every c already in the concept bank. We define as EL\u2082(nD, dD, Bi, c) the probability distribution generated by L\u2082 when asked to generate similar concepts to c using the second template in Figure 1. To provide more context to L\u2082, the conversation with L\u2081 to generate B\u2080 is used as historic data. By explicitly asking L\u2082 for concepts similar to the existing ones, we guide the model to populate the domain with concepts that are closely aligned with the established set. Formally, the iterative process is Bi+1 = B\u1d62 \u222a \u222ac\u2208B\u1d62 {ed,c ~ EL\u2082(nD, dD, Bi, c)}. To manage this expansion efficiently, we apply a stopping criterion similar to that of the initial generation phase: expansion ceases when |(Bi+1 \\ Bi)| < \u03bb\u2082 \u00b7 |B\u1d62|. Again,"}, {"title": "Concept filtering:", "content": "LLMs are prone to hallucinate [48]. Since B is the starting point for the rest of the pipeline, it is important to reduce the number of concepts in B \\ D (concepts generated that do not belong to the target domain). To this end, we use an additional LLM, L\u2083, to validate each one of the concepts generated by L\u2081. By setting L\u2081 \u2260 L\u2083, we can use it as a regulatory mechanism, since the differences in architecture and weights would mitigate the likelihood of both LLMs making a mistake in the same concept. Using the third template dis-played in Figure 1, VL\u2083 (nD, dD, c) represents the decision\u00b3 of L\u2083 about whether or not the concept c belongs to D. Only concepts validated by L\u2083 are retained in the final bank of concepts B = \u222a\u1d62=\u2080 {c\u2208 Bi : VL\u2083 (nD, dD, c) = True }."}, {"title": "3.2 Stage 2: Collecting Domain-Specific Images", "content": "Uncurated Real-Data Retrieval: In this stage, we aim to compile domain-specific real-world images leveraging our concept bank B. For each concept c, we generate a textual embedding, tc = TE(c), where TE(\u00b7) is the text en-coder of a chosen vision-language model. This model provides a unified embed-ding space for both text and images, enabling direct comparison with visual data. We search an extensive index of uncurated images, employing the same vision-language model's visual encoder, VE(\u00b7), to compute the visual embed-dings v\u1d62 = VE(I\u1d62) for each image. The selection of images is based on the cosine similarity\n$\\displaystyle sim(t_c, v_i) = \\frac{t_c \\cdot v_i}{||t_c|| ||v_i||}$\ncontinue generating json"}]}]}