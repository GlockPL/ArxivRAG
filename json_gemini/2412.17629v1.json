{"title": "Graph Neural Networks Are Evolutionary Algorithms", "authors": ["Kaichen Ouyang", "Shengwei Fu"], "abstract": "In this paper, we reveal the intrinsic duality between graph neural networks (GNNs) and evolutionary algorithms (EAs), bridging two traditionally distinct fields. Building on this insight, we propose Graph Neural Evolution (GNE), a novel evolutionary algorithm that models individuals as nodes in a graph and leverages designed frequency-domain filters to balance global exploration and local exploitation. Through the use of these filters, GNE aggregates high-frequency (diversity-enhancing) and low-frequency (stability-promoting) information, transforming EAs into interpretable and tunable mechanisms in the frequency domain.Extensive experiments on benchmark functions demonstrate that GNE consistently outperforms state-of-the-art algorithms such as GA, DE, CMA-ES, SDAES, and RL-SHADE, excelling in complex landscapes, optimal solution shifts, and noisy environments. Its robustness, adaptability, and superior convergence highlight its practical and theoretical value.Beyond optimization, GNE establishes a conceptual and mathematical foundation linking EAs and GNNs, offering new perspectives for both fields. Its framework encourages the development of task-adaptive filters and hybrid approaches for EAs, while its insights can inspire advances in GNNs, such as improved global information propagation and mitigation of oversmoothing.GNE's versatility extends to solving challenges in machine learning, including hyperparameter tuning and neural architecture search, as well as real-world applications in engineering and operations research. By uniting the dynamics of EAs with the structural insights of GNNs, this work provides a foundation for interdisciplinary innovation, paving the way for scalable and interpretable solutions to complex optimization problems.", "sections": [{"title": "1. Introduction", "content": "The core characteristic of complex systems lies in their multi-agent interactions and adaptability, enabling the emergence of global patterns from local behaviors. Neural networks can be viewed as a type of complex system, composed of interacting neurons[1-3]. These neurons influence each other through intricate weights, biases, and nonlinear activation functions, thereby facilitating the transmission and processing of information[4-7]. Similarly, the process of biological evolution can be understood as the interaction and exchange of information between different genes. Through genetic variation, recombination, and natural selection, the evolutionary process gradually optimizes genotypes, ultimately producing individuals with higher fitness[8-10]. This paper posits that graph neural networks (GNNs)[11, 12]-a model based on graph structures that learns node representations by aggregating information from neighboring nodes can fundamentally be seen as a form of evolutionary process. The information propagation in GNNs mirrors the mechanisms of diversity generation and adaptive optimization found in the evolution of genotypes.\nThe evolutionary process, driven by the genetic makeup of species, is a highly complex and adaptive system. Through genetic crossover and mutation, diversity is introduced into the population, enabling the exploration of a broader adaptive space[9, 13, 14]. Simultaneously, existing genes, under the influence of natural selection, evolve toward higher fitness. This dynamic balance-fostering the generation of diversity while guiding the optimization of adaptability is the core mechanism that sustains evolution[15-17]. Evolutionary algorithms (EAs) abstract and simulate this process by introducing the concepts of \"exploration\" and \"exploitation\" to describe this balance[18-22]. Exploration corresponds to genetic crossover and mutation, which generate diverse solutions and broaden the search space, while exploitation mirrors the role of natural selection, refining candidate solutions by guiding them toward regions of higher fitness. Exploration ensures the algorithm avoids being trapped in local optima, while exploitation guarantees that potential regions of the solution space are thoroughly explored. This balance between global search (exploration) and local optimization (exploitation) not only underpins the success of evolutionary algorithms but also represents a universal mechanism in complex adaptive systems. The elegance and effectiveness of this mechanism are evident in biological evolution and have inspired its widespread application in the design of optimization algorithms.\nInspired by the rapid advancements in deep learning in recent years, the balance between global and local processes is similarly reflected in Graph Neural Networks (GNNs)[11, 12, 23, 24]. In GNNs, each node aggregates information from its neighboring nodes to facilitate local information propagation and representation updates. This \"local aggregation\" process captures the relationships between nodes and the structural characteristics of the data, resembling the adaptation of genotypes to specific environments in evolutionary processes. However, relying solely on local aggregation limits the range of information propagation and can lead to the phenomenon of oversmoothing, where the features of different nodes become increasingly homogenized[25-30]. This phenomenon, to some extent, mirrors the loss of population diversity in evolutionary algorithms\u2014when solutions within a population become overly concentrated around local optima, the algorithm loses its ability to explore broader solution spaces, thereby constraining its potential for global search[31-34]. To address this, GNNs must expand the range of information propagation through a larger receptive field, ensuring the capture of global patterns and achieving a dynamic balance between global exploration and local aggregation[28, 35-42]. This balancing mechanism closely parallels the \"exploration and exploitation\" trade-off in evolutionary algorithms and raises a profound question: is there a deeper connection between Graph Neural Networks and evolutionary algorithms? Are the similarities they exhibit in solving optimization problems driven by an underlying mathematical duality, or are they merely a phenomenological analogy? In other words, is the information propagation in GNNs and the genetic propagation in evolutionary algorithms merely structurally similar, or do they share a unified theoretical framework at their core?\nTo address this question, we begin by reinterpreting evolutionary algorithms (EAs) from the perspective of graph neural networks (GNNs). In evolutionary algorithms, individuals can be represented as nodes in a graph, with the interactions between these nodes modeled through an adjacency matrix. Similar genotypes are more likely to interact, which corresponds to the phenomenon of reproductive isolation in biological evolution. This can be intuitively represented by higher edge weights in the adjacency matrix, where individuals with similar genotypes are more likely to mate and exchange genetic information, while those with significantly different genotypes are less likely to form direct interactions. The node update process in EAs can further be understood as the superposition of information at different frequencies, which corresponds to the information aggregation process in spectral GNNs[11, 12, 43-46]. In EAs, the update of an individual arises from the interaction and exchange of diverse genetic information within the population. This process can be decomposed into two types of frequency contributions: on the one hand, low-frequency information represents the global consistency or stability of the population, reflecting the inheritance of advantageous genes validated during evolution. On the other hand, high-frequency information captures local diversity among individuals, which is introduced through mutation or crossover mechanisms, reflecting genetic variability. The inclusion of high-frequency information provides the necessary exploratory capability for EAs, helping the population avoid becoming trapped in local optima. By designing appropriate interaction and update rules, EAs achieve a dynamic balance between high-frequency and low-frequency information. This balance is critical to the effectiveness of EAs: excessive reliance on low-frequency information leads to homogeneity within the population, resulting in a loss of genetic diversity and limiting the algorithm's ability to explore the solution space. Conversely, overemphasizing high-frequency information amplifies differences between individuals, disrupting global adaptability and preventing the algorithm from focusing on high-fitness regions. Only by appropriately adjusting the contributions of these two types of frequency information can EAs organically combine global exploration and local exploitation within the solution space.\nFrom another perspective, graph neural networks (GNNs) can be viewed as a type of evolutionary algorithm. The process of aggregating high- and low-frequency information bears a striking resemblance to the process of evolution. The aggregation of low-frequency information can be likened to the inheritance of \"advantageous traits\" in genes during evolution, where existing adaptive traits are retained and reinforced. In contrast, the aggregation of high-frequency information resembles genetic crossover and mutation, introducing new traits with significant variations to enhance the diversity of the population. Spectral-based GNNs achieve this balance between high- and low-frequency information through the design of appropriate filters, which essentially mirrors the \"exploration versus exploitation\" trade-off in evolutionary algorithms[44, 47-51]. The aggregation of low-frequency information emphasizes exploiting existing advantages, while high-frequency information aggregation expands the scope of exploration in the solution space. Taking this analogy further, each iteration of a GNN can be seen as an adaptive adjustment of the filter's weight distribution between high- and low-frequency information. This weight optimization is dynamically achieved through gradient descent, enabling the GNN to continuously adjust the proportion of high- and low-frequency information based on the specific task requirements. This process reflects the mathematical essence of \"directional selection\" in evolution, where natural selection determines how much low-frequency stability and high-frequency innovation should be retained or introduced in the population. Taking this analogy further, each iteration of a GNN can be seen as an adaptive adjustment of the filter's weight distribution between high- and low-frequency information. This weight optimization is dynamically achieved through gradient descent, enabling the GNN to continuously adjust the proportion of high- and low-frequency information based on the specific task requirements. This process reflects the mathematical essence of \"directional selection\" in evolution, where natural selection determines how much low-frequency stability and high-frequency innovation should be retained or introduced in the population.\nInspired by the above discussion, we delved deeper into the potential connections between graph neural networks (GNNs) and evolutionary algorithms, uncovering profound mathematical correspondences between the two. Based on this discovery, we propose a novel evolutionary algorithm, termed Graph Neural Evolution (GNE). Specifically, we leverage the similarities between individuals to construct an adjacency matrix, modeling the process of individual updates in evolution as a filtering operation on a graph. By designing appropriate filter functions, we can effectively aggregate both global and local information, thereby enhancing the population's search capabilities. More importantly, the ratio of global to local information aggregation during population updates can be explicitly represented using polynomial functions. This not only enhances the interpretability of evolutionary algorithms, traditionally considered black-box models, but also provides theoretical support for further optimization and analysis.\nThis duality bridges the seemingly independent fields of graph neural networks (GNNs) and evolutionary computation, enabling mutual inspiration and cross-disciplinary learning. The proposed GNE algorithm introduces an innovative framework for designing evolutionary algorithms, redefining the core process of \"exploration and exploitation\" from a frequency-domain perspective. By treating the superposition of different frequency components in a population as a filtering operation, the design of evolutionary algorithms shifts from traditional spatial-domain update mechanisms to optimizing frequency components in the frequency domain. This transformation not only decouples the update process of evolutionary algorithms from complex nonlinear black-box models but also significantly enhances their interpretability and controllability through the explicit description of the filtering function. Conversely, the node update mechanisms in graph neural networks can also draw inspiration from evolutionary algorithms. For example, incorporating more diverse crossover and mutation operators can enhance the flexibility of information exchange between nodes, mitigating the over smoothing problem and improving the distinctiveness of node representations. This cross-disciplinary interaction not only opens new avenues for the advancement of both fields but also paves the way for modeling more complex systems on a broader scale.\nIn the following sections, we first systematically review the fundamental principles of evolutionary algorithms and graph neural networks (GNNs), laying a solid foundation for the subsequent theoretical analysis. Next, we delve into the mathematical connections between evolutionary algorithms and GNNs, based on which we propose the GNE (Graph Neural Evolution) algorithm. This innovative framework reconstructs the core processes of evolutionary algorithms from a frequency-domain perspective. We then conduct a detailed analysis of GNE's performance in optimization tasks by comparing it with several classical and widely recognized evolutionary strategies, particularly focusing on its adaptability to scenarios where the optimal solution of the objective function shifts or noise is introduced. Experimental results demonstrate that GNE not only maintains high convergence efficiency but also dynamically adjusts population information, exhibiting remarkable robustness and flexibility. Additionally, as part of our exploratory research, we analyze the impact of different polynomial-based filter functions on GNE's performance, uncovering the profound influence of filter design on the algorithm's outcomes. This provides valuable insights and directions for further improvements to the GNE model. In the final section, we objectively examine the limitations of GNE and discuss potential areas for improvement. We also explore the future possibilities of integrating evolutionary computation and GNNs, highlighting their potential for mutual development and synergy. This perspective offers new avenues for fostering collaborative innovation between the two fields."}, {"title": "2. Related Works", "content": null}, {"title": "2.1 Evolutionary Algorithms", "content": "Evolutionary Algorithms (EAs) are a class of optimization methods inspired by the biological mechanisms of natural evolution, rooted in Darwin's principle of \"survival of the fittest.\" By simulating processes such as natural selection, genetic recombination, and mutation, EAs operate on a population of solutions to search for the optimal solution in the solution space. The Genetic Algorithm (GA) is one of the earliest and most extensively studied EAs[8]. It utilizes operations such as selection, crossover, and mutation to generate new candidate solutions, performing well in solving discrete and multi-modal optimization problems. However, GA often struggles with slow convergence and susceptibility to local optima when tackling high-dimensional continuous optimization problems. To address these limitations, Differential Evolution (DE) was introduced, leveraging the differences between individuals in the population to generate new solutions, thereby significantly enhancing global search capabilities and optimization efficiency[52]. Subsequently, the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) was developed, dynamically adjusting the covariance matrix to capture the directional distribution of the population, making it particularly effective for high-dimensional continuous optimization problems[53]. More recently, researchers have proposed strategies such as the Search Direction Adaptation Evolution Strategy (SDAES)[54] and Learning Adaptive Differential Evolution by Natural Evolution Strategies (RL-LSHADE)[55], which demonstrate remarkable advantages in dynamically adjusting parameters and search directions for large-scale optimization problems.\nFrom GA to DE, CMA-ES, and more advanced variants, the design of evolutionary algorithms has always centered on the balance between exploration and exploitation, which is the key to their success. Exploration aims to introduce diversity and expand the search space, helping to avoid local optima. Specifically, exploration corresponds to operations like crossover and mutation, which enable broad sampling of the solution space. In contrast, exploitation focuses on refining the search around promising solutions to improve precision and gradually approach the optimal solution. Maintaining this balance between exploration and exploitation is crucial in evolutionary algorithms, as any imbalance may lead to a decline in efficiency.\nThis dynamic balance between exploration and exploitation can be interpreted through the lens of signal processing, where the update process of evolutionary algorithms is essentially a combination of low-frequency and high-frequency information. Exploration, driven by mutation, resembles the introduction of high-frequency information, which enhances population diversity and broadens the search space. Exploitation, on the other hand, is akin to low-frequency processing, concentrating on promising regions and reinforcing stable patterns in the population to improve overall fitness. In this sense, the update mechanism of evolutionary algorithms can be seen as a \"filtering\" process: The high-frequency components preserve the diversity of the population, preventing it from getting trapped in local optima, while the low-frequency components reduce noise and amplify the influence of high-quality genetic information within the population. This process of signal optimization ultimately leads to a steady improvement in the overall fitness of the population, accelerating convergence toward the optimal solution."}, {"title": "2.2 Graph Neural Networks", "content": "Graph Neural Networks (GNNs) are a class of deep learning methods designed for processing graph-structured data, aiming to effectively extract and represent features in the non-Euclidean domain of graphs. GNNs are built upon the unique properties of graphs, where nodes and edges represent entities and their relationships, respectively. The study and development of GNNs are primarily conducted from two perspectives: spatial domain and spectral domain[47, 56]. From the spatial perspective, GNNs focus on local aggregation by directly leveraging the neighborhood information of nodes. Through an iterative message-passing mechanism, the representation of each node is updated by aggregating information from its neighbors, gradually learning a more expressive embedding for each node. For instance, the Graph Convolutional Network (GCN)[11], a representative method in the spatial domain, can be mathematically expressed as:\n$H^{(l+1)} = \\sigma(\\tilde{A}H^{(l)}W^{(l)})#eq.1$\nwhere, $\\tilde{A} = D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}$ represents the normalized adjacency matrix, $A = A + I$ denotes the adjacency matrix with added self-loops, $D$ is the degree matrix, $H^{(l)}$ reports the node feature matrix at layer $l$, $W^{(l)}$ represents the trainable weight matrix, and $\\sigma$ is the activation function.\nGCN performs neighborhood aggregation by averaging the features of neighboring nodes, effectively propagating structural information to the central node. However, this simple aggregation mechanism overlooks the varying importance of different neighbors. To address this limitation, the Graph Attention Network (GAT) introduces an attention mechanism to dynamically assign different weights to neighbors[12]. The update rule for GAT is as follows Eq. (2).\n$h_i^{(l+1)} = \\sigma (\\sum_{j \\in N(i)} \\alpha_{ij} W h_j^{(l)}) #eq.2$\nwhere $\\alpha_{ij}$ denotes the attention weight computed for neighbor $j$ relative to central node $i$, indicating the importance of node $j$ to node $i$.\nSpatial-based methods are intuitive and flexible, making them widely applicable to many graph tasks such as node classification, link prediction, and graph clustering. However, they primarily rely on local neighborhood information and may struggle to effectively capture global structural properties of the graph.\nThe spectral perspective approaches GNNs from the frequency domain, utilizing the spectral properties of the graph Laplacian to define convolutional operations on graphs. The graph Laplacian is defined as Eq. (3).\n$L = D - A#eq.3$\nwhere, $D$ represents the degree matrix and $A$ denotes the adjacency matrix. The normalized Laplacian is further expressed as Eq. (4).\n$L_{norm} = I - D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}#eq.4$\nSpectral methods perform graph convolution by applying a graph Fourier transform to signalsx = {$X_1, X_2... X_N$}, mapping them to the frequency domain. Specifically, the graph Fourier transform is based on the eigen-decomposition of the Laplacian matrix as Eq. (5).\n$U^T x, where\\  L = U\\Lambda U^T#eq.5$\nwhere, $U$ represents the eigenvector matrix of the Laplacian and $\\Lambda$ denotes the diagonal matrix of eigenvalues. The convolution operation in the spectral domain is defined as Eq. (6).\n$y = Ug(\\Lambda)U^Tx#eq.6$\nwhere, $g(\\Lambda)$ is a filter function applied to the eigenvalues. While spectral methods are theoretically elegant and capture the global structure of the graph, early approaches faced challenges such as high computational cost and poor generalization to graphs with different structures. To overcome these limitations, Chebyshev networks introduced an approximation using Chebyshev polynomials, significantly reducing the computational complexity. The convolution operation is approximated as Eq. (7).\n$g(\\Lambda) \\approx \\sum_{k=0}^{K} \\theta_k T_k(L) #eq.7$\nwhere, $T_k(L)$ represents the Chebyshev polynomial of order $k$, $L$ is the rescaled Laplacian, and $\\theta_k$ is Learnable parameters. Based on this, GCN further proposed a first-order approximation, simplifying the computation while maintaining high efficiency. In general, existing spectral-domain-based Graph Neural Networks (GNNs) can be uniformly expressed in Eq. (8)[48].\n$Z = \\phi (g(\\Lambda)\\varphi(X)) #eq.8$\nwhere, $Z$ represents the prediction, $\\phi$ and $\\varphi$ are functions, such as multi-layer perceptrons (MLPs), and $g$ is a polynomial. Although spatial and spectral methods differ in their starting points and implementation, they can both be viewed as processes of propagating information between nodes to iteratively update and optimize node representations. During training, the representations are guided by loss functions and optimized through gradient descent, analogous to the evolutionary process of optimizing individuals in a population. If nodes are considered as individuals in a population, the training of GNNs can be seen as an evolution process, where nodes progressively evolve by exchanging information with their neighbors. Compared to spatial methods, spectral methods offer stronger mathematical interpretability due to their reliance on graph frequency characteristics. By leveraging filtering operations in the frequency domain, spectral methods can better capture the global structure of the graph and reveal potential relationships between nodes. As a result, modern GNNs aim to combine the computational efficiency of spatial methods with the interpretability of spectral methods, enabling unified frameworks that effectively model both local and global information."}, {"title": "3. Graph Neutral Networks are Evolutionary Algorithms", "content": "Similar to how graph neural networks update nodes based on graph structures, evolutionary algorithms can also establish a connection with graph neural networks by constructing a similarity matrix among individuals in the population. In evolutionary algorithms, each individual in the population can be viewed as a node in the graph, while the similarity matrix describes the interactions between these nodes. Individuals with higher similarity can be considered to have more similar genotypes, making them more likely to interact with each other. This mechanism aligns closely with the concept of \"reproductive isolation\" in biological evolution, where only sufficiently similar individuals are more likely to exchange and transmit genetic information. To ensure that the similarity matrix reflects as much information about the population as possible, the center of the population $X_o$ is used as a reference point. The similarity between individuals is then measured by calculating the cosine similarity of the difference vectors $Z_i = X_i - X_o$ between each individual $X_i$ and the population center $X_o$. The similarity $A_{ij}$ between individual $i$ and individual $j$ is represented as shown in Eq. (9). To ensure numerical stability and computational adaptability, the similarity is further enhanced using an exponential function and normalized accordingly.\n$A_{ij} = \\frac{Z_iZ_j}{||Z_i||*||Z_j||}#eq.9$\nAfter obtaining the similarity matrix A, we can further construct the normalized Laplacian matrix $L_{norm} = I - D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}$.Subsequently, we perform spectral decomposition on the Laplacian matrix to obtain the diagonal matrix of eigenvalues$\\Lambda = diag[\\lambda_1,...\\lambda_N]$ and the corresponding eigenvector matrix $U = [U_1,... U_N]$. Here, $\\Lambda_t$ represents the t-th largest eigenvalue, and $U_t$ represents its corresponding eigenvector.The expression for $\\lambda$ and $U_t$, when expanded, can be further detailed as shown in Eq. (10).\n$\\lambda = \\frac{1}{2} \\sum_{i} \\sum_{j} A_{ij} (\\frac{U_{ti}}{\\sqrt{d_i}} -  \\frac{U_{tj}}{\\sqrt{d_j}})^2#eq.10$\nwhere, $U_{ti}$ and $U_{tj}$ represent the components of individuals i and j in the eigenvector $U_t$, while $d_i$ and $d_j$ denote the degrees of individuals i and j, respectively. A larger $\\Lambda_t$ indicates greater variability in the population $X$ along the direction of $U_t$,which can be interpreted as the frequency of population information in that direction. The transition from low frequencies to high frequencies gradually reflects a shift from global consistency to individual diversity within the population.\nIn spectral-based graph neural networks, filter $g(\\Lambda) \\approx \\sum_{k=0}^{K} \\theta_kT_k(L)$ is typically approximated using polynomials$T_k(L)$, where the $\\theta_k$ are learnable parameters. These parameters represent the rules learned by the graph neural network to assign different weights to different frequency components. In the context of evolutionary algorithms, low-frequency information within a population can be viewed as stable and high-quality genetic information that has been validated through natural selection. On the other hand, high-frequency information represents more distinct genetic components, which can be attributed to genetic recombination and mutation. The process of natural selection can essentially be seen as a filtering mechanism for the population's genetic information, gradually refining the genotype distribution towards one with higher fitness. Similarly, we can define a filter to simulate the role of natural selection in filtering genetic information, as shown in Eq. (11). In Eq. (11), the $\\theta_k$ is replaced with $C_k$ to reflect the inherent completeness of the natural selection process, emphasizing its ability to efficiently filter population information during evolution without requiring external intervention.\n$g(\\Lambda) \\approx \\sum_{k=0}^{K} C_kT_k(L) #eq.11$\nBy adjusting the $C_k$ of the filter, we can flexibly control the algorithm's focus on different frequency components during the update process. A higher weight on the low-frequency components means that the overall information of the population is more aggregated, enhancing the algorithm's exploitation ability. Conversely, a higher weight on the high-frequency components indicates that more diverse information within the population is being aggregated, thereby improving the algorithm's exploration capability. As a result, the design of the filter function $g$ essentially determines the algorithm's balance and preference between exploration and exploitation. This mechanism not only provides the algorithm with greater flexibility but also enhances its interpretability. To ensure numerical stability, the values of $g$ have been normalized within a specific range. Through Eq. (9)\u2013(11), we effectively complete the filtering process for the evolutionary algorithm's population, with the updated population expressed by Eq. (12).\n$X_{new} = Ug(\\Lambda)U^TX#eq.12$\nIn Eq. (8), existing spectral-based graph neural networks typically apply nonlinear transformations $\\varphi$ and $\\phi$ to the original feature matrix X or the filtered feature matrix. These transformations are usually functions similar to multi-layer perceptrons (MLPs). During the training process of graph neural networks, the parameters of these nonlinear transformations are continuously updated using the gradient descent algorithm with a specific learning rate. In this process, the changes in $\\phi$ and $\\varphi$ can be viewed as a resampling operation in their respective feature spaces, where the features are updated along the optimization direction with a certain step size. This resampling process adjusts their distribution gradually to better fit the requirements of specific problems, thereby enhancing the model's representational capability and optimization performance for the target task. Similarly, in evolutionary algorithms, nonlinear transformations $\\phi$ and $\\varphi$ can be implemented by resampling the current population along the optimization direction. This process corresponds to progressively adjusting the population's distribution through resampling, making it more concentrated in high-fitness regions and thus improving the optimization results. The overall update process of the population can be described by Eq. (13), which integrates the ideas of graph neural networks to create a novel evolutionary algorithm-Graph Neural Evolution (GNE). The pseudocode for this algorithm is presented in Algorithm 1.\n$X_{new} = \\phi(Ug(\\Lambda)U^T\\varphi(X))#eq.13$"}, {"title": "4. Experimental results and analysis", "content": "In this section, we compare the performance of GNE with five well-established and widely recognized evolutionary algorithms: Genetic Algorithm (GA), Differential Evolution (DE), Evolution Strategy with Covariance Matrix Adaptation (CMA-ES), Strategy Based on Search Direction Adaptation (SDAES), and Learning Adaptive Differential Evolution by Natural Evolution Strategies (RL-SHADE). The comparison is conducted on nine classical benchmark functions to evaluate the convergence and robustness of GNE.The detailed descriptions of these benchmark functions, including their mathematical expressions, bounds, dimensions, and characteristics, are provided in Appendix A.1. Furthermore, we assess GNE's performance under challenging scenarios, including when the optimal solution of a function is shifted and when noise is added to the function, to verify its generalization capability and resilience to noise. Finally, as an extension, we explore GNE models under different polynomial bases and compare their performance to gain deeper insights into their effectiveness.In Sections 4.1 and 4.2, GNE employs Chebyshev polynomials to approximate the filter function, which plays a pivotal role in balancing high- and low-frequency components during the optimization process. The specific formulation and details of the Chebyshev polynomial-based filters are presented in Appendix A.2."}, {"title": "4.1 Comparison of GNE and Other Evolutionary Algorithms", "content": "In this section, we compare GNE with five well-established and widely recognized evolutionary algorithms: GA, DE, CMA-ES, SDAES, and RL-SHADE. The comparison is conducted on nine classical benchmark functions: Sphere, Schwefel, Schwefel 2.22, Schwefel 2.26, Rosenbrock, Quartic, Rastrigin, Ackley, and Levy. For each algorithm, the population size N is set to 30, and the maximum number of iterations Tis set to 500. Each algorithm is independently executed 30 times, and we calculate the mean and standard deviation of the results."}, {"title": "4.2 Robustness of GNE for Optimal Solution Deviation", "content": "To verify whether the proposed GNE algorithm exhibits overfitting to the optimal solution, we use the Sphere function as an example. The original optimal solution of the Sphere function is located at the origin (0 point). Subsequently, the optimal solution is intentionally shifted by offsets of -10, -30, -50, -70, -90, +10, +30, +50, +70, and +90. This setup is designed to test the robustness of GNE when the optimal solution of the function deviates from its original position."}, {"title": "4.3 Robustness of GNE in Noisy Function Environments", "content": "In real-world scenarios, function evaluations are often affected by measurement errors and uncertainties. In this section, we introduce random noise uniformly distributed in the range [0, 1] to the 9 benchmark functions to evaluate the performance of GNE under noisy function conditions. This setup tests the algorithm's robustness when noise is present in the computation of function values."}, {"title": "4.4 Performance of GNE with Different Polynomial Bases", "content": "As an extension, we conducted a preliminary evaluation of the performance of the GNE algorithm under 12 different polynomial bases and tested them on the benchmark functions. These polynomial bases, listed in the following order, include: Chebyshev, Bessel, Fibonacci, Fourier, Gegenbauer, Hermite, Jacobi, Laguerre, Legendre, Lucas, Monomial, and Bernstein. This exploration aims to investigate how different polynomial bases influence the performance of the GNE algorithm."}, {"title": "5. Conclusion", "content": "In this work, we introduced Graph Neural Evolution (GNE), a novel algorithm that bridges evolutionary computation and graph neural networks (GNNs) through a unified frequency-domain perspective. By modeling the population updates of evolutionary algorithms as a filtering process on a graph, GNE effectively aggregates high-frequency and low-frequency information, achieving a dynamic balance between exploration and exploitation. This innovative approach not only enhances the interpretability of evolutionary algorithms but also establishes a profound theoretical connection between these two seemingly distinct fields.The experimental results demonstrate the remarkable performance of GNE across various optimization challenges. GNE excels in scenarios involving optimal solution shifts and noisy environments, consistently outperforming state-of-the-art evolutionary algorithms such as GA, DE, CMA-ES, RL-SHADE, and SDAES. Its robustness and adaptability are evident in its ability to maintain high accuracy, stability, and convergence speed, even under challenging conditions. By effectively balancing exploration and exploitation, GNE delivers superior optimization results for both unimodal and multimodal functions, showcasing its potential as a versatile and powerful optimization framework.Despite these achievements, there is significant room for improvement and further exploration. The polynomial filters employed in this study were selected based on initial experimentation and are not necessarily optimal for all problem types. Future work could focus on developing adaptive filtering mechanisms, where the polynomial basis evolves dynamically during the optimization process. For instance, filters could emphasize high-frequency components in the early stages to enhance exploration and gradually shift toward low-frequency components in later stages to strengthen exploitation. Additionally, the potential of hybrid filters combining multiple polynomial bases or non-polynomial filters could be explored to further enhance GNE 's performance.Another promising avenue lies in extending GNE to multi-objective optimization problems, enabling it to handle more complex real-world tasks with competing objectives. Hierarchical frameworks or problem-specific adaptations could further broaden GNE 's applicability to challenging optimization scenarios. Furthermore, GNE's ability to act as a general-purpose optimizer opens up exciting opportunities in machine learning, such as hyperparameter tuning, neural architecture search, and combinatorial optimization, as well as applications in engineering design and operational research. These extensions could significantly enhance GNE's impact across diverse fields.This work also contributes to a deeper understanding of the relationship between evolutionary algorithms and GNNs. By drawing parallels between the filtering of genetic information in evolutionary algorithms and the frequency-based information aggregation in spectral GNNs, we establish a theoretical foundation for cross-disciplinary innovation. Future research could explore how evolutionary concepts, such as genetic recombination or adaptive mutation strategies, can inspire improvements in GNN architectures, mitigating issues such as oversmoothing and improving their generalization capabilities.In summary, we present Graph Neural Evolution (GNE) as a significant step forward in evolutionary computation and graph-based learning. By combining frequency-domain insights with practical algorithm design, GNE redefines the core principles of exploration and exploitation in optimization. Its robustness, adaptability, and performance across diverse scenarios highlight its potential for broad applicability. Looking forward, the outlined directions for improvement and exploration offer exciting possibilities for advancing GNE's capabilities, inspiring future research,"}]}