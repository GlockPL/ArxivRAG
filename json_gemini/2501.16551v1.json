{"title": "PackDiT: Joint Human Motion and Text Generation via Mutual Prompting", "authors": ["Zhongyu Jiang", "Wenhao Chai", "Zhuoran Zhou", "Cheng-Yen Yang", "Hsiang-Wei Huang", "Jenq-Neng Hwang"], "abstract": "Human motion generation has advanced markedly with the advent of diffusion models. Most recent studies have concentrated on generating motion sequences based on text prompts, commonly referred to as text-to-motion generation. However, the bidirectional generation of motion and text, enabling tasks such as motion-to-text alongside text-to-motion, has been largely unexplored. This capability is essential for aligning diverse modalities and supports unconditional generation. In this paper, we introduce PackDiT, the first diffusion-based generative model capable of performing various tasks simultaneously, including motion generation, motion prediction, text generation, text-to-motion, motion-to-text, and joint motion-text generation. Our core innovation leverages mutual blocks to integrate multiple diffusion transformers (DiTs) across different modalities seamlessly. We train PackDiT on the HumanML3D dataset, achieving state-of-the-art text-to-motion performance with an FID score of 0.106, along with superior results in motion prediction and in-between tasks. Our experiments further demonstrate that diffusion models are effective for motion-to-text generation, achieving performance comparable to that of autoregressive models.", "sections": [{"title": "1. Introduction", "content": "Human motion capture is widely used across multiple industries, including film production, video game development, and virtual reality (VR). However, setting up a motion capture studio is expensive, and the quality of the captured motion heavily depends on the actors' performance. With advancements in diffusion models [15, 33-35], recent years have seen substantial progress in Motion Generation [13, 17, 38, 39, 41-43, 45], which aims to automatically generate rich, realistic human motion sequences.\nMotion generation encompasses the production of human motion sequences with or without conditions from other modalities, such as action classes, text, audio, music, and speech. Among these, text stands out for its ability to convey detailed information about actions, speeds, directions, and goals, either explicitly or implicitly. For instance, HumanML3D [12] is a comprehensive text-to-motion generation dataset that provides well-annotated text-motion pairs derived from HumanAct12 [11] and AMASS [23]. Recent studies leverage such datasets to explore diffusion-based models for text-to-motion generation and autoregressive models for motion-to-text understanding.\nHowever, few methods can do both text-to-motion and motion-to-text generation. Recently, MotionGPT [17] uses the auto-regressive paradigm to achieve this goal. Addressing the challenges of effectively generating and integrating motion and text, we propose a novel framework, PackDiT, the first diffusion-based text-motion joint generation model. PackDiT stands out for its flexibility and capability to handle multiple tasks within a unified architecture, leveraging two independent Diffusion Transformers (DiTs), Motion DiT and Text DiT, with mutual blocks and multi-stage training strategies. PackDiT is initially pre-trained unconditionally, then jointly trained and fine-tuned, enhancing fidelity and alignment.\nWe evaluate PackDiT on the HumanML3D dataset [12] across a range of tasks and corresponding metrics. Compared to other state-of-the-art text-to-motion generative models, PackDiT achieves superior performance on the FID metric with fewer parameters. Additionally, PackDiT demonstrates leading performance in motion prediction and in-between tasks. Notably, we are the first to show that a diffusion-based generative model can perform motion-to-text generation, achieving comparable results to large language models (LLMs) trained on extensive text corpora.\nIn particular, we make the following contributions:\n\u2022 We are the first diffusion-based model that can accomplish diverse motion-relevant tasks, including text-to-motion, motion-to-text, and motion-text joint generation, etc.\n\u2022 We show that by adding mutual blocks between text and motion diffusion generative models (e.g. DiT), we can easily package two separated models to achieve good joint generation ability.\n\u2022 Our experiments show that our proposed method achieves state-of-the-art text-to-motion performance with FID as 0.106, as well as the motion prediction and motion in-between tasks."}, {"title": "2. Related Works", "content": null}, {"title": "2.1. Diffusion Model", "content": "For many years, researchers have been eager to find an effective method to generate various kinds of data, e.g., text, images, audio, and etc. After the creations of VAE [20], GAN [10], Normalizing Flows [29] and etc., diffusion models [2-5, 15, 33-35] are proposed and shown to provide the best quality of generated results by training the model to gradually denoise the randomly initialized noise data and generate the final result. Based on diffusion probabilistic model (DPM) [32], Ho et al. proposed denoising DPM (DDPM), which utilizes U-Net [30] to denoise the noisy data step-by-step to recover the original data. During forward diffusion of a DDPM, noisy data are generated by adding Gaussian noise to the original data step-by-step. In contrast, reverse diffusion aims to predict and remove the added Gaussian noise and gradually recover the original data. DDIM [33] is then proposed to accelerate the reverse diffusion of DDPM by skipping certain steps, and Score Matching Network [35] takes advantage of Stochastic Differential Equations (SDE) to build a more general and effective diffusion pipeline. To further scale up the diffusion model, Peebles et al. [25] propose the Diffusion Transformer (DiT), which utilizes transformers as the backbone of diffusion models.\nHowever, all previous works only focus on single-modality generation, while joint multi-modal generation is also critical for synthetic data generation and real applications. UniDiffuser [2] proposes a unified transformer-based diffusion model for joint Image-Text generation, which integrates text and image tokens into a unified diffusion model. Ruan et al. [31] propose a joint multi-modal generation pipeline for video-audio generation via feature fusing and rotation layers. Compared to other joint generation pipelines [2, 31, 36, 37], our approach does not necessitate training a unified generation model for all modalities. Instead, we employ independent generation models for each modality, integrating them through cross-attention layers. This design enhances the flexibility of PackDiT, handling joint generation tasks more effectively and efficiently."}, {"title": "2.2. Text-condition Human Motion Generation", "content": "Human motion generation aims to generate realistic and controllable human pose sequences. Usually, people adopt SMPL [22] or keypoint [6, 18, 21] as the representation of 3D human pose instead of 3D keypoint joints. Researchers have been exploring using text, action, audio, music, or even scenes and objects as the conditions to guide the human motion generation. Among all those conditions, text has a remarkable capacity to convey information related to various actions, speeds, directions, and destinations, either explicitly or implicitly. This feature makes the text an appealing medium for generating human motion. Text2Action [1] is the first to leverage GAN to generate a variety of motions from a given natural language description. Recently, diffusion models have been adopted to motion generation tasks successfully as well [8, 14, 16, 42, 43]. However, although these methods have achieved excellent results in motion generation, they cannot simultaneously accomplish the task of action understanding, such as motion-to-text. Recently, MotionGPT [17] uses the autoregressive paradigm of transformers to unify text-to-motion and motion-to-text within a single framework. In this paper, we use diffusion for the same purpose for the first time. As shown in Tab 1, compared to MotionGPT, our proposed framework additionally accomplishes joint generation."}, {"title": "3. Method", "content": "To simulatneously tackle the motion-to-text, text-to-motion, and joint generation issues, we propose PackDiT, which is a flexible motion and text generation pipeline and consists of two DiT models, i.e., Motion DiT, DM and Text DiT, DT. As shown in Table 1, compared with previous works based on diffusion models or large language models, PackDiT is able to achieve the most motion- and text-related generation tasks. For motion representations, we follow [12, 17] and represent motion of frame i as Mi \u2208 R263, where Mi = {Ri,h, v, Jr, Jp, Jv, F}. Ri is the global rotation of the human body, hi is the height of the root point, v is the velocity of the root point, Jr is the relative rotation of each of joints, Jp is the position of each joint in canonical view, Ju is the velocity of each joint, and F represent each foot is on the ground or now. Following UniDiffuser [2], a text encoder, BERT [9], and a text decoder, GPT-2 [27], are utilized for text generation."}, {"title": "3.1. Unconditional Motion Generation", "content": "To build a flexible multi-task joint generation diffusion pipeline, all inputs and outputs should be formulated into tokens, which allow us able to integrate mutual blocks for information exchange between Text and Motion. Therefore, we adopt DiT [25] as our baseline diffusion model. As shown on the left of Fig 2, for each timestep, ti, motion representations M = {M0, . . ., Mn} are converted to motion tokens after Patchify. Then, the Motion DiT, DM, generates the noise. The training procedure follows DDIM [33]. Therefore, the training loss of DM is\n\nL\u043c = ||\u0454 \u2013 Dm(\u221a\u0101tM+ \u221a1 \u2013 \u0101te, t)||2."}, {"title": "3.2. Unconditional Text Generation", "content": "Following UniDiffuser [2], to make the diffusion model generate texts, we utilize a text encoder, Tenc, and a text decoder, Tdec, for better text generation quality. As shown on the right of Fig 2, similar to our motion diffusion model, after encoder and forward diffusion, the text latent noise is fed to Text DiT, DT. After the reverse diffusion, all generated text tokens are treated as the prefix of the text decoder and used to generate the corresponding texts. Following DDIM [33], the training objective of DT is\n\nLT = ||\u0454 \u2013 DT(\u221a\u0101tT + \u221a1 \u2013 \u0101te, t)||2.\n\nHowever, the diffusion model struggles to directly generate text tokens because they are discrete and have relatively high dimensions. To reduce the dimensionality of the encoded text tokens from the text encoder and create more continuous text representations, we further train an additional projection model, P, which projects the encoded text tokens to the text latent tokens, T, used in DT, and re-projects the generated text tokens to the text decoder."}, {"title": "3.3. Mutual Prompting", "content": "Compared with other joint generation pipelines [2, 31, 36, 37], our pipeline does not require training a unified generation model for all modalities. Instead, the generation models for each modality operate independently and are integrated using mutual blocks. This architectural choice makes PackDiT significantly more flexible and capable of handling joint generation tasks more efficiently. Each modality-specific model can be optimized independently, allowing for specialized fine-tuning and improvements without affecting the entire system. This flexibility extends to scaling the system for new modalities, as new generation models can be added without extensive reconfiguration of the existing pipeline. As a result, PackDiT offers a robust and adaptable diverse joint generation.\nDuring training or inference, the intermediate tokens, M and T, from DM and DT generates qm, km, UM, IT, KT, and \u03c5\u03c4. Then, the mutual blocks are operated:\n\nM = M + Softmax (IKT)\u043e\u0442,\n\u221adk\nT = T + Softmax(ITEM ITKM)UM.\n\u221adk\n\nThe cross-attention layer is inserted into each self-attention-based DiT block to fuse the information from all modalities and achieve flexible training or inference."}, {"title": "3.4. Training Recipe", "content": "Step 1: Unconditional Pre-train. Since there are two independent DiTs in PackDiT, we can apply unconditional Pre-train on both of them to get better initial weights for the subsequent tasks. As shown in Algorithm 1, during unconditional pre-train, motion tokens and text tokens are sampled from the training dataset and are fed to DM and DT separately for standard unconditional diffusion training.\nStep 2: Joint Generation Training. To make PackDiT able to conduct joint Motion and Text generation and better align the features from two modalities, we conduct joint generation training. We sample the same timestep tm = t\u315c ~ Uniform(1, . . ., t) and the mutual blocks are enabled during training. Thus, both DiTs are trained together for feature alignment and joint generation.\nStep 3: Motion-to-Text and Text-to-Motion Fine-tuning. As demonstrated in Algorithm 1, during training for either the Motion-to-Text or Text-to-Motion task, only the generating modality undergoes forward diffusion. In contrast, the condition modality is directly fed to the conditional DiT after Patchify, with the timestep set to 0. Consequently, through the mutual blocks between the two DiTs, PackDiT effectively performs reliable conditional generation.\nStep 4: Joint Fine-Tuning. To train PackDiT on all four tasks, we employ a joint training approach, assigning a certain probability to each task at each iteration. For optimal performance in the evaluations detailed in Section 4, we further fine-tune the model on specific tasks, i.e., Text-to-Motion and Motion-to-Text generation, after the initial joint training phase. Therefore, the final training objective of PackDiT is to minimize the loss L:\n\nL = LM + 1. \u03a3\u03c4,\n\nwhere X is the term to balance the two objectives."}, {"title": "4. Experimental Results", "content": null}, {"title": "4.1. Evaluation Metrics and Datasets", "content": "Evaluation Metrics. Following [12, 13, 17, 38], Frechet Inception Distance (FID) is our primary metric for motion quality evaluation, which evaluates the feature distribution similarity between generated and real motions as detailed in [12]. Meanwhile, to measure the diversity of the generated motions, we use the Diversity (DIV) metric, which calculates the variance in features extracted from the motions as used in [12]. For text-motion retrieval evaluation, the accuracy of matching motions to their corresponding textual descriptions is assessed using the motion-retrieval precision (R Precision) metric, based on the feature space from [12], and measured by Top 1/2/3 retrieval accuracy. To evaluate the quality of generated motion captions, we adopt linguistic metrics from natural language processing studies as outlined in [13], including BLEU [24], and CIDEr [40].\nDataset. The HumanML3D dataset [12] is a comprehensive repository of 3D human motion sequences curated to advance research in human motion analysis and generation. It encompasses a diverse range of activities including walking, running, dancing, and more complex actions sourced from the AMASS dataset [23]. Comprising 14,616 motion sequences of durations between 2 and 10 seconds, each sequence is accompanied by multiple detailed textual annotations, enhancing its applicability for tasks such as text-to-motion and motion-to-text generation. The dataset incorporates a variety of actors to ensure broad representation across human movement patterns."}, {"title": "4.2. Training and Evaluation Details", "content": "We utilize a single NVIDIA A100 to train and evaluate PackDiT, which is developed on OpenDiT [44] and MotionGPT [17]. The number of parameters for each DiT of proposed PackDiT Tiny and Small is around 75M and 120M, which are similar to the setup of LMM [43]. With the batch size as 128, PackDiT is trained with the Adam [19] optimizer, and the learning rate is set to 10-4. The patch size is set to 1 for both DM and DT during evaluation, and more patch size setup is discussed in Ablation Study 4.4. As mentioned in Sec 3, a text encoder, a text decoder, and a projection model are used in the Text Generation pipeline. Following [2], BERT [9], and GPT-2 [27] are used as the text encoder and text decoder, respectively. The projection model, P, is trained with projection dimension 64 when the encoder and decoder models are frozen. The unconditional pre-train takes around 10 epochs. Then, the PackDiT is jointly trained with all tasks for 200 epochs. To achieve the best performance of PackDiT, the models used for evaluation are fine-tuned on specific tasks for 300 epochs after joint training with all tasks. The motion representations of PackDiT follows [12, 17] for fair comparison."}, {"title": "4.3. Experimental Results of All Tasks", "content": "Evaluation on Motion-related Tasks. Our proposed method is compared with other SOTA methods on the HumanML3D [12] dataset. As shown in Table 2, our PackDiT-Tiny and PackDiT-Small achieve 0.498 and 0.504 R@1 on text-to-motion task, which demonstrates a comparable performance with previous SOTA method LMM [43], while using a smaller amount of training data. In other tasks like motion prediction and motion in-between tasks, PackDiT achieves 0.701 and 0.119 FID scores, outperforming MotionGPT [17] by a large margin."}, {"title": "4.4. Ablation Studies", "content": "To further analyze the PackDiT, we conduct ablation study on several hyperparameters, training strategies and alternative models used by us. All ablation studies are evaluated on Text-to-Motion tasks with PackDiT-Tiny as default.\nTarget Dimension of Projection Model. As shown in Tab 4, the best performance is achieved when the target dimension of the projection model, P, is set to 256. 128 and 256 are closer to the dimension of BERT's hidden states and save more information from the original text tokens. However, based on the trade-off between accuracy and efficiency, we choose Dimp = 64 when we are training PackDiT-Tiny.\nText Encoder. Following [2], we utilize BERT [9] as our text encoder for the text generation pipeline. To further investigate the PackDiT, T5 [28], an Encoder-Decoder based Large Language Model, is applied to our text pipeline for a comparison. Only the encoder part of the T5-base is integrated. As illustrated in Tab 4, the performance of Text-to-Motion with BERT surpasses the T5 version by a remarkable margin since the pre-train weights of T5 are based on translation, summarization, question answering, and etc., which may not be suitable for motion-related tasks.\nPatch Size. We change the patch size of motion diffusion model and conduct an ablation study. According to Tab 4, the patch size 1 provides the best performance while patch size 4 significantly impacts the performance. We find that once the dimension of input tokens is similar to the hidden dimension of DiTs, the generation results are not reliable.\nUnconditional Pre-train. As indicated in Table 4, the Unconditional Pre-train improves the final Text-to-Motion Generation performance. This pre-training phase allows PackDiT to develop a better understanding of the underlying data distribution, which is crucial for generating realistic and coherent motions. By initializing the model with weights that are already attuned to the data characteristics, the subsequent training process is more efficient and effective. We plan to utilize more unpaired motion sequences and motion descriptions for future works and further improve the effectiveness of the Unconditional Pre-train."}, {"title": "5. Limitations", "content": "The performance of PackDiT heavily relies on the quality and diversity of the HumanML3D dataset. Limited data variety may hinder the generalization of the model. Also, current evaluation metrics such as FID and Recall may not fully capture the quality and realism of generated motions and texts, suggesting a need for more comprehensive performance assessment methods."}, {"title": "6. Conclusion", "content": "In this work, we presented PackDiT, a novel diffusion-based framework for joint human motion and text generation. PackDiT's unique dual Diffusion Transformer (DiT) architecture with mutual blocks enables efficient handling of multiple generation tasks, including text-to-motion, motion-to-text, motion prediction, and motion in-between generation within a unified model structure. This approach addresses critical limitations in previous models, which often restricted generation to single modalities or required complex configurations. Extensive experiments on the HumanML3D dataset demonstrate that PackDiT achieves state-of-the-art results, particularly in text-to-motion generation with an FID score of 0.106, as well as strong performance in motion prediction and in-between tasks. The mutual prompting mechanism facilitates enhanced information exchange, enabling PackDiT to produce high-quality, diverse outputs that closely align with human-generated data. PackDiT thus establishes a flexible and robust foundation for multi-modal generation, with broad implications for synthetic data creation, immersive environments, and human-computer interaction applications, setting a new standard for joint motion and text synthesis."}]}