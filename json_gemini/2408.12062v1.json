{"title": "Enhancing Sampling Protocol for Robust Point Cloud Classification", "authors": ["Chongshou Li", "Pin Tang", "Xinke Li", "Tianrui Li"], "abstract": "Established sampling protocols for 3D point cloud learning, such as Farthest Point Sampling (FPS) and Fixed Sample Size (FSS), have long been recognized and utilized. However, real-world data often suffer from corrputions such as sensor noise, which violates the benignness assumption of point cloud in current protocols. Consequently, they are notably vulnerable to noise, posing significant safety risks in critical applications like autonomous driving. To address these issues, we propose an enhanced point cloud sampling protocol, PointDR, which comprises two components: 1) Downsampling for key point identification and 2) Resampling for flexible sample size. Furthermore, differentiated strategies are implemented for training and inference processes. Particularly, an isolation-rated weight considering local density is designed for the downsampling method, assisting it in performing random key points selection in the training phase and bypassing noise in the inference phase. A local-geometry-preserved upsampling is incorporated into resampling, facilitating it to maintain a stochastic sample size in the training stage and complete in-sufficient data in the inference. It is crucial to note that the proposed protocol is free of model architecture altering and extra learning, thus minimal efforts are demanded for its re-placement of the existing one. Despite the simplicity, it sub-stantially improves the robustness of point cloud learning, showcased by outperforming the state-of-the-art methods on multiple benchmarks of corrupted point cloud classification. The code will be available upon the paper's acceptance.", "sections": [{"title": "1 Introduction", "content": "In the rapidly evolving field of 3D data perception via deep learning (Qi et al. 2017a,b; Guo et al. 2020), point cloud sampling serves as a critical component in the standard learning and recognition pipeline (Hu et al. 2020; Qian et al. 2022; Yu et al. 2022; Zhang et al. 2022b). Following the legacy of pioneer works (Qi et al. 2017a,b), existing sampling protocols are primarily designed and optimized for clean data, without taking into account corruptions. However, due to the high complexity of real-world, point cloud data are almost always incomplete and with noise in practice (Ren et al. 2022), posing threats to 3D deep learning applications. For example, noisy background points or slight perturbations generated by inaccurate processing or sensor error can significantly decrease the deep model performance (Ren et al. 2022; Sun et al. 2022). Such performance drops can lead to serious safety consequences, especially in critical 3D applications like autonomous driving. Therefore, it is necessary to rethink and redesign point cloud sampling protocols with a focus on robustness against corruptions to ensure reliable 3D deep learning in real-world conditions.\nOne crucial limitation of established sampling protocols is that they are sub-optimal under the corrupted data distribution. For instance, the protocol samples a fixed number of points in data preparation, namely, Fixed Sample Size (FSS) (Qi et al. 2017a). This convention overlooks the facts that point clouds in the real world naturally vary in size and density. These varied sizes are even obvious in particular corruptions such as occlusions and density-related noise (Sun et al. 2022; Ren et al. 2022). Another aspect is that the widely used Farthest Point Sampling (FPS) (Eldar et al. 1997) for key points selection is especially vulnerable to outliers due to its inherent basis of Euclidean distance and sensitivity to sparse points (Yan et al. 2020). Several works have considered updating a specific step to deal with this issue, like PointASNL (Yan et al. 2020) and ADS (Hong, Chou, and Liu 2023). The learning-based methods put extra"}, {"title": "2 Related Work", "content": "Point Cloud Sampling. Point cloud sampling techiniques typically consist of: 1) downsampling, also known as \u201csimplification\" (Dovrat, Lang, and Avidan 2019), and 2) up-sampling (Zhang et al. 2022b). These techiniques are divided into non-learning-based and learning-based methods (Zhang et al. 2022b). Traditional non-learning-based down-sampling techniques include Farthest Point Sampling (FPS) (Eldar et al. 1997), Random Sampling (RS) (Hu et al. 2020), Poisson Disk Sampling (PDS) (Ying et al. 2013), and voxelization (Lv, Lin, and Zhao 2021). Conversely, learning-based downsampling methods account for downstream tasks (Dovrat, Lang, and Avidan 2019; Lang, Manor, and Avidan 2020; Nezhadarya et al. 2020; Qian et al. 2020, 2023). Upsampling is categorized into learning-based (Yu et al.\nPoint Cloud Robust Classification. PointNet (Qi et al. 2017a) has been a trailblazer in utilizing deep learning for point cloud analysis, with notable extensions such as Point-Net++ (Qi et al. 2017b), GDANet (Xu et al. 2021), Point Transformer (PCT) (Guo et al. 2021), and CurveNet (Xiang et al. 2021). However, the performance of these models significantly deteriorates with corrupted real-world data (Uy et al. 2019; Ren et al. 2022; Sun et al. 2022). To tackle this issue, existing literature offers three main types of so-lutions. The first focuses on modifying the model by altering its structure or training strategies, such as pooling operations based on sorting (Sun et al. 2020) and model aggregation (Dong et al. 2020). The second type includes certified methods, exemplified by Pointguard, which theoretically enhances model robustness through certified classification (Liu, Jia, and Gong 2021). The third type is data-driven approaches that directly cleanse corrupted data, with notable methods including IF-defense (Wu et al. 2020) and DUP-Net (Zhou et al. 2019). This paper aims to advance robustness from a new perspective by refining point cloud sampling protocol during data preparation.\nPoint Cloud Augmentation. Point cloud augmentation is a widely recognized practice in the deep learning community, employed to improve the generalization capabilities of neural networks. Traditional augmentation methods, including random scaling, rotation, and jitter, are somewhat limited in their effectiveness for point cloud analysis (Zhu, Fan, and Weng 2024). Recent advancements have introduced sophisticated techniques such as PointCutMix (Zhang et al. 2022a), PointAugment (Li et al. 2020), PointMixup (Chen et al. 2020), and PointWOLF (Kim et al. 2021). However, they suffer from various limitations. For instance, while PointMixup (Chen et al. 2020) and PointWOLF (Kim et al. 2021) largely rely on predefined transformations, PointAug-ment (Li et al. 2020) emphasizes global transformations, often at the expense of local geometric details. To our knowledge, the sampling augmentation of point cloud for robust classification has been largely unexplored. In this work, we aim to enhance sampling protocols specifically tailored for robust point cloud classification, addressing this critical gap."}, {"title": "3 Proposed Sampling Protocol", "content": "Mainstream 3D classification models follow the protocol focusing on a clean point cloud with a fixed sample size. Formally, they consider an input of a point cloud P = {pi}\\{1, where p\u2081 \u2208 R\u00b3 and N is the fixed number of points. Specif-"}, {"title": "3.2 Proposed Sampling Techniques", "content": "Before proposing the new sampling protocol, we introduce two key techniques of point cloud sampling.\nPoint Reweighting. The point-wise weight can be defined by the concept of Isolation Rate. At first, we calculate the radius of a sphere containing k nearest neighbors of each point in P, which is given by,\nri = \\underset{qj \\in N}{max} ||Pi - qj|| 2 \\tag{3}\nwhere NCP is the set of k neighbors of i-th point pr. We further define Isolation Rate for each point as wi, given by,\nwi = Prded; (d < r), D\u2081 = {||qj \u2013 Pi||2 : \u2200qj \u2208 N } \\tag{4}\nwhere r = Median({d}\\\u2081) is the median of all radius and Pry (X) is the probability of X given condition Y. The isolation rate of a point suggests the extent of a point being isolated, i.e., far from others in a probability way. Although a few associated concepts were proposed to calculate the exact local radius of points (Sotoodeh 2006) and identify outliers, the isolation rate is naturally fit for point weighting due to the probability representation. We apply it in our downsampling stage of the proposed protocol.\nLocal-geometry-preserved Interpolation. A learning-free interpolation algorithm is proposed for our new sampling protocol. The algorithm's objective is to densify the current point cloud while preserving the local geometry of each point. To achieve this purpose, we meticulously interpolate each point and its neighbor, confining the interpolation on the tangent plane. The precise step is outlined in Algorithm 1, with mathematical details provided in Supplementary materials. This tangent-plane-based interpolation approach ensures that the upsampled point cloud retains the"}, {"title": "3.3 PointDR: Enhanced Sampling Protocol", "content": "Based on the above techniques, we propose the enhanced sampling protocol, PointDR, including resample and down-sample protocols.\nResampling Protocol. During the inference stage, we incorporate the upsampling into preprocessing when the input sample size is insufficient. Given an input point cloud P, we implement PU \u2206P if |P| < N. Here \u2206P is generated by performing Algorithm 1 on N \u2013 |P| points of P. During the training of point cloud processing models, we randomize the number of points in the training data. In particular, we sample N + AN points from the original N-size point cloud P, where AN is a random variable given by [2U[-1,1] . N \u2212 N] with continuous uniform distribution U[\u00b7, \u00b7]. The sign of AN indicates distinctive strategies of resampling, namely point adding and dropping, given by\nP = \\begin{cases}\nPUAP & \\DeltaN > 0, \\\\\n\u03a1\u0394\u03a1 & \\DeltaN < 0.\n\\end{cases} \\tag{5}\nwhere P is the resampled point cloud and AP is sampled by\n\u0394\u03a1 ~ \\begin{cases}\n{\u0393 : \u0393 \u0421 \u0418, |\u0413| = \u0394\u039d} & \\DeltaN > 0, \\\\\n{\u0393 : \u0393 \u2286 \u2116, |\u0413| = -\u2206N} & \\DeltaN < 0,\n\\end{cases} \\tag{6}\nwhere U is the point-wise upsampled P by Algorithm 1,\nand Nk is the KNN points of a random center p\u2081 with k ~ U[|\u2206N], N]. This stochastic strategy aims to enhance the model's adaptability, given that point cloud data in real-world scenarios inherently exhibit non-fixed variable sizes.\nDownsampling Protocol. Recall the iteration rule of FPS in Equation (1), which is sensitive to outliers. To address the issue, during the inference stage, we incorporate point weights to modify the equation into a weighted version,\nst = arg max wi min ||Pi - s||2. \\tag{7}\nPiEP SESt\nThis formulation introduces a new objective into FPS down-sampling, which is suppressing the selection of points with high weights. In practice, we binarize the point weights via a quantile-based threshold w (e.g., w is 0.95 quantile of"}, {"title": "4 Experimental Studies", "content": "Dataset and Model. We utilize models trained on ModelNet40 (Wu et al. 2015b) to conduct experiments on three corrupted datasets: ModelNet40-C, PointCloud-C, and OmniObject-C. The ModelNet40-C (Sun et al. 2022) and PointCloud-C (Ren et al. 2022) are datasets applying 15 and 7 distinct corruptions to ModelNet40's test set, total-ing 2,468 objects. The OmniObject-C, based on OmniOb-ject3D (Wu et al. 2023), has 362 objects corrupted by the methods proposed in (Ren et al. 2022). For 3D deep models, we employ PointNet (Qi et al. 2017a), PointNet++ (Qi et al. 2017b), GDANet (Xu et al. 2021), CurveNet (Xiang et al. 2021), PCT (Guo et al. 2021), following the pipeline in ModelNet40-C including batch size and training protocol. We note that all experiments are run on NVIDIA GeForce RTX 3090 GPUs.\nParameters Setting. The number of nearest neighbors k used in point weight computation is set to 20, which follows the common setup. During the inference phase, the down-sampling protocol applies a threshold w of 0.95, exploring the learning as depicted in Figure 3, meaning that FFPS fil-ters out points within the lowest 5% of point weights.\nEvaluation Protocol. We report the error rates (ER) and mean error rates (mER) across multiple corruptions on the three corrupted datasets for performance evaluation. A smaller ER indicates a superior performance. More implementation details are referred to Appendix."}, {"title": "4.2 Main Results", "content": "Overall Results. Mean error rates (mERs) for the three corrupted datasets are presented in Table 1. To facilitate a comprehensive comparison, we include multiple baseline models. The results clearly indicate that the proposed PointDR"}, {"title": "4.3 Ablation Studies", "content": "In the ablation studies, we use PCT (Guo et al. 2021) and CurveNet (Xiang et al. 2021) as 3D deep models on two datasets: ModelNet40-C (Sun et al. 2022) and PointCloud-"}, {"title": "4.4 Visualization Study", "content": "Isolation Rate. In Figure 4, we visualize the distribution of point-wise isolation rates for three example objects. The proposed rate effectively identifies boundary points and outliers, thereby enhancing subsequent point cloud sampling and improving learning robustness against corruption.\nLocal-geometry-preserved Interpolation. Figure 5 vi-sually compares the results of three upsampling techniques on four example objects. It is evident that both Jitter and SI (Huang et al. 2022) struggle with corrupted data, particularly when it is sparse and non-uniform. In contrast, the proposed LG method effectively combines completion and uniformity in the upsampling process."}, {"title": "5 Conclusion", "content": "This work focuses on the safety issue of 3D point cloud deep learning. It indicates that the current sampling protocol is not optimized for corrupted 3D point cloud analysis, thus posing a potential threat to 3D applications. Therefore, we introduce PointDR, an enhanced sampling protocol for point"}]}