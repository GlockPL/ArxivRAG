{"title": "Enhancing Sampling Protocol for Robust Point Cloud Classification", "authors": ["Chongshou Li", "Pin Tang", "Xinke Li", "Tianrui Li"], "abstract": "Established sampling protocols for 3D point cloud learning, such as Farthest Point Sampling (FPS) and Fixed Sample Size (FSS), have long been recognized and utilized. However, real-world data often suffer from corrputions such as sensor noise, which violates the benignness assumption of point cloud in current protocols. Consequently, they are notably vulnerable to noise, posing significant safety risks in critical applications like autonomous driving. To address these issues, we propose an enhanced point cloud sampling protocol, PointDR, which comprises two components: 1) Downsampling for key point identification and 2) Resampling for flexible sample size. Furthermore, differentiated strategies are implemented for training and inference processes. Particularly, an isolation-rated weight considering local density is designed for the downsampling method, assisting it in performing random key points selection in the training phase and bypassing noise in the inference phase. A local-geometry-preserved upsampling is incorporated into resampling, facilitating it to maintain a stochastic sample size in the training stage and complete insufficient data in the inference. It is crucial to note that the proposed protocol is free of model architecture altering and extra learning, thus minimal efforts are demanded for its replacement of the existing one. Despite the simplicity, it substantially improves the robustness of point cloud learning, showcased by outperforming the state-of-the-art methods on multiple benchmarks of corrupted point cloud classification.", "sections": [{"title": "1 Introduction", "content": "In the rapidly evolving field of 3D data perception via deep learning (Qi et al. 2017a,b; Guo et al. 2020), point cloud sampling serves as a critical component in the standard learning and recognition pipeline (Hu et al. 2020; Qian et al. 2022; Yu et al. 2022; Zhang et al. 2022b). Following the legacy of pioneer works (Qi et al. 2017a,b), existing sampling protocols are primarily designed and optimized for clean data, without taking into account corruptions. However, due to the high complexity of real-world, point cloud data are almost always incomplete and with noise in practice (Ren et al. 2022), posing threats to 3D deep learning applications. For example, noisy background points or slight perturbations generated by inaccurate processing or sensor error can significantly decrease the deep model performance (Ren et al. 2022; Sun et al. 2022). Such performance drops can lead to serious safety consequences, especially in critical 3D applications like autonomous driving. Therefore, it is necessary to rethink and redesign point cloud sampling protocols with a focus on robustness against corruptions to ensure reliable 3D deep learning in real-world conditions.\nOne crucial limitation of established sampling protocols is that they are sub-optimal under the corrupted data distribution. For instance, the protocol samples a fixed number of points in data preparation, namely, Fixed Sample Size (FSS) (Qi et al. 2017a). This convention overlooks the facts that point clouds in the real world naturally vary in size and density. These varied sizes are even obvious in particular corruptions such as occlusions and density-related noise (Sun et al. 2022; Ren et al. 2022). Another aspect is that the widely used Farthest Point Sampling (FPS) (Eldar et al. 1997) for key points selection is especially vulnerable to outliers due to its inherent basis of Euclidean distance and sensitivity to sparse points (Yan et al. 2020). Several works have considered updating a specific step to deal with this issue, like PointASNL (Yan et al. 2020) and ADS (Hong, Chou, and Liu 2023). The learning-based methods put extra effort into module training and may be potentially overfitting. Overall, none of them propose a comprehensive and alternative solution to overcome the sampling protocol limitations.\nTo overcome these limitations, we propose an enhanced point cloud sampling protocol, PointDR, by revising the Downsampling process and integrating Resampling process before inputting, as illustrated in Figure 1. The implementation of the proposed protocol involves randomizing the sampling during training and processing noisy point clouds during inference. To achieve this, the downsampling process assisted by point reweighting is applied to ensure that potential outliers are not captured as key points. The point weight is named as isolation rate evaluating the extent of local isolation for a point. Moreover, the proposed resampling process randomizes the sample size during training; and restores insufficient point clouds in the inference stage. Inspired by shape-invariant perturbation (Huang et al. 2022), we realize the upsampling of resampling process via a tangent plane interpolation technique that enhances the density of point cloud data while preserving local geometry. Overall, the enhanced sampling protocol is learning-free thus straightforward to implement and can be seamlessly integrated into the existing point cloud analysis pipeline.\nOur contributions are summarized as follows:\n\u2022 We first comprehensively revisit the long-existing sample protocol for point cloud learning through the lens of data corruption. Based on the analysis, we propose an alternative protocol to enhance the robustness of point cloud learning.\n\u2022 We develop two learning-free techniques as the key of protocol, namely, point reweighting and tangent plane interpolation, which can deal with point cloud corruption in different aspects. The whole proposed protocol is free of model architecture change and extra learning, thus it can be implemented to replace the current protocol with minimal pains and fits almost all 3D deep models.\n\u2022 Extensive experiments are conducted on multiple corrupted 3D point cloud datasets. The results have demonstrated that the proposed protocol is able to improve the robustness of 3D point cloud classification and outperform the latest methods."}, {"title": "2 Related Work", "content": "Point Cloud Sampling. Point cloud sampling techiniques typically consist of: 1) downsampling, also known as \u201csimplification\" (Dovrat, Lang, and Avidan 2019), and 2) upsampling (Zhang et al. 2022b). These techiniques are divided into non-learning-based and learning-based methods (Zhang et al. 2022b). Traditional non-learning-based downsampling techniques include Farthest Point Sampling (FPS) (Eldar et al. 1997), Random Sampling (RS) (Hu et al. 2020), Poisson Disk Sampling (PDS) (Ying et al. 2013), and voxelization (Lv, Lin, and Zhao 2021). Conversely, learning-based downsampling methods account for downstream tasks (Dovrat, Lang, and Avidan 2019; Lang, Manor, and Avidan 2020; Nezhadarya et al. 2020; Qian et al. 2020, 2023). Upsampling is categorized into learning-based (Yu et al. 2018b,a; Yifan et al. 2019; Dai et al. 2020; Ye et al. 2021; Qiu, Anwar, and Barnes 2022; He et al. 2023a,b) and non-learning-based approaches (Alexa et al. 2003; Lipman et al. 2007; Huang et al. 2009, 2013; Wu et al. 2015a). The non-learning-based sampling techiniques are particularly susceptible to outliers due to their inherent structural limitations; meanwhile, the learning-based methods are either also sensitive to noise or dependent on downstream tasks and prone to overfitting. This paper introduces simple yet effective robust point cloud sampling techiniques to overcome these challenges.\nPoint Cloud Robust Classification. PointNet (Qi et al. 2017a) has been a trailblazer in utilizing deep learning for point cloud analysis, with notable extensions such as PointNet++ (Qi et al. 2017b), GDANet (Xu et al. 2021), Point Transformer (PCT) (Guo et al. 2021), and CurveNet (Xiang et al. 2021). However, the performance of these models significantly deteriorates with corrupted real-world data (Uy et al. 2019; Ren et al. 2022; Sun et al. 2022). To tackle this issue, existing literature offers three main types of solutions. The first focuses on modifying the model by altering its structure or training strategies, such as pooling operations based on sorting (Sun et al. 2020) and model aggregation (Dong et al. 2020). The second type includes certified methods, exemplified by Pointguard, which theoretically enhances model robustness through certified classification (Liu, Jia, and Gong 2021). The third type is data-driven approaches that directly cleanse corrupted data, with notable methods including IF-defense (Wu et al. 2020) and DUP-Net (Zhou et al. 2019). This paper aims to advance robustness from a new perspective by refining point cloud sampling protocol during data preparation.\nPoint Cloud Augmentation. Point cloud augmentation is a widely recognized practice in the deep learning community, employed to improve the generalization capabilities of neural networks. Traditional augmentation methods, including random scaling, rotation, and jitter, are somewhat limited in their effectiveness for point cloud analysis (Zhu, Fan, and Weng 2024). Recent advancements have introduced sophisticated techniques such as PointCutMix (Zhang et al. 2022a), PointAugment (Li et al. 2020), PointMixup (Chen et al. 2020), and PointWOLF (Kim et al. 2021). However, they suffer from various limitations. For instance, while PointMixup (Chen et al. 2020) and PointWOLF (Kim et al. 2021) largely rely on predefined transformations, PointAugment (Li et al. 2020) emphasizes global transformations, often at the expense of local geometric details. To our knowledge, the sampling augmentation of point cloud for robust classification has been largely unexplored. In this work, we aim to enhance sampling protocols specifically tailored for robust point cloud classification, addressing this critical gap."}, {"title": "3 Proposed Sampling Protocol", "content": "3.1 Existing Protocol for Noisy Point Cloud\nMainstream 3D classification models follow the protocol focusing on a clean point cloud with a fixed sample size. Formally, they consider an input of a point cloud P = {pi}\\{1, where p\u2081 \u2208 R\u00b3 and N is the fixed number of points. Specifically, this existing protocol used farthest point sampling (FPS) to select key points from P as anchors and no further processing for input. FPS is achieved by updating a subset SCP with the points st iteratively, given by\nst = arg max min ||pi - s||2, (1)\npiEP SES\nwhere t is the iteration time.\nHowever, in this work, we consider real-world applications, where point clouds are always noisy and with non-fixed sample size. A noisy point cloud P' can be formulated by\nP' = P\\Ps UO (2)\nwhere Ps is the subset of clean point cloud P and O is the added noise points, i.e., outliers. For instance, they could be caused by occlusion and sensor error (Sun et al. 2022), respectively. The formulation (2) indicates the robustness issue of the current sampling protocol working on noisy point clouds from three aspects:\n\u2022 The presence of outliers O could affect model trained on outlier-free data. In addition, FPS is essentially sensitive to outliers since Eq. (1) tends to sample far and sparse points.\n\u2022 The subtraction of Ps from P results in a loss of information for model input. There is no counterpart mechanism against it in the current protocol.\n\u2022 The uncertainty in sample size of both Ps and O introduces variability in the final point number of P', which violates fixed-number input in current sampling.\nOverall, the above issues are evident in recent works (Ren et al. 2022; Sun et al. 2022) demonstrating the unideal performance of conventional sampling protocol against point cloud corruption. To overcome the issues, we attempt to revise the protocol for robust point cloud learning."}, {"title": "3.2 Proposed Sampling Techniques", "content": "Before proposing the new sampling protocol, we introduce two key techniques of point cloud sampling.\nPoint Reweighting. The point-wise weight can be defined by the concept of Isolation Rate. At first, we calculate the radius of a sphere containing k nearest neighbors of each point in P, which is given by,\nri = max ||pi - qj|| 2 (3)\nqjEN\nwhere NCP is the set of k neighbors of i-th point pr. We further define Isolation Rate for each point as wi, given by,\nwi = Prded; (d < ri), D\u2081 = {||qj \u2013 Pi||2 : \u2200qj \u2208 N } (4)\nwhere r = Median({d}\\{D\u2081}) is the median of all radius and Pr[X] (X) is the probability of X given condition Y. The isolation rate of a point suggests the extent of a point being isolated, i.e., far from others in a probability way. Although a few associated concepts were proposed to calculate the exact local radius of points (Sotoodeh 2006) and identify outliers, the isolation rate is naturally fit for point weighting due to the probability representation. We apply it in our downsampling stage of the proposed protocol.\nLocal-geometry-preserved Interpolation. A learning-free interpolation algorithm is proposed for our new sampling protocol. The algorithm's objective is to densify the current point cloud while preserving the local geometry of each point. To achieve this purpose, we meticulously interpolate each point and its neighbor, confining the interpolation on the tangent plane. The precise step is outlined in Algorithm 1, with mathematical details provided in Supplementary materials. This tangent-plane-based interpolation approach ensures that the upsampled point cloud retains the"}, {"title": "3.3 PointDR: Enhanced Sampling Protocol", "content": "Based on the above techniques, we propose the enhanced sampling protocol, PointDR, including resample and downsample protocols.\nResampling Protocol. During the inference stage, we incorporate the upsampling into preprocessing when the input sample size is insufficient. Given an input point cloud P, we implement PU \u2206P if |P| < N. Here \u2206P is generated by performing Algorithm 1 on N \u2013 |P| points of P. During the training of point cloud processing models, we randomize the number of points in the training data. In particular, we sample N + AN points from the original N-size point cloud P, where AN is a random variable given by [2U[-1,1] . N \u2212 N] with continuous uniform distribution U[\u00b7, \u00b7]. The sign of AN indicates distinctive strategies of resampling, namely point adding and dropping, given by\nP = { PUAP \u0394\u039d > 0, P\u0394P \u0394\u039d < 0. (5)\nwhere P is the resampled point cloud and AP is sampled by\n\u0394\u03a1 ~ { {\u0413 : \u0413 \u0421 \u0418, |\u0413| = \u0394\u039d} \u0394\u039d > 0, {\u0413 : \u0413 \u2286 \u2116, |\u0413| = -\u2206N} \u0394\u039d < 0, (6)\nwhere U is the point-wise upsampled P by Algorithm 1, and Nk is the KNN points of a random center p\u2081 with k ~ U[|\u2206N], N]. This stochastic strategy aims to enhance the model's adaptability, given that point cloud data in real-world scenarios inherently exhibit non-fixed variable sizes.\nDownsampling Protocol. Recall the iteration rule of FPS in Equation (1), which is sensitive to outliers. To address the issue, during the inference stage, we incorporate point weights to modify the equation into a weighted version,\nst = arg max wi min ||Pi - s||2. (7)\npiEP SESt\nThis formulation introduces a new objective into FPS downsampling, which is suppressing the selection of points with high weights. In practice, we binarize the point weights via a quantile-based threshold w (e.g., w is 0.95 quantile of {w1,\u2026\u2026, wn}) and replace the continuous weights. We empirically find that the binary weights are easy to integrate with FPS implementation and show a better performance. The binarized version method is called Filtered FPS (FFPS). During the training stage, we perform weighted downsampling (WD) by stochastic subset selection, namely,\nSt ~ Cat(P; w), (8)\nwhere Cat(;) is the categorical distribution, and w = {W1,\u2026\u2026, wn}. In addition, we extend the downsampling method to 3D models free of key point selection operations, like GDANet (Xu et al. 2021). In this case, We simply replace the pooling operation on features of the whole point cloud by pooling on the downsample-selected subset.\nComputational Cost. We design the sampling protocol in the way that poses minimal computational effort beyond the original protocol. Particularly, the implementation of FPS in conventional protocol involves the calculation of point paired distances with complexity of O(N2). The proposed point reweighting and interpolation (for all points) techniques can utilize the same paired distances and induce extra operations with O(kN) complexity. Such extra computation effort is minor since k << N. More experimental results of computational costs are presented in Appendix."}, {"title": "4 Experimental Studies", "content": "4.1 Experimental Setup\nDataset and Model. We utilize models trained on ModelNet40 (Wu et al. 2015b) to conduct experiments on three corrupted datasets: ModelNet40-C, PointCloud-C, and OmniObject-C. The ModelNet40-C (Sun et al. 2022) and PointCloud-C (Ren et al. 2022) are datasets applying 15 and 7 distinct corruptions to ModelNet40's test set, totaling 2,468 objects. The OmniObject-C, based on OmniObject3D (Wu et al. 2023), has 362 objects corrupted by the methods proposed in (Ren et al. 2022). For 3D deep models, we employ PointNet (Qi et al. 2017a), PointNet++ (Qi et al. 2017b), GDANet (Xu et al. 2021), CurveNet (Xiang et al. 2021), PCT (Guo et al. 2021), following the pipeline in ModelNet40-C including batch size and training protocol. We note that all experiments are run on NVIDIA GeForce RTX 3090 GPUs.\nParameters Setting. The number of nearest neighbors k used in point weight computation is set to 20, which follows the common setup. During the inference phase, the downsampling protocol applies a threshold w of 0.95, exploring the learning as depicted in Figure 3, meaning that FFPS filters out points within the lowest 5% of point weights.\nEvaluation Protocol. We report the error rates (ER) and mean error rates (mER) across multiple corruptions on the three corrupted datasets for performance evaluation. A smaller ER indicates a superior performance. More implementation details are referred to Appendix.\n4.2 Main Results\nOverall Results. Mean error rates (mERs) for the three corrupted datasets are presented in Table 1. To facilitate a comprehensive comparison, we include multiple baseline models. The results clearly indicate that the proposed PointDR significantly enhances PCT and CurveNet; mERs decrease by approximately 10% across all datasets, with the most substantial improvement observed in PointCloud-C."}, {"title": "4.3 Ablation Studies", "content": "In the ablation studies, we use PCT (Guo et al. 2021) and CurveNet (Xiang et al. 2021) as 3D deep models on two datasets: ModelNet40-C (Sun et al. 2022) and PointCloud- C (PCC) (Ren et al. 2022). Further details on implementation and hyperparameters are provided in the supplementary materials.\nSampling Protocols in Training. We compared various sampling protocols during the training phase. As shown in Table 5, the combination of randomized size sampling (RSS) and stochastic downsampling consistently delivers the best performance. removing RSS or substituting the proposed downsampling method with FPS significantly degrades the results. Notably, FFPS achieves the second-based performance, highlighting the importance of point reweighting.\nRandom Size Sampling. We investigate the impact of various resampling techniques during training. As shown in Table 6, we explore different methods for increasing and reducing sample sizes. The results indicate that the proposed KNN method plays a crucial role in enhancing performance. This suggests that stochastically determining the localness of the point dropping, i. e, neighbor size, can improve robustness against corruptions. Additionally, we compare our upsampling method in Algorithm 1 with Shape-invariant perturbation (SI), which conducts per-point perturbation on the tangent plane (Huang et al. 2022). The superiority of our method over SI shows that perturbation direction based on neighbor points preserves more local information than random directions.\nEffect of Quantile-based Threshold w of FFPS. As il-"}, {"title": "4.4 Visualization Study", "content": "Isolation Rate. In Figure 4, we visualize the distribution of point-wise isolation rates for three example objects. The proposed rate effectively identifies boundary points and outliers, thereby enhancing subsequent point cloud sampling and improving learning robustness against corruption.\nLocal-geometry-preserved Interpolation. Figure 5 visually compares the results of three upsampling techniques on four example objects. It is evident that both Jitter and SI (Huang et al. 2022) struggle with corrupted data, particularly when it is sparse and non-uniform. In contrast, the proposed LG method effectively combines completion and uniformity in the upsampling process."}, {"title": "5 Conclusion", "content": "This work focuses on the safety issue of 3D point cloud deep learning. It indicates that the current sampling protocol is not optimized for corrupted 3D point cloud analysis, thus posing a potential threat to 3D applications. Therefore, we introduce PointDR, an enhanced sampling protocol for point"}]}