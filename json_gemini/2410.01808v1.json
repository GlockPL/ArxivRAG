{"title": "AI Horizon Scanning - White Paper p3395\nPart I. Areas of Attention", "authors": ["Marina Cort\u00eas", "Andrew R. Liddle", "Christos Emmanouilidis", "Anthony E. Kelly", "Ken Matusow", "Ragu Ragunathan", "Jayne M. Suess", "George Tambouratzis", "Janusz Zalewski", "David A. Bray"], "abstract": "Generative Artificial Intelligence (AI) models may carry societal transformation to an\nextent demanding a delicate balance between opportunity and risk. This manuscript is the\nfirst of a series of White Papers informing the development of IEEE-SA's p3995 'Standard\nfor the Implementation of Safeguards, Controls, and Preventive Techniques for Artificial\nIntelligence (AI) Models' Chair: Marina Cort\u00eas. In this first horizon-scanning we identify\nkey attention areas for standards activities in AI. We examine different principles for reg-\nulatory efforts, and review notions of accountability, privacy, data rights and mis-use. As\na safeguards standard we devote significant attention to the stability of global infrastruc-\ntures and consider a possible over-dependence on cloud computing that may result from\ndensely coupled AI components. We review the recent cascade-failure-like Crowdstrike\nevent in July 2024, as an illustration of potential impacts on critical infrastructures from\nAI-induced incidents in the (near) future. Upcoming articles in this IEEE-SA White Pa-\nper series will focus on regulatory initiatives, technology evolution and the role of AI in\nspecific domains.", "sections": [{"title": "1 Introduction", "content": "We live in an era of unprecedented technological leaps. As we move towards the end of this\ndecade it will become increasingly challenging to discern whether online content is accurate\nor inauthentic [1]. The massive increase in information we have access to has made under-\nstanding the world harder, not easier. Across higher education, culture, physics, to name a\nfew areas, life has become very confusing [2]. The rapid development and adoption of Gener-\native Artificial Intelligence (GenAI) technologies, currently based on Large Language Models\n(LLMs), adds both novelty and uncertainty to our interpretation of an already complex world.\nIn the age of GenAI the distinction between human and bot-generated content, as well as au-\nthentic and inauthentic content is becoming increasingly challenging to societies across the\nboard on the planet.\nThe present authors are all ongoing volunteer contributors to the emerging IEEE\u00b9 standard\np3395 \u2018Standard for the Implementation of Safeguards, Controls, and Preventive Techniques\nfor Artificial Intelligence (AI) Models', Chair: Marina Cort\u00e9s, [3]. Ours was the first IEEE\nworking group in AI standards, emerging at the time of the issuing of the US presidential\nExecutive Order on AI development and use, on October 30th, 2023 [4]. While IEEE is a\nfully-international organisation independent of any national government, the Executive Order\nnevertheless provided important impetus and we held our kickoff meeting on November 2nd,\n2023, just three days after its announcement.\nOur aim here is to offer a cross-section of perspectives and realities, which reflect the\ndiversity of nationalities and expertise within our working group (see also Section 14). The\nreflections in this article are meant to be timely, but tentative, responses to a rapidly-changing\nlandscape. The document reflects our optimism that our society is equipped to respond with\nthe expertise necessary to address the organisational challenges posed by AI in its many forms."}, {"title": "1.1 Our article series", "content": "The present article focuses on highlighting a variety of attention areas that will be crucial\nto the beneficial development of AI technologies, and is the first in a sequence of works that\nexplore different areas of AI. In companion articles we will provide:"}, {"title": "1.2 Generative AI: Historical Context setting", "content": "Large Language Models emerged from a long tradition of information theory and noisy chan-\nnel models. Recursive neural networks used for machine learning-based natural language\nprocessing were augmented in 2017 with a mechanism that enabled highly-efficient parallel\nprocessing of input strings [9]. Though the proposed \u2018transformer'architecture was focused\non text strings as inputs, the authors of Ref. [9] concluded the discussion of their approach\nwith plans to apply their transformer architecture for inputs other than text. Indeed, in their\nNVIDIA-GTC plenary the seven Google-based authors noted that the architecture was de-\nsigned to cover different modalities. The mathematical underpinning of the transformer-based\ntechnology is accessibly described in a series of YouTube videos by 3BluelBrown.\nBased on transformers and pre-trained unsupervised deep-learning models, a class of AI\ntechnology, large generative AI models (LGAIM), has emerged with the capability to produce\nnovel outputs of a sophistication not seen earlier [10]. The Large Language Models (LLMs)\nfirst came to wide public attention through generating output text from input text, par-\nticularly OpenAI's ChatGPT. However many other types of input/output can be effectively\ntreated as text (\u2018tokenised') and processed the same way, giving M-LLMs (Multi-modal LLMs)\nwhich cover multiple modalities. A taxonomy of generative AI models released since 2021 iden-\ntifies nine categories of modality [11], though admittedly the final category is \u2018other' so as\nto include those that do not fit the first eight. Those eight are text-to-image, text-to-three-\ndimensional views, image-to-text, text-to-video, text-to-audio, text-to-text, text-to-code, and\ntext-to-science.\nWhile the generative AI models present great potential to sectors of society such as edu-\ncation, research, and arts, they also pose significant risks [10, 12, 13]. These include:\n\u2022 generating erroneous answers and presenting fictitious text as real [14],\n\u2022 outputting responses biased against gender, race, or ethnicity [15],\n\u2022 unpredictable behaviour in complex and possibly critical environments, and\n\u2022 opening wide avenues for intentional misuse and exploitation.\nSince generative AI models use publicly-available news articles, academic papers, social media\nposts, photos, and even chatbot chats as their training data, Lucchi [16] argues that these\ntools pose legal issues concerning the ownership of the generated contents copyright, and the\nfair use of the training data. Regarding this last article we can view it in two ways:\n\u2022 On one hand one can argue that the use is fair, since if the data are considered public\ndomain then it can be used, provided sensitive personal information is removed;"}, {"title": "1.3 The growing challenges of discerning authentic versus inauthentic in-\nformation and identity", "content": "It is increasingly apparent that we may be entering an extended era where inauthenticity and\nauthenticity will be difficult to discriminate, this involving multiple forms of content including\nbiometrics and more, which we will address in more detail in Ref. [7].\nIn isolated pockets, governments appear to be increasingly aware of this challenge. In\npluralistic societies, such as the United States or the European Union, central governments\ntraditionally have had a role to play in verifying the authentic vs. inauthentic nature of public\ninformation. However public trust in these centralised institutions as arbiters for identifying\ndubiously-sourced information content has been eroding.\nTo add to this challenge, both the political sphere and advertising market tend to benefit\nby presenting information as fully reliable when it can be argued that this is only somewhat the\ncase (for example, when the distinction between fact and belief fades). One route to address\nthis comes from autocratic governments, who have somewhat of a 'home-field' advantage\nbecause they feature only one singular narrative. Autocratic regimes use specialised tools like\nfiltering, censorship, and repression to ensure that only this narrative (authentic or not) is\nseen by a majority of their population.\nPluralistic societies have a more difficult task ahead of them. It can be argued that the\nlast ten years might pale in comparison to the challenges of fact-checking in a world flooded by\nboth media and mediums of questionable authenticity. In 2019-2020, the non-profit People-\nCentered Internet coalition proposed that a key software vendor, which owned a substantial\nvolume of Customer Relationship Management (CRM) data including \u2018out of band' questions\nas part of the CRM data package, could use 'out of band' management questions with the\naim of supplying an additional level of identity trust for the end entity of transactions. In the"}, {"title": "1.4 Reports and perspectives on AI", "content": "In the lead up to the horizon-scanning effort which is the focus of this and our later companion\narticles, we can highlight of a few developments as well as mentioning key milestones, such as\nglobal reports on the state of AI technology. In a companion article [6], we perform a more\ndetailed overview of emerging technologies and industry trends.\nIn October 2023, the \u2018State of AI' report [18] was published, a document of nearly of\n200 pages, led by AI investors Nathan Benaich and the Air Street Capital team. The report\nmentions how computational technology might be, in current times, understood as the \"new\noil\". It also emphasises how, generally speaking, actors in this highly-competitive industry\nare applying growing efforts to \u201cclone or surpass proprietary performance\". In particular\nthe report highlights how discussions around AI safety have \"exploded into the mainstream,\nprompting action from governments and regulators around the world. However, this flurry of\nactivity conceals profound divisions within the AI community and a lack of concrete progress\ntowards global governance, as governments around the world pursue conflicting approaches.\u201d\n(extract from original in italic).\nA more recent article (January 2024), \u201cThousands of AI authors on the future of AI\"\n[19], provides a detailed assessment of the impact of AI technology via an opinion poll of AI\nresearchers on a set of pre-defined questions, giving the time interval over which various AI\nmilestones are expected to be achieved. The aggregate forecasts by the study give at least a\n50% chance of AI systems achieving several milestones by 2028, for example\n\u2022 autonomously constructing a payment processing site from scratch.\n\u2022 autonomously downloading and fine-tuning a Large Language Model.\nNotably most milestones in Ref. [19] are predicted to be achieved significantly earlier than in\na similar survey undertaken only one year before. While the questions asked might not be the\nmost aligned with our purposes here, the discussion and statistical analysis is rigorous and\nmeets peer-review standards.\nA detailed and authoritative view on the current AI safety situation is provided by the\nMay 2024 \"International Scientific Report on the Safety of Advanced AI (Interim report)\"\nled by AI pioneer Yoshua Bengio [13]. It focuses particularly on the technological status and\noutlook, and while highlighting the main areas of risk it largely restricts itself to possible\ntechnological, rather than sociological and regulatory, solutions.\nAn online \u2018AI Risk Repository' has been developed at MIT [20]. It classifies over 700 risks\ninto a taxonomy featuring 43 categories within 7 broad domains:\n\u2022 Discrimination and toxicity.\n\u2022 Privacy and security.\n\u2022 Misinformation."}, {"title": "1.5 Research on artificial intelligence", "content": "It is important to stress in this context that reliable and independent sources of data on state-\nof-the-art AI technology and its many manifestations are difficult to obtain. It is challenging\nto separate actual performance of current models amid hype on one hand, and doomerism\non the other (see also Section 10 for AI doomerism). For example, one leading organization\nproviding objective data on disinformation, the Stanford Internet Observatory, has recently\nbeen shut down [21]. Moreover, access to immense amounts of data on AI projects is restricted\nby business enterprises and is classified by those businesses as closed source and proprietary."}, {"title": "1.6 Work of Service: the need for volunteer groups", "content": "In April 2024, the US National Academies of Science, of Engineering, and of Medicine or-\nganised a workshop entitled \u201cEvolving Technological, Legal and Social Solutions to Counter\nDisinformation in Social Media A Workshop\u201d see Ref. [22]. This two-day workshop was\ndesigned to foster new research and collaborations, and build implementable solutions for\na whole-of-society approach to mitigating disinformation and its detrimental effects. The\nwebpage of the event can be found here, and all videos of the event are available here.\nThis workshop is valuable because\n\u2022 It was a global conference unifying several themes, industries, countries, and contexts.\n\u2022 The workshop's website includes a large number of citable sources and accompanying\ndocumentation.\n\u2022 It offered rich content with speakers from different academic and non-academic fields,\nincluding various sectors in the industry, and governmental agencies.\n\u2022 It supplied multi-national views from the US, EU, China, UK, and others.\nDocumentation on the findings of the workshop will be produced by the National Academies,\nin the form of Conference Proceedings.\nWe will refer to the content of the workshop throughout these introductory remarks on\nour horizon scanning effort of the p3395 working group. In this section we wish to remark\nand echo a view which was shared there, and also in other volunteer standard groups (our\nown IEEE-SA activity, ISO, etc.). In their closing remarks both chairs of the workshop,\nNobel laureate Saul Perlmutter (Lawrence Berkeley National Laboratory and UC Berkeley)\nand Joan Donovan (Boston University) have emphasised the need for funding and support for\nindependent research on the effects and evolution of the technology.\nEric Horvitz of Microsoft argued that this area of research is way underfunded. Multi-\ndisciplinary meetings like Ref. [22] are needed every few months to do integrative work. In\nRef. [22], Washington University's Kate Starbird highlighted the concern that independent\nresearchers and horizon-scanning efforts are not incentivised to assess the evolving AI problem\nspace nor to describe the risks of the technology in a way that avoids vested interests. She"}, {"title": "2 Global algorithmic pipeline - critical infrastructures", "content": "Our planet is currently engaged in the development of a so-called \u2018global operating system'.\nThat is to say, the setting up of networks of automated algorithmic pipelines reaching un-\nprecedented scales of computerization and interconnectedness. It controls the movement of\ngoods and services in both the real and the online environments, and relies on continuous flow\nof information amongst devices, servers, and satellites, all constrained by a complex network\nof international agreements and limitations.\nResearchers and developers in the AI systems community are concerned about how long\nsystem coherence can be maintained across such a vast distributed systems network. We\nare currently facing unprecedented scales of automation. It is crucial to consider the various\ndimensions in which GenAI use in smart manufacturing contexts risks the disruption of global\nsupply chains and provides challenges around security, quality and safety."}, {"title": "2.1 Cascade Failures", "content": "In a worst-case scenario, algorithmic (or cascade) failure might lead to breakdown of supply\nchains. If such breakdown of supplies were to reach large enough scales, the structures that\nsupport societal organisation could be at risk of rupture. Such scenarios were already discussed\nas possible causes of breakdown of supply chains by Tristan Harris and Aza Raskin on March\n9th 2023, in the AI dilemma [23]. The risk of system cascade-failures was pointed to in by\nVint Cerf, widely celebrated as one of the Founding Fathers of the internet\u00b2, in a seminar\nin January 2024 Ref. [24]. Cerf alerted to the possibility of cascade failures affecting the\ncoherence of global algorithmic pipelines. As an example Cerf mentioned the possibility of\ngetting logged out of accounts by multi-factor identification (with online banking interfaces\nat highest risk). The crash of a container ship into a Baltimore bridge in early 2024 caused\nsome initial concern of the possibility of disrupted supply of goods.\nDistributed systems are those most at risk, due to possible over reliance on cloud com-\nputing. Banking, telecommunication companies, airline and internet providers are all sectors\nrequiring dedicated attention. The July 2024 global outage caused by a failed upgrade of the\nCrowdstrike Falcon security software, reported here and affecting almost 10 million computers,\noffering a taste of what might be ahead.\nQuoting from Chapter 16 of \u2018Building Secure and Reliable Systems' [25], \"Each component\nplays a vital role in returning a disaster-stricken system to an operational state. Even if your\nIncident Response (IR) team is highly skilled, without procedures or automated systems,\nits ability to respond to a disaster will be inconsistent. If your technical procedures are\ndocumented but aren't accessible or usable, they'll likely never be implemented.\u201d Chapter 1\nillustrates this with a hilarious, albeit consequentially minor, example of a historical cascade\nfailure in Google's password management system.\nThe algorithmic closure of automated pipelines is therefore an essential priority, in our\nopinion. The large-scale automation of the pipeline of algorithms we are trying to implement\nat planetary scale must be ensured to connect end-to-end, and close consistently. This is\neven more true for algorithmic pipelines supporting 'Critical Infrastructures'. These include"}, {"title": "2.2 Crowdstrike Falcon Security Update", "content": "Before we go on to sketch our stance on AI safety, we review an IT system-update control\nfailure event. We will consider the Crowdstrike event in July 2024, and discuss it in context\nof automated system updates, and feedback loops.\nCrowdstrike was not an implementation process that involved an AI component, per se.\nRather it was a software evaluation issue, and further involved standard system privilege\nmanagement activities. The incident affected 8.5 million bots across the Azure backbone\nworldwide, all requiring individual human intervention and performing a full system reboot,\nbefore functionality could be recovered. The effect was felt all the way across Azure, it\ncomprising Azure's entire footprint. It is possible that Crowdstrike happened because we are\nin the era of massive automation. For our purposes here the question is not so much whether\nCrowdstrike is a direct failure from the tie-in to the AI component, but whether an incident of\nthis scale is very likely to be uncorrelated from the current era we are living in, of large scale\ndeployment of the novel generation of AI models. Either way, we believe important lessons\ncan be learned from the Crowdstrike incident:\n\u2022 explicitly forbidding single points of failure in critical infrastructures\n\u2022 management of standard system-privilege activities\n\u2022 software testing before roll out remains a priority, with automated system updates\ndeemed the ones requiring the highest scrutiny.\nCan an (over) dependence on AI components enhance the risks of this type of event happening\nin the near future? Crowdstrike may be an opportunity and a harbinger of a future where\nthese sorts of instabilities in the pipeline and distributed systems and platforms become more\ncommonplace, as well as harder for us to analyse and prevent. Here we draw attention\nto over-reliance on cloud computing, and implementation of robust protocols of automated\nsystem-update, particularly when infrastructures deemed critical to the normal functioning\nof societies are implicated, or at risk."}, {"title": "3 Overarching theme: who has regulatory sovereignty?", "content": "We lay out the encompassing theme of our first article, in the regulatory domain, in Sections\n3.1, 3.2, and 3.3, and outline a broad-stroke division of organisational and regulatory initia-\ntives. Globally we observe an overarching dialogue between three paradigms for the choice of\nregulation level: state, market, and individual.\n1. Government-level sovereignty which we address in Section 3.1. We address topics of\nautocracy and mass scanning in Section 3.1.1.\n2. Tech-level or platform-level sovereignty (Self-regulating industry, or private sector player)\nwhich we address in Section 3.2."}, {"title": "3.1 Government-level sovereignty", "content": "In government regulatory efforts one of the main challenges is to reduce the number of moving\nparts in the regulation pipeline and streamline the way the different parts interact. As in all\ntop-down regulation schemes, one must not underestimate the non-deterministic nature of the\nsystems that are being rolling out.\nThe good news is that we have democratised technologies that used to be available to a\nfew. But inevitably we are going to have people that will use the technology as bad actors;\nhow do we deter that, without losing the benefits? In the context of US regulation, it has\nbeen argued that one can wait for the full features of the European Digital Services Act (DSA,\nsee Ref. [26]) to be fleshed out and use the best ingredients therein [22]. Additionally, as a\nchallenge to government-based sovereignty, it has been argued by tech-industry experts that\nonly the technically competent could regulate AI.\nWhich legislation would we choose to implement if the approval process was\nfrictionless?\nIn Ref. [1] the moderator Aziz Huq (University of Chicago Law) asked: \u201cWhat would be the\ntop priority for passing legislation if we had friction-free regulatory approval? For example,\nwhich elements of European regulation would you propose to be adopted in the US?\" Meaning\nif our proposals were met with no resistance by the regulatory or legislative committees what\nwould our proposals be? What concrete measures would be propose when political constraints\nare absent? The panelists answered in the following way:\n\u2022 David Bray advocated giving individuals digital dignity, through the ability to control\nhow their data can be used by others. Bray emphasised that our data is our voice, and\nas such, it is in our interest to work for our right to choose how our data is used.\n\u2022 Jeff Kosseff argued for the need to protect free speech as a fundamental requirement\nfor democracy. Further the US Naval Academy scholar stressed the fact that we give\nup on democracy when we start to regulate free speech. We see this in the example of\nthe Supreme Court in Brazil ordering certain accounts to be taken out of Twitter/X. In\nsuch examples the central authorities may issue orders for precision removal of specific\ncontent that does not favor those authorities. He proposed a vast investment in media\nliteracy education, for instance in schools and libraries.\n\u2022 Josh Braun sought regulations of digital advertising space, limiting commercial data\nacquisition for targeting to rebalance contextual versus targetted advertising. The aim\nhere is to make contextual advertising more attractive again. One goal is to help news\noutlets to be financed in such a way that does not depend on advertising, while also\ndisfavouring hyper-targeting of advertising. He also picked up on the media literacy\nissue, noting by analogy with driving practices that while teaching safe driving (c.f.\ntechnology use) is a good directive, legislating for safer cars and roads needs also to be\npart of the solution.\""}, {"title": "3.1.1 Autocracy and mass scanning", "content": "In the context of AI regulation, countries without large AI sectors of their own (for example\nChile and Brazil) are able to go in the direction of \u2018autocratic' regulatory approaches in a way\nthat the more AI-vested societies are not. The Chinese documentation on AI regulation is a\ngood example of how an autocratic government can develop legislation packages, the broader\ncontext of the technology industry in China being described in, for example, Ref. [27"}]}