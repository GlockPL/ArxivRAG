{"title": "FineSurE: Fine-grained Summarization Evaluation using LLMs", "authors": ["Hwanjun Song", "Hang Su", "Igor Shalyminov", "Jason Cai", "Saab Mansour"], "abstract": "Automated evaluation is crucial for streamlining text summarization benchmarking and model development, given the costly and time-consuming nature of human evaluation. Traditional methods like ROUGE do not correlate well with human judgment, while recently proposed LLM-based metrics provide only summary-level assessment using Likert-scale scores. This limits deeper model analysis, e.g., we can only assign one hallucination score at the summary level, while at the sentence level, we can count sentences containing hallucinations. To remedy those limitations, we propose FineSurE, a fine-grained evaluator specifically tailored for the summarization task using large language models (LLMs). It also employs completeness and conciseness criteria, in addition to faithfulness, enabling multi-dimensional assessment. We compare various open-source and proprietary LLMs as backbones for FineSurE. In addition, we conduct extensive benchmarking of FineSurE against SOTA methods including NLI-, QA-, and LLM-based methods, showing improved performance especially on the completeness and conciseness dimensions. The code is available at https://github.com/DISL-Lab/FineSurE-ACL24.", "sections": [{"title": "1 Introduction", "content": "Text summarization stands out as an important task in natural language processing, aiming to generate a condensed summary of a provided text while retaining its essential information (Gupta and Gupta, 2019; Song et al., 2023). Despite the enhanced quality of summaries produced by LLMs, the development of automated methods for evaluation remains a challenge (Kry\u015bci\u0144ski et al., 2020; Maynez et al., 2020). Conventional reference-based metrics, such as ROUGE (Lin, 2004), have exhibited a weak correlation with actual human judgments (Liu et al., 2023). Consequently, human evaluation remains an essential step for accurately assessing the quality of generated summaries, even considering its inherent costs and time-consuming nature.\nRecently, the need for better automatic evaluators has become an important research topic, aiming to streamline evaluation processes and ease manual efforts in model development (Gao et al., 2023). This effort provides valuable insights into whether generated summaries align with predefined quality standards, including aspects like faithfulness. Various approaches have been explored, including approaches based on neural language inference (NLI) (Laban et al., 2022) and question-answering (QA) (Fabbri et al., 2022; Zhong et al., 2022). In addition, LLMs have recently proven their potential to be an automated tool for human-like evaluation (Liu et al., 2023; Wang et al., 2023). The latest LLM-based method, G-Eval (Liu et al., 2023), demonstrated a Spearman correlation coefficient of over 0.5 with Likert-scale human judgments on the news domain using GPT-4.\nDespite these advancements, we contend that the current LLM-based automated methods still fall short in achieving precise evaluation, primarily attributed to the coarse-grained evaluation pipeline and the ambiguity in evaluation dimensions. Specifically for coarse-grained evaluation, the evaluation dimensions-namely faithfulness, coherence, and relevance -are frequently assessed at the summary-level, resulting in Likert-scale scores for each summary (Gao and Wan, 2022; Shen and Wan, 2023; Liu et al., 2023; Wang et al., 2023). This Likert-scale scoring method lacks fine-grained information on errors in generated summaries. For instance, it does not provide a breakdown of the number of summary sentences with quality issues or specify"}, {"title": "2 Related Work", "content": "Efforts to assess the quality of texts generated by language models have led to numerous initiatives in designing effective automated evaluators across various research directions (Lin, 2004; Fabbri et al., 2022; Zhong et al., 2022; Wang et al., 2023).\nSimilarity-based Evaluator. The evaluation of generated text can be measured by the n-gram overlap with the reference text, employing met-"}, {"title": "3 FineSurE Framework", "content": "LLMs enhance the quality of summarization, but they rather suffer from hallucination, information emission and verbosity (Ji et al., 2023; Saito et al., 2023), requiring revisiting evaluation dimensions. Therefore, we advocate for a thorough assessment of the two evaluation criteria, \"completeness\" and \"conciseness,\" in addition to \"faithfulness.\" These two dimensions can effectively assess both information emission and verbosity while also complementing each other in evaluating information inclusion and summary succinctness.\n*   Faithfulness: The summarizer does not manipulate the information in the input text (i.e., intrinsic) and add any information not directly inferable from the input text (i.e., extrinsic).\n*   Completeness: The summarizer ensures the inclusion of all keyfacts from the input text in the output summary.\n*   Conciseness: The summarizer refrains from incorporating information outside the keyfacts in the output, maintaining a succinct and focused summary.\nNote that, adhering to the precise definition of faithfulness in the recent work (Pagnoni et al., 2021), we categorize error types into a total of seven categories, with \"out of context\" as an extrinsic error, and \"predicate,\" \"entity,\" \"circumstance,\" \"coreference,\" \"discourse link,\" and \"grammatical\" as intrinsic errors. See examples in Appendix A."}, {"title": "3.2 Evaluation Pipeline", "content": "We discuss the evaluation pipeline implementing the dimensions discussed previously. We employ LLMs as a tool to conduct fact checking and key-fact alignment tasks. Specifically, we design two"}, {"title": "3.3 Prompt Engineering", "content": "We explore various prompt engineering strategies (Wei et al., 2022; Yu et al., 2023) to identify the most suitable one for our evaluation pipeline:\n*   Basic Prompt: A default question prompt in plain text, e.g., is the summary sentence supported by the transcript?\n*   Instruction: The prompt is provided using a step-by-step instruction using \"Instruction:\".\n*   Categorization: The prompt solves a categorization task by providing target categories.\n*   Reasoning: The prompt uses a chain-of-thought approach, incorporating a reasoning step.\n*   Evidence Mapping: The prompt requests an exact quote from the input to confirm the decision made by LLMs.\nCombining all the above techniques was not always superior. Evaluation prompts are recommended to use instruction format with categorization and reasoning for faithfulness evaluation, as in Figure 3, and only instruction format for completeness and conciseness evaluation, as in Figure 4. See the detailed ablation in Appendix G."}, {"title": "3.4 Keyfact Extraction", "content": "The list of keyfacts is essential for evaluating the completeness and conciseness using FineSurE. Humans are best suited to generate these keyfacts as they understand the priorities in different domains, such as medicine or sales. However, in some cases, obtaining human keyfacts can be challenging. FineSurE works with human keyfacts by default, but for cases where no human keyfacts are provided, it can employ the LLM to extract keyfacts automatically. This process is entirely automated, utilizing prompts tailored for keyfact"}, {"title": "4 Evaluation", "content": "Datasets To evaluate the automated evaluator's performance, we need datasets with human annotations for sentence-level faithfulness and keyfact-level alignment. Since no single dataset includes both types of annotations, we opt for two separate datasets. FRANK (Pagnoni et al., 2021) is a benchmark dataset of 2, 246 summaries for factuality evaluation metrics. It encompasses summaries of nine summarization systems on CNNDM (Hermann et al., 2015) and XSUM (Narayan et al., 2018), providing fine-grained annotations of sentence-level factuality error types. On the other hand, REALSumm (Bhandari et al., 2020) is another dataset of 2, 500 summaries from 25 summarization systems for automated metrics based on CNNDM. It includes a list of human keyfacts, along with corresponding annotations indicating their presence in the summary. FRANK and REALSumm obtain the inter-annotator agreement (IAA) scores of 0.58 (cohen's kappa) and 0.66 (Krippendorff's alpha) for three annotators, respectively.\nLLMs as Evaluators We use the GPT-4-turbo (gpt-4-1106-preview) (Achiam et al., 2023) by default in main evaluation, but test with various open-source and proprietary LLMs, including Mixtral-8x7B (Jiang et al., 2024), Phi-2, Llama-2/-3 (Touvron et al., 2023), GPT-3.5-turbo, and GPT-4-omni (gpt-40-2024-05-13), in Section 4.2. We set the temperature to 0 and clear the history for every evaluation instance, following the literature (Shen et al., 2023). We use HuggingFace models for"}, {"title": "4.1 Main Results: Evaluators Comparison", "content": "Table 1 summarizes the agreement between automated evaluators and human scores in faithfulness evaluation at three different granularities. FineSurE significantly outperforms similarity-, NLI-, and QA-based evaluators at all levels of evaluation.\nIt is important to note that none of the existing methods provide sentence-level evaluation results, relying instead on summary-level scoring, such as Likert-scale scores. It is noteworthy that FineSurE has the capability to assess whether each sentence contains a factual error or not, demonstrating remarkable alignment with human sentence-level judgments, with a balanced accuracy of 86.4%.\nGiven the strong alignment with human judgment, using LLMs as an evaluator holds great promise for enhancing the reliability of evaluation"}, {"title": "4.1.3 Stability in Evaluation Results", "content": "Concerns arise about evaluation result stability with LLMs due to their inherent text generation randomness, even at temperature 0. Despite LLM-based methods relying on Likert-scale evaluation, such as G-Eval, showing significant fluctuations in judgment alignment (Shen et al., 2023; Liu et al., 2023), Table 4 demonstrates that FineSurE (GPT-4) maintains much higher agreement in summary-level evaluation scores across three distinct runs. This underscores the benefit of employing fine-grained percentage scores derived from sentence- and keyfact-level assessments."}, {"title": "4.2 LLMs as Evaluators Comparison", "content": "It is interesting to observe how the evaluation agreement varies based on the choice of LLMs, given the abundance of open-source and proprietary LLMs.\nSuccess Ratio. The primary limitation of open-source LLMs is their comparatively lower success ratio in following prompts, compared to proprietary LLMs; only Llama3-70B-Inst exhibits a high success ratio comparable to proprietary LLMs. Upon analyzing failure cases, the top three reasons are: (1) the output is either not in JSON format or an incorrect JSON format, (2) the output consists of meaningless text, e.g., python codes or no output at all, and (3) the JSON output includes only a few lines of sentences or keyfacts.\nFurthermore, the maximum token length in context for open-source LLMs is notably shorter compared to proprietary LLMs. GPT-4 series can process up to 128K tokens, whereas open-source LLMs generally handle up to 8K input tokens. This results in prompt truncation when handling lengthy"}, {"title": "4.3 Evaluation using FineSurE", "content": "As an actual application of an automated evaluator, we gather summaries generated by four open-source and four proprietary LLMs, and subsequently assess their summarization quality using the FineSurE algorithm (see the prompt for summarization we used in Appendix H). Figure 2 shows the percentage scores of the eight LLMs for faithfulness, completeness, and conciseness. The summaries are generated for 100 examples sourced from CNNDM, all of which are also included in"}, {"title": "5 Conclusion", "content": "We introduce FineSurE, a novel automated evaluator designed for fine-grained and multi-dimensional text summarization evaluation. The evaluation process is broken down into fact checking and keyfact alignment, providing detailed insights, where key-facts can be either provided by humans or generated by LLMs. Our experiments include a thorough comparison with existing evaluators, exploration of performance using eight opensource or proprietary LLMs, and real quality assessment of recent LLM-generated summaries. The results indicate the potential effectiveness of FineSurE as a text summarization evaluator, showcasing its promising capabilities in advancing automated evaluation for text summarization."}, {"title": "Limitations", "content": "Our automated evaluator is primarily tested on the news domain due to the limited availability of benchmark datasets with fine-grained human annotations. We emphasize the critical importance of constructing a high-quality benchmark dataset with high diversity in input domains, length, and types. Also, the prompts for evaluation may need to be tuned if a different summary is expected like the summary from the medical domain. Lastly, other aspects can be considered for text summarization, such as toxicity and social bias. We leave these challenges as future work."}, {"title": "Ethics Statement", "content": "This paper focuses on designing an automatic evaluator using LLMs for text summarization. Therefore, we do not anticipate any negative ethical and social impact."}, {"title": "Acknowledgements", "content": "The first author, Hwanjun Song, was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. RS-2024-00334343) and Artificial Intelligence industrial convergence cluster development project funded by the Ministry of Science and ICT (MSIT, Korea) & Gwangju Metropolitan City (No. BA00000772)."}, {"title": "D Measurement", "content": "We utilize several measurement to compute the agreement with human judgements at the three different levels.\nFor sentence-level assessment, we utilize human binary annotations indicating the presence of factuality errors for each sentence, denoted as \"0\" for no error and \"1\" for error. Similarly, the LLM returns the binary decision of \"0\" (No) and \"1\" (Yes) by the fact checking prompt per sentence, as shown in Figure 3. Then, the balanced accuracy (bACC) is computed by:\nbACC = (sensitivity + specificity)/2, (3)\nwhere sensitivity refers to the true positive rate, which measures the proportion of correct predictions by LLMs out of all positive predictions. On the other hand, specificity is the true negative rate, measuring the proportion of correct predictions out of all negative predictions.\nFor the summary-level assessment, let D = {d1,...,dk} be the set of input documents and S = {s1,...,sk} be the set of summaries corresponding to the document set. Supposing that Fgt and Fpred are the functions that returns the percentage score of a specific evaluation dimensions based on human and predicted labels, respectively (F can be any scoring function in Eq. (1)-(2)). Then, the summary-level correlation is calculated as follows:\nCorr ([Fgt(d1, s1),..., Fgt (dk, sk)],\n[Fpred(d1, s1),..., Fpred(dk, sk)]),(4)\nwhere Corr is one of the Pearson and Spearman correlation measure. The measurement reveals the agreement between the percentage scores generated by automated evaluators and human judgments.\nFor system-level assessment, we consolidate the percentage scores across all input documents, determining the average percentage score for each summarization model. Let Fm = {Fm(d1,s1),..., Fm(dk, sk)} represents the percentage scores derived from the labels assigned by a summarization system m. Then, we make a list of the average percentage score for all summarization systems, [Fm1, Fm2,...] and compute their ranking by using the Rank function, returning the list [rankm1, rankm2,...], where rankm is the ranking of the model m. Given a list of ground-truth rankings [rankm*1, rankm*2,...] using the human scores, we compute the rank correlation by:\nSpearman ([rankm1, rankm2,...],\n[rankm*1, rank*m2...]).(5)"}, {"title": "A Factuality Error Type", "content": "Following the work (Fu and Frank, 2023), we use seven categories to define factuality error types, namely \"out of context\", \"predicate,\" \"entity,\" \"circumstance,\" \"coreference,\" \"discourse link,\" and \"grammatical\". Table 8 provides the detailed description and example of the error categories."}, {"title": "B Main Prompt", "content": "We employ LLMs as a tool to conduct fact checking and keyfact alignment tasks. Specifically, we design two prompts tailored for the two tasks, as shown in Figures 3-4."}, {"title": "C Keyfact Extraction", "content": "The list of key facts is crucial for evaluating completeness and conciseness. Ideally, they should be generated by humans, as the key facts in text summarization heavily depend on what information humans prioritize in various domains. For instance, in a medical scenario, keyfacts should encompass all medical symptoms and the doctor's recommended"}, {"title": "E Qualitative Analysis for Completeness", "content": "Table 9 shows an example showing the limitation of the existing Likert-scale based evaluation method, such as G-Eval. Although the model summary includes eight out of ten human keyfacts, G-Eval output a very low completeness score, i.e., two out of five. However, the proposed FineSurE exhibits the percentage score similar to that of human judgement. This is because our framework conducts a human-like keyfact alignment to determine the completeness score, which is more fine-"}, {"title": "F Fair Comparison with G-Eval", "content": "For a truly fair comparison, we recognize the necessity to tune G-Eval for our two dimensions of completeness and conciseness. Hence, we opted to adjust G-Eval's prompts to align with the evaluation dimensions of FineSurE. We employed two"}, {"title": "G Prompt Engineering", "content": "We test several prompt engineering techniques, including instruction format, solving categorization"}, {"title": "H Summarization using LLMs", "content": "We consistently employ the prompt shown in Figure 16 across all LLMs, generating model summaries based on the given input text.\nFurthermore, since the model summary is null when parsing fails, there is no hallucination, but also no summary sentences align with any key facts. Therefore, in cases where the LLMs produce incor-"}, {"title": "I Extension of REALSumm", "content": "Although the REALSumm data contains human labels indicating which keyfacts are included in the model summary, there are no human labels indicating which summary sentences align with the keyfacts. The former is used to compute the ground-truth completeness score, while the latter is used to compute the conciseness score. Therefore, we conducted a human evaluation to verify which summary sentences align with the set of key facts. Specifically, three human annotators were asked to mark \"yes\" if at least one keyfact in the keyfact list could be inferred from each summary sentence, otherwise \"no\". This is quite simple task compared with faithfulness evaluation, since human ground-truth keyfacts are available in REALSumm data. The extended dataset is available with our FineSurE framework at https://github.com/DISL-Lab/FineSurE."}]}