{"title": "Pre-Training and Prompting for Few-Shot Node Classification on Text-Attributed Graphs", "authors": ["Huanjing Zhao", "Beining Yang", "Yukuo Cen", "Junyu Ren", "Chenhui Zhang", "Yuxiao Dong", "Evgeny Kharlamov", "Shu Zhao", "Jie Tang"], "abstract": "The text-attributed graph (TAG) is one kind of important real-world graph-structured data with each node associated with raw texts. For TAGs, traditional few-shot node classification methods directly conduct training on the pre-processed node features and do not consider the raw texts. The performance is highly dependent on the choice of the feature pre-processing method. In this paper, we propose P2TAG, a framework designed for few-shot node classification on TAGs with graph pre-training and prompting. P2TAG first pre-trains the language model (LM) and graph neural network (GNN) on TAGs with self-supervised loss. To fully utilize the ability of language models, we adapt the masked language modeling objective for our framework. The pre-trained model is then used for the few-shot node classification with a mixed prompt method, which simultaneously considers both text and graph information. We conduct experiments on six real-world TAGs, including paper citation networks and product co-purchasing networks. Experimental results demonstrate that our proposed framework outperforms existing graph few-shot learning methods on these datasets with +18.98% ~ +35.98% improvements.", "sections": [{"title": "1 INTRODUCTION", "content": "The few-shot node classification task involves identifying the classes of nodes in a given graph structure using only a limited number of labeled examples. This task has practical applications in areas such as social network analysis, recommendation systems, and more. Inspired by the successful experiences in the field of Computer Vision (CV), several works, such as G-Meta and TENT [7, 16, 34, 43], apply meta-learning to graphs to address the few-shot node classification problem. These methods learn transferable knowledge from meta-tasks, enabling rapid adaptation to new, unseen labels. Unlike images, graphs represent a form of structured data. Through graph pre-training, models can also accumulate a substantial amount of domain-specific knowledge. Recently, a series of graph pre-training methods [30, 40, 50, 54, 57] emerged with self-supervised learning (SSL) to yield generalized node representations in the absence of labels. These methods mainly include contrastive and generative ones. Contrastive SSL methods utilize data augmentation to generate multiple views of data for contrastive loss. Generative SSL methods such as GraphMAE [15] aim to reconstruct the (masked) attributes of the graph. These self-supervised learning methods have greatly contributed to many aspects, such as graph augmentation and graph-based pretext tasks. They only consider part of the self-supervised learning on TAGs, making the training of GNNs independent of LMs text encoding."}, {"title": "2 PRELIMINARIES", "content": "In this section, we introduce the background of our paper including text-attributed graph and few-shot node classification.\nNotations. Denote a graph G = (V, &), where V is a set of n nodes and & is a set of edges between nodes. A \u2208 Rnxn is the adjacency matrix where its entry A(i, j) \u2265 0, if nonzero, denotes there is an edge between node i and j with edge weight A(i, j). In practice, the network could be either directed or undirected. If G is directed, we have A(i, j) \u2260 A(j, i); if G is undirected, we have A(i, j) = A(j, i)."}, {"title": "2.1 Text-Attributed Graph", "content": "DEFINITION 1 (TEXT-ATTRIBUTED GRAPH). A text-attributed graph (TAG) is a graph G = (V, &, S), where each node vi \u2208 V is associated with a text sequence si \u2208 S and & represents the set of edges between nodes.\nGiven the above definition, we can formally define our problem for self-supervised learning on TAG.\nPROBLEM 1 (SELF-SUPERVISED LEARNING ON TAGS). Given a TAG G = (V, &, S), the problem of self-supervised learning (SSL) on TAG is to learn a unified low-dimensional space representation of each text-attributed node vi \u2208 V with text sequence si. The goal is to find a function f : V \u2192 Rd, where d \u00ab |V|.\nPrevious methods implement self-supervised learning on TAG through separated text encoders and GNNs. Built upon these works, we explore self-supervised learning by integrating LMs and GNNs."}, {"title": "2.2 Few-shot Node Classification", "content": "Few-shot node classification are tasks that involve training a machine learning model with a limited number of labeled nodes, and subsequently predicting the classes of other nodes based on these initial labels. The N-way K-shot few-shot node classification tasks T on a graph is described as follows with the node label set C. Each task T\u0118 \u2208 T selects N labels from C, forming a subset Ct = {C1, C2, ..., CN} \u2208 C. The Te consists of two parts as follows:\nTt = {St, Qt},\nwhere St means the support set and Qt means the query set. The machine learning model is trained on St and subsequently evaluated on Qt. Both the support set and the query set consist of nodes and their corresponding labels, which shown as follows:\nSt = {(01, C1), (U2, C2), ..., (UN\u00d7K, CN\u00d7K)},\nQt = {(01, c\u2081), (02, c\u2082), ..., (UNxQ, CNxQ)},\nwhere v and v' represent nodes, their corresponding labels are denoted as c and c'. In the support and query sets, each label Ci \u2208 Ct associated with K nodes and Q nodes, respectively. For the support set, there exists c(i-1)*K+j \u2208 Ci, where i \u2208 {1, 2, .., N} and j \u2208 {1, 2, .., K}. Similarly, there exists c\u02bb(i\u22121)*Q+j \u2208 Ci, where i \u2208 {1, 2, .., N} and j \u2208 {1, 2, .., Q} for the query set."}, {"title": "3 METHODOLOGY", "content": "We tackle few-shot node classification by adhering to the pre-training and prompting paradigm, as depicted in Figure 1. To better leverage the unique characteristics of the TAGs, we propose a new self-supervised framework for pre-training."}, {"title": "3.1 Pre-training Framework", "content": "In this part, we introduce our proposed pre-training framework in detail. For self-supervised learning on TAGs, previous works usually take two separate steps. The first step is to encode the raw texts to node features using bag-of-words, word2vec [24], or pre-trained language models [4, 6, 8, 12, 19, 22, 42]. Most graph self-supervised methods [15, 30, 50, 54, 57] use the node features pre-processed in the first step to construct a self-supervised objective. The two-step process can bring non-negligible information loss. To address this issue, we propose an end-to-end self-supervised framework to train directly on the TAGs. Inspired by the recent prosperity of pre-trained language models (LM), we choose an effective LM to encode the raw texts. To model the relations between nodes (texts), we can utilize powerful graph neural networks (GNNs). As illustrated in Figure 1, our framework consists of two core modules, including a pre-trained language model and a GNN encoder. The GNN encoder is to help make better node representations. In our framework, we mainly use the DeBERTa-base [12] with 100M parameters as the LM of our framework. DeBERTa utilizes two techniques based on BERT [6] and RoBERTa [22] and significantly improves the performance on the natural language understanding and generation tasks. The choice of the LMs is flexible, and we also explore other LMs in our experiments.\nThe pre-training of LMs and GNNs is much more challenging because we need to 1) choose an appropriate self-supervised training objective to avoid over-fitting and 2) sample small mini-batches to address the high computational and space costs of large LMs.\nSelf-supervised training objective. The training objective plays an important role in self-supervised learning. Different from conventional graph self-supervised learning, the design of self-supervised objective for TAGs are more challenging. Our model architecture contains two different modules with different scales of model parameters (large LM v.s. small GNN), making the training much more difficult. Simple self-supervised objectives will probably make the model overfitted. To make a harder self-supervised objective, we adopt the masked language modeling (MLM) introduced in [6] as our objective. Given a node vi, we first feed the text sequence Si = [T1, T2,..., Tn\u2081] into a language model to get the hidden representations associated with vi where Tt is t-th token of si and ni is the length of si. In the training stage, we randomly mask a pre-defined portion of tokens by replacing them with a special token named [MASK]. We use s = [T', T2,..., T\u2081] to denote the text sequence after masking and each token T' is a random variable with the following distribution:\nPr (T' = [MASK]) = p and Pr (T' = T\u2081) = 1 \u2212 p.   (1)\nHere, p\u2208 (0, 1) is a hyper-parameter representing the mask rate. When working with BERT-like language models, we usually add a starting token (e.g., [CLS]) and an ending token (e.g., [SEP]) to the sequence. Therefore, each node is represented as a sequence of hidden vectors:\n[00, 01,..., 0n\u2081, 0ni+1] = fLM ([[CLS], T', T2, ..., Th\u2081, [SEP]]),   (2)\nwhere ot \u2208 Rd is the hidden representations of t-th token of s. Notice that the first hidden vector on corresponds to the special token [CLS] and can be treated as the \"summary\" representation of the whole text sequence si. To capture the correlations among nodes, we then use a graph neural network to propagate the hidden representations of nodes where the input of node vi, denoted as xi, is the first hidden vector oo of si. The node representations after passing a GNN encoder are denoted as H = fGNN (G,X). The propagated node presentations are exploited to construct the self-supervised training objective.\nFor each node vi, we concatenate the hidden representation hi with the output vector ot of each token and then feed the concatenated vector to an MLP as:\nZt = MLP([h, o ]), where t = 0, \u00b7\u00b7 \u00b7, (ni + 1).   (3)\nThe objective of the masked language model (MLM) is to predict the masked tokens in the original text sequence si, which can be expressed as:\nLi = \u03a3^{N}_{i=1} \u03a3^{N_i}_{t=1} 1(T'_t = [MASK]) log Pr(T_t | z_t).  (4)\nThe indicator 1 (T' = [MASK]) ensures that the loss is only applied on masked tokens. We can control the difficulty of the SSL task via a flexible hyperparameter, i.e., mask rate."}, {"title": "3.2 Mini-batch Training", "content": "Due to the heavy language models, we need to adopt a mini-batch training strategy even if we train our model on small graph datasets. There are several genres of mini-batch training techniques for GNNs. To trade off the efficiency and flexibility, we choose a subgraph-based method, GraphSAINT [48], as our training strategy. At every training step, GraphSAINT constructs a mini-batch by sampling a subgraph from the original graph and generates node representations according to the sampled subgraph. In this work, we adopt the random walk sampler to preserve the connectivity of the whole graph. The sampling process starts by selecting r root nodes from the whole node set V uniformly at random. Starting from each root node, a random walk of length I is sampled from the original graph structure. We then have a sampled node set Vs by adding all the nodes that occurred in the random walks and the subgraph Gs induced by Vs is used in the GNN encoder to generate node representations in the minibatch."}, {"title": "3.3 Graph-Text Mixed Prompt Learning", "content": "To prevent tuning the whole pre-trained model in the few-shot downstream tasks, we adopt the prompt learning paradigm, i.e., using a few trainable parameters to be the substitution of full-scale model parameters. The goal of prompt design is to mitigate the gap between the pre-train objective and the downstream tasks. It is highly challenging since there are both text (LM side) and structure information (GNN side) on TAG graphs. We try to investigate the joint prompting method, namely the graph prompt and the text prompt, the detailed design as follows. To simplify the discussion, we start with a target node v (e.g., the 0-th node in Figure 1). We further introduce the concept of the ego graph node set Vego. We select up to 100 first-order neighbors of the given node, along with the node itself, to form the node set of the ego graph. Then we define the induced ego graph from Vego as Gego.\n3.3.1 Graph prompt design. Inspired by the recently proposed graph prompt literature [28, 29], we aim to bridge gap through a small yet informative prompt graph. We first craft a prompt graph Gp = (Vp, &p) serving as an ancillary component connected to the target ego node set Vego. Gp comprises ||Gp || nodes (i.e., tokens), the internal edges between tokens can be defined as:\nAGp (i, j) = {:  1   if Sim(pi, pj) \u2265 6inner, 0 otherwise,   (5)\nwhere the Sim() denotes the similarity function, and the pi denotes the feature of i-th tokens; in this paper, we use dot product as the implicit operation. Then, let the xk denotes on the k-th node feature of the Gego, the inter-connectivity between Gp and Gego is given by:\nAGego, Gp (k, j) = { 1  if Sim(xk, pj) \u2265 6inter, 10  otherwise.   (6)\nHere, the dinner and ointer are two hyper-parameters indicating the pruning value, i.e., the entries on the matrix higher than oinner and router are formed as edge respectively.\nInitialize tokens via the label text. While the prompt graph servers as an efficient substitute for full-scale model parameters, we empirically find that the naive Gp performs sub-optimally. We attribute this to the inconsistency in the pre-trained task objectives between our masked token prediction task and previous graph-based ones [28, 29] (e.g., the link prediction task).\nReferencing Figure 2 as an example, during the pre-training phase where token prediction tasks are undertaken, the model is equipped to align input with the pre-trained label space. Consequently, the prompt should slightly modify the input distribution to approximate the downstream label space more closely. Hence, unlike traditional cumbersome and hand-crafted text prompt templates, we discovered that employing label text embeddings for prompt graph initialization is more direct and efficient. Under the homophily hypothesis, the target node's embedding is immediately positioned more closer to the desired label space. Specifically, we utilize the pre-trained fLM as our label text encoder, each label text (e.g., \"Animals\", \"Science Fiction and Fantasy\", and \"Activities, Crafts and Games\") can be represented as sy\u2081. Then we can formalize the initialization of Gp as follows:\nP(0) = { FLM (Syi)  if i \u2264 N, Pi Random Initialization otherwise.   (7)\nHere, the p(0) denotes the initial embedding of i-th prompt node, and for the additional prompt nodes, we adopt the random initilization. Then the inner structure of each nodes will be constructed through the Eq. 5. The links among the prompt nodes and the target ego graph Gego are built by the Eq. 6.\n3.3.2 Text prompt design. As shown in Equation 3, we use the joint representation of the LM model output (text information) and the GNN model output (structure information). Therefore, to align the pre-train and downstream tasks, we use a similar strategy to concat both output features in the downstream scenario.\nThen, instead of making intricate text-based prompts, we use a rather simple way: letting the second concat feature be a trainable parameter, namely wt \u2208 R1\u00d7d, i.e., treating the LM model output itself as a tunable parameter. The initialization process can be defined similarly as:\nw\u2081 = fLM (So),   (8)\nthe so shows the text sequence with the target node v. Our ablation study in Section 4.3 shows our text prompt's effectiveness.\n3.3.3 Final prompt learning forward function. Assuming we have a node ego graph Go and its text so, the prompt learning forward function is:\n2 = MLP(READOUT(fGNN (Gv; Gp)),wt).   (9)\nThe parameters of fLM and fGNN are fixed during prompt learning, the parameters of Gp, the wt and the task head (e.g., the MLP layers) are all trainable."}, {"title": "3.4 Model Inference", "content": "Finally, we incorporate the pretrained models fLM, \u0192GNN, and the prompt graph Gp in the model inference stage. We outline three inference methods. The first utilizes the trained LM, referred to as P2TAG (LM). It computes the output of the [CLS] token of the LM for each node in the graph via a mini-batch manner. The second method involves further feeding the node [CLS] outputs from P2TAG (LM) into a GNN encoder, referred to as P2TAG (GNN). In the training stage, the node representation of vi is estimated according to the sampled neighborhood in the subgraph, and thus, varies with the sampling process. The randomness introduced by the sampler is unpleasant during the test stage. To get the exact node representation, we use the full neighbor sampling strategy when performing inference. That is, The node representation of vi at the l-th layer is obtained by aggregating the representations of its full neighborhood at the (l - 1)-th layer. The computation process is performed layer by layer. Taking the node representations at the (l - 1)-th layer as the input, we can compute a batch of node representations at the f-th by first loading their full neighborhoods and then aggregating the representations of those neighbor nodes at the (l-1)-th layer. The third method involves conducting few-shot node classification through prompting, as described in Section 3.3. The P2TAG inference loop of our framework is listed in Algorithm 2."}, {"title": "4 EXPERIMENT", "content": "In this section, we conduct comprehensive experiments to validate the effectiveness of our framework, particularly emphasizing the enhanced performance for few-shot node classification tasks."}, {"title": "4.1 Experimental Setup", "content": "Datasets. We conduct experiments across two categories of datasets. All these datasets are publicly available and widely used in TAG-related research. The statistics of the datasets are given in Table 2.\n\u2022 Open Graph Benchmark (OGB). ogbn-arxiv is a directed citation graph between all computer science (CS) ArXiv papers indexed by Microsoft Academic Graph (MAG) [32]. We convert the abbreviated labels into their full forms as specified on the arXiv website, such as transforming \"NA\" into \"Numerical Analysis\". ogbn-products is an undirected and unweighted graph representing an Amazon product co-purchasing network [1].\n\u2022 Amazon Review. Amazon Review includes graphs composed of products, which are constructed based on co-purchased and co-viewed patterns. We use Children, History, Computers and Photo datasets compiled by [39]. For the four datasets, we extract bag-of-words features and utilize principal component analysis (PCA) to reduce dimensions, generating a 100-dimensional feature for each node.\nIn terms of the class split, ogbn-arxiv adopts the same partitioning strategy as TENT [34]. Meanwhile, for other datasets, we employ a random division approach. Notice that our P2TAG differs from meta-learning methods, as it does not require the establishment of specific training and validation tasks. Instead, we ensure fairness using the same test tasks as other baselines.\nCompared methods. We choose three types of methods for comparison, all of which can address the problem of few-shot node classification. These include methods based on meta-learning on graphs, self-supervised pre-training methods on text attribute graphs, and methods following the paradigm of pre-training and prompting. Specifically, meta-learning methods such as GPN [7], G-Meta [16] and TENT [34]; GIANT [3], proposed P2TAG (LM) and P2TAG (GNN) as self-supervised pre-training methods; G2P2 [36] follows the same pre-training and prompting paradigm as proposed P2TAG. In the pre-training phase, we concurrently train both the LM and GNN, considering two scenarios during inference. P2TAG (LM) derives vector representations from the original text of nodes. Meanwhile, P2TAG (GNN) leverages these node representations as inputs to the GNN, resulting in the output obtained.\nEvaluation. GPN, G-Meta, and TENT are methods based on meta-learning, where they are learned on train tasks and are subsequently evaluated on test tasks. In contrast, the proposed P2TAG along with G2P2 and GIANT, are evaluated exclusively on test tasks. Specifically, P2TAG (LM), P2TAG (GNN), and GAINT infer the classes of nodes in the query set by fitting a logistic regression classifier on the support set. Furthermore, P2TAG and G2P2 extend their approach by constructing prompts through the support set. For a detailed comparison, we conduct experiments in four different settings for OGB datasets: 5-way 3-shot, 5-way 5-shot, 10-way 3-shot, and 10-way 5-shot. Considering the number of labels, we use three settings for Amazon Review datasets: 3-way 3-shot, 3-way 5-shot, and 3-way 10-shot.\nParameter configuration. For our framework, the selection of LMs and GNNs is flexible. In our experiment, we choose a representative LM - DeBERTa-base and a powerful GAT model for the main experiments. The DeBERTa-base is a 100M-parameter pre-trained language model with a hidden size of 768. We keep the same hidden size of the GAT model with DeBERTa-base. We also explore other LMs in the ablation studies. We use AdamW optimizer [23] with learning rate lr = 1e-5 for model optimization. We run 3 epochs for all datasets. We construct 5 groups of test tasks for each N-way K-shot setting, with each group consisting of 50 tasks specifically formulated from the test set label. In each task, the support set length is K, and the query set length Q is set to 10. We calculate"}, {"title": "4.2 Performance Analysis", "content": "Our main results are summarized in Table 3 and Table 4. The proposed P2TAG (LM) outperforms the meta-learning methods, increasing the average accuracy on six datasets from +2.27% ~ +35.51%; the P2TAG (GNN) achieves an average improvement with +16.38% ~ +27.55%; the P2TAG performs best on most datasets, with an average improvement of +18.98% ~ +35.98%. Compared with the pre-training method that also utilize raw text such as GI-ANT and G2P2, our P2TAG still has better performance, which demonstrates its effectiveness. On the ogbn-arxiv dataset, although G2P2 constructs prompts on the basis of pre-training, its performance is lower than that of GIANT, P2TAG (LM) and P2TAG (GNN). This underscores the importance of a well pre-trained model for few-shot node classification. Pre-training on TAGs often comes with increased time expenditure, such as G2P2 taking over 30 days on the ogbn-products dataset. The proposed P2TAG employs a more general approach to jointly train LM and GNN, consuming less than one day on this dataset. The P2TAG, employing mixed prompts, further enhances performance on the pre-trained model P2TAG (LM) and P2TAG (GNN). It secures the best outcomes on most datasets, with an average enhancement of 2.89% compared to P2TAG (GNN), demonstrating the effectiveness of prompts. On the History dataset, P2TAG (LM) achieves the second-best results; however, after passing through the GNN encoder, P2TAG (GNN) experiences an average of 4.63% decrease. This might be attributed to the quality of the topological structure within the data. This hypothesis is further supported by the minimal improvement observed when comparing meta-learning methods that do not utilize raw texts (GPN, G-Meta, TENT) to the node features method. The proposed graph prompt improvement strategy, which initializes through label text, mitigates this issue. On the History dataset, P2TAG further enhances performance compared to P2TAG (LM), achieving an average improvement of 3.8% across three settings."}, {"title": "4.3 Ablation Studies", "content": "In the previous sections, we demonstrate the powerful performance of the P2TAG (LM), P2TAG (GNN), and P2TAG. This part analyzes the impact of the different types of prompting and LMs.\nEffect of LMs. To better analyze the impact of LMs, we explore other LMs such as e5-v2-base with 110M parameters [33]. We also try larger LMs such as DeBERTa-large with 350M parameters and e5-v2-large with 330M parameters. The results are reported in Table 5. Generally, the results of LMs are quite similar, with differences within 1.5%. The reason that e5-large does not achieve better results may be attributed to insufficient training iterations. This paper selects DeBERTa-base, intending to address the joint learning problem of LMs and GNNs in a more general manner. There remains room for further exploration in the specific choice of LMs.\nEffect of different prompt types. In Section 3.3, we discussed two significant enhancements. Here, we further investigate these improvements by dissecting them and presenting the findings in Figure 3. We use the P2TAG w/o PG when the label text embedding is not utilized to initialize our prompt graph, and the P2TAG w/o wt when node text embedding is not employed to bolster the prompted graph embedding (i.e., the graph embedding is directly inputted into MLP layers). We report the results on Computers and Photo datasets. The results reveal that our prompting approach outperforms both the P2TAG (LM) and baseline G2P2. Furthermore, the absence of PG has a marginally greater impact than omitting wt across both datasets, underscoring the importance of structural effectiveness in the TAG context."}, {"title": "4.4 Hyperparameter Analysis", "content": "In this part, we conduct a comprehensive analysis of hyperparameters to elucidate their impact on the performance of our framework. We analyze the four key hyperparameters in our experiments. The \"sequence length\" means the reserved length of raw texts. The \"mask rate\" represents the proportion of masked tokens in training. The \"walk length\" means the length of random walks used in the GraphSAINT sampler for mini-batch training. The \"token num\" denotes the number of tokens in the graph prompt. The results of hyperparameter experiments are shown in Table 6.\nPreprocessing: length of truncated sequences. Since each batch will be fed to both the language model and the GNN encoder in training, we need to increase the batch size to better leverage the ability of GNNs. However, the batch size is limited due to the large memory cost of the LM. Therefore, considering both training efficiency and information loss, we explore using a smaller truncated length in our experiments to utilize a larger batch size.\nPre-training: mask rate. A smaller mask rate represents an easier self-supervised task and might make the model overfitted. To make the self-supervised task harder, we need to choose a large enough mask rate to let the model learn better. From the results, we find that our framework achieves the best performance with a 75% mask rate on the ogbn-arxiv dataset.\nSampling: lengths of random walks. For each batch, the sampler conducts multiple random walks from different root nodes with the same length. The GNN encoder of our framework relies on the topology of the sampled mini-batch graph to propagate and aggregate the information between nodes. Therefore, the length of random walks used to construct the mini-batch will influence the model performance.\nPrompting: number of tokens. Incorporating the prompt graph as a trainable parameter highlights the importance of token quantity for downstream tasks. Intuitively, more tokens might create a richer internal structure, offering enhanced information. However, managing more parameters often complicates convergence in few-shot scenarios. To accurately incorporate the textual information of labels into prompting, we typically set the number of tokens to match the number of classes in the task. Surprisingly, 2 tokens also yield results, implying a minimal internal structure. This suggests the effectiveness of token features in improving performance."}, {"title": "5 RELATED WORK", "content": "In this section, we introduce the related work, including graph representation learning and few-shot node classification."}, {"title": "5.1 Graph Representation Learning", "content": "Graph neural networks (GNNs) provide the foundation for applying deep learning on graphs and yielding good results on several downstream tasks. The earlier works [5, 31, 35, 38] perform convolution on small-scale graphs using all topological relations in a semi-supervised way. The subsequent works focus on sampling strategies [2, 10, 48] and model architecture [26, 37] to enhance the scalability of GNNs and apply them to large-scale graphs. GraphSAGE [10] first proposed the idea of neighborhood sampling, and later it was applied in a real-world recommendation system by PinSAGE [45]. GraphSAINT [48], first samples subgraphs [20] and runs full-batch GNNs on sampled subgraphs. Additionally, there are works [52, 53] focusing on the hierarchy within the graph to obtain better representations. The self-supervised learning methods on GNNs are developed via contrastive and generative ways. The contrastive methods [11, 25, 30, 46, 49, 57] adopt data augmentation with or without negative sampling to construct samples to optimize the contrastive loss under different augmentations. GRACE [57] aims to maintain node uniformity across views. BGRL [30] designs two encoders for two views with data augmentation. Some studies focus on graph generative learning. GraphMAE [15] and GraphMAE2 [14] are proposed to reconstruct masked attributes of corrupted nodes for representation learning.\nThe semantics and graph topology of TAGs can express real-world relationships between entities or objects. Most GNNs do not consider the text processing in the TAG but directly use the numerical features, which are generated through text encoding, as attributes of nodes. Recent studies [3, 44] utilize graph topology to enhance the representation during text pre-training. Despite the promising results, the language model is still independent of the GNNs [21, 56]. GraphFormer [41] designs a nested architecture of GNNs and Transformers. GLEM [51] implements the fusion of LMs and GNNs on large-scale text-attributed graphs with the variational expectation-maximization framework. Although these methods progress in integrating LMs and GNNs, there is still a lack of discussions on self-supervised learning on large-scale text-attribute graphs. Our proposed pre-train framework enhances LMs utilizing GNNs and achieves joint training with the objective of self-supervision. Yan et al. [39] release the benchmark on TAGs, and provide multiple clean TAG datasets. These datasets also lay a solid foundation for our experiments."}, {"title": "5.2 Few-shot Node Classification", "content": "Few-shot node classification on graphs to categorize nodes within a graph with limited labeled nodes. Drawing inspiration from the success of meta-learning in few-shot classification tasks within computer vision [9, 27], several studies apply meta-learning techniques to graph-based tasks. GFL [43] and GPN [7] utilize prototype networks to learn the distance from nodes to classes. Meta-GNN [55] optimizes the graph neural network with model-agnostic meta-learning. G-Meta [16] addresses the meta-learning problem on both single and multiple graphs by extracting local subgraphs. TENT [34] enhances the model's generalization capabilities by adapting at three levels: nodes, edges, and tasks.\nWith the increasing attention towards LLMs, several works attempt to follow the paradigm of pre-training and prompt tuning to address the problem of few-shot node classification. Prog [29] incorporates both node-level and edge-level tasks by constructing prompts at the graph level, but lacks analysis of the text attribute. ENG [47] employs LLM with designed prompts to refine node texts and reconstructs the adjacency matrix semantically, which is then used as input for the GNN. G2P2 [36] enhances text information through graph structure, aligning text representations in three forms during the pre-training phase. In the tuning phase, it uses the neighborhood text of the target node and label text to generate the initial parameters of prompts while freezing the parameters of the LM and GNN during the tuning process."}, {"title": "6 CONCLUSION", "content": "Our paper focuses on few-shot node classification on the TAG. We address this problem by employing a graph pre-training and prompting approach. The proposed framework P2TAG utilizes a masked language modeling objective for the joint training of the language model and GNN model. We also propose a new prompting method that mixes graph and text information, enabling the pre-trained model on TAG to better adapt to downstream few-shot node classification tasks. We conduct experiments on six real-world TAG datasets and our P2TAG framework achieves state-of-the-art results on the six datasets with +18.98% ~ +35.98% improvement."}, {"title": "7 ACKNOWLEDGEMENT", "content": "This work is supported by National Key R&D Program of China 2021ZD0113304, Natural Science Foundation of China (NSFC) 62276148 and 62425601, CCF-Zhipu AI Large Model Fund (Grant 202213), Zhipu AI - Anhui University Joint Research Center on Foundation Model and the University Synergy Innovation Program of Anhui Province (GXXT-2023-050), the New Cornerstone Science Foundation through the XPLORER PRIZE, and Tsinghua-Bosch Joint ML Center."}, {"title": "A APPENDIX", "content": "A.1 Implementation Notes\nRunning environment. The experiments are conducted on a Linux machine with AMD EPYC 7642 48-Core Processor, 1T RAM, and 8 NVIDIA A100 (80G). Our code is implemented with PyTorch 1.10 and Python 3.8.\nModel Configuration. Our framework involves the joint training of LM and GNN, with some of the key parameters presented in Table 7. For more detailed configurations, please find in our code."}, {"title": "A.2 Comparing P2TAG with GPrompt", "content": "Though P2TAG shares a similar task and intuition with GPrompt [17]"}, {"title": "Pre-Training and Prompting for Few-Shot Node Classification on Text-Attributed Graphs", "authors": ["Huanjing Zhao", "Beining Yang", "Yukuo Cen", "Junyu Ren", "Chenhui Zhang", "Yuxiao Dong", "Evgeny Kharlamov", "Shu Zhao", "Jie Tang"], "abstract": "The text-attributed graph (TAG) is one kind of important real-world graph-structured data with each node associated with raw texts. For TAGs, traditional few-shot node classification methods directly conduct training on the pre-processed node features and do not consider the raw texts. The performance is highly dependent on the choice of the feature pre-processing method. In this paper, we propose P2TAG, a framework designed for few-shot node classification on TAGs with graph pre-training and prompting. P2TAG first pre-trains the language model (LM) and graph neural network (GNN) on TAGs with self-supervised loss. To fully utilize the ability of language models, we adapt the masked language modeling objective for our framework. The pre-trained model is then used for the few-shot node classification with a mixed prompt method, which simultaneously considers both text and graph information. We conduct experiments on six real-world TAGs, including paper citation networks and product co-purchasing networks. Experimental results demonstrate that our proposed framework outperforms existing graph few-shot learning methods on these datasets with +18.98% ~ +35.98% improvements.", "sections": [{"title": "1 INTRODUCTION", "content": "The few-shot node classification task involves identifying the classes of nodes in a given graph structure using only a limited number of labeled examples. This task has practical applications in areas such as social network analysis, recommendation systems, and more. Inspired by the successful experiences in the field of Computer Vision (CV), several works, such as G-Meta and TENT [7, 16, 34, 43], apply meta-learning to graphs to address the few-shot node classification problem. These methods learn transferable knowledge from meta-tasks, enabling rapid adaptation to new, unseen labels. Unlike images, graphs represent a form of structured data. Through graph pre-training, models can also accumulate a substantial amount of domain-specific knowledge. Recently, a series of graph pre-training methods [30, 40, 50, 54, 57] emerged with self-supervised learning (SSL) to yield generalized node representations in the absence of labels. These methods mainly include contrastive and generative ones. Contrastive SSL methods utilize data augmentation to generate multiple views of data for contrastive loss. Generative SSL methods such as GraphMAE [15] aim to reconstruct the (masked) attributes of the graph. These self-supervised learning methods have greatly contributed to many aspects, such as graph augmentation and graph-based pretext tasks. They only consider part of the self-supervised learning on TAGs, making the training of GNNs independent of LMs text encoding."}, {"title": "2 PRELIMINARIES", "content": "In this section, we introduce the background of our paper including text-attributed graph and few-shot node classification.\nNotations. Denote a graph G = (V, &), where V is a set of n nodes and & is a set of edges between nodes. A \u2208 Rnxn is the adjacency matrix where its entry A(i, j) \u2265 0, if nonzero, denotes there is an edge between node i and j with edge weight A(i, j). In practice, the network could be either directed or undirected. If G is directed, we have A(i, j) \u2260 A(j, i); if G is undirected, we have A(i, j) = A(j, i)."}, {"title": "2.1 Text-Attributed Graph", "content": "DEFINITION 1 (TEXT-ATTRIBUTED GRAPH). A text-attributed graph (TAG) is a graph G = (V, &, S), where each node vi \u2208 V is associated with a text sequence si \u2208 S and & represents the set of edges between nodes.\nGiven the above definition, we can formally define our problem for self-supervised learning on TAG.\nPROBLEM 1 (SELF-SUPERVISED LEARNING ON TAGS). Given a TAG G = (V, &, S), the problem of self-supervised learning (SSL) on TAG is to learn a unified low-dimensional space representation of each text-attributed node vi \u2208 V with text sequence si. The goal is to find a function f : V \u2192 Rd, where d \u00ab |V|.\nPrevious methods implement self-supervised learning on TAG through separated text encoders and GNNs. Built upon these works, we explore self-supervised learning by integrating LMs and GNNs."}, {"title": "2.2 Few-shot Node Classification", "content": "Few-shot node classification are tasks that involve training a machine learning model with a limited number of labeled nodes, and subsequently predicting the classes of other nodes based on these initial labels. The N-way K-shot few-shot node classification tasks T on a graph is described as follows with the node label set C. Each task T\u0118 \u2208 T selects N labels from C, forming a subset Ct = {C1, C2, ..., CN} \u2208 C. The Te consists of two parts as follows:\nTt = {St, Qt},\nwhere St means the support set and Qt means the query set. The machine learning model is trained on St and subsequently evaluated on Qt. Both the support set and the query set consist of nodes and their corresponding labels, which shown as follows:\nSt = {(01, C1), (U2, C2), ..., (UN\u00d7K, CN\u00d7K)},\nQt = {(01, c\u2081), (02, c\u2082), ..., (UNxQ, CNxQ)},\nwhere v and v' represent nodes, their corresponding labels are denoted as c and c'. In the support and query sets, each label Ci \u2208 Ct associated with K nodes and Q nodes, respectively. For the support set, there exists c(i-1)*K+j \u2208 Ci, where i \u2208 {1, 2, .., N} and j \u2208 {1, 2, .., K}. Similarly, there exists c\u02bb(i\u22121)*Q+j \u2208 Ci, where i \u2208 {1, 2, .., N} and j \u2208 {1, 2, .., Q} for the query set."}, {"title": "3 METHODOLOGY", "content": "We tackle few-shot node classification by adhering to the pre-training and prompting paradigm, as depicted in Figure 1. To better leverage the unique characteristics of the TAGs, we propose a new self-supervised framework for pre-training."}, {"title": "3.1 Pre-training Framework", "content": "In this part, we introduce our proposed pre-training framework in detail. For self-supervised learning on TAGs, previous works usually take two separate steps. The first step is to encode the raw texts to node features using bag-of-words, word2vec [24], or pre-trained language models [4, 6, 8, 12, 19, 22, 42]. Most graph self-supervised methods [15, 30, 50, 54, 57] use the node features pre-processed in the first step to construct a self-supervised objective. The two-step process can bring non-negligible information loss. To address this issue, we propose an end-to-end self-supervised framework to train directly on the TAGs. Inspired by the recent prosperity of pre-trained language models (LM), we choose an effective LM to encode the raw texts. To model the relations between nodes (texts), we can utilize powerful graph neural networks (GNNs). As illustrated in Figure 1, our framework consists of two core modules, including a pre-trained language model and a GNN encoder. The GNN encoder is to help make better node representations. In our framework, we mainly use the DeBERTa-base [12] with 100M parameters as the LM of our framework. DeBERTa utilizes two techniques based on BERT [6] and RoBERTa [22] and significantly improves the performance on the natural language understanding and generation tasks. The choice of the LMs is flexible, and we also explore other LMs in our experiments.\nThe pre-training of LMs and GNNs is much more challenging because we need to 1) choose an appropriate self-supervised training objective to avoid over-fitting and 2) sample small mini-batches to address the high computational and space costs of large LMs.\nSelf-supervised training objective. The training objective plays an important role in self-supervised learning. Different from conventional graph self-supervised learning, the design of self-supervised objective for TAGs are more challenging. Our model architecture contains two different modules with different scales of model parameters (large LM v.s. small GNN), making the training much more difficult. Simple self-supervised objectives will probably make the model overfitted. To make a harder self-supervised objective, we adopt the masked language modeling (MLM) introduced in [6] as our objective. Given a node vi, we first feed the text sequence Si = [T1, T2,..., Tn\u2081] into a language model to get the hidden representations associated with vi where Tt is t-th token of si and ni is the length of si. In the training stage, we randomly mask a pre-defined portion of tokens by replacing them with a special token named [MASK]. We use s = [T', T2,..., T\u2081] to denote the text sequence after masking and each token T' is a random variable with the following distribution:\nPr (T' = [MASK]) = p and Pr (T' = T\u2081) = 1 \u2212 p.   (1)\nHere, p\u2208 (0, 1) is a hyper-parameter representing the mask rate. When working with BERT-like language models, we usually add a starting token (e.g., [CLS]) and an ending token (e.g., [SEP]) to the sequence. Therefore, each node is represented as a sequence of hidden vectors:\n[00, 01,..., 0n\u2081, 0ni+1] = fLM ([[CLS], T', T2, ..., Th\u2081, [SEP]]),   (2)\nwhere ot \u2208 Rd is the hidden representations of t-th token of s. Notice that the first hidden vector on corresponds to the special token [CLS] and can be treated as the \"summary\" representation of the whole text sequence si. To capture the correlations among nodes, we then use a graph neural network to propagate the hidden representations of nodes where the input of node vi, denoted as xi, is the first hidden vector oo of si. The node representations after passing a GNN encoder are denoted as H = fGNN (G,X). The propagated node presentations are exploited to construct the self-supervised training objective.\nFor each node vi, we concatenate the hidden representation hi with the output vector ot of each token and then feed the concatenated vector to an MLP as:\nZt = MLP([h, o ]), where t = 0, \u00b7\u00b7 \u00b7, (ni + 1).   (3)\nThe objective of the masked language model (MLM) is to predict the masked tokens in the original text sequence si, which can be expressed as:\nLi = \u03a3^{N}_{i=1} \u03a3^{N_i}_{t=1} 1(T'_t = [MASK]) log Pr(T_t | z_t).  (4)\nThe indicator 1 (T' = [MASK]) ensures that the loss is only applied on masked tokens. We can control the difficulty of the SSL task via a flexible hyperparameter, i.e., mask rate."}, {"title": "3.2 Mini-batch Training", "content": "Due to the heavy language models, we need to adopt a mini-batch training strategy even if we train our model on small graph datasets. There are several genres of mini-batch training techniques for GNNs. To trade off the efficiency and flexibility, we choose a subgraph-based method, GraphSAINT [48], as our training strategy. At every training step, GraphSAINT constructs a mini-batch by sampling a subgraph from the original graph and generates node representations according to the sampled subgraph. In this work, we adopt the random walk sampler to preserve the connectivity of the whole graph. The sampling process starts by selecting r root nodes from the whole node set V uniformly at random. Starting from each root node, a random walk of length I is sampled from the original graph structure. We then have a sampled node set Vs by adding all the nodes that occurred in the random walks and the subgraph Gs induced by Vs is used in the GNN encoder to generate node representations in the minibatch."}, {"title": "3.3 Graph-Text Mixed Prompt Learning", "content": "To prevent tuning the whole pre-trained model in the few-shot downstream tasks, we adopt the prompt learning paradigm, i.e., using a few trainable parameters to be the substitution of full-scale model parameters. The goal of prompt design is to mitigate the gap between the pre-train objective and the downstream tasks. It is highly challenging since there are both text (LM side) and structure information (GNN side) on TAG graphs. We try to investigate the joint prompting method, namely the graph prompt and the text prompt, the detailed design as follows. To simplify the discussion, we start with a target node v (e.g., the 0-th node in Figure 1). We further introduce the concept of the ego graph node set Vego. We select up to 100 first-order neighbors of the given node, along with the node itself, to form the node set of the ego graph. Then we define the induced ego graph from Vego as Gego.\n3.3.1 Graph prompt design. Inspired by the recently proposed graph prompt literature [28, 29], we aim to bridge gap through a small yet informative prompt graph. We first craft a prompt graph Gp = (Vp, &p) serving as an ancillary component connected to the target ego node set Vego. Gp comprises ||Gp || nodes (i.e., tokens), the internal edges between tokens can be defined as:\nAGp (i, j) = { 1  if Sim(pi, pj) \u2265 6inner, 0 otherwise,  (5)\nwhere the Sim() denotes the similarity function, and the pi denotes the feature of i-th tokens; in this paper, we use dot product as the implicit operation. Then, let the xk denotes on the k-th node feature of the Gego, the inter-connectivity between Gp and Gego is given by:\nAGego, Gp (k, j) = { 1  if Sim(xk, pj) \u2265 6inter, 10  otherwise.  (6)\nHere, the dinner and ointer are two hyper-parameters indicating the pruning value, i.e., the entries on the matrix higher than oinner and router are formed as edge respectively.\nInitialize tokens via the label text. While the prompt graph servers as an efficient substitute for full-scale model parameters, we empirically find that the naive Gp performs sub-optimally. We attribute this to the inconsistency in the pre-trained task objectives between our masked token prediction task and previous graph-based ones [28, 29] (e.g., the link prediction task).\nReferencing Figure 2 as an example, during the pre-training phase where token prediction tasks are undertaken, the model is equipped to align input with the pre-trained label space. Consequently, the prompt should slightly modify the input distribution to approximate the downstream label space more closely. Hence, unlike traditional cumbersome and hand-crafted text prompt templates, we discovered that employing label text embeddings for prompt graph initialization is more direct and efficient. Under the homophily hypothesis, the target node's embedding is immediately positioned more closer to the desired label space. Specifically, we utilize the pre-trained fLM as our label text encoder, each label text (e.g., \"Animals\", \"Science Fiction and Fantasy\", and \"Activities, Crafts and Games\") can be represented as sy\u2081. Then we can formalize the initialization of Gp as follows:\nP(0) = { fLM (Syi)  if i \u2264 N, Pi Random Initialization otherwise.  (7)\nHere, the p(0) denotes the initial embedding of i-th prompt node, and for the additional prompt nodes, we adopt the random initilization. Then the inner structure of each nodes will be constructed through the Eq. 5. The links among the prompt nodes and the target ego graph Gego are built by the Eq. 6.\n3.3.2 Text prompt design. As shown in Equation 3, we use the joint representation of the LM model output (text information) and the GNN model output (structure information). Therefore, to align the pre-train and downstream tasks, we use a similar strategy to concat both output features in the downstream scenario.\nThen, instead of making intricate text-based prompts, we use a rather simple way: letting the second concat feature be a trainable parameter, namely wt \u2208 R1\u00d7d, i.e., treating the LM model output itself as a tunable parameter. The initialization process can be defined similarly as:\nw\u2081 = fLM (So),  (8)\nthe so shows the text sequence with the target node v. Our ablation study in Section 4.3 shows our text prompt's effectiveness.\n3.3.3 Final prompt learning forward function. Assuming we have a node ego graph Go and its text so, the prompt learning forward function is:\n2 = MLP(READOUT(fGNN (Gv; Gp)),wt).  (9)\nThe parameters of fLM and fGNN are fixed during prompt learning, the parameters of Gp, the wt and the task head (e.g., the MLP layers) are all trainable."}, {"title": "3.4 Model Inference", "content": "Finally, we incorporate the pretrained models fLM, \u0192GNN, and the prompt graph Gp in the model inference stage. We outline three inference methods. The first utilizes the trained LM, referred to as P2TAG (LM). It computes the output of the [CLS] token of the LM for each node in the graph via a mini-batch manner. The second method involves further feeding the node [CLS] outputs from P2TAG (LM) into a GNN encoder, referred to as P2TAG (GNN). In the training stage, the node representation of vi is estimated according to the sampled neighborhood in the subgraph, and thus, varies with the sampling process. The randomness introduced by the sampler is unpleasant during the test stage. To get the exact node representation, we use the full neighbor sampling strategy when performing inference. That is, The node representation of vi at the l-th layer is obtained by aggregating the representations of its full neighborhood at the (l - 1)-th layer. The computation process is performed layer by layer. Taking the node representations at the (l - 1)-th layer as the input, we can compute a batch of node representations at the f-th by first loading their full neighborhoods and then aggregating the representations of those neighbor nodes at the (l-1)-th layer. The third method involves conducting few-shot node classification through prompting, as described in Section 3.3. The P2TAG inference loop of our framework is listed in Algorithm 2."}, {"title": "4 EXPERIMENT", "content": "In this section, we conduct comprehensive experiments to validate the effectiveness of our framework, particularly emphasizing the enhanced performance for few-shot node classification tasks."}, {"title": "4.1 Experimental Setup", "content": "Datasets. We conduct experiments across two categories of datasets. All these datasets are publicly available and widely used in TAG-related research. The statistics of the datasets are given in Table 2.\n\u2022 Open Graph Benchmark (OGB). ogbn-arxiv is a directed citation graph between all computer science (CS) ArXiv papers indexed by Microsoft Academic Graph (MAG) [32]. We convert the abbreviated labels into their full forms as specified on the arXiv website, such as transforming \"NA\" into \"Numerical Analysis\". ogbn-products is an undirected and unweighted graph representing an Amazon product co-purchasing network [1].\n\u2022 Amazon Review. Amazon Review includes graphs composed of products, which are constructed based on co-purchased and co-viewed patterns. We use Children, History, Computers and Photo datasets compiled by [39]. For the four datasets, we extract bag-of-words features and utilize principal component analysis (PCA) to reduce dimensions, generating a 100-dimensional feature for each node.\nIn terms of the class split, ogbn-arxiv adopts the same partitioning strategy as TENT [34]. Meanwhile, for other datasets, we employ a random division approach. Notice that our P2TAG differs from meta-learning methods, as it does not require the establishment of specific training and validation tasks. Instead, we ensure fairness using the same test tasks as other baselines.\nCompared methods. We choose three types of methods for comparison, all of which can address the problem of few-shot node classification. These include methods based on meta-learning on graphs, self-supervised pre-training methods on text attribute graphs, and methods following the paradigm of pre-training and prompting. Specifically, meta-learning methods such as GPN [7], G-Meta [16] and TENT [34]; GIANT [3], proposed P2TAG (LM) and P2TAG (GNN) as self-supervised pre-training methods; G2P2 [36] follows the same pre-training and prompting paradigm as proposed P2TAG. In the pre-training phase, we concurrently train both the LM and GNN, considering two scenarios during inference. P2TAG (LM) derives vector representations from the original text of nodes. Meanwhile, P2TAG (GNN) leverages these node representations as inputs to the GNN, resulting in the output obtained.\nEvaluation. GPN, G-Meta, and TENT are methods based on meta-learning, where they are learned on train tasks and are subsequently evaluated on test tasks. In contrast, the proposed P2TAG along with G2P2 and GIANT, are evaluated exclusively on test tasks. Specifically, P2TAG (LM), P2TAG (GNN), and GAINT infer the classes of nodes in the query set by fitting a logistic regression classifier on the support set. Furthermore, P2TAG and G2P2 extend their approach by constructing prompts through the support set. For a detailed comparison, we conduct experiments in four different settings for OGB datasets: 5-way 3-shot, 5-way 5-shot, 10-way 3-shot, and 10-way 5-shot. Considering the number of labels, we use three settings for Amazon Review datasets: 3-way 3-shot, 3-way 5-shot, and 3-way 10-shot.\nParameter configuration. For our framework, the selection of LMs and GNNs is flexible. In our experiment, we choose a representative LM - DeBERTa-base and a powerful GAT model for the main experiments. The DeBERTa-base is a 100M-parameter pre-trained language model with a hidden size of 768. We keep the same hidden size of the GAT model with DeBERTa-base. We also explore other LMs in the ablation studies. We use AdamW optimizer [23] with learning rate lr = 1e-5 for model optimization. We run 3 epochs for all datasets. We construct 5 groups of test tasks for each N-way K-shot setting, with each group consisting of 50 tasks specifically formulated from the test set label. In each task, the support set length is K, and the query set length Q is set to 10. We calculate"}, {"title": "4.2 Performance Analysis", "content": "Our main results are summarized in Table 3 and Table 4. The proposed P2TAG (LM) outperforms the meta-learning methods, increasing the average accuracy on six datasets from +2.27% ~ +35.51%; the P2TAG (GNN) achieves an average improvement with +16.38% ~ +27.55%; the P2TAG performs best on most datasets, with an average improvement of +18.98% ~ +35.98%. Compared with the pre-training method that also utilize raw text such as GI-ANT and G2P2, our P2TAG still has better performance, which demonstrates its effectiveness. On the ogbn-arxiv dataset, although G2P2 constructs prompts on the basis of pre-training, its performance is lower than that of GIANT, P2TAG (LM) and P2TAG (GNN). This underscores the importance of a well pre-trained model for few-shot node classification. Pre-training on TAGs often comes with increased time expenditure, such as G2P2 taking over 30 days on the ogbn-products dataset. The proposed P2TAG employs a more general approach to jointly train LM and GNN, consuming less than one day on this dataset. The P2TAG, employing mixed prompts, further enhances performance on the pre-trained model P2TAG (LM) and P2TAG (GNN). It secures the best outcomes on most datasets, with an average enhancement of 2.89% compared to P2TAG (GNN), demonstrating the effectiveness of prompts. On the History dataset, P2TAG (LM) achieves the second-best results; however, after passing through the GNN encoder, P2TAG (GNN) experiences an average of 4.63% decrease. This might be attributed to the quality of the topological structure within the data. This hypothesis is further supported by the minimal improvement observed when comparing meta-learning methods that do not utilize raw texts (GPN, G-Meta, TENT) to the node features method. The proposed graph prompt improvement strategy, which initializes through label text, mitigates this issue. On the History dataset, P2TAG further enhances performance compared to P2TAG (LM), achieving an average improvement of 3.8% across three settings."}, {"title": "4.3 Ablation Studies", "content": "In the previous sections, we demonstrate the powerful performance of the P2TAG (LM), P2TAG (GNN), and P2TAG. This part analyzes the impact of the different types of prompting and LMs.\nEffect of LMs. To better analyze the impact of LMs, we explore other LMs such as e5-v2-base with 110M parameters [33]. We also try larger LMs such as DeBERTa-large with 350M parameters and e5-v2-large with 330M parameters. The results are reported in Table 5. Generally, the results of LMs are quite similar, with differences within 1.5%. The reason that e5-large does not achieve better results may be attributed to insufficient training iterations. This paper selects DeBERTa-base, intending to address the joint learning problem of LMs and GNNs in a more general manner. There remains room for further exploration in the specific choice of LMs.\nEffect of different prompt types. In Section 3.3, we discussed two significant enhancements. Here, we further investigate these improvements by dissecting them and presenting the findings in Figure 3. We use the P2TAG w/o PG when the label text embedding is not utilized to initialize our prompt graph, and the P2TAG w/o wt when node text embedding is not employed to bolster the prompted graph embedding (i.e., the graph embedding is directly inputted into MLP layers). We report the results on Computers and Photo datasets. The results reveal that our prompting approach outperforms both the P2TAG (LM) and baseline G2P2. Furthermore, the absence of PG has a marginally greater impact than omitting wt across both datasets, underscoring the importance of structural effectiveness in the TAG context."}, {"title": "4.4 Hyperparameter Analysis", "content": "In this part, we conduct a comprehensive analysis of hyperparameters to elucidate their impact on the performance of our framework. We analyze the four key hyperparameters in our experiments. The \"sequence length\" means the reserved length of raw texts. The \"mask rate\" represents the proportion of masked tokens in training. The \"walk length\" means the length of random walks used in the GraphSAINT sampler for mini-batch training. The \"token num\" denotes the number of tokens in the graph prompt. The results of hyperparameter experiments are shown in Table 6.\nPreprocessing: length of truncated sequences. Since each batch will be fed to both the language model and the GNN encoder in training, we need to increase the batch size to better leverage the ability of GNNs. However, the batch size is limited due to the large memory cost of the LM. Therefore, considering both training efficiency and information loss, we explore using a smaller truncated length in our experiments to utilize a larger batch size.\nPre-training: mask rate. A smaller mask rate represents an easier self-supervised task and might make the model overfitted. To make the self-supervised task harder, we need to choose a large enough mask rate to let the model learn better. From the results, we find that our framework achieves the best performance with a 75% mask rate on the ogbn-arxiv dataset.\nSampling: lengths of random walks. For each batch, the sampler conducts multiple random walks from different root nodes with the same length. The GNN encoder of our framework relies on the topology of the sampled mini-batch graph to propagate and aggregate the information between nodes. Therefore, the length of random walks used to construct the mini-batch will influence the model performance.\nPrompting: number of tokens. Incorporating the prompt graph as a trainable parameter highlights the importance of token quantity for downstream tasks. Intuitively, more tokens might create a richer internal structure, offering enhanced information. However, managing more parameters often complicates convergence in few-shot scenarios. To accurately incorporate the textual information of labels into prompting, we typically set the number of tokens to match the number of classes in the task. Surprisingly, 2 tokens also yield results, implying a minimal internal structure. This suggests the effectiveness of token features in improving performance."}, {"title": "5 RELATED WORK", "content": "In this section, we introduce the related work, including graph representation learning and few-shot node classification."}, {"title": "5.1 Graph Representation Learning", "content": "Graph neural networks (GNNs) provide the foundation for applying deep learning on graphs and yielding good results on several downstream tasks. The earlier works [5, 31, 35, 38] perform convolution on small-scale graphs using all topological relations in a semi-supervised way. The subsequent works focus on sampling strategies [2, 10, 48] and model architecture [26, 37] to enhance the scalability of GNNs and apply them to large-scale graphs. GraphSAGE [10] first proposed the idea of neighborhood sampling, and later it was applied in a real-world recommendation system by PinSAGE [45]. GraphSAINT [48], first samples subgraphs [20] and runs full-batch GNNs on sampled subgraphs. Additionally, there are works [52, 53] focusing on the hierarchy within the graph to obtain better representations. The self-supervised learning methods on GNNs are developed via contrastive and generative ways. The contrastive methods [11, 25, 30, 46, 49, 57] adopt data augmentation with or without negative sampling to construct samples to optimize the contrastive loss under different augmentations. GRACE [57] aims to maintain node uniformity across views. BGRL [30] designs two encoders for two views with data augmentation. Some studies focus on graph generative learning. GraphMAE [15] and GraphMAE2 [14] are proposed to reconstruct masked attributes of corrupted nodes for representation learning.\nThe semantics and graph topology of TAGs can express real-world relationships between entities or objects. Most GNNs do not consider the text processing in the TAG but directly use the numerical features, which are generated through text encoding, as attributes of nodes. Recent studies [3, 44] utilize graph topology to enhance the representation during text pre-training. Despite the promising results, the language model is still independent of the GNNs [21, 56]. GraphFormer [41] designs a nested architecture of GNNs and Transformers. GLEM [51] implements the fusion of LMs and GNNs on large-scale text-attributed graphs with the variational expectation-maximization framework. Although these methods progress in integrating LMs and GNNs, there is still a lack of discussions on self-supervised learning on large-scale text-attribute graphs. Our proposed pre-train framework enhances LMs utilizing GNNs and achieves joint training with the objective of self-supervision. Yan et al. [39] release the benchmark on TAGs, and provide multiple clean TAG datasets. These datasets also lay a solid foundation for our experiments."}, {"title": "5.2 Few-shot Node Classification", "content": "Few-shot node classification on graphs to categorize nodes within a graph with limited labeled nodes. Drawing inspiration from the success of meta-learning in few-shot classification tasks within computer vision [9, 27], several studies apply meta-learning techniques to graph-based tasks. GFL [43] and GPN [7] utilize prototype networks to learn the distance from nodes to classes. Meta-GNN [55] optimizes the graph neural network with model-agnostic meta-learning. G-Meta [16] addresses the meta-learning problem on both single and multiple graphs by extracting local subgraphs. TENT [34] enhances the model's generalization capabilities by adapting at three levels: nodes, edges, and tasks.\nWith the increasing attention towards LLMs, several works attempt to follow the paradigm of pre-training and prompt tuning to address the problem of few-shot node classification. Prog [29] incorporates both node-level and edge-level tasks by constructing prompts at the graph level, but lacks analysis of the text attribute. ENG [47] employs LLM with designed prompts to refine node texts and reconstructs the adjacency matrix semantically, which is then used as input for the GNN. G2P2 [36] enhances text information through graph structure, aligning text representations in three forms during the pre-training phase. In the tuning phase, it uses the neighborhood text of the target node and label text to generate the initial parameters of prompts while freezing the parameters of the LM and GNN during the tuning process."}, {"title": "6 CONCLUSION", "content": "Our paper focuses on few-shot node classification on the TAG. We address this problem by employing a graph pre-training and prompting approach. The proposed framework P2TAG utilizes a masked language modeling objective for the joint training of the language model and GNN model. We also propose a new prompting method that mixes graph and text information, enabling the pre-trained model on TAG to better adapt to downstream few-shot node classification tasks. We conduct experiments on six real-world TAG datasets and our P2TAG framework achieves state-of-the-art results on the six datasets with +18.98% ~ +35.98% improvement."}, {"title": "7 ACKNOWLEDGEMENT", "content": "This work is supported by National Key R&D Program of China 2021ZD0113304, Natural Science Foundation of China (NSFC) 62276148 and 62425601, CCF-Zhipu AI Large Model Fund (Grant 202213), Zhipu AI - Anhui University Joint Research Center on Foundation Model and the University Synergy Innovation Program of Anhui Province (GXXT-2023-050), the New Cornerstone Science Foundation through the XPLORER PRIZE, and Tsinghua-Bosch Joint ML Center."}, {"title": "A APPENDIX", "content": "A.1 Implementation Notes\nRunning environment. The experiments are conducted on a Linux machine with AMD EPYC 7642 48-Core Processor, 1T RAM, and 8 NVIDIA A100 (80G). Our code is implemented with PyTorch 1.10 and Python 3.8.\nModel Configuration. Our framework involves the joint training of LM and GNN, with some of the key parameters presented in Table 7. For more detailed configurations, please find in our code."}, {"title": "A.2 Comparing P2TAG with GPrompt", "content": "Though P2TAG shares a similar task and intuition with GPrompt [17"}]}]}