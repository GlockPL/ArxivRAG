{"title": "A General Error-Theoretical Analysis Framework for Constructing Compression Strategies", "authors": ["Boyang Zhang", "Daning Cheng", "Yunquan Zhang", "Meiqi Tu", "Fangmin Liu", "Jiake Tian"], "abstract": "The exponential growth in parameter size and computational complexity of deep models poses significant challenges for efficient deployment. The core problem of existing compression methods is that different layers of the model have significant differences in their tolerance to compression levels. For instance, the first layer of a model can typically sustain a higher compression level compared to the last layer without compromising performance. Thus, the key challenge lies in how to allocate compression levels across layers in a way that minimizes performance loss while maximizing parameter reduction. To address this challenge, we propose a Compression Error Theory (CET) framework, designed to determine the optimal compression level for each layer. Taking quantization as an example, CET leverages differential expansion and algebraic geometry to reconstruct the quadratic form of quantization error as ellipsoids and hyperbolic paraboloids, and utilizes their geometric structures to define an error subspace. To identify the error subspace with minimal performance loss, by performing orthogonal decomposition of the geometric space, CET transforms the optimization process of the error subspace into a complementary problem. The final theoretical analysis shows that constructing the quantization subspace along the major axis results in minimal performance degradation. Through experimental verification of the theory, CET can greatly retain performance while compressing. Specifically, on the ResNet-34 model, CET achieves nearly 11\u00d7 parameter compression while even surpassing performance comparable to the original model.", "sections": [{"title": "1. Introduction", "content": "Existing research shows that increasing model size and training data can significantly enhance the performance and learning capability of deep models. However, such improvements often come with a sharp increase in computational complexity and storage requirements, especially in resource-constrained hardware and latency-sensitive scenarios. Thus, a critical challenge is how to efficiently compress model parameters while maintaining nearly unchanged performance.\nAgainst this backdrop, mixed compression techniques have become a mainstream approach for model compression. Due to the varying contributions of different layers to overall model performance, different layers exhibit significantly different tolerance levels to compression. Adopting layer-wise differentiated compression strategies can thus maximize compression rates while minimizing performance degradation. For example, in low-rank decomposition, different layers have varying ranks. In quantization, some critical layers require higher precision, while others can use lower precision. However, the main challenge with this approach lies in the exponentially growing search space for determining the optimal mixed-precision quantization configuration. Specifically, for a neural network with L layers, where each layer can select from four possible bit-widths (e.g., 2/3/4/8 bits), the search space grows to $4^L$. The huge search space makes it almost impossible to find the optimal configuration that can maintain good generalization performance and meet hardware efficiency.\nSeveral representative compression methods have been proposed to address these challenges. For instance, DMBQ (Zhao et al., 2021) pre-emptively searches for the optimal bit-width configuration in the distribution space and dynamically selects compression settings during training. The HAWQ (Dong et al., 2019b) series computes layer-wise Hessian information to assess the relative sensitivity of each layer, thereby determining the optimal compression configuration. However, these approaches suffer from the following limitations: (1) Lack of optimality explanation: Existing compression configurations often rely on heuristic approaches, lacking a clear theoretical foundation to justify their optimality. (2) Neglect of error correlation: These methods typically decouple the optimization of model performance error and compression error, failing to systematically analyze their theoretical interdependence. (3) Limited adaptability to aggressive quantization: At lower bit widths, these methods struggle to handle degradation caused by compression errors, which can disrupt retraining and result in significant performance drops.\nTo address the aforementioned challenges, we propose a general Compression Error Theory (CET) framework, which systematically derives the optimal compression configuration. As shown in Figure 1, taking quantization as an example, CET first establishes the relationship between quantization-induced parameter errors and performance loss errors using total differentiation, and then applies algebraic geometry to transform the quadratic form of loss errors into a geometric representation. This geometric representation directly guides the selection of the quantization parameter space, clearly identifying the optimal quantization direction and range, and is more adaptable to low-bit-width compression. Next, by utilizing the theory of orthogonal complements, the process of solving the error subspace is converted into a complementary problem, allowing for the determination of the best quantization configuration. Unlike traditional methods, CET does not require retraining and can directly achieve the optimal compression configuration. Experimental results show that CET can successfully maximize model compression with minimal performance degradation. Specifically, for the ResNet-34 model, CET achieves nearly 11 parameter compression while even surpassing performance comparable to the original model. Existing compression error analysis methods rarely explore the problem from the perspective of algebraic geometry and spatial structure. Our approach leverages algebraic geometry and total differential analysis to examine the geometric properties and spatial structure of parameters during the compression process, using these insights to guide the direction and magnitude of compression. Although CET is based on quantization, its applicability extends beyond quantization and can be widely applied to other compression methods. The main contributions are as follows:\n1) We propose a general Compression Error Theory (CET) framework, which derives the optimal compression configuration through theoretical analysis. CET is independent of specific compression methods and has broad applicability.\n2) The proposed CET combines total differentiation and algebraic geometry to precisely guide the selection of the compression parameter space, avoiding the need for retraining commonly required by traditional methods.\n3) Experimental results show that CET can maximize model compression with minimal performance degradation. For instance, on the ResNet-34 model, CET achieves nearly 11 parameter compression while even surpassing performance comparable to the original model."}, {"title": "2. Related Works", "content": "2.1. Neural Network in Function\nWe present the analysis of neural networks as composite functions. All our conclusions are independent of the structure of the neural network. First, for an n-layer neural network model, the loss of the model is optimized according to the following equation\n$\\min_W f(W) = E_{sample} l(W, Sample) = \\frac{1}{m} \\sum_{(xi, Yi) \\in D} l(W, xi, Yi)$,\n$l(W, xi, Yi) = L(model_n(xi, W), Yi)$,\n$model_n = h_1(h_2(h_3(h_4(\\ldots(h_{n+1}, W_n)\\ldots, W_4), W_3), W_2), W_1)$,\n(1)\nwhere $f(\\cdot)$ represents the loss of the model on a dataset, $E$ stands for expectation, $m$ is the size of the dataset, $l(.)$ is the loss function for a sample, and $(x_i, Y_i)$ denotes a sample in the dataset along with its corresponding label, $L(\\cdot)$ represents the loss function, such as the cross-entropy function; $h_i$, with $i \\in [1, ..., n]$ represents the $(n \u2013 i + 1)$th layer in the neural network, $W = (w_n, w_{n-1}, \\ldots, w_1)^T$, where $w_i$ is the parameter in $h_i(\\cdot)$, and for the reason of a unified format, $h_{n+1}$ denotes the sample $x$. When the model is treated as a complex high-dimensional nonlinear mapping, it encapsulates the structural constraints of the network and the characteristics of the loss function. Its local properties can be studied through differential and algebraic geometry methods to uncover the local shape characteristics of the function.\n2.2. Quantization for Compression\nThe computational units of deep models are primarily composed of matrix multiplication. Quantization accelerates the multiplication process by converting floating-point parameters into lower-bit formats, thus speeding up the inference process. For a single layer of a neural network, it is represented as $Y = X \\cdot W \\in R^{S \\times C_{out}}$, where $X \\in R^{S \\times C_{in}}$ is the activation input and $W \\in R^{C_{in} \\times C_{out}}$ is the weight matrix. Taking integer uniform quantization as an example, the b-bit quantization process maps the FP16/32 weight tensor $W$ to a lower-bit integer $W_q$.\n$Q(W) = clamp(\\frac{W}{\\Delta} + z, 0, 2^b-1)$\nwhere $\\Delta = \\frac{max(W) - min(W)}{2^b}$\n$\\min (W)$\n(2)\nThe notation is $[\\cdot]$ means the nearest rounding operation, $\\Delta$ is the quantization step size and $z$ represents the zero point. When adopting minimum square error (MSE) as the criterion, the quantization process is expressed as the following minimization error problem:\n$\\min \\|W-Q(W)\\|^2 \\quad s.t. Q(W) \\in \\Pi_b$\n(3)\n$Q(W): R^D \\times Z^+ \\rightarrow I_b$ is the quantization function (Equation 2). Existing methods focus on reducing parameter errors through hybrid quantization schemes, which can be divided into two categories: search-based and tolerance-based methods. Search-based methods, such as those by (Wang et al., 2019; Lou et al., 2019; Wu et al., 2018) treat the quantized network as a whole and use various bit-width allocations to evaluate the model. The evaluation results are then used to guide the search process to find the optimal solution. However, these methods are computationally expensive, difficult to parallelize, and due to their iterative search nature, require hundreds or thousands of GPU hours.\nTo optimize efficiency, another category of tolerance-based methods measures each layer's tolerance to quantization errors. When the tolerance of a layer is higher, the layer can be quantized with a lower bit-width. Various sensitivity metrics have been proposed in practice, such as the Kullback-Leibler divergence between the quantized layer output and the full-precision layer output ((Cai et al., 2020)), the maximum eigenvalue of the Hessian ((Dong et al., 2019b)), the trace of the Hessian ((Dong et al., 2019a; Yao et al., 2020)), the Gaussian-Newton matrix approximation of the Hessian ((Chen et al., 2021)), or the quantization factor ((Tang et al., 2022)). All tolerance-based methods minimize the sum of tolerance across layers under the constraint of target compression ratio. Although these methods are effective in practice, they lack a theoretical foundation to justify the optimality of their results. Furthermore, since these methods do not consider the relationship between model performance and parameter errors, they struggle to address model degradation caused by compression errors at lower bit widths, which lead to retraining failures or sharp performance drops."}, {"title": "3. Compression Error Theoretical Analysis Framework", "content": "3.1. Error Correlation and Optimization\nTypically, compression involves two types of errors: the change in parameters after compression, $\\Delta w$, and the change in loss caused by the parameter variation, $\\Delta L$. CET first establishes the relationship between the two and performs a unified analysis, aiming to determine the direction and magnitude of $\\Delta w$ to minimize $\\Delta L$.\nThe mathematical essence of many compression schemes such as quantization and decomposition is to introduce compression errors into the original parameters. After compression, for a sample, the model loss $l$ during inference is reformulated as the following equation\n$l_k(w, x_j, Y_j) = L(h_1(h_2(h_n(x_i, (h_2(h_n(x_i, W_n + \\delta_n)\\ldots, w_2 + \\delta_2), w_1 + \\delta_1), y_i)$, (4)\nwhere $\\delta_i \\in \\Delta w$, $i \\in \\{1,\\ldots, n\\}$ denotes the noise error on the weights after the k-level compression. In quantization, the k-level represents different bit widths. CET directly associates the compression error and the change of the loss function through total differentials. According to total differentials, the following equation can be obtained\n$\\Delta L = l(w, x_i, Y_i) \u2013 l(W, x_i, Y_i) = \\sum_{i=1}^{n} \\frac{\\partial l}{\\partial w_i} \\delta_i + \\frac{1}{2} \\delta_i^T I \\delta_j + O(\\|(\\delta_i)\\|^n)$ (5)\nwhere $I$ represents the Hessian matrix and $O(\\|(\\delta_i)\\|^n)$ represents the high-order term, is inner product. For the loss on the whole dataset, we can gain\n$\\min \\Delta_L = \\min f(w) \u2013 f(w) = \\frac{1}{m} \\sum_{(X, Y) \\in D} \\sum_{i=1}^{n} \\frac{\\partial l}{\\partial w_i} \\delta_i + \\frac{1}{2} \\delta_i^T I \\delta_j + O(\\|(\\delta_i)\\|^n)$\n(6)\nwhere $f(w) = \\sum l(\\cdot)$. This equation directly links compression $\\Delta w$ and model performance $\\Delta_L$. Although higher-order differentials provide theoretical support for CET, their usage requires meeting the following conditions. First, the function must be smooth and differentiable; second, the parameter changes must be small enough. According to the chain rule, multi-layer neural networks are continuously differentiable concerning all parameters, meaning that they are inherently smooth and differentiable. Thus, Eq. 5 generally satisfies $C^k$ continuity. Since the scale of compression determines the parameter variation, we primarily focus on the magnitude of the error. When the variable $d$ is sufficiently small, the actual change in the loss function can be accurately described by the total differential $df$. Therefore, determining the \"sufficiently small\u201d threshold in the practical model is crucial. Since each layer can accommodate different sizes of parameter errors, we compute the gap between theory and practice, denoted as $U_{sk}(x)$.\n$U_{sk}(x_i) : |l(w \u00b1 d_i, x_i, Y_i) \u2013 (l(W, x_i, Y_i) + \\sum_{i=1}^{n} \\frac{\\partial l_k}{\\partial w_i} . \\delta_i + \\frac{1}{2} \\delta_i^T \\delta_j + O(\\|(\\delta_i)\\|^n)|$ (7)\nThe left-hand side of the equation represents the loss caused by actual noise interference, while the right-hand side represents the theoretical loss caused by noise. The parameter $k$ controls the compression level. When the neighborhood of compression error is smaller than $10^{-3}$, we consider the actual error to be close to the theoretical error. For weights, ideally, the first-order term in a well-trained model should be zero. Since higher-order terms are uncomputable, the impact of the second-order term is typically considered. Hence, the update for the optimization term is given by the following equation,\n$\\min \\Delta_L = \\min f(w) \u2013 f(w) = \\frac{1}{2} \\delta_i^T I \\delta_j$\n(8)\n$Hd_i$ is a quadratic expression, and $I$ is composed of the second-order derivatives of the whole model. We hope that the loss decreases or increases small and slowly after compression. Next, CET mainly uses algebraic geometry to analyze this quadratic expression.\n3.2. Reconstruction of the Compression Subspace\nThe expression of $Hd_i$ serves as an abstract representation of the quadratic term of a function, which can describe geometric surfaces in high-dimensional space, where $I$ acts as the coefficient matrix. Its fundamental geometric form is given by:\n$\\delta_i^T I \\delta_j = c$ (9)\nwhere $c$ is a constant that represents the isosurface of the geometric shape. This equation constrains the weight vector $w$ within an n-dimensional space, where $n$ is the dimensionality of the weight vector. As shown in Figure 1, the eigenvalues and eigenvectors of $I$ define the geometric properties of the surface: the eigenvalues indicate the degree of stretching or compression along the principal axes, while the eigenvectors determine the orientation of these axes. Furthermore, the definiteness of $I$ dictates the global shape of the surface. Specifically, a positive definite matrix corresponds to a closed surface (e.g., an ellipsoid), whereas an indefinite matrix may result in an open surface (e.g., a hyperbolic paraboloid) (Hartshorne, 2013). Hence, a key step in CET is to determine the overall geometric shape of the surface, which serves as the foundation for guiding the direction and magnitude of model compression. We use the eigendecomposition to perform a standard form transformation on Eq.9.\n$Q(y) = \\lambda_1 y_1 + \\lambda_2 y_2 + \\cdots + \\lambda_n y_n$ (10)\nwhere $y = P^T w$ is the new coordinate system after the eigenvector transformation. $P$ is an orthogonal matrix composed of $H$(eigenvectors). The quadratic expression is a weighted sum in the direction of each principal axis, and its geometric shape is completely determined by the eigenvalues and eigenvectors. When $I$ is positive definite at the convergence point $W$, the overall shape of the level surface $c$ forms a closed ellipsoid, indicating that the convergence point is locally convex. Conversely, if $I$ is indefinite at the convergence point, the level surface becomes an open hyperbolic paraboloid. Negative definiteness at the convergence point, where the level surface would exhibit a fully concave open structure corresponding to a local maximum, is not possible in this context. This ensures that such cases are excluded.\nGiven that our objective is to minimize or slow the increase in loss after compression, the compression vector should align as closely as possible with the direction of the long axis of the ellipsoid or hyperbolic paraboloid. In this direction, the curvature is smaller, the changes in the level surface are more gradual, and the compression vector has a minimal impact on the loss value. Figure 1 illustrates the transition of CET from a quadratic algebraic representation to a geometric interpretation, defining the optimization path for the compression vector through geometry. Next, after determining the direction of the quantized subspace, the next step is how to efficiently solve this space.\n3.3. Solution of the Compression Subspace\nTo construct the subspace of the long axis, we leverage the concept of complementary spaces. In the $R^n$ space defined by the curvature of the surface, the Hessian matrix's eigenvectors form a complete orthogonal basis. This allows us to decompose $R^n$ into two complementary subspaces, satisfying the following equation\n$R = V_{long} \\oplus V_{short}$ (11)\nHere, $\\oplus$ denotes the direct sum relationship, where $V_{long}$ represents the subspace of the long axis, and $V_{short}$ represents the subspace of the short axis. Our goal is to find a compression vector that resides in the long-axis subspace. To achieve this, CET reformulates the problem by solving for the zero space of the short-axis subspace. This approach effectively identifies vectors orthogonal to the short-axis subspace and hence aligns with the long-axis subspace.\n$\\lambda_1 \\psi_1(\\delta_i) = 0,$\n$\\lambda_2 \\psi_3(\\delta_i) = 0,$\n$\\vdots$\n$\\lambda_m \\psi_m(\\delta_i) = 0$\n$i \\in \\{1, 2, ..., n\\}$ (12)\nwhere $\\lambda_m$ represents the eigenvalues corresponding to the short-axis subspace, and $\\psi_m$ denotes the transformed eigenvectors associated with these eigenvalues and noise $\\delta_i$ on the weights. $n$ (the number of parameters) is much larger than $m$ (the number of eigenvalues), which means that this is an indeterminate system of equations. By constructing the zero space of the short-axis subspace, CET isolates vectors orthogonal to $V_{short}$, thereby enabling the identification of compression vectors in the long-axis subspace.\nThe solution to the indeterminate equation system is ill-posed, thus requiring further constraints on Eq.12:\n\u2022 The ideal solution to Eq.12 is that every term $\\psi$ equals zero, which corresponds to no quantization. This result is intuitive, as the absence of quantization minimizes parameter error. However, CET avoids this trivial solution by constraining the model size. Specifically, the condition Modelsize $M_{compress} < M_{origin}$ is introduced, ensuring that the parameter error is strictly non-zero and the model volume is effectively compressed.\n\u2022 When the solved parameter error $\\delta$ becomes excessively large, Eq.8 indicates that the loss change $\\Delta L$ will also increase significantly. Thus, we not only require the compression vector to exist within the long-axis subspace but also minimize its magnitude. A smaller magnitude implies reduced compression loss along the long-axis direction. CET imposes an additional minimization constraint on the parameter error, namely min($\\|\\delta_i\\|^2$).\nWith this refinement, Eq.12 is updated to the following form:\n$\\lambda_1 \\psi_1(\\delta_i) = 0,$\n$\\lambda_2 \\psi_3(\\delta_i) = 0,$\n$\\vdots$\n$\\lambda_m \\psi_m(\\delta_i) = 0$\ns.t. $M_{compress} < M_{origin}$,\n[$\\|\\delta_i\\|^2 < \\epsilon$.\n(13)\nwhere $\\epsilon$ approaches 0, $i \\in \\{1,2,..., n\\}$. By solving the above system of equations, we can directly obtain the parameter error $\\Delta w$ that minimizes $\\Delta L$, thereby determining the level of model compression.\n3.4. CET Algorithm for Quantization\nWe attempt to apply the CET (Compression Error Theoretical) framework to model quantization. CET regards quantization as a process of introducing noise perturbation into the model parameters, where the error grows as the bit-width decreases. The detailed procedure is illustrated in Algorithm 1. CET utilizes the Lanczos algorithm to compute the eigenvalues and eigenvectors of the Hessian matrix. Subsequently, it formulates the indeterminate equation system based on Eq.13 and solves for the perturbation vector $\\delta$ using gradient descent. Once $\\delta$ is obtained, two approaches are proposed to calculate the bit-width: The first method directly computes the bit-width for each layer using the quantization formula provided in the algorithm. The second method compares the actual quantization loss under different bit-widths and selects the $\\delta$ that aligns with the true quantization error. CET combines both approaches to determine the optimal bit-width. When the first method fails to yield accurate results due to the impact of outliers, the second method is adopted to ensure reliability. Regarding time complexity, the primary computational bottleneck lies in the Lanczos algorithm, with a complexity of O($n^2$). The remaining steps of CET are computationally efficient, with a complexity of O(n).\nCET is highly extensible and can be applied to other model compression techniques. Most compression methods, such as quantization, decomposition, and parameter sharing, can be viewed as processes that introduce noise perturbations into the model parameters. CET provides a theoretical framework for analyzing these compression-induced errors and determines the optimal compression level in practice. As a general-purpose method, CET is not tailored to any specific compression technique but can be generalized across a wide range of compression methods."}, {"title": "4. Experiments", "content": "4.1. Datasets and Details\nWe evaluate the performance of CET on various models on ImageNet. The ImageNet-1K dataset(Krizhevsky et al., 2017) consists of 1.28 million training and 50K validation images. ImageNet-1K is usually used as the benchmark for model compression. The calibration set is taken from the ImageNet validation set. SWAG dataset (Zellers et al., 2018) consists of 113K multiple-choice questions about grounded situations. The Stanford Question Answering Dataset (SQUAD) (Rajpurkar et al., 2016) is a collection of question-answer pairs derived from Wikipedia articles. In SQUAD, the correct answers to questions can be any sequence of tokens in the given text. MNLI (Williams et al., 2017) is a dataset for natural language reasoning tasks. Its corpus is a collection of textual implication annotations of sentences through crowdsourcing. The task is to predict whether the premise sentence and the hypothesis sentence are logically compatible (entailment, contradiction, neutral).\nFollowing existing compression work, CET utilizes the Lanczos algorithm to compute the eigenvalues and eigenvectors of the Hessian matrix, with the maximum number of iterations set to 100. The Lanczos algorithm avoids explicitly constructing the Hessian matrix, efficiently approximating its eigenvalues and significantly reducing computational complexity. In the higher-order expansion (Eq.7), when the actual perturbation error and the theoretically derived perturbation error are both less than $10^{-3}$, the discrepancy between theory and practice becomes negligible. For Eq.13, it is formulated as an optimization problem where quantization error is minimized using the Adam optimizer. To ensure experimental consistency, all models use the same settings without tuning hyperparameters. The quantization bit-widths for each layer are calculated using the quantization algorithm (Zhang et al., 2024), without fine-tuning or retraining. All experiments are conducted on two NVIDIA A800 GPUs, and the code is implemented in PyTorch, which will be made publicly available.\n4.2. Ablation\nDifferential Expansion. Differential expansion requires ensuring that the neighborhood around the expansion point is sufficiently small to guarantee that the theoretical approximation effectively represents the actual values within this neighborhood. Figure 2 shows the discrepancies between theoretical and actual values under different perturbation errors introduced into each layer. Taking ResNet-18 as an example, the gap between theoretical and actual values is minimal across layers, which validates the effectiveness of the CET. Additionally, error perturbation experiments reveal that different layers exhibit distinct levels of error tolerance.\nGradient and Short Axis Eigenvalue. Theoretically, the gradient of a well-trained model approach zero. Practical inspection reveals the first-order gradient values are small, with most being less than $10^{-5}$. As a result, the influence of the first-order term on the loss can be safely neglected.\n4.3. Comparison\nAlgorithm Accuracy. As shown in Table 1, we conducted a comprehensive comparison between CET and existing quantization methods (Zhang et al., 2018; Lin et al., 2017; Zhou et al., 2016; Choi et al., 2018; Han et al., 2015; Wang et al., 2019; Dong et al., 2019a). For ResNet-18 (He et al., 2016), CET achieved nearly lossless weight quantization, with model performance degrading by only 0.01%, while significantly reducing the model size, achieving a compression ratio of over 11x. This demonstrates the effectiveness of CET's geometric analysis based on second-order information, enabling precise weight compression along the long-axis direction and selecting appropriate error tolerance for each layer. Furthermore, to investigate the impact of activation compression on weight quantization, we performed quantization on activations with different bit widths (8-bit and 4-bit). The results show that CET can maximize model compression while maintaining accuracy. Notably, CET does not rely on fine-tuning but directly determines the optimal bit-width allocation, which distinguishes it from methods like MCKP that require fine-tuning.\nFor ResNet-50, CET demonstrates significant performance advantages under higher compression rates. Compared with HAWQ and HAWQ-V2, which also utilize second-order information, CET achieves smaller accuracy degradation under more extreme compression scenarios (-0.99% vs. -1.91%). Additionally, compared with HAQ, which searches for optimal bit-width allocation via reinforcement learning, CET achieves a higher weight compression rate with minimal accuracy degradation and significantly lower computational cost. For example, after achieving a 12.74\u00d7 weight compression, CET reduces model accuracy by only 0.034%, effectively realizing lossless compression. Compared with MCKP, CET achieves minimal performance degradation even at a higher compression rate (13.53\u00d7), while MCKP experiences similar performance degradation at a lower compression rate (12.24\u00d7). This highlights CET's stronger robustness and generalization capabilities under higher compression demands, achieving superior performance retention at extreme compression ratios.\nFor ResNet-34, CET even improves model accuracy while achieving nearly 11\u00d7 compression. This is due to ResNet-34's second-order geometric property, where the Hessian matrix at the convergence point is indefinite. CET performs quantization along the eigenvector direction corresponding to negative eigenvalues, reducing the loss and resulting in higher accuracy for the quantized model compared to the original. CET rigorously analyzes the Hessian matrix's shape using algebraic geometry, enabling it to select the optimal quantization direction. In Table 2, finally, we further evaluate CET on the lightweight and efficient MobileNet-V2 architecture. The results show that CET achieves significant performance improvements even at higher compression rates. This further validates the rationality and broad applicability of CET's second-order geometric analysis.\nTable 3 presents CET's performance across various NLP datasets. With 4-bit mixed-precision weight quantization, CET achieves accuracy comparable to the original model. While Zhang et al.'s method shows a slight advantage on the MNLI dataset, its low compression rate makes it difficult to maintain stable accuracy under high compression.\nAlgorithm Efficiency. The CET algorithm demonstrates high computational efficiency when solving indeterminate equations using gradient descent. Experiments show that it converges in approximately 2000 iterations, taking only a few minutes. Moreover, the main computational complexity and time consumption of the CET algorithm stem from the Lanzcos algorithm. This is because it requires calculating the Hessian information for each sample in the calibration set. This time consumption characteristic aligns with the computational speed of existing algorithms.\n4.4. Discussion\nLimitations. Ideally, the CET framework requires precise Hessian information to ensure accurate analysis of quantization errors. However, since directly computing the full Hessian matrix is infeasible in practice, we approximate it using the Lanzcos algorithm. While this approximation introduces some degree of error, our perturbation analysis of the Lanzcos algorithm reveals that it has a strong error tolerance. The approximate Hessian information obtained is robust in most cases, ensuring the practical applicability of the CET framework.\nGenerality. The CET framework is algorithm-agnostic and features strong interpretability and adaptability. It does not rely on any specific compression strategy but rather provides a universal theoretical foundation. Consequently, CET can be seamlessly applied to various compression strategies. Offering an effective geometric analysis framework, it helps these methods identify optimal compression configurations."}, {"title": "5. Conclusion", "content": "We proposed a Compression Error Theory (CET) framework designed to determine the optimal compression level for each layer of a model. During quantization, CET reconstructs the quadratic form of quantization error into different geometric structures and reformulates the optimization process as a complementary problem to solve the error subspace. Theoretical analysis shows that constructing the quantization subspace along the long axis minimizes the impact on model performance. Extensive experimental results further validate the effectiveness of CET. In the future, CET is a general compression theory framework that can be extended to other compression methods, such as weight decomposition."}]}