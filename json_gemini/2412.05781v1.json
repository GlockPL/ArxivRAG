{"title": "Open-Source Acceleration of Stable-Diffusion.cpp", "authors": ["Jingxu Ng", "Cheng Lv", "Pu Zhao", "Wei Niu", "Juyi Lin", "Yanzhi Wang"], "abstract": "Stable diffusion plays a crucial role in generating high-quality images. However,\nimage generation is time-consuming and memory-intensive. To address this, stable-\ndiffusion.cpp (Sdcpp) emerges as an efficient inference framework to accelerate\nthe diffusion models. Although it is lightweight, the current implementation of\nggml_conv_2d operator in Sdcpp is suboptimal, exhibiting both high inference\nlatency and massive memory usage. To address this, in this work, we present\nan optimized version of Sdcpp leveraging the Winograd algorithm to accelerate\n2D convolution operations, which is the primary bottleneck in the pipeline. By\nanalyzing both dependent and independent computation graphs, we exploit the\ndevice's locality and parallelism to achieve substantial performance improvements.\nOur framework delivers correct end-to-end results across various stable diffusion\nmodels, including SDv1.4, v1.5, v2.1, SDXL, and SDXL-Turbo. Our evaluation\nresults demonstrate a speedup up to 2.76\u00d7 for individual convolutional layers\nand an inference speedup up to 4.79\u00d7 for the overall image generation process,\ncompared with the original Sdcpp.\nHomepage: https://github.com/SealAILab/stable-diffusion-cpp", "sections": [{"title": "Introduction", "content": "Stable diffusion [1] has established itself as a powerful tool for generating high-quality images. How-\never, the computational demands of image generation pose significant challenges, particularly in terms\nof inference latency and memory consumption. To address these limitations, stable-diffusion.cpp\n(Sdcpp) [2] emerges as an efficient inference framework to accelerate the diffusion models. Sdcpp\nis a C/C++ implementation of the stable diffusion model, designed for efficient inference on CPUs\n(and potentially GPUs with the appropriate configuration) without external dependencies. The imple-\nmentation is based on GGML [3], which works in the same way as llama.cpp [4]. It is lightweight\nwithout external dependencies. However, the current implementation of the computation-intensive 2D\nconvolution operator in Sdcpp remains inefficient, incurring massive inference latency. To mitigate\nthis problem, we apply the Winograd algorithm [5] to optimize convolution operations in Sdcpp\nfor faster inference and reduced memory cost. Specifically, we analyze both dependent and inde-\npendent computation graphs, leveraging the device's locality and parallelism to achieve substantial\nperformance gains in convolution operations. Our evaluations across various models and image\nsizes demonstrate our significant improvements in terms of the inference speed. Notably, we can\nachieve an inference acceleration up to 2.76\u00d7 for individual convolution layers and a speedup up to\n4.79x for the whole image generation process, compared with the original Sdcpp. The homepage is\nhttps://github.com/SealAILab/stable-diffusion-cpp."}, {"title": "Techniques", "content": "As the current implementation of 2D convolution operator in Sdcpp is relatively slow with high\nmemory usage, we apply the Winograd algorithm to optimize convolution operations in Sdcpp,\nspeeding up the generation process and reducing the computation and memory costs. With Wino-\ngrad, the convolution operation is split into multiple steps: (i) preprocess of filter and activation\nweights, (ii) element-wise multiplication between the preprocessed tensor, and (iii) postprocess of\nthe intermediate results. In this work, we analyze both the dependent and independent computation\ngraphs, leveraging the device's locality and parallelism to achieve substantial performance gains in\nconvolution operations.\nTo enhance locality, we apply scatter-store and gather-load optimizations, ensuring that the data\nplacement fits within the L1 cache during the loading process, thereby minimizing cache swapping.\nFor parallelism, we exploit the independent operations within the Winograd algorithm, distributing in-\ndependent computations across multiple threads and cores to reduce image generation latency. In fact,\nGGML [3] employs a shared-state approach, which assigns cores with the same workload. Instead,\nin our optimization, upon finishing their current computations, the computation cores dynamically\nreceive the next computational block. This action can benefit the calculation on computational units\nwith P-core (performance core) and E-core (efficiency core), such as the M-series Macs. By dynami-\ncally assigning workloads based on core performance, the approach ensures efficient utilization and\nbalanced computation across various computation cores."}, {"title": "Evaluation", "content": "Operator Support. Previously, a number of operators in the original Sdcpp are not supported in\nAndroid, leading to incorrect image generation results for certain SD models such as SDXL. Currently,\nin our optimization, we have supported all operators in multiple SD models for various devices such\nas Mac, Android, and AMD devices. We can also support the operators for the diffusion transformer\nmodels, which are widely used in video generation models such as Open-Sora [6]. Besides, the\nquantization of the operators is also supported in our implementation. In future work, we will further\noptimize the speed of these operators in the common operator set.\nModel Support. We have implemented an end-to-end acceleration pipeline of stable diffusion\nbased on our operator library. Our framework can generate correct end-to-end results for all SDv1.4,\nv1.5, v2.1, SDXL, and SDXL-Turbo, tested on Mac (GPU, Metal [7]) and Android (Qualcomm,\nOpenCL [8]) devices. It can also support other variants such as Realistic Vision and user-specified\narbitrary LoRA [9] modules as well."}, {"title": "Single Convolution Layer Speedup Performance", "content": "We demonstrate the performance of our convolution operator implementation using the Winograd\nalgorithm for multiple convolutional layers from the Sdcpp sampling process as shown in Table 1.\nSpecifically, we focus on the layers which are adopted more frequently in the sampling process. As"}, {"title": "Overall Speedup Performance for Image Generation", "content": "We demonstrate the inference speedup performance for image generation on M1 pro in Table 2.\nAs observed, our method achieves faster inference speed compared with Sdcpp under different\nconfigurations of various image sizes and models. Specifically, when the image size becomes larger\nsuch as 1024 \u00d7 1024, our improvements over Sdcpp is more significant (such as the 4.79\u00d7 speedup\nfor FP32), demonstrating our superior acceleration performance."}, {"title": "Visualization", "content": "We demonstrate some generation examples from the original Sdcpp and ours in Figure 1, with the\nSDXL-Turbo model and 5 steps for both. We can observe that with our improvements, the generated\nimages are more realistic than those of the original Sdcpp, under the same prompt."}, {"title": "Conclusion", "content": "The current implementation of ggml_conv_2d in Sdcpp remains slow and memory-intensive. We op-\ntimize Sdcpp using the Winograd algorithm to overcome these limitations. Our enhanced framework\nsupports end-to-end image generation for all tested models, including SDv1.4, v1.5, v2.1, SDXL,\nand SDXL-Turbo, consistently delivering correct results. Comprehensive testing across these models\nhighlights the substantial speedup achieved by our optimizations."}]}