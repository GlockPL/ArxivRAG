{"title": "RadioRAG: Factual Large Language Models for Enhanced Diagnostics in Radiology Using Dynamic Retrieval Augmented Generation", "authors": ["Soroosh Tayebi Arasteh", "Mahshad Lotfinia", "Keno Bressem", "Robert Siepmann", "Dyke Ferber", "Christiane Kuhl", "Jakob Nikolas Kather", "Sven Nebelung", "Daniel Truhn"], "abstract": "Large language models (LLMs) have advanced the field of artificial intelligence (AI) in medicine. However LLMs often generate outdated or inaccurate information based on static training datasets. Retrieval augmented generation (RAG) mitigates this by integrating outside data sources. While previous RAG systems used pre-assembled, fixed databases with limited flexibility, we have developed Radiology RAG (RadioRAG) as an end-to-end framework that retrieves data from authoritative radiologic online sources in real-time. RadioRAG is evaluated using a dedicated radiologic question-and-answer dataset (RadioQA). We evaluate the diagnostic accuracy of various LLMs when answering radiology-specific questions with and without access to additional online information via RAG. Using 80 questions from RSNA Case Collection across radiologic subspecialties and 24 additional expert-curated questions, for which the correct gold-standard answers were available, LLMs (GPT-3.5-turbo, GPT-4, Mistral-7B, Mixtral-8x7B, and Llama3 [8B and 70B]) were prompted with and without RadioRAG. RadioRAG retrieved context-specific information from www.radiopaedia.org in real-time and incorporated them into its reply. RadioRAG consistently improved diagnostic accuracy across all LLMs, with relative improvements ranging from 2% to 54%. It matched or exceeded question answering without RAG across radiologic subspecialties, particularly in breast imaging and emergency radiology. However, degree of improvement varied among models; GPT-3.5-turbo and Mixtral-8x7B-instruct-v0.1 saw notable gains, while Mistral-7B-instruct-v0.2 showed no improvement, highlighting variability in its effectiveness. LLMs benefit when provided access to domain-specific data beyond their training data. For radiology, RadioRAG establishes a robust framework that substantially improves diagnostic accuracy and factuality in radiological question answering.", "sections": [{"title": "1. Introduction", "content": "Artificial intelligence (AI) is in the process of changing diagnostic radiology by enhancing image analysis, improving diagnostic accuracy, and streamlining workflow processes\u00b9. Recent advances in large language models (LLMs) 2\u20134 have demonstrated potential in extracting structured information from radiological reports, enhancing data mining capabilities, improving diagnostic accuracy, and enabling more reliable speech recognition. However, the use of LLMs in radiology comes with challenges, most prominently the risk of generating inaccurate information and perpetuating biases9\u201311. Strategies like human feedback12 and prompt engineering have been employed to refine outputs but ultimately cannot solve the problem1,13,14. This is due to the fact that LLMs have to rely on their internal knowledge which is incomplete and may be biased. Rather, it was proposed that LLMs should be used as reasoning engines15 with access to external sources that they can access. This approach is called retrieval augmented generation (RAG)16 and may remedy two problems: firstly, the risk of hallucinating information is reduced, since source material can be used and cited17. Secondly, LLMs can access up-to-date information through RAG, while conventional LLM querying has to rely on the information fed to the model during training.\nRecent studies have demonstrated the effectiveness of Retrieval-Augmented Generation (RAG) in answering general clinical questions18,19. However, its application in radiology has not been explored. In this study, we introduce Radiology RAG (RadioRAG) as a novel framework tailored specifically for typical inquiries in diagnostic radiology.\nRadioRAG employs LLMs as reasoning engines to process user questions. It determines which external sources to query for relevant information, collects the source data, and then compiles a comprehensive answer for the user. Most existing solutions that employ RAG use static, pre-compiled literature databases18,19. In contrast, RadioRAG accesses up-to-date information from radiopaedia to collect its source data. This architecture enables real-time gathering of contextually relevant information and dynamically constructing the database. To our knowledge, RadioRAG is the first implementation of this paradigm in radiology. The hypothesis that we investigated were: 1) the real-time context retrieval system reduces the occurrence of hallucinations and 2) RadioRAG improves the accuracy of LLM responses to detailed questions."}, {"title": "2. Materials and Methods", "content": "This retrospective study was conducted in compliance with the Declaration of Helsinki and the relevant guidelines and regulations. The study protocol was approved by the Institutional Review Board (IRB) of the Medical Faculty of RWTH Aachen University (No. EK 028/19)."}, {"title": "2.1. RSNA Cases", "content": "The existing datasets for medical (QA) answering, such as MultiMedQA10, MedMCQA20, and PubMedQA21, focus on general medicine and do not cater to the specific needs of diagnostic radiology. To address this gap, we created a tailored dataset, RSNA-RadioQA, using 80 peer-reviewed cases from the Radiological Society of North America (RSNA) Case Collection (https://cases.rsna.org/). Our dataset covers 19 radiology subspecialties, with at least 5 cases per subspecialty, prioritizing the most recently published cases. Questions were created by providing the clinical history from the RSNA's case description along with the image characteristics as described in the figure caption. Since we concentrated on LLM without image processing capabilities, the image itself was not provided. Care was taken to exclude any hints at differential diagnoses."}, {"title": "2.2. Expert-Curated Cases", "content": "To assess the generalizability of RadioRAG and mitigate the risk of data contamination in our test data, we developed a new benchmark that we call RadioQA. Data contamination is a significant"}, {"title": "2.3. System Design", "content": "Figure 2 gives an overview over the design of RadioRAG in an end-to-end framework. The following sections detail each component of the process."}, {"title": "2.3.1. Key-phrase Extractor", "content": "To investigate the reasoning capabilities of the LLMs in our study, we isolated automated query generation and matching to external sources by using the same model for all LLMs: using GPT-3.5-turbo via its API, the system extracts up to five search key-phrases from a given radiological question. The prompt specified to GPT-3.5-turbo is: \"You are a helpful expert medical research assistant. I have a medical question, particularly in the field of radiology. Please summarize the question to extract the most representative keywords for use in online scientific article searches. Return a maximum of five keywords that are scientifically relevant to radiology.\" Two examples were provided to the model within the prompt (two-shot approach):\n\u2022\tFirst Example - Question: \"I am looking at a prostate MRI and see a lesion in the right posterolateral peripheral zone. The lesion is hypointense in T2 imaging, hyperintense in DWI, hypointense in ADC and has a strong and early signal enhancement after contrast administration. What kind of lesion could this be?\"\n\u2022\tFirst Example - Completion: \"hypointense T2; hyperintense DWI; hypointense ADC; prostate lesion; right posterolateral peripheral.\"\n\u2022\tSecond Example - Question: \"A 9 month-old patient presents with sudden lower abdominal pain, nausea, and vomiting. The patient has a recent history of a respiratory infection. Ultrasound shows a target-sign in the right lower quadrant. What is the most likely diagnosis?\"\n\u2022\tSecond Example - Completion: \"lower abdominal pain; nausea and vomiting; respiratory infection; ultrasound target sign; right lower quadrant.\""}, {"title": "2.3.2. Online Context and the Database", "content": "After acquiring relevant search keywords, the model searches through articles on www.radiopaedia.org, selecting the five articles most pertinent to each keyword. These articles are then segmented into chunks of 1,000 tokens, each with a 200 token overlap. Each chunk is converted into a vector using the 'text-embedding-ada-002' embedding function from OpenAl and temporarily stored in a vector database managed by Chroma (https://www.trychroma.com/)."}, {"title": "2.3.3. Retriever", "content": "With the database prepared, the original query is also embedded into a vector using the same embedding function. This query vector is then compared against all vectors in the database using cosine similarity to retrieve the top three most similar vectors (k=3). These vectors are matched to their textual form, and the relevant text is prepared for the next stage. The LangChain framework is used for this retrieval process."}, {"title": "2.3.4. Large Language Model (LLM)", "content": "The final stage involves the respective LLM under investigation, which receives the original query along with the contextually relevant text fragments retrieved in the previous step. The LLM is instructed to provide a concise answer in one sentence, based solely on the provided context. If the answer is unknown, the LLM must explicitly state this. We used the following prompt: \"Use the following pieces of retrieved context to answer the question. If you don't know the answer, say 'I don't know.' Answer concisely in one sentence.\" This process contrasts with traditional LLM QA methods that involve responding to queries without additional context, typically prompted with: \"You are a helpful expert medical research assistant. Answer the following question concisely in one sentence\". To ensure reproducibility, a temperature value of 0 was set for all LLM responses, except for those involving Mistral and Mixtral models, where a minimum temperature of 0.1 was necessary (for which we set it at 0.1). A top-p value of 1 was consistently used across all cases. Through this choice, provided results were reproducible for the same model version."}, {"title": "2.4. Evaluation", "content": "To assess the efficacy of RadioRAG across varying scales of language models, we tested both smaller and larger LLMs. We included GPT-3.5-turbo, GPT-4, Mistral-7B-instruct-v0.2, Mixtral-8x7B-instruct-v0.1, Llama3-8B, and Llama3-70B-instruct. This set of models represents the state-of-the-art in size and capabilities. Each model was integrated into the RadioRAG pipeline and evaluated in both a conventional QA setup and within the RadioRAG framework.\nThe performance of all models was evaluated by comparing their responses within the RadioRAG framework to those in conventional QA, using ground-truth answers as the benchmark. Although various metrics like naturalness, fluency, and coherence are commonly used in LLM evaluation24\u201326, we prioritized accuracy27,28 as the primary metric due to the specific nature of our application, which demands correct and concise diagnostic answers. Accuracy was measured by scoring responses as true (1) if they correctly addressed the query and false (0) otherwise. Factuality was also assessed by verifying the suitability of the sources the LLMs cited for each answer. Additionally, the models' ability to recognize and admit when the available information was insufficient was evaluated, requiring them to state \"I don't know\" in such instances. Any inaccuracies or omissions in expressing uncertainty were considered deviations from expected factuality and transparency."}, {"title": "2.5. Statistical Analysis", "content": "Statistical analysis was conducted using Python (v3.11) with SciPy (v1.11) and NumPy (v1.24) packages. To evaluate the variability, bootstrapping was employed with 10,000 redraws for the metrics to determine the mean, standard deviation, 95% confidence intervals (CI), and to calculate p-values for differences in accuracy between the RadioRAG and non-RadioRAG setups29. \u03a4\u03bf account for multiple comparisons, the p-values were adjusted for multiplicity using the false discovery rate. A p-value < 0.05 was considered statistically significant."}, {"title": "2.6. Code and Data Availability", "content": "All source codes and datasets used in this study are publicly available to ensure transparency and reproducibility. The code has been developed using Python v3.11 with PyTorch v2.1 and is hosted on GitHub at https://github.com/tayebiarasteh/RadioRAG. We utilized the LangChain v0.1.0 framework for the RadioRAG pipeline, with Chroma serving as the vector database. The OpenAl API v1.12 provided access to the GPT-4 and GPT-3.5-turbo models as well as the 'text-embedding-ada-002' embedding function from OpenAl. Additionally, the Replicate API v0.25 facilitated cloud execution of Mistral and Mixtral models without local GPU requirements, and Ollama (https://ollama.com) framework for execution of the newly-released Llama3 open-source models. The underlying datasets, i.e., the RSNA-RadioQA and RadioQA datasets, are publicly accessible and included in the supplemental materials of this publication. All final LLM responses for this study were generated between April 1 and April 25, 2024."}, {"title": "3. Results", "content": ""}, {"title": "3.1. Dataset Characteristics", "content": "Median and mean age over all patients in the RSNA-RadioQA dataset was 44 \u00b1 [StD] 21 years, with a range of 2 days to 80 years."}, {"title": "3.2. RadioRAG Enhances Diagnostic Performance of LLMs", "content": "Typical responses of the LLMs to an exemplary question from the RSNA-RadioQA dataset are given in RadioRAG consistently increased the accuracy of the LLMs' responses on the RadioQA dataset as illustrated by Detailed results are given in the accuracy of GPT-3.5-turbo increased from 0.66 \u00b1 0.05 to 0.74 \u00b1 0.05 (P=0.031), of GPT-4 from 0.78 \u00b1 0.05 to 0.79 \u00b1 0.05 (P=0.283), of Mixtral-8x7B-instruct-v0.1 from 0.65 \u00b1 0.05 to 0.76 \u00b1 0.05 (P=0.018), of Llama3-8B from 0.58 \u00b1 0.06 to 0.59 \u00b1 0.05 (P=0.386), and of Llama3-70B from 0.66 \u00b1 0.05 to 0.69 \u00b1 0.05 (P=0.298). The only exception was Mistral-7B-instruct-v0.2, which exhibited no change (0.55 \u00b1 0.06 in both cases; P=0.455). Similarly, for the RadioQA benchmark, all LLMs demonstrated improvements. In particular, Mixtral-8x7B-instruct-v0.1, Llama3-8B, and Mistral-7B all exhibited significant improvements, while the bigger models also showed improvements, yet without reaching the significance threshold."}, {"title": "3.3. Open-Weights Models Benefit from RadioRAG", "content": "While RadioRAG consistently improved diagnostic performance across all tested LLMs, the degree of improvement varied significantly between models. The most complex model, GPT-4 exhibited accuracy improvements of 2% and 6%, respectively for the two QA datasets, while GPT-3.5 exhibited accuracy improvements of 11% and 21% respectively. We observed stronger increases for the open-source models Mixtral-8x7B-instruct-v0.1 (up to 47%) and Llama3-8B (up to 33%). Importantly, while open-source models had inferior performance in the non-RAG setting, RadioRAG rendered these models competitive with GPT-4."}, {"title": "3.4. RadioRAG Enforces Factuality in LLMs", "content": "We found that RadioRAG guided the LLMs to ground their answers in factual content from the source data. Generally, answers aligned closely with the source data. However, we also observed that this strict adherence to source materials can occasionally lead to inaccuracies if the retrieved articles are not entirely relevant to the query."}, {"title": "4. Discussion", "content": "In this study, we introduced Radiology RAG (RadioRAG), a novel framework that enhances the diagnostic accuracy of LLMs by utilizing contextually relevant data from established radiological sources. To benchmark RadioRAG we developed two datasets which we make publicly available: RSNA-RadioQA for internal testing and RadioQA as an external benchmark.\nOverall, our findings show that RadioRAG outperformed conventional methods, yielding more accurate and reliable outputs in radiological contexts.\nRadioRAG has limitations. First, the on-the-fly generation of a database can be time-consuming, potentially extending the time it takes for RadioRAG to respond compared to conventional QA setups. To mitigate this, we have optimized the framework to select up to 5 key-phrases and retrieve 5 articles per key-phrase, resulting in a total of 25 articles. Second, the reliance on continually querying online scientific sources, in our case www.radiopaedia.org, could overload the website, especially if multiple users are accessing it simultaneously, potentially leading to downtimes. Therefore, while RadioRAG offers a compelling proof-of-concept, more research into its efficiency and computational demands is essential before it can be implemented in clinical practice. Future deployments should consider establishing agreements with source websites to ensure fair use and manage the load effectively.\nIn conclusion, RadioRAG introduces real-time data retrieval to enhance the accuracy and factuality of LLMs in radiological diagnostics. This development sets a new standard for Al-driven diagnostics and offers a foundation for further work that could improve diagnostic processes and patient care in healthcare."}]}