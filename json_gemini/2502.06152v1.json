{"title": "The Value of Information in Human-AI Decision-making", "authors": ["Ziyang Guo", "Yifan Wu", "Jason Hartline", "Jessica Hullman"], "abstract": "Humans and AIs are often paired on decision tasks with the expectation of achieving com-plementary performance, where the combination of human and AI outperforms either one alone. However, how to improve performance of a human-AI team is often not clear without knowing more about what particular information and strategies each agent employs. We provide a decision-theoretic framework for characterizing the value of information and consequently, opportunities for agents to better exploit available information-in AI-assisted decision workflow. We demonstrate the use of the framework for model selection, empirical evaluation of human-AI performance, and explanation design. We propose a novel information-based instance-level explanation technique that adapts a conventional saliency-based explanation to explain information value in decision making.", "sections": [{"title": "Introduction", "content": "As the performance of artificial intelligence (AI) models improves, workflows in which human and AI model-based judgments are combined to make decisions are sought in medicine, finance, and other domains. Though statistical models often make more accurate predictions than human experts on average [\u00c6gisd\u00f3ttir et al., 2006, Grove et al., 2000, Meehl, 1954], whenever humans have access to additional information over the AI, there is potential to achieve complementary performance by pairing the two, i.e., better performance than either the human or AI alone. For example, a physician may have access to additional information that may not be captured in tabular electronic health records or other structured data [Alur et al., 2024b].\nHowever, evidence of complementary performance between humans and AI is limited, with many studies showing that human-AI teams underperform an AI alone [Bu\u00e7inca et al., 2020, Bussone et al., 2015, Green and Chen, 2019, Jacobs et al., 2021, Lai and Tan, 2019, Vaccaro and Waldo, 2019, Kononenko, 2001]. A solid understanding of such results is limited by the fact that most analyses of human-AI decision-making focus on ranking the performance of human-AI teams or each individually using measures like posthoc decision accuracy. This approach is problematic for several reasons. First, it does not account for the best achievable performance based on the information available at the time of the decision [Kleinberg et al., 2015, Guo et al., 2024, Rambachan, 2024]. Second, it cannot provide insight into the potential for available information to improve the decisions, making it difficult to design interventions that improve the team's performance.\nIn contrast, identifying information complementarities that contribute to the maximum achievable decision performance of a human and AI model such as when one of the agents has access to infor-mation not contained in the other's judgments, or has not fully integrated information available in the environment into their judgments provides more actionable information for intervening to improve the decision pipeline. For example, if human experts are found to possess decision-relevant information over the AI, we might collect further data to improve the AI model. If the model predictions con-tain decision-relevant information not contained in human decisions, we might design more targeted explanations to help humans integrate under-exploited information.\nWe contribute a decision-theoretic framework for characterizing the value of information available within an AI-assisted decision workflow. In our framework, information is considered valuable to"}, {"title": "Related work", "content": "Human-AI complementarity Many empirical studies of human-AI collaboration focus on AI-assisted human decision-making for legal, ethical or safety reasons [Bo et al., 2021, Boskemper et al., 2022, Bondi et al., 2022, Schemmer et al., 2022]. However, a recent meta-analysis by Vaccaro et al. [2024] finds that on average, human-AI teams perform worse than the better of either humans or AI alone. In response, a growing body of work seeks to evaluate and enhance complementarity in human-AI systems [Bansal et al., 2021b, 2019, 2021a, Wilder et al., 2021, Rastogi et al., 2023, Mozannar et al., 2024b]. The present work differs from much of this prior work by approaching human-AI complementarity from the perspective of information value and use, including whether the human and AI decisions provide additional information that is not used by the other.\nEvaluation of human decision-making with machine learning Our work contributes to the development of methods for evaluating decisions of human-AI teams [Kleinberg et al., 2015, 2018, Lakkaraju et al., 2017, Mullainathan and Obermeyer, 2022, Rambachan, 2024, Guo et al., 2024, Ben-Michael et al., 2024, Shreekumar, 2025]. Kleinberg et al. [2015] first proposed that evaluations of human-AI collaboration should be based on what information is available at the time of decisions. Our work contributes to definition of Bayesian best-attainable-performance benchmarks [Hofman et al., 2021, Wu et al., 2023, Agrawal et al., 2020, Fudenberg et al., 2022]. Closest to our work, Guo et al. [2024] use a rational Bayesian agent faced with deciding between the human and AI recommendations as the theoretical upper bound on expected performance of any human-AI team. This benchmark provides a basis for identifying informational \u201copportunities\u201d within a decision problem.\nHuman information in machine learning One popular approach for developing machine learning models is to incorporate human information or expertise in model predictions [Alur et al., 2024a,b, Corvelo Benz and Rodriguez, 2023, Mozannar et al., 2024a, Bastani et al., 2021, Madras et al., 2018, Raghu et al., 2019, Keswani et al., 2022, 2021, Okati et al., 2021]. Corvelo Benz and Rodriguez [2023] propose multicalibration over human and AI model confidence information to guarantee the existence of an optimal monotonic decision rule. Alur et al. [2023] propose a hypothesis testing framework to evaluate the added value of human expertise over AI forecasts. Our work shares the motivation of incorporating human expertise but targets a broader scope by quantifying the information value for all available signals and agent decisions implied in a human-AI decision pipeline, rather than focusing solely on improving model performance."}, {"title": "Methodology", "content": "Our framework takes input as a decision problem associated with an information model and outputs the value of information in any available signals to any agent, conditioning on the existing information in their decisions within a Bayesian decision theoretic framework. Our framework provides two separate functions to quantify the value of information globally across the data-generating process and locally in a realization drawn from the data-generating process. We also introduce a robust analysis approach to information order, which enables to compare the agent-complementary information in signals for all possible decision problems.\nDecision Problem A decision problem consists of three key elements. We illustrate with an example of a we weather decision.\n\u2022 A payoff-relevant state $w$ from a space $\\Omega$. For example, $\\omega \\in \\Omega = \\{0, 1\\} = \\{\\text{no rain, rain}\\}$.\n\u2022 A decision $d$ from the decision space $D$ characterizing the decision-maker (DM)'s choice. For example, $d \\in D = \\{0, 1\\} = \\{\\text{not take umbrella, take umbrella}\\}$.\n\u2022 A payoff function $S : D \\times \\Omega \\rightarrow \\mathbb{R}$, used to assess the quality of a decision given a realization of the state. For example, $S(d = 0, w = 0) = 0, S(d = 0, \\omega = 1) = -100, S(d = 1, \\omega = 0) = -50, S(d = 1, \\omega = 1) = 0$, which punishes the DM for selecting an action that does not match the weather.\nInformation Model We cast the information available to a DM as a signal defined within an information model. We use the definition of an information model in Blackwell et al. [1951]. The information model can be represented by a data-generating model with a set of signals.\n\u2022 Signals. There are $n$ \"basic signals\" represented as random variables $\\Sigma_1, ..., \\Sigma_n$, from the signal spaces $\\Sigma_1, ..., \\Sigma_n$. These represent information obtained by a decision-maker, e.g., $\\Sigma_1 = \\{\\text{cloudy, not cloudy}\\}$, $\\Sigma_2 \\in \\{0, ..., 100\\}$ for temprature Celsius, etc. The decision-maker observes a signal, which is a combination of the basic signals, represented as a set $V \\subset 2^{\\{\\Sigma_1, ..., \\Sigma_n\\}}$. For example, a signal representing a combination of two basic signals $V = \\{\\Sigma_1, \\Sigma_2\\}$ observed by the decision-maker might consist of cloudiness $\\Sigma_1$ and the temperature $\\Sigma_2$ of the day. Given a signal composed of $m$ basic signals, we write the realization of $V$ as $v = (\\sigma_{j_1}, ..., \\sigma_{j_m})$, where the realizations $\\sigma_{j_i} \\in \\Sigma_{j_i}$, are sorted by the index of the basic signals $j_i \\in [n]$. The union $V$ of two signals $V_1, V_2$ takes the set union, i.e., $V = V_1 \\cup V_2$. Though $V$ is initially defined as a set of random variables, we will slightly abuse notation $V$ to represent a random variable that is drawn from the joint distribution of the basic signals in it.\n\u2022 Data-generating process. A data-generating process is a joint distribution $\\pi \\in \\Delta(\\Sigma_1 \\times ... \\times \\Sigma_n \\times \\Omega)$ over the basic signals and the payoff-relevant state. However, the DM may only observe a subset $V$ of the $n$ basic signals. We can define the Bayesian posterior belief upon receiving a signal $V = v$ from the data-generating model as\n$\\Pr[w|v] = \\frac{\\pi(v, w)}{\\pi(v)} = \\frac{\\pi(v, w)}{\\sum_{w'}\\pi(v, w')}$\nwhere we slightly abuse notation to write $\\pi(v, w)$ as the marginal probability of the signal realized to be $v$ and the state being $w$ with expectation over unobserved signals.\nInformation value Our framework quantifies the value of information in a signal $V$ as the extent to which the payoff could be improved by the ideal use of $V$ over a baseline information set. We suppose a rational Bayesian DM who knows the data-generating process, observes a signal realization, updates their prior to arrive at posterior beliefs, and then chooses a decision to maximize their expected payoff based on the posterior belief. Formally, the rational DM's expected payoff given a (set of) signal(s) $V$ is\n$R^{\\pi, S}(V) = \\mathbb{E}_{v \\sim \\pi} [\\max_{d \\in D} \\mathbb{E}_{w \\sim \\Pr(w|v)}[S(d, w)]]$\nWe use $\\emptyset$ to represent a null signal, such that $R^{\\pi, S}(\\emptyset)$ is the expected payoff of a Bayesian rational DM who has no access to a signal but only uses their prior belief to make decisions. In this case, the Bayesian rational DM will take the best fixed action under the prior, and their expected payoff is:\n$R^{\\pi, S}(\\emptyset) = \\max_{d \\in D} \\mathbb{E}_{w \\sim \\pi}[S(d, w)]$\n$R^{\\pi, S}(\\emptyset)$ defines the maximum expected payoff that can be achieved with no information. Bayesian decision theory quantifies the information value of $V$ by the payoff improvement of $V$ over the payoff obtained without information.\nDefinition 3.1. Given a decision task with payoff function $S$ and an information model $\\pi$, we define the information value of $V$ as\n$IV^{\\pi, S}(V) = R^{\\pi, S}(V) - R^{\\pi, S}(\\emptyset)$\nWe adopt the same idea to define the agent-complementary information values in our framework."}, {"title": "Agent-Complementary Information Value", "content": "Given the above definitions, it becomes possible to measure the additional value that new signals can provide over the information already captured by an agent's decisions. Here, agent may refer to a human, an AI system, or a human-AI team. The intuition behind our approach is that any information that is used by decision-makers should eventually reveal itself through variation in their behaviors. We recover the information value in agent decisions by offering the decisions as a signal to the Bayesian rational DM. We model the agent decisions as a random variable $D'$ from the action space $D$, which follows a joint distribution $\\pi_D \\in \\Delta(\\Omega \\times \\Sigma_1 \\times ... \\times \\Sigma_n \\times D)$ with the state and signals. The expected payoff of a Bayesian rational DM who knows $\\pi_D$ is given by the function:\n$R^{\\pi, S}(D') = \\mathbb{E}_{d_b \\sim \\pi_D} [\\max_{d \\in D} \\mathbb{E}_{w \\sim \\Pr(w|D'=d_b)}[S(d, w)]]$\nWe seek to identify signals $V$ that can potentially improve agent decisions by analyzing the in-formation value in the combined signal $D' \\cup V$ and the information value in $D'$, which we define as agent-complementary information value.\nDefinition 3.2. Given a decision task with payoff function $S$ and an information model $\\pi$, we define the agent-complementary information value of $V$ on agent decisions $D'$ as\n$ACIV^{\\pi, S}(V; D') = R^{\\pi, S}(D' \\cup V) - R^{\\pi, S}(D')$\nIf the ACIV of a signal $V$ is low, this means either that the IV of $V$ is low (e.g., it is not correlated with $w$), or that the agent has already exploited the information in $V$ (e.g., the agent relies on $V$ to make their decisions such that their decisions correlate with $w$ in the same way as $V$ correlates with $w$). If, however, the ACIV of $V$ is high, then at least in theory, the agent can improve their payoff by incorporating $V$ in their decision making.\nFurthermore, ACIV can reveal complementary information between different types of agents. For instance, if we view AI predictions as $V$ and treat human decisions as the existing agent signal $D'$, a large ACIV indicates that AI predictions add considerable value beyond what humans alone achieve. In the reverse scenario, if human decisions serve as $V$ and AI predictions are $D'$, we can measure how much humans contribute on top of the AI. We demonstrate further usage of ACIV in Section 4 and Section 5."}, {"title": "Instance-level Agent-Complementary", "content": "Instance-level Agent-Complementary Information Value ($ILIV$) evaluates the additional information contributed by a single realization of a signal rather than the entire joint distribution. This finer-grained view is critical for tasks where we need to understand the information value on individual instances, such as when asking whether a human expert should trust an individual prediction from an AI or how to help a human expert understand how the AI model is exploiting information about an instance.\nTo quantify the information value of a realization $V = v$, we construct $Z_v$ as a binary variable indicating whether signal $V$ is realized as $v$, i.e., $Z_v = \\mathbb{1}[V = v]$. The data-generating model defining the joint distribution of $Z_v$, $\\pi_v \\in \\Delta(\\Omega \\times \\{0, 1\\})$, can be constructed through transforming the original data-generating model $\\pi$. Because $Z_v$ is a garbling of $V$ [Blackwell et al., 1951], there always exists a a Markov matrix $\\Gamma : V \\rightarrow \\{0, 1\\}$ such that the new data-generating process $\\pi_v$ can be constructed through $\\pi_v = \\Gamma\\pi$.\nDefinition 3.3. Given a decision task with payoff function $S$ and an information model $\\pi$, we define the instance-level agent-complementary information value of the realization $V$ as:\n$ILIV^{\\pi, S}(v; D') = R^{\\pi_v, S}(D' \\cup Z_v) - R^{\\pi, S}(D')$\nThis local measure captures how much additional payoff can be gained by incorporating the specific realization $v$ into the agent decisions $D'$. Summing ILIV over all possible realizations of $V$ recovers the global agent-complementary information value (ACIV).\nProposition 3.4.\n$ACIV^{\\pi, S}(V; D) = \\sum_{v \\in V} ILIV^{\\pi, S}(v; D')$\nWe apply ILIV to define an information-based explanation technique (ILIV-SHAP) that extends SHAP to explains how the information value of AI predictions complements human decisions $D^{Human}$ for specific instances. Vanilla SHAP [Lundberg and Lee, 2017] defines a saliency-based explanation"}, {"title": "Robustness Analysis of Information Order", "content": "Ambiguity about the appropriate payoff function is not uncommon in human-AI decision settings due to challenges of eliciting utility functions and potential variance in these functions across decision-makers or groups of instances; e.g., doctors penalize certain false negative results differently when diagnosing younger versus older patients [Mclaughlin and Spiess, 2023]. We therefore define the partial order of complementary information value using Blackwell order. Blackwell's comparison of signals [Blackwell et al., 1951] defines a (set of) signal $V_1$ as more informative than $V_2$ if $V_1$ has a higher information value on all possible decision problems. We identify this partial order by decomposing the space of decision problems via a basis of proper scoring rules [Li et al., 2022, Kleinberg et al., 2023].\nDefinition 3.7 (Blackwell Order of Information). A (set of) signal $V_1$ is Blackwell more informative than $V_2$ if $V_1$ achieves a higher payoff on any decision problem,\n$R^{\\pi, S}(V_1) \\geq R^{\\pi, S}(V_2), \\forall S$\nWe test the Blackwell order between signals on a basis of proper scoring rules induced from decision problems. The basis is the set of V-shaped scoring rules, parameterized by the kink of the piecewise-linear utility function.\nDefinition 3.8. (V-shaped scoring rule) A V-shaped scoring rule with kink $\\mu \\in (0, \\frac{1}{2}]$ is defined as\n$S_\\mu(d, w) = \\begin{cases}\n\\frac{1 - |w - \\mu|}{2} & \\text{if } d \\leq \\mu \\\\\n\\frac{1}{2} + \\frac{|1 - w - \\mu|}{2} & \\text{else,}\n\\end{cases}$\nWhen $\\mu' \\in (\\frac{1}{2}, 1)$, the V-shaped scoring rule can be symmetrically defined by $S_{\\mu'} = S_{1 - \\mu'}(1 - y, w)$. Intuitively, the kink $\\mu$ represents the threshold belief where the decision-maker switches between two actions. Larger $\\mu$ means that the decision-makers will prefer $d = 1$ more. The closer $\\mu$ is to 0.5, the more indifferent the decision-maker is to $d = 0$ or $d = 1$.\nProposition 3.9 shows that if $V_1$ achieves a higher information value on the basis of V-shaped proper scoring rules than $V_2$, then $V_1$ is Blackwell more informative than $V_2$. Proposition 3.9 follows from the fact that any best-responding payoff can be linearly decomposed into the payoff on V-shaped scoring rules."}, {"title": "Experiment I: Model Comparison on Chest Radiographs Di-agnosis", "content": "We apply our framework to a well-known cardiac dysfunction diagnosis task [Rajpurkar et al., 2018, Tang et al., 2020, Shreekumar, 2025]. We apply the framework to analyze how much complementary information value a set of possible AI models offer to human decision-makers."}, {"title": "Data and Model", "content": "We use data from the MIMIC dataset [Goldberger et al., 2000], which contains anonymized electronic health records from Beth Israel Deaconess Medical Center (BIDMC), a large teaching hospital in Boston, Massachusetts, affiliated with Harvard Medical School. Specifically, we utilize chest x-ray images and radiology reports from the MIMIC-CXR database [Johnson et al., 2019] merged with patient and visit information from the broader MIMIC-IV database [Johnson et al., 2023]. The payoff-related state, cardiac dysfunction $w \\in \\{0, 1\\}$, is coded based on two common tests, the NT-proBNP and the troponin, using the age-specific cutoffs from Mueller et al. [2019] and Heidenreich et al. [2022]. We use the labels from Irvin et al. [2019] as the human decisions (without AI's assistance) in the diagnosis task, which is a rule-based tool labeling the symptoms as positive, negative, or uncertain, i.e., $d \\in \\{+, ?, -\\}^2$. We fune-tuned five deep-learning models on the cardiac dysfunction diagnosis task, VisionTransformer [Alexey, 2020], SwinTransformer [Liu et al., 2021], ResNet [He et al., 2016], Inception-v3 [Szegedy et al., 2016], and DenseNet [Huang et al., 2017]. Our training set contains"}, {"title": "Results", "content": "Can the AI models complement human judgment? We first analyze the agent-complementary information values in Figure 1, using the Brier score as the payoff function. We find that all AI models provide complementary information value to human judgment. This highlights the same takeaways as section 5, that the AI model has considerable potential to improve human decisions. As shown in Figure 1 (comparison between $R^{\\pi, S}(D^{Human} \\cup D^{AI})$ and $R^{\\pi, S}(D^{Human})$), all AI models capture at least 20% of the total available information value that is not exploited by human decisions. This motivates deploying at AI to assist humans in this scenario.\nFrom the other direction, the human decisions also provide complementary information to all AI models, comparing $R^{\\pi, S}(D^{AI} \\cup D^{AI})$ with $R^{\\pi, S}(D^{AI})$ in Figure 1. In scenarios where partial automation is possible, this observation might inspire, for example, further investigation into what information the humans may have access to that is not represented in AI training data.\nWhich AI model offers the most decision-relevant information over human judgments? Figure 1 shows that Vision Transformer contains slightly higher information value than the other models and Inception v3 contains slightly lower information value than the other models. However, these differences are slight. If the payoff function were questionable in any way, an organization may not want to trust the model rankings. This motivates evaluating model performance over many possible losses to test if there is a Blackwell ordering of models. Across all the V-shaped payoff functions, we find that VisionTransformer is Blackwell more informative and Inception v3 is Blackwell less informative than all other models. By Proposition 3.9, we test the payoff of models on all V-shaped scoring rules, shown in Figure 4. The Vision Transformer achieves a higher information value on all V-shaped scoring rules, implying a higher information value on all decision problems. This analysis highlights the insufficiency of accuracy-based model comparisons to account for all downstream decision problems: 1) while accuracy may rank models in one way, there may exist decision problems where the order between models is reversed; 2) while two models may seem comparable in accuracy, one can be more informative than the other for all decision problems, which is robustly good."}, {"title": "Experiment II: Behavioral Analysis on Deepfake Detection", "content": "We analyze a deepfake video detection task [Dolhansky et al., 2020], where participants are asked to judge whether a video was created by generative AI. We apply the framework to benchmark the use of available information by the human, AI, and human-AI team."}, {"title": "Data and Model", "content": "We define the information model on the experiment data of Groh et al. [2022]. They recruited 5,524 non-expert participants through Prolific. Participants were asked to examine the videos, and pro-vided with assistance from a computer vision model, which achieved 65% accuracy on a holdout dataset. They reported their decisions in two rounds. They first reviewed the video and reported an initial decision ($D^{Human}$) without access to the model. Then, in a second round, they were told the Al's recommendation ($D^{AI}$) and chose whether to change their initial decision, resulting in a final decision ($D^{Human-AI}$). Participants' decisions (both initial and final) were elicited as a percent-age indicating how confident they were that the video was a deepfake, measured in 1% increments: $d \\in \\{0\\%, 1\\%, ..., 100\\%\\}$. We round the predictions from the AI to the same 100-scale probability scale available to study participants.\nWe use the Brier score as the payoff function: $S(w, d) = 1 - (w - d)^2$, with the binary payoff-related state: $w \\in \\{0, 1\\} = \\{\\text{genuine, fake}\\}$. The scale of Brier score is [0,1] and a random guess ($d \\sim \\text{Bernoulli}(0.5)$) achieves 0.75 payoff under Brier score. We choose the Brier score instead of"}, {"title": "Results", "content": "How much decision-relevant information do each agent's decisions offer? We first compare the information value of the AI predictions to the decision problem to that of the human decisions in the first round (without AI assistance). To contextualize the value of each, we first construct a scale ranging from the expected payoff of rational DM with no information, i.e., $R^{\\pi, S}(\\emptyset)$, to that of the rational agent who has access to all information, i.e., $R^{\\pi, S}(\\Sigma_1 \\cup ... \\cup \\Sigma_n \\cup D^{Human} \\cup D^{Human-AI} \\cup D^{AI})$. The lower-bound represented by the rational DM with no information is 0.75 in Brier score, which is equivalent to the payoff achieved by a random guess drawn from Bernoulli(0.5).\nUsing this scale, Figure 2(a) shows that AI predictions provide about 65% of the total possible information value over the no information baseline, while human decisions only provide about 15%. Hence, human decisions are only weakly informative for the problem.\nWe next consider the human-AI decisions. Given that the AI predictions contained a significant portion of the total possible information value, we might hope that when participants are provided with AI predictions, their performance comes close to the full information baseline. However, the information value contained in human-AI decisions is also only take a small proportion of the total possible information value (30%). This aligns with the findings by Guo et al. [2024] that humans tend to rely on the AI but are bad at distinguishing when AI predictions are correct.\nHow much additional decision-relevant information do the available features offer over each agent's decisions? To understand what information might improve human decisions, we assess"}, {"title": "Experiment III: Information-based Local Explanation on Recidivism Prediction", "content": "We apply our framework to a recidivism prediction task, where the decision-maker decides whether to release a defendent. We apply the framework to evaluate and augment saliency-based explanations."}, {"title": "Data and Model", "content": "We use the dataset from COMPAS dataset [Angwin et al., 2022], which contains 11,758 defendents with associated features capturing demographics, information about their current charges, and their prior criminal history. We merge the dataset with the experimental data from Lin et al. [2020] to represent the human decisions, which contains recidivism decisions on a subset of instances from Angwin et al. [2022]'s experiment on laypeople. Human decisions were elicited on a 30-point probability scale. Merging the two datasets produces 9,269 instances (defendants). For the AI model, we trained an extreme gradient boosting (XGBoost) model [Chen and Guestrin, 2016] on a training set with 6,488 instances, achieving an AUC of 0.84 on a hold-out test set with 2,781 instances. We round the model predictions to the same 30-point probability scale available to study participants in Lin et al. [2020].\nWe use Brier score as the payoff function, with the binary payoff-related state $w$ indicating whether the defendent gets rearrested within two years. We use the features of defendent contained in COMPAS dedaset as the signals in our demonstration: demographic features (age, sex, and race), information about the current charge (type of offense and whether it is a misdemeanor or a felony), prior criminal history (e.g., past number of arrest and charges for several offense categories) and the predicted score from COMPAS system. We use the hold-out test set to estimate the data-generating process, which defines the joint distribution of state, signals, human decisions and AI predictions.\nConstructing instance-level signals. Denote the XGBoost model's predictions as a random vari-able $Y_{pred} = f(X)$, where $X$ denotes the random variable of data features and $f(.)$ denotes the model's predictive function. We construct $Z_{f(x)}$ as $\\mathbb{1}[Y_{pred} = f(x)]$ for every $x$ and use it to calculate"}, {"title": "Results", "content": "How well does SHAP explain complementary information offered by the AI prediction over what humans already know? Figure 3 (A) and (B) compares the distribution of feature-attribution scores from SHAP to those from our ILIV-SHAP. We observe multiple discrepancies in feature importance across the methods. For instance, in Figure 3(A), the age feature negatively corre-lates with the AI's prediction, indicating that younger defendants tend to receive higher predictions. However, Figure 3(B) ILIV-SHAP shows no such association between age and the Al's information value over human decisions. This implies that, although age influences the AI's prediction, on average it does not provide additional information that humans lack.\nConversely, some features that correlate strongly with the ILIV of the AI prediction are relatively unimportant for the AI's raw prediction. For example, in Figure 3(A), low decile scores predicted by the COMPAS (colored in blue) have a relatively small impact on the AI's numerical output compared with other features. In contrast, Figure 3(B) reveals that some low decile scores yield large positive ILIV, suggesting that this feature might offer valuable information for human decision-makers despite not significantly changing the AI's raw prediction.\nHow can an information-based explanation enhance understanding of AI predictions? We argue that an information-based explanation (via ILIV-SHAP) can serve as a supplement to saliency-based explanations, providing users a sense of whether the individual prediction can help them make decisions. First, an information-based explanation conveys whether the prediction correlates with the payoff-related state. For example, Figure 3(C) shows that the model prediction $f(x)$ deviates +0.383 from the prior prediction $Ef(X)]$ on instance 4. However, the information-based explanation of the same instance in Figure 3(D) shows that the ILIV of the AI prediction over human decisions does not change much from the ILIV of the prior prediction (which is very low at 0.005). This suggests to the user that focusing on the AI prediction on this instance is not necessary to good decision-making.\nSecond, the information-based explanation conveys to the user whether the AI gets useful informa-tion beyond their own information from a certain feature. In Figure 3(A), the defendent's prior count record (priors_count=3) changes the AI prediction the most (+0.186). However, Figure 3(D) shows that priors_count = 3 contributes marginally to the ILIV (+0.00004). Therefore, ILIV-SHAP con-veys that even if priors_count=3 changes the AI prediction significantly, the AI does not necessarily better predict the payoff-related state as a result of it."}, {"title": "Discussion and Limitations", "content": "We propose a decision-theoretic framework for assigning value to information in human-AI decision-making. Our methods quantify the additional information value of any signals over an agent's decisions. The three demonstrations show how quantified information value can be used in model selection, empricial evaluation, and explanation design. These demonstrations are just a few of many possible uses cases. For example, alternative explanation approaches could be compared via their information value to human decisions. Information value analysis could drive elicitation of human private signals or decision rules to further improve the pairing. New explanation strategies could integrate visualized information value with conventional depictions of feature importance.\nInformation value cannot definitively establish that particular signals were used by a human. It is always possible that the human has other private signals offering equivalent information to a feature being analyzed. Our framework cannot account for private unobservable signals even though they might have strong correlation with the payoff-related state and agent deicions. However, quantifying the value of observed information is a tool toward learning about information that may exist beyond a problem definition.\nOur framework quantifies the best-attainable performance improvement from integrating signals in decisions. This does not necessarily mean that empirically those signals will lead to agents better performing. However, the motivating idea is that Bayesian decision theory provides a theoretical basis that can be adapted to support informative comparisons to human behavior. For example, if we suspect a human decision-maker uses AI predictions and their own predictions strictly monotonically, we could constrain the Bayesian decision-maker to only make monotonic decisions with AI prediction and their own predictions."}]}