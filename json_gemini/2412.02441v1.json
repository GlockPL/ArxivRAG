{"title": "Artificial Expert Intelligence through PAC-reasoning", "authors": ["Shai Shalev-Shwartz", "Amnon Shashua", "Gal Beniamini", "Yoav Levine", "Or Sharir", "Noam Wies", "Ido Ben-Shaul", "Tomer Nussbaum", "Shir Granot Peled"], "abstract": "Artificial Expert Intelligence (AEI) seeks to transcend the limitations of both Artificial General Intelligence (AGI) and narrow AI by integrating domain-specific expertise with critical, precise reasoning capabilities akin to those of top human experts. Existing AI systems often excel at predefined tasks but struggle with adaptability and precision in novel problem-solving. To overcome this, AEI introduces a framework for \u201cProbably Approximately Correct (PAC) Reasoning\". This paradigm provides robust theoretical guarantees for reliably decomposing complex problems, with a practical mechanism for controlling reasoning precision. In reference to the division of human thought into System 1 for intuitive thinking and System 2 for reflective reasoning [Tversky and Kahneman, 1974], we refer to this new type of reasoning as System 3 for precise reasoning, inspired by the rigor of the scientific method. AEI thus establishes a foundation for error-bounded, inference-time learning.", "sections": [{"title": "Introduction", "content": "What does it take to be a great scientist or domain expert? Beyond technical skills and mastery of domain-specific tools-where AI already excels\u2014there is an elusive quality that distinguishes leading human experts: actual intelli-gence, as manifested by the ability to innovatively synthesize knowledge, while at the same time think critically in the sense of separating correct from incorrect statements and acknowledging the boundaries of knowledge. As a result, great scientists and experts advance humanity by solving novel problems. This paper introduces Artificial Expert Intel-ligence (AEI) as a new paradigm that combines current AI's knowledge and skills with the intelligence characteristic of top human experts. We propose a concrete, constructive definition of AEI which provides a blueprint for building AEI systems.\nOn our path to understanding the intelligence aspect exhibited by human experts, let us consider the ongoing discussion on the definition of intelligence. Legg and Hutter [2007] compiled a list of 70 informal definitions of intelligence. In his famous imitation game, Turing [1950] tackled the \u201ccan machines think\u201d question by their ability to be indistinguishable from a human during a conversation. This ability is a specific skill, and therefore does not seem to convey the full generality of what intelligence is. Minsky [1988] defines intelligence as the ability to solve the hard problems that intelligent humans would solve, extending the scope of skills, but lacks reference to adaptivity to new scenarios. Indeed, Schank [1991] defines intelligence as getting better over time, without specifying how the system gets better over time. This allows naive forms of improvement, such as memorizing facts. According to Kurzweil"}, {"title": "From PAC-learning to PAC-reasoning", "content": "Probably Approximate Correct (PAC) learning [Valiant, 1984] aims at formalizing the notion of deducing a function $g : X \\rightarrow Y$ based on empirical observations (training examples) of pairs $(x, g(x))$. The model imposes two critical assumptions. First, it is assumed that there is a distribution $D$ over $X$ such that we can sample independent $x$'s from $D$ and receive their $g$ value as \u201clabels\u201d. Secondly, that $g$ belongs to some predefined hypothesis class $H$ known to the learner as prior knowledge. The main premise of PAC-learning is that if the number of examples seen during training is sufficiently large with respect to the VC-dimension [Vapnik, 2000] of $H$, then we can \u201cprobably approximately\" find $g$. In particular, for finite $H$, the required number of examples (sample complexity) grows logarithmically with $|H|$.\nThis is a very powerful guarantee as we can choose $H$ to be the set of all functions we can implement in Python using $d$ characters and learn this class by order of $d$ examples. The \u201cdevil\", however, is the computational complexity of learning, defined by the time required to find a good candidate in $H$, which for this hypothesis class is exponential in $d$. And so, even for medium size programs with 100 characters, the time complexity of learning is unrealistically large. Deep learning tackles the problem by defining the hypothesis class as a differentiable space with a specific structure (a neural network architecture) and searching a good function in $H$ is performed by Stochastic Gradient Descent (SGD). For some problem-structure pairs, this approach works very well for example, Convolutional Neural Networks (CNN) [LeCun et al., 1998] for computer vision and Transformers [Vaswani, 2017] for next-word prediction. However, deep learning fails to generalize even for some simple problems, such as learning to multiply two numbers, so while we solved the computational complexity problem, we are back to a sample complexity problem.\nSuch failures and others motivated practitioners to break the prediction task into a \u201cchain of thoughts\" or, more generally, into a reasoning chain.\nIn some sense, reasoning imposes structure on the function we aim to learn at inference time, as opposed to CNNs and Transformers, which impose structure during training time. In particular, the imposed structure is via decom-posing the original problem into simpler sub-problems. Assuming that the sample complexity of each subproblem is significantly smaller than that of the original problem, we unlock the potential to significantly deviate from what we saw during training time. However, errors in the solution of each subproblem accumulate and create an overall decaying precision, which effectively puts a cap on the length of the reasoning chain. Formally, suppose that we have decomposed the original problem into $k$ subproblems and let $e_i$ be the probability that the $i$'th subproblem errs over random $x \\sim D$, then the probability that the entire reasoning chain will succeed is\n$\\prod_{i=1}^{k}(1-e_i) \\approx \\exp(-\\sum_{i=1}^{k}e_i)$\nIf the average (over the space of sub-problems) of $e_i$ is a fixed $e > 0$, then the above equals to $\\exp(-ke)$. This would be the case for intuitive reasoners.\nIn what follows, we start with formalizing the structure imposed by reasoning at inference time, and then we introduce a mechanism for controlling the magnitude of all of the $e_i$ during inference, hence enabling arbitrarily long reasoning chains. In particular, either the reasoner succeeds and can guarantee a (probably approximately) correct solution, or admits failure and responds \"I don't know\", as an expert would. We refer to this framework as PAC-reasoning."}, {"title": "Bottom-up Reasoning", "content": "Similar to the classical PAC-learning model, we are aiming to find a function $g : X \\rightarrow Y$ and we assume that there is a distribution $D$ over $X$ such that we can sample independent $x$'s from $D$. Unlike the classical PAC model, we do not assume that $Y = \\{0, 1\\}$ represents labels, and we do not assume that we have access to $g(x)$ for $x$'s that we sample. Furthermore, we do not assume that $g$ belongs to some predefined hypothesis class $H$. Instead, we make a different assumption on the structure of $g$, aiming to formalize a notion of decomposability. This structure also defines a different loss model that our reasoner receives as an alternative to the \"zero-one loss with respect to labels\" that the classical PAC model employs. Our decomposability assumption for the bottom-up reasoner relies on the concept of computation graphs.\nDefinition 1 (Computation Graph) A computation graph is defined by $(\\[k], E, f_1,..., f_k)$, where $(\\[k], E)$ are the sets of vertices and edges of a directed acyclic graph, with the vertices sorted topologically\u00b9, and with $v_1$ being the only vertex with no incoming edges. Given an input $x$, the computation graph applies the following:\n\u2022 The output of $v_1$ is $o_1(x) = f_1(x)$\n\u2022 For $i = 2, 3, ..., k$, the output of $v_i$ is $o_i(x) = f_i(o_j(x) : (j, i) \\in E)$\nThe bottom-up reasoning process aims at building a computation graph that approximates $g$. It is done by building a search tree (using BFS or DFS), until reaching a root-to-leaf path in the tree that describes a computation graph that approximates $g$. In other words, node $i$ in the root-to-leaf path in the search tree corresponds to vertex $i$ in a computation graph (based on topological order). This is done while relying on an actor and critic policies at each reasoning step, as well as a \"decomposition oracle\u201d for terminating the search process:\n\u2022 The actor receives a context $c \\in \\Sigma^*$ (which represents free text information obtained so far, represented as a sequence of tokens over an alphabet $\\Sigma$) as well as a computation graph for the path from the root to the current node. The actor proposes a set of possibilities to append one more vertex to the computation graph. Suppose that the current computation graph has $i - 1$ vertices. Then each possibility is defined by a function $f_i$ and a subset $J \\subset \\{1,..., i - 1\\}$. This yields a function $o_i$ whose output on $x$ is $o_i(x) = f_i(o_j(x) : j \\in J)$. In addition, the actor provides\u00b2 an Example Validator (EV) for each such $f_i$, such that given an input $a$ to $f_i$, the EV outputs True if $f_i(a)$ is correct and False otherwise. In other words, the EV produces a spec for the intended functionality of $f_i$ via a unit test. The critic, discussed in the next item below, is responsible for producing inputs $a$ for the test. We refer to the set of proposals made by the actor as a proposal class, $P$, and each element of $P$ is a triplet of $(f_i, J, EV)$. It is convenient to think of $P$ as a finite class, and for simplicity, the analysis we present later is for this case. However, using standard techniques from learning theory, it is not hard to generalize the results to some infinite classes.\n\u2022 The critic receives the proposal class, $P$, and should output a subset $P_g \\subset P$ that contains only proposals which are $e_i$-approximately correct (for a value of $e_i$ to be specified later on). We say that a proposal is $e_i$-approximately correct if $P_{x\\sim D}[EV(f_i, a(x)) = False] < e_i$, where the probability is over an i.i.d. sampling of $x \\sim D$ and $a(x) = (o_j(x) : j \\in J)$ is obtained by running the current computation graph on $x$. We discuss the required value of $e_i$ as well as how to guarantee that the critic is (probably approximately) correct later on.\n\u2022 Search Termination by decomposition oracle: at some step of the search, the actor may think that the com-putation graph corresponding to the current node in the search tree is representing a correct decomposition of"}, {"title": "Top-Down Reasoning", "content": "In Section 2.1 we presented a bottom-up reasoning process. We call it bottom-up because a computation graph imple-ments each operation before using it. A different reasoning approach is a top-down process in which we allow \u201cforward references\u201d to un-implemented functions. To motivate top-down reasoning, consider the following stand-alone Python implementation of the merge-sort algorithm.\nWe've intentionally written the code with a top-down structure, starting with the main function we want to imple-ment (\"merge_sort\"). The main function relies on a helper function (\u201cmerge\u201d). We implement the helper function after completing the main function.\nAttempting to implement merge_sort using a computation graph, we immediately notice some undesired phenom-ena. First, while the code itself does not depend on the length of the array we aim to sort, the structure of a computation graph does depend on it. So we need a dedicated computation graph per each length of the array. Furthermore, even if we fix the size of the array, we notice that the \"merge\" function is being called multiple times with different inputs. As a result, we need a dedicated vertex in the computation graph for each call to each function. In other words, we need to re-implement the helper functions again and again. Besides being inefficient, this also unnecessarily enlarges the number of reasoning steps we need to perform.\nThis motivates a different approach, which we call a top-down reasoning process. We are again aiming to approx-imate a function $g : X \\rightarrow Y$. In the aforementioned example, this function would be \u201cmerge_sort\"."}, {"title": "Discussion", "content": "The introduction of Artificial Expert Intelligence (AEI) marks a significant step toward building AI systems that are not only capable of skillful execution but also embody the precision, adaptability, and rigor characteristic of human experts. By integrating the PAC-reasoning framework, AEI addresses two critical challenges in AI: error accumulation in long reasoning chains and the inability to adapt reliably to novel and complex problems. The key innovation lies in the establishment of a \"System 3\" reasoning process, which parallels the scientific method, combining empirical validation with structured problem-solving. AEI's ability to blend bottom-up and top-down reasoning processes offers a flexible approach to tackling a broad spectrum of problems, from algorithmic challenges to engineering and scientific problem-solving. This adaptability positions AEI as a versatile tool for advancing problem-solving methodologies in domains where existing AI systems struggle to generalize or ensure accuracy.\nThe reliance on domain-specific decomposition oracles and example validators introduces challenges in scaling the system to highly diverse or poorly understood domains. Future research should explore methods to generalize AEI's principles to broader, less structured domains, potentially by improving its ability to autonomously construct validators and decomposition oracles.\nAEI represents a paradigm shift in AI, emphasizing not just capability but also precision. As AEI systems evolve, they have the potential to redefine our expectations of artificial intelligence, offering a new era of hyper-accelerated innovation driven by expert-level reasoning."}]}