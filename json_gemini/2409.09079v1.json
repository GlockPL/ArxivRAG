{"title": "D3-GNN: Dynamic Distributed Dataflow for Streaming Graph Neural Networks", "authors": ["Rustam Guliyev", "Aparajita Haldar", "Hakan Ferhatosmanoglu"], "abstract": "Graph Neural Network (GNN) models on streaming graphs entail algorithmic challenges to continuously capture its dynamic state, as well as systems challenges to optimize latency, memory, and throughput during both inference and training. We present D3-GNN, the first distributed, hybrid-parallel, streaming GNN system designed to handle real-time graph updates under online query setting. Our system addresses data management, algorithmic, and systems challenges, enabling continuous capturing of the dynamic state of the graph and updating node representations with fault-tolerance and optimal latency, load-balance, and throughput. D3-GNN utilizes streaming GNN aggregators and an unrolled, distributed computation graph architecture to handle cascading graph updates. To counteract data skew and neighborhood explosion issues, we introduce inter-layer and intra-layer windowed forward pass solutions. Experiments on large-scale graph streams demonstrate that D3-GNN achieves high efficiency and scalability. Compared to DGL, D3-GNN achieves a significant throughput improvement of about 76x for streaming workloads. The windowed enhancement further reduces running times by around 10x and message volumes by up to 15x at higher parallelism.", "sections": [{"title": "1 INTRODUCTION", "content": "The ubiquity of large-scale, semi-structured data, such as knowledge graphs, social networks, financial transactions, and e-commerce networks, has fostered graph learning [41]. To this end, Graph Neural Networks (GNNs) have proven to achieve greater performance on tasks like node classification [17], link prediction [57], and graph classification [9], compared to traditional approaches [53]. GNNs combine neural networks (NN) with graph topology, allowing them to generate semantically richer node embeddings used for downstream prediction tasks. Applications of these models and the various tasks have since been extensively studied in recommendation systems [13, 55], computer vision [2, 18], social networks [52], fraud detection [26], and more.\nSeveral frameworks have been developed to facilitate distributed GNN model development and deployment in static settings [46]. However, most of the graphs observed in the real world are dynamic or streaming [5]. For example, in social networks, new users join and existing users may update their profiles or interests, leading to node updates and additions. The rate of ingestion in a streaming graph can be high, such as 30K edges/sec in Alibaba graph [38]. In addition to streaming updates, it is also important to consider low-latency query settings in the design of GNN systems. The state-of-the-art methods typically follow the ad-hoc querying setting, where an external actor is expected to initiate the execution of a GNN query.\nHowever, a rather unexplored setting, which is of high importance for latency-critical applications, is the online setting where queries are implicit to graph topology. Consider a dynamic social network where the system needs to autonomously identify and monitor potential spam accounts or harmful content creators without external prompt. This requires the GNN system to continuously analyze the graph for such behaviors, a task that cannot be efficiently handled by ad-hoc systems due to their reliance on periodic entire-graph inferences, thus making them ill-suited for near-real-time requirements. We bridge these gaps by introducing a distributed, end-to-end solution that is highly flexible and is designed to handle streaming graphs in online query settings.\nUnder streaming graph updates, previously generated representations become stale and require cascading GNN inference operations over a set of influenced nodes to maintain an up-to-date state. During inference (forward pass), the cascading nature of operations causes neighborhood explosion [5], which makes it challenging to maintain consistent throughput while keeping representations abreast of the updates. Identifying the set of influenced nodes (i.e., nodes affected by graph updates) is also resource-intensive, requiring a full L-hop traversal of the out-neighborhood. The irregular access patterns and data-skew on graphs also incur additional overheads, due to central (hub) nodes being involved in the majority of computations. This worsens the problems caused by 'neighborhood explosion' and introduces significant load-imbalance. Also, updates"}, {"title": "2 RELATED WORK", "content": "To the best of our knowledge, there is no other streaming dataflow solution for GNN computations tackling the online query execution model, either as a research or as an industrial offering. Existing systems are not well-suited for low-latency streaming graph learning and inference operations."}, {"title": "2.1 Streaming Graph Processing Systems", "content": "Over the past decade, a variety of graph management systems and algorithms have been designed for streaming graph analytics [6, 30, 34, 44]. These systems are optimized for running analytics algorithms, such as subgraph matching and subgraph counting, on such dynamic data [29]. However, none of these systems are designed to perform on graph learning tasks such as GNNs [8].\nRecently, with the advent of streaming systems, dataflow-based graph systems have been developed. The Gelly-streaming library, for instance, built on top of Apache Flink, tackles streaming graph algorithms such as finding connected components and bipartite matching. GraphX [15] aims to unify optimizations from specialized graph processing environments (e.g., Pregel) with general-purpose processing environments (e.g., MapReduce, Apache Spark). Other dataflow-based methods have been devised for incremental graph algorithms, such as Naiad [31], GraphTau [21], Tornado [42], and KickStarter [48]. None of these tackle graph learning and GNN computations in the streaming setting.\nBy contrast, D3-GNN provides a dataflow-based streaming graph system targeting GNN computations and applications. It enables incremental GNN computations while preserving primary aspects of streaming systems [3]. Note that studies on enhancing dynamic GNN training (e.g., edge events, continual learning, memory modules [28, 33, 40, 49]) are orthogonal to our work, and most can be implemented within our modular D3-GNN ecosystem."}, {"title": "2.2 Distributed GNN Systems", "content": "Extending from graph analytics systems, the recent advent of GNNs has brought attention to push-based distributed systems. These"}, {"title": "3 BACKGROUND", "content": "This section presents the background needed to cover how D3-GNN provides a fault-tolerant dataflow pipeline, and how it achieves distributed GNN inference and training on streaming graphs."}, {"title": "3.1 Graph Streams", "content": "Real-world graphs are often dynamic in nature, exhibiting changes in their topology as well as node and edge features over time. We denote a multi-modal graph as G = (V, E, XV, XE) comprising nodes \u0e02\u2208 V, edges eu,v \u2208 E \u2286 VXV. Additionally, nodes and edges may contain some features. To simplify the presentation, we consider a single feature associated with each node, denoted by xz Vo\u2208V, and similarly consider edge features xe Ve \u2208 E. We use edge streams to ingest topological data, while node features as feature stream. Nonetheless, the granularity of updates is not limited to this particular paradigm. For example, one can employ vertex streams (streaming nodes along with their local neighborhoods) jointly with a matching partitioner like Fennel [45]. More generally, each streaming event is timestamped and may be a create, delete, or update operation on a graph element (vertex/edge/feature/sub-graph)."}, {"title": "3.2 Distributed Streaming Dataflow", "content": "To build D3-GNN, we leverage Apache Flink, a stateful stream processing system that started as a research project and is now widely used in the industry for processing data streams. Flink uses a pipeline of data transformation tasks, called operators, to consume input streams and emit output streams. Operators can be parallelized across threads and machines, with each parallel sub-operator instance performing local computations. Fault-tolerance is guaranteed through replayable source operators and intermittent checkpoints to reliable storage. States can be recovered from this storage and used for the re-scaling of partitions. Flink employs a variation of the Chandy-Lamport algorithm [4] for distributed checkpointing, ensuring that correctness is maintained.\nAlthough Flink pipelines are typically represented as Directed Acyclic Graphs (DAGs), we extend the framework by introducing a novel way to add nested cycles to enable sending back the gradients and performing state replication. This supports fault-tolerant, iterative computations for more complex tasks as part of our solution. We enhance the expressiveness and flexibility of Flink and develop a set of novel modules to better handle a wider range of streaming use cases. Our end-to-end solution enables efficient and scalable processing of iterative streams, while maintaining fault-tolerance by including in-flight iterative events within checkpoints."}, {"title": "3.3 Graph Neural Network", "content": "In Graph Neural Networks (GNNs), the learning process often involves the generation of node (or edge) embeddings, which are used to perform downstream tasks such as node classification, link prediction, and similarity queries. The Message Passing GNN (MPGNN [14]) paradigm views the computation task for each node in a GNN layer as a message generation process along incoming edges, followed by an aggregation operation at the receiving node. The node then updates its representation, which is used by the next"}, {"title": "3.4 Graph Partitioning", "content": "Graph partitioning divides a graph's nodes or edges into disjoint subsets, or partitions, aiming to minimize 'cut-edges' or 'replicated-nodes'. This NP-Hard problem seeks to balance computing loads and reduce network communication. However, most existing algorithms focus on static graphs, making them ill-suited for dynamic, real-time inputs due to lengthy computation times.\nIn response, D3-GNN involves streaming graph partitioning that compute partitions in real-time as each graph event arrives. We separate logical from physical partitioning for enhanced system scalability. Utilizing HDRF, CLDA [39], METIS and Random streaming vertex-cut partitioners, we achieve data-parallelism by distributing incoming edge streams. Additionally, D3-GNN presents a feature-granular master-replica synchronization method to manage vertex replication among sub-operators."}, {"title": "4 METHODOLOGY", "content": "D3-GNN performs asynchronous and incremental inference, enabling the production of up-to-date node representations under streaming graph updates. In cases where re-training is necessary, it performs synchronous backpropagation in a distributed fashion, while avoiding stale states and eliminating bursty resource provisioning. Stale states are a result of outdated information during training, causing asymmetric forward and backward functions. This can occur due to asynchronous updates in graph topology and node embeddings, while backpropagation is taking place, and can negatively impact model accuracy. To be able to tackle this, current systems would maintain a training environment that pulls a static graph snapshot to perform backpropagation, which results in bursty resource provisioning and extensive data migration. Our approach eliminates the need for a separate training environment, reducing the computational burden and improving overall efficiency while providing full-graph, static training guarantees."}, {"title": "4.1 System Overview", "content": "D3-GNN involves a dynamic dataflow pipeline, as depicted in Figure 1. This pipeline leverages a unified message format that supports create, update, and delete events for edges, vertices, and features. The process begins with a source Dataset parser. While we utilize temporal edge-list files to stream per-edge addition events and per-node features, in practice, any stream source, such as database logs, edge devices, or message queues, can be used. Coupling our system with Flink allows us to seamlessly integrate a broad range of such external data sources. After the events are generated, the streaming Partitioner consumes them, assigning and routing the events to their designated parts. This facilitates data-parallelism for subsequent operators. Before the GNN operators receive them, incoming events pass through a Splitter. This filter improves memory efficiency by ensuring that GNN layers do not process unnecessary events. We categorize events into three classes to cover the majority of inductive use-cases in GNNs:\n\u2022 Topology data: Received by all GNN layers, except for the output layer when the task is node-level.\n\u2022 Feature data: Only the first GNN layer receives this, as subsequent layers inherit their features from the preceding one.\n\u2022 Train & test data: Solely received by the final output layer for training and reporting real-time metrics.\nSubsequent GNN layers are distributed into their associated Graph Storage operators, facilitating model-parallelism. Furthermore, this design allows us to control the parallelism of GNN layers independently, thereby better accommodating explosive workloads. These operators incrementally store their graph partition as events arrive. Each sub-operator handles a subset of graph parts and houses the actual GNN layer. In addition, it includes a set of Plugins that monitor local graph updates and execute computations at a granular level, like individual feature updates. This design allows us to devise incremental GNN computations, ensuring that the graph partition and GNN code remain co-located, unlike in sampling-based systems. Such a configuration grants quick access to essential data"}, {"title": "4.2 GNN Inference", "content": "In order to maintain up-to-date node representations in an online fashion, the system must continuously track all influenced nodes I whose representations have become outdated due to new topological or feature update to the graph. This is necessary due to cascading effect of updates during model inference. For a GNN with L layers and an input graph having average in-degree $d_{in}$ and average out-degree $d_{out}$, an edge addition is expected to influence |I| = $\\Sigma_{l=0}^{L-1} d_{out}^{l}$ nodes. Computing the set of influenced nodes is $O(d_{out}^{L})$ as it requires fetching L-1 out-neighborhood. Updating node representations, in turn, requires retrieving their L-hop in-neighborhood (roots of computation graph) and performing forward pass on them. This in turn means constructing |I| computation graphs each with $d_{in}^{l}$ source vertices (i.e., $O(d_{in}^{L})$ each). As such, the total cost of supporting a single edge update becomes $O(d_{out}^{L} + [\\Sigma_{l=0}^{L-1} d_{out}^{l}]*d_{in}^{L})$. Consequently, when dealing with large graphs, performing low-latency inference can quickly become an impractical and difficult task.\nIn our approach, we avoid explicitly tracking influenced nodes or repeatedly pulling neighborhoods for constructing local computation graphs. Instead, as depicted in Figure 1, we treat the chained GraphStorage dataflow as an implicit computation graph, naturally partitioned by breadth (data-parallelism) and depth (model-parallelism), and enable incremental cascades based on changes on graph topology and features. In doing so, D3-GNN achieves a much lower cost $O(d_{in}d_{out}^{L-1})$ of updating influenced node representations with a single edge addition."}, {"title": "4.2.1 Incremental aggregation", "content": "In MPGNN (Section 3.3), aggregators summarize the messages that arrive to the node from all of its in-neighbors. These are typically permutation-invariant functions such as sum, mean, and concatenation. We note that, such functions can be incrementally updated by maintaining a relatively small state using exact or probabilistic data structures. To formalize this approach, in D3-GNN, we develop AGGREGATORS as instances of synopsis operations that are cached at each master node to maintain incremental computations. The states of Aggregators are updated by remotely invoking one of the following method interfaces at the master node:\nreduce(msg, count = 1) to add a new message\nreplace(msgnew, msgold) to update a message\nremove(msg, count = 1) to delete a message\nOur incremental formulation for the AGGREGATOR is customizable, being contingent only on the restrictions of synopsis operators, as they have to be mergeable, commutative, and invertible. It can thus support any UPDATE and MESSAGE neural network definitions, with their properties having no impact on the incremental functionality. Moreover, under massive graph updates, we do not require any graph locking mechanisms and allow many cascades to be simultaneously updating the system. Since the aggregators are permutation-invariant and feature updates follow causally consistent dataflow, the incremental model is eventually consistent.\nTherefore, the tracking of influenced nodes and the retrieval of node neighborhoods occur automatically, by following the dataflow between operators. These are triggered by external updates at each layer of the GNN.\nMost temporal GNN architectures make use of memory modules which are new embeddings that do not conflict with the above essential properties of the aggregators. This allows our model to be adaptable to temporal GNNs without compromising the integrity of its core functions. Integrating nonlinear recurrent units like LSTMs and GRUs, which diverge from the traditional aggregator paradigm, presents a more complex challenge. These units, by design, necessitate the retention of an extensive message history for each node. A permutation-invariant version of this approach would enable us to streamline the model by retaining only the most recent hidden state for each aggregator, aligning seamlessly with the operational protocols of our described D3-GNN interfaces. For instance, the process for replacing a message could be elegantly bifurcated into two distinct phases: initially, the model would apply the sign inverse of the old message to effectively nullify its impact, followed by the computation of the new message."}, {"title": "4.2.2 Streaming forward pass", "content": "This method describes pure streaming approach for inference, where the next layer representations are immediately updated by cascading through the computation graph as described earlier. The arriving edges cause MESSAGES to be sent to destination aggregators. As the Aggregators receive updates, the algorithm generates up-to-date representations through Update function and forwards it to the next GraphStorage along the chain. Note that, we employ vertex-cut partitioning in our system, therefore some vertices are replicated and the corresponding"}, {"title": "4.2.3 Explosion Factor", "content": "Since our GNN layers are fully decoupled and our graph is only logically partitioned, we can vary the parallelisms of Graph Storage operators independently. Hence, to tackle GNNs' neighborhood explosion challenge, we introduce a new system hyper-parameter called 'explosion factor' (\u03bb). This enables us to vary the parallelism $p_i$ of each individual Graph Storage operator, i.e., the number of sub-operators that perform the same task in a data-parallel manner. Namely, given an initial parallelism p and L layers of the GNN, we assign the actual parallelism for each Storage operator (layer) as $p_i = p * \\lambda^{i-1}$ for $i \\in [1, . . ., L]$. This parameter must be selected considering the frequency of training, as even though the forward pass is always benefited by higher \u03bb, because neighborhood explosion has reverse effect on layer-wise workload during backward pass for training."}, {"title": "4.2.4 Windowed forward pass", "content": "We propose intra-layer and inter-layer windowing to mitigate neighborhood explosion, data skews, and master node imbalances. Unlike the streaming algorithm (Algorithm 1), which executes the forward and reduce functions immediately, the windowing approaches introduce a time-based window that delays their execution.\nIntra-layer windowing (which delays the forward functions for each vertex) is especially beneficial for hub-vertices. It allows us to send a single, most up-to-date update for a batch of forward requests. By doing this, we can optimize network usage and reduce cascades in the subsequent layer, thereby minimizing effects of neighborhood explosion.\nAlthough our system utilizes vertex-cut partitioners to balance per-edge computations, the presence of nodes that receive updates only at master nodes can introduce additional skews. Thus, inter-layer windowing delays the emission of reduce messages for each destination node. It batches the corresponding edges, calculates a local, partial aggregation for each destination node, and emits a single reduce message summarizing the batched edges.\nAlgorithm 2 provides the pseudo-code for the abstract windowed forward pass. Depending on the intraLayerWindow and interLayerWindow functions, we propose three windowing algorithms. We employ timers to manage windowing with a 10ms coalescing interval, ensuring that timer threads are not overwhelmed.\nIn the Tumbling Windowed approach, each forward node and reduce destination are allocated a window of a specific duration. This naturally enables the batching of intra and inter-layer computations based on the frequency of the corresponding cascades during its interval. By adjusting the window interval, we can effectively control the latency overheads of this method.\nShifts in dynamic patterns can cause some edges to become highly active for a specific duration. For instance, a concert might lead to a sudden surge in merchandise sales. Such phenomena can introduce workload skews in sub-operators, even when using tumbling windowing. To address this, we propose Session Windowing. In this approach, the aforementioned functions are evicted after a certain, fixed period of inactivity. This algorithm is similar to the tumbling one, with the primary distinction being that adding a vertex already in a window further postpones its eviction time.\nNodes in the real world can exhibit varied and evolving frequencies. Therefore, the inactivity duration considered a \"session\" for a specific node is dynamic. To account for this, we introduce Adaptive Session Windowing, where session intervals are determined based on the windowed exponential mean of past frequencies. To enable low-storage computations of the windowed exponential"}, {"title": "4.3 GNN Training", "content": "Aside from causing load imbalance, external workloads occasionally change the distribution of \"true\" representations. This phenomenon is more commonly referred to as \"concept drift.\" To stay abreast of these changes, model re-training becomes necessary. In this section, we describe our approach in D3-GNN that allows for this re-training on the same cluster while preventing staleness.\nThe distributed training must be coordinated to ensure there are no inconsistencies when performing the backward pass during streaming graph updates. Gradients sent back through the computation graph will become invalid if the topology changes during this time. Furthermore, after the training is concluded, intermediate node embeddings and aggregators need to be recalculated to mirror the updated model.\nTo address this, we introduce a specialized, fault-tolerant Training Coordinator process within the job manager. This process oversees the entire GNN training life-cycle, which includes initiating and terminating the distributed training loop, computing epochs and batch sizes, and averting staleness.\nIn GNN training, we append an output GraphStorage operator following the final embedding layer. This operator captures the final node representation, true labels, and a loss function (L). During job definition, the loss function is seamlessly integrated into the"}, {"title": "4.3.1 Coordination of distributed training", "content": "The distributed sub-operators of the final layer need to coordinate to determine the start and end times for their training. To initiate this process, we employ a majority-voting mechanism, wherein the coordinator begins the training loop once more than half of the available output sub-operators signal the StartTraining command.\nThe decision to start training, made by the output sub-operators, could be either periodic (e.g., in D3-GNN, it is triggered when a pre-defined batch size is reached in the final Storage operator) or adaptive (e.g., based on test performance).\nUpon entering the training mode, the coordinator halts the Splitter from consuming external updates. Thanks to its streaming design, such stoppage gradually cascades back to the stream source (Dataset), preventing memory bloating issues for prior operators. It is important to note that iterative messages still flow freely during the training phase.\nThe asynchronous pipeline implies that there might still be in-flight (unprocessed) events flowing through the pipeline. To flush out these remaining messages, the coordinator employs a termination detection algorithm as described in Section 5. Correctly flushing the pipeline, combined with the halt of external updates, ensures that staleness issues do not arise during backpropagation.\nLastly, before initiating the training loops, the output sub-operators share their training data sizes, aggregate them, and then determine a batch size. We choose epochs to be static during the definition of the pipeline."}, {"title": "4.3.2 Distributed backpropagation", "content": "Once the computation graph is frozen, backpropagation begins in each logical partition of the final (output) layer:\n(1) Fetch the prediction layer inputs for each train label from the current batch (node embeddings for node-based tasks or source and destination node embeddings for edge-based tasks).\n(2) Perform predictions and evaluate the loss function L."}, {"title": "4.3.3 Model synchronization and forward pass", "content": "In the first layer, instead of sending back gradients, the system starts the model update and forward pass cycle to recompute up-to-date embedding representations and AGGREGATOR states. This procedure is similar to the streaming forward pass. However, since our graph is now static (external updates are halted due to buffering the incoming graph stream), we introduce several optimizations with layer-by-layer computations in three synchronous phases:\nPhase 1 (Model Update). Since our model is distributed across sub-operators, the gradients and model parameters must be synchronized after training. Each distributed model runs its local optimizer (e.g., SGD, Adam, Adamax) to update its model parameters. Vertex embeddings are also updated if trainable (i.e., if $x_v$ are not received as input), which triggers their replica synchronization. Once completed, each sub-operator broadcasts its local parameters"}, {"title": "4.4 Streaming Graph Partitioning", "content": "The above inference and training methodologies allow for incremental updates and caching within logical parts alongside master/replica synchronization steps. We now present our partitioning scheme to support the distributed execution of our hybrid-parallel pipeline. In particular, we discuss how we optimize the latency of a streaming partitioner operator for our purposes, and how this operator is used to assign as well as re-scale parts.\nWhen distributing the workload, the Partitioner operator identifies the correct destination sub-operators by assigning part numbers to the incoming stream data."}, {"title": "4.4.1 Distributed partitioner logic", "content": "We build D3-GNN to utilize any streaming partitioning algorithm within its Partitioner operator. In our implementation, we utilize HDRF, CLDA and Random vertex-cut streaming partitioners. Distributing those requires a shared-memory model for storing partial degree and partition tables, which is not supported by Flink. Without it, a single thread needs to be allocated to the partitioner, which causes a significant bottleneck when scaling up the system. Hence, we develop a novel Partitioner operator to support correct, concurrent thread distribution for streaming partitioners. It distributes the main partitioning logic among arbitrary number of threads while having"}, {"title": "4.4.2 Re-scaling logical parts", "content": "Assigning physical partitions alone does not allow flexible re-scaling of Graph Storage operators (e.g., if the number of physical partitions changes due to failure), nor does it support different parallelisms across the chained Graph Storage operators (e.g., to better cope with the exponential load induced by neighborhood explosion). To tackle this issue, we define the total number of available parts (num_partitions) to be the same as the maximum possible parallelism of the system (max_parallelism), while actually partitioning the graph events using keyBy(operator.part). In other words, the streaming Partitioner assigns only logical parts while the physical part is computed using a hash of the assigned logical part. As a consequence, multiple logical parts may map to the same sub-operator. Flink treats the logical parts (keys) in complete isolation; each part maintains its own context (state tables, timers, etc).\nOperators store data (state) in two ways: Operator State stores data for a given sub-operator, which can be accessed by all elements arriving at sub-operator, while Keyed State stores data at the granularity of a unique key, and each arriving element can only access data that is assigned to its particular key. Upon re-scaling D3-GNN, Operator State is either randomly redistributed to new sub-operators, or broadcast (in entirety) to all remaining sub-operators to then perform recovery logic. Keyed State, however, is distributed to the new sub-operator containing that key. Our flexible mapping of keys to sub-operators allows for re-scaling of the physical partitions based on availability, and a fixed hash function (for logical to physical parts) guarantees fault-tolerant recovery. Hence, we are able to delegate the fault tolerance logic to Flink and ensure state redistribution and correct operation even under variable parallelisms.\nWhen operator's parallelism gets closer to max_parallelism, some sub-operators may remain constantly idle due to never being"}, {"title": "5 SYSTEM COMPONENTS AND OPTIMIZATIONS", "content": "This section describes the system-level functionalities and optimizations we introduce within D3-GNN to further facilitate streaming GNN pipeline."}, {"title": "5.1 Communication", "content": "To enhance communication efficiency within D3-GNN, we introduce custom serializers for frequently used data types, including vertex, edge, and remote method invocation. Additionally, for tensor serialization, we employ compression techniques. We also introduce a selectiveBroadcast primitive, which enables broadcasting an event to specific portions of the graph. For instance, replicating a vertex from a master to its replicas. This method circumvents the repeated serialization typically found in P2P communication, thus conserving computational resources.\nTo ensure fault-tolerant, iterative communication in D3-GNN, we use in-memory, SPSC, array-based queues. We apply a unified IterationHead logic to wrap operators with terminal feedback edges, which allows for thread-safe event consumption without racing with external ones. Events directed to specific head-operators are collected in separate IterationTail operators, which are co-located with their respective heads to maintain their queues. Our iteration model can handle nested, multi-layered iterations. Fault-tolerance is achieved by stopping queue consumption from heads and resuming it from tails after checkpointing in-queue messages."}, {"title": "5.2 Storage", "content": "D3-GNN involves a custom in-memory storage backend which takes advantage of unboxed data structures. The backend uses two adjacency lists (one for in-edges and one for out-edges) to store edges. A task-manager-local storage option is provided to avoid duplicating data in the cluster. This serves for storing vertex master tables and any other global data structures like CountMinSketch for Adaptive Windowing plugin.\nDealing with tensor garbage collection at large scales could result in excessive memory consumption and potential heap overflows, primarily because their memory is allocated outside the JVM. To counteract this challenge, we've established a per-thread tensor"}, {"title": "5.3 Termination Detection", "content": "Introducing nested iterations in D3-GNN requires a valid distributed termination detection algorithm. For that, similar to TrainingCoordinator (Section 4.3), we develop a TerminationCoordinator. It periodically collects termination states from all iteration \"heads\" and proceeds with regular dataflow termination once all are ready to be terminated. The \"heads\" are ready to be terminated if they have not received events since the last collection and if they have not got scheduled timers. Last condition is necessary to avoid staleness in window-based inference methods (Section 4.2.4). This algorithm is also used to flush the pipeline in the case of GNN training (Section 4.3)."}, {"title": "6 PERFORMANCE EVALUATION", "content": "Datasets. To test the performance of D3-GNN on streaming graphs, we use five datasets: sx-superuser [35], reddit-hyperlink [24], stackoverflow [35], ogb-products [5] and wikikg90Mv2 [19]. The data is treated as an incoming stream of edge addition and feature update events to the graph, ordered by the edge timestamps. sx-superuser is temporal network of user interactions on a stack exchange website. It contains 1.4M edges and 200k nodes. reddit-hyperlink dataset contains an edge-list of directed subreddit mentions derived from the Reddit Social Network, with 286K edges and 36K nodes. stackoverflow dataset has question-answers and comments from the StackOverflow social network, with 63.5M edges and 2.6M nodes. ogb-products dataset is a co-purchasing network from Amazon with 62M edges and 2.6M nodes. wikikg90Mv2 is a knowledge-graph dataset spanning 601M edges and 91M nodes. Edge deletion events are also supported in D3-GNN but are not present in the datasets evaluated.\nExperimental setup and baselines. Experiments are executed on a Slurm cluster with 10 machines, where each machine contains Xeon E5-2660 v3 @ 2.6 GHz (20 cores/40 threads) and 64GB RAM. We use Apache Flink and Deep Java Library with PyTorch as our primary ML framework.\nThe generated graph representations from the temporal network datasets in the experiments can be further used in a wide range of applications, including fraud detection, social modelling and recommendations by adding an output layer to the final representations. Hence, we focus on generating streaming node representations and build a distributed 2-layer, GraphSAGE model with 64 output dimensions using D3-GNN. As a baseline, we employ the distributed version of DGL [58] with our enhancements to emulate our incremental algorithm described in Section 4.2. Since DGL does not support dynamic edge additions, we label each graph edge with a timestamp. For each edge, we simulate topology updates in DGL"}, {"title": "7 CONCLUSION", "content": "In this work, we introduced D3-GNN, the first distributed, hybrid-parallel system optimized for GNN inference and training in the face of streaming graph updates. Diving into the relatively untouched domain of online query settings for GNNs, D3-GNN excels by incrementally maintaining node embeddings with minimal latency and ensuring fault-tolerant graph data management. We demonstrated the strong scalability of the system at higher parallelism, with high throughput and low runtime. Furthermore, D3-GNN introduced several algorithmic and systems optimizations, such as improving load balance by intra-layer and inter-layer windowing that reduced runtime by x10 and communication by x7 in our experiments. Significant improvements over potential alternative designs, including DGL, also highlight the contributions of D3-GNN in handling streaming GNN workloads that require near real-time processing. Lastly, the introduction of a stale-free, synchronous training algorithm by D3-GNN underscores its potential in the machine learning landscape, eliminating the need for separate training environments and addressing bursty resource provisioning challenge."}]}