{"title": "Learning Fused State Representations for Control from Multi-View Observations", "authors": ["Zeyu Wang", "Yao-Hui Li", "Xin Li", "Hongyu Zang", "Romain Laroche", "Riashat Islam"], "abstract": "Multi-View Reinforcement Learning (MVRL) seeks to provide agents with multi-view observations, enabling them to perceive environment with greater effectiveness and precision. Recent advancements in MVRL focus on extracting latent representations from multiview observations and leveraging them in control tasks. However, it is not straightforward to learn compact and task-relevant representations, particularly in the presence of redundancy, distracting information, or missing views. In this paper, we propose Multi-view Fusion State for Control (MFSC), firstly incorporating bisimulation metric learning into MVRL to learn task-relevant representations. Furthermore, we propose a multiview-based mask and latent reconstruction auxiliary task that exploits shared information across views and improves MFSC's robustness in missing views by introducing a mask token. Extensive experimental results demonstrate that our method outperforms existing approaches in MVRL tasks. Even in more realistic scenarios with interference or missing views, MFSC consistently maintains high performance.", "sections": [{"title": "1. Introduction", "content": "In robotic manipulation tasks, acquiring accurate 3D scene information, including understanding target position, orientation, occlusions, and stacking relationships among objects in complex environments, is crucial for effective grasping and interaction with objects (Ze et al., 2023; Akinola et al., 2020). However, directly using 3D inputs is often bottlenecked by high data collection costs and increased computational complexity. MVRL has recently emerged as a promising alternative, leveraging 2D observations from multiple perspective cameras to enhance spatial perception while reducing computational demands (Li et al., 2019). Despite these advantages, MVRL still struggles with two common challenges: i) the difficulty in learning and fusing compact representation from redundant and high-dimensional multi-view observations, and ii) the problem of dealing with missing or interfering views in real-world scenarios.\nChen et al. (2021) averages keypoint estimations from different camera views to obtain a single world coordinate prediction. Hwang et al. (2023) learns distinct representations for each view using a VAE-based architecture and derives the final fused representation for MVRL via a weighted fusion mechanism. Jangir et al. (2022) introduces the use of a cross-view attention mechanism, offering a more flexible approach for representation fusion. However, these multi-view representation learning methods lack explicit mechanisms to focus on task-relevant features, often incorporating irrelevant information into the final representation. Recent works on bisimulation metric learning (Castro et al., 2021; Zhang et al., 2021; Zang et al., 2022) capture task-relevant representations by measuring behavioral similarities between states based on their rewards and transition distributions, and appear to be a promising solution to mitigate this issue.\nIn this work, we integrate the self-attention mechanism with bisimulation metric, offering a powerful approach to multi-view representation learning and fusion. Inspired by BERT (Devlin, 2018) and ViT (Dosovitskiy, 2020), a learnable fusion [state] embedding for MVRL tasks is introduced to avoid biases from fixed views, and dynamically capture the globally optimal fusion of multi-view information. By aligning the fusion of multi-view representations based on the similarity of task-relevant information, we enable robust multi-view integration and efficient control. However, bisimulation metric learning heavily depends on rewards to fuse state representations, which may cause it to overlook fine-grained information or interactions unique to each view that are not directly reflected in the reward signal. Secondly, bisimulation assumes complete and high-quality observations, making it vulnerable to scenarios where observations are incomplete, as it lacks mechanisms to compensate for missing views.\nTo overcome these limitations, we propose an enhanced approach that integrates multiview-based masking and latent reconstruction along with bisimulation metric learning."}, {"title": "2. Related Work", "content": "Multi-View Representation Learning. Multi-view representation learning uses multiple sources (or views) of a shared context. These views may include multiple camera views of the same scene (as in our work), as well as multi-modal inputs (e.g., audio and video) or synthetic views of the unimodal measurements (e.g., time-sequenced data). Li et al. (2018) categorized existing methods into two approaches: representation alignment and representation fusion. Representation alignment aims to capture the relationships among multiple different views through feature alignment, such as minimizing the distance between representations of different views (Feng et al., 2014; Li et al., 2003), maximizing similarity between views (Bachman et al., 2019; Frome et al., 2013), and maximizing the correlation of variables across views (Andrew et al., 2013). Representation fusion integrates representations from different views to form a compact representation for downstream tasks (Geng et al., 2022; Xie et al., 2020; Karpathy & Fei-Fei, 2015). Both strategies aim to harness the complementary information provided by multiple views for more comprehensive data representation. In this work, we propose a multi-view representation fusion method designed for control. Our approach leverages the properties of sequential decision-making tasks and multi-view data to achieve task-relevant and robust representation fusion.\nMulti-View Reinforcement Learning. Effective state representation learning in MVRL aims to transform high-dimensional observations into a compact latent space. Li et al. (2019) propose a VAE-based algorithm that minimizes Euclidean distance between states encoded from different views, assuming a consistent primary view. Keypoint3D (Chen et al., 2021) uses 3D reconstruction to learn keypoints from third-person views, requiring camera calibration. Lookcloser (Jangir et al., 2022) introduces a cross-view attention mechanism for aggregating egocentric and third-person representations without calibration, though at higher computational cost. F2C (Hwang et al., 2023) uses conditional variational information bottlenecks (CVIBs) to model state space, showing robustness to missing views. MV-MWM (Seo et al., 2023b), based on Dreamer-v2 (Hafner et al., 2021), applies dual masking and pixel-level reconstruction tasks during pretraining. Since MV-MWM relies on expert demonstrations, we did not include a direct comparison. MVD (Dunion & Albrecht, 2024) employs contrastive learning to disentangle multi-view representations for control tasks. For a detailed comparison of MFSC with other algorithms, refer to Table 1 and Appendix B.2."}, {"title": "3. Preliminaries", "content": "This section establishes the foundational concepts underlying our method, i.e., the Multi-View Markov Decision Process (MV-MDP) and the bisimulation metric."}, {"title": "3.1. Multi-View Markov Decision Processes", "content": "A Multi-View Markov Decision Process is defined as the following tuple M = (S, A, \u00d5, P, N, R, \u03b3). S represents the set of states s in the environment, A is a set of actions a, O = {OK}K_1 represents the set of K observations \u00f5 = {ok}K_1. With the assumption that each multi-view observation \u00f5\u2208 \u00d5 uniquely determines its generating state s\u2208 S, we can obtain the latent state regarding its multi-view observation by a projection function \u03c6 (\u00f5) : \u00d5 \u2192 S. Therefore, s and \u03c6 (\u00f5) can be used interchangeably. P(s'|s,a) = Pr(st+1 = s'|st = s,at = a) is the transition dynamics distribution. The corresponding transition function under the multi-view observation space is defined \u00f5 ~ P(\u00f5'|\u00f5, a), where P (\u00f5'|\u00f5, a) = \u222b\u03a9(\u00f5'|s') P (s'|s, a) and \u03a9(\u00f5|s) = \u03a0Kk=1 Pr (ok'|st = s) is the probability distribution of joint observation. R(s, a) \u2208 R is the immediate reward function for taking action a at state s, \u03b3\u2208 [0,1) is the discount factor. The goal of the agent is to find the optimal policy \u03c0(a|s) to maximize the expected reward: $E_{s_0,a_0,...} [\\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t)]$"}, {"title": "3.2. Bisimulation", "content": "Bisimulation is used to formalize the concept of state similarity. The core principle is that two states are deemed equivalent if their future behaviors, i.e. state transitions and rewards given the same actions, are indistinguishable under the same policy. Castro et al. (2021) introduced the MICo update operator as a mathematical tool for constructing a \u03c0-bisimulation metric. This metric evaluates state similarity by focusing exclusively on the actions dictated by a specific policy \u03c0, rather than considering the entire action space.\nDefinition 3.1. Castro et al. (2021) Let M be the set of all measures over S. The MICo update operator F\u03c0 : M \u2192 M is defined as:\n$F^{\\pi} (g)(s_i, s_j) = (1 - c) \\cdot |r_i - r_{s_j}| + c \\cdot W(g)(P_{\\pi i}, P_{\\pi j}),$\nwhere $s_i, s_j \\in S, r_{\\pi i} = \\sum_{a \\in A} \\pi(a|s_i) r, P_{\\pi i} = \\sum_{a \\in A} \\pi(a|s_i) P$, and W(g) denotes the Wasserstein distance between distributions, computed with cost function g. c controls the trade-off between prioritizing immediate reward differences and accounting for long-term future state dynamics.\nTheorem 3.2. Castro et al. (2021) The MICo update operator $F^\\pi$ has a unique fixed point g."}, {"title": "4. Theoretical Analysis", "content": "MVRL introduces the challenge of dealing with observations that may contain redundant or irrelevant information across different views. To address this, we employ bisimulation metrics to capture task-relevant information from multi-view observations. This section formalizes our theoretical framework, extending the bisimulation metric from single-view RL to multi-view setting and demonstrating its effectiveness in simplifying MVRL tasks."}, {"title": "4.1. Bisimulation Metric in Multi-View Setting", "content": "Formally, we define the bisimulation metric for policy \u03c0 on a multi-view setting as:\n$F_{\\pi} G_{\\pi}(\\tilde{o}_i, \\tilde{o}_j) =(1 \u2212c) \\cdot |r_{\\pi \\tilde{o}_i}- r_{\\pi \\tilde{o}_j}|$\n$+cE_{s' ~ \\tilde{P}(\\cdot |\\tilde{o}_i, a)} [G_{\\pi}(\\phi(\\tilde{o}'_i), \\phi(\\tilde{o}'_j))],$\nwhere $\\tilde{o}_i, \\tilde{o}_j \\in O^1 \\times O^2 \\times ... \\times O^K$, and the operator $F_{\\pi}$ has a unique fixed point G. Learning the exact bisimulation metric is computationally challenging. In this paper, we employ a \"base metric\u201d by setting $G_{\\pi}$ as a simple cosine distance and guide the representation learning, a learnable aggregator \u03c6, to align with the behavioral pattern demonstrated in Eq.(1). Consequently, Eq.(1) is rewritten as:\n$F_{\\pi} G_{\\pi} (\\phi (\\tilde{o}_i), \\phi (\\tilde{o}_j)) = (1 \u2212 c) \\cdot |r_{\\pi \\phi(\\tilde{o}_i)}- r_{\\pi \\phi(\\tilde{o}_j)}|$\n$+ c \\cdot E_{s' ~ \\tilde{P}} [G_{\\pi} (\\phi(\\tilde{o}'_i), \\phi(\\tilde{o}'_j))].$\nThe aggregator \u03c6 maps multi-view observations into a more compact space of summarizations Z, defined as: \u03c6 : $O^1 \\times O^2 \\times ... \\times O^K \\rightarrow Z \\in R^d$. By clustering observations to be similar under the bisimulation metric, the original MVRL problem can be approximated as solving a latent MDP M = (Z, A, $P_Z$, \u03a9, R, \u03b3, $P_0$)."}, {"title": "4.2. Latent MDP Construction Assumptions", "content": "We establish the following assumption to support the construction of the latent MDP:\nAssumption 4.1. There exists a latent space Z with dimension lower than that of $O^1 \\times O^2 \\times ... \\times O^K$, and a bounded approximation error \u03b7 > 0, such that the induced latent MDP M = (Z, A, $P_Z$, \u03a9, R, \u03b3, $P_0$) satisfies:\n1. Markovian Property: $P(z'|z, a) = \\int_{Z'} P(\\tilde{o}'|\\tilde{o},a)$ for all $\\tilde{o} = (o^1, o^2, ..., o^K) \\in O^1 \\times O^2 \\times \\cdot\\cdot\\cdot \\times O^K$ such that \u03c6(\u00f5) = z \u2208 Z, and same for R.\n2. Value Function Approximation: For every $\\tilde{o} \\in O$, there exists z \u2208 Z such that $|V^*(\\tilde{o}) \u2013 V^*(z)| \\leq \\eta$.\nThis assumption ensures that Z retains sufficient information for value estimation while reducing redundancy."}, {"title": "4.3. Bounding Value Function Differences", "content": "To quantify the impact of the above transformation, we derive the following lemma:"}, {"title": "5. Learning Fused State Representations", "content": "In this section, we detail how the aggregator \u03c6 is constructed to produce fused state representations. Fig.1 outlines the overall framework, which comprises two key components: (i) Self-Attention Fusion Module, which integrates bisimulation metrics to guide the aggregator in extracting task-relevant features from multi-view observations; and (ii) Multiview-based Masking and Latent Resconstruction, which serves as an auxiliary task to further promote cross-view state aggregation."}, {"title": "5.1. Self-Attention Fusion Module", "content": "In this section, we will introduce how bisimulation can be incorporated into multi-view representation learning and aggregation. Specifically, our method comprises two core submodules: (i) Convolutional Feature Embedding, generating embeddings of the original high-dimensional multi-view observations; (ii) Self-Attention Fusion Module, integrating multi-view representations based on bisimulation metric.\nConvolutional Feature Embedding. The feature encoding module uses a Convolutional Neural Network (CNN) encoder to encode single-view image observations into fixed-dimensional embeddings. Given a multi-view observations $\\tilde{o} = {o^1,o^2,...,o^k}$, where $o^i \\in R^{H \\times W \\times C^i}$, the CNN encodes each image into a single-view representation $x^i$, where $x^i \\in R^d$.\nSelf-Attention Fusion Module. Similar to the [class] token used in BERT (Devlin, 2018) and ViT (Dosovitskiy, 2020), we prepend a learnable fusion [state] embedding $x^0 \\in R^d$ to the sequence of multi-view embedded representations. The state fusion representation $x^0$, trained via self-attention and the bisimulation metric, serves as the final fused representation of multi-view observations and is used for downstream RL tasks. Additionally, akin to ViT, a trainable positional embedding 1D tensor $E_{pos}$ is incorporated into the multi-view embeddings to encode view-specific information:\n$Z_0 = [x^0,x^1,x^2,...,x^k] + E_{pos}, E_{pos} \\in R^{(k+1)\\times d}.$\nThe embedded sequence is then fed into the Self-Attention Fusion Module. Specifically, the Self-Attentioni Fusion Module consists of L attention layers. Each layer is composed of a Multi-Headed Self-Attention (MHSA) layer, a layer normalization (LN), and Multi-Layer Perceptron (MLP) blocks. The process can be described as follows:\n$Z_l = MHSA(LN(Z_{l-1})) + Z_{l-1}, l=1...L$\n$Z_l = MLP(LN(Z'_{l})) + Z'_{l}. l=1 ... L$\nThe output after L attention layers is $Z_L = {z_0^L, z_1^L,...,z_k^L}$, where $z^L_0$ represents the final state fusion embedding. To extract task-relevant representations from multi-view observations, bisimulation metric learning is integrated into the state fusion process. Specifically, the bisimulation metric for a given policy \u03c0 (Eq.2) employs the measurement G, as defined in SimSR (Zang et al., 2022). Here, G is based on cosine distance, which offers lower computational complexity compared to the Wasserstein distance while effectively mitigating representation collapse.\nIn RL, the critic in actor-critic algorithms like SAC (Haarnoja et al., 2018), can be decomposed into two function approximators, $\u03c8_\u03b8$ and $\u03c6_\u03c9$, with parameters \u03b8 and \u03c9 respectively: $Q_{\u03b8\u03c9} = \u03c8_\u03b8 (\u03c6_\u03c9 (s))$. Here, $\u03c8_\u03b8$ serves as the value function approximator, while $\u03c6_\u03c9$ is the state aggregator, with the goal of aligning the distances between representations to match the cosine distance. Therefore, the parameterized representation distance $G^\u03c0_\u03c6\u03c9$ can be defined as an approximation to the original observation distance $G^\u03c0$:\n$G^{\\pi} (\\tilde{o}_i, \\tilde{o}_j) \\approx G_{\\varphi_{\\omega}} (\\tilde{o}_i, \\tilde{o}_j) := 1 - cos_{\\varphi_{\\omega}} (\\tilde{o}_i, \\tilde{o}_j)$\n$= 1 - \\frac{\\varphi_{\\omega}(\\tilde{o}_i)^T \\cdot \\varphi_{\\omega}(\\tilde{o}_j)}{\\|\\varphi_{\\omega}(\\tilde{o}_i)\\| \\cdot \\|\\varphi_{\\omega}(\\tilde{o}_j)\\|}$\nTherefore, the objective of state fusion with bisimulation metric is:\n$\\mathcal{L}_{fus} = E_{(\\tilde{s}_i,r,a,\\tilde{s}'),(\\tilde{s}_j,r,a,\\tilde{s}'_j) \\sim \\mathcal{D}} (G_{\\varphi_{\\omega}} (\\tilde{s}_i, \\tilde{s}_j) - G_{\\pi})^2,$\nwhere\n$\\begin{cases}\nG_{\\pi} = r_{\\pi \\tilde{o}_i} - r_{\\pi \\tilde{o}_j} + \\gamma G_{\\varphi_{\\omega}} (\\tilde{o}'_i, \\tilde{o}'_j)\\\\\n\\tilde{s}' \\sim P (\\cdot | \\varphi_{\\omega} (\\tilde{s}), a), \\tilde{s}' \\sim P (\\cdot | \\varphi_{\\omega} (\\tilde{s}), a)\n\\end{cases}$\nP is latent state dynamics model and D is the replay buffer. For detailed explanations of the latent state dynamics model and the reward scaling mechanism, please refer to the Appendices C.2 and C.3. By incorporating bisimulation metrics during state aggregation, our method is able to focus on the causal features that directly influence rewards, effectively integrating information from multi-view observations."}, {"title": "5.2. Multiview-based Masking and Latent Reconstruction", "content": "Although bisimulation learning provides an effective approach to aggregating task-relevant information across multiple views, it overlooks the local details within each view, as it focuses on global state transitions and task reward rather than the details of each individual view. To address these challenges, we introduce a multiview-based masking and latent reconstruction auxiliary task, fostering the learning of cross-view dependencies. The masking and reconstruction mechanism strengthens bisimulation's capacity to capture richer multi-view information, enabling more robust fusion and control.\nSpecifically, we randomly mask a portion of the original multi-view image observations $ \\{o^1,o^2,...,o^k\\} $. The masked observation sequences $ \\{\\tilde{o}^1, \\tilde{o}^2, ..., \\tilde{o}^k \\} $ are then processed through the CNN Encoder and the Self-Attention Fusion Module, resulting in a set of masked state embeddings $ \\{ \\hat{z}^L_0, \\hat{z}^L_1, ..., \\hat{z}^L_k \\} $. Motivated by the success of SimSiam (Chen & He, 2021) in self-supervised learning, we use an asymmetric architecture to calculate the distance between the reconstructed latent states and the target states. The state embeddings of the masked observations are passed through a prediction head to get the final reconstructed/predicted states $ \\{ \\acute{z}^L_0, \\acute{z}^L_1, ..., \\acute{z}^L_k \\} $. We construct the reconstruction loss using cosine similarity, ensuring that the final predicted result closely approximates its corresponding target. The final objective function of masking and latent reconstruction can be formulated as:\n$\\mathcal{L}_{rec} = \\frac{1}{k+1} \\sum_{i=0}^{k} \\frac{\\acute{z}^L_i \\cdot \\hat{z}^L_i}{\\|\\acute{z}^L_i\\| \\|\\hat{z}^L_i\\|}.$\nThe masking and latent reconstruction serves as an auxiliary task and is optimized together with the multi-view state fusion module. Thus, the overall loss function of MFSC is:\n$\\mathcal{L}_{mfsc} = \\mathcal{L}_{fus} + \\lambda \\mathcal{L}_{rec}$\n\u03bb represents the weighting coefficient for the two loss functions."}, {"title": "6. Experiment", "content": "Through our experiments, we aim to investigate the following questions: (1) How does the performance of MFSC compare to existing MVRL algorithms? (2) Has MFSC successfully captured task-relevant representations? And, how does the Self-Attention Fusion Module operate across different views? (3) How does the MFSC perform in the presence of noise interference? (4) To what extent can MFSC handle tasks with missing views? (5) What is the relative importance and contribution of each part of MFSC?"}, {"title": "7. Discussion", "content": "Limitations and Future Work. One limitation of our approach lies in its reliance on masking techniques to handle missing views. For certain views containing crucial information, accurately reconstructing the true environmental state remains challenging, even when inferring from other observations. This is due to the incomplete complementarity of information between multiple views, particularly in scenarios involving complex state transitions or occlusions. To address this limitation, future research could explore the integration of state-space models (Lee et al., 2020) to better capture temporal dependencies, enabling more accurate state estimation even in the absence of certain views.\nConclusion. We propose a novel framework, Multi-view Fusion State for Control (MFSC), to address the challenge of representation learning in MVRL. MFSC integrates the self-attention mechanism with bisimulation metric learning, guiding the fusion of task-relevant representations from multi-view observations. Additionally, we introduce an auxiliary task, multiview-based masking and latent reconstruction, to capture cross-view information and enhance robustness to missing views. Experimental results demonstrate that MFSC effectively aggregates task-relevant details and shows robustness in scenarios with missing views. Finally, visualization analyses confirm the capability of MFSC to capture task-relevant information and dynamically fuse multi-view observations."}, {"title": "Impact Statement", "content": "The proposed MFSC framework advances MVRL by tackling key challenges in learning task-relevant representations from multi-view observations. By integrating bisimulation metric learning with a masking-based latent reconstruction auxiliary task, MFSC enhances robustness and adaptability in environments with redundant, distracting, or incomplete inputs. These improvements are particularly beneficial for applications such as autonomous robotics, self-driving systems, and intelligent surveillance, where robust perception under real-world uncertainties is critical.\nFrom an ethical standpoint, biases in sensor placement, data modalities, or patterns of missing observations could introduce unintended disparities in learned policies. Future research should explore strategies to mitigate these biases, ensuring consistent and reliable deployment across diverse scenarios. Moreover, as reinforcement learning gains traction in safety-critical domains, rigorous validation and improved interpretability of learned policies are essential to prevent unintended consequences and enhance trustworthiness."}, {"title": "A. Proofs", "content": "Lemma 4.2. Given a latent MDP M constructed by a learned aggregator \u03c6 : $O^1 \\times O^2 \\times ... \\times O^K \\rightarrow Z$ that clusters multi-view observations in a \u025b-neighborhood. The optimal value functions of the original MDP and the latent MDP are bounded as:\n$|V^*(\\tilde{o}) \u2013 V^*(\\phi(\\tilde{o}))| \\leq \\frac{2\\varepsilon}{(1 \u2212 \\gamma)(1 \u2212 c)}$\nProof. The proof follows straightforwardly from DBC (Zhang et al., 2021). From Theorem 5.1 in Ferns et al. (2004) we have:\n$(1 \u2013 c)|V^*(\\tilde{o}) \u2013 V^*(\\phi(\\tilde{o}))| \\leq g(\\tilde{o}, d) +  \\max_{\\tilde{u} \\in O^1xO^2x...xO^K} g(\\tilde{u}, d),$\nwhere g is the average distance between a multi-view observation and all other multi-view observations in its equivalence class under the bisimulation metric d. By specifying a \u025b-neighborhood for each cluster of multi-view observations, we can replace g:\n$(1 \u2013 c)|V^* (\\tilde{o}) \u2013 V^*(\\phi(\\tilde{o}))| \\leq 2\\varepsilon + \\frac{\\gamma}{1-\\gamma} 2\\varepsilon$\n$|V^*(\\tilde{o}) \u2013 V^*(\\phi(\\tilde{o}))| : \\leq \\frac{1}{1-c} (2\\varepsilon +  \\frac{\\gamma}{1-\\gamma} 2\\varepsilon)$\n$ \\qquad \\qquad \\qquad  \\leq \\frac{2\\varepsilon}{(1 \u2212 \\gamma) (1 \u2013 c)}$\nAs \u025b \u2192 0, the optimal value function of the aggregated MDP converges to the original value function. By defining a learning error for \u03c6, $L := sup_{\\tilde{o}, \\tilde{o}_i \\in O^1xO^2x...xO^K}| ||\u03c6(\\tilde{o}_i) \u2013 \u03c6(\\tilde{o}_j)||_1 \u2013 d(\\tilde{o}_i, \\tilde{o}_j)|$, we can also update the bound in Lemma 4.2 to incorporate L : $|V^* (\\tilde{o}) - V^*(\\phi(\\tilde{o}))| \\leq \\frac{2\\varepsilon+2L}{(1-\\gamma)(1-c)}$"}, {"title": "B. Additional Related Work Discussion", "content": "In this section, we provide a more detailed discussion on state representation learning in RL and a detailed comparison between our method and other related works."}, {"title": "B.1. State Representation Learning in RL", "content": "Well-constructed state representations enable agents to better understand and adapt to complex environments, thus improving task performance and decision-making efficiency. For example, methods such as CURL (Laskin et al., 2020b) and DrQ series (Kostrikov et al., 2020; Yarats et al., 2021a), leverage data augmentation techniques such as cropping and color jittering to enhance model generalization. However, their performance is highly dependent on the specific augmentations applied, leading to variability in results. Masking-based approaches (Seo et al., 2023b; Yu et al., 2022b; Seo et al., 2023a; Liu et al., 2022), selectively obscure parts of the input to mitigate redundant information and improve training efficiency. Although these methods show promise in filtering out irrelevant data, they carry the risk of unintentionally discarding task-critical information, potentially affecting overall agent performance. Bisimulation-based methods (Zhang et al., 2021; Zang et al., 2022) focus on constructing reward-sensitive state representations to ensure that states with similar values are close in the representation space, promoting sample efficiency and consistent decision-making. Another line of research explores the causal relationships between state representations and control (Wang et al., 2022; Lamb et al., 2022; Efroni et al., 2021; 2022; Fu et al., 2021). By analyzing the causal links between states and actions, these methods aim to improve agents' understanding and control of the environment, further optimizing RL performance."}, {"title": "B.2. Comparison with Additional Related Work", "content": "MV-MWM (Seo et al., 2023b). MV-MWM proposed an RL framework that trained a multi-view masked autoencoder for representation learning and a world model to enhance the model-based RL algorithm's robustness to view variations in robotic manipulation tasks. MV-MWM adopts a dual masking strategy at both the convolutional feature and view levels, requiring an additional decoder to perform pixel-level reconstruction, which aims to recover task-specific details from raw observations. Our method applies masking at the pixel level but conducts reconstruction in the latent space, thereby avoiding"}]}