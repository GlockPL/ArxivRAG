{"title": "FPBOOST: Fully Parametric Gradient Boosting FOR Survival Analysis", "authors": ["Alberto Archetti", "Eugenio Lomurno", "Diego Piccinotti", "Matteo Matteucci"], "abstract": "Survival analysis is a critical tool for analyzing time-to-event data and extracting valuable clinical insights. Recently, numerous machine learning techniques leveraging neural networks and decision trees have been developed for this task. Among these, the most successful approaches often rely on specific assumptions about the shape of the modeled hazard function. These assumptions include proportional hazard, accelerated failure time, or discrete estimation at a predefined set of time points. In this study, we propose a novel paradigm for survival model design based on the weighted sum of individual fully parametric hazard contributions. We build upon well-known ensemble techniques to deliver a novel contribution to the field by applying additive hazard functions, improving over approaches based on survival or cumulative hazard functions. Furthermore, the proposed model, which we call FPBoost, is the first algorithm to directly optimize the survival likelihood via gradient boosting. We evaluated our approach across a diverse set of datasets, comparing it against a variety of state-of-the-art models. The results demonstrate that FPBoost improves risk estimation, according to both concordance and calibration metrics.", "sections": [{"title": "INTRODUCTION", "content": "Survival analysis is a field of statistics with a pivotal role in healthcare data analysis, providing the ability to estimate the timing and associated uncertainty of clinical events. This capability is essential for aiding physicians in making safety-critical decisions based on data. Beyond healthcare, survival analysis has found applications in various fields, such as predicting equipment failures in industry or forecasting customer churn in relationship management. This widespread adoption underscores the importance of temporal risk estimation in diverse real-world scenarios [1].\n\nThe primary objective of survival models is to construct a time-dependent function $S(t|x)$ conditioned on a set of features $x$, such as clinical indicators for hospitalized patients, known as the survival function. This function represents the probability that an event of interest has not occurred by time $t$, expressed as\n$S(t|x) = P(T > t|x)$.\nIn practical applications, the event of interest can take several forms. In healthcare contexts, for example, it may denote patient mortality, disease recurrence, or hospital discharge. As another example, in customer relationship management it might represent a client's initial purchase [2].\nTraditional survival models often rely on simplifying assumptions, such as risk proportion over between different subjects being constant over time [3]. While potentially beneficial in limited data scenarios, these simplifying assumptions can constrain model generalization [4]. Machine learning techniques have allowed to advanced these models by incorporating tree-based estimations and neural networks, significantly enhancing generalization capabilities and enabling the modeling of non-linear interactions within high-dimensional features. However, these advanced models still operate under certain constraints, such as time discretization in neural-based approaches [5] or accelerated failure time assumptions in gradient boosting methods [6].\nIn this context, we introduce Fully Parametric Gradient Boosting (FPBoost), a novel architecture designed to model hazard functions through the composition of multiple fully parametric hazard functions. Hazard functions are related to survival functions as they measure the instantaneous risk of a subject experiencing the event of interest. FPBoost aims to combine the strengths of ensemble learning with tree-based gradient boosting [7], offering a flexible model with robust generalization capabilities and minimal assumptions. Modeling hazard as a weighted sum of multiple, fully-parametric functions, referred to as heads, allows FPBoost to be trained by maximizing the survival likelihood. This, in turn, removes the need for simplified expressions such as partial likelihood [3] or discrete losses [5]. Additionally, the continuous nature of the learned survival functions allows for a fine-grained estimation of the event distribution, without the need for interpolation techniques [8]. Finally, the use of gradient-boosted trees with tabular data, which is the most common data format in survival applications, can be competitive against neural network techniques [9], as highlighted by the empirical results presented in this study.\nWe evaluate FPBoost in the right-censored, single-event setting, which is the most common application of survival analysis. The performance of FPBoost is benchmarked against state-of-the-art survival models, which include both tree-based and neural network-based models. Performance metrics include the concordance index [10] and the integrated Brier score [11]. Our experiments demonstrate that FPBoost outperforms alternative"}, {"title": "BACKGROUND AND RELATED WORK", "content": "Survival analysis is a field of statistics focused on modeling the probability of an event of interest occurring over time for a population of subjects. The primary objective of survival models is thus to estimate a survival function $S(t|x)$, which measures the probability of surviving, i.e., not experiencing the event up to time $t$ as\n$S(t|x) = P(T > t|x)$.\nHere, $T$ is the time-to-event random variable and $x \\in R^d$ a d-dimensional vector encoding the subject's features. The survival function exhibits several key properties. It is monotonically non-increasing, starts at 1 for $t = 0$, and asymptotically approaches 0 as $t$ tends to infinity indicating that, given an infinite time frame, all subjects will ultimately experience the event of interest [2].\nA core aspect of survival analysis is the ability to handle censored data. Censoring occurs when subjects do not experience the event of interest within the study period. For these individuals, we can only ascertain that they survived up to a certain time point, but their true event time remains unknown. Consequently, a survival dataset comprises set of $N$ triplets $(x_i, d_i, t_i)$, where:\n\\begin{itemize}\n    \\item $x_i$ is the feature vector for subject $i$.\n    \\item $d_i$ is a binary indicator of whether the subject experienced the event during the study (1) or was censored (0).\n    \\item $t_i$ is the observed time, corresponding to either the event time or the censoring time.\n\\end{itemize}\nThis framework is commonly referred to as right-censored, single-event survival analysis and will be the focus of this work."}, {"title": "Non-Parametric Models", "content": "Survival models can be cathegorized into three groups: non-parametric, semi-parametric, and fully parametric [2]. The group of non-parametric models comprises statistical estimators that provide information about data without any prior assumption on the event distribution. Non-parametric models rely on some notion of similarity between groups of individuals to improve the prediciton complexity. The most common non-parametric model is the Kaplan-Meier estimator [1], which is often used to plot the general survival behavior of a population. The KM estimator is defined as\n$S(t) = \\prod_{t_i \\leq t} (1 - \\frac{e_i}{r_i})$\nwhere $t_i$ are the unique observed event times, $e_i$ is the number of events at time $t_i$, and $r_i$ is the number of subjects at risk before $t_i$. Notably, the KM estimator is not conditioned on the subjects' features as it is tailored to provide aggregate information about the overall event distribution within the data. Another popular non-parametric model is the Random Survival Forest [13], which builds a set of decision trees with the CART [14] method by maximizing the event distribution difference between nodes. Each leaf contains a non-parametric estimation of the subjects corresponding to the leaf. The final prediction is obtained by averaging the predictions of the trees in the forest."}, {"title": "Semi-Parametric Models", "content": "Semi-parametric models are crucial tools in survival analysis. These models focus on predicting the hazard function, a quantity related to the survival function, which measures the instantaneous risk for subjects that have survived up to time $t$:\n$h(t|x) = \\lim_{At \\to 0} \\frac{P(t < T < t + At|T \\geq t, x)}{At}$\nThe survival function is related to the hazard function as\n$S(t|x) = exp(-H(t|x)) = exp(-\\int_{0}^{t} h(u|x)du)$\nwhere $H(t|x)$ is the cumulative hazard function, defined as the integral of the hazard function from 0 to $t$ [2]. The Cox model [3] is a prominent semi-parametric model and serves as a primary baseline for machine learning-based survival analysis. This model relies on two key assumptions: (i) linear dependency between features and risk of experiencing an event (ii) the ratio between hazard functions of different subjects is constant over time. This latter is often referred to as proportional hazard assumption. While potentially limiting when the model is applied to large datasets, these assumptions provide a strong bias, enabling effective generalization even with limited data samples. In particular, the Cox model defines the hazard function as the product of a baseline hazard, $h_0(t)$, and the exponential of a subject-dependent risk factor $r(x)$:\n$h(t|x) = h_0(t) exp(r(x)) = h_0(t) \\cdot exp(\\beta^T x)$,\nwhere $h_0(t)$ is a non-parametric hazard estimation common to all samples, such as the Breslow estimator [15]. The parameters $\\beta$ are trained using the partial log-likelihood loss:\n$L = \\frac{1}{N} \\sum_{i=1}^{N} - \\delta_i \\beta^T x_i - log \\sum_{j:t_j \\geq t_i} exp(\\beta^T x_j)$."}, {"title": "Fully Parametric Models", "content": "Fully parametric survival models estimate the entire survival function using a set of parameters. Historically, these models assumed that the event occurrence followed a particular probability distribution, such as Weibull, LogNormal, or LogLogistic with parameters $\\Theta$. Given this assumption, for right-censored single-event survival data, the distribution parameters can be estimated by maximizing the survival likelihood as\n$\\hat{\\Theta} = arg \\max_{\\Theta} \\prod_{i=1}^{N} [h(t_i|\\Theta)]^{\\delta_i} S(t_i|\\Theta)$.\nBuilding upon standard fully parametric distributions, Deep Survival Machines (DSM) [17] propose a parameter estimation neural network to construct a mixture of predefined probability distributions. The final survival function is then computed as a weighted sum of these distributions. DSM is trained using a combination of ELBO losses and a regularization prior loss in a Bayesian framework.\nOther popular models do not rely on predefined statistical distributions to construct survival estimations, but leverage neural networks to estimate the event probability directly at a fixed set of time intervals [18, 5]. These neural networks have a single output per time bin, representing the event probability for that interval. One such model is DeepHit [19], a discrete-time survival model capable of handling multiple event types. It consists of a shared feature extraction network followed by event-specific subnetworks estimating the probabilities for each event. While these discrete-time models have shown promising practical results, they may struggle with fine-grained or long-term prediction horizons due to their fixed-time nature. To address this limitation, some studies have proposed interpolation techniques between time points [5, 8]."}, {"title": "FULLY PARAMETRIC SURVIVAL BOOSTING", "content": "This work proposes FPBoost, a novel technique for survival analysis, based on the weighted sum of fully parametric hazard functions with trainable parameters. Parameter estimation is carried out via gradient boosting, optimizing the negative log-likelihood loss function.\nThe hazard function of FPBoost is composed of $J$ heads, each corresponding to a parametric distribution. In this work, heads follow either Weibull or LogLogistic distributions. However, any distribution differentiable with respect to its parameters can be used. Figure 1 depicts an example of a 4-headed FPBoost architecture with heads 1 and 2 following a Weibull distribution and heads 3 and 4 a LogLogistic distribution. As shown in Table 1, these distributions depend on two parameters: a scale parameter $\\eta$ and a shape parameter $k$. The distribution parameters are estimated from the input features $x$ using a set of regression trees. Additionally, another set of trees estimates a set of non-negative weighting parameters $w$, one for each head. The hazard function of FPBoost is defined as\n$h(t) = \\sum_{j=1}^{J} w_j h_j(t / \\eta_j, k_j)$\nwhere $w_j$ are the learned head weights and $\\eta_j, k_j$ are the parameter estimations from the regression trees for the j-th head. To improve readability, we define $\\Theta$ as the vector containing all the distribution parameters $\\eta_j, k_j$ and head weighting factors $w_j$ for each of the $J$ heads. This way, each entry of $\\Theta$ can be estimated with any regression model given input $x$. To ensure that the distribution parameters and weighting factors are non-negative, we apply a ReLU activation function to $\\Theta$, setting negative estimations to 0 [20].\nNotably, the FPBoost hazard is differentiable with respect to the distribution parameters and weights. Therefore, FPBoost is trained by minimizing the negative log-likelihood loss function, which derives from equations 1 and 2:\n$L_{lik} = - \\frac{1}{N} \\sum_{i=1}^{N} \\delta_i log(h(t_i|\\Theta)) - H(t_i|\\Theta)$.\nIn order to prevent overfitting, we add to $L_{lik}$ an ElasticNet regularization term [21] as\n$L_{reg} = \\alpha (\\gamma ||\\Theta||_1 + (1 - \\gamma) ||\\Theta||_2^2)$\nwhere $\\alpha \\geq 0$ is a weighting parameter and $\\gamma \\in [0, 1]$ controls the ratio between the L1 and L2 penalties.\nThe training is performed using a standard gradient boosting algorithm. Specifically, for each parameter $\\theta_j \\in \\Theta$, an empty list of trees is initialized. Then, at each iteration $m$, each of these lists is populated by a new tree $r_j^{(m)}(x)$ fitted on the negative gradient of the loss function, called pseudo-residual. Parameter estimation occurs by summing the estimations of all trees belonging to each list, weighted by a learning rate $\\lambda > 0$ [7]. The pseudocode of FPBoost is provided in Algorithm 1.\nWhile the idea of leveraging a mixture of parametric functions is not new in the literature [17], nor is ensemble learning [13, 22], FPBoost proposes several innovations. First, the weighted sum is applied directly to the hazard function, unlike previous works that applied it to the survival or cumulative hazard function. Given the prominence and popularity of models based on hazard assumptions, such as proportionality or accelerated failure time, we argue that directly learning parameters in hazard-space can be beneficial for effective training. Second, FPBoost directly maximizes the survival likelihood, without relying on simplified custom losses such as the partial likelihood of proportional models or discrete losses of neural-network-based models. This is possible due to the assumption that the hazard is a composition of differentiable parametric hazards. Lastly, FPBoost leverages decision tree ensembles for parameter estimation, which have been historically extremely effective in dealing with tabular data, even against neural networks and deep learning [9].\nTo the best of our knowledge, FPBoost is the first model to combine tree-based learning via gradient boosting, composition of fully parametric hazards, and direct optimization of the survival likelihood into an empirically competitive survival analysis architecture."}, {"title": "EXPERIMENTS", "content": "This section covers the experimental setup to evaluate the performance of FPBoost alongside the set of baseline survival models."}, {"title": "Datasets", "content": "To ensure fair evaluation and consistency with similar studies, we selected datasets from well-known benchmarks in survival analysis, covering different conditions like breast cancer, lung cancer, AIDS, and cardiovascular diseases. Table 2 collects the summary statistics of these datasets.\nThe AIDS [23] dataset originates from a trial comparing three-drug and two-drug regimens in HIV-infected patients. The primary event of interest was the time to an AIDS-defining event or death. The high censoring percentage resulted from the trial being terminated early after reaching a predefined level of statistical significance.\nThe Breast Cancer [24] dataset is derived from a study aimed at validating a 76-gene prognostic signature for predicting distant metastases breast cancer patients. The study includes gene expression profiling of frozen samples from 198 patients.\nThe German Breast Cancer Study Group (GBSG2) [25] dataset targets breast cancer recurrence post-treatment, evaluating hormone therapy's impact on recurrence. Collected from a randomized study in Germany, it includes covariates like age, menopausal status, tumor size, and node status.\nThe Molecular Taxonomy of Breast Cancer International Consortium (METABRIC) [4] aims to understand breast cancer through molecular taxonomy to develop personalized treatments based on tumor genetic profiles. The dataset encompasses a mix of clinical features and genomic data, with a patient cohort from Canada and UK.\nThe Study to Understand Prognoses Preferences Outcomes and Risks of Treatment (SUPPORT) [26] focuses on critically ill hospitalized patients, conducted in two phases between 1989 and 1997. It includes administrative and clinical follow-up for six months post-inclusion in the study.\nThe Veteran Administration Lung Cancer Trial (Veterans) [27] dataset focuses on lung cancer patients treated with two different chemotherapy regimens. This dataset is frequently used in simple survival benchmarks due to its small sample size.\nThe Worcester Heart Attack Study (WHAS) [28] deals with cardiovascular health, tracking 461 patients post-myocardial infarction from 1997 to 2001. It includes biometric parameters and temporal features like hospital stay length and follow-up dates."}, {"title": "Metrics", "content": "We evaluated survival models using concordance index (C-Index) and integrated Brier score (IBS). The C-Index [10] measures the predictive accuracy of survival models by evaluating the proportion of concordant pairs relative to all comparable pairs within a dataset. A pair of subjects i and j is considered comparable if, given $t_i < t_j$, then $d_i = 1$. A pair of comparable subjects is concordant when the predicted mean time aligns with the actual event times.\nThe Brier score [11] asses the calibration of probability estimates over time by computing the weighted squared difference between the binary survival indicator of a subject and the predicted survival probability. The Brier score at time t is defined as:\n$BS(t) = \\frac{1}{N} \\sum_{i=1}^{N} w_i(t) (1(t_i > t) - S(t|x_i))^2$,\nwhere $1(\\cdot)$ is an indicator function and $w_i(t)$ is a weighting factor that adjusts the censoring bias. This adjustment is the Inverse Probability of Censoring Weighting (IPCW) [10, 29], which assigns weights based on the inverse probability of censoring at a given time as\n$w_i(t) = \\begin{cases}\n\\frac{\\delta_i / G(t_i)}{\\delta_i / G(t_i)} \\text{ if } t_i \\leq t\\\\\n\\frac{1 / G(t)}{1 / G(t)} \\text{ if } t > t\n\\end{cases}$\nwhere $G(t)$ represents the Kaplan-Meier estimate of the censoring distribution, calculated over the dataset with inverted censoring indicators $\\delta$. The overall calibration of a survival model over time can be summarized by integrating the Brier score across the entire study period, yielding the Integrated Brier Score (IBS)."}, {"title": "Experimental Procedure", "content": "To ensure a robust evaluation, each dataset is divided into 10 seeded splits of equal size. All subsequent measurements are averaged over 10 independent executions. During each execution, two consecutive splits are defined as validation and test sets, with the remaining eight as the training set, following a standard 10-fold cross-validation approach. This method ensures that each data sample is considered once in the final test evaluation. Data splits are performed with stratification on the event binary variable to maintain a comparable level of event representation across different splits."}, {"title": "RESULTS AND DISCUSSION", "content": "This section presents and analyzes the empirical evaluation of FPBoost against state-of-the-art survival models described in the Survival Models Section, specifically RSF, Cox, CoxBoost, DeepSurv, DSM, and DeepHit. Tables 3 and 4 report the performance of each model according to the C-Index and IBS metrics, respectively. Additionally, Table 5 summarizes the mean performance across all datasets. For improved readability, all results and metric reports are scaled up by a factor of 100.\nThe C-Index results in Table 3 demonstrate FPBoost's competitive performance across all datasets, outperforming other models in all cases except AIDS, where it is marginally surpassed by the Cox model. On average, FPBoost improves the C-Index by 4.07 points across all datasets and models, representing an approximately 8% improvement given that a random guessing model has a C-Index of 50 [2]. When compared specifically to proportional models (Cox, CoxBoost, and DeepSurv), FPBoost's average improvement is 4.49, highlighting the potential limitations of the proportional hazard assumption in capturing complex data patterns. The performance gap widens further when comparing FPBoost to neural network-based models (DeepSurv, DSM, and DeepHit), with an average improvement of 5.90. This suggests that neural networks, despite their capacity to learn complex non-linear patterns, may require more sophisticated tuning to prevent overfitting and ensure strong generalization.\nTable 4 demonstrates FPBoost's strong calibration performance according to the IBS metric, corroborating the trends analyzed on C-Index. Here, FPBoost outperforms all baselines except in the Veterans and WHAS datasets, where it ranks as the second-best model in the former. The average IBS improvement of FPBoost across all datasets is 2.42, again representing approximately a 10% improvement, given that a random guessing model has an IBS of 25 [2]. Consistently with the C-Index results, the IBS improvement relative to proportional hazard models is 2.03, increasing to 3.43 for neural network-based models, while the difference with tree-based models is smaller at 0.70.\nIn summary, the empirical results demonstrate FPBoost's competitive performance against various state-of-the-art survival models, both tree-based and neural network-based. Performance improvements are evident in terms of both concordance and calibration. These findings suggest that FPBoost's tree-based nature, combined with direct optimization of the survival likelihood, represents a promising approach for developing complex, competitive, and adaptable survival models."}, {"title": "CONCLUSION", "content": "In this study, we introduced FPBoost, a novel survival analysis model that leverages a weighted sum of fully parametric hazard functions optimized through gradient boosting. Our approach addresses several limitations of existing survival models by avoiding restrictive assumptions such as proportional hazards, accelerated failure time, or discrete time estimations. The empirical evaluation of FPBoost across diverse datasets demonstrated its competitive concordance and calibration performance compared to state-of-the-art survival models, including both tree-based and neural network-based approaches. These results highlight the potential of combining fully parametric hazard functions with ensemble learning techniques in survival analysis, alongside direct optimization of the survival likelihood.\nFuture work could explore the incorporation of additional parametric distributions, the application of FPBoost to competing risks scenarios, and the development of interpretability techniques to provide insights into the model's predictions. Furthermore, investigating the model's performance on larger, more diverse datasets beyond the healthcare context or in real-world applications could further validate its practical utility. In conclusion, FPBoost represents a significant advancement in survival analysis, offering a flexible and accurate approach to modeling time-to-event data, aiding researchers and practitioners in risk estimation."}]}