{"title": "ADVERSARIAL SUPERVISION MAKES\nLAYOUT-TO-IMAGE DIFFUSION MODELS THRIVE", "authors": ["Yumeng Li", "Margret Keuper", "Dan Zhang", "Anna Khoreva"], "abstract": "Despite the recent advances in large-scale diffusion models, little progress has\nbeen made on the layout-to-image (L2I) synthesis task. Current L2I models either\nsuffer from poor editability via text or weak alignment between the generated im-\nage and the input layout. This limits their usability in practice. To mitigate this, we\npropose to integrate adversarial supervision into the conventional training pipeline\nof L2I diffusion models (ALDM). Specifically, we employ a segmentation-based\ndiscriminator which provides explicit feedback to the diffusion generator on the\npixel-level alignment between the denoised image and the input layout. To encour-\nage consistent adherence to the input layout over the sampling steps, we further\nintroduce the multistep unrolling strategy. Instead of looking at a single timestep,\nwe unroll a few steps recursively to imitate the inference process, and ask the\ndiscriminator to assess the alignment of denoised images with the layout over a\ncertain time window. Our experiments show that ALDM enables layout faithful-\nness of the generated images, while allowing broad editability via text prompts.\nMoreover, we showcase its usefulness for practical applications: by synthesizing\ntarget distribution samples via text control, we improve domain generalization of\nsemantic segmentation models by a large margin (~12 mIoU points).", "sections": [{"title": "INTRODUCTION", "content": "Layout-to-image synthesis (L2I) is a challenging task that aims to generate images with per-pixel\ncorrespondence to the given semantic label maps. Yet, due to the tedious and costly pixel-level\nlayout annotations of images, availability of large-scale labelled data for extensive training on this\ntask is limited. Meanwhile, tremendous progress has been witnessed in the field of large-scale\ntext-to-image (T2I) diffusion models (Ramesh et al., 2022; Balaji et al., 2022; Rombach et al.,\n2022). By virtue of joint vision-language training on billions of image-text pairs, such as LAION\ndataset (Schuhmann et al., 2022), these models have demonstrated remarkable capability of synthe-\nsizing photorealistic images via text prompts. A natural question is: can we adapt such pretrained\ndiffusion models for the L2I task using a limited amount of labelled layout data while preserving\ntheir text controllability and faithful alignment to the layout? Effectively addressing this question\nwill then foster the widespread utilization of L2I synthetic data.\nRecently, increasing attention has been devoted to answer this question (Zhang & Agrawala, 2023;\nMou et al., 2023; Xue et al., 2023). Despite the efforts, prior works have suffered to find a good\ntrade-off between faithfulness to the layout condition and editability via text, which we also empir-\nically observed in our experiments (see Fig. 1). When adopting powerful pretrained T2I diffusion\nmodels, e.g., Stable Diffusion (SD) (Rombach et al., 2022), for L2I tasks, fine-tuning the whole\nmodel fully as in (Xue et al., 2023) can lead to the loss of text controllability, as the large model\neasily overfits to the limited amount of training samples with layout annotations. Consequently, the\nmodel can only generate samples resembling the training set, thus negatively affecting its practical\nuse for potential downstream tasks requiring diverse data. For example, for downstream models\ndeployed in an open-world, variety in synthetic data augmentation is crucial, since annotated data\ncan only partially capture the real environment and synthetic samples should complement real ones.\nConversely, when freezing the T2I model weights and introducing additional parameters to accom-\nmodate the layout information (Mou et al., 2023; Zhang & Agrawala, 2023), the L2I diffusion mod-\nels naturally preserve text control of the pretrained model but do not reliably comply with the layout\nconditioning. In such case, the condition becomes a noisy annotation of the synthetic data, under-\nmining its effectiveness for data augmentation. We hypothesize the poor alignment with the layout\ninput can be attributed to the suboptimal MSE loss for the noise prediction, where the layout infor-\nmation is only implicitly utilized during the training process. The assumption is that the denoiser\nhas the incentive to utilize the layout information as it poses prior knowledge of the original image\nand thus is beneficial for the denoising task. Yet, there is no direct mechanism in place to ensure\nthe layout alignment. To address this issue, we propose to integrate adversarial supervision on the\nlayout alignment into the conventional training pipeline of L2I diffusion models, which we name\nALDM. Specifically, inspired by Sushko et al. (2022), we employ a semantic segmentation model\nbased discriminator, explicitly leveraging the layout condition to provide a direct per-pixel feedback\nto the diffusion model generator on the adherence of the denoised images to the input layout.\nFurther, to encourage consistent compliance with the given layout over the sampling steps, we pro-\npose a novel multistep unrolling strategy. At inference time, the diffusion model needs to consecu-\ntively remove noise for multiple steps to produce the desired sample in the end. Hence, the model\nis required to maintain consistent adherence to the conditional layout over the sampling time hori-\nzon. Therefore, instead of applying discriminator supervision at a single timestep, we additionally\nunroll backward multiple steps over a certain time window to imitate the inference time sampling.\nThis way the adversarial objective is designed over a time horizon and future steps are taken into\nconsideration as well. Enabled by adversarial supervision over multiple sampling steps, our ALDM\ncan effectively ensure consistent layout alignment, while maintaining initial properties of the text\ncontrollability of the large-scale pretrained diffusion model. We experimentally show the effective-\nness of adversarial supervision for different adaptation strategies (Mou et al., 2023; Qiu et al., 2023;\nZhang & Agrawala, 2023) of the SD model (Rombach et al., 2022) to the L2I task across different\ndatasets, achieving the desired balance between layout faithfulness and text editability (see Table 1).\nFinally, we demonstrate the utility of our method on the domain generalization task, where the se-\nmantic segmentation network is evaluated on unseen target domains, whose samples are sufficiently\ndifferent from the trained source domain. By augmenting the source domain with synthetic images\ngenerated by ALDM using text prompts aligned with the target domain, we can significantly en-\nhance the generalization performance of original downstream models, i.e., ~ 12 mIoU points on the\nCityscapes-to-ACDC generalization task (see Table 4)."}, {"title": "RELATED WORK", "content": "The task of layout-to-image synthesis (L2I), also known as semantic image synthesis (SIS), is to\ngenerate realistic and diverse images given the semantic label maps, which prior has been studied\nbased on Generative Adversarial Networks (GANs) (Wang et al., 2018; Park et al., 2019; Wang\net al., 2021; Tan et al., 2021; Sushko et al., 2022). The investigation can be mainly split into two\ngroups: improving the conditional insertion in the generator (Park et al., 2019; Wang et al., 2021;\nTan et al., 2021), or improving the discriminator's ability to provide more effective conditional su-\npervision (Sushko et al., 2022). Notably, OASIS (Sushko et al., 2022) considerably improves the\nlayout faithfulness by employing a segmentation-based discriminator. However, despite good lay-\nout alignment, the above GAN-based L2I models lack text control and the sample diversity heavily\ndepends on the availability of expensive pixel-labelled data. With the increasing prevalence of dif-\nfusion models, particularly the large-scale pretrained text-to-image diffusion models (Nichol et al.,\n2022; Ramesh et al., 2022; Balaji et al., 2022; Rombach et al., 2022), more attention has been de-\nvoted to leveraging pretrained knowledge for the L2I task and using diffusion models. Our work\nfalls into this field of study.\nPITI (Wang et al., 2022) learns a conditional encoder to match the latent representation of\nGLIDE (Nichol et al., 2022) in the first stage and finetune jointly in the second stage, which un-\nfortunately leads to the loss of text editability. Training diffusion models in the pixel space is ex-\ntremely computationally expensive as well. With the emergence of latent diffusion models, i.e., Sta-\nble Diffusion (SD) (Rombach et al., 2022), recent works (Xue et al., 2023; Mou et al., 2023; Zhang\n& Agrawala, 2023) made initial attempts to insert layout conditioning into SD. FreestyleNet (Xue\net al., 2023) proposed to rectify the cross-attention maps in SD based on the label maps, while it also\nrequires fine-tuning the whole SD, which largely compromises the text controllability, as shown in\nFigs. 1 and 4. On the other hand, OFT partially updates SD, T2I-Adapter (Mou et al., 2023) and\nControlNet (Zhang & Agrawala, 2023) keep SD frozen, combined with an additional adapter to ac-\ncommodate the layout conditioning. Despite preserving the intriguing editability via text, they do\nnot fully comply with the label map (see Fig. 1 and Table 1). We attribute this to the suboptimal\ndiffusion model training objective, where the conditional layout information is only implicitly used\nwithout direct supervision. In light of this, we propose to incorporate the adversarial supervision\nto explicitly encourage alignment of images with the layout conditioning, and a multistep unrolling\nstrategy during training to enhance conditional coherency across sampling steps.\nPrior works (Xiao et al., 2022; Wang et al., 2023b) have also made links between GANs and diffusion\nmodels. Nevertheless, they primarily build upon GAN backbones, and the diffusion process is\nconsidered as an aid to smoothen the data distribution (Xiao et al., 2022), and stabilize the GAN\ntraining (Wang et al., 2023b), as GANs are known to suffer from training instability and mode\ncollapse. By contrast, our ALDM aims at improving L2I diffusion models, where the discriminator\nsupervision serves as a valuable learning signal for layout alignment."}, {"title": "ADVERSARIAL SUPERVISION FOR L21 DIFFUSION MODELS", "content": "L2I diffusion model aims to generate images based on the given layout. Its current training and\ninference procedure is inherited from unconditional diffusion models, where the design focus has\nbeen on how the layout as the condition is fed into the UNet for noise estimation, as illustrated in\nFig. 2 (A). It is yet under-explored how to enforce the faithfulness of L2I image synthesis via direct\nloss supervision. Here, we propose novel adversarial supervision which is realized via 1) a semantic\nsegmenter-based discriminator (Sec. 3.1 and Fig. 2 (B)); and 2) multistep unrolling of UNet (Sec. 3.2\nand Fig. 2 (C)) to induce faithfulness already from early sampling steps and consistent adherence to\nthe condition over consecutive steps."}, {"title": "DISCRIMINATOR SUPERVISION ON LAYOUT ALIGNMENT", "content": "For training the L2I diffusion model, a Gaussian noise \u0454 ~ N(0, I) is added to the clean variable xo\nwith a randomly sampled timestep t, yielding xt:\n$xt = \\sqrt{\\alpha_t}x_0 + \\sqrt{1 - \\alpha_t}\\epsilon$,\nwhere \u03b1t defines the level of noise. A UNet (Ronneberger et al., 2015) denoiser eo is then trained to\nestimate the added noise via the MSE loss:\n$L_{noise} = E_{\\epsilon\\sim N(0,I), y, t} [||\\epsilon - \\epsilon_{\\theta}(x_t, y, t) ||^2] = E_{\\epsilon, x_0, y, t} [||\\epsilon - \\epsilon_{\\theta}(\\sqrt{\\alpha_t}x_0 + \\sqrt{1 - \\alpha_t}\\epsilon, y, t) ||^2]$.\nBesides the noisy image xt and the time step t, the UNet additionally takes the layout input y. Since\ny contains the layout information of 20 which can simplify the noise estimation, it then influences\nimplicitly the image synthesis via the denoising step. From xt and the noise prediction \u20ac\u03b8, we can\ngenerate a denoised version of the clean image as:\n$\\hat{x}^{(t)}_{0} = \\frac{x_t - \\sqrt{1 - \\alpha_t}\\epsilon_{\\theta}(x_t, y, t)}{\\sqrt{\\alpha_t}}$.\nHowever, due to the lack of explicit supervision on the layout information y for minimizing Lnoise,\nthe output often lacks faithfulness to y, as shown in Fig. 3. It is particularly challenging when y\ncarries detailed information about the image, as the alignment with the layout condition needs to be\nfulfilled on each pixel. Thus, we seek direct supervision on to enforce the layout alignment. A\nstraightforward option would be to simply adopt a frozen pre-trained segmenter to provide guidance\nwith respect to the label map. However, we observe that the diffusion model tends to learn a mean\nmode to meet the requirement of the segmenter, exhibiting little variation (see Table 3 and Fig. 6).\nTo encourage diversity in addition to alignment, we make the segmenter trainable along with the\nUNet training. Inspired by Sushko et al. (2022), we formulate an adversarial game between the\nUNet and the segmenter. Specifically, the segmenter acts as a discriminator that is trained to classify\nper-pixel class labels of real images, using the paired ground-truth label maps; while the fake images\ngenerated by UNet as in (Eq. (3)) are classified by it as one extra \"fake\" class, as illustrated in\narea (B) of Fig. 2. As the task of the discriminator is essentially to solve a multi-class semantic\nsegmentation problem, its training objective is derived from the standard cross-entropy loss:\n$L_{Dis} = -E \\left[\\sum_{i,j} \\sum_{c=1}^{N} Y_{i,j,c} \\log \\left(Dis(\\hat{x}_0)_{i,j,c}\\right)\\right] - E \\left[\\sum_{i,j} \\log \\left(Dis(\\hat{x}^{(t)}_0)_{i,j,c=N+1}\\right)\\right]$,"}, {"title": "MULTISTEP UNROLLING", "content": "where N is the number of real semantic classes, and H \u00d7 W denotes spatial size of the input. The\nclass-dependent weighting Ye is computed via inverting the per-pixel class frequency\n$\\gamma_c = \\frac{H \\times W}{\\sum_{i,j} \\mathbb{I} [Y_{i,j,c} = 1]}$.\nfor balancing between frequent and rare classes. To fool such a segmenter-based discriminator,\nproduced by the UNet as in (Eq. (3)) shall comply with the input layout y to minimize the loss\n$L_{adv} = -E \\left[\\sum_{i,j} \\sum_{c=1}^{N} \\gamma_c \\cdot Y_{i,j,c} \\log \\left(Dis(\\hat{x}^{(t)}_0)_{i,j,c}\\right)\\right]$.\nSuch loss poses explicit supervision to the UNet for using the layout information, complementary\nto the original MSE loss. The total loss for training the UNet is thus\n$L_{DM} = L_{noise} + \\lambda_{adv} L_{adv}$,\nwhere \u03bbadv is the weighting factor. The whole adversarial training process is illustrated Fig. 2 (B).\nAs the discriminator is improved along with UNet training, we no longer observe the mean mode\ncollapsing as with the use of a frozen semantic segmenter. The high recall reported in Table 2\nconfirms the diversity of synthetic images produced by our method.\nAdmittedly, it is impossible for the UNet to produce high-quality image via a single denoising\nstep as in (Eq. (3)), especially if the input xt is very noisy (i.e., t is large). On the other hand, adding\nsuch adversarial supervision only at low noise inputs (i.e., t is small) is not very effective, as the\nalignment with the layout should be induced early enough during the sampling process. To improve\nthe effectiveness of the adversarial supervision, we propose a multistep unrolling design for training\nthe UNet. Extending from a single step denoising, we perform multiple denoising steps, which are\nrecursively unrolled from the previous step:\n$x_{t-1} = \\frac{x_t - \\sqrt{1 - \\alpha_t} \\epsilon_{\\theta}(x_t, y, t)}{\\sqrt{\\alpha_t}} + \\sqrt{1 - \\alpha_{t-1}} \\cdot \\epsilon_{\\theta}(x_t, y, t)$,\n$\\hat{x}^{(t-1)}_{0} = \\frac{x_{t-1} - \\sqrt{1 - \\alpha_{t-1}} \\epsilon_{\\theta}(x_{t-1}, y, t - 1)}{\\sqrt{\\alpha_{t-1}}}$.\nAs illustrated in area (C) of Fig. 2, we can repeat (Eq. (8)) and (Eq. (9)) K times, yielding\n{ . All these denoised images are fed into the segmenter-based discriminator\nas the \"fake\" examples:\n$L_{adv} = - \\frac{1}{K+1} E \\sum_{i=0}^{K} \\sum_{i,j} \\sum_{c=1}^{N} \\gamma_c \\cdot Y_{i,j,c} \\log \\left(Dis(\\hat{x}^{(t-i)}_0)\\right)$.\nBy doing so, the denoising model is encouraged to follow the conditional label map consistently over\nthe time horizon. It is important to note that while the number of unrolled steps K is pre-specified,\nthe starting time step t is still randomly sampled.\nSuch unrolling process resembles the inference time denoising with a sliding window of size K.\nAs pointed out by Fan & Lee (2023), diffusion models can be seen as control systems, where the\ndenoising model essentially learns to mimic the ground-truth trajectory of moving from noisy image\nto clean image. In this regard, the proposed multistep unrolling strategy also resembles the advanced\ncontrol algorithm - Model Predictive Control (MPC), where the objective function is defined in terms\nof both present and future system variables within a prediction horizon. Similarly, our multistep\nunrolling strategy takes future timesteps along with the current timestep into consideration, hence\nyielding a more comprehensive learning criteria.\nWhile unrolling is a simple feed-forward pass, the challenge lies in the increased computational\ncomplexity during training. Apart from the increased training time due to multistep unrolling, the\nmemory and computation cost for training the UNet can be also largely increased along with K."}, {"title": "EXPERIMENTS", "content": "Sec. 4.1 compares L2I diffusion models in terms of layout faithfulness and text editability. Sec. 4.2\nfurther evaluates their use for data augmentation to improve domain generalization."}, {"title": "LAYOUT-TO-IMAGE SYNTHESIS", "content": "Experimental Details. We conducted experiments on two challenging datasets: ADE20K (Zhou\net al., 2017) and Cityscapes (Cordts et al., 2016). ADE20K consists of 20K training and 2K vali-\ndation images, with 150 semantic classes. Cityscapes has 19 classes, whereas there are only 2975\ntraining and 500 validation images, which poses special challenge for avoiding overfitting and pre-\nserving prior knowledge of Stable Diffusion. Following ControlNet (Zhang & Agrawala, 2023), we\nuse BLIP (Li et al., 2022b) to generate captions for both datasets.\nBy default, our ALDM adopts ControlNet (Zhang & Agrawala, 2023) architecture for layout con-\nditioning. Nevertheless, the proposed adversarial training strategy can be combined with other L2I\nmodels as well, as shown in Table 1. For all experiments, we use DDIM sampler (Song et al., 2020)\nwith 25 sampling steps. For more training details, we refer to Appendix A.1.\nEvaluation Metrics. Following (Sushko et al., 2022; Xue et al., 2023), we evaluate the image-\nlayout alignment via mean intersection-over-union (mIoU) with the aid of off-the-shelf segmentation\nnetworks. To measure the text-based editability, we use the recently proposed TIFA score (Hu\net al., 2023), which is defined as the accuracy of a visual question answering (VQA) model, e.g.,"}, {"title": "CONCLUSION", "content": "In this work, we propose to incorporate adversarial supervision to improve the faithfulness to the\nlayout condition for L2I diffusion models. We leverage a segmenter-based discriminator to explicitly\nutilize the layout label map and provide a strong learning signal. Further, we propose a novel mul-\ntistep unrolling strategy to encourage conditional coherency across sampling steps. Our ALDM can\nwell comply with the layout condition, meanwhile preserving the text controllability. Capitalizing\nthese intriguing properties of ALDM, we synthesize novel samples via text control for data augmen-\ntation on the domain generalization task, resulting in a significant enhancement of the downstream\nmodel's generalization performance."}, {"title": "ACKNOWLEDGEMENT", "content": "We would like to express our genuine appreciation to Shin-I Cheng for her dedicated support\nthroughout the experimental testing."}, {"title": "ETHICS STATEMENT", "content": "We have carefully read the ICLR 2024 code of ethics and confirm that we adhere to it. The method\nwe propose in this paper allows to better steer the image generation during layout-to-image syn-\nthesis. Application-wise, it is conceived to improve the generalization ability of existing semantic\nsegmentation methods. While it is fundamental research and could therefore also be used for data be-\nyond street scenes (having in mind autonomous driving or driver assistance systems), we anticipate\nthat improving the generalization ability of semantic segmentation methods on such data will benefit\nsafety in future autonomous driving cars and driver assistance systems. Our models are trained and\nevaluated on publicly available, commonly used training data, so no further privacy concerns should\narise."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "Regarding reproducibility, our implementation is based o publicly available models Rombach et al.\n(2022); Xiao et al. (2018); Zhang & Agrawala (2023); Song et al. (2020) and datasets Zhou et al.\n(2017); Cordts et al. (2016); Sakaridis et al. (2021) and common corruptions Hendrycks & Diet-\nterich (2018). The implementation details are provided at the end of section 3, in the paragraph\nImplementation Details. Details on the experimental settings are given at the beginning of section\n4. Further training details are given in the Appendix in section A. We plan to release the code upon\nacceptance."}, {"title": "SUPPLEMENTARY MATERIAL", "content": "This supplementary material to the main paper is structured as follows:\n\u2022 In Appendix A, we provide more experimental details for training and evaluation.\n\u2022 In Appendix B, we include the ablation study on the unrolling step K, and more quantitative\nevaluation results.\n\u2022 In Appendix C, we provide more visual results for both L2I task and improved domain\ngeneralization in semantic segmentation.\n\u2022 In Appendix D, we discuss the failure cases of our approach, and potential solution for\nfuture research.\n\u2022 In Appendix E, we discuss the theoretical connection with prior works and potential future\nresearch directions, which can be interesting for the community for further exploration and\ndevelopment grounded in our framework."}, {"title": "EXPERIMENTAL DETAILS", "content": "We finetune Stable Diffusion v1.5 checkpoint and adopt ControlNet for the layout conditioning. All\ntrainings are conducted on 512 \u00d7 512 resolution. For Cityscapes, we do random cropping and for\nADE20K we directly resize the images. Nevertheless, we directly synthesize 512 \u00d7 1024 Cityscapes\nimages for evaluation. We use AdamW optimizer and the learning rate of 1 \u00d7 10-5 for the diffusion\nmodel, 1 \u00d7 10-6 for the discriminator, and the batch size of 8. The adversarial loss weighting factor\n\u03bbadv is set to be 0.1. The discriminator is firstly warmed up for 5K iterations on Cityscapes and\n10K iterations on ADE20K. Afterward, we jointly train the diffusion model and discriminator in an\nadversarial manner. We conducted all training using 2 NVIDIA Tesla A100 GPUs."}, {"title": "TIFA EVALUATION", "content": "Evaluation of the TIFA metric is based on the performance of the visual question answering (VQA)\nsystem, e.g. mPLUG (Li et al., 2022a). By definition, the TIFA score is essentially the VQA ac-\ncuracy, given the question-answer pairs. To quantitatively evaluate the text editability, we design\na list of prompt templates, e.g., appending \u201csnowy scene\u201d to the original image caption for image\ngeneration. Based on the known prompts, we design the question-answer pairs. For instance, we\ncan ask the VQA model \u201cWhat is the weather condition?\", and compute TIFA score based on the\naccuracy of the answers.\""}, {"title": "FEATURE-BASED DISCRIMINATOR FOR ADVERSARIAL SUPERVISION", "content": "Thanks to large-scale vision-language pretraining on massive datasets, Stable Diffusion (SD) (Rom-\nbach et al., 2022) has acquired rich representations, endowing it with the capability not only to gen-\nerate high-quality images, but also to excel in various downstream tasks. Recent work VPD (Zhao\net al., 2023) has unleashed the potential of SD, and leveraged its representation for visual percep-\ntion tasks, e.g., semantic segmentation. More specifically, they extracted cross-attention maps and\nfeature maps from SD at different resolutions and fed them to a lightweight decoder for the specific\ntask. Despite the simplicity of the idea, it works fairly well, presumably due to the powerful knowl-\nedge of SD. In the ablation study, we adopt the segmentation model of VPD as the feature-based\ndiscriminator. Nevertheless, different from the joint training of SD and the task-specific decoder\nin the original VPD implementation, we only train the newly added decoder, while freezing SD to\npreserve the text controllability as ControlNet."}, {"title": "THEORETICAL DISCUSSION", "content": "In the proposed adversarial training, the denoising UNet of the diffusion model can be viewed as\nthe generator, the segmentation model acts as the discriminator. For the diffusion model, the dis-\ncriminator loss is combined with the original reconstruction loss, to further explicitly incorporate\nthe label map condition. Prior works (Gur et al., 2020; Larsen et al., 2016; Xian et al., 2019) have\ncombined VAE and GAN, and hypothesized that they can learn complementary information. Since\nboth VAE and diffusion models (DMs) are latent variable models, the combined optimization of\ndiffusion models with an adversarial model follows this same intuition - yet with all the advantages\nof DMs over VAE. The combination of the latent diffusion model with the discriminator is thus, in\nprinciple, a combination of a latent variable generative model with adversarial training. In this work,\nwe have specified the adversarial loss such that relates our model to optimizing the expectation over\nxo in Eq. (6), and for the diffusion model, we refer to the MSE loss defined on the estimated noise\nin Eq. (2), which can be related to optimizing xo with respect to the approximate posterior q by op-\ntimizing the variational lower bound on the log-likelihood as originally shown in DDPM (Ho et al.,\n2020). Our resulting combination of loss terms in Eq. (7) can thus be understood as to optimize over\nthe weighted sum of expectations on 20."}, {"title": "FUTURE WORK", "content": "In this work, we empirically demonstrated the effectiveness of proposed adversarial supervision and\nmultistep unrolling. In the future, it is an interesting direction to further investigate how to better\nincorporate the adversarial supervision signal into diffusion models with thorough theoretical justi-\nfication.\nFor the multistep unrolling strategy, we provided a fresh perspective and a crucial link to the ad-\nvanced control algorithm - MPC, in Sec. 3.2. Witnessing the increasing interest in Reinforcement\nlearning from Human Feedback (RLHF) for improving T2I diffusion models (Fan et al., 2023; Xu\net al., 2023), it is a promising direction to combine our unrolling strategy with RL algorithm, where\nMPC has been married with RL in the context of control theory (Wang et al., 2023a) to combine the\nbest of both world. In addition, varying the supervision signal rather than adversarial supervision,\ne.g., from human feedback, powerful pretrained models, can be incorporated for different purposes\nand tailored for various downstream applications. As formulated in Eq. (10), we simply average the\nlosses at different unrolled steps, similar to simplified diffusion MSE loss (Ho et al., 2020). Future\ndevelopment on the time-dependent weighting of losses at different steps might further boost the\neffectiveness of the proposed unrolling strategy."}]}