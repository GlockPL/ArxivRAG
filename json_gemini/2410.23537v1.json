{"title": "ALISE: Accelerating Large Language Model Serving with Speculative Scheduling", "authors": ["Youpeng Zhao", "Jun Wang"], "abstract": "Large Language Models (LLMs) represent a revolutionary advancement in the contemporary landscape of artificial general intelligence (AGI). As exemplified by ChatGPT, LLM-based applications necessitate minimal response latency and maximal throughput for inference serving. However, due to the unpredictability of LLM execution, the first-come-first-serve (FCFS) scheduling policy employed by current LLM serving systems suffers from head-of-line (HoL) blocking issues and long job response times.\nIn this paper, we propose a new efficient LLM inference serving framework, named ALISE. The key design paradigm of ALISE is to leverage a novel speculative scheduler by estimating the execution time for each job and exploiting such prior knowledge to assign appropriate job priority orders, thus minimizing potential queuing delays for heterogeneous workloads. Furthermore, to mitigate the memory overhead of the intermediate key-value (KV) cache, we employ a priority-based adaptive memory management protocol and quantization-based compression techniques. Evaluations demonstrate that in comparison to the state-of-the-art solution vLLM, ALISE improves the throughput of inference serving by up to 1.8x and 2.1\u00d7 under the same latency constraint on the Alpaca and ShareGPT datasets, respectively.", "sections": [{"title": "1\nINTRODUCTION", "content": "In recent years, large language models (LLMs) have demonstrated ubiquitous performance across various natural language processing (NLP) tasks [1, 3, 30, 37]. Notably, LLMs can generalize well to perform multiple tasks without additional fine-tuning, which makes them versatile and adaptable for downstream AI applications [17]. Inference serving is crucial to applications driven by LLMs, such as AI chatbots. The interactive nature of such applications requires the system to produce inference results for user requests at low latency and high throughput.\nThe inference process of LLMs exhibits the unique characteristics of autoregressiveness, where LLMs generate new tokens sequentially based on the input (prompt) tokens and all prior generated tokens. LLM inference typically requires running multiple iterations of the model to complete the token generation. General DNN inference serving systems [5, 10, 23, 36] focus on deterministic workloads such as image classification and object detection, which are highly predictable. The autoregressive nature makes these solutions not work and scale well for LLM workloads, as in LLM inference, each token is generated based on the preceding ones, and this sequential dependency can result in unpredictable execution runtime for generated sequences.\nExisting LLM systems focus on improving serving throughput performance with custom CUDA kernels [6, 7, 16], iteration-level scheduling [34, 35], and efficient memory management [13, 25, 39]. Unfortunately, despite large strides toward improving the performance of serving LLMs, today's system platforms continue to struggle to provide low latency, especially in real-world workloads. We argue that this struggle stems from the non-preemptive first-come-first-served (FCFS) [38] scheduling strategy, which causes head-of-line (HoL) blocking issues when processing heterogeneous workloads. Run-to-completion scheduling is recognized for its HoL blocking issue, which is especially troublesome for generative inference tasks. This occurs when prior scheduled tasks, potentially lengthy in execution, hinder the timely processing of subsequent shorter tasks. Such queuing delays could significantly deteriorate the quality of service (QoS) provided. How to efficiently handle heterogeneous requests with variable execution time and perform appropriate scheduling to maintain high throughput remains a challenging problem.\nIn this paper, we introduce ALISE, to accelerate LLM serving with a novel speculative scheduler. A key insight behind ALISE is to estimate the execution time of heterogeneous LLM requests. Specifically, we adopt a retrieval-based length predictor, where a user prompt is first embedded into high-dimension vectors using a pre-trained text encoder, and the output length is accurately predicted using an ensemble of a query database and an all-MLP decoder. Based on such prior knowledge, we can estimate each job's execution time and leverage priority queues to perform preemptive scheduling, thus alleviating HoL issues and minimizing the total response time. To efficiently manage the intermediate KV cache of preempted jobs, we design a priority-based adaptive memory management mechanism that dynamically performs swapping operations (e.g., upload and offload) of the KV cache for preempted jobs based on each job's estimated wait time (EWT). Furthermore, we utilize the quantization technique to compress the KV cache to lower precision further reducing the overall memory overhead."}, {"title": "2 BACKGROUND AND MOTIVATION", "content": "Transformer Architecture. The Transformer architecture [31] has significantly advanced natural language processing (NLP) and has been foundational in large language models (LLMs). A standard transformer model comprises an embedding layer to project sequences of tokens to hidden dimensions and stacks of transformer layers to capture long-term dependencies between input tokens using the self-attention mechanism. The embedding layer is responsible for converting discrete words (tokens) into high-dimensional vectors that can be processed by neural networks, capturing both the semantic and positional information of individual tokens [15].\nA transformer layer includes two main components: a multi-head attention (MHA) module and a position-wise feed-forward network (FFN).\nAt the core of transformer layers lies the attention module [31]. The relevant operations are given in Equation 1 and 2. The LayerNorm and residual connection are omitted for simplicity. Three intermediate tensors are involved, namely, query Q, key K, and value V. The attention weights are calculated by first computing the dot product between Q and K, then scaling the product by the square root of hidden dimension d divided by the number of heads h, and finally going through a softmax operation (\u03c3(\u00b7)). The attention scores Attn(Q, K, V) are calculated by multiplying the attention weights by V.\n$Attn(Q, K, V) = \\sigma(\\frac{QK^T}{\\sqrt{d/h}}).V$  (1)\nThe MHA output is obtained by simply concatenating the outputs of all attention heads along the head dimension, with each head being an attention module.\n$MHA(Q, K, V) = Concat(Attn_1, ..., Attn_h)$ (2)\nLLM Inference. The generative inference of LLMs adopts an incremental decoding approach where the system computes the activations for all input prompt tokens in a single step and then iteratively decodes one new token using the input prompt and all previously generated tokens. The inference procedure of the LLM inference can be divided into two parts, including the prefilling and decoding stages. As shown in the example, during the prefilling stage, LLMs first process all the input tokens in a single pass; then, during the decoding stage, a previously generated output token is fed back into the model as an input and generates the next output token. Therefore, the decoding stage unfolds iteratively, processing one token at a time, until the sequence length reaches a maximum threshold or an \"(EOS)\" token is emitted.\nThe distinctive autoregressive characteristics open up the opportunity of reusing intermediate states, specifically, the key (K)"}, {"title": "3 METHODOLOGY", "content": "In this work, we develop a new speculative scheduling mechanism for LLM inference and build ALISE to tackle the challenges outlined in Section 2. When users submit their requests to the inference service, the retrieval-based speculative scheduler first predicts the executive runtime and sends the job profile to priority queues to assign the appropriate priority level at the granularity of iteration, which favors the job with the shortest remaining time to address the HoL blocking issues.\nOnce a job is selected, it will be automatically submitted to the GPU model executor, which retrieves the corresponding KV cache for computation. ALISE also features an adaptive KV Cache manager that dynamically swaps the KV cache of preempted jobs between GPU HBM and CPU memory that overlaps with computation to avoid additional latency.\n3.1 Speculative Scheduler\nOutput Length Prediction. A key aspect of enabling preemptive scheduling for LLM workloads is to predict the output sequence length. Previous works have attempted to perform sequence prediction using smaller LLMs [19, 40] or proxy models [12, 22], however, they generally impose extensive engineering efforts and incur non-negligible overhead. In this work, we propose to adopt a retrieval-based approach that leverages a vector database (DB) for estimating output length with high accuracy and low memory and latency overhead. \nWe first transform the user prompt into high-dimension vectors using a pre-trained BERT encoder network [8]. Next, we conduct a similarity search for input in the vector DB and retrieve top-k historical queries with similarity scores above a pre-defined threshold. The final output length is predicted using the weighted average of retrieved queries. If no similar queries are found in the vector\n3.2 Adaptive Memory Management\nOur proposed speculative scheduler provides iteration-level preemption to manage job scheduling orders, however, directly applying our methodology to LLM inference serving encounters the obstacle of additional memory overhead. In FCFS, we only need to store the intermediate KV cache for the executed jobs in the batch. In contrast, with preemptive scheduling, more jobs are in pending states in the GPU, and additional memory space must be allocated to store their KV cache for future use. Determined by the model size, batch size, and sequence length, the KV cache could consume significant memory space, potentially causing out-of-memory (OOM) errors, and halting the execution [25, 39].\nA straightforward solution to mitigate the memory overhead issues is to defer all the newly arrived jobs in the priority queue when the GPU memory reaches the maximum capacity. However, this solution loses its validity when incoming new jobs have higher priority (shorter execution time), but are blocked due to memory space limitation. In resource-constrained settings, such as single GPU-CPU systems, our speculative scheduling would be downgraded to FCFS, and the HoL blocking issues would resurface. Another practical solution is to perform deletion for the unused KV cache for jobs in the queues to make room for the new jobs. But this solution suffers from two major downsides: 1) the deleted KV cache has to be recomputed when low-priority jobs are scheduled for execution, which induces additional compute latency; 2) with the aging mechanism, the killed low-priority jobs would be promoted to the high-priority queue after passing the age threshold, which then kills the currently executed jobs, leading to potential deadlocks.\nDynamic Swapping. Previous works have explored leveraging the memory of different hierarchies to accelerate LLM inference in resource-constrained environments, however, they are mostly for offline batched inference and schedule the KV cache swapping at either head or token level [25, 39]. In this work, we design a priority-based KV cache manager that dynamically swaps the KV cache for preempted jobs between GPU HBM and CPU memory at the request level. Specifically, ALISE performs dynamic swapping operations of the KV cache based on the metric of estimated wait time (EWT). Generally, jobs with higher EWT tend to be offloaded to the CPU DRAM to make space for high-priority jobs, while jobs with lower EWT tend to be uploaded for later execution. Additionally, the offloading and uploading operations are overlapped with computation to avoid additional latency.\nFor a job with priority p, we can calculate the EWT by summing all the execution time of jobs with higher priority as follows:\n$EWT(J) = \\sum_{m<p} Predictor(J)$ (6)\nDue to the existence of the aging mechanism (age threshold K), we also need to compare the time needed to promote Ji to the high-priority queue for execution. Therefore, we reformulate Equation 6 as follows:\n$EWT(J) = min(\\sum_{m<p} Predictor(J), T_{promote}(J, K))$ (7)\nLeveraging the above EWT definitions, we can decide the order of offloading and uploading of each job by setting the maximum number of jobs allowed in the GPU, which is constrained by the total available GPU memory space.\nKV Compression. Previous works have utilized quantization to accelerate attention computation by compressing model weights [9, 14]. In this work, we leverage quantization for a different purpose, i.e., compressing the unused KV cache to reduce memory overhead. We adopt a fine-grained channel-wise quantization for the KV cache to ensure robust performance and minimal information loss. More specifically, we use the following formula to quantize KV cache to b-bit integers in memory and de-quantize them to their original format (FP16 in this work) for computation:\n$x_q = round(\\frac{x + z}{\\lambda}), x = \\lambda(x_q - z)$ (8)\nwhere zero point $z = \\lfloor \\frac{(max-min)}{2} \\rfloor$, and the scaling factor $\\lambda = \\frac{max-min}{2^{b-1}}$. In this work, we choose the 8-bit integer for quantizing the KV cache to ensure minimal degradation on text generation quality.\n3.3 Implementation\nALISE is implemented on top of PyTorch [20] and HuggingFace Transformer libraries [33] with 4.5K lines of Python. Our execution engine is based on vLLM [13], which we modify to support iteration-level preemptive scheduling and interact with the KV cache manager for dynamic swapping. ALISE also features customized fused kernels for LayNorm, Attention, and ReLU operations, just like other systems for transformer-based LLMs [13, 16, 35]. We implement these kernels based on python-based Triton compiler [29]. To increase the compute utilization of GPUs and improve inference efficiency, we adopt the iteration-level continuous batching technique [35], where we apply batching only to a handful of operations, such as Linear, LayerNorm, and ReLU, which are compatible with irregularly shaped tensors."}, {"title": "4 EVALUATION", "content": "4.1 Experimental Setup\nHardware. We conduct our experiments on a single customized high-performance GPU instance, configured with two NVIDIA Tesla V100 GPUs with 32 GB VRAM, and a 2.60 GHz Intel Xeon CPU connected over PCIe 4.0\u00d716 and 1024 GB DRAM.\nWorkloads. To emulate the LLM workloads, we synthesize traces of user requests based on the Alpaca [27] and ShareGPT [28] dataset since there is no publicly available request trace for LLMs. The Alpaca dataset is a GPT-3.5 generated instruction dataset and is widely used for LLM inference evaluation [13, 40]. The ShareGPT dataset is a public dataset that collects the chatbot conversation from ChatGPT users, which exhibit greater variance in contents and sequence length, as shown in Figure 7. We tokenize each dataset and use their input and output lengths to simulate real-world user requests. In terms of request timestamps, we generate the arrival time based on the Poisson distribution.\nModels. Following prior works on LLM serving, we use the popular OPT [37], one of the most representative open-sourced LLM families widely used in both industry and academia, for our main evaluation. We choose three model size configurations, namely 2.7B,\n4.2 End-to-End Performance\nThe first row of Figure 6 shows the results on the Alpaca dataset. We can see that as the request rate increases, the normalized latency gradually increases but then explodes after a certain threshold. Such observation aligns with previous works [13, 25, 35, 39], where LLM inference is often memory-bound, and once the request rate reaches the memory capacity of the serving system, the response latency grows infinitely as the request queue pool continues to expand. On the Alpaca dataset, ALISE obtains 1.3 1.8\u00d7 higher throughput compared to vLLM while maintaining similar latency results. This is due to ALISE's speculative scheduling strategy that dynamically adjusts the priority order of each request thus reducing the response delay. For instance, as shown in Figure 6(b), under the latency constraints of 200 ms, ALISE processes 1.5\u00d7 more requests than vLLM. When compared against ORCA, ALISE sustains up to 3.1\u00d7 higher request rates. The advantage of ALISE is more pronounced on the ShareGPT dataset, as shown in the second row of Figure 6. This is because the ShareGPT dataset contains longer input prompts and\n4.3 Performance Analysis\nImpact of KV Cache Management. We further demonstrate the benefits of our proposed adaptive KV cache management by comparing our design with two strawman solutions, namely Recompute and Defer strategy. The Recompute strategy deletes the KV cache of preempted low-priority jobs and recomputes the KV cache when jobs are rescheduled for execution, while the Defer strategy simply defers newly arrived jobs when the GPU does not have enough memory space for executing the job.\nWe perform the experiments on the OPT models with varying request rates on the Alpaca dataset. As shown in Figure 8, when"}, {"title": "5 RELATED WORK", "content": "DNN Inference Serving. Recent years have witnessed the exponential of AI-driven applications in real-world practices, and numerous dedicated systems [5, 10, 23, 36] have been developed to meet such growing demand. They generally focus on batching [5], scheduling [10], and caching optimizations [36] for serving small-scale models such as ResNet. More recently, INFaaS [23] automates the model selection and deployment process by navigating the best trade-off between model variants and performances. However, these systems fail to take into account the multi-iteration characteristics and the unpredictability of LLM inference, resulting in missed opportunities for further optimizations.\nLLM Inference Serving. Several specialized inference serving systems have been developed for Transformer-based LLMs [13, 16, 35]. FasterTransformer is among the first to optimize training and inference serving for large transformer-based LLMs by utilizing GPU kernel optimizations and model parallelism [16]. ORCA further develops iteration-level scheduling and selective batching by leveraging the multi-iteration nature of LLM inference [35]. vLLM proposes PagedAttention to store the KV cache in a non-contiguous paged memory to alleviate memory fragmentation [13]. While they have achieved promising results, existing LLM systems use the non-preemptive first-come-first-served (FCFS) [38] to process LLM"}, {"title": "6 CONCLUSION", "content": "In this work, we present a new efficient LLM inference serving solution, termed ALISE, to address the head-of-line (HoL) blocking issues in the existing FCFS scheduling strategy. ALISE leverages a novel retrieval-based speculative to schedule incoming jobs by their remaining execution time preemptively, thus minimizing potential queuing delays for heterogeneous workloads. To alleviate the memory overhead of preempted jobs, we design adaptive memory management that dynamically performs swapping operations for unused KV cache and quantization-based compression. Experiments demonstrate that ALISE obtains up to 1.8\u00d7 and 2.1\u00d7 throughput improvement over the state-of-the-art systems, such as VLLM, on the Alpaca and ShareGPT datasets, respectively."}]}