{"title": "Instruction-Driven Game Engine: A Poker Case Study", "authors": ["Hongqiu Wu", "Xingyuan Liu", "Yan Wang", "Hai Zhao"], "abstract": "The Instruction-Driven Game Engine (IDGE) project aims to democratize game development by enabling a large language model (LLM) to follow free-form game descriptions and generate game-play processes. The IDGE allows users to create games simply by natural language instructions, which significantly lowers the barrier for game development. We approach the learning process for IDGEs as a Next State Prediction task, wherein the model autoregressively predicts the game states given player actions. The computation of game states must be precise; otherwise, slight errors could corrupt the game-play experience. This is challenging because of the gap between stability and diversity. To address this, we train the IDGE in a curriculum manner that progressively increases its exposure to complex scenarios. Our initial progress lies in developing an IDGE for Poker, which not only supports a wide range of poker variants but also allows for highly individualized new poker games through natural language inputs. This work lays the groundwork for future advancements in transforming how games are created and played.", "sections": [{"title": "Introduction", "content": "Game developers dedicate creativity to offer immersive experiences to game players. Players immerse themselves in games and offer valuable feedback to developers. This makes a symbiotic relationship between creators and customers. However, as depicted in the comic from Figure 1, there are disconnections between them, due to diverse preferences of players across age, gender, and cultural backgrounds. Despite the fact that many today's games allow for customization of basic characters and appearances, it is an impossible task for developers to craft every aspect of the game to suit the need of every player. Our study seeks to reconcile such a divide.\nGame engines, as the heart of game development, are conventionally driven by programming languages. This technical barrier often deters enthusiasts from realizing their game development dreams. In response, we propose a novel concept: Instruction-Driven Game Engine (IDGE), a game engine enabling anyone to fashion a game through natural language instructions and generating the resultant game-play process. Distinct from recent advancements in video-based games (Bruce et al., 2024; Team et al., 2024b), our focus in this paper is on the text-based game states. We leverage Unity to render these states to visual display.\nIDGE is a neural engine, meaning it is built upon neural networks, specifically large language models (LLMs) (Brown et al., 2020; OpenAI, 2023; Touvron et al., 2023; Yang et al., 2023). It is designed to follow a game script, a detailed instruction that blueprints the game, e.g. settings, rules, elements, and drive the progression of game-play as interacting with players. IDGEs frame the operation of engines as a Next State Prediction task, which autoregressively predicts the next game state based on the user-specified game script, previous game state, and current player action.\nTraining an IDGE faces the dual challenges of stability and diversity. The former seeks to provide a stable and precise game-play throughout lengthy contexts, while the latter seeks to follow diverse preferences across the large player base. Unfortunately, we empirically see an ironic twist: the model trained directly from naive game logs is neither stable nor diverse. Therefore, we employ a standard-to-diverse curriculum learning methodology, which gradually introduces complexity into the training process, incrementally enhancing the model's diversity while preserving its stability."}, {"title": "Instruction-Driven Game Engine", "content": "In this section, we introduce dialogue-style LLMs as the setup for IDGEs. We then formulate the learning problem as Next State Prediction."}, {"title": "From Instruction-Driven Dialogue to Instruction-Driven Game Engine", "content": "Most LLMs (Brown et al., 2020; OpenAI, 2023; Touvron et al., 2023; Yang et al., 2023) have been fine-tuned on dialogue-style corpora, where it is endowed with the ability to interact with users. The resultant models can follow a system instruction provided by users and lead to a dialogue process in line with it.\nLikewise, an IDGE works through interaction, too. Its system instruction specifically refers to a game script that accurately describes the desired game. In game-play, the IDGE interacts with players (users), concurrently processing player inputs, (e.g. moves, targets), to dynamically generate the game states as responses.\nIn Figure 2, we demonstrate how a poker IDGE facilitates a variant of Texas Hold'em: the player first inputs the game script in natural language. Based on this game script, the IDGE simulates the game-play process with the player state by state. The player performs the action, e.g. check, call, raise, and the engine computes and returns the resultant game state. It is a dialogue-like process and will continue till the game concludes."}, {"title": "Next State Prediction", "content": "Causal language models learn the interplay of words through the autoregressive process of next token prediction (Vaswani et al., 2017; Brown et al., 2020). From a game-play perspective, the minimum component is no single token, but rather each game state. A game state is a single frame that contains all real-time game information, e.g. characters, items, missions. Essentially, the task of any game engines is exactly to compute the next state according to the prior ones. Therefore, we may formulate the learning of IDGEs as a Next State Prediction (NSP) problem.\nGiven a sequence of game states s {80,81,\u2026\u2026,ST}, an IDGE with parameters \u03b8 seeks to maximize the likelihood:\n$\\sum_{t=1}^{T}log p_{\\theta}(s_{t}|s_{0},s_{1},...,s_{t-1},x_{t},z)$"}, {"title": "Differential State Prediction", "content": "The inference complexity of NSP scales quadratically with the sequence length. Therefore, decoding a lengthy game state may fall into trouble. Empirically, the game state only undergoes a slight change between two successive moments t and t + 1, with the majority of the state remaining the same. This phenomenon can be potentially general across various games when the intervals between states are short. We thus introduce Differential State Prediction (DSP), an efficient variant of NSP, where the engine is simplified to predict solely the difference of two states:\n$\\sum_{t=1}^{T}log p_{\\theta}(\\Delta s_{t}|s_{t-1}, x_{t}, z)$\nwhere Ast is the difference of st\u22121 and st. DSP is more efficient compared to NSP in most situations, significantly accelerating the inference during game-play. In our experiments, we find that DSP also produces slightly better performance.\nTo reconstruct st from Ast and st\u22121, there will be a merge function st = M(\u2206st, St\u22121). In this work, each game state is implemented as a dict. Hence, M refers to the coding of updating dict elements. The following section will demonstrate concrete examples of NSP/DSP for a poker game."}, {"title": "Data for IDGE", "content": "Our training data is sourced from two methods. First, we develop a poker simulator and obtain the training data from its game logs. The simulator supports ten representative poker games: Texas Hold'em, Omaha, Omaha HL, Short-deck Hold'em, 2-to-7 triple Draw, A-to-5 triple Draw, 2-to-7 single Draw, Badugi, Badeucey, and Badacey. Additionally, it allows for further configuration of several common poker elements, e.g. type of suits, numbers. By adjusting these elements, one can derive virtually infinite variations beyond aforementioned ten poker games. Moreover, we realize that if the game logs are sampled completely in uniform, the occurrence of some rare states, such as some superior card combinations, would be extremely low. The resultant engine trained on such data may fall short in low-frequency situations, even though the dataset is large. Therefore, we balance the data by up/down-sampling the game logs to ensure that all possible situations occur similarly. After obtaining game logs, we transform each log into a training sample as in Figure 2 for NSP and DSP. Each sample is made up of three parts: the game script z, player input xt, and game states st. If we were to draw an analogy with ChatGPT, they respectively play the roles of the system, user, and assistant.\nThe second part of the data is generated by GPT3.5. Based on the state prediction data from the simulator, we prompt GPT3.5 to augment the game scripts and generate the corresponding new game states. This process is manually done by skilled prompting, which incorporates scenarios beyond typical poker games, expanding the diversity of training data as a result."}, {"title": "Curriculum Learning", "content": "Straightforwardly, we could utilize the data generated in \u00a7 3 to fine-tune a base model by maximizing Eq. 2/3 and obtain the IDGE. However, the resultant IDGE may struggle with stability and diversity: neither can it accurately predict the next game state nor comprehend the user-specified game script in natural language. Therefore, we devise a progressive curriculum learning process (Bengio et al., 2009), to incrementally enhance the IDGE's diversity while preserving stability.\nWarmup: Training on Core Set In \u00a7 2, we utilize a set of core functions to facilitate the process of state prediction. Though the model can be exposed to all core functions via fine-tuning, we observe that it struggles to call the core functions properly. This phenomenon is much more severe in unseen contexts. We attribute this to the cold start problem that the model merely memorizes the names of the core functions during training, without knowing their underlying implementation. To this end, we introduce a pre-learning phase to warmup the model. We develop an instruction tuning dataset of 1k samples derived from the core set, where each core function is translated to a natural language instruction and the model is trained to implement the function in a way of instruction following. This phase offers a profound comprehension of the model's usage of core functions.\nStandard: Training on Standard Game Scripts The next step is to train the model on the standard data introduced in \u00a7 3 by optimizing NSP/DSP. In this phase, the model is forged into an engine, predicting game-play state by state following the game scripts, and is combined with pre-learned core functions organically.\nDiverse: Training on Rephrased Game Scripts While the standard data already includes the prototype game scripts, mastering the prototype descriptions can be too restrictive for users. Rather, it is more natural for them to describe their desired games in free-form natural language. Rather than exhaustively crafting new natural language data, we introduce Segment Rephrasing (SR), a technique that rephrases a portion of the game script to encourage the model to follow diverse natural language. Specifically, given a game script, we segment it into chunks and randomly rephrase several of them. To largely keep the semantics intact, there is only a very low probability that the entire script will be rephrased. The rephrasing process is done by GPT3.5. These rephrased game scripts enable the model fully \"to customers\". In addition, these scripts will be more challenging to understand, which potentially generalizes the model to unseen scenarios. Readers may refer to Table 5 in Appendix B for real human-written examples.\nWe summarize the training pipeline for the IDGE: 1) train on the core set Dcs (1k); 2) train by optimizing NSP/DSP on the standard dataset D (10k); 3) rephrase the standard data Dsr and train on the sum of D and Dsr (20k).\nThe warmup, standard, and diverse process correspond to the easy, medium, and hard curriculum. It serves for a smooth transfer of the IDGE from standardization to diversity."}, {"title": "Experimental Results", "content": "In this section, we evaluate the IDGE in two scenarios. The former is automatically generated by our simulator, which can be considered as a test set that has the same distribution as the training set. The latter resembles the real-world situations, where proficient poker players are directly enlisted as annotators to create new game scripts. Subsequently, the test data is obtained by playing the games online by themselves with the IDGE."}, {"title": "Training and Evaluation Setup", "content": "We develop the IDGE based on CodeGemma-7b (Team et al., 2024a)\u00b9. CodeGemma is a code LLM that is additionally pre-trained on large code corpora. We find that CodeGemma works better than similar-sized natural language models like LLaMA3 (Dubey et al., 2024). We train each model using LORA (Hu et al., 2022) with r = 8, a = 32, and the optimal learning rate in 1.5e-4 and 3e-4. The warmup of the learning rate is set to 30 steps and the total batch size is set to 8 on 8 chips. For each curriculum, we train 3 epochs. To ensure the stability of outputs, we leverage greedy decoding.\n\u2022 In-domain evaluation: The model has been exposed to a broad range of variants based on ten existing poker games during training. We sampled some unseen variants of these ten games from the poker simulator for evaluation. Then, we program some random players that randomly select an action as their input to interact with the IDGE. This manner allows for a quick and automatic assessment of the IDGE's basic performance as well as the effectiveness of training methods. Specifically, each type of games is played for 20 rounds. There are totally 200 rounds of games in the in-domain test set. The state prediction accuracy is determined through two steps. First, we compare the predicted code snippet and the ground truth. If not exactly matched, we execute both snippets on the input state respectively and then compare two outputs.\n\u2022 Out-of-domain evaluation: The in-domain evaluation is limited to a number of predefined poker games with configurable essential elements. To evaluate our the IDGE's performance in scenarios more closely aligned with the real world, we further recruit 5 proficient poker players as our engine testers. Each of them is asked to create 1~2 new poker games based on their personal preferences and craft the game script using natural language. They are free to tailor the game scripts, for example, crafting the entirely new elements and strategies not found in existing poker games. Subsequently, we invite them to play 10 rounds of the game with distinct configurations for each new game by themselves and record all player inputs and game states throughout the game-play. This forms our out-of-domain test set that comprises 8 distinct game scripts and 80 rounds of games."}, {"title": "In-Domain", "content": "Round-level Table 2 shows the round-level success rates of a number of fine-tuned models. The success rate is counted if the engine correctly handles all states in a round. The results of CodeGemma from NSP to DSP suggest the advantage of predicting the difference of two states, which results in both accuracy and efficiency boost. The best results occur when the model undergoes segment rephrasing (SR) and the full curriculum (CS + SR) respectively. The resultant CodeGemma achieves 100% success rates on all ten poker variants. This suggests the effectiveness of SR to enhance the model's understanding on the game scripts. In the following, we will show that SR is more important in the face of out-of-domain games.\nState-level We also introduce GPT4 as a strong baseline in our experiment, which is prompted with additionally five in-context samples (5-shot). Surprisingly, in 200 rounds of games, it is unable to successfully complete any single round. One might question why GPT4 completely fails in this task, significantly behind fine-tuned CodeGemma-7b. To conduct a more in-depth analysis, we compute the state-level accuracy in Table 3. We find that, though GPT4 is strong in programming, it performs badly in managing nuanced poker cards. For example, it is very likely to mess up the order, hallucinating new cards or missing some of them. This drawback is pronounced in deal and show. In contrast, deal is a much easier task for humans. We conjecture that current LLMs have not been exposed to highly sophisticated data and tasks as for the IDGEs during their training. The accumulation of errors in all these aspects eventually leads non-fine-tuned models to zero success rates in round-level evaluation. It is important to note that an IDGE should be all-round at each aspect; otherwise, the overall performance will degenerate in a way of Buckets effect.\nIn contrast, for fine-tuned CodeGemma, Table 3 shows that it has performed close to 100% accuracy in most states with only a half of training samples (5k). Such high accuracy correlates positively to its stable round-level performance in Table 2. We notice that CS is particularly beneficial for show, where the model is responsible to calculate the hand combinations and compare their strength, the most challenging task in poker games. There are a large number of relevant core functions in this process. Hence, it becomes critical for the model to adapt to core functions in advance."}, {"title": "Out-of-Domain", "content": "Table 5 in Appendix B illustrates the eight scripts created by human players. Most of them are creative new games with a large gap from standard poker. For example, in script 6, the creator defines a group of novel combinations \"Stardust X\".\nTable 4 reports the round-level success rates of our IDGE, fine-tuned based on CodeGemma-7b with and without SR. We first find that the model not underwent SR fails to be fully instructable by players. For example, it cannot understand the tricky dealing process in Magic Dealer described in free natural language, though it is a simple variant from standard dealing. In contrast, the model underwent SR treats this with ease. The rephrased samples encourage the model to learn the alignment between prototype game scripts and diverse natural language, thereby better balancing stability and diversity. Additionally, the full IDGE demonstrates remarkable generalizability in the face of novel and unseen games. For example, in 6-card Draw, the IDGE effectively generalizes from managing 5-card hands to 6-card hands, while in Dragonie, which is an upgrade version of Badugi, the IDGE learns to pick out cards with distinct suits while determining the consecutiveness of their values. For more challenging Stardust, where the creator introduces a series of entirely new cards and combinations, the IDGE successfully passes eight of the ten rounds of the game."}, {"title": "Conclusion", "content": "This paper introduces the Instruction-Driven Game Engine (IDGE), offering game enthusiasts a brand new game development and game-play experience. The IDGE understands the player-specified game rules and simulates the entire game-play process. We formulate the learning of IDGEs as Next State Prediction and leverage a curriculum learning approach to enhance stability and diversity. Experiments demonstrate our poker IDGE can accurately complete the majority of user-defined games."}, {"title": "Broader Impact", "content": "This paper presents the initial progress of IDGE in the case of Poker. Such a paradigm theoretically applies to all types of games. However, our progress is constrained by several bottlenecks.\nInference Latency We have demonstrated that IDGEs go well with turn-based strategy (TBS) games. For real-time strategy (RTS) games, players may make more than one action per second. The inference latency of current LLMs cannot meet the real-time requirements of such games.\nContext Window Generally, as games become more complicated, the length of game states increases, posing a challenge to satisfy our independence assumption. This may significantly challenge both the comprehension ability of LLMs and the cache of KV states.\nAccessibility The kernel data of most commercial games is not publicly available, which is why we developed a poker simulator to generate the training data for this paper.\nWe are delighted to observe that there have been continuous advancements in inference frameworks such as vLLM (Kwon et al., 2023), as well as efficient long-text generation methods like StreamingLLM (Xiao et al., 2024) and TempLORA (Wang et al., 2024). We believe that the ongoing development of LLM technologies will ultimately address the limitations of latency and the context window. Regarding the issue of accessibility, we look forward to more companies providing open interfaces as SC2LE (Vinyals et al., 2017), HOK Arena (Wei et al., 2022) to offer kernel data.\nThe recent released Delta-Engine (Wu et al., 2024a) is largely inspired from our work. It exclusively focuses on game development. The development process can be ideally eternal, by expanding the engine incrementally. Unlike the IDGE, the delta-engine does not simulate the game-play process. The resultant game-play is rendered by external modules."}, {"title": "Related Work", "content": "A game engine is a fundamental software designed for game development. Famous game engines include Unreal, Unity, CoCos, etc. Pygame is also a simple game engine. We spotlight two crucial properties of a game engine. The first is functionality, i.e. providing a wide variety of basic tools to facilitate the development process. The next is secondary development, i.e. rich and flexible interfaces to allow developers to customize games. In this work, we introduce a new concept, instruction-driven game engine (IDGE), a neural game engine learned on basis of large language models (OpenAI, 2023; Touvron et al., 2023; Jiang et al., 2023; Yang et al., 2023; Qin et al., 2023). As opposed to typical game engines, the IDGE acquires its functionality power by instruction tuning on the core set (Raffel et al., 2020; Ouyang et al., 2022) and allows for low-barrier game development by issuing natural language descriptions.\nSome research efforts have explored the AI applications in games (Gallotta et al., 2024), e.g. nonplay characters (Shanahan et al., 2023; Uludagli and Oguz, 2023), interactive drama (Wu et al., 2024b; Han et al., 2024), game commentators (Eladhari, 2018; Ranella and Eger, 2023). A great amount of work focuses on AI as players, e.g. for Atari (Mnih et al., 2013), Minecraft (Fan et al., 2022; Wang et al., 2023), StarCraft (Vinyals et al., 2019), NetHack (K\u00fcttler et al., 2020; Lowe et al., 2020), Werewolf (Xu et al., 2023); However, our work diverges from all of them in that we treat AI as the playground, attempting to build a game engine that is defined by instructions (game scripts) and game states. The former focuses on the way AI behaves, while the latter focuses on the way AI would react in the face of any possible behaviors from human beings and agents. More recent work comes up with learning for a foundation agent, a single agent with generalizable skills to behave in various environments, e.g. SIMA (Team et al., 2024b), an instruction-driven agent proficient in multiple simulated environments; CRADLE (Tan et al., 2024), a powerful agent capable of playing complex AAA games like Red Dead Redemption 2 by controlling the keyboard and mouse. However, our work targets the IDGE for a specific group of games, Poker, as an initial step for building a foundation IDGE. Poker is a widely studied information game of immense popularity (Bowling et al., 2017; Moravc\u00edk et al., 2017; Gupta, 2023; Kim, 2023; Zhao et al., 2022).\nIn this paper, the entire training cycle for IDGE is a way of curriculum learning (Bengio et al., 2009). Recent studies show the potential of curriculum learning in empowering the language models to tackle more challenging tasks (Vakil and Amiri, 2023; Wu et al., 2023a). The proposed segment rephrasing technique is related to perturbation training (Zhu et al., 2020; Wu et al., 2023b), which smooths the structured natural language in the semantic space."}, {"title": "Out-of-Domain Game Scripts", "content": ""}, {"title": "System Demonstration", "content": ""}]}