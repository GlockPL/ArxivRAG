{"title": "SIDDA: SInkhorn Dynamic Domain Adaptation for Image\nClassification with Equivariant Neural Networks", "authors": ["Sneh Pandya", "Purvik Patel", "Brian D. Nord", "Mike Walmsley", "Aleksandra \u0106iprijanovi\u0107"], "abstract": "Modern neural networks (NNs) often do not generalize well in the presence of a \"covariate shift\";\nthat is, in situations where the training and test data distributions differ, but the conditional distribution of\nclassification labels given the data remains unchanged. In such cases, NN generalization can be reduced to a\nproblem of learning more robust, domain-invariant features. Domain adaptation (DA) methods include a\nbroad range of techniques aimed at achieving this; however, these methods have struggled with the need\nfor extensive hyperparameter tuning, which then incurs significant computational costs. In this work, we\nintroduce SIDDA, an out-of-the-box DA training algorithm built upon the Sinkhorn divergence, that can\nachieve effective domain alignment with minimal hyperparameter tuning and computational overhead. We\ndemonstrate the efficacy of our method on multiple simulated and real datasets of varying complexity,\nincluding simple shapes, handwritten digits, and real astronomical observations. These datasets exhibit\ncovariate shifts due to noise, blurring, and differences between telescopes. SIDDA is compatible with a variety\nof NN architectures, and it works particularly well in improving classification accuracy and model calibration\nwhen paired with symmetry-aware equivariant neural networks (ENNs). We find that SIDDA consistently\nenhances the generalization capabilities of NNs, achieving up to a \u2248 40% improvement in classification\naccuracy on unlabeled target data, while also providing a more modest performance gain of 1% on labeled\nsource data. We also study the efficacy of DA on ENNs with respect to the varying group orders of the\ndiedral group DN, and find that the model performance improves as the degree of equivariance increases.\nFinally, we find that SIDDA enhances model calibration on both source and target data, with the most\nsignificant gains in the unlabeled target domain-achieving over an order of magnitude improvement in\nthe expected calibration error and Brier score. SIDDA's versatility across various NN models and datasets,\ncombined with its automated approach to domain alignment, has the potential to significantly advance\nmulti-dataset studies by enabling the development of highly generalizable models.", "sections": [{"title": "1. Introduction", "content": "Deep neural networks (NNs) excel at extracting complex\nfeatures from data, making them a powerful tool for a\nwide range of tasks, including classification, regression,\nand anomaly detection. Unfortunately, some extracted\nfeatures can be very dataset-specific, which makes\nit challenging for NN models to generalize to data\nthat differs from the training data, even when the\ndifferences are subtle. For instance, a significant drop in\nperformance occurs when the input distribution changes\nbetween the training and test datasets, despite the\nconditional distribution of the labels given the inputs\nremaining the same\na scenario commonly referred to\nas a \"covariate shift\" [Farahani et al., 2020, Liu et al.,\n2022].\nGeneralization allows models to perform well across\ndiverse data domains, ranging from subtle variations\nin input distributions to entirely different datasets or\nenvironments. Differences between training and testing\ndata can be due to data origin or quality [Dodge and\nKaram, 2017, Gide et al., 2016, Dodge and Karam,\n2016, Ford et al., 2019], or even single-pixel level\ndifferences, which can cause the NN to give inaccurate\npredictions [Su et al., 2019]. Generalization capabilities,\nin turn, aid the efficiency and applicability of NNs in\nboth science and industry, as they would otherwise need\nto be continually retrained on new data. For example,\nin astronomy, a generalized model trained on data from\none telescope should accurately predict properties of\ndata from another telescope that has different noise\ncharacteristics or resolution, significantly accelerating\nthe process of identifying or characterizing celestial\nobjects across surveys.\nDomain adaptation (DA) is a group of methods\nthat aim to improve the generalization capabilities of\nNNs by enabling the NN to learn features in the data\nthat persist across domains [Wang and Deng, 2018,\nCsurka, 2017, Wilson and Cook, 2020]. It is often\napplied to problems where one has access to labeled data\nfrom a \"source\" domain, but would also like the model\nto perform well on unlabeled \"target\" domain data. A\nlarge group of distance-based DA methods tackles the\ncovariate shift problem by minimizing some distance\nmetric between internal NN latent representations\n(distributions) of the source and target data. This, in\nturn, forces the NN to extract mainly domain-invariant\nfeatures, which makes both latent data distributions\nwell-aligned.\nSome well-known distance-based DA methods use\nmaximum mean discrepancy (MMD) [Gretton et al.,\n2008], correlation alignment (CORAL) [Sun et al., 2016],\ncontrastive domain discrepancy (CDD) [Kang et al.,\n2019], the Kullback-Leibler (KL) divergence [Kullback\nand Leibler, 1951], or the Wasserstein distance [Panare-\ntos and Zemel, 2018], which is derived from the optimal"}, {"title": "2. Methods", "content": "The major components of our work are DA and\nequivariance, which we combine to create a more\nefficient and robust NN classifier. Within DA, we\nimplement the Sinkhorn divergence, a symmetrized\nand regularized variant of OT distances that offers\nconsiderable improvement in DA over traditional\nmethods. We construct a training program that\nconstantly adjusts the loss landscape and regularization\nstrength of the Sinkhorn plan, offering optimal domain\nalignment with minimal hyperparameter tuning."}, {"title": "2.1. Domain Adaptation", "content": "DA comprises a set of techniques aimed at aligning the\nlantent distributions of NNs in the presence of covariate\nshifts in data. Typically, DA operates in settings where\none can access labeled source images $x \\in X_s \\subseteq R^{m\\times m}$,\nand unlabeled target images $x^* \\in X_t \\subseteq R^{m\\times m}$ from\n$X_s$ and $X_t$ source and target data domains, where $m$\ndenotes the number of pixels in each dimension (height\nand width) of the image.\nConsider the latent vectors $z \\in Z_s \\subseteq R^l$ and $z^* \\in\nZ_t \\subseteq R^l$, where $Z_s$ and $Z_t$ denote the latent spaces of the\nsource and target domains, respectively, and $l$ represents\nthe dimension of the latent vectors (i.e., the width\nof the corresponding neural network layer). Latent\ndistributions refer to the probability distributions over\nthese latent vectors, and during training, DA minimizes\na statistical distance measure between them. DA is\nincorporated through an additional loss term, $L_{DA}$,\nalongside the standard task loss (e.g., cross-entropy for\nclassification, $L_{CE}$), to promote alignment between\nthe two latent distributions. The total loss function is then:\n$L_X \\coloneqq L_{CE} + L_{DA}$.\nIn practice, a delicate balance between the two terms\nmust be achieved to ensure proper alignment.\nThere are numerous DA methods, each with its\nown strengths and limitations. One commonly used\napproach is MMD [Gretton et al., 2008, 2012], where the\ndistance between the means of the latent embeddings\nfrom the source and target domains serves as the DA\nloss function. In DA, comparisons are often made\nbetween distributions that are not explicitly known\nbut can be sampled. MMD can be combined with\nkernel methods, which map probability distributions\ninto a high-dimensional reproducing kernel Hilbert\nspace (RKHS) [Gretton et al., 2012], providing a more\nflexible method for comparing distributions. This\napproach allows for analyzing distributions through well-\ndefined operations in the RKHS, even when the original\ndistributions are not well-defined. The MMD between\ntwo probability distributions $\u03bc$ and $\u03bd$ - representing\ndistributions over latent vectors $z$ and $z^*$, respectively\nis\n$MMD(\u03bc, \u03bd) = | E_{z,z'\\sim \u03bc} [k(z, z')] + E_{z^*,z^{*'}\\sim \u03bd} [k(z^*, z^{*'})]$\n$-2E_{z\\sim \u03bc, z^*\\sim \u03bd} [k(z, z^*)]|^{1/2}$,\nwhere $k$ represents the kernel function, and $z, z'$, and\n$z^*, z^{*'}$ are individual samples from latent distributions\n$\u03bc$ and $\u03bd$, respectively.\nDespite its utility, MMD has several theoretical\nand implementation-related shortcomings. First, its\nefficacy is highly sensitive to the choice of $k$. In\ntypical applications, the Gaussian kernel $k(z, z^*) =$\n$exp(-\\frac{||z-z^*||^2}{\u03f5})$ is used with kernel bandwidth $\u03f5$. Other\nkernel options include the linear kernel $k(z, z^*) =$\n$z^Tz^*$, the Laplacian kernel $k(z, z^*) = exp(-\\frac{||z-z^*||}{\u03f5})$,\nand others. Most kernels generally belong to a one-\nparameter family (e.g., $\u03f5$ for Gaussian and Laplacian\nkernels) and must be carefully tuned, or complex linear\ncombinations of kernels with many different parameter\nvalues must be used. The specific choice of kernel\ndepends heavily on the nature of the problem. That"}, {"title": "2.2. Optimal Transport and The Sinkhorn Divergence", "content": "OT distances and their symmetrized variants, such as\nSinkhorn divergences, offer an alternative to MMD.\nTraditionally, computing OT is prohibitively expensive\n[Peyr\u00e9 and Cuturi, 2020]. Entropic regularization, OT,\n[Dessein et al., 2018] provides a more efficient method\nfor estimating OT distances. The regularized OT is\ndefined as\n$OT_\u03c3 (\u03bc, \u03bd) = \\underset{\u03b3\u2208U(\u03bc,\u03bd)}{min} \\sum_{i,j} \u03b3_{ij}d(z_i, z_j)^p + \u03c3H(\u03b3)$,\nwhere $d(z_i, z_j)^p$ is the distance between source feature\n$z_i$ and target feature $z_j^*$. When $p = 1$, this distance\nbecomes the Earth Mover's distance [Rubner et al.,\n1998], and when $p = 2$, it becomes the quadratic\nWasserstein distance. The transport plan $\u03b3 \u2208 U(\u03bc, \u03bd)$ is\na joint probability distribution between $\u03bc$ and $\u03bd$, where\nthe set of admissible transport plans $U(\u03bc,\u03bd)$ is defined\nby the marginal constraints:\n$\\sum_j \u03b3_{ij} = \u03bc_i, \\sum_i \u03b3_{ij} = \u03bd_j$.\nThe entropy $H(\u03b3) = \u2212 \\sum_{i,j} \u03b3_{ij} log \u03b3_{ij}$ regularizes the\ntransport plan $\u03b3$, and $\u03c3$ controls the regularization\nstrength. One limitation of OT, is that $OT_\u03c3(\u03bc, \u03bc) \\neq 0$,\nimplying a non-zero cost even when transporting a\ndistribution to itself, leading to bias in the measure.\nTo correct this bias, the Sinkhorn divergence\n$S_\u03c3(\u03bc,\u03bd)$, defined as\n$S_\u03c3 (\u03bc,\u03bd) = \\frac{1}{2} OT_\u03c3(\u03bc,\u03bd) - \\frac{1}{2}OT_\u03c3(\u03bc,\u03bc) - \\frac{1}{2}OT_\u03c3(\u03bd,\u03bd)$,\ncan compensate for the bias in OT [Feydy et al., 2018].\nAs $\u03c3 \u2192 0$, $S_\u03c3(\u03bc,\u03bd)$ converges to the (biased) optimal\ntransport $OT_0$, and as $\u03c3 \u2192 \u221e$, it interpolates towards\nMMD loss [Feydy et al., 2018]. For small values of $\u03c3$,\nan unbiased transport plan that still enjoys the benefits\nof OT-based distances can be constructed."}, {"title": "2.3. Dynamic Sinkhorn Divergences for Domain\nAdaptation", "content": "For this work, $L_{DA}$ in Equation 1 is specifically the\nSinkhorn divergence $S_\u03c3(\u03bc,\u03bd)$. However, a careful\nbalance between $L_{CE}$ and $L_{DA}$ must be achieved to\noptimize the classification task while simultaneously\nmaximizing domain alignment.\nFinding the best weights for each of the loss\nterms can be very challenging and time-consuming.\nFurthermore, a single choice of weights might not\nbe the best choice throughout the whole training\nprocedure. To manage this balance, we employ dynamic\nweighting of the losses by introducing two trainable\nparameters, $\u03b7_1$ and $\u03b7_2$, which dynamically adjust the\ncontributions of the loss terms for each task. These\nparameters ensure that no single loss term dominates\nthe optimization process, allowing the loss landscape\nto be optimally adjusted for both tasks. Drawing\ninspiration from Kendall et al. [2018], we use the\nfollowing for the total loss function:\n$L = \\frac{1}{2\u03b7_1}L_{CE} + \\frac{1}{2\u03b7_2}L_{DA} +log(\u03b7_1\u03b7_2|)$,\nwhere $\u03b7_1$ and $\u03b7_2$ are trainable scalars, and their values\nare jointly learned with the model weights during\ntraining. The inclusion of the term $log(\u03b7_1\u03b7_2)$ acts\nas a regularization to prevent $\u03b7_1$ and $\u03b7_2$ from collapsing\nto unstable values, such as zero. As $\u03b7_i \\to 0$, the\ncorresponding loss term is more heavily weighted. To\nensure that no single component dominates, we impose\nthe additional constraint $\u03b7_2 \u2265 0.25\u03b7_1$. In general, the\nDA term must not dominate over the classification\nloss, which the above inequality enforces. For our\nimplementation, we found that this threshold worked\nbest, but such a cutoff may not always be optimal.\nIn Kendall et al. [2018], the two weight terms $\u03b7_1$\nand $\u03b7_2$ were introduced for the dynamic weighting of the\nlosses. These terms explicitly minimize the regression\nuncertainty associated with each loss term, as their\nmodel outputs a Gaussian distribution with variance $\u03b7_i^2$\nfor each task. In the case of classification, their weight\nterms become $1/\u03b7_i^2$. Since uncertainties are not one of\nthe network outputs in our case, the exact written form\nof loss weights is not important and the extra factor of\ntwo can very well be absorbed into the trainable weight\nparameter.\nThe level of regularization $\u03c3$ in $S_\u03c3(\u03bc,\u03bd)$ is another\ncritical hyperparameter [Feydy et al., 2018]. When $\u03c3$ is\ntoo small, the transport plan's entropic regularization\ndiminishes, and the OT plan's bias makes it susceptible\nto overfitting to specific sample locations, hindering\nthe overlap between $\u03bc$ and $\u03bd$ latent distributions.\nConversely, if $\u03c3$ is too large, the regularization\ninterpolates toward MMD, removing the unique benefits\nof using $S_\u03c3(\u03bc,\u03bd)$. To address this, we adopt a unique,\ndynamic regularization per epoch of training $\u2113$, $\u03c3_\u2113$,\nwhere the transport plan is continually updated. We\ncompute $\u03c3_\u2113$ iteratively as:\n$\u03c3_\u2113 = max(0.05 \\cdot (0.05-max D_{ij}, 0.01))$ .\nIn this formulation, $\u03c3_\u2113$ is dynamically adjusted based\non the maximum pairwise distance $D_{ij} = ||z_i - z_j||^2$"}, {"title": "2.4. The Jensen-Shannon Distance", "content": "Recent advancements in DA theory have introduced\nthe Jensen-Shannon (JS) divergence [Lin, 1991] as\na fundamental tool for understanding the inherent\nlimitations of DA [Shui et al., 2022]. The JS divergence\nis a symmetrized statistical distance metric. For two\nlantent distributions $\u03bc$ and $\u03bd$, the JS divergence $D_{JS}$ is\ndefined as\n$D_{JS}(\u03bc||\u03bd) = \\frac{1}{2} D_{KL}(\u03bc||\u03c4) + \\frac{1}{2} D_{KL} (\u03bd||\u03c4)$,\nwhere $D_{KL}$ denotes the KL divergence [Kullback and\nLeibler, 1951], defined as\n$D_{KL} (\u03bc || \u03bd) = \\int p(z)log(\\frac{p(z)}{q(z)}) dz$.\nHere, $p$ and $q$ denote probability densities of the latent\nfeatures $z$ in the source and target distributions $\u03bc$ and\n$\u03bd$. $\u03c4 = \\frac{(\u03bc+\u03bd)}{2}$ represents the mixture distribution of $\u03bc$\nand $\u03bd$. The JS divergence offers two key advantages over\n$D_{KL}$: it is symmetric (since, in general, $D_{KL}(\u03bc||\u03bd) \\neq$\n$D_{KL}(\u03bd|\u03bc)$), and it is always finite. Additionally, the\nsquare root $\\sqrt D_{JS}$ defines a metric known as the Jensen-\nShannon distance.\nFor DA applications, there exists a lower bound\non the target domain loss [Shui et al., 2022]:\n$L_t(z^*) \u2265 L_s(z) \u2013 \\sqrt{ D_{JS}(\u03bc||\u03bd)}$,\nwhere $L_s$ is the source domain loss, $L_t$ is the target\ndomain loss, and $D_{JS}(\u03bc||\u03bd)$ is the JS distance\nbetween the source and target latent distributions\n$\u03bc$ and $\u03bd$, respectively. This bound emphasizes that\nperfect alignment between source and target domains\nis fundamentally constrained by two components: $L_s$\nand the JS distance between the source and target\ndistributions. A smaller JS distance implies that the\nfeature distributions of the source and target domains\nare closely aligned, which lowers the bound on $L_t$ and\nenables better transferability of the learned model.\nConversely, a larger JS distance indicates a greater\ndiscrepancy, limiting the potential for minimizing target\ndomain loss through adaptation alone.\nThe similarity between the source and the target\nlantent distributions $\u03bc$ and $\u03bd$ is inherently influenced\nby the feature extraction capabilities of the neural\nnetwork. DA methods aim to align features in the latent\ndistribution; however, these features are ultimately\nlimited by the network architecture. As a toy example,\nconsider the case of image classification, where the\narchitecture is a multi-layer perceptron (MLP). Many\nimage classification tasks exhibit translation invariance,\ninherent in CNNs, but not in MLPs. DA on this\ntask with CNNs will likely be more successful than\nwith MLPs, as the translation invariance of the CNN\nfurther restricts the allowable features, and thus, the\ncost of alignment will be smaller. In particular,\nwe define \"robust\" features as those that respect\nthe underlying data symmetries. More specifically,\nthey yield similar classification probabilities under\nisometries that preserve the symmetries of the images.\nIf these symmetries persist in both the source and\ntarget domain, which is typically true except for\nextreme symmetry-breaking perturbations, then the\ncost of aligning robust features will be less than\nfeatures learned from symmetry-agnostic architectures.\nConsequently, it is reasonable to expect that ENNS"}, {"title": "3. Data", "content": "We evaluate the performance of our method on three\nsimulated datasets and one real astronomical dataset:\n(1) a single-channel dataset of shapes consisting of lines,\ncircles, and rectangles; (2) a single-channel dataset\nresembling astronomical objects, including stars, spirals,\nand elliptical galaxies; (3) the multichannel MNIST-M dataset [Ganin et al., 2016]; and (4) the Galaxy\nZoo (GZ) Evo dataset of observed galaxies [Walmsley\net al., 2024]. The shapes and astronomical objects\ndatasets are constructed using DeepBench [Voetberg\net al., 2023]. All datasets used in our experiments can\nbe found on Zenodo."}, {"title": "3.1. Covariate Shifts", "content": "We use images from three simulated datasets, shown\nin Figure 2, to study the performance on induced\ncovariate shifts between the source and target domains.\nFor all of our simulated datasets, we introduce\nfixed levels of Poisson noise in the target domain.\nAdditionally, for MNIST-M, we also study the effects\nof PSF blurring in the target domain. By studying\nthese two distinct covariate shifts, we evaluate the\nrobustness of our method on covariate shifts relevant\nto data in realistic settings, particularly in the context\nof astrophysics and cosmology.\nThis is implemented for an image $I$ with grid values\n$(\u03b6, \u03be)$ and channels $c$ as\n$I_{Poisson} (\u03b6, \u03be, c; S) = I(\u03b6, \u03be, c) + \\frac{P(I(\u03b6, \u03be, c))}{S} + P(-\\frac{P(I(\u03b6, \u03be, c))}{S})$\nwhere $S$ is the signal-to-noise ratio, and $P$ denotes the\nPoisson distribution with rate parameter $\u03bb = |I|$.\nWe incur PSF noise in each image channel by convolving\nthe images with a Gaussian kernel $G$ of kernel width $\u03f5$:\n$I_{PSF} (\u03b6, \u03be) = (I * G)(\u03b6, \u03be)$,\nwhere\n$G(\u03b6, \u03be) = \\frac{1}{2\u03c0\u03f5^2} exp(-\\frac{\u03b6^2+\u03be^2}{2\u03f5^2})$"}, {"title": "3.2. Simulated Images", "content": "For the shapes dataset, we use DeepBench [Voetberg\net al., 2023], an open-source library for generating\nsimulated datasets, and randomly construct rectangles,\nlines, and circles with varying radii (for circles), heights\nand widths (for rectangles), and lengths (for lines). The\nobject positions, orientations, and thickness are also\nrandomly assigned to introduce variance in the dataset.\nPoisson noise in the images is normalized with respect\nto the original image signal. We set a signal-to-noise\nratio $S = 0.05$. Example images from the dataset can\nbe seen in the left two panels of Figure 2.\nWe also use DeepBench for simulating astronomical\nobjects. We generate astronomical objects resembling\nspiral galaxies, elliptical galaxies, and stars. For spiral\ngalaxies, we randomly assign the centroid, winding\nnumber, and pitch to ensure morphological variation.\nThe pitch is the angle indicating how tightly the arms\nare wound, while the winding number measures the\ntotal number of arm rotations from the center to the\ngalaxy's edge. For elliptical galaxies, we vary the\namplitude, radius, ellipticity, S\u00e9rsic index [Sersic, 1958],\nand rotation, as well as the centroid location. The\namplitude sets the brightness level, ellipticity describes\nthe degree of deviation from a circle, the S\u00e9rsic index\ncontrols light concentration (higher values indicate more\ncentral concentration), and the rotation defines the\norientation angle of the galaxy's major axis. Lastly,\nwe apply similar variations to generate stars, with the\nnumber of stars in each image uniformly distributed\nin the range [0,10]. We use a fixed Poisson noise\nlevel of $S = 0.2$ to generate noisy target domain\nimages. This level of noise was chosen as it allows for\na sufficient decrease in target domain performance for\nmodels without DA. Example images are shown in the\nmiddle two panels of Figure 2. Both of these datasets\ncontain 12,000 training images (with 20% being used\nfor validation) and 3,000 test images in each domain.\nThe images are square with 100 pixels on each side,\nand they are single-channel: each sample image has\ndimensions 100 x 100 x 1.\nMNIST-M [Ganin et al., 2016] is a dataset that\ncombines the handwritten digits of MNIST [Deng, 2012]\nwith randomly extracted color photos from BSDS500\n[Arbelaez et al., 2011] as background images. The\noriginal dataset contains 59,001 training images and\n90,001 test images, out of which we use a balanced\nsubset of 15,000 training (with 20% set aside for\nvalidation) and 5,000 testing images in each domain.\nSince this is a three-channel dataset, images have a\ndimension of 32 \u00d7 32 \u00d7 3. We then create two types\nof target domain covariate shifts: 1) we set a signal-to-\nnoise ratio of $S = 0.05$ for Poisson noise, and 2) a kernel\nwidth of $\u03f5 = 2$ for PSF blurring. Example images are\nshown in the right two panels of Figure 2."}, {"title": "3.3. Real-Sky Galaxy Image Dataset", "content": "We use the GZ Evo dataset [Walmsley et al., 2024]\nto test cross-domain robustness in a more realistic\nscenario, where the covariate shift is present due\nto differences between images from two different\nastronomical surveys. These differences are due to\ndifferent levels of observational noise, PSF blurring,\npixel scale, as well as differences in populations of\nobservable astronomical objects (how distant or how\nfaint a resolved object can be). GZ is a citizen\nscience project that labels galaxy images through online\nparticipation. GZ Evo combines labeled image datasets\nacross several surveys and iterations of GZ. Within\nGZ Evo, we use the GZ2 Dataset from the Sloan\nDigital Sky Survey (SDSS) [Willett et al., 2013] as the\nsource, and a GZ Dark Energy Spectroscopic Instrument\n(DESI) dataset that combines observations from the\nDESI Imaging Surveys (DECals, MzLS, BASS, DES)\n[Walmsley et al., 2021, 2023] as the target. Older GZ\nSDSS data contains objects up to magnitude 17 in\nthe r band, and redshifts below 0.25, while newer\nGZ DESI includes fainter objects up to magnitude\n19 and more distant objects with redshifts below 0.4."}, {"title": "4. Network Architectures and Experiments", "content": "We evaluate our method on two sets of NNs: (1) CNNs\nconstructed in PyTorch and (2) ENNs constructed in\nescnn, a PyTorch-based library for easy construction"}, {"title": "4.1. Equivariant Neural Networks", "content": "The efficacy of DA is limited by the feature extraction\ncapabilities of the NN and its performance on the\nsource domain. For image classification tasks, CNNs\nare natural choices due to their translation invariance\nand locality. There are, however, often additional\nsymmetries inherent in the data, such as rotational and\nreflection invariance, that can be leveraged to enhance\nfeature extraction and improve performance.\nENNs are a subclass of CNNs that can exploit\nhigher-order symmetries besides the typical translation\nequivariance of CNNs [Cohen and Welling, 2016a,b]. Of\ninterest for 2D images are symmetries of the Euclidean\ngroup E(2) in particular, the 2D special orthogonal\ngroup SO(2) and the orthogonal group O(2) and its\nassociated subgroups. These (sub)groups allow ENNs to\ninherit symmetries of the circle and N-gon, respectively.\nAs SO(2) and O(2) are continuous, they contain an\ninfinite number of irreducible representations and have\nassociated challenges when constructing architectures.\nFor this reason, the discrete subgroup of O(2), the\ndiedral group DN, is used in this work, which is\nstraightforward to construct using open-source software,\nsuch as escnn [Cesa et al., 2022, Weiler and Cesa, 2019].\nFor image data (particularly astronomical data),\nrotational symmetry (with or without reflections) is"}, {"title": "4.2. Training", "content": "We train all networks typically ($\\mathcal{L} = L_{CE}$) and with\nSIDDA (Equation 6) and study performance differences\nin the source and target domain for the two techniques.\nIn all experiments, we use the AdamW optimizer\n[Loshchilov and Hutter, 2019] with an initial learning\nrate of $10^{-2}$, a weight decay of $10^{-3}$, and a batch size\nof 128. A multiplicative learning rate decay of 0.1 is\napplied twice sequentially during training to stabilize\nconvergence.\nFor all experiments, we use data augmentation\ncomprising random rotations, flips, and affine transla-\ntions. For the CNN, the augmentation instills approx-\nimate equivariance to encourage the model to learn\nrotation-invariant features. This is done to prevent\npredisposing the ENNs to perform better, but it also\nencourages faster convergence for the CNN. This later\nallows a more fruitful comparison between the latent dis-\ntributions between ENNs, which have inherent rotation\ninvariance to discrete rotations, and the CNNs, which\nhave approximate invariance [Hansen et al., 2024].\nFor experiments with DA, an initial warm-up\nphase is implemented, during which only classification\ntasks are trained (using only $L_{CE}$). A similar 0.1\nmultiplicative learning rate decay as in the case of\nexperiments without DA is also used. Early stopping\nand model-saving criteria are based on the following:\nfor experiments without DA, we use the best validation\nloss on the source domain for classification; for\nDA experiments, we use the sum of the validation\nclassification loss on the source domain and validation"}, {"title": "4.3. Calibration", "content": "Despite the impressive predictive capability of NNs\nin classification tasks across various fields, many real-\nworld applications of NN-based classifiers also consider\nthe confidence of each output class. Specifically,\nmany NN-based classifiers can be uncalibrated,\nwherein the predicted class probabilities can frequently\nmisrepresent the true class likelihood and lead to\nunder or overconfident predictions. In data-sensitive\nor safety-sensitive settings such as medicine and\nbiology-proper model calibration is essential for\ndeploying NN-based classifiers [Carse et al., 2022].\nSimilarly, in cosmology, simulation-based inference\n(SBI) pipelines that rely on trained classifiers must\nensure proper calibration to guarantee that the inferred\nlikelihood ratios or posterior probabilities are accurate\nand trustworthy [Cole et al., 2022].\nCalibration techniques vary from regularization\nduring training (either through architectural choices or\nadditional loss terms) to post hoc methods that scale\npredicted probabilities (see Wang [2024] for a review).\nDA-based methods, however, have traditionally not\nbeen considered in the realm of regularization methods\nfor model calibration. We will show that including DA\nwith SIDDA not only improves accuracy (see Sections\n5.1 and 5.2), but also calibration.\nWe evaluate model calibration using the Brier score\nand the Expected Calibration Error (ECE). The Brier\nscore is the mean prediction error over all the classes:\n$Brier Score = \\frac{1}{C} \\sum_{i=1}^{C} (y_i - \\delta_{i\u0177})^2$"}, {"title": "4.4. Neural Network Latent Distributions", "content": "The distributions over the source and target latent\nencodings, $z$ and $z^*$, are the fundamental objects used\nin DA techniques. Probing the latent distributions\ncan give crucial insights into the success and failure\npoints of DA. The dimensionality of latent distributions\nis typically too large for visualization and analysis\n(256 in experiments used in this work). Therefore,\ndimensionality reduction techniques are often employed\nbefore visualizing the latent distributions. Techniques\nlike t-SNE and UMAP [van der Maaten and Hinton,\n2008, McInnes et al., 2018] use local metrics, like\npairwise distances or nearest-neighbor graphs to\npreserve the structure of the data at small scales while\nembedding it into a lower-dimensional space. However,\nlocal metrics, and therefore these techniques, are limited\nbecause they primarily focus on preserving relationships\nwithin small neighborhoods of the data, often at the\nexpense of capturing global structures or long-range\ndependencies that are critical for understanding the\noverall geometry or topology of the dataset. In contrast,\nthe isomap [Tenenbaum et al., 2000] is a non-linear\ndimensionality reduction technique that estimates the\nglobal geometry of a latent vector manifold by using\ninformation of the nearest neighbors for each point in\nthe latent space.\nIn this work, we use isomaps to visualize latent\ndistributions, and we use the mean Silhouette score to\nquantify the inter-class (between clusters) and intra-\nclass (within a cluster) distances and evaluate the\nquality of the clustering. The silhouette score is\n$S = \\frac{1}{Q} \\sum_{i=1}^{Q} \\frac{b(i) \u2013 a(i)}{max(a(i), b(i))}$"}, {"title": "5. Results", "content": "All results are computed from three trained NNs, each\nwith a different random seed for initializing weights.\nFor each set of three trained networks, we estimate\n10 uncertainties on our diagnostic metrics. We refer\nto a model trained without DA (i.e., only with cross-\nentropy loss) as \u201c<model>\u201d e.g., \"CNN\" or \"D4\". In\ncontrast, we refer to a model trained with SIDDA as\n\u201c<model>-DA\u201d e.g., \"CNN-DA\" or \"D4-DA\"."}]}