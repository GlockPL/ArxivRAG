{"title": "SEEING EYE TO AI: HUMAN ALIGNMENT VIA GAZE-BASED RESPONSE REWARDS FOR LARGE LANGUAGE MODELS", "authors": ["Angela Lopez-Cardona", "Carlos Segura", "Alexandros Karatzoglou", "Sergi Abadal", "Ioannis Arapakis"], "abstract": "Advancements in Natural Language Processing (NLP), have led to the emergence of Large Language Models (LLMs) such as GPT, Llama, Claude, and Gemini, which excel across a range of tasks but require extensive fine-tuning to align their outputs with human expectations. A widely used method for achieving this alignment is Reinforcement Learning from Human Feedback (RLHF), which, despite its success, faces challenges in accurately modelling human preferences. In this paper, we introduce GazeReward, a novel framework that integrates implicit feedback \u2013 and specifically eye-tracking (ET) data \u2013 into the Reward Model (RM). In addition, we explore how ET-based features can provide insights into user preferences. Through ablation studies we test our framework with different integration methods, LLMs, and ET generator models, demonstrating that our approach significantly improves the accuracy of the RM on established human preference datasets. This work advances the ongoing discussion on optimizing AI alignment with human values, exploring the potential of cognitive data for shaping future NLP research.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in Natural Language Processing (NLP) have led to the emergence of Large Language Models (LLMs) like GPT [OpenAI, 2023], Llama [Dubey et al., 2024], Claude [Anthropic, 2024], and Gemini [Team et al., 2024], which excel across a range of tasks. These models, often consisting of billions of parameters, are trained on massive datasets and typically require extensive fine-tuning to align their outputs with human expectations 1. Several works have focused on refining the way LLMs interpret and respond to user intent [Wang et al., 2023a], which has led to the development of novel alignment techniques. A common approach to achieving human alignment involves leveraging explicit human feedback as preference information. Currently, the most widely adopted method is Reinforcement Learning from Human Feedback (RLHF) [Ouyang et al., 2022]. RLHF has been implemented in many state-of-the-art LLMs [Cui et al., 2024, OpenAI, 2023, Bai et al., 2022a], and has been shown to help align models to human instructions and mitigate the generation of toxic or harmful content [Kiegeland et al., 2024]. However, a persistent challenge with this approach is the difficulty of acquiring sufficient high-quality training data [Casper et al., 2023].\nTo be able to capture the complexities of real-world user instructions, there is a need for meticulously handcrafted data [Wang et al., 2023a], which are resource-expensive and difficult to scale [Yang et al., 2023]. Obtaining high-quality feedback from human annotators, usually provided after examining a model response, suffers from several caveats [Casper et al., 2023]. For instance, low inter-annotator agreement can result in inconsistent evaluations of the same model output due to varying interpretations, domain expertise, or biases. Moreover, \u201cscalable oversight\u201d \u2013 the ability to"}, {"title": "2 Preliminaries", "content": null}, {"title": "2.1 Large Language Models-Human Alignment", "content": "LLMs-Human Alignment typically involves training LLMs 2 on datasets curated by humans (learning from human feedback data) [Ouyang et al., 2022]. This can be achieved through Supervised Fine-Tuning (SFT), where the model is trained on pairs of prompts (x) and corresponding human-generated responses (y) [Liu et al., 2024]. Alternatively, alignment can be pursued via preference optimization, using a human preference dataset that differentiates between a better response (yw) and a worse one (yi) for the same prompt (x): D =\n{(x(i),y(i)w,y(i)i)}1.\nTo this day, RLHF [Ouyang et al., 2022] remains the most popular technique used in state-of-the-art LLMs like GPT-4 [OpenAI, 2023], Claude [Bai et al., 2022a], Bard [Google, 2023], and Llama 2-Chat [Touvron et al., 2023]. Different implementations of RLHF can vary in terms of data collection, training processes, and choice of RL algorithms. Typically, RLHF [Ouyang et al., 2022] involves three main steps: (1) collecting feedback, (2) training a RM based on that feedback, and (3) optimising the LLMs using RL techniques, such as Proximal Policy Optimization (PPO) [Schulman et al., 2017]. Since RLHF was first introduced, several advancements have been made, including fine-grained reward systems [Bai et al., 2022a, Wu et al., 2023a, Dong et al., 2023a, Wang et al., 2023c, 2024b], or replaced the original PPO algorithm with other RL techniques [Wu et al., 2023b].\nAn alternative to RLHF is DPO [Rafailov et al., 2023], which employs an offline RL approach to optimize language models based on preference data, without the need for a separate reward model. While DPO can be used independently, it is often complementary to other training methods such as SFT or statistical rejection sampling, to further improve human alignment based on a RM [Zhao et al., 2023, Liu et al., 2024, Dubey et al., 2024]. Statistical rejection sampling, also called best-of-N or top-k-over-N [Bai et al., 2022a, Touvron et al., 2023, Dubey et al., 2024] is another widely used technique. Moreover, certain methods perform human alignment without RL to avoid instabilities, and fine-tune the model on filtered samples by a RM, or other sources [Dong et al., 2023b, Yuan et al., 2023].\nA significant challenge in all human alignment techniques is data acquisition [Casper et al., 2023]. This includes problems such as evaluator misalignment, supervision difficulties, and feedback quality issues [Casper et al., 2023]. However, as AI systems continue to improve, LLMs are increasingly employed for tasks that were traditionally handled by humans, such as data annotation and generation. Unlike human feedback, AI-generated feedback offers better scalability, enabling faster and more cost-effective data collection. For example, RLAIF, introduced by Bai et al. [2022a], is a promising approach that trains reward models based on preferences generated by off-the-shelf LLMs. Variations of RLAIF have been explored in several studies [Lee et al., 2023, Jiao, 2023, Cui et al., 2024, Li et al., 2024, Yang et al., 2024]. In the context of self-generating instructions, approaches like Self-Instruct [Wang et al., 2023b], Self-Refine [Madaan et al., 2023], and Self-Alignment [Li et al., 2023] demonstrate how models can autonomously generate datasets based on their learned human preferences.\nDifferent alignment methods like RLHF and RLAIF rely on the RM to incorporate the human feedback. The RM learns to predict human preference based on labeled examples, serving as a proxy for human judgment later. Therefore, the success of language model alignment relies heavily on the quality of the underlying reward model [Pace et al., 2024], which in turn dictates the behaviour of the resultant chatbot [Shen et al., 2023]. Even in LLM inference, methods like best-of-N sampling use the RM to evaluate model outputs [Cui et al., 2024]. RM has also become crucial for generating synthetic data for preference alignment. In recent RLAIF methods, reward modeling has expanded beyond its traditional role and is now used to generate artificial feedback."}, {"title": "2.1.1 Reward modeling", "content": "In the original implementation [Ouyang et al., 2022], the goal of RM training is to train a classifier that predicts the probability of human preference p* between two completions (Equation 1), modelled by a Bradley-Terry model [Bradley and Terry, 1952]. The typical setup involves showing two completions, with preferences being measured using win-loss-tie outcomes or a Likert scale to capture the strength of preference [Bai et al., 2022b]. The data is processed into prompt-chosen-rejected trios, where the chosen completion, yw, is preferred over the rejected one, y\u0131, forming the basis for training [Ouyang et al., 2022]."}, {"title": "2.2 Eye-Tracking", "content": "Eye-tracking (ET) systems monitor oculomotor behavior, such as eye movements and fixations, offering valuable insights into visual attention, information processing, and expands our understanding of reading and language comprehension. [Zhang and Hollenstein, 2024]. Specifically, ET data often include fixations \u2013 pauses in eye movement to focus on specific areas [Mathias et al., 2022]; saccades \u2013 rapid movements between two points [McGuire and Tomuro, 2021]; scanpaths - sequences of fixations that reveal saccades and regressions [Yang and Hollenstein, 2023]; and other temporal and spatial gaze behavior features [Zhang and Hollenstein, 2024]. Incorporating ET data into NLP tasks often involves the use of several features listed in Table 1.\nWhile several publicly available datasets such as ZUCO [Hollenstein et al., 2020a], ZUCO2 [Hollenstein et al., 2018], PROVO [Luke and Christianson, 2018], ETSA-I [Mishra et al., 2016], ETSA-II [Mishra et al., 2018], GECO [Cop et al., 2017], GECO-MT [Colman et al., 2022] are widely used in ET research, obtaining real ET data for NLP tasks remains a challenge. This is primarily due to the cost and precision requirements of ET equipment, the unavailability of gaze data during inference, as well as privacy concerns [Khurana et al., 2023]. To address these challenges, two main approaches have been proposed. The first involves integrating ET data into the model during training through methods like Multi-task learning (MTL), which eliminates the need for ET data during inference [Mishra et al., 2018, Klerke et al., 2016, Ren and Xiong, 2023, Yu et al., 2024, Deng et al., 2024]. The second approach involves techniques that directly predict users' gaze behaviour [Deng et al., 2024, 2023a, Zhang and Hollenstein, 2024, Wang et al., 2024a], creating synthetic ET data for any given text or stimulus [Deng et al., 2023b, Bolliger et al., 2023, Khurana et al., 2023, Li and Rudzicz, 2021, Hollenstein et al., 2021, 2022]."}, {"title": "3 GazeReward: Reward modeling with ET Feedback", "content": "In this section, we discuss the proposed framework for augmenting the RM using implicit feedback derived from ET signals (Figure 2). Initially, we generate the ET features (subsection 3.1). For this step, we consider two state-of-the-art ET prediction models. Next, we combine the ET features with the text (subsection 3.2), producing different types of combined embeddings, and finally pass them as input into the RM to obtain the reward for the prompt and its corresponding response (subsection 3.3)."}, {"title": "3.1 Eye-tracking features generation", "content": "As discussed insubsection 2.2, obtaining organic ET features for NLP applications presents several challenges. In this work, we consider an approach inspired by RLAIF research, where feedback is artificially generated from pre-trained LLMs and, in particular, from ET prediction models. Specifically, we incorporate the output of two different ET prediction models [Li and Rudzicz, 2021, Huang and Hollenstein, 2023] and evaluate the impact of different set of features. As input to these models, we pass the same text as we do in the RM: a combination of prompt x and response y. The output is a set of ET features, denoted as fet, for each token fet = { f1, f2, ..., fw } \u2208 Rw\u00d7f, where w represents the number of tokens in the tokenizer used by the ET prediction model, and f is the number of features. Depending on the specific model, between one and five synthetic features fet = {f1, f2,..., fw} \u2208 Rw\u00d7f are generated per token for the input text."}, {"title": "3.2 RM augmentation using eye-tracking features", "content": "We implement two different approaches for incorporating ET features into the RM, as shown in Figure 2. In the first approach, GazeConcat, we concatenate the ET embeddings with the text embeddings. In the second approach, GazeAdd, we add the ET embeddings to the text embeddings. Furthermore, we concatenate the prompt and the response to be evaluated and pass them through the pre-trained embedding layer of RM, to generate the embeddings H = {h1,h2,..., hn} \u2208 Rnd, where n is the number of tokens in the tokenizer used by the RM and d is the model embedding size.\nTo project these features to the model embedding size (d), we use a Multilayer Perceptron (MLP) ET feature projector fp(). The fp() consists of two linear layers, two dropout layers, two Layer Normalization layers, and ReLU activation, designed for stable, non-linear ET feature representation and overfitting prevention. The model's input dimension dynamically adjusts to accommodate the number of features used during training. The ET features projector can be formulated as embETF = fp(fet) \u2208 Rw\u00d7d (Figure 2). This formula describes the projection of ETF features (fet) through a function fp, resulting in an embedding matrix embETF with dimensions w \u00d7 d, where w represents the number of time steps and d the embedding dimension.\nGazeConcat: The ET embedding, denoted as embETF, is concatenated with the text embedding H to form the input for the RM. To distinguish between the two modalities, we introduce two special tokens: <eye> and </eye>, which flag the start and end of the ET embedding, respectively (Figure 2). These special tokens are randomly initialized as one-dimensional vectors and added to the embedding layer or the RM model for training. The final input is structured as: (emb(<eye>) \u25e6 embETF \u00b0 emb(</eye>) \u25cb H). The same process is applied to the attentions masks.\nGazeAdd: The input to the RM consists of the ET embedding embETF and the text embedding H, which are added in an elementwise fashion: (embETF + H). The two ET prediction models use different tokenizers, which also differ from those used by the base models in the RM. As a result, the number of tokens n in the input for the RM and the number of tokens w generated by the ET prediction model may not match. To address this embedding alignment issue, and have the same dimension, we remap the ET features from the w-token space to the n-token space used by each base model in the RM. Further implementation details can be found in Appendix A.1.3."}, {"title": "3.3 Reward Model", "content": "The RM's architecture and hyperparameters are identical to those of the pretrained LLM, except that the classification head used for next-token prediction is replaced with a regression head that outputs a scalar reward [Touvron et al., 2023]. This scalar reward indicates the quality of the model generation, corresponding to the predicted score for the final reply in a conversation. Differences in these rewards represent the log-odds that one response is preferred over another. The loss function is defined in Equation 2, where yw refers to the preferred response in a pair of completions yw and y\u0131. The dataset D consists of human comparisons, where r\u00f8 (x, yw), r\u00f8 (x, y\u0131) represents the RM 0 scalar outputs for the preferred and less preferred completions, respectively Ouyang et al. [2022].\nloss(0) = -E(x,yw,y1)~D [log (\u03c3 (r\u00f8 (x, Yw) ro (x, y)))]\nIn the proposed method, we augment the traditional RM, which uses text input (a combination of the prompt x and response y), by incorporating (artificial) implicit feedback through ET features generated from the same text. These ET features provide valuable information for capturing human preferences, thereby improving the RM's performance."}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Experimental setup", "content": "Datasets. For our experiments, we use the OpenAssistant Conversations dataset's (OASST1) [K\u00f6pf et al., 2023] and HelpSteer2 [Wang et al., 2024b] (Table 2). OASST1 is a human-generated, human-annotated, assistant-style conversation, created through global crowdsourcing and widely used for human alignment tasks [K\u00f6pf et al., 2023, Dettmers et al., 2023, Wu et al., 2024a]. We filtered all non-English text, as the ET prediction models were exclusively trained on English data. Among the different responses in the dataset, we selected the two most distinct responses to compare the chosen and the rejected responses [Wang et al., 2024c]. HelpSteer2 is a more recent, English-only dataset that has been used in studies such as Wang et al. [2024c,b]. The dataset provides annotations for five response attributes: helpfulness, correctness, coherence, complexity, and verbosity. To transform it into a preference dataset, we designate the response with the higher helpfulness score as the chosen response and the other as the rejected response, following a method similar to that used in DPO training [Wang et al., 2024b] (see Appendix A.1.1 for more details about the datasets).\nDataset Preparation. To tune LLMs for human-AI interaction, we need to define a chat dialogue protocol that allows the model to understand human instructions and rate them. To this end, we adopt a chat protocol that utilizes special header and termination tokens, similar to the format used in Llama 3. For example, in the case of the Llama 3 8B model, the concatenation of a prompt and its corresponding response would follow this structure:  Example Prompt  Example Response  (see Appendix A.1.1 for more details).\nModels. As RM base models we use the pretrained checkpoint of Hugging Face for Llama 3 8B [Dubey et al., 2024], Llama 3 8B-instruct [Dubey et al., 2024] and Mistral 7B [Jiang et al., 2023a].\nET prediction models. In our analyses, we utilise two state-of-the-art ET prediction models, both pre-trained to predict ET features and kept frozen in our implementation. The input to these models is the same text used for the RM, with minimal modifications (Appendix A.1.2). The first model [Huang and Hollenstein, 2023], consists of a T5 embedding layer [Raffel et al., 2020], a two-layer BiLSTM [Hochreiter and Schmidhuber, 1997], and a one-hidden-layer MLP. This model was trained on the Dundee, GECO [Cop et al., 2017], ZuCo1 [Hollenstein et al., 2018], and ZuCo2 [Hollenstein et al., 2020b] datasets, and predicts total reading time (TRT) per token (Figure 3). The second model [Li and Rudzicz, 2021], is based on RoBERTa [Liu et al., 2019] with a regression head on each token. This head is a linear layer that outputs five features: FFD, fixProp, GPT, TRT, and nFix (Table 1). The model is initialized with pre-trained weights and fine-tuned on the ZuCo1 [Hollenstein et al., 2018], ZuCo2 [Hollenstein et al., 2020b] and PROVO [Luke and Christianson, 2018] datasets. Since ROBERTa's maximum sequence length is 512 tokens and our input sequences are longer, we employ a sliding window approach. The input is split into 512-token segments with a 50-token overlap, and the results are combined using a linear weighted approach. Further details on these models and their integration into our framework are provided in Appendix A.1.2.\nBaseline models. To evaluate the improvement in accuracy for a RM that incorporates implicit feedback, and specifically ET signals, we compare the same RM with and without the ET embeddings. For each dataset and model, we train and"}, {"title": "4.2 Results", "content": "The results of our experiments on the OASST and HelpSteer datasets, covering all possible combinations of ET features, models, and inclusion methods, as shown in Table 3 and Table 4 respectively. For the Mistral model, results for the GazeAdd method are unavailable due to the inability to map features between the ET prediction model's tokenizer and the reward model's tokenizer (details in Appendix A.1.3). In what follows, we discuss the main findings."}, {"title": "Effect of Model Initialization", "content": "We evaluate the impact of model initialization on performance. Open-access LLMs typically come in two forms: a pre-trained version without human alignment and a final version that has undergone alignment with human feedback. Since we lack access to intermediate checkpoints, we experiment with both pre-trained models (Mistral 7B and Llama 3) and models that are already human-aligned (Llama 3 Instruct). Our goal is to confirm that our method is effective for RM initialized with both pre-trained and human-aligned checkpoints. When comparing accuracy improvements relative to the baseline (without ET features), all models show considerable gains from incorporating implicit feedback. Notably, the Llama 3 8B and Mistral 7B models, which had no prior alignment, demonstrate over 20% performance improvement from the incorporation of ET features, indicating that unaligned models benefit more from implicit feedback."}, {"title": "Inclusion method", "content": "The results shown Table 3 and Table 4 indicate that both GazeConcat methods and GazeAdd introduce a substantial performance improvements to the RM. Across both datasets, concatenating embeddings (GazeConcat) delivers more consistent results. Incorporating ET information through specialized separator embeddings allows the model to process both text and ET features more robustly. However, in the HelpSteer dataset (Table 4), directly adding ET information to the text embeddings (GazeAdd) results in the greatest improvement over the baseline."}, {"title": "Eye-tracking (ET) feature importance", "content": "Different ET features capture distinct aspects of reading behaviour and information processing, influencing model performance uniquely [Zhang and Hollenstein, 2024]. Here, we examine how model performance varies when incorporating three different feature combinations generated by two different ET prediction models: fcomb\u2081 \u2013 TRT generated by the first ET prediction model; fcomb2.5 \u2013 five features (nFix, FFD, GPT, TRT, fixProp) generated by the second ET prediction model; and fcomb2.2 \u2013 TRT and FFD generated by the second ET prediction model. TRT and FFD are widely used in ET research [Huang et al., 2023, Huang and Hollenstein, 2023, Zhang and Hollenstein, 2024, Maharaj et al., 2023, Wang et al., 2022], and they have been shown to correlate with attention scores from pre-trained transformer models [Wang et al., 2024a, Bensemann et al., 2022, Sood et al., 2020a]. Other studies have used gradient-based saliency methods to explore similar correlations [Hollenstein and Beinborn, 2021, Wu et al., 2024b]. When comparing results, we observe that the RM benefits from implicit feedback regardless of the ET feature combination or ET prediction model used. Specifically, in most cases, fcomb1 yields the best results, particularly with the GazeAdd method. For GazeConcat, fcomb2.2 performs best for the OASST1 dataset, while fcomb\u2081 excels in HelpSteer2. We attribute the superior performance of fcomb\u2081 to how the ET prediction model generating the fixations was trained, including the data and preprocessing methods used (see Appendix A.1.2). Moreover, when comparing fcomb2.2 and f comb2.5 - both generated by the same model \u2013 only in one case does integrating nFix, GPT, and fixProp improves performance. In some instances, using fcomb2.5 results in worse performance than the baseline, confirming findings provided by prior studies, which suggest that features related to reading time, such as FFD and TRT, contribute most to performance gains."}, {"title": "RewardBench", "content": "As a side contribution, we evaluate our best performing models (trained on the OAAST1 dataset) on RewardBench. This evaluation is not intended to directly compare our method with larger, more resource-intensive RM, but rather to show that through the integration of multimodal signals like ET features we can significantly enhance the performance of RM models. The results shown in Table 5 demonstrate consistent improvements as previously observed, with gains exceeding 40% for the Mistral model a notable gain considering that the base RM is the same. We note that the performance of the baseline models is impacted by RM trained on base models with less than 9B parameters and on relatively small datasets (see details in Appendix B)."}, {"title": "5 Related Work", "content": "Reward Modelling. The most popular approach to reward modeling follows the framework introduced by Ouyang et al. [2022]. Several studies have examined alternative versions for refining RMs. For instance, Bai et al. [2022a] proposed more fine-grained reward structures, evaluating helpfulness and harmlessness separately. Other approaches have explored different reward modelling strategies [Wu et al., 2023a, Dong et al., 2023a, Wang et al., 2023c]. Another line of research has focused on Process Based Reward Models (PRMs) [Lightman et al., 2024, Uesato et al., 2022] which differ from conventional RMs by predicting the correctness of intermediate steps, rather than solely evaluating final outputs. Other studies implement data augmentation techniques [Shen et al., 2023], or cross-attention mechanisms between encoded input text and candidate pairs [Jiang et al., 2023b]. Moreover, some works have leveraged synthetic preference data for reward modelling [Cui et al., 2024, Jiao, 2023]. Wu et al. [2024a] built upon the LLM-as-a-Judge framework Zheng et al. [2023] by introducing LLM-as-a-Meta-Judge, which evaluates the model's judgments to generate preference pairs that enhance its decision-making capabilities. Finally, Pace et al. [2024] incorporated a self-training approach to improve reward model training. However, to date, no research has explored the integration of ET or other implicit feedback signals into RM.\nEye-tracking in Natural Language Processing. Several studies have investigated the use of ET data for a variety of NLP tasks, such as named entity recognition [Hollenstein and Zhang, 2019, Ren and Xiong, 2023, Yu et al., 2024, Hollenstein et al., 2019], text comprehension [Ahn et al., 2020, Reich et al., 2022, Sood et al., 2020b], language modelling [Huang et al., 2023, Huang and Hollenstein, 2023, Deng et al., 2023b], and question answering [Zhang and Hollenstein, 2024, Wang et al., 2024a]. Other applications include code comprehension [Alakmeh et al., 2024], code summarization [Zhang et al., 2024] and hallucination detection [Maharaj et al., 2023]. Eye-tracking has also been applied to sentiment analysis and sarcasm detection tasks [Mishra et al., 2016, 2017, 2018, Barrett et al., 2018, Huang et al., 2023, Khurana et al., 2023, Hollenstein et al., 2019, Yang and Hollenstein, 2023, Kiegeland et al., 2024, Deng et al., 2023a, Mathias et al., 2018, McGuire and Tomuro, 2021]. The most relevant work to our approach is by\nKiegeland et al. [2024], which introduced a dataset generation method using ET signals for DPO, building on the controlled sentiment generation framework proposed by Deng et al. [2023a], Yang and Hollenstein [2023]. While this study has contributed to the first steps towards integrating ET for human alignment in LLMs, it is task- and dataset-specific, often relying on ranking criteria that underutilize the potential of ET feedback. In contrast, our approach presents a more general framework by directly incorporating implicit feedback into the RM, rather than limiting its application to dataset creation."}, {"title": "6 Discussion", "content": "In this work, we introduced a novel framework for integrating implicit feedback into the Reward Model, a key component for aligning LLMs and generating synthetic data for further alignment. We validated our approach using widely-adopted, open-source models such as Llama 3 and Mistral, for initializing the RM. By employing two different models to generate ET features, our results show that incorporating implicit feedback consistently improves the RM's ability to predict user preferences, regardless of the model used and without the need to reach large parameter counts or train on massive datasets. Additionally, our method leverages ET features generated by models, making it fully scalable and applicable to various human alignment methods, including those that involve artificially generated datasets. This work advances the ongoing discussion on optimizing AI alignment with human values and shows the potential of multimodal signals for NLP research."}, {"title": "6.1 Limitations & Future Work", "content": "Data. A limitation of our study is that both ET prediction models were trained on a relatively small datasets (Appendix A.1.2) that are not tailored to our tasks. Future work could benefit from directly collecting ET data specifically for LLM-generated responses, to offer insights into human reading comprehension and information processing of prompts, which could further improve model performance. Additionally, since the ET prediction models used in our experiments were trained on English corpora, the method's generalizability to other languages requires further investigation. Moreover, we explored two methodologies for integrating ET features into the RM, but other approaches could prove more effective. For instance, ET features could be used to modify the RM's attention mask, as suggested by Zhang and Hollenstein [2024]. Regarding dataset selection, both models used, Mistral 7B [Jiang et al., 2023a] and Llama 3 [Dubey et al., 2024], were fine-tuned on publicly available data, though specific details on the datasets are limited. Therefore, we cannot discount the possibility that the datasets we used may have been part of the models' pretraining, particularly for Llama 3 7B Instruct, which has already undergone human alignment. However, as we"}, {"title": "A Appendix", "content": null}, {"title": "A.1 Implementation details", "content": "This section provides further details on the implementation of our method. Subsection A.1.1 provides more details on the datasets used and their preprocessing steps. In subsection A.1.2, further information is given about the models used for generating ET features, along with the specific preprocessing required for each. Subsection A.1.3 explains the"}, {"title": "A.1.1 Dataset processing", "content": "In this subsection, we provide more details about the datasets used and the preprocessing to train the RM. We use two different datasets: OpenAssistant Conversations dataset's (OASST1) [K\u00f6pf et al., 2023] and HelpSteer2 [Wang et al., 2024b].\nOASST1. A human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages in 35 different languages, resulting in over 10,000 complete and fully annotated conversation trees. The basic data structure is a Conversation Tree (CT), with nodes representing written messages in a conversation. A CT's root node represents an initial prompt, given by the prompter. The data was collected using a web-app interface as a product of a worldwide crowd-sourcing effort involving over 13,500 volunteers, dividing the collection of each tree into five separate steps: prompting, labelling prompts, adding reply messages as prompter or assistant, labelling replies, and ranking assistant replies.\nHelpSteer2. A CC-BY-4.0-licensed open-source helpfulness dataset, designed to train state-of-the-art RM consisting on 10,000 response pairs. It collects prompts mainly from ShareGPT focusing on user inputs and filtering out non-English and programming-related prompts for quality. The prompts are clustered into topics and sampled based on complexity to ensure diversity. Multi-turn prompts are generated using an in-house model, with responses sourced from various internal models and human annotators. For each response, they annotate five attributes (helpfulness, correctness, coherence, complexity, and verbosity) on a Likert-5 scale involving multiple annotators for each response, ensuring high-quality ratings across five attributes.\nConversation format and dataset preparation.\nTo fine-tune LLMs for human-AI interaction, we need to define a chat protocol. We use a multi-message chat setup with a special header and termination tokens, similar to the one in Llama 3 Dubey et al. [2024]. The header tokens differentiate the turns between the user and the system. For this, we use the apply_chat_template feature from FastTokenizers in the transformers library.\nThe tokenizer used by the Meta-Llama-3-8B-Instruct model already incorporates this chat format since this model has already undergone human alignment. Therefore, we use this format in our experiments. For the other two models, we employ the default chat format provided by their respective tokenizers. We add new tokens in the embeddings layer for these chat formats and we train them as part of our process. Below, we provide an example of the template for each model."}, {"title": "A.1.2 Eye-tracker features generation models", "content": "Special tokens are removed from the text before it is tokenized with the corresponding tokenizer used for the ET generator model. This is done to ensure that special tokens related to the chat format are not included in the input and are not assigned ET features to them, since these tokens are just for the RM to understand"}]}