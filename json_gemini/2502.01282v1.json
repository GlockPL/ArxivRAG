{"title": "Rational Gaussian wavelets and corresponding model driven neural networks*", "authors": ["Attila Mikl\u00f3s \u00c1mon", "Kristian Fenech", "P\u00e9ter Kov\u00e1cs", "Tam\u00e1s D\u00f3zsa"], "abstract": "In this paper we consider the continuous wavelet transform using Gaussian wavelets multiplied by an appropriate rational term. The zeros and poles of this rational modifier act as free parameters and their choice highly influences the shape of the mother wavelet. This allows the proposed construction to approximate signals with complex morphology using only a few wavelet coefficients. We show that the proposed rational Gaussian wavelets are admissible and provide numerical approximations of the wavelet coefficients using variable projection operators. In addition, we show how the proposed variable projection based rational Gaussian wavelet transform can be used in neural networks to obtain a highly interpretable feature learning layer. We demonstrate the effectiveness of the proposed scheme through a biomedical application, namely, the detection of ventricular ectopic beats (VEBs) in real ECG measurements.", "sections": [{"title": "I. INTRODUCTION", "content": "REPRESENTING signals using wavelet coefficients has proved beneficial in a variety of fields. Even though the modern notion of the wavelet transform is usually dated from [1] written by Alex Grossmann and Jean Morlet, historical accuracy requires that previous works now included in broader wavelet theory are also mentioned. These include results achieved by Haar, Franklin, Littlewood and Paley and others (we recommend [2] for a thorough overview).\nThe widespread use of wavelets in various signal and image processing applications is no coincidence. In addition to important mathematical properties, the wavelet transform provides the so-called time-scale representation of signals, which is closely associated with time-frequency representations. In other words, the wavelet transform allows us to study the frequency profile of signals in certain time windows. In this way, representing signals using wavelet transforms inherently provides interpretable information about their behavior. This property has been widely expoited in signal processing related fields such as fault detection, where sudden changes in the frequency profile of signals can indicate the appearance of faults [3]. Contrary to classical time-frequency approaches such as the Gabor-transform (see e.g. [4]), wavelet representations allow for different time localisation properties at different scales (frequencies), which is also well exploited in applications. Finally, the introduction of multiresolution analysis (see [5] and e.g. [4]) allowed for a practically realizable way to construct orthogonal wavelets. In turn, quick algorithms realizing the discrete wavelet transform were developed [6] allowing for many applications in signal and image processing [7], [3], [8], [9]. For a more comprehensive review of wavelets and their history, the authors recommend [10].\nIn most applications, a fixed so-called mother (or analyzing) wavelet is used to perform wavelet analysis of the signals. These may have certain general beneficial properties such as a compact support, symmetry or smoothness. In addition, a mother wavelet may be selected due to its morphological similarity to the signals to be analyzed, for example so-called Ricker wavelets are frequently employed to represent ECG signals [11], [12]. In practical applications however, signals are always represented by a finite set of wavelet coefficients and fixing a mother wavelet may not be able to capture the behavior of the signal to a satisfactory degree. In order to overcome this limitation, a variety of adaptive wavelet transforms have been introduced (see e.g. [13], [14], [15]). For example in [13], [14], various adaptive discrete wavelet transforms are discussed with the purpose to approximate the signals well using only a few wavelet coefficients. These methods however are difficult to use in many real life scenarios, because the discrete wavelet coefficients lack the time invariant property and the resulting mother wavelets often do not possess desirable properties such as smoothness and symmetry. For this reason, adaptive continuous wavelet transforms were also investigated. In [15] for example, the center frequency parameter of Morlet wavelets was subject to optimization to match the behavior of the processed signals.\nIn this work, we present a new class of mother wavelets referred to as rational Gaussian wavelets (RGW). Instead of previous approaches, where parameters of existing wavelets were optimized (e.g. [15]), the proposed wavelet class has arbitrary degrees of freedom. This allows for the construction of highly unusual mother wavelet morphologies (see e.g. Fig. 3), while guaranteeing smoothness and symmetry. Admissibility is proved for the RGW class. In addition, every parameter of the proposed class of wavelets can be optimized using gradient based methods which allows the proposed transform to be easily included in modern machine learning (ML) architectures.\nML methods have become increasingly important in signal processing [16], [17]. This is in large part due to their ability to approximate highly nonlinear operators based solely on data. Deep learning methods have achieved remarkable results in image processing [18], natural language processing [19] and biomedical signal processing [20], [21], [22] among other disciplines. One of the most cited limitations of ML methods however is that well performing models usually consist of a large number of parameters, which are difficult to interpret by humans. For example, the popular GPT-3 model contains \u2248 175 billion parameters [23]. These factors make the deployment of such models expensive in the case of real time, and especially safety critical applications (such as autonomous driving and biomedical signal processing).\nTo overcome this issue, several recent research directions have been considered. The taxonomy of explainable artificial intelligence methods proposed in [24] identifies two important, distinct ways to approach the question of explainability. So-called post-hoc methods aim to provide explanations for the behavior of large deep learning models (see e.g. [24] and [25]). Transparent ML approaches on the other hand try to propose partially interpretable ML models. Recent examples of transparent ML schemes include physics informed neural networks [26], [27], [28] which incorporate classical physical models with meaningful parameters into ML models. Another advancement in transparent machine learning was the development of so-called deep unfolding networks [29], which implement the iterative steps of classical numerical methods as layers of a neural network. It is important to mention that the use of the wavelet transform has also been considered previously as a means of achieving transparent ML models. In particular, the recently introduced WaveletKernelNet [11] (WKN) exploits the relationship between convolution and wavelet coefficients to create a partially interpretable neural network relying on the wavelet transform. Nevertheless in section III-B we provide a qualitative comparison between the chosen model architecture and WKN.\nIn this work, of particular interest to us are so-called variable projection (VP) based ML methods [20], [30], [31]. VP operators implement adaptive orthogonal projections in Hilbert spaces and can be used to obtain interpretable and sparse signal representations. They have been successfully integrated into various ML models including neural networks [20] and support vector machines [21]. In our recent work [32], we proposed the approximation of continuous wavelet coefficients using VP operators. In fact, a VP-neural network architecture was considered, where the first layer was responsible for representing the input using only a few wavelet coefficients. The learnable parameters of the layer consisted of the scale and translation parameters associated with the wavelet coefficients. The output of the layer was the collection of wavelet coefficients defined by these learned parameters. In this way, the proposed model learned which time and scale parameters provided the most crucial information for the classification of the input signals. In the current study, this idea is expanded upon by introducing a variable projection based continuous wavelet transform which uses the rational Gaussian wavelet class that we introduce in this paper. In this approach, not only translation and scale parameters, but also parameters governing the morphology of the mother wavelet can be learned. As shown in a case study in section IV, this allows for sparser signal representations and clearer explainability than similar alternative ML methods.\nIn summary, the main contributions of our paper are the following:\n1) The introduction of rational Gaussian wavelets (RGW). Through an arbitrary number of parameters, these wavelets allow for the manipulation of the mother wavelet's morphology.\n2) We show that the RGW class is admissible, which allows for signal reconstruction based on the continuous wavelet coefficients (in L2(R) norm).\n3) We show that parameters of an RGW mother wavelet can be optimized using gradient based methods. This allows for simple inclusion into the variable projection framework and subsequently ML models utilizing back-propagation.\n4) We provide an upper bound on the error of wavelet coefficient estimates acquired using variable projection operators for signals with a compact support.\n5) We provide a case study on arrhythmia detection from ECG signals. We show that the proposed RGW-VP Net architecture can be used to successfully classify ventricular ectopic heartbeats (VEBs) from ECG data. The interpretability of the learned parameters is examined, and it is shown that the model learns information which corresponds to VEB descriptions in the medical literature. In addition, it is shown that the proposed model requires fewer parameters to achieve state-of-the-art classification accuracy.\nThe rest of the paper is organized as follows. In section II a brief overview of adaptive wavelet transforms and related literature is provided to motivate the introduction of RGW. In section III we introduce rational Gaussian wavelets and discuss their most important properties. Section III-B reviews variable projection operators and variable projection networks (VP-NET). A realization of the VP-NET architecture implementing a continuous wavelet transform using RGW is also introduced. Section IV contains our experiments and related discussions about ECG classification. Finally, in section V we summarize our findings and discuss future research directions. Proofs and related calculations can be found in the appendix."}, {"title": "II. MOTIVATION FOR ADAPTIVE CONTINUOUS WAVELET TRANSFORMS", "content": "In this section we discuss wavelet transforms and provide motivation for the introduction of the proposed rational Gaussian wavelets."}, {"title": "A. Wavelet transforms", "content": "Consider the family of functions\n$\\psi_{\\lambda, \\tau}(t) = \\frac{1}{\\sqrt{\\lambda}} \\psi(\\frac{t - \\tau}{\\lambda}), (t, \\lambda, \\tau \\in \\mathbb{R}, \\lambda \\neq 0)$,\nwhere the function \u03c8 is referred to as the \"mother\" or \"analyzing\" wavelet. The wavelet transform of a function f with respect to \u03c8 is defined as a function of two variables\n$W_f(\\lambda, \\tau) := \\int_{-\\infty}^{+\\infty} f(t)\\overline{\\psi_{\\lambda, \\tau}(t)}dt,$\nwhenever the integral exists. In Eq. (1), $\\overline{\\psi}$ denotes the complex conjugate of \u03c8. We note that the existence criteria is fulfilled for example if $f, \\psi \\in L_2(\\mathbb{R})$ by H\u00f6lder's inequality. For a fixed scale-translation pair $(\\lambda, \\tau) \\in \\mathbb{R} \\backslash \\{0\\} \\times \\mathbb{R}$, the number $W_{\\psi}f(\\lambda, \\tau)$ will also be referred to as a wavelet coefficient corresponding to $(\\lambda, \\tau)$.\nThe admissibility theorem (see e.g. [4]) describes a set of conditions on \u03c8 which allow for signal reconstruction in $L_2$ norm.\nTheorem 1 (Admissibility property). Suppose the wavelet $\\psi \\in L_1(\\mathbb{R}) \\cap L_2(\\mathbb{R})$ satisfies\n1) $\\int_{-\\infty}^{+\\infty} |\\psi(t)|^2 dt = ||\\psi||_2 = 1,$\n2) $\\int_{-\\infty}^{+\\infty} \\frac{|\\hat{\\psi}(\\xi)|^2}{\\xi} d\\xi = M < \\infty,$\nwhere $\\hat{\\psi}$ denotes the Fourier transform of \u03c8. Then, for any $f \\in L_2(\\mathbb{R})$ the following results hold:\n1) Energy conservation:\n$\\frac{1}{M}\\int_{-\\infty}^{+\\infty} \\int_{-\\infty}^{+\\infty} |W_f(\\lambda, \\tau)|^2 \\frac{d\\lambda d\\tau}{\\lambda^2} = \\int_{-\\infty}^{+\\infty} |f(t)|^2dt.$\n2) Define the function $f_{\\epsilon}$ as\n$f_{\\epsilon}(t) := \\frac{1}{M}\\int_{\\epsilon}^{\\frac{1}{\\epsilon}} \\int_{-\\infty}^{+\\infty} W_f(\\lambda, \\tau)\\psi_{\\lambda,\\tau}(t) \\frac{d\\lambda d\\tau}{\\lambda^2}.$\nThen, f can be reconstructed from the wavelet coefficients $W_f$ in the sense that $f_{\\epsilon} \\rightarrow f$ in $L_2(\\mathbb{R})$ as $\\epsilon \\rightarrow 0+$.\nIn section II we state a theorem that guarantees admissibility for the proposed family of wavelets. For further discussion of reconstruction formulas and a proof of theorem 1, we refer to [4], [33].\nIn applications, wavelet coefficients are computed in a numerical fashion. This also means that the parameters $(\\lambda, \\tau)$ have to be restricted to a (finite) subset of $\\mathbb{R} \\backslash \\{0\\} \\times \\mathbb{R}$. A general way to express this (see e.g. [4]) is to consider the wavelet coefficients $W_f(\\Lambda_n, T_m)$ $(m, n \\in \\mathbb{Z})$, where\n$\\Lambda_n := \\alpha^{-n}, T_m := m\\beta.$\nIn this case, the coefficients are calculated using the functions $\\psi_{\\Lambda_n,T_m}(t) = \\alpha^{n/2}\\psi(\\alpha^n\\cdot t-m\\cdot\\beta)$. The computational cost of the wavelet transform increases as \u03b1 tends to 1 and \u03b2 tends to 0. Usually, the parameters are chosen as \u03b1 = 2 and \u03b2 = 1, a choice, where the scales $\\Lambda_n$ correspond to octaves in music [4]. As mentioned in the introduction, different strategies have been developed to calculate the required wavelet coefficients $W_{\\psi}f(\\Lambda_n, T_m)$.\nA popular approach is to construct the family $\\Psi_{\\Lambda_n, T_m}$ so that they form an orthogonal basis in $L_2(\\mathbb{R})$ using multiresolution analysis. The so-called discrete wavelet transform (DWT) algorithm uses such orthogonal wavelets and has the benefit, that discrete wavelet coefficients can be used to achieve perfect signal reconstruction. Even though fast DWT methods [6] have been developed relying on digital filtering, the use of orthogonal wavelets has its disadvantages. One problem is that, for some signal analysis tasks, a finer resolution of the coefficients is desirable [34]. In addition, DWT algorithms are not translation invariant by default which makes their use difficult for some signal processing tasks. It should be noted that certain translation invariant (and from the reconstruction point of view redundant) variations, such as the stationary wavelet transform, [35] exist. Finally, when constructing orthogonal wavelets there is a trade off between \"desirable\" properties. For example, Daubechies' wavelets are r times differentiable, however their support increases with r meaning that smoother mother wavelets become less well localised in time. Another example is that the often desired symmetry property can only be approximated for orthogonal wavelets.\nFor the above reasons, continuous wavelet transform (CWT) algorithms are also frequently used for signal analysis [34]. These approximate the wavelet coefficients (1) using numerical quadrature formulas. CWT algorithms cannot achieve perfect signal reconstruction. On the other hand, CWT algorithms retain the important translation invariance property, and can be used with mother wavelets which were not constructed using multiresolution analysis. This allows for the use of wavelets with desirable properties such as Gaussian wavelets [36], which can be acquired by differentiating the Gaussian function. For example, Ricker's wavelet has enjoyed popularity for biological signal processing and fault detection applications [11], [12]. The adaptive wavelet family proposed in section III is also closely connected to Gaussian wavelets."}, {"title": "B. Adaptive wavelet transforms", "content": "Choosing the correct analyzing wavelet for the CWT algorithm can be a challenging problem. For this example, the analyzing wavelets were chosen as a complex Morlet and a Ricker wavelet.\nwavelet decomposition schemes. Even though these are usually quick and guarantee that the transformation can be inverted, they suffer from the usual drawbacks associated with DWT. Namely, the smoothness and and symmetry properties of the learned wavelets cannot be guaranteed. In addition, for most of these algorithms time invariance of the computed wavelet coefficients is also not fulfilled, which makes their application difficult for the analysis of signals. Finally, DWT algorithms compute approximation and detail coefficients corresponding to the discretization shown in Eq. (2). If instead the computed dilation and translation coefficients were subject to optimization, sparser signal representations may be achieved as shown in [32].\nFor the above reasons, adaptive CWT algorithms also enjoyed growing attention. Several papers optimized the parameters of Morlet wavelets [15], [37] for various fault-detection applications. An important benefit of adaptive CWT schemes, is that they adapt the morphology of the analyzing wavelet to match the input signals, while simultaneously ensuring smoothness and symmetry properties. In [15] for example, properties associated with Morlet wavelets are retained even though the centre frequency parameter of the analyzing wavelet is modified through numerical optimization. Although adaptive CWT methods do not guarantee perfect signal reconstruction, in applications (e.g. fault detection) invertibility is not the most crucial property of the transformation. In fact, in these applications a sparse representation of the input signals is usually preferred, which allows for clearer distinction between fault classes [3]. CWT is ideal for this, since in addition to learning parameters of the analyzing wavelet, adaptive CWT algorithms can also treat the dilation and translation parameters of the computed wavelet coefficients as free parameters. This idea was utilized for example in [11] and [32], where different CWT algorithms were used as feature extraction layers in neural networks. In this paper, we also demonstrate the utility of the proposed wavelet transformation using such wavelet transform enchanced machine learning models (see sections III-B and IV)."}, {"title": "III. RATIONAL GAUSSIAN WAVELETS", "content": "Gaussian wavelets are defined by\n$\\Psi_n(t) := C_n\\frac{d^n}{dt^n}e^{-t^2/2},$\nwhere $n \\in \\mathbb{N}$ and the constant satisfies $C_n = 1/||\\frac{d^n}{dt^n} e^{-t^2/2}||_2$.\nThese types of wavelets are commonly used in signal and image processing, especially since they exhibit good localisation properties on both time and frequency space. In particular, Ricker wavelets (the n = 2 case in Eq. (3)) have been successfully applied for fault detection [11], and biological signal processing [12].\nIn this paper, we consider the case n = 1 and multiply the Gaussian $e^{-t^2/2}$ with an appropriate rational function, whose poles and zeros will act as free parameters in our construction. If the rational term satisfies the following criteria, the resulting family of wavelets will satisfy the conditions of theorem 1. Consider first, the polynomials\n$r_z(t) := (t - z)(t + \\overline{z})(t - \\overline{z})(t + z)$\n$(z \\in \\mathbb{C}, \\Im(z) \\neq 0, t \\in \\mathbb{R}),$\nwhere $\\Im(z)$ denotes the imaginary part of the complex parameter z and $\\overline{z} = -\\Re(z) + i\\Im(z)$. We note that polynomials of the form $r_z(x) = (x - z)(x - \\overline{z})$ may also be used if $\\Re{z} = 0$ for the proposed construction. We denote the class of polynomials satisfying Eq. (4) by $\\mathcal{P}_e$. Clearly, the elements of $\\mathcal{P}_e$ are real valued even functions, who do not vanish on the real line. Using the class $\\mathcal{P}_e$ we can introduce the class of rational functions\n$\\mathcal{V} := \\{v(t) \\mid v(t) = \\frac{1}{\\prod_{k=1}^{n-1} q_k(t)} : q_k \\in \\mathcal{P}_e, n \\in \\mathbb{N}\\}$.\nThen, the rational functions v \u2208 V are also real and do not vanish anywhere on R. Consider now the polynomial\n$\\mathcal{P}(t) := t \\cdot \\prod_{k=1}^{p} (t - t_k)(t + t_k)$\n$(t_k \\in \\mathbb{C} \\backslash \\{0\\}, p \\in \\mathbb{N}).$\nWith the conditions stated in Eq. (6), P is an odd polynomial satisfying P(t) = -P(-t) $(t \\in \\mathbb{R})$. Now, it is possible to state the following definition. We note that for the biomedical application detailed in this work, we were particularly interested in real-valued wavelets (see section IV). For this reason, and for simplicity we assumed $\\Im(t_k) = 0$ in our experiments. It should also be noted, that complex variations of rational Gaussian wavelets are possible, as long as P remains an odd function and V contains even functions, who do not vanish on the real line (see the proof of admissibility in the appendix).\nDefinition 1 (Rational Gaussian wavelet). The function $\\Psi \\in L_2(\\mathbb{R})$ is called a rational Gaussian wavelet (RGW), if\n$\\Psi_{\\eta}(t) := C(\\eta) \\cdot \\mathcal{P}_n(t) \\cdot v_{\\eta}(t) \\cdot e^{-t^2/2}, (t \\in \\mathbb{R}, \\eta \\in \\mathbb{C}^{p+n})$\nwhere $\\mathcal{P}_n$ is a polynomial satisfying Eq. (6), $v_{\\eta} \\in \\mathcal{V}$ and \u03b7 contains the poles z of $v_{\\eta}$ and the non-zero roots of $\\mathcal{P}_n$. Moreover, the constant $C(\\eta) \\in \\mathbb{R}$ is chosen such that $||\\Psi||_2 = 1$ holds.\nMother wavelets defined according to Eq. (7) clearly belong to $L_2(\\mathbb{R})$, in fact the following theorem holds.\nTheorem 2 (Admissibility of rational Gaussian wavelets). Suppose $\\psi \\in L_2(\\mathbb{R})$ is defined according to Eq. (7). Then, there exists a number $M \\in \\mathbb{R}^+$, such that\n$\\int_{-\\infty}^{\\infty} \\frac{|\\Psi(\\xi)|^2}{\\xi} d\\xi = M$\nholds.\nThe proof of theorem 2 can be found in the appendix.\nThe admissibility property guarantees that the proposed RGW wavelet transform can be inverted with respect to the $L_2$ norm. It should be noted, that theorem 2 also holds, if we replace the set of possible rational terms V with the linear space\n$\\mathcal{W} := \\{ \\sum_{j=0}^{k} C_k v_j(x) : v_j \\in \\mathcal{V}, k \\in \\mathbb{N}\\}$.\nhowever, for simplicity we state the results using the definition above. As seen in the appendix, the proof relies on two properties of the RGWs: symmetry (they are odd functions), and the fact that the denominator of v in Eq. (7) does not dissappear anywhere. These properties however are also satisfied if v is chosen from W instead of V.\nRGW mother wavelets depend on a number of free parameters. The modification of these parameters greatly influences the morphology of the analyzing wavelet. For example, moving the poles of $v_n$ close to the real line leads to the appearance of sharp peaks in the mother wavelet. This spike-like morphology resembles a number of real life signals which appear in fault detection and biomedical applications (see e.g. Fig. 3).\nIt is important to note, that despite the sharp oscillations shown in Fig. 3, RGWs retain smoothness and symmetry. This adaptive property of the proposed wavelets allows for the capture of important signal behavior using only a few wavelet coefficients. That is, a sparse wavelet coefficient representation of a large class of signals may be obtained by optimizing the poles of $v_n$ and the zeros of $P_n$ with respect to an objective function. This makes the continuous wavelet transform utilizing RGWs a powerful trainable feature extractor. Since RGWs are also analytic with respect to the poles of $v_n$ and $P_n$, gradient based methods (such as backpropagation algorithms) may be used to optimize these parameters. This allows for an easy integration of the scheme into existing machine learning models such as neural networks."}, {"title": "A. Wavelet transform based machine learning models", "content": "In this section, we discuss methods to extend existing machine learning models with the proposed RGW based continuous wavelet transform. Feature extraction schemes relying on the wavelet transform have been studied before (see e.g. [12], [3], [38], [32], [11]). In many works, the wavelet transform (discrete or continuous) is used as a static feature extractor [12], [3], [38]. That is, a wavelet coefficient representation of the input signals is obtained, which is then passed to a machine learning algorithm to solve a regression or classification problem. The feature extraction step in this case is conducted separately from the training of the ML model. Since wavelet coefficients represent information about the signals' time-frequency profile, the magnitude of coefficients corresponding to fixed scale and translation parameters conveys interpretable information about signal behavior at given times and frequencies.\nIn most CWT based machine learning methods however, a large number of wavelet coefficients is computed, then passed to the underlying machine learning algorithms. A more adaptive approach was proposed in [11], where the following observation was made. For a fixed scale parameter \u03bb \u2208 R\\{0\\}, the function $W_{\\psi} f(x, \\cdot) : \\mathbb{R} \\rightarrow \\mathbb{C}$ can be expressed by\n$W_{\\psi} f(x,\\cdot) = (f * \\psi_{\\lambda,0})(\\tau) = \\int_{-\\infty}^{+\\infty} f(t) \\overline{(\\frac{1}{\\lambda} \\cdot \\psi(\\frac{t-\\tau}{\\lambda}))}dt.$\nIn other words, wavelet coefficients can be computed at fixed scales using convolution with the kernel $\\psi_{\\lambda,0}$. Exploiting this observation, in [11] the use of convolution layers in neural networks with scaled wavelet kernels was proposed. The learnable parameters of these wavelet kernel convolution layers were the scales $\\lambda_k$ and initial translation $\\tau_k$ $(k = 1, ..., M)$. Then, wavelet coefficients were approximated by computing the (discrete) convolutions\n$f * \\psi_{\\lambda_k,\\tau_k}$ $(k = 1, ..., M)$,\nwhere $f \\in \\mathbb{R}^N$ and $\\psi_{\\lambda_k,\\tau_k} \\in \\mathbb{R}^N$ denote N-point equidistant samplings of f and $\\psi_{\\lambda_k,\\tau_k}$ respectively. This approach means, that the input signals are represented with a large number of wavelet coefficients, therefore, in wavelet kernel net pooling layers are utilized to obtain a sparse signal representation. We note that the proposed RGW wavelets could easily be adapted for use with wavelet kernel net proposed in [11] and may perform well for certain applications. We plan to investigate such constructions in the future, however because of the following reason we opted for a variable projection based neural network architecture in this study (see section III-B).\nThe main benefit of optimizing the zeros and poles of the rational term $\\mathcal{P}_n(x) \\cdot v_n(x)$ in Eq. (7), is that mother wavelet morphologies similar to the input signals may be obtained. This is beneficial, because in this way, precise reconstructions of the input signals may be obtained using only a few wavelet coefficients (see section III-B and Fig. 3). In this way, the computation of a large number of wavelet coefficients using the wavelet kernel net would be contrary to our efforts."}, {"title": "B. Variable projection and wavelet coefficients", "content": "In this section we summarize a variable projection based methodology which allows for the inclusion of the RGW into machine learning, and specifically, neural network models. The discussed methodology was originally developed in [20] for neural networks and [21] for support vector machines (SVMs). In addition, we use of our previous findings in [32], where we showed the benefit of approximating wavelet coefficients using variable projection operators. An important contribution of the current study in contrast to [32], is that instead of only learning appropriate translation and scale parameters of the wavelet coefficients and considering a fixed mother wavelet, in this work we propose variable projection transformations which can be used to also optimize the poles and zeros in Eq. (7).\nIn practical applications, signals are usually only available in a discretely sampled form. That is, instead of $f \\in L_2(\\mathbb{R})$, we may only consider its (equidistant) sampling $f \\in \\mathbb{R}^N$ $(N \\in \\mathbb{N})$. Consider an $m << N$ dimensional subspace in $\\mathbb{R}^N$ spanned by the linearly independent basis vectors $u_1, ..., u_m \\in \\mathbb{R}^N$. Denote this subspace by\n$\\mathcal{U} := \\text{span}\\{u_1, ..., u_m\\} \\subset \\mathbb{R}^N$\nand by $\\hat{f} \\in \\mathcal{U}$ the element which minimizes\n$||f - u||$ $(u \\in \\mathcal{U})$\nIt is well known that $\\hat{f}$ exists uniquely and may be expressed by the orthogonal projection\n$\\hat{f} = P_{\\mathcal{U}}(f) := \\Phi \\Phi^+ f,$\nwhere the columns of $\\Phi \\in \\mathbb{R}^{N \\times m}$ coincide with the basis vector $u_k$ $(k = 1, ..., m)$ and $\\Phi^+$ denotes the Moore-Penrose pseudo inverse [30]. Suppose the basis vectors $u_k$ depend on a real parameter vector \u03b7 \u2208 $\\mathbb{R}^q$ $(q \\in \\mathbb{N})$ and thus\n$\\mathcal{U}(\\eta) := \\text{span}\\{u_1(\\eta), ..., u_m(\\eta)\\}$.\nThen, the projection operator $P_{\\mathcal{U}(\\eta)}$ also depends on \u03b7 and is referred to as a variable projection operator. Given a family of subspaces G(\u03b7) spanned by $u_k(\\eta)$ $(k = 1, ..., m)$, the best possible approximation (if it exists) of $f \\in \\mathbb{R}^N$ can be found by minimizing\n$E_2(\\eta) := ||f - \\hat{f}(\\eta)||^2 = ||f - \\Phi(\\eta)\\Phi(\\eta)^+f||^2$\nover \u03b7 \u2208 $\\mathbb{R}^q$. This nonlinear optimization problem is usually referred to as a separable nonlinear least squares problem (SNLLS), since for any fixed value of \u03b7, minimizing Eq. (9) degrades to a least squares fitting task. SNLLS problems can be solved using gradient based optimization schemes due to the work of Golub and Pereyra, who in [30], [31] provided an explicit formula for the derivatives $\\frac{\\partial}{\\partial \\eta} E_2(\\eta)$. Using this formula, the gradients of $E_2(\\eta)$ may be calculated analytically. It is important to note however, that the differentiability of $E_2$ assumes that the partial derivatives $\\frac{\\partial}{\\partial \\eta} u_k(\\eta)$ exist (see e.g. [30]). In other words, the basis functions $u_1(\\eta), ..., u_m(\\eta)$ should be differentiable with respect to the parameters \u03b7.\nIn [32] we proposed to approximate continuous wavelet coefficients using variable projections. More precisely, let $m \\in \\mathbb{N}$ denote the number of wavelet coefficients used to represent the signal f. Let\n$\\eta := [\\lambda_1, \\tau_1, ..., \\lambda_m, \\tau_m] \\in \\mathbb{R}^{2\\cdot m}$\nand let $\\psi \\in \\mathbb{R}^N$ be an equidistantly sampled analyzing wavelet from $L_2(\\mathbb{R})$. Suppose the sampling was done over the effective support (a subset of R over which $\\psi(x)$ significantly differs from 0) of \u03c8. Define $\\Psi(\\eta) \\in \\mathbb{R}^{N \\times m}$ as the matrix, whose k-th column consists of the sampling of $\\psi_{\\lambda_k, \\tau_k}$ $(k = 1, ..., m)$. Then, one has\n$W_{\\psi} f(x_k, \\tau_k) \\approx (\\Psi(\\eta)^+f)_k$ $(k = 1, ..., m)$.\nWe can provide the following error estimate for the obtained approximation if the input signal f is smooth and compactly supported. These properties can be assumed for a large class of signals encountered in applications (see e.g. section IV).\nTheorem 3 (Error of variable projection based continuous wavelet coefficients). Let \u03c8 \u2208 L2(R) be a fixed mother wavelet and suppose f \u2208 L2(R) is continuously differentiable and compactly supported on [a, b] C R. Let a =: t0 < t1 < ... < tN\u22121 := b be an equidistant sampling of [a, b] with h := t1 \u2212 t0. In addition, consider \u03b7 = (\u03bb1, \u03c41, . . . , \u03bbm, \u03c4m) \u2208 R2m with \u03bbk > 0 (k = 1, . . . , m). Finally, let fk := f(tk) (k = 0, . . . , N \u2212 1). Then we have\n|$W_{\\psi f}(\\lambda_k, \\tau_k) - h \\cdot (\\Psi(\\eta)^+f)_k | < \\frac{h \\cdot M_1 (b - a)}{2} + h \\cdot ||f|| \\cdot ||\\Psi^*(\\eta)||_{\\infty} \\cdot (\\frac{\\kappa(G(\\eta))}{||G(\\eta)||_{\\infty}} + 1)$,\nwhere\n$M_1 := \\text{max}_{\\xi \\in [a, b], k = 1, ..., m} |f'(\\xi) \\cdot \\psi_{\\lambda_k, \\tau_k}(\\xi)|,$\nthe columns of \u03a8(\u03b7) contain the samplings of \u03c8\u03bbk,\u03c4k (k = 1, . . . , m) over tj (j = 0, . . . , N \u2212 1) and G(\u03b7) is the Gram-matrix constructed form the columns of \u03a8(\u03b7). In Eq. (10), \u03ba(G(\u03b7)) denotes the condition number of G(\u03b7) using the matrix infinity norm and \u03a8\u2217(\u03b7) denotes the adjungate of \u03a8(\u03b7).\nThe proof of theorem 3 is provided in the appendix. A consequence of this theorem, is that the proposed approximations tend to the actual wavelet coefficients as the sampling frequency of f increases.\nThere is a number of arguments which support the approximation of wavelet coefficients using the above-described variable projection based methodology in ML models. Firstly, in contrast to the Wavelet Kernel Net approach, there is no need for pooling and dropout operations to obtain a sparse representation of the input. Furthermore, by minimizing (9), one obtains wavelet coefficients which are optimal with respect to the reconstruction error in the least squares sense. Finally, since $E_2(\\eta)$ may be optimized using gradient based methods, variable projection operators can be included into neural network architectures (and other machine learning models [21]) as separate layers and trained using backpropagation schemes. These types of neural networks, often referred to as VP-Net, were introduced in [20] and have been successfully used with a number of different varaible projection operators to solve environmental recognition [39], fault detection [21] and biomedical signal processing problems [20], [21]."}, {"title": "C. The RGW-VP layer", "content": "Suppose we would like to solve a classification (or regression) problem using supervised learning. Consider the mappings $P_{\\Psi(\\eta)"}, "mathbb{R}^N \\rightarrow \\mathbb{R}^N$ and $C_{\\Psi(\\eta)} : \\mathbb{"]}