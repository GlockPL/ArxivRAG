{"title": "Navigation World Models", "authors": ["Amir Bar", "Gaoyue Zhou", "Danny Tran", "Trevor Darrell", "Yann LeCun"], "abstract": "Navigation is a fundamental skill of agents with visual-motor capabilities. We introduce a Navigation World Model (NWM), a controllable video generation model that predicts future visual observations based on past observations and navigation actions. To capture complex environment dynamics, NWM employs a Conditional Diffusion Transformer (CDiT), trained on a diverse collection of egocentric videos of both human and robotic agents, and scaled up to 1 billion parameters. In familiar environments, NWM can plan navigation trajectories by simulating them and evaluating whether they achieve the desired goal. Unlike supervised navigation policies with fixed behavior, NWM can dynamically incorporate constraints during planning. Experiments demonstrate its effectiveness in planning trajectories from scratch or by ranking trajectories sampled from an external policy. Furthermore, NWM leverages its learned visual priors to imagine trajectories in unfamiliar environments from a single input image, making it a flexible and powerful tool for next-generation navigation systems.", "sections": [{"title": "Introduction", "content": "Navigation is a fundamental skill for any organism with vision, playing a crucial role in survival by allowing agents to locate food, shelter, and avoid predators. In order to successfully navigate environments, smart agents primarily rely on vision, allowing them to construct representations of their surroundings to assess distances and capture landmarks in the environment, all useful for planning a navigation route.\nWhen human agents plan, they often imagine their future trajectories considering constraints and counter-factuals. On the other hand, current state-of-the-art robotics navigation policies (Sridhar et al., 2024; Shah et al., 2023) are \"hard-coded\", and after training, new constraints cannot be easily introduced (e.g, \"no left turns\"). Another limitation of current supervised visual navigation models is that they cannot dynamically allocate more computational resources to address hard problems. We aim to design a new model that can mitigate these issues.\nIn this work, we propose a Navigation World Model (NWM), trained to predict the future representation of a video frame based on past frame representation(s) and action(s) (see Figure 1(a)). NWM is trained on video footage and navigation actions collected from various robotic agents. After training, NWM is used to plan novel navigation trajectories by simulating potential navigation plans and verifying if they reach a target goal (see Figure 1(b)). To evaluate its navigation skills, we test NWM in known environments, assessing its ability to plan novel trajectories either independently or by ranking an external navigation policy. In the planning setup, we use NWM in a Model Predictive Control (MPC) framework, optimizing the action sequence that enables NWM to reach a target goal. In the ranking setup, we assume access to an existing navigation policy, such as NoMaD (Sridhar et al., 2024), which allows us to sample trajectories, simulate them using NWM, and select the best ones. Our NWM achieves competitive standalone performance and state-of-the-art results when combined with existing methods.\nNWM is conceptually similar to recent diffusion-based world models for offline model-based reinforcement learning, such as DIAMOND (Alonso et al.) and GameNGen (Valevski et al., 2024). However, unlike these models, NWM is trained across a wide range of environments and embodiments, leveraging the diversity of navigation data from robotic and human agents. This allows us to train a large diffusion transformer model capable of scaling effectively with model size and data to adapt to multiple environments. Our approach also shares similarities with Novel View Synthesis (NVS) methods like NeRF (Mildenhall et al., 2021), Zero-1-2-3 (Liu et al., 2023), and GDC (Van Hoorick et al., 2024), from which we draw inspiration. However, unlike NVS approaches, our goal is to train a single model for navigation across diverse environments and model temporal dynamics from natural videos, without relying on 3D priors.\nTo learn a NWM, we propose a novel Conditional Diffusion Transformer (CDiT), trained to predict the next image state given past image states and actions as context. Unlike a DiT (Peebles and Xie, 2023), CDiT's computational complexity is linear with respect to the number of context frames, and it scales favorably for models trained up to 1B parameters across diverse environments and embodiments, requiring 4\u00d7 fewer FLOPs compared to a standard DiT while achieving better future prediction results.\nIn unknown environments, our results show that NWM benefits from training on unlabeled, action- and reward-free video data from Ego4D. Qualitatively, we observe improved video prediction and generation performance on single images (see Figure 1(c)). Quantitatively, with additional unlabeled data, NWM produces more accurate predictions when evaluated on the held-out Stanford Go (Hirose et al., 2018) dataset.\nOur contributions are as follows. We introduce a Navigation World Model (NWM) and propose a novel Conditional Diffusion Transformer (CDiT), which scales efficiently up to 1B parameters with significantly reduced computational requirements compared to standard DiT. We train CDiT on video footage and navigation actions from diverse robotic agents, enabling planning by simulating navigation plans independently or alongside external navigation policies, achieving state-of-the-art visual navigation performance. Finally, by training NWM on action- and reward-free video data, such as Ego4D, we demonstrate improved video prediction and generation performance in unseen environments."}, {"title": "Related Work", "content": "Goal conditioned visual navigation is an important task in robotics requiring both perception and planning skills (Sridhar et al., 2024; Shah et al.; Pathak et al., 2018; Mirowski et al., 2022; Chaplot et al.; Fu et al., 2022). Given context image(s) and an image specifying the navigation goals, goal-conditioned visual navigation models (Sridhar et al., 2024; Shah et al.) aim to generate a viable path towards the goal if the environment is known, or to explore it otherwise. Recent visual navigation methods like NoMaD (Sridhar et al., 2024) train a diffusion policy via behavior cloning and temporal distance objective to follow goals in the conditional setting or to explore new environments in the unconditional setting. Previous approaches like Active Neural SLAM (Chaplot et al.) used neural SLAM together with analytical planners to plan trajectories in the 3D environment, while other approaches like (Chen et al.) learn policies via reinforcement learning. Here we show that world models can use exploratory data to plan or improve existing navigation policies.\nDifferently than in learning a policy, the goal of a world model (Ha and Schmidhuber, 2018) is to simulate the environment, e.g, given the current state and action to predict the next state and an associated reward. Previous works have shown that jointly learning a policy and a world model can improve sample efficiency on Atari (Hafner et al., b,a; Alonso et al.), simulated robotics environments (Seo et al., 2023), and even when applied to real world robots (Wu et al., 2023). More recently, Hansen et al. proposed to use a single world model that is shared across tasks by introducing action and task embeddings while Yang et al.; Lin et al. (2024b) proposed to describe actions in language, and Bruce et al. (2024) proposed to learn latent actions. World models were also explored in the context of game simulation. DIAMOND (Alonso et al.) and GameNGen (Valevski et al., 2024) propose to use diffusion models to learn game engines of computer games like Atari and Doom. Our work is inspired by these works, and we aim to learn a single general diffusion video transformer that can be shared across many environments and different embodiments for navigation.\nIn computer vision, generating videos has been a long standing challenge (Kondratyuk et al.; Blattmann et al., 2023; Girdhar et al., 2023; Yu et al., 2023; Ho et al., 2022; Tulyakov et al., 2018b; Bar-Tal et al., 2024). Most recently, there has been tremendous progress with text-to-video synthesis with methods like Sora (Brooks et al., 2024) and MovieGen (Polyak et al., 2024). Past works proposed to control video synthesis given structured action-object class categories (Tulyakov et al., 2018a) or Action Graphs (Bar et al., 2021). Video generation models were previously used in reinforcement learning as rewards (Escontrela et al., 2024), pretraining methods (Tomar et al., 2024), for simulating and planning manipulation actions (Finn and Levine, 2017; Liang et al., 2024) and for generating paths in indoor environments (Hirose et al., 2019b; Koh et al., 2021). Interestingly, diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020) are useful both for video tasks like generation (Voleti et al., 2022) and prediction (Lin et al., 2024a), but also for view synthesis (Chan et al., 2023; Poole et al.; Tung et al., 2025). Differently, we use a conditional diffusion transformer to simulate trajectories for planning without explicit 3D representations or priors."}, {"title": "Navigation World Models", "content": "Next, we turn to describe our NWM formulation. Intuitively, a NWM is a model that receives the current state of the world (e.g, an image observation) and a navigation action describing where to move and how to rotate. The model then produces the next state of the world with respect to the agent's point of view.\nWe are given an egocentric video dataset together with agent navigation actions $\\mathcal{D} = \\{(x_0, a_0, ..., x_T, a_T)\\}_{i=1}$, such that $x_i \\in \\mathbb{R}^{H\\times W \\times 3}$ is an image and $a_i = (u, \\phi)$ is a navigation command given by translation parameter $u \\in \\mathbb{R}^2$ that controls forward/backward and right/left motion, as well as and a yaw rotation angle $\\phi \\in \\mathbb{R}$.\nThe navigation actions $a_i$ can be fully observed (as in Habitat (Savva et al., 2019)), e.g, moving forward towards a wall will trigger a response from the environment based on physics, which will lead to the agent staying in place, whereas in other environments we compute the navigation action based on the change in the agent's location."}, {"title": "Formulation", "content": "Next, we turn to describe our NWM formulation. Intuitively, a NWM is a model that receives the current state of the world (e.g, an image observation) and a navigation action describing where to move and how to rotate. The model then produces the next state of the world with respect to the agent's point of view.\nWe are given an egocentric video dataset together with agent navigation actions $\\mathcal{D} = \\{(x_0, a_0, ..., x_T, a_T)\\}_{i=1}$, such that $x_i \\in \\mathbb{R}^{H\\times W \\times 3}$ is an image and $a_i = (u, \\phi)$ is a navigation command given by translation parameter $u \\in \\mathbb{R}^2$ that controls forward/backward and right/left motion, as well as and a yaw rotation angle $\\phi \\in \\mathbb{R}$.\nThe navigation actions $a_i$ can be fully observed (as in Habitat (Savva et al., 2019)), e.g, moving forward towards a wall will trigger a response from the environment based on physics, which will lead to the agent staying in place, whereas in other environments we compute the navigation action based on the change in the agent's location."}, {"title": "Diffusion Transformer as World Model", "content": "As mentioned in the previous section, we design $\\mathcal{F}_{\\theta}$ as a stochastic mapping so it can simulate stochastic environments. This is achieved using a Conditional Diffusion Transformer (CDiT) model, described next.\nConditional Diffusion Transformer Architecture. The architecture we use is a temporally autoregressive transformer model utilizing the efficient CDiT block (see Figure 2), which is applied $\\times N$ times over the input sequence of latents with input action conditioning.\nCDiT enables time efficient autoregressive modeling by constraining the attention in the first attention block only to tokens from the target frame which is being denoised. To condition on tokens from past frames, we incorporate a cross attention layer, such that every query token from the current target attends to tokens from past frames, which are used as keys and values. The cross attention then contextualizes the representations using a skip connection layer.\nTo condition on continuous actions like the translation $u$, rotation $\\phi$, and time shift $k$, along with the diffusion timestep $t$, we map each scaler to sine-cosine features, then apply a 2-layer MLP, $G$, mapping it from $\\mathbb{R}$ to $\\mathbb{R}^d$. Finally we sum all embeddings into a single vector used for conditioning:\n$\\xi = G_u(\\psi(u)) + G_\\phi(\\psi(\\theta)) + G_k(\\psi(k)) + G_t(\\psi(t))$\n$\\xi$ is then fed to an AdaLN (Xu et al., 2019) block to generate scale and shift coefficients that modulate the Layer Normalization (Lei Ba et al., 2016) outputs, as well as the outputs of the attention layers. To train on unlabeled data, we simple omit explicit navigation actions when computing $\\xi$ (see Eq 3).\nAn alternative approach is to simply use DiT (Peebles and Xie, 2023), however, applying a DiT on the full input is computationally expensive. Denote $n$ the number of input tokens per frame, and $m$ the number"}, {"title": "Navigation Planning with World Models", "content": "Here we move to describe how to use a trained NWM to plan navigation trajectories. Intuitively, if our world model is familiar with an environment, we can use it to simulate navigation trajectories, and choose the ones which reach the goal. In an unknown, out of distribution environments, long term planning might rely on imagination.\nFormally, given the latent encoding $s_0$ and navigation target $s^*$, we look for a sequence of actions $(a_0, ..., a_T)$ that maximizes the likelihood of reaching $s^*$. Let $S(s_T, s^*)$ represent the unnormalized score for reaching state $s^*$ with $s_T$ given the initial condition $s_0$, actions $a = (a_0,..., a_T)$, and states $s = (s_1,...s_T)$ obtained by autoregressively rolling out the NWM: $s \\sim F_\\theta(\\cdot|s_0, a)$.\nWe define the energy function $\\mathcal{E}(s_0, a_0,...,a_T, s_T)$, such that minimizing the energy corresponds to maximizing the unnormalized perceptual similarity score and following potential constraints on the states and actions:\n$\\mathcal{E}(s_0, a_0, \\dots, a_T, s_T) = -S(s_T, s^*) + \\sum_{\\tau=0}^{T}\\mathbb{I}(a_\\tau \\notin \\mathcal{A}_{valid}) + \\sum_{\\tau=0}^{T}\\mathbb{I}(s_\\tau \\notin \\mathcal{S}_{safe}),$\nThe similarity is computed by decoding $s^*$ and $s_T$ to pixels using a pretrained VAE decoder (Blattmann et al., 2023) and then measuring the perceptual similarity (Zhang et al., 2018a; Fu et al., 2024). Constraints like \"never go left then right\" can be encoded by constraining $a_\\tau$ to be in a valid action set $\\mathcal{A}_{valid}$, and \"never explore the edge of the cliff\" by ensuring such states $s_\\tau$ are in $\\mathcal{S}_{safe}$. $\\mathbb{I}(\\cdot)$ denotes the indicator function that applies a large penalty if any action or state constraint is violated."}, {"title": "Formulation", "content": "Here we move to describe how to use a trained NWM to plan navigation trajectories. Intuitively, if our world model is familiar with an environment, we can use it to simulate navigation trajectories, and choose the ones which reach the goal. In an unknown, out of distribution environments, long term planning might rely on imagination.\nFormally, given the latent encoding $s_0$ and navigation target $s^*$, we look for a sequence of actions $(a_0, ..., a_T)$ that maximizes the likelihood of reaching $s^*$. Let $S(s_T, s^*)$ represent the unnormalized score for reaching state $s^*$ with $s_T$ given the initial condition $s_0$, actions $a = (a_0,..., a_T)$, and states $s = (s_1,...s_T)$ obtained by autoregressively rolling out the NWM: $s \\sim F_\\theta(\\cdot|s_0, a)$.\nWe define the energy function $\\mathcal{E}(s_0, a_0,...,a_T, s_T)$, such that minimizing the energy corresponds to maximizing the unnormalized perceptual similarity score and following potential constraints on the states and actions:\n$\\mathcal{E}(s_0, a_0, \\dots, a_T, s_T) = -S(s_T, s^*) + \\sum_{\\tau=0}^{T}\\mathbb{I}(a_\\tau \\notin \\mathcal{A}_{valid}) + \\sum_{\\tau=0}^{T}\\mathbb{I}(s_\\tau \\notin \\mathcal{S}_{safe}),$\nThe similarity is computed by decoding $s^*$ and $s_T$ to pixels using a pretrained VAE decoder (Blattmann et al., 2023) and then measuring the perceptual similarity (Zhang et al., 2018a; Fu et al., 2024). Constraints like \"never go left then right\" can be encoded by constraining $a_\\tau$ to be in a valid action set $\\mathcal{A}_{valid}$, and \"never explore the edge of the cliff\" by ensuring such states $s_\\tau$ are in $\\mathcal{S}_{safe}$. $\\mathbb{I}(\\cdot)$ denotes the indicator function that applies a large penalty if any action or state constraint is violated."}, {"title": "Experimental Setting", "content": "For all robotics datasets (SCAND (Karnan et al., 2022), TartanDrive (Triest et al., 2022), RECON (Shah et al., 2021), and HuRON (Hirose et al., 2023)), we have access to the location and rotation of robots, allowing us to infer relative actions compare to current location (see Eq. 2). To standardize the step size across agents, we divide the distance agents travel between frames by their average step size in meters, ensuring the action space is similar for different agents, additionally we filter out backward movements, following NoMaD (Sridhar et al., 2024). Additionally, we use unlabeled Ego4D (Grauman et al., 2022) videos, where the only action we consider is time shift. SCAND provides video footage of socially compliant navigation in diverse environments, TartanDrive focuses on off-road driving, RECON covers open-world navigation, HuRON captures social interactions. We train on unlabeled Ego4D videos and GO Stanford (Hirose et al., 2018) serves as an unknown evaluation environment. For the full details, see Appendix B.1.\nWe evaluate predicted navigation trajectories using Absolute Trajectory Error (ATE) for accuracy and Relative Pose Error (RPE) for pose consistency (Sturm et al., 2012). To check how semantically similar are world model predictions to ground truth images, we apply LPIPS (Zhang et al., 2018b) and DreamSim (Fu et al., 2024), measuring perceptual similarity by comparing deep features, and PSNR for pixel-level quality. For image and video synthesis quality, we use FID (Heusel et al., 2017) and FVD (Unterthiner et al., 2019) which evaluate the generated data distribution. See Appendix B.1 for more details.\nWe consider all the following baselines.\n\u2022 DIAMOND (Alonso et al.) is a diffusion world model based on the UNet (Ronneberger et al., 2015) architecture. We use DIAMOND in the offline-reinforcement learning setting following their public code. The diffusion model is trained to autoregressively predict at 56x56 resolution alongside an upsampler to get to 224x224 resolution predictions. We alter their action embedding to use a linear layer to work with our continuous actions.\n\u2022 GNM (Shah et al., 2023) is a general goal-conditioned navigation policy trained on a dataset soup of robotic navigation datasets with a fully connected trajectory prediction network. GNM is trained on multiple datasets including SCAND, TartanDrive, GO Stanford, and RECON.\n\u2022 NoMaD (Sridhar et al., 2024) extends GNM using a diffusion policy for predicting trajectories for robot exploration and visual navigation. NoMaD is trained on the same datasets used by GNM and on HuRON.\nIn the default experimental setting we use a CDIT-XL of 1B parameters with context of 4 frames, a total batch size of 1024, and 4 different navigation goals, leading to a final total batch size of 4096. We use the Stable Diffusion (Blattmann et al., 2023) VAE tokenizer, similar as in DiT (Peebles and Xie, 2023). We use the AdamW (Loshchilov, 2017) optimizer with a learning rate of 8e \u2013 5. After training, we sample 5 times from each model to report mean and std results. XL sized model are trained on 8 H100 machines, each with 8 GPUs. Unless otherwise mentioned, we use the same setting as in DiT-*/2 models."}, {"title": "Ablations", "content": "Models are evaluated on single-step 4 seconds future prediction on validation set trajectories on the known environment RECON. We evaluate the performance against the ground truth frame by measuring LPIPS, DreamSim, and PSNR. We provide qualitative examples in Figure 3.\nWe compare CDiT (see Section 3.2), compared to a standard DiT in which all context tokens are fed as inputs. We hypothesize that for navigating known environments, the capacity of the model is the most important, and the results in Figure 5, indicate that CDiT indeed performs better with models of up to 1B parameters, while consuming less than \u00d72 FLOPs. Surprisingly, even with equal amount of parameters (e.g, CDiT-L compared to DiT-XL), CDiT is \u00d74 faster and performs better.\nWe train models with variable number of goal states given a fixed context, changing the number of goals from 1 to 4. Each goal is randomly chosen between \u00b116 seconds window around the current state. The results reported in Table 1 indicate that using 4 goals leads to significantly improved prediction performance in all metrics."}, {"title": "Video Prediction and Synthesis", "content": "We evaluate how well our model follows ground truth actions and predicts future states. The model is conditioned on the first image and context frames, then autoregressively predicts the next state using ground truth actions, feeding back each prediction. We compare predictions to ground truth images at 1, 2, 4, 8, and 16 seconds, reporting FID and LPIPS on the RECON dataset. Figure 4 shows performance over time compared to DIAMOND at 4 FPS and 1 FPS, showing that NWM predictions are significantly more accurate than DIAMOND. Initially, the NWM 1 FPS variant performs better, but after 8 seconds, predictions degrade due to accumulated errors and loss of context and the 4 FPS becomes superior. See qualitative examples in Figure 3."}, {"title": "Generation Quality", "content": "To evaluate video quality, we auto-regressively predict videos at 4 FPS for 16 seconds to create videos, while conditioning on ground truth actions. We then evaluate the quality of videos generated using FVD, compared to DIAMOND (Alonso et al.). The results in Figure 6 indicate that NWM outputs higher quality videos."}, {"title": "Planning Using a Navigation World Model", "content": "Next, we turn to describe experiments that measure how well can we navigate using a NWM. We include the full technical details of the experiments in Appendix B.2."}, {"title": "Generalization to Unknown Environments", "content": "Here we experiment with adding unlabeled data, and ask whether NWM can make predictions in new environments using imagination. Clearly, In this experiment we train a model on all in-domain datasets, as well as a susbet of unlabeled videos from Ego4D, where we only have access to the time-shift action. We train a CDiT-XL model and test it on the Go Stanford dataset as well as other random images. We report the results in Table 4, finding that training on unlabeled data leads to significantly better video predictions according to all metrics, including improved generation quality. We include qualitative examples in Figure 8. Compared to in-domain (Figure 3), the model breaks faster and expectedly hallucinates paths as it generates traversals of imagined environments."}, {"title": "Limitations", "content": "We identify multiple limitations. First, when applied to out of distribution data, we noticed that the model tends to slowly lose context and generates next states that resemble the training data, a phenomena that was observed in image generation and is known as mode collapse (Thanh-Tung and Tran, 2020; Srivastava et al., 2017). We include such an example in Figure 10. Second, while the model can plan, it struggles with simulating temporal dynamics like pedestrian motion (although in some cases it does). Both limitations are likely to be solved with longer context and more training data. Additionally, the model currently utilizes 3 DOF navigation actions, but extending to 6 DOF navigation and potentially more (like controlling the joints of a robotic arm) are possible as well, which we leave for future work."}, {"title": "Conclusion", "content": "Our proposed Navigation World Model (NWM) offers a scalable, data-driven approach to learning navigation policies. NWM is trained across diverse environments with our CDiT architecture, and is able to adapt flexibly to various scenarios. NWM can independently plan or rank an external policy by simulating navigation outcomes, which also allows it to incorporate new constraints. This approach bridges learning from video, visual navigation, and model-based planning and could potentially open the door to self-supervised systems that not only perceive but also act."}, {"title": "Standalone Planning Optimization", "content": "As described in Section 3.3, we use a pretrained NWM to standalone-plan goal-conditioned navigation trajectories by optimizing Eq.5. Here, we provide additional details about the optimization using the Cross-Entropy Method (Rubinstein, 1997) and the hyperparameters used. Full standalone navigation planning results are presented in Section B.2.\nWe optimize trajectories using the Cross-Entropy Method, a gradient-free stochastic optimization technique for continuous optimization problems. This method iteratively updates a probability distribution to improve the likelihood of generating better solutions. In the unconstrained standalone planning scenario, we assume the trajectory is a straight line and optimize only its endpoint, represented by three variables: a single translation $u$ and yaw rotation $\\phi$. We then map this tuple into eight evenly spaced delta steps, applying the yaw rotation at the final step. The time interval between steps is fixed at $k=0.25$ seconds. The main steps of our optimization process are as follows:\n\u2022 Initialization: Define a Gaussian distribution with mean $\\mu = (\\mu_{\\Delta x}, \\mu_{\\Delta y}, \\mu_{\\phi})$ and variance $\\Sigma = diag(\\sigma^2_{\\Delta x}, \\sigma^2_{\\Delta y}, \\sigma^2_{\\phi})$ over the solution space.\n\u2022 Sampling: Generate $N = 120$ candidate solutions by sampling from the current Gaussian distribution.\n\u2022 Evaluation: Evaluate each candidate solution by simulating it using the NWM and measuring the LPIPS score between the simulation output and input goal images. Since NWM is stochastic, we evaluate each candidate solution $M$ times and average to obtain a final score.\n\u2022 Selection: Select a subset of the best-performing solutions based on the LPIPS scores.\n\u2022 Update: Adjust the parameters of the distribution to increase the probability of generating solutions similar to the top-performing ones. This step minimizes the cross-entropy between the old and updated distributions.\n\u2022 Iteration: Repeat the sampling, evaluation, selection, and update steps until a stopping criterion (e.g., convergence or iteration limit) is met.\nFor simplicity, we run the optimization process for a single iteration, which we found effective for short-horizon planning of two seconds, though further improvements are possible with more iterations. When navigation constraints are applied, parts of the trajectory are zeroed out to respect these constraints. For instance, in the \"forward-first\" scenario, the translation action is $u = (\\Delta x, 0)$ for the first five steps and $u = (0, \\Delta y)$ for the last three steps."}, {"title": "Experiments and Results", "content": "We elaborate on the metrics and datasets used."}, {"title": "Experimental Study", "content": "We elaborate on the metrics and datasets used."}, {"title": "Standalone Planning", "content": "For standalone planning, we run the optimization procedure outlined in Section A for 1 step, and evaluate each trajectories for 3 times. For all datasets, we initialize $ \\mu_{\\Delta y}$ and $\\mu_{\\phi}$ to be 0, and $\\sigma_{\\Delta y}^2$ and $\\sigma_{\\phi}^2$ to be 0.1. We use different $(\\mu_{\\Delta x}, \\sigma_{\\Delta x}^2)$ across each dataset: (-0.1, 0.02) for RECON, (0.5,0.07) for TartanDrive, (-0.25, 0.04) for SCAND, and (-0.33,0.03) for HuRoN. We include the full standalone navigation planning results in Table 5. We find that using planning in the stand-alone setting performs better compared to other approaches, and specifically previous hard-coded policies."}]}