{"title": "Contrastive Learning-based Multi-Modal Architecture for Emoticon Prediction by Employing Image-Text Pairs", "authors": ["Ananya Pandey", "Dinesh Kumar Vishwakarma"], "abstract": "The emoticons are symbolic representations that generally accompany the textual content to visually enhance or summarize the true intention of a written message. Although widely utilized in the realm of social media, the core semantics of these emoticons have not been extensively explored based on multiple modalities. Incorporating textual and visual information within a single message develops an advanced way of conveying information. Hence, this research aims to analyze the relationship among sentences, visuals, and emoticons. For an orderly exposition, this paper initially provides a detailed examination of the various techniques for extracting multimodal features, emphasizing the pros and cons of each method. Through conducting a comprehensive examination of several multimodal algorithms, with specific emphasis on the fusion approaches, we have proposed a novel contrastive learning-based multimodal architecture. The proposed model employs the joint training of dual-branch encoder along with the contrastive learning to accurately map text and images into a common latent space. Our key finding is that by integrating the principle of contrastive learning with that of the other two branches yields superior results. The experimental results demonstrate that our suggested methodology surpasses existing multimodal approaches in terms of accuracy and robustness. The proposed model attained an accuracy of 91% and an MCC-score of 90% while assessing emoticons using the Multimodal-Twitter Emoticon dataset acquired from Twitter. We provide evidence that deep features acquired by contrastive learning are more efficient, suggesting that the proposed fusion technique also possesses strong generalisation capabilities for recognising emoticons across several modes.", "sections": [{"title": "1 Introduction", "content": "In the digital age of the social media platforms and internet, an exciting new way of human interaction has emerged. It involves the combination of concise, readable text messages and imagery ideograms known as emoticons. Emoticons are tiny symbols that represent individuals, settings, and objects. These symbolic expressions have gained widespread acceptance as a standard for communication on the web [1]. It is commonly used not just on Twitter but also on other well-known platforms like YouTube, WhatsApp, Telegram, Facebook, Instagram, and LinkedIn. According to Google Trends, the popularity of emoticons has been on the rise over the last decade, as seen in Figure 1. Emoticon prediction based solely on text has garnered attention and has been studied extensively from the perspective of Natural Language Processing. Aoki et al. [2]; Barbieri et al. [3]; Barbieri et al. [4]; Eisner et al. [5]; Ljubesic et al. [6]; and Boutet et al. [7] are few prominent exceptions encompass research dedicated to the semantics and usage of emoticons.\nAn immense amount of research has been conducted in the domain of emoticon prediction based on textual content. However, there is a dire need for further research on predicting emoticons based on multiple modalities. Hence, this research article demonstrates the importance of integrating visual information with texts in the realm of multimodal communication. Specifically, we highlight how combining texts and images in online communities can lead to more precise emoticon prediction models. We examine the utilization of emoticons within the popular social media platform Twitter. We propose a multimodal strategy to forecast the emoticons associated with a Twitter post, considering both its textual content and accompanying image. We rely on visual modality to enhance the process of selecting the most suitable emoticons for a post. Our research demonstrates that incorporating both text and images in posts enhances the precision of emoticon prediction in comparison to relying solely on textual information. It may be inferred that textual and visual content incorporate distinct yet complementary aspects of using emoticons.\nIn a nutshell, an efficient method for determining the correct emoticon for different types of content can benefit a wide range of applications, such as recommending emoticons for text messages, sentiment analysis [8], hate speech detection [9], humour identification, sarcasm recognition [10]-[11], generating emoticon-rich posts, etc. Considering the fact that emoticons have the potential to deceive humans, automated emoticon forecasting software might pave the way for better language understanding. Therefore, by understanding the semantics of emoticons, we may enhance highly subjective tasks like emotion, sentiment and sarcasm recognition."}, {"title": "2 Related Work", "content": "This section covers all the latest research studies on emoticon prediction, with an emphasis on both single and multimodal approaches. To make things simple and easy for readers to follow, all of the research outcomes are provided in a tabular manner depending on several factors in Table 1."}, {"title": "3 Proposed Methodology", "content": "This section contains an in-depth analysis of the proposed paradigm for multimodal emoticon predictions. The problem is outlined in the first subsection. Then, a Contrastive Learning based Multimodal Architecture depicted Figure 4 in consists mainly of 3 components is introduced. The proposed model employs a dual-branch encoder design to accurately map text and images into a common latent space. This functionality is achieved through the joint training of both the encoders. Transformer-based visual encoder, Transformer-based textual encoder and an additional component involves the use of contrastive learning to uncover the hidden relationships within the text and pictures. It has been demonstrated that by integrating the principle of contrastive learning with that of the other two branches yields superior results. For simplicity, emoticon prediction based on text-image analysis is referred to multimodal emoticon prediction."}, {"title": "3.1.1 Task Definition", "content": "The task of multimodal emoticon prediction is defined as follows: Let $I$ and $T$ denote the sample spaces for an image and text, respectively. An example is comprised of a singular text string accompanied by supplementary visual data. Hence, each example consists of three elements: an image, a piece of text, and an emoticon as a class label. The expression for it is shown below:\n$E = \\{(I^{0}, T^{0}, L^{0}), (I^{1}, T^{1}, L^{1}), ... ..., (I^{i}, T^{i}, L^{i}), ... ..., (I^{m-1}, T^{m-1}, L^{m-1})\\}$\nwhere, $E$ denotes the entire set of instance triplets, $I^{j}$ symbolizes the images information, $T^{j}$ represents the text-based data, $L^{i}$ denotes emoticons numbered from 0 \u2014 9 as a class label for the $i^{th}$ sample, and $m$ is the count of the total number of examples in the entire dataset.\nMultimodal emoticon prediction aims to learn a mapping function $F: (I^{i}, T^{i}) \u2192 L^{i}$ predict most suitable emoticon for a multimodal tweet $\\{(I^{i},T^{i}, L^{i}|0 \u2264 i \u2264 m \u2212 1)\\}$. For multimodal emoticon prediction task, $L^{i} \u2208 \\{0,1,2,3,4,5,6,7,8,9\\}$, where 0 represents  while 9 denotes "}, {"title": "3.2 Contrastive Learning based Multimodal Architecture", "content": "We have proposed a multimodal architecture based on the principle of contrastive learning for the emoticon prediction task to effectively simulate the relationship and compatibility between image and text content. Figure 4 illustrates the proposed architecture. The proposed multimodal architecture comprises of three primary components: An Image encoder, a Text encoder, and a Contrastive learning component. The Image encoder is responsible for acquiring image embeddings, while the Text encoder acquires textual embeddings. The Contrastive learning element examines the pertinent attributes and similarities between the textual and image embeddings obtained in the preceding steps. The proposed model is demonstrated in the form of pseudocode in Table 2."}, {"title": "3.2.1 Transformer-based Visual Encoder", "content": "This sub-section will provide a comprehensive explanation of the proposed methodology for extracting pertinent details from visual cues. In the realms of image, audio and text analysis, the performance of deep neural network-based architectural designs succeeds over the conventional hand-crafted approaches for classification [38], [39], [40], [41]. The primary reason for their effectiveness lies in their capacity to enhance end-to-end relationships, facilitate autonomous feature learning, ensure efficient scalability, establish semantic representations, and offer flexibility. Furthermore, several researchers have been working to enhance the performance of pre-trained and custom-built ConvNets over the last few years by including attention [42], [43], [44] an additional architectural design component.\nThe utilization of ConvNets is not deemed essential in modern times. This is because a standalone transformer model [45] can effectively handle visual classification tasks by directly processing sequences of patches of images. Each patch is then converted into a vector via a Large Language Model referred as LLMs and processed using a transformer architecture."}, {"title": "3.2.2 Transformer-based Textual Encoder", "content": "The latest advancements in transformer-based models incorporate a self-attention mechanism to prioritise relevant information while disregarding irrelevant information. Recent studies primarily utilised these architectures [46] exclusively for text processing. Although there are different variants of BERT that aim to extract text features, but they differ from the [47] architecture design due to their reliance on absolute position encoding rather than relative position codification [48]. The [47] utilises relative positional encoding to incorporate information between pairwise positions, whereas absolute positional encoding does not take this into consideration. In absolute positional encoding, the embeddings for each location are initialised at random. As a result, the relationship between different positions is unknown. Instead of random embedding initialization, relative positional encoding generates a pairwise vector of size $(V,2 * V - 1)$, with the row index representing the desired word and the column index representing its position distance from previous and subsequent words. This information regarding relative positioning is dynamically integrated into the keys and values as part of the computation process in attention modules. Therefore, it is advantageous to use a transformer-based model that supports relative positional encoding. This feature provides greater flexibility to the model and leads to more accurate result.\nConsidering these factors, we have employed the encoder of [47] to transform the text into their respective embedding's. During the first step, the sentence T provided as input to the Transformer-based text encoder is partitioned into tokens $T = \\{t_{1}, t_{2}, ... ..., t_{n}\\}$, and each token is subsequently transformed into a vector representation $T_{vectorization} = \\{t^{v}_{1}, t^{v}_{2},......, t^{v}_{n}\\}$. Next, the acquired vectors are sent for relative position encoding. The results achieved through\n$e_{0} = [I_{class}; I_{1E}; I_{2E}; ... ...; I_{nE}] + E_{position}$\n$e_{n} = Multi \u2013 headed self attention(Layer Normaliztion(e_{n-1})) + e_{n-1}$\n$e_{n} = Multi \u2013 layer perceptron((e_{n}))+ e_{n}$\n$Y = Linear projection head(e_{n})$\nwhere, $H \u00d7 W$ denotes the height and width of the original image sample $I$, $C$ symbolizes the depth of an image, $P \u00d7 P$ denotes the resolution for each transformed patch $I_{p}$ of an image, $h = \\{1,2, ..., H\\}$ denotes number of attention heads, $E\u2208 R^{D\u00d7(P^{2}.C)}$ and $E_{position} \u2208 R^{D\u00d7(N+1)}$."}, {"title": "3.2.3 Contrastive Learning", "content": "The primary goal of multi-modal learning is to understand the relationships between images and text in a given batch $B = \\{I^{i}, T^{i}\\}_{i=1}^{m}$, which consists of $m$ examples represented as $(T^{i}, T^{i})$. Motivated by the architectural design of Contrastive visual-textual pre-training, we applied the concept of a similarity matrix to examine the relevant features and similarities between the embeddings of image-text pairs acquired by the transformer-based encoder in previous stages. In order to do this, firstly contrastive visual-textual pre-training model [49] trains an image encoder $f(I^{i}, L^{i})$ and a text encoder $g(T^{i}, L^{i})$, such that the embeddings of image-text pairings $\\{I^{i}, T^{i}\\}_{i=1}^{m} =\\{f(I^{i}, L^{i}), g(T^{i}, L^{i}) \\}_{i=1}^{m}$ become more similar to each other. It is important to mention that Y and Z are unit vectors of size $D \u2013 256$ that have been normalised using the L2 norm in the encoding phase. These vectors are then located on the similar hypersphere. Contrastive visual-textual pre-training model employs the loss function $C (I,T)$ as specified in the cited approach [13] with the goal of ensuring that pairs (Y, Z) possess both similarity and distance. The formulation is represented by the Eq. (5) and Eq. (6):\n$\\frac{1}{m} \\sum^{<vpo}_{Y i=1} \\frac{\\exp (\\text{sim}(Y_{i}, Z_{i})/\\tau}{\\sum^{m}_{i=1} \\exp (\\text{sim}(Y_{i}, Z_{i})/\\tau}$\n$L_{Contrastive \\text{visual-textual}} (C (Y_{i}, Z_{i}) + CY_{i}, Z_{i})$\nSimilar to other approaches in computational linguistics, [49] uses a dot product to calculate similarity $(sim(..))$ between two vectors. It also employs a learnable temperature parameter $(\\tau)$ to adjust the magnitude of the observed similarity and $L_{contrastive \\text{visual-textual}}$ is the mean loss of combined image and text pairs. In general, the model learns a multi-modal embedding space by training its encoders to maximize the cosine similarity between the picture and text embeddings of the N correct pairs in the batch, while minimizing the cosine similarity between the embeddings of the $N^{2} \u2013 N$ incorrect pairings. Hence, by representing both images and texts using the coherent embedding space, our proposed contrastive learning-based model is capable of doing two essential tasks: (a) Optimize the cosine similarity between the image and text embeddings for N real pairs in the batch, aiming to maximize it. (b) Additionally, minimize the cosine similarity between the embeddings of N(N \u2212 1) erroneous pairings. During the pre-training phase of [49], the model is trained using a contrastive loss function, as depicted in Eq. (6). The main objective of this loss function is to promote the aggregation of embeddings for related or positive pairings (text and picture that match) while simultaneously driving apart the embeddings for unrelated or negative pairs (text and image that do not match). The purpose of the model is to minimise the loss for positive pairs and maximise the loss for negative pairs. The pair with the maximum cosine similarity scores, as indicated by Eq. (7), is between two D-dimensional vectors, referred to as Y and Z. These obtained vectors are then fed into the simple artificial neural network and processed using softmax as an activation function to provide probabilities for each label.\n$\\text{Sim}(I,T)= \\delta \\frac{YZ}{\\lVert Y\\rVert \\lVert Z \\rVert} = \\frac{\\sum^{2i=1} Y_{i}Z_{i}}{{\\sqrt{\\sum^{m}_{i=1}(Y_{i})^{2}} \\times {\\sqrt{\\sum^{m}_{i=1}(Z_{i})^{2}}}}$"}, {"title": "4 Experimental Setup and Results", "content": "The following section of the research article will present a detailed description of the optimal experimental configurations, the dataset utilised, and the experimental results obtained using our proposed approach."}, {"title": "4.1 Experimental Configuration", "content": "It is crucial to determine an appropriate range for hyper-parameters in a learning algorithm, as they play a significant role in controlling the learning process. The hyper-parameters underwent a random testing process using various values. As a result, the values that yielded the most optimal outcomes for our proposed framework were subsequently set as fixed. Since, the joint embedding from both modalities was obtained using the concept of contrastive learning. Therefore, the hyper-parameter values used were almost similar to those in [49]. The encoders utilised for both images and text in our proposed model referred as Contrastive Learning based Multimodal Architecture are equipped with 12 number of attention heads. The image samples denoted as $I \u2208 R^{H\u00d7W\u00d7C}$ from the dataset are scaled to a resolution of 224 \u00d7 224 before being passed to a visual encoder. Adam is used as an optimizer with the default learning rate of 0.001. The values of \u03b2\u2081 and \u03b22 for using Adam are set to 0.9 and 0.99. A dropout rate of 0.2 has been implemented. The contrastive loss function is also used (shown in Equation (5)) with a temperature scaling factor $\u03c4 = 0.1$ and margin $M = 0.7$ to combine the embeddings of images and text with the highest similarity score. The proposed architecture is fine-tuned for 20 epochs with a batch size of 32. By the 20th epoch, it becomes evident that there has been no noticeable rise in accuracy, as the results reach a point of saturation. From a total of 21k samples, 16k were allocated for training purposes, while the remaining 5k samples were dedicated to testing our proposed model."}, {"title": "4.2 Hardware Configuration", "content": "An Azure virtual machine with premium specifications was used for training and evaluation of the proposed model. This machine provides 100 GB of hard drive space, NVIDIA-A100 GPU with 80 GB of graphics memory, CUDA version 12.4, and 384 GB of RAM. We utilised an 8 \u00d7 A100 GPU cluster for 41 days, which is roughly comparable to 1000 GPU hours, and a pool memory of 640 GB to train the entire model from end to end. To derive useful information from text-image pairings, models were constructed using the Pytorch frameworks."}, {"title": "4.3 Dataset Description", "content": "We have employed the Multimodal-TwitterEmoticon dataset, provided by Ebrahimian et al. [13], to evaluate our suggested approach for emoticon prediction. Multimodal-TwitterEmoticon dataset consists of 21k English tweets, each containing an image, the accompanying text, and a single emoticon. Emoticons used in tweets varies based on their popularity. Figure 7(A) illustrates the frequency in percentage for most widely used emoticons. These ten most prevalent and often used emoticons have been taken into account for analyses. Figure 7(B) also depicts the statistical information regarding the number of samples associated with each 10 emoticons across the entire dataset where 0 represents  while 9 denotes . The experiment focuses on predicting emoticons based on image-text pairs. From a total of 21k samples, 16k were allocated for training purposes, while the remaining 5k samples were dedicated to testing our proposed model."}, {"title": "4.4 Experimental Results, Baseline Comparison and Analysis", "content": "We evaluate our proposed Contrastive Learning-Based Multimodal Architecture on Multimodal-TwitterEmoticon dataset provided by provided by Ebrahimian et al. [13]. The experimental results of our model are presented in Table 3. In addition to calculating the overall accuracy, macro-average and weighted-average, we have computed the Precision, Recall, and F1 score for each class separately to better analyse the results. By taking into consideration every aspect of the confusion matrix represented in Figure 10, Matthews Correlation Coefficient (MCC \u2013 Score) provides a more fair evaluation. Also, when the repercussions of false positives and false negatives differ, MCC becomes more relevant than F1. Hence, we have also calculated MCC-Score.\nFigure 8 shows the training and validation loss curves, which can be used to better rely on the results. Receiver operating characteristic (ROC) curve is considered to be more significant when evaluating model's performance. Unlike Accuracy, which solely evaluates the number of right predictions, this statistic takes into account the trade-offs between Recall and Precision. It reveals the degree to which the model can differentiate across categories. A higher AUC indicates that the model is effective at making accurate predictions. Figure 9 displays the ROC curve, which has an area under the curve (AUC) of 0.82."}, {"title": "4.5 Ablation Study", "content": "To evaluate the efficacy of each individual element in the proposed architecture, we have performed an ablation study. In the ablation experiment, the text encoder proposed in the study is substituted with BERT-base, BERT-large[52], Roberta[53] and T5 models, while the image encoder is replaced with the ResNet-101[54], EfficientNet-B7[55], ResNext[56], RegNet[57] and Vit-base-patch32 models. The results have undergone analysis using both unimodal and multimodal configurations."}, {"title": "5 Conclusion and Future Directions", "content": "The main aim of this research is to introduce an innovative and novel framework called Contrastive Learning-Based Multimodal Architecture for the prediction of emoticons. This architecture is designed to work effectively with image-text pairs. Our suggested approach surpasses existing cutting-edge techniques in analysing the interplay between caption and visual modalities. The sentiment elicited by a phrase can exhibit variability in various scenarios, depending upon the context. Thus, it is essential to employ a blend of textual and visual information to attain more accurate prediction. Motivated by this, we developed a novel dual-branch architecture comprises of three primary components: Transformer-based visual encoder, Transformer-based textual encoder and an additional component involves the use of contrastive learning to uncover the hidden relationships within the text and images. In general, the model learns a multi-modal embedding space by the joint training of two encoders to maximize the cosine similarity between the picture and text embeddings of the N correct pairs in the batch, while minimizing the cosine similarity between the embeddings of the N2 \u2013 N incorrect pairings. A thorough analysis on one of the standard datasets referred as Multimodal-TwitterEmoticon shown that our suggested strategy outperforms strong baseline models.\nDespite the excellent outcomes that have been achieved, there remain numerous opportunities for research. These includes creation of publically accessible dataset with an objective of mutli-label emoticon prediction, enhancing feature extraction methods, integrating adversarial learning capabilities to the fusion module, investigating a broader range of multimedia data, encompassing both video and acoustic formats, in order to potentially uncover more detailed semantic relations."}]}