{"title": "Assessment of LLM Responses to End-user Security Questions", "authors": ["Vijay Prakash", "Kevin Lee", "Arkaprabha Bhattacharya", "Danny Yuxing Huang", "Jessica Staddon"], "abstract": "Answering end user security questions is challenging. While large language models (LLMs) like GPT, LLAMA, and Gemini are far from error-free, they have shown promise in answering a variety of questions outside of security. We studied LLM performance in the area of end user security by qualitatively evaluating 3 popular LLMs on 900 systematically collected end user security questions.\nWhile LLMs demonstrate broad generalist \"knowledge\" of end user security information, there are patterns of errors and limitations across LLMs consisting of stale and inaccurate answers, and indirect or unresponsive communication styles, all of which impacts the quality of information received. Based on these patterns, we suggest directions for model improvement and recommend user strategies for interacting with LLMs when seeking assistance with security.", "sections": [{"title": "1. Introduction", "content": "Large language model-based conversational assistants (\"LLMs\" hereinafter) have grown increasingly popular among users for distilling complex information in user-friendly ways. This capability has made LLMs an enticing resource for computer security and online safety. For instance, they provide financial advice as chatbots [1], and are endorsed as reliable tools for fraud detection by security vendors [2], [3], [4]. The use of LLMs for computer security advice is the natural evolution of the user practice of seeking online advice for security-relevant tasks such as strong password creation [5]. However, the quality of LLM responses to end user security questions has not been rigorously studied.\nThere is a critical need to evaluate the quality of LLM support in the user security context as adoption accelerates.\nThere are already well-documented risks to relying on LLMs for information broadly, including hallucinations, challenges in reasoning, and vulnerability to concept drift [6], [7], [8], [9], [10], [11], [12], [13]. Further, LLMs have been found to perpetuate known misconceptions in security and online safety [14]. A larger and broader evaluation of LLM security responses (beyond known misconceptions [14]) is needed in order to comprehensively outline the boundaries of LLM security \"knowledge\" to inform users about LLM shortcomings, and support future model development and improvement.\nTo address this gap, we evaluate the quality of responses generated by three LLMs\u2014GPT, LLaMA, and Gemini-to end user questions about security and online safety. We first collected user security and online safety questions covering 7 security areas (many of which are aligned with prior work [15]) from frequently asked questions (FAQs) published by various consumer safety and commercial organizations (Section 3.1). Then, for each question, we identified authoritative sources for assessing the quality of the generated LLM responses. Finally, we conducted a qualitative assessment following the information integrity framework [16], of the 1,244 LLM responses (generated using \u201czero-shot prompting\" [17]) to measure their quality, specifically, their accuracy, completeness, and relevance. In addition, we evaluated an aspect of communication style, directness, in which necessary and important information is presented first to make efficient use of user attention [18].\nOur assessment is thus guided by the following research questions (RQs):\nRQ1: What is the information quality of LLM responses to user security questions?\nRQ2: What are the patterns of deficient or erroneous LLM responses to user security questions?\nRQ3: What actionable suggestions can improve LLM performance for users seeking security advice and developers building models?\nWe found that LLMs provide high quality information expressed in a user-friendly manner when prompted with general knowledge security questions. However, we also found consistent deficiencies and errors that are present across LLMs. Surfacing these limitations can help users more effectively engage with LLMs and support model development improvements.\nIn particular, LLMs often fail to leverage research findings that could improve communication of important nuances in guidance and reduce misinterpretation of more specialized concepts. For example, LLMs provide outdated password guidance, neglect the risks of app permissions, fail to connect phishing and HTTPS URLs, and confuse zero-knowledge proofs and zero-knowledge encryption\u2014all topics well-studied in research. In addition, our findings tentatively suggest LLMs may be influenced by marketing materials in that they tend to over-promise the capabilities of technology and products (e.g., endorsing VPNs for phishing protection), and miss important criticisms (e.g., neglecting the transparency benefits of open source code and data breach handling).\nWe also note several instances in which safety guardrails appear to block users from receiving answers to benign"}, {"title": "2. Background and Related Work", "content": "Security and privacy advice for non-experts. Prior work has explored many aspects of user security and privacy advice, including the sources and behavioral impact of ad-vice by demographic segment [5], [20], user advice source selection [20], [21], [22] and availability of advice [23]. The security mental models and misconceptions that advice would ideally address [24], [25] and how they are impacted by mass media [26], are also well-studied.\nFrom user studies, the research community has found that expert security advice can be complex and difficult for users to follow [27], [15]. By conducting user studies involv-ing users and administrators, Murray et al. found available security advice to be contradictory and ambiguous resulting in user disagreement regarding the value of advice [28]\nClosest to this paper is Chen et al.'s [14] evaluation of LLM ability to refute security misconceptions held by the general public. Chen et al. found that Bard and ChatGPT correctly negate security misconceptions approximately 70% of the time in their evaluation.\nTo the best of our knowledge, this paper is only the second to consider the quality of LLM user security advice and the first to broadly assess LLM advice across security topics. Our evaluation extends [14], which focuses on a set of known security misconceptions, both in terms of the breadth of topics and evaluation criteria, including both content and communication style. While the scope of our work is different and broader than [14] we do find evidence of some of the misconceptions they study, i.e., \"Websites that use HTTPS are trustworthy,\" and \"HTTPS protocol could protect against phishing\". We also find incorrect links in responses, although at small scale and only when models listed them voluntarily."}, {"title": "3. Method: Question Corpus & LLM Evaluation", "content": "We explored the RQs through three tasks. First, we built a corpus of questions and responses reflecting end user security concerns. Then, we formulated prompts based on the questions to evaluate multiple LLMs. Finally, we assessed the information quality and directness of the LLM responses. In this section we describe how we implemented these tasks to prioritize common end-user questions and reliably measure LLM performance on the questions."}, {"title": "3.1. Question Collection", "content": "Answering our RQs requires a corpus of questions rep-resentative of common end user security concerns. We were unable to obtain any corpus from prior work. Chen et al. had studied common user security misconceptions, but the questions used were generated to follow a limited template (e.g. \"I've heard of this claim: {MISCONCEPTION}. Is it true?\") [14]. Redmiles et al. had gathered a corpus of 140 user security questions across 12 security and privacy topics to assess the quality of web security / privacy advice, but the corpus was not available in its entirety. Learned through communication with the authors.We found other potential user security-oriented datasets that were not worded as questions or were tailored to particular platforms and geographic regions. Given the gap in question sources, we built a corpus of questions by mining dominant security topics and using them alongisde question templates to extract user questions from trusted resources. We define trusted resources as organizations that either have a mission tied to user security (e.g. govern-ment agencies, and universities) or a business that strongly depends on customer security (e.g., financial services, and security vendors).2 Dominant security topics. Through a thematic analysis of security content trusted resources made available to users, namely, National Institute of Standards and Technology (NIST) [49], National Cyber Security Centre (NCSC) [50], Cybersecurity and Infrastructure Security Agency (CISA) [51], and National Cybersecurity Alliance (NCA) [52], we identified seven high level security categories and underlying fine grained security topics. Our Authentication, Software Updates and Safe Browsing categories appear to align well with categories in [15] (e.g. \u201cPasswords\" and \"Network Security\", \u201cSoftware\u201d, \u201cBrowsers\u201d, respectively) although the questions in our respective corpora likely differ.\nQuestions and Authoritative Answer Sources. To col-lect questions in the 7 security categories we developed Google search queries related to topics. With the queries we identified websites meeting our trusted resource criteria that are popular sources"}, {"title": "3.2. Response Generation", "content": "Prompt engineering is an active research area and recent studies suggest it is at best an emerging practice for users (e.g., [54]), so we chose the default LLM conversation tem-plates as the best proxy for a typical user-LLM interaction. With the default templates, we appended our questions to generate responses from GPT, Gemini, and LLaMA. The exact conversation template used to generate responses for all of our evaluations is below:\n1 conversation_messages = [\n{\"role\": \"system\",\n\"content\": \"You are a helpful assistant.\"},\n{\"role\": \"user\",\n\"content\": \"Hello!\"},\n{\"role\": \"assistant\",\n\"content\": \"Hello! How can I assist you today?\"},\n{\"role\": \"user\",\n\"content\": \"-> Questions go here. <-\"}]\n2\n3\n4\n5\n6\n7\n8\n9"}, {"title": "3.3. Response Evaluation", "content": "We carefully reviewed LLM responses, assessing both content and communication style. With respect to content, we focus on information integrity and evaluate the accuracy, completeness, and relevancy of LLM responses as in Flowerday et al. [16]. We also consider communication style by way of directness (also known as the inverted pyramid style of writing [55]), where necessary and important information is presented first. This communication style is often recommended for web [56], [57], [18] and nonfiction writing. For our study, we define evaluation criteria as the following:\n1) Accuracy: The proportion of verifiably correct informa-tion\n2) Thoroughness: The degree to which all necessary an-swer information is provided\n3) Relevancy: The proportion of information that is relevant\n4) Directness: Whether the initial text is responsive to the question asked.\nSince there was no established codebook for interpreting these attributes in the user security context; we developed a codebook (to be made available) that enabled the authors to consistently assess these attributes on a 3-point Likert scale. Each author coded LLM responses manually to evaluate the 4 attributes. The authors used information present in the authoritative source of the question, along with their subject matter expertise, to assess all the facts in a response by following the detailed definition of four earlier mentioned criteria to best of their ability. In our codebook, the 3-point Likert scale for accuracy, thoroughness and relevancy is summarized as: the attribute is completely achieved (e.g., in the case of accuracy, all information is determined to be correct), the attribute is partially achieved, and the attribute is only minimally achieved. Directness is assessed as a binary label, i.e., the initial LLM text is directly responsive to the question asked or it is not. The authors also added notes explaining their evaluation of the attributes; these notes facilitated the analysis in Section 3.5.\nExamples of coded responses and the complete defini-tion of each code point and its options in our codebook can be accessed here.\nThese attributes are motivated by the need to manage risk in Al systems and past research. For example, a correct but incomplete answer could create user risk. Relevancy is important given user difficulty with prioritizing advice [15] and reconciling contradictions [28]. Finally, directness reduces the risk of losing user attention due to unnecessary information [18]."}, {"title": "3.3.1. Evaluation Criteria & Codebook", "content": "We carefully review LLM responses, assessing both content and communication style. With respect to content, we focus on information integrity and evaluate the accuracy, completeness, and relevancy of LLM responses as in Flowerday et al. [16]. We also consider communication style by way of directness (also known as the inverted pyramid style of writing [55]), where necessary and important information is presented first. This communication style is often recommended for web [56], [57], [18] and nonfiction writing. For our study, we define evaluation criteria as the following:\n1) Accuracy: The proportion of verifiably correct informa-tion\n2) Thoroughness: The degree to which all necessary an-swer information is provided\n3) Relevancy: The proportion of information that is relevant\n4) Directness: Whether the initial text is responsive to the question asked.\nSince there was no established codebook for interpreting these attributes in the user security context; we developed a codebook (to be made available) that enabled the authors to consistently assess these attributes on a 3-point Likert scale. Each author coded LLM responses manually to evaluate the 4 attributes. The authors used information present in the authoritative source of the question, along with their subject matter expertise, to assess all the facts in a response by following the detailed definition of four earlier mentioned criteria to best of their ability. In our codebook, the 3-point Likert scale for accuracy, thoroughness and relevancy is summarized as: the attribute is completely achieved (e.g., in the case of accuracy, all information is determined to be correct), the attribute is partially achieved, and the attribute is only minimally achieved. Directness is assessed as a binary label, i.e., the initial LLM text is directly responsive to the question asked or it is not. The authors also added notes explaining their evaluation of the attributes; these notes facilitated the analysis in Section 3.5.\nExamples of coded responses and the complete defini-tion of each code point and its options in our codebook can be accessed here.\nThese attributes are motivated by the need to manage risk in Al systems and past research. For example, a correct but incomplete answer could create user risk. Relevancy is important given user difficulty with prioritizing advice [15] and reconciling contradictions [28]. Finally, directness reduces the risk of losing user attention due to unnecessary information [18]."}, {"title": "3.3.2. Inter-rater Reliability (IRR)", "content": "To measure the quality of responses we followed the standard practice of qualitative coding. First, we sampled a small percentage of questions and coded their responses over multiple rounds. We used Fleiss Kappa for IRR calculation because we had more than two coders [58]. In our study, 4 coders coded 72 GPT responses independently across 3 coding rounds, and achieved IRR with substantial agreement on each code point [59]. Our Fleiss Kappa for the third coding round was 0.73, 0.63, and 0.76 for Accuracy, Thorough-ness, and Directness, respectively. We could not calculate Kappa for Relevancy because a single rating (\"Relevant\" with 100% agreement) was assigned to all the questions we coded. After IRR establishment, we divided the rest of the question responses among the coders for evaluation.\nBreakdown of the 72 responses used to establish IRR. Out of 858 collected questions, 30 were used during IRR. 10 questions out of these 30 were rephrased with Google search APIs (more details are in the prompt sensitivity experiment Section 3.4.1) to generate another set of 42 semantically similar questions, totaling 72(10 + 42 + 20).\nPost-IRR evaluation of 828 responses. In our dataset, LLMs responses to open-ended questions contain ~335 words on average. Evaluation of 828 responses of that size is a large amount of work, so we onboarded a fifth researcher to help code responses. The fifth researcher was trained to code responses using our codebook on some of the 72 responses the four researchers had coded during the IRR establishment phase, and the remaining were used to measure the IRR among 5 coders. After onboarding the 5th coder, the remaining, 828 questions were split among the five researchers for evaluation.\\Coder background. All the researchers involved in coding are the co-authors of this paper and are well-experienced in computer security. Two of them are PhD students, two of them are university professors, and one them works in industry after completing their PhD.\nTotal number of responses assessed and final evaluation assigned to them. In our study, we evaluated 900 GPT responses in total, 72 during the IRR and 828 post-IRR. During the IRR establishment, coders did not have 100% agreement in their evaluation of some responses, so to assign a final rating these coders regrouped to discuss and assign a final collaborative rating. After dividing the rest of the questions, if a coder felt unsure about their assessment they collaborated with another coder. In addition, every coder's evaluation was spot-checked by a second researcher to ensure that the original assessment was correct."}, {"title": "3.4. LLM Response Reliability", "content": "Before making conclusions about LLM performance, it is necessary to gauge response reliability over multiple invocations of the same, or semantically similar, prompts. In [14] some unreliability was found in responses to repeated and paraphrased queries. To the best of our knowledge, LLM response reliability has not been studied for open-ended user security questions, so we conducted two analyses to gauge response reliability for end user security questions."}, {"title": "3.4.1. Reliability for Paraphrased Questions", "content": "Our methodology to measure reliability in LLM re-sponses across paraphrased questions (referred to as \"queries\" in [14]) was the following:\n1) Question selection: First we sampled 16 questions from our list of 858 questions such that they covered all the security categories in Table 2 and security question themes mentioned in the Section 3.1.\n2) Rephrasing questions: We paraphrased original ques-tions (OQs) using Google's autocomplete API [60] (used by Google in the search bar) and related question sugges-tions [61] (often presented on the Google search result page as \"People also ask\", \"More to ask\", or \"Questions related to your search\"). Using these Google APIs 16 questions were paraphrased 274 times, referred as para-phrased questions (PQs) here; then using the transformer-based model Universal Semantic Encoder (USE) [62] we measured how semantically close a Google API paraphrase is to the OQ to reduce the list of PQs to 157 for 15 OQs. Finally, 4 researches manually verified whether the remaining 157 PQs for the 15 OQs were good paraphrasings. At the conclusion, we had 10 OQs that were paraphrased to 42 questions (PQs) in total. The number of times each OQ was rephrased was in range of 1 to 13.\n3) Response generation: We collected responses for all 10 OQs and their 42 PQs by presenting them to an LLM model as prompts with the setup described in 3.2.\n4) Manual response similarity assessment: Each pair of responses (one for an OQ and another for its PQ) was coded by two researchers independently on a similarity level of ordinal scale completely similar, mostly sim-ilar, insufficiently similar. Researchers assigned sim-ilarity levels following the guidelines described in our codebook. Any disagreements between two researcher assigned similarity levels were resolved with discussion and a single similarity level was assigned."}, {"title": "3.4.2. Reliability for Repeated Questions", "content": "Our methodology for measuring consistency in LLM re-sponses across repeated questions was the following: (i) We sampled 7 questions from our list of 858 questions; (ii) Each question was presented to an LLM model 5 times to collect their response; (iii) All 5 responses for a question were first individually rated by two researchers on our evaluation criteria mentioned in 3.3 following the guidelines mentioned in our codebook; (iv) Finally, for each question researchers rated 5 responses as a group on similarity level of ordinal scale completely similar, mostly similar, insufficiently similar.\nConclusion: To gauge response reliability for user security questions, we gathered LLM responses to repeated and paraphrased questions and two authors manually reviewed to assess the semantic similarity of the responses. We found LLM responses to generally be semantically similar and consequently we generated a single response for a given question and LLM in the remainder of the evaluation. See the detailed results of our evaluation in Appendix A."}, {"title": "3.5. LLM Error Pattern Discovery", "content": "We looked for error patterns mistakes or knowledge deficiencies that exist in multiple LLM responses in the LLMs we studied (RQ2). After evaluating LLM responses according to the 4 attributes (accuracy, completeness, rele-vancy and directness) we used inductive thematic analysis [63] to find error patterns:\n1) Tagging imperfect GPT responses. For all the GPT responses that did not fully achieve at least one of our 4 evaluation attributes (i.e., were either not completely accu-rate, thorough, or relevant, and/or were not direct), we used each coder's notes to iteratively and collaboratively develop a codebook capturing the nature of attribute deficiencies. In particular, we (i) started with a codebook consisting of error characterizations (termed \"tags\") independently proposed by coders. Each tag in the codebook was clearly defined along with examples. (ii) All the coders convened to discuss the codebook to achieve consensus on the tags and their meaning. (iii) Coders started tagging imperfect responses following the tag codebook. When a researcher encountered a error not represented in our codebook, they brought it up for discussion with the rest of the researchers. (iv) We repeated steps (ii) \u2013 (iv) iteratively until all the imperfect responses were tagged. After this analysis, the imperfect responses were covered by a total of 11 tags.\n2) Qualitative analysis to find error patterns in GPT responses. Tags facilitated the identification of error patterns by focusing attention on the groups of responses that were most likely to have similar deficiencies. For example, we discovered the error pattern Lack of adherence to research, by reviewing responses tagged with Lack of adoption of re-cent best practices from research, Broad statements, and Missing a few official facts, solutions, and aspects tags, then narrowed them down to an associated security category or topic(s).\nAfter tagging responses, we looked for error patterns in GPT responses using the assigned tags. Our pattern discovery methodology was following: (i) all the researchers reviewed responses grouped under an error theme and noted them for discussion with the other researchers. (ii) The patterns on which all the researchers agreed were included in the findings. After this analysis, we had 13 patterns across 304 imperfect responses."}, {"title": "4. Findings", "content": "In our evaluation, GPT, LLaMA, and Gemini attempted to answer almost all the questions in our corpus, however, often with errors in correctness, thoroughness, or relevancy. In this section, we briefly discuss the evaluation results and then discuss the error patterns seen across the three models in more detail, along with cause hypotheses. We discuss guidance for users and developers based on these patterns in Section 5.\nAs mentioned earlier, assessing the information integrity of LLM responses was time-consuming. Over 6 months we evaluated 900 GPT responses for GPT (including estab-lishing a high IRR). Out of these 900 GPT responses, 415 were perfect (i.e., correct, thorough, relevant, and direct), and 485 were imperfect. GPT responses were accurate, thorough, relevant, and direct 73%, 68%, 98%, and 83% of the time, respectively.\n1) There is a downward trend in GPT's accuracy from fac-tual (highest accuracy), conceptual, to procedural ques-tions (lowest accuracy), where knowledge category is listed in order of cognitive processing required [53].\n2) GPT performance is lower overall, and in each security categories individually, for questions that are related to a product, platform or companycompared to the questions that are not. The probability of a GPT response being completely accurate, thorough, and relevant given that the question is related to a product, platform or company is 44%. The probability increases to 60% when the question is not related to a product, platform or company.\n3) GPT performs worse in the authentication and anti-virus/anti-malware security categories possibly due to the large number of product, platform or company-related questions (81.15% of the 398 questions)."}, {"title": "4.1. LLM Content Error Patterns", "content": "We found several error patterns in LLM responses that could negatively impact user security. We found these pat-terns through error thematic analysis of imperfect responses from GPT and confirmed their existence in LLaMA and Gemini responses.\nPattern 1: Lack of adherence to research. LLMs failed to incorporate recent research findings in their responses making them outdated, incorrect, and incomplete. We no-ticed this pattern predominantly in password guidance (65 responses) and HTTPS over-reliance (15 responses) across three models respectively.\nPassword guidance. On password guidance, all of the LLMs we evaluated still suggested users employ complexity and update their passwords regularly. The excerpt from Gemini's response. These suggestions are at odds with recent research that suggest that complexity requirements reduce usability [64], and regular password updates (barring data breach incidents) don't offer their intended benefits, being less important than other important factors [65], [66], [67].\nHTTPS over-reliance. All three LLMs recommended blindly trusting HTTPS URLs in emails or visit-ing websites occasionally. The excerpt from LLaMA's response. HTTPS in phishing emails is a common occurrence nowadays as per the 2019 Anti-Phishing Working Group (APWG) findings that reported 58% of URLs in phishing emails used HTTPS [69].\nApp permissions risks. All the LLMs neglected to mention the risk of permissions when asked questions about the necessity of app permissions.Prior work has highlighted risks.\nNone of the LLMs mention the other important findings from the research for questions \u201cWhy do I need antivirus software?\u201d and \u201cWhat can happen if someone breaks into my home Wi-Fi?", "2": "Over-prescription of security technologies and products. GPT presented an overly optimistic picture of security solutions either without mentioning their short-coming or recommending them for unintended purposes often making the responses not completely accurate and incomplete. We observed this pattern across 19 responses among three LLMs.\n1) Virtual private networks (VPNs): GPT and Gemini suggested using VPN to avoid phishing attacks and encrypt Bluetooth traffic. For the question \"What are the benefits of using a VPN?\"\n2) Multi-factor authentication (MFA): All of the LLMs suggested using MFA to protect against spoofing where attackers falsely present themselves as someone else.\n3) Firewalls: GPT and LLaMA suggested using firewalls to protect against phishing.\nWe hypothesize that a possible cause for these error patterns is data quality where unverified claims from market-ing materials and blog posts about technology and security products were learned by the models.\nPattern 3: Missing transparency arguments that build trust. We noticed that LLMs frequently failed to bring up the transparency arguments that would build trust among the users open source code base, public vulnerability disclosure programs (bug bounty programs), and previous handling of data breaches and security incidents when answering questions related to password managers, VPNs, and MFA providers.\nGPT, for instance, failed to bring up any of these ar-guments consistently when recommending or talking about password managers, even when specifically asked about password managers that haven't open-sourced their code base or at least made an official statement about implemen-tation. Gemini sporadically brought up this argument, while LLaMA falsely claimed that Google password manager is open source for the question \"Why should I trust Google password manager with my passwords?\"\nPattern 4: Missing threat angles. All three LLMs left out solutions and risks from different threat angle, leaving users vulnerable to threats. We observed this pattern in 34 responses across three models in our evaluations. Below, we list some of the observed cases involving a variety of products and use cases.\nPassword manager. For questions about password man-agers, master password compromise, or unusual activity related to the master password, LLMs did not mention revoking access to the unusual account and changing the password of all the accounts whose credentials were stored.\nIoT and smart devices. For the question \"How can I make sure my IoT devices are secure?\"\nAntivirus and online safety For the question \"How can I remove malware from my computer?\" Pattern 5: Incorrect and incomplete user interface (UI) steps: When asked procedural questions (or \u201chow-to", "6": "Security misinformation due to hallucinations. Hallucination is a known problem in LLMs consisting of nonsensical and incorrect claims, resulting in misinformation for non-tech-savvy users."}, {"title": "4.2. LLM Communication Error Patterns", "content": "The goal of alignment is ensuring LLM-generated con-tent is compatible with human preferences and values to make them more usable, safe, and reliable. LLMs are aligned with user preferences using reinforcement learning [83], [84].\nPattern 1: Misinterpreting the questions. All three models misinterpreted questions with homonyms and sometimes user requirements.\nPattern 2: Overly restrictive safety guardrails. We noticed that Gemini and LLaMA sometimes refuse to answer simple end-user questions deeming them too dangerous.\nPattern 3: LLM-splaining. Previous works have shown that users exhibit short attention spans while reading on-line [56], [57], [18] so starting responses with facts that user already knows (inferred from the question), generic, or encouraging/patronizing statements (for e.g., \"That's a great question! It's important to feel secure about your passwords.\") could lose user attention. We call this form of indirectness LLM-splaining. All three models LLM-splained on numerous occasions (260 times).\nPattern 4: Responses geared towards developers and system administrators. All the models presented solutions and facts that were too advanced for end-users on some occasions."}, {"title": "5. Guidance and Open Problems", "content": "Previous research has found that decision making in AI-assisted contexts improves when humans and AI comple-ment each other, taking their respective weaknesses into account [85]. Without an understanding of AI weaknesses, humans may over- or under-rely on AI for assistance [86].\nOn the human side, three well-established weaknesses or challenges for which AI can potentially compensate are: 1. The challenge of keeping up with security guidance and tools, 2. User mental models of security that may not be aligned with good security practices [87] and 3. Barriers to user acceptance of security advice [88]. We consider the findings of Section 4 in the context of these challenges to distill guidance for end-users and model developers toward improving human-LLM interaction in the user security as-sistance context. We also identify places where additional research is needed to extend this guidance."}, {"title": "5.1. User Guidance", "content": "We discuss what our evaluation suggests regarding secu-rity topics and question types best suited to LLMs, how user questions are expressed (i.e., prompt engineering), and how users can assess LLM responses.\nWhen to use LLMs. Based on our evaluation, we discour-age users from relying on LLMs for procedural questions (e.g., \"How do I...?", "What is malware?\"), basic risk assessment questions (e.g., \u201cWill 2 factor authentication protect me?": "and core functionality of established products.\nHow to communicate with LLMs. We used questions from published FAQs and question templates that our own review found to be written in \"plain\" English (i.e., no technical, rare or otherwise lesser known words and phrases [90]). We conducted a small experiment to test sensitivity of GPT to rephrased questions from this corpus (Section 3.4) and did not find sensitivity to phrasing.\nHowever, we note that the question of whether LLMs are sensitive to phrasing in organic user security questions has not been addressed by our experiments and some sensitivity to phrasing was found in [14].\nIn addition, there are several other open prompt engi-neering questions that would allow for important extensions of user guidance in this area.\nUser assessment of LLM responses. While this may change with LLM-enabled search, none of the LLMs we experimented with routinely cite sources when answering end user security questions."}, {"title": "5.2. Developer Guidance", "content": "In this section we discuss the primary areas of perfor-mance problems in our evaluation, organized according to the user challenges described at the beginning of Section 5. While we suggest directions for improvements, we are not prescriptive, as model developers are better positioned to identify root causes and craft solutions.\nGenerating correct security content. Our evaluation sug-gests augmenting LLM training data with more security research and reducing the support of security product mar-keting materials will improve LLM performance.\nAligning mental models with sound security practices. Our question corpus was gathered with the goal of broadly representing user concerns in security, rather than focusing on areas known to have potentially problematic user models.\nWe encourage model developers to use security user mental models research to explore potential areas of model bias and consider relying more heavily on authoritative sources in those areas.\nEncouraging adoption of security information. We rec-ommend developers tailor the balance between content and sources based on the nature of the user question."}, {"title": "6. Limitations and Future Work", "content": "The findings of this study depend on the questions used for the assessment. As for the computer security questions, there is no definite and exhaustive list, so we acknowledge limiting our assessment to the best of our ability to curate end-user security questions.\nSince we have not recruited users in our study to ask security questions, we acknowledge that in our questions we might miss the exact natural language users use to interact with LLMs.\nWe also acknowledge that our work is restricted to security-related end-user questions, due to the limited hu-man evaluation capability and time availability.\nLastly, the error reasons we hypothesized are our spec-ulations based on issues mentioned in NLP literature about LLMs, but we haven't verified them."}, {"title": "Appendix", "content": ""}, {"title": "1. Validation of LLM Response Reliability", "content": "Are LLMs responses consistent across paraphrased questions?\nTakeaway LLM responses are mostly insensitive to para-phrasing of open-ended user security questions.\nAre LLMs responses consistent across repeated questions?\nTakeaway LLM responses are mostly consistent given the same repeated questions."}, {"title": "2. Heuristics for Users", "content": "To give users simple heuristics to detect errors in LLM responses, we calculated conditional probability of error in other evaluation criterial given that user sees an error in one evaluation criteria"}, {"title": "3. More Error Themes and Patterns in Models", "content": "In our Error Theme Analysis (ETA), we found"}]}