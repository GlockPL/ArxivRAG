{"title": "Movie Recommendation with Poster Attention via Multi-modal Transformer Feature Fusion", "authors": ["Linhan Xia", "Yicheng Yang", "Ziou Chen", "Zheng Yang", "Shengxin Zhu"], "abstract": "Pre-trained models learn general representations from large datsets which can be fine-turned for specific tasks to significantly reduce training time. Pre-trained models like generative pretrained transformers (GPT), bidirectional encoder representations from transformers (BERT), vision transfomers (ViT) have become a cornerstone of current research in machine learning. This study proposes a multi-modal movie recommendation system by extract features of the well designed posters for each movie and the narrative text description of the movie. This system uses the BERT model to extract the information of text modality, the ViT model applied to extract the information of poster/image modality, and the Transformer architecture for feature fusion of all modalities to predict users' preference. The integration of pre-trained foundational models with some smaller data sets in downstream applications capture multi-modal content features in a more comprehensive manner, thereby providing more accurate recommendations. The efficiency of the proof-of-concept model is verified by the standard benchmark problem the MovieLens 100K and 1M datasets. The prediction accuracy of user ratings is enhanced in comparison to the baseline algorithm, thereby demonstrating the potential of this cross-modal algorithm to be applied for movie or video recommendation.", "sections": [{"title": "Introduction", "content": "The development and application of information technology has generated overload information [1] and it have become a difficult task for users to retrieve relevant information that matches their needs from the overload information. It may even cause a decrease in the quality of users' decision-making as well as an increase in their anxiety level [2].\nIn order to help users better filter information through reasonable information management and filtering mechanisms, recommender systems have emerged. The role of a recommender systems is to filter information that may interest users based on their interests, behaviors and preferences, and to provide personalized content in the massive amount of information to enhance user experience and satisfaction [3].\nRecommender systems usually integrate data mining, machine learning and other means to analyze users' historical behavior, social relationships and content charac- teristics of the information to build user-item interaction model [4]. The advent of recommender systems in the mid-1990s saw the emergence of algorithms that matched and customized content for users. These systems have since become a pervasive feature of e-commerce, social media, audio-visual entertainment, and numerous other fields.\nThe development of deep learning in recent years has greatly empowered recommender systems, and some of these methods are already working in real applications. For instance, YouTube uses Convolution Neural Networks(CNN) to extract poster fea- tures combined with user preferences [5]; Netflix uses recurrent neural networks (RNN) to track users' reading history to capture changes in user interests [6]; twitter uses graph neural networks (GNN) to analyze users' social relationships and interaction his- tory to achieve personalized topic recommendations [7]. What's more, recommendation systems can also be used for medical care [8] and knowledge discovery [9].\nHowever, with the popularization of internet applications and the proliferation of multimedia contents, it is difficult to meet users' needs with single-modal informa- tion. Among them, the data sparsity problem is a serious challenge. In the context of a recommendation system, the term 'sparsity problem' is used to describe a situation where interaction data between users and items is very sparse. This makes it chal- lenging for the system to accurately learn and predict users' preferences. Cross-modal recommender systems are supposed to capture content features more comprehensively by integrating and analyzing data from multiple modalities and thus provide more accurate recommendations. For instance, in movie recommendation, the data sparsity problem is alleviated by combining information such as text descriptions and poster images of movies.\nThis research proposes a movie recommender system that combines cross-modal data analysis and deep learning. The proposed system employs pre-trained model to mine and fuse multi-source information and uses a transformer architecture to fuse them. The multi-modal data set includes user characteristics and multi-modal data of movies. Result on the MovieLens benchmark problems shows that better prediction accuracy can be achieved in comparison to some traditional algorithms. This indicates the potential of multi-model recommendation."}, {"title": "Related Work", "content": "In the past two decades, the development of recommendation algorithms has had a profound impact on people's lives. Major content platforms such as Bytedance, Weibo, and Twitter have adopted different recommendation algorithms to serve their users and help them discover personalized content. With the development of the Internet industry, numerous researchers have put forward a variety of different methods. This chapter describes the relevant work in detail from two aspects: traditional approaches and multi-modal fusion approaches."}, {"title": "Single modal approaches of recommendation system", "content": "Single modal recommendation algorithms can be divided into three main branches: content based approach [10], collaborative recommendation approach [11] and hybrid recommender system [12].\nContent-based recommender systems have evolved over decades as well as relatively matured, and their core features are relying on content features and user character- istics to achieve accurate recommendations. Roy et al. provide an extensive overview of these principles, and they emphasize the importance of understanding user prefer- ences and content features to generate accurate recommendations [13]. The process of understanding user preference is often referred to as profile inference [14]. In addition, Dahdouh et al. emphasized the significance of tools for recommender system develop- ment, with particular reference to big data technologies such as Hadoop and Spark in content-based recommender systems [15].\nContent based recommender systems rely on content features to generate rec- ommendations. Based on the traditional algorithmic perspective, some scholars have proposed to analyze the text images or other content forms of the items to match with user preferences. Beel et al. have used the TF-IDF approach to capture the semantic relevance of the documents to achieve accurate recommendations in the publication domain [16]. Latent semantic analysis (LSA) is a modeling technique based on singular value decomposition, which aims to reduce the dimensionality of document features and discover the potential topics in them. Bergamaschi et al. used LSA to model the topics of the documents, which significantly improves the effectiveness of the recom- mender system [17]. In addition, rule-based recommender system also favored by some researchers. Bouihi et al. proposed a recommendation algorithm based on learning history rule and learning performance rule for recommendation based on embedded coding [18]. Simple Bayesian classifier is a probabilistic based classification algorithm commonly used in text classification and recommender systems with the advantages of computational efficiency and simplicity of implementation. Roy et al. in their study pointed out the application of simple Bayes in recommender systems especially in news recommendation and email filtering [13]. Zhang et al. proposed a method for linearly classifying text content using SVMs and thus realizing accurate recommenda- tion approach, emphasizing its role in improving accuracy and generalization ability [19]. Gao et al. proposed to utilize a linear mixed model for group recommender sys- tems, based on this approach the accuracy and adaptability of the recommendation system in group recommendation are improved effectively [20]. Zuo et al. proposed a"}, {"title": "Multi-modal fusion recommendation system", "content": "The advent of multimedia platforms has prompted a surge of interest among researchers in recommender systems based on multi-modality. These systems are able to consider multi-modal data (such as input text and image) simultaneously, offering a more comprehensive approach to information processing.\nIn the level of multi-modal recommendation algorithms, Saga et al. proposed a way to extract features of product images based on convolutional neural networks (CNNs), combined with information from other modalities such as user reviews to achieve pre- diction of users' purchasing tendencies [48]. Choudhury et al. proposed a trust matrix approach combining user similarity with weighted trust propagation in multi-modality, where active users are evaluated by different trust filters and cold users generate optimized score predictions through their preferences [49]. Nikzad-Khasmakhi et al. proposed Berters to recommend relevant experts on specific topics based on three dif- ferent scores: authority, text similarity and reputation [50]. Chung et al. proposed a multi-modal collaborative recommendation algorithm based on the attention mecha- nism, which firstly utilizes a convolutional network to represent the image features at a high order, and the user habits and preferences learned by the LSTM to perform the feature fusion to realize the recommendation algorithm [51]. In order to better embed multi-modal information, Sun et al. proposed Multi-modal Knowledge Graph Atten- tion Network (MKGAT), which first propagates the multi-modal knowledge graph for knowledge propagation, and then aggregates the representations of different modalities for recommendation in order to improve the recommendation quality [52].\nThe core of multi-modal recommendation algorithm is how to fuse the information of different modes. By fusing the information of multiple modes together, complemen- tary information can be provided for the model to improve the accuracy of the model [53]. For this reason, some scholars proposed different fusion methods.\nLinear weight based fusion method is the simplest fusion approach which combines information from different modalities in a linear manner. Foresti and Snidaro and Yang et al. applied this approach to detecting and tracking pedestrians [54, 55]. Zhu et al. proposed a low-rank tensor multi-modal fusion approach with an attentional mechanism, which improves the efficiency and reduces the computational complexity at the same time [56]. Wei et al. proposed the use of cross-attention architecture to achieve fusion of image modalities and text modalities for downstream tasks [57]. Wang et al. proposed a multi-modal token fusion method for transformer-based multi-modal token fusion (TokenFusion), where TokenFusion dynamically detects uninformative tokens and replaces these tokens with projected and aggregated multi-modal features [58]. This design helps the model learn the correlation between different modalities."}, {"title": "Pre-trained models", "content": "In our research, we implement recommendation algorithm based on feature extraction through pre-trained models. Pre-trained models have become a cornerstone of current research in machine learning, especially in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Pre-training models learn general representations from large datasets, which can be fine-tuned for specific tasks to significantly reduce training time.\nIn the field of NLP, pre-trained models such as BERT (Bidirectional Encoder Representations from transformers) [59], Generative Pre-trained Transformer (GPT) [60] and Robustly optimized BERT approach (RoBERTa) [61]. They are pre-trained in a corpus, initially learn the knowledge of human society and embed it in their own parameters and can be fine-tuned in downstream tasks to achieve the target results.\nIn the field of CV, pre-trained models such as VGGNet [62], Residual networks (ROBERTa) [63] and Vision transformer (ViT) [64] based on transformer architecture. These models are pre-trained on large-scale image datasets to learn to extract image features and migrate these features to other tasks including feature extraction, target detection and semantic segmentation."}, {"title": "Methodology", "content": "In this section, we elaborate the composition and structure of our cross-modal movie recommendation algorithm based on pre-trained model feature extraction and Trans- former architecture feature fusion. The model uses BERT model to extract the information of text modality, the ViT model to extract the information of image modality, and the Transformer architecture for feature fusion of all modalities to pre- dict the users' ratings of movies. The performance indexes are significantly improved compared with traditional collaborative filtering algorithms."}, {"title": "Framework of the proposed model", "content": "We propose a deep learning based cross-modal neural network for movie recommen- dation. The framework of our model is illustrated in Figure 1. The input data of the model comprises multi-modal information from both users and movies. Users' infor- mation includes the user ID, age, ZIP code, occupation, and gender, while movies' information comprises the movie ID, genre, poster, title, and introduction. These data are divided into three modalities: textual information modality (e.g., title and intro- duction), image information modality (e.g., movie poster) and structured data (e.g., genre, occupation, etc.). We use different embedding methods to embed information in each of the three modalities. For image modality we use ViT model to extract the features, for text modality we use BERT model to extract the features, and for struc- tured data we design a multilayer neural network based embedding structure to embed the information. After feature extraction, we design a transformer-based feature fusion module, and input the fusion result into the classification function to get the user's rating estimation of a certain movie so as to realize personalized recommendation."}, {"title": "Textual feature extractor", "content": "For the information of text modality we use BERT model for feature extraction. We denote the input text sequence as $S_{input} = [W_1,W_2,..., W_{len}]$, where $W_i (1 \\leq x < len)$ is a word (later denoted as Token) in the input text sequence. The BERT model firstly adds \" [CLS]\" token (denoted as $W_{CLS}$) and \"[SEP]\" token (denoted as $W_{SEP}$) before and after the text sequence as the start and end of the sequence respectively. The BERT model encodes the sequence to obtain a hidden state matrix $H = [h_{CLS}, h_1, h_2,..., h_{len}, h_{SEP}]$ where $h_{CLS}$ and $h_{SEP}$ are the hidden value of $h_{CLS}$ and $h_{SEP}$. For the matrix H, its dimension is [768, len+2], where 768 is the dimension of the encoded output of the BERT model for each Token. We extract $h_{CLS}$ as the representation of the whole sequence semantics, its dimension is 768 in our research.\n1. Global context integration: the first token in BERT model interacts attentively with other tokens in the whole sequence during the process of multi-head self- attention computation, so that it can learn the global information of the whole sequence.\n2. Special pre-trained task: the pre-trained task of the BERT model consists of two stages, one is the Masked Language Model (MLM) and the other is the Next Sen- tence Prediction (NSP) [59]. In the NSP task, the BERT model needs to determine whether two sentences are contextual or not, and this process requires $h_{CLS}$ as the semantic representation for downstream judgment, which makes the BERT model"}, {"title": "Image feature extractor", "content": "For image modal features, we perform feature extraction using Vision Transformer (ViT), model that utilizes the Transformer architecture for image feature extraction. In contrast to traditional convolutional neural networks (e.g., VGGNet and ResNet), the input to ViT is not a complete matrix of pixels, but rather the image is sliced into fixed-size patches, each of which is flattened and mapped to a high-dimensional space by a linear link to form a vector representation. Assuming that the original length and width of an image are H and W respectively, the image will be cut into P \u00d7 P patches, each of which has a size of $\\frac{H}{P}x\\frac{W}{P}$.\nThe calculation process of linear transform is shown as:\n$x_i^0 = Linear (x_{patch_i}),$   (1)\nwhere $x_i$ is ith patch and $x_i^0$ is the corresponding linear representation after transformation.\nSimilar to the BERT model, ViT processes these patch vectors by adding a special classification token [CLS] in front of the sequence of these patch vectors, and the initial value of this token is a vector that can be learned during the training process. The whole input sequence can be represented as:\n$z_0 = [x_{CLS}; x_1^0; x_2^0; ...;x_n^0] + E_{pos},$   (2)\nwhere $E_{pos}$ is position code, which is used to retain the position information of the patch from the image."}, {"title": "Structural data feature extractor", "content": "To handle structured features such as user ID, movie ID, ZIP code, etc., we use a dense layer for embedding encoding. This approach projects these discrete features into a continuous high-dimensional vector space. For this purpose we adopt an up- sampling method to embed the low dimensional features into a 768 dimensional high latitude vector space step by step, firstly for each structured feature we use different low dimensional coding methods such as one-hot encoding for the occupation fea- ture, and then the low dimensional vectors are up-sampled by the dense layer until their dimension reaches 768. In the proposed method the ReLU function is applied as activation function."}, {"title": "Feature fusion based on Transformer", "content": "After feature extraction, we need to carry out feature fusion of multi-source features to make full use of their complementary information. To this end, a feature fusion method based on Transformer is proposed to capture the global dependencies between different modal features through self-attention mechanism to achieve fusion.\nIn the proposed method, we first reconstruct different feature vectors into a unified feature sequence, and input each feature vector into transformer as a Token. We add special tokens before and after the sequence: [CLS] and [SEP], the former to mark the semantic information of the whole sequence, and the latter to mark the end of the sequence.\nThe constructed feature sequence is denoted as $S = [V_{CLS}, V_1, V_2, ..., V_{10}, V_{SEP}]$. Where $V_i(1 \\leq i \\leq 10)$ is ith feature vector in the sequence, $V_{CLS}$ and $V_{SEP}$ are special tokens.\nSince a transformer is a model without sequential information, in order to enable the model to distinguish different feature tokens, we use fixed-position coding to add location information to feature vectors. Position coding is a vector with the same input feature dimension, that is, 768 dimensions. By introducing position coding model, the position of each feature vector in the sequence can be identified and sequence information can be captured. The calculation formula of location encoding reads as\n$P_{(pos,2i)} = sin(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}),$   (4)\n$P_{(pos,2i+1)} = Cos(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}),$   (5)\nwhere pos represents the position, i represents the dimension. In practice, we add the positional coding matrix P to a feature sequence V to get the positional encoded feature sequence, the process reads as:\n$V^{Pos} = V_i + P.$   (6)\nThe position encoded feature sequence is used as input for the transformer encoder. The transformer encoder consists of multiple self-attention layers and feedforward neural network layers that capture global associations between features through multi- head self-attention mechanisms."}, {"title": "", "content": "At the heart of the self-attention mechanism lies the calculation of the importance of each token to the other tokens in the input sequence, which is achieved through three matrices: Query (Q), Key (K), and Value (V). These matrices are obtained from the input features by linear transformations as:\n$Q = V_{pos}.W_Q,$   (7)\n$K = V_{pos}.W_K,$   (8)\n$V = V_{pos}.W_V,$   (9)\nwhere, $W_Q, W_K, W_V$ are trainable weight matrix. After we get Q, K, V we can conduct self attention calculation as:\n$Attention(Q, K, V) = SoftMax(\\frac{Q K^T}{\\sqrt{d_k}}) \\cdot V,$   (10)\nwhere $d_k$ is dimension of key. With this calculation, the model is able to assign a weight to each feature vector, thus capturing the relationship between the features.\nThe multi-head self-attention mechanism further parallelizes the above processes and enhances the expressive power of the model. The process reads as:\n$MultiHead (Q, K, V) = Concat(head_1, ..., head_h). W_o,$   (11)\nwhere, $head_i = Attention(Q, K, V)$, h is number of heads and $W_o$ is weight matrix of output layer. After several self-attention layers and feed forward neural network layers, we get the output matrix of Transformer encoder as\n$H = Transformer Encoder (V^P),$   (12)\nwhere, $H \\in R^{12\\times768}$ is output of transformer encoder. Base on the output, we extract\n[CLS] token as representation of whole sequence S.\n$h_{CLS} = H[0].$   (13)\nBased on this feature aggregation, we get a global feature vector $h_{CLS}$ containing multi-modal information. And then, the representation can be applied in classification layer. In our research we implement SoftMax function as output function.\n$y = SoftMax (W h_{CLS} + b).$   (14)"}, {"title": "Results", "content": "In this section, we describe in detail the experiments we conducted to verify the validity of the model and test its performance. In this experiment, two datasets, MovieLens 1M and MovieLens 100k, were used as experimental data respectively to verify the validity of the model in different data. And we reproduce some classic algorithms such as item-CF, user-CF, and SVD. At the same time, we refer to the literature of Mu et al. [3] in multi-modal movie recommendation algorithm, and reproduce their work to compare with our proposed model."}, {"title": "Hardware and Environment configure", "content": "Our experiment used Google Colab as the computing platform, configured with an Nvidia A100 GPU and extended RAM. In the experimental environment configuration, the Python validation is 3.10.0, CUDA version is 12.1, Pytorch version is 2.3.0+cu121, torchvision version is 0.18.0+cu121, and transformers version is 4.30.0. In addition, the \"Google-Bert/Bert-base-uncased\" is used as the BERT model and the \"google/ VIT-base-Patch16-224 \"is used as the ViT model.\nWe split the MovieLens data set into a training set, a test set, and a verifier in a ratio of 9:0.5:0.5. We use the RMSE as an indicator to evaluate the predictive power of the model. The fomula of RMSE reads as:\n$RMSE = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}(y_i-\\hat{y_i})^2},$   (15)\nwhere, N is number of samples and yi is true label, $\\hat{y_i}$ is predicted label."}, {"title": "Introduction of dataset", "content": "The MovieLens dataset was collected and published by the GroupLens research group at the University of Minnesota to support research on movie recommender systems. This dataset is one of the widely used and studied data sets in the field of recommen- dation system, which contains users' ratings of movies and related movie information. The data sets used in this study are MovieLens 1M and MovieLens 100k. The Movie- Lens 100k dataset contains 100,000 movie ratings from 943 users and 1,682 movie ratings. Each user has reviewed at least five movies with ratings ranging from 1 to 5. The dataset also includes the year of release and genre of the film's title. Similar to MovieLens 100k The MovieLens 1M dataset contains the same content but the dataset is much larger, with 1 million ratings from more than 6,000 users and 3,900 movies. The MovieLens dataset covers all types of movies from all ages, and the diverse rating behaviors of users can be used to mine the preferences and rating patterns of different user groups. This research uses movie posters and movie introductions in movie data from IMDB datasets.\nIn test A, we compare the proposed model with multiple baseline approaches includes three different categories: traditional method which used non-deep learning method and single modal information; deep learning method which used deep-learning technique but only single modal and multi-modal method which used deep-learning and multiple modals information.\nFor traditional method, we replicate User-CF, item-CF and SVD algorithm. For deep learning method we replicate Siet et al.'s [30] work and Aljunid et al.'s [31] work. For multi-modal method, we replicate the algorithm proposed by Mu et al. [3], which use convolution neural network to embed the poster information and use dense layer to embed other information such as users' age, ZIP code movies' genre and so on. For poster data, we implement crawling technique to obtain movie poster and movie's introduction information from IMDB open source dataset in the data preparation phase. To evaluate the effect of multi-modal information, we divide the test into two different situations: single modal and cross modal, where single modal's input exclude poster and cross modal's input include poster. We evaluated the performance of differ- ent algorithms based on RMSE metrics. To verify the effectiveness of the cross-modal algorithms, we set up a control group without inputting movie poster information."}, {"title": "Test B: explore the influence of size of dataset and learning rate", "content": "In order to explore the performance in datasets of different sizes and the impact of different learning rates on model performance, we used the MovieLens 100k and MovieLens 1M datasets to train and evaluate the model proposed in this study under different learning rates. The learning rate is an important hyperparameter in deep learning, which can be pre-adjusted. A larger learning rate means the model will con- verge faster, but more unstable, and a smaller learning rate means slower convergence [65]. We set up the model's optimizer with a learning rate of 0.001, 0.0005 and 0.0001.\nAt a learning rate of 0.0005, our proposed model performs 0.902 and 0.893 on the two datasets respectively, which are both slightly ahead of 0.0001 and 0.001. In addition, the model trained on a small dataset performs better on the training set, and"}, {"title": "Discussion and Conclusions", "content": "In this research, we propose and evaluate a multi-modal information-based recom- mendation system. The proposed model extracts text modal information with BERT model and extracts image modal information with ViT model. It apply transformer architecture to fuse extracted feature and predicts users' rating. The main goal of this research is to verify the effectiveness of the recommendation system based on cross-modal information.\nExperiments on the MovieLens 100k and 1M datasets show that our system out- performs traditional and deep learning baseline models. On MovieLens 100k, our system achieved an RMSE of 0.902, compared to 0.987 for a unimodal approach, indi- cating that multimodal data enhances prediction accuracy. Testing different learning rates (0.001, 0.0005, and 0.0001) revealed that 0.0005 yielded the best performance in both datasets. Additionally, as data volume increased, the system's generalization improved, achieving an RMSE of 0.893 on MovieLens 1M, demonstrating that more data enhances performance.\nDespite the positive results of this study, there are still some limitations. Firstly, the large number of parameters in the pre-trained model and the high computational overhead in the tuning process may affect the economics of time-based recommenda- tion in future applications, and secondly, the inclusion of multimodal data, especially image data, brings about noise enhancement. In future work, the focus should be on model simplification based on knowledge distillation and exploring techniques to deal with noise."}, {"title": "Declarations", "content": "\u2022 Funding: This work was funded by the Natural Science Foundation of China (12271047); Guangdong Provincial Key Laboratory of Interdisciplinary Research and Application for Data Science, BNU-HKBU United International Col- lege (2022B1212010006); UIC research grant (R0400001-22; UICR0400008-21; UICR0400036-21CTL; UICR04202405-21); Guangdong College Enhancement and Innovation Program (2021ZDZX1046).\n\u2022 Conflict of interest: We declare that we have no financial and personal relationships with other people or organizations that can inappropriately influence our work, and there is no professional or other personal interests of any nature or kind in any product, service, and/or company that could be construed as influencing the position presented in, or the review of, the manuscript entitled.\n\u2022 Data availability: The experiment data is available in MovieLens official website (https://movielens.org/) and IMDB official website (https://www.imdb.com/), the data used in the experiment also can be downloaded through https://github.com/ Xia12121/MovieLens-dataset-with-poster."}]}