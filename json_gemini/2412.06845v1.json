{"title": "Fully Open Source Moxin-LLM Technical Report", "authors": ["Pu Zhao", "Xuan Shen", "Zhenglun Kong", "Yixin Shen", "Sung-En Chang", "Timothy Rupprecht", "Lei Lu", "Enfu Nan", "Changdi Yang", "Yumei He", "Xingchen Xu", "Yu Huang", "Wei Wang", "Yue Chen", "Yong He", "Yanzhi Wang"], "abstract": "Recently, Large Language Models (LLMs) have undergone a significant transfor-\nmation, marked by a rapid rise in both their popularity and capabilities. Leading\nthis evolution are proprietary LLMs like GPT-4 and GPT-01, which have captured\nwidespread attention in the AI community due to their remarkable performance\nand versatility. Simultaneously, open-source LLMs, such as LLaMA and Mistral,\nhave made great contributions to the ever-increasing popularity of LLMs due to the\nease to customize and deploy the models across diverse applications. Although\nopen-source LLMs present unprecedented opportunities for innovation and\nresearch, the commercialization of LLMs has raised concerns about transparency,\nreproducibility, and safety. Many open-source LLMs fail to meet fundamental\ntransparency requirements by withholding essential components like training code\nand data, and some use restrictive licenses whilst claiming to be \u201copen-source,\u201d\nwhich may hinder further innovations on LLMs. To mitigate this issue, we\nintroduce Moxin 7B, a fully open-source LLM developed in accordance with the\nModel Openness Framework (MOF), a ranked classification system that evaluates\nAl models based on model completeness and openness, adhering to principles of\nopen science, open source, open data, and open access. Our model achieves the\nhighest MOF classification level of \u201copen science\" through the comprehensive\nrelease of pre-training code and configurations, training and fine-tuning datasets,\nand intermediate and final checkpoints. Experiments show that our model achieves\nsuperior performance in zero-shot evaluation compared with popular 7B models\nand performs competitively in few-shot evaluation.", "sections": [{"title": "Introduction", "content": "The field of natural language processing has witnessed the most exciting discoveries of the last ten\nyears with the emergence of large language models (LLMs). At the forefront of this evolution are\nLLMs such as GPT-4 [1], Claude [2], and Gemini [3], which have captured the attention of the AI\ncommunity due to their performance and versatility. Meanwhile, the recent emergence of openly\naccessible yet highly capable LLMs such as LLaMA [4], Falcon [5], and Mistral [6] allow researchers\nand practitioners to easily obtain, customize, and deploy LLMs in more various environments and for\nmore diverse use cases. The trends have made people eagerly asking about what's next and some\nsuggest \"a general intelligence\" is right around the corner."}, {"title": "Related Work", "content": "Despite the growing influence and accessibility of open-source LLMs, a notable challenge emerged:\nmany model producers restrict visibility and access to their training, fine-tuning, and evaluation\nprocesses, including crucial components such as their training code and data [7]. Some model\nproducers even use restrictive licenses whilst claiming to be \u201copen-source.\u201d This practice creates\nbarriers for the broader AI research community to study, replicate, and innovate upon advanced LLMs.\nIn parallel, it prevents businesses from fully leveraging open-source models for innovative industrial\napplications, as its commercialization has raised concerns about transparency, reproducibility, and\nsafety.\nTo unlock the full potential of LLMs and open innovation, we must return to democratize this research\nby putting the model into the hands of more researchers and making the datasets the models train on\nfully open-source. This requires moving beyond the simple sharing of model weights to embrace\ncomplete transparency in training, datasets, and implementation detail, which is crucial for fostering\na more inclusive and collaborative research environment that can sustain a healthy open-source\necosystem [8].\nTo achieve this goal, we introduce Moxin 7B, a fully open-source LLM developed by complying with\nthe Model Openness Framework (MOF) introduced by [9]. The MOF provides a systematic ranking\nclassification system to rate AI models based on their completeness and openness, incorporating the\nprinciples of open science, open source, open data, and open access. By promoting transparency and\nreproducibility, the MOF serves as a crucial tool to combat \u201copenwashing\u201d practices and establishes\ncompleteness and openness as primary criteria alongside the core tenets of responsible AI. Wide\nadoption of the MOF will cultivate a more open Al ecosystem, benefiting research, innovation, and\nadoption of state-of-the-art models.\nOur open-source LLM has released pre-training code and configurations, training and fine-tuning\ndata, and intermediate and final checkpoints, aiming to make continuous commitments to fully\nopen-source LLMs. Our model achieves the highest MOF classification level of \u201copen science.\"\nIt is noteworthy that this commitment to openness has not compromised performance: our base\nmodel achieves superior performance in zero-shot evaluation compared with popular 7B models\nand performs competitively in few-shot evaluation. Remarkably, our chat model can outperform 7B\nbaselines like Llama2-7B-chat. Our homepage is https://github.com/moxin-org/Moxin-LLM."}, {"title": "Models, Tokenizers, and Training", "content": "Models. State-of-the-art large language models (LLMs) typically comprise a substantial number\nof parameters, often approaching or exceeding 100 billion [4, 1, 3]. To facilitate broader acces-\nsibility, smaller models with fewer than 20 billion parameters, and even those around 7 billion\nparameters, have been developed [10, 11, 4, 6, 12, 13]. In addition, efficiency-enhancing techniques,\nsuch as implementing MAMBA-based architectures in Jamba, have been employed to optimize\nperformance [12, 13].\nTokenizers. Tokenizers are essential to convert raw data into a suitable format for model processing.\nMany contemporary models employ Byte-Pair Encoding (BPE)[14], with OpenAI's tiktoken\ntokenizer[15] being a notable implementation. However, for languages that handle tokens differently\nfrom Romance languages, alternatives such as SentencePiece [16] are utilized, as seen in XLNet [17].\nHugging Face offers an excellent summary of state-of-the-art tokenizers with practical examples [18].\nMoreover, tokenization extends beyond text modalities; many foundational models now include\nmultimodal capabilities, processing documents, audio, images, and even videos [19, 20, 21, 22].\nTraining. To enhance the performance of smaller models beyond their inherent limitations, various\ntraining strategies can be employed. A notable example is the application of Mixture of Experts\n(MoE) training, which has achieved significant success in models like Mixtral [23]."}, {"title": "Data curation methods", "content": "Researchers commonly collect large datasets for training language models (LMs)[24] by performing\nweb crawls. However, these datasets often contain undesirable content, necessitating data curation to\nimprove their quality. To enhance model performance [25, 26, 24, 27], several data curation techniques"}, {"title": "Open-source datasets", "content": "As the scale of LMs has increased in recent years [4, 41, 42, 1], the community has correspondingly\ncurated larger datasets to support their training. Early datasets include the C4 dataset, containing\n160 billion tokens, and The Pile [32], which comprises 300 billion tokens. More recently, even\nlarger datasets have been introduced: RefinedWeb [25] with 600 billion tokens, Dolma [43] with 3\ntrillion tokens, FineWeb [44] with 15 trillion tokens, and RedPajama-v2 [45] containing 30 trillion\ntokens. In addition to these general-purpose datasets, large domain-specific datasets have also been\ndeveloped. For instance, StackV2 [46], a code-focused dataset, includes 900 billion tokens, and\nFineWeb-Edu [44], a high-quality filtered educational text dataset, contains 1.3 trillion tokens."}, {"title": "Model Training", "content": ""}, {"title": "Model Architecture", "content": "We opt to extend the Mistral model architecture [6] due to its ability to achieve high performance\nwhile maintaining efficient inference speeds. The original Mistral 7B model demonstrates superior\nperformance compared to multiple 7B language models and even outperforms larger models on\nvarious evaluation benchmarks. Notably, it surpasses the LLaMA 34B model [47] in tasks such as\nmathematics and code generation.\nThe original Mistral model leverages grouped-query attention\n(GQA)[48] and sliding window attention (SWA)[49]. GQA reduces\nmemory requirements during decoding, allowing for larger batch\nsizes and higher throughput, and it significantly accelerates inference\nspeed-an essential factor in real-time applications. Meanwhile, SWA\neffectively handles long sequences without incurring substantial com-\nputational overhead. By incorporating these techniques, the model\nachieves significant improvements in performance and efficiency,\nwhich we have adopted in our extended model.\nBuilding upon the original Mistral model, which consists of 32 blocks,\nwe have extended the architecture to 36 blocks. Furthermore, we\nalso employ GQA to partition the query heads into multiple groups, each sharing a single key head\nand value head. This approach interpolates between multi-query attention (MQA) and multi-head\nattention (MHA) in large language models, striking a balance between the computational speed of\nMQA and the representational quality of MHA, thereby providing a favorable trade-off. Additionally,\nour model incorporates a rolling buffer cache with a fixed attention span, effectively limiting cache\nsize and preventing excessive memory usage when processing long sequences."}, {"title": "Training Data", "content": "Data are fundamental to the pre-training of LLMs. Preparing such training data requires careful con-\nsideration of multiple challenges, including handling sensitive information, ensuring comprehensive\nknowledge coverage, and achieving higher efficiency with improved data quality.\nIn this section, we detail the processes of preparing textual data from general domains and coding\ndata related to programming languages."}, {"title": "Text Data", "content": "We use a mix of data from SlimPajama [50] and DCLM-BASELINE [38] as our text training data.\nDuring the training of LLaMA, it was demonstrated that the performance of a 7B model continues to\nimprove even after being trained on more than 1T tokens [51]. Given the outstanding performance of\nLLaMA, its data collection methodology was rapidly replicated, leading to the release of RedPajama,\nan open-source dataset containing 1.2 trillion tokens [52]."}, {"title": "Coding Data", "content": "Programming is crucial for LLMs to support various downstream tasks, such as code completion\nfrom natural language descriptions, documentation generation for individual functions, and auto-\ncompletion of code snippets. Furthermore, as code is generally better structured and organized than\nnatural language, training on code data may improve the LLM reasoning capabilities [62]. Therefore,\nWe use part of the-stack-dedup dataset [63] during the pretraining.\nThe Stack comprises more than 6TB of permissively-licensed source code files across 358 program-\nming languages [63]. This carefully curated resource was designed to enhance the code generation\ncapabilities of LLMs. It facilitates the synthesis of programs by code-generating AI systems from\nboth natural language descriptions and existing code snippets.\nTo construct the Stack dataset, 220.92 million active GitHub repositories were collected from event\narchives published between 2015 and 2022 on GHArchive. Of these repositories, only 137.36 million\nwere publicly accessible on GitHub, resulting in 51.76 billion downloaded files. After initial filtering,\n5.28 billion unique files were identified, with an uncompressed size of 92.36 TB.\nTo ensure data quality, near-deduplication was implemented within the preprocessing pipeline in\naddition to exact deduplication. Specifically, MinHash with 256 permutations was computed for\nall documents, and Locality Sensitive Hashing was employed to identify clusters of duplicates.\nWithin these clusters, Jaccard similarities were calculated to detect near-duplicates using a similarity\nthreshold of 0.85. Approximately 40% of permissively licensed files were identified as (near-)\nduplicates and subsequently removed."}, {"title": "Capability Enhancement", "content": "LLMs are expected to demonstrate capabilities such as reasoning, mathematical problem-solving,\nand knowledge memorizing. However, a significant challenge lies in that, in the pre-training process,\nhigh-quality capability-related data is sparsely distributed in the entire corpus, and thereby it is\ndifficult for models to be proficient at these above-mentioned capabilities. Previous research, such\nas work on Qwen [10], GLM-130B [64], Nemotron-4 [65], has tried to incorporate instruction-\nbased or high-quality data during the pre-training stage to enhance these abilities. In our study,\nwe collect open-source data from HuggingFace, primarily utilizing the training datasets of various\nevaluation benchmarks such as MMLU [66] and HellaSwag [67]. These data are used experimentally\nto investigate the relationship between high-quality, capability-focused training data and model\nperformance."}, {"title": "Training Configuration", "content": "The total number of tokens used for pre-training our Moxin-7B model is over 2T, and the pre-training\nprocess consists of three phases. In the first phase, we use pre-training corpora with the context length\nof 2k. In the second phase, we use pre-training corpora with the context length of 4k. In the third\nphase, we utilize the capability-specific enhancement data. We provide the model performance with\nonly the first two phases and also with all three phases to validate the performance of the third phase.\nWe use Colossal-AI [68] as our training framework. Colossal-AI is a unified deep learning system\nthat provides the fullest set of acceleration techniques for the AI community. With its modular design,\nColossalAI allows for a free combination of these techniques to achieve the best training speedup.\nColossal-AI's optimized parallelism and heterogeneous training methods are employed to achieve\nsuperior system performance compared to baseline systems. These methods are provided through\nuser-friendly APIs, requiring minimal code modifications.\nDuring training, AdamW [69] with $\\beta_1 = 0.9$, $\\beta_2 = 0.95$, $\\epsilon = 1e-8$ and weight decay = 0.1 is used\nto optimize the model. We use the cosine learning rate decay and the learning rate decays to 10% of\nits maximum. Learning Rate is set to 2e-6."}, {"title": "Alignment", "content": "Following the pre-training phase, we fine-tune the model into a helpful and harmless AI assistant.\nIn our Alignment stage, we mainly use supervised fine-tuning (SFT), during which we fine-tune\nthe model to follow diverse human instructions by high-quality instruction data. We use the Tulu\nv2 dataset [70] for instruction tuning. The dataset consists of a mix of FLAN, Open Assistant 1,\nShareGPT, GPT4-Alpaca, LIMA, and so on."}, {"title": "Long-Context", "content": "To deal with the long-context problem, our model leverages grouped-query attention (GQA) [48],\nsliding window attention (SWA) [49], and Rolling Buffer Cache [6]. GQA reduces the memory\nrequirement during decoding, allowing for higher batch sizes hence higher throughput.\nBesides, SWA can handle longer sequences more effectively at a reduced computational cost, thereby\nalleviating a common limitation in LLMs. SWA exploits the stacked layers of a transformer to\nattend information beyond the window size W. At the last layer, with SWA, using a window size of\nW = 4096, we have a theoretical attention span of approximately 14K tokens or above.\nOur model adopts Rolling Buffer Cache which limits the cache size using a rolling buffer cache with\na fixed attention span. The cache has a fixed size of W, and the keys and values for the timestep i\nare stored in position i mod W of the cache. As a result, when the position i is larger than W, past\nvalues in the cache are overwritten, and the size of the cache stops increasing. On a sequence length\nof 32k tokens, this reduces the cache memory usage by 8\u00d7, without impacting the model quality.\nWith the above techniques, our model can support 32K context length with fast inference and low\nmemory cost."}, {"title": "Evaluation", "content": "We conducted comprehensive performance comparisons against leading language models of compa-\nrable scale, including Mistral-7B [6], LLaMA 2-7B [51], Gemma-7B [41], and Qwen v2-7B [11].\nThese models were selected based on their demonstrated excellence within the 7B or 8B category and\nrepresent diverse development approaches from various research organizations worldwide. To ensure\na robust evaluation, we re-run all benchmarks with the same evaluation pipeline for fair comparisons.\nSpecifically, we use lm-evaluation-harness [71] and opencompass [72] for evaluation."}, {"title": "Evaluation Tasks", "content": "We evaluate the model performance on various tasks below.\n\u2022 AI2 Reasoning Challenge (ARC) [76] - a set of genuine grade-school level, multiple-choice\nscience questions, assembled to encourage research in advanced question-answering. The\ndataset is partitioned into a Challenge Set (ARC-C) and an Easy Set (ARC-E), where the\nformer contains only questions answered incorrectly by both a retrieval-based algorithm and\na word co-occurrence algorithm.\n\u2022 HellaSwag [67] - a test of commonsense natural language inference, which is easy for\nhumans (95%) but challenging for SOTA models. It consists of 70,000 multiple-choice\nquestions. Each question presents a scenario followed by four possible outcomes, asking the\nmodel to select the most reasonable conclusion.\n\u2022 MMLU [77] - a test to measure a text model's multitask accuracy. The test covers 57 tasks,\nincluding elementary mathematics, US history, computer science, law, etc.\n\u2022 Winogrande [78] - an adversarial and difficult Winograd benchmark at scale, for common-\nsense reasoning. It contains 44,000 multiple-choice questions with two options each. It\nrequires the model to choose the appropriate entity word for the pronoun in the descriptive\ntext based on the scenario.\n\u2022 PIQA [79] - the task of physical commonsense reasoning and a corresponding benchmark\ndataset Physical Interaction: Question Answering (PIQA). Physical commonsense knowl-\nedge is a major challenge on the road to true AI-completeness, including robots that interact\nwith the world and understand natural language. PIQA focuses on everyday situations with\na preference for atypical solutions."}, {"title": "Evaluation Results", "content": "We name the initial model as Moxin-7B-original, which presents the foundation model before\nfine-tuning on the training data of the evaluation datasets. After subsequent partial fine-tuning of\nMoxin-7B-original on the training data of the evaluation datasets, we developed Moxin-7B-finetuned,\nenabling direct assessment of how targeted fine-tuning affects model performance."}, {"title": "Zero-Shot Evaluation", "content": "We report the result of base models for zero-shot evaluation in Table 2. The tasks are listed below.\nAfter training with the training data of evaluation tasks, our Moxin-7B-finetuned can achieve superior\nperformance compared with state-of-the-art (SOTA) baselines. This significant increase from the\nbase model demonstrates the effectiveness of our fine-tuning approach. The improved performance is\nparticularly notable on complex reasoning tasks like PIQA, where the score increased from 78.07%\nto 82.24%, matching or exceeding several leading models. Consequently, our models emerge as an\nexcellent candidate for real-world applications.\n\u2022 AI2 Reasoning Challenge (0-shot)"}, {"title": "Few-Shot Evaluation", "content": "Table 3 presents our zero-shot evaluation results across multiple benchmark tasks. The tasks and\ntheir few-show settings are listed below. Thanks to its rigorous and high-quality training corpus, our\nmodel demonstrates a remarkable competitive edge in tasks that involve language understanding and\nknowledge application. Our Moxin-7B-original achieves superior performance than LLaMA2-7B in\nthis scenario. After training with the training data of evaluation tasks, our Moxin-7B-finetuned can\nachieve competitive performance compared with SOTA baselines.\nConsequently, our models emerge as an excellent choice for a multitude of real-world applications\nwhere the reliance on robust language comprehension and extensive knowledge is paramount.\n\u2022 AI2 Reasoning Challenge (25-shot)\n\u2022 HellaSwag (10-shot)\n\u2022 MMLU (5-shot)\n\u2022 Winogrande (5-shot)"}, {"title": "Alignment Evaluation", "content": "We evaluate the alignment performance on MTBench [80].\nIt is a two-round conversation dataset with 80 questions. It\ncovers eight dimensions (reasoning, roleplay, math, coding,\nwriting, humanities, STEM, and information extraction) with\n10 questions for each dimension. The model needs to answer\nthe first question and then refine its previous response follow-\ning additional specific instructions. We use GPT-4 as a judge\nmodel to provide scores (between 1-10) for the quality of re-\nsponses. Our Moxin-7B-chat achieves superior performance\non MTbench compared with baselines, as shown in Table 4."}, {"title": "Conclusion", "content": "The field of Large Language Models has witnessed a significant shift toward open-source develop-\nment, fostering innovation within the AI community. However, a critical challenge emerges: many\npurportedly open-source models withhold essential components necessary for full understanding and\nreproducibility, creating barriers that limit both academic advancement and commercial adoption.\nThis does not not only hamper scientific progress but also prevent businesses from fully leveraging\nthese models for innovative applications, ultimately diminishing potential societal benefits and eco-\nnomic value creation. To address these limitations, we introduce Moxin 7B, a fully open-source\nlanguage model developed in accordance with the Model Openness Framework (MOF), providing\ncomprehensive access to pre-training code, configurations, training and fine-tuning datasets, and all\nintermediate checkpoints. Our evaluation results demonstrate that the Moxin 7B achieves superior\nzero-shot evaluation results compared to popular 7B models while maintaining competitive few-shot\ncapabilities. We wish to see more work that establishes new standard for reproducible research in\nlanguage model development, fostering a more inclusive and economically vibrant AI ecosystem."}]}