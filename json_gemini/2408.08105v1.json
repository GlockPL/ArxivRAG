{"title": "Multimodal Causal Reasoning Benchmark: Challenging Vision Large Language Models to Infer Causal Links Between Siamese Images", "authors": ["Zhiyuan Li", "Heng Wang", "Dongnan Liu", "Chaoyi Zhang", "Ao Ma", "Jieting Long", "Weidong Cai"], "abstract": "Large Language Models (LLMs) have showcased exceptional ability in causal reasoning from textual information. However, will these causalities remain straightforward for Vision Large Language Models (VLLMs) when only visual hints are provided? Motivated by this, we propose a novel Multimodal Causal Reasoning benchmark, namely MuCR, to challenge VLLMs to infer semantic cause-and-effect relationship when solely relying on visual cues such as action, appearance, clothing, and environment. Specifically, we introduce a prompt-driven image synthesis approach to create siamese images with embedded semantic causality and visual cues, which can effectively evaluate VLLMs' causal reasoning capabilities. Additionally, we develop tailored metrics from multiple perspectives, including image-level match, phrase-level understanding, and sentence-level explanation, to comprehensively assess VLLMs' comprehension abilities. Our extensive experiments reveal that the current state-of-the-art VLLMs are not as skilled at multimodal causal reasoning as we might have hoped. Furthermore, we perform a comprehensive analysis to understand these models' shortcomings from different views and suggest directions for future research. We hope MuCR can serve as a valuable resource and foundational benchmark in multimodal causal reasoning research.", "sections": [{"title": "Introduction", "content": "Causal reasoning is the process of identifying the relationship between a cause and its effect, which is regarded as a fundamental capability of artificial intelligence (Liu et al. 2024e). Recent advancements in Large Language Models (LLMs) have led to significant improvements in causal reasoning within the textual modality (Jin et al. 2023; Bagheri et al. 2024; Ashwani et al. 2024). These advanced LLMs can infer the cause-and-effect relationship through a chain-of-thought strategy (Bao et al. 2024) and provide reasonable explanations (Kiciman et al. 2023). They can even reason about complex causal relationships that typically demand expert-level topological structure analysis (Vashishtha et al. 2023). Despite these advancements, existing linguistic evaluation benchmarks (Singh et al. 2021; Du et al. 2022; Jin et al. 2023) are beginning to fall short in assessing the more advanced capabilities of the latest Vision Large Language Models (VLLMs) such as GPT-4o (OpenAI 2024b), Claude 3.5 (ClaudeAI 2024b), and Gemini 1.5 (DeepMind 2024). Specifically, a major limitation involves the inadequate evaluation of their advanced visual comprehension abilities, such as multi-image understanding (Zhao et al. 2024a). Although some studies (Zellers et al. 2019; Hessel et al. 2022; Ko et al. 2023) have explored causal reasoning through visual question answering (VQA), these efforts typically require VLLMs to extract visual information from a single image input. Whether VLLMs can identify visual cues across multiple images for causal inference remains an unexplored field.\nFollowing this, we propose an intriguing question: Can VLLMs achieve the same level of causal reasoning comprehension based solely on visual cues as what LLMs succeed within the textual modality? Figure 1 shows a comparison of a causal reasoning case depicted in two modalities. Most existing benchmarks do not address this question. Especially, as shown in Figure 2 (a), we identify three major drawbacks in previous benchmarks: (1) Absence of visual modality:"}, {"title": "Related Work", "content": "Causal Reasoning. The ability to perform causal reasoning is widely considered a core feature of artificial intelligence. With the development of LLMs, they have exhibited increasingly robust capabilities in causal reasoning tasks. Previous benchmarks, such as Com2sense (Singh et al. 2021) and CausalBank (Li et al. 2021), are becoming insufficient for evaluating linguistic abilities. To address this, Romanou et al. (2023) introduced the CRAB benchmark, which requires LLMs to capture explicit causal relationships between variables in real-world scenarios. Similarly, Jin et al. (2023) proposed Cladder to investigate whether LLMs can coherently reason about causality using a set of well-defined formal rules. However, these benchmarks focus solely on the text modality, leaving the crucial question of multimodal reasoning unaddressed. Zellers et al. (2019) introduced Visual Commonsense Reasoning (VCR) to challenge visual understanding by moving beyond object recognition toward cognition-level understanding. Additionally, Hessel et al. (2022) introduced Sherlock to challenge VLLMs in identifying visual clues scattered throughout a scene and making reasoning inferences combined with commonsense and life experience. Although these benchmarks have considered the visual modality, they still fail to evaluate the advanced visual capabilities of the latest VLLMs, particularly in multi-image understanding. In this paper, we make an initial attempt to explore multimodal causal reasoning tasks by challenging VLLMs to infer causal links between siamese images and verify their comprehensive understanding at image, phrase, and sentence levels.\nMulti-Image Understanding. The field of multi-image understanding has gained significant traction for many years, with various tasks proposed to evaluate the ability of VLLMs to comprehend the content and relationship between multiple images. Existing tasks can be broadly divided into two categories: multi-image similarity and multi-image difference. Multi-image similarity tasks involve identifying the same item or drawing analogies across different images, which include person re-identification, such as CUHK (Li et al. 2014) and PRW (Zheng et al. 2016), visual analogies like VASR (Bitton et al. 2023) and VISALOGY (Sadeghi, Zitnick, and Farhadi 2015), and object re-identification in-"}, {"title": "The MuCR Dataset", "content": "In this section, we detail the construction of the MuCR dataset. Figure 3 illustrates the systematic workflow of our multimodal cause-and-effect benchmark generation. Our process begins with generating core caption pairs, each consisting of one caption describing the cause and the other stating the effect. We then leverage the language capabilities of LLMs to entail these paired captions into contextually relevant descriptions, enhancing the consistency of sentences to facilitate the creation of cause-and-effect image pairs. Finally, we employ diffusion models to generate numerous siamese images based on these descriptions, annotating cue phrases and causality explanations for each pair."}, {"title": "Generating Core Caption Pairs", "content": "The MuCR benchmark is designed to evaluate the comprehension capabilities of VLLMs to perform multimodal causal inference. To achieve this, we first focus on generating core caption pairs that clearly delineate the cause-and-effect relationships. This process involves a refinement loop carried out by a team of six volunteers, who collaboratively brainstorm and refine the captions. Figure 4 illustrates the core caption pairs generation process. To avoid individual bias, we group two volunteers as a team: one processes and refines captions based on initial inspirations and iterative feedback, while the other reviews and provides feedback to improve the captions' quality. Additionally, we ask the volunteers to design four paired captions as a group, each sharing similar causalities but containing different visual cues. These groups are intended to explore the capability of distinguishing similar causalities occurring in different subjects across various scenarios. Furthermore, to maintain the diversity of our dataset, we include a portion of non-human cases. While many causality scenarios feature humans as subjects, we also incorporate cases involving animals, plants, comic characters, and their interactions. Finally, this process results in a total of 400 cause-and-effect caption pairs (see supplementary materials for more examples and further details)."}, {"title": "Producing Contextual Description Pairs", "content": "While core caption pairs effectively depict the cause-and-effect relationship, they often lack contextual details such as appearance, clothing colour, and environmental context that serve as crucial visual cues for high-quality cause-and-effect image synthesis. The absence of these specifics could introduce randomness in image creation, which may lead to inconsistencies and potentially undermine the perceived causality between the siamese images. Figure 6 highlights the drawbacks of missing context and the advantages of incorporating context. As shown in Figure 6 (a), although the two columns of images accurately represent the core caption, mismatched clothing disrupts the sense of causality, making it difficult to form coherent pairs. In contrast, the example in Figure 6 (b) demonstrates that incorporating contextual information and transforming core captions into contextual descriptions effectively resolves this issue and reduces randomness in image synthesis.\nTo address this issue, we leverage the linguistic capabilities of LLMs to enhance core caption pairs by enriching contextual details such as appearance, clothing, environment, and atmosphere. By maintaining these elements consistently across images, our approach not only effectively depicts causality at a semantic level but also improves visual coherence (see supplementary materials for a comparison between the identity-preserving technique and our method). Additionally, we introduce subtle changes, such as variations in facial expressions, within the contextual description pairs to reflect the passage of time. These detailed variations emphasize the impact of causality over time, making the connection between siamese images more natural and coherent."}, {"title": "Creating Images and Annotations", "content": "We employ diffusion models with contextual descriptions as prompts to generate cause-and-effect image pairs. Specifically, we utilize DALL-E (Ramesh et al. 2021), DeepAI (DeepAI 2024), and Stability-AI (Stability AI 2023) through their respective APIs for image synthesis, aiming to minimize model bias and enhance the diversity of the generated images. We also incorporate two styles (photographic and comic) when creating these images. Each diffusion model is required to generate 4 images per sentence, resulting in 24 images for every cause-and-effect pair (9600 images in total). Then, the volunteers are asked to manually select the two best representations among them that effectively depict causality at the semantic level and maintain a sense of consistency at the visual level. This results in 400 pairs of cause-and-effect images across various categories (humans, animals, plants, characters, and mixtures) and different styles (photograph and comic). Figure 5 illustrates some examples featuring various categories and styles from our MuCR benchmark as well as the distribution overview of categories and styles.\nIn addition to image synthesis, we require volunteers to create text annotations for each cause-and-effect image pair. As shown in Figure 3, it consists of phrase-level list (cue phrases) and sentence-level description (cause-and-effect explanations). The cue phrases comprise a list of four options, each being a word or phrase. Among these, only one phrase correctly explains or is highly relevant to the causality, while the other three are striking elements in the images but do not serve as proper cues. For example, the correct phrase \"bad weather\" in Figure 3 effectively links \"found dark clouds in th sky\" and \"caught in a heavy rainstorm\", while the other three do not. The sentence-level annotation is designed to verify whether the VLLMs truly understand multimodal causality and can provide reasonable explanations. To achieve this, we require volunteers to structure the explanation by first describing the content of the cause im-"}, {"title": "Evaluation Metrics", "content": "Image-level Metric\nThe image-level score consists of two parts: cause-to-effect (C2E) score and effect-to-cause (E2C) score. This scoring is designed to assess whether the VLLMs can identify visual cues and semantic causality between images and make the correct choice from four potential images. Given the cause image $I_c$ with the corresponding question $Q_1$, the model is required to select the optimal choice among four potential effect images ${I_E^{(i)}}_{i=1}^4$. The C2E score can be computed as follows:\n$I_{E}^* = \\underset{I_E^{(i)}}{\\operatorname{argmax}} S_l (Q_1, I_c, I_E^{(i)}), $                                               (1)\n$f_1(I_E^*) = \\begin{cases} 1, I_{E}^* = I_E^{\\text{gt}} \\\\ 0, \\text{otherwise} \\end{cases}$                                          (2)\nwhere $S_l$ measures the strength of the causal relationship for ${I_c, I_E^{(i)}}$. $I_E^*$ represents VLLMs' prediction from potential effect images. $I_E^{\\text{gt}}$ represents the optimal choice among four potential images. $f_1$ represents the function to calculate the C2E score. Similarly, the E2C score is calculated in the same manner but in the opposite direction.\nPhrase-level Metric\nThe phrase-level metric is called Cue score, which tests VLLMs' capability to distinguish the correct cue from a list of fraudulent phrases according to the siamese images. Given the cause-and-effect image pairs ${I_c, I_E}$ with the corresponding question $Q_P$, the model is required to select the optimal choice among four potential cue phrases ${T_P^{(i)}}_{i=1}^4$. The Cue score can be computed as follows:\n$T_{P}^* = \\underset{T_P^{(i)}}{\\operatorname{argmax}} S_P (Q_P, I_C, I_E, T_P^{(i)}),$                                   (3)\n$f_P(T_P^*) = \\begin{cases} 1, T_{P}^* = T_P^{\\text{gt}} \\\\ 0, \\text{otherwise} \\end{cases}$                                          (4)\nwhere $S_P$ measures the strength of the causal relationship for ${I_C, I_E} P_T^{(i)}$. $T_P^{(i)}$ represents VLLMs' prediction from potential cue phrases. $T_P^{\\text{gt}}$ represents the optimal choice among four potential images. $f_P$ represents the function to calculate the Cue score.\nSentence-level Metric\nOur final metric is designed to evaluate VLLMs' ability to explain causality. This sentence-level metric is called the explanation (Exp) score. Given the condition ${I_c, I_E}, T_P}$ with the corresponding question $Q_S$, the Exp score can be computed as follows:\n${E(I_C), E(I_E), E(T_P)} = S_S (Q_S, I_C, I_E, T_P),$                                                (5)\n$S_1 = f_{\\text{sim}} (E(I_C), A(I_C), A(I_C), A(I_C)),$                                                (6)\n$S_2 = f_{\\text{sim}} (E(I_E), A(I_E), A(I_E), A(I_E)),$                                                (7)\n$S_3 = f_{\\text{sim}} (E(T_P), A(T_P), A(T_P), A(T_P)),$                                                 (8)\n$f_S (E(I_C), E(I_E), E(T_P)) = \\sum_{i=1}^{3} \\lambda_i S_i,$                                             (9)\nwhere $S_S$ represents the explanation generation process for ${I_c, I_E}, T_P} \\to {E(I_C), E(I_E), E(T_P)}$. $E(I_C)$ represents the cause image explanation. $E(I_E)$ represents the effect image explanation. $E(T_P)$ represents causality explanation with the cue phrase $T_P$. ${A_1, A_2, A_3}$ represents the human annotation. $f_{\\text{sim}}$ measures the similarity between model prediction and real human annotation. $\\lambda$ is the coefficient number. $f_S$ represents the function to calculate the Exp score."}, {"title": "Experiments", "content": "Experimental Setup\nWe evaluate several popular open-source models on our benchmark, including BLIP2 (Li et al. 2023), Open-Flamingo (Awadalla et al. 2023), InstructBLIP (Dai et al. 2023), MiniGPT4 (Zhu et al. 2023), and LLaVA (Liu et al. 2024c). Additionally, we assess large-scale in-house models such as Claude (ClaudeAI 2024b), Gemini (DeepMind 2024), and GPT-4o (OpenAI 2024b). Human performance from crowd workers is also established as a comparison baseline on MuCR benchmark. Specifically, as most open-source models only accept a single image form, we use a composite image consisting of smaller pictures as our visual input for all the models, as shown in Figure 7 (a). For parameter settings, we set $\\lambda_1, \\lambda_2$, and $\\lambda_3$ to 0.25, 0.25, and 0.5 in Equation (9), respectively. We use GPT-4 (OpenAI 2023) as our $f_S$ function to compute the semantic similarity score, scoring from 0 to 10 (see supplementary materials for a discussion of reliance on GPT-4 as a scoring function).\nExperimental Results"}, {"title": "Analysis", "content": "Effectiveness of General LLM-Enhancing Strategies.\nWe investigate the effectiveness of popular chain-of-thought strategies, including CoT (Kojima et al. 2022) and CCoT (Mitra et al. 2024), as well as in-context learning methods such as ICL (Brown et al. 2020) and MMICL (Zhao et al. 2024b). Table 2 presents the performance impact of these different strategies on the MuCR benchmark. The results show significant improvements in the Cue score; however, these general strategies exhibit limited or even negative improvements in the image-level and sentence-level scores for both open-source and in-house models.\nCategory and Style Impact. We also study the influence of category and style on the model's performance, using GPT-4V as the baseline. Our analysis focuses on the correlation between correctness and these factors. Figure 9 illustrates the results for various categories and styles. In terms of category, plant and mixture tags emerge as performance bottlenecks. Regarding style, the comic style achieves significantly higher performance compared to the photographic style on the image-level score.\nVisual Input Form. In addition to LLM-enhancing strategies, category, and style, we explore an intriguing question: does the form of visual inputs impact the final output? Fig-"}, {"title": "Conclusion", "content": "In this study, we introduce a new multimodal causal reasoning benchmark called MuCR, designed to assess VLLMs' ability to reason causally based solely on visual cues. We also develop specialized metrics across three levels to thoroughly evaluate the causal reasoning capabilities of VLLMs from various perspectives. Our extensive experiments provide insights into the performance of the current SOTA VLLMs on our benchmark. Lastly, we conduct an in-depth analysis to identify the models' shortcomings and propose directions for future research."}, {"title": "Supplementary Materials", "content": "Generating Core Caption Pairs\nOur MuCR benchmark begins with the creation of core caption pairs, where one caption outlines the cause and the other describes the effect. These pairs establish the semantic causality that serves as the foundation for guiding the subsequent synthesis of siamese images. As detailed in the main paper, we employ a structured refinement loop that transforms initial brainstorming ideas into precise caption pairs, clearly depicting the cause-and-effect relationships. This process is guided by the principle: \u201cWhether the expression is concrete and can be effectively represented through visual means\". Here, we discuss the rationale behind this rule and explain why volunteers are instructed to create core caption pairs in accordance with it.\nProducing Contextual Description Pairs\nOur causal-and-effect image synthesis approach leverages the linguistic capabilities of large language models (LLMs) to incorporate a more comprehensive range of contextual information. While traditional identity-preserving image synthesis methods (e.g., LORA (Hu et al. 2021) and Face0 (Valevski et al. 2023) focus on image personalization by retaining facial details through a face encoder during the generation process (Wang et al. 2024). However, these methods often neglect broader contextual details such as clothing,\nenvironment, and atmosphere. In contrast, our approach not only preserves the human facial identity (appearance) but also consider additional contextual details (clothing, environment, and overall atmosphere), ensuring that the overall coherence of the image is maintained even when alterations are introduced through causal reasoning. Another significant gap comes from the guidance modality. Furthermore, as illustrated in Figure 11, existing identity-preserving techniques predominantly rely on guided images, which limits their effectiveness for semantically-driven image generation. While these methods can accurately preserve facial identity in newly synthesized images, they struggle to integrate cause-and-effect relationships across images.\nCreating Images and Annotations\nThe study employs diffusion models with contextual prompts to generate cause-and-effect image pairs using APIs like Stability-AI (Stability AI 2023). The models produce images in two styles\u2014photographic and comic\u2014with each model generating 24 images per cause-and-effect pair. Volunteers then select the two best images that represent causality and maintain visual consistency. This results in 400 cause-and-effect image pairs across various categories. In addition to image generation, volunteers create text annotations for each pair, including a list of cue phrases and a sentence-level description. The cue phrases include one correct option that accurately explains the causality, while the others are visually striking but irrelevant. Volunteers also provide structured explanations describing the cause image, the effect image, and the causal link between them. To reduce bias, three volunteers independently annotate each pair. We explain examples from each category in the following paragraphs to illustrate the generated results.\nIn the human category, take the last generated pair as an"}]}