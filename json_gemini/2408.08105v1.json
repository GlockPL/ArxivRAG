{"title": "Multimodal Causal Reasoning Benchmark: Challenging Vision Large Language Models to Infer Causal Links Between Siamese Images", "authors": ["Zhiyuan Li", "Heng Wang", "Dongnan Liu", "Chaoyi Zhang", "Ao Ma", "Jieting Long", "Weidong Cai"], "abstract": "Large Language Models (LLMs) have showcased exceptional ability in causal reasoning from textual information. However, will these causalities remain straightforward for Vision Large Language Models (VLLMs) when only visual hints are provided? Motivated by this, we propose a novel Multimodal Causal Reasoning benchmark, namely MuCR, to challenge VLLMs to infer semantic cause-and-effect relationship when solely relying on visual cues such as action, appearance, clothing, and environment. Specifically, we introduce a prompt-driven image synthesis approach to create siamese images with embedded semantic causality and visual cues, which can effectively evaluate VLLMs' causal reasoning capabilities. Additionally, we develop tailored metrics from multiple perspectives, including image-level match, phrase-level understanding, and sentence-level explanation, to comprehensively assess VLLMs' comprehension abilities. Our extensive experiments reveal that the current state-of-the-art VLLMs are not as skilled at multimodal causal reasoning as we might have hoped. Furthermore, we perform a comprehensive analysis to understand these models' shortcomings from different views and suggest directions for future research. We hope MuCR can serve as a valuable resource and foundational benchmark in multimodal causal reasoning research.", "sections": [{"title": "Introduction", "content": "Causal reasoning is the process of identifying the relationship between a cause and its effect, which is regarded as a fundamental capability of artificial intelligence (Liu et al. 2024e). Recent advancements in Large Language Models (LLMs) have led to significant improvements in causal reasoning within the textual modality (Jin et al. 2023; Bagheri et al. 2024; Ashwani et al. 2024). These advanced LLMs can infer the cause-and-effect relationship through a chain-of-thought strategy (Bao et al. 2024) and provide reasonable explanations (Kiciman et al. 2023). They can even reason about complex causal relationships that typically demand expert-level topological structure analysis (Vashishtha et al. 2023). Despite these advancements, existing linguistic evaluation benchmarks (Singh et al. 2021; Du et al. 2022; Jin et al. 2023) are beginning to fall short in assessing the more advanced capabilities of the latest Vision Large Language Models (VLLMs) such as GPT-40 (OpenAI 2024b), Claude 3.5 (ClaudeAI 2024b), and Gemini 1.5 (DeepMind 2024). Specifically, a major limitation involves the inadequate evaluation of their advanced visual comprehension abilities, such as multi-image understanding (Zhao et al. 2024a). Although some studies (Zellers et al. 2019; Hessel et al. 2022; Ko et al. 2023) have explored causal reasoning through visual question answering (VQA), these efforts typically require VLLMs to extract visual information from a single image input. Whether VLLMs can identify visual cues across multiple images for causal inference remains an unexplored field.\nFollowing this, we propose an intriguing question: Can VLLMs achieve the same level of causal reasoning comprehension based solely on visual cues as what LLMs succeed within the textual modality? Most existing benchmarks do not address this question. Especially, we identify three major drawbacks in previous benchmarks: (1) Absence of visual modality:"}, {"title": "Related Work", "content": "Causal Reasoning. The ability to perform causal reason-ing is widely considered a core feature of artificial intelli-gence. With the development of LLMs, they have exhibited increasingly robust capabilities in causal reasoning tasks. Previous benchmarks, such as Com2sense (Singh et al. 2021) and CausalBank (Li et al. 2021), are becoming in-sufficient for evaluating linguistic abilities. To address this, Romanou et al. (2023) introduced the CRAB benchmark, which requires LLMs to capture explicit causal relation-ships between variables in real-world scenarios. Similarly, Jin et al. (2023) proposed Cladder to investigate whether LLMs can coherently reason about causality using a set of well-defined formal rules. However, these benchmarks fo-cus solely on the text modality, leaving the crucial ques-tion of multimodal reasoning unaddressed. Zellers et al. (2019) introduced Visual Commonsense Reasoning (VCR) to challenge visual understanding by moving beyond object recognition toward cognition-level understanding. Addition-ally, Hessel et al. (2022) introduced Sherlock to challenge VLLMs in identifying visual clues scattered throughout a scene and making reasoning inferences combined with com-monsense and life experience. Although these benchmarks have considered the visual modality, they still fail to eval-uate the advanced visual capabilities of the latest VLLMs, particularly in multi-image understanding. In this paper, we make an initial attempt to explore multimodal causal rea-soning tasks by challenging VLLMs to infer causal links be-tween siamese images and verify their comprehensive un-derstanding at image, phrase, and sentence levels.\nMulti-Image Understanding. The field of multi-image understanding has gained significant traction for many years, with various tasks proposed to evaluate the ability of VLLMs to comprehend the content and relationship between multiple images. Existing tasks can be broadly divided into two categories: multi-image similarity and multi-image dif-ference. Multi-image similarity tasks involve identifying the same item or drawing analogies across different images, which include person re-identification, such as CUHK (Li et al. 2014) and PRW (Zheng et al. 2016), visual analogies like VASR (Bitton et al. 2023) and VISALOGY (Sadeghi, Zitnick, and Farhadi 2015), and object re-identification in-"}, {"title": "The MuCR Dataset", "content": "In this section, we detail the construction of the MuCR dataset. Our process begins with generating core caption pairs, each consisting of one caption describing the cause and the other stating the effect. We then leverage the language capabilities of LLMs to entail these paired captions into contextually relevant descriptions, enhancing the consistency of sentences to facilitate the creation of cause-and-effect image pairs. Finally, we employ diffusion models to generate numerous siamese images based on these descriptions, annotating cue phrases and causality explanations for each pair.\nGenerating Core Caption Pairs\nThe MuCR benchmark is designed to evaluate the comprehension capabilities of VLLMs to perform multimodal causal inference. To achieve this, we first focus on generating core caption pairs that clearly delineate the cause-and-effect relationships. This process involves a refinement loop carried out by a team of six volunteers, who collaboratively brainstorm and refine the captions. To avoid individual bias, we group two volunteers as a team: one processes and refines captions based on initial inspirations and iterative feedback, while the other reviews and provides feedback to improve the captions' quality. Additionally, we ask the vol-unteers to design four paired captions as a group, each shar-ing similar causalities but containing different visual cues. These groups are intended to explore the capability of dis-tinguishing similar causalities occurring in different subjects across various scenarios. Furthermore, to maintain the diver-sity of our dataset, we include a portion of non-human cases. While many causality scenarios feature humans as subjects, we also incorporate cases involving animals, plants, comic characters, and their interactions. Finally, this process results in a total of 400 cause-and-effect caption pairs (see supplementary materials for more examples and further details).\nProducing Contextual Description Pairs\nWhile core caption pairs effectively depict the cause-and-effect relationship, they often lack contextual details such as appearance, clothing colour, and environmental context that serve as crucial visual cues for high-quality cause-and-effect image synthesis. The absence of these specifics could introduce randomness in image creation, which may lead to inconsistencies and potentially undermine the perceived causality between the siamese images. Figure 6 highlights the drawbacks of missing context and the advantages of in-corporating context. As shown in Figure 6 (a), although the-"}, {"title": "Creating Images and Annotations", "content": "We employ diffusion models with contextual descriptions as prompts to generate cause-and-effect image pairs. Specifically, we utilize DALL-E (Ramesh et al. 2021), DeepAI (DeepAI 2024), and Stability-AI (Stability AI 2023) through their respective APIs for image synthesis, aiming to minimize model bias and enhance the diversity of the generated images. We also incorporate two styles (photo-graphic and comic) when creating these images. Each dif-fusion model is required to generate 4 images per sentence, resulting in 24 images for every cause-and-effect pair (9600 images in total). Then, the volunteers are asked to manually select the two best representations among them that effec-tively depict causality at the semantic level and maintain a sense of consistency at the visual level. This results in 400 pairs of cause-and-effect images across various categories (humans, animals, plants, characters, and mixtures) and dif-ferent styles (photograph and comic). In addition to image synthesis, we require volunteers to create text annotations for each cause-and-effect image pair. It consists of phrase-level list (cue phrases) and sentence-level description (cause-and-effect explanations). The cue phrases comprise a list of four op-tions, each being a word or phrase. Among these, only one phrase correctly explains or is highly relevant to the causal-ity, while the other three are striking elements in the images but do not serve as proper cues. For example, the correct phrase \u201cbad weather\u201d effectively links \u201cfound dark clouds in th sky\u201d and \u201ccaught in a heavy rainstorm\u201d, while the other three do not. The sentence-level annotation is designed to verify whether the VLLMs truly understand multimodal causality and can provide reasonable explanations. To achieve this, we require volunteers to structure the explanation by first describing the content of the cause im-"}, {"title": "Evaluation Metrics", "content": "Image-level Metric\nThe image-level score consists of two parts: cause-to-effect (C2E) score and effect-to-cause (E2C) score. This scoring is designed to assess whether the VLLMs can identify visual cues and semantic causality between images and make the correct choice from four potential images. Given the cause image $I_c$ with the corresponding question $Q_1$, the model is required to select the optimal choice among four potential effect images ${I_i}_{i=1}^4$. The C2E score can be computed as follows:\n$I^* = \\underset{I_i}{\\operatorname{argmax}} S_t(Q_1, I_c, I_i),$   (1)\n$f_1(I^*) = \\begin{cases}  1,  I^* = I,\\  0, otherwise \\end{cases}$   (2)\nwhere $S_t$ measures the strength of the causal relationship for $I_i$. $I^*$ represents VLLMs\u2019 prediction from poten-tial effect images. $I$ represents the optimal choice among four potential images. $f_1$ represents the function to calculate the C2E score. Similarly, the E2C score is calculated in the same manner but in the opposite direction.\nPhrase-level Metric\nThe phrase-level metric is called Cue score, which tests VLLMs\u2019 capability to distinguish the correct cue from a list of fraudulent phrases according to the siamese images. Given the cause-and-effect image pairs ${I_c, I_E}$ with the corresponding question $Q_P$, the model is required to select the optimal choice among four potential cue phrases ${T_i}_{i=1}^4$. The Cue score can be computed as follows:\n$T^* = \\underset{T_i}{\\operatorname{argmax}} S_P(Q_P, I_C, I_E, T_i),$   (3)\n$f_P(T^*) = \\begin{cases}  1, T^*=T,\\  0, otherwise \\end{cases}$   (4)\nwhere $S_P$ measures the strength of the causal relationship for ${I_C, I_E}, T_i$. $T^*$ represents VLLMs\u2019 prediction from potential cue phrases. $T$ represents the optimal choice among four potential images. $f_P$ represents the function to calculate the Cue score.\nSentence-level Metric\nOur final metric is designed to evaluate VLLMs\u2019 ability to explain causality. This sentence-level metric is called the explanation (Exp) score. Given the condition ${I_c, I_E}, T$ with the corresponding question $Q_S$, the Exp score can be computed as follows:\n${E(I_C), E(I_E), E(T)} = S_s(Q_s, I_c, I_E, T),$   (5)\n$S_1 = f_{sim}(E(I_C), A^{(I_C)}_1, A^{(I_C)}_2, A^{(I_C)}_3),$   (6)\n$S_2 = f_{sim}(E(I_E), A^{(I_E)}_1, A^{(I_E)}_2, A^{(I_E)}_3),$   (7)\n$S_3 = f_{sim}(E(T), A^{(T)}_1, A^{(T)}_2, A^{(T)}_3),$   (8)\n$f_s(E(I_C), E(I_E), E(T)) = \\sum_{i=1}^3 A_iS_i,$   (9)\nwhere $S_s$ represents the explanation generation process for ${I_c, I_E}, T$ $ {E(I_C), E(I_E), E(T)}$. $E(I_C)$ represents the cause image explanation. $E(I_E)$ represents the effect image explanation. $E(T)$ represents causality explanation with the cue phrase $T$. ${A_1, A_2, A_3}$ represents the human annotation. $f_{sim}$ measures the similarity between model prediction and real human annotation. $A$ is the coefficient number. $f_s$ represents the function to calculate the Exp score."}, {"title": "Experiments", "content": "Experimental Setup\nWe evaluate several popular open-source models on our benchmark, including BLIP2 (Li et al. 2023), Open-Flamingo (Awadalla et al. 2023), InstructBLIP (Dai et al. 2023), MiniGPT4 (Zhu et al. 2023), and LLaVA (Liu et al. 2024c). Additionally, we assess large-scale in-house mod-els such as Claude (ClaudeAI 2024b), Gemini (DeepMind 2024), and GPT-40 (OpenAI 2024b). Human performance from crowd workers is also established as a comparison baseline on MuCR benchmark. Specifically, as most open-source models only accept a single image form, we use a composite image consisting of smaller pictures as our visual input for all the models. For parameter settings, we set $A_1$, $A_2$, and $A_3$ to 0.25, 0.25, and 0.5 in Equation (9), respectively. We use GPT-4 (OpenAI 2023) as our $f_s$ function to compute the semantic similarity score, scoring from 0 to 10.\nExperimental Results\nTable 1 presents the main results of popular VLLMs and human on MuCR benchmark, leading to the follow-ing findings: (1) All open-source models underperform on MuCR: Among the open-source models, LLaVA 1.6 13B demonstrates the highest perfor-mance with a 23.75% C2E score, a 21.50% E2C score, and a 35.75% Cue score. Nonetheless, these results are roughly equivalent to the random baseline, highlighting consider-able potential for improvement. (2) All in-house models fall short of human performance on MuCR: While GPT-4V and GPT-40 outperform most other in-house models, they still do not reach the level of human performance. For instance, GPT-40 achieves a Cue score of 57.25% compared to the human performance"}, {"title": "Analysis", "content": "Effectiveness of General LLM-Enhancing Strategies.We investigate the effectiveness of popular chain-of-thought strategies, including CoT and CCOT, as well as in-context learning methods such as ICL and MMICL. Table 2 presents the performance impact of these different strategies on the MuCR benchmark. The re-sults show significant improvements in the Cue score; how-ever, these general strategies exhibit limited or even negative improvements in the image-level and sentence-level scores for both open-source and in-house models.\nCategory and Style Impact. We also study the influence of category and style on the model's performance, using GPT-4V as the baseline. Our analysis focuses on the corre-lation between correctness and these factors. Figure 9 illus-trates the results for various categories and styles. In terms of category, plant and mixture tags emerge as performance bottlenecks. Regarding style, the comic style achieves sig-nificantly higher performance compared to the photographic style on the image-level score.\nVisual Input Form. In addition to LLM-enhancing strate-gies, category, and style, we explore an intriguing question: does the form of visual inputs impact the final output? Figure 7 illustrates the three types of visual input forms we examined. Table 3 shows that all in-house models benefit significantly from using split image input forms, resulting in marked performance improvements. This improvement may be due to the fact that in-house models are trained with multi-image input forms, allowing them to process each im-age individually rather than as a composite, thereby enhanc-ing their visual perception of each image.\nCase Study. In this section, we conduct case studies to identify the shortcomings of VLLMs, as illustrated in Figure 8. For open-source models, the primary limitation lies in their visual perception capabilities. In case (a), the model can only recognize the presence of peo-ple in the garden but fails to distinguish their appearance."}, {"title": "Conclusion", "content": "In this study, we introduce a new multimodal causal reasoning benchmark called MuCR, designed to assess VLLMs' ability to reason causally based solely on visual cues. We also develop specialized metrics across three levels to thor-oughly evaluate the causal reasoning capabilities of VLLMs from various perspectives. Our extensive experiments pro-vide insights into the performance of the current SOTA VLLMs on our benchmark. Lastly, we conduct an in-depth analysis to identify the models' shortcomings and propose directions for future research."}, {"title": "Supplementary Materials", "content": "Generating Core Caption Pairs\nOur MuCR benchmark begins with the creation of core cap-tion pairs, where one caption outlines the cause and the other describes the effect. These pairs establish the semantic causality that serves as the foundation for guiding the subse-quent synthesis of siamese images. As detailed in the main paper, we employ a structured refinement loop that trans-forms initial brainstorming ideas into precise caption pairs, clearly depicting the cause-and-effect relationships. This process is guided by the principle: \u201cWhether the expression is concrete and can be effectively represented through visual means\". Here, we discuss the rationale behind this rule and explain why volunteers are instructed to create core caption pairs in accordance with it.\nFigure 10 compares the initial spark and core caption pairs in image synthesis. The comparison reveals that the ini-tial spark often contains semantically ambiguous elements, leading to visual gaps in the generated images. For instance, the phrase \u201cthe baker left the cake in the oven\u201d might result in an image depicting only a cake in the oven, as the diffu-sion model may struggle to interpret or visually represent the action \u201cleft\u201d. Another issue is subject conflict. For example, the phrase \u201cthe food became inedible\u201d might simply produce an image of unappealing food on a plate. However, within a cause-and-effect scenario, a human would easily infer that \u201cfood\u201d refers specifically to the \u201ccake.\u201d In contrast, our core caption pairs resolve these ambiguities by translating them into more concrete actions, such as replacing \u201ccareless\u201d with \u201cplayed his phone.\u201d This refinement significantly improves the quality of the generated images and the semantic causal-ity between the pairs.\nTable 4 shows generated paired-caption examples (i.e., four captions sharing similar causalities but involving differ-ent visual cues are paired as a group) for various scenarios (i.e., cases involving humans, animals, plants, comic char-acters, and mixtures). Abstract expressions are concretized during the paired-caption generation process according to the causality. For instance, the scenario \u201cdriving at exces-sive speed\u201d is rephrased in terms of its potential outcomes, such as \u201cgetting a speeding ticket\u201d or \u201cbeing pulled over by a police officer\u201d. Similarly, the concept of \u201cblooming\u201d is il-lustrated through its possible consequence, \u201cattracting bees to gather nectar\u201d. This process leverages causal reasoning to ground abstract ideas in real-world outcomes, thereby en-hancing the intelligibility and reproducibility of the gener-ated captions.\nProducing Contextual Description Pairs\nOur causal-and-effect image synthesis approach leverages the linguistic capabilities of large language models (LLMs) to incorporate a more comprehensive range of contex-tual information. While traditional identity-preserving im-age synthesis methods (e.g., LORA and Face0 focus on image personalization by retaining facial details through a face encoder during the generation process . However, these meth-ods often neglect broader contextual details such as clothing,\nenvironment, and atmosphere. In contrast, our approach not only preserves the human facial identity (appearance) but also consider additional contextual details (clothing, envi-ronment, and overall atmosphere), ensuring that the overall coherence of the image is maintained even when alterations are introduced through causal reasoning. Another signifi-cant gap comes from the guidance modality. Furthermore, as illustrated in Figure 11, existing identity-preserving tech-niques predominantly rely on guided images, which limits their effectiveness for semantically-driven image generation. While these methods can accurately preserve facial iden-tity in newly synthesized images, they struggle to integrate cause-and-effect relationships across images.\nCreating Images and Annotations\nThe study employs diffusion models with contextual prompts to generate cause-and-effect image pairs using APIs like Stability-AI. The models produce images in two styles\u2014photographic and comic\u2014with each model generating 24 images per cause-and-effect pair. Vol-unteers then select the two best images that represent causal-ity and maintain visual consistency. This results in 400 cause-and-effect image pairs across various categories. In addition to image generation, volunteers create text anno-tations for each pair, including a list of cue phrases and a sentence-level description. The cue phrases include one cor-rect option that accurately explains the causality, while the others are visually striking but irrelevant. Volunteers also provide structured explanations describing the cause image, the effect image, and the causal link between them. To re-duce bias, three volunteers independently annotate each pair. We explain examples from each category in the following paragraphs to illustrate the generated results.\nIn the human category, take the last generated pair as an"}, {"title": "The Case of Correct Prediction", "content": "Considering the cause in the first image and effect in the second, which of the following four cue phrases [venom, snake, plant, death] best describe causality link between two images?\nGPT-40 Direct\nThe cause-and-effect link between the two images would best be described by the cue phrase \"venom\". The first image shows a snake biting a mouse, which likely injects venom. The second image shows the snake beside a lifeless mouse, suggesting the mouse's death was caused by the venom from the snake's bite.\nGPT-40 ICL\nBased on the images, the snake appears to be about to bite a mouse in the first image, and in the second image, the mouse is lying down lifeless with the snake next to it. The best cue phrase that describes the cause-and-effect link between these two images is \"venom\". The snake's venomous bite leads to the mouse's death.\nGPT-40 COT\nTo determine the best cue phrase from [\"venom\", \"snake\", \"plant\", \"death\"] to describe the cause-and-effect link shown in the image, let's analyze both images in detail: First Image: This image shows a snake biting a mouse, which probably injects venom. Second Image: This image shows a mouse lying on the ground with a snake nearby. Given this analysis, the cue phrases that describe the causality link are \"venom\" and \"death\". However, if we had to choose only one: Venom directly relates to the action of the snake in the first image, leading to the effect shown in the second image. \"Death\" only describe the result and is a more general term. Therefore, \"venom\" is the most precise cue phrase that best captures the causality link between these two images."}, {"title": "The Case of Wrong Prediction", "content": "Considering the cause in the first image and effect in Question: the second, which of the following four cue phrases [boss, work, talk, work state] best describe causality link between two images?\nGPT-40 Direct\nThe cue phrase that best describes the cause-and- effect link between the two images is \"work\". In the first image, a woman is depicted looking overwhelmed with a large amount of paper wprk on her desk. This situation indicates a busy or stressful work environment. The second image shows a serious conversation between two individuals in a workplace setting, likely related to the large workload or the issues presented in the first image. Hence, \"work\" encapsulates the cause (workload) and the effect (serious discussion at work).\nGPT-40 ICL\nThe best cue phrase to describe the cause-and- effect link between the two images is \"boss\". In the first image, awoman appears to be very tired or possibly exhausted at work. In the second, another person (which could be a boos) is talking to her in a way that seems possibly demaning. Putting these together, it seems the demaning nature of the \"boss\" may be causing exhaustion for the individual in the first picture.\nGPT-40 COT\nlet's analyze the images step by step to understand the cause-and-effect and find the most fitting cue phrase among the options provided: ['boss', 'work', 'talk', 'work state']. Cause Image Analysis: The individual appears to be overwhelmed and possibly stressed, sitting amidst a significant amount of paperwork and documentation. The expression suggests exhaustion or frustration due to the large amount of work. Effect Image Analysis: The individual is seen in a conversation with another person in what appears to be a workplace, judging by the files and documents around. The mood still shows seriousness, but there is now communication taking place, possibly seeking help, explaining the work progress, or resolving an issue. Linking Cuase and Effect: Considering the cause (overwhelmed by work) leads to the effect (seeking interaction/discussion to resolve or explain the work situation). Hence, the core of the situation seems to stem from the individual's workload and managing it through communication. Review Cue Phrase: \"Boss\": This does not directly link to the cause of being overwhelmed or the interaction. \"Work\": This describe the subject matter, but it doesn't link to the corrective or responsive action seen in the second image. \"Talk\": This directly relates to the action taken in the effect, responding to the cause. \"Work state\": This is descriptive of the cause but doesn't connect it effectively shown in the effect. Given these observations, the best cue phrase that descibe the cause-and-effect relationship is \"talk\"."}, {"title": "Experimental Results", "content": "closely align with human evaluations, though they tend to be slightly higher. This indicates that GPT-4 is a reliable proxy for human judgment in evaluating semantic scores, provid-ing consistent results with only a minor deviation.\nTable 5 presents the comprehensive results of popular VLLMs and human performance on the MuCR benchmark, encompassing both zero-shot and supervised settings. The key findings are as follows: (1) All open-source models underperform on MuCR: As discussed in the main paper, the primary limitation of open-source models lies in their inadequate visual perception capabilities. Notably, LLaVA 1.6 demonstrates a significant perfor-"}]}