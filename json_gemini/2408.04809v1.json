{"title": "On the Geometry of Deep Learning", "authors": ["Randall Balestriero", "Ahmed Imtiaz Humayun", "Richard G. Baraniuk"], "abstract": "In this paper, we overview one promising avenue\nof progress at the mathematical foundation of deep\nlearning: the connection between deep networks and\nfunction approximation by affine splines (continuous\npiecewise linear functions in multiple dimensions).\nIn particular, we will overview work over the past\ndecade on understanding certain geometrical prop-\nerties of a deep network's affine spline mapping, in\nparticular how it tessellates its input space. As we\nwill see, the affine spline connection and geometrical\nviewpoint provide a powerful portal through which\nto view, analyze, and improve the inner workings of\na deep network.", "sections": [{"title": "Introduction", "content": "Machine learning has significantly advanced our abil-\nity to address a wide range of difficult computational\nproblems and is the engine driving progress in mod-\nern artificial intelligence (AI). Today's machine learn-\ning landscape is dominated by deep (neural) net-\nworks, which are compositions of a large number of\nsimple parameterized linear and nonlinear operators.\nAn all-too-familiar story of the past decade is that of\nplugging a deep network into an engineering or scien-\ntific application as a black box, learning its parame-\nter values using copious training data, and then sig-\nnificantly improving performance over classical task-\nspecific approaches based on erudite practitioner ex-\npertise or mathematical elegance.\nDespite this exciting empirical progress, however,\nthe precise mechanisms by which deep learning works\nso well remain relatively poorly understood, adding\nan air of mystery to the entire field. Ongoing at-\ntempts to build a rigorous mathematical framework\nhave been stymied by the fact that, while deep net-\nworks are locally simple, they are globally compli-\ncated. Hence, they have primarily been studied\nas \"black boxes\" and mainly empirically. This ap-\nproach greatly complicates analysis to understand\nthe both the success and failure modes of deep net-\nworks. This approach also greatly complicates deep\nlearning system design, which today proceeds al-\nchemistically rather than from rigorous design prin-\nciples. And this approach greatly complicates ad-\ndressing higher level issues like trustworthiness (can\nwe trust a black box?), sustainability (ever-growing\ncomputations lead to a growing environmental foot-\nprint), and social responsibility (fairness, bias, and\nbeyond).\nIn this paper, we overview one promising avenue\nof progress at the mathematical foundation of deep\nlearning: the connection between deep networks and\nfunction approximation by affine splines (continuous\npiecewise linear functions in multiple dimensions).\nIn particular, we will overview work over the past\ndecade on understanding certain geometrical prop-\nerties of a deep network's affine spline mapping, in\nparticular how it tessellates its input space. As we\nwill see, the affine spline connection and geometrical\nviewpoint provide a powerful portal through which\nto view, analyze, and improve the inner workings of\na deep network.\nThere are a host of interesting open mathemati-\ncal problems in machine learning in general and deep\nlearning in particular that are surprisingly accessible\nonce one gets past the jargon. Indeed, as we will see,\nthe core ideas can be understood by anyone know-\ning some linear algebra and calculus. Hence, we will\npose numerous open questions as they arise in our\nexposition in the hopes that they entice more math-\nematicians to join the deep learning community.\nThe state-of-the-art in deep learning is a rapidly\nmoving target, and so we focus on the bedrock of\nmodern deep networks, so-called feedforward neural\nnetworks employing piecewise linear activation func-\ntions. While our analysis does not fully cover some\nvery recent methods, most notably transformer net-\nworks, the networks we study are employed therein as\nkey building blocks. Moreover, since we focus on the\naffine spline viewpoint, we will not have the oppor-\ntunity to discuss other interesting geometric work in"}, {"title": "Deep Learning", "content": "Machine learning in 200 words or less. In su-\npervised machine learning, we are given a collection\nof n training data pairs {(x_i,y_i)}_{i=1}^n; x_i is termed\nthe data and y_i the label. Without loss of generality,\nwe will take x \\in \\mathbb{R}^D, Y_i \\in \\mathbb{R}^C to be column vectors,\nbut in practice they are often tensors.\nWe seek a predictor or model f with two basic prop-\nerties. First, the predictor should fit the training\ndata: f(x_i) \\approx y_i. When the predictor fits (near) per-\nfectly, we say that it has interpolated the data. Sec-\nond, the predictor should generalize to unseen data:\nf(x') \\approx y', where (x', y') is test data that does not\nappear in the training set. When we fit the train-\ning data but do not generalize, we say that we have\noverfit.\nOne solves the prediction problem by first design-\ning a parameterized model f_\\Theta with parameters \\Theta and\nthen learning or training by optimizing \\Theta to make\nf_\\Theta(x_i) as close as possible to y_i on average in terms\nof some distance or loss function \\mathcal{L}, which is often\ncalled the training error.\nDeep networks. A deep network is a predictor or\nmodel constructed from the composition of L inter-\nmediate mappings called layers\n$$f_\\Theta(x) = (f^{(L)} \\circ ... \\circ f^{(1)}) (x).$$ \nHere \\Theta is the collection of parameters from each layer,\n\\Theta^{(l)}, l = 1, ..., L. We will omit the parameters \\Theta or\n\\Theta^{(l)} from our notation except where they are critical,\nsince they are ever-present in the discussion below.\nThe l-th deep network layer f^{(l)} takes as input the\nvector z^{(l-1)} and outputs the vector z^{(l)} by combin-\ning two simple operations\n$$z^{(l)} = f^{(l)} (z^{(l-1)}) = \\sigma \\left(W^{(l)}z^{(l-1)}+b^{(l)}\\right),$$ \nwhere z^{(0)} = x and z^{(L)} = \\hat{y} = f(x). First the\nlayer applies an affine transformation to its input.\nSecond, in a standard abuse of notation, it applies\na scalar nonlinear transformation \\sigma called the ac-\ntivation function to each entry in the result.\nThe entries of z^{(l)} are called the layer-l neurons or\nunits, and the width of the layer is the dimensional-\nity of z^{(l)}. When layers of the form (2) are used in\n(1), deep learners refer to the network as a multilayer\nperceptron (MLP).\nThe parameters \\Theta^{(l)} of the layer are the elements\nof the weight matrix W^{(l)} and the bias vector b^{(l)}.\nSpecial network structures have been developed to\nreduce the generally quadratic cost of multiplying by\nthe W^{(l)}. One notable class of networks constraints\nW^{(l)} to be a circulant matrix, so that W^{(l)} z^{(l)} cor-\nresponds to a convolution, giving rise to the term\nConvNet for such models. Even with this simplifica-\ntion, it is common these days to work with networks\nwith billions of parameters.\nThe most widespread activation function in mod-\nern deep networks is the rectified linear unit (ReLU)\n$$\\sigma(u) = max{u, 0} =: ReLU(u).$$ \nThroughout this paper, we focus on networks that\nuse this activation, although the results hold for any\ncontinuous piecewise linear nonlinearity (e.g., abso-\nlute value \\sigma(u) = |u|). Special activations are often\nemployed at the last layer f^{(L)}, from the linear acti-\nvation \\sigma(u) = u to the softmax that converts a vector\nto a probability histogram. These activations do not\naffect our analysis below. It is worth point out, but\nbeyond the scope of this paper, that it is possible\nto generalize the results we review below to a much\nlarger class of smooth activation functions (e.g., sig-\nmoid gated linear units, Swish activation) by adopt-\ning a probabilistic viewpoint.\nThe term \"network\" is used in deep learning be-\ncause compositions of the form (1) are often depicted\nas such; see Figure 1.\nLearning. To learn to fit the training data with a\ndeep network, we tune the parameters W^{(l)}, b^{(l)}, l =\n1,..., L such that, on average, when training datum\nx_i is input to the network, the output \\hat{y} = f(x_i)\nis close to y_i as measured by some loss function \\mathcal{L}.\nTwo loss functions are ubiquitous in deep learning;"}, {"title": "Affine Splines", "content": "As we will now explain, deep networks are tractable\nmultidimensional extensions of the familiar one-\ndimensional continuous piecewise linear functions de-\npicted on the left in Figure 2. When such continuous\npiecewise functions are fit to training data, we refer\nto them as affine splines for short.\nDeep networks implement one particular extension\nof the affine spline concept to a multidimensional do-\nmain and range. As we will see in the next section, a\ndeep network generalizes the intervals of the indepen-\ndent variable over which a piecewise affine function is\nsimply affine (recall Figure 2) to an irregular tessel-\nlation (tiling) of the network's D-dimensional input\nspace into convex polytopes. Let \\Omega denote the tessel-\nlation and w \\in \\Omega an individual tile. (The jargon for\nthe polytope tiles is \u201clinear region\u201d.\nGeneralizing the straight lines defining the func-\ntion on each interval in Figure 2, a deep network\ncreates an affine transformation on each tile such\nthat the overall collection is continuous. Figure 3\ndepicts an example for a toy deep network with a\ntwo-dimensional input space; here the tiles are poly-\ngons. This all can be written as\n$$f(x) = \\sum_{w \\in \\Omega} (A_w x + c_w)1\\{x\\in w\\},$$ \nwhere the matrix A_w and vector c_w define the affine\ntransformation from tile w to the output.\nBoth the tessellation \\Omega and A_w, c_w from the affine\ntransformations are functions of the deep network\nweights W^{(l)} and biases b^{(l)}. Geometrically, envi-\nsion Figure 3 with a cloud of n training data points\n(x_i, y_i); learning uses optimization to adjust the\nweights and biases to create a tessellation and affine\ntransformations such that the affine spline predic-\ntions \\hat{y} come as close as possible to the true labels\ny_i as measured by the squared error loss (4), for ex-\nample."}, {"title": "Deep Network Tessellation", "content": "As promised, let us now see how a deep network cre-\nates its input space tessellation . Without loss\nof generality, we start with the first layer f^{(1)} whose\ninput is x and output is z^{(1)}. The k-th entry in z^{(1)}\n(the value of the k-th neuron) is calculated simply as\n$$z_k^{(1)} = \\sigma \\left(w_k^{(1)} \\cdot x + b_k^{(1)}\\right),$$"}, {"title": "Visualizing the Tessellation", "content": "The toy, low-dimensional examples in Figures 3 and 4\nare useful for building intuition, but how can we gain\ninsight into the tessellation of a deep network with\nthousands or more of input and output dimensions?\nOne way to proceed is to compute summary statistics"}, {"title": "The Self-Similar Geometry of the Tessellation", "content": "It has been known since the late 1980s that even a\ntwo-layer neural network is a universal approxima-\ntor, meaning that, as the number of neurons grows,\none can approximate an arbitrary continuous func-\ntion over a Borel measurable set to arbitrary pre-\ncision . But, unfortunately, while two-layer\nnetworks are easily capable of interpolating a set of\ntraining data, in practice they do a poor job general-\nizing to data outside of the training set. In contrast,\nd"}]}