{"title": "On the Geometry of Deep Learning", "authors": ["Randall Balestriero", "Ahmed Imtiaz Humayun", "Richard G. Baraniuk"], "abstract": "Machine learning has significantly advanced our abil- ity to address a wide range of difficult computational problems and is the engine driving progress in mod- ern artificial intelligence (AI). Today's machine learn- ing landscape is dominated by deep (neural) net- works, which are compositions of a large number of simple parameterized linear and nonlinear operators. An all-too-familiar story of the past decade is that of plugging a deep network into an engineering or scien- tific application as a black box, learning its parame- ter values using copious training data, and then sig- nificantly improving performance over classical task- specific approaches based on erudite practitioner ex- pertise or mathematical elegance. Despite this exciting empirical progress, however, the precise mechanisms by which deep learning works so well remain relatively poorly understood, adding an air of mystery to the entire field. Ongoing at- tempts to build a rigorous mathematical framework have been stymied by the fact that, while deep net- works are locally simple, they are globally compli- cated. Hence, they have primarily been studied as \"black boxes\" and mainly empirically. This ap- proach greatly complicates analysis to understand the both the success and failure modes of deep net- works. This approach also greatly complicates deep learning system design, which today proceeds al- chemistically rather than from rigorous design prin- ciples. And this approach greatly complicates ad- dressing higher level issues like trustworthiness (can we trust a black box?), sustainability (ever-growing computations lead to a growing environmental foot- print), and social responsibility (fairness, bias, and beyond). In this paper, we overview one promising avenue of progress at the mathematical foundation of deep learning: the connection between deep networks and function approximation by affine splines (continuous piecewise linear functions in multiple dimensions). In particular, we will overview work over the past decade on understanding certain geometrical prop- erties of a deep network's affine spline mapping, in particular how it tessellates its input space. As we will see, the affine spline connection and geometrical viewpoint provide a powerful portal through which to view, analyze, and improve the inner workings of a deep network. There are a host of interesting open mathemati- cal problems in machine learning in general and deep learning in particular that are surprisingly accessible once one gets past the jargon. Indeed, as we will see, the core ideas can be understood by anyone know- ing some linear algebra and calculus. Hence, we will pose numerous open questions as they arise in our exposition in the hopes that they entice more math- ematicians to join the deep learning community. The state-of-the-art in deep learning is a rapidly moving target, and so we focus on the bedrock of modern deep networks, so-called feedforward neural networks employing piecewise linear activation func- tions. While our analysis does not fully cover some very recent methods, most notably transformer net- works, the networks we study are employed therein as key building blocks. Moreover, since we focus on the affine spline viewpoint, we will not have the oppor- tunity to discuss other interesting geometric work in", "sections": [{"title": "Introduction", "content": "Machine learning has significantly advanced our abil- ity to address a wide range of difficult computational problems and is the engine driving progress in mod- ern artificial intelligence (AI). Today's machine learn- ing landscape is dominated by deep (neural) net- works, which are compositions of a large number of simple parameterized linear and nonlinear operators. An all-too-familiar story of the past decade is that of plugging a deep network into an engineering or scien- tific application as a black box, learning its parame- ter values using copious training data, and then sig- nificantly improving performance over classical task- specific approaches based on erudite practitioner ex- pertise or mathematical elegance.\nDespite this exciting empirical progress, however, the precise mechanisms by which deep learning works so well remain relatively poorly understood, adding an air of mystery to the entire field. Ongoing at- tempts to build a rigorous mathematical framework have been stymied by the fact that, while deep net- works are locally simple, they are globally compli- cated. Hence, they have primarily been studied as \"black boxes\" and mainly empirically. This ap- proach greatly complicates analysis to understand the both the success and failure modes of deep net- works. This approach also greatly complicates deep learning system design, which today proceeds al- chemistically rather than from rigorous design prin- ciples. And this approach greatly complicates ad- dressing higher level issues like trustworthiness (can we trust a black box?), sustainability (ever-growing computations lead to a growing environmental foot- print), and social responsibility (fairness, bias, and beyond).\nIn this paper, we overview one promising avenue of progress at the mathematical foundation of deep learning: the connection between deep networks and function approximation by affine splines (continuous piecewise linear functions in multiple dimensions). In particular, we will overview work over the past decade on understanding certain geometrical prop- erties of a deep network's affine spline mapping, in particular how it tessellates its input space. As we will see, the affine spline connection and geometrical viewpoint provide a powerful portal through which to view, analyze, and improve the inner workings of a deep network.\nThere are a host of interesting open mathemati- cal problems in machine learning in general and deep learning in particular that are surprisingly accessible once one gets past the jargon. Indeed, as we will see, the core ideas can be understood by anyone know- ing some linear algebra and calculus. Hence, we will pose numerous open questions as they arise in our exposition in the hopes that they entice more math- ematicians to join the deep learning community.\nThe state-of-the-art in deep learning is a rapidly moving target, and so we focus on the bedrock of modern deep networks, so-called feedforward neural networks employing piecewise linear activation func- tions. While our analysis does not fully cover some very recent methods, most notably transformer net- works, the networks we study are employed therein as key building blocks. Moreover, since we focus on the affine spline viewpoint, we will not have the oppor- tunity to discuss other interesting geometric work in"}, {"title": "Deep Learning", "content": "Machine learning in 200 words or less. In su- pervised machine learning, we are given a collection of n training data pairs {(xi,yi)}i=1n; xi is termed the data and yi the label. Without loss of generality, we will take x \u2208 RD, yi \u2208 RC to be column vectors, but in practice they are often tensors.\nWe seek a predictor or model f with two basic prop- erties. First, the predictor should fit the training data: f(xi) \u2248 yi. When the predictor fits (near) per- fectly, we say that it has interpolated the data. Sec- ond, the predictor should generalize to unseen data: f(x') \u2248 y', where (x', y') is test data that does not appear in the training set. When we fit the train- ing data but do not generalize, we say that we have overfit.\nOne solves the prediction problem by first design- ing a parameterized model f\u0398 with parameters \u0398 and then learning or training by optimizing \u0398 to make f\u0398(xi) as close as possible to yi on average in terms of some distance or loss function L, which is often called the training error.\nDeep networks. A deep network is a predictor or model constructed from the composition of L inter- mediate mappings called layers [GBCB16]\nf\u0398(x) = (fL ... f1)(x).\nHere \u0398 is the collection of parameters from each layer, \u0398(l), l = 1, ..., L. We will omit the parameters \u0398 or \u0398(l) from our notation except where they are critical, since they are ever-present in the discussion below.\nThe l-th deep network layer f(l) takes as input the vector z(l\u22121) and outputs the vector z(l) by combin- ing two simple operations\nz(l) = f(l)(z(l\u22121)) = \u03c3(W(l)z(l\u22121) + b(l)).\nThe most widespread activation function in mod- ern deep networks is the rectified linear unit (ReLU)\n\u03c3(u) = max{u, 0} =: ReLU(u).\nThroughout this paper, we focus on networks that use this activation, although the results hold for any continuous piecewise linear nonlinearity (e.g., abso- lute value \u03c3(u) = |u|). Special activations are often employed at the last layer f(L), from the linear acti- vation \u03c3(u) = u to the softmax that converts a vector to a probability histogram. These activations do not affect our analysis below. It is worth point out, but beyond the scope of this paper, that it is possible to generalize the results we review below to a much larger class of smooth activation functions (e.g., sig- moid gated linear units, Swish activation) by adopt- ing a probabilistic viewpoint [BB18].\nThe term \"network\" is used in deep learning be- cause compositions of the form (1) are often depicted as such; see Figure 1.\nLearning. To learn to fit the training data with a deep network, we tune the parameters W(l), b(l), l =\n1, ..., L such that, on average, when training datum xi is input to the network, the output y = f(xi)\nis close to yi as measured by some loss function L.\nTwo loss functions are ubiquitous in deep learning;\n(1)\nwhere z(0) = x and z(L) = y = f(x). First the layer applies an affine transformation to its input. Second, in a standard abuse of notation, it applies a scalar nonlinear transformation \u03c3 called the ac- tivation function to each entry in the result. The entries of z(l) are called the layer-l neurons or units, and the width of the layer is the dimensional- ity of z(l). When layers of the form (2) are used in (1), deep learners refer to the network as a multilayer perceptron (MLP).\nThe parameters \u0398(l) of the layer are the elements of the weight matrix W(l) and the bias vector b(l). Special network structures have been developed to reduce the generally quadratic cost of multiplying by the W(l). One notable class of networks constraints W(l) to be a circulant matrix, so that W(l)z(l) cor- responds to a convolution, giving rise to the term ConvNet for such models. Even with this simplifica- tion, it is common these days to work with networks with billions of parameters."}, {"title": "Affine Splines", "content": "As we will now explain, deep networks are tractable multidimensional extensions of the familiar one- dimensional continuous piecewise linear functions de- picted on the left in Figure 2. When such continuous piecewise functions are fit to training data, we refer to them as affine splines for short.\nDeep networks implement one particular extension of the affine spline concept to a multidimensional do- main and range. As we will see in the next section, a deep network generalizes the intervals of the indepen- dent variable over which a piecewise affine function is simply affine (recall Figure 2) to an irregular tessel- lation (tiling) of the network's D-dimensional input space into convex polytopes. Let \u03a9 denote the tessel- lation and \u03c9 \u2208 \u03a9 an individual tile. (The jargon for the polytope tiles is \u201clinear region\u201d [MPCB14].)\nGeneralizing the straight lines defining the func- tion on each interval in Figure 2, a deep network creates an affine transformation on each tile such that the overall collection is continuous. Figure 3 depicts an example for a toy deep network with a two-dimensional input space; here the tiles are poly- gons. This all can be written as [BB21]\nf(x) = \u2211\u03c9\u2208\u03a9(Awx + cw)1{x\u2208\u03c9},\nwhere the matrix Aw and vector cw define the affine transformation from tile \u03c9 to the output.\nBoth the tessellation \u03a9 and Aw, cw from the affine transformations are functions of the deep network weights W(l) and biases b(l). Geometrically, envi- sion Figure 3 with a cloud of n training data points\n(xi, yi); learning uses optimization to adjust the weights and biases to create a tessellation and affine transformations such that the affine spline predic- tions y\u02c6i come as close as possible to the true labels yi as measured by the squared error loss (4), for ex- ample.\nDeep Network Tessellation\nAs promised, let us now see how a deep network cre- ates its input space tessellation [BB21]. Without loss of generality, we start with the first layer f(1) whose input is x and output is z(1). The k-th entry in z(1)\n(the value of the k-th neuron) is calculated simply as\nzk(1) = \u03c3(wk(1) \u00b7 x + bk(1)),"}, {"title": "Deep Network Tessellation", "content": "where the dot denotes the inner product, wk(1) is the k-th row of the weight matrix W(1), and \u03c3 is the ReLU activation function (3). The quantity inside the activation function is the equation of a D \u2212 1- dimensional hyperplane in the input space RD that is perpendicular to wk(1) and offset from the origin by bk(1)/||wk(1)||^2. This hyperplane bisects the input\n(1)\n(1)\nspace into two half-spaces; one where zk > 0 and one where zk(1) = 0.\nThe collection of hyperplanes corresponding to each neuron in z(1) create a hyperplane arrangement. It is precisely the intersections of the half-spaces of the hyperplane arrangement that tessellate the input space into convex polytope tiles (see Figure 4).\nThe weights and biases of the first layer determine not only the tessellation of the input space but also an affine transformation on each tile to implement (5). Explicit formulas for Aw, cw are available in [BCAB19]. It should be clear that, since all of the transformations in (6) are continuous, so must be the affine spline (5) corresponding to the first layer.\nThe tessellation corresponding to the composition of two or more layers follows an interesting subdivi- sion process akin to a \"tessellation of tessellations\" [BCAB19]. For example, the second layer creates a hyperplane arrangement in its input space, which happens to be the output space of layer one. Thus, these hyperplanes can be pulled back through layer one to its input space by performing the same pro- cess as above but on a tile-by-tile basis relative to the layer-one tessellation and its associated affine trans- forms. The effect on the layer-two hyperplanes is that they are folded each time they cross a hyperplane cre- ated by layer 1. Careful inspection of the tessellation in Figure 3 reveals many examples of such hyperplane folding. Similarly, the hyperplanes created by layer three will be folded every time they encounter a hy- perplane in the input space from layers one or two.\nMuch can be said about this folding process, in- cluding a formula for the dihedral angle of a folded hyperplane as a function of the network's weights and biases. However, the formulae for the angles and affine transformations unfortunately become un- wieldy for more than two layers. Finding simplifica- tions for these attributes is an interesting open prob- lem as are the connections to other subdivision pro- cesses like wavelets and fractals.\nThe theory of hyperplane arrangements is rich and tells us that, generally speaking, the number of tiles grows rapidly with the number of neurons in each layer. Hence, we can expect even modestly sized deep networks to have an enormous number of tiles in their input space, each with a corresponding affine transformation from input to output space. Impor- tantly, though, the affine transformations are highly coupled because the overall mapping (5) must remain continuous. This means that the class of functions that can be represented using a deep network is con- siderably smaller than if the mapping could be un- coupled and/or discontinuous. Understanding what deep learning practitioners call the network's \"im- plicit bias\" remains an important open problem.\nVisualizing the Tessellation\nThe toy, low-dimensional examples in Figures 3 and 4 are useful for building intuition, but how can we gain insight into the tessellation of a deep network with thousands or more of input and output dimensions?\nOne way to proceed is to compute summary statistics"}, {"title": "Visualizing the Tessellation", "content": "about the tessellation, such as how the number of tiles scales as we increase the width or depth of a network (e.g., [MPCB14]); more on this below. An alternative is to gain insight via direct visualization.\nSplineCam is an exact method for computing and visualizing a deep network's spline tessellation over a specified low-dimensional region of the input space, typically a bounded two-dimensional planar slice [HBBB23]. SplineCam uses an efficient graph data structure to encode the intersections of the hyper- planes (from the various layers) that pass through the slice and then uses a fast heuristic breadth-first search algorithm to identify tiles from the graph. All of the computations besides the search can be vec- torized and computed on GPUs to enable the visual- ization of even industrial-scale deep networks.\nAn interesting avenue for future research in- volves the efficient extension of SplineCam to higher-dimensional slices both for visualization and the com- putation of summary statistics.\nThe main goal of this paper is to demonstrate the broad range of insights that can be garnered into the inner workings of a deep network through a focused study of the geometry of its input space tessellation. To this end, we now tour five examples relating to deep network approximation, optimization, and data synthesis. But we would be remiss if we did not point to the significant progress that has been made leveraging other important aspects of the spline view of deep learning, such as understanding how affine splines emerge naturally from the regularization typ-"}, {"title": "The Self-Similar Geometry of the Tessellation", "content": "It has been known since the late 1980s that even a two-layer neural network is a universal approxima- tor, meaning that, as the number of neurons grows, one can approximate an arbitrary continuous func- tion over a Borel measurable set to arbitrary pre- cision [Cyb89]. But, unfortunately, while two-layer networks are easily capable of interpolating a set of training data, in practice they do a poor job general- izing to data outside of the training set. In contrast, deep networks with L \u226b 2 layers have proved over the past 15 years that they are capable of both inter- polating and generalizing well.\nSeveral groups have investigated the connections between a network's depth and its tessellation's capacity to better approximate. [MPCB14] was the"}, {"title": "Geometry of the Loss Function", "content": "Frankly, it seems an apparent miracle that deep net- work learning even works. Because of the composi- tion of nonlinear layers and the myriad local minima of the loss function, deep network optimization re- mains an active area of empirical research. Here we look at one analytical angle that exploits the affine spline nature of deep networks.\nOver the past decade, a menagerie of different deep network architectures has emerged that innovate in different ways on the basic architecture (1), (2). A natural question for the practitioner is: Which archi- tecture should be preferred for a given task? Approx- imation capability does not offer a point of differen- tiation, because, as we just discussed, as their size (number of parameters) grows, most deep networks attain a universal approximation capability.\nnetwork can approximate that matters, but rather how it learns to approximate. Empirical studies have indicated that this is because the so-called loss land- scape of the loss function L(\u0398) navigated by gradient descent as it optimizes the deep network parameters is much smoother for ResNets as compared to Con- vNets (see Figure 6). However, to date there has been no analytical work in this direction.\nUsing the affine spline viewpoint, it is possible to analytically characterize the local properties of the deep network loss landscape and quantitatively com- pare different deep network architectures. The key is that, for the deep networks under our consideration trained by minimizing the squared error (4), the loss landscape Las a function of the deep network param- eters W(l), b(l) is a continuous piecewise quadratic function [RBB23, SPD+20] that is amenable to anal-\nsis (see Figure 6).\nThe optimization of quadratic loss surfaces is well-understood. In particular, the eccentricity of a quadratic loss landscape is governed by the sin- gular values of the Hessian matrix containing the second-order quadratic terms. Less eccentric (more bowl shaped) losses are easier for gradient descent to quickly navigate to the bottom. Similarly, the local eccentricity of a continuous piecewise quadratic loss function and the width of each local minimum basin are governed by the singular values of a \u201clocal Hes- sian matrix\u201d that is a function of not only the deep network parameters but also the deep network archi- tecture. This enables us to quantitatively compare different deep network architectures in terms of their singular values.\nPractitioners know that deep networks with skip connections\nz(l) = \u03c3(W(l)z(l\u22121) + b(l)) + z(l\u22121)\nsuch as so-called ResNets, are much preferred over ConvNets, because empirically their gradient descent learning converges faster and more stably to a bet- ter minimum. In other words, it is not what a deep"}, {"title": "The Geometry of Initialization", "content": "As we just discussed, even for the prosaic squared er- ror loss function (4), the loss landscape as a function of the parameters is highly nonconvex with myriad lo- cal minima. Since gradient descent basically descends to the bottom of the first basin it can find, where it starts (the initialization) really matters. Over the years, many techniques have been developed to im- prove the initialization and/or help gradient descent find better minima; here we look at one of them that is particularly geometric in nature.\nWith batch normalization, we modify the definition of the neural computation from (6) to\nzk(1) = \u03c3(wk(1) \u00b7 x \u2212 \u03bck(1)vk(1)),\nwhere \u03bck(1) and vk(1) are not learned by gradient de- scent but instead are directly computed as the mean and standard deviation of wk(1) \u00b7 x over the training data inputs involved in each gradient step in the op- timization. Importantly, this includes the very first step, and so batch normalization directly impacts the initialization from which we start iterating on the loss landscape.\nAstute readers might see a connection to the stan- dard data preprocessing step of data normalization and centering; the main difference is that this pro- cessing is performed before each and every gradient learning step. Batch normalization often greatly aids the optimization of a wide variety of deep networks, helping it to find a better (lower) minimum quicker. But the reasons for its efficacy are poorly understood.\nWe can make progress on understanding batch nor- malization by again leaning on the affine spline view- point. Let's focus on the effect of batch normalization at initialization just before gradient learning begins; the effect is pronounced and it is then easy to extrap- olate regarding what happens at subsequent gradient steps. Prior to learning, a deep network's weights are initialized with random numbers. This means that the initial hyperplane arrangement is also random.\nThe key finding of [BB22] is that batch normaliza- tion adapts the geometry of a deep network's spline tessellation to focus the network's attention on the training data xi. It does this by adjusting the angles and offsets of the hyperplanes that form the bound- aries of the polytopal tiles to increase their density in regions of the input space inhabited by the training data, thereby enabling finer approximation there.\nMore precisely, batch normalization directly adapts each layer's input space tessellation to min- imize the total least squares distance between the tile boundaries and the training data. The resulting data-adaptive initialization aligns the spline tessella- tion with the data not just at initialization but before every gradient step to give the learning algorithm a much better chance of finding a quality set of weights and biases."}, {"title": "The Dynamic Geometry of Learning", "content": "Current deep learning practice treats a deep network as a black box and optimizes its internal parameters (weights and biases) to minimize some end-to-end training error like the squared loss in (4). While this approach has proved mightily successful empirically, it provides no insight into how learning is going on\ninside the network nor how to improve it. Clearly, as we adjust the parameters to decrease the loss function using gradient descent, the tessellation will change dynamically. Can we use this insight to learn some- thing new about what goes on inside a deep network during learning?\nConsider a deep network learning to classify pho- tos of handwritten digits 0-9. Figure 9 deploys SplineCam to visualize a portion of a 2D slice of the input space of the network defined by three data points in the MNIST handwritten digit training dataset [HBB24]. At left, we see that the tessella- tion at initialization (before we start learning) is in disarray due to the random weights and biases and nonuse of batch normalization (more on this later). The tessellation is random, and the training error is large.\nIn the middle, we see the tessellation after conver- gence to near-zero training error, when most of the digits are on the correct side of their respective de- cision boundaries. Not shown by the figure is the fact that the network also generalizes well to unseen test data at this juncture. High density suggests that even a continuous piecewise affine function can be\nFirst, constraining the bias to be zero forces the network into a cen- tral hyperplane arrangement tessellation that is not amenable to aligning with the data. Second, random- izing both the weights and biases splays the tiles over the entire input space, including many places where the training data is not. Third, batch normalization focuses the hyperplanes from all three of the layers onto the regions where the training data lives.\nOne interesting avenue for future research in this direction is developing new normalization schemes that replace the total least squares optimization to enforce a specific kind of adaptivity of the tessella- tion to the data and task at hand."}, {"title": "The Dynamic Geometry of Learning", "content": "quite rugged around these points [BPB20]. Indeed, the false coloring indicates that the 2-norms of the A matrices has increased around the training im- ages, meaning that their \"slopes\" have increased. As a consequence, the overall spline mapping f(x) is now likely more rugged and more sensitive to changes in the input x as measured by a local (per-tile) Lip- schitz constant. In summary, at (near) interpola- tion, the gradient learning iterations have in some sense accomplished their task (near zero training er- ror) but with elevated sensitivity of f(x) to changes in x around the training data points as compared to the random initialization.\nInterpolation is the point that would typically be recommended to stop training and fix the network for use in an application. But let's see what hap- pens if we continue training about 37 times longer. At right in Figure 9, we see that, while the training error has not changed after continued training (it is still near zero, meaning correct classification of nearly all the training data), the tessellation has metamor- phosed. There are now only half as many tiles in this region, and they have all migrated to define the decision boundary, where presumably they are being used to create sharp decisions. Around the training data, we now have a very low density of tiles with low 2-norm of their A matrices, and thus presumably a much smoother mapping f(x). Hence, the sensitivity of f(x) as measured by a local Lipshitz constant will\nbe much lower than just after interpolation.\nWe designate this state of affairs delayed robust- ness; it is one facet of the general phenomenon of grokking that has been discovered only recently [PBE+22]. A dirty secret of deep networks is that f(x) can be quite unstable to small changes in x (which seems expected given the high degree of non- linearity). This instability makes deep networks less robust and more prone to attacks like causing a 'barn' image to be classified as a 'pig' by adding a nearly undetectable but carefully designed attack signal to the picture of a barn. Continuing learning to achieve grokking and delayed robustness is a new approach to mitigating such attacks in particular and making deep learning more stable and predictable in general.\nCan we translate the visualization of Figure 9 into a metric that can be put into practice to compare or improve deep networks? This is an open research question, but here are some first steps [HBB24]. De- fine the local complexity (LC) as the number of tiles in a neighborhood V around a point x in the input space. While exact computation of the LC is combi- natorially complex, an upper bound can be obtained in terms of the number of hyperplanes that inter- sect V according to Zaslavsky's Theorem, with the assumption that V is small enough that the hyper- planes are not folded inside V. Therefore, we can use the number of hyperplanes intersecting V as a proxy for the number of tiles in V.\nFor the experiment reported in Figure 9, we com- puted the LC in the neighborhood of each training data point in the entire training dataset and then av- eraged those values. From the above discussion, high LC around a point x in the input space implies small, dense tiles in that region and a potentially unsmooth and unstable mapping f(x) around x. The values reported in Figure 9 confirm that the LC does in- deed capture the intuition that we garnered visually. One interesting potential application of the LC is as a new progress measure that serves as a proxy for a deep network's expressivity; LC is task-agnostic yet informative of the training dynamics.\nOpen research questions regarding the dynamics of deep network learning abound. At a high level, it is clear from Figure 9 that the classification function be-"}, {"title": "The Geometry of Generative Models", "content": "A generative model aims to learn the underlying patterns in the training data in order to generate new, similar data. The current crop of deep genera- tive models includes transformer networks that power large language models for text synthesis and chat- bots and diffusion models for image synthesis. Here we investigate the geometry of models that until re- cently were state-of-the-art, such as Generative Ad- versarial Networks (GANs) and Variational Autoen- coders (VAEs) that are often based on ReLU and other piecewise linear activation functions.\nDeep generative models map from typically a low-dimensional Euclidean input space (called the param- eter space) to a manifold M of roughly the same di- mension in a high-dimensional output space. Each point x in the parameter space synthesizes a cor- responding output point y = f(x) on the manifold (e.g., a picture of a bedroom). Training on a large number of images yilearns an approximation to the mapping f from the parameter space to the manifold. It is beyond the scope of this review, but learning the parameters of a deep generative model is usually more involved than simple gradient descent [GBCB16]. It is useful for both training and synthesis to view the points x from the parameter space as governed by some probability distribution, e.g., uniform over a bounded region of the input space.\nIn the case of a GAN based on ReLU or similar activation functions, the manifold Mis a continuous"}, {"title": "The Geometry of Generative Models", "content": "piecewise affine manifold; see Figure 10. Points on the manifold are given by (5) as x sweeps through the input space.\nA major issue with deep generative models is that, if the training data is not carefully sourced and cur- rated, then they can produce biased outputs. A deep generative model like a GAN or VAE is trained to approximate both the structure of the true data man- ifold from which the training data was sampled and the data distribution on that manifold. However, all too often in practice, training data are obtained based on preferences, costs, or convenience factors that pro- duce artifacts in the training data distribution on the manifold. Indeed, it is common in practice for there to be more training data points in one part of the manifold than another. For example, a large frac- tion of the faces in the CelebA dataset are smiling, and a large fraction of those in the FFHQ dataset are female with dark hair. When one samples uni- formly from a model trained with such biased data, the biases will be reproduced when sampling from the trained model, which has far-reaching implica- tions for algorithmic fairness and beyond.\nWe can both understand and ameliorate sampling biases in deep generative models by again leveraging their affine spline nature. The key insight for the bias issue is that the tessellation of the input space is carried over onto the manifold. Each convex tile \u03c9 in the input space is mapped to a convex tile M(\u03c9)"}, {"title": "The Geometry of Generative Models", "content": "on the manifold using the affine transform\n\u039c(\u03c9) = {\u0391wx + Cw, x \u2208 \u03c9},\nand the manifold M is the union of the M(\u03c9). This straightforward construction enables us to analyti- cally characterize many properties of M via (5).\nIn particular, it is easy to show that the mapping (9) from the input space to the manifold warps the tiles in the input space tessellation by Aw, causing their volume to expand or contract by\nvol(M(\u03c9))vol(\u03c9) = \u221adet(ATA\u03c9).\nKnowing this, we can take any trained and fixed generative model and determine a nonuniform sam- pling of the input space according to (10) such that the sampling on the manifold is provably uniform and free from bias. The bonus is that this proce- dure, which we call MAximum entropy Generative NETwork (MaGNET) [HBB22a], is simply a post- processing procedure that does not require any re-training of the network.\nLike MaGNET, this polarity sampling approach ap- plies to any pre-trained generative network and so has broad applicability. See Figure 12 for an illustrative toy example and [HBB22b] for numerous examples with large-scale generative models, including using polarity sampling to boost the performance of exist- ing generative models to state-of-the-art.\nThere are many interesting open research questions around affine splines and deep generative networks. One related to the MaGNET sampling strategy is that it assumes that the trained generative network actually learned a good enough approximation of the true underlying data manifold. One could envision exploring how MaGNET could be used to test such an assumption.\nWhile there are several ways to envision extending the concept of a one-dimensional affine spline (recall Figure 2) to high-dimensional functions and opera-\n(9)\n(10)"}, {"title": "Discussion and Outlook", "content": "While there are several ways to envision extending the concept of a one-dimensional affine spline (recall Figure 2) to high-dimensional functions and opera-"}]}