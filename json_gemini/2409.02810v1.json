{"title": "A hybrid FEM-PINN method for time-dependent partial differential equations", "authors": ["Xiaodong Feng", "Haojiong Shangguan", "Tao Tang", "Xiaoliang Wan", "Tao Zhou"], "abstract": "In this work, we present a hybrid numerical method for solving evolution partial differential equations (PDEs) by merging the time finite element method with deep neural networks. In contrast to the conven-tional deep learning-based formulation where the neural network is defined on a spatiotemporal domain, our methodology utilizes finite element basis functions in the time direction where the space-dependent coefficients are defined as the output of a neural network. We then apply the Galerkin or collocation projection in the time direction to obtain a system of PDEs for the space-dependent coefficients which is approximated in the framework of PINN. The advantages of such a hybrid formulation are twofold: statistical errors are avoided for the integral in the time direction, and the neural network's output can be regarded as a set of reduced spatial basis functions. To further alleviate the difficulties from high dimensionality and low regularity, we have developed an adaptive sampling strategy that refines the training set. More specifically, we use an explicit density model to approximate the distribution induced by the PDE residual and then augment the training set with new time-dependent random samples given by the learned density model. The effectiveness and efficiency of our proposed method have been demonstrated through a series of numerical experiments.", "sections": [{"title": "1. Introduction", "content": "Evolution equations, including both time-dependent ordinary and partial differential equations (ODEs / PDEs), are used Many numerical approaches have been developed for such problems, e.g. the finite difference method, the spectral method, and the finite element method. Recently solving PDEs with deep learning methods has been receiving increasing attention [1, 2, 3]. Typical techniques include physics-informed neural networks (PINNs) [4], the deep Ritz methods [5], the weak adversarial networks [6], etc. Although deep learning-based approaches have shown a lot of potential in solving high-dimensional PDEs, there still exist many numerical issues in adapting the neural network approximation to the problem studied."}, {"title": "1.1. Related work", "content": "In this work, we pay particular attention to the error of PINNs for evolution equations which may grow too fast and limit the application of PINNs in long-term integration. Many efforts have been made to address this issue. We now briefly review the relevant works.\nImproved PINNs: PINNs represent the approximate PDE solution as a single neural network, which takes a space-time tuple as input and is trained by minimizing the PDE residual on random collocation points in the space-time domain. To improve the performance of PINNs on long-term integration, many approaches have been developed which mainly focus on seeking a more effective training strategy. In [7, 8], a marching-in-time strategy is proposed by splitting the time domain into many small segments, where the training is done segment by segment and the approximate solution at the end of one segment is used as the initial condition for the next segment. In [9], backward-compatibility PINNs (bc-PINNs) are proposed, where the obtained solution in previous time segments is used as a constraint for the model training in the current time segment. In [10], Causal PINNs are developed to incorporate causality into the training process by introducing causality weights. In [11], a unified scalable framework for causal sweeping strategies is developed.\nEvolution deep neural networks (EDNNs): EDNNs [12, 13] are formulated with the Dirac-Frenkel variational principle to train networks by minimizing the residual sequentially in time, where the model parameters are time-dependent rather than global in the whole space-time domain. Traditional time-marching methods can be used to update the model parameters. Compared to PINNs, the training of EDNNs is more expensive while it is more flexible to adapt EDNNs to constraints such as Hamiltonian conservation [14]. The efficiency of EDNNs is improved in [15] by making a part of model parameters time-dependent and in [16] by updating randomized sparse subsets of model parameters at each time step.\nOperator learning: The main idea is to learn an operator that maps the solution from the current time step to the next time step. For example, physics-informed DeepONet [17, 18] can be used to learn a solution operator over a short time interval $t \\in [0, \\Delta t]$. Starting with $n = 2$, the model's prediction at $n\\Delta t$ can be obtained from the trained model using the approximate solution at $(n - 1)\\Delta t$ as the input. Other examples include auto-regressive networks [19], transformer [20], etc.\nHybrid strategies: These approaches try to hybridize classical numerical methods with deep learning techniques by either adapting neural networks to augment classical PDE solvers [21, 22] or adapting classical numerical approaches to improve the performance of PINNs [23]. For example, in [23], a coupled automatic and numerical differentiation approach is proposed to take advantage of the regularization induced by numerical discretization. In [24], a deep adaptive basis Galerkin approach is proposed where the orthogonal polynomial expansion is employed in time direction and the expansion coefficients are modeled as the output of a deep neural network."}, {"title": "1.2. Our contribution", "content": "The main contributions of this work are summarized as follows:\n\u2022 We have developed a hybrid numerical method by merging the time finite element method with deep neural networks. The approximate solution is a linear combination of the time finite element basis functions, where the coefficients are given by the output of a neural network. We subsequently apply Galerkin or collocation projection to eliminate the time and use PINN to approximate the system of PDEs for the coefficients. This strategy has some advantages: First, the numerical difficulties induced by random sampling and causality are avoided in the time direction since the projection can be done accurately. Second, all the coefficients define a set of reduced basis functions on the computation domain, which are learned through the neural network. The approximate solution can also be regarded as a time-dependent linear combination of these reduced basis functions, which shares similarities with the low-rank representation in the study of high-dimensional problems.\n\u2022 We have proposed a deep adaptive sampling strategy to enhance the numerical efficiency. Mesh refinement in the time direction is straightforward. Particular attention needs to be paid to the random sampling in the physical space, especially for high-dimensional and low-regularity problems. Using a spatially conditional bounded KRnet and a discrete distribution in the time direction, we have constructed a joint density model to learn the distribution induced by the PDE residual, based on which new time-dependent samples are generated to refine the training set."}, {"title": "2. Preliminaries", "content": ""}, {"title": "2.1. Physics-informed neural networks (PINNs)", "content": "We begin with a brief overview of physics-informed neural networks (PINNs). We consider a general time-dependent PDE\n$u_t(x,t) + N[u](x,t) = f(x,t), \\quad x \\in \\Omega \\subset \\mathbb{R}^d, t \\in [0, T]$,\nsubject to the initial and boundary conditions\n$u(x, 0) = g(x), \\quad x \\in \\Omega,$\n$B[u](x,t) = b(x,t), \\quad x \\in \\partial \\Omega, t \\in [0, T]$,\nwhere $N[\\cdot]$ is a linear or nonlinear differential operator, and $B[\\cdot]$ is a boundary operator corresponding to Dirichlet, Neumann, Robin or periodic boundary conditions.\nFollowing the original work of Raissi et al. [4], we represent the unknown solution $u(x,t)$ with a deep neural network $u_\\theta(x,t)$, where $\\theta$ denotes all tunable parameters (e.g. weights and biases). Then, a physics-informed model can be trained by minimizing the following composite loss function\n$\\mathcal{L}(\\theta) = \\lambda_{ic}\\mathcal{L}_{ic}(\\theta) + \\lambda_{bc}\\mathcal{L}_{bc}(\\theta) + \\lambda_r\\mathcal{L}_r(\\theta)$,\nwhere\n$\\mathcal{L}_{ic}(\\theta) = \\frac{1}{N_{ic}}\\sum_{i=1}^{N_{ic}} |u_\\theta(x_i^{ic}, 0) - g(x_i^{ic})|^2,$\n$\\mathcal{L}_{bc}(\\theta) = \\frac{1}{N_{bc}}\\sum_{i=1}^{N_{bc}} |B[u_\\theta](x_i^{bc}, t_i^{bc}) - b(x_i^{bc}, t_i^{bc})|^2,$\n$\\mathcal{L}_r(\\theta) = \\frac{1}{N_r}\\sum_{i=1}^{N_r} |\\frac{\\partial u_\\theta}{\\partial t}(x_i^r, t_i^r) + N[u_\\theta](x_i^r, t_i^r) - f(x_i^r, t_i^r)|^2.$\nHere $\\{x_i^{ic}\\}_{i=1}^{N_{ic}}$, $\\{x_i^{bc}, t_i^{bc}\\}_{i=1}^{N_{bc}}$ and $\\{x_i^r, t_i^r\\}_{i=1}^{N_r}$ can be the vertices of a fixed mesh or points that are randomly sampled at each iteration of a gradient descent algorithm. The gradients with respect to both the input variables $(t, x)$ and the model parameters $\\Theta$ can be efficiently computed via automatic differentiation [25]. Moreover, the hyper-parameters $\\{\\lambda_{ic}, \\lambda_{bc}, \\lambda_{r}\\}$ allow the flexibility of assigning a varying learning rate to each loss term to balance their interplay during the training process, which may be user-specified or tuned automatically."}, {"title": "2.2. Continuous time finite element method", "content": "Evolution PDEs are often approximated by spatial finite elements together with a sequential time-marching scheme. Another choice is to construct a finite element approximation space on the space-time domain. We briefly review the time finite element method for first-order ordinary differential equations.\nConsider the following model problem:\n$u'(t) + N[u](t) = f(t), \\quad t \\in [0, T]$,\n$u(0) = 0,$\nwhere $N$ is a linear or nonlinear operator and $f(t) \\in L_2(I)$ with $I = [0, T]$."}, {"title": "2.2.1. Galerkin projection", "content": "We let $X := \\{u \\in H^1(I) : u(0) = 0\\}$ be the trial space and $Y := L_2(I)$ the test space. The primal variational formulation of (5) is as follows.\n$\\left\\{\\begin{array}{l} \\text{Find } u \\in X \\text{ such that} \\\\ (u', v) + (N[u], v) = (f, v), \\forall v \\in Y, \\end{array}\\right.$"}, {"title": "2.2.2. Collocation projection", "content": "Collocation projection provides a flexible strategy especially when $N$ is nonlinear [26, 27].Let $\\{s_k\\}_{k=1}^{K}$ be Gaussian-type quadrature points on the reference interval [0, 1]\n$0 \\leq s_1 < s_2 < \\dots < s_K \\leq 1.$\nConsider a partition of [0, T] with\n$0 = t_0 < t_1 < \\dots < t_M = T, \\quad h_i = t_i - t_{i-1}, i = 1, \\dots, M.$\nDefine\n$s_{m,k} = t_{m-1} + h_m s_k, \\quad 1 \\leq k \\leq K, 1 \\leq m \\leq M.$\nWe seek the approximate solution by enforcing the equation on the collocation points, i.e.,\n$\\left\\{\\begin{array}{l} \\text{Find } u \\in X_N \\cap C^1(I) \\text{ such that} \\\\ u'(s) + N[u](s) = f(s), \\quad s \\in \\underset{m=1}{\\overset{M}{\\bigcup}}\\{s_{m,k}\\}_{k=1}^{K}, \\end{array}\\right.$\nwhere $X_N \\cap C^1(I)$ defines a finite element approximation space with $C^1$ elements and $N + 1$ is equal to the total number of collocation points. It is shown in [28] by selecting the collocation points carefully collocation projection yields the same order of accuracy as Galerkin projection. Typical piecewise polynomials with at least $C^1$ regularity include piecewise cubic Hermite polynomials and cubic spline functions."}, {"title": "3. Methodology", "content": "Now we are ready to present our approach for evolution equations (1). We aim to seek an approximate solution of the following form\n$u(x, t; \\theta) = \\sum_{i=0}^{N} \\omega_i(x; \\theta)\\phi_i(t)$,\nwhere $\\{\\phi_i(t)\\}_{i=0}^N$ is a pre-specified set of time finite element basis functions, $\\omega_i : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ are modeled by the output of a neural network $\\omega(x, \\theta) : \\mathbb{R}^d \\rightarrow \\mathbb{R}^{N+1}$, and $\\theta$ includes all tunable model parameters. More precisely, $\\omega(x; \\theta)$ is a fully-connected neural network defined as\n$\\omega(x; \\theta) := a_h_{L-1} \\circ h_{L-2} \\circ \\dots \\circ h_1(x) \\quad \\text{for } x \\in \\mathbb{R}^d,$\nwhere $L \\in \\mathbb{N}^+$, $a \\in \\mathbb{R}^{M_{L-1}\\times(N+1)}$, $h_l(x_l) := \\sigma(W_lx_l + b_l)$ with $W_l \\in \\mathbb{R}^{M_l\\times M_{l-1}}$ ($M_0 := d$) and $b_l \\in \\mathbb{R}^{M_l}$ for $l = 1, 2, \\dots, L - 1$. Then $\\theta := \\{a, W_l, b_l : 1 \\leq l < L - 1\\}$. $\\sigma(x)$ is an activation function which acts on $x$ componentwisely to return a vector of the same size as $x$. We let $M_l = M$ be a fixed number for all $l$ and $\\mathcal{F}_{L,M}$ the set consisting of all $\\omega$ with depth $L$ and width $M$."}, {"title": "3.1. A hybrid FEM-PINN method", "content": "We consider the following hypothesis space\n$\\mathcal{U}_N := \\left\\{ u_N(x, t; \\theta) = \\sum_{i=0}^{N} \\omega_i(x; \\theta)\\phi_i(t), \\quad \\omega = (\\omega_0, \\dots, \\omega_N) \\in \\mathcal{F}_{L,M} \\right\\}.$\nThe Galerkin projection along the time direction yields that\n$\\left\\{\\begin{array}{l} \\text{Find } u_N \\in \\mathcal{U}_N \\text{ such that} \\\\ (\\partial_t u_N(x, \\cdot), v_N) + (N[u_N](x, \\cdot), v_N) = (f(x, \\cdot), v_N) \\quad \\forall v_N \\in \\text{span}\\{\\psi_j(t) | 0 \\leq j \\leq N\\}, \\forall x \\in \\Omega. \\end{array}\\right.$\nwhere $(\\cdot, \\cdot)$ indicates the inner product of two functions with respect to time. More specifically, we have\n$\\sum_{i=0}^{N} (\\partial_t\\phi_i(t), \\psi_j(t)) \\omega_i(x; \\theta) + \\left(N\\left[\\sum_{i=0}^{N} \\phi_i(t)\\omega_i(x; \\theta)\\right], \\psi_j(t)\\right) = (f(x, t), \\psi_j(t)), \\forall j = 0, 1, \\dots, N, \\forall x \\in \\Omega.$\nIf N is linear with respect to $\\omega$ and time-independent, the above system can be further simplified as follows:\n$\\sum_{i=0}^{N} (\\partial_t\\phi_i(t), \\psi_j(t))\\omega_i(x; \\theta) + \\sum_{i=0}^{N} (\\phi_i(t), \\psi_j(t))N(\\omega_i(x; \\theta)) = (f(x, t), \\psi_j(t)), \\forall j = 0, 1, \\dots, N.$\nNow let us turn to the collocation projection along the time direction. Let $\\mathcal{S}_t = \\underset{m=1}{\\overset{M}{\\bigcup}}\\{s_{m,k}\\}_{k=1}^{K}$, where $s_{m,k}$ is defined in equation (13). The collocation formulation can be written as\n$\\left\\{\\begin{array}{l} \\text{Find } u_N \\in \\mathcal{U}_N \\text{ such that} \\\\ (\\partial_t u_N(x, s; \\theta) + N[u_N](x, s; \\theta) = f(x, s), \\forall s \\in \\mathcal{S}_t, \\forall x \\in \\Omega. \\end{array}\\right.$\nMore specifically,\n$\\sum_{i=0}^{N} \\partial_t\\phi_i(s)\\omega_i(x; \\theta) + N\\left[\\sum_{i=0}^{N} \\phi_i(s)\\omega_i(x; \\theta)\\right] = f(x, s), \\forall s \\in \\mathcal{S}_t, \\forall x \\in \\Omega.$"}, {"title": "3.1.1. Some remarks on the hybrid form", "content": "PINN is formulated as a least-square method in terms of the hypothesis space given by the neural network. The error of $u_N(x, t; \\hat{\\theta})$ satisfies\n$\\mathbb{E}||u_{\\text{exact}}(x, t) - u_N(x, t; \\hat{\\theta})|| \\leq ||u_{\\text{exact}}(x, t) - u_N(x, t; \\theta^*)|| + \\mathbb{E}||u_N(x, t; \\theta^*) - u_N(x, t; \\hat{\\theta})||$\nfor a proper norm, where $\\theta^*$ is the minimizer of $\\mathcal{L}(\\theta)$, $\\hat{\\theta}$ is the minimizer of $\\mathcal{\\hat{L}}(\\theta)$ and the expectation $\\mathbb{E}[\\cdot]$ is with respect to random samples. On the right-hand side, the first term is the approximation error determined by the hypothesis space and the second term is the statistical error introduced by the random samples.\nIt is well known that PINN may fail to predict convection when the frequency is large although the hypothesis space is capable of yielding a good approximate solution [8]. According to the inequality (31), the reason is twofold: the non-convex structure of the loss landscape and the statistical error. First, the best approximation may not be obtained due to non-convexity of the loss function even the statistical error is zero. The change of the loss landscape can be achieved by adding a regularization term. For example, bc-PINNs have a penalty term to force the model to remember what was learned before [9]. Second, the available strategies that improve the performance of PINNs for time integration can be understood through the reduction of statistical error. Assume that $N_t$ random samples are used in the time direction. The most straightforward strategy is to divide the time interval as $[0, T] = \\bigcup_{i=1}^{n}[i\\Delta T, (i + 1)\\Delta T]$ with $\\Delta T = T/n$ and train the model sequentially in each time segment. After such a decomposition, the variation in time is reduced implying that the Monte Carlo approximation of the loss given by the random samples is more accurate due to variance reduction. The better the loss is discretized by random samples, the smaller the statistical error is. Another strategy, called causal training, is proposed in [10]. A weighted residual loss function is defined as\n$\\mathcal{L}_r(\\theta) = \\frac{1}{N_t}\\sum_{i=1}^{N_t} \\lambda_i \\mathcal{L}_r(t_i, \\theta),$\nwhere $\\mathcal{L}_r(t_i, \\theta)$ is the residual loss at $t = t_i$, and\n$\\lambda_i = \\exp\\left(-\\epsilon \\sum_{j=1}^{i-1} \\mathcal{L}_r(t_j, \\theta)\\right) \\quad i = 2, 3, \\dots, N_t$\nwith $0 < \\epsilon < \\infty$. The intuition is that the model will not be trained until the model is well trained for small $t_i$, which is consistent with the causality induced by evolution. Note that\n$\\frac{1}{N_t}\\sum_{i=1}^{N_t} \\lambda_i \\mathcal{L}_r(t_i, \\theta) \\approx \\frac{1}{T}\\int_{0}^{T} \\Lambda(t)\\mathcal{L}_r(t, \\theta)dt$\nis the Monte Carlo approximation of a weighted loss with $N_t$ uniform random samples. If $\\Lambda(t) > 0$ and the exact solution is included in the hypothesis space, the same $\\theta^*$ will be reached. $\\Lambda(t)$ is a decreasing function by definition while $\\mathcal{L}_r(t, \\theta)$ is in general an increasing function due to the accumulation of errors with time. If $\\Lambda(t)$ and $\\mathcal{L}_r(t, \\theta)$ are well balanced, their product varies much less in time, corresponding to a small variance in terms of the uniform distribution. Such a balance is mainly achieved by the selection of the so-called causality parameter $\\epsilon$. If $\\epsilon$ fails to introduce a variance reduction for $\\Lambda(t)\\mathcal{L}_r(t, \\theta)$, the statistical error will not be reduced, implying that the training results may get worse. This explains the sensitivity of the training strategy on $\\epsilon$.\nBased on the above observations, we intend to use the hybrid form (15) to alleviate the difficulties induced by the statistical errors in the time direction. We also note that the coefficients for the time finite element basis functions are given by the outputs of a neural network, which corresponds to learning a set of reduced basis functions in the physical space since the output layer of the neural network is a linear combination of these basis functions."}, {"title": "3.2. Deep adaptive sampling method", "content": "Random samples are used for the integration in the physical space. To reduce the statistical errors, we consider the adaptive sampling method [29, 30]. For simplicity, we only consider the interior residual $\\mathcal{L}_r(\\theta)$, and the time interval is [0, 1]. As suggested in [29], we relax the objective function $\\mathcal{L}_r(\\theta)$ as:\n$\\mathcal{L}_r(\\theta) = \\sum_{i=0}^{N} \\lambda_i \\mathcal{L}_{r,i}(\\theta) = \\sum_{i=0}^{N} \\lambda_i \\int r_i^2(x; \\theta) p_i(x) dx \\approx \\frac{1}{N_r}\\sum_{i=0}^{N} \\lambda_i \\sum_{j=1}^{N_{r,i}} r_i^2(x_j^{(i)}; \\theta),$\nwhere $\\lambda_i > 0$, $\\sum_{i=0}^{N} \\lambda_i = 1$, the set $\\{x_j^{(i)}\\}_{j=1}^{N_{r,i}}$ is generated with respect to the probability density function $p_i(x) > 0$ instead of a uniform distribution. We associate $\\mathcal{L}_{r,i}(\\theta)$ with a weight $\\lambda_i$. The minimizer of $\\mathcal{L}_r(\\theta)$ is also the solution to the problem if the exact solution is included in the hypothesis space. To reduce the error induced by the Monte Carlo approximation, we may adjust $p_i(x)$ to make the residuals $r_i^2(x; \\theta)$ nearly uniform. To do this, we refine the training set gradually by adding new samples according to the distribution induced by $r^2(x; \\theta^{(k)})$, where $k$ indicates the adaptivity iteration and $\\theta^{(k)}$ is the optimal model parameter given by the previous training set. Once the training set is updated, the model will be retrained, based on which the training set will be refined again. In a nutshell, the model and the training set are updated alternately.\nThe main problem is that we need to obtain new samples from $N + 1$ distributions induced by $r^2(x; \\theta^{(k)})$. To handle this issue, we adopt a time-dependent density estimation strategy. Specifically, we augment the spatial variable in different terms $(\\mathcal{L}_{r,i})$ with an extra dimension $s$, and consider a set $\\{s_i\\}_{i=0}^{N}$ of grid points on the time interval. For the Galerkin approach, $s_i$ can be interpreted as pre-defined nodes of finite element mesh; for the collocation approach, $s_i$ can be viewed as a reordering of pre-defined Gaussian nodes $s_{m,k}$ (see Section 2.2.2). We define a weighted empirical measure\n$\\delta_{\\Lambda}(A) = \\sum_{i=0}^{N} \\lambda_i \\delta_{s_i}(A)$\nfor any $A \\subset [0, 1]$ with $\\delta_{s_i}$ being the Dirac measure and let $r(x, s; \\theta)$ be an interpolation function satisfying\n$r(x, s; \\theta) = r_i(x; \\theta) \\quad \\text{if } s = s_i, i = 0, \\dots, N.$\nLet $p_{x,s}(x, s) = p_{x|s}(x|s)p_s(s)$ be a joint PDF. Choosing $p_s(s)ds = d_{\\Lambda}(ds)$. We have\n$\\int\\int r^2(x, s; \\theta) p_{x,s}(x, s) dxds = \\sum_{i=0}^{N} \\int r_i^2(x; \\theta) p_{x|s}(x|s_i) dx,$\nwhich is consistent with equation (32) if $p_{x|s}(x|s_i) = p_i(x)$. Using $p_{x,s}(x,s)$, the objective functional is discretized as\n$\\mathcal{\\hat{L}}_r(\\theta) \\approx \\frac{1}{N}\\sum_{i=1}^{N} r^2(x^{(i)}, s^{(i)}; \\theta),$\nwhere $\\{(x^{(i)}, s^{(i)})\\}_{i=1}^N$ are sampled from $p_{x,s}(x,s)$. We will use a density model with the form $p_{x|s}(x|s)p_s(s)$ to approximate the distribution induced by $r^2(x, s; \\theta)$. New samples from the trained density model will be added to the training set for refinement."}, {"title": "3.2.1. Model ps(s)", "content": "Without loss of generality, we assume that $s \\in [0, 1]$. We aim to find a invertible transformation $z = f(s)$ such that\n$p_s(s) = p_z(f(s))|\\text{det} V_sf|, \\quad Z \\sim \\mathcal{U}[0, 1]$,\nwhere $\\mathcal{U}$ denotes the uniform distribution. We use the bounded polynomial spline layer $f_{\\text{poly}}$ [31] to parameterize $f$. Specifically, let $0 = l_0 < l_1 < \\dots < l_{m-1} < l_m = 1$ be a given partition of the unit interval and $\\{k_j\\}_{j=0}^{m}$ be the corresponding weights satisfying $\\sum_{j=0}^{m} k_j = 1$. A piecewise linear polynomial can be defined as follows:\n$p(s) = \\frac{k_{j+1} - k_j}{l_{j+1} - l_j}(s - l_j) + k_j, \\quad \\forall s \\in [l_j, l_{j+1}].$\nThen the corresponding cumulative probability function $f_{\\text{poly}}$ admits the following formulation:\n$f_{\\text{poly}}(s) = \\frac{k_{j+1} - k_j}{2(l_{j+1} - l_j)}(s - l_j)^2 + k_j(s - l_j) + \\sum_{i=0}^{j-1} \\frac{k_i + k_{i+1}}{2}(l_{i+1} - l_i), \\quad \\forall s \\in [l_j, l_{j+1}].$\nTo satisfy $\\int_{0}^{1} p(s)ds = 1$, we can model $\\{k_j\\}_{j=0}^{m}$ as\n$k_j = \\frac{\\exp(k_j)}{C} \\quad \\forall j = 0, \\dots, m,$"}, {"title": "3.2.2. Model px\\s(xls)", "content": "For $x \\in \\mathbb{R}^d$, we seek a invertible transformation $z = f(x,s) \\in \\mathbb{R}^d$ for any given $s$ such that\n$p_{x|s}(x|s) = p_{z|s}(z|s) |\\frac{\\partial f(x,s)}{\\partial x}|, \\quad Z|S \\sim \\mathcal{U}[-1, 1]^d, \\forall s.$\nHere we employ conditional bounded KR-net $f_{\\text{B-KRnet}}(\\cdot, s)$ [32] to parameterize $f(\\cdot, s)$. The basic idea of conditional bounded KRnet is to define the structure of $f(x, s)$ in terms of the Knothe-Rosenblatt rearrangement. The transformation $f(\\cdot, s)$ inspired by the Knothe-Rosenblatt (K-R) rearrangement [33] has a low-triangular structure\n$z = f(x,s) = \\begin{pmatrix} f_1(x_1, s) \\\\ f_2(x_1, x_2, s) \\\\ \\vdots \\\\ f_d(x_1, \\dots, x_d, s) \\end{pmatrix}.$\nThe sub-transformations $f_1, \\dots, f_d$ consist of polynomial spline layers and coupling layers [34]. More details can be found in [32, 35]. Let $f_{\\text{B-KRnet}}(\\cdot, s; \\theta_{f,2})$ indicate the conditional invertible transport map induced by bounded KR-net, where $\\theta_{f,2}$ includes the model parameters. Then an explicit PDF model $p_{\\text{B-KRnet}}(x, s; \\theta_{f,2})$ can be obtained by letting $f = f_{\\text{B-KRnet}}$ in equation (42)\n$p_{\\text{B-KRnet}}(x|s; \\theta_{f,2}) = p_z(f_{\\text{B-KRnet}}(x, s)) |\\text{det} V_x f_{\\text{B-KRnet}}|.$"}, {"title": "3.2.3. Adaptive sampling approach", "content": "Now we model a continuous joint density distribution $p_{\\theta_f}(x, t)$\n$p_{\\theta_f}(x, t) = p_{\\text{poly}}(t; \\theta_{f,1})p_{\\text{B-KRnet}}(x|t; \\theta_{f,2}),$\nwhere $\\theta_f = \\{\\theta_{f,1}, \\theta_{f,2}\\}$. To seek the \"optimal\" parameter $\\theta_f$, we can minimize the following objective\n$D_{KL}(f_\\theta(x, t)||p_{\\theta_f}(x, t)) = D_{KL}(f_\\theta(x, t)||p_{\\text{poly}}(t; \\theta_{f,1})p_{\\text{B-KRnet}}(x|t; \\theta_{f,2}))$\n$\\begin{aligned} &= \\int\\int f_\\theta(x, t) \\log (f_\\theta(x, t)) dxdt - \\int\\int f_\\theta(x, t) \\log (p_{\\text{poly}}(t; \\theta_{f,1})p_{\\text{B-KRnet}}(x|t; \\theta_{f,2})) dxdt, \\end{aligned}$\nwhere $D_{KL}$ indicates the Kullback-Leibler (KL) divergence and $f_\\theta(x,t) \\propto r^2(x,t; \\theta)$ is the induced measure by continuous residual squared $r^2(x, t; \\theta)$\n$r(x, t; \\theta) = \\partial_t u_N(x, t; \\theta) - N[u_N](x, t; \\theta) - f(x, t).$\nThe first term on the right-hand side in (46) corresponds to the differential entropy of $f_\\theta(x, t)$, which does not affect the optimization with respect to $\\theta_f$. So minimizing the KL divergence is equivalent to minimizing"}, {"title": "4. Numerical experiments", "content": "In this section, we conduct some numerical experiments to demonstrate the effectiveness of the proposed method, including one convection equation, one Allen-Cahn equation, one two-dimensional and low regu-larity test problems, and two high-dimensional linear or nonlinear problems. Throughout all benchmarks, we will employ the fully-connected neural network equipped with hyperbolic tangent activation functions (Tanh) and initialized using the Glorot normal scheme [36", "37": "and an exponential learning rate decay with a decay-rate of 0.9 every 1,000 training iterations. All experiments are implemented by JAX [38"}]}