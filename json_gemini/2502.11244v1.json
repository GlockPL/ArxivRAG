{"title": "SOTERIA: Language-Specific Functional Parameter Steering for Multilingual Safety Alignment", "authors": ["Somnath Banerjee", "Sayan Layek", "Pratyush Chatterjee", "Animesh Mukherjee", "Rima Hazra"], "abstract": "Ensuring consistent safety across multiple languages remains a significant challenge for large language models (LLMs). We introduce SOTERIA, a lightweight yet powerful strategy that locates and minimally adjusts the \"functional heads\" most responsible for harmful content generation in each language. By altering only a fraction of parameters, SOTERIA drastically reduces policy violations without sacrificing overall model performance, even in low-resource settings. To rigorously evaluate our approach, we also present XThreatBench, a specialized multilingual dataset capturing fine-grained harmful behaviors drawn from real policy guidelines. Experiments with leading open-source LLMs (e.g., Llama, Qwen, Mistral) show that SOTERIA consistently improves safety metrics across high-, mid-, and low-resource languages. These findings highlight a promising path toward scalable, linguistically attuned, and ethically aligned LLMs worldwide. The source code and dataset are available at: https://github.com/NeuralSentinel/Soteria", "sections": [{"title": "1 Introduction", "content": "LLMs such as GPT-40 (OpenAI et al., 2024), Claude (Anthropic), and Llama (Touvron et al., 2023) have revolutionized the AI landscape by delivering impressive performance across tasks ranging from text generation to question answering. These breakthroughs stem from extensive pre-training on large, diverse corpora (Zhou et al., 2023; Kamalloo et al., 2023; Nguyen et al., 2024a). Yet, much of the early research on LLMs' multilingual capabilities relied on translating English queries into non-English, a strategy that obscures genuine multilingual performance (Zhao et al., 2024). Although newer LLMs feature advanced tokenizers that handle non-English inputs more effectively, key safety measures, including red teaming and content filtering remain predominantly English centric (Zhang et al., 2023; Gurgurov et al., 2024).\nAs a result, non-English use cases are comparatively under-protected, and especially smaller-parameter models (e.g., 8B or 7B) often implemented in low-resource settings are at greater risk of generating harmful or culturally insensitive outputs (Banerjee et al., 2025). Moreover, prior work on safety mechanisms has focused mainly on English, overlooking the nuances and needs of broader linguistic communities (Banerjee et al., 2024b; Hazra et al., 2024a). In this context, it becomes clear that robust, multilingual safety protocols are essential to protect users and maintain linguistic sensitivity across the globe (Wang et al., 2024; Lu and Koehn, 2024).\nA major obstacle to robust multilingual safety lies in the limitations of early tokenizers (Petrov et al., 2023; Hong et al., 2024), which were not designed properly to capture the rich morphological and script diversity in global languages (Ali et al., 2024). As a result, LLMs built on these tokenizers struggle to generate linguistically relevant and accurate outputs in non-English settings, undermining the effectiveness of any safety measures. While newer models incorporate more sophisticated multilingual tokenizers\u00b9, prior efforts largely treated multilingual support as an afterthought added later via fine-tuning rather than integrated as a core capability (Richburg and Carpuat, 2024). This approach often relies on \"bridging strategies,\" such as translating queries into English before applying moderation filters, a practice that can distort content classification (Bang et al., 2023; Lai et al., 2024). Even extensive fine-tuning typically fails to address deeper, English-dominant architectural constraints, especially for languages with multiple scripts or highly complex morphology. Moreover, creating large-scale multilingual datasets for each fine-tuning cycle is prohibitively expensive and time-intensive (Yu et al., 2022). Although scaling up to larger-parameter models can bolster multilin-"}, {"title": "2 Related work", "content": "Mechanistic interpretability: This section explores how internal LLM components (neurons, layers, attention heads) shape model behaviors (Geiger et al., 2021; Stolfo et al., 2023; Gurnee et al., 2023). Early work identified key neurons (Hendrycks, 2023; Chen et al., 2024), but recent studies underscore attention heads' critical roles in various language tasks (Vig, 2019; Wu et al., 2025). Ablation approaches reveal certain heads are crucial for syntactic parsing and factual reasoning (Michel et al., 2019; Meng et al., 2023),\nyet their safety implications remain underexplored (Gould et al., 2023; Wang et al., 2023). This gap highlights the need for fine-grained analysis to enhance transparency and safety.\nSafety alignment: Efforts to ensure LLM safety focus on mitigating adversarial prompts (Xie et al., 2018), designing robust filtering (Xiao et al., 2024), and maintaining dynamic oversight (Kenton et al., 2024; Wang et al., 2024). Early studies (Yao et al., 2024) expose key vulnerabilities and propose ethical risk frameworks. Subsequent work (Sachdeva et al., 2025; Banerjee et al., 2024a) reveals how subtle prompt manipulations can evade safeguards, prompting research into attack strategies (Wolf et al., 2024) and defenses like RAIN (Li et al., 2023). Others emphasize dynamic monitoring (Bhardwaj et al., 2024) and adaptive safety mechanisms, including safety arithmetic (Hazra et al., 2024b) for test-time alignment and SafeInfer (Banerjee et al., 2024b), SafeDecoding (Xu et al., 2024) for decoding-time alignment."}, {"title": "3 Methodology", "content": "In this section, we present our methodology for identifying and mitigating harmful behavior in LLMs. We first introduce the underlying components of autoregressive LLMs (Section 3.1), focusing on their transformer decoder layers and attention mechanisms. We then describe our framework (Section 3.2) for identifying important attention heads that are crucial for task-solving and language-specific processing, followed by the procedure to remove harm-inducing directions from these heads."}, {"title": "3.1 Preliminaries", "content": "We define an autoregressive LLM as M, which comprises multiple transformer decoder layers, denoted by L. Each transformer decoder layer consists of two fundamental modules - multi-head attention (MHA) and feed-forward network (FFN). The outputs of MHA and FFN modules in layer $l \\in L$ are denoted by $atn^l$ and $mlp^l$, respectively. The hidden state of a transformer decoder layer $l$ is denoted by $h_{tl}$. The hidden state $h_{tl}$ is computed as shown in Equation 1 where $h_{tl-1}$ represents the hidden state from the previous layer $l-1$.\n$h_{tl} = h_{tl-1} + mlp^l + atn^l $ (1)\nMathematically, the output $atn^l$ of MHA module is further obtained using Equation 2 in which each attention head is represented as $h_i$ where $i \\in I$ denotes the $i^{th}$ attention head and $|I|$ denotes the"}, {"title": "3.2 Our framework", "content": "In our framework, we first identify important attention heads (i.e., $atn_i^l$ for the $i^{th}$ head) and subsequently remove the harm direction from the target model.\nIdentifying important attention heads: Our objective is to identify attention heads that contribute to both task-solving and language-specific processing. To analyze the role of attention heads in task completion across languages, we translate all tasks into a specific language l. Unlike prior approaches (Tang et al., 2024), we emphasize task relevance to ensure that the identified heads capture task-specific linguistic information. Following (Todd et al., 2024), each task t comprises a dataset containing a set of prompts, denoted by $P_t$. A prompt $p_t \\in P_t$ is represented as $P_t = [(q_{k1}, r_{k1}),...,(q_{kK}, r_{kK}), q_{kQ}]$, where the target answer $r_{kQ}$ for question $q_{kQ}$ is not included in the prompt. Using this prompt $p_t$, the next token prediction function M(p) ranks the correct answer highest, allowing us to assess the contribution of specific attention heads to both task performance and language processing.\nWe provide the prompt $p_t$ to language model L so that it can predict the correct answer for the question $q_{kQ}$. Our objective is to identify model components with a causal role in multilingual processing during the prediction of $r_{kQ}$. For each attention head $atn_i^l$ and task dataset $P_t$, we compute mean condition activations $\\bar{atn_i^l}$ in Equation 5. In Equation 5, $atn_i^l(p_t)$ is the attention output of prompt $p_t$ for $i^{th}$ attention head.\n$\\bar{atn_i^l} = \\frac{1}{|P_t|} \\sum_{p_t \\in P_t} atn_i^l(p_t)$ (5)\nIn parallel, we have a corrupted prompt $p_t^c$ where the responses are shuffled $p_t^c = [(q_{k1}, r'_{k1}),..., (q_{kK}, r'_{kK}), q_{kQ}]$. Next, we pass the corrupted prompt through the language model L and replace a specific attention head activation $atn_i^l(p_t)$ with the actual mean task conditioned activation $\\bar{atn_i^l}$. We attempt to understand how much the actual task conditioned activation can help to predict the correct answer. Further we measure the causal indirect effect (CIE) toward recovering"}, {"title": "4 Language and dataset", "content": "Languages: Following (Deng et al., 2024a), we consider twelve languages across high-, medium- and low-resource categories. From the high-resource language category, we consider English (En), Chinese (Zh), German (De), French (Fr), and Spanish (Es). For the medium-resource language category, Arabic (Ar), Thai (Th), Bulgarian (Bg), and Hindi (Hi). For low-resource language category, we include Tamil (ta), Bengali (bn), and Telugu (te).\nDatasets: We assess SOTERIA using two established datasets, MultiJail (Deng et al., 2024b) and XSafety (Wang et al., 2024). In addition, we introduce a new multilingual safety dataset XThreatBench, constructed based on the policy violations outlined by Meta (Qi et al., 2023a). A detailed description of each dataset follows. We include the dataset details of XSafety and the corresponding"}, {"title": "5 Experimental setup", "content": "In this section, we first introduce the language models used in our evaluation, selected for their multilingual capabilities and diverse linguistic distributions. Next, we define our evaluation metric, attack success rate (ASR), to quantify safety violations. Subsequently, we describe the jailbreak attack baselines. To benchmark our proposed safety mechanism, we compare it against existing English"}, {"title": "6 Main results", "content": "Here we demonstrate the results from SOTERIA across different languages in Figure 3 and Figure 4.\nResults for different datasets:\nMultiJail: Evaluation of our proposed method SOTERIA across multiple language models demonstrates substantial disparities in adversarial robustness across high-resource, medium-resource, and"}, {"title": "7 Language universals", "content": "We extend our experiments by applying the SOTERIA framework across all languages together, rather than treating each language independently. However to do so, one needs to identify a set of attention heads that are active for all languages, i.e., capturing the universal characteristics of languages, aka language universals (Dryer, 1998). For each language $l \\in L$, we first measure the average indirect effect (AIE) of each attention head, $AIE(atn_i^l)$, and select the top k heads based on these values. We then compile a consensus across languages by identifying the heads that rank in the top k for at least 75% of the languages. This majority-based criterion ensures that we capture heads consistently important across the different languages. Finally, we use this refined set of heads in the harm-direction removal phase, thereby reinforcing the safety alignment in a way that remains robust across all the different languages. We call this version of the model SOTERIAU indicating its universal nature.\nResults: We observe that the SOTERIAU consistently produces lower ASR compared to three base models across all tested languages and model backbones (see Table 1). For example, for the MultiJail dataset, Llama 3.1's ASR in English drops from 43% (base) to 26% (safe), while in Chinese it decreases from 51% to 20%. Similar reductions are observed for Qwen 2 (35%"}, {"title": "8 LLM jailbreaks", "content": "We employ recent jailbreak methods to evaluate the robustness of SOTERIA.\nPOATE (Sachdeva et al., 2025): The POATE jailbreak method manipulates LLMs using contrastive reasoning, subtly reframing harmful queries into their opposites. Unlike direct exploits, it combines adversarial templates to bypass safety measures and trigger unintended responses.\nRefusal direction (Arditi et al., 2024): LLMs' refusal behaviour follows a single identifiable direction in activation space. Removing this refusal direction (RDR) bypasses safety measures, enabling harmful responses, while adding it increases refusals. This discovery led to a white-box jailbreak method using a rank-one weight modification to disable refusals with minimal impact on other functions.\nResults: For both the MultiJail and XThreatBench evaluations for the Llama 3.1 8B model, our strategy consistently yields lower ASR than the baseline jailbreaks, indicating a substantial reduction in the model's vulnerability (see Table 2). In MultiJail, POATE's high threat setting decreases from 0.53 to 0.33, and RDR drops from 0.49 to 0.29. Mid and"}, {"title": "9 ASR vs. % heads probed", "content": "Figure 6 shows how the ASR changes as we vary the percentage of attention heads in the model, for three different resource settings. All three settings initially exhibit their highest ASRs at 25% heads, suggesting that using only a small fraction of heads leaves the model more vulnerable. When the percentage of heads increases to 50%, ASRs drop noticeably across the board, indicating a clear gain in robustness at this midpoint. If we use more than 50% heads, increasingly smaller improvement rates are observed. This shows that after a certain point, adding more heads brings less benefit. Assuming that each layer in a 8B model has ~ 32 heads and there are ~ 32 such layers, we need to probe 0.5 x 32 x 32 = 512 heads. Further the dimension of the corresponding projection matrix $W_O^l$ is ~ 4096 x 128. Thus, roughly the % of heads probed is only $(\\frac{512(heads) \\times 128 (dimension)}{4096 (params)}) \\times 100 \\sim 3%$"}, {"title": "10 Conclusion", "content": "We introduce SOTERIA, a lightweight yet powerful safety alignment method that fine-tunes language-specific \"functional neurons\u201d in multilingual LLMs. By adjusting only a fraction of parameters, SOTERIA effectively curbs policy violations across high-, mid-, and low-resource languages without compromising overall performance. Our XThreatBench dataset, derived from real-world policy violations, demonstrates that this targeted parameter steering outperforms baseline safety approaches. These results highlight the value of language-aware interpretability and the practicality of scalable multilin-"}, {"title": "11 Limitation", "content": "A key limitation of SOTERIA lies in its reliance on per-language functional neuron identification, which requires accurate language segmentation and task-based data in each target language. In practice, resource constraints, limited training data, and complexities in script variation or morphology can reduce the precision of head selection. Moreover, although SOTERIA improves safety across many languages, it does not guarantee comprehensive coverage of every cultural nuance or emergent harmful behavior."}, {"title": "12 Ethical Consideration", "content": "In designing and evaluating SOTERIA, we prioritized responsible data use and clear ethical practices: XThreatBench was curated exclusively from synthetic or publicly available prompts crafted to evaluate harmful scenarios without including any personal or sensitive user data. We aligned our methodology with widely recognized industry norms, ensuring minimal data collection and protecting user privacy. Moreover, we respected the cultural nuances that shape perceptions of harm by incorporating broad content moderation principles from organizations like Meta and OpenAI. By balancing robust multilingual safety mechanisms with careful attention to legitimate expression and cultural diversity, our approach aims to foster a more secure yet equitable AI environment."}, {"title": "A.1 Result for XSafety dataset", "content": "The results presented in Table 3 illustrate the substantial improvements achieved by integrating the SOTERIA framework across a wide range of languages and language models. The comparison between the baseline models (B) and the safe models (S) reveals a significant reduction in unsafe outputs across high-, mid-, and low-resource languages. This consistent improvement underscores the effectiveness of SOTERIA as a robust and scalable solution for mitigating unsafe content generation in multilingual LLMs.\nIn high-resource languages such as English, Chinese, German, French, and Spanish, the impact of SOTERIA is particularly noteworthy. For example, in English, the unsafe output rate for the Llama 3.1 model drops from 0.12 in the baseline to 0.05 with SOTERIA. Similar improvements are observed in Chinese (0.14 to 0.07) and German (0.12 to 0.03), reflecting a substantial reduction in unsafe behavior. The safe versions of models like Qwen 2 and Mistral show comparable improvements, with Qwen 2 reducing the unsafe rate in Chinese from 0.03 to 0.02 and Mistral achieving a reduction in English from 0.11 to 0.03. These results demonstrate that SOTERIA not only improves safety for individual models but also generalizes effectively across different architectures and languages.\nMid-resource languages such as Bulgarian, Hindi, Thai, and Arabic pose additional challenges due to their relatively limited training data. Despite these difficulties, SOTERIA delivers significant reductions in unsafe outputs across all models. For instance, in Bulgarian, the unsafe rate for Llama 3.1 drops from 0.17 to 0.08, a nearly 50% improvement. Similar trends are seen in Hindi, where the rate falls from 0.12 to 0.05, and Thai, with a reduction from 0.11 to 0.05. Qwen 2 also demonstrates strong performance improvements in these languages, particularly in Hindi, where it reduces the unsafe rate to 0.05. Even in Arabic, which presents unique challenges, models like Mistral and Phi 3.5 achieve remarkably low unsafe rates, indicating that Soteria is effective in maintaining safety across diverse linguistic and cultural contexts.\nThe performance of SOTERIA in low-resource lan"}, {"title": "A.2 XSafety (language universal)", "content": "In Table 4 for high-resource languages such as English, Chinese, German, French, and Spanish, the reduction in unsafe outputs is substantial. For example, in English, the unsafe rate for Llama 3.1 drops from 0.12 to 0.06, and in German, it declines from 0.12 to 0.07. Similar improvements are observed across other high-resource languages. Qwen 2 reduces the unsafe rate in French from 0.04 to 0.02 and shows consistent gains across other languages like Chinese and Spanish. Mistral stands out in English, where it brings down the unsafe rate from 0.11 to 0.02. These reductions reflect the precision with which SOTERIA identifies and mitigates unsafe content while maintaining the language models' core functionality.\nThe mid-resource languages \u2013 Bulgarian, Hindi, Thai, and Arabic \u2013 further illustrate SOTERIA'S adaptability. Bulgarian, for instance, sees a significant improvement with Llama 3.1 reducing the unsafe rate from 0.17 to 0.09, and Hindi experiences a similar reduction from 0.12 to 0.07. Mistral also achieved substantial progress in Bulgarian, reducing unsafe outputs to 0.09. These results are a clear indicator that SOTERIA effectively addresses the unique challenges presented by languages with moderately available resources, ensuring more controlled output across different linguistic patterns and complexities.\nIn low-resource languages such as Bengali, Telugu, and Tamil, where limited data often results in higher baseline unsafe rates, Soteria continues to deliver meaningful reductions. Llama 3.1 reduces the unsafe rate in Bengali from 0.13 to 0.08, while Telugu sees an improvement from 0.11 to 0.05. Tamil shows equally promising results, with multiple models significantly lowering unsafe outputs. Notably, Mistral reduces the unsafe rate in Tamil to 0.01, demonstrating that SOTERIA can extend its impact even to data-scarce settings without requiring extensive retraining or language-specific adjustments.\nOverall, the results highlight SOTERIA's capacity to improve model safety at scale, offering a practical and efficient approach to reducing unsafe outputs across languages with diverse resource levels. The consistent reduction in unsafe rates across models and languages indicates that SoTERIA is not only scalable but also robust in its generalization across"}, {"title": "B Attention head patterns and their implications", "content": "One intriguing characteristic of LLMs is how their top-valued language-specific attention heads tend to cluster by resource level of the language. Analyses of a smaller-parameter model (e.g., Llama 3.1 8B-parameter variant) reveal that high-resource languages (such as English, Chinese, Spanish, German, and French) and mid-resource languages (such as Hindi, Arabic, Thai, and Bulgarian) exhibit peak attention heads in roughly the same mid-level layers (e.g., layers 12\u201320 with head indices 16\u201324). Meanwhile, for low-resource languages the strongest attention heads manifest in later layers (e.g., layers 28-31 with head indices 15-23) (see Figure 7).\n(1) Language-specific universal heads: Despite the differences in where each language's top heads appear, some heads consistently contribute to cross-lingual understanding \u2013 the so-called \u201cuniversal\u201d heads. Identifying and enhancing these universal heads can make the model's latent space more cohesive across languages, improving zero-shot or few-shot performance for underrepresented languages.\n(2) Future directions: Beyond raw performance, attention-head analysis also provides new insights to tackle task-specific attention heads, misalignment, and hallucination issues. If certain heads consistently carry problematic correlations, shifting or refining their latent space (\"steer them to a safe side\") can enhance overall alignment and trustworthiness.\nThese findings underscore the delicate interplay between multilingualism and architectural depth in multilingual models. By homing in on the most influential heads and understanding why they appear where they do, we gain powerful levers for improving cross-lingual performance, minimizing unsafe content generation, and facilitating more robust language support, even for the world's most resource sparse tongues."}]}