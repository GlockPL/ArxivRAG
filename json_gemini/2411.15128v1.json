{"title": "Health AI Developer Foundations", "authors": ["Atilla P. Kiraly", "Sebastien Baur", "Kenneth Philbrick", "Fereshteh Mahvar", "Liron Yatziv", "Tiffany Chen", "Bram Sterling", "Nick George", "Fayaz Jamil", "Jing Tang", "Kai Bailey", "Faruk Ahmed", "Akshay Goel", "Abbi Ward", "Lin Yang", "Andrew Sellergren", "Yossi Matias", "Avinatan Hassidim", "Shravya Shetty", "Daniel Golden", "Shekoofeh Azizi", "David F. Steiner", "Yun Liu", "Tim Thelin", "Rory Pilgrim", "Can Kirmizibayrak"], "abstract": "Robust medical Machine Learning (ML) models have the potential to revolutionize healthcare by accelerating clinical research, improving workflows and outcomes, and producing novel insights or capabilities. Developing such ML models from scratch is cost prohibitive and requires substantial compute, data, and time (e.g., expert labeling). To address these challenges, we introduce Health AI Developer Foundations (HAI-DEF), a suite of pre-trained, domain-specific foundation models, tools, and recipes to accelerate building ML for health applications. The models cover various modalities and domains, including radiology (X-rays and computed tomography), histopathology, dermatological imaging, and audio. These models provide domain specific embeddings that facilitate AI development with less labeled data, shorter training times, and reduced computational costs compared to traditional approaches. In addition, we utilize a common interface and style across these models, and prioritize usability to enable developers to integrate HAI-DEF efficiently. We present model evaluations across various tasks and conclude with a discussion of their application and evaluation, covering the importance of ensuring efficacy, fairness, and equity. Finally, while HAI-DEF and specifically the foundation models lower the barrier to entry for ML in healthcare, we emphasize the importance of validation with problem- and population-specific data for each desired usage setting. This technical report will be updated over time as more modalities and features are added.", "sections": [{"title": "1. Introduction", "content": "Machine learning (ML) models, trained on diverse data ranging from genomic sequences to clinical images, have the potential to transform healthcare in applications ranging from accelerating drug discovery to enabling personalized diagnoses. In day-to-day clinical workflows, ML models have also been developed to help automate manual processes, assist with triage, diagnosis, or prognosis, and more, with the goal of helping improve quality of care or efficiency.\nHowever, building robust ML models for these domains entails challenges. Development often requires large, labeled datasets, which are expensive and time-consuming to create and curate. Beyond cost, sharing these datasets across institutions is often restricted due to privacy and other considerations. Data scarcity, especially for rare conditions and underrepresented populations, further hinders dataset curation and limits generalizability. Finally, significant compute resources are often necessary to train large models or when utilizing large datasets, and clinical and technical modality-specific expertise, especially in the use of DICOMs for pathology and radiology, is often needed to correctly prepare data for ML models.\nTo help address these challenges, we present Health AI Developer Foundations (HAI-DEF), with the goal of catalyzing the development and adoption of AI in healthcare. HAI-DEF includes foundation models, tooling, recipes, and ready-to-use research endpoints. These resources were created to enable researchers and developers to both iterate on research ideas quickly and have a lower-friction path to incorporating AI in real-world use settings. In the initial phase, we offer research endpoints and open-weight models, enabling generation of high-quality embeddings from medical images (chest X-rays (CXR), histopathology patches, skin images, computed tomography (CT) images) and audio recordings (health acoustics like coughs and breaths). These embeddings offer a compact yet information rich representation of the given data. By leveraging these embeddings and tooling, researchers and developers can build robust models that perform well across diverse clinical settings with significantly less labeled data, shorten model training times, and reduce computational costs."}, {"title": "2. Models", "content": "HAI-DEF encompasses multiple distinct models, each tailored to a specific use case and trained using advanced techniques on large, diverse datasets. The following list is a snapshot of our current progress, and our goal is to expand this model set in the near future.\nHAI-DEF encompasses multiple distinct models, each tailored to a specific data modality and trained using various techniques on large, diverse datasets. The following list snapshots our current available models, which we expect to expand in the near future. More information about how to access these models can be found at the HAI-DEF developer site (for CXR, Path and Derm Foundation) and respective GitHub repositories for the research endpoints (HeAR and CT Foundation)."}, {"title": "2.1. CXR Foundation", "content": "CXR Foundation (Sellergren et al., 2022) is a set of 3 models, all using an EfficientNet-L2 (Xie et al. (2020)) image encoder backbone. The three models learned representations of CXRs by leveraging both the image data and the clinically relevant information available in corresponding radiology reports."}, {"title": "2.2. Path Foundation", "content": "Path Foundation (Lai et al., 2023) is a Vision Transformer (ViT) (Dosovitskiy et al., 2020) encoder for histopathology image patches trained with self-supervised learning (Masked Siamese Networks, Assran et al. (2022)). It incorporates pathology-specific optimizations, including approaches to help learn stain-agnostic features and to generalize across patch magnifications. Training for this model utilized hematoxylin and eosin (H&E) stained whole slide images (WSIs) from The Cancer Genome Atlas (TCGA) (National Cancer Institute (NCI), 2024)."}, {"title": "2.3. Derm Foundation", "content": "Derm Foundation (Rikhye et al., 2024) is a BiT ResNet-101x3 (Kolesnikov et al., 2019) image encoder trained using a two-stage approach on over 16K natural and dermatology images (Liu et al., 2020). First, contrastive learning (ConVIRT, Zhang et al. (2020)) was applied on a large number of image-text pairs from the internet (Sun et al. (2017)). Second, the model was fine-tuned to identify dermatology conditions on a mix of datasets, including tele-dermatology and skin cancer datasets."}, {"title": "2.4. HeAR", "content": "HeAR (Baur et al., 2024) is a ViT audio encoder trained using a Masked Autoencoder (MAE) approach (He et al., 2021) on a large dataset of 313 million unlabelled, non-medical audio clips. The model learns to reconstruct masked spectrogram patches, capturing rich acoustic representations of health-related sounds like coughs and breathing patterns."}, {"title": "2.5. CT Foundation", "content": "CT Foundation provides embeddings suitable for downstream classification tasks. The underlying model is VideoCoCa (Yan et al., 2023), a video-text model designed for efficient transfer learning from 2D Contrastive Captioners (CoCa) (Yu et al., 2022). CT Foundation was trained on CT volumes and radiology reports from over 500,000 cases across different anatomic regions, using a similar approach to Yang et al. (2024)."}, {"title": "3. Evaluations", "content": "We evaluated the efficacy of our domain-specific foundation models on a suite of data-efficient classification tasks, benchmarking their performance against generic models when possible. Across various levels of training data subsampling, classifiers leveraging foundation model embeddings consistently outperformed those using generic embeddings, demonstrating superior data efficiency.\nBeyond data efficiency, our foundation models demonstrate strong generalization capabilities across diverse tasks within their respective domains. The CXR foundation model (ELIXR-B) achieved strong performance on tasks spanning classification, semantic search, visual question answering, and report quality assurance (Xu et al., 2023). The Derm foundation model effectively handles data covering 419 skin conditions, with subgroup analysis revealing no statistically significant performance difference across Fitzpatrick skin types (Rikhye et al., 2024). HeAR, our health acoustics model, generalizes better when tested on audio recordings from unseen devices, compared to other strong audio encoders (CLAP (Elizalde et al., 2023), TRILL (Shor et al., 2020), FRILL (Peplinski et al., 2020), BigSSL-12 (Zhang et al., 2022)), signifying its robust generalization capabilities (Baur et al., 2024). Finally, the Path foundation model exhibits strong performance on a wide range of tasks encompassing 17 unique tissue types and 12 cancer types, including tumor detection, grading, subtyping, and tissue type classification (Lai et al., 2023)."}, {"title": "3.1. CXR Foundation", "content": "Area under the curve (ROC AUC) metrics with both linear and non-linear models applied to CXR embeddings were evaluated. On public datasets, such as ChestX-ray14 and CheXpert, results improved the data-accuracy trade-off for models developed across a range of training dataset sizes and several findings. When evaluating the original CXR Foundation's ability to develop tuberculosis models, data efficiency gains were more striking: models trained on the embeddings of just 45 images achieved non-inferiority to radiologists in detecting tuberculosis on an external validation dataset (Sellergren et al., 2022). For both tuberculosis and severe COVID-19 outcomes, we have shown that non-linear classifiers trained on CXR Foundation embeddings outperformed a model that was fine-tuned on the entire dataset (Sellergren et al., 2022)."}, {"title": "3.3. Derm Foundation", "content": "To evaluate Derm Foundation, we trained and evaluated classifiers for ten tasks. We used the SCIN (Skin Condition Image Network) Ward et al. (2024) dataset to train and test these classifiers. SCIN is an open access dataset created by generating representative images from internet users with the goal of providing an accessible diverse dermatology dataset. The results are presented in Figure 5."}, {"title": "3.4. HeAR", "content": "HeAR has been trained on a large collection of short (2 seconds, sampled at 16kHz, mono-audio) clips of sounds made by humans, such as coughing, breathing, or speaking. No segmentation or padding was used. The audio typically contains background noise, which helps make the encoder robust to such noise.\nBy reconstructing masked sounds, it learns meaningful representations that capture underlying patterns useful for downstream tasks. This embedding model can be used to explore the possibility of developing classifiers for health-related non-semantic acoustics. Examples of such tasks include detecting tuberculosis using cough sounds, detecting dementia using non-semantic voice patterns, and detecting air exhalation volume from exhalation sounds.\nWe observed in Baur et al. (2024) that HeAR tends to perform better than available competing audio encoders for such tasks, while typically needing less data, as shown in Figure 6. Deployment and test-time scenarios are typically very different from lab tests. For health acoustics, it typically means that the recording devices and data collection protocols can be different. Deep learning models can be brittle to such distribution shifts, and yet we observed that HeAR is more robust to such changes."}, {"title": "3.5. CT Foundation", "content": "CT Foundation was trained using three-dimensional computed tomography (CT) scans and corresponding radiology reports across different study types, including Head/Neck, Neck, Spine, Heart, Angiography, Chest, Abdomen and Pelvis, and Extremities. To test CT Foundation's utility and generalizability, we evaluated the embeddings across seven classification tasks using multilayer perceptron models with increasing amounts of training data. Tasks were diverse, spanning head, chest, and abdominopelvic regions, each involving the detection of abnormalities. These tasks were related to classifying intracranial hemorrhage, calcifications in the chest and heart, lung cancer prediction in the chest, suspicious abdominal lesions, nephrolithiasis, and abdominal aortic aneurysm in abdominopelvic CTs. All binary labels with the exception of the lung cancer and hemorrhage tasks were automatically extracted from the clinical radiology reports, which were written by board certified radiologists. The lung cancer prediction task was drawn from NLST and used pathology-confirmed cancer outcomes within 2 years of the lung screening task for cancer positive labels. The hemorrhage task was further verified and labeled by board certified radiologists. The evaluation set to measure the performance was held constant. shows the results in ROC AUC and accuracy for these tasks. All but one of the ROC AUC measured tasks achieved a score of 0.8 or greater."}, {"title": "4. Discussion", "content": "The ML models within HAI-DEF have been made available for research use over the last 3 years as research endpoints where users can upload images and receive embeddings. HAI-DEF now expands this collection of research endpoints with open-weight models and containerized per-modality solutions that can be deployed as endpoints in user-managed environments for CXR, Path and Derm Foundation. This unlocks use cases beyond research, and enables use on datasets that cannot be processed by a Cloud system due to privacy, institutional policies, or other considerations.\nThese foundation model research endpoints have been adopted by many researchers, with millions of API calls as of this writing. Users, including both machine learning researchers and clinicians, have utilized these research endpoints to explore a diverse range of applications and have found them valuable for improving performance and building models quickly for research purposes. Use cases advanced by researchers include:using Path Foundation to help distinguish different types of sarcoma at University College London; and using CXR Foundation for identifying necrotizing enterocolitis on neonatal radiology images at Guys St. Thomas Trust.\nAs we expand the routes by which foundation models can be consumed, it is important to consider various trade-offs. Using model weights directly is the most flexible way to consume the models; for example, the models can be leveraged as part of existing ML software infrastructure or as a constituent model in an ensemble for real time use. However, this requires developers to process the data into a format that models expect (which might differ between different models and modalities). In this regard, consuming the models via endpoints that provide additional preprocessing logic may make it easier to fetch and process data for inference. Deploying endpoints on cloud infrastructure (for instance as a Google Cloud Vertex AI endpoint) also offers scalability without needing researchers or infrastructure staff to manage the complexity of ensuring robustness to fluctuating usage volumes.\nData use restrictions are another factor that drives differences between the open-weights versus research endpoint approaches. Google-maintained research endpoints can only be used for research scenarios and with de-identified data. This may render it unsuitable for use cases with strict data locality requirements. On the other hand, certain datasets have usage terms that prohibit direct availability of the downstream trained models, or are associated with other sensitivities that might make releasing model weights infeasible. The endpoint approach can respect these restrictions while providing an alternative route to enabling downstream use by other researchers. Finally, another route combines some elements of both open-weights and endpoint approaches: user-deployed endpoints (using the ready-to-deploy containers on Google Cloud's Model Garden) have the scalability and preprocessing benefits of research endpoints, in addition to meeting the data locality and sensitivity requirements for some users.\nWith regards to downstream use cases, even though embedding models have been trained on a large diverse datasets (a compute- and data-intensive process), making them useful for specific tasks should always be done via validation and/or tuning on data specific to the problem and patient population of interest. This fine-tuning can be especially important for rarer examples that the embedding models may not have \u2018seen' many of during development. For many applications, we and other researchers observe that using a foundation model can reduce the amount of data needed (i.e., in the low-data regime). In some instances, we additionally find benefits (such as generalization) in a higher data regime. For example, we have found HeAR to display more robust performance when testing on a new audio collection smartphone.\nReducing bias and promoting fairness and equity are important when using ML in healthcare applications. Fine-tuning with local data specific to the use case, patient population, data acquisition protocol or devices is likely essential to achieving higher performance. Embedding models, by reducing barriers to entry can democratize use of AI in low resource settings. Crucially, Weng et al. (2024) emphasizes that regardless of the model's origin (a foundation model or otherwise), rigorous evaluation of the final downstream model is necessary to guarantee fairness and mitigate potential biases. We are also looking forward to community feedback and lessons learnt applying these models in a diverse range of tasks. One interesting but challenging goal is to incorporate feedback or additional datasets into the foundation models for improvements and further use by the community.\nBy establishing a positive feedback loop, these models could improve over time and become even more useful for all users.\nLike all ML models, models provided in HAI-DEF have limitations to consider. Though individual details differ, each model in HAI-DEF has been trained on varying amounts of data based on availability; please refer to each associated manuscript for more details. As discussed above, we emphasize the importance of rigorous evaluation per use case and population, to improve overall performance and address issues of bias, fairness, and equity. Finally, while we strove to make the models useful for a variety of use cases, some applications may need further development. For example, the models were developed with a focus on classification tasks, and prognosis tasks will need to be further evaluated. Image segmentation and generation tasks are also currently not supported. Further, specific requirements such as smaller models (e.g. for on-device applications on a mobile device) or lower latency will need other techniques such as distillation to the target model size of interest."}, {"title": "A. Appendix", "content": "At its highest magnification, digital pathology images are gigapixel-sized, measuring in the tens to hundreds of thousands of pixels per slide. The Path embeddings model has been trained to produce high quality machine learning embeddings for image patches of size 224 \u00d7 224 pixels cropped from pathology images. At high magnification, these image patches represent a relatively small portion of the slide, and enable the representation of intra-slide heterogeneity in different parts of the tissue specimen. However, computing patch embeddings across such high magnification pathology images can require the generation of tens of thousands of embeddings for a single whole slide image.\nThe endpoint that was used for evaluations runs as a deployable online prediction REST service within Vertex AI and can generate embeddings for data stored both within Cloud (Cloud DICOM store or Google Cloud Storage) and from external data sources (i.e., sending the data as part of the request). The REST service supports requests that may contain multiple patch embeddings per-data-source and/or multiple data sources. As with all Vertex AI prediction services the service can be configured to horizontally scale to meet demand. To generate embeddings, the endpoint will: 1) retrieve the imaging necessary to complete the embedding generation, 2) generate the embedding, and 3) return the embeddings to the caller. For Cloud data sources, it is optimal to co-locate the Vertex AI endpoint hosting embedding generation in the same data center that hosts the images in order to improve data transport efficiency.\nWhen embeddings are generated from data not stored in Cloud, data are sent directly within the request as a base64 encoded compressed image. This increases the amount of imaging data that can fit within the request. The endpoint supports decoding commonly used compression methods (e.g., lossy JPEG, PNG for lossless data transmission, etc.). Regardless of exactly how imaging is encoded, on receipt, the endpoint extracts the pixel data directly from the request and then generates and returns the embedding result. Multiple factors affect the time required to generate embeddings and the associated cost, including: the compute (CPU and GPU) that backs the Vertex AI embedding generation service, the data source (e.g., DICOM store, Google Cloud Storage, or external data), the image compression format, and the total size of the embedding request.\nDICOM embeddings were generated from non-overlapping regions of a DICOM VL Whole Slide Microscopy Image. The DICOM instance was encoded using: 256 \u00d7 256 pixel frames that were encoded using JPEG baseline transfer syntax and TILED_FULL organization. Google Cloud Storage embeddings were generated from JPEG images saved to Google Cloud Storage. Image dimensions matched the embedding model patch dimensions (224 \u00d7 224 pixels) to enable stored images to be"}, {"title": "A.1. Background and Methodology For Path Foundation Evaluations", "content": "Health Al Developer Foundations"}]}