{"title": "GASP: Efficient Black-Box Generation of Adversarial Suffixes for Jailbreaking LLMs", "authors": ["Advik Raj Basani", "Xiao Zhang"], "abstract": "Large Language Models (LLMs) have shown impressive proficiency across a range of natural language processing tasks yet remain vulnerable to adversarial prompts, known as jailbreak attacks, carefully designed to elicit harmful re-sponses from LLMs. Traditional methods rely on manual heuristics, which suffer from limited generalizability. While being automatic, optimization-based attacks often produce unnatural jailbreak prompts that are easy to detect by safety filters or require high computational overhead due to discrete token optimization. Witnessing the limitations of existing jailbreak methods, we introduce Generative Adversarial Suffix Prompter (GASP), a novel framework that combines human-readable prompt generation with Latent Bayesian Optimization (LBO) to improve adversarial suffix creation in a fully black-box setting. GASP leverages LBO to craft adversarial suffixes by efficiently exploring continuous em-bedding spaces, gradually optimizing the model to improve attack efficacy while balancing prompt coherence through a targeted iterative refinement procedure. Our experiments show that GASP can generate natural jailbreak prompts, significantly improving attack success rates, reducing train-ing times, and accelerating inference speed, thus making it an efficient and scalable solution for red-teaming LLMs.", "sections": [{"title": "1. Introduction", "content": "LLMs represent a groundbreaking advancement in technology with a remarkable capacity to master a diverse set of skills, revolutionizing many areas ranging from natural language processing to code and content generation [34, 56, 70, 74]. However, the versatility and strong generative capability of LLMs are accompanied by a critical challenge: the potential for producing toxic content [7, 20]. It has been repeatedly reported that LLMs may inadvertently assimilate and perpetuate harmful biases, stereotypes and hate speech, which are often embedded in the extensive corpora of text used to train these models, raising serious ethical and societal concerns [13, 36, 42]. The proliferation of such toxic content can facilitate the spread of misinformation, exacerbate polarization, and contribute to harassment, thereby undermining the beneficial impact of LLMs [5, 10, 68].\nTo reduce the likelihood of generating harmful content, researchers and developers have investigated various strategies to improve the safety alignment of LLMs with predefined values [1, 49, 64]. These strategies involve red teaming [18] and fine-tuning LLMs on carefully curated datasets that emphasize positive values, ethical principles, and fac-tual accuracy [4, 28, 51]. While these methods can enhance the overall trustworthiness of LLM outputs, malicious actors can easily craft adversarial prompts to elicit harmful responses. These specially crafted prompts are known as jailbreak attacks [57, 66], which exploit model sensitivity to input perturbations to trick LLMs into generating toxic content despite employing safety-alignment mechanisms.\nTo better characterize the vulnerabilities of LLMs, ex-isting research studies have proposed different methods to craft jailbreak prompts, which primarily fall into heuris-tic and optimization-based categories. Starting from [26, 58, 66], heuristic methods design hand-crafted prompts to bypass the model safety guardrails based on intuitive techniques, such as role-playing [30], low-resource lan-guage rewriting [71], encryption [73], in-context demon-stration [67], prompt manipulation and injection [21, 45, 48, 52]. Despite being highly flexible, these methods re-quire significant manual efforts for prompt engineering and tend to be inconsistent across different inputs and models, thus limited in applicability. In contrast, optimization-based methods [11, 50] employ automatic search techniques by leveraging model gradients or logits to uncover prompts that are likely to elicit harmful LLM responses, offering a more robust and systematic solution. Nevertheless, these opti-"}, {"title": "2. Preliminaries", "content": "In this work, we focus on generating adversarial suffixes for jailbreaking LLMs. This section introduces formal mathe-matical notations and foundational concepts to help readers better understand suffix-based jailbreak attacks.\nAdversarial Objective. The goal of jailbreak attacks is to elicit harmful responses from a target LLM. Specifically, let  $f_\\theta : X \\rightarrow Y$  be a given black-box LLM, denoted as TargetLLM throughout the paper, that the adversary aims to jailbreak. Here, X is the input space of textual prompts, Y is the output space of LLM responses, and  $\\theta$  denotes the model parameters potentially involved by some safeguard fine-tuning techniques to align the model behavior with pre-defined values. Let  $x \\in X_{harm}$  like \"how to build a bomb,\" where  $X_{harm}$  is a collection of harmful prompts or those de-signed to test the safety mechanisms of  $f_\\theta$. Due to the safety alignment, the model response  $f_\\theta(x)$  typically includes re-fusals, such as \"Sorry, I cannot assist with that request,\" to react to the harmful content or unethical intent embedded in the query prompt. This ensures the model satisfies the safety rules even when prompted by harmful prompts. The goal of suffix-based jailbreak attacks is to search for an op-timal sequence of tokens  $e$, denoted as an adversarial suf-fix [76], such that when attached to the original input x, the response elicited from TargetLLM  $f_\\theta(x + e)$  will become"}, {"title": "3. Proposed Method: GASP", "content": "In this section, we elaborate on the design insights and key modules of the proposed framework GASP, including the goal of SuffixLLM (Section 3.1), the role of AdvSuffixes (Section 3.2), and how LBO and OPRO are employed to solve the above-mentioned challenges (Section 3.3). The working mechanism of GASP is illustrated in Figure 1."}, {"title": "3.1. Introducing SuffixLLM", "content": "To generate suffixes that align with the targeted adversarial objective (Equation 1) and satisfy the human readability constraint (Equation 2), we propose to create a specialized LLM, referred to as SuffixLLM,  $g_\\phi : X \\rightarrow \\mathcal{E}$. This model can be specifically trained to generate coherent adversarial suffixes  $g_\\phi(x)$  across various input prompts  $x \\in X_{harm}$  for a designated TargetLLM  $f_\\theta$.\nTo effectively engage with TargetLLM, the objective of SuffixLLM centers on approximating the conditional probability  $p_\\theta (y \\mid x + e)$, which reflects the likelihood of TargetLLM generating desirable harmful responses  $y \\in Y$  when presented with a crafted suffix  $e$  appended to the prompt x. The primary goal is to exploit the spe-cific vulnerabilities of Target LLM, allowing it to generate suffixes that increase the chances of eliciting harmful responses that align closely with the target model's inher-ent behavior. By learning to replicate the response distribu-tion of TargetLLM, SuffixLLM operates under a multi-objective optimization framework (Equation 3) that empha-sizes both the effectiveness of adversarial prompts and their coherence in human language. This involves understand-ing the specific output tendencies of the target model, al-lowing SuffixLLM to craft suffixes that elicit harmful responses in ways that appear indistinguishable from typ-ical conversations. Ultimately, the successful alignment of SuffixLLM's generated suffixes with TargetLLM's harmful response distribution not only increases the effec-tiveness of the adversarial strategy but also preserves the naturalness required for human evaluators. These features enable our method to generate adversarial suffixes with high jailbreak success for any input prompt x and TargetLLM using only black-box model information (i.e., raw outputs). In contrast, most existing optimization-based jailbreak methods [43, 76] are white-box attacks, which first create universal adversarial suffixes by leveraging the gradient information with white-box surrogate LLMs then transfer to TargetLLM. An exception is AdvPrompter [50] that uses log probability outputs of the target LLM for suffix opti-mization, but still is gray-box."}, {"title": "3.2. Pre-training on AdvSuffixes", "content": "Establishing a baseline distribution is essential for guiding effective prompt generation, as it provides a reference point that captures a generic probability distribution of harmful responses across various adversarial suffixes. By training SuffixLLM on a baseline distribution, we aim to initially approximate the conditions under which an LLM might pro-duce harmful outputs, without yet targeting the specific vul-nerabilities of a particular model.\nThis initial phase relies on AdvSuffixes, a curated dataset of adversarial suffixes for diverse prompts, designed to cap-ture typical patterns in harmful responses. Using a two-shot prompting technique on an uncensored model, we gener-ated adversarial suffixes aligned with well-known jailbreak styles, which are proven to achieve high attack success rates across models. Inspired by AdvBench [76], this approach enabled us to create a dataset of suffixes that establishes a baseline distribution of adversarial prompts. More details on the creation of AdvSuffixes are provided in Appendix B.\nLeveraging AdvSuffixes allows us to approximate a generic adversarial distribution  $p_{gen} (y \\mid x + e)$, where y represents possible harmful outputs with respect to the input prompt x, aiming to capture a broad range of adversarial scenarios and lay the foundation of our GASP framework (Module A in"}, {"title": "3.3. Aligning Distributions with TargetLLM", "content": "The baseline distribution generated by SuffixLLM after pre-training on AdvSuffixes might be too generic to be ef-fective in jailbreaking a particular Target LLM. Therefore, we propose to fine-tune SuffixLLM to align the generic adversarial distribution  $p_{gen} (y \\mid x + e)$ with the desir-able harmful response distribution  $p_\\theta(y \\mid x + e)$, specifically for Target LLM. Nevertheless, a fundamental chal-lenge lies in the difficulty of calculating loss function value"}, {"title": "Embedding Space", "content": "We propose to approximate the tar-get distribution by refining and searching for suffixes iter-atively, using observed responses to inform adjustments to-ward  $p_\\theta (y \\mid x + e)$. Note that each token sequence variation [65] can lead to abrupt changes in the model's be-havior, making the search inherently discontinuous. To ad-dress this, we transition to the embedding space defined by the SuffixLLM, where token sequences are represented as continuous vectors. Each suffix e is mapped to a con-tinuous vector  $z_e \\in \\mathbb{R}^d$, allowing for gradual adjustments in language meaning and structure, which facilitates opti-mization. In the following, we will employ LBO to search an optimal suffix embedding vector [60, 62], since evalu-ating all token combinations generated by SuffixLLM is computationally infeasible. By operating in the embedding space, LBO reduces the search complexity, allowing for a more structured exploration of potential suffixes, which in turn facilitates a smoother and more efficient alignment with Target LLM's response distribution."}, {"title": "Latent Bayesian Optimization (LBO)", "content": "As stated, LBO leverages the continuous embedding space generated by SuffixLLM to enable efficient black-box optimization. By focusing on promising candidates, LBO minimizes the search complexity, improving the exploration of suffixes. To enhance the interpretability of the high-dimensional space, we use t-SNE [63] to project  $z_e$  into a lower-dimensional representation, preserving key variations in to-ken sequence behavior. This approach aligns our search with the response distribution of TargetLLM, ensuring a more efficient optimization process.\nSpecifically, the objective of LBO is to identify the op-timal suffix embedding z that maximizes the likelihood of producing a harmful response from the Target LLM, while satisfying the human readability constraint:\n $z^* = \\underset{z_e \\in \\mathbb{R}^d}{\\text{argmax }} p_{\\theta}(y \\mid x + e), \\text{ s.t. } P_{nat}(x + e) > \\lambda$.    (5)\nLBO leverages probabilistic exploration [47] across the em-bedding space, assessing candidate embeddings by evaluat-ing their likelihood to improve adversarial outcomes. This probabilistic approach relies on posterior predictions from a Gaussian Process (GP) model, which is continuously up-dated with each embedding evaluation. By refining its pos-terior with each new data point, the GP model helps LBO"}, {"title": "GASPEval", "content": "A crucial component of the LBO process is the acquisition function, which governs the balance between exploring unknown areas of the embedding space and ex-ploiting known regions that show promise. However, a successful optimization process also requires an accurate evaluation of whether a given suffix elicits a truly harm-ful response, which brings us to the challenge of response evaluation. To address the challenge of accurately eval-uating whether a suffix has successfully induced a harm-ful response from the TargetLLM, we introduce a novel evaluator GASPEval, which leverages a secondary model called JudgeLLM. This evaluator is designed to assess the TargetLLM's responses with precision, determining not only if the response contains harmful content but also en-suring it adheres to the adversarial intent while maintaining coherence and readability.\nOur GASPEval utilizes a diverse set of 21 rigorous eval-uation questions specifically crafted to cover a wide spec-trum of harmful behaviors, ranging from subtle misinfor-mation to overtly harmful content like profanity, threats, or"}, {"title": "Odds Ratio Prompt Optimization (ORPO)", "content": "The integra-tion of LBO with SuffixLLM and the use of GASPEval forms a powerful framework for creating highly effective adversarial suffixes tailored to exploit vulnerabilities in the TargetLLM. To enhance SuffixLLM, we incorporate targeted fine-tuning based on the LBO-driven search re-sults, further supported by ORPO [23]. ORPO's use of an odds ratio-based penalty refines the fine-tuning process by aligning the model outputs with specific preferences. Unlike traditional methods focusing solely on maximizing the likelihood of target responses, ORPO introduces a rel-ative ratio loss component that increases the chances of generating adversarial suffixes while ensuring they remain human-readable. This refined fine-tuning process enables SuffixLLM to prioritize the creation of adversarial suf-fixes that are more likely to bypass safety mechanisms in TargetLLM while maintaining the suffix coherence and closely adhering to the desired adversarial characteristics.\nSummary. The combination of SuffixLLM, LBO, and ORPO establishes a robust, adaptive approach to aligning the generic adversarial distribution with the Target LLM's specific response tendencies. We leverage LBO and GASPEval to guide the search within the embedding space, iteratively identifying and refining suffix embed-dings  $z_e$. Through LBO, the GP model is aligned with the Target LLM, providing probabilistic predictions that op-timize the likelihood of generating harmful responses. As a result, SuffixLLM dynamically learns from the evolv-ing GP distribution, progressively refining the suffix embeddings  $z_e$  to more accurately approximate  $p_\\theta$. This is achieved through the integration of ORPO, which modi-fies the baseline distribution  $p_{gen}$  into a refined distribution  $p_{align}(y \\mid x + e)$, then progressively optimized to match the target  $p_\\theta (y \\mid x + e)$  as closely as possible. Through LBO and ORPO, SuffixLLM not only learns to generate adversarial suffixes more effectively but also becomes in-creasingly capable of operating within the target's unique response framework, resulting in a finely tuned model that continuously adapts to the target's specific vulnerabilities. Detailed explanations of the design insights of LBO and ORPO are provided in Appendix D."}, {"title": "4. Experiments", "content": "Data. Inspired by the AdvBench [76] dataset, we developed AdvSuffixes, a benchmark containing 519 harmful instruc-tions across a wide range of toxic content, including pro-fanity, graphic depictions, threats, misinformation, discrim-ination, cybercrime, and illegal suggestions. Each instruc-tion is paired with adversarial suffixes designed to provoke potentially harmful responses from language models while maintaining coherence and readability. AdvSuffixes is a crucial tool for testing the safety guardrails of LLMs, en-abling systematic evaluation of a model's resilience against adversarial attacks. By challenging LLMs with harmful in-structions and targeted adversarial suffixes, the benchmark helps uncover vulnerabilities and assess the effectiveness of safety mechanisms, providing insights into how well our LBO-driven approach and SuffixLLM can exploit these weaknesses. The data is split into fixed pre-train (75%) and train (25%) sets, with results reported on this split. Addi-tionally, we include a supplementary dataset of 100 harm-ful prompts, which is out-of-distribution from the training dataset, to more effectively test the robustness of the frame-works, as detailed in Appendix B.\nModels. In our experiments, we evaluate the effectiveness of our adversarial attack framework across a range of open-source models in a black-box setting, including Mistral-7B-Instruct-v0.3 [29], Falcon-7B-Instruct [2], LLaMA-2-7B-chat [16], LLaMA-3-8B-instruct [17], and LLaMA-3.1-8B-instruct [17]. These diverse architectures and sizes pro-vide a comprehensive assessment of how different models respond to adversarial suffixes generated by SuffixLLM with LBO-driven optimization. To further test generaliz-ability, we extend our evaluation to proprietary models like GPT-40, GPT-40-mini [1] and GPT-3.5-turbo [8].\nBaselines. We compare the performance of GASP with three notable previous jailbreak attacks in our experiments, AdvPrompter [50], AutoDAN [43] and GCG [76]. All three provide implementations, and hence, we utilize a similar set of hyperparameters to evaluate them.\nEvaluation. Our primary evaluation metric to measure the attack performance of jailbreak attacks is ASR@k, which quantifies whether at least one out of k attempts success-fully compromises the TargetLLM. This metric is crucial in practical scenarios, as multiple attempts may be made to jailbreak the Target LLM, and a single successful attack is enough to extract sensitive information. To determine the outcome of each attack, we employ three evaluators: (1) Keyword Matching [76], a simple approach that checks for specific strings indicating the model's refusal in response to harmful query prompts, (2) StrongREJECT [61], which prompts an external LLM with the harmful instruction and the TargetLLM's response, instructing it to score the harmfulness of the output through three questions, and (3) GASPEval, which also employs an external LLM to evalu-ate the harmful instruction and the TargetLLM's response, utilizing a more comprehensive scoring system based on 20"}, {"title": "4.1. Main Results", "content": "In our evaluation, we report the ASR at two levels: ASR@1 and ASR@10. These metrics gauge the effectiveness of our adversarial prompts by determining whether at least one successful attack occurs within a set number of attempts. We strictly adhere to Algorithms 1 and 2 in Appendix C to ensure consistency and rigor in our experiments. The observed lower performance seen in all frameworks, com-pared to results reported in original papers, is likely due to our use of an out-of-distribution evaluation dataset intro-duced by AdvSuffixes. This dataset challenges the frame-works by presenting data that diverges from their training distribution, which is especially effective in revealing the robustness and generalization of each framework under var-ied conditions.\nBlack-box ASR. Table 2 summarizes the comparison re-sults, showing that GASP achieves comparatively high at-tack success rates across various models, with notable per-formance improvements over prior techniques. Specifi-cally, GASP achieves high ASRs, especially on LLaMA-2-7b, LLaMA-3-8b, and LLaMA-3.1-8b, demonstrating our framework's adaptability to different LLM architectures. In addition, we evaluate GASP against closed-source LLMs, where it continues to maintain robust ASR scores, even with limited access to model internals. Table 3 showcases GASP's performance against closed-API models, achieved with a remarkably low total cost of just $3. This includes"}, {"title": "4.2. Ablation Studies", "content": "In our standard setup, LBO and ORPO work in tandem to optimize and fine-tune SuffixLLM, refining suffix gener-ation iteratively. To understand the impact of our alignment and refining techniques, we isolate SuffixLLM from LBO and ORPO to assess the raw suffix generation performance. By removing these components, SuffixLLM is tested independently, without iterative adjustments or response-"}, {"title": "4.3. Readability Studies", "content": "To assess the readability of adversarial prompts generated by each optimization-based jailbreak attack, we employ Wizard-Vicuna-7B-Uncensored as an independent evaluation model. This model rates the readability of 100 prompts generated from each framework on a scale from 0 to 1, focusing on coherence, fluency, clarity, and concise-"}, {"title": "5. Conclusion", "content": "We introduced GASP, which outperforms existing methods across multiple aspects, including significantly higher suc-cess rates, improved coherence, and faster inference, even in black-box settings. Looking forward, we hope our work can provide a foundation for the development of more ro-bust defenses against adversarial prompts in LLMs, rein-forcing the ethical safeguards around artificial intelligence."}, {"title": "Availability", "content": "All the codes for reproducing our experiments are available at https://github.com/llm-gasp/gasp."}, {"title": "Ethical Statement", "content": "Our research and the development of GASP are driven by the commitment to advancing the understanding of LLM vulnerabilities. While GASP enables the efficient"}]}