{"title": "FedTSA: A Cluster-based Two-Stage Aggregation Method for Model-heterogeneous Federated Learning", "authors": ["Boyu Fan", "Chenrui Wu", "Xiang Su", "Pan Hui"], "abstract": "Despite extensive research into data heterogeneity in federated learn- ing (FL), system heterogeneity remains a significant yet often overlooked chal- lenge. Traditional FL approaches typically assume homogeneous hardware re- sources across FL clients, implying that clients can train a global model within a comparable time. However, in practical FL systems, clients often have heteroge- neous resources, which impacts their capacity for training tasks. This discrepancy highlights the significance of exploring model-heterogeneous FL, a paradigm that allows clients to train different models based on their resource capabilities. To ad- dress this, we introduce FedTSA, a cluster-based two-stage aggregation method tailored for system heterogeneity in FL. FedTSA starts by clustering clients based on their capabilities, then conducts a two-stage aggregation, i.e., conventional weight averaging for homogeneous models as Stage 1, and deep mutual learning with a diffusion model for aggregating heterogeneous models as Stage 2. Ex- tensive experiments not only show that FedTSA outperforms the baselines, but also explore various factors influencing model performance, thereby validating FedTSA as a promising approach for model-heterogeneous FL.", "sections": [{"title": "1 Introduction", "content": "Federated learning (FL) has emerged as a promising paradigm for privacy-preserving machine learning [27, 30, 54]. However, system heterogeneity, due to varying client capabilities, poses significant challenges for deploying FL in practice [18, 29, 52, 55]. Traditional FL frameworks operate under the assumption that all clients share the same model structure [3, 32, 46]. In fact, the real-world environments include devices with different capabilities, ranging from resource-constrained devices to high-performance GPU-enabled machines. Some devices may struggle with the computational demands of the global model, leading to prolonged training times, while others might be under- utilized, which can lead to inefficiencies and potential training biases [19, 34]. A vanilla idea is to allow clients to adopt different model architectures tailored to their capabilities. For instance, a resource-constrained Raspberry Pi might use a two- layer convolutional neural network (CNN) model, while a GPU-equipped PC could em- ploy a ResNet18 model [15]. When using traditional FL algorithms like FedProx [28],"}, {"title": "2 Related work", "content": "Model-heterogeneous FL. Different from the conventional FL paradigm [28, 32, 48, 49], model-heterogeneous FL [11] allows clients to train models of varying complexi- ties based on their processing abilities, thus mitigating the system heterogeneity issues. Inspired by the idea of KD [16, 35], [24] proposes FedMD, a logit-based FL algorithm. Instead of transferring model parameters, FedMD employs logits as the communica- tive data medium and trains the global model by aggregating and approaching these logits. However, FedMD relies on a public dataset to align the input data, leading to privacy concerns and extra communication costs. This problem exists in several similar works, like [5, 20]. Subsequent works such as FedDF [31] and FedGen [58] address these limitations by either leveraging unlabeled data or training a generator to produce synthetic data to achieve data-free KD. However, FedDF requires prior knowledge to choose proper unlabeled data, while FedGen suffers from high computation cost as the generators are deployed and worked in each client. Moreover, whether it is unlabeled data or data generated by generative adversarial networks (GAN) [14], the quality of the data cannot be guaranteed, which will potentially lead to a decrease in model per- formance. Based on the idea of class prototype [41], FedProto [44] aggregates hetero- geneous models by exchanging and aligning prototypes. In addition, HeteroFL [10] achieves model-heterogeneous FL by only aggregating the overlapping submodel parts, thus enabling clients to train submodels with different sizes. Despite existing methods adopt different strategies to achieve model-heterogeneous FL, none of them illustrate how to allocate different models to the clients. Diffusion Models. With large-scale training data and model parameters, various text-to-image generative models are now capable of creating high-quality images [9, 36, 40]. Among them, diffusion models [17] have achieved significant success in the image synthesis domain and AI-generated content (AIGC) fields, such as the popular Stable Diffusion [38] and DALL-E2 [37]. A typical diffusion model includes a forward diffusion stage and a reverse diffusion stage [8]. Given a textual prompt, the diffusion model is able to convert Gaussian noise into a text-compliant image through an itera- tive denoising step. Despite diffusion models showcasing excellent performance, only"}, {"title": "3 Motivation Study", "content": "We motivate this study by presenting the impact of system heterogeneity through a small-scale experiment. We establish a heterogeneous system comprising our types of devices: three Raspberry Pi 4B units, three LattePanda 2 Alpha 864s [23], two PCs without GPU, and one PC equipped with an NVIDIA 3080Ti GPU. These devices range from embedded systems to PCs with GPUs, effectively presenting the system hetero- geneity prevalent in real-world scenarios. The core specifications of each device type are detailed in Table 1. Given the limited resources of the Raspberry Pi, we select a FL training task using the MNIST dataset and a CNN model. This model includes two convolutional layers with max pooling, followed by two fully connected layers. Each device serves as a FL client, with an additional machine serving as the central server for aggregation. We employ FedAvg [32] for this study."}, {"title": "4 Methodology", "content": "4.1 Problem Definition In this work, we consider a system-heterogeneity setting in FL. Suppose we have n clients with their private local dataset Di from distribution Pi(x, y), with x and y rep- resenting the input data and their respective labels. Unlike most works that assume clients have similar hardware resources, we consider the presence of heterogeneous re- source distributions among clients, denoted as R1, R2, ..., Rn. Due to this challenge, a standardized global model Wg loses its applicability, as clients with limited resources cannot afford the computational costs if Wg is complex. Therefore, the problem be- comes how to enable clients to deploy different models W1, W2, ..., Wn according to their respective resources R1, R2, ..., Rn, while still achieve FL training on the server side. Similar to conventional FL approaches, for the ith client, the training procedure is to minimize the loss as follows:\n\\[\\mathop{\\arg \\min }\\limits_{{W_i}} \\frac{1}{N}\\sum\\limits_{i = 1}^n {\\left| {{D_i}} \\right|} \\;{{\\ell }_{ce}}\\left( {f\\left( {{W_i};x} \\right),y} \\right),\\]\nwhere N is the total number of the training data over all clients, f(Wi, x) denotes the output of model Wi give the input data x, and \\( {{\\ell }_{ce}} \\) is the cross entropy-loss. However, the varied architectures of models W\u2081 imply that the model parameters from clients are incompatible with elementary weight averaging, since they have different dimensions. Therefore, a new strategy is required to aggregate these model parameters on the server side to achieve global update. To tackle this dilemma, we propose a two-stage agg- gation framework FedTSA, where clients with similar resources conduct conventional weight averaging within a cluster, while clients with significantly different resources contribute to each other by DML outside the cluster.\n4.2 Two-Stage Aggregation Framework Figure 2 presents the FedTSA framework, highlighting two stages of aggregation, i.e., in-cluster weight averaging aggregation (Stage 1) and server-side DML aggregation (Stage 2). In Stage 1, we harness a resource-driven clustering methodology to catego- rize clients, leading to n distinct clusters. Each cluster receives a unique pruning rate, creating models of varying complexities. Then, clients conduct local updates, but only updates from the same cluster perform weight aggregation, as they share an identical model architecture and thus possess equivalent parameter dimensions. These cluster- based models are sent to the server for Stage 2 aggregation. Stage 2 in Figure 2 depicts how models with different architectures achieve aggre- gation with the help of DML and a diffusion model. Here, data generated from the"}, {"title": "4.3 Resource-oriented Clustering with Pruned Models", "content": "In our FedTSA framework, clustering serves as the initial step, grouping clients accord- ing to hardware resources to enhance efficient training and reduce tail latency. Unlike most existing works that group clients to address data heterogeneity [4, 33], our focus is on ensuring resource homogeneity within clusters. While one could categorize re- sources by hardware like CPU and memory, solely using these attributes is insufficient. Environmental factors, such as temperature shifts and voltage variations, can impact de- vice performance. Moreover, the network condition plays a fundamental role, especially given the multiple communication rounds inherent to the FL paradigm. We propose to employ a proxy task to acquire the training duration of each client, which will act as a key attribute for clustering. This duration serves as a reflection of"}, {"title": "4.4 DML with Diffusion Model", "content": "In this section, we present the process of server-side DML with the help of a diffusion model. Denoising diffusion probabilistic models (DDPM) [17] have been developed as probabilistic models, aiming at learning the underlying data distribution p(x). This is achieved through a gradual denoising process applied to a normally distributed variable, which corresponds to the inverse process of a fixed Markov Chain with a length of T,\n\\[{x_t} = \\sqrt {{a_t}} {x_{t - 1}} + \\sqrt {1 - {a_t}} {e_t},\\;t \\in \\left\\{ {1,2, \\ldots ,T} \\right\\},\\]\nwhere sequence {xt} changes from x1 to xf sampled from a Gaussian distribution \\( N(0, I) \\). At each step, Gaussian noise et is added, drawn from \\( N(0, I) \\). {at}1=1 is pre-defined constants as the vanilla DDPM. The denoising process is designed to be the inverse of the diffusion process, wherein the Gaussian noise \\( {x_\\top } \\sim N(0, I) \\) is progres- sively restored back to its original state x1 through denoising steps, for t = T,\u2026\u2026,1. To enhance efficiency, FedTSA avoids the long training process that is usually re- quired for diffusion models. We simply develop a prompt pool denoted as y and lever- age a pre-trained Stable Diffusion v1-4 Model [38] for inference. By allowing clients to upload prompts to the server's prompt pool, we have effectively mitigated concerns about data distribution and potential privacy issues. Moreover, before uploading textual"}, {"title": "5 Evaluation", "content": "5.1 Experimental Setup Datasets and Models. We evaluate the performance of FedTSA using three benchmark datasets, i.e., CIFAR-10, CIFAR-100 [22], and Tiny-ImageNet [7]. Different data distri- butions are considered in the experiments. In the IID setting, data is evenly distributed to each client in terms of both quantity and class distribution. In the non-IID settings, we"}, {"title": "5.2 Performance Evaluation and Analysis", "content": "Performance Comparison. From the results presented in Table 3, we can observe that FedTSA outperforms other FL baselines across both IID and non-IID settings, high- lighting its promising performance and robustness to data heterogeneity. An exception is noted in the IID setting of CIFAR-10 dataset, where FedTSA slightly lags behind the upper bound performance of FedAvg. However, we should notice that the upper bounds for FedAvg and FedProx are based on an idealized scenario where all clients have un- pruned models. Given this context, FedTSA still maintains a dominant performance over most upper bounds across both IID and non-IID settings. This superior perfor- mance is attributed to FedTSA's innovative strategy. Contrasting with algorithms that conventionally replace the global model with a local model for round updates, FedTSA fosters inter-client learning through DML. Specifically, FedTSA utilizes the KL di- vergence loss to learn the average logits from other client models, thereby effectively obviating the need to rely on a singular global model. Consequently, each model not only gains knowledge from its counterparts but also sidesteps potential information loss that may arise from direct replacement. Besides, we can notice that FedProto presents the poorest accuracy, revealing that prototype-based FL methods do not perform as"}, {"title": "Effect of Loss Function in DML", "content": "In the DML process, we evaluate three loss functions: KL-only, CE-only, and their linear combination. The KL-only function uses only KL divergence loss, reflecting the model's reliance on ensemble knowledge from generated data. In contrast, CE-only involves training on generated data without inter- model knowledge exchange. The linear combination loss is formulated as follows:\n\\[L_i(\\mathop w_i;{\\cal X},{\\cal Y}) = \\alpha {L_{kl}} + (1 - \\alpha ){L_{ce}},\\]\nwhere Lkl is KL divergence loss, Lce is cross-entropy (CE) loss, and \u03b1 is a weighting factor. Figure 4 shows the accuracy differences among these approaches. KL-only con- verges fastest initially but ultimately underperforms KL+CE in accuracy. While KL+CE is slower and less stable initially, it achieves the highest final accuracy. When the weight"}, {"title": "6 Discussion", "content": "FedTSA enables clients to send prompts that help generate data for the diffusion model. Concerns may arise regarding privacy, particularly the possibility of servers knowing the details of training tasks from these prompts. In fact, it is common practice for servers to be cognizant of the tasks being executed by clients. This knowledge is essential for the server to select the appropriate model architecture and initialize the model. Despite being aware of the task type, the server does not need to know the specific data of the clients, still achieving privacy protection. Furthermore, it is not mandatory for the la- bel information within a prompt to match an actual label from the local dataset. Any label related to the training task can be used. The primary objective of the data gener- ation process is to align the logits produced by heterogeneous models, facilitating their aggregation. This approach also addresses concerns regarding discrepancies between generated data and the actual data of certain clients. The generated data only serves as a bridge in Stage 2, ensuring the transfer of knowledge across heterogeneous models. Despite FedTSA showing excellent performance, some trade-offs exist. First, the diffusion model introduces additional computational overhead to generate image data. However, it also offers better flexibility to switch different training tasks by simply adjusting the prompts than fixed public datasets. Second, like other existing model- heterogeneous FL methods, FedTSA has been only verified on CV tasks. The adaption and experiments of FedTSA on natural language processing tasks are our main effort direction in the future."}, {"title": "7 Conclusion", "content": "This paper proposes FedTSA, a novel cluster-based two-stage aggregation method for model-heterogeneous FL. FedTSA first conducts clustering to generate heterogeneous cluster-based models, then leverages DML on the server side to aggregate these models based on the data generated by a diffusion model. Experimental results on CV datasets show that FedTSA surpasses baselines in both IID and non-IID settings. Further exper- iments on the influential factors evaluate the effect of these factors on model training. In future research, we will investigate methods for achieving model-heterogeneous FL without any external data, and make FedTSA available for more scenarios, such as nat- ural language processing and multimodal tasks."}, {"title": "A More Implementation Details", "content": "A.1 Environment We conduct experiments under Python 3.8.0 and PyTorch 1.31.1. We use a NVIDIA A100 provided by RunPod for computation. Weights&Biases7 is leveraged to track and log the experimental results. Regarding the diffusion model, we choose the Stable Diffusion model v1-4 from Hugging Faces. A.2 Datasets CIFAR-10 and CIFAR-100. CIFAR-10 and CIFAR-100 are benchmark datasets in the field of Computer Vision (CV), which are also served as popular datasets for evaluating FL methods. CIFAR-10 consists of 60000 32\u00d732 images in 10 classes, with 6000 im- ages per class. CIFAR-100 also consists of 60,000 images but is categorized into 100 classes, each class containing 600 images. CIFAR-10 provides a fundamental platform for image classification tasks, offering a relatively simpler challenge. CIFAR-100, on the other hand, presents a more challenging scenario with more fine-grained categories. Tiny-ImageNet. Tiny-ImageNet is a simplified version of the larger ImageNet dataset. It consists of 100000 64\u00d764 images in 200 classes, which serves as a bridge between simpler datasets like CIFAR and the full-scale ImageNet dataset. Due to its increased complexity, training on Tiny-ImageNet is more demanding, often used to fur- ther assess a method's efficacy. It is noteworthy that accuracy levels on Tiny-ImageNet tend to be significantly lower compared to those achieved on CIFAR datasets, reflecting its heightened challenge. A.3 Models and the Pruned Details In all our experiments, we employ the ResNet18 model, which is also leveraged in baseline studies such as HeteroFL and FedProto. To introduce model heterogeneity, we apply a pruning rate to the hidden size, adjusting the input and output channels. The lower the pruning rate, the simpler the model becomes, whereas the higher the pruning rate, the more complex it is. This pruning process is specifically applied to the first convolutional layer, all subsequent residual blocks, and the input size of the first fully connected layer. B Data samples generated by diffusion model We implement a Conditional Generative Adversarial Network (CGAN) to generate im- ages in the style of CIFAR-10-style and present a visualized comparison with images generated by a diffusion model. Figure 1 presents the results. In this figure, each row"}]}