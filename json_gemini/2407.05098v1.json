{"title": "FedTSA: A Cluster-based Two-Stage Aggregation Method for Model-heterogeneous Federated Learning", "authors": ["Boyu Fan", "Chenrui Wu", "Xiang Su", "Pan Hui"], "abstract": "Despite extensive research into data heterogeneity in federated learn-ing (FL), system heterogeneity remains a significant yet often overlooked chal-lenge. Traditional FL approaches typically assume homogeneous hardware re-sources across FL clients, implying that clients can train a global model within acomparable time. However, in practical FL systems, clients often have heteroge-neous resources, which impacts their capacity for training tasks. This discrepancyhighlights the significance of exploring model-heterogeneous FL, a paradigm thatallows clients to train different models based on their resource capabilities. To ad-dress this, we introduce FedTSA, a cluster-based two-stage aggregation methodtailored for system heterogeneity in FL. FedTSA starts by clustering clients basedon their capabilities, then conducts a two-stage aggregation, i.e., conventionalweight averaging for homogeneous models as Stage 1, and deep mutual learningwith a diffusion model for aggregating heterogeneous models as Stage 2. Ex-tensive experiments not only show that FedTSA outperforms the baselines, butalso explore various factors influencing model performance, thereby validatingFedTSA as a promising approach for model-heterogeneous FL.", "sections": [{"title": "1 Introduction", "content": "Federated learning (FL) has emerged as a promising paradigm for privacy-preservingmachine learning [27, 30, 54]. However, system heterogeneity, due to varying clientcapabilities, poses significant challenges for deploying FL in practice [18, 29, 52, 55].Traditional FL frameworks operate under the assumption that all clients share the samemodel structure [3, 32, 46]. In fact, the real-world environments include devices withdifferent capabilities, ranging from resource-constrained devices to high-performanceGPU-enabled machines. Some devices may struggle with the computational demandsof the global model, leading to prolonged training times, while others might be under-utilized, which can lead to inefficiencies and potential training biases [19, 34].A vanilla idea is to allow clients to adopt different model architectures tailored totheir capabilities. For instance, a resource-constrained Raspberry Pi might use a two-layer convolutional neural network (CNN) model, while a GPU-equipped PC could em-ploy a ResNet18 model [15]. When using traditional FL algorithms like FedProx [28],"}, {"title": "2 Related work", "content": "Model-heterogeneous FL.Different from the conventional FL paradigm [28, 32, 48,49], model-heterogeneous FL [11] allows clients to train models of varying complexi-ties based on their processing abilities, thus mitigating the system heterogeneity issues.Inspired by the idea of KD [16, 35], [24] proposes FedMD, a logit-based FL algorithm.Instead of transferring model parameters, FedMD employs logits as the communica-tive data medium and trains the global model by aggregating and approaching theselogits. However, FedMD relies on a public dataset to align the input data, leading toprivacy concerns and extra communication costs [53]. This problem exists in severalsimilar works, like [5, 20]. Subsequent works such as FedDF [31] and FedGen [58] addressthese limitations by either leveraging unlabeled data or training a generator to producesynthetic data to achieve data-free KD. However, FedDF requires prior knowledge tochoose proper unlabeled data, while FedGen suffers from high computation cost as thegenerators are deployed and worked in each client. Moreover, whether it is unlabeleddata or data generated by generative adversarial networks (GAN) [14], the quality ofthe data cannot be guaranteed, which will potentially lead to a decrease in model per-formance. Based on the idea of class prototype [41], FedProto [44] aggregates hetero-geneous models by exchanging and aligning prototypes. In addition, HeteroFL [10]achieves model-heterogeneous FL by only aggregating the overlapping submodel parts,thus enabling clients to train submodels with different sizes. Despite existing methodsadopt different strategies to achieve model-heterogeneous FL, none of them illustratehow to allocate different models to the clients.Diffusion Models.With large-scale training data and model parameters, varioustext-to-image generative models are now capable of creating high-quality images [9,36, 40]. Among them, diffusion models [17] have achieved significant success in theimage synthesis domain and AI-generated content (AIGC) fields, such as the popularStable Diffusion [38] and DALL-E2 [37]. A typical diffusion model includes a forwarddiffusion stage and a reverse diffusion stage [8]. Given a textual prompt, the diffusionmodel is able to convert Gaussian noise into a text-compliant image through an itera-tive denoising step. Despite diffusion models showcasing excellent performance, only"}, {"title": "3 Motivation Study", "content": "We motivate this study by presenting the impact of system heterogeneity through asmall-scale experiment. We establish a heterogeneous system comprising our types ofdevices: three Raspberry Pi 4B units, three LattePanda 2 Alpha 864s [23], two PCswithout GPU, and one PC equipped with an NVIDIA 3080Ti GPU. These devices rangefrom embedded systems to PCs with GPUs, effectively presenting the system hetero-geneity prevalent in real-world scenarios. The core specifications of each device typeare detailed in Table 1. Given the limited resources of the Raspberry Pi, we select aFL training task using the MNIST dataset and a CNN model. This model includes twoconvolutional layers with max pooling, followed by two fully connected layers. Eachdevice serves as a FL client, with an additional machine serving as the central serverfor aggregation. We employ FedAvg [32] for this study."}, {"title": "4 Methodology", "content": "4.1 Problem Definition\nIn this work, we consider a system-heterogeneity setting in FL. Suppose we have nclients with their private local dataset $D_i$ from distribution $P_i(x, y)$, with x and y rep-resenting the input data and their respective labels. Unlike most works that assumeclients have similar hardware resources, we consider the presence of heterogeneous re-source distributions among clients, denoted as $R_1, R_2, ..., R_n$. Due to this challenge, astandardized global model $W_g$ loses its applicability, as clients with limited resourcescannot afford the computational costs if $W_g$ is complex. Therefore, the problem be-comes how to enable clients to deploy different models $W_1, W_2, ..., W_n$ according totheir respective resources $R_1, R_2, ..., R_n$, while still achieve FL training on the serverside. Similar to conventional FL approaches, for the $i^{th}$ client, the training procedure isto minimize the loss as follows:\n$\\arg \\min_{W_i} \\frac{1}{N} \\sum_{i=1}^{|D_i|} l_{ce}(f(W_i; x), y),  (1)$\nwhere N is the total number of the training data over all clients, $f(W_i, x)$ denotes theoutput of model $W_i$ give the input data x, and $l_{ce}$ is the cross entropy-loss. However,the varied architectures of models $W_i$ imply that the model parameters from clients areincompatible with elementary weight averaging, since they have different dimensions.Therefore, a new strategy is required to aggregate these model parameters on the serverside to achieve global update. To tackle this dilemma, we propose a two-stage aggre-gation framework FedTSA, where clients with similar resources conduct conventionalweight averaging within a cluster, while clients with significantly different resourcescontribute to each other by DML outside the cluster."}, {"title": "4.2 Two-Stage Aggregation Framework", "content": "Figure 2 presents the FedTSA framework, highlighting two stages of aggregation, i.e.,in-cluster weight averaging aggregation (Stage 1) and server-side DML aggregation(Stage 2). In Stage 1, we harness a resource-driven clustering methodology to catego-rize clients, leading to n distinct clusters. Each cluster receives a unique pruning rate,creating models of varying complexities. Then, clients conduct local updates, but onlyupdates from the same cluster perform weight aggregation, as they share an identicalmodel architecture and thus possess equivalent parameter dimensions. These cluster-based models are sent to the server for Stage 2 aggregation.Stage 2 in Figure 2 depicts how models with different architectures achieve aggre-gation with the help of DML and a diffusion model. Here, data generated from the"}, {"title": "4.3 Resource-oriented Clustering with Pruned Models", "content": "In our FedTSA framework, clustering serves as the initial step, grouping clients accord-ing to hardware resources to enhance efficient training and reduce tail latency. Unlikemost existing works that group clients to address data heterogeneity [4, 33], our focusis on ensuring resource homogeneity within clusters. While one could categorize re-sources by hardware like CPU and memory, solely using these attributes is insufficient.Environmental factors, such as temperature shifts and voltage variations, can impact de-vice performance. Moreover, the network condition plays a fundamental role, especiallygiven the multiple communication rounds inherent to the FL paradigm.We propose to employ a proxy task to acquire the training duration of each client,which will act as a key attribute for clustering. This duration serves as a reflection of"}, {"title": "4.4 DML with Diffusion Model", "content": "In this section, we present the process of server-side DML with the help of a diffusionmodel. Denoising diffusion probabilistic models (DDPM) [17] have been developed asprobabilistic models, aiming at learning the underlying data distribution p(x). This isachieved through a gradual denoising process applied to a normally distributed variable,which corresponds to the inverse process of a fixed Markov Chain with a length of T,\n$x_t = \\sqrt{a_t}x_{t-1} + \\sqrt{1 - a_t}e_t, t \\in \\{1, 2, ...., T\\},   (5)$\nwhere sequence {xt} changes from $x_1$ to $x_T$ sampled from a Gaussian distribution$N(0, I)$. At each step, Gaussian noise $e_t$ is added, drawn from $N(0, I)$. $\\{a_t\\}_{t=1}^{T}$ ispre-defined constants as the vanilla DDPM. The denoising process is designed to be theinverse of the diffusion process, wherein the Gaussian noise $x_T \\sim N(0, I)$ is progres-sively restored back to its original state $x_1$ through denoising steps, for $t = T, ...., 1$.To enhance efficiency, FedTSA avoids the long training process that is usually re-quired for diffusion models. We simply develop a prompt pool denoted as y and lever-age a pre-trained Stable Diffusion v1-4 Model [38] for inference. By allowing clientsto upload prompts to the server's prompt pool, we have effectively mitigated concernsabout data distribution and potential privacy issues. Moreover, before uploading textual"}, {"title": "5 Evaluation", "content": "5.1 Experimental Setup\nDatasets and Models.We evaluate the performance of FedTSA using three benchmarkdatasets, i.e., CIFAR-10, CIFAR-100 [22], and Tiny-ImageNet [7]. Different data distri-butions are considered in the experiments. In the IID setting, data is evenly distributedto each client in terms of both quantity and class distribution. In the non-IID settings, we"}, {"title": "5.2 Performance Evaluation and Analysis", "content": "Performance Comparison.From the results presented in Table 3, we can observe thatFedTSA outperforms other FL baselines across both IID and non-IID settings, high-lighting its promising performance and robustness to data heterogeneity. An exceptionis noted in the IID setting of CIFAR-10 dataset, where FedTSA slightly lags behind theupper bound performance of FedAvg. However, we should notice that the upper boundsfor FedAvg and FedProx are based on an idealized scenario where all clients have un-pruned models. Given this context, FedTSA still maintains a dominant performanceover most upper bounds across both IID and non-IID settings. This superior perfor-mance is attributed to FedTSA's innovative strategy. Contrasting with algorithms thatconventionally replace the global model with a local model for round updates, FedTSAfosters inter-client learning through DML. Specifically, FedTSA utilizes the KL di-vergence loss to learn the average logits from other client models, thereby effectivelyobviating the need to rely on a singular global model. Consequently, each model notonly gains knowledge from its counterparts but also sidesteps potential information lossthat may arise from direct replacement. Besides, we can notice that FedProto presentsthe poorest accuracy, revealing that prototype-based FL methods do not perform as"}, {"title": "6 Discussion", "content": "FedTSA enables clients to send prompts that help generate data for the diffusion model.Concerns may arise regarding privacy, particularly the possibility of servers knowingthe details of training tasks from these prompts. In fact, it is common practice for serversto be cognizant of the tasks being executed by clients. This knowledge is essential forthe server to select the appropriate model architecture and initialize the model. Despitebeing aware of the task type, the server does not need to know the specific data of theclients, still achieving privacy protection. Furthermore, it is not mandatory for the la-bel information within a prompt to match an actual label from the local dataset. Anylabel related to the training task can be used. The primary objective of the data gener-ation process is to align the logits produced by heterogeneous models, facilitating theiraggregation. This approach also addresses concerns regarding discrepancies betweengenerated data and the actual data of certain clients. The generated data only serves asa bridge in Stage 2, ensuring the transfer of knowledge across heterogeneous models.Despite FedTSA showing excellent performance, some trade-offs exist. First, thediffusion model introduces additional computational overhead to generate image data.However, it also offers better flexibility to switch different training tasks by simplyadjusting the prompts than fixed public datasets. Second, like other existing model-heterogeneous FL methods, FedTSA has been only verified on CV tasks. The adaptionand experiments of FedTSA on natural language processing tasks are our main effortdirection in the future."}, {"title": "7 Conclusion", "content": "This paper proposes FedTSA, a novel cluster-based two-stage aggregation method formodel-heterogeneous FL. FedTSA first conducts clustering to generate heterogeneouscluster-based models, then leverages DML on the server side to aggregate these modelsbased on the data generated by a diffusion model. Experimental results on CV datasetsshow that FedTSA surpasses baselines in both IID and non-IID settings. Further exper-iments on the influential factors evaluate the effect of these factors on model training.In future research, we will investigate methods for achieving model-heterogeneous FLwithout any external data, and make FedTSA available for more scenarios, such as nat-ural language processing and multimodal tasks."}, {"title": "A More Implementation Details", "content": "A.1 Environment\nWe conduct experiments under Python 3.8.0 and PyTorch 1.31.1. We use a NVIDIAA100 provided by RunPod for computation. Weights&Biases is leveraged to trackand log the experimental results. Regarding the diffusion model, we choose the StableDiffusion model v1-4 from Hugging Faces."}, {"title": "A.2 Datasets", "content": "CIFAR-10 and CIFAR-100.CIFAR-10 and CIFAR-100 are benchmark datasets in thefield of Computer Vision (CV), which are also served as popular datasets for evaluatingFL methods. CIFAR-10 consists of 60000 32\u00d732 images in 10 classes, with 6000 im-ages per class. CIFAR-100 also consists of 60,000 images but is categorized into 100classes, each class containing 600 images. CIFAR-10 provides a fundamental platformfor image classification tasks, offering a relatively simpler challenge. CIFAR-100, onthe other hand, presents a more challenging scenario with more fine-grained categories.Tiny-ImageNet.Tiny-ImageNet is a simplified version of the larger ImageNetdataset. It consists of 100000 64\u00d764 images in 200 classes, which serves as a bridgebetween simpler datasets like CIFAR and the full-scale ImageNet dataset. Due to itsincreased complexity, training on Tiny-ImageNet is more demanding, often used to fur-ther assess a method's efficacy. It is noteworthy that accuracy levels on Tiny-ImageNettend to be significantly lower compared to those achieved on CIFAR datasets, reflectingits heightened challenge."}, {"title": "A.3 Models and the Pruned Details", "content": "In all our experiments, we employ the ResNet18 model, which is also leveraged inbaseline studies such as HeteroFL and FedProto. To introduce model heterogeneity, weapply a pruning rate to the hidden size, adjusting the input and output channels. Thelower the pruning rate, the simpler the model becomes, whereas the higher the pruningrate, the more complex it is. This pruning process is specifically applied to the firstconvolutional layer, all subsequent residual blocks, and the input size of the first fullyconnected layer."}, {"title": "B Data samples generated by diffusion model", "content": "We implement a Conditional Generative Adversarial Network (CGAN) to generate im-ages in the style of CIFAR-10-style and present a visualized comparison with imagesgenerated by a diffusion model. Figure 1 presents the results. In this figure, each row"}]}