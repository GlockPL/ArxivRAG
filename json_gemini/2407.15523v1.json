{"title": "TOM: A Development Platform For Wearable Intelligent Assistants", "authors": ["NUWAN JANAKA", "SHENGDONG ZHAO", "DAVID HSU", "SHERISSE TAN JING WEN", "KOH CHUN KEAT"], "abstract": "Advanced digital assistants can significantly enhance task performance, reduce user burden, and provide personalized guidance to\nimprove users' abilities. However, the development of such intelligent digital assistants presents a formidable challenge. To address\nthis, we introduce TOM, a conceptual architecture and software platform (https://github.com/TOM-Platform) designed to support\nthe development of intelligent wearable assistants that are contextually aware of both the user and the environment. This system\nwas developed collaboratively with AR/MR researchers, HCI researchers, AI/Robotic researchers, and software developers, and it\ncontinues to evolve to meet the diverse requirements of these stakeholders. TOM facilitates the creation of intelligent assistive AR\napplications for daily activities and supports the recording and analysis of user interactions, integration of new devices, and the\nprovision of assistance for various activities. Additionally, we showcase several proof-of-concept assistive services and discuss the\nchallenges involved in developing such services.", "sections": [{"title": "1 INTRODUCTION", "content": "With advancements in Machine Learning (ML) and Artificial Intelligence (AI) technologies, digital assistants have become\nintegral to everyday life. These include voice assistants like Siri, Alexa, or Google, and some even support visual modality\n(e.g., [1, 30]), enabling rich inputs and outputs. Similar to science fiction, advanced digital assistants can practically\naid users in performing both familiar and new tasks, reduce task load and errors, and enhance task performance [15].\nMoreover, these assistants offer personalization, optimizing support for individual needs and broadening accessibility.\nDespite existing interaction paradigms such as Heads-Up Computing [44] and Dynamicland [38] aiming to realize\ndigital assistance in daily activities, several challenges persist. These include a lack of understanding of the required\nsystem capabilities, development guidance, and a platform for rapid development of such assistance (see Sec 2-3 for\ndetails).\nTo tackle these challenges, we introduce TOM, an intelligent wearable assistive system developed by identifying user,\nresearcher, and developer needs. TOM facilitates the creation and analysis of assistive applications, enabling context\nand user understanding and supporting multimodal interactions with AR/MR devices and ML/AI technologies. Through\ndeveloping several proof-of-concept services (e.g., running coach assistance, translation, and querying assistance), we"}, {"title": "2 RELATED WORK", "content": "Our work, which envisions an intelligent wearable assistive system, is related to context-aware and assistive Augmented\nReality (AR) systems."}, {"title": "2.1 Context-Aware Systems", "content": "In general, context can be defined as \"any information used to characterize the situation of an entity. An entity is a person,\nplace, or object that is considered relevant to the interaction between a user and an application, including the user and the\napplication themselves, and, by extension, the environment in which the user and applications are embedded. A system is\ncontext-aware if it uses context to provide relevant information and/or services to the user, where relevancy depends on\nthe user's task.\u201d [2, 17]. In the development of context awareness in augmented reality (AR) or pervasive augmented\nreality, Grubert et al. [21] propose a taxonomy considering context sources (i.e., context factors to which AR systems\ncan adapt), context targets (i.e., content that should be adapted), and context controllers (i.e., how the adaptation should\nbe made). Although they outline high-level categories of context sources (e.g., human, environmental, system factors)\nand context targets (e.g., system input, system output, system configuration), they do not offer specific guidance for\npotential capabilities or implementations for context-aware AR systems. While Dey et al. [17] provide guidelines and\nan architecture for developing context-aware applications through the Context Toolkit, they do not address the various\nstakeholders' requirements for such systems. Therefore, we first attempt to identify the capabilities for an intelligent\nwearable context-aware AR system and then adapt the factors and guidelines provided by Dey et al. [17] and Grubert et\nal. [21] to develop such a system.\nThe Platform for Situated Intelligence (\\psi), developed by Bohus, Andrist, et al. [6, 10], shares certain high-level\nobjectives and architectural similarities with TOM; both systems utilize timestamped multimodal streaming data from\nvarious sensors to facilitate rapid development and research within interactive systems. However, TOM specifically\nfocuses on wearable, user-centered AR applications, necessitating distinct requirements and system capabilities such\nas user sensing, compared to traditional situated interactive systems targeted by \\psi. Moreover, the interactions and\ninterfaces of TOM are optimized to provide intelligent assistance during daily activities while minimizing interference\nwith daily tasks, with the intention of enabling dynamic context- and user-dependent adaptive interfaces."}, {"title": "2.2 Assistive AR Systems", "content": "Augmented and Mixed Reality (AR/MR) have been used to develop assistive systems that augment users' perception\nwith digitally superimposed content in the physical world [7, 37]. These systems have been applied in various domains\nsuch as education, training, repair and maintenance, healthcare and medicine, and education due to their advantages in\nminimizing errors and reducing cognitive load [8, 9, 39]. However, most of these systems are tailored to specific tasks\nand lack adaptability for various daily activities.\nSeveral toolkits have been proposed for developing assistive AR systems, yet they are often designed for specialized\npurposes. For instance, RagRug [20] and DXR (Data visualizations in eXtended Reality) [35] are designed for situated"}, {"title": "3 TOM: THE OTHER ME", "content": ""}, {"title": "3.1 Envisioned Usage Scenarios", "content": "Consider Jane, who regularly uses Jerry, a digital assistant developed using TOM, in her daily life. Jerry sees what\nJane sees, hears what Jane hears, knows her preferences, and understands her emotional and physical conditions.\nScenario. Unable to decide on a dish to prepare for herself and her toddler and wishing to try something new,\nshe opens the refrigerator and asks, \"Hey, Jerry. Can you suggest a new dish for us?\" Jerry scans and identifies the\ningredients in the refrigerator, finds possible dishes, and renders three new dish suggestions with their images. Jane finds\nthe second suggestion appealing and inquires about the preparation process. Jerry then guides her through preparing\nthe new dish, providing real-time, step-by-step feedback superimposed in real-world objects.\nLater, Jane receives a delivery of a play table set for her toddler, ordered through Jerry during an online browsing\nsession. She notices her toddler's eagerness to assist in assembling the set. Examining the package, she asks, \"Hey Tom,\ncan you help me build this?\" By identifying the play table set and retrieving instructions, Jerry displays step-by-step\nvirtual instructions superimposed on the physical parts, which Jane follows while involving her toddler. Suddenly, her\ntoddler accidentally drops a piece of the set, striking his leg and causing him to cry. Jane becomes panicked. Sensing\nthe situation, Jerry instructs her to remain calm and inspect her toddler's leg. As Jane consoles her child, Jerry assesses\nthe situation and provides first-aid instructions. During the first aid, Jerry asks whether to contact her husband, family"}, {"title": "3.2 System Capabilities", "content": "In our quest for an envisioned intelligent wearable assistant, Jerry, we observed that while certain capabilities are\nsupported by existing context-aware and assistive AR/MR systems, a complete integration of these capabilities into\na single system is lacking. The Heads-Up Computing Paradigm [44], while theoretically supporting our envisioned\nuse cases, does not provide guidance on implementing such a system or the capabilities required to further research\noptimal Human-AI interactions during daily activities. Drawing from literature, our experience working with AR/MR\nand AI researchers, and testing assistive Human-AI interfaces (including early prototypes of TOM) with participants\nand their feedback, we have formulated the following system capabilities. These are categorized based on three major\nstakeholders' requirements, which, though distinct, have overlapping capabilities.\nJust-in-time Assistance for Users. Users should be able to interact (C1a) with the system (i.e., provide input and\nreceive feedback) naturally and optimally to obtain the desired assistance [22, 44]. Such assistance should be delivered\njust in time to match the user's current needs or proactively when users have limited knowledge of system capabilities\n[4, 32, 41], with minimal interference in the user's ongoing activities while accommodating the user's cognitive\ncapabilities [5, 26]. To achieve this, the system should understand the user (C1b) and context (C1c) to provide the\nmost appropriate feedback to support the user's ongoing activities [17, 44]. Such understanding aids in modeling the\nhuman and the world to minimize awareness mismatch between user expectations and system feedback and maintaining\nprofiles [34].\nData Recording and Analysis for Researchers. To understand user interactions with such a system and to design\noptimal interactions, researchers need to record (C2a), visualize (C2b), and analyze (C2c) the data and develop\nmodels [15, 19, 27]. This involves collecting data to support real-time and retrospective observations, training models\nto predict optimal feedback and analyzing their performance, and understanding the underlying reasons for user and\nsystem behaviors [15, 19, 29].\nEase of Development for Developers. Considering the variety of activities users may engage in and their unique\nassistance requirements, the system should enable developers to create different assistive features easily. This requires\nthat developers can integrate new devices (C3a) easily (e.g., sensors to understand new contexts or actuators to\nprovide optimal feedback), deploy new assistance and models (C3b) (e.g., to predict optimal feedback), and access\nand control current data (C3c) (e.g., from existing devices or models)."}, {"title": "3.3 Conceptual Architecture", "content": "To support the above requirements, we consider three main entities: user (i.e., the individual receiving assistance),\ncontext (i.e., the user's perceptual space and associated tasks), and the system, TOM, as shown in Figure 1a, following\nthe high-level context sources [21].\nSeparating the user from the context enables us to develop user interaction models [44]. These models sense and\nunderstand the user (C1\u2081, e.g., cognitive states, affective states, physical states [21]) to provide personalized feedback.\nThus, TOM maintains user profiles to cater to individual preferences and capabilities.\nGiven that daily activities, such as cooking, typically involve both digital (e.g., viewing a recipe) and physical tasks\n(e.g., selecting the proper portion), TOM offers system-level support to connect the digital world with the physical\nworld by understanding the context (C1c, e.g., physical environment) and utilizing pervasive augmented reality [21].\nThis involves a multi-modal and multifaceted understanding of the environment (e.g., visual and auditory scene\nunderstanding, understanding the ongoing activities and associated objects, understanding the relationship between the\nuser and environment, understanding the social context) as well as understanding the devices that facilitate interactions\n(e.g., device resource availability).\nIn terms of input, TOM supports the user's explicit multi-modal inputs (C1a, such as voice and gesture) as well as\nimplicit inputs (C1a, like gaze and physiological data), in addition to processing multi-modal context information.\nAfter understanding the context (e.g., ongoing activity) and user (e.g., intention), TOM activates a context-aware\nservice, employing domain knowledge to generate real-time proactive suggestions through reasoning and planning.\nThese suggestions are conveyed to users as multi-sensory feedback (C1a), tailored to their cognitive capacity, including\nvisual, auditory, and/or haptic modalities. The feedback is dynamically updated based on the user's actions; for instance,\nif the user does not follow a given suggestion, TOM formulates the next appropriate suggestion, considering the user's\ncurrent status and context, facilitating a closed-loop control system (See Appendix A-Figure 6).\nWhen multiple TOM users are involved in a collaborative activity (e.g., group discussion on an artifact), each TOM\nsystem enables multi-agent coordination to complete the collaborative activity optimally."}, {"title": "3.4 System Architecture", "content": "We develop the following system/physical architecture based on the requirements and envisioned use cases."}, {"title": "3.4.1 Devices and Technologies", "content": "Following a user-centric approach [44], TOM uses wearable devices that align with\nhuman input-output channels (e.g., eyes, hands), such as Optical See-Through Head-Mounted Displays (OHMD,"}, {"title": "3.5 Client-Server Architecture", "content": "Given the limited computational resources of wearable devices [15], TOM is implemented as a client-server architecture,\nas illustrated in Figure 2. The server hosts services for processing and orchestrating data, supports ML/AI inferences,\nand provides real-time feedback to clients. Clients, such as OHMDs and smartwatches, send sensor data to the server\nand display feedback to the user. This separation also allows TOM to be device-agnostic, supporting various OHMDs\nthrough the same server (C3a).\nServer Architecture. Designed for flexibility and simplicity, the server acts as a one-stop platform for deploying\nvarious services optimized for different activities and switching between them as needed. Adapting the architecture\nof the Context Toolkit [17], the TOM server is implemented with independent components under three\ndistinct layers: Widgets (i.e., components that listen for sensors and receive input data), Processors (i.e., stateless\ncomponents that process and transform input data from Widgets or output data from Services), and Services (C3b, i.e.,\nstateful components that process data from Widgets and/or Processors to produce desired outcomes)\u00b9. These layers\nare interconnected with Sensors/Inputs and Actuators/Outputs linked to Clients. This setup supports the separation\nof concerns, distributed communication, context storage, and resource discovery. A specialized service, the Context\nService, determines the most suitable Service based on current input data (e.g., through explicit user interactions or\nautomatically determined by context data), switching services to support ongoing user activities."}, {"title": "3.6 Implementation", "content": "The Server is implemented in Python (3.9), chosen for its extensive user base and support of numerous\nML/AI libraries with multiprocessing capabilities. The built-in library, SQLAlchemy, also supports Data Storage.\nThe OHMD or mobile phone clients are developed using Unity3D (2021.3) and MRTK2 to provide\nAR/MR content. This setup accommodates various devices (e.g., HoloLens2 with UWP, Nreal with Android) and enables\nmixed reality capabilities with OpenXR support. Other clients, such as the Smartwatch , designed to\nsense the user, are implemented using WearOS due to its widespread use. Additionally, a web client is\nused for visualization, utilizing HTML/VueJS and NodeJS.\nFor more details, please refer to https://github.com/TOM-Platform and Appendix A, which details the supported\ncapabilities, devices, technologies, and data."}, {"title": "4 DEMONSTRATION DURING DAILY ACTIVITIES", "content": "We have implemented several proof-of-concept services to support daily activities, realizing our vision of an intelligent,\nwearable, proactive assistant."}, {"title": "4.1 During Exercise: Running Assistance", "content": "Scenario. Jack uses Jerry (implemented using TOM) to assist with his running exercises. He wears an OHMD and a\nsmartwatch (connected to a Server operating on a laptop\u00b3). He initiates his running (speed or distance) training using\nvoice interactions. Jerry provides route options, and he selects one using either voice commands or mid-air gestures.\nDuring his run, Jerry provides personalized running coach instructions (e.g., speeding up or slowing down based on his\ncurrent speed, training plan, and user profile) and proactive suggestions (e.g., encouraging feedback based on duration,\nalerting about potential dangers like traffic lights based on environment sensing, giving direction cues based on location,\nindicating waterpoints based on location) using either visual or auditory modality when required (i.e., by default, Jerry\nwill provide only essential details, such as the time, to reduce the display clutter and information overload). At the end\nof his run, he receives a summary of the exercise."}, {"title": "4.2 During Dining and Shopping: Translation and Querying Assistance", "content": "Scenario. Jack visits a new restaurant and discovers that the menu is only in Mandarin, which he cannot understand\n(See Figure 5 for details). He verbally requests Jerry's assistance, and it displays the translated menu in English,\nsuperimposed on the original menu. When he shows prolonged gaze duration on a particular menu item, Jerry"}, {"title": "5 LIMITATIONS AND FUTURE IMPROVEMENTS", "content": "In addition to the identified technical limitations from specific demonstrations, interaction design challenges surfaced\nin daily activities. Situational impairments, especially in dynamic environments, restrict certain user interactions (e.g.,\ndimished voice command accuracy in outdoor wind), underscoring the need for methods to support seamless input\nmodality transitions [25]. Inaccuracies in Al-generated suggestions also contribute to user mistrust, necessitating more\ntransparent Al explanations [14, 18, 40].\nMoreover, TOM's current implementation exhibits limitations. A notable area is the automatic switching of services\nbased on user inputs to optimize service execution for ongoing activities, when multiple services match expected\nassistance, requiring further research in this area. Despite TOM's support for the Large Language Model (LLM) in\nfacilitating human-like conversations and tasks [12, 13, 31, 42], integrating Large Action Models (LAM) [30] could\nenhance interaction efficiency with external applications and improve user action understanding. Effective live moni-\ntoring is available, but TOM needs better visualizations for comprehensive retrospective analysis of long-term user\nbehaviors. Aggregated visualization techniques, similar to ARGUS [15], and retrospective analysis such as PilotAR [24]\ncould aid in this. Additionally, advancing the system's grasp of users' cognitive states and their activity correlations\nrequires sophisticated modeling and simulation [27, 29]. Similarly, facilitating effective multi-agent collaboration while\npreserving user autonomy in multi-user TOM scenarios remains a significant research challenge.\nFinally, developing such systems implicates privacy, security, safety, and ethical challenges. Despite Institutional\nReview Board (IRB) approval for user studies, real-world deployment raises critical privacy, safety, and social acceptability\nconcerns, considering both users and bystanders [3, 16, 36]. Issues include monitoring and recording users' physical and\ncognitive states, capturing bystanders' behaviors without consent, securely handling sensitive data, and anonymizing\ndata for aggregate analysis. Although on-device/edge computing provides partial solutions [43], the limitations of\ncurrent devices necessitate further advancements."}, {"title": "6 CONCLUSION", "content": "We have presented the anticipated capabilities of developing an intelligent wearable assistive system and introduced\nTOM, an architecture and open-source implementation that enables researchers and\ndevelopers to create and analyze assistive applications for supporting daily activities. We welcome contributions from\nthe community to expand its supported devices and usage scenarios. We envision that TOM will serve as a software\nplatform for researchers and developers to develop innovative, intelligent assistance in various tasks, facilitating\nhuman-computer, human-AI, and human-robot interactions. Our future plans include extending TOM's capabilities to\nenable remote robot interactions, where humans can share information (e.g., intentions) with a remote robot to execute\ntasks."}, {"title": "A DETAILED SYSTEM ARCHITECTURE", "content": "Figure 6 depicts the high-level components of the system architecture. The implemented Server currently supports\nthree distinct types of Services: running assistance, providing speed/distance training with directional support; learning\nassistance, enabling inquiries about objects in the frame selected through gestures and/or gaze; and translation assistance,\nwhich facilitates text translation within the frame."}]}