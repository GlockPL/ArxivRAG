{"title": "TOM: A Development Platform For Wearable Intelligent Assistants", "authors": ["NUWAN JANAKA", "SHENGDONG ZHAO", "DAVID HSU", "SHERISSE TAN JING WEN", "KOH CHUN KEAT"], "abstract": "Advanced digital assistants can significantly enhance task performance, reduce user burden, and provide personalized guidance to improve users' abilities. However, the development of such intelligent digital assistants presents a formidable challenge. To address this, we introduce TOM, a conceptual architecture and software platform (https://github.com/TOM-Platform) designed to support the development of intelligent wearable assistants that are contextually aware of both the user and the environment. This system was developed collaboratively with AR/MR researchers, HCI researchers, AI/Robotic researchers, and software developers, and it continues to evolve to meet the diverse requirements of these stakeholders. TOM facilitates the creation of intelligent assistive AR applications for daily activities and supports the recording and analysis of user interactions, integration of new devices, and the provision of assistance for various activities. Additionally, we showcase several proof-of-concept assistive services and discuss the challenges involved in developing such services.", "sections": [{"title": "1 INTRODUCTION", "content": "With advancements in Machine Learning (ML) and Artificial Intelligence (AI) technologies, digital assistants have become integral to everyday life. These include voice assistants like Siri, Alexa, or Google, and some even support visual modality (e.g., [1, 30]), enabling rich inputs and outputs. Similar to science fiction, advanced digital assistants can practically aid users in performing both familiar and new tasks, reduce task load and errors, and enhance task performance [15]. Moreover, these assistants offer personalization, optimizing support for individual needs and broadening accessibility. Despite existing interaction paradigms such as Heads-Up Computing [44] and Dynamicland [38] aiming to realize digital assistance in daily activities, several challenges persist. These include a lack of understanding of the required system capabilities, development guidance, and a platform for rapid development of such assistance (see Sec 2-3 for details).\nTo tackle these challenges, we introduce TOM, an intelligent wearable assistive system developed by identifying user, researcher, and developer needs. TOM facilitates the creation and analysis of assistive applications, enabling context and user understanding and supporting multimodal interactions with AR/MR devices and ML/AI technologies. Through developing several proof-of-concept services (e.g., running coach assistance, translation, and querying assistance), we"}, {"title": "2 RELATED WORK", "content": "Our work, which envisions an intelligent wearable assistive system, is related to context-aware and assistive Augmented Reality (AR) systems."}, {"title": "2.1 Context-Aware Systems", "content": "In general, context can be defined as \"any information used to characterize the situation of an entity. An entity is a person, place, or object that is considered relevant to the interaction between a user and an application, including the user and the application themselves, and, by extension, the environment in which the user and applications are embedded. A system is context-aware if it uses context to provide relevant information and/or services to the user, where relevancy depends on the user's task.\u201d [2, 17]. In the development of context awareness in augmented reality (AR) or pervasive augmented reality, Grubert et al. [21] propose a taxonomy considering context sources (i.e., context factors to which AR systems can adapt), context targets (i.e., content that should be adapted), and context controllers (i.e., how the adaptation should be made). Although they outline high-level categories of context sources (e.g., human, environmental, system factors) and context targets (e.g., system input, system output, system configuration), they do not offer specific guidance for potential capabilities or implementations for context-aware AR systems. While Dey et al. [17] provide guidelines and an architecture for developing context-aware applications through the Context Toolkit, they do not address the various stakeholders' requirements for such systems. Therefore, we first attempt to identify the capabilities for an intelligent wearable context-aware AR system and then adapt the factors and guidelines provided by Dey et al. [17] and Grubert et al. [21] to develop such a system.\nThe Platform for Situated Intelligence (\\psi), developed by Bohus, Andrist, et al. [6, 10], shares certain high-level objectives and architectural similarities with TOM; both systems utilize timestamped multimodal streaming data from various sensors to facilitate rapid development and research within interactive systems. However, TOM specifically focuses on wearable, user-centered AR applications, necessitating distinct requirements and system capabilities such as user sensing, compared to traditional situated interactive systems targeted by \\psi. Moreover, the interactions and interfaces of TOM are optimized to provide intelligent assistance during daily activities while minimizing interference with daily tasks, with the intention of enabling dynamic context- and user-dependent adaptive interfaces."}, {"title": "2.2 Assistive AR Systems", "content": "Augmented and Mixed Reality (AR/MR) have been used to develop assistive systems that augment users' perception with digitally superimposed content in the physical world [7, 37]. These systems have been applied in various domains such as education, training, repair and maintenance, healthcare and medicine, and education due to their advantages in minimizing errors and reducing cognitive load [8, 9, 39]. However, most of these systems are tailored to specific tasks and lack adaptability for various daily activities.\nSeveral toolkits have been proposed for developing assistive AR systems, yet they are often designed for specialized purposes. For instance, RagRug [20] and DXR (Data visualizations in eXtended Reality) [35] are designed for situated"}, {"title": "3 TOM: THE OTHER ME", "content": ""}, {"title": "3.1 Envisioned Usage Scenarios", "content": "Consider Jane, who regularly uses Jerry, a digital assistant developed using TOM, in her daily life. Jerry sees what Jane sees, hears what Jane hears, knows her preferences, and understands her emotional and physical conditions.\nScenario. Unable to decide on a dish to prepare for herself and her toddler and wishing to try something new, she opens the refrigerator and asks, \"Hey, Jerry. Can you suggest a new dish for us?\" Jerry scans and identifies the ingredients in the refrigerator, finds possible dishes, and renders three new dish suggestions with their images. Jane finds the second suggestion appealing and inquires about the preparation process. Jerry then guides her through preparing the new dish, providing real-time, step-by-step feedback superimposed in real-world objects.\nLater, Jane receives a delivery of a play table set for her toddler, ordered through Jerry during an online browsing session. She notices her toddler's eagerness to assist in assembling the set. Examining the package, she asks, \"Hey Tom, can you help me build this?\" By identifying the play table set and retrieving instructions, Jerry displays step-by-step virtual instructions superimposed on the physical parts, which Jane follows while involving her toddler. Suddenly, her toddler accidentally drops a piece of the set, striking his leg and causing him to cry. Jane becomes panicked. Sensing the situation, Jerry instructs her to remain calm and inspect her toddler's leg. As Jane consoles her child, Jerry assesses the situation and provides first-aid instructions. During the first aid, Jerry asks whether to contact her husband, family"}, {"title": "3.2 System Capabilities", "content": "In our quest for an envisioned intelligent wearable assistant, Jerry, we observed that while certain capabilities are supported by existing context-aware and assistive AR/MR systems, a complete integration of these capabilities into a single system is lacking. The Heads-Up Computing Paradigm [44], while theoretically supporting our envisioned use cases, does not provide guidance on implementing such a system or the capabilities required to further research optimal Human-AI interactions during daily activities. Drawing from literature, our experience working with AR/MR and AI researchers, and testing assistive Human-AI interfaces (including early prototypes of TOM) with participants and their feedback, we have formulated the following system capabilities. These are categorized based on three major stakeholders' requirements, which, though distinct, have overlapping capabilities.\nJust-in-time Assistance for Users. Users should be able to interact (C1a) with the system (i.e., provide input and receive feedback) naturally and optimally to obtain the desired assistance [22, 44]. Such assistance should be delivered just in time to match the user's current needs or proactively when users have limited knowledge of system capabilities [4, 32, 41], with minimal interference in the user's ongoing activities while accommodating the user's cognitive capabilities [5, 26]. To achieve this, the system should understand the user (C1b) and context (C1c) to provide the most appropriate feedback to support the user's ongoing activities [17, 44]. Such understanding aids in modeling the human and the world to minimize awareness mismatch between user expectations and system feedback and maintaining profiles [34].\nData Recording and Analysis for Researchers. To understand user interactions with such a system and to design optimal interactions, researchers need to record (C2a), visualize (C2b), and analyze (C2c) the data and develop models [15, 19, 27]. This involves collecting data to support real-time and retrospective observations, training models to predict optimal feedback and analyzing their performance, and understanding the underlying reasons for user and system behaviors [15, 19, 29].\nEase of Development for Developers. Considering the variety of activities users may engage in and their unique assistance requirements, the system should enable developers to create different assistive features easily. This requires that developers can integrate new devices (C3a) easily (e.g., sensors to understand new contexts or actuators to provide optimal feedback), deploy new assistance and models (C3b) (e.g., to predict optimal feedback), and access and control current data (C3c) (e.g., from existing devices or models)."}, {"title": "3.3 Conceptual Architecture", "content": "To support the above requirements, we consider three main entities: user (i.e., the individual receiving assistance), context (i.e., the user's perceptual space and associated tasks), and the system, TOM, as shown in Figure 1a, following the high-level context sources [21].\nSeparating the user from the context enables us to develop user interaction models [44]. These models sense and understand the user (C1b, e.g., cognitive states, affective states, physical states [21]) to provide personalized feedback. Thus, TOM maintains user profiles to cater to individual preferences and capabilities."}, {"title": "3.4 System Architecture", "content": "We develop the following system/physical architecture based on the requirements and envisioned use cases."}, {"title": "3.4.1 Devices and Technologies", "content": "Following a user-centric approach [44], TOM uses wearable devices that align with human input-output channels (e.g., eyes, hands), such as Optical See-Through Head-Mounted Displays (OHMD,"}, {"title": "3.5 Client-Server Architecture", "content": "Given the limited computational resources of wearable devices [15], TOM is implemented as a client-server architecture, as illustrated in Figure 2. The server hosts services for processing and orchestrating data, supports ML/AI inferences, and provides real-time feedback to clients. Clients, such as OHMDs and smartwatches, send sensor data to the server and display feedback to the user. This separation also allows TOM to be device-agnostic, supporting various OHMDs through the same server (C3a).\nServer Architecture. Designed for flexibility and simplicity, the server acts as a one-stop platform for deploying various services optimized for different activities and switching between them as needed. Adapting the architecture of the Context Toolkit [17], the TOM server (Figure 2b) is implemented with independent components under three distinct layers: Widgets (i.e., components that listen for sensors and receive input data), Processors (i.e., stateless components that process and transform input data from Widgets or output data from Services), and Services (C3b, i.e., stateful components that process data from Widgets and/or Processors to produce desired outcomes)\u00b9. These layers are interconnected with Sensors/Inputs and Actuators/Outputs linked to Clients. This setup supports the separation of concerns, distributed communication, context storage, and resource discovery. A specialized service, the Context Service, determines the most suitable Service based on current input data (e.g., through explicit user interactions or automatically determined by context data), switching services to support ongoing user activities.\n\u00b9 TOM does not incorporate Interpreters and Aggregators, unlike the Context Toolkit; their functions are managed by either Processors or Services in TOM, minimizing stateful components to enhance testability."}, {"title": "Client Architecture.", "content": "As shown in Figure 2c, Clients mirror the Server's architecture. However, instead of Services, they interact with the Server to stream sensor data and receive real-time feedback for actuators. Time-critical processing can also be implemented in clients (i.e., on-device) to overcome potential latency issues between the client and server."}, {"title": "Data Flow and Communication.", "content": "Data or messages, tagged with source and time, are transferred between different layers (e.g., Input -> Widget -> Processor -> Service -> Output, Figure 2b, Appendix A-Figure6) via message channels controlled by configuration files (C3c, e.g., Figure3b). This arrangement allows for the reuse of various components across multiple Services and supports distributed communication, thus reducing development efforts. Moreover, each component can store the data it handles in a local database for post-analysis (C2c, e.g., visualization, aggregation) or ML model training (C3). The WebSocket protocol is employed for real-time bidirectional communication between Clients and the Server. REST APIs are used for communication with external APIs, while WebRTC and the Real-Time Streaming Protocol (RTSP) facilitate the streaming of real-time video data."}, {"title": "3.6 Implementation", "content": "The Server (Figure 2a-Y) is implemented in Python (3.9), chosen for its extensive user base and support of numerous ML/AI libraries with multiprocessing capabilities. The built-in library, SQLAlchemy, also supports Data Storage.\nThe OHMD or mobile phone clients (Figure 2a-X1) are developed using Unity3D (2021.3) and MRTK2 (2.8) to provide AR/MR content. This setup accommodates various devices (e.g., HoloLens2 with UWP, Nreal with Android) and enables mixed reality capabilities with OpenXR support. Other clients, such as the Smartwatch (Figure 2a-X2), designed to sense the user, are implemented using WearOS due to its widespread use. Additionally, a web client (Figure 2a-X3) is used for visualization, utilizing HTML/VueJS and NodeJS.\nFor more details, please refer to https://github.com/TOM-Platform and Appendix A, which details the supported capabilities, devices, technologies, and data."}, {"title": "4 DEMONSTRATION DURING DAILY ACTIVITIES", "content": "We have implemented several proof-of-concept services to support daily activities, realizing our vision of an intelligent, wearable, proactive assistant."}, {"title": "4.1 During Exercise: Running Assistance", "content": "Scenario. Jack uses Jerry (implemented using TOM) to assist with his running exercises. He wears an OHMD and a smartwatch (connected to a Server operating on a laptop\u00b3). He initiates his running (speed or distance) training using voice interactions. Jerry provides route options, and he selects one using either voice commands or mid-air gestures. During his run, Jerry provides personalized running coach instructions (e.g., speeding up or slowing down based on his current speed, training plan, and user profile) and proactive suggestions (e.g., encouraging feedback based on duration, alerting about potential dangers like traffic lights based on environment sensing, giving direction cues based on location, indicating waterpoints based on location) using either visual or auditory modality when required (i.e., by default, Jerry will provide only essential details, such as the time, to reduce the display clutter and information overload). At the end of his run, he receives a summary of the exercise."}, {"title": "Limitations.", "content": "During preliminary user testing, we identified several device and technical limitations. These include impaired visibility of the OHMD's visual feedback in outdoor environments, misrecognition of voice commands due to background noise and user fatigue during running, the OHMD's weight affecting the exercise experience, and instability of visual feedback from frequent head movements (content jumps) [23, 25]. Additionally, participants requested adaptive user interfaces tailored to their preferences and environment [28] for enhanced visibility."}, {"title": "4.2 During Dining and Shopping: Translation and Querying Assistance", "content": "Scenario. Jack visits a new restaurant and discovers that the menu is only in Mandarin, which he cannot understand (See Figure 5 for details). He verbally requests Jerry's assistance, and it displays the translated menu in English, superimposed on the original menu. When he shows prolonged gaze duration on a particular menu item, Jerry"}, {"title": "System.", "content": "The current translation assistance system uses an OHMD camera to scan and recognize texts, translates them, and employs a Large Language Model (LLM) to adapt them to the local context. Subsequently, TOM displays the translations aligned with the original menu. Based on the user's explicit (e.g., voice) or implicit (e.g., gaze) inputs, TOM presents the menu images and detailed descriptions and verbally answers questions. Similarly, the querying assistance system scans and recognizes objects and texts using the OHMD camera, based on the user's explicit interactions (e.g., gaze plus voice, gesture plus voice). It then interacts with an LLM to provide responses to the user's verbal queries."}, {"title": "Limitations.", "content": "Similar to the running assistance, additional limitations were noted in the translation and querying assistance. These include the inability to recognize small text in dim lighting, occasional retrieval of incorrect images for specific dishes, and delays (2-8 seconds) in displaying feedback, which is attributable to the response times of external APIs (e.g., ChatGPT, Bing Image Search)."}, {"title": "5 LIMITATIONS AND FUTURE IMPROVEMENTS", "content": "In addition to the identified technical limitations from specific demonstrations, interaction design challenges surfaced in daily activities. Situational impairments, especially in dynamic environments, restrict certain user interactions (e.g., diminished voice command accuracy in outdoor wind), underscoring the need for methods to support seamless input modality transitions [25]. Inaccuracies in Al-generated suggestions also contribute to user mistrust, necessitating more transparent Al explanations [14, 18, 40].\nMoreover, TOM's current implementation exhibits limitations. A notable area is the automatic switching of services based on user inputs to optimize service execution for ongoing activities, when multiple services match expected assistance, requiring further research in this area. Despite TOM's support for the Large Language Model (LLM) in facilitating human-like conversations and tasks [12, 13, 31, 42], integrating Large Action Models (LAM) [30] could enhance interaction efficiency with external applications and improve user action understanding. Effective live monitoring is available, but TOM needs better visualizations for comprehensive retrospective analysis of long-term user behaviors. Aggregated visualization techniques, similar to ARGUS [15], and retrospective analysis such as PilotAR [24] could aid in this. Additionally, advancing the system's grasp of users' cognitive states and their activity correlations requires sophisticated modeling and simulation [27, 29]. Similarly, facilitating effective multi-agent collaboration while preserving user autonomy in multi-user TOM scenarios remains a significant research challenge.\nFinally, developing such systems implicates privacy, security, safety, and ethical challenges. Despite Institutional Review Board (IRB) approval for user studies, real-world deployment raises critical privacy, safety, and social acceptability concerns, considering both users and bystanders [3, 16, 36]. Issues include monitoring and recording users' physical and cognitive states, capturing bystanders' behaviors without consent, securely handling sensitive data, and anonymizing data for aggregate analysis. Although on-device/edge computing provides partial solutions [43], the limitations of current devices necessitate further advancements."}, {"title": "6 CONCLUSION", "content": "We have presented the anticipated capabilities of developing an intelligent wearable assistive system and introduced TOM, an architecture and open-source implementation (https://github.com/TOM-Platform) that enables researchers and developers to create and analyze assistive applications for supporting daily activities. We welcome contributions from the community to expand its supported devices and usage scenarios. We envision that TOM will serve as a software platform for researchers and developers to develop innovative, intelligent assistance in various tasks, facilitating human-computer, human-AI, and human-robot interactions. Our future plans include extending TOM's capabilities to enable remote robot interactions, where humans can share information (e.g., intentions) with a remote robot to execute tasks."}]}