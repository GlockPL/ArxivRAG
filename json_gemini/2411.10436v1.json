{"title": "Mitigating Hallucination in Multimodal Large Language Model via\nHallucination-targeted Direct Preference Optimization", "authors": ["Yuhan Fu", "Ruobing Xie", "Xingwu Sun", "Zhanhui Kang", "Xirong Li"], "abstract": "Multimodal Large Language Models (MLLMs)\nare known to hallucinate, which limits their\npractical applications. Recent works have at-\ntempted to apply Direct Preference Optimiza-\ntion (DPO) to enhance the performance of\nMLLMs, but have shown inconsistent improve-\nments in mitigating hallucinations. To ad-\ndress this issue more effectively, we introduce\nHallucination-targeted Direct Preference Op-\ntimization (HDPO) to reduce hallucinations\nin MLLMs. Unlike previous approaches, our\nmethod tackles hallucinations from their di-\nverse forms and causes. Specifically, we de-\nvelop three types of preference pair data tar-\ngeting the following causes of MLLM hallu-\ncinations: (1) insufficient visual capabilities,\n(2) long context generation, and (3) multi-\nmodal conflicts. Experimental results demon-\nstrate that our method achieves superior per-\nformance across multiple hallucination evalua-\ntion datasets, surpassing most state-of-the-art\n(SOTA) methods and highlighting the potential\nof our approach. Ablation studies and in-depth\nanalyses further confirm the effectiveness of\nour method and suggest the potential for fur-\nther improvements through scaling up.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have been veri-\nfied in various field (OpenAI, 2024; Dubey et al.,\n2024; Sun et al., 2024), while they encounter chal-\nlenges such as hallucination. Multimodal Large\nLanguage Models (MLLMs) are also known to hal-\nlucinate (Bai et al., 2024). Specifically, they often\nproduce unfaithful content that does not align with\nthe visual input, which undermines their reliability\nand practicality, particularly in critical applications\nsuch as autonomous driving (Cui et al., 2024) or\nmedical tasks (Liu et al., 2023a). Hence, address-\ning MLLM hallucination (M-hallu) is essential.\nRecent efforts have aimed at mitigating M-hallu\nthrough various approaches, including inference-\nstage strategies like contrastive decoding (Leng\net al., 2024), and post-hoc corrections that em-\nploy external visual models to refine responses (Yin\net al., 2023). While these methods are simple and\ntraining-free, they do not fundamentally enhance\nthe model's intrinsic capabilities.\nMeanwhile, some pioneer preference optimiza-\ntion methods like Direct Preference Optimization\n(DPO) (Rafailov et al., 2024) have been introduced,\nwhich encourage the model to learn from the com-\nparisons between positive and negative samples, al-\nleviating hallucinations (Zhao et al., 2023; Pi et al.,\n2025). However, most current methods cannot de-\nliver consistent improvements across all types of\nMLLM hallucination tasks (e.g., VQA and caption-\ning tasks, as shown in our experiments of Table\n1). Additionally, it appears that the model's im-\nprovement on specific tasks is closely related to the\nformat of the training data. For instance, the DPO\ndata of SeVa (Zhu et al., 2024) primarily consists\nof VQA, which explains its strong performance on\nVQA-related hallucination evaluation. However,\nits results on captioning tasks are relatively unsatis-\nfactory. Moreover, these methods do not explicitly\nconsider diverse sources of M-hallu. Hence, we\nargue that if we focus on mitigating multimodal\nhallucinations, we should be able to address di-\nverse types of hallucination causes and tasks, and\ndesign hallucination-targeted preference pairs for\nDPO accordingly. Our goal is to comprehensively\nalleviate all multimodal hallucination problems, in-\ncluding both discriminative tasks (e.g., VQA) and\ngenerative tasks (e.g., image captioning).\nDifferent from the hallucinations in LLMs, M-\nhallu primarily arises from the following three as-"}, {"title": "2 Related Work", "content": "Recently, lots of works have explored various ap-\nproaches to mitigating M-hallu (Liu et al., 2023b;\nChen et al., 2024; Lee et al., 2024; Yu et al., 2024a;\nWu et al., 2024), showing promising results and\noffering valuable insights.\nTraining Free Methods. VCD (Leng et al., 2024)\nmitigates M-hallu by reducing model's knowledge\nbias through contrastive decoding, which is effec-\ntive but requires model to inference twice for each\ntoken prediction, resulting in higher memory con-\nsumption and increased overhead in real-world ap-\nplications. OPERA (Huang et al., 2024) tackles\nhallucinations by identifying some common pat-\nterns of decoding attention scores when model hal-\nlucinates and applying special decoding strategies,\nwhile these measures increase inference load and\nslow down processing speed. WoodPecker (Yin\net al., 2023) utilizes external feedback to reduce\nhallucinations, but its reliance on external tools\nadds complexity without enhancing the model's\nintrinsic capabilities. Some works further focus on\nthe internal state of MLLMs to discover their abnor-\nmalities (Zhang et al., 2024). To seek improvement\nin the intrinsic capabilities of models, we turn to\ntraining paradigms as our approach.\nDPO methods for improving MLLM. HA-DPO\n(Zhao et al., 2023) views hallucinations as mod-\nels' preferences. By leveraging ChatGPT (Achiam\net al., 2023) alongside ground truth annotations\nfrom existing image datasets, it generates positive\nexamples aligned with image content, while the\nmodel's original outputs serve as negative exam-\nples for direct preferences optimization. Although\neffective, the construction of negative examples is\nsuboptimal, as it may not fully capture the diverse\nforms of M-hallu. SeVa (Zhu et al., 2024) generates\nnegative examples by introducing noise into images"}, {"title": "2.2 Causes of Hallucinations in Multimodal\nLarge Language Models", "content": "There are substantial works exploring M-hallu, of-\nfering insightful perspectives. VCD suggests that\nlanguage prior within MLLM is a key factor in in-\nducing hallucinations. The Less is More (Yue et al.,\n2024) highlights that hallucinations are more preva-\nlent in longer texts. In contrast, Eyes Wide Shut\n(Tong et al., 2024) identifies limitations in the cur-\nrent CLIP-based visual encoders used in MLLMs,\nshowing that they fail to capture fine-grained de-\ntails. Furthermore, SID (Huo et al., 2024) points\nout that tokens with lower weights in the early\nlayers can trigger subsequent hallucinations. Mean-\nwhile, PhD (Liu et al., 2024b) demonstrates that\nM-hallu stems from conflicts between multimodal\ninformation, and counterintuitive images particu-\nlarly prone to causing hallucinations. Collectively,"}, {"title": "3 Method", "content": "In this section, we provide a brief preliminaries of\nMLLM and DPO, followed by a detailed explana-\ntion of our proposed method for constructing three\ntypes of hallucination-targeted preference data."}, {"title": "3.1 Preliminaries", "content": "Multimodal Large Language Models. MLLMs\nutilize LLMs to predict the probability distribution\nof the next token for each textual input. Given a\nprompt x that includes both an image and a text\nquery, MLLMs generate a corresponding text re-\nsponse y. By incorporating visual information,\nMLLMs enhance the capabilities of LLMs, en-\nabling multimodal understanding.\nDirect Preference Optimization. To better align\nLLMs with human preferences, preference opti-\nmization methods have been developed. Among\nthese, Reinforcement Learning from Human Feed-\nback (RLHF) is a widely recognized method,\nthough it involves training a reward model, which\ncan be quite challenging. In contrast, Direct Prefer-\nence Optimization (DPO) (Rafailov et al., 2024) uti-\nlizes preferences data directly, without the need for\na reward model. This makes DPO the approach we\nemploy. Given a pre-processed preference dataset\nD containing x, yc, and yr, where x represents the\ninput prompt, yc is the preferred response, and yr is\nthe rejected response, DPO optimizes the language\nmodel through the following loss function:\nLd =\n-ED\n[log \u03c3(\nBlog \\frac{\u03c0\u03b8(Ycx)}{\u03c0ref (Ycx)}\n\\frac{\u03c0\u03b8 (Yrx)}{\u03c0ref (Yrx)}\n)]"}, {"title": "3.2 Analysis for Construction", "content": "The primary aim of our approach is to tackle the\nhallucination problem in MLLMs by constructing\nhallucination-targeted preference pairs, rather than\nrelying on general preference data. Without loss\nof generality, we adopt a generalized data format:\nimage-descriptive text data, which we believe more\neffectively captures various forms of hallucination.\nFor DPO in MLLMs, we require a preference\ndataset D, denoted as (I, x, yc, yr), where I is the"}, {"title": "3.3 Sample Construction of Visual Distracted\nHallucination", "content": "Inspired by SID, we induce vision-and-text asso-\nciation hallucinations by leveraging vision tokens\nwith low attention scores in the self-attention mod-\nule. Formally, for the transformer block in the\nauto-regressive decoder, text instructions, vision\ninputs, and generated tokens are concatenated and\nprojected into three vectors: Q, K and V. The self-\nattention computes the relevance of each element\nto the others as follows to get the attention matrix:\nA\n=\nsoftmax(\n\\frac{QKT}{\\sqrt{d}}\n+ M)\nwhere d represents the dimension of Q, K, V, M\nrepresents the casual mask. A \u2208 R(b,h,n,n), where\nb, h, and n denote batch size, number of key-value\nheads, and total token number, respectively. We\ndenote the A\u00bf as the attention matrix after Layer i\nof MLLMs. Then we calculate vision token impor-\ntance scores (Score\u00bf(v)) based on A\u017c:\nScore; (v) = -\\Sigma A(i,j) h\nj=1\nDuring the model's auto-regressive decoding pro-\ncess, we retain the K vision tokens with the lowest\nimportance scores, and the resulting decoded re-\nsponse serves as negative samples. By removing\nthe most important visual token from the model\nin this way, amplifying the influence of relatively\nirrelevant visual tokens, thus constructing visual\ninformation distracted hallucinations as negative\nsamples, uring MLLMs to pay attention to more\nimportant visual information."}, {"title": "3.4 Sample Construction of Long Context\nHallucination", "content": "As previously mentioned, the occurrence of hal-\nlucinations tends to increase as models generate\nlonger responses. To illustrate this more clearly,\nwe present CHAIR scores by varying the 'max new"}, {"title": "3.5 Sample Construction of Multimodal\nConflicts Hallucination", "content": "One of the more challenging yet often overlooked\nscenarios in mainstream evaluation tasks involves\nconflicts between modalities. In such cases, models\nmay naturally favor textual content due to their au-\ntoregressive generating manner and the larger pro-\nportion of the language model component, leading\nto incorrect outputs. To address this, we construct\npositive and negative pairs with conflicting prefixes\nand apply DPO to optimize the model.\nSpecifically, we utilize GPT-40-mini to rewrite\ndetails of the positive examples through prompt-\ning, generating information conflicting with the\nimage contents. These conflicting informations\nare then placed at the beginning of normal ques-\ntions, prompting the model to produce incorrect\nresponses. As shown in fig. 3, the model is indeed\nprone to being hallucinated by the conflicting pre-\nfixes. We take the model's incorrect outputs as\nnegative examples. Further details on the prompts\ncan be found in fig. 9. Unlike previous types of\ndata, the questions for training of MCH contain\nconflicting prefixes, as we aim for the model to\ngenerate correct responses in the query even when\npresented with conflicting information."}, {"title": "3.6 Implement details", "content": "For LCH, which requires longer responses, we\nsampled 6k examples with over 300 tokens from\nShareGPT4V. For MCH, we randomly sampled 6k\nexamples from ShareGPT4V. For VDH, we obtain\n6k examples from ShareGPT4V and 4k examples\nfrom VG with positive examples from HA-DPO\nto enhance data diversity; the preserved K is 500,\nwith other settings aligned with SID (e.g., i = 2).\nDetails of data can be found in appendix D."}, {"title": "4 Experiment", "content": "In this section, we empirically investigate the evalu-\nation of HDPO. We begin by describing the exper-\nimental settings, including the evaluation datasets\nused in our study and training details. Next, we\npresent the results on various hallucination eval-\nuation datasets, demonstrating the promising per-\nformance of HDPO. Additionally, we validate the"}, {"title": "4.1 Experimental Settings", "content": "We evaluate the effectiveness of HDPO in mitigat-\ning hallucinations across both captioning tasks and\nsimplified visual question answering (VQA) tasks\nusing three evaluation datasets as follows:\nCHAIR (Rohrbach et al., 2018) is an evaluation\nmethod used in image captioning tasks to assess\nobject hallucinations in model responses. There\nare two metrics: CHAIRS and CHAIR. CHAIRS\nmeasures hallucinations at the sentence level, while\nCHAIR measures them at the image level respec-\ntively. We conduct the CHAIR evaluation on the\nMSCOCO dataset following the setting in OPERA\nwith 500 random images. For each image, the\nmodel is prompted with: \"Please describe this im-\nage in detail.\" to obtain their descriptions. By de-\nfault, we set the max new token to 512. More"}, {"title": "4.1.2 Training details", "content": "Following previous works (Chen et al., 2023; Zhu\net al., 2024; Pi et al., 2025), we select the LLaVA-\nv1.5-7B (Liu et al., 2024a) and LLaVA-v1.5-13B\nas base models for experiments, which allows for\neasy comparison with other existing works. The\nLLaVA's weights are pretrained and further fine-\ntuned using supervised fine-tuning (SFT) before"}, {"title": "4.2 Results on Hallucination Evaluation", "content": "We compare the performance of HDPO with sev-\neral baselines and the experimental results are\nshown in table 1. We selected Vlfeedback(Li\net al., 2024), POVID(Zhou et al., 2024a), CLIP-\nDPO(Ouali et al., 2025), HA-DPO, SeVa, BPO,\nand CSR(Zhou et al., 2024b) as our baselines. We\ncan observe and analyze as follows:\nHDPO achieves SOTA level on hallucination.\nThe results indicate that HDPO performs well in\nmitigating hallucinations, achieving almost SOTA\nlevel, especially on generative tasks. This outcome\nis natural, as our data contains only descriptive con-\ntent, leading to relatively strong performance on\ngenerative tasks. However, despite not using VQA-\ntype data, HDPO still performs well on VQA tasks.\nThis demonstrates that our approach addresses the\nfundamental issue of hallucinations in the model,\nrather than merely excelling in specific task types.\nWhile there is a slight decrease on AMBER Cover.\nmetric, we believe this is due to an inherent trade-\noff between achieving more complete object cov-\nerage and reducing hallucination rates. Notably,\nHDPO is able to reduce the HalRate from 35.1%\nto 15.8%, representing an improvement of nearly\n55%, with only a minimal drop in Cover. compared\nto the original model, which is delighted to accept.\nIn fact, striking the optimal balance between these\nmetrics remains an open question and lies outside\nthe scope of this paper.\nWe also evaluate HDPO on a comprehensive\nbenchmark, MM-Vet (Yu et al., 2024b), where we\nobserve a slight improvement. This aligns with\nour expectations, as the model is not fine-tuned on\na wide range of tasks and data types, but instead\nfocused specifically on reducing hallucinations.\nBrief analysis on other baselines. Some base-\nlines lack comprehensive performance on halluci-\nnation evaluation. SeVa, though effective on AM-\nBER's discriminative tasks, shows no improvement"}, {"title": "4.3 Results on Different Base LLMS", "content": "We also conduct experiments across base LLMs of\ndifferent sizes to verify our HDPO's universality.\nSpecifically, we apply HDPO to the widely-used\nLLaVA-v1.5-13B for MLLM hallucination evalu-\nation. The results are shown in table 2, demon-\nstrating that the model's performance remains con-\nsistent with expectations, with improvements in\nhallucination mitigation. It also implies that our\ngenerated hallucination-targeted DPO data is effec-\ntive for different LLM sizes."}, {"title": "4.4 In-Depth Model Analyses", "content": "We propose our method for generating different\ntypes of hallucination-based preference pairs. The\nresults from our main experiment demonstrate our\nmethod's superior performance in mitigating hallu-\ncinations. However, do they truly work effectively\nin the scenarios we claim? Below, we briefly de-\nsign two more challenging sub-tasks of hallucina-\ntion that align with our claims, aiming to further\nshowcase the effectiveness of our data construction\nof LCH and MCH."}, {"title": "4.4.1 Longer Description Experiments", "content": "To evaluate the effectiveness of LCH on longer\nresponses, we have conducted an extended experi-\nment on the AMBER generative task. Specifically,\nwhen the model is asked the question \"Describe this\nimage in detail\", we append the instruction \"answer\nin 800 words\" to encourage longer responses. As\nindicated in table 3, HDPO shows good and stable\nperformance in handling longer responses, with the"}, {"title": "4.4.2 Multimodal Conflicts Experiments", "content": "In real-world scenarios, multimodal conflicts are\ncommon when using MLLMs. To better evaluate\nthe model's performance under such conditions, we\ndesign a more challenging task. Specifically, we\nrandomly select 200 questions from the generative\ntask in the AMBER dataset. First, LLaVA-1.5-7B\nis used to generate answers for these questions to\nget coarse-grained image descriptions. Next, GPT-\n40-mini rewrites the details in the descriptions, fol-\nlowing the construction method of MCH. We then\nintroduce the incorrect information as a prefix to\nthe question and ask the model to describe the im-\nage while influenced by the conflicting context.\nThe experimental results are shown in table 4,\ndemonstrating that despite encountering conflict-\ning prefixes, our HDPO maintains promising per-\nformance. Compared to other baselines, HDPO\nachieves the best scores in CHAIR\u300f, HalRate, and\nCognition, along with improved coverage over the\noriginal model, which aligns with our expectations.\nIt reveals that our HDPO shows significant im-\nprovement in the model's performance under this"}, {"title": "4.5 Ablation Study", "content": "To demonstrate the contributions of VDH, LCH,\nand MCH to overall performance, we progressively\nremove each component and report the results. As\nshown in table 5, the model's performance declined\nas we removed each data type. The model achieves\nthe best performance when all three data types are\nincluded. These experimental results confirm the\nindividual contributions of each component."}, {"title": "4.6 Scaling Law of HDPO", "content": "We analyze the impact of data size on our method.\nThe performance of LLaVA-v1.5-7B fine-tuned on\ndatasets of varying sizes but the same proportions\nare shown in fig. 4. As the data size increases, the\neffectiveness of our approach also improves, high-\nlighting the potential for scaling up. This demon-\nstrates the superior performance of HDPO."}, {"title": "5 Conclusion", "content": "In this paper, we present HDPO, a novel approach\ndesigned to effectively mitigate hallucinations in\nMLLMs. We analyze three types of hallucinations\nobserved in MLLMs and create hallucination pref-\nerence data based on the identified causes. Ex-\ntensive experiments across different benchmarks\ndemonstrate the ability of HDPO to reduce hallu-\ncinations in MLLMs, showing effectiveness."}, {"title": "Limitations", "content": "In this paper, we introduce HDPO, which effec-\ntively mitigates the hallucination problem in cur-\nrent multimodal large language models. However,\nseveral issues remain unresolved. Specifically, we\nhave not yet developed distinct strategies for con-\ntrolling data quality, and the generation of auto-\nmated negative examples lacks methods for further\nverification and optimization, which could improve\nthe effectiveness of our approach. Additionally,\nthere may be opportunities to further enhance the\nquality of positive examples. Moreover, our con-\nstruction methods and strategies could potentially\nbe integrated with other techniques for processing\nmore high-quality preference data, which may fur-\nther improve the model's performance. Fine-tuning\nlarger models with extensive, integrated datasets\nmay not only enhance overall reasoning capabili-\nties but also increase the model's robustness against\nhallucinations. This represents a promising area\nfor further investigation, and we leave these open\nquestions for future research."}, {"title": "Ethics Statement", "content": "This work mitigates hallucinations of multimodal\nlarge language models to enhance their reliability\nand practicality. We have carefully considered the\nethical implications of our work. The models and\ndatasets we used are publicly available and com-\nmonly used, and our findings may inherit the biases\nand limitations carried out in these resources."}, {"title": "A Details of AMBER's Metrics", "content": "An Automated Multi-dimensional Benchmark for\nMulti-modal Hallucination Evaluation (AMBER)\nis an LLM-free multi-dimensional benchmark, of-\nfering a cost-effective and efficient evaluation\npipeline. It can be used to evaluate both generative\nand discriminative tasks including hallucinations\nrelated to existence, attributes, and relations. Its\ngenerative evaluation aligns with our desired assess-\nment of long descriptions, while the other dimen-\nsions provide insights into the model's performance\non relatively simple VQA tasks, thereby reflecting\nthe model's hallucination comprehensively. For\nits generative task, four metrics are used: CHAIR,\nCover, Hal and Cog. For its discriminative task, we\ncalculate and report the average F1 score. Addi-\ntionally, we also calculate AMBER Score denoted\nas AMBER-S, which reflects overall performance.\nCHAIR measures the frequency of hallucina-\ntory objects in the responses, Cover evaluates the\nobject coverage, Hal represents the proportion\nof responses containing hallucinations, and Cog\nassesses whether the hallucinations produced by\nMLLMs resemble those found in human cognition.\nAMBER Score is calculated as follows:\nAMBER Score =\n\\frac{1}{2}\n\u00d7 (1 \u2013 CHAIR + F1)"}, {"title": "B Comparison of noise and token\npreservation", "content": "We also conduct experiments to compare the im-\npact of adding noise versus preserving visual\ntokens. Specifically, we use 6k samples from\nShareGPT4V to construct negative samples by in-\ntroducing diffusion noise and preserving visual to-\nkens, and train the LLaVA-v1.5-7B model by direct\npreference optimization. The results of these exper-\niments are presented in table 6. As the experimen-\ntal results show, using visual token preservation\ncan achieve better performance on hallucination\nevaluation."}, {"title": "C Baseline Selection of 13B", "content": "For the experiments on the 13B model, we select\nseveral recent strong baselines, including SeVa and\nCSR, using their open-sourced checkpoints for eval-\nuation. Additionally, we reimplement HA-DPO on\nLLaVA-v1.5-13B, as the original repository does\nnot provide this checkpoint. We also attempt to"}, {"title": "D Details about Our data", "content": "We obtain positive examples for our dataset from\ntwo sources: VG(with positive examples in HA-\nDPO) and Sharegpt4V. After extracting positive\nexamples from ShareGPT4V, we found them to be\ntoo long. To mitigate length bias, we used GPT40-\nmini to rewrite them to match the length of the neg-\native examples. The prompt used is shown in fig. 7.\nFor positive examples sourced from HA-DPO, af-\nter generating negative examples, we followed the\noriginal approach by rewriting the negative exam-\nples using GPT4o-mini. The prompt used is shown\nin fig. 6. Also, we can adopt the method in HA-\nDPO to create more data. For k and i, we make an\nempirical choice based on performance and origi-\nal settings."}, {"title": "D.2 Long Context Hallucination", "content": "We used LLaVA-1.5-7B to continue generating text\nfor the positive examples, with the system prompt\nin fig. 5, and the hint phrases in fig. 8.\nBy excluding the last two sentences, we aim to\nincrease the concentration of hallucinated content\nin the tail of the response. Generating three contin-\nuations at a time maintains an approximate balance\nin the average length between positive and negative\nexamples."}, {"title": "D.3 Multimodal Conflicts Hallucination", "content": "We utilize GPT-40-mini to modify the details of the\npositive examples, following the prompt shown in"}, {"title": "D.4 Effect of data ratio", "content": "We did not conduct detailed experiments compar-\ning different data type ratios. However, throughout\nthe experiments, all tested ratios showed signifi-\ncant improvements over the original model. We\nreport the best-performing dataset from our experi-\nments. Determining the optimal ratio of different\ndata types is inherently a more challenging and\ngeneral problem, which goes beyond the scope of\nthis paper."}]}