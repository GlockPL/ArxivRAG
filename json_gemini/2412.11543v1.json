{"title": "Error Diversity Matters: An Error-Resistant Ensemble Method for Unsupervised Dependency Parsing", "authors": ["Behzad Shayegh", "Hobie H.-B. Lee", "Xiaodan Zhu", "Jackie Chi Kit Cheung", "Lili Mou"], "abstract": "We address unsupervised dependency parsing by building an ensemble of diverse existing models through post hoc aggregation of their output dependency parse structures. We observe that these ensembles often suffer from low robustness against weak ensemble components due to error accumulation. To tackle this problem, we propose an efficient ensemble-selection approach that avoids error accumulation. Results demonstrate that our approach outperforms each individual model as well as previous ensemble techniques. Additionally, our experiments show that the proposed ensemble-selection method significantly enhances the performance and robustness of our ensemble, surpassing previously proposed strategies, which have not accounted for error diversity.", "sections": [{"title": "1 Introduction", "content": "Syntactic parsing, a fundamental task in natural language processing (NLP), refers to identifying grammatical structures in text (Zhang 2020), which can help downstream NLP tasks such as developing explainable models (Amara, Sevastjanova, and El-Assady 2024). Unsupervised syntactic parsing is particularly beneficial for processing languages or domains with limited resources, as it eliminates the need for human-annotated training data (Kann et al. 2019). In addition, autonomously discovering patterns and structures helps to provide evidence to test linguistic theories (Goldsmith 2001) and cognitive models (Bod 2009). Two common types of syntactic parsing include: (1) constituency parsing, organizing words and phrases in a sentence into a hierarchy (Chomsky 1967), and (2) dependency parsing, predicting dependence links between words in a sentence (Tesni\u00e8re, Osborne, and Kahane 2015). The latter is particularly interesting given its benefits to other fields, e.g., RNA structure prediction (Anonymous 2024).\nUnsupervised dependency parsing has seen diverse approaches over decades (Smith and Eisner 2005; Cai, Jiang, and Tu 2017; Han et al. 2020). Yang et al. (2020) show that the joint of two different models can outperform both individuals, suggesting the varied contributions of different models. We follow our previous work on constituency parsing (Shayegh et al. 2024; Shayegh, Wen, and Mou 2024) and build an ensemble of different unsupervised dependency parsers to leverage their diverse expertise.\u00b9 We regard the ensemble output as the dependency structure that maximizes its overall similarity to all the individual models' outputs. For the similarity metric, we employ the unlabeled attachment score (UAS; Nivre and Fang 2017), the evaluation metric of the task.\nIn our work, we observe that a na\u00efve application of Shayegh et al. (2024)'s ensemble is sensitive to weak individuals in dependency parsing. As shown in Figure 1, a best-to-worst incremental ensemble experiment encounters a significant drop in performance when weaker individuals are added. This drop is due to the accumulation of individual errors, which arises from a low error diversity.\nIt is important to distinguish between two types of diversity: expertise diversity and error diversity. The former refers to the phenomenon that different models excel on different subsets of samples, while the latter indicates that models are wrong in different ways for each sample. Although both types of diversity are crucial for successful ensembles (Zhang and Ma 2012), it is essential to emphasize that expertise diversity serves as a motivation to build an ensemble, whereas error diversity is a requirement for its success."}, {"title": "2 Related Work", "content": "Over the years, researchers have proposed various models to tackle unsupervised dependency parsing under different setups (Le and Zuidema 2015; He, Neubig, and Berg-Kirkpatrick 2018; Shen et al. 2021). The longest line of research began with Klein and Manning (2004) introducing the dependency model with valence (DMV), a probabilistic generative model of a sentence and its dependency structure, which is extended by Headden III, Johnson, and McClosky (2009) to include categories of valence and lexical information. In the deep learning era, Jiang, Han, and Tu (2016) utilize neural networks to learn probability distributions in DMV. Han, Jiang, and Tu (2017, 2019a,b) further equip neural DMVs with lexical and contextual information. More recently, Yang et al. (2020) introduce two second-order variants of DMV, which incorporate grandparent-child or sibling information, respectively. In this work, we build ensembles of different models to leverage their diverse knowledge.\nAll of the above work focuses on projective dependency parse structures, meaning that all the dependency arcs can be drawn on one side of the sentence without crossing each other. The projectivity constraint comes from the context-free nature of human languages (Chomsky 1956; Gaifman 1965). Given that most English sentences are characterized by projective structures (Wu et al. 2020), we also follow previous studies and focus on this type of parses.\nUnsupervised dependency parsing typically considers unlabeled structures, i.e., dependency arcs are not categorized by their linguistic types (e.g., subject or object of a verb). We also follow the previous line of research and focus on unlabeled structures."}, {"title": "2.2 Ensemble-Based Dependency Parsing", "content": "Che et al. (2018) and Lim et al. (2018) propose to build ensembles of dependency parsers to smooth out their noise and reduce the sensitivity to the initialization of the neural networks. They use the average of networks' softmax outputs for prediction. This approach is restricted as it requires the models to have the same output space, which does not hold in our scenario as we aim to leverage the knowledge of diverse models with different architectures.\nKulkarni et al. (2022) and Shayegh et al. (2024) show the effectiveness of post hoc aggregation methods for ensemble-based supervised and unsupervised constituency parsing. Kulkarni, Eulenstein, and Li (2024) borrow different post hoc aggregation methods from graph studies, including the maximum spanning tree (Gavril 1987), conflict resolution on heterogeneous data (Li et al. 2014), and a customized Ising model (CIM; Ravikumar, Wainwright, and Lafferty 2010), and employ them for ensemble-based dependency parsing. They compare the performance of these aggregation methods and show the superiority of CIM. However, this method does not ensure the validity of the ensemble output as a projective dependency parse structure. Nevertheless, we include CIM as a baseline which underperforms when applied to our unsupervised setting, performing worse than the best individual.\nIn this work, we develop a dependency-structure aggregation method, based on which we further build an ensemble of unsupervised dependency parsers with different designs."}, {"title": "2.3 Ensemble Selection", "content": "Ensemble selection refers to selecting a set of models to form an ensemble, which has been extensively discussed in the machine learning literature (Kuncheva 2004; Caruana, Munson, and Niculescu-Mizil 2006). Caruana et al. (2004) introduce a forward-stepwise-selection method, inspired by the field of feature selection (Liu and Motoda 2007); in their approach, individuals are incrementally added to the ensemble based on the performance boost on a validation set. Such an approach is prone to overfitting to the validation set (Kohavi and John 1997); in addition, it is a time-consuming process to build the ensemble, as we need to build many ensembles at each increment.\nAnother line of research predicts the success of an ensemble by looking at the individuals' properties (Ganaie et al. 2022; Mienye and Sun 2022). One commonly adopted criterion is to keep diversity among the ensemble individuals (Minku, White, and Yao 2010; Wood et al. 2023a). This brings multiple benefits, such as bringing different expertise (expertise diversity; Zhang and Ma 2012) and smoothing out individual errors (error diversity; Zhou 2012). Two branches of related studies are: (1) how to make a balance between the diversity and the quality of the individuals (Chandra, Chen, and Yao 2006; Wood et al. 2023b), and"}, {"title": "3 Approach", "content": "In this work, we propose an ensemble approach to unsupervised dependency parsing. In general, ensemble methods in machine learning consist of two stages: obtaining individual models and aggregating the individuals' predictions. We will address both in the rest of this section."}, {"title": "3.1 Aggregating Dependency Parses", "content": "We aim to aggregate parses of different individual models, potentially with different architectures and output formats. We propose to have post hoc aggregation methods applied to the dependency parses obtained by previous methods. Inspired by our previous work (Shayegh et al. 2024), we formulate our ensemble under the Minimum Bayes Risk (MBR) framework. Consider a set of individual outputs A1,..., Ak given a sentence. The ensemble output A* is defined as the dependency parse structure maximizing its similarity to all the individuals:\n\n$A^* = \\underset{A \\in \\mathcal{A}}{\\text{argmax}} \\sum_{k=1}^{K} \\text{similarity}(A, A_k)$ \nwhere A is the set of all the possible dependency parse structures and similarity is a customizable similarity measure for dependency parses.\nIn particular, we propose to use the sentence-level unlabeled attachment score (UAS; Nivre and Fang 2017) as the similarity measure because it is the main evaluation metric for unlabeled, projective, unsupervised dependency parsing. Formally, UAS is the accuracy of head detection for every word (Le and Zuidema 2015; Nivre and Fang 2017; Yang et al. 2020). In other words,"}, {"title": "3.2 Ensemble Selection", "content": "In this work, we build an ensemble of various unsupervised dependency parsers. We further propose to perform ensemble selection, as we notice in Figure 1 that an ensemble including all individuals underperforms a moderately sized ensemble.\nWe hypothesize that, if ensemble individuals lack diversity in their errors, they introduce systematic bias and may hurt the ensemble performance. Therefore, it is important to balance the quality of individuals and their diversity. Our ensemble selection objective is to find K \u2286 {1,\u2026\u2026, K} being the set of selected ensemble individuals maximizing\nobjective(K) = $\\sum_{K \\in K} UAS(A_K, A_{gold}) + \\alpha \\cdot \\text{diversity}(\\{A_k\\}_{k \\in K})$"}, {"title": "4 Experiments", "content": "We performed experiments on the Wall Street Journal (WSJ) corpus in the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993). We adopted the standard split: Sections 2-21 for training, Section 22 for validation, and Section 23 for testing. For training and validation, we followed previous work and only used sentences with at most 10 words after being stripped of punctuation and terminals. We used the entire test set for evaluation, regardless of sentence lengths. As all our individuals are unsupervised parsers, we did not use linguistic annotations in the training set. We use an annotated validation set with only 250 sentences for validation during training and ensemble selection."}, {"title": "Ensemble Individuals", "content": "We consider the following unsupervised dependency parsers as our ensemble components:\n\u2022 CRFAE (Cai, Jiang, and Tu 2017), which is a discriminative and globally normalized conditional random field autoencoder model that predicts the conditional distribution of the structure given a sentence.\n\u2022 NDMV (Jiang, Han, and Tu 2016), a dependency model with valence (DMV; Klein and Manning 2004) model that learns probability distributions using neural networks. We used Viterbi expectation maximization to train the model.\n\u2022 NE-DMV (Jiang, Han, and Tu 2016), which applies the neural approach to the extended DMV model (Headden III, Johnson, and McClosky 2009; Gillenwater et al. 2010). We employed Tu and Honavar (2012) for initialization.\n\u2022 L-NDMV (Han, Jiang, and Tu 2017), an extension of NDMV that utilizes lexical information of tokens.\n\u2022 Sib-NDMV (Yang et al. 2020), which applies the neural approach to Sib-DMV, an extension of DMV that utilizes information of sibling tokens.\n\u2022 Sib&L-NDMV (Yang et al. 2020), a joint model incorporating Sib-NDMV and L-NDMV.\nFor L-NDMV, Sib-NDMV, and Sib&L-NDMV, we use Naseem et al. (2010) for initialization. For the hyperparameters and configurations of individuals, we use the default values specified in the respective papers or codebases. We train five instances of each model using different random seeds to assess the stability of our approach. We observe that CRFAE, NDMV, and NE-DMV exhibit instability and lower performance in average than what the authors reported. To achieve comparable performance, we run the training for these models 20 times and select the top 25% based on their performance on the validation set. We report a comparison between the quoted results and our replication results in Appendix B, showing the success of our replication."}, {"title": "Setups of Our Ensemble", "content": "Our aggregation method does not have hyperparameters. However, we can build weighted ensembles with rational numbers as weights by virtually duplicating individuals (Shayegh, Wen, and Mou 2024). We report the results of weighted ensembles where the individuals are weighted by their performance on the validation set.\nFor ensemble selection, we finetune the balancing hyperparameter \u03b1 in Eqn. (8) with one decimal-place precision based on the performance of the selected 5-individual ensembles. Our pilot study shows that weighting individuals in selected ensembles during either the selection phase, the inference phase, or both, does not improve the performance. Therefore, we include the two approaches as alternative performance boosters.\nBaselines. We compare our ensemble approach against individual parsers, some of which are the most competitive methods for the task. Additionally, we compare our aggregation approach against customized Ising model (CIM; Ravikumar, Wainwright, and Lafferty 2010), which calculates per-sample vote weights based on individuals' correlation over samples in classification tasks. Indeed, Kulkarni, Eulenstein, and Li (2024), as the only previous work on post hoc ensemble-based dependency parsing, shows that CIM outperforms alternatives, including maximum spanning tree (Gavril 1987) and conflict resolution on heterogeneous data (Li et al. 2014). We strictly follow them to employ CIM in dependency parsing."}, {"title": "4.2 Results and Analyses", "content": "Table 1 shows the main results on the WSJ dataset, where we performed five runs of each individual and built ensembles of corresponding runs. We may or may not exclude Sib&L-NDMV as it is a fusion model, and one may argue about including it in an ensemble where Sib-NDMV and L-NDMV are also present.\nWe first evaluate the CIM aggregation method (Row 6). The CIM is considered an unweighted approach, as it does not use labeled validation data. We see that this approach does not retain the performance of the best individual (Row 5) and is thus a failed approach."}, {"title": "Ensemble Selection", "content": "Figure 2 illustrates the performance of ensembles by different numbers of selected individuals. The selection methods include using only individuals' performance (w/o diversity), the ensemble objective given in Eqn. (8) employing different diversity metrics, and the classic forward-stepwise-selection approach (ensemble validation).\nOur results show that ignoring diversity lags behind most other methods, underscoring the importance of diversity in ensemble selection. On the other hand, society entropy outperforms the other approaches when a larger number of ensemble components are selected. This is expected, given the higher likelihood of error accumulation with a greater number of weak individuals and that society entropy is the only metric that accounts for error diversity. Moreover, this metric demonstrates significantly more stability than the others for a small number of individuals, while being comparable to Kuncheva's diversity\u2014the best baseline. Overall, society entropy performs consistently well, making it the most reliable approach for selecting any number of ensemble components.\nAdditionally, we present a stability and efficiency analysis in Table 2 for selecting a small number of individuals, as this not only represents a more realistic use case for ensemble selection but also addresses the instability observed in the performance of Kuncheva's diversity and ensemble validation in Figure 2. To this end, we randomly sampled 15 out of the 25 individuals (in Figure 2) 30 times, and for each set, we selected 3, 4, and 5 individuals using different strategies. Results show that society entropy excels and is more stable than the diversity-free and Kuncheva's diversity approaches, achieving performance comparable to ensemble validation but ~ 500x faster, making it a practical method.\nFinally, we report in Table 1 the performance of selected ensembles. Here, we selected comparable numbers of individuals (five models for Rows 9-12 and six models for Rows 16-17) from all individuals across all runs. Our results are consistent with our analysis, demonstrating that our society entropy is comparable with ensemble validation, outperforming other ensemble-selection methods. Moreover, our society entropy-based selected ensemble outperforms all unselected ensembles and individuals, exhibiting the highest unsupervised dependency parsing performance among all competitors."}, {"title": "Inference Efficiency", "content": "In Table 3, we report the inference runtime for every individual, along with the execution time of our aggregation algorithm. While the inference of ensembles is generally slow, as it requires the inference of all the individuals, results show that our proposed aggregation step does not further slow down this process significantly, demonstrating the efficiency of our proposed reduction to the dependency parsing model of Eisner (1996)."}, {"title": "Performance by Dependents' Part-of-Speech (POS)", "content": "We would like to investigate the effect of our ensemble approach from a linguistic perspective. To this end, we report in Figure 3 the breakdown performance of our ensemble and its individuals by the POS tags of the dependents. We note that different individuals have their expertise in different types of dependents. In fact, NE-DMV surpasses the others in NNP and VBD cases, L-NDMV is outstanding in IN, RB, and VB, while Sib-NDMV outperforms other individuals in all other types. In most cases, our ensemble roughly matches the performance of the best individual, outperforming all the individuals in terms of overall performance."}, {"title": "5 Conclusion", "content": "In this work, we propose a post hoc ensemble approach to unsupervised dependency parsing, equipped with a diversity-aware ensemble-selection method. We emphasize the importance of error diversity alongside expertise diversity and introduce society entropy as a measure that accounts for both. Our experiments support our claims, demonstrating the effectiveness of our ensemble approach and the critical role of error diversity in ensemble selection."}, {"title": "Future Work", "content": "We identify future directions from algorithmic, linguistic, and machine learning perspectives. On the algorithmic side, it is intriguing to develop aggregation methods for other structures, including non-projective dependency parses or other graph structures in tasks such as drug discovery. For the linguistic aspect, it is promising to investigate cross-task ensembles by aggregating diverse knowledge of different constituency and dependency parsers. From the machine learning perspective, our ensemble selection is limited to classification tasks or those that can be framed as classification. We aim to explore ensemble-selection approaches, incorporating error diversity, for a broader range of problems.\nIn parallel with our work, Charakorn, Manoonpong, and Dilokthanakul (2024) identify a similar differentiation between expertise diversity (called specialization) and general diversity (which includes error diversity) in multi-agent reinforcement learning (MARL). It further supports our claim that both aspects of diversity should be taken into account, and suggests a great promise for our approach in MARL."}, {"title": "A F\u2081-based Aggregation", "content": "Shayegh et al. (2024) demonstrate the effectiveness of employing F1(Tp, Tr) = $\\frac{2|C(T_p)\u2229C(T_r)|}{|C(T_p)|+|C(T_r)|}$ as the ensemble objective in unsupervised constituency parsing, where C(T) is the set of induced phrases by the constituency tree T. Adopting such a phrasal F\u2081 score requires the notion of phrases in the parse structure, which is not directly available in dependency parsing. To this end, we consider each word in a dependency parse structure, along with its dependents and their descendants, as a dependency phrase, based on which we can compute the F\u2081 score.\nWe may arrange dependency phrases of a dependency parse in a hierarchy (Eroms 2000) which we call Dependency-Based Phrase Structure Tree (DPST; Figure 4). To achieve this, we consider a non-leaf node and a direct leaf node for every word. The non-leaf node of each word becomes the parent of the non-leaf nodes corresponding to the word's dependents. Notice that this is a one-to-one conversion, and we can retrieve the dependency parse structure corresponding to any DPST by constraining the latter to have exactly one leaf node for every non-leaf node. In a DPST, every non-leaf node represents a dependency phrase consisting of its descendant leaves. It reduces our aggregation formulation to the tree averaging in Shayegh et al. (2024)."}, {"title": "A.2 Algorithm", "content": "We develop a dynamic programming (DP) algorithm to find the average of DPSTs. Notice that the average tree must be a DPST to be convertible into a dependency parse structure. The non-binarity and the constraint of DPSTs make our tree-averaging problem different from Shayegh et al. (2024). To preserve the DPST constraint, at each step of DP recursion, we consider one word as the direct leaf of the current node and exclude it from the child phrases. For non-binarity, we allow the DP recursion to exclude some non-leaf nodes and connect their children to their parents. The following is a detailed description along with a complexity analysis of our algorithm.\nWe want to, for a given n-word sentence, find the DPST that maximizes the overall F\u2081 score against a set of individual DPSTs. We denote the search result by\n\nT* = argmax \u2211 F1 (T, Tk) \n\n= argmax \u2211 2|C(T) \u2229 C(Tk)| \n\nwhere T is the set of all possible DPSTs, Tk is the k-th individual, and C(T) denotes the set of dependency-based phrases in T. In a DPST, every non-leaf node has exactly one leaf and leaves represent words. Therefore, |T| = n for every DPST T. This discards the denominator in Eqn. (19):\n T*= argmax \u2211 |C(T) \u2229 C(Tk)| \n\n= argmax \u2211 1 [c\u2208 C(Tk)] \n\nh(c) \nHere, h(c) is the number of occurrences of the phrase c in the individuals, which we call hit count.\nFor our DP algorithm, we define our recursion variable Hbe to be the total hit count of the best substructure over the subsequence b:e of the sentence, starting from bth and ending before the eth word:\n\nHb:e= max \u2211 h(c) \nwhere Tb:e is the set of all possible substructures over b:e. The base cases are Hb:b = 0 and Hb:b+1 = h(b:b+1). The former is for our recursion reference only, while the latter is because the only possible DPST over a single word is a single node having the word as its leaf, representing only a single-word dependency phrase.\nFor recursion, we first force every node to have a binary non-leaf branching. Then, we allow the exclusion of some nodes from the structure, connecting its children to its parents to achieve non-binarity. Note that every included node has to have exactly one leaf, and every excluded node has to have no leaves. This is due to the fact that every non-leaf node in a DPST has to have exactly one leaf. To this end, for every subsequence b:e we define two additional DP variables Hline and Hexcl to be the total hit count of the best substructures including and excluding the phrase b:e, respectively. \nTo find Incl , we need to choose the best word in b:e as the leaf node for the corresponding DPST, i.e., the head of the b:e phrase.\nIncl = max {Hb:j+h(b:e) + Hj+1:e} \nwhere either b:j or j+1:e may be an empty sequence.\nTo find Hexcl, we need to choose the best breaking point to split b:e into two branches.\nHexel= max {Hb:j + Hj:e}"}, {"title": "A.3 Results", "content": "We report the performance of our F\u2081-based ensemble approach in Table 4. We include the corpus-level phrasal F1 score as an evaluation metric to show the effect of our different aggregation methods. We see that the F\u2081-based approach obtains lower performance than the best individual, thus considered a failed attempt. Although the weighted variant (Row 8) is able to outperform all individuals in terms of F1, it still underperforms in terms of UAS, demonstrating the importance of using the task-specific evaluation metric as the ensemble objective.\nAdditionally, Rows 6-9 show that F\u2081 and UAS are not perfectly correlated. Therefore optimizing one does not necessarily improve the other."}, {"title": "B Validation of Replication", "content": "We report in Table 5 the quoted and replicated results for our ensemble components. Since CRFAE, NDMV, and NE-DMV exhibit unstable and subpar performance compared to their quoted results, we train 20 runs each and select the top-five performing runs based on their performance on the validation set. As demonstrated, our replication is successful, providing a fair foundation for the subsequent experiments and analyses."}, {"title": "C Case Studies", "content": "Table 6 presents three cases illustrating how our ensemble selection strategy, using society entropy as the diversity measure, performs against two baselines: one without considering diversity and another using Kuncheva's diversity measure. With each strategy, we select three individuals from a candidate pool. All strategies select one variant of L-NDMV (denoted by L-NDMV1) and one variant of Sib-NDMV, while their third selection differs. The baselines choose another L-NDMV variant (denoted by L-NDMV2) with a UAS performance at around 63%, while our method picks an NE-DMV variant with a much lower performance of around 52%. However, in cases where both L-NDMV1 and NE-DMV err, NE-DMV produces errors that are diverse from those of L-NDMV1, resulting in scattered errors among the selected components, making it easier to eliminate the errors. In contrast, two L-NDMV variants tend to agree as expected, misdirecting the ensemble where they make identical errors.\nTake Table 6a as a particular example. L-NDMV1 assigns the wrong labels at positions 1, 2, 4, and 5, while Sib-NDMV assigns the correct labels. At the aforementioned positions, L-NDMV2 makes the exact same errors, cementing them in the resulting ensembles. By contrast, NE-DMV produces different errors at positions 1 and 4 (error diversity), scattering the errors in the selected ensemble by our selection approach. Furthermore, NE-DMV assigns the correct labels at positions 2 and 5, while having mistakes in other positions like 7, providing its unique expertise (expertise diversity) in support of Sib-NDMV.\nThese examples show the importance of error diversity along with expertise diversity in eliminating errors and achieving a better ensemble performance."}]}