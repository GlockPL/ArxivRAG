{"title": "GraphTeam: Facilitating Large Language Model-based Graph Analysis via Multi-Agent Collaboration", "authors": ["Xin Li", "Qizhi Chu", "Yubin Chen", "Yang Liu", "Yaoqi Liu", "Zekai Yu", "Weize Chen", "Chen Qian", "Chuan Shi", "Cheng Yang"], "abstract": "Graphs are widely used for modeling relational data in real-world scenarios, such as social networks and urban computing. While large language models (LLMs) have achieved strong performance in many areas, existing LLM-based graph analysis approaches either integrate graph neural networks (GNNs) for specific machine learning tasks (e.g., node classification), limiting their transferability, or rely solely on LLMs' internal reasoning ability, resulting in suboptimal performance. To address these limitations, we take advantage of recent advances in LLM-based agents, which have shown capabilities of utilizing external knowledge or tools for problem solving. By simulating human problem-solving strategies such as analogy and collaboration, we propose a multi-agent system based on LLMs named GraphTeam, for graph analysis. GraphTeam consists of five LLM-based agents from three modules, and the agents with different specialities can collaborate with each other to address complex problems. Specifically, (1) input-output normalization module: the question agent extracts and refines four key arguments (e.g., graph type and output format) from the original question, facilitating the problem understanding, and the answer agent organizes the results to meet the output requirement; (2) external knowledge retrieval module: we first build a knowledge base consisting of relevant documentation and experience information, and then the search agent retrieves the most relevant entries from the knowledge base for each question. (3) problem-solving module: given the retrieved information from search agent, the coding agent uses established algorithms via programming to generate solutions, and in case the coding agent does not work, the reasoning agent will directly compute the results without programming. Extensive experiments on six graph analysis benchmarks demonstrate that GraphTeam achieves state-of-the-art performance with an average 25.85% improvement over the best baseline in terms of accuracy.", "sections": [{"title": "1 Introduction", "content": "Graph is a commonly used data structure for modeling relational data. Correspondingly, tasks and methods of graph analysis have been extensively exploited in Al field [2, 5, 41] and achieved significantly success in many fields, such as social network [29, 30] and unban computing [48, 52]. Recently, with the surge of LLM, some researchers begin to explore the feasibility of graph analysis with LLMs [32, 37, 40, 49].\nExisting LLM-based graph analysis methods mainly fall into two categories. The first category [7, 26, 27, 34] usually combines LLMs with graph neural networks (GNNs) for modeling. A typical paradigm is to first encode the input graph with a GNN, and then feed the node encodings to an LLM as language token embeddings. However, this line of work is specialized for graph machine learning tasks, especially node classification, and has limited transferability to general graph analysis problems. The second category [11, 14] usually flattens graphs into textual descriptions, and purely relies on LLMs for analyzing [10, 19, 25, 39, 51]. Strategies like in-context learning [5] and chain-of-thought reasoning [45] are widely used in these methods to improve performance. However, these methods analyze graphs by step-by-step reasoning on raw inputs without utilizing any external knowledge or tools, severely limiting their performance.\nFortunately, LLMs have been shown to be capable of using external knowledge or tools for solving problems, known as a key feature of LLM-based agents [43, 47]. Moreover, inspired by the division of labor and cooperation mechanisms in human society, researchers propose LLM-based multi-agent systems [9, 17, 33, 46], where agents with different specialities can discuss and collaborate with each other to address complex problems. However, previous multi-agent frameworks are designed for general purposes and have few knowledge relevant to graph analysis, and thus fail to outperform the aforementioned methods tailored for graph analysis."}, {"title": "2 Related Work", "content": "2.1 LLM-based Graph Analysis\n2.1.1 Benchmarks. To evaluate the ability of large language models (LLMs) in solving graph analysis problems, researchers have developed various benchmarks [8, 12, 14, 16]. Among them, NLGraph [42] is one of the first benchmark for assessing LLMs' graph analysis capabilities, focusing on basic graph theory reasoning and simple GNN computations. Graphwiz [8] evaluates LLMs on graph problems of varying complexity, including NP-complete problems. GraphInstruct [28] includes 21 graph analysis tasks covering node-level, node-pair-level, and graph-level problems. Talk like a Graph [14] measures basic graph understanding abilities, while LLM4DyG [50] focuses on dynamic graph tasks related to temporal and spatial information. In our experiments, we employ the above five benchmarks and also build an additional one focusing on GNNs for assessing the abilities of different methods.\n2.1.2 Methods. There is a rising trend of research to utilize LLMs for various graph analysis tasks [20, 24]. Here we focus on the most relevant methods that employ LLMs as predictors [7, 26, 27, 34]. Related work can be roughly divided into two categories. The first category typically integrates LLMs with graph neural network (GNNs), and is specialized for graph machine learning tasks such as node classification [40, 49]. For example, GraphGPT [40] employs instruction tuning to fine-tune a GNN-enhanced LLM for node classification. GraphTranslator [49] encodes nodes in a graph by a GNN, and then project node embeddings into the embedding space of LLM tokens for zero-shot classification. However, these methods are designed and optimized for a specific task, and cannot generalize well to the diverse analysis tasks in the above benchmarks. The second category usually flatten graphs into textual descriptions of adjacency lists, and then rely on LLM itself to analyze graphs with strategies like in-context learning [5], chain-of-thought reasoning [45] for inference [10, 19, 25, 39, 51]. For example, GUNDAM [32] collects natural language-based question-answer pairs of graph analysis problems to fine-tune LLMs, and performs step-by-step reasoning for inference. However, recent studies have shown that the reasoning depths of current LLMs are still shallow [21, 23], and will probably fail for graph analysis problems that require complex reasoning. In contrast, we encourage LLMs to leverage external knowledge and tools, instead of performing step-by-step reasoning on raw inputs."}, {"title": "2.2 Multi-Agent Collaboration of LLMs", "content": "Autonomous agents aim to solve problems with self-directed planning and actions. Recently, researchers introduce the concept of LLM-based agent that is capable of understanding natural language instruction, interacting with humans, perceiving external environments, and performing various actions [3, 22, 43, 47]. Moreover, inspired by the collaboration of human in solving problems, researchers introduce LLM-based multi-agent systems, where multiple agents work in cooperation with a due division of labor.\nFor example, ChatDev [33] is a virtual software company where LLM-based agents are responsible for different software development tasks (such as user interface design or software testing) for collaboration. MetaGPT [17] is a meta-programming framework that simulates human workflows. With proper developing pipeline, MetaGPT can generate effective programming solutions. AgentVerse [9] is a multi-agent framework inspired by human collaboration dynamics. It involves four iterative stages of expert recruitment, decision discussion, action and evaluation. AutoGen [46] is another multi-agent framework for general purposes. Through tool invocation and code generation, AutoGen can accomplish a wide variety of tasks. MACRec [44] is a novel framework designed to enhance recommendation systems through multi-agent collaboration. This system includes various specialized agents and can easily solve various recommendation tasks. However, these multi-agent frameworks are not specialized for graph analysis, e.g., they have few knowledge of relevant Python libraries and no experience base of similar problems. Consequently, they can hardly beat the SOTA performance on the aforementioned graph analysis benchmarks."}, {"title": "3 Methodology", "content": "In this section, we propose GraphTeam, a multi-agent system based on LLMs for graph analysis. We will start by introducing the overall framework, and then present the details of each agent in the system."}, {"title": "3.1 Framework Overview", "content": "As shown in Fig. 2, the entire framework of GraphTeam consists of five agents from three functional groups, i.e., question and answer agents for input-output normalization, search agent for external knowledge retrieval, as well as coding and reasoning agents for problem solving.\nInput-Output Normalization: This functional group aims to refine key information from input questions, and map the results to desired output formats. Specifically, the question agent is responsible for processing the original problem descriptions, extracting key arguments to assist the system in solving it. The answer agent is responsible for transferring the formats of calculated results, and conduct self-checking to ensure the consistency with problem requirements.\nExternal Knowledge Retrieval: This functional group only includes the search agent to extract relevant information from external knowledge base. Specifically, we first build a knowledge base for graph analysis based on documentation of popular Python libraries and experiences of previous problem-solving processes. With problems refined by the upstream question agent as queries, the search agent then retrieves information beneficial to downstream problem solving from the knowledge base.\nProblem Solving: This functional group aims to calculate the results based on the refined problem of question agent and the extracted knowledge of search agent. Specifically, the coding agent attempts to write Python codes to solve the problem. When the generated codes cannot be executed normally, a retry mechanism will be employed to fix the errors in the codes. If the coding agent still fails after several retries, the reasoning agent will solve the problem directly without programming."}, {"title": "3.2 Question Agent", "content": "Abstraction is a preliminary step for human beings to solve problems [36]. Inspired by this, we introduce a question agent to extract and refine four key arguments from the original problem descriptions. Formally, given a graph analysis problem with its original description denoted as q, we employ an LLM denoted as Question(q) to generate refined question $q_r$, graph type $q_t$, input graph $q_g$ and output format $q_f$:\n$(q_r, q_t, q_g, q_f) \\leftarrow \\text{Question}(q),$   (1)\nwhere $q_r$ is a condensed version of original instruction, $q_t$ indicates the graph type (e.g., directed, undirected, weighted, etc.), $q_g$ contains the graph to be analyzed, and $q_f$ specifies how the answer should be presented (e.g., numerical value, list, etc.). By explicitly identifying these key information, downstream agents can better understand a task and its requirements, thereby enhancing the overall analysis capability."}, {"title": "3.3 Search Agent", "content": "During the solving process of a hard problem, it is common for human beings to seek help from external sources [13] or make analogy to a previously solved one [6]. Therefore, we first build a knowledge base with documentation and experience information to enhance the programming ability of the multi-agent system, and then ask the search agent to retrieve relevant entries from the knowledge base for each question.\n3.3.1 Knowledge Base Construction. The knowledge base has two parts, i.e., documentation base $K_{doc}$ and experience base $K_{exp}$."}, {"title": "3.4 Coding and Reasoning Agents", "content": "Given the outputs of question and search agents, the coding agent will generate and execute Python codes to solve the problems. In practice, the generated codes may encounter compilation or running errors. Following the trial-and-error strategy [35, 36] for problem solving, we introduce a retry mechanism that attempts to fix the codes with previous codes and error messages. More formally, the operation of coding agent in the n-th trial can be written as:\n$(\\text{result}_a, \\text{code}_g, \\text{error}_a) \\leftarrow \\text{Coding}(q_r, q_t, q_g, q_f, \\text{knowledge}_q, {\\{\\text{code}\\}}_{i=1}^{n-1}, {\\{\\text{error}\\}}_{i=1}^{n-1}),$   (3)\nwhere resulta, codeg, errora are respectively the execution result, generated code, and error message of the n-th trial. The last two parameters are the codes and error messages from previous n - 1 trials. If the codes runs normally, we define errora as None.\nIf the coding agent fails to fix the codes before reaching the maximum number of trials, we will employ the reasoning agent to directly answer the question without programming:\n$\\text{result}_q \\leftarrow \\text{Reasoning}(q_r, q_t, q_g, q_f).$   (4)"}, {"title": "3.5 Answer Agent", "content": "Based on the output format qf, the answer agent will organize the results given by the coding or reasoning agents as final results. Similar to the retry mechanism of coding agent, we also introduce a self-checking mechanism that iteratively refines the results according to format requirement qf. Formally, the operation of answer agent can be written as:\n$\\text{output}_n \\leftarrow \\text{Answer}(q_f, \\text{output}_{n-1}),$   (5)\nwhere outputn is the output in the n-th iteration, and output0 is resultq generated by the coding or reasoning agents."}, {"title": "3.6 Summary", "content": "We formalize the inference pipeline of GraphTeam in Alg. 2. Most agent functions (including Question, Coding, Reasoning and Answer) in Alg. 2 are all implemented by an LLM with different prompt templates. The Search function is implemented based on LlamaIndex [1], and the Solver function in Alg. 1 is implemented by a simplified version of GraphTeam without experience base."}, {"title": "4 Experiments", "content": "In this section, we conduct experimental evaluation to answer the following research questions (RQs): RQ1: How effective is our proposed GraphTeam compared to the state-of-the-art (SOTA) baselines? RQ2: Have each component of GraphTeam played their roles effectively? RQ3: How does our framework perform with respect to different task categories and output formats of graph analysis problems? RQ4: How do the hyper-parameters of certain components in the system have an impact on the overall performance?"}, {"title": "4.1 Experimental Setup", "content": "4.1.1 Datasets. To evaluate the effectiveness of our system in solving graph analysis problems, we use six benchmarks for LLM-based graph analysis: Talk like a Graph [14] primarily studies some fundamental problems in graph analysis and explores the cognitive abilities of LLMs regarding graphs. LLM4DyG [50] focuses on basic problems related to dynamic graphs, while GraphWiz [8], NLGraph [42], and Graph Instruct [28] are used to evaluate the understanding and mastery of algorithms in fundamental graph theory.\nGNN-AutoGL (details shown in Appendix B) is a benchmark we constructed in this paper to study the ability of LLMs in deploying GNNs with AutoGL [15].\n4.1.2 Metrics. For benchmarks with definite answers [8, 14, 28, 42, 50], we use accuracy as the evaluation metric. For GNN-AutoGL, since the task requires a complex pipeline (including identifying proper APIs, filling correct hyper-parameters, successfully training and evaluating the corresponding GNN model on a specified dataset, we evaluate whether the generated codes can be executed correctly and whether the key hyper-parameters (e.g., dataset name, model name, hidden dimension, dropout) are correct as the evaluation criteria.\nSpecifically, we extract the generated code from the model response and compare the execution result with the reference solution to assess whether the model-generated results match the given standard answers or reference solutions. For the first five benchmarks [8, 14, 28, 42, 50], the evaluation results are binary based on the exact match between the execution results and the standard answers or reference solutions. For GNN-AutoGL, we first evaluate whether the code can be executed without errors; if it cannot, it scores 0. Otherwise, we check whether the hyper-parameters(eg. model name, dataset name, hidden dimension, dropout) meet the requirements. Each correct hyper-parameter earns partial credit, and if all hyper-parameters are correct, the score is 1.\n4.1.3 Settings. We choose GPT-40-mini [31] as the base LLM for all agents of our GraphTeam. 4.1.4 Baselines. We directly compare with the previously reported SOTA performance for each benchmark."}, {"title": "4.2 Main Results (RQ1)", "content": "We present the results in Tab.1. GraphTeam surpasses previous SOTA by 1.83-44.61% in accuracy.  To further demonstrate GraphTeam's performance on challenging problems, we assess its performance on the \"hard\" subset of each benchmark."}, {"title": "4.3 Ablation Study (RQ2)", "content": "To evaluate the contribution of each component, we conduct a systematic ablation study. We assess the impact of Input-Output Normalization by removing the question or answer agent. We examine the influence of Knowledge Retrieval components in the search agent, specifically documentation and experience. Additionally, the effectiveness of Problem Solving designs are evaluated by eliminating the coding or reasoning agent, as well as the retry mechanism from the coding agent. This comprehensive ablation enables us to quantify each component's significance to the system performance.\nInput-Output Normalization: For most benchmark problems, the question agent significantly contributes (25.44%) by breaking the problem into individual components.The answer agent's correction of the format is also an essential part of correctly answering the questions.\nExternal Knowledge Retrieval: The search agent plays a significant role. When solving graph analysis problems, providing the large language model with prior knowledge of related problem-solving experiences or API documentation can help the model solve the problem more effectively. They can derive better solutions from prior experiences, effectively addressing similar problems. Documentation can enhance the model's programming capabilities.\nProblem Solving: The coding agent plays a crucial role. Without programming and relying solely on reasoning, the system's performance would significantly decrease, indicating that programming is an effective approach for solving graph analysis problems. The retry mechanism does not always have a significant overall effect."}, {"title": "4.4 Analysis on Different Groupings (RQ3)", "content": "To provide a more nuanced understanding of GraphTeam's capabilities and limitations, we conduct a detailed analysis of its performance across various task categories and output formats."}, {"title": "4.5 Hyper-parameter Sensitivity (RQ4)", "content": "We conduct a study on key hyper-parameters using the NLGraph [42] benchmark to determine optimal settings or optimization strategies. Fig. 5 presents our analysis of several important hyper-parameters. For the number of candidate experiences Nexp in knowledge base construction, our experiments show that larger Nexp values correlate with better system performance, as they provide more candidate experiences for problem-solving assistance. The similarity matching threshold  between experiences and documents achieves optimal problem-solving performance at 0.85, enabling correct matching of similar experiences when available and document retrieval when experiences are absent."}, {"title": "5 Conclusion", "content": "To address the limitations of previous LLM-based graph analysis methods, we draw inspiration from human problem-solving processes, and design a multi-agent system named GraphTeam based on LLMs. GraphTeam consists of five agents from three key modules, namely input-output normalization, external knowledge retrieval, and problem solving. The agents can cooperate with each other for solving complex graph analysis problems. Extensive experiments on six benchmarks demonstrate that GraphTeam achieves state-of-the-art results with an average improvement of 25.85% in terms of accuracy. For future work, we will consider to fine-tune the LLMs in GraphTeam for better performance, and further optimize the system pipeline to accommodate more tasks."}, {"title": "A Prompt Templates", "content": "A.1 Question Agent\nRequirement Analyst"}, {"title": "A.2 Search Agent", "content": "Research Assistant"}, {"title": "A.3 Coding Agent", "content": "Graph Learning Specialist"}, {"title": "A.4 Reasoning Agent", "content": "Graph Learning Expert"}, {"title": "A.5 Answer Agent", "content": "Output Format Specialist"}, {"title": "B GNN-AutoGL Benchmark Construction", "content": "For the GNN-AutoGL benchmark, we use a unified template for construction. Specifically, we ask the large language model (LLM) to write code that constructs a GNN model using AutoGL based on the given prompt. Given the numerous model parameters, specifying each parameter and requiring the LLM to write corresponding model code is challenging. Therefore, we select only some parameters as requirements and evaluate whether the LLM correctly includes them. For a problem with K specified parameters, if the code runs successfully, we use a Python script to assess the correctness of the k specified parameters, resulting in a score of k/K. If the code fails to run, the score is zero."}]}