{"title": "Learning Robust Named Entity Recognizers From Noisy Data With Retrieval Augmentation*", "authors": ["Chaoyi Ai", "Yong Jiang", "Shen Huang", "Pengjun Xie", "Kewei Tu"], "abstract": "Named entity recognition (NER) models often struggle with noisy inputs, such as those with spelling\nmistakes or errors generated by Optical Character Recognition processes, and learning a robust NER\nmodel is challenging. Existing robust NER models utilize both noisy text and its corresponding gold\ntext for training, which is infeasible in many real-world applications in which gold text is not available.\nIn this paper, we consider a more realistic setting in which only noisy text and its NER labels are\navailable. We propose to retrieve relevant text of the noisy text from a knowledge corpus and use it\nto enhance the representation of the original noisy input. We design three retrieval methods: sparse\nretrieval based on lexicon similarity, dense retrieval based on semantic similarity, and self-retrieval\nbased on task-specific text. After retrieving relevant text, we concatenate the retrieved text with the\noriginal noisy text and encode them with a transformer network, utilizing self-attention to enhance\nthe contextual token representations of the noisy text using the retrieved text. We further employ a\nmulti-view training framework that improves robust NER without retrieving text during inference.\nExperiments show that our retrieval-augmented model achieves significant improvements in various\nnoisy NER settings.", "sections": [{"title": "1. Introduction", "content": "Named entity recognition (NER) (Chinchor and Robin-\nson, 1997) is an important task in natural language process-\ning that has a variety of applications and serves as a foun-\ndation for several downstream tasks such as machine trans-\nlation (H\u00e1lek, Rosa, Tamchyna and Bojar, 2011), intelligent\nquestion answering (Lee, Hwang and Jang, 2007), knowl-\nedge graphs (Thukral, Dhiman, Meher and Bedi, 2023), and\nevent extraction (Zhang, Wei, Li and Yan, 2022; Edouard,\nCabrio, Tonelli and Le Thanh, 2017). The performance of\nNER directly impacts the performance of these downstream\ntasks. However, current NER models are trained on clean\ntext and pre-trained models used for NER are also based\non clean text. In reality, input text may contain noises,\ne.g., spelling mistakes or errors from Optical Character\nRecognition (OCR) processes, which can lead to a drop\nin NER performance and propagate errors to downstream\ntasks. Therefore, robust NER that can handle both clean and\nnoisy text is a valuable task in need of further research.\nExisting models (Namysl, Behnke and K\u00f6hler, 2020,\n2021) for robust NER rely on the use of both noisy text and\nits corresponding gold text for training. Specifically, they are\ntrained to output similar results from noisy and gold texts."}, {"title": "2. Problem Definition", "content": "We model the named entity recognition (NER) task\nas a sequence labeling problem. Given a sentence w =\n{x1,..., xn} with n tokens, sequence labeling aims to predict\na label sequence y = {1,\u2026, yn} for each position. The\nBIO tagging scheme, which stands for Beginning, Inside\nand Outside subtags, is often used."}, {"title": "3. Method", "content": "Our proposed framework consists of two main compo-\nnents: a Retrieval Module and a NER model with multi-view\nlearning, as depicted in Figure 3. The Retrieval Module is\nused to obtain text related to the input text from a back-\nground knowledge corpus. The NER model is a transformer-\nbased model with two views. In the retrieval-based view, we\nconcatenate the original noisy text with the retrieved text\nand input them into the transformer-based model, hoping\nto enhance the contextual token representation of the noisy\ntext leveraging the information from the retrieved text. In the\noriginal noisy text view, the noisy text is directly inputted\ninto the NER model without retrieval. We employ multi-\nview learning to encourage two views to produce similar\ncontextual embeddings or predicted label distributions, ul-\ntimately improving the accuracy of the original noisy text\nview, which can be used when retrieval is impossible or\nundesirable."}, {"title": "3.1. Retrieval Module", "content": "We design three retrieval methods: sparse retrieval based\non lexicon similarity, dense retrieval based on semantic\nsimilarity, and self retrieval based on task-specific text."}, {"title": "3.1.1. Sparse retrieval based on lexicon similarity", "content": "Wikipedia, the largest encyclopedia in the world,\nboasts a vast collection of clean and well-organized text on a\nwide range of topics and domains. The simplest method for\nretrieving information from Wikipedia is through the BM25\nalgorithm (Robertson and Walker, 1994), a string matching-\nbased approach."}, {"title": "3.1.2. Dense retrieval based on semantic similarity", "content": "The BM25 algorithm requires that retrieved strings con-\ntain at least one word from the input query, making it less\neffective for retrieving results from noisy text containing\nerrors. Fuzzy retrieval based on Levenshtein distance may\nbe used instead. However, due to the vast size of Wikipedia,\nthe word with the smallest Levenshtein distance to the noisy\ntext may not necessarily be the correct word, resulting in\nsub-optimal results. To address this, we apply a dense re-\ntrieval method, which encodes sentences into embeddings\nand then retrieves text through K-Nearest Neighbors. To\nobtain the encoder, we utilize contrastive learning (Gao\net al., 2021) to fine-tune a pre-trained model using clean\ntext from Wikipedia and its noisy version with simulated\ncorruptions. We use the InfoNCE loss (Oord, Li and Vinyals,\n2018) as the training loss:\nL\u2081 = -log \\frac{e^{sim(h^{noisy}_{i}, h^{gold}_{i})/\u03c4}}{\\sum_{j=1}^{N}e^{sim(h^{noisy}_{i}, h^{gold}_{j})/\u03c4}} \\qquad (1)\nwhere hnoisy is the embedding of noisy text, hgold is the\nembedding of the corresponding gold text, \u03c4 is the tem-\nperature hyperparameter, sim (\u00b7,\u00b7) is the cosine similarity,\nand N is the batchsize. We select the best encoder model\nby measuring recall@k of using simulated noisy text of\nWikipedia to retrieve the corresponding gold text. recall@k\nis measured by comparing the top k retrieved results with the\ngold text and calculating the proportion of times the gold text\nis included in the top k results, as shown by the following\nequation:\nrecall@k = \\frac{\\sum_{i=1}^{n}[[gold_{i} \\in R_{topk}(noisy_{i})]]}{n} \\qquad (2)"}, {"title": "3.1.3. Self retrieval based on task-specific text", "content": "The dense retrieval method encodes an entire sentence\ninto a single embedding. A more fine-grained retrieval\nmethod is to use BERTScore (Zhang et al.), which calculates\nthe average maximum cosine similarity between the contex-\ntual token embeddings of the input sentence and the retrieved\nsentence, which is then used to determine the precision (P)\nand recall (R). We use Fl-score (harmonic mean of P and\nR) to rank the retrieved sentences. However, BERTScore\nhas high computational complexity and a large memory\nfootprint, making it difficult to use on large datasets like\nWikipedia. Therefore, we apply it to retrieve text from the\ntraining dataset itself, which is small and contains sentences\nof the same domain as the query sentence. For the dataset\nself, it is smaller and from the same domain, where the same\nwords may have the same semantics, appear multiple times\nand may or may not be changed by a noising process. The\nBERTScore's F1-score can be used to retrieve and rank the\nnoisy dataset itself. Since our experiment is inductive, with\ntest instances being independent of each other, we retrieve\nfrom the same training set for all sets: training, validation,\nand test."}, {"title": "3.2. NER model with multi-view learning", "content": "For the NER model, we use a pre-trained transformer-\nbased model and a CRF layer as the backbone. Our method\nemploys two views: the original noisy text view and the\nretrieval-based view. Both views share the same parame-\nters in their pre-trained transformer-based models and CRF\nlayers. The retrieval-based view combines the noisy text\nwith the corresponding retrieved text from the Retrieval\nModule to form a concatenated input [x, [X], x], where\nx is the noisy text, x is the corresponding retrieved text,\nand [X] is a special token dependent on the pre-trained\ntransformer-based model used by NER (e.g., \"[SEP]\" in\nBERT and \"\\</s>\" in RoBERTa). The transformer-based\nembedding model, utilizing its self-attention mechanism,\nenhances the contextual token representations of the noisy\ntext by the retrieval text, ultimately leading to improved NER\nperformance. Note that in the retrieval-based view, we only\ninput the embeddings of the original noisy text into the CRF\nlayer, discarding the embedding of the retrieval text since we\ndo not need to predict their labels.\nAs discussed earlier, retrieval is slow and may not be\nsuitable for time-sensitive scenarios. Therefore, we use\nmulti-view learning to improve the original noisy text view\nwith the help from the retrieval-based view. For the original\nnoisy text view, denote the contextual token representations"}, {"title": "Ltext (\\theta) = -log p_{\\theta}(\\hat{y}|x) \\qquad (4)", "content": "Similarly, the negative log-likelihood loss of the retrieval-\nbased view is\nLretrieval (\\theta) = -log p_{\\theta}(\\hat{y}|x, \\hat{x}) \\qquad (5)\nTo jointly train the two views by multi-view learning, we\nuse L2 distance or KL distance to reduce the gap between\nthe two views. Specifically, the L2 distance is the distance\nbetween the token representations of the two views after the\ntransformer-based embedding model:\nL_{L_2} (\\theta) = \\sum_{i=1}^{n} ||r_i - r'_i||^2 \\qquad (6)\nThe KL distance is the distance between the predicted label\ndistributions of the two views after the CRF layer:\nL_{KL}(\\theta) = KL \\big(p_{\\theta}(y|x, \\hat{X})||p_{\\theta}(y|x)\\big) \\qquad (7)\n= CE \\big(p_{\\theta}(y|x, \\hat{X}), p_{\\theta}(y|x)\\big) - H\\big(p_{\\theta}(y|x, \\hat{X})\\big) \\qquad (8)\n\\equiv CE \\big(p_{\\theta}(y|x, \\hat{X}), p_{\\theta}(y|x)\\big) \\qquad (9)\n= - \\sum_{y \\in \\mathcal{Y}(x)} p_{\\theta}(y|x, \\hat{x}) log \\, p_{\\theta}(y|x) \\qquad (10)\n= -\\sum_{i=1}^{n} q_{\\theta}(y_i | x, \\hat{x}) \\log q_{\\theta} (y_i | x) \\qquad (11)\nwhere KL (||) denotes the Kullback-Leibler divergence,\nCE (||) represents the Cross-Entropy, H (.) signifies the\nEntropy, and q_{\\theta} (y_k| *) is proportional to the product of the\nforward score and backward score generated by the classic\nforward-backward algorithm."}, {"title": "L(\u03b8) = Ltext(\u03b8) + Lretrieval (\u03b8) + LMV(\u03b8) \\qquad (12)", "content": "In the training phase, we simultaneously train the two\nviews by optimizing the sum of the loss functions:"}, {"title": "4. Experiment", "content": "In light of the absence of publicly accessible noisy\ndatasets specifically designed for Named Entity Recogni-\ntion (NER) tasks \u2013 with an emphasis on noisy text rather\nthan noisy labels \u2013 we have undertaken the development\nof our own noisy datasets. These custom datasets are con-\nstructed using two established NER corpora as a foundation:\nthe WNUT-17 dataset (Derczynski, Nichols, van Erp and\nLimsopatham, 2017) collected from social media and the\nCONLL-03 English dataset (Tjong Kim Sang and De Meul-\nder, 2003) derived from news.\nWe induce two types of noise into clean text, i.e., spelling\nmistakes and errors caused by Optical Character Recog-\nnition (OCR). To simulate spelling mistakes, we employ\nthe NAT model proposed by Namysl et al. (2020) (Namysl\net al., 2020). This model uses a character confusion ma-\ntrix to generate errors, where the probabilities of insertion,\ndeletion, and substitution are each equal to p/3, given an\noverall noise level of p. The alphabet \u03a3, which excludes\nthe symbol &, comprises all letters present in the original\ncorpus. In applying this method, we first insert the & symbol\nbetween every pair of adjacent letters in the original text,\nas well as before the first letter and after the last letter.\nNext, we utilize the character confusion matrix to create the\nmodified sequence. Specifically, insertions transform & into\na character from \u2211, deletions change the original character\nto e, and substitutions replace the original character with\nanother distinct character from \u03a3. Finally, we remove the\n\u025b symbols to produce the resulting noisy text containing\nspelling errors.\nIn order to simulate errors introduced by OCR, we make\nuse of the Text Recognition Data Generator (TRDG) pack-\nage to convert each sentence into an image, employing 90\ndistinct fonts. Subsequently, we introduce random distor-\ntions to the sentences within the images or incorporate back-\nground noise. To extract potentially noisy text from these\nimages, we employ the Tesseract-OCR engine. Considering\nthat OCR may alter sentence length, we align the noisy OCR\nsentences with their respective clean counterparts using the\nLevenshtein Distance Algorithm, consistent with prior work\n(Namysl et al., 2021). In our approach, the number of labels\ncorresponds to the total number of words recognized follow-\ning the OCR process. This differs from previous research\n(Namysl et al., 2021), where the number of labels equaled\nthe number of original words, which does not reflect real-\nworld scenarios."}, {"title": "4.2. Retrieval Module Configuration", "content": ""}, {"title": "4.2.1. BM25 retrieval (sparse retrieval)", "content": "We employ the 2022-07-01 version of the Wikipedia\ndata dump as our primary data source, which is subse-\nquently converted into plain text format for efficient pro-\ncessing. For the implementation of the BM25 retrieval al-\ngorithm, we utilize Elasticsearch to index and retrieve sen-\ntences within the Wikipedia corpus. In this configuration, the\nsmallest searchable unit is defined as a single sentence. Fur-\nthermore, the matching algorithm is configured to \u201cmatch\u201d,\nsignifying that the retrieved results must contain a minimum\nof one word from the query sentence."}, {"title": "4.2.2. Dense retrieval", "content": "In our dense retrieval experiment, we utilize the entire\nWikipedia corpus to retrieve, consistent with the BM25\nretrieval method, and an encoder trained using the unsu-\npervised SimCSE approach (Gao et al., 2021). The encoder\nis fine-tuned on the XLM-ROBERTa-large model through\ncontrastive learning, employing simulated noisy text and\nclean text from Wikipedia. Two distinct noise induction\ntechniques, spelling mistakes and OCR errors, are applied\nseparately. In order to address challenges over RAM usage\nand computational speed and inspired by BERT-whitening\n(Su, Cao, Liu and Ou, 2021), we generate sentence embed-\ndings, perform PCA for dimensionality reduction, and save\nthe embeddings as npy files. We leverage the FAISS library\nto implement the Inverted File System (Mazur, 1979) using\nthe index_factory function, incorporating the Hierarchical\nNavigable Small World (HNSW) technique (Malkov and\nYashunin, 2018) and the cosine distance metric. Further\ndetails can be found in the Appendix A."}, {"title": "4.2.3. BERTScore retrieval (self retrieval)", "content": "The official implementation of BERTScore can only\ncalculate paired inputs, which is very slow for a large number\nof query and target sentences. We modify the source code by\nfirst encoding all the query and target sentences to contextual\ntoken embeddings, then doing matching for each pair of\nsentences, and finally outputting their similarity scores. It is\nworth noting that each noisy text is tested individually and\ndoes not interfere with each other, i.e., inductive inference.\nTherefore, we independently retrieve texts for the training,\ndevelopment, and testing sets from the same source corpus,\ni.e., the training set."}, {"title": "4.2.4. Postprocessing", "content": "We use top-10 retrieved results. For better utilization\nof retrieval results of Wikipedia, we follow previous work\n(Wang, Shen, Cai, Wang, Wang, Xie, Huang, Lu, Zhuang,\nTu, Lu and Jiang, 2022c) and use three options, i.e., using\nthe matched paragraph, using the matched sentence, and\nusing the matched sentence but removing wiki anchors, i.e.,\nmention hyperlinks, which can provide rich and useful clues"}, {"title": "4.3. NER Module Configuration", "content": "We use XLM-ROBERTa-large (Conneau et al., 2020) as\nthe pre-trained transformer-based model, tune the learning\nrate of the transformer within [1\u00d7106,1\u00d7105], and use grid\nsearch to find the ratio of CRF learning rate and transformer\nlearning rate within [3000, 12000]."}, {"title": "4.4. Main Results", "content": "We employ entity-level F1 scores as the evaluation met-\nric for Named Entity Recognition (NER). For each experi-\nmental setting, we run four different seeds and then average\nthe results. The NER performance is shown in Table 3 and\nTable 4 for spelling mistakes and OCR noise respectively."}, {"title": "4.5. Comparing the model using noisy text vs. using clean text", "content": "We do a cross-experiment using noisy text and clean text.\nThe results can be seen in Table 5 and Table 6 for spelling\nmistakes and OCR noise respectively. The model trained on\nnoisy text can perform well on clean text, which indicates\nthat the robust NER model is capable of recognizing both\nnoisy text and clean text."}, {"title": "5. Related Work", "content": "Robust representations One way to make a model more\nrobust is to make the representations less sensitive to noisy\ninput. (Zheng, Song, Leung and Goodfellow, 2016; Cheng,\nTu, Meng, Zhai and Liu, 2018) add noise to input in each\ne"}]}