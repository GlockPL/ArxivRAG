{"title": "Learning Robust Named Entity Recognizers From Noisy Data With Retrieval Augmentation", "authors": ["Chaoyi Ai", "Yong Jiang", "Shen Huang", "Pengjun Xie", "Kewei Tu"], "abstract": "Named entity recognition (NER) models often struggle with noisy inputs, such as those with spelling mistakes or errors generated by Optical Character Recognition processes, and learning a robust NER model is challenging. Existing robust NER models utilize both noisy text and its corresponding gold text for training, which is infeasible in many real-world applications in which gold text is not available. In this paper, we consider a more realistic setting in which only noisy text and its NER labels are available. We propose to retrieve relevant text of the noisy text from a knowledge corpus and use it to enhance the representation of the original noisy input. We design three retrieval methods: sparse retrieval based on lexicon similarity, dense retrieval based on semantic similarity, and self-retrieval based on task-specific text. After retrieving relevant text, we concatenate the retrieved text with the original noisy text and encode them with a transformer network, utilizing self-attention to enhance the contextual token representations of the noisy text using the retrieved text. We further employ a multi-view training framework that improves robust NER without retrieving text during inference. Experiments show that our retrieval-augmented model achieves significant improvements in various noisy NER settings.", "sections": [{"title": "1. Introduction", "content": "Named entity recognition (NER) (Chinchor and Robin- son, 1997) is an important task in natural language process- ing that has a variety of applications and serves as a foun- dation for several downstream tasks such as machine trans- lation (H\u00e1lek, Rosa, Tamchyna and Bojar, 2011), intelligent question answering (Lee, Hwang and Jang, 2007), knowl- edge graphs (Thukral, Dhiman, Meher and Bedi, 2023), and event extraction (Zhang, Wei, Li and Yan, 2022; Edouard, Cabrio, Tonelli and Le Thanh, 2017). The performance of NER directly impacts the performance of these downstream tasks. However, current NER models are trained on clean text and pre-trained models used for NER are also based on clean text. In reality, input text may contain noises, e.g., spelling mistakes or errors from Optical Character Recognition (OCR) processes, which can lead to a drop in NER performance and propagate errors to downstream tasks. Therefore, robust NER that can handle both clean and noisy text is a valuable task in need of further research.\nExisting models (Namysl, Behnke and K\u00f6hler, 2020, 2021) for robust NER rely on the use of both noisy text and its corresponding gold text for training. Specifically, they are trained to output similar results from noisy and gold texts."}, {"title": "2. Problem Definition", "content": "We model the named entity recognition (NER) task as a sequence labeling problem. Given a sentence w = {x1,..., xn} with n tokens, sequence labeling aims to predict a label sequence y = {1,\u2026, yn} for each position. The BIO tagging scheme, which stands for Beginning, Inside and Outside subtags, is often used. A typical architecture of sequence labeling for NER is shown in Figure 2."}, {"title": "3. Method", "content": "Our proposed framework consists of two main compo- nents: a Retrieval Module and a NER model with multi-view learning, as depicted in Figure 3. The Retrieval Module is used to obtain text related to the input text from a back- ground knowledge corpus. The NER model is a transformer- based model with two views. In the retrieval-based view, we concatenate the original noisy text with the retrieved text and input them into the transformer-based model, hoping to enhance the contextual token representation of the noisy text leveraging the information from the retrieved text. In the original noisy text view, the noisy text is directly inputted into the NER model without retrieval. We employ multi-view learning to encourage two views to produce similar contextual embeddings or predicted label distributions, ul- timately improving the accuracy of the original noisy text view, which can be used when retrieval is impossible or undesirable."}, {"title": "3.1. Retrieval Module", "content": "We design three retrieval methods: sparse retrieval based on lexicon similarity, dense retrieval based on semantic similarity, and self retrieval based on task-specific text."}, {"title": "3.1.1. Sparse retrieval based on lexicon similarity", "content": "Wikipedia 1, the largest encyclopedia in the world, boasts a vast collection of clean and well-organized text on a wide range of topics and domains. The simplest method for retrieving information from Wikipedia is through the BM25 algorithm (Robertson and Walker, 1994), a string matching- based approach."}, {"title": "3.1.2. Dense retrieval based on semantic similarity", "content": "The BM25 algorithm requires that retrieved strings con- tain at least one word from the input query, making it less effective for retrieving results from noisy text containing errors. Fuzzy retrieval based on Levenshtein distance may be used instead. However, due to the vast size of Wikipedia, the word with the smallest Levenshtein distance to the noisy text may not necessarily be the correct word, resulting in sub-optimal results. To address this, we apply a dense re- trieval method, which encodes sentences into embeddings and then retrieves text through K-Nearest Neighbors. To obtain the encoder, we utilize contrastive learning (Gao et al., 2021) to fine-tune a pre-trained model using clean text from Wikipedia and its noisy version with simulated corruptions. We use the InfoNCE loss (Oord, Li and Vinyals, 2018) as the training loss:\n$$L\u2081 = -log \\frac{e^{sim(h_{noisy}, h_{gold}) / \\tau}}{\\sum_{j=1}^{N} e^{sim(h_{noisy}, h_{gold}^j) / \\tau}}$$ (1)\nwhere $h_{noisy}$ is the embedding of noisy text, $h_{gold}$ is the embedding of the corresponding gold text, $\\tau$ is the tem- perature hyperparameter, sim (\u00b7,\u00b7) is the cosine similarity, and N is the batchsize. We select the best encoder model by measuring recall@k of using simulated noisy text of Wikipedia to retrieve the corresponding gold text. recall@k is measured by comparing the top k retrieved results with the gold text and calculating the proportion of times the gold text is included in the top k results, as shown by the following equation:\n$$recall@k = \\frac{\\sum_{i=1}^{n} \\mathbb{I} [gold_i \\in R_{topk}(noisy_i)]}{n}$$ (2)"}, {"title": "3.1.3. Self retrieval based on task-specific text", "content": "The dense retrieval method encodes an entire sentence into a single embedding. A more fine-grained retrieval method is to use BERTScore (Zhang et al.), which calculates the average maximum cosine similarity between the contex- tual token embeddings of the input sentence and the retrieved sentence, which is then used to determine the precision (P) and recall (R). We use Fl-score (harmonic mean of P and R) to rank the retrieved sentences. However, BERTScore has high computational complexity and a large memory footprint, making it difficult to use on large datasets like Wikipedia. Therefore, we apply it to retrieve text from the training dataset itself, which is small and contains sentences of the same domain as the query sentence. For the dataset self, it is smaller and from the same domain, where the same words may have the same semantics, appear multiple times and may or may not be changed by a noising process. The BERTScore's F1-score can be used to retrieve and rank the noisy dataset itself. Since our experiment is inductive, with test instances being independent of each other, we retrieve from the same training set for all sets: training, validation, and test."}, {"title": "3.2. NER model with multi-view learning", "content": "For the NER model, we use a pre-trained transformer- based model and a CRF layer as the backbone. Our method employs two views: the original noisy text view and the retrieval-based view. Both views share the same parame- ters in their pre-trained transformer-based models and CRF layers. The retrieval-based view combines the noisy text with the corresponding retrieved text from the Retrieval Module to form a concatenated input [x, [X], x], where x is the noisy text, x is the corresponding retrieved text, and [X] is a special token dependent on the pre-trained transformer-based model used by NER (e.g., '[SEP]' in BERT and '</s>' in RoBERTa). The transformer-based embedding model, utilizing its self-attention mechanism, enhances the contextual token representations of the noisy text by the retrieval text, ultimately leading to improved NER performance. Note that in the retrieval-based view, we only input the embeddings of the original noisy text into the CRF layer, discarding the embedding of the retrieval text since we do not need to predict their labels.\nAs discussed earlier, retrieval is slow and may not be suitable for time-sensitive scenarios. Therefore, we use multi-view learning to improve the original noisy text view with the help from the retrieval-based view. For the original noisy text view, denote the contextual token representations"}, {"title": "4. Experiment", "content": "4.1. Dataset Construction\nIn light of the absence of publicly accessible noisy datasets specifically designed for Named Entity Recogni- tion (NER) tasks \u2013 with an emphasis on noisy text rather than noisy labels \u2013 we have undertaken the development of our own noisy datasets. These custom datasets are con- structed using two established NER corpora as a foundation: the WNUT-17 dataset (Derczynski, Nichols, van Erp and Limsopatham, 2017) collected from social media and the CONLL-03 English dataset (Tjong Kim Sang and De Meulder, 2003) derived from news.\nWe induce two types of noise into clean text, i.e., spelling mistakes and errors caused by Optical Character Recog- nition (OCR). To simulate spelling mistakes, we employ the NAT model proposed by Namysl et al. (2020) (Namysl et al., 2020). This model uses a character confusion ma- trix to generate errors, where the probabilities of insertion, deletion, and substitution are each equal to p/3, given an overall noise level of p. The alphabet \u03a3, which excludes the symbol \u025b, comprises all letters present in the original corpus. In applying this method, we first insert the \u025b symbol between every pair of adjacent letters in the original text, as well as before the first letter and after the last letter. Next, we utilize the character confusion matrix to create the modified sequence. Specifically, insertions transform \u025b into a character from \u2211, deletions change the original character to \u025b, and substitutions replace the original character with another distinct character from \u03a3. Finally, we remove the \u025b symbols to produce the resulting noisy text containing spelling errors.\nIn order to simulate errors introduced by OCR, we make use of the Text Recognition Data Generator (TRDG) pack- age2 to convert each sentence into an image, employing 90 distinct fonts. Subsequently, we introduce random distor- tions to the sentences within the images or incorporate back- ground noise. To extract potentially noisy text from these images, we employ the Tesseract-OCR engine\u00b3. Considering that OCR may alter sentence length, we align the noisy OCR sentences with their respective clean counterparts using the Levenshtein Distance Algorithm, consistent with prior work (Namysl et al., 2021). In our approach, the number of labels corresponds to the total number of words recognized follow- ing the OCR process. This differs from previous research (Namysl et al., 2021), where the number of labels equaled the number of original words, which does not reflect real- world scenarios."}, {"title": "4.2. Retrieval Module Configuration", "content": "4.2.1. BM25 retrieval (sparse retrieval)\nWe employ the 2022-07-01 version of the Wikipedia data dump as our primary data source, which is subse- quently converted into plain text format for efficient pro- cessing. For the implementation of the BM25 retrieval al- gorithm, we utilize Elasticsearch to index and retrieve sen- tences within the Wikipedia corpus. In this configuration, the smallest searchable unit is defined as a single sentence. Fur- thermore, the matching algorithm is configured to \u201cmatch\u201d, signifying that the retrieved results must contain a minimum of one word from the query sentence.\n4.2.2. Dense retrieval\nIn our dense retrieval experiment, we utilize the entire Wikipedia corpus to retrieve, consistent with the BM25 retrieval method, and an encoder trained using the unsu- pervised SimCSE approach (Gao et al., 2021). The encoder is fine-tuned on the XLM-ROBERTa-large model through contrastive learning, employing simulated noisy text and clean text from Wikipedia. Two distinct noise induction techniques, spelling mistakes and OCR errors, are applied separately. In order to address challenges over RAM usage and computational speed and inspired by BERT-whitening (Su, Cao, Liu and Ou, 2021), we generate sentence embed- dings, perform PCA for dimensionality reduction, and save the embeddings as npy files. We leverage the FAISS library 6 to implement the Inverted File System (Mazur, 1979) using the index_factory function, incorporating the Hierarchical Navigable Small World (HNSW) technique (Malkov and Yashunin, 2018) and the cosine distance metric. Further details can be found in the Appendix A.\n4.2.3. BERTScore retrieval (self retrieval)\nThe official implementation of BERTScore 7 can only calculate paired inputs, which is very slow for a large number of query and target sentences. We modify the source code by first encoding all the query and target sentences to contextual token embeddings, then doing matching for each pair of sentences, and finally outputting their similarity scores. It is worth noting that each noisy text is tested individually and does not interfere with each other, i.e., inductive inference. Therefore, we independently retrieve texts for the training, development, and testing sets from the same source corpus, i.e., the training set.\n4.2.4. Postprocessing\nWe use top-10 retrieved results. For better utilization of retrieval results of Wikipedia, we follow previous work (Wang, Shen, Cai, Wang, Wang, Xie, Huang, Lu, Zhuang, Tu, Lu and Jiang, 2022c) and use three options, i.e., using the matched paragraph, using the matched sentence, and using the matched sentence but removing wiki anchors, i.e., mention hyperlinks, which can provide rich and useful clues"}, {"title": "4.3. NER Module Configuration", "content": "We use XLM-ROBERTa-large (Conneau et al., 2020) as the pre-trained transformer-based model, tune the learning rate of the transformer within [1\u00d7106,1\u00d7105], and use grid search to find the ratio of CRF learning rate and transformer learning rate within [3000, 12000]."}, {"title": "4.4. Main Results", "content": "We employ entity-level F1 scores as the evaluation met- ric for Named Entity Recognition (NER). For each experi- mental setting, we run four different seeds and then average the results. The NER performance is shown in Table 3 and Table 4 for spelling mistakes and OCR noise respectively.\nFor the first row, 0 or no denotes the original clean text with- out noise and the other denote different types of noisy text; OV is the original noisy text view and RV is the retrieval- based view. There are many settings that correspond to each column, as detailed below:\n\u2022 noisy text only w/o context only uses the original noisy text without the retrieved text.\n\u2022 w/o context natas/hunspell only uses the original noisy text without the retrieved text but the noisy text is corrected before inputting by natas/hunspell.\n\u2022 w/o context natas/hunspell + check adds a check process which only retains words corrected rightly but keeps the original noisy words if corrected wrongly.\n\u2022 w/gold/self uses both the original noisy text and the external context which is the corresponding gold text or the self retrieval context.\n\u2022 w/para/sent/sent-link uses both the original noisy text and the external context which is the matched paragraph, the matched sentence and the matched sentence without wiki anchors.\n\u2022 full denotes using only the original noisy text con- catenated with retrieved context.\n\u2022 KL denotes using multi-view learning with the KL loss.\n\u2022 L2 denotes using multi-view learning with the L2 loss.\nThe baseline model is noisy text only w/o context. We are the first approach to robust NER with only noisy training text and without its corresponding gold text, so there is not SoTA. We employ the Deep Dominance method (Dror, Shlomov and Reichart, 2019) to perform statistical significance testing. In Tables 3 and 4, it is evident that our proposed method exhibits a significant improvement in NER performance over the baseline.\nWe also use the post-correction model NATAS 8 (H\u00e4m\u00e4l\u00e4i- nen and Hengchen, 2019) and the spell checker Hunspell to correct the words before feeding them into the NER model, but find that the performance is worse than only using noisy text, which is probably because of erroneous corrections. Furthermore, when we keep the corrected words which are error-corrected the same as the gold words while not correcting them if error-correcting wrongly, which can be"}, {"title": "4.5. Comparing the model using noisy text vs. using clean text", "content": "We do a cross-experiment using noisy text and clean text. The results can be seen in Table 5 and Table 6 for spelling mistakes and OCR noise respectively. The model trained on noisy text can perform well on clean text, which indicates that the robust NER model is capable of recognizing both noisy text and clean text."}, {"title": "5. Related Work", "content": "Robust representations One way to make a model more robust is to make the representations less sensitive to noisy input. (Zheng, Song, Leung and Goodfellow, 2016; Cheng, Tu, Meng, Zhai and Liu, 2018) add noise to input in each epoch and use pairs of noisy input and the corresponding gold input for training. (Piktus, Edizel, Bojanowski, Grave, Ferreira and Silvestri, 2019; Jones, Jia, Raghunathan and Liang, 2020) improve robust encoding by embedding mis- spellings close to their correct variants and mapping sen- tences to a smaller discrete space. However, these methods all require gold text, which is impractical in real scenarios.\nNoisy sequence labeling There are a few previous studies of noisy sequence labeling (Namysl et al., 2020, 2021), which use gold text to improve the performance on noisy text. However, gold text may be unavailable or undesirable in many scenarios, such as there are only noisy text in the real scenario. In contrast, our work does not rely on gold"}, {"title": "6. Conclusion", "content": "In this paper, we propose a robust Named Entity Recog- nition (NER) model that is able to effectively handle noisy text without the need for corresponding gold text during training. Our approach utilizes retrieval augmentation and we jointly train a retrieval-based view and an original noisy text view using multi-view learning. Our approach results in significant improvements in handling both misspelling mistakes and OCR errors."}, {"title": "A. the details of dense retrieval", "content": "The dense retrieval approach utilizes the entire Wikipedia, same as BM25 retrieval. Our encoder is trained on a dataset of randomly sampled sentences from Wikipedia, prepro- cessed using the unsupervised SimCSE method (Gao et al., 2021), with a sentence size set to 106. To simulate noisy text, we introduce noise into these sentences and partition the dataset into train, development, and test sets in an 8:1:1 ratio. Our experiments involve two distinct noise induction methods, namely spelling mistakes and OCR errors, con- ducted separately.\nFor spelling mistakes, we introduce noise at random levels ranging from 0 to 0.4. In the case of OCR er- rors, we incorporate random fronts and random OCR error induction approaches. We employ the XLM-ROBERTa- large model, fine-tuned using contrastive learning on the simulated noisy text and the corresponding original clean text sourced from Wikipedia. The sentence embeddings are generated by averaging the embeddings of the first and last layers. Model selection is based on the (Recall@1 + Recall@4 + Recall@16 + Recall@64) metric, which"}, {"title": "B. the details of contrastive learning training", "content": "The details of contrastive learning training is as Ta- ble 7 and Table 8. not train is the XLM-ROBERTa-large model which is not fined tune. recall_avg = (recall@1 + recall@4 + recall@16 + recall@64)/4, typos_mix or OCR_mix is the uniform mixing noisy dataset. Each column in the experimental results represents a distinct setting, denoting the effectiveness of the model trained through contrastive learning in retrieving various forms of noisy or mixed noisy text, as measured by the recall@k metric."}, {"title": "C. the statistics of errors in the correction experiment", "content": "The statistics of errors in the correction experiment is as Table 9 and Table 10. The ent is the number of entities, ent_true is the number of correct entities in which all words"}]}