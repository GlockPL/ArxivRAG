{"title": "Causality for Large Language Models", "authors": ["Anpeng Wu", "Kun Kuang", "Minqin Zhu", "Yingrong Wang", "Yujia Zheng", "Kairong Han", "Baohong Li", "Guangyi Chen", "Fei Wu", "Kun Zhang"], "abstract": "Recent breakthroughs in artificial intelligence have driven a paradigm shift, where large language models (LLMs) with billions or trillions of parameters, such as Chat-GPT, LLAMA, PaLM, Claude, and Qwen, are trained on vast datasets, achieving unprecedented success across a series of language tasks. However, despite these successes, LLMs still rely on probabilistic modeling, which often captures spurious correlations rooted in linguistic patterns and social stereotypes, rather than the true causal relationships between entities and events. This limitation renders LLMs vulnerable to issues such as demographic biases, social stereotypes, and LLM hallucinations. These challenges highlight the urgent need to integrate causality into LLMs, moving beyond correlation-driven paradigms to build more reliable and ethically aligned AI systems. While many existing surveys and studies focus on utilizing prompt engineering to activate LLMs for causal knowledge or developing benchmarks to assess their causal reasoning abilities, most of these efforts rely on human intervention to activate pre-trained models. How to embed causality into the training process of LLMs and build more general and intelligent models remains unexplored. Recent research highlights that LLMs function as causal parrots, capable of reciting causal knowledge without truly understanding or applying it. These prompt-based methods are still limited to human interventional improvements. This survey aims to address this gap by exploring how causality can enhance LLMs at every stage of their lifecycle-from token embedding learning and foundation model training to fine-tuning, alignment, inference, and evaluation-paving the way for more interpretable, reliable, and causally-informed models. Additionally, we further outline six promising future directions to advance LLM development, enhance their causal reasoning capabilities, and address the current limitations these models face.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) are a class of artificial intelligence models that are designed to process and generate human-like text by leveraging vast amounts of data and computational power [1, 2, 3, 4, 5, 6]. These models are built using deep learning architectures, specifically transformer networks [7], and are typically trained on massive datasets comprising text from diverse sources such as books, websites, social media, and other digital texts [1, 2, 3, 8, 9, 10, 11]. Key characteristics of large language models include:\nSize and Scale: LLMs contain billions to trillions of parameters, which are the internal configurations that the model learns during training. Examples of these models include OpenAI's GPT-3 [11], GPT-4 [12], Meta's LLAMA [2, 3], Google's PaLM [13], Anthropic's Claude, and Alibaba's Qwen [14]. The larger the model, the more nuanced its understanding and generation of language can be.\nTraining on Vast Datasets: LLMs are trained on extensive corpora of text data, encompassing a broad range of data sources. These include publicly available internet content, such as websites, blogs, and social media platforms, as well as more structured and formal sources like books, academic papers, and news articles. By leveraging this vast volume of text, LLMs can learn intricate statistical patterns, including grammar, semantics, context, and relationships between entities.\nCapabilities: LLMs can be directly transferred to a wide range of human language-related tasks, including: (a) Natural language understanding: LLMs can interpret and comprehend the meaning of text, making them useful for tasks such as question answering and information retrieval. (b) Natural language generation: They can generate coherent and contextually relevant text, often mimicking human writing styles. (c) Problem-solving and reasoning: LLMs are capable of logical reasoning and solving complex problems.\nDespite their remarkable capabilities, the rapid advancements in LLMs raise significant concerns regarding their ethical use, inherent biases, and broader societal implications [4, 15, 16, 17]. These models often rely on statistical correlations learned from training data to generate responses, rather than achieving a genuine understanding of the questions posed. This limitation frequently leads to issues such as hallucinations where the model generates false or nonsensical information and the reinforcement of biases present in the training data. These flaws significantly undermine the reliability, accuracy, and safety of large language models (LLMs) in real-world applications, particularly in critical domains like healthcare and law. In these contexts, generating incorrect diagnoses or treatment recommendations can endanger patient health and safety [18, 19], while erroneous legal information may compromise the fairness and legitimacy of judicial decisions [20, 21]. Such risks further underscore the pressing need for continued research aimed at improving the interpretability, reliability, and ethical alignment of these models [4, 15, 16, 17, 22].\nCausality refers to the relationship between cause and effect, where one event directly influences another, providing an explanation of why and how something happens. Unlike correlations, which only show that two variables move together, causality establishes a directional and actionable link, allowing us to understand the mechanisms behind changes. Causality is a significant hallmark of human intelligence, crucial for scientific understanding and rational decision-making [23, 24, 25, 26]. However, current LLMs are primarily trained on vast datasets to capture statistical correlations rather than causal relationships, which limits their ability to reason about the underlying mechanisms that govern the world.\nWhile LLMs excel at tasks involving language understanding, generation, and pattern recognition, they often struggle with tasks that require deeper causal reasoning. Without an understanding of causality, LLMS may produce outputs that are contextually relevant but not logically sound, leading to potential issues such as hallucinations, biased outputs, and an inability to perform well on decision-making tasks that depend on causal relationships. Incorporating causality into LLMs is essential for several reasons. First, it helps models move beyond superficial correlations, enabling them to generate more reliable and interpretable outputs. Second, causality improves fairness by allowing models to account for confounding factors and systemic biases present in the data, ultimately producing more ethically aligned predictions. Third, it enhances the ability of models to handle complex tasks, such as medical diagnosis, policy planning, and economic forecasting, where understanding the causal relationships is critical. Moreover, causality allows LLMs to perform counterfactual reasoning, which is vital for exploring \u201cwhat-if\" scenarios and making informed decisions [26]. Overall, integrating causal reasoning into LLMs represents a significant step forward in the development of AI systems that are not only capable of understanding language but also reasoning about the world in a more human-like and scientifically robust manner.\nWhile many existing surveys and studies [25, 26, 75] focus on utilizing prompt engineering to activate LLMs in extracting causal entities, recovering causal relations between events, and answering counterfactual questions,"}, {"title": "2 The Background of Large Language Models and Causality", "content": "Large language models (LLMs) have rapidly gained prominence for their remarkable performance across a wide spectrum of natural language processing tasks [1, 2, 3, 8, 9, 10, 11], particularly following the launch of ChatGPT in November 2022. The impressive language comprehension and generation capabilities of these models are largely attributed to autoregressive training on vast, diverse datasets of human-generated text. Despite being a relatively nascent area of research, the field of LLMs has undergone swift and significant advancements, yielding innovations in various domains [4, 22, 15, 16, 17]. Nevertheless, the question of how LLMs might integrate or benefit from causal reasoning remains largely unexplored. While LLMs excel at recognizing patterns and correlations in text, the incorporation of causal reasoning could unlock new avenues for more robust decision-making and predictive modeling. Developing LLMs with causality has the potential to enhance not only language tasks but also applications in areas requiring causal inference, such as healthcare, economics, and policy analysis [18, 19, 20, 21]."}, {"title": "2.1 What are Large Language Models?", "content": "Large Language Models (LLMs) are a class of advanced machine learning architectures designed to process and generate natural language through training on vast, diverse corpora of human-generated text [4, 15]. These models predominantly utilize deep learning frameworks, with the transformer architecture being the most prominent [7]. Through this architecture, LLMs are capable of modeling intricate dependencies between words, phrases, and sentences, enabling them to capture the rich linguistic structures inherent to human language [76]. The transformative power of LLMs lies in their ability to perform autoregressive training, wherein they predict the next word in a sequence based on all preceding words. This process allows the models to generate text that is not only grammatically correct but also contextually coherent, thereby mimicking human-like text production [4, 15, 16, 67, 2, 3]. Crucially, LLMs learn these representations without requiring explicit human intervention in feature design, making them versatile across a broad range of natural language processing (NLP) tasks. This self-supervised learning paradigm has reshaped the field, significantly reducing the need for task-specific models and enabling a new era of universal language understanding and generation [4, 15].\nUnlike traditional machine learning tasks, the development pipeline of LLMs is significantly more complex, encompassing several critical stages, including token embedding, foundation model pretraining, supervised fine-tuning, reinforcement learning from human feedback (RLHF) for alignment, post-training prompt-based inference, and evaluation. These stages are outlined as follows:\nToken Embedding: Raw text is transformed into numerical representations (embeddings) that the model can process. These embeddings capture both semantic and syntactic information, providing the foundation for the model's understanding of language [8, 77].\nFoundation Model Pretraining: The model undergoes extensive pretraining on large-scale, diverse corpora using self-supervised learning techniques. During this phase, the model acquires a general understanding of language patterns, structures, and context, learning representations that are applicable across a wide array of tasks without the need for task-specific annotations [2, 12, 60, 67].\nSupervised Fine-Tuning: Following the pretraining phase, the model is further refined through supervised fine-tuning on labeled datasets tailored to specific downstream tasks, such as machine translation, text summarization, or question answering. This process enhances the model's ability to generate task-specific outputs with greater precision and reliability [78, 79, 80].\nAlignment: This critical phase focuses on aligning the model's outputs with human values, ethical considerations, and desired behaviors. Reinforcement learning from human feedback (RLHF) is often employed, allowing the model to optimize its responses based on human judgments, thereby ensuring the generated content is more aligned with societal and ethical standards [81, 82, 83].\nInference: After training, the model is deployed to real-world applications where the core of its operation lies in prompt engineering. By carefully crafting input prompts, the model utilizes its learned representations to generate coherent text, retrieve information, or engage in conversations across various NLP tasks in practical, dynamic environments. Prompt engineering plays a crucial role in guiding the model's responses to meet the user's intent more effectively, ensuring optimal performance in diverse applications [84, 85].\nEvaluation: The model's performance is rigorously evaluated across multiple dimensions, including task-specific accuracy, generalization to unseen data, ethical alignment, and robustness. These assessments ensure that the model not only performs effectively in target tasks but also adheres to ethical guidelines and demonstrates resilience in diverse and challenging real-world scenarios [86, 87].\nOver the past few years, the development of LLMs has been marked by a series of landmark models that have fundamentally advanced our understanding of language representation and generation. These models include, but are not limited to, OpenAI's GPT-3 [11], GPT-4 [12], Meta's LLaMA [2, 3], Google's PaLM [13], Anthropic's Claude, and Alibaba's Qwen [14]. Beyond traditional NLP tasks, LLMs are now being integrated into a wide range of cutting-edge research and real-world applications, from scientific discovery and healthcare to policy analysis. Their unparalleled ability to process and generate language at scale is driving transformative advancements across diverse fields, underscoring their pivotal role in shaping the future of AI-driven innovation [88]."}, {"title": "2.2 Limitations of Large Language Models", "content": "Large Language Models (LLMs) have achieved remarkable success across diverse applications; however, they face significant challenges and limitations that undermine their effectiveness and reliability [4, 15, 16, 17, 22]. A primary concern is their dependence on statistical correlations derived from extensive training datasets, which often leads to biased responses based on superficial patterns rather than a genuine understanding of entities and events [25, 58]. Notably, correlation does not imply causation. Existing Transformer-based LLMs, such as GPT-3 [11], GPT-4 [12], LLaMA [2, 3], PaLM [13], Claude, and Qwen [14], primarily capture spurious correlations between tokens and their interactions within the training corpus. Consequently, these models are highly susceptible to demographic biases and social stereotypes, which can lead to biased responses. For instance, as shown in , when the LLaMa3 model is asked, \"Who does 'he' refer to in the sentence, 'The physician hired the secretary because he is highly recommended'?,", "he": "defaults to social stereotypes that predominantly associate the role of a secretary with women. As a result, the model concludes that", "refers": "to the physician rather than the secretary, reflecting the biases embedded in its training data [58].\nAdditionally, LLMs frequently grapple with high-frequency co-occurrence issues [89]. As demonstrated in , when prompted with the question,", "the": "bird's nest?\", lowercase prompts may elicit specific responses related to Chinese landmarks, such as the National Stadium, while uppercase prompts yield more general definitions. When asked,", "What": "is a bird's nest?\", the model accurately identifies it as \"a structure created by birds to lay their eggs, incubate them, and raise their young.\" However, the inconsistency in responses based on subtle variations in prompt capitalization underscores the limitations of these models in effectively managing context and meaning. Minor changes in prompts can yield significant variations in output, which presents challenges in both prompt design and query interpretation [32, 90]. This behavior reflects the reality that LLMs predominantly function as high-frequency probability models, reiterating prevalent associations from the training corpus and reinforcing fixed impressions present in the data, which can lead to deterministic yet incorrect answers. In contrast, humans, when faced with ambiguous or hallucinatory queries, tend to provide responses that convey uncertainty [91].\nFurthermore, due to the autoregressive nature of learning in Transformers [7], LLMs often struggle with reverse inference, particularly in complex or lengthy interactions. Models such as GPT-40 exhibit challenges in recalling prior information, which can compromise their ability to generate coherent and contextually appropriate responses. For instance, while GPT-40 can accurately produce subsequent lines of text, it may falter when tasked with recalling earlier lines (Table 3), thereby revealing significant limitations in temporal reasoning. These issues raise serious concerns regarding the reliability and interpretability of LLMs, especially in high-risk environments"}, {"title": "2.3 Motivation on Large Language Models with Causality", "content": "Causality refers to the relationship between cause and effect, wherein one event (the cause) directly influences another event (the effect). Causal learning serves as a powerful statistical modeling tool for explanatory analysis, playing a crucial role in decision-making processes and constituting a fundamental component of interpretable artificial intelligence [92, 93, 94, 95]. There are extensive works using causality techniques to enhance models that go beyond spurious correlation to understand causal relationships between variables, which allows for more robust predictions, improved decision-making, and enhanced interpretability of models. Relevant techniques include intervention [96, 97], Structural Causal Modeling (SCM) [98], front-door and back-door adjustment [98], propensity score weighting [99], doubly robust learning [100], and causal representation learning [101, 61].\nIntegrating causality into Large Language Models offers a promising method to address their inherent limitations, as mentioned in Section 2.2. By incorporating causal learning, models can move beyond reliance on high-frequency co-occurrence patterns, allowing them to focus on meaningful causal dependencies between tokens rather than depending solely on statistical correlations [36, 37, 38, 39]. Embedding causal reasoning into LLMs reduces their vulnerability to spurious correlations, enabling more accurate representation of the underlying causal dynamics within the training corpus. This is crucial for mitigating biases and ensuring more consistent, reliable outputs. Additionally, a causality-inspired reversal corpus can enhance the models' ability to manage reverse inference tasks, improving coherence in complex, multi-step interactions [102, 103].\nRecent studies have explored integrating causality into LLMs, with much of the existing literature focusing on leveraging LLMs for causal-related tasks [50, 51, 52, 53, 54, 55]. This is primarily achieved by crafting prompts designed to activate the models' latent causal knowledge and elicit common-sense reasoning [26, 75]. For instance, the CausalCoT framework [25] introduces a chain-of-thought prompting strategy inspired by causal inference engines, guiding LLMs to first identify key elements of a question, including the causal graph, causal query, and relevant data (e.g., conditional or interventional do-probabilities). Similarly, Tan et al. [57] investigate LLMs' capacity for counterfactual reasoning by intervening on specific nodes within the chain-of-thought reasoning process in arithmetic word problems. These causal chain-of-thought prompts [25, 53] are instrumental in helping models explicitly link causes to their effects. Moreover, Zhang et al. [61] propose Causal Prompting, a novel approach aimed at mitigating biases in LLMs through the application of front-door adjustments from causal inference. By carefully structuring prompts, researchers guide the models to recall and incorporate relevant causal information into their outputs, ultimately generating more coherent and causally informed responses [25, 49, 50, 51, 54, 57, 65]. All causal prompts are provided in Table 4.1-4.4 in Appendix.\nProviding a step-by-step causal chain-of thought [25, 53], prompts encourages the model to", "think": "in a causal manner, reciting relevant knowledge from its training data and mapping causal relationships between different elements. Although these methods enhance LLMs' ability to process and respond to questions involving causality [36, 37, 98], the model itself does not independently understand or reason through these causal structures. The advancements achieved thus far rely heavily on human intervention to design prompts that can effectively exploit pre-trained models. This highlights a critical gap in the research: embedding causality directly into the training process to create models that are inherently capable of causal reasoning, thus making them more intelligent and generalizable. To address this gap, this paper explores how causal reasoning can be integrated across the lifecycle of LLMs. As outlined in Section 2.1, we investigate how causal reasoning can enhance LLM performance at key stages from token embedding learning and foundational model training to fine-tuning, alignment, inference, and evaluation."}, {"title": "3 Causality for Pre-training", "content": "Pre-training is the foundational phase in the large language model (LLM) training pipeline, equipping models with essential language understanding capabilities that can be applied across a wide range of downstream tasks. In this stage, the LLM is exposed to massive amounts of usually unlabeled text data, typically in a self-supervised learning setup. The goal is to enable the model to learn generalizable linguistic patterns and representations."}, {"title": "3.1 Traditional Pre-trained Models", "content": "Traditional pre-trained language models are primarily categorized into encoder-only, decoder-only, and encoder-decoder architectures. Encoder-only models, like BERT [8], specialize in encoding input text into contextual representations for understanding tasks such as classification and entity recognition. Decoder-only models, such as GPT [10, 11] and LLAMA [2, 3], focus on generating text by predicting the next token in a sequence, making them ideal for language generation tasks. Encoder-decoder models, exemplified by T5 [9], integrate both encoding and decoding processes to handle sequence-to-sequence tasks like translation and summarization. Central to these models is the concept of token embedding, which transforms discrete tokens into continuous vector representations, capturing semantic and syntactic relationships and enabling effective learning from textual data during pre-training.\nBERT (Bidirectional Encoder Representations from Transformers) [8]. BERT is a pre-trained language model designed to produce contextualized word embeddings by considering both the left and right context of a token within a sentence. Unlike traditional embedding methods like Word2Vec or GloVe, which generate static word embeddings, BERT creates dynamic embeddings that change depending on the context. Traditional models assign the same vector to a word regardless of context, while BERT achieves context-aware embeddings through its Transformer encoder architecture, which leverages self-attention to process all tokens in a sentence simultaneously. This allows BERT to capture token relationships in both directions, adjusting each token's representation based on its surrounding context.\nDuring training, BERT is optimized using two primary tasks: Masked Language Modeling (MLM) and Next Sentence Prediction (NSP). In MLM, 15% of the input tokens are randomly masked, and the model must predict these masked tokens based on their context, which is formalized by the cross-entropy loss:\n$$L_{MLM} = - \\sum_{i \\in M} log P(x_i | x\\_{\\setminus M}),$$\nwhere $M$ is the set of masked tokens, and $x_{\\setminus M}$ represents the surrounding context. In NSP, the model predicts whether two sentences are consecutive in the original text, with the binary cross-entropy loss:\n$$L_{NSP} = - log P(y | A, B),$$\nwhere $y$ is the label indicating whether sentence B follows sentence A. These tasks enable BERT to learn both token-level and sentence-level relationships. Later models such as ROBERTa [77], ALBERT [104], and Distil-BERT [105] refined the training process\u2014e.g., RoBERTa removed the NSP task\u2014and improved the architecture for greater efficiency and scalability, resulting in better performance with reduced computational overhead."}, {"title": "3.2 Debiased Token Embedding", "content": "Demographic biases and social stereotypes are common in pre-trained models [31], and stable learning and invariant learning have been developed to address these issues by identifying and learning the causal features of input data, especially in fields like computer vision and network data [106, 101, 107]. However, their application in language models remains underexplored. Understanding how to incorporate causal embeddings into token representations could help mitigate biases and improve models' ability to reason through cause-effect relationships, ultimately enhancing performance in tasks that require deep logical and causal reasoning. Causality Token Embedding is proposed to enhance token representations by identifying and learning causal features, enabling models to capture cause-effect relationships better. The development of Causality Token Embedding can be approached in two ways: one involves counterfactual data augmentation, and the other through debiasing token embeddings by learning causal features [108]. Additionally, in the following sections, we will introduce counterfactual data augmentation for the training corpus and a causal transformer architecture for dense embedding.\nDropout and CDA [27]. Webster et al. [27] explores model correlations, which are defined as associations between words or concepts that may emerge through probing or when models exhibit brittleness in downstream applications. Pre-trained models can encode undesired artifacts in their token embeddings, leading to incorrect assumptions in new examples, such as associating professions with a specific gender. In [27], the authors define several metrics to detect and measure gendered correlations in pre-trained models. These include coreference resolution, where models are tested on their ability to avoid gender bias in resolving pronouns, and semantic textual similarity (STS), where the model's similarity scores between gendered and professional terms are compared. They also introduce Bias-in-Bios, a metric that measures discrepancies in model performance across genders in real-world settings like profession classification.\nTo mitigate these gendered correlations, the paper explores two main strategies: Dropout regularization and Counterfactual Data Augmentation (CDA). Dropout is applied during training to reduce overfitting and disrupt gendered associations in the model's attention mechanisms. CDA involves augmenting training data by swapping gendered terms (e.g., replacing", "he": "with", "she": ") to teach the model to be neutral to such correlations. Both techniques were found to reduce gender bias while maintaining overall model performance.\nCausal-Debias [31]. The Causal-Debias framework offers a pioneering solution to mitigating biases in pre-trained language models (PLMs) by integrating causal learning principles into the fine-tuning process, which could be directly applied to the pre-training processes. Unlike conventional methods that separate bias mitigation and task performance optimization, Causal-Debias merges these goals through the use of causal interventions and invariant risk minimization (IRM). The framework effectively distinguishes between causal (label-relevant) and non-causal (bias-related) factors embedded in token representations, addressing spurious correlations resulting from demographic biases and social stereotypes. By generating counterfactual data\u2014modifying bias-related attributes such as gendered or racial terms and training the model with an invariant loss across diverse environments, Causal-Debias ensures that the model generalizes well across tasks while mitigating biases. This approach directly tackles the challenge of bias resurgence, where previously mitigated biases reappear during fine-tuning, a limitation observed in many existing debiasing techniques.\nBy unifying the debiasing process with token embedding learning, the framework provides a robust and practical solution for building fair and accountable NLP models. However, its reliance on external corpora and predefined bias-related word lists poses challenges, particularly in extending its application to broader or more nuanced demographic groups and biases. This dependence may limit its scalability or effectiveness in handling more complex, intersectional biases that extend beyond predefined categories. Nonetheless, Causal-Debias represents a significant advancement in the field, emphasizing the importance of causal learning in achieving meaningful debiasing across various environments.\nAdditionally, methods like Auto-Debias[28], ContextDebias[29], and MABEL [30] reduce biases in pre-trained language models (PLMs) by introducing different bias-neutralizing objectives. Then, since counterfactual data augmentation and causal transformer architectures can enhance token embeddings, though they are not limited to this use case, I will explore these approaches in the following two subsections respectively."}, {"title": "3.3 Counterfactual Training Corpus", "content": "In this section, we will delve into the composition and significance of the training corpus used for Large Language Models (LLMs). The corpus often reflects the language patterns found across vast internet-scale data sources,"}, {"title": "3.4 Causal Foundation Model", "content": "Finally, and most importantly, the construction of the Foundation Model is crucial for Large Language Models (LLMs). The majority of current LLMs use the decoder model of Transformer as their foundation model. This choice excels in handling large-scale text generation tasks, particularly in scenarios requiring strong contextual dependency management. The Transformer decoder leverages the self-attention mechanism to capture long-range dependencies, enabling the model to learn complex patterns and linguistic structures from vast corpora, thereby producing high-quality and coherent text outputs. However, one significant issue is that the attention module in the Transformer primarily captures interactions between words, which makes it susceptible to demographic biases and social stereotypes. This raises a fundamental question: how can we design models that not only manage word interactions but also capture causal relationships between entities, as well as between inputs and outputs? A promising solution lies in the development of Causal Attention mechanisms within the Transformer architecture, a concept that has been explored previously in the domain of computer vision [36, 37].\nCausal Attention Module (CaaM) [36]. This paper focuses on eliminating confounding factors in visual recognition tasks through causal intervention. In the Causal Attention Module (CaaM), confounders are identified in a self-supervised way. The fundamental idea is based on the causal intervention formula: $P(Y | do(X)) = E_{T\\in\\Tau} \\sum_{t} P(Y | X,t)P(t)$, where $P(Y | X,t)$ is the prediction in each partitioned environment $t$, and $P(t)$ is the environment's probability. This formula removes bias caused by confounders by adjusting for different environments. An improved version for more complex cases, where both confounders $S$ and mediators $M$ are present, is given as $P(Y | do(X)) = \\sum_{s\\in S} \\sum_{m\\in M} P(Y | X,s,m)P(m | X)P(s)$, ensuring that mediators are treated separately from confounders. For training, CaaM uses an adversarial approach where a minimization objective ensures robust causal features are learned: $min_{A, A',f,g,h} X E(f,x, D)+E(g, A(x), T_{1})+E(h, A(x), D)$, and a maximization objective updates the data partitions to better capture confounders: $max_{A, D} IL(h, A(x), T_{1}(\\theta))$. Through these formulas, CaaM implements a causal attention mechanism in visual recognition tasks, improving model robustness when handling biased data.\nCausal Attention (CATT) [37]. This paper introduces Causal Attention (CATT), a novel attention mechanism designed to remove confounding effects in vision-language models. The approach is based on the front-door adjustment, i.e., $P(Y|do(X)) = \\sum_{z} P(Z = z|X)\\sum_{\\x}P(X = x)P(Y|Z = z, X = x)$, which estimates the causal effect of input X on output Y via a mediator Z, helping to address spurious correlations in data. In this formulation, $P(Z = z|X)$ is the probability of selecting Z given X, which is calculated by In-Sample Attention (IS-ATT), $P(X = x)$ represents Cross-Sample Sampling (CS-ATT) from other examples, and $P(Y|Z = z, X = x)$ models the final prediction using the sampled values. To efficiently implement the Causal Attention mechanism and reduce the computational cost of sampling, the paper introduces the Normalized Weighted Geometric Mean (NWGM) approximation:\n$$P(Y|do(X)) \\approx Softmax[g(\\tilde{Z}, X)], \\ \\ \\  \\tilde{Z} = \\sum_{z} P(Z = z|h(X))z \\ \\ \\  X = \\sum_{x} P(X = x\\| f(X))x$$", "where": ",  represents the predictive function modeled by a neural network, and  and  are the estimates for In-Sample and Cross-Sample sampling, respectively. The variables $h(X)$ and $f(X)$ are embedding functions for transforming the input X into query sets for IS-Sampling and CS-Sampling. The NWGM approximation absorbs the outer sampling operations, ensuring that only one forward pass is needed through the network.\nThese methods offer valuable insights for pre-trained language models (PLMs), encouraging the disentanglement of causal language patterns from spurious correlations within data. This approach has the potential to improve model robustness, particularly when handling out-of-distribution or biased text inputs. Building on these"}, {"title": "4 Causality for Fine-Tuning", "content": "To make a pre-trained foundation model useful for both specific and general tasks", "31": "such as feature extraction", "45": ".", "41": ".", "42": ".", "fine-tuning": "Effect_{p"}, "P(\\hat{Y} = \\hat{y}|do(P = p)) \u2013 P(\\hat{Y} = \\hat{y}|do(P = 0))$.This formula quantifies the difference in predictions between models fine-tuned with and without pre-trained data, highlighting the potential loss of knowledge during standard fine-tuning. CET mitigates this by using K-nearest neighbors (KNN) in the feature space to capture colliding effects: $Effect^{i}_{0} = P(\\hat{Y}^{(i)}|X = x^{(i,k)})W_{p}(x^{(i)}, x^{(i,k)})$. This equation preserves pre-trained knowledge by weighing the influence of similar samples in the hidden space. The overall unified objective combines vanilla fine-tuning and causal objectives to balance knowledge retention and learning from new data: $max_{D_{IES}} \\sum_{k}^{K}P(\\hat{Y}^{(i)}|X = x^{(i,k)})W_{p}(x^{(i)}, x^{(i,k)})+\\sum_{X^{TEST}} P(\\hat{Y}^{(i)} | X = x^{(i)})$. CET outperforms existing methods, effectively preserving commonsense knowledge and improving model performance on downstream tasks.\nDistilling Counterfactuals (DISCO) [43"]}