{"title": "Instance-wise Uncertainty for Class Imbalance in Semantic Segmentation", "authors": ["Lu\u00eds Almeida", "In\u00eas Dutra", "Francesco Renna"], "abstract": "Semantic segmentation is a fundamental computer vision task with a vast number of applications. State of the art methods increasingly rely on deep learning models, known to incorrectly estimate uncertainty and being overconfident in predictions, especially in data not seen during training. This is particularly problematic in semantic segmentation due to inherent class imbalance. Popular uncertainty quantification approaches are task-agnostic and fail to leverage spatial pixel correlations in uncertainty estimates, crucial in this task. In this work, a novel training methodology specifically designed for semantic segmentation is presented and tested in datasets of road scene images. Training samples are weighted by the uncertainty of each class instance estimated by an ensemble. This is shown to increase performance on minority classes, boost model generalization and robustness to domain-shift when compared to using the inverse of class proportions or no class weights at all. This method addresses the challenges of class imbalance and uncertainty estimation in semantic segmentation, potentially enhancing model performance and reliability across various applications.", "sections": [{"title": "1 Introduction", "content": "Semantic segmentation stands as a fundamental task in the computer vision domain, finding widespread adoption in areas such as autonomous driving [11, 25, 34], biomedical imaging [19, 38, 40] and more. This task involves classifying each pixel in an image into predefined categories, enabling detailed scene understanding crucial for various applications. State of the art semantic segmentation approaches predominantly employ deep earning solutions due to their exceptional performance. However, despite remarkable advancements in deep learning algorithms, the reliability and robustness of deployed AI systems remain critical concerns. These models often exhibit overconfidence when encountering unfamiliar data domains and lack robust mechanisms for estimating prediction uncertainty [15]. Addressing the challenge of uncertainty modeling and quantification in these models is a pivotal step in advancing the field.\nOne of the major challenges in semantic segmentation tasks is class imbalance. This problem arises because deep learning models tend to overfit to the best-represented classes, leading to suboptimal performance on worst-represented classes [3]. Class imbalance is particularly problematic in scenarios where the distribution of classes is inherently skewed, such as in biomedical imaging, where certain pathologies are rare, or in autonomous driving, where certain objects may be less frequently encountered.\nThe primary solution for mitigating class imbalance involves weighting the classes in the loss function according to the proportion of pixels of each class present in the data. By assigning higher weights to worse-represented classes, the model is encouraged to focus more on them during training, thereby improving its performance. However, this is typically not enough to significantly improve model performance on minority classes, and may even induce it to produce mis- calibrated confidence scores [5].\nIn this work, a novel training method for semantic segmentation is proposed. It leverages predictive uncertainty at the instance level to weight the loss function, thus making the model focus more on pixels of harder classes. Through diverse experiments, it is shown that this novel method not only increases the performance on minority classes but is also capable to provide good uncertainty estimates and greatly increase robustness to domain shift. It was also found that models adopting this training methodology significantly performed better on minority classes than models using the inverse of class proportions as class weights in the loss function. This makes the usage of instance-wise uncertainty as weights in the loss function a great option for imbalanced datasets."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Background", "content": "In the context of Machine Learning, uncertainty is usually decomposed into two different components according to its source, referred to as aleatoric uncertainty and epistemic uncertainty [20]. Aleatoric uncertainty (AU) encompasses the uncertainty that stems from the data, mainly due to noise and the complexity of the process that generated it [23]. Epistemic uncertainty (EU) refers to the uncertainty that arises from ignorance about the best model, i.e., from unawareness about the best model assumptions and uncertainty about its parameters. The predictive uncertainty for an input sample x is the sum of its epistemic uncertainty and aleatoric uncertainty. The most prominent way of formalizing these concepts is by utilizing a Bayesian framework [2]. Let Y be a vector of class probabilities for x, D a dataset and wa set of model weights. The predictive distribution P(Y|x, D) is given by\n$$P(Y|x, D) = E_{P(w|D)}[P(Y|x, w)]$$ (1)\nThe predictive uncertainty for a given input x is then given by the entropy H of the predictive distribution of x, i.e.,\n$$Predictive Uncertainty = H[E_{P(w|D)}[P(Y|x, w)]]]$$ (2)"}, {"title": "2.2 Uncertainty Quantification in Semantic Segmentation", "content": "The problem of uncertainty quantification (UQ) has been extensively studied across several domains and tasks, such as image classification, image semantic segmentation and natural language processing. Popular methods in the literature such as Monte-Carlo (MC) Dropout [13] and its variants [24, 39], Deep Ensembles [21] and Variational Inference (VI) [14] are task-agnostic and have all been used for quantifying uncertainty in semantic segmentation tasks. All these methods implicitly approximate (1) and use a dispersion metric (such as variance or entropy) to measure uncertainty.\nMC Dropout uses the dropout [37] technique to effectively \"turn off\" neurons and to obtain a different model prediction in each forward pass, with uncertainty being the variance of the confidence scores. This can be viewed as sampling different sets of model architectures to approximate P(w|D). However, since dropout is applied randomly, it may yield architectures with high loss values in each sample and lead to poor uncertainty estimates [12].\nEnsembles were first proposed to quantify uncertainty by randomly initializing the weights of M different fm models (with the same architecture) and independently training them on the same dataset. The confidence scores of these models are averaged into a single predictive distribution and the uncertainty is given by its entropy, i.e., H[\u2211m=1 fm(x)]. Thus, ensembles rely on random initialization of model weights to foster predictive diversity and to get low loss samples from P(w|D) by training these models. Nonetheless, random initialization of model weights does not allow for the usage of transfer learning techniques and has been found to be lackluster in achieving predictive diversity. To solve this problem, approaches such as using different architectures in the ensemble [10] have been proposed.\nVI is another method that has been used to approximate the posterior P(w/D) [14]. Using a family of parametric distributions Q(w), VI finds the parameters that minimize the difference between Q(w) and the posterior. This difference between distributions is measured by the Kullback-Leibler Divergence in (4):\n$$KL(Q||P) = E_Q \\left[log \\frac{Q(w)}{P(w|D)}\\right]$$ (4)\nSince this equation explicitly uses the posterior, it cannot be directly minimized. Thus, the Evidence Lower Bound (5) is maximized instead, since this is equivalent to minimizing the KL divergence up to an added constant.\n$$ELBO = E \\left[log \\frac{P(y|x, w)}{Q(w)}\\right]$$ (5)\nThe KL divergence can then be obtained by KL(Q||P) = -ELBO+log P(y|x) [14]. One pioneering work that leverages VI is Bayes By Backprop [2]. By using the Gaussian reparameterization trick [29], it is possible to approximate the ELBO as\n$$F(D,\\theta) \\sim \\sum_{i=1}^n log Q(w_i|\\theta) - log P(w_i) \u2013 log P(D|w_i)$$\nwhere \u03b8 represents the parameters of the variational posterior, which is chosen to be a diagonal Gaussian distribution with mean \u00b5 and standard deviation \u03c1. For the prior distribution P(w), a scale mixture of two Gaussians is used. The transform that yields a sample of weights w is defined as w = \u00b5+log(1+exp(\u03c1))\u00b7\u03b5, where \u20ac ~ N(0, I). In each optimization step, a sample of weights is drawn according to this transform and these are then used in the model to compute the gradients with the usual backpropagation algorithm. Parameters \u00b5and \u03c1 are then updated according to these gradients. Unfortunately, VI suffers from problems such as determining an appropriate variational family of distributions and the bias which is introduced by said family, as the true posterior P(w|D) is often more complex than Q(w). For semantic segmentation, it has also been pointed that works using variational inference relying on sampling for the ELBO gradients estimation is not viable in semantic segmentation due to its high-dimensionality of inputs and outputs [6].\nUQ methodologies that do not try to approximate the posterior also exist. Deep Deterministic Uncertainty [27] is a single model approach that models each class in feature space using Gaussian discriminant analysis, which enables EU and AU estimates to be obtained. A similar method leveraging Quantum Information Potential Field to decompose the probability density function of the data in feature space to a reproducing kernel hilbert space has also been introduced in [35]. However, it has been argued that single model UQ provides poorer uncertainty estimates than the previously presented approaches [31].\nMost of these works propose task-agnostic UQ methods which do not leverage existing pixel correlations that are crucial in semantic segmentation. Furthermore, works that propose the usage of uncertainty values to improve model performance have limitations. Closely related to this work, [22] computes pixel- wise uncertainty masks using MC Dropout and uses them to weight the cross entropy loss function. However, MC Dropout requires multiple forward passes to compute the uncertainty of an input, which significantly slows down training. Additionally, by using pixel-wise uncertainty as weights for each pixel in the loss function, one fails to consider the uncertainty of the neighborhood pixels."}, {"title": "3 Semantic Segmentation with Instance Uncertainty", "content": "In this section, a novel method for semantic segmentation using instance-based uncertainty estimates is presented. Semantic segmentation can be viewed as a classification problem where the goal is to classify each pixel of an image as belonging to one of C classes. Thus, the output of a semantic segmentation method is a segmentation mask where each pixel is assigned a class. Since the classes in semantic segmentation problems are usually imbalanced due to the inherent difference of the number of pixels in each of them, models have more difficulty learning features of the minority classes. The predictive uncertainty of the model associated with each pixel can be viewed as a measure of how hard it is to classify, thus can be used as a weight in the loss function to make the model focus more on these pixels. A simple method to improve the performance of semantic segmentation models in less well represented classes is then achieved by solving the following problems:\n1. Quantifying the uncertainty for each pixel of each training sample.\n2. Computing the uncertainty for each instance of a class in each training sample.\n3. Using the estimated uncertainties to weight the training sample.\nTo compute the pixel-wise uncertainty for each training sample, an UQ method can be adapted and used for semantic segmentation. Since ensembles have been known to achieve state of the art performance on UQ tasks, are simple to implement and can be run in parallel, this was the adopted approach to solve problem 1. Ensembles can generate pixel-wise uncertainty masks by combining the output confidence scores of each model, thus obtaining a predictive distribution for each pixel. The entropy of each of these distributions is then computed, resulting in pixel-wise uncertainty masks.\nThe purpose of using instance-wise uncertainty instead of settling for the pixel level is to incorporate the uncertainty information of the region where each pixel belongs in their weight for the loss function. Independent pixel uncertainty estimates may result in irregular weights, therefore hindering the training process. This can be circumvented by using the average uncertainty of the pixels in each region of the image, yielding smoother weights. However, choosing a meaningful neighborhood region to consider for each pixel is not obvious and can be done in multiple ways. One possible solution is to use the average uncertainty of the pixels in each instance. An instance of class c is defined as a set of pixels that belong to c such that there is a sequence of adjacent pixels in the set that connects each of its pixels.\nFor each pixel p, its instance uncertainty value is given by (7), where I denotes an instance from the set R of all instances in the input and PU denotes the pixel-wise predictive uncertainty. 1I(p) denotes the indicator function, that is, 1I (p) = 1 if pixel p belongs to instance I and is zero otherwise.\n$$Instance Uncertainty (p) = \\sum_{I \\in R} 1_I(p).\\frac{\\sum_{k \\in I} PU(k)}{|I|}$$\nThe problem of computing all instances for each class can then be solved by viewing the image as a connected graph and instances as strongly connected components. By determining all connected components in each input and their corresponding average uncertainty, an instance-wise uncertainty mask is obtained, thereby solving problem 2.\nThe most intuitive and direct way of using these instance-wise uncertainty masks to weight the loss function is by computing the Hadamard product between them and the cross entropy (CE) output tensor. However, since the uncertainty estimate for a pixel can be smaller than 1, this can heavily decrease the weights of low uncertainty pixels in the loss function and hinder model performance. This can be mitigated by adding 1 to each element in the uncertainty mask. Solving problem 3, the uncertainty weighted loss is given by (8), where \u0177 and y denote the tensor of predicted logits and the ground truth, respectively. IU denotes the computed instance uncertainty mask and denotes the Hadamard product.\n$$L(\\hat{y}, y) = CE(\\hat{y}, y) \\odot (1 + IU_I)^2$$"}, {"title": "4 Implementation", "content": "The first step in building an ensemble is choosing the model architectures that integrate it. Considering a single architecture and randomly initializing model weights prohibits the usage of pre-trained model weights through transfer learning, which is known to enhance the performance of deep learning models. Additionally, due to the problems in achieving ensemble diversity through this method, three different backbone architectures (MobileNetV2 [17], ResNet50 and ResNet101 [16]) of a DeepLabV3+ [7] segmentation model were chosen. Although the original Deep Ensembles work suggests using five models, it is argued that reducing the number of models in the ensemble to three does not significantly impact performance [1, 9], thus motivating the choice of ensembling only three different models. Differing at the stage where the uncertainty masks are computed, two approaches are considered:\n1. Train models M1, M2, M3 independently on the training set, using the standard cross entropy loss. Build an ensemble using these models and compute the instance-wise uncertainty masks for each training sample. Train new models M1, M2, M3 by incorporating these uncertainty masks in their loss function, given by (8). The advantage in this scenario is that by using good performing models, the uncertainty masks will be of higher quality. However, this adds the computational overhead of training an ensemble of three models solely to obtain uncertainty estimates.\n2. Build an ensemble using models M1, M2, M3 and train them in parallel. For each training sample, obtain the instance-wise uncertainty mask and use it to compute each individual model loss function (8). This way, the models directly compute uncertainty masks for training on the fly, without the need"}, {"title": "5 Evaluation Metrics", "content": "For the semantic segmentation task, the Intersection-Over-Union (IoU) metric is the most popular to assess model performance. For a given class c, its IoU value is given by equation (9). TP denotes the number of class c pixels that were correctly classified (True Positives), FNc denotes the number of class c pixels that were classified as belonging to another class (False Negatives) and FP denotes the number of pixels of other classes that were incorrectly classified as belonging to class c (False Positives). The mean Intersection-over-Union (mIoU) is simply the average of each class IoU.\n$$IoU (c) = \\frac{TP}{TP + FN + FP}$$   $$mIoU = \\frac{\\sum_{c=1}^{C} IoU (c)}{C}$$ (9)\nAlthough several uncertainty metrics have been proposed, such as expected calibration error [28] and the Brier score [4], accurately and consistently measuring uncertainty is still an open problem. Specific to semantic segmentation, the Patch Accuracy vs Patch Uncertainty metric [26] (PAvPU) is used to evaluate the degree to which a model correctly estimates uncertainty. A patch is defined as a window of w by w pixels. Each patch in an image is grouped into four classes according to two criteria: Accuracy and Certainty. If the number of correctly classified pixels in a patch divided by its size is greater than some threshold (usually 0.5), the patch is considered accurate and inaccurate otherwise. If the estimated mean uncertainty in a patch is lower than some threshold (the mean pixel uncertainty of the training set is recommended), the patch is considered certain and uncertain otherwise. Let nac, Nic, Nau, Niu be the number of patches that are accurate and certain, inaccurate and uncertain, accurate and uncertain and inaccurate and uncertain, respectively. The PAvPU is given by (10).\n$$PAUPU = \\frac{N_{ac} + N_{iu}}{N_{ac} + N_{iu} + N_{ai} + N_{ic}}$$"}, {"title": "6 Experiments", "content": "In this section, all of the performed experiments to test the proposed framework are reported. The CityScapes [8] and ACDC [32] datasets were used as benchmarks due to their compatibility and extensive study in the literature. The CityScapes dataset contains 5000 images with high quality pixel level annotations of 19 different classes of road scenes. Of these, 2975 are training images, 500 for validation and 1525 for testing. The ACDC dataset was designed to be compatible with CityScapes, as it is comprised of 4006 road scene images of the same classes as CityScapes. However, unlike CityScapes, its images are captured in adverse weather conditions. The public dataset contains 1600 training images and 406 validation images. The test set is private."}, {"title": "6.1 Measuring the impact of instance-wise uncertainty in model performance", "content": "The goal of the first experiment was to understand how weighting the loss function using uncertainty estimates impacts model performance. For this reason, only a single dataset (ACDC) was used. Since ACDC's test set is private, all results come from the validation set. The first proposed method was tested by training three different DeepLabV3+ models with different backbone architectures, as mentioned in the preceding section. Instance-wise uncertainty masks"}, {"title": "6.2 Improving model performance in domain-shift using instance-wise uncertainty", "content": "To understand whether the proposed uncertainty masks improve the model's ability to generalize beyond its training data and its robustness to domain-shift, all models trained in the previous experiment were tested on the CityScapes validation set. Since CityScapes and ACDC are class compatible and their main difference is the presence of adverse weather conditions in ACDC, considering models trained on the ACDC dataset and evaluating them on CityScapes represents a very significant domain-shift from adverse weather to normal weather conditions. The class IoU values on the CityScapes validation set obtained by the 180 DeepLabV3+ models with different backbones trained on ACDC are reported in Table 5."}, {"title": "7 Conclusion", "content": "In this work, a novel training approach for semantic segmentation that levarages instance-wise uncertainty to circumvent class imbalance was presented. Compared to currently available methods in the literature, the generation of uncertainty masks is done by computing the mean uncertainty of relevant neighborhood pixels, which corresponds to the mean uncertainty in each instance. By training as few as three models, an ensemble can be formed and used to compute an instance-wise uncertainty mask. These masks are then used to weight the cross entropy loss function, giving larger weights to pixels of class instances with higher uncertainty and vice-versa. The instance-wise uncertainty masks can be computed either during the training of an ensemble or by leveraging previously trained models. Although there is some computational overhead, models using uncertainty masks to weight their loss function were found to significantly improve their performance on minority classes, crucial for semantic segmentation due to class imbalance being very common, when compared to the usage of no loss weights or the usage of the inverse of class proportions. Instance-wise uncertainty also drastically improved the generalization and reduced the overfitting of some models, which leads to safer real world deployment. Reducing the computational overhead of computing instance-wise uncertainty masks and finding other ways to integrate them in the training process represents an interesting path for future work in improving semantic segmentation performance."}]}