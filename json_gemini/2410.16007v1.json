{"title": "ARE LANGUAGE MODEL LOGITS CALIBRATED?", "authors": ["Charles J. Lovering", "Michael Krumdick", "Viet Dac Lai", "Varshini Reddy", "Nilesh Kumar", "Rik Koncel-Kedziorski", "Chris Tanner"], "abstract": "Some information is factual (e.g., \"Paris is in France\"), whereas other information is probabilistic (e.g., \u201cthe coin flip will be a [Heads/Tails].\u201d). We believe that good Language Models (LMs) should understand and reflect this nuance. Our work investigates this by testing if LMs' output probabilities are calibrated to their textual contexts. We define model \u201ccalibration\" as the degree to which the output probabilities of candidate tokens are aligned with the relative likelihood that should be inferred from the given context. For example, if the context concerns two equally likely options (e.g., heads or tails for a fair coin), the output probabilities should reflect this. Likewise, context that concerns non-uniformly likely events (e.g., rolling a six with a die) should also be appropriately captured with proportionate output probabilities. We find that even in simple settings the best LMs (1) are poorly calibrated, and (2) have systematic biases (e.g., preferred colors and sensitivities to word orderings). For example, gpt-40-mini often picks the first of two options presented in the prompt regardless of the options' implied likelihood, whereas Llama-3.1-8B picks the second. Our other consistent finding is mode-collapse: Instruction-tuned models often over-allocate probability mass on a single option. These systematic biases introduce non-intuitive model behavior, making models harder for users to understand.", "sections": [{"title": "INTRODUCTION", "content": "We investigate the extent to which language model (LM) output probabilities are calibrated to the numeric content of their contexts. Consider the contexts below:\n(1) From 17 blue marbles and 99 red marbles, Tommy reached blindly into a bag and grabbed a\nmarble that was the color [blue/red]\n(2) From 98 blue marbles and 99 red marbles, Tommy reached blindly into a bag and grabbed a\nmarble that was the color [blue/red]\nWe believe the probabilities for the next generated token should be calibrated to the relevant numeric\ncontent, capturing some of the nuance of probabilistic information. E.g., in the examples above, the"}, {"title": "What Do We Mean By Calibration?", "content": "LMs output probabilities are calibrated to their contexts if the probabilities of relevant tokens\ncorrespond to the numeric content implicitly or explicitly defined by their contexts. If tokens\n$t_1, t_2,... t_n$ are indicated by context $C$ to have probabilities $P = p_1, p_2 ... p_n$, a calibrated\nLM $m$ outputs corresponding probabilities $\u03c0_i = m(t_i|c)$:\n$\u03a1_\u03af \\propto \u03c0_\u03af$,\n$i \u2208 1,2... n$.\nWe measure calibration by the distance between $P$ and $II = \u03c0_1, \u03c0_2,... \u03c0_\u03b7$, Section 3.2."}, {"title": "RELATED WORK", "content": "Models struggle to calibrate internal uncertainty with textual outputs. A related but separate\nline of work has studied the calibration in neural networks in terms of their ability to match their\nprediction confidence with prediction accuracy: a network with 0.8 confidence on each prediction\nshould classify 80% of examples correctly (Guo et al., 2017; Minderer et al., 2021). Yona et al."}, {"title": "EXPERIMENTAL DESIGN", "content": "We run two experiments that evaluate if model probabilities are calibrated to the numeric values either\nexplicit or implicit in context, Section 3.4 (Distributions) and Section 3.5 (Probabilities). See\nAppendix appendix A for more information on reproducing these experiments."}, {"title": "PROBLEM SETUP", "content": "Each problem assumes a context C to be continued by a relevant token among t1, t2, ... tn =TCV\nwhere V is the full vocabulary. Each token ti is associated with a probability pi forming a distribution,\nP = P1,P2,... pn. In Example 1, T is {red, blue} with P = {red : 0.75, blue: 0.25}. For a\nmodel m, we define its probabilities over these words as II = \u03c01, \u03c02,\u00b7 where \u03c0\u2081 = m(ti|C), with\none addendum. We sum the probabilities of all tokenizations for a given word, which can include\ncapitalization and spaces: \u03c0i = \u2211s\u2208Tokenizations(t\u2081) m(s|C). E.g., we sum probabilities for \u201cred\u201d,"}, {"title": "METRICS AND EVALUATION", "content": "Our primary metrics below help us ask how well a model calibrates to numeric information in context.\nProbability Mass (PM): Calibration, the relative probability mass on tokens in T, only matters\nif the PM that falls upon T is sufficient, PM(T) = \u2211t: \u03c4 \u03a0t. If PM(T) for a model is low (say,\n0.30), this indicates that the model is not capturing the intended relationship between the context and\nT. However, when PM is high (say, 0.75), we can start to meaningfully ask questions about how\nprobability mass is allocated among, say, red and blue. While our metric for measuring calibration\n(introduced below) would also capture when the PM is too low, separately measuring this information\nmakes it easier to understand and compare model behavior.\nRelative Entropy (RE): To detect mode collapse, we measure RE, the ratio between entropy H of\nmodel probabilities and the calibrated probabilities, H(\u03a0)/H(P). An RE less than one implies the\nprobability mass is overly concentrated.\nWasserstein Distance (WD): To measure calibration, we use WD. WD (Kantorovich, 1939) captures\nthe movement between one distribution (or set of values) and another."}, {"title": "MODELS", "content": "We test open-source models that have both base and instruction-tuned versions: Mistral-7B-v0.1,\nMistral-7B-v0.3, Mixtral-8x7B-v0.1, Yi-1.5-9B, Yi-1.5-34B, Meta-Llama-3.1-8B,\ngemma-2-9b, gemma-2-27b (Jiang et al., 2023; 2024; Young et al., 2024; Dubey et al., 2024; Team"}, {"title": "EXPERIMENT: Distributions", "content": "This experiment tests if models can produce a uniform distribution in their output probabilities.\nExample 3, repeated below, is representative of the instances in this dataset:\n(3) Sampling from a uniform distribution with support from 2 inclusive to 5 exclusive, I sampled\nthe number [0/1/2/3...9]\nWe test across different numeric ranges, range inclusivities, and templates. The numbers scale from\n10 to 903 and we use five different templates; there are 4500 unique instances. In some of our\nanalyses, we group problem ranges and inclusivities that support the same tokens: For example, [12,\n15) and [92, 95) are grouped. There are 615 such groups.\nFrom here, the question is if the distribution of the next token is calibrated to the uniform dis-\ntribution $I_T = U(2,5)$. As noted above, we report WD, which for this example would be:\n$WD(I_T,U(2,5)) = WD(I_T, [0, 0, 1/3, 1/3, 1/3, 0, ...,0]$, where T includes digits (tokens) 0 to\n9 and U(2, 5) covers all digits (with 0 for unsupported digits)."}, {"title": "Additional Metrics", "content": "We introduce three metrics to understand if there are patterns in the mode.\nMode Probability is the maximum probability of II averaged across all examples in the dataset,\n$AVG_{dataset} max \u03a0$.\nMode Stability is the rate at which the most likely token is preserved between base and chat\nversions of a model averaged across all examples in the dataset, $AVG_{dataset} (1(arg max I_{base} =$\n$arg max I_{chat}))$.\nMode Frequency examines whether there are biases for particular tokens (numbers). This metric\nmeasures the frequency of the mode averaged across distributions. Because different distributions\ncover different digits, we group the distributions by underlying problem range and inclusivity: E.g.,\n[2, 5) and [132, 135) are grouped. This is equivalent to: (1) setting the temperature to 0 (greedy\nsampling), (2) averaging over the groups, keeping the digits dimension, (3) taking the maximum for\neach group, and (4) averaging across all maximums, $AVG(max_{digits} AVG_{distributions, keepdim}(\u03a0\u207a=0))$."}, {"title": "EXPERIMENT: Probabilities", "content": "This experiment tests whether the models' output probabilities reflect values in their context. The\nfollowing is a representative example of the dataset:\n(4) From 17 red marbles and 99 blue marbles, Billy reached blindly into a bag and grabbed a\nmarble with the color [red/blue]\nWe use five templates, three numeric scales with 100 configurations each, and 110 pairs of options\n(e.g., red/blue, or orange/purple). The numeric scales are (1) all numbers under ten, (2) ten numbers\nsampled under 100, and (3) ten numbers sampled under 1000. To identify potential biases, we analyze\nhow model behavior varies with respect to all of the settings listed above. Some factors, such as\nthe numeric context, should influence the model's behavior while others, such as the ordering of the\noptions, should not.\nTo simplify the analysis of this large number of configurations, we categorize the model behavior\ninto one of six potential reference points based on WD. In practice, we find that models typically\nfollow one of these six behaviors, making this a useful summary of performance."}, {"title": "RESULTS", "content": "There are a few patterns across all models. Table 1 (left) reports the primary metrics. Chat (instruction-\ntuned) models are less calibrated and have a lower RE than their base counterparts. The base models\nare more calibrated than the Random baselines. All tested gpt-* models have similar calibration\nscores to the open-source chat models.\nWe find evidence that the reduced performance of chat models results from their tendency to over-\nallocate probability on a subset of valid tokens. Table 1 (right) provides additional metrics that\nsuggest mode-collapse. For chat models, the most likely token receives, on average, 66% of the"}, {"title": "Distributions: Systematic Behaviors", "content": "(1) Chat models place a majority of probability mass on a subset of valid tokens;\n(2) The mode is often stable between base to chat models at the instance level.\n(3) We see poor calibration at the instance level and disproportionate outcomes."}, {"title": "Probabilities", "content": "High-level results. All models perform worse than some simple baselines. Random-=1.0 performs\nbetter than all models. Pick higherp=0.7, which allocates 0.7 probability mass to the option associated\nwith the higher value, further outperforms all models. Still, we see interesting performance differences\nacross models.\nThere are two key parallels between the results in this set of experiments and those in the last section:\n(1) Instruction tuning decreases the entropy of model output probabilities. The decrease in entropy\nis often due to a large increase of probability on the token with the corresponding higher value\n(replicating Pick Higher). Unlike the previous experiment, this tends to improve the calibration\nscore overall. Table 2 displays high-level results. (2) Model behavior is systematic (and uncalibrated).\nUnderstanding (2) requires disaggregating the results and examining model behavior across different\ncolors and color orderings."}, {"title": "HUMAN STUDY", "content": "Our experiments so far have demonstrated that these models are generally poorly calibrated. In this\nsection, we try to provide greater context for these results by directly comparing the calibration\nof proprietary models against humans. We do so using a variation of the storied game, Matching\nPennies. The only foolproof approach to winning this game is to behave as randomly as possible.\nResults are presented in Table 3. The two main takeaways are: 1) Models also produce non-random\nsequences in this setup, further demonstrating their lack of calibration; 2) When prompted, the\nmodel's performance improves and is on par with human performance."}, {"title": "EXPERIMENTAL DESIGN", "content": "This experiment is based on a simple heads/tails game. Our motivation for including this experiment\nis to allow a human baseline in a setting where random behavior is optimal. This section makes no\ndistinction between calibrated probabilities and unbiased outputs because we don't have access to\nthis information for humans. However, to \u201cperform\" well, or appear calibrated, requires either the\nmodel/person to (1) produce calibrated probabilities which when sampled from will yield random"}, {"title": "RESULTS", "content": "When a model is instructed to be random (denoted by \u201c+instr\u201d) Player Win Rate (PWR) improves. For\ngpt-40, PWR increases by +30%. Explicit instruction following ability can thus improve calibration\nat the outcome level. Notably, when prompted, model performance is similar to human performance."}, {"title": "Humans and Proprietary (OpenAI) Models Are Both Non-Random (Uncalibrated)", "content": "(1) Even top-performing proprietary models are unable to produce random behavior\n(2) Human behavior is also non-random. Even if models are currently similar to humans in\nthis regard, we argue that future work endowing models with the ability to produce calibrated\noutputs would improve human interactions with models."}, {"title": "DISCUSSION", "content": "Why Aren't LLMs Already Calibrated? Should the negative log-likelihood loss endow a pre-\ntrained language model with calibration? If there were n \u2192 \u221e examples of Heads and Tails being\nflipped in the data and the true distribution was reflective of some $P_{heads}$, then the lowest loss would be\nachieved via a perfectly calibrated model. Thus, the answer is empirical: What does the training data\nlook like? Finding the disparate and analogous scenarios that call for randomness may be difficult.\nNon-calibrated outputs frequently occurring for such scenarios might impact model optimization\nenough to overcome examples where the loss is explicitly aligned with a calibrated model. For\nexample, patterns derived from people preferring the number 7 (out of 10) or 42 (out of 100) or Heads\n(compared to Tails) or answering with rounded numbers that end in 0 or 5 might reduce calibration.\nBeyond impacting the loss, it could be the case that some well-trained models are already capable\nof producing calibrated results; the models may \u201cknow\" the calibrated output but adjust it to match\nnon-random tendencies, akin to pragmatics.\nShould Models Be Calibrated? Our work highlights how models that perform remarkably in\nother types of evaluations fall short in scenarios that call for calibrated outputs. Some may have\nexpected our results given that these models are trained on data generated by people who are also\nwidely biased. Others might have expected that with the range of data seen in training (data in the\nwild, code outputs, endemic instances of distributions) models would learn to calibrate. However, it\nseems curious that models are frequently able to correctly generate text describing what the proper\ncalibrated distribution should be for a given scenario, but fail to represent that distribution internally\nwhen prompted to simulate the same scenario.\nCan We Calibrate Models with Fine Tuning? We expect that it is possible to fine-tune models\nto improve calibration. Already we see some improvements with generic alignment procedures,\nthough these improvements often come at a tradeoff of an (overly large) reduction in model entropy.\nMitigating this mode collapse behavior is a compelling area for future work."}, {"title": "CONCLUSION", "content": "All models we test are not only not calibrated, but behave in different systematically uncalibrated\nways. A common theme is that top-performing models (e.g., gemma-2-27b and gpt-* models) Pick\nHigher, allocating most of the probability mass upon single output token. This is an example of\nmode collapse. Though mode-collapse is commonly studied in computer vision and image generation\n(Thanh-Tung & Tran, 2020; Srivastava et al., 2017) inter alia, O'Mahony et al. (2024) show how\nRLHF reduces diversity in model outputs, leading to this problem in text generation. Our results are\nsimilar: Instruction-tuned models produce valid outputs, but the probability distribution is collapsed\nto a small portion of the valid answers that are equally correct. While we focus on controlled problem\nsettings, these issues leave vulnerabilities. For instance, an adversary could leverage that a model is\ngoing to behave in a systematically arbitrary manner."}, {"title": "DETAILS", "content": "All data and experiments will be released at https://drive.google.com/drive/folders/\n1 jAcKRqAuv61syDaCtkSXbq3KBYwrzIW_?usp=sharing. All details required for reproducing our\nresults like problem templates and prompts will also be released there. Appendix A report the\nprompts (formatted as messages) and problem templates.\nWe use the models listed in Table 4. For visual acuity, we put the full model codes into this table\nrather than throughout the paper."}, {"title": "HUMAN STUDY", "content": "Appendix B is the implementation for our n-gram model. In practice, we find that most n from 1 to 5\ndo similarily well. In the paper we report a 4-gram model."}]}