{"title": "MULTI-KNOWLEDGE FUSION NETWORK FOR TIME SERIES REPRESENTATION LEARNING", "authors": ["Sagar Srinivas Sakhinana", "Shivam Gupta", "Krishna Sai Sudhir Aripirala", "Venkataramana Runkana"], "abstract": "Forecasting the behaviour of complex dynamical systems such as interconnected sensor networks characterized by high-dimensional multivariate time series(MTS) is of paramount importance for making informed decisions and planning for the future in a broad spectrum of applications. Graph forecasting networks(GFNs) are well-suited for forecasting MTS data that exhibit spatio-temporal dependencies. However, most prior works of GFN-based methods on MTS forecasting rely on domain-expertise to model the nonlinear dynamics of the system, but neglect the potential to leverage the inherent relational-structural dependencies among time series variables underlying MTS data. On the other hand, contemporary works attempt to infer the relational structure of the complex dependencies between the variables and simultaneously learn the nonlinear dynamics of the interconnected system but neglect the possibility of incorporating domain-specific prior knowledge to improve forecast accuracy. To this end, we propose a hybrid architecture that combines explicit prior knowledge with implicit knowledge of the relational structure within the MTS data. It jointly learns intra-series temporal dependencies and inter-series spatial dependencies by encoding time-conditioned structural spatio-temporal inductive biases to provide more accurate and reliable forecasts. It also models the time-varying uncertainty of the multi-horizon forecasts to support decision-making by providing estimates of prediction uncertainty. The proposed architecture has shown promising results on multiple benchmark datasets and outperforms state-of-the-art forecasting methods by a significant margin. We report and discuss the ablation studies to validate our forecasting architecture.", "sections": [{"title": "1 INTRODUCTION", "content": "Accurate multivariate time series forecasting(MTSF) is critical for a broad spectrum of domains that have significant financial or operational impacts, including retail and finance, intelligent transportation systems, logistics and supply chain management, and many others. However, MTSF can be challenging due to the complexity of the relationships between time series variables and the unique characteristics of the MTS data, such as non-linearity, heterogeneity, sparsity, and non-stationarity. In this context, Spatial-temporal graph neural networks(STGNNs) have been widely studied for modeling the long-range intra-temporal dependencies and complex inter-dependencies among the variables in the MTS data for improved multi-horizon forecast accuracy. The explicit relationships among variables are based on prior knowledge provided by human experts in the form of a predefined or explicit graph, while implicit relationships among variables within the MTS data are obtained through neural relational inference methods(Deng & Hooi (2021); Kipf et al. (2018)). The implicit relationships are highly-complex and non-linear, can change over time, and uncover hidden relationships unknown to human experts which are not obvious. The existing \"human-in-the-loop\" STGNNS(Yu et al. (2017), Li et al. (2017), Guo et al. (2020)) incorporate domain-specific knowledge of the relational-structural dependencies among the interdependent variables while simultaneously learning the dynamics from the MTS data. However, arguably, the explicit graph structures in most real-world scenarios are either unknown, inaccurate, or partially available, thus resulting in suboptimal forecasting. Even if available, the explicit graph structure represents a simplified view of dependencies and often fails to capture the non-static spatial-temporal dependencies within the MTS data. Precisely it falls short of accurately inferring the latent time-conditioned underlying relations that drive the co-movements among variables in the substantial MTS data. On the contrary, a recent class"}, {"title": "2 PROBLEM DEFINITION", "content": "Lets us assume a historical time series data, with n-correlated variables, observed over T training steps is represented by $X=(x_1,...,x_T)$, where the subscript refers to time step. The observations of the n-variables at time point t are denoted by $x_t=(x_t^{(1)},x_t^{(2)},...,x_t^{(n)})\\in\\mathbb{R}^{(n)}$, where the superscript refers to variables. Under the rolling-window method for multi-step forecasting, where at the current time step t, we predefine a fixed-length look-back window to include the prior $\\tau$-steps of historical MTS data to predict for the next v-steps. In the context of MTSF, the learning problem can be formalized using the rolling window method. The goal is to use a historical window of n-correlated variables, represented by the $X_{(t-\\tau: t-1)}\\in\\mathbb{R}^{n \\times \\tau}$, which have been observed over previous $\\tau$-steps prior to current time step t, to predict about the future values of n variables for the next v-steps denoted as $X_{(t:t+\\nu-1)}\\in\\mathbb{R}^{n \\times \\nu}$. The MTSF problem is further formulated on the graph and hypergraph structure to capture the spatial-temporal correlations among multitudinous correlated time series variables. We represent the historical inputs as continuous-time spatial-temporal graphs, denoted as $G_t=(V,E,X_{(t-\\tau: t-1)}, A^{(0)})$. $G_t$ is composed of a set of nodes(V), edges(E) that describe the connections among the variables and node feature matrix $X_{(t-\\tau: t-1)}$ that changes over time, where t is the current time step. The adjacency matrix, $A^{(0)}\\in\\{0,1\\}^{\\|V\\|\\times\\|V\\|}$, describes the explicit fixed-graph"}, {"title": "3 OUR APPROACH", "content": "The overall neural forecasting architecture of our framework is illustrated in Figure 1. It consists of three main components: The projection layer, spatial learning, and temporal learning components. The spatial learning component includes two modules: graph and hypergraph learning modules. The hypergraph learning module infers the discrete dependency hypergraph structure to capture the interrelations between time-series variables. It also performs higher-order message-passing schemes to learn the time-conditioned optimal hypernode-level representations by modeling the hypergraph-structured MTS data. The graph learning module utilizes the predefined graph, which represents the relational structure of the variables obtained from domain-expertise knowledge to obtain the graph-structured MTS data. It performs spatial graph-filtering through neighborhood aggregation schemes to compute the optimal node-level representations that better capture the underlying dynamics within the MTS data. The temporal inference component performs a convex combination of the latent explicit-graph and implicit-hypergraph representations and learns the time-evolving inter-dependencies to provide pointwise forecasts and uncertainty estimations. Overall, joint optimization of different learning components of the proposed framework effectively captures the complex relationships between time-series variables and makes accurate forecasts."}, {"title": "3.1 PROJECTION LAYER", "content": "The projection layer utilizes a gated linear networks(GLN, Dauphin et al. (2017)) to learn the non-linear representations of the input data, $X_{(t-\\tau: t-1)}\\in\\mathbb{R}^{n \\times \\tau}$ through a gating mechanism to compute a transformed feature matrix, $X'_{(t-\\tau: t-1)}\\in\\mathbb{R}^{n \\times d}$ as follows,\n$X'_{(t-\\tau: t-1)}=(\\sigma(W_0X_{(t-\\tau: t-1)}) \\& W_1X_{(t-\\tau: t-1)})W_2$\nwhere $W_0, W_1, W_2 \\in \\mathbb{R}^{\\tau \\times d}$ are trainable weight matrices, & denotes the element-wise multiplication. $\\sigma$ is the non-linear activation function."}, {"title": "3.2 SPATIAL-INFERENCE", "content": "The spatial inference component of our framework is illustrated in Figure 2. The spatial-learning component encodes non-linear input data, $X_{(t-\\tau: t-1)}$ to obtain graph and hypergraph representations using two modules: the hypergraph learning module and the graph learning module. The hypergraph learning module performs joint hypergraph inference and representation learning, while the graph learning module performs graph representation learning. The outputs of these two modules are fused using a convex combination approach to regulate the flow of information encoded by each module. The details of each module are discussed in subsequent sections."}, {"title": "3.2.1 HYPERGRAPH INFERENCE AND REPRESENTATION LEARNING", "content": "The hypergraph learning module is composed of two units: the hypergraph inference(HgI) unit and the hypergraph representation learning(HgRL) unit. The HgI unit is a structural modeling approach"}, {"title": "3.2.2 GRAPH REPRESENTATION LEARNING(GRL)", "content": "We represent the MTS data as continuous-time spatio-temporal graphs based on predefined graphs obtained from domain-specific knowledge. We utilize the Temporal Graph Convolution Neural Network(T-GCN, Zhao et al. (2019)) to compute optimal node-level representations by modeling"}, {"title": "3.3 TEMPORAL-INFERENCE", "content": "The mixture-of-experts(MOE) mechanism combines the predictions of multiple subnetworks (ex-perts) to produce a final prediction. In this specific framework, the experts are graph and hypergraph learning modules. The expert predictions are combined through a gating mechanism in an input-dependent manner by calculating a weighted sum of their predictions. The goal of training in this framework is to achieve two objectives: 1) Identifying the optimal distribution of weights for the gat-ing function that precisely captures the underlying distribution within the MTS data, and 2) Training the experts using the distribution weights specified by the gating function. The fused representations are obtained by combining the predictions of the experts using the calculated weights as follows,\n$h'''_{(t)}=\\sigma(g''(\\mathbf{h}'_{(t)})) + (1 - g'')(\\mathbf{h}''_{(t)}); g''=\\sigma(\\mathbf{f}''(\\mathbf{h}'_{(t)}) + \\mathbf{f}'''(\\mathbf{h}''_{(t)}))\\newline$\nwhere $\\mathbf{h}'_{(t)}$, $\\mathbf{h}''_{(t)}$ are computed by the graph and hypergraph learning modules, respectively. $\\mathbf{f}''$ and $\\mathbf{f}'''$ are linear projections. The temporal inference component consists of a stack of 1 \u00d7 1 convolu-tions. The fused representations are fed as input to the temporal inference component. This compo-nent aims to model the non-linear temporal dynamics of inter-series dependencies among variables underlying the spatial-temporal MTS data and predicts the pointwise forecasts, $X_{(t:t+\\nu-1)}$. Our pro-posed framework uses the spatial-then-time modeling approach to learn the higher-order structure representation and dynamics in MTS data. This approach first encodes the spatial information of the relational structure, including both explicit graph and implicit hypergraph, which captures the com-plex spatial dependencies. By incorporating the temporal learning component, the framework then analyzes the evolution of these dependencies over time, which helps to improve the interpretability and generalization of the framework. This approach is beneficial for dealing with real-world appli-cations that involve entangled complex spatial-temporal dependencies within the MTS data, which can be challenging to model using traditional methods. Additionally, by minimizing the negative Gaussian log likelihood, the uncertainty estimates of forecasts can be provided through the w/Unc-EIKF-Net framework(i.e., EIKF-Net with Local Uncertainty Estimation). Minimizing the nega-tive Gaussian log likelihood is equivalent to maximizing the likelihood of the model's predictions given the true values. This allows the framework to provide more accurate and reliable uncertainty estimates of forecasts. In summary, our proposed methods(EIKF-Net, w/Unc- EIKF-Net) allow for simultaneously modeling the latent interdependencies and then analyze the evolution of these dependencies over time in the sensor network based dynamical systems in an end-to-end manner."}, {"title": "4 DATASETS", "content": "We conduct experiments to verify the performance of proposed models(EIKF-Net, w/Unc- EIKF-Net) on large-scale spatial-temporal datasets. The real-world traffic datasets(PeMSD3, PeMSD4, PeMSD7, PeMSD7(M), and PeMSD8) were collected by the Caltrans Performance Measurement System(PeMS, Chen et al. (2001)), which measures the traffic flow in real-time. We preprocess all the datasets by aggregating the 30-second intervals data to 5-minute average intervals data(Choi et al. (2022)) to ensure consistency and fair comparison with previous works. In addition, we also utilize publicly available traffic flow prediction datasets(METR-LA, PEMS-BAY) presented by Li"}, {"title": "5 EXPERIMENTAL RESULTS", "content": "Table 1 presents a comparison of the forecast errors of proposed models(EIKF-Net and w/Unc-EIKF-Net) with those of several baseline models on five different datasets (PeMSD3, PeMSD4, PeMSD7, PeMSD7M, and PeMSD8). The forecast errors are evaluated for a 12(7)-step-prior to 12(v)-step-ahead forecasting task which is a popular and well-established benchmark in the MTSF task. The performance of the proposed models was evaluated using multiple metrics such as mean absolute error(MAE), root mean squared error(RMSE), and mean absolute percentage error(MAPE). Using multiple evaluation metrics in multi-horizon prediction tasks provides a comprehensive eval-uation of the proposed models performance with the baselines. The results for the baseline models were reported in a previous study by Choi et al. (2022). The proposed models(EIKF-Net, w/Unc-EIKF-Net) consistently demonstrate state-of-the-art performance compared to baseline models on various benchmark datasets based on multiple evaluation metrics. The results show that the pro-posed models demonstrate the best performance with lower forecast error on benchmark datasets. Specifically, they report a 12.2%, 14.8%, 8.8%, 10.6% and 8.9% significant drop in the RMSE metric compared to the next-best baseline models on PeMSD3, PeMSD4, PeMSD7, PeMSD8, and PeMSD7(M) datasets, respectively. In addition to the pointwise forecasts, the w/Unc- EIKF-Net model(i.e., EIKF-Net with local uncertainty estimation) provides time-varying uncertainty esti-mates. Its performance is slightly worse than the EIKF-Net model but still outperforms several strong baselines in the literature, as reflected in the lower prediction error. In brief, the empirical results show the efficacy of the proposed neural forecasting architecture in modeling the complex and nonlinear spatio-temporal dynamics underlying the MTS data to provide better forecasts. The appendix provides more detailed information on the experimental setup, ablation studies, and other additional experimental results on multi-horizon forecasting. Moreover, the appendix discusses the experimental results that support the EIKF-Net framework's ability to handle missing data and pro-vide more insights into the w/Unc- EIKF-Net framework for estimating uncertainty. Furthermore, the appendix also includes time series visualizations of model predictions with the uncertainty esti-mates compared to the ground truth."}, {"title": "6 CONCLUSION", "content": "We propose a framework that combines implicit and explicit knowledge for learning the dynamics of MTS data to provide accurate multi-horizon forecasts. The experimental results on real-world"}, {"title": "HYPERGRAPH ATTENTION NETWORK(HGAT)", "content": "The HgAT generalizes the local- and global-attention-based convolution operation(Veli\u010dkovi\u0107 et al. (2017); Brody et al. (2021)) on the spatio-temporal hypergraphs. The hypergraph encoder(HgAT) performs inference on the hypergraph-structured MTS data characterized by the incidence matrix, $I\\in\\mathbb{R}^{n \\times m}$, and feature matrix, $X_{(t-\\tau: t-1)}\\in\\mathbb{R}^{n \\times d}$, to compute the transformed feature matrix $H'\\in\\mathbb{R}^{n \\times d}$. Each row in H\u2019 represents the hypernode representations $\\mathbf{h}_i\\in\\mathbb{R}^{d}, \\forall i \\in HV$ at time step t, where HV is the set of hypernodes. The HgAT operator captures the interdependencies and relations among the time-series variables by encoding both the structural and feature characteristics of the spatio-temporal hypergraphs in the hypernode representations $\\mathbf{h}_i$. The HgAT operator is designed to adapt to capture the changes in the dependencies of the time-series variables over time in the hypern-ode representations $\\mathbf{h}_i$. Let $N_{j,i}=\\{i\\mid I_{i,j}=1\\}$ represent the subset of hypernodes i incident with any hyperedge j. The intra-edge neighborhood of the incident hypernode i is given by $N_{j,i}\\\\i$. It is a localized group of semantically-correlated time series variables and captures higher-order re-lationships. The inter-edge neighborhood of hypernode i, $N_{i,j}=\\{j\\mid I_{i,j}=1\\}$, spans the spectrum of the set of hyperedges j incident with hypernode i. The HgAT operator leverages the relational inductive bias encoded by the hypergraph's connectivity to perform intra-edge and inter-edge neigh-borhood aggregation schemes, which allows it to explicitly model the fine-grained spatial-temporal correlations between the time series variables. The intra-edge neighborhood aggregation focuses on the interrelations between a specific hypernode and its immediate neighboring hypernodes incident with a specific hyperedge, while inter-edge neighborhood aggregation takes into account the inter-relations between a specific hypernode and all other hyperedges incident with it. We peform the attention-based intra-edge neighborhood aggregation for learning the latent hyperedge representa-tions as follows,\n$\\mathbf{h}^{(t,l)} = \\sum_{z=1}^{Z} (\\sum_{i \\in N_{j,i}} \\alpha_{j,i}^{(t,l,z)}) W^{(2)} \\mathbf{h}_{i}^{(t,l-1,z)}), l=1...L_{HgAT}$"}, {"title": "HYPERGRAPH TRANSFORMER(HGT)", "content": "The proposed HgT operator is an extension of transformer networks(Vaswani et al. (2017)) to handle arbitrary sparse hypergraph structures with full attention as a desired structural inductive bias. The HgT operator allows the framework to attend to all hypernodes and hyperedges in the hypergraph, which incentivizes learning the fine-grained interrelations unconstrained by domain-specific hierar-chical structural information underlying the MTS data. This can facilitate the learning of optimal hypergraph representations by allowing the model to span large receptive fields for global reasoning of the hypergraph-structured data. The HgT operator does not rely on structural priors, unlike ex-isting methods such as the method of stacking multiple HgNN layers with residual connections(Fey (2019), Xu et al. (2018)), virtual hypernode mechanisms(Gilmer et al. (2017); Ishiguro et al. (2019); Pham et al. (2017)), or hierarchical pooling schemes(Ramp\u00e1\u0161ek & Wolf (2021), Gao & Ji (2019), and Lee et al. (2019)) to model the long-range correlations in the hypergraph-structured data. The permutation-invariant HgT module, by exploiting global contextual information can model the pair-wise relations between all hypernodes in the hypergraph-structured data. As a result, the HgT mod-ule acts as a drop-in replacement to existing methods for modeling the hierarchical dependencies and relationships among time series variables in the spatio-temporal hypergraphs leading to more robust and generalizable representations on a wide range of downstream tasks. The transformer encoder(Vaswani et al. (2017)) consists of alternating layers of multiheaded self-attention (MSA) and multi-layer perceptron(MLP) blocks to capture both the local and global contextual informa-tion. To improve the performance and regularize the model, we apply Layer normalization(LN(Ba"}, {"title": "A.3.1 SPATIAL-TEMPORAL GRAPH REPRESENTATION LEARNING", "content": "The Temporal Graph Convolutional Network(T-GCN, Zhao et al. (2019)) operates on a sequence of dynamic graphs, where graph structure is fixed, and node attributes change over time. Each graph represents the graph-structured MTS data at a specific time step. The T-GCN operator utilizes Gated Recurrent Units(GRU, Cho et al. (2014b)) to model the spatio-temporal dynamics of the in-put dynamic graph sequence. In a traditional GRU, the update gate, reset gate, and hidden state are computed using matrix multiplication with weight matrices. However in T-GCN, these matrix mul-tiplications are replaced with Graph Convolutional Networks(GCN, Kipf & Welling (2016)). The T-GCN operator analyzes graph-structured MTS data over time. It propagates information between nodes across different time steps, which enables the model to capture the complex spatio-temporal dependencies between the graphs. The T-GCN operator utilizes the predefined graph to propagate information between nodes by averaging the node representations in their local neighborhood at each time step computed as follows,\n$U_t=\\sigma (W_u [f (A^{(0)}, X_{(t-\\tau: t-1)}), H'''_{t-1}] + B_u)$\n$R_t=\\sigma (W_r [f (A^{(0)}, X_{(t-\\tau: t-1)}), H'''_{t-1}] + B_r)$\n$C_t=tanh (W_c [f (A^{(0)}, X_{(t-\\tau: t-1)}), (R_t * H'''_{t-1})] + B_c)$\n$H'''_t=U_t * H'''_{t-1} + (1 - U_t) * C_t$\nwhere $f (A^{(0)}, X_{(t-\\tau: t-1)})$ denote the GCN operator. $U_t, R_t$ denote the update gate and reset gate at time t. $W_r, W_u$, and $W_c$ are learnable weight matrices and $B_u, B_r$, and $B_c$ are learnable biases. In summary, the node representation matrix, $H'''$ captures the spatio-temporal dynamics at different scales underlying the discrete-time dynamic graphs, where each row in H\u201d represents the hypernode representations $h'_t\\in\\mathbb{R}^{d}, \\forall i \\in V$. Some of the key advantages of T-GCN operator over traditional methods include its ability to handle large and sparse spatio-temporal graphs."}, {"title": "A.4 ABLATION STUDY", "content": "We conduct a comprehensive ablation study to determine the impact of each component of the EIKF-Net framework by removing or altering them and observing the effect on the overall perfor-mance. We evaluate the individual contributions of components on the overall model performance. This study provides insights and helps identify components critical for the framework to achieve bet-ter performance. The baseline for our ablation study is the EIKF-Net framework, which integrates the spatial and temporal learning components to capture the fine-grained inter and intra-time-series correlations for modeling the nonlinear dynamics of complex interconnected sensor networks. The spatial learning component comprises two main modules: the explicit graph and the implicit hy-pergraph learning modules. The former operates on the predefined graph topology to capture the pair-wise semantic relationships between the variables in the graph-structured data to model the dynamics of the interconnected networks. The latter operates on the implicit hypergraph topology to capture time-evolving high-order spatial correlations in the hypergraph-structured data to model the observations of the dynamical interacting networks. We systematically exclude the components under evaluation to derive a set of model variants for verifying the importance of different compo-nents. We investigate the ablated variant model's performance compared with the baseline model to disentangle the relative gains of each learning component. The ablation study will shed light on the relationship between the variant and the baseline model and their generalization performance for MTSF, advancing the importance of underlying mechanisms. We provide the details about each ablated variant as follows."}, {"title": "A.5 ADDITIONAL STUDY ON THE SIGNIFICANCE OF HYPERGRAPH INFERENCE APPROACH.", "content": "In this section, we study the impact of jointly learning the implicit hypergraph structure & hyper-graph representations from MTS data on the forecast accuracy compared to jointly learning the implicit graph structure and its corresponding graph representations. In the case of implicit graph structure learning, the goal is to capture hidden relationships and dependencies among the differ-ent variables in the MTS data represented in a graph structure that might not have been apparent with domain expertise knowledge. STGNNs perform message-passing schemes on the inferred im-plicit graph topology to learn the more expressive graph representations capturing the patterns and relationships underlying the MTS data for better forecasts. In the recent past, there has been an increased focus on developing methods for joint learning of discrete graph structure and represen-tations for forecasting on MTS data. The existing methods include GTS(Graph for Time Series, Shang et al. (2021)), Graph Deviation Network(GDN, Deng & Hooi (2021)), and MTS forecast-ing with GNNs(MTGNN, Wu et al. (2020)). We design a set of variant models by substituting the hypergraph inference and representation learning with the corresponding implicit graph inference and representation learning methods mentioned earlier. We conduct ablation experiments on the PeMSD4 and PeMSD8 benchmark datasets to evaluate the effectiveness of each design choice. We set the forecast horizon as 12. This study can help to understand the specific design choice's effec-tiveness, which can contribute to improved overall framework performance on long-term prediction tasks. The ablated variant, \"w/ GTS: EIKF-Net\" refers to a variant of the EIKF-Net framework with the GTS module, but does not include the implicit hypergraph learning module. Similarly, \u201cw/ GDN: EIKF-Net\" and \"w/ MTGNN: EIKF-Net\" are also ablated variants."}, {"title": "A.6 ADDITIONAL STUDY ON THE IMPACT OF HYPERGRAPH LEARNING PARADIGMS.", "content": "Our framework performs joint learning of hypergraph structure and representations through two learning units: hypergraph structure learning (i.e., through embedding-based similarity metric learn-ing) and hypergraph representation learning. The objective is to learn the optimal hypergraph structures and representations for the downstream forecasting task. There exist different learning paradigms for optimizing the two separate learning units, that includes joint learning(JL), adaptive learning(AL, Wu et al. (2022); Pilco & Rivera (2019)), and iterative learning(IL, Chen et al. (2019)) of hypergraph structures and representations, respectively. We conduct ablation experiments to in-vestigate the impact of learning paradigms on the overall performance of the framework. The ablated variants, \"w/ AL: EIKF-Net\" and \"w/ IL: EIKF-Net\" refers to variants of the EIKF-Net framework that utilizes the Adaptive Learning(AL) and Iterative Learning(IL) module respectively instead of the Joint Learning(JL) paradigm. Table 5 reports the experimental results. However, the results indi-cate the strong performance of JL paradigm and highlights its advantage compared to other learning paradigms."}, {"title": "A.7 POINTWISE PREDICTION ERROR FOR MULTI-HORIZON FORECASTING", "content": "Figure 3 shows the proposed neural forecasting framework(EIKF-Net) multistep-ahead forecasts error at each horizon on the benchmark datasets. Lower RMSE, MAPE, and MAE indicate better model performance on MTSF task. Across all the prediction horizons, our neural forecasting archi-tecture has reduced forecast errors more than the baselines. Our proposed neural forecast framework"}, {"title": "A.8 IRREGULAR TIME SERIES FORECASTING", "content": "Large, complex interconnected sensor networks span a diverse range of real-world applications and suffer from inherent drawbacks of low-data quality due to inevitable and pervasive intermittent sen-sor failure, faulty sensors, etc., during the data acquisition process. We simulate the data availabil-ity/missingness with two different types of missingness patterns(Roth & Liebig (2022); Cini et al. (2021)) to evaluate the EIKF-Net framework effectiveness in handling the missing data. The sim-ulation methods mimic the missingness patterns occurring in continuous time with asynchronous temporal patterns commonly observed in real-world data of large, complex sensor networks. Firstly, the point-missing pattern simulates sensor failure by randomly dropping observations of each vari-able within a given historical window, with missing ratios ranging from 10% to 50%. Secondly, the block-missing pattern simulates sensor failure by randomly masking out available data for each sensor within a given historical window, with missing ratios ranging from 10% to 50%. In addition, we simulate a sensor failure with probability $P_{failure}=0.15%$ for generating blocks of missing MTS data. Our tailored deep learning-based framework consists of spatial and temporal inference com-ponents to process and analyze MTS data characterized by complex dependencies and relationships between variables that change over time. We conduct further research to investigate the importance of different components of the EIKF-Net framework on multi-horizon forecasting accuracy when dealing with irregular and missing data. We conduct an additional ablation study on the benchmark model(EIKF-Net) by removing either the spatial or temporal learning components to evaluate their individual contribution to the forecasting accuracy in missing data scenarios. These studies demon-strate the robustness and reliability of the EIKF-Net framework in real-world applications where missing data are ubiquitous."}, {"title": "A.9 SENSITIVITY ANALYSIS", "content": "We conduct a hyperparameter study to find the impact of hyperparameters on the performance of proposed framework. The goal is to find the set of hyperparameter values that lead to the optimal performance of the framework on the benchmark datasets, respectively. The algorithm hyperparameters are the embedding size(d), batch size(b), learning rate(lr), and the number of hyperedges(|HE|). We have tuned the following hyperparameters over specific ranges of values: em-bedding dimension(d) \u2208 {2, 6, 10, 18, 24}, the number of hyperedges(|HE|) \u2208{2, 5, 8}, batch size(b) \u2208 {2, 6, 10, 18, 24, 32, 64}, and the learning rate(lr) \u2208{1 \u00d7 10\u22121,1 \u00d7 10-2,1 \u00d710\u22123,1 \u00d7 10-4}. We have chosen the hyperparameter ranges to avoid Out-Of-Memory(OOM) errors on GPUs by limiting the model size and the memory requirements. We utilize the grid-search technique for hyperparam-eter optimization. Across all the datasets, we find the set of hyperparameter values that results in the lowest model forecast error on the validation set on a chosen evaluation metric as the optimal set of values. We utilize MAE and RMSE to evaluate the framework performance, with lower values indi-cating better performance. Figure 4 shows how the EIKF-Net framework performance changes as the embedding size(d) and the number of hyperedges(|HE|) vary in a predefined range across all the datasets. It helps to understand the effect of these hyperparameters on the framework performance. We discuss the optimal set of hyperparameter configurations for each dataset as described below,\n\u2022 For PeMSD3, we set the batch size(b) to 18, the initial learning rate(lr) to 1 \u00d7 10-3, and the embedding size(d) to 18. The number of hyperedges(|HE|) is 5.\n\u2022 For PeMSD4, we set the batch size(b) to 48, the initial learning rate(lr) to 1 \u00d7 10-3, and the embedding size(d) to 18. The number of hyperedges(|HE|) is 5.\n\u2022 For PeMSD7, we set the batch size(b) to 12, the initial learning rate(lr) to 1 \u00d7 10-3, and the embedding size(d) to 18. The number of hyperedges(HE) is 5."}, {"title": "A.10 TIME SERIES FORECASTING VISUALIZATION", "content": "We visualize the ground truth, pointwise forecasts, and time-varying uncertainty estimates of our framework predictions shown in Figures 5 and 6. However, existing methods for MTSF provide pointwise forecasts by modeling the nonlinear spatial-temporal dependencies existing within net-works of interconnected sensors and fall short in measuring uncertainty."}, {"title": "A.11 FORECASTING UNCERTAINTY", "content": "The EIKF-Net framework is a supervised learning algorithm that utilizes the mean absolute er-ror(MAE) as the loss function to train the model. We compute the MAE error between the model pointwise forecasts(represented by $\\hat{X}_{(t:t+\\nu-1)})$ and ground-truth data (represented by $X_{(t:t+\\nu-1)})$ as described below,\n$L_{MAE} (\\theta)=\\frac{1}{\\nu} \\sum_{i=t}^{t+\\nu-1} |\\hat{X}_i-X_i|$\nThe training process aims to minimize the MAE loss function, represented by $L_{MAE} (\\theta)$, by adjusting the model parameters \u03b8. The w/Unc- EIKF-Net is a variation of the EIKF-Net designed to estimate the uncertainty of the model predictions and enables more reliable decision-making. The w/Unc-EIKF-Net framework is a method for predicting the time-varying uncertainty in multistep-ahead forecasts. The predicted forecasts($\\hat{X}_{(t:t+\\nu-1)}$) are modeled as a heteroscedastic Gaussian distribution with mean and variance given by $\\mu_{\\phi} (X_{(t-\\tau: t-1)})$ and $\\sigma^2 (X_{(t-\\tau: t-1)})$, respectively, where $X_{(t-\\tau: t-1)}$ is the input time series. It is described as follows,\n$\\hat{X}_{(t:t+\\nu-1)} \\sim \\mathcal{N} (\\mu_{\\phi}(X_{(t-\\tau: t-1)}), \\sigma^2 (X_{(t-\\tau: t-1)}))$\nThe predicted mean and standard deviation are obtained as follows,\n$\\mu_{\\phi}(X_{(t-\\tau: t-1)}), \\sigma(X_{(t-\\tau: t-1)})=f_{\\theta}(H''')$\nThe neural network(fo) takes in the output of the temporal inference component, repre-sented by H\"', as input. The network then predicts the mean($\\mu_{\\phi}(X_{(t-\\tau: t-1)})$) and standard deviation($\\sigma(X_{(t-\\tau: t-1)})$) of a normal distribution for future observations($\\hat{X}_{(t:t+\\nu-1)}$). The maxi-mum likelihood estimate(MLE) of the predicted Gaussian distribution denoted as $\\hat{X}_{(t:t+\\nu-1)}$. It is obtained as follows,\n$\\hat{X}_{(t:t+\\nu-1)}=\\mu_{\\phi}(X_{(t-\\tau: t-1)})$\""}, {"title": "A.12 BASELINES", "content": "We briefly discuss the well-known and well-understood algorithms which provide a benchmark for evaluating the proposed neural forecasting models(w/Unc- EIKF-Net, w/Unc- EIKF-Net) perfor-mance on the MTSF task.\n\u2022 HA Hamilton (2020) uses the average of the predefined historical window based observa-tions to predict the next value.\n\u2022 ARIMA is a statistical analysis model for handling non-stationary time series data.\n\u2022 VAR(Hamilton ("}]}