{"title": "MULTI-KNOWLEDGE FUSION NETWORK FOR TIME SERIES REPRESENTATION LEARNING", "authors": ["Sagar Srinivas Sakhinana", "Shivam Gupta", "Krishna Sai Sudhir Aripirala", "Venkataramana Runkana"], "abstract": "Forecasting the behaviour of complex dynamical systems such as interconnected sensor networks characterized by high-dimensional multivariate time series(MTS) is of paramount importance for making informed decisions and planning for the future in a broad spectrum of applications. Graph forecasting networks(GFNs) are well-suited for forecasting MTS data that exhibit spatio-temporal dependencies. However, most prior works of GFN-based methods on MTS forecasting rely on domain-expertise to model the nonlinear dynamics of the system, but neglect the potential to leverage the inherent relational-structural dependencies among time series variables underlying MTS data. On the other hand, contemporary works attempt to infer the relational structure of the complex dependencies between the variables and simultaneously learn the nonlinear dynamics of the interconnected system but neglect the possibility of incorporating domain-specific prior knowledge to improve forecast accuracy. To this end, we propose a hybrid architecture that combines explicit prior knowledge with implicit knowledge of the relational structure within the MTS data. It jointly learns intra-series temporal dependencies and inter-series spatial dependencies by encoding time-conditioned structural spatio-temporal inductive biases to provide more accurate and reliable forecasts. It also models the time-varying uncertainty of the multi-horizon forecasts to support decision-making by providing estimates of prediction uncertainty. The proposed architecture has shown promising results on multiple benchmark datasets and outperforms state-of-the-art forecasting methods by a significant margin. We report and discuss the ablation studies to validate our forecasting architecture.", "sections": [{"title": "1 INTRODUCTION", "content": "Accurate multivariate time series forecasting(MTSF) is critical for a broad spectrum of domains that have significant financial or operational impacts, including retail and finance, intelligent transportation systems, logistics and supply chain management, and many others. However, MTSF can be challenging due to the complexity of the relationships between time series variables and the unique characteristics of the MTS data, such as non-linearity, heterogeneity, sparsity, and non-stationarity. In this context, Spatial-temporal graph neural networks(STGNNs) have been widely studied for modeling the long-range intra-temporal dependencies and complex inter-dependencies among the variables in the MTS data for improved multi-horizon forecast accuracy. The explicit relationships among variables are based on prior knowledge provided by human experts in the form of a pre-defined or explicit graph, while implicit relationships among variables within the MTS data are obtained through neural relational inference methods(Deng & Hooi (2021); Kipf et al. (2018)). The implicit relationships are highly-complex and non-linear, can change over time, and uncover hidden relationships unknown to human experts which are not obvious. The existing \"human-in-the-loop\" STGNNS(Yu et al. (2017), Li et al. (2017), Guo et al. (2020)) incorporate domain-specific knowledge of the relational-structural dependencies among the interdependent variables while simultaneously learning the dynamics from the MTS data. However, arguably, the explicit graph structures in most real-world scenarios are either unknown, inaccurate, or partially available, thus resulting in suboptimal forecasting. Even if available, the explicit graph structure represents a simplified view of dependencies and often fails to capture the non-static spatial-temporal dependencies within the MTS data. Precisely it falls short of accurately inferring the latent time-conditioned underlying relations that drive the co-movements among variables in the substantial MTS data. On the contrary, a recent class of STGNNs(Shang et al. (2021); Deng & Hooi (2021); Wu et al. (2020); Kipf et al. (2018)) jointly infer the discrete dependency graph structure describing the implicit relations between variables while simultaneously learning the dynamics in MTS data. Despite the success, these approaches neglect to exploit the predefined graph of the inter-relationships among variables obtained from the domain-expertise knowledge resulting in suboptimal performance on the graph time-series forecasting. In addition, implicit graph structure learning from MTS data suffers from inherent limitations of pairwise associations. While in contrast, the relations within the complex dynamical systems of interconnected networks could go beyond pairwise connections. Hypergraph, a generalization of a graph, offers a natural fit for modeling the higher-order structural relations underlying the interconnected networks in complex high-dimensional data. Moreover, the standard STGNNs focus on learning pointwise forecasts but do not provide uncertainty estimates of forecasts. To overcome the challenges, we propose an explicit-implicit knowledge fusion neural network(EIKF-Net) framework with a joint learning paradigm on the explicit-implicit interaction structure for a thorough understanding of the underlying dependencies between time series variables, while simultaneously learning the complex dynamics of the MTS data for better forecast accuracy and to provide reliable uncertainty estimates of forecasts. The proposed framework consists of two main components: spatial and temporal learning components. We adopt a space-then-time(STT, Gao & Ribeiro (2022)) approach, where spatial message-passing schemes are performed prior to the temporal-encoding step. The spatial learning component is further composed of an implicit hypergraph and explicit graph learning modules. The former infers the implicit hypergraph structure, which captures the hierarchical interdependencies among variables in MTS data. Simultaneously it performs hypergraph representation learning schemes to encode the spatio-temporal dynamics underlying the hypergraph-structured MTS data into the latent hypernode-level representations. The latter performs the graph representation learning schemes to encode the pair-wise spatial relations between the multiple co-evolving variables to capture the spatio-temporal dynamics within the graph-structured MTS data into the latent node-level representations. We perform convex combination(i.e., \u201cmix up\") of the latent graph and hypergraph representations through a gating mechanism. It leads to more accurate latent representations of the complex non-linear dynamics of the MTS data. The mixup representations allow the framework to capture different types of dependencies that exist at different observation scales(i.e., correlations among variables could potentially differ in the short and long-term views in the MTS data). The temporal learning component focusses on learning the time-evolving dynamics of interdependencies among the variables present in the MTS data to provide accurate multi-horizon forecasts with predictive uncertainty estimates. To summarize, our work presents an end-to-end methodological framework to infer the implicit interaction structure from MTS data. It simultaneously learns the spatio-temporal dynamics within the explicit graph and implicit hypergraph structured MTS data using graph and hypergraph neural networks, respectively, to capture the evolutionary and multi-scale interactions among the variables in the latent representations. It performs inference over these latent representations for downstream MTSF task and models the time-varying uncertainty of the forecasts in order to provide more accurate risk assessment and better decision making by estimating predictive uncertainty. The framework is designed to offer better generalization and scalability for large-scale spatio-temporal MTS data-based forecasting tasks as those found in real-world applications."}, {"title": "2 PROBLEM DEFINITION", "content": "Lets us assume a historical time series data, with n-correlated variables, observed over T training steps is represented by $X=(x_1,...,x_T)$, where the subscript refers to time step. The observations of the n-variables at time point t are denoted by $x_t=(x_t^{(1)}, x_t^{(2)},...,x_t^{(n)}) \\in \\mathbb{R}^{(n)}$, where the superscript refers to variables. Under the rolling-window method for multi-step forecasting, where at the current time step t, we predefine a fixed-length look-back window to include the prior $\\tau$-steps of historical MTS data to predict for the next $v$-steps. In the context of MTSF, the learning problem can be formalized using the rolling window method. The goal is to use a historical window of n-correlated variables, represented by the $X_{(t-\\tau: t-1)}\\in \\mathbb{R}^{n \\times \\tau}$, which have been observed over previous $\\tau$-steps prior to current time step t, to predict about the future values of n variables for the next $v$-steps denoted as $X_{(t:t+v-1)}\\in \\mathbb{R}^{n \\times v}$. The MTSF problem is further formulated on the graph and hypergraph structure to capture the spatial-temporal correlations among multitudinous correlated time series variables. We represent the historical inputs as continuous-time spatial-temporal graphs, denoted as $G_t=(V,E,X_{(t-\\tau: t-1)}, A^{(0)})$. $G_t$ is composed of a set of nodes(V), edges(E) that describe the connections among the variables and node feature matrix $X_{(t-\\tau: t-1)}$ that changes over time, where t is the current time step. The adjacency matrix, $A^{(0)}\\in{0,1}^{\\|V\\| \\times \\|V\\|}$, describes the explicit fixed-graph"}, {"title": "3 OUR APPROACH", "content": "The overall neural forecasting architecture of our framework is illustrated in Figure 1. It consists of three main components: The projection layer, spatial learning, and temporal learning components. The spatial learning component includes two modules: graph and hypergraph learning modules. The hypergraph learning module infers the discrete dependency hypergraph structure to capture the interrelations between time-series variables. It also performs higher-order message-passing schemes to learn the time-conditioned optimal hypernode-level representations by modeling the hypergraph-structured MTS data. The graph learning module utilizes the predefined graph, which represents the relational structure of the variables obtained from domain-expertise knowledge to obtain the graph-structured MTS data. It performs spatial graph-filtering through neighborhood aggregation schemes to compute the optimal node-level representations that better capture the underlying dynamics within the MTS data. The temporal inference component performs a convex combination of the latent explicit-graph and implicit-hypergraph representations and learns the time-evolving inter-dependencies to provide pointwise forecasts and uncertainty estimations. Overall, joint optimization of different learning components of the proposed framework effectively captures the complex relationships between time-series variables and makes accurate forecasts."}, {"title": "3.1 PROJECTION LAYER", "content": "The projection layer utilizes a gated linear networks(GLN, Dauphin et al. (2017)) to learn the non-linear representations of the input data, $X_{(t-\\tau: t-1)}\\in \\mathbb{R}^{n \\times \\tau}$ through a gating mechanism to compute a transformed feature matrix, $\\tilde{X}_{(t-\\tau: t-1)} \\in \\mathbb{R}^{n \\times d}$ as follows,\n$\\tilde{X}_{(t-\\tau: t-1)} = (\\sigma(W_0X_{(t-\\tau: t-1)}) \\& W_1X_{(t-\\tau: t-1)})W_2$\nwhere $W_0, W_1, W_2 \\in \\mathbb{R}^{\\tau \\times d}$ are trainable weight matrices, $\\& $ denotes the element-wise multiplication.\n$\\sigma$ is the non-linear activation function."}, {"title": "3.2 SPATIAL-INFERENCE", "content": "The spatial inference component of our framework is illustrated in Figure 2. The spatial-learning component encodes non-linear input data, $\\tilde{X}_{(t-\\tau: t-1)}$ to obtain graph and hypergraph representations using two modules: the hypergraph learning module and the graph learning module. The hypergraph learning module performs joint hypergraph inference and representation learning, while the graph learning module performs graph representation learning. The outputs of these two modules are fused using a convex combination approach to regulate the flow of information encoded by each module. The details of each module are discussed in subsequent sections."}, {"title": "3.2.1 HYPERGRAPH INFERENCE AND REPRESENTATION LEARNING", "content": "The hypergraph learning module is composed of two units: the hypergraph inference(HgI) unit and the hypergraph representation learning(HgRL) unit. The HgI unit is a structural modeling approach that aims to infer the discrete hypergraph topology capturing the hierarchical interdependence relations among time-series variables for a hypergraph-structured representation of the MTS data. The HgI unit utilizes a similarity metric learning method to implicitly learn the task-relevant relational hypergraph structure from the hypergraph embeddings. The hypernodes and hyperedges of the hypergraph are represented by differentiable embeddings $z_i$ and $z_j$, respectively, where $1<i<n$ and $1<j<m$. The embeddings $z_i$ and $z_j$ capture the global-contextual behavioral patterns of hypernodes and hyperedges, respectively. These embeddings $z_i, z_j \\in \\mathbb{R}^d$ are continuous vector representations in the d-dimensional vector space, which allows the HgI unit to adapt and update as it processes new information. We compute the pairwise similarity of any pair $z_i$ and $z_j$ as follows,\n$P_{i,j} = \\sigma([S_{i,j} \\| \\| 1 - S_{i,j}])$ ; $S_{i,j} = \\frac{z_i^Tz_j}{\\|z_i\\| \\|z_j\\| + \\epsilon}$ (4)\nwhere $\\| \\cdot \\|$ denotes vector concatenation. We apply the sigmoid activation function to map the pairwise scores, $P_{i,j}$ into the range [0, 1]. $P \\in \\mathbb{R}^{nm\\times 2}$ denote the hyperedge probability over hypernodes of the hypergraph, where $k\\in{0,1}$. $P_j^{(k)}$ encodes the relation between a pair of hypernodes and hyperedges (i, j) to a scalar $\\in [0,1]$. $P_j^{(0)}$ represents the probability of a hypernode i connected to the hyperedge j, and $P_j^{(1)}$ represents contrariwise probability. We utilize the Gumbel-softmax trick(Jang et al. (2016); Maddison et al. (2016)) to sample a discrete hypergraph structure described by an incidence matrix, $I \\in \\mathbb{R}^{nxm}$. This allows for a differentiable way to sample the latent structure from the hyperedge probability distribution $P_{i,j}$. Thus by utilizing the Gumbel-softmax trick, the hypergraph structure can be learned in an end-to-end differentiable way, allowing for the use of gradient-based optimization methods to train the model using an inductive-learning approach. It is described as,\n$I_{i,j}=\\frac{exp ((g_{i,j}^{(1)}+P_{i,j}^{(1)}))/\\gamma)}{\\sum_{k \\in{0,1}}{exp((g_{i,j}^{(k)}+P_{i,j}^{(k)}))/\\gamma}}$ (5)\nwhere $g_{i,j}^{(k)}$~Gumbel(0, 1) = -log(-log(U(0, 1)) where U is uniform distribution. $\\gamma$, denotes the temperature parameter with 0.05. We regularize the learned hypergraph to be sparse by optimizing the probabilistic hypergraph distribution parameters to drop the redundant hyperedges over hypernodes. The downstream forecasting task acts as the indirect supervisory information for revealing the high-order structure,i.e., the hypergraph relation structure behind the observed data. The MTS data is represented as hypernode-attributed spatio-temporal hypergraphs. A hypergraph representation learning unit(HgRL) is used to compute optimal hypernode-level representations by capturing the spatio-temporal dynamics within the hypergraph-structured MTS data. These representations are then used to perform inference on the downstream multi-horizon forecasting task. In short, HgRL is a neural network architecture that utilizes both Hypergraph Attention Network(HgAT) and Hypergraph Transformer(HgT) as its backbone. The HgT uses multi-head self-attention mechanisms to learn the latent hypergraph representations, $h_\\Psi^{(t)}$, at time t without leveraging any prior knowledge about the structure of the hypergraph. The HgAT performs higher-order message-passing schemes on the hypergraph to aggregate information and compute the latent hypernode representations, $h_{\\Upsilon}^{(t)}$, at time t. The HgAT and HgT form a powerful backbone for HgRL, allowing it to effectively learn hypergraph representations by capturing complex relationships and dependencies within the hypergraph-structured MTS data. We provide the implementation details and a more in-depth explanation in the appendix for further information. We regulate the information flow from $h_\\Psi^{(t)}$ and $h_{\\Upsilon}^{(t)}$ by applying a gating mechanism to produce a weighted combination of representations as described below,\n$h'(t) = \\sigma(g'(h_{\\Upsilon}^{(t)})) + (1 - g') h_\\Psi^{(t)} ; g'=\\sigma(f'(h_{\\Upsilon}^{(t)}) + f'(h_{\\Psi}^{(t)}))$ (6)\nwhere $f'$ and $f_\\Theta$ are linear projections. Fusing representations can be beneficial for modeling the multi-scale interactions underlying the spatio-temporal hypergraph data and also help to mitigate overfitting. The spatio-temporal data often contains correlations between variables that change over time or at different observation scales. By fusing representations, the proposed framework incorporates the most relevant information to capture the time-evolving underlying patterns in the MTS data, which leads to more accurate and robust forecasts. In brief, the hypergraph learning module optimizes the discrete hypergraph structure through the similarity metric learning technique. It formulates the posterior forecasting task as message-passing schemes with hypergraph neural networks to learn the optimal hypergraph representations, which leads to more accurate and expressive representations of the MTS data for better forecast accuracy."}, {"title": "3.2.2 GRAPH REPRESENTATION LEARNING(GRL)", "content": "We represent the MTS data as continuous-time spatio-temporal graphs based on predefined graphs obtained from domain-specific knowledge. We utilize the Temporal Graph Convolution Neural Network(T-GCN, Zhao et al. (2019)) to compute optimal node-level representations by modeling the graph topology dependencies and feature attributes within the graph-structured MTS data. These graph representations are further processed by the downstream temporal inference component for learning the non-linear temporal dynamics of inter-series correlations among the variables. In short, the T-GCN performs neighborhood aggregation schemes on predefined graph topology to compute the optimal node-level representations, $\\tilde{h}_i^{(t)}$, at a specific time t. It effectively captures fine-grained, data-source specific patterns accurately. We discuss in the appendix a more detailed description of the technique."}, {"title": "3.3 TEMPORAL-INFERENCE", "content": "The mixture-of-experts(MOE) mechanism combines the predictions of multiple subnetworks (experts) to produce a final prediction. In this specific framework, the experts are graph and hypergraph learning modules. The expert predictions are combined through a gating mechanism in an input-dependent manner by calculating a weighted sum of their predictions. The goal of training in this framework is to achieve two objectives: 1) Identifying the optimal distribution of weights for the gating function that precisely captures the underlying distribution within the MTS data, and 2) Training the experts using the distribution weights specified by the gating function. The fused representations are obtained by combining the predictions of the experts using the calculated weights as follows,\n$\\hat{h}^{(t)} = \\sigma(g\"(h'(t))) + (1 - g\") \\hat{h}'(t); g\"= \\sigma(f\"(h'(t)) + f\"(h'(t)))$ (7)\nwhere h'(t), $\\hat{h}(t)$ are computed by the graph and hypergraph learning modules, respectively. $f\"$ and $f^\\prime\\prime$ are linear projections. The temporal inference component consists of a stack of 1 \u00d7 1 convolutions. The fused representations are fed as input to the temporal inference component. This component aims to model the non-linear temporal dynamics of inter-series dependencies among variables underlying the spatial-temporal MTS data and predicts the pointwise forecasts, $X_{(t:t+v-1)}$. Our proposed framework uses the spatial-then-time modeling approach to learn the higher-order structure representation and dynamics in MTS data. This approach first encodes the spatial information of the relational structure, including both explicit graph and implicit hypergraph, which captures the complex spatial dependencies. By incorporating the temporal learning component, the framework then analyzes the evolution of these dependencies over time, which helps to improve the interpretability and generalization of the framework. This approach is beneficial for dealing with real-world applications that involve entangled complex spatial-temporal dependencies within the MTS data, which can be challenging to model using traditional methods. Additionally, by minimizing the negative Gaussian log likelihood, the uncertainty estimates of forecasts can be provided through the w/Unc-EIKF-Net framework(i.e., EIKF-Net with Local Uncertainty Estimation). Minimizing the negative Gaussian log likelihood is equivalent to maximizing the likelihood of the model's predictions given the true values. This allows the framework to provide more accurate and reliable uncertainty estimates of forecasts. In summary, our proposed methods(EIKF-Net, w/Unc- EIKF-Net) allow for simultaneously modeling the latent interdependencies and then analyze the evolution of these dependencies over time in the sensor network based dynamical systems in an end-to-end manner."}, {"title": "4 DATASETS", "content": "We conduct experiments to verify the performance of proposed models(EIKF-Net, w/Unc- EIKF-Net) on large-scale spatial-temporal datasets. The real-world traffic datasets(PeMSD3, PeMSD4, PeMSD7, PeMSD7(M), and PeMSD8) were collected by the Caltrans Performance Measurement System(PeMS, Chen et al. (2001)), which measures the traffic flow in real-time. We preprocess all the datasets by aggregating the 30-second intervals data to 5-minute average intervals data(Choi et al. (2022)) to ensure consistency and fair comparison with previous works. In addition, we also utilize publicly available traffic flow prediction datasets(METR-LA, PEMS-BAY) presented by Li et al. (2018a). We aggregate the datasets into 5-minute average intervals data which leads to 288 observations per day. This helps to demonstrate the effectiveness and benefits of the proposed methodological framework for analyzing and modeling complex spatio-temporal MTS data over existing approaches. Additional details about the benchmark datasets are described in the appendix."}, {"title": "5 EXPERIMENTAL RESULTS", "content": "Table 1 presents a comparison of the forecast errors of proposed models(EIKF-Net and w/Unc-EIKF-Net) with those of several baseline models on five different datasets (PeMSD3, PeMSD4, PeMSD7, PeMSD7M, and PeMSD8). The forecast errors are evaluated for a 12(7)-step-prior to 12(v)-step-ahead forecasting task which is a popular and well-established benchmark in the MTSF task. The performance of the proposed models was evaluated using multiple metrics such as mean absolute error(MAE), root mean squared error(RMSE), and mean absolute percentage error(MAPE). Using multiple evaluation metrics in multi-horizon prediction tasks provides a comprehensive evaluation of the proposed models performance with the baselines. The results for the baseline models were reported in a previous study by Choi et al. (2022). The proposed models(EIKF-Net, w/Unc-EIKF-Net) consistently demonstrate state-of-the-art performance compared to baseline models on various benchmark datasets based on multiple evaluation metrics. The results show that the proposed models demonstrate the best performance with lower forecast error on benchmark datasets. Specifically, they report a 12.2%, 14.8%, 8.8%, 10.6% and 8.9% significant drop in the RMSE metric compared to the next-best baseline models on PeMSD3, PeMSD4, PeMSD7, PeMSD8, and PeMSD7(M) datasets, respectively. In addition to the pointwise forecasts, the w/Unc- EIKF-Net model(i.e., EIKF-Net with local uncertainty estimation) provides time-varying uncertainty estimates. Its performance is slightly worse than the EIKF-Net model but still outperforms several strong baselines in the literature, as reflected in the lower prediction error. In brief, the empirical results show the efficacy of the proposed neural forecasting architecture in modeling the complex and nonlinear spatio-temporal dynamics underlying the MTS data to provide better forecasts. The appendix provides more detailed information on the experimental setup, ablation studies, and other additional experimental results on multi-horizon forecasting. Moreover, the appendix discusses the experimental results that support the EIKF-Net framework's ability to handle missing data and provide more insights into the w/Unc- EIKF-Net framework for estimating uncertainty. Furthermore, the appendix also includes time series visualizations of model predictions with the uncertainty estimates compared to the ground truth."}, {"title": "6 CONCLUSION", "content": "We propose a framework that combines implicit and explicit knowledge for learning the dynamics of MTS data to provide accurate multi-horizon forecasts. The experimental results on real-world datasets demonstrate the effectiveness of the proposed framework and have shown improved forecast estimates and reliable uncertainty estimations. For future work, we would endeavor to generalize the framework to handle much larger scale graph datasets for forecasting, synthetic-private data generation, missing-data imputation etc."}, {"title": "A APPENDIX", "content": null}, {"title": "A.1 ADDITIONAL RESULTS", "content": "Table 2 compares the performance of different models(EIKF-Net, w/Unc- EIKF-Net, and several baseline models) on two benchmark datasets (METR-LA and PEMS-BAY) for multi-horizon forecasting tasks. The performance of each model is evaluated using MAE, RMSE, and MAPE metrics. The forecast errors are reported for 3-step-ahead, 6-step-ahead, and 12-step-ahead forecast horizons. The lower the forecast errors, the better the performance of the models. The results for the baseline methods are taken from a previous study by Jiang et al. (2021). On both METR-LA and PEMS-BAY datasets, the experimental results show that the proposed models (EIKF-Net, w/Unc- EIKF-Net) outperform the baseline models on different evaluation metrics on a range of forecast horizons. On the PEMS-BAY dataset, the proposed method(EIKF-Net) has forecast errors that are 40.6%, 33.5%, and 28.4% lower than the next-to-best baseline model's forecast errors for the 3-step-ahead, 6-step-ahead, and 12-step-ahead forecast horizons, respectively, as measured by the MAPE metric."}, {"title": "A.2 HYPERGRAPH ATTENTION NETWORK(HGAT)", "content": "The HgAT generalizes the local- and global-attention-based convolution operation(Veli\u010dkovi\u0107 et al. (2017); Brody et al. (2021)) on the spatio-temporal hypergraphs. The hypergraph encoder(HgAT) performs inference on the hypergraph-structured MTS data characterized by the incidence matrix, $I\\in \\mathbb{R}^{n\\times m}$, and feature matrix, $X_{(t-\\tau: t-1)}\\in \\mathbb{R}^{n \\times d}$, to compute the transformed feature matrix $H^\\prime\\in \\mathbb{R}^{n\\times d}$. Each row in H\u2019 represents the hypernode representations $h_i\\in \\mathbb{R}^{d}$, $\\forall i\\in H_V$ at time step t, where $H_V$ is the set of hypernodes. The HgAT operator captures the interdependencies and relations among the time-series variables by encoding both the structural and feature characteristics of the spatio-temporal hypergraphs in the hypernode representations h. The HgAT operator is designed to adapt to capture the changes in the dependencies of the time-series variables over time in the hypernode representations h. Let $N_{j,i} = {i \\mid I_{i,j}=1 }$ represent the subset of hypernodes i incident with any hyperedge j. The intra-edge neighborhood of the incident hypernode i is given by $N_{j,i} \\setminus i$. It is a localized group of semantically-correlated time series variables and captures higher-order relationships. The inter-edge neighborhood of hypernode i, $N_{i,j} = {j \\mid I_{i,j}=1 }$, spans the spectrum of the set of hyperedges j incident with hypernode i. The HgAT operator leverages the relational inductive bias encoded by the hypergraph's connectivity to perform intra-edge and inter-edge neighborhood aggregation schemes, which allows it to explicitly model the fine-grained spatial-temporal correlations between the time series variables. The intra-edge neighborhood aggregation focuses on the interrelations between a specific hypernode and its immediate neighboring hypernodes incident with a specific hyperedge, while inter-edge neighborhood aggregation takes into account the inter-relations between a specific hypernode and all other hyperedges incident with it. We peform the attention-based intra-edge neighborhood aggregation for learning the latent hyperedge representations as follows,\nh_(t, l) = \\sigma(\\sum\\_{z\\prime\\in N_j\\i} (\\sum\\_{i \\in N\\_j}\\alpha_(j,i) W^{(2)} h^{(t, l-1,z)})), l=1...L\\_{HgAT}\n(8)"}, {"title": "A.3 HYPERGRAPH TRANSFORMER(HGT)", "content": "The proposed HgT operator is an extension of transformer networks(Vaswani et al. (2017)) to handle arbitrary sparse hypergraph structures with full attention as a desired structural inductive bias. The HgT operator allows the framework to attend to all hypernodes and hyperedges in the hypergraph, which incentivizes learning the fine-grained interrelations unconstrained by domain-specific hierarchical structural information underlying the MTS data. This can facilitate the learning of optimal hypergraph representations by allowing the model to span large receptive fields for global reasoning of the hypergraph-structured data. The HgT operator does not rely on structural priors, unlike existing methods such as the method of stacking multiple HgNN layers with residual connections(Fey (2019), Xu et al. (2018)), virtual hypernode mechanisms(Gilmer et al. (2017); Ishiguro et al. (2019); Pham et al. (2017)), or hierarchical pooling schemes(Ramp\u00e1\u0161ek & Wolf (2021), Gao & Ji (2019), and Lee et al. (2019)) to model the long-range correlations in the hypergraph-structured data. The permutation-invariant HgT module, by exploiting global contextual information can model the pairwise relations between all hypernodes in the hypergraph-structured data. As a result, the HgT module acts as a drop-in replacement to existing methods for modeling the hierarchical dependencies and relationships among time series variables in the spatio-temporal hypergraphs leading to more robust and generalizable representations on a wide range of downstream tasks. The transformer encoder(Vaswani et al. (2017)) consists of alternating layers of multiheaded self-attention (MSA) and multi-layer perceptron(MLP) blocks to capture both the local and global contextual information. To improve the performance and regularize the model, we apply Layer normalization(LN(Ba et al. (2016))) and residual connections after every block. The transformer encoder is inspired by"}, {"title": "A.3.1 SPATIAL-TEMPORAL GRAPH REPRESENTATION LEARNING", "content": "The Temporal Graph Convolutional Network(T-GCN, Zhao et al. (2019)) operates on a sequence of dynamic graphs, where graph structure is fixed, and node attributes change over time. Each graph represents the graph-structured MTS data at a specific time step. The T-GCN operator utilizes Gated Recurrent Units(GRU, Cho et al. (2014b)) to model the spatio-temporal dynamics of the input dynamic graph sequence. In a traditional GRU, the update gate, reset gate, and hidden state are computed using matrix multiplication with weight matrices. However in T-GCN, these matrix multiplications are replaced with Graph Convolutional Networks(GCN, Kipf & Welling (2016)). The T-GCN operator analyzes graph-structured MTS data over time. It propagates information between nodes across different time steps, which enables the model to capture the complex spatio-temporal dependencies between the graphs. The T-GCN operator utilizes the predefined graph to propagate information between nodes by averaging the node representations in their local neighborhood at each time step computed as follows,\nU\\_t = \\sigma(W\\_u [ f (A(0), X(t-r: t-1)), H(t-1)] + B\\_u) (15)\nR\\_t = \\sigma(W\\_r [ f (A(0), X(t-r: t-1)), H(t-1)] + B\\_r) (16)\nC\\_t = tanh (W\\_c [ f (A(0), X(t-r: t-1)), (R\\_t * H(t-1))] + B\\_c) (17)\nH'' = U\\_t * H(t-1) + (1 - U\\_t) * C\\_t (18)\nwhere $f(A^{(0)}, X_{(t-\\tau: t-1)})$ denote the GCN operator. Ut, Rt denote the update gate and reset gate at time t. Wr, Wu, and Wc are learnable weight matrices and Bu, Br, and Bc are learnable biases. In summary, the node representation matrix, $H^{\\prime\\prime\\prime}$ captures the spatio-temporal dynamics at different scales underlying the discrete-time dynamic graphs, where each row in $H^{\\prime\\prime\\prime}$ represents the hypernode representations $h'_{(t)}\\in \\mathbb{R}^{d}$, $\\forall i\\in V$. Some of the key advantages of T-GCN operator over traditional methods include its ability to handle large and sparse spatio-temporal graphs."}, {"title": "A.4 ABLATION STUDY", "content": "We conduct a comprehensive ablation study to determine the impact of each component of the EIKF-Net framework by removing or altering them and observing the effect on the overall performance. We evaluate the individual contributions of components on the overall model performance. This study provides insights and helps identify components critical for the framework to achieve better performance. The baseline for our ablation study is the EIKF-Net framework, which integrates the spatial and temporal learning components to capture the fine-grained inter and intra-time-series correlations for modeling the nonlinear dynamics of complex interconnected sensor networks. The spatial learning component comprises two main modules: the explicit graph and the implicit hypergraph learning modules. The former operates on the predefined graph topology to capture the pair-wise semantic relationships between the variables in the graph-structured data to model the dynamics of the interconnected networks. The latter operates on the implicit hypergraph topology to capture time-evolving high-order spatial correlations in the hypergraph-structured data to model the observations of the dynamical interacting networks. We systematically exclude the components under evaluation to derive a set of model variants for verifying the importance of different components. We investigate the ablated variant model's performance compared with the baseline model to disentangle the relative gains of each learning component. The ablation study will shed light on the relationship between the variant and the baseline model and their generalization performance for MTSF, advancing the importance of underlying mechanisms. We provide the details about each ablated variant as follows."}, {"title": "A.5 ADDITIONAL STUDY ON THE SIGNIFICANCE OF HYPERGRAPH INFERENCE APPROACH.", "content": "In this section, we study the impact of jointly learning the implicit hypergraph structure & hypergraph representations from MTS data on the forecast accuracy compared to jointly learning the implicit graph structure and its corresponding graph representations. In the case of implicit graph structure learning, the goal is to capture hidden relationships and dependencies among the different variables in the MTS data represented in a graph structure that might not have been apparent with domain expertise knowledge. STGNNs perform message-passing schemes on the inferred implicit graph topology to learn the more expressive graph representations capturing the patterns and relationships underlying the MTS data for better forecasts. In the recent past, there has been an increased focus on developing methods for joint learning of discrete graph structure and representations for forecasting on MTS data. The existing methods include GTS(Graph for Time Series, Shang et al. (2021)), Graph Deviation Network(GDN, Deng & Hooi (2021)), and MTS forecasting with GNN"}]}