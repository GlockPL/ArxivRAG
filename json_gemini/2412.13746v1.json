{"title": "RAG-RewardBench: Benchmarking Reward Models in\nRetrieval Augmented Generation for Preference Alignment", "authors": ["Zhuoran Jin", "Hongbang Yuan", "Tianyi Men", "Pengfei Cao", "Yubo Chen", "Kang Liu", "Jun Zhao"], "abstract": "Despite the significant progress made by ex-\nisting retrieval augmented language models\n(RALMs) in providing trustworthy responses\nand grounding in reliable sources, they often\noverlook effective alignment with human pref-\nerences. In the alignment process, reward mod-\nels (RMs) act as a crucial proxy for human val-\nues to guide optimization. However, it remains\nunclear how to evaluate and select a reliable\nRM for preference alignment in RALMs. To\nthis end, we propose RAG-RewardBench, the\nfirst benchmark for evaluating RMs in RAG\nsettings. First, we design four crucial and chal-\nlenging RAG-specific scenarios to assess RMs,\nincluding multi-hop reasoning, fine-grained ci-\ntation, appropriate abstain, and conflict robust-\nness. Then, we incorporate 18 RAG subsets, six\nretrievers, and 24 RALMs to increase the diver-\nsity of data sources. Finally, we adopt an LLM-\nas-a-judge approach to improve preference an-\notation efficiency and effectiveness, exhibit-\ning a strong correlation with human annota-\ntions. Based on the RAG-RewardBench, we\nconduct a comprehensive evaluation of 45 RMs\nand uncover their limitations in RAG scenar-\nios. Additionally, we also reveal that existing\ntrained RALMs show almost no improvement\nin preference alignment, highlighting the need\nfor a shift towards preference-aligned training.", "sections": [{"title": "1 Introduction", "content": "Retrieval augmented generation (RAG) (Guu et al.,\n2020; Lewis et al., 2020; Izacard et al., 2023; Asai\net al., 2024b) has emerged as a widely adopted ap-\nproach for enabling large language models (LLMs)\nto access long-tailed and up-to-date knowledge\nby retrieving relevant information from external\nsources at inference. Existing retrieval augmented\nlanguage models (RALMs) leverage RAG to ad-\ndress the inherent knowledge limitations of LLMs,\neffectively reducing factual errors (Vu et al., 2024)\nand providing better attributions (Gao et al., 2023).\n\ning paradigm, namely preference-aligned RAG\ntraining. The alignment process, as illustrated in\nFigure 1(b), involves the reward model (RM) acting\nas a proxy for human values by providing feedback\non the generated responses. Based on the signals\nfrom the reward model, preference learning algo-\nrithms, such as PPO (Schulman et al., 2017) and\nDPO (Rafailov et al., 2023), optimize the policy\nmodel, ultimately resulting in the aligned RALM.\nReward models are central to this process. How-\never, whether they can provide high-quality reward\nmodeling for RALMs remains underexplored.\nConstructing a comprehensive benchmark for\nreward models in RAG settings requires consider-\nation of the following three key factors: (1) De-\nsigning well-crafted RAG scenarios: Existing\nbenchmarks for reward models primarily focus on\nevaluation in general scenarios. However, in RAG\nscenarios, human preferences introduce new align-\nment requirements. For instance, privacy protec-\ntion requires that RALMs must not disclose any\nuser privacy information from the private retrieval\ndatabase (Zeng et al., 2024). Additionally, users\noften prefer generated responses that properly at-\ntribute information to the retrieved documents; (2)\nCollecting diverse data sources: Data collection\nshould encompass a wide range of diverse sources,\navoiding reliance on a single domain, retriever, or\nRALM, to prevent any biases in the evaluation of\nthe reward model (Liu et al., 2024b); (3) Providing\nhigh-quality preference judgments: Compared to\nRewardBench (Lambert et al., 2024b) with an aver-\nage prompt length of 47, RAG needs to incorporate\na much larger number of retrieved documents in\nthe prompt. This makes it challenging for human\nannotators to efficiently process the long context\nand provide reliable preference judgments.\nIn this paper, we propose RAG-RewardBench,\na benchmark for systematically evaluating reward\nmodels in RAG settings to facilitate the alignment\nof RALMs. Our RAG-RewardBench is designed\nbased on the three key factors mentioned above:\n(1) Beyond general helpfulness and harmless-\nness, we carefully design four crucial and challeng-\ning RAG-specific scenarios, including multi-hop\nreasoning (i.e., users prefer logically coherent rea-\nsoning paths, rather than inconsistent ones), fine-\ngrained citation (i.e., users favour precise and rel-\nevant citations, rather than lengthy or excessive\nones), appropriate abstain (i.e., when unable to\nanswer with retrieved documents, actively abstain-"}, {"title": "2 Related Works", "content": "2.1 Retrieval Augmented Language Models\nThe construction of retrieval augmented language\nmodels currently adopts two main paradigms: in-\ncontext learning and supervised fine-tuning. The\nformer (Huang et al., 2023a; Ram et al., 2023; Shi\net al., 2024) integrates relevant retrieved documents\ndirectly into the prompt, allowing LLMs to gen-\nerate responses without altering their parameters.\nSince LLMs are not inherently trained to incorpo-\nrate retrieved content, they often struggle to appro-\npriately utilize the retrieved information, resulting\nin unfaithful responses or vulnerability to distrac-\ntions from irrelevant content (Wu et al., 2024).\nTo address the limitations, the latter (Asai et al.,\n2024a; Zhang et al., 2024c; Yu et al., 2024b,c;\nDong et al., 2024; Wang et al., 2024a; Xu et al.,\n2024; Lin et al., 2024; Huang et al., 2024c) trains\nRALMs on datasets constructed for RAG scenarios,\nallowing them to handle retrieved information more\neffectively. Although both paradigms have their\nmerits, they are not well-aligned with human pref-\nerences, making it challenging for RALMs to dis-\ntinguish between high-quality responses and subop-\ntimal ones. To this end, some works (Nakano et al.,\n2021; Liu et al., 2023; Li et al., 2024b; Huang\net al., 2024b; Song et al., 2024) adopt RLHF or\nDPO to optimize RALMs, enabling them to gener-\nate higher-quality citations that align with human\npreferences. However, these works focus solely on\ncitation quality to provide reward signals, without\noffering a comprehensive reward model for RAG."}, {"title": "2.2 Reward Models", "content": "Acting as an essential role in aligning LLMs with\nhuman preferences, current reward models are de-\nsigned to estimate human preferences between dif-\nferent candidates. Reward models mainly fall into\nthree categories: discriminative RMs, generative\nRMs, and implicit RMs. Discriminative RMS (Liu\net al., 2024a; Yang et al., 2024b; Wang et al., 2024f)\nare typically trained using the Bradley-Terry loss\n(Bradley and Terry, 1952), where a scalar score\nis assigned to each response. Instead of assigning\nscores, generative RMs (Kim et al., 2024; Wang\net al., 2024d; Zhang et al., 2024b) are prompted to\ndirectly generate which response is better. Another\ntype is implicit RMs (Ivison et al., 2023; Bella-\ngente et al., 2024), which are policy models trained\nusing DPO. Although it does not explicitly define\na reward function, the probabilities assigned by the\npolicy model can serve as an implicit reward signal."}, {"title": "2.3 Reward Model Evaluation", "content": "As the diversity of reward models continues to ex-\npand, a growing number of benchmarks are emerg-\ning to address the need for standardized evaluation.\nRewardBench (Lambert et al., 2024b) is the first\ncomprehensive framework for assessing RMs in\nchat, reasoning, and safety domains. Given a tuple\n(x, yc, yr), where x is the prompt, yc is the cho-\nsen response, and yr is the rejected response, the\nreward model predicts whether yc is better than yr.\nFollowing this work, M-RewardBench (Gureja\net al., 2024) extends the evaluation to multilingual\nscenarios. Furthermore, RMB (Zhou et al., 2024)\nbroadens the evaluation scope by including 49 real-\nworld scenarios. RM-Bench (Liu et al., 2024b)\nis designed to evaluate RMs based on their sen-\nsitivity to subtle content differences and style bi-\nases. VL-RewardBench (Li et al., 2024c) provides\na dataset to evaluate the vision-language generative\nRMs. These works contribute to the advancement\nof benchmarking RMs. However, a notable gap\nremains in the development of a benchmark specif-\nically tailored for RMs in the RAG scenarios."}, {"title": "3 The RAG-RewardBench Benchmark", "content": "In this section, we introduce the construction of\nRAG-RewardBench shown in Figure 2. First, we\ndesign four practical and challenging RAG-specific\nscenarios for RM evaluation. Then, we adopt 18\ndatasets, six retrievers, and 24 RALMs to synthe-"}, {"title": "3.1 Design of Well-Crafted RAG Scenarios", "content": "Building on previous works (Lambert et al., 2024b;\nZhou et al., 2024; Liu et al., 2024b), we first evalu-\nate preferences in the RAG setting from two gen-\neral aspects: helpfulness and harmlessness. For\nhelpfulness subset, human preferences lean towards\nresponses that, faithful to the retrieved documents,\nprovide useful, relevant, and accurate information,\noffering a clear answer that effectively addresses\nthe user's query. Considering the diverse user re-\nquirements in real-world applications, we sample\nqueries from the following seven RAG datasets:\nNQ (Kwiatkowski et al., 2019) (i.e., open-domain\nQA), SimpleQA (Wei et al., 2024) (i.e., open-\ndomain QA), ASQA (Stelmakh et al., 2022) (i.e.,\nlong-form QA), BioASQ (Tsatsaronis et al., 2015)\n(i.e., biomedical QA), FreshQA (Vu et al., 2024)\n(i.e., time-sensitive QA), ExpertQA (Malaviya\net al., 2023) (i.e., domain-specific QA), Multi-\nFieldQA (Bai et al., 2024) (i.e., long-context QA).\nFor harmlessness subset, human values require\nthat the responses generated by RALMs should\nnot contain harmful or biased information from the\nretrieved documents. Due to the susceptibility of\nknowledge databases in RAG systems to poisoning\nattacks (Zou et al., 2024; Xiang et al., 2024), which\ncan cause RALMs to generate malicious responses.\nWe sample harmful queries from XStest (R\u00f6ttger\net al., 2024) to assess the safety ability of RMs\nin RAG settings. Furthermore, existing research\n(Huang et al., 2023b; Qi et al., 2024) highlights that\nwhen knowledge databases contain sensitive infor-\nmation, RAG systems are prone to leaking private\ndata under carefully crafted prompts. Following\nZeng et al. (2024), we construct a Privacy dataset\nto evaluate RMs in privacy-sensitive scenarios.\nBeyond the basic helpfulness and harmlessness,\nwe propose four challenging RAG-specific scenar-\nios to evaluate reward models as follows:\n(1) Multi-hop Reasoning: Recent work (Tang\nand Yang, 2024) reveals that existing RAG sys-\ntems are inadequate at answering multi-hop queries,\nwhich require reasoning over evidence from multi-\nple documents. To enhance RALMs' ability to han-\ndle multi-hop queries, the reward model should be\ncapable of identifying logical errors and inconsis-\ntent reasoning paths in responses. We construct the\nmulti-hop reasoning subset based on HotpotQA\n(Yang et al., 2018), MuSiQue (Trivedi et al., 2022),\nand MultiHop-RAG (Tang and Yang, 2024).\n(2) Fine-grained Citation: RALMs should be\nable to ground the generated responses to the re-"}, {"title": "3.2 Collection of Diverse Data Sources", "content": "To increase the diversity of data sources, we sam-\nple multiple real-world queries from 18 subsets\nmentioned above across different domains. The\nsubset distribution is shown in Figure 7. To avoid\nbiases introduced by a single retriever, we use five\nopen-source retrievers, including BM25 (Robert-\nson et al., 2009), DPR (Karpukhin et al., 2020), E5\n(Wang et al., 2022), BGE (Xiao et al., 2023), and\nGTR (Ni et al., 2022). To obtain more realistic re-\ntrieval results, we also use Google Search\u00b9 with\nthe entire web as the retrieval corpus. As shown in\nFigure 9, the length of the retrieval results varies.\nAfter collecting the queries and their retrieval\nresults, we input them together as prompts into\nRALMs. Table 5 shows the generation prompt for\nRALMs. We adopt 24 popular RALMs to generate\nresponses, ranging from open-source models (3B\nto 70B) to commercial models (e.g., 01-mini, GPT-\n40, Gemini-1.5-Pro, Claude 3.5 and Command R),\nwith the different distribution shown in Figure 3."}, {"title": "3.3 Judgment of High-Quality Preferences", "content": "Different from RewardBench, which has an aver-\nage prompt length of 47, RALMs require incorpo-\nrating a much larger number of retrieved results\ninto the prompt shown in Figure 9. To address the\nchallenges posed by RAG's long-context prompts\n(Zhang et al., 2024a), we adopt an LLM-as-a-judge\napproach to enhance both preference annotation"}, {"title": "4 Evaluations", "content": "4.1 Evaluation Setup\nWe perform a comprehensive evaluation across var-\nious reward models on RAG-RewardBench. For\ndiscriminative RMs (), we select a large num-\nber of models that perform well on RewardBench,\nsuch as Skywork-Reward-Gemma-2-27B-v0.2 (Liu\net al., 2024a), Llama-3.1-Nemotron-70B-Reward\n(Wang et al., 2024e), URM-LLaMa-3.1-8B (Lou\net al., 2024), and InternLM2-20B-Reward (Cai\net al., 2024). For generative RMs (), we consider\nmodels specifically designed for reward modeling,\nsuch as Skywork-Critic-Llama-3.1-70B (Shiwen\net al., 2024) and Self-taught-Evaluator-Llama3.1-\n70B (Wang et al., 2024d), and incorporate powerful\nLLMs like Gemini-1.5-Pro (Reid et al., 2024) and\nQwen-2.5-72B-Instruct (Yang et al., 2024a). For\nimplicit RMs (), we adopt Llama-3.1-T\u00fclu-3-8B\n(Lambert et al., 2024a), Mixtral-8x7B-Instruct-v0.1\n(Jiang et al., 2024), and other policy models to com-\npute the response probabilities.\nGiven a tuple (x, yc, yr), where x is the prompt,\nYe is the chosen response, and yr is the rejected\nresponse, the RM needs to predict whether ye is\nbetter than yr. Following RewardBench, we use ac-\ncuracy as the evaluation metric, where the accuracy\nof random guessing is 50%. We notice positional\nbias in generative RMs, so we swap the positions\nof yc and yr, run the evaluation twice, and report"}, {"title": "4.2 Evaluation Results", "content": "Table 2 shows the evaluation results of 45 reward\nmodels in RAG-RewardBench. We rank the reward\nmodels by their average scores across all subsets.\nWe can find the following conclusions: (1) RAG-\nRewardBench is highly challenging for existing\nreward models, even though they have achieved\nvery high performance (over 90% accuracy) in gen-\neral scenarios. In RAG-RewardBench, the best-\nperforming model, Skywork-Critic-Llama-3.1-70B\n(Shiwen et al., 2024), achieves only 78.3% accu-\nracy, while powerful LLMs such as GPT-40-mini,\n01-mini, and Gemini-1.5-Pro perform at around\n60% to 70%. (2) In the four RAG-specific scenar-\nios we designed, the RM's performance decreases\nto varying extents. For example, in the fine-grained\ncitation subset, the accuracy drops by an average of\n10% compared to the helpfulness subset. This in-\ndicates that existing RMs have difficulty capturing\nsubtle errors in in-line citations within responses,\nhighlighting the need for specialized RMs tailored\nspecifically for RALMs. (3) The RMs in the top 10\nare generally generative or discriminative models\ntrained with 27B or 70B parameters. We believe\nthat using generative models for reward modeling\nin RAG tasks holds significant promise, especially\nas we observe that Self-taught-Evaluator-Llama3.1-\n70B can autonomously generate evaluation metrics\nthat are well-suited to the characteristics of RAG."}, {"title": "4.3 Analysis", "content": "Alignment Evaluation of RALMs. Consider-\ning that current state-of-the-art RALMs are pri-\nmarily trained through supervised fine-tuning, it\nnaturally raises the question of whether models de-\nveloped using this training paradigm are capable\nof aligning with human preferences. To investi-"}, {"title": "5 Conclusion", "content": "In this paper, we propose RAG-RewardBench,\nthe first benchmark for evaluating reward models\nin RAG settings, including 1,485 high-quality pref-\nerence pairs to facilitate the alignment of RALMs.\nBeyond helpfulness and harmlessness, we design\nfour crucial and challenging RAG-specific scenar-\nios, including multi-hop reasoning, fine-grained ci-\ntation, appropriate abstain, and conflict robustness.\nTo increase the data source diversity, we adopt 18\ndatasets, six retrievers and 24 RALMs. We con-\nduct experiments with 45 RMs, revealing the lim-\nitations of existing RMs on RAG-RewardBench.\nWe find that current RALMs show almost no im-\nprovement in preference alignment, highlighting\nthe need for a shift towards preference-aligned\ntraining. We release our benchmark and code pub-\nlicly at https://huggingface.co/datasets/\njinzhuoran/RAG-RewardBench/ for future work."}, {"title": "Limitations", "content": "In this work, we primarily focus on constructing\nRAG-RewardBench and analyzing the limitations\nof existing reward models across various RAG-\nspecific scenarios. Although our benchmark effec-\ntively highlights the performance gaps in current\nreward models, we acknowledge that developing\na reward model specifically tailored for RAG re-\nmains an open challenge. In future work, we plan\nto design a specialized generative reward model ca-\npable of better understanding long-context inputs\nand enhancing the alignment of RAG models with\nhuman preferences. This model will aim to address\nthe unique requirements of RAG tasks, such as\nhandling multi-document reasoning, fine-grained\nattribution, and contextual faithfulness. Addition-\nally, providing process-level or fine-grained reward\nsignals for RAG is a promising research direction."}, {"title": "Ethics Statement", "content": "Some preference pairs in RAG-RewardBench may\ncontain offensive prompts and responses. We rec-\nommend that users of RAG-RewardBench exer-\ncise caution and apply their own ethical guidelines\nwhen using the dataset, particularly in sensitive\ncontexts."}]}