{"title": "Adjusting Regression Models for Conditional Uncertainty Calibration", "authors": ["Ruijiang Gao", "Mingzhang Yin", "James McInerney", "Nathan Kallus"], "abstract": "Conformal Prediction methods have finite-sample distribution-free marginal coverage guarantees. However, they generally do not offer conditional coverage guarantees, which can be important for high-stakes decisions. In this paper, we propose a novel algorithm to train a regression function to improve the conditional coverage after applying the split conformal prediction procedure. We establish an upper bound for the miscoverage gap between the conditional coverage and the nominal coverage rate and propose an end-to-end algorithm to control this upper bound. We demonstrate the efficacy of our method empirically on synthetic and real-world datasets.", "sections": [{"title": "1 Introduction", "content": "A central challenge in supervised machine learning (ML) is the estimation of a target variable Y \u2208 Y based on a vector of inputs X. This issue involves creating a predictive function f(Y|X), which is constructed based on a dataset D = {(Xi, Yi)}~1,\nN"}, {"title": "2 Related Work", "content": "Conformal Prediction. We extend the existing body of research on conformal prediction in regression problems [7-12]. Many of these approaches traditionally depend on a fixed predictive function and focuses on designing better non-conformity score to improve conditional coverage or sharper intervals. Our method focuses on optimizing the predictive function for a given differentiable non-conformity score to improve the conditional coverage. In more recent studies, various methods have been introduced to enhance model training with the goal of reducing the size of prediction sets in classification problems [13, 14], or focusing on lower-dimensional hyperparameters rather than directing the training of all model parameters [15-17]. Similar to ours, [18] proposes a differentiable objective to improve the conditional coverage of deep learning classifiers while our paper focuses on regression tasks and we propose a novel Kolmogorov-Smirnov distance-based objective which is a sufficient condition for achieving nominal conditional coverage rate.\nApproximate Conditional Coverage. In an early work, [19] introduces a number of variants of conditional validity and achieves them using a modified inductive conformal prediction. Recently, a series of works aimed to improve the imbalanced coverage of conformal prediction. Motivated by fairness concerns, [20] proposed the equalized coverage method that has guaranteed coverage conditional on a group index. The coverage, however, is only guaranteed for the subgroups sharing the same value of a pre-specific sensitive attribute. [21] designed a regularization to encourage the independence of a coverage indicator of a miscoverage event and the predictive interval length. The regularization is a necessary but not sufficient condition for valid conditional coverage, and its effectiveness hinges on empirical validations. Noticing that the conditional coverage is equivalent to a set of moment conditions to hold for all measurable functions, [22] proposes a type of conditional coverage given a class of covariate shifts. It can provide conditional coverage over groups and over multiple pre-specified shifts, while the applications are mostly designed for group-conditional coverage with pre-specified groups and for coverage under covariate shifts with given tilting functions. In contrast, our proposed method does not require the variables to be conditioned on are known a priori."}, {"title": "3 Problem Statement", "content": "Consider i.i.d. pairs of covariates X\u1d62 and a target variable Y\u1d62, i.e. D = {(Xi, Yi)}\u1d62=1,\nN from an underlying distribution P. We observe data D and the covariates XN+1 of\na new data point. The non-conformity score function V:X\u00d7Y \u2192 R measures how\nthe prediction of our predictive model conforms to the true target Y. For example,\ngiven a fitted response function f(x), we can take the score to be the absolute residual\nV(x,y) = |y - f(x)|. In the split conformal prediction framework, the dataset is split"}, {"title": "3.1 Connection with Kolmogorov-Smirnov (KS) Distance", "content": "We find a proper distance measure to achieve valid conditional coverage is the Kolmogorov-Smirnov (KS) distance. The connection is established based on the following proposition.\nProposition 3 (Conditional Coverage Rate). Denote the cumulative distribution functions (CDFs) for the marginal and conditional non-conformity score distributions P(V) and P(V|X = x) are F(v) and G\u2093(v), respectively, then the asymptotic conditional coverage rate for the split conformal prediction is Gx(F\u207b\u00b9(1 \u2212 \u03b1)).\nWe include all the proofs in Appendix A. For a given nominal level 1 - a, we then minimize the difference between the conditional coverage of the conformal prediction and the nominal level |1 \u2013 a Gx(F\u207b\u00b9(1-a))| to achieve the desired coverage rate. However, this requires us to train a separate model for each a, which may create unnecessary computation costs. Hence, we choose to minimize the objective over all possible a, and the objective becomes\n$$max |1 - \u03b1 - G\u2093(F\u207b\u00b9(1 \u2212 \u03b1))| = max |F(v) \u2013 G\u2093(v)|$$\n\u03b1\nv\n$$=KS(P(V), P(V|X = x))$$\nwhere the first equality comes from setting v = F\u207b\u00b9(1 \u2212 a), and the second equality\nis by the definition of the KS distance.\nWe can thus achieve the target conditional coverage by controlling the KS distance\nbetween the conditional and marginal non-conformity score distributions. Taking the\nKS distance as an additive regularization, the objective in Equation (5) becomes\n$$min E(y - f\u03b8(x))\u00b2 + \u03bbsup KS(P(V|X = x), P(V)).$$\nX\nHere we use KS distance as the distance function d(\u00b7, \u00b7) in Equation (5). The supr\naims to improve the worst conditional coverage for all possible x."}, {"title": "3.2 Optimizing KS Distance", "content": "In practice, we do not have access to the true distributions of either marginal or conditional non-conformity distributions, which makes estimating the KS distance challenging. We thus choose to use empirical CDFs to estimate them. If we have access to random samples {Vi}\u1d62=1\nn and {V\u1d62\u02e3}\u1d62=1\nn drawn from P(V) and P(V|X = x)\nrespectively. The empirical KS distance can be written as\n$$max |\\frac{1}{n}\\sum_{i=1}^{n}I[V_i \\leq t] - \\frac{1}{n}\\sum_{i=1}^{n}I[V_i^x \\leq t]|$$\nt"}, {"title": "4 Experiments", "content": "We use both synthetic and real-world datasets to validate the proposed method. The code is available at https://github.com/ruijiang81/adjusting_reg_model.\nNonconformity Score: We use three popular forms of conformity scores in our experiments:\n\u2022 Residual: V(x,Y) = |Y \u2212 f(x)|. It quantifies the absolute error between the true target and the predicted value.\n\u2022 Normalized: V(x, Y) = |Y \u2212 f(x)|/\u03c3(x), where \u03c3(x) is the estimated standard deviation output by a generated model or a Bayesian neural network. Compared to the residual score, the normalized score is adaptive and can form confidence intervals that are wider for data points that have higher uncertainty. We use the conditional generative models to calculate the standard deviation.\n\u2022 Quantile: V(x,Y) = max{f\u03b1\u2097\u2092 \u2013 Y, Y \u2013 f\u03b1\u2095\u1d62}, where f\u03b1\u2097\u2092, f\u03b1\u2095\u1d62 are the predicted quantiles output by method like quantile regression. For confidence level \u03b1, we choose \u03b1\u2097 = (1 \u2212 a)/2 and \u03b1\u2095 = (1 + a)/2.\nBaseline: We compare against the following baselines:\n\u2022 Conformal Prediction (CP): We use the standard split conformal prediction method for each conformity score.\n\u2022 Orthogonal Quantile Regression [21] (OQR):OQR improves standard quantile regression by regularizing on an objective, which is a necessary condition after perfect conditional coverage is achieved. We also compare against the corresponding conformalized algorithm Conformal Orthogonal Quantile Regression (COQR).\n\u2022 Generative Model: Since our algorithm uses a generative model to improve conditional coverage, a natural choice is to use the generative model directly for uncertainty quantification. As we shall see, the generative model generally does not enjoy the marginal coverage guarantee and may have worse conditional coverage.\nSee Appendix B for a detailed discussion about the baselines."}, {"title": "4.1 Synthetic Data", "content": "We use two synthetic data-generating processes to illustrate the benefit of our method. We use linear regression as the model class for the synthetic data and report the performance trained with mean squared loss (MSE) and our algorithm. We report the marginal coverage, conditional coverage, set size, and MSE in Table 1 for a = 0.9. We use the Mixture Density Network [25] as the conditional generative model. We choose\nx = 1000 and y = 10. We use 2000 training samples, 1000 calibration samples, and\n10000 test samples. We repeat the experiment over five runs.\nFor the first setting, we consider the setting where some subgroups may be under- covered. X ~ U[-1.5, 2.5], Y = {0 +\u20ac if 2 < X < 2.2\\ 2+\u20ac otherwise, where \u20ac ~ N(0,1), then a linear regression model would significantly undercover for the subgroup 2 \u2264 X \u2264 2.2.\nThis corresponds to a case where some small subgroup of the population has a different target distribution."}, {"title": "4.2 Real-World Data", "content": "We use 6 UCI datasets to further validate the proposed method. The complete data statistics is included in Appendix C. The model class is a three-layer feed-forward neural network with LeakyReLU activation. We choose \u03bb = 100 and y = 10. Since we do not have access to the underlying data-generating process using real-world datasets,\nwe use the worst-slab coverage (WSLAB) [27] as the approximated conditional cov- erage. WSLAB measures the worst coverage on randomly sampled slabs on the test data, which is a surrogate measure of the worst group coverage. If the WSLAB has a good coverage, the method should have a good subgroup coverage over all slabs. Moreover, it is efficient to compute with a O(n) time [28] and has been used in many conformal prediction papers as a surrogate measure to examine conditional coverage"}, {"title": "4.3 Ablation Study", "content": "In our objective Equation (11), we use A to balance the MSE loss and the conditional coverage. In this section, we show the results for our synthetic data settings for different values of X. We report the Marginal Coverage, Conditional Coverage, Set Size and MSE for log(x) = \u22121, 0, 1, 2, 3 when 1 - a = 90% in Figure 3.\nSince conformal prediction enjoys the marginal coverage guarantee, the marginal coverage is not affected by \u03bb. As A increases, the confidence intervals of KS-CP get"}, {"title": "5 Conclusion", "content": "In this paper, we investigate how to train uncertainty-aware regression functions to improve the conditional coverage of the function after applying the conformal pre- diction procedure by matching the marginal and conditional non-conformity score"}, {"title": "Appendix A Proof", "content": "Proof for Proposition 3. By Theorem 1, assuming continuous nonconformity score, we have\n$$1 \u2212 a < P(V(X, Y) \\leq q*) \\leq 1 \u2212 a + \\frac{1}{n+1}$$\nTake the inverse CDF of p(V),\n$$F^{-1}(1 \u2212 a) \\leq q* < F^{-1}(1-a +\\frac{1}{n+1}).$$\nThen take the CDF of p(V|X),\n$$G_x(F^{-1}(1-a)) < P(V(X,Y) \\leq q*|X = x) = G_x(q*) < G_x(F^{-1}(1 \u2212 a +\\frac{1}{n+1})).\\$$\nThus the asymptotic coverage rate given x is\n$$G_x(F^{-1}(1 \u2212 \u03b1)),$$\nwhen n\u2192\u221e\nProof for Proposition 4. When V(x, Y) = |Y \u2212 f(x)|,\nKS(Pv (V), Qv (V)) = max |Fv (v) \u2013 Gv (v)|\nv\n= max |Pv (V \u2264 v) \u2013 Qv (V \u2264 v)|\nv\n= max|Py(|Y \u2212 f(x)| \u2264 v) \u2013 Qy(|Y \u2212 f(x)| \u2264 v)|\nv\n= max |Py (f(x) \u2013 v < Y < f(x) + v) \u2013 Qy (f(x) \u2013 v < Y < f(x) + v)|\nv\n= max |Py (Y < f(x) + v) \u2013 Py (Y \u2264 f(x) \u2013 v) \u2013 Qy(Y \u2264 f(x) + v) + Qy(Y < f(x) \u2013 v)|\nv\n\u2264 max |Py (Y < f(x) + v) \u2013 Qy(Y < f(x) + v)|\nv\n+ max |Py(Y < f(x) \u2013 v) \u2013 Qy (Y \u2264 f(x) -\nv\nv|\n= 2KS(Py (Y), Qy (Y)),\nwhere the inequality is by the triangle inequality.\nSimilarly, for the normalized non-conformity score when V(x, Y) = |Y\u2212 f(x)|/\u03c3(x),\nKS(Pv (V), Qv (V)) = max |Fv (v) \u2013 Gv (v)|\nv\n= max |Py (|Y \u2212 f(x)| \u2264 \u03c5\u03c3(x)) \u2013 Qy(|Y \u2212 f(x)| \u2264 \u03c5\u03c3(x))|\nv\n= max |Py (f(x) \u2013 \u03c5\u03c3(x) < Y < f(x) + \u03c5\u03c3(x)) \u2013 Qy (f(x) \u2013 \u03c5\u03c3(x) \u2264 X \u2264 f(x) + \u03c5\u03c3(x))|"}, {"title": "Appendix B Baseline", "content": "Here we describe each baseline in detail. T\nConformal Prediction (CP): The Conformal Prediction (CP) is the typical split conformal prediction. It randomly splits data into a training set and a validation set. A regression function \u0177 = fe(x) is fit on the training data, and the fitted function is used to compute the nonconformity score on the validation set. The complete algorithm is shown in Algorithm 2.\nCov(V,L)\nOrthogonal Quantile Regression (OQR): OQR is based on the observation that a necessary condition for conditional coverage is the independence between the coverage identifier V = 1[Y \u2208 \u0108(X)] and the set size L = |\u0108(X)|. It proposes an orthogonal regularization based on Pearson correlation as R(V, L) = \u2713Var(V) Var(L), or using Hilbert-Schmidt independence criterion (HSIC) to account for nonlinear dependence. The objective function of OQR is an additive combination of quantile regression loss and the orthogonal regularization.\nGenerative model (MDN): We use mixture density network (MDN) for our toy example. MDN models the data likelihood as\nM\n$$p(y|x) = \\sum\\limits_{i=1}^{M}\u03c0_i\u03c6(y; x, \u03b8_i).$$\n\u03c0\u1d62 are the mixture component weights, and \u03c6(y; x, 0\u1d62) = N(\u03bc\u03b8\u1d62 (x), diag(\u03c3)\u1d62\u00b2(x))).\nGenerative model (CVAE): We use Conditional Variational AutoEncoder (CVAE)\nin the real-world data experiments. For condition c, latent z, and target x, CVAE\noptimizes the variational lower bound\n$$E[log p(x|c, z)] \u2013 KL(q(z|x, c)||p(z|c)).$$"}, {"title": "Appendix F Discussion on Hyperparameter Selection", "content": "F.1 Selection of \u5165\nThe parameter is used to control the tradeoff between prediction accuracy and the conditional coverage. The selection of A is highly application-dependent and relies on the requester's need on the robustness of the confidence interval. For example, in the application of drug discovery, a good conditional coverage may be preferred over prediction accuracy, then the requester can set a relatively large A. In practice, the requester can set a maximum tolerance level of the predictive accuracy (e.g., the MSE) and tune the A such that the conditional coverage can be maximized within the specified tolerance.\nWhen \u2192 \u221e, the objective is equivalent to only minimize the regularization term. However, the convergence of the predictive set heavily depends on many factors such as the underlying data distribution and the non-conformity scores. In some cases, there may be infinitely many equally good solutions (in terms of the regularization term) with different predictive sets.\nTo see why it is the case, we can take a look at our synthetic data setup II. In this toy example, any function y = c, c \u2208 R can minimize the regularization term KS(P(V), P\u2084(V|X)) when P\u00f8 is perfect. All these solutions will have a perfect conditional coverage, which is the purpose of the regularization.\nHowever, if c is far away from 0, the predictive sets will be very wide. In fact, with different values of c, the sizes of the predict sets are generally different. Therefore, the final predictive set outputted will be highly dependent on the initial model parameters since there are many global optimums. However, all of these solutions will have similar conditional coverage when \u2192 \u221e while we cannot control the set size since there is no penalty for wide confidence intervals. Future work can also consider to penalize the interval length in the objective function. In practice, the MSE term can be used to control the predictive accuracy of the function, and therefore control the set size of the predictive sets.\nF.2 Selection of y\nFor the choice of y, intuitively the y should not be too large or too small. If y is too small, the objective may be far from the indicator function we hope to approximate. If y is too large, the objective will be too similar to the discontinuous indicator function and bring challenges to the optimization problem. We empirically demonstrate it using our synthetic data.\nWe plot the worst miscoverage rate across a with \u03bb = 100 under the synthetic data setup I with a y grid of [1, 3, 5, 7, 9, 11, 20, 50, 70, 100, 200]. The results are shown in Figure F3. We can see the miscoverage is relatively high with very small and large values of y while the miscoverage rate is low and relatively stable when y is between 3 and 50, which justifies our choice of y = 10."}, {"title": "Appendix G Discussion on Computation Cost", "content": "Compared to the standard conformal prediction method, our method requires learning a separate conditional density model. The computation costs for optimizing the conditional density model can vary heavily depending on the model class. For example, the cost of estimating the kernel density estimator is O(mn), where m is CDF grid size and n is the sample size. The cost of training a deep conditional generative model can vary heavily depend on the model architecture and number of parameters. The computation time of estimating the KS distance is also O(mn)."}, {"title": "Appendix H Discussion on Adaptive Set Size Change", "content": "First, we would like to note that our method can only lead to global set size change for residual and normalized score change. To see why, the set size of the residual non- conformity score is 2q* and the set size of the normalized non-conformity score is 20(x)q*, where q* is the quantile of the marginal non-conformity score. By changing the function f (our method), we can only influence the set size through q*, which leads to a global change in the set size.\nHowever, our method can have an adaptive set size change for some non-conformity score such as quantile scores (the predictive set is [\u011d\u0131(x) \u2013 q*, \u00e2n(x) + q*] and the set size is \u00e2n (x) \u2013 \u011d\u0131(x) + 2q*). To see an example, we segment the community dataset to black communities (the fraction of black residents is greater than 50%) and non-black communities to check the sub-group coverage with the same experimental setup in the paper. The results are shown in Table H2. Here KS-CP changes the set size adaptively and improves subgroup coverage.\nWe have added the discussion above in the revised manuscript."}]}