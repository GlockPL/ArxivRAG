{"title": "Enhancing Reasoning through Process Supervision with Monte Carlo Tree Search", "authors": ["Shuangtao Li", "Shuaihao Dong", "Kexin Luan", "Xinhan Di", "Chaofan Ding"], "abstract": "Large language models (LLMs) have demonstrated their remarkable capacity across a variety of tasks. However, reasoning remains a challenge for LLMs. To improve LLMs' reasoning ability, process supervision has proven to be better than outcome supervision. In this work, we study using Monte Carlo Tree Search (MCTS) to generate process supervision data with LLMs themselves for training them. We sample reasoning steps with an LLM and assign each step a score that captures its \"relative correctness,\" and the LLM is then trained by minimizing weighted log-likelihood of generating the reasoning steps. This generate-then-train process is repeated iteratively until convergence. Our experimental results demonstrate that the proposed methods considerably improve the performance of LLMs on two mathematical reasoning datasets. Furthermore, models trained on one dataset also exhibit improved performance on the other, showing the transferability of the enhanced reasoning ability.", "sections": [{"title": "Introduction", "content": "Although today's large language models (LLMs) can perform excellently on a variety of language tasks (Zhao et al. 2023), even approaching human levels, reasoning remains an unaddressed challenge for them (Huang and Chang 2023; Valmeekam et al. 2022). To enhance their reasoning ability, some studies use Chain-of-Thought (CoT) prompting (Nye et al. 2021; Wei et al. 2022; Kojima et al. 2022) to encourage LLMs to decompose given problems and think step by step, but the elicited reasoning ability is still limited. Nevertheless, these works emphasize the importance of step-by-step reasoning for LLMs.\nTo enhance LLMs' reasoning ability, some researchers propose Rejection sampling Fine-Tuning (RFT) (Yuan et al. 2023), which generates reasoning paths using LLMs and filters out those leading to incorrect answers. Moreover, the Self-Taught Reasoner (STaR) (Zelikman et al. 2022) use the ground truth answer as a hint for LLMs to generate reasoning paths that lead to correct answers. However, this approach may also produce incorrect reasoning paths that happen to arrive at the correct answer. Similarly, Iterative Reasoning Preference Optimization (Pang et al. 2024) samples correct and incorrect CoTs to form preference pairs and iteratively applies Direct Preference Optimization (DPO) (Rafailov et al. 2024). These training methods fall into the category of outcome-supervised fine-tuning, as they directly supervise the LLMs to produce correct final answers rather than correct reasoning paths.\nProcess supervision provides LLMs with more precise and fine-grained feedback. Lightman et al. (Lightman et al. 2023) find that their process reward models (PRM) perform significantly better than outcome reward models (ORM). Using a PRM, reasoning steps can be rewarded, enabling reinforcement learning to train LLMs to generate better CoTs. However, Lightman et al. rely on human annotators to label the reasoning steps, which is highly expensive, particularly for challenging math problems. To automatically label reasoning steps, some works employ (Wang et al. 2024a,b; Luo et al. 2024) Monte Carlo Sampling to estimate the \"correctness\" of the steps, showing that PRMs trained on their automatically labeled data can even outperform those trained on PRM800K (Lightman et al. 2023), a human-labeled dataset. Wang et al. (Wang et al. 2024a) further train LLMs with their PRMs using reinforcement learning, and find that it is better than training with ORMs. ReST-MCTS* (Zhang et al. 2024b) leverages Monte Carlo Tree Search (MCTS) (Kocsis and Szepesv\u00e1ri 2006; Browne et al. 2012) to annotate the process reward of each step for training a PRM, and use the PRM to guide MCTS in turn.\nSome studies propose methods that do not require a PRM to provide process supervision to LLMs. Step-DPO (Lai et al. 2024) and Self-Explore (Hwang et al. 2024) identify the first incorrect step in a reasoning path through step-by-step verification, creating a pairwise dataset for subsequent preference learning. MCTS-DPO (Xie et al. 2024) utilizes MCTS to generate pairwise data for preference learning, with training and data generation performed iteratively. However, MCTS-DPO labels reasoning steps only as chosen or rejected, which does not accurately capture the quality of the steps, which can lead to suboptimal performance. Additionally, MCTS-DPO forms preference pairs using only the best and worst steps, discarding other steps that are potentially valuable.\nIn this paper, we study using MCTS to generate process supervision data. We apply MCTS at each step along the reasoning paths generated by an LLM, assigning a score that captures \"relative correctness\" to the sampled next steps."}, {"title": "Related Work", "content": "A key technique for enhancing reasoning is Chain-of-Thought (CoT) prompting (Nye et al. 2021; Wei et al. 2022; Kojima et al. 2022; Wang et al. 2022), which encourages LLMs to think about problems step by step and generate reasoning chains. It is also found that reasoning chains with more steps are more likely to lead to correct answers (Fu et al. 2022), further highlighting the importance of step-by-step reasoning. Furthermore, tree search algorithms, such as MCTS, are integrated with LLMs during inference to search for correct reasoning paths, resulting in significant improvements in performance on reasoning tasks (Hao et al. 2023; Yao et al. 2024; Qi et al. 2024; Zhang et al. 2024a), but at the cost of a substantial increase in inference compute.\nWhile the aforementioned studies enhance reasoning during inference, another research direction aims to instill reasoning ability into LLMs through training. Some studies train LLMs using responses generated by the models themselves (Yuan et al. 2023; Zelikman et al. 2022; Pang et al. 2024; Trung et al. 2024), employing supervised fine-tuning or reinforcement learning. Question synthesis has also been shown to be effective for generating training data (Yu et al. 2023; Liu et al. 2024; Lu et al. 2024; Li et al. 2024), where several data augmentation techniques are commonly applied.\nRecently, an increasing number of studies have demonstrated that process supervision is more effective than outcome supervision, for training both reward models (Lightman et al. 2023; Wang et al. 2024a,b; Luo et al. 2024) and LLMs (Lai et al. 2024; Hwang et al. 2024; Xie et al. 2024; Wang et al. 2024a; Chen et al. 2024). Learned process reward models are usually used for reinforcement learning and for selecting the best reasoning path from a set of sampled paths. In this paper, we explore training LLMs with process supervision without relying on reward models, thereby avoiding the complexity and instability of reinforcement learning."}, {"title": "Proposed Methods", "content": "Assume that we have a dataset of reasoning problems (e.g., mathematical problems) and their corresponding answers $P = \\{(x^i, y^i)\\}_{i=1}^N$. Our goal is to enhance the reasoning ability of the target LLM with $P$ without human annotation. In addition, we assume that we are not accessible to LLMS stronger than the target LLM, so that our methods can be applied to the strongest LLMs."}, {"title": "Data Generation", "content": "We generate a training dataset $D = \\{(x^i, p_j^i, s, c)\\}_{i=1}^N$, where $x^i$ denotes the i-th problem in $P$, $p_j^i$ denotes the j-th partial solution to $x^i$, $s$ denotes the next steps of the partial solution $p_j^i$, and $c$ denotes the scores assigned to $s$.\nFor each problem in $P$, We regard each individual step in problem-solving steps as a tree node, and the steps are separated by two newline characters. We perform MCTS at each step along the reasoning paths generated by the LLM, exploring the best next steps. Specifically, for each partial solution $p_j^i$ of the problem $x^i$, we follow the iterative procedure below:\n1. Selection. Starting from the root node (i.e., $p^i$), we select the child node with the highest Upper Confidence Bound (UCB) (Kocsis and Szepesv\u00e1ri 2006) value until the current node is either not fully expanded or represents a final step of the solution (e.g., \"The final answer is 3.\").\n2. Expansion. If the current node does not represent a final step and has not been fully expanded, we sample the next"}, {"title": "Iterative Training", "content": "We iteratively train the LLM after generating training data using the LLM from the last iteration, starting with the pretrained LLM at the first iteration. In each iteration, a certain number of problems are sampled from $P$ for data generation.\nAt the i-th iteration, the LLM is trained by minimizing the following loss:\n$\\mathcal{L}(\\pi_{\\theta_i}) = -E_{(x,p,s,r) \\sim D_i} [r \\log \\pi_{\\theta_i}(s | x,p))] + D_{KL}(\\pi_{\\theta_i}(s | x,p) || \\pi_{\\theta_{i-1}}(s | x,p))$\nwhere $\\pi_{\\theta_i}$ denotes the LLM at the i-th iteration. The first term is weighted negative log-likelihood. Inspired by reinforcement learning from human feedback (Ouyang et al. 2022), we incorporate a KL penalty in the second term to mitigate the distribution shift, which is a challenge in offline reinforcement learning. In fact, our training method can be regarded as a form of reinforcement learning."}, {"title": "Experiments", "content": "We apply the proposed methods to Llama-3.1-8B-Instruct\u00b9 and deepseek-math-7b-instruct\u00b2, and evaluate the performance on two popular mathematics datasets, GSM8K (Cobbe et al. 2021) and MATH (Hendrycks et al. 2021)."}, {"title": "Main Results", "content": "The experimental results are presented in Table 1. We report the results of iterative training until the accuracies do not increase anymore. It can be observed that our methods consistently outperform the baselines by large margins, with accuracies improving during iterative training. However, the performance converges quickly and fails to continually improve over more iterations. As a result, not all the problems in the training sets of MATH and GSM8K are used for training: only about 2,000 in MATH and 2,000 in GSM8K are used. In addition, the performance converges more quickly on GSM8K than on MATH, which we attribute to the lower difficulty of GSM8K, making it easier for models to learn. It is noteworthy that Step-level DPO achieves only very marginal improvements on MATH, which indicates that our data generation method is significantly superior."}, {"title": "Transferability Evaluation", "content": "The MATH dataset consists of high school math competition problems, while GSM8K comprises grade school math problems. If the LLMs' mathematical reasoning ability has been improved, they should perform better on both datasets. Therefore, we evaluate the transferability of the models by training it on MATH/GSM8K and testing it on GSM8K/MATH to verify whether their mathematical reasoning ability has indeed been enhanced.\nAs shown in Table 2, our methods outperform Zero-shot-CoT on unseen datasets, indicating they indeed learn mathematical reasoning ability. As expected, the improvements are less substantial than those observed in non-transfer experiments, since the problems in the two datasets require different mathematical skills and knowledge."}, {"title": "Conclusion and Limitations", "content": "In this work, we propose a process supervision data generation method utilizing MCTS and a training approach, for improving the reasoning ability of LLMs. We evaluate the proposed methods on two well-known mathematical datasets and demonstrating the effectiveness. The trained models also outperform the pre-trained models on datasets they are not trained on, showing the transferability of their learned knowledge and skills.\nHowever, the performance converges quickly during iterative training, failing to continually improve over many iterations, and we do not even use all the problems in the training sets. Training for more iterations or using more problems not only fails to improve the performance but actually degrades it. Future research could study the underlying reasons for this phenomenon and how to achieve more substantial improvements with more iterations.\nIn addition, the models in our experiments are trained using LoRA, which could have limited the magnitude of the observed improvements."}]}