{"title": "GSAVS: Gaussian Splatting-based Autonomous\nVehicle Simulator", "authors": ["Rami Wilson"], "abstract": "Modern autonomous vehicle simulators feature an ever-growing library of assets,\nincluding vehicles, buildings, roads, pedestrians, and more. While this level of\ncustomization proves beneficial when creating virtual urban environments, this\nprocess becomes cumbersome when intending to train within a digital twin or a\nduplicate of a real scene. Gaussian splatting emerged as a powerful technique in\nscene reconstruction and novel view synthesis, boasting high fidelity and rendering\nspeeds. In this paper, we introduce GSAVS, an autonomous vehicle simulator that\nsupports the creation and development of autonomous vehicle models. Every asset\nwithin the simulator is a 3D Gaussian splat, including the vehicles and the envi-\nronment. However, the simulator runs within a classical 3D engine, rendering 3D\nGaussian splats in real-time. This allows the simulator to utilize the photorealism\nthat 3D Gaussian splatting boasts while providing the customization and ease of\nuse of a classical 3D engine.", "sections": [{"title": "1 Introduction", "content": "Simulating dangerous scenes is necessary to create robust autonomous driving models. Since some\ndriving scenarios are too dangerous to recreate in reality (car crashes, close calls, etc.), a simulation\nis a safe alternative to provide enough training data variety for autonomous vehicle models.(15) This\nfurther improves the robustness of autonomous vehicle models and allows them to navigate a greater\nvariety of driving scenarios.\nCurrent state-of-the-art simulators, such as CARLA (5), excel at representing the complexity of\nurban environments. Classic mesh-based approaches are utilized to represent scenes, with separate\nassets to represent every component. This allows for the customization of assets to fit any use case,\nincluding scene parameters such as lighting, traffic rules, intersections, and any other parameters that\ncan potentially affect the driving model.\nThe use of standard 3D file formats is what gives these simulators their extensibility. For example, in\nCARLA, while users can use the generated assets and scenes, users also have the ability to create and\nimport new vehicles and maps in the standard .fbx format. (5) This allows for the creation of custom\nmaps and scenes with the corresponding assets, making simulators like CARLA a platform capable\nof representing scenes with a high level of granular control.\nThe goal of any autonomous vehicle simulator is to train a model that is capable of bridging the\nsim-to-real gap. That is, a model's performance in a virtual simulator should ideally transfer to the\nreal world. However, in practice, we find that is not the case. For example, the ego vehicle trained in\na simulator may have different dynamics than the real-world vehicle. These dynamics can include\ndifferent lighting, different agents, noise in the input data, etc. This means that learned actions are\nusually not executed in the real world as the model expects, and even states that the model perceives\ncould be different from training (13)."}, {"title": "2 Simulation", "content": "The environment is a relatively large 3D Gaussian splatting asset where the simulation occurs, and all\nassets are instantiated within. This approach, compared to the standard mesh-based approach, allows\nfor higher photorealism in the training environment. Novel views and lighting information can be\nobserved by the ego vehicle, all preserved in an asset that is much smaller than the input source or\neven a classical mesh-based digital twin of the same input scene (4).\nData intended to be used to capture any asset, including the environment, largely follows best\npractices for capturing good input data for standard 3D Gaussian splatting (12). However, there is a\nkey difference between capturing standard subjects of 3D Gaussian splatting and driving simulator\nenvironments. When using single-view RGB images, the input data is no longer a closed-loop\nsequence. Data captured for 3D Gaussian splatting requires even coverage of the object and sufficient\noverlapping features between each image. Structure-from-motion (SfM) can match common features\nbetween images, resulting in a sufficiently accurate point cloud. Unfortunately, this is not the case\nwhen capturing driving scenarios. While sequential pairs of images may have many features in\ncommon, the first and final likely do not, provided the input data doesn't include the vehicle driving\nalong the same feature multiple times. More so, the quality of the extraction of common features is\nusually directly affected by camera frame rate, vehicle speed, lighting conditions, etc.\nUsing standard data capturing methods with data obtained from a vehicle, we can largely alleviate\nthis issue by using multi-view RGB input (6). When using single-view RGB input, the data does\nnot contain enough views of every feature in the scene. The data also does not contain enough\ninformation to place every feature's point clouds accurately in the scene. However, using multi-view\ndata allows us to capture different views of the same features from all sides of the vehicle. This\ncreates common features across multiple images, which hopefully results in enough data for feature\nextraction and accurate point cloud generation.\nIdeal datasets contain multiple views of the environment from the ego vehicle, preferably at regular\nintervals. At any time $t$ in the simulation, the quality of the scene varies depending on of the input\ndata contained images of the scene at time $t$. If not, then the scene will have to be synthesized directly\nby the model. As the input data becomes more sparse, this synthesized scene becomes less accurate.\nSo, to preserve quality, a higher frequency of data capture is preferable and yields a better result. The\nnuScenes dataset (2) was used in this project for creating the environment.\nThe nuScenes dataset (2) provides multi-view images of a vehicle driving through various scenes.\nMost relevant to our scene reconstruction, the data contains 6 RGB camera streams, all capturing\nfeatures at the same time. Training the standard 3D Gaussian splatting process with nuScenes data\nyields results that retain a lot of the features present in the scene but contain a lot of the common issues\ncommon with poor quality Gaussian splats, including inaccurate features and \"floaters\"; artifacts that\nremain in the image, usually as a result of sparse data."}, {"title": "2.1 Environment", "content": "The environment is a relatively large 3D Gaussian splatting asset where the simulation occurs, and all\nassets are instantiated within. This approach, compared to the standard mesh-based approach, allows\nfor higher photorealism in the training environment. Novel views and lighting information can be\nobserved by the ego vehicle, all preserved in an asset that is much smaller than the input source or\neven a classical mesh-based digital twin of the same input scene (4).\nData intended to be used to capture any asset, including the environment, largely follows best\npractices for capturing good input data for standard 3D Gaussian splatting (12). However, there is a\nkey difference between capturing standard subjects of 3D Gaussian splatting and driving simulator\nenvironments. When using single-view RGB images, the input data is no longer a closed-loop\nsequence. Data captured for 3D Gaussian splatting requires even coverage of the object and sufficient\noverlapping features between each image. Structure-from-motion (SfM) can match common features\nbetween images, resulting in a sufficiently accurate point cloud. Unfortunately, this is not the case\nwhen capturing driving scenarios. While sequential pairs of images may have many features in\ncommon, the first and final likely do not, provided the input data doesn't include the vehicle driving\nalong the same feature multiple times. More so, the quality of the extraction of common features is\nusually directly affected by camera frame rate, vehicle speed, lighting conditions, etc.\nUsing standard data capturing methods with data obtained from a vehicle, we can largely alleviate\nthis issue by using multi-view RGB input (6). When using single-view RGB input, the data does\nnot contain enough views of every feature in the scene. The data also does not contain enough\ninformation to place every feature's point clouds accurately in the scene. However, using multi-view\ndata allows us to capture different views of the same features from all sides of the vehicle. This\ncreates common features across multiple images, which hopefully results in enough data for feature\nextraction and accurate point cloud generation.\nIdeal datasets contain multiple views of the environment from the ego vehicle, preferably at regular\nintervals. At any time $t$ in the simulation, the quality of the scene varies depending on of the input\ndata contained images of the scene at time $t$. If not, then the scene will have to be synthesized directly\nby the model. As the input data becomes more sparse, this synthesized scene becomes less accurate.\nSo, to preserve quality, a higher frequency of data capture is preferable and yields a better result. The\nnuScenes dataset (2) was used in this project for creating the environment.\nThe nuScenes dataset (2) provides multi-view images of a vehicle driving through various scenes.\nMost relevant to our scene reconstruction, the data contains 6 RGB camera streams, all capturing\nfeatures at the same time. Training the standard 3D Gaussian splatting process with nuScenes data\nyields results that retain a lot of the features present in the scene but contain a lot of the common issues\ncommon with poor quality Gaussian splats, including inaccurate features and \"floaters\"; artifacts that\nremain in the image, usually as a result of sparse data."}, {"title": "2.2 Road Spline Track", "content": "Due to the sparse nature of the input data, the quality of novel views quickly diminishes as the view\nstrays away from the original point of view of the input data. Namely, any scene synthesized from\nthe Gaussian splat of the environment that differs greatly from the point of view of the input data will\nyield unreliable results. However, since driving simulations are constrained in nature (i.e., both the\nsimulated vehicle and data-capture vehicle need to drive on the road, not drive through buildings, on\nsidewalks, etc.), the views and, by extension, the positions of the ego vehicle and the data capture\nvehicle should be roughly similar. That is, the data capture vehicle route constrains where the ego\nvehicle may go. This observation allows us to utilize the camera extrinsics resulting from the 3D\nGaussian splatting process to construct a spline where we know the environment would look the most\naccurate.\nWhen the simulation is initialized, camera extrinsics from the 3D Gaussian splat asset are processed.\nA spline is constructed, where each camera position (X, Y, Z) serves as a knot on the spline. This\nspline effectively serves as the original path that the vehicle took when capturing the input data.\nIn order to ensure that physics acts on the ego vehicle accurately within the Unity Engine, we need to\ncreate a way for the ego vehicle to interact with the environment. Namely, we need to find a way to\nhave the ego vehicle be able to interact with the road, as well as other features of the environment\nsuch as buildings, curbs, etc. As 3D Gaussian splats are inaccurate in large unbounded scenes (10),"}, {"title": "3 Autonomous Driving", "content": "The ego vehicle used in simulations, for all the benefits mentioned above, is also a 3D Gaussian\nsplatting asset. Since capturing the ego vehicle is an easier task for the 3D Gaussian process as it is\na bounded scene of an object, it doesn't suffer from the same downfalls as creating an asset of the\nenvironment.\n3D Gaussian splatting was the chosen method for the other assets for the graphical fidelity, accurate\nlighting, and compact size compared to a classical texture and mesh asset. However, the asset still\ncaptures a large part of the background, so pre-processing is required to clean up the asset. The\nopen-source tool SuperSplat (14) was used to manually clean the assets, removing all 3D Gaussians\nthat don't contribute to the asset's final form. Even with the time taken to clean the assets, it takes\nconsiderably less time and resources to create an ego vehicle asset than with classical mesh-based\ntechniques while achieving photorealistic results. More importantly, 3D Gaussian splats take less\ncompute to render at high frame rates.\nThe ego vehicle asset is functionally different from a standard vehicle model in Unity. Unity renders\n3D Gaussian splats as point clouds, and so another level of pre-processing is required to convert the\nego vehicle asset into an asset controllable by the Unity engine.\nFirst, a large box collider is attached to the ego vehicle asset. This allows for collision detection\nbetween the ego vehicle and the track walls, as well as any collisions with other vehicle agents.\nSecond, four separate wheel colliders are placed in matching locations on the ego vehicle asset. This\nallows for collision with the RoadTrackAsset so that the ego vehicle remains on the track. The wheel\ncolliders also allow the vehicle to accept inputs, namely, torque vectors and steering angles that move\nthe ego vehicle front, back, left, and right."}, {"title": "3.1 Ego Agent", "content": "The ego vehicle used in simulations, for all the benefits mentioned above, is also a 3D Gaussian\nsplatting asset. Since capturing the ego vehicle is an easier task for the 3D Gaussian process as it is\na bounded scene of an object, it doesn't suffer from the same downfalls as creating an asset of the\nenvironment.\n3D Gaussian splatting was the chosen method for the other assets for the graphical fidelity, accurate\nlighting, and compact size compared to a classical texture and mesh asset. However, the asset still\ncaptures a large part of the background, so pre-processing is required to clean up the asset. The\nopen-source tool SuperSplat (14) was used to manually clean the assets, removing all 3D Gaussians\nthat don't contribute to the asset's final form. Even with the time taken to clean the assets, it takes\nconsiderably less time and resources to create an ego vehicle asset than with classical mesh-based\ntechniques while achieving photorealistic results. More importantly, 3D Gaussian splats take less\ncompute to render at high frame rates.\nThe ego vehicle asset is functionally different from a standard vehicle model in Unity. Unity renders\n3D Gaussian splats as point clouds, and so another level of pre-processing is required to convert the\nego vehicle asset into an asset controllable by the Unity engine.\nFirst, a large box collider is attached to the ego vehicle asset. This allows for collision detection\nbetween the ego vehicle and the track walls, as well as any collisions with other vehicle agents.\nSecond, four separate wheel colliders are placed in matching locations on the ego vehicle asset. This\nallows for collision with the RoadTrackAsset so that the ego vehicle remains on the track. The wheel\ncolliders also allow the vehicle to accept inputs, namely, torque vectors and steering angles that move\nthe ego vehicle front, back, left, and right."}, {"title": "3.2 Vehicle Agent", "content": "Similar to the ego vehicle, any agent vehicles in the scene are also instantiated as a 3D Gaussian splat\nasset. This allows the environment to be seen not only as photorealistic but also as any novel agent\nvehicle operating in the scene.\nThe vehicle agent assets are also pre-processed to remove any 3D Gaussians not contributing to the\nfinal form of the asset. A tightly bounding box is placed around the vehicle agent so that collisions\nwith the ego vehicle are detected by the model.\nA vehicle agent controller determines the dynamics of the vehicle agents in the scene. For any\ntime step $t$, the vehicle agent controller updates the position of the vehicle agents in a forward\ndirection along the spline. The upward vector applied to the agent vehicles is the same upward vector\nextracted from the camera extrinsics. The forward vector of the vehicle agents aligns with the spline\nforward vectors at any position $p$ along the spline, allowing agent vehicles to be dynamic within\nthe environment and allowing for the addition of obstacles in the digital twin that weren't originally\npresent in the real world scene."}, {"title": "4 Experiments", "content": "We evaluate the simulator on 3 different tasks, observing resource usage for each task, as well as\nmodel training accuracy.\n\u2022 No dynamic agents, small scene. The ego vehicle must drive in a straight line from the\nstarting point to the goal.\n\u2022 No dynamic agents, large scene with one turn. The ego vehicle must initially drive in a\nstraight line from the starting point, then turn right and continue driving to the goal. The\nlarge scene is roughly twice the drivable area as the small scene.\n\u2022 Dynamic agents, small scene. The ego vehicle must drive in a straight line, but there is one\ndynamic agent driving in front of the ego vehicle. The ego vehicle must avoid collisions\nwith the dynamic vehicle agent on the way to the goal.\nThe simulator is configured to utilize the ml-agents library(11). It is an open-source project that\nallows for the training of the ego vehicle agent within the environment. The ego vehicle is initialized\nat one end of the spline, and a goal collider is initialized at the other. An episode of training is\ndesigned to end negatively if the ego vehicle collides with either the RoadBlockAsset walls or any\nagent vehicle. An episode ends positively if the ego agent collides with the goal collider. The ego\nvehicle uses default parameters for motor force, break force, vehicle mass, steering angle, and drag,\nas well as any other parameters."}, {"title": "5 Results", "content": "The training was performed over 250,000 episodes for each task using the standard ml-agents (11)\nPPO policy. Table 1 reports the results of letting the ego vehicle train on the three different tasks. We\nmeasure the accuracy of the model, seeing how well it performs after training with 50 test episodes.\nWe also measure resource utilization, including the average FPS, average GPU utilization percentage,\nand average VRAM usage, all as percentages.\nWhile the model performed best with task 1, it still didn't perform perfectly, even with the simple task\nof driving straight toward the goal. We can see that this isn't an unusual result, as even mesh-based\nsimulators such as CARLA (5), which don't suffer from noisy ego vehicle observations due to\nphotorealism, experience similar results in a similar task.\nRegarding accuracy, we can see that the model performed best within a small scene, getting above\n80 percent accuracy in both cases. We can also observe that when the problem space increased to\na larger scene, such as with task 2, the model accuracy dropped to about 70 percent. This can be\nattributed to a more complex problem space, as the model would need to learn turning parameters in\naddition to motor and break force parameters.\nWith resource utilization, due to the simple nature of task 1, we can use its results as a baseline to\nanalyze performance and compare its results to the other two tasks. In task 2, the drivable area for the\nego vehicle is roughly double in size, but the increase in average GPU utilization and average VRAM\nusage has not doubled compared to task 1 and increased by a relatively minor amount. With task 3,\ndespite adding dynamic agents to the scene, we observe that the increase in average GPU utilization\nand average VRAM usage is marginal, only increasing by at most 3 percent. We also see that average\nFPS only marginally increases when the drivable area is doubled in size and essentially remains the\nsame when adding dynamic agents to the scene. We can attribute these resource utilization benefits to\nthe efficiency of using 3D Gaussian splatting assets within the simulator. Since 3D Gaussian splatting\nassets are made of point clouds that take up less space than an identical mesh-based asset, resource\nutilization reflects this efficiency in asset visualization."}, {"title": "6 Future Works", "content": "While 3D Gaussian splatting has proven to be a revolutionary new paradigm in novel-view synthesis,\nmany surveys show the drawbacks of the method. 3D Gaussian splatting can produce residual artifacts\nin the final image, resulting in a less accurate model for an autonomous vehicle training environment\n(3; 4; 6; 1). Also, the standard method of 3D Gaussian splatting does not support dynamic Gaussian\nsplats, which means that simulators still need to move dynamic components at runtime. Since most\nphysics engines are not 3D Gaussian splatting native, a level of translation, between a 3D Gaussian\nsplatting asset and a standard asset (in our paper it was converting the 3D Gaussian splat into a Unity\nasset), still needs to be done. We recognize 4 possible methods to improve this 3D Gaussian splatting\nsimulator, which would vastly improve fidelity, performance, and usability.\nStandard 3D Gaussian splatting results in assets that are unable to be relighted with different lighting\nconditions in 3D engines such as Unity (7; 4). However, methods such as Relightable 3D Gaussians\n(7) have been developed to result in assets that can achieve photorealistic relighting. This would\nenable simulators to dynamically adjust the environment to any lighting environment, increasing\ntraining diversity and improving the model.\nWhile the environment is a 3D Gaussian splatting asset, any agent that needs to be dynamic to\nthe scene is made as a separate 3D Gaussian splatting asset, initialized into the environment, and\ndynamically animated within the environment. While this allows for novel driving scenarios to\nbe created within the simulator not initially present in the input data, other dynamic elements\nare not represented. This can include pedestrians, vehicles on the other side of the road, traffic\nlights, etc. Dynamic Gaussian splatting methods have been introduced, such as S\u00b3Gaussian (9) and\nDrivingGaussian (21) that allow for dynamic Gaussians to be present in the scene. The ego vehicle\nwould perceive the environment as a real vehicle would, with multiple dynamic agents in the scene,\nwhich may or may not interact with the ego vehicle, further improving accuracy in the model."}, {"title": "7 Conclusion", "content": "We have presented GSAVS, an autonomous vehicle simulator that allows for the creation and\ndevelopment of autonomous vehicle models where all assets, including the environment, are 3D\nGaussian splats. This allows for photo-realism with relatively low compute requirements relative\nto classical mesh-based simulators. Using a 3D engine, we are able to extract information from the\ninput data and utilize it to create a digital twin of a real-world scene, further augmenting it with 3D\nGaussian splat obstacles and increasing training variety to create a more robust model."}]}