{"title": "Anatomically-Grounded Fact Checking of Automated Chest X-ray Reports", "authors": ["R. Mahmood", "D. M. Reyes", "G. Wang", "P. Yan", "P. Kaviani", "M. Kalra", "K.C.L. Wong", "N. D'Souza", "L. Shi", "J. Wu", "T. Syeda-Mahmood"], "abstract": "With the emergence of large-scale vision-language models, realistic radiology reports may be generated using only medical images as input guided by simple prompts. However, their practical utility has been limited due to the factual errors in their description of findings. In this paper, we propose a novel model for explainable fact-checking that identifies errors in findings and their locations indicated through the reports. Specifically, we analyze the types of errors made by automated reporting methods and derive a new synthetic dataset of images paired with real and fake descriptions of findings and their locations from a ground truth dataset. A new multi-label cross-modal contrastive regression network is then trained on this datsaset. We evaluate the resulting fact-checking model and its utility in correcting reports generated by several SOTA automated reporting tools on a variety of benchmark datasets with re- sults pointing to over 40% improvement in report quality through such error detection and correction.", "sections": [{"title": "1. Introduction", "content": "With the emergence of large-scale vision-language models (VLMs), several researchers have turned to medical ap- plications of automated report generation for medical im- ages such as chest X-rays [1, 3, 4, 16, 19, 23, 24, 29]. A preliminary radiology report produced by such models is helpful in emergency room settings where radiologists may not be readily available and the interpretation needs to be performed by residents or other clinical staff. However, the predominance of hallucinations and factual errors have made such report generators less practical in clinical work- flows. Figure 1b shown an example of the such an error in an automatically generated report in the sentence high-"}, {"title": "2. Related Work", "content": "Current methods for detecting and correcting for errors in generative AI reports have been primarily developed for training and fine-tuning large language models or vision- language models [6, 10, 11, 21, 27, 39, 43]. They use di- rect policy optimization (DPO) [22] or proximal policy op- timization (PPO) [42] along with reward models [45] to assess fine-grained subjective performance using the rein- forcement learning with human feedback(RLHF) paradigm. While capturing human feedback is possible through Me- chanical Turks for general LLM or VLMs, building sim- ilar reward models would be difficult for radiology reports needing large clinician time and attention. Methods for fact- checking during inference exist primarily for language-only models where patterns of phrases found repeatedly in text are used to spot errors or by consulting other external textual sources for checking the veracity of information in an agen- tic fashion [10, 21, 27, 32]. More recently, language models are also being used for fact-checking other LLM-generated reports [26] which are not as suitable for radiology reports since these models themselves have errors. Bootstrapping them with an independent source of verification would still be desirable.\nThe closest work to us is an image-driven fact checking method described in [14] which reused a vision-language model (CLIP) pre-trained on chest X-ray data and a binary SVM classifier to classify findings as real or fake in auto- mated radiology reports [14]. This approach, while promis- ing had several limitations. First, since radiology report sen- tences describe multiple findings in a sentence, using such full sentences for training the model could make an entire sentence misclassified and removed during report correc- tion. Using full-length sentences also binds this method to sentence styles used in the training data and limits its gen- eralizability across report generators. Next, the CLIP con- trastive model used was pre-trained only on real pairs of images and reports and so features derived from such mod- els are not suitable for discrimination between real and fake findings. Finally, the binary classifier used does not offer explanation of the errors nor anatomically locate the find- ing as done in our approach.\nWhile the general problem of correcting generated text from language models has been studied during training, in- ference, and post-hoc phases [18, 26, 40], correction for ra- diology reports so far have been through simplistic methods"}, {"title": "3. Extracting findings and their locations", "content": "To make our fact-checking approach agnostic to auto- mated reporting tools' sentence writing styles, we need to abstract the findings described in reports into structured rep- resentations. We adopt the fine-grained finding patterns (FFL) work described in [29], and restrict them to cover the core finding and its anatomical location as:\n$F_i = <T_i N_i C_i A_i >$  (1)\nwhere $T_i$ is the finding type, $N_i = yes|no$ indicates a present or absent finding respectively, $C_i$ is the normal- ized core finding name, $A_i$ is the anatomical location spec- ified with laterality. Each finding is normalized to a stan- dard vocabulary (e.g. enlarged cardiac silhouette versus car- diomegaly) using a comprehensive clinician-curated chest X-ray lexicon reported in [28, 36]. Based on the vocabulary captured in the lexicon, a total of 101,088 distinct FFL pat- terns can be formed which are sufficient to capture the va- riety of findings reported in automated reporting tools. The FFL label extraction algorithm reported in [29] is known"}, {"title": "3.1. Developing a synthetic dataset", "content": "Given a dataset of chest X-rays and their associated re- ports, we extract all real FFL patterns and anatomical loca- tions of regions covered by the FFL patterns. We then de- rive a synthetic dataset starting from these real FFL patterns to reflect the types of errors made by automated reporting tools. As reported in [40], these errors include false pre- dictions, omissions, incorrect finding locations or incorrect severity assessments. In this paper, we focus on modeling incorrect findings and their locations.\nThe synthetic dataset created for training our fact- checking model can be described in terms of finding- location (FL) pairs. Let F be the total list of possible findings in chest X-ray datasets. Let < I, R > be the sample set of corresponding image-report pairs in a gold dataset D. Since each report Ri will contain a variable number of findings, an existing multi-label set of sample $D_i \\in D =< I_i, R_i >$ can be denoted by its real FL pairs $FL_{iReal} = {f_{lij}} = {< f_{ij}, l_{ij} >}$ where:\n$f_{ij} =< T_{ij} N_{ij} C_{ij} >, l_{ij} =< X_{ij}, Y_{ij}, W_{ij}, h_{ij} >$  (2)\nHere $f_{ij} \\in F_{iReal}$ is the jth real finding in report Ri and $l_{ij}$ is the bounding box for the finding $f_{ij}$ in image $I_i$ start- ing at $(X_{ij}, Y_{ij})$ of width $w_{ij}$ and height $h_{ij}$ in normalized coordinates ranging from 0 to 1. Since locations are be- ing modeled through $l_{ij}$, we drop the textual description $A_j$ from the FFL pattern $f_j$ for purposes of FC model genera- tion while retaining it still for report evaluation."}, {"title": "4. Developing a fact-checking model", "content": "The overall workflow for training our FC model is il- lustrated in Figure 4 where the dataset of synthetic and real FL pairs along with their images are used to train our fact-checking model. Given a mini batch B of train- ing dataset of images $I = {I_i}$, and finding-location pairs ${FL_i} = {FL_{iReal} \\cup FL_{iFake}}$, we learn a fact-checking model (FC-Model) that separates real pairs $(I_i, FL_{iReal})$ from fake pairs $(I_i, FL_{iFake})$. For this, we learn a suitable representation space in which images are brought close to their real FFL labels and separated from their fake labels using a contrastive encoder. The resulting representations of images and text are combined to learn the veracity of the finding and its location using a regression sub-network. The overall end-to-end architecture of the FC model is il- lustrated in Figure 5.\nMulti-label cross-modal contrastive encoder:\nFor building the encoder, we consider the finding labels of the FL-pairs only as $F_{iReal}$ and $F_{iFake}$ taken respectively from $FL_{iReal}$ and $FL_{iFake}$. Starting from a pre-trained CLIP model on chest X-rays [14, 33], we train its image en- coder (ViT-B/32 Transformer) and its encoder (masked self- attention Transformer) and their projection layers which are"}, {"title": "5. Results", "content": "We now describe several experiments conducted to eval- uate the accuracy and efficacy of the model in error detec- tion and correction of automated reports.\n3.1 Datasets: We selected 5 annotated chest X-ray datasets which had both location and finding annotations for our model evaluations as shown in Table 2. Of these, the train- ing partition of the ChestImagenome silver dataset was the"}, {"title": "6. Discussion & Conclusions", "content": "In this paper, we have presented a new fact-checking ap- proach that detects errors in the identity and location of findings. It also corrects the reports to lead to improved quality in the resulting automated reports. From the results we see that this is possible even when the FC-model itself is not perfect in its prediction accuracy. Furthermore, no customization was needed for the FC model when a differ- ent choice of the report generator is available. Future work will address the current limitations of the model in handling omitted findings from reports. Overall, our paper showed that by carefully constructing synthetic datasets designed to elicit errors, we can develop discriminative models to cor- rect the output of generative models at inference time, a re- sult that may have significance beyond the domain of chest X-rays."}]}