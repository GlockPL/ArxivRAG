{"title": "Model Comparisons: XNet Outperforms KAN", "authors": ["Xin Li", "Zhihong Xia", "Xiaotao Zheng"], "abstract": "In the fields of computational mathematics and artificial intelligence, the need for precise data model-\ning is crucial, especially for predictive machine learning tasks. This paper explores further XNet, a novel\nalgorithm that employs the complex-valued Cauchy integral formula, offering a superior network archi-\ntecture that surpasses traditional Multi-Layer Perceptrons (MLPs) and Kolmogorov-Arnold Networks\n(KANs). XNet significant improves speed and accuracy across various tasks in both low and high-\ndimensional spaces, redefining the scope of data-driven model development and providing substantial\nimprovements over established time series models like LSTMs.", "sections": [{"title": "Introduction", "content": "We initially proposed a novel method for constructing real networks from the complex domain using the\nCauchy integral formula in Li et al. (2024); Zhang et al. (2024), utilizing Cauchy kernels as basis functions.\nThis work comprehensively compares these networks with KANs, which use B-spline as basis functions in\nLiu et al. (2024), and MLPs to highlight our significant improvements.\nMulti-layer perceptrons (MLPs) (Haykin (1994); Cybenko (1989); Hornik et al. (1989)), recognized as\nfundamental building blocks in deep learning, have their limitations despite their wide use, particularly in\nits accuracy, and large number of parameters needed in structures such as in transformers (Vaswani et al.\n(2017)), and lack interpretability without post-analysis tools (Cunningham et al. (2023)). The Kolmogorov-\nArnold Networks (KANs) were introduced as a potential alternative, drawing on the Kolmogorov-Arnold\nrepresentation theorem (Kolmogorov (1956); Braun & Griebel (2009)), and demonstrate their efficiency\nand accuracy in computational tasks, especially in solving PDEs and function approximation (Sprecher\n& Draghici (2002); K\u00f6ppen (2002); Lin & Unbehauen (1993); Lai & Shen (2021); Leni et al. (2013);\nFakhoury et al. (2022)).\nIn the swiftly advancing domain of deep learning, the continuous search for novel neural network de-\nsigns that deliver superior accuracy and efficiency is pivotal. While traditional activation functions such as\nthe Rectified Linear Unit (ReLU) (Nair & Hinton (2010)) have been widely adopted due to their straight-\nforwardness and efficacy in diverse applications, their shortcomings become evident as the complexity of\nchallenges escalates. This is particularly true in areas that demand meticulous data fitting and the solutions\nof intricate partial differential equations (PDEs). These limitations have paved the way for architectures\nthat merge neural network techniques with PDEs, significantly enhancing function approximation capabil-\nities in high-dimensional settings (Sirignano & Spiliopoulos (2018); Raissi et al. (2019); Jin et al. (2021);\nWu et al. (2024); Zhao et al. (2023)).\nTime series forecasting is critical in various sectors including finance, healthcare, and environmental\nscience. While LSTM models are well-regarded for their ability to capture temporal dependencies (Yu et al.\n(2019); Zhao et al. (2017)), KAN models have also shown promise in managing time series predictions\n(Hochreiter & Schmidhuber (1997); Staudemeyer & Morris (2019); Xu et al. (2024)). Our study compares\nthese models, providing insights into their applications and theoretical foundations. We also examine\nthe performance of transformers and our novel XNet model in time series forecasting in the appendix,\nhighlighting their capabilities in managing sequential data (Vaswani et al. (2017); Wen et al. (2023)).\nInspired by the mathematical precision of the Cauchy integral theorem, Li et al. (2024) introduced the\nXNet architecture, a novel neural network model that incorporates a uniquely designed Cauchy activation\nfunction. This function is mathematically expressed as:\n$\\Phi_{\\alpha}(x) = \\frac{A_1 * x}{x^2 + d^2} + \\frac{A_2}{x^2 + d^2},$'\nwhere $A_1, A_2$, and $d$ are parameters optimized during training. This design is not only a theoretical ad-\nvancement but also empirical advantageous, offering a promising alternative to traditional models for many\napplications. By integrating Cauchy activation functions, XNet demonstrates superior performance in func-\ntion approximation tasks and in solving low-dimensional PDEs compared to its contemporaries, namely\nMultilayer Perceptrons (MLPs) and Kolmogorov-Arnold Networks (KANs). This paper will systematically\ncompare these architectures, highlighting XNet's advantages in terms of accuracy, convergence speed, and\ncomputational demands.\nFurthermore, empirical evaluations reveal that the Cauchy activation function possesses a localized\nresponse with decay at both ends, significantly benefiting the approximation of localized data segments.\nThis capability allows XNet to fine-tune responses to specific data characteristics, a critical advantage over\nthe globally responding functions like ReLU.\nThe implications of this research are significant. It has been demonstrated that the XNet can serve\nas an effective foundation for general AI applications, our findings in this paper indicate that it can even\noutperform meticulously designed networks tailored for specific purposes.\nPrincipal Contributions\nOur study elucidates several critical advancements in the domain of neural network architectures and\ntheir applications:\n(i) Enhanced Function Approximation Capabilities: We conduct a comparative analysis between XNet\nand KAN within the context of function approximation, demonstratting the superior performance of\nXNet, particularly in handling the Heaviside step function and complex high-dimensional scenarios.\nDetailed examinations are presented in Sections 3.1 through 3.3, showcasing empirical validations\nthat underscore XNet's robust adaptability across varying dimensions.\n(ii) Superiority in Physics-Informed Neural Networks: Utilizing the Poisson equation as a benchmark, we\ndemonstrate XNet's enhanced efficacy within the Physics-Informed Neural Network (PINN) frame-\nwork. Our results indicate that XNet significantly outstrips the performance metrics of both Multi-\nLayer Perceptron (MLP) and KAN, as detailed in Section 3.5. This investigation not only highlights\nXNet's prowess but also sets a new benchmark for subsequent applications in the field.\n(iii) Innovation in Time Series Forecasting-By innovatively substituting the conventional feedforward\nneural network (FNN) with XNet in the LSTM architecture, we introduce the XLSTM model. In\na series of time series forecasting experiments, XLSTM consistently surpasses traditional LSTM\nmodels in accuracy and reliability, establishing a new frontier in predictive analytics."}, {"title": "Experimental Setup", "content": "Our research is designed to rigorously evaluate the capabilities of KAN and XNet across three fundamental\ndomains: function approximation, solving partial differential equations (PDEs), and time series prediction.\nThis structured evaluation allows us to systematically assess the performance and applicability of each\nmodel in varied computational tasks.\nFunction Approximation: We divide the function approximation experiments based on the dimen-\nsionality and complexity of the functions:\n\u2022 Low-Dimensional Functions: Both irregular and regular functions are tested to evaluate the models'\nability to handle variations in functional behavior and data distribution irregularities.\n\u2022 High-Dimensional Functions: Smooth functions that simulate complex real-world phenomena are\nused to examine the models' generalization in higher-dimensional spaces.\nEvaluation metrics for accuracy, computational efficiency, and convergence are applied to each functional\ntype.\nSolving Partial Differential Equations: We utilize a series of well-known differential equations from\nphysics and engineering to test the efficacy of KAN and XNet. These include:\n\u2022 Both linear and non-linear systems to provide a comprehensive assessment reflective of common\nscientific computing scenarios.\nWe consider the Poisson equation:\n$\\nabla^2v(x,y) = f(x,y), f(x,y) = -2\\pi^2 sin(\\pi x) sin(\\pi y),$\nwith the boundary conditions,v(\u22121,y) = v(1,y) = v(x, \u22121) = v(x, 1) = 0. The PDE has the explict\nsolution, v(x, y) = sin(\u03c0x)sin(\u03c0y), as shown in the figure 2. In the subsection, we aim to compare the\nperformance of three neural network architectures: PINN, KAN, and XNet.\nTime Series Prediction: The proficiency of the models in capturing temporal dynamics and depen-\ndencies is explored through:\n\u2022 The use of both synthetic and real-world time series datasets, which range from financial market data\nto weather forecasting, focusing on predictive accuracy, response time, and robustness at various\ntemporal scales."}, {"title": "RESULTS", "content": "In Section 3.1, we perform the heaviside function approximation tasks using KAN and XNet. In Section\n3.2, we conduct 2D smooth function approximation tasks using KAN and XNet. Section 3.3 evaluates\nthe approximation of high-dimensional functions. In Section 3.4, we employ PINN, KAN, and XNet to\nconstruct physics-informed machine learning models for solving the 2D Poisson equation. In Section 3.5,\nwe apply XNet to improve the performance of LSTM across various scenarios, then compare with KAN."}, {"title": "Heaviside step function apprxiamtion", "content": "The experimental comparison between XNet, B-spline, and KAN demonstrates XNet's superior approxi-\nmation ability. Except for the first example, all other examples are from the referenced article, with KAN\nsettings matching those from the original experiments. This ensures a fair comparison, fully proving that\nXNet has stronger approximation capabilities in various benchmarks.\nAs shown in Figure 6 and 7, both B-Spline and KAN exhibit \"overshoot,\" leading to local oscillations\nat discontinuities. We speculate that this is due to the fact that a portion of KAN's output is represented\nby B-Splines. While adjusting the grid can alleviate this phenomenon, it introduces complexity in tuning\nparameters (see Table 12 in appendix A.1). In contrast, XNet demonstrates superior performance, providing\nsmooth transitions at discontinuities. Notably, in terms of fitting accuracy in these regions, XNet's MSE is\n1,000-fold times smaller than that of KAN."}, {"title": "Function Approximation with exp(sin(\u03c0\u03c7) + y\u00b2) and xy", "content": "The function used is f(x, y) = exp(sin(\u03c0x) + y\u00b2). Following the procedure described in the article, 1,000\npoints were used for training and another 1,000 points for testing. After sufficient training, the model's\npredictions were evaluated on a 100 \u00d7 100 grid. The KAN structure consists of a two hidden layer with\nconfiguration [2, 1, 1], We compare its computational efficiency with the XNet model using two examples:\nexp(sin(\u03c0\u03b1) + y\u00b2) and xy .\nFollowing the official model configurations, XNet with 5,000 basis functions is trained with adam,\nwhile KAN is initialized to have G = 3, trained with LBFGS, with increasing number of grid points ev-\nery 200 steps to cover G = 3, 5, 10, 20, 50. Overall, both networks performed similarly on these two-\ndimensional examples (see Table 3 and 4). However, XNet produced a more uniform fit, with no significant\nlocal oscillations (see Figure 9). In contrast, KAN exhibited sharp variations in certain regions, consistent\nwith the behavior observed in the heaviside step function (see Section 3.1)."}, {"title": "Approximation with high-dimensional functions", "content": "We continue to compare the approximation capabilities of KAN and XNet in solving high-dimensional\nfunctions. Following the procedure described in the article, 8000 points were used for training and another\n1000 points for testing. XNet is trained with adam, while KAN is initialized to have G = 3, trained with\nLBFGS, with increasing number of grid points every 200 steps to cover G = 3, 5, 10, 20, 50.\nFirst, we consider the four-dimensional function exp (\u00bd (sin (\u03c0(x\u00b2 + x2)) + x3x4)). For this case,\nthe KAN structure is configured as [4,4,2,1], while XNet is equipped with 5,000 basis functions. Under the\nsame number of iterations, XNet achieves higher accuracy in less time (see Table 5), the MSE is 1,000-fold\nsmaller than that of K\u0391\u039d.\nNext, we consider the 100-dimensional function exp($\\frac{1}{100}\\sum_{i=1}^{100} sin^2(\\frac{x_i}{2}))$. For this case, the KAN\nstructure is configured as [100,1,1], while XNet has 5,000 basis functions. Under the same number of\niterations, XNet achieved higher accuracy in less time compared to KAN (see Table 6).\nAs dimensionality increases, the computational efficiency of KAN decreases significantly, while XNet\nshows an advantage in this regard. The approximation accuracy of both networks declines with increasing\ndimensions, which we hypothesize is related to the sampling method and the number of samples used."}, {"title": "Possion function", "content": "We aim to solve a 2D poisson equation $\\nabla^2v(x, y) = f(x,y), f(x,y) = -2\\pi^2sin(\\pi x)sin(\\pi y)$, with\nboundary condition v(\u22121, y) = v(1,y) = v(x, \u22121) = v(x, 1) = 0. The ground truth solution is v(x, y) =\nsin(\u03c0x)sin(\u03c0y). We use the framework of physics-informed neural networks (PINNs) to solve this PDE,\nwith the loss function given by\n$loss_{pde} = \\alpha loss_i + loss_b := \\alpha \\frac{1}{N_i} \\sum_{i=1}^{N_i} |v_{xx}(z_i) + v_{yy}(z_i) - f(z_i)|^2 + \\frac{1}{N_b} \\sum_{i=1}^{N_b} v(z_i)^2,$\nwhere we use $loss_i$ to denote the interior loss, discretized and evaluated by a uniform sampling of $n_i$\npoints $z_i = (x_i, y_i)$ inside the domain, and similarly we use $loss_b$ to denote the boundary loss, discretized\nand evaluated by a uniform sampling of $n_b$ points on the boundary. $\\alpha$ is the hyperparameter balancing the\neffect of the two terms.\nWe compare the KAN, XNet and PINNs using the same hyperparameters $n_i = 2500, n_b = 200$, and\n$\\alpha = 0.01$. We measured the error in the $L^2$ norm (MSE) and observed that XNet achieved a smaller error,\nrequiring less computational time, as shown in Figure 13. A width-200 XNet is 50 times more accurate and\n2 times faster than a 2-Layer width-10 KAN; a width-20 XNet is 3 times more accurate and 5 times faster\nthan a 2-Layer width-10 KAN (see Table 7). Therefore we speculate that the XNet might have the potential\nof serving as a good neural network representation for model reduction of PDEs. In general, KANs and\nPINNs are good at representing different function classes of PDE solutions, which needs detailed future\nstudy to understand their respective boundaries."}, {"title": "XNet enhance the LSTM", "content": "Time prediction tasks can generally be categorized into two types: those driven by mathematical and phys-\nical models, and those that are data-driven. In the former, time prediction can often be formulated as a\nfunction approximation problem, while the latter involves noisy data, cannot be easily described by de-\nterministic partial differential equations (PDEs). In this subsection, we introduce the XLSTM algorithm,\nwhich enhances the standard LSTM framework by replacing its feed-forward neural network (FNN) com-\nponent with XNet. Across various examples, XLSTM consistently demonstrates superior predictive perfor-\nmance compared to the traditional LSTM. In the following experiments, we will demonstrate that XLSTM\nalso significantly outperforms the KAN model in noisy time series examples. The KAN implementation\nfor time series prediction is sourced from this repository: https://github.com/Nixtla/neuralforecast\nExample 1: Predicting a Synthetic Time Series\nThe time series is generated by the following equations:\nx = 0.1 * xoxi + 0.1 * sin(xoxi) + sin(x;) + \u03bc\u00b2, i = 1, 2, ..., n\nand\nx = x1, x\u2081 = x2\u22121, x2 = x\u00b2-1, x = x-1,\nwhere the initial conditions 20, x1, x2, x3, x2 ~ rand(0,0.2) are randomly sampled in the range [0,0.2],\nand the noise term \u03bc\u00b2 is sampled from a normal distribution, \u03bc\u00b2 ~ N(0, noise). This generates a time\nseries {f' = x;}i=1,...,n, with n = 200. In this example, the time series is governed by relatively simple\nfunctions. The task of predicting the sixth data point using the first five data points becomes a high-\ndimensional function approximation problem.\nFigures [15] and [16] show a comparison of the predictive performance of LSTM and XLSTM on two\nscenarios: one with no noise (noise = 0) and one with moderate noise (noise = 0.05). The results indicate\nthat XLSTM significantly outperforms LSTM in both settings, particularly under non-noisy conditions.\nWhen there is no noise, XLSTM achieves an MSE of 3.4252 \u00d7 10-11, which is lower than that of LSTM\n(1.5925 \u00d7 10-7). Similarly, XLSTM's RMSE and MAE are drastically lower than LSTM's, while the\ncomputation time remains comparable. In the presence of moderate noise (noise = 0.05), although XLSTM\ndoes not show a significant advantage in metrics such as MSE, it is clear from Figure (15) that XLSTM\ncaptures the underlying patterns of the data better than LSTM."}, {"title": "Summary and Outlook", "content": "1. XNet vs. KAN for Function Approximation Recently, KAN has gained popularity as a function\napproximator. However, our experiments demonstrate that XNet outperforms Kan, particularly when ap-\nproximating discontinuous or high-dimensional functions."}, {"title": "A Appendix", "content": "The numerical experiments presented below were performed in Python using the Tensorflow-CPU proces-\nsor on a Dell computer equipped with a 3.00 Gigahertz (GHz) Intel Core i9-13900KF. When detailing grids\nans k for KAN models, we always use values provided by respective authors (Kan)."}, {"title": "A.1 FUNCTION APPROXIMATION", "content": "For 1d heaciside function, we set different configurations. The results are shown as follows\nFor 2d functions, loss function\nfor high-dimensional functions, loss functions"}, {"title": "A.2 Time sereis", "content": "In Section 3.5, we present two examples to forecast future unknown data using LSTM and XLSTM. In the\nfunction-driven example (15), the loss functions of LSTM and XLSTM are shown in Figure 21; for the\ntask of predicting Apple\u00c3\u00a2\u00c2\u20ac\u00c2\u2122\u2122s stock price, the loss functions of LSTM and XLSTM are illustrated in\nFigure 22."}, {"title": "A.3 Time series", "content": "There exists two types of time prediction applications. One is driven by mathematical and physical models,\nwhere time prediction can essentially be viewed as a function approximation. The other is data-driven,\nwhere the data often contains significant noise and cannot be easily described by PDEs. In this section, we\nintroduce the XLSTM algorithm, which replaces the FNN component in the standard LSTM framework\nwith XNet. In the following examples, XLSTM consistently demonstrates superior predictive performance."}, {"title": "A.4 high-dimensional function 1", "content": "The time series is generated by the following equations:\nx = 0.1 * x * x + 0.5 * sin(x * x) + 1 * sin(x4), i = 1, 2, ..., n\nand\nx = x1, x\u2081 = x1, x2 = x1, x\u2081 = x-1,\nwith\nxo, x1, x2, x3, xq ~ rand(0, 0.2).\nThis generates the time series {f\u00b2 = x;}i=1,...,n. We consider the data n=200. In this example, the time\nseries is driven by simple functions. Specifically, when the task is to predict the sixth data point using the\nfirst five, it essentially becomes a high-dimensional function approximation problem.\nWe first split the data into a training set (80%) and a validation set (20%) and performed predictions\nusing different models including 2-Layer width-10 FNN, 1-layer width-10 LSTM, width-10 XNet and\nwidth-10 XLSTM.\nFor each training iteration, the first five data points were used as input, and the model predicted the sixth\ndata point, which was then compared with the target values. After five thousand iterations, the training\nprocess was considered complete. On the test set, we used the first five data points as input to predict the\nsixth, sliding through the sequence until all predictions were made. In essence, this can be viewed as a\nfunction-fitting problem.\nXLSTM demonstrates stronger predictive capabilities compared to standard LSTM. With the same\ntraining cost, XLSTM improves accuracy by a factor of fifty."}, {"title": "A.5 electric power", "content": "In this experiment, the time series represents electricity consumption in Zone 1 of the United States, with\nthe test period from 01/01/2017 00:00 to 01/14/2017 21:20. The data is sourced from https://www.kaggle.com/datasets/fedesoriano/e\npower-consumption. The 2,000 data points are divided into two parts: 1,602 for training and 398 for testing.\nDuring training, the model takes the first 10 data points as input and predicts the 11th, comparing it with\nthe target.\nXNet enhanced transformer and Istm model. transformer has little advantage in this case"}]}