{"title": "Personalized Federated Learning for Cross-view Geo-localization", "authors": ["Christos Anagnostopoulos", "Alexandros Gkillas", "Nikos Piperigkos", "Aris S. Lalos"], "abstract": "In this paper we propose a methodology combining Federated Learning (FL) with Cross-view Image Geo-localization (CVGL) techniques. We address the challenges of data privacy and heterogeneity in autonomous vehicle environments by proposing a personalized Federated Learning scenario that allows selective sharing of model parameters. Our method implements a coarse-to-fine approach, where clients share only the coarse feature extractors while keeping fine-grained features specific to local environments. We evaluate our approach against traditional centralized and single-client training schemes using the KITTI dataset combined with satellite imagery. Results demonstrate that our federated CVGL method achieves performance close to centralized training while maintaining data privacy. The proposed partial model sharing strategy shows comparable or slightly better performance than classical FL, offering significant reduced communication overhead without sacrificing accuracy. Our work contributes to more robust and privacy-preserving localization systems for autonomous vehicles operating in diverse environments.", "sections": [{"title": "I. INTRODUCTION", "content": "Cross-view Geo-localization can provide a robust solution to the limitations of traditional GPS-based localization for vehicles. While GPS is generally reliable, it can fail in certain scenarios such as dense urban environments or when network connectivity is lost [1]. In these cases, relying solely on GPS can lead to inaccuracies and inconsistencies in vehicle localization. This can be accomplished by matching points of interest between the different views, the ground view and the satellite image. The arrival of Deep Learning (DL) techniques in feature extraction and their predominance over handcrafted techniques facilitated a new era characterized by improved accuracy and efficiency in processing and interpreting com- plex visual data, enabling more reliable Cross-view Geo- localization [2], [3].\nData-driven models require large volumes of raw data for training something which can pose challenges advantages in terms of privacy and security [4]. On the other hand, Federated Learning frameworks offer a promising solution by enabling models to be trained in a decentralized manner without the need to exchange local raw data, thus providing advantages in terms of data privacy and resource allocation [5].\nHowever, models participating in an FL training scheme often face challenges due to heterogeneous datasets, as autonomous vehicles typically operate in diverse environments [6]. \u03a4\u03bf address this issue of local dataset heterogeneity, we propose a more personalized federated learning scenario. This ap- proach implements a coarse-to-fine methodology, enforcing each client to selectively share only parts of its local model pa- rameters. By doing so, we can better accommodate the unique characteristics of each vehicle's operating environment while still benefiting from collaborative learning. Furthermore, the aggregations of the components of local models responsible for extracting coarse, general features, which are more uni- versal across different environments, can effectively improve overall model robustness and produce patterns that benefit all participants. Meanwhile, fine-grained feature extractors remain specific to each vehicle, capturing unique characteristics of local environments (e.g., urban vs. rural areas). This prevents potential performance degradation that could occur from ag- gregating highly specialized feature. In addition, by selectively sharing only the coarse feature extractors, our approach not only enhances model performance but also improves compu- tational efficiency by reducing the amount of data transmitted during the federated learning process. The main contributions of this paper can be summarized as follows:\n\u2022 We propose the first methodology that combines fed- erated learning with Cross-view image Geo-localization techniques. Our approach leverages deep neural networks to extract robust features from both satellite and ground- view images in a distributed setting, resulting in more accurate models that are resilient to view changes and discriminative for feature correspondences.\n\u2022 We introduce a personalized federated learning scenario to address dataset heterogeneity from autonomous vehi- cles in diverse environments. Our approach allows clients to selectively share parts of their local model parame- ters using a coarse-to-fine methodology. We aggregate components responsible for extracting general features, enhancing model robustness, while keeping fine-grained features specific to each vehicle to capture unique local characteristics. This prevents performance degradation from aggregating specialized features and improves com- putational efficiency by reducing data transmission.\n\u2022 We conduct a thorough evaluation of the proposed fed-"}, {"title": "II. PRELIMINARIES", "content": "Cross-view Geo-localization addresses the problem of esti- mating a mobile agent's position, similar to Visual Odometry, but with a key distinction. While Visual Odometry matches image sequences from the same sensor, CVGL aims to match images from different sensors, and more specifically, the mobile agent's camera and geo-tagged aerial imagery. This approach leverages the complementary information provided by ground-level and aerial perspectives to determine the agent's location, offering a robust alternative to traditional localization methods, especially in scenarios where GPS might be unreliable or insufficient.\nResearch in this area typically focuses on two main prob- lems, image retrieval techniques to match a ground view image to a geo-tagged aerial image, providing a rough estimate of the agent's position, and techniques using an initial rough pose estimation to determine the agent's pose with higher accuracy, which is also the problem we focus on in this study. Ini- tially, CVGL methods relied on pixel-wise techniques [7], [8]. However, with the advent of feature-based methods, there was a shift towards related approaches [9]. Following the rise of Deep Learning (DL) in computer vision, numerous DL-based works in CVGL have been presented, further advancing the field. Some implementations that can be found in the literature utilize technologies like Convolutional Neural Networks [2], [10], [11], Transformer methods [3], [12], [13], and Generative Adversarial Networks based methods [14], [15]."}, {"title": "B. Federated Learning in Automotive Domain", "content": "Although federated learning has been extensively explored in various fields such as image processing and computer vi- sion, its application in the area of autonomous driving remains largely under-investigated. The current body of literature presents only a handful of studies investigating the potential benefits of federated learning in this domain. For instance, study [16] utilized a federated learning framework to address object detection in automotive environments, achieving perfor- mance on par with centralized deep learning while enhancing the speed of local model training. Similarly, studies [17] and [18] utilized the FedAvg approach to develop precise local models for predicting wheel steering angles in autonomous vehicles.\nAdditionally, studies [19] and [16] delved into the theoreti- cal aspects of federated learning in vehicular networks, exam- ining the training processes of local models, data distribution challenges, and the non-i.i.d. characteristics of autonomous driving datasets. Studies in [20], [21] provide a federated learning framework to tackle the lidar super-resolution prob- lem, thus enhancing the quality of a low-cost sensors utilizing lidar data from different autonomous vehicles under different environmental conditions. Furthermore, in [22], the authors applied federated learning principles to train a deep learning- based feature detector, integrated it into a SLAM system and evaluated its performance using odometry metrics. Lastly, study [23] focused on semantic segmentation through feder- ated learning, establishing a benchmark platform that incor- porated two datasets and multiple leading federated learning algorithms. To our knowledge, this study is the first to combine Cross-view Geo-localization with Federated Learning, intro- ducing a personalized FL scenario. In this approach, clients selectively share parts of the Deep Learning-based feature extractor using a coarse-to-fine methodology, aggregating only the components responsible for extracting general features."}, {"title": "III. METHODOLOGY", "content": "The problem of Cross-view Geo-localization can be classi- fied under two types. The first type involves a ground view query image Q with unknown geographic coordinates, which must be matched against a set of aerial images {I1, I2, ..., In} that are geotagged, resulting in a coarse ego vehicle's position estimation. The second type, which is going to be examined in this study, uses an initial rough pose estimation and relates the ground view image with an already selected aerial image centered on the initial pose, resulting in predictions far more accurate in terms of translation and rotation.\nThe relation of a pixel in the ground view image, given that it uses a pinhole camera model, and the a point in the camera's world coordinate system is\n$x_g = PX_w$\nwhere xg and Xw are the positions of the point in ground view image and the world using homogeneous coordinates,\nP = K(R| \u2212 Rt) is the camera matrix, K is the camera's calibration matrix, and R, t are the camera's rotation and translation in the world coordinate system. In addition Xw can be mapped to a pixel in the aerial image $x_s = f(X_w,a)$, where f : R3 \u2192 R2 is the projection function to the aerial image plane, xs is the position of the point in the aerial image, and a is the real world distance of each pixel of the aerial map. Therefore, we define a projective relationship between"}, {"title": "B. Federated Learning CVGL", "content": "To define the CVGL federated learning problem, we con- sider a network with N autonomous vehicles (agents), each possessing a local private dataset of size kj which can be defined as:\n$D_j = \\{I_s^i, I_g^i, R_{gt}^i\\}$\nwhere j\u2208 {1,..., N}, i \u2208 {1,...,kj}, $I_s^i \u2208 R^{H\u00d7W\u00d7C_s}$ and $I_g^i \u2208 R^{H_g\u00d7W_g\u00d7C_g}$ represent the satellite images and the ground view images, $R_{gt}^i \u2208 R^{3\u00d73}$ be the ground truth rotation matrix and and $t \u2208 R^3$ represents the ground truth translations. Here, H, and W, are the height and width of the satellite images, respectively, and Cs is the number of channels. Similarly, Hg and Wg are the height and width of the ground view images, respectively, and Cg is the number of channels.\nWithin the federated learning framework, these devices collab- oratively train a global model, orchestrated by a central server, which minimizes the aggregation of local objectives.\n1) Client side: Each client n utilizes its privately owned dataset Dj to optimize a local deep learning-based CVGL model based on the work presented in [24]. More specifically, the model illustrated in Figure 1 can be divided into two components, similar to a typical vSLAM system: the front- end and the back-end.\nThe front-end consists of two parallel VGG-based branches [25] following an autoencoder architecture. Although these deep learning-based extractors are identical in structure, they do not share the same weights, allowing them to better adapt to their respective domains. In order to be less sensitive to object distortions we employ feature maps, which can encode high-level information, instead of RGB values. Therefore, both branches are responsible for simultaneously extracting multiple feature maps from different layers of the networks, denoted as $M_s^l \u2208 R^{H\u00d7W\u00d7C_s}$, for the satellite images and $M_g^l \u2208 R^{H\u00d7W\u00d7C_g}$ for the ground view images, where l indicates the layer of the network.\nThis integration of features from different layers ensures that the model benefits from a rich, multi-scale feature repre- sentation, which is essential for effectively processing and correlating satellite and ground view images. Subsequently, the extracted satellite features are projected to the ground view plane, resulting to a new feature map $M_{2g}^l\u2208 R^{H\u00d7W\u00d7C_g}$ and subsequently the local loss function for each client j, becomes\n$\\ell_j^i = ||M_{2g}^i - M_{g}^i||$\nThe projection function depends on the pose of the camera and uses the homography of the ground plane making the approximation that the majority of the points between the two views lie on the ground plane.\nThe backend runs an optimization process that refines the estimated pose, ensuring precise alignment between the ground and satellite images through non-linear optimization that minimizes the difference between projected points. This implementation employs the Levenberg-Marquardt (LM) [26] optimization algorithm to solve the resulting non-linear least squares problem. The LM optimization is implemented in a differentiable manner, operating in a feed-forward pass in a coarse-to fine methodology. At each level l, it is computed the Jacobian and the Hessian matrix\n$\\frac{\\partial M_{2g}^i}{\\partial \\xi} = J = \\frac{\\partial M_{2g}^i}{\\partial p_s^i} . \\frac{\\partial p_s^i}{\\partial \\xi^i} = (J_1).(J_2)^T$\nwhere ps are satellite feature map coordinates. In each al- gorithmic iteration we have as small pose update $\u2206\u03be =(H^l_i)^{-1}(J^l_i)^T e$ which results in an updated pose:\n$\u03be_{j}^{l+1} = \u03be_j^l + \u2206\u03be$\nwhere $\u2206\u03be = \\{\u2206R^l, \u2206t^l\\}$. The optimization is applied at the coarsest feature level and then progressively advances to finer levels, resulting in an optimization process executed as many times as the number of feature maps extracted for each view. The inputs of each step are the corresponding feature maps for the layer l + 1 and the estimated rotation and translation from the previous layer. This is repeated iteratively until convergence is achieved or a maximum of 7 iterations is reached.\nThe network is trained end-to-end using ground truth (GT)"}, {"title": "camera poses for supervision. The loss function is defined as follows:", "content": "$L_j = \u03a3\u03a3 (||R_{lt}^{ji} - R^i|| + ||t_{lt}^{ji} - t^i||)$ where $R_{lt}^{ji}$ and $t_{lt}^{ji}$ are the predicted poses at the t-th iteration and l-th level, respectively, and $R^i$ and $t^i$ are the GT camera poses for client j.\n2) Server-side: On the server-side, the primary objective is to compute a global model by integrating the local models received from the participating autonomous agents as illus- trated in Figure 2. Specifically, the server aggregates the local models to produce a new global model, denoted as $\u03b8_g$, through a weighted average fusion method [27]. The mathematical formulation for this process is as follows:\n$\u03b8_g = \\frac{1}{N} \\Sigma_{j=1}^{N} w_j \u03b8_j$ In equation (6), $w_j$ represents the weight corresponding to the size of the local dataset of the j-th device. This weight ensures that models trained on larger datasets have a proportionately greater influence on the global model. The weighted average fusion approach is critical as it balances the contribution of each local model, taking into account the variability in dataset sizes across different devices. After the local models are merged using this method, the centralized server disseminates the updated global model back to all participating devices. This distribution allows each device to update its local model with the newly computed global parameters, ensuring that all agents benefit from the collective training process.\nThe process of aggregating and distributing the global model is repeated for T communication rounds. During each round, local models are trained, aggregated, and redistributed, pro- gressively refining the global model. This iterative process continues until the global model converges to an optimal solution.\nWe consider two scenarios for model exchange: (a) Full Model Sharing, where he entire model is exchanged between clients and (b) Partial Encoder Sharing where the clients share only the encoders of their local model parameters, employing a coarse-to-fine approach aiming at the adaption to diverse data distributions, and the optimization of computational resources."}, {"title": "IV. EXPERIMENTS", "content": "We used the KITTI raw dataset [28] combined with satellite images retrieved from Google Maps, following the procedure described in [2]. The dataset was split into five parts, each containing over 3000 images with non-overlapping sequences. The testing dataset comprises images from a different area to ensure generalization, allowing us to evaluate the model's performance in unseen environments."}, {"title": "B. Metrics", "content": "We use the metrics proposed in [2] for the evaluation of the estimated pose in the lateral and longitudinal directions as well as orientation. An estimation is considered successful if the value is below certain thresholds. Specifically, for translation, in the lateral or longitudinal direction, the threshold d is set to 1, 3, and 5 meters, and for the azimuth angle, the threshold is set to 1\u00ba, 3\u00ba, and 5\u00b0. The evaluation also includes combined metrics, such as latitude below 1 meter and azimuth below 1 degree."}, {"title": "C. Experimental Setup", "content": "In the first two scenarios, we examined two extreme cases where both models followed traditional centralized training but with different data accessibility. The first model had access to all data from all agents, while the second model had access to"}, {"title": "V. CONCLUSION", "content": "This paper presents a novel combination of Federated Learn- ing with Cross-view Geo-localization to address the limitations of GPS-based localization in autonomous vehicles. The pro- posed coarse-to-fine approach show significant improvements over single-client training and perform close to centralized training. This highlights the potential of FL to balance data privacy concerns with the need for robust, generalizable models. Overall, our work contributes to the advancement of collaborative learning in autonomous driving systems, paving the way for more robust and adaptable localization methods. Future work could explore the scalability of the approach with larger numbers of clients, and the integration of this method with other sensors and localization techniques."}]}