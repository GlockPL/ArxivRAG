{"title": "FlowLearn: Evaluating Large Vision-Language Models on Flowchart Understanding", "authors": ["Huitong Pan", "Qi Zhang", "Cornelia Caragea", "Eduard Dragut", "Longin Jan Latecki"], "abstract": "Flowcharts are graphical tools for representing complex concepts in concise visual representations. This paper introduces the FlowLearn dataset, a resource tailored to enhance the understanding of flowcharts. FlowLearn contains complex scientific flowcharts and simulated flowcharts. The scientific subset contains 3,858 flowcharts sourced from scientific literature and the simulated subset contains 10,000 flowcharts created using a customizable script. The dataset is enriched with annotations for visual components, OCR, Mermaid code representation, and VQA question-answer pairs. Despite the proven capabilities of Large Vision-Language Models (LVLMs) in various visual understanding tasks, their effectiveness in decoding flowcharts-a crucial element of scientific communication-has yet to be thoroughly investigated. The FlowLearn test set is crafted to assess the performance of LVLMs in flowchart comprehension. Our study thoroughly evaluates state-of-the-art LVLMs, identifying existing limitations and establishing a foundation for future enhancements in this relatively underexplored domain. For instance, in tasks involving simulated flowcharts, GPT-4V achieved the highest accuracy (58%) in counting the number of nodes, while Claude recorded the highest accuracy (83%) in OCR tasks. Notably, no single model excels in all tasks within the FlowLearn framework, highlighting significant opportunities for further development.", "sections": [{"title": "Introduction", "content": "Flowcharts are visual tools that simplify complex processes and concepts across various domains, condensing intricate information into concise visual representations that enhance both comprehension and communication. In this paper, a flowchart is defined as a diagram that outlines a sequence of operations using standardized symbols like rectangles for steps and arrows to indicate process flow, as demonstrated in Figure 1.\nFlowchart comprehension, particularly in the domain of computer vision and LVLMs, remains underexplored despite their extensive application. Resources like ACL-Fig [13] that include scientific flowcharts are limited and often provide only basic figure captions and sparse inline reference annotations. Flowcharts' complex nature-requiring text recognition, identification of various visual elements (e.g., boxes, nodes, symbols), and understanding of node connections-demands more comprehensive annotations for effective evaluation, emphasizing the need for specialized resources.\nFurther underscoring the inadequacy of current resources, our preliminary analysis of 208 flowcharts from ACL-Fig using Gemini-Pro-Vision [9] yielded a low BLEU score of 0.006 (refer to supplementary material for details). However, this score doesn't fully represent the model's comprehension capabilities. The captions in ACL-Fig, sometimes as brief as 'Figure 2: Alignment learning algorithm', do not provide robust ground truths for meaningful evaluation. With a median caption length of just nine words, ACL-Fig is inadequate for evaluating flowchart understanding.\nAddressing these gaps, we introduce the FlowLearn Dataset\u00b9, which includes both scientific and simulated flowcharts. The scientific subset features 3,858 flowcharts sourced from scientific literature, annotated with captions (median length of 25 words) and in-figure text. The simulated subset consists of 10,000 flowcharts generated from Mermaid code. This simulated subset enhances the dataset by providing detailed annotations of visual components, thereby enabling quantitative evaluations of component-specific tasks. Additionally, both subsets include Visual Question Answering (VQA) question-answer pairs, further enhancing their utility for training and model evaluation.\nIn addition to introducing a novel dataset tailored for enhancing flowchart comprehension, this paper provides a rigorous analysis of the performance of contemporary LVLMs in interpreting flowcharts. Our findings reveal significant room for improvement in LVLMS, with no single model excelling across all tasks within the FlowLearn framework. For instance, in tasks involving simulated flowcharts, GPT-4V achieved the highest accuracy (58%) in counting the number of nodes, while Claude recorded the highest accuracy (83%) in OCR tasks. This varied performance highlights specific areas where LVLM capabilities could be further developed. Given the rapid advancements in the fields of Large Language Models (LLMs) and LVLMS, FlowLearn is both timely and valuable, providing a foundation for future research in visual data interpretation and automated reasoning, and setting new benchmarks in the field."}, {"title": "Related Works", "content": "This section overviews interdisciplinary research at the intersection of computer vision and natural language processing, focusing on the comprehension of visual figures in scientific contexts."}, {"title": "Scientific Figure Datasets", "content": "Significant efforts have been made to develop methodologies and datasets aimed at extracting and understanding scientific figures.\nMethods such as PDFigCapX [16], PDFMEF [28], PDFFIGURES [6], and PDFFIGURES2 [5] facilitate the extraction of figures, captions, and related information from scholarly articles. Whereas datasets like VIS30K [4] and PDFFIGURES2 advance figure extraction by providing detailed annotations for figure locations. Moreover, ACL-Fig [13] and DocFigure [12] focus on figure classification, enhancing the understanding of various figure types, including bar charts and architecture diagrams. Additionally, specialized datasets like SciCap [11] and Parsing-AUC [24] concentrate on image captioning and summarization for experimental results figures.\nDespite advancements, a significant gap exists in datasets with detailed annotations for flowcharts. For instance, ACL-Fig contains only about 208 flowcharts, primarily as architecture diagrams and neural networks, often duplicated, with brief captions or blurry figures from pre-2000 papers. CSDia [27] focuses on logical diagrams but lacks detailed caption information as it is not derived from the scientific literature."}, {"title": "Visual Understanding Resources and Methods", "content": "Research in image captioning [21, 34], particularly in scientific chart image captioning [8], has seen significant advancements, exemplified by works such as Parsing-AUC [24], which combines figure semantics extracted via OpenCV with textual information from the main text to generate comprehensive figure summaries for AUC figures. The field of VQA [14, 1] has progressed substantially, with datasets like VL-ICL Bench [35] providing benchmarks for multimodal in-context learning. In scientific contexts, CSDia [27] employs models based on Diagram Parsing Nets to tackle VQA tasks. In scientific contexts, FigureSeer [26] automates figure localization, classification, and summarization. It extracts key visual components such as axes, legends, and data points for analysis, emphasizing the importance of figure decomposition in enhancing the understanding of scientific figures.\nIn non-scientific contexts, innovative approaches such as Neural-Symbolic VQA [29] and ILP [25] have been developed to transform neural network outputs into symbolic representations, which are then used to formulate answers. Additionally, the research highlighted in [15] stresses the importance of incorporating structural information in visualization retrieval processes. Their findings indicate a marked preference among survey participants for evaluating similarity based on visual elements rather than merely pixel-level details, suggesting a deeper, more structural approach to image analysis that could greatly benefit VQA systems.\nSeveral works have focused on extracting objects and their relationships from scientific figures. Notably, [7] pioneers the conversion of scientific equation images into LaTeX format. ChartDetective [20] introduces an interactive application for converting result chart images into SVG, preserving semantics and component relationships. However, it is essential to note that this approach relies on user interaction and takes about 4 minutes for a single conversion. For non-scientific domains, Flow2Code [10] converts hand-drawn flowcharts to simple code by using object detection and rules.\nThe development of LVLMs has marked a significant leap in visual understanding, with models capable of integrating advanced vision techniques with LLMs. These models can learn simultaneously from images and texts, tackling various tasks such as visual question answering and image captioning. The OpenCompass Multimodal Leaderboard\u00b2 rank these LVLMs, includes entries such as GPT-4V [33], Gemini [9], LLaVA [18], Claude [2], InternLM [31], Qwen-VL [3], Step-1V\u00b3, and DeepSeek [19]. All of these models were trained on diverse datasets, including datasets for VQA, optical character recognition (OCR), and academic-related VQA. These are essential building blocks for achieving flowchart comprehension and enhancing the understanding of scientific flowcharts. However, among these models, only DeepSeek explicitly stated that its in-house training datasets included flowchart-related VQA. This highlights a potential area for future research and development, suggesting that incorporating more flowchart-specific data may enhance model training and performance.\nIn conclusion, there is a notable gap in resources specifically tailored for flowchart comprehension. Existing datasets and methods primarily focus on general scientific figures without addressing the unique complexities of flowcharts. Our FlowLearn dataset fills this gap by providing detailed annotations for flowcharts and evaluating LVLMs' ability to interpret these diagrams. By enhancing LVLMs' understanding of flowcharts, our work extends existing knowledge and introduces crucial resources for research aimed at enhancing automated visual reasoning and comprehension."}, {"title": "FlowLearn Dataset", "content": "To address the scarcity of resources for flowchart comprehension, we introduce the FlowLearn Dataset. This dataset comprises two distinct subsets: Scientific Flowcharts and Simulated Flowcharts. An overview of the FlowLearn dataset, illustrating its components, is depicted in Figure 1. Table 1 details the common VQA tasks applicable to both subsets, while Table 2 lists the VQA tasks that are unique to the Simulated Flowcharts subset."}, {"title": "Scientific Flowchart Dataset", "content": "The Scientific Flowcharts Dataset comprises a large collection of flowchart images extracted from scholarly articles across diverse scientific domains. This dataset serves as a crucial resource for enhancing visual comprehension of scientific content.\nWe initiated our dataset creation by downloading 27,000 scientific articles from ArXiv. Using PDFFigures 2.0 [5], we extracted figures and related metadata. Additional metadata were parsed by the SciPDF Parser\u2074, which utilizes GROBID for parsing PDFs.\nOur selection process involved rule-based filtering combined with manual verification to identify flowcharts. We selected figures based on keywords relevant to flowcharts in captions, such as \u201cillustration\", \"flowchart\", \"model\", \"step\", \"overall\", and \"graphical representation\". Figures with unrelated keywords like \"normalized\" and \"plot\" were omitted. This meticulous curation yielded 3,858 flowcharts from 2,674 documents, focusing on images that prominently feature arrows, indicative of flowchart structures.\nEach flowchart in our dataset is accompanied by comprehensive metadata, exemplified in Figure 1. The Scientific Paper Meta includes parsed text from the source articles, along with related information such as authors and titles. The Figure Meta encompasses the figure caption and the in-text reference of the figure. Additionally, we annotated all text appearing within each flowchart using PaddleOCR [30]. These annotations support various subtasks crucial to flowchart comprehension, including OCR and flowchart description."}, {"title": "Simulated Flowcharts", "content": "Recognizing that understanding flowcharts goes beyond caption generation, we developed the Simulated Flowcharts subset to enhance comprehension of diagrammatic components like arrows and nodes, which can be labor-intensive to annotate in scientific diagrams.\nThis subset was generated using Mermaid, a JavaScript tool that translates Markdown-inspired text definitions into flowcharts. Sample Mermaid code can be seen in Figure 1 and Table 2. We utilized Python scripts to introduce variability in the flowchart definitions in terms of the following aspects: 1) Nodes: Each flowchart contains between 3 to 10 nodes, with node text consisting of randomized English words. 2) Links: The number of links between nodes is randomized, with all nodes connected by at least one link, mimicking real-world flowchart structures. We randomize the type of arrow links between nodes, including solid lines, bold lines, or dashed lines. 3) Background Color and Flowchart Orientation: Background colors are randomly generated in hexadecimal format. The orientation of the flowcharts is randomized, encompassing all available options in the Mermaid syntax.\nWe generated a total of 10,000 samples. Each sample includes: 1) Flowchart Images: Available in JPEG and SVG formats. 2)Mermaid Code: Provided for each sample to facilitate programmatic understanding and manipulation of the flowchart structure. 3) Visual Component Annotations: Detailed annotations are provided, which include the node text and the precise locations of text nodes, arrowheads, and tails, all derived from SVG. These annotations are crucial for tasks such as object detection and structural analysis, enabling a deeper understanding of the flowchart components.\nThe generation script provides fine-grained control over the creation of simulated flowchart samples, enabling integrated training and experimentation for a wide range of applications."}, {"title": "Visual Question Answering", "content": "To evaluate the flowchart understanding capabilities using the FlowLearn dataset, we developed tailored VQA question-answer pairs for each tested flowchart. Examples of prompts, questions and answers for each task are detailed in Table 1 and Table 2. We have ensured that all prompts are elaborately detailed based on findings from VL-ICL [35], which demonstrated that more detailed prompts significantly enhance VQA performance compared to shorter ones. Our own experiments confirm this finding, as we observed that detailed prompts consistently outperform shorter ones in eliciting accurate responses from models.\nThe common VQA tasks for both subsets include:\nOCR: We randomly place a red box over one of the annotated texts within the flowchart and prompt models to identify and return the enclosed words.\nTrue/False: We generate statements related to the flowchart and query the model to determine their veracity. For Scientific Flowcharts, we initially create two accurate statements using sentences from the figure caption, subsequently verified by annotators for their correctness based on the flowchart. In cases with insufficient caption data, annotators generate additional statements relate to the flowchart. For false statements, annotators alter a few words in a true statement to reverse its meaning, ensuring the vocabulary remains consistent with the original author's style. This process yields one true and one false statement for each tested scientific flowchart. For Simulated Flowcharts, we use predefined templates to create True and False statements, such as: \"An arrow exists between node '{a}' and node '{b}'\" and \"An arrow points from node '{a}' to node '{b}'.\" where {a} and {b} are placeholders for node texts identified in Visual Component Annotations (Section 3.2).\nDescription: We prompt models to generate descriptions for the flowcharts. For scientific flowcharts, the reference answers are derived from their captions; for simulated flowcharts, reference answers are generated by converting mermaid code to sentences using templates, \"{a} points to {b}.\"\nAdditionally, the Simulated Flowcharts includes 3 unique tasks:\nMermaid Code: Models are tasked with generating Mermaid code that represents the flowchart. This task assesses the model's ability to comprehensively recognize flowchart components, including text nodes and arrows.\nNumber of Nodes and Arrows: Models answer questions regarding the count of text nodes and arrows present in the flowchart. This task offers a quantitative measure of the model's comprehension, though it is less comprehensive than the Mermaid Code task."}, {"title": "Experiment Setups", "content": "In this section, we detail the experimental setup used to assess the capabilities of various Large Vision-Language Models (LVLMs) using the FlowLearn Dataset. Our primary objective is to evaluate how effectively these models comprehend and interpret flowcharts from both the Scientific and Simulated subsets. We have implemented all VQA tasks outlined in Section 3.3, which probe various facets of flowchart comprehension-from fundamental text recognition to more comprehensive overall understanding."}, {"title": "Models", "content": "We selected LVLMs for evaluation based on their rankings in the OpenCampass multi-modal leaderboard as of April 2024. Access to some models was facilitated through APIs, including Step-1V-32K, GPT-4V, Gemini-Pro-Vision, and Claude-3-Opus-20240229. Additional models assessed in our study were LLaVA-V1.6-Vicuna-34B, InternLM-XComposer2-VL-7B, Qwen-VL-Chat from 2024/01/25, and DeepSeek-VL-7B-chat. Our selection strategy aimed to choose the best model available from each top-ranked model family, such as selecting Claude-3-Opus from the Claude series."}, {"title": "Evaluation Metrics", "content": "To evaluate the performance of the models, we categorized the VQA tasks into three groups, each assessed by tailored evaluation metrics:\nAccuracy: We measure the accuracy for tasks including OCR, True/False Statements, Number of Nodes, and Number of Arrows. This metric is straightforward and evaluates whether the responses are correct or incorrect based on the ground truth. Specifically for True/False Statements, we calculated average accuracy separately for the true and false subsets, and an overall average accuracy to provide a comprehensive view of model performance.\nSimilarity: For description tasks, we assess the closeness of model-generated descriptions to reference descriptions using four similarity metrics: BLEU [22], ROUGE-L [17], BERT score [32] and Sentence Transformer Similarity (SBERT) [23]. The BERT score utilizes pre-trained BERT embeddings to assess semantic coherence through cosine similarity of matched words. Similarly, SBERT converts both response and reference sentences into embeddings with the 'all-MiniLM-L6-v2' model, using cosine similarity to quantitatively gauge how closely the generated text matches the target description. We also calculate median word count of responses.\nMermaid Code Generation: We developed two sets of metrics specifically tailored for evaluating the correctness of generated Mermaid code:\n\u2022 Node-Level Evaluation: This metric checks if all nodes present in the ground truth are included in the model's response. Each node is only considered correct if it exactly matches the spelling in the ground truth.\n\u2022 Link-Level Evaluation: This metric assesses the generated response includes all the links present in the ground truth. A link is deemed correct if both the start and end nodes are accurately predicted, regardless of the arrow type. We also permit some syntactical flexibility in how node descriptions are expressed, allowing the use of either the node variable name or the node text.\nFor both evaluation levels, we compute F1-score, precision, and recall for each sample and average these metrics across all samples."}, {"title": "Response Parsing", "content": "Given the variability in how LVLMs generate responses, which may not always exactly match the ground truth even when correct, we have developed specific rules to parse and evaluate the responses:\nOCR: A prediction is deemed correct if it includes the exact phrase from the ground truth.\nTrue/False Statements: The response is assessed for the presence of 'true' or 'false', case-insensitively. Responses lacking these tokens or containing both are marked as incorrect.\nNumber of Nodes or Arrows: We extract the first numeric token in the response, also converting English words representing numbers into numeric tokens. If no such token appears, the response is marked as incorrect.\nMermaid Code Prediction: We focus on statements encapsulated within triple backticks (```) in model responses. From these, we extract nodes and links according to the Mermaid syntax rules."}, {"title": "Settings", "content": "For our evaluations, we utilized the testing subset of the FlowLearn dataset, which included assessments of 500 scientific flowcharts and 2,000 simulated flowcharts. Due to cost constraints and API limitations, we limited our evaluations to 100 samples per task for Claude-3-Opus, GPT-4V, and Step-1V. All other evaluations were conducted using an NVIDIA A100 80GB GPU.\nWe opted for few-shot prompting as our evaluation strategy to align the output of the LVLMs more closely with the ground truth. According to Zong et al. [35], few-shot prompting, particularly with 2-shot samples, generally yields the most significant performance improvement in general vision-language VQA tasks across various LVLMs. Additionally, using 2-shot samples provides a balanced approach for evaluating True/False statements, as it allows an equal representation of both true and false scenarios within the prompts. This method ensures that the models are not biased toward one answer type over the other, facilitating a more accurate and fair assessment of model capabilities.\nFor consistency, we employ the prompt format shown in Table 3 for evaluation."}, {"title": "Experiment Results", "content": "In this section, we present the results from our evaluation of the LVLMs across three distinct groups of VQA tasks within the FlowLearn dataset. Each task group was designed to test different aspects of model performance using specialized evaluation metrics. For a focused review of performance across a limited subset of 100 samples involving all models and all tasks, please refer to Section 2 of the Supplementary Materials. The findings there align closely with the results discussed here. Sample model responses to all VQA tasks are shown in Section 3 and 4 of the Supplementary Materials."}, {"title": "Accuracy Tasks", "content": "The first group of tasks evaluates the accuracy of the LVLMs in responding to queries that require precise, binary, or short phrase answers. These tasks are foundational for assessing flowchart comprehension. The performance of each model on these accuracy tasks is summarized in Table 4, leading to several key observations:\n1) No clear winner across all accuracy tasks. For scientific flowcharts, Gemini-Pro-Vision showed the strongest performance on the full test set. However, on smaller subsets, GPT-4V and Step-1V also demonstrated strong performances. For simulated flowcharts, on the full test set, InternLM excelled in True/False statements, Gemini-Pro in OCR tasks, and Qwen-VL in counting nodes and arrows.\n2) Irrelevant model responses. Although most models generally produced task-related responses, irrelevant responses were still observed. For True/False tasks, Qwen-VL and LLaVA often scored close to zero, indicating a lack of 'true' or 'false' tokens in its responses.\n3) Challenges in counting nodes and arrows. Counting tasks, which require comprehensive image understanding rather than partial recognition, proved difficult for most models, leading to lower average scores. Notably, despite its underperformance in other areas, Qwen-VL's results were comparatively better in these tasks."}, {"title": "Similarity Tasks (Description)", "content": "The second group of tasks assesses LVLMs' ability to generate accurate descriptions of flowcharts, with detailed performance data for each model presented in Table 5.\nFor scientific flowcharts, DeepSeek and Gemini achieved the highest scores on most metrics. Qwen-VL produced the longest responses, while DeepSeek provided the shortest. We also reviewed responses from the widely-used GPT4V. Out of 100 samples, only 24 were error-free. Analysis at the sentence level yielded 597 sentences with a 59% accuracy rate, meaning 41% of sentences contained errors. Sentences were marked as incorrect only if evidence from the image contradicted the text. Supplementary material includes examples. We identified several common trends:\n1) Generally, models provided satisfactory responses, accurately inferring full names from acronyms and offering reasonable academic interpretations of the depicted processes.\n2) Models handled complex images effectively, including those with high resolutions and multiple parts.\n3) Longer descriptions were more error-prone, whereas shorter ones, while accurate, lacked comprehensive coverage, explaining the lower sample-wise compared to sentence-wise accuracy.\n4) In many cases, models produced logical but inaccurate descriptions. Examples of this are shown in Table 6.\nFor simulated flowcharts, Gemini outperformed other models across most metrics, typically scoring higher than for scientific flowcharts. This discrepancy likely stems from the structured, template-generated reference answers for simulated flowcharts, contrasted with the varied language and additional contextual information in scientific flowchart captions."}, {"title": "Mermaid Code Task", "content": "This task assesses the comprehensive ability of LVLMs to encapsulate their understanding of a flowchart in a code format, summarizing aspects such as OCR, counting nodes and arrows, and recognizing relationships between nodes. The performance of each model on the Mermaid Code task for simulated flowcharts is summarized in Table 7. In evaluations on the full dataset, Gemini achieved the highest scores across all metrics. On a smaller evaluation subset, Claude demonstrated superior performance, particularly excelling in node-level prediction with an F1 score of 94%."}, {"title": "Ablation Study on Chain-of-Thought", "content": "For complex tasks such as converting a flowchart into Mermaid code, a methodical approach can be beneficial. This process typically involves several sequential steps: initially detecting text nodes, then recognizing the links between them, and finally compiling these information into a standardized format, such as Mermaid code. Given the multi-step nature of this task, we hypothesized that introducing a chain-of-thought (CoT) process could potentially enhance model performance. Consequently, we conducted an experimental ablation study on the simulated subset using Gemini-Pro-Vision, a model that incurs no querying cost and can be evaluated on the full test set. Notably, this model has shown the best performance on the Mermaid code task (Section 5.3).\nFor this experiment, we modified the 2-shot example answers using a structured template (Table 8) that guides the model through a step-by-step reasoning process. In this template, {1} is replaced with all text appearing in the simulated flowchart, {2} is derived from the flowchart description generated as per the templates described in Section 3.3, and {3} is the corresponding Mermaid code. Additionally, we appended the phrase \"Let's think step by step\" at the end of the original prompt (as illustrated in Table 2) to further emphasize the sequential reasoning process.\nWe selected Gemini, the top-performing model for the Mermaid Code Task, for this ablation study. Surprisingly, as shown in Table 7, the performance using the CoT approach indicated a slight decrease compared to the original model configuration without it. This unexpected outcome suggests that while the chain-of-thought method is intended to foster clearer and more structured reasoning, it may introduce additional complexities or dependencies that hinder the model's ability to synthesize and process information efficiently. Further analysis and refinement of the chain-of-thought implementation may be necessary to fully realize its potential benefits and address these challenges."}, {"title": "Discussion", "content": "The initial version of the FlowLearn dataset presents inherent limitations, offering opportunities for future enhancements."}, {"title": "Scientific Flowchart Subset", "content": "First, True/False statements are missing for the training set of scientific flowcharts. Annotators generate these statements only for test samples, which is time-consuming, averaging three minutes per pair. Future versions should aim to include these statements for all entries.\nSecond, the dataset size is currently limited. With fewer than 4,000 images, FlowLearn's scientific flowchart collection is small compared to larger visual-language datasets. While the inclusion of simulated flowcharts helps to mitigate this limitation by broadening the scope of the training data, expanding the collection of scientific flowcharts would be advantageous.\nThird, the descriptive task is limited. The descriptive task for scientific flowcharts is currently evaluated against figure captions. However, the descriptive text for scientific diagrams is often scattered throughout the associated literature, as outlined in Context247: Contextualizing Scientific Figures and Tables. A more robust approach would involve annotators extracting and collating descriptive text from the full text of scientific articles to provide a more comprehensive base for evaluating LVLM-generated descriptions."}, {"title": "Simulated Flowchart Subset", "content": "The simulated flowchart subset was designed to augment the scientific subset by offering a more granular evaluation of flowchart comprehension and providing additional training data. Future iterations could improve upon this by incorporating a greater diversity of diagram types, such as state diagrams and quadrant charts, to enrich the dataset further. While FlowLearn currently focuses exclusively on flowcharts, expanding the range of diagram types could enhance its applicability."}, {"title": "Model Selection", "content": "Our model selection was biased towards LVLMs due to their broad capabilities and general applicability. However, many task-specific, smaller visual-language models may also be well-suited for these tasks. Future work will explore the potential of these models, which might offer more specialized insights or efficiencies in specific aspects of flowchart comprehension."}, {"title": "Conclusion", "content": "In this study, we introduced and evaluated the FlowLearn dataset, a novel resource aimed at advancing the understanding of flowcharts for visual-language models. Our experiments spanned various tasks, including OCR, True/False assessments, counting nodes and arrows, flowchart description, and generating Mermaid code, across two distinct subsets: scientific and simulated flowcharts.\nOur findings demonstrate that while LVLMs are capable of impressive performance on certain tasks, challenges remain. Notably, the models excelled at OCR and True/False statements in certain contexts but struggled with the more complex task of accurately generating Mermaid code from flowcharts. This underscores a broader issue: LVLMs often struggle to fully comprehend the intricate relationships between visual components and to synthesize this information into structured code formats effectively.\nGiven the rapid advancements in the fields of LLMs and LVLMs, the FlowLearn dataset is timely and provides valuable insights into a relatively underexplored area. It not only serves as a critical tool for benchmarking and refining these models but also helps illuminate the specific difficulties they encounter with visual reasoning in a structured context. By pushing the boundaries of what LVLMs can understand and achieve, we can bridge the gap between human and machine comprehension of visual and language tasks, paving the way for more intelligent and capable automated systems."}]}