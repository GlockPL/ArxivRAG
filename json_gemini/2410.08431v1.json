{"title": "oRetrieval Augmented Generation for 10 Large Language Models and its Generalizability in Assessing Medical Fitness", "authors": ["Yu He Ke", "Liyuan Jin", "Kabilan Elangovan", "Hairil Rizal Abdullah", "Nan Liu", "Alex Tiong Heng Sia", "Chai Rick Soh", "Joshua Yi Min Tung", "Jasmine Chiat Ling Ong", "Chang-Fu Kuo", "Shao-Chun Wu", "Vesela P. Kovacheva", "Daniel Shu Wei Ting"], "abstract": "Large Language Models (LLMs) offer potential for medical applications, but often lack the specialized knowledge needed for clinical tasks. Retrieval Augmented Generation (RAG) is a promising approach, allowing for the customization of LLMs with domain-specific knowledge, well-suited for healthcare. We focused on assessing the accuracy, consistency and safety of RAG models in determining a patient's fitness for surgery and providing additional crucial preoperative instructions.\nMethods: We developed LLM-RAG models using 35 local and 23 international preoperative guidelines and tested them against human-generated responses, with a total of 3682 responses evaluated.\nClinical documents were processed, stored, and retrieved using Llamaindex. Ten LLMs (GPT3.5, GPT4, GPT4-0, Llama2-7B, Llama2-13B, LLama2-70b, LLama3-8b, LLama3-70b,\nGemini-1.5-Pro and Claude-3-Opus) were evaluated with 1) native model, 2) with local and 3) international preoperative guidelines.\nFourteen clinical scenarios were assessed, focusing on 7 aspects of preoperative instructions. Established guidelines and expert physician judgment determined correct responses.\nHuman-generated answers from senior attending anesthesiologists and junior doctors served as a comparison. Comparative analysis was conducted using Fisher's exact test and agreement for inter-rater agreement within human and LLM responses.\nResults: The LLM-RAG model demonstrated good efficiency, generating answers within 20 seconds, with guideline retrieval taking less than 5 seconds. This performance is faster than the\n10 minutes typically estimated by clinicians. Notably, the LLM-RAG model utilizing GPT4 achieved the highest accuracy in assessing fitness for surgery, surpassing human-generated responses (96.4% vs. 86.6%, p=0.016). The RAG models demonstrated generalizable\nperformance, exhibiting similarly favorable outcomes with both international and local guidelines. Additionally, the GPT4 LLM-RAG model exhibited an absence of hallucinations and produced correct preoperative instructions that were comparable to those generated by clinicians.\nConclusions: This study successfully implements LLM-RAG models for preoperative healthcare tasks, emphasizing the benefits of grounded knowledge, upgradability, and scalability for effective deployment in healthcare settings.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have gained significant attention for their clinical applications potential\u00b9, and have been demonstrated to match human performance in basic clinical tasks such as rating American Society of Anesthesiologists (ASA) scoring\u00b2. However, where complex tasks, such as clinical assessment and management are given, the response only relies on pre-train knowledge and is not grounded on institutional practicing guidelines. Most importantly, hallucinations from LLMs pose significant safety and ethical concerns\u00b3.\nSurgery cancellations on the day of surgery due to medical unfitness\u2074, incorrect physician instructions\u2075, and non-compliance to preoperative instructions\u2076 pose a significant economic impact, with operating room expenses estimated between USD 1400 to 1700 per hours\u2077. Thorough preoperative evaluations can minimize these cancellations\u2078, but traditional preoperative evaluations are inherently labor-intensive and costly. The utilization of domain-specific LLM for delivering preoperative instructions presents substantial potential for personalized preoperative medicine.\nOptimization of LLMs with Retrieval Augmented Generation (RAG)\nIn the rapidly evolving field of LLMs, the challenge of optimizing performance to meet specific needs is a key focus. While out-of-the-box LLMs offer impressive capabilities, techniques like fine-tuning and RAG present promising avenues for further enhancing their accuracy and relevance.\nThe primary challenges in fine-tuning LLMs stem from various factors including the need for extensive retraining datasets, particularly for complex fields like healthcare; and technical hurdles such as limitations in context tokens and the computational demands typically quantified in petaflops for GPU memory\u00b9\u2070.\nRetrieval Augmented Generation (RAG) is an innovative approach for tailoring LLMs to specific tasks, and a scalable solution agnostic to various LLM-based healthcare applications. It offers an easier solution without the need for extensive training examples or time as required by fine-tuning, and accessibility to updated customized knowledge without significant time in creating up-to-date ground truth and retraining required by fine-tuning. Unlike traditional LLMs, RAG functions similarly to a search engine, retrieving relevant, customized text data in response to queries. This capability effectively turns RAG into a tool that integrates specialized knowledge into LLMs, enhancing their baseline capabilities. In healthcare, for instance, LLMs equipped with RAG and embedded with extensive clinical guidelines (LLM-RAG) can yield more accurate outputs\u00b9\u00b9. Currently, two primary open-source frameworks for RAG exist - LangChain\u00b9\u00b2 and Llamaindex\u00b9\u00b3. Although the retrieval process of RAG can be technically challenging, RAG's utility in contexts with smaller, more focused knowledge corpora remains significant.\nThis study aims to develop and evaluate an LLM-RAG pipeline for preoperative medicine using various LLMs and guidelines. The primary objective is to assess the pipeline's accuracy in determining patients' fitness for surgery. The secondary objective is to evaluate the LLM-RAG's"}, {"title": "Methods", "content": "ability to provide accurate, consistent and safe preoperative instructions, including if the patient should be seen by a nurse or doctor, fasting guidelines, medication management, and optimization strategies.\nDevelopment of Retrieval Augmented Generation (RAG) Framework\nThe LLM-RAG pipeline framework is composed of multiple distinct components:\nRetrieval Augmented Generation Pipeline\nTo utilize clinical documents with RAG frameworks, they must be converted into text format. Conventional and vanilla RAG utilized open-source tools like Langchain to provide loaders that extract text and preserve metadata for retrieval reference. However, this automated process may not efficiently retrieve pertinent information and filter out irrelevant information such as citations, accurately interpret visual elements like diagrams or relationships within structured data such as tables. The text is then segmented into chunks for embedding, with the ideal chunk size for healthcare applications still being explored for better semantic knowledge encoding. Advanced RAG techniques in Llamaindex could offer potential solutions\u00b9\u2074 for the aforementioned challenges in traditional RAG pipelines.\nIn the current study, we explore an advanced LLM-RAG framework using Python 3.11 with Llamaindex, for its optimized and streamlined pipeline for RAG. Specifically, Llamadex-based Auto-Merging Retrieval was used based on its unique advantages for enhanced retrieval.\nAuto-Merging Retrieval is known for its unique structure of processed chunks and retrieval logic. All chunks are structured in a tree-like fashion structured by hierarchical nodes. During retrieval, more parent chunks would be gathered together when a certain amount (based on a predefined threshold) of smaller chunks are retrieved. Hence, such a process is also called \u201cmerging\u201d, by adding smaller chunks related to the key piece of information identified. Therefore, it provides a continuous information flow and improved contextual representation of information. Specifically, we set \"similarity_top_k\" or the maximal allowable number of information pieces to 30, justified by balancing insufficient important clinical information identified and the overflow of irrelevant information as noises to the current system. Other minor reasons are finding an adequate amount of information retrieved for long-context window LLMs, maintaining task performance without losing logical flow in prompts and clinical cases, and reducing operational burden including cost and computational power with adequate prompt size,\nPrompt Engineering\nPrompt engineering following the guidance by Bertalan et al was followed\u00b9\u2075. Key principles we emphasized included specificity, contextualization, and open-endedness to elicit comprehensive LLM responses. We also employed role-playing in the prompts. Our approach involved an iterative process of prompt refinement and sample response generation. We continued this process until we achieved satisfactory LLM output."}, {"title": "Large Language Model and Response Generation", "content": "A list of pre-trained foundational LLMs is selected for this case study, including GPT 3.5\u00b9\u2076, GPT 4\u00b9\u2077, GPT4o\u00b9\u2078, LLAMA2-7B\u00b9\u2079, LLAMA2-13B\u00b9\u2079, LLAMA2-70B\u00b2\u2070, LLAMA3-8B\u00b2\u00b9, LLAMA3-70B\u00b2\u00b2, Gemini-1.5-Pro\u00b2\u00b3, and Claude-3-Opus\u00b2\u2074. Current LLM selections are justified by evaluating the existing best-performing cloud-based model including GPT families and Gemini, and best-performing local LLMs including Llama families and Claude. Both local LLM deployment and cloud-based computational solutions are important considerations for future LLM integration into clinical workflow. Detailed characteristics of these LLMs are provided in Table 1. The same set of knowledge extracted from the guideline knowledge corpus was used as user prompts in various scenarios. Additionally, the clinical questions were input as system prompts for all models.\nLLM Inference\nTo ensure consistency in the responses generated by various Large Language Models (LLMs), we standardized key parameters: the temperature was set at 0.1, the maximum output token length was fixed at 2048, and the Top-P value was established at 0.90. This setup aimed to minimize hallucinations and produce responses that were both meaningful and reproducible.\nGPT Based Models\nFor the GPT-based models, we utilized the cloud-based OpenAl playground platform to conduct our experiments.\nLLAMA Based Models\nInference for LLAMA models were carried out using Google Cloud Platform (GCP) GPUs, specifically 2xA100 (80GB) through Vertex Al. We sourced the models from Meta's official Hugging Face repository for the LLM-RAG inferences. However, for LLAMA2 models, we had to refine the process by selecting the top 10 retrievals (information pieces) to feed into the LLM for inference. Additionally, the maximum tokens generated were reduced to 1024, given the LLAMA2's context length limitation of 4k.\nGemini-1.5-Pro and Claude-3-Opus Models\nFor the Gemini-1.5-Pro responses, we employed GCP Vertex Al's language generation playground. On the other hand, the Claude-3 Opus model was implemented via GCP's notebooks and through API calls.\nDetailed in Figure 1 is the operational framework of the LLM-RAG model, providing a schematic representation of the interplay of the algorithmic workflow integral to the system's functionality."}, {"title": "Evaluation Framework", "content": "This study utilized two distinct sets of preoperative guidelines. The first comprised 35 local protocols from a major tertiary hospital in Singapore, adapted from established international perioperative standards. The second consisted of 23 international guidelines sourced from various anesthesia societies (Supplementary Table 1). All guidelines, complete with diagrams and figures, were extracted in their native PDF format and loaded into separate LLM-RAG systems. Both sets provided comprehensive protocols for patient assessment, medication management, and specific surgical procedures.\nB. Clinical scenarios\nThis study assessed the performance of the LLM-RAG system on 14 de-identified clinical scenarios. These scenarios, encompassing a diverse range of patients and surgical complexities, were randomly selected from pre-operative clinic notes. No two similar conditions were selected. To ensure patient anonymity while preserving the natural language structure of clinic notes, the notes underwent a meticulous de-identification process, removing all patient identifiers and surgical dates. Given the anonymized nature of the data and the minimal risk posed by the study procedures, ethics approval was not deemed necessary.\nSix key aspects of preoperative instructions were assessed, with the primary outcome being the assessment of the patient's fitness for surgery. This was complemented by five additional parameters: 1) fasting guidelines, 2) suitability for preoperative carbohydrate loading, 3) medication instructions, 4) healthcare team directives, and 5) types of preoperative optimizations required. These aspects were selected due to their established significance in the current medical literature and their potential impact on surgical outcomes\u00b2\u2075.\nC. Output\nFour junior doctors (1-7 years of anesthesia experience) and four attending anesthesiologists (1 from Singapore General Hospital, Singapore, 1 from Harvard Medical School, United states, 2\nA. Preoperative Guidelines"}, {"title": "Results", "content": "The study is not funded. All the authors had full access to the study data. The full raw data is available within the appendix and upon request. The manuscript was reviewed by all the other authors. YHK and DSWT were responsible for the decision to submit the manuscript for publication.\nA total of 3,682 components were evaluated (448 human-generated and 3,234 LLM-generated). The LLM-RAG models took on average 1 second for retrieval and 15-20 seconds for results generation, while the human evaluators took an average of 10 minutes to generate the full preoperative instructions. The GPT4_international model emerged as the accurate model with the highest accuracy in predicting medical fitness for surgery (96.4%) compared to human-generated answers (86.6%), as well as its non-RAG counterpart (92.9%) and RAG counterpart with local guidelines (92.9%) (Figure 2 and Supplementary Table 2). A detailed example of the prompt, clinical scenario and the GPT4-international response (response 1) can be found in Table 3. The GPT4_international model performed better than humans in evaluations of patient's fitness for surgery (OR = 4.84, p-value = 0.016).\nThe GPT-4 RAG model, when using local guidelines, accurately predicted whether a patient should be seen by a nurse or a doctor 93.0% of the time (Supplementary Table 3). This performance is notably higher compared to its non-RAG counterpart, which achieved an accuracy of 86.0%. Furthermore, the GPT4 models also had the highest accuracy when assessing for fitness for operation in sicker, ASA 3 patients. The comparison, the Gemini and LLAMA2-13b models had less than 50% accuracy when assessing ASA 3 patients (Supplementary Figure 1).\nThe secondary outcomes show that the GPT4_international model was better than humans at generating what medical optimization was required by the scenario (71.0% vs 55.0%, p=0.026), but the human-generated answers were better than the GPT4_international model at generation of medication instructions order (91.0% vs 98.0%, p=0.035) (Supplementary Table 4 and 5). There were no significant overall differences in GPT4_international answers and human answers (83.0% vs 81.0%, p=0.710), and this was not changed at 65% sensitivity analysis (p=0.688) and 85% sensitivity analysis (p=0.710) (Supplementary Table 6).\nThe S.C.O.R.E evaluation showed that the GPT4 RAG model was able to have high reproducibility of results (4.86 out of 5) and provided safe instructions (4.93 out of 5) (Supplementary Table 6). The false negative rate of identifying medical fitness was 62.5% in human evaluators and 25% in GPT4_international (Supplementary Table 7).\nThe IRR for human-generated answers was consistently lower than that for GPT-4 International across all categories (Supplementary Table 8). The IRR for GPT-4 International in predicting medical fitness was 0.93. Additionally, GPT-4 International demonstrated high consistency in providing instructions for healthcare workers (IRR = 0.96) and identifying types of optimization requirements (IRR = 0.92)."}, {"title": "Discussion", "content": "from Chang Gung Memorial Hospital, Taiwan) independently responded to the primary outcome assessment. Additionally, the junior doctors, reflecting the standard global practice of preoperative assessment often performed by junior doctors or anesthetic nurses\u00b2\u2076, also responded to the secondary outcomes. To maintain the integrity of the study and ensure unbiased responses, the participants were blinded to the study's objectives. The human-generated answers were then collated and aggregated for comparison with the LLM-generated answers.\nThe 'correct' answers in the study were based on established preoperative guidelines and reviewed by an expert panel made up of two practicing perioperative anesthesiologists. Where there were disagreements, discussions were made between the two panelists to come to a final decision. In ambiguous cases, like the suspension of ACE inhibitors before surgery, both potential answers were considered correct\u00b2\u2077. This was to reflect the real-world complexities of preoperative decision-making, especially where evidence for one choice over another was scarce. The study focused on scenarios with clear directives regarding the postponement of operations for additional optimizations.\nFor the secondary objectives, which involved preoperative instructions with multiple components, a response was deemed \"correct\" if it aligned with at least 75% of the guidelines. This threshold acknowledges the inherent subjectivity in preoperative instructions, where the clinical significance of omissions can vary. For example, omitting a recommendation for CPAP use in a high-risk sleep apnea patient could be critical, whereas omitting a preference for morning surgery in a non-critical case might be less consequential (Supplementary, Scenario 1). Given the absence of established accuracy thresholds in this context, we conducted a sensitivity analysis by evaluating performance at both 65% and 85% accuracy cutoffs.\nOnly the LLMs were evaluated to determine whether a patient should be seen by a nurse anesthetist or a doctor. The assessment criteria were based on Singapore's local guidelines, which state that ASA I or II patients over 21 years old, not undergoing high-risk surgery or facing a high risk of complications, and without any abnormal investigation results, can be evaluated by a nurse anesthetist. These guidelines are more conservative than those in other countries due to the heavily doctor-led nature of Singapore's healthcare system, and they account for variations in thresholds across different institutions.\nWe assessed the LLM-RAG systems' responses for accuracy, consistency and safety. To prioritize patient safety, any response containing critical medical errors (e.g., incorrect fasting instructions or medication dosages) was categorized as \u201cHallucination\u201d and automatically considered wrong even if the rest of the instructions were correct.\nA comparative analysis is performed against the human-generated responses and the best-performing LLM-RAG model using Fisher's exact test. Consistency within the human and LLM answers were analyzed using percentage agreement for interrater reliability (IRR). All statistical evaluations are performed in the Python 3.6 environment.\nThis study highlights the potential of integrating LLM-RAG models into healthcare workflows, such as preoperative medicine. Our findings indicate that the LLM-RAG system can outperform doctors in assessing a patient's fitness for surgery and deliver comparably accurate, yet more consistent evaluations for other preoperative assessments. These results suggest that LLM-RAG models could complement and assist clinicians, improving efficiency and reducing workload in specific preoperative tasks.\nLLM and Localized, Domain-specific Models\nThe emergence of fine-tuned models with ULMS\u00b2\u2078 and BioMedLM from Stanford-CRFM\u00b2\u2079 exemplifies the trend toward specialization in LLM applications. These domain-specific models are tailored to understand and process medical information, offering enhanced accuracy and relevance in clinical settings\u00b3\u2070. The capability of LLM-RAG models to process vast amounts of data and generate responses based on comprehensive, updated guidelines positions them as potentially valuable tools in standardizing preoperative assessments.\nThis study also highlights a key advantage of the LLM-RAG \u2013 its ability to incorporate local healthcare practices and adapt international recommendations to the specific context. This is evident in the GPT responses, where generic referrals to \"medicine colleagues\" were transformed into the specific \"Optimize diabetes control with the Internal Medicine Perioperative Team (IMPT)\" within the local context (Scenario 1, GPT4 response). This ability to tailor recommendations based on geographical variations strengthens the potential of RAG-LLMs for real-world healthcare applications.\nLLM-RAG as a subspecialty clinical aid\nThe results of our study are particularly relevant in the context of the evolving landscape of elective surgical services, which have increasingly shifted towards day surgery models, reduced hospital stays, and preoperative assessments conducted in outpatient clinics\u00b3\u00b9. By employing a simple vanilla RAG framework, Langchain, and Pinecone retrieval agent, we observed significant improvements and improved clinical alignment for pre-operation assessment in LLM healthcare applications. For complex clinical use cases, such as clinical decision tools for medication-related queries, advanced RAG frameworks such as Llamaindex and improved chunking, embedding, and retrieval are expected. The potential role of LLM-RAG in this setting as a clinical adjunct is, therefore, of considerable interest as manpower constraints span across medical providers.\nFurthermore, subjectivity in clinical decisions due to variations in human judgment underscores the potential value of LLM-RAG systems in enhancing consistency in clinical decision-making. GPT models, for example, have demonstrated more consistent responses compared to anesthesiologists in tasks like ASA scoring\u00b2. This consistency is a crucial advantage, particularly in a field where uniformity in evaluation and decision-making can significantly impact patient outcomes.\nA qualitative analysis of LLM-RAG model responses compared to human-generated answers"}, {"title": "Conclusion", "content": "revealed potential discrepancies in information completeness. In Scenario 4, for example, the GPT4 models included specific instructions for all the medications (Keppra, Paracetamol, and Tramadol) on the surgical day. Conversely, the majority of the human evaluators did not give instructions for the analgesics. This observed difference could be attributed to a lack of universally accepted guidelines for continuing certain medications (e.g. analgesics) on the surgical day. These findings suggest that LLM-RAG models, by comprehensively incorporating available information, may be less susceptible to such variability (with higher IRR), potentially leading to more consistent and improved preoperative instructions for current clinical workflows.\nAugmenting Preoperative Workflow\nA valuable application of this pipeline will be augmenting the preoperative workflow. In many pre-op clinics, including ours, patients are screened to determine whether they should be evaluated by a Nurse Practitioner or a Medical Doctor. In some cases, the decision is whether patients should be seen in advance or on the day of surgery, particularly if they are healthy and low-risk. If this triage can safely be done by the pipeline, it will save significant effort and costs. Additionally, if this approach can help draft patient instructions, it will save valuable time and may help decrease clinician burnout.\nGeneralizability of RAG with International and Local Guidelines\nA significant finding of this study is the generalizability of the LLM-RAG models when utilizing both international and local preoperative guidelines. The ability of the RAG system to accurately interpret and apply these diverse guidelines underscores its versatility in various healthcare settings. This generalizability is crucial, as it demonstrates that LLM-RAG models can effectively standardize preoperative assessments across different regions, adhering to local practices while maintaining alignment with broader international standards. This flexibility enhances the practical utility of LLM-RAG systems, ensuring they can be seamlessly integrated into diverse clinical environments to support and optimize patient care.\nLLM-RAG and environmental sustainability\nThe adoption of LLM-RAG models may also offer benefits in environmental sustainability, particularly when compared to fine-tuning, which requires large computation power\u00b9\u2070,\u00b3\u00b2 to a higher carbon footprint\u00b3\u00b3. In contrast, LLM-RAG models allow for efficient access to domain-specific information without the need for extensive retraining. The cost of building an LLM-RAG model could be further brought down with the latest GPT4-turbo-preview, offering a lower cost per token and a much larger context size of 128k (vs 8k for GPT 4).\nChallenges and Limitations\nThe study's findings are based on simulated clinical scenarios, which may limit their generalizability to real-world settings. Additionally, variations in individual hospital protocols can lead to different thresholds for assessing surgical fitness. Although efforts were made to standardize the clinical scenarios following both local and international guidelines to minimize ambiguities regarding fitness for surgery, these standardizations may not account for all possible variables in actual clinical practice.\nFine-tuning, as another attractive LLM technique, was not explored for assessing its performance in patients' pre-operation assessment in the current study. This is mainly attributed to the limitation of training dataset numbers less than traditionally recommended amounts (at least 50 examples are suggested by OpenAl documentation). Further experimentation on finetuning LLMs would be necessary to compare their performance with the current LLM-RAG framework.\nThe low hallucination rate of the multiple LLM models such as GPT, Gemini, and Claude is encouraging; however, the potential for factually incorrect or misleading outputs necessitates a cautious approach to integrating Al in healthcare. Our evaluation focused on clinically relevant portions of the model's output. While hallucinations outside these areas might be less pertinent or unlikely to directly harm patients, a broader evaluation framework encompassing the full spectrum of potential outputs is important. This would provide a more comprehensive understanding of the model's strengths and weaknesses, allowing for further refinement and ensuring responsible clinical implementation.\nFurthermore, the dynamic nature of perioperative medicine and its evolving practice guidelines necessitate regular updates to LLM-RAG models. Continuous integration of the latest medical guidelines and evidence-based practices is crucial to maintaining their accuracy and clinical relevance. This underscores the importance of establishing robust training and updating protocols for these models. Notably, this adaptability can be a significant advantage of LLM-RAG models. For instance, while the majority of human-provided fasting instructions traditionally required patients to abstain from all intake after midnight, most LLM-RAG models adopted the updated protocol, recommending no solids for six hours and no fluids for two hours before surgery.\nThe absence of a standardized evaluation framework for RAG-LLM models in medicine highlights the need for a measured implementation approach. Further research is necessary to develop robust and reliable benchmarks specific to the healthcare domain.\nThe ethical implications and inherent biases of employing LLMs in clinical environments demand careful consideration. In this study, the chosen clinical scenarios were structured to yield clear decisions about delaying surgeries for medical optimization. However, real-world clinical situations often involve nuanced decisions, particularly in critical areas like cancer treatment, where the choice to postpone surgery exists in a realm of ethical ambiguity. Users of LLM-RAG models must recognize that in complex ethical landscapes where nuanced recommendations are needed, the model might lean towards certain decisions influenced by its training data. These models are best utilized as supportive tools that complement but do not replace, the expert judgment of medical professionals.\nThis study demonstrates the feasibility and potential benefits of integrating LLM-RAG models into preoperative healthcare workflows. The model exhibited accuracy comparable to, or"}]}