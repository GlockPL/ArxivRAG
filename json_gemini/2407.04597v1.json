{"title": "Feature Attenuation of Defective Representation Can Resolve Incomplete Masking on Anomaly Detection", "authors": ["YeongHyeon Park", "Sungho Kang", "Myung Jin Kim", "Hyeong Seok Kim", "Juneho Yi"], "abstract": "In unsupervised anomaly detection (UAD) research, while state-of-the-art models have reached a saturation point with extensive studies on public benchmark datasets, they adopt large-scale tailor-made neural networks (NN) for detection performance or pursued unified models for various tasks. Towards edge computing, it is necessary to develop a computationally efficient and scalable solution that avoids large-scale complex NNs. Motivated by this, we aim to optimize the UAD performance with minimal changes to NN settings. Thus, we revisit the reconstruction-by-inpainting approach and rethink to improve it by analyzing strengths and weaknesses. The strength of the SOTA methods is a single deterministic masking approach that addresses the challenges of random multiple masking that is inference latency and output inconsistency. Nevertheless, the issue of failure to provide a mask to completely cover anomalous regions is a remaining weakness. To mitigate this issue, we propose Feature Attenuation of Defective Representation (FADeR) that only employs two MLP layers which attenuates feature information of anomaly reconstruction during decoding. By leveraging FADER, features of unseen anomaly patterns are reconstructed into seen normal patterns, reducing false alarms. Experimental results demonstrate that FADeR achieves enhanced performance compared to similar-scale NNs. Furthermore, our approach exhibits scalability in performance enhancement when integrated with other single deterministic masking methods in a plug-and-play manner.", "sections": [{"title": "1. Introduction", "content": "When building an automated anomaly detection (AD) system, we first encounter a significant challenge of data imbalance problem due to the rarity of anomaly situations in the early stages of manufacturing. This has been dealt with by adopting an unsupervised anomaly detection (UAD) method that only exploits prevalent normal samples for the training. In recent years, UAD researches have shown significant advancements, particularly with the exploration of state-of-the-art (SOTA) models on public benchmark datasets. The focus has often revolved around the use of large-scale tailored neural networks (NN) to achieve better AD performance or the development of unified models capable of handling multiple tasks that are trained with known multiple-category products. However, towards edge computing environments, there is a growing need to shift towards methods that are not only computationally efficient but also scalable.\nOne key observation from current SOTA methods is their reliance on single deterministic masking methods to address challenges related to random or multiple masking methods, which often result in increased inference latency and output inconsistency. However, incomplete mask issues can occur while leveraging a single deterministic masking method. First of all, collection of large quantities of abnormal samples is infeasible and cannot cover all types of defects that may occur at test time. Therefore, creating a model that can perfectly mask defective regions is practically impossible. For another example, consider a pre-trained attention-based zero-shot masking method. It presents an attractive feature for masking suspected defective regions but is partially imperfect to mask defects. This is because it was not trained to mask defects. Therefore, we need to warrant that any single deterministic masking method can fully cover defective regions.\nTo address this issue, we introduce an effective method to resolve the possible missing out of defective regions when exploiting single deterministic masking in the reconstruction-by-inpainting-based UAD approach. Our study aims to optimize UAD performance with minimal changes to the NN structure by using a reconstruction-by-inpainting U-Net, which effectively prevents the identity shortcut (IS) issue. We propose further refinements to enhance its efficacy through a detailed analysis. Our method, dubbed Feature Attenuation of Defective Representation (FADER), leverages a simple architecture comprising only two Multi-Layer Perceptron (MLP) layers to attenuate defective feature information by soft feature masking during the decoding process. FADER divides a visual defect obfuscated image into patches and predicts the patch-wise reconstruction errors. At this time, a high value of reconstruction error means that the patch is very likely to the defective region. FADER attenuates the encoded feature representation of suspected defective patches before transmitting those to the decoder. The decoder receives a feature map with attenuated defective representation, which can promote reconstruction into a seen normal pattern. Note that, we exploit the active learning strategy to train FADER because there is no label of defective feature attenuation. Active learning allows training NNs with minimal or no labeled data.\nExperimental results with the public industrial datasets MVTec AD and VisA demonstrate the capability of FADER that resolves the incomplete mask issue of single deterministic masking methods. Furthermore, our approach suggests scalability and versatility by exhibiting improved performance when integrated with other UAD models in a plug-and-play manner.\nOverall, our contributions are summarized as follows:\n\u2022 We propose an effective solution to resolve the possible incomplete mask issue that may not fully cover defective regions. This solution mitigates unintended false negatives caused by the features of unmasked defective regions during the masking process in the reconstruction-by-inpainting approach. Additionally, this component only works as a two-layer MLP, which can minimize increase in computational complexity.\n\u2022 We propose FADER as a scalable solution by allowing us to plug and play with other inpainting design components or freely adjust the detailed design of FADER components."}, {"title": "2. Related work", "content": "The detection of anomaly instances, known as AD, has garnered significant interest because it is a critical task across a wide range of industries. Unsupervised anomaly detection is a widely adopted approach due to anomalous data scarcity and difficult labeling situations."}, {"title": "2.1. Reconstruction-by-inpainting methods", "content": "An UAD model can be trained by minimizing the reconstruction error with normal samples only. Then, we can find anomalous samples in the test stage by the reconstruction error between the input and output of the model. Many recent studies dealt with the strategy that changes NN structure to improve the UAD performance. U-Net like structures that integrates skip connections into the autoencoder (AE) are utilized in recent studies. Other strategies to construct NNs include: 1) replacing each layer within AEs with other types such as convolutions, MLPs, or transformers, 2) adding other branches of neural propagation such as generative adversarial networks (GAN), and 3) changing the computational flow without structural change of the NN for example normalizing flow or diffusion process. These recent advancements are indeed adequate efforts to achieve high performance. However, they require huge computational resources, which can be a drawback for practical applications in industrial settings.\nReconstruction-by-inpainting methods have emerged to achieve better performance by effectively preventing an UAD model from accurate reconstruction of unseen anomalous patterns. These are visually summarized in Figure 2. Specifically, they can be categorized into three cases, random masking, multiple disjoint masking, and progressive masking from the initial masks. While multiple random masking methods ensure a good performance, they commonly lead to inference latency due to multiple masking and output inconsistency due to random masking.\nTo address this issue, methods of single deterministic masking are developed. EAR leverages the attention of ImageNet pre-trained DINO-VIT to generate a single deterministic mask. It has a strength that does not need a training process for mask generation. On the other hand, AMI-Net proposes a clustering-based learnable single masking method. The clustering process of AMI-Net allows spatial context-aware masking through token embedding. These methods have a strength of output consistency and fast inference. However, compared to multiple random masking methods, it can potentially fail to provide a mask that completely cover defective regions.\nTo resolve this problem we propose a design component that effectively complements previous binary masking that masks defective features in the decoding process. In addition, to meet the computational efficiency, we make it an extremely tiny design by employing MLP with two layers only."}, {"title": "2.2. Feature representations on UAD", "content": "While most UAD methods image-level reconstruction, some recent studies choose a feature-level reconstruction approach. This method generates an anomaly score by measuring the distance between feature maps. Their experimental results show that defective feature representations are effectively converted into normal feature representations to help reduce false detection. They also proposed a way to avoid an IS issue caused by skip connections. DSR addresses the IS issue by leveraging a latent vector codebook of quantized normal representations for subspace reprojection. HVQ-Trans point out the importance of vector quantization (VQ) in preventing the IS issue and propose a codebook switching framework to replenish frail normal patterns that can be caused by codebook collapse.\nCRAD exploits continuous memory of the normal features to replace suspected abnormal features with the synthetic normal feature representations. They select suspected abnormal features using combined distance of cosine similarity and mean squared error between original input features and fused normal features that are synthesized from the continuous memory of normal.\nWe propose a strategy to attenuate defective representations by predicting suspected defective patches to address the IS issue. At the same time, we make it an easy-to-access design, two-layered MLP, for plug-and-play with other existing models."}, {"title": "3. Methods", "content": ""}, {"title": "3.1. Overview", "content": "An overview of FADER is shown in Figure 3. We integrate the proposed scalable cooperative NN component, dubbed FADER, with a pre-trained UAD model to address the incomplete mask issue. FADER generates a soft mask of suspected defective features from visually defective obfuscated image I'. This soft mask is placed on skip connections to avoid an IS issue. As a result, accurate reconstruction of anomalous patterns by incomplete mask will be mitigated by applying our method."}, {"title": "3.2. Method to resolve incomplete mask", "content": "Our method, FADER, focuses on attenuating defective features possibly delivered through skip connection in incomplete mask issues. At this time, the degree of attenuation varies depending on the predicted patch-wise error by FADER. To train FADER, we should provide ground truth (GT) of feature attenuation but there is no label since it is hard to interpret the feature-level information by humans. Thus, we need to find a breakthrough to train FADER without human label.\nWe take the active learning strategy that allows training NNs with minimal or no labeled data. By leveraging this, the GT for FADER training will be constructed by the reconstruction error between the input image I and the reconstruction result \u00ce in a patch-wise manner. Then, we update FADER to predict the target GT and iterate these two steps during the target training epoch. The graphical representation of this procedure is shown in Figure 4.\nFor patch-size operation, we need to set the patch size. In (b) of Figure 2, the mosaic obfuscation-based hint is provided on masked regions to promote the accurate reconstruction of normal patterns. Note that, an optimal optimal mosaic scale for each product is found and provided by EAR. However, when the mosaic scale and patch size are different from each other, unintended hint of defective informations will be partially passed through skip connection and degrades UAD performance. Thus, we set the patch size for tokenization equal to the mosaic scale to spatially synchronize patch token of FADER and mosaic-based hint.\nBy referring to the above settings, the input I and the reconstruction result \u00ce are divided into patch units and tokenized. These are represented as \\(\\tau_i\\) \\(\\tau^\\prime_i\\) with an index i. Sequentially, the L2 between \\(\\tau_i\\) \\(\\tau^\\prime_i\\) is calculated to generate the GT loss \\(l_i\\). The set of patch-wise GT loss is denoted by \\(l_{gt}\\) (1). Patch-wise error prediction through FADER also begins with the tokenization process. The two-layer MLP, \\(\\mathscr{F}_{FADER}\\), performs loss prediction for each token \\(\\tau\\) as (2).\n\\[l = \\{l_i, 1 \\leq i \\leq n\\} \\text{ s.t. } l_i = \\mathscr{L}_2(\\tau_i, \\tau^\\prime_i) \\quad (1)\\]\n\\[\\hat{l} = \\{\\hat{l}_i, 1 \\leq i \\leq n\\} \\text{ s.t. } \\hat{l}_i = \\mathscr{F}_{FADER}(\\tau) \\quad (2)\\]\nFADER learns to predict patch-wise error considering the suspected defective rank among patches. We employ ranking loss as (3) to train patch-wise error prediction. This allows the model to learn the relative numerical differences of the patch-wise reconstruction error. For reference, training with conventional loss functions such as \\(\\mathscr{L}_2\\) focuses only on the absolute difference of each patch-wise error. This conventional loss function is not recommended to train FADER because includes possible side effect on training. For example when some patches have similar reconstruction error as GT, FADER possibly mislearn the degree of suspected defective and those attenuation rankings may be reversed.\n\\[\\begin{aligned} &\\mathscr{L}_{mrank}(l, \\hat{l}) = \\text{max} \\big( 0, -\\text{I}(l_i, l_j) \\cdot (\\hat{l}_i - \\hat{l}_j) + \\xi \\big) \\quad (3) \\\\ &\\text{s.t. } \\text{I}(l_i, l_j) = \\begin{cases} +1, & \\text{if } l_i > l_j \\\\ -1, & \\text{otherwise} \\end{cases} \\end{aligned}\\]"}, {"title": "3.3. Soft mask for relative attenuation", "content": "When the predicted values for patch-wise error show an equal level overall, it indicates an anomaly-free situation. Conversely, when a significant disparity exists among the predicted patch-wise errors, we can decide there is a suspected defective representation to be masked.\nExisting masking methods highly rely on binary mask \\(M_B\\). This binary masking method has a critical drawback. In scenarios such as the above anomaly-free situation, unnecessary masked regions will be obligatorily generated for some patches that have relatively larger values than others. Then, it will result the inaccurate normal reconstructions.\nTo resolve the issue of binary masking, we propose a soft masking strategy. We first normalize predicted patch-wise error values \\(\\hat{l}\\) into range (0, 1). Then, flip the value of \\(\\hat{l}\\) into (1, 0). We call it a soft mask and mark it with \\(M_S\\). Then, we apply \\(M_S\\) to each skip connection as (4).\n\\[\\Phi_{Dec}^d \\big( \\text{concat}\\big( \\Phi_{Enc}^{d}, \\Phi \\cdot \\sigma (M_S(I)) \\big) \\big) \\quad (4)\\]"}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Experimental setup", "content": "To evaluate our method, we use two public industrial visual inspection datasets MVTec AD and VisA. MVTec AD provides a total of 15 subtasks with 10 objects and 5 textures. Another dataset, VisA, provides 12 different objects including cases where multiple objects exist in a single image. All the training processes are performed with only normal samples. The UAD performance is evaluated by both normal and defective samples.\nImplementation details. On the line of succession EAR, our goal is to achieve better UAD performance while avoiding increasing the scale of NNs. We also avoid scratch learning an NN for a given task. For this, we attach FADER on the pre-trained reconstruction-by-inpainting model EAR. The pre-trained U-Net that we used includes five convolutional blocks (D = 5) for each encoder and decoder. The layer of d-th depth in the encoder is concatenated to the same level decoder via skip connection except for the deepest layer.\nFor each encoder block, we repeat 'convolution \u2192 batch normalization \u2192 leaky ReLU activation' three times. In the third layer of each encoder block, the stride value is set to 2 for downscaling. The decoder is constructed by repeating 'upsampling \u2192 convolution \u2192 batch normalization \u2192 leaky ReLU activation' three times. Note that, the upscaling in the decoding process is performed with the nearest interpolation by the scaling factor 2.\nTraining conditions. In EAR, U-Net was pre-trained on the MVTec AD dataset. EAR also provides an optimal mosaic scale \\(m^*\\) for visual obfuscation-based hint providing. We exploit the \\(m^*\\) as the patch size for the tokenization process in FADER as aforementioned. FADER is trained for 100 epochs with only normal samples. The training process includes a hyperparameter tuning only for conditions related to the learning rate \\(\\eta\\) as follows: 1) 1e-3, 1e-4, and 1e-5 for \\(\\eta\\) and 2) fixed \\(\\eta\\), learning rate warm-up, and SGDR for learning rate scheduling. While training FADER we also conduct fine-tuning of the decoder. We set the learning rate for fine-tuning to the 0.1\\(\\eta\\). This fine-tuning ensures an accurate reconstruction of normal patterns, preserving contained generalization ability.\nEvaluation metric. To evaluate the performance of UAD experiments, the area under the receiver operating characteristic curve (AUROC) is used. The AUROC is measured based on the anomaly scores for each normal and defective sample within the test set at both the image level and the pixel level. The anomaly scoring function is constructed with multi-scale gradient magnitude similarity (MSGMS) that allows the detection of various sizes of defects. The anomaly score of the UAD model for the unseen anomalous patterns will be close to 1 or relatively larger compared to the seen normal cases."}, {"title": "4.2. Visual analysis of soft mask", "content": "The visual comparison between the binary mask and soft mask is shown in Figure 1. The first and the second columns show the input images and the binary masks over them respectively. Those binary masks are generated from the ImageNet pre-trained DINO-VIT attention map. The last third column shows the soft mask of FADER. We observed incomplete mask cases for binary attention mask around the insulation tearing on the right side of the cable and the broken leg of the transistor. This incomplete mask problem of defective regions is resolved by exploiting the soft mask of FADER as shown in the third column. In the soft mask \\(M_S\\), the defective representation will be greatly attenuated during transmission via skip connection of U-Net. This prevents accurate reconstruction of the unseen defective patterns and an IS issue of U-Net.\nIn case of the capsule in Figure 1, the defective region is fully covered by a binary attention mask. When the mask is larger than the defective region, it hinders the reconstruction of normal patterns. Moreover, the EAR method may also give an unnecessary hint of defective representations. On the other hand, the soft mask of FADER recognizes defective regions with an appropriate mask size than the binary mask as shown in the figure. This helps cut out the defective representation and unnecessary hints on defective regions."}, {"title": "4.3. Performance of industrial inspection", "content": "In this section, we quantitatively evaluate our method by comparing existing SOTA methods. We value the importance of edge computing capabilities in the industry. Thus, our goal is to maximize the UAD performance without significantly increasing the scale of NNs. To guarantee the quality of the product, the highest priority is to determine whether given product is acceptable or not at the manufacturing site. Therefore, we compare the image-level AUROC. The models to compare the performance are based on U-Net or similar scales of NNs. The measured performance with MVTec AD dataset is summarized in Table 1.\nFADER is a two-layer MLP attached to pre-trained EAR. Comparing the performance with the plain EAR case, adding FADeR achieves notable performance improvements in the capsule, carpet, metal nut, and pill cases. In the hazelnut and transistor cases, the performance slightly decreases compared to the plain EAR, but it is still comparable to or better than the other competitive methods. Overall, FADER achieves the best performance in 10 out of the 15 subtasks on the MVTec AD dataset, with an average AUROC of 0.964."}, {"title": "4.4. Generalizing FADeR on other attentions", "content": "The characteristic of attention depends on the NN structure and the training dataset. From the visual comparison shown in Figure 6, we can see that the region of interest and its intensity vary depending on the pre-trained attention model.\nOur purpose is to overcome the possible incomplete mask issue in binary attention masks. To generalize the effectiveness of FADER in various pre-trained attention other than DINO-VIT, we also apply FADER to the original plain ViT (Plain-ViT) and WinCLIP. WinCLIP features a zero-shot defect detection by highlighting suspected anomalous regions by leveraging text prompts of defective descriptions.\nIn this experiment, we additionally measure pixel-level AUROC to confirm that the soft mask resolves the incomplete mask issue of defective regions. An increase in pixel-level AUROC indicates that precise defective recognition at pixel-level. The measured image-level and pixel-level AUROC with MVTec AD dataset is summarized in Table 2.\nDINO-VIT is known to give strong attention to patterns that are notably different from their surroundings. When it used for AD purposes, this feature is advantageous to make strong attention to suspected defective regions that are different from surrounding normal patterns. By leveraging this, we generate attention masks with DINO-VIT to cover defective regions in a zero-shot manner. In contrast, the other two attention models show different attention map compared to DINO-VIT. Thus, the incomplete masking cases will also vary when changing the pre-trained attention model for single deterministic mask generation. In addition, it affects the UAD performance. As a result, We confirm that EAR based on DINO-VIT achieves the best AUROC. On the other hand, EAR with other attention cases shows degraded UAD performances because their low-contrast attention makes a large mask.\nThe results of applying FADER are shown to the right side of the '\u2192' in Table 2. The image-level or pixel-level AUROC was increased in 44 out of 45 independent subtasks shown in Table 2, with average increases 2.90% and 3.6%, respectively. The increase in pixel-level AUROC suggests that the soft mask effectively cuts out defective representation in skip connection."}, {"title": "4.5. Ablation study", "content": "We can conduct an ablation study for FADER on 1) a NN structure, 2) a masking method (binary or soft masking), 3) a mask scaling method to apply to a feature map, and 4) an NN to predict patch-wise error for mask generation. FADER employs a two-layer MLP to minimize the increase in computational complexity of the NN. It predicts the patch-wise reconstruction error independently of surrounding patch information. To confirm the positive effect of patch-wise error prediction when reflecting the relationships between patches, we explore the ViT alter to MLP. To demonstrate the effectiveness of the proposed method, we experiment with varying the remaining three conditions.\nThe advantage of using ViT lies in its capability to perform patch-wise error prediction based on understanding inter-patch dependencies through multi-head self-attention. However, there is a marginal performance drop compared to ours in the ViT case as shown in the second column of Table 3. We confirm that the use of a two-layer MLP is the most effective considering the unnecessity of understanding surrounding information for predicting each patch-wise reconstruction error, along with ViT's computational complexity and the AUROC decrease.\nAs shown in the third column, adopting binary masking compared to soft masking makes degrades the UAD performance. Because it performs unnecessary binary feature masking on normal patches. When the soft mask is utilized with bilinear interpolation, some defective representations will leak via a smoothed patch border. Due to this, the case of the bilinear interpolation shows a marginal performance drop compared to the best-performing case, as shown in the fourth column."}, {"title": "4.6. Experiments with other masking methods", "content": "We aim to demonstrate the scalability of FADER with other single deterministic masking methods by this experiment. We adopt two defect segmentation models DSR and MSFlow and defect token prediction model AMI-Net. DSR and MSFlow are trained to perform self-supervised defect segmentation based on synthetic defective samples. AMI-Net is trained to mask outlier tokens by leveraging token embedding and clustering techniques.\nEven if a model trained for defect segmentation is used for masking, the incomplete mask issue will still appear. For example, their defect segmentation performance is slightly lower than AUROC of 1.0 which means defective regions are still partially missed. The detail reasons for incomplete mask issues for each model are as follows: 1) The mask of DSR and AMI-Net tightly fits or smaller than the defect regions. 2) The mask boundaries of MSFlow become ambiguous due to multi-resolution fusion.\nExperimental results are summarized in Table 4. Before applying FADER (left side of each \u2018\u2192'), three models show degraded UAD performances compared to DINO-VIT. However, we confirm that both the UAD performance and the defect localization are improved by applying FADER, shown in each right side of the '\u2192'. We cloud easily achieve a performance gain of 1.7% to 3.8% without any change of the pre-trained U-Net. Thus, we conclude that FADER effectively resolves the incomplete mask issue regardless of the changes in the masking method."}, {"title": "4.7. Experiments on another industrial dataset", "content": "To extend the effectiveness of our method beyond the MVTec AD dataset, we conduct additional evaluations on the VisA dataset. In studies on the VisA dataset, there are very few cases where a single small NN is used. Therefore, we compare the performance with a model twice the scale of U-Net which is widely used structures in recent studies. Those models commonly feature a 2-stage NN architecture: stage 1 performs reconstruction-by-inpainting, and stage 2 conducts defect segmentation. The measured AUROCs are summarized in Table 5. Our method achieves the best AUROC of 0.960 as shown in the last column of Table 5."}, {"title": "5. Conclusion", "content": "This study focuses on resolving the incomplete masking issue of single masking in the reconstruction-by-inpainting-based UAD approach. We propose a simple but powerful NN defective feature attenuation method, FADER. FADER is designed with two-layer MLP to cut out unintended defective information passing caused by an incomplete mask issue. There is no label to train NN for defective feature attenuation but we also effectively overcome this by exploiting an active learning strategy.\nExperimental results on the MVTec AD dataset demonstrated that FADeR outperforms other similar-scale NNs. Also, its scalability and effectiveness are further confirmed through integration with various masking methods and industrial datasets. This suggests the practical feasibility and versatility of FADER in industrial anomaly detection scenarios. In conclusion, FADER offers a reasonable solution by effectively addressing the issues of existing techniques, ensuring better detection accuracy and computational efficiency suitable for edge computing environments."}]}