{"title": "MULTILEVEL INTERPRETABILITY OF ARTIFICIAL NEURAL NETWORKS: LEVERAGING FRAMEWORK AND METHODS FROM NEUROSCIENCE", "authors": ["Zhonghao He", "Jascha Achterberg", "Katie Collins", "Kevin Nejad", "Danyal Akarca", "Yinzhu Yang", "Wes Gurnee", "Ilia Sucholutsky", "Yuhan Tang", "Rebeca Ianov", "George Ogden", "Chole Li", "Kai Sandbrink", "Stephen Casper", "Anna Ivanova", "Grace W. Lindsay"], "abstract": "As deep learning systems are scaled up to many billions of parameters, relating their internal structure to external behaviors becomes very challenging. Although daunting, this problem is not new: neuroscientists and cognitive scientists have accumulated decades of experience analyzing a particularly complicated system - the brain. In this work, we argue that interpreting both biological and artificial neural systems requires analyzing those systems at multiple levels of analysis, with different analytic tools for each level. We first lay out a joint grand challenge among scientists who study the brain and who study artificial neural networks: understanding how distributed neural mechanisms give rise to complex cognition and behavior. We then present a series of analytical tools that can be used to analyze biological and artificial neural systems, organizing those tools according to Marr's three levels of analysis: implementation, algorithm/representation, and behavior/computation. Overall, the multilevel interpretability framework provides a principled way to tackle neural system complexity; links structure, computation, and behavior; clarifies assumptions and research priorities at each level; and paves the way toward a unified effort for understanding intelligent systems, may they be biological or artificial.", "sections": [{"title": "1 Introduction", "content": ""}, {"title": "1.1 Shared Goals and Joint Challenges", "content": "Interpretability research aims to provide a human-understandable explanation for model outputs and behaviors based on the input and model's internal structure [Doshi-Velez and Kim, 2017]. The field's goal is to generate mechanistic explanations of how neural networks perform computations and produce behaviors [Nanda et al., 2023, Olsson et al., 2022], which could help predict the behavior of such networks across a wide range of scenarios and possibly solve notable problems of AI systems, such as hallucination and toxic output [Ji et al., 2023]. Being able to interpret AI systems is therefore a key capability to be able to understand whether models are appropriately fair, reliable, robust, and worthy of user trust [Doshi-Velez and Kim, 2017]."}, {"title": "1.2 Interpretability Today", "content": "There are several notable research agendas and approaches to interpretability today (see [?] for a review). Mechanistic Inter- pretability (MI) is a bottom-up approach focused on understanding how the circuits within a network give rise to its behavior [Nanda et al., 2023]. The main assumptions underlying MI are (1) that features 3 are the fundamental unit of analysis, (2) that features are connected into computational circuits via network weights, and (3) that features and circuits are universal across models [Olah et al., 2020]. While mechanistic interpretability mainly focuses on analyzing trained neural networks, Developmental Interpretability is an emerging research direction that attempts to draw inferences on the computations of neural networks by analyzing their training process and learning dynamics [Nanda et al., 2023, Hoogland et al., 2024, Davies et al., 2023], often by applying tools from singular learning theory [Watanabe, 2009]. Finally, Representation Engineering (RepE), inspired by Hopfieldian view in neuroscience [Barack and Krakauer, 2021], adopts a top-down approach to model transparency by focusing on localizing and editing latent representations of concepts [Zou et al., 2023].\nHow do these approaches relate to one another? Should we focus on circuits, features, and/or representations? How do we draw principled links between the internal organization of a model and its behavior?\nBelow, we show that different interpretability tools can be put to work to address questions at different levels of analysis. Identifying those levels can help interpretability researchers establish the questions that are most appropriate to answer with each tool. We also believe it can help organize and unify disparate approaches and spur new research questions."}, {"title": "1.3 Our Contributions", "content": "Building on key methods and findings from neuroscience research, we propose four ways to enhance the interpretability of machine learning models: Marr's conceptual framework, methods, lessons learned, and promising research directions.\nAs an example, mapping the full set of neurons and their connections in a tiny worm C. elegans by itself was insufficient for describing how the nervous system gives rise to the worm's behavior [Krakauer et al., 2017]."}, {"title": "1.4 How this paper is organized", "content": "We first introduce Marr's levels of analysis, a conceptual multilevel framework for analyzing information processing systems, and explain how this framework can improve our understanding of system behaviors.\nWe then survey methods from neuroscience/cognitive science that can yield insights at each level of analysis and compare these techniques with techniques used in interpretability research. When describing each technique, we give explicit examples of some of what has been learned through such techniques at this level and discuss any limitations.\nIn the discussion section, we explore what we think may be necessary for understanding neuoral systems and their behaviors, notable lessons from neuroscience that may help interpretability, and acknowledge differences between biological and artificial neural systems that may limit the connections between their studies."}, {"title": "2 Multilevel Analysis of Complex Neural Systems", "content": ""}, {"title": "2.1 Marr's Levels of Analysis: a framework for understanding neural systems", "content": "David Marr proposed Marr's three levels of analysis as a conceptual framework for understanding complex neural networks[Marr, 2010]:\n1. Computational. What is the function of a neural system? What is the desirable behavior (outputs) as a function of inputs, current system state, and time?\n2. Representational/Algorithmic. What is the series of computations that can achieve the computational function of the system? How should the relevant information be represented to implement these computations?\n3. Implementation. What is the neural substrate of the algorithm? Which physical components of the system realize the algorithm/representation in question?\nFor example, we could describe what a cash register at the checkout counter in a supermarket does:\n\u2022 At the computational level we know that the cash register performs addition.\n\u2022 At the representation/algorithmic level we could choose from decimal, binary or Roman numerals as representations; for decimal numerals the addition algorithm follows the steps of (i) adding least significant digits first (ii) carrying to more significant ones if the sum exceeds ten."}, {"title": "2.2 The benefits of multi-level analysis of neural systems", "content": "Marr's levels of analysis is a conceptual framework for studying computational systems. The benefits of multi-level analysis frameworks include:\n\u2022 Holistic understanding. A holistic understanding of cognitive behaviors would require analysis across all three levels [Griffiths et al., 2023]. In contrast, research at only one or two levels generates an incomplete understanding of the brain and interested behaviors [Krakauer et al., 2017]. Each of three levels offers distinctive and non-redundant understanding of neural systems. Missing computational analysis would risk being mislead by wrong understanding of task [Bechtel and Shagrir, 2015]; analysis at the representation/algorithm level allows us to understand how the task is done by the neural systems without committing to specific realized implementation [Sober, 1999, Krakauer et al., 2017]; and the implementation level decpompose and localizes the computations in neural network by mapping them onto physical components [Bechtel and Richardson, 2010], which paves the way for establishing causality between internal components to the final output.\n\u2022 Understanding Emergence. Because of emergence, lower-level descriptions say little about neural network behav- iors[Anderson, 1972]. Behaviors and cognition are emergent from the organization of neurons and circuits, having effects that are not apparent in any single neurons [Miller et al., 2024], and therefore should be firstly studied at the computation level [Krakauer et al., 2017, Marr, 2010]. As Marr put it, you should start with aerodynamics if you are interested in bird flight, rather than study the materials of feathers; flocking in birds should be studied at the 'group of birds' level, as some behavioral rules such as \"steer to the average heading of your neighbors\" can never be found by studying an individual bird [Reynolds, 1987].\n\u2022 Consistency and Constraints. Once we delineate three levels of analysis, it should be straightforward to see that different levels of analysis should be consistent with each other. Computational level identify problems to be solved in an environment and determines a computational function for them; through algorithmic and representational analysis we understand how information is encoded and transformed when solving the problems, and implementation identifies the physical form to support such function, representations, and algorithms [Bechtel and Shagrir, 2015].\nThree levels of analysis also enables us to see the constraints applied across levels. Vilas et al. [2024] termed as \"mutual constraints\". Taking a top-down approach, a computational explanation identifies how environmental information constrains the functions performed by the systems; it therefore narrows down the search for possible algorithms and implementations [Griffiths et al., 2023]. With a bottom-up approach, knowledge of the physical system helps to specify what algorithms are implementable, and therefore limit the range of functions possibly performed by the system [Bechtel and Shagrir, 2015].\n\u2022 Goals and questions at each level. One benefit of specifying the level of analysis is to understand what is the goal of the analysis, what questions can be asked, what form the answer should take, and what other analysis can be complementary and therefore necessary for a more complete understanding."}, {"title": "3 The case of deception", "content": "Deception is a behavior that stimulates research interests from both cognitive and brain sciences [Ganis and Keenan, 2009] and AI researchers [Hagendorff, 2024, Shevlane et al., 2023, Williamson and Prybutok, 2024]. Because deception is a cognitive behavior of critical legal, moral and social implications, there is strong interest from the brain sciences community in studying deception [Karim et al., 2010]. There is also growing evidence of AI systems exhibiting deceptive behaviors [Park et al., 2024], such as providing false information to players to win a game [, FAIR, Xu et al., 2023] or to a human worker for the completion of its task [Achiam et al., 2023]. More recently, the mechanistic interpretability community has developed interest in reverse engineering the mechanism for deception in language models in order to control this behavior [Olah and Jermyn, 2023].\nWhat might we gain from a Marr's-levels analysis of AI deception? When investigating the basis of this behavior, we would ask different research questions depending on the level of analysis.\nComputational Level: what is the function of deceptive behavior in the context of a model's overall behavior? What is considered success vs. failure? What aspects of a model's input and internal states determine whether it is going to lie or respond truthfully? What is a formal definition of \"lying\" and \"deception\" when applied to an AI system and when is it theoretically possible for a model to possess such capabilities?\nAlgorithmic/Representational Level: what are possible algorithms underlying deceptive behavior and which of these algorithms is realized in a particular AI model that has been shown to exhibit deceptive behavior? How are inputs combined in order to determine whether deception or truth will be used? How does the model represent the true state of the world and the intention to deceive vs respond truthfully? How do these representations combine to determine the output?\nImplementation Level: what are the physical circuits that implement the deception algorithm? Are representations of the true state of the world and representations of the intention to deceive/respond truthfully localized to particular parts of the AI system or distributed throughout? What is the role of individual layers/units/attention heads in supporting deceptive behavior?\nA reader familiar with the interpretability literature might recognize that much of the previous research has concentrated on implementation questions, with some work also starting to tackle the algorithmic level. Yet the computational level, which is perhaps primary, remains relatively underexplored."}, {"title": "4 Computational/Functional Level of Analysis", "content": "The computational level of analysis aims to study the information-processing system as a whole and to understand the goal of the system. The key at this level is to emphasize a high level description of what functions a system implements without committing to a particular algorithmic strategy [Marr, 2010, Griffiths et al., 2023, Willems, 2011].\nExamples of questions to ask at the computational level:\n\u2022 What is the system capable of? What patterns exist in its behaviors? [Ewert and Ewert, 1980, Watson, 1913]\n\u2022 What are some objectives or computational constraints of the system that shape its functions? [Laughlin and Sejnowski, 2003, Gershman et al., 2015, McCoy et al., 2023]\n\u2022 What are the rules governing different functions such as learning and adaptation? [Dayan and Abbott, 2005, Botvinick et al., 2019]\n\u2022 Do any general principles in function emerge across many different behaviors (or different species)? [Hulse et al., 2018, Dewsbury, 1978]\nIn brain studies, the computational level is frequently tackled with behavioral studies Tinbergen [1963a], Krakauer et al. [2017], sometimes accompanied by abstract mathematical models used to summarize a wide range of behavioral findings. Well-characterized behavior gives insights to the computational goals of a neural system and can provide hints as to the algorithms used to achieve those goals [Blokpoel, 2018]. Computational study also guide the search for algorithms and representations [Griffiths et al., 2010]\nExisting interpretability studies often do not intimately engage with the computational level (problems to be solved by the systems [McCoy et al., 2023], why they are solving those problems [Laughlin and Sejnowski, 2003], environmental constraints [Bechtel, 2009], and, ideal solutions expressed by Bayesian models [Griffiths et al., 2015]) but rather focus on mapping functions to structures (we call implementation level of analysis). Where it exists, research at the computational level of ANNs usually focuses on the general principles of deep learning systems (e.g., scaling law Kaplan et al. [2020], Hestness et al. [2017], Seung et al. [1992], memorization Carlini et al. [2022], hallucination Zhang et al. [2023], inductive biases White and Cotterell [2021], rather than aligning with the interpretability research agenda that focuses on specific behaviors of interest. We believe a synergy between the two can potentially advance our understanding of neural network behaviors.\nIn this section, following an approach from the cognitive science/neuroscience communities that takes computational analysis as the first step into investigating neural systems [Marr, 2010], we highlight the importance of studies at the computation level and the links between different levels. Under this mindset, we advocate an approach that first curates a list of behaviors of interest in the computational system under study (e.g, the mathematics ability of an LLM; see sections 2.2 and 2.4 in the survey paper [Anwar et al., 2024] for a detailed discussion) and understands basic computational principles behind those behaviors, studying problems, tasks, and goals before they try to identify algorithms for it and locating circuits [Krakauer et al., 2017, Griffiths et al., 2010, McCoy et al., 2023].\nA key takeaway of this section for the interpretability community is to reflect on the question of whether you are starting at the top or at the bottom. While the classic bottom-up approach of mechanistic interpretability can help generate explanations with high granularity, a top-down approach may lead to more robust and general explanations of model behavior in some instances. As the neuroscientist Horace Barlow wrote in 1961, \"A wing would be a most mystifying structure if one did not know that birds flew\" [Barlow et al., 1961]."}, {"title": "4.1 Neuroethology", "content": "Ethology is the study of natural animal behavior. Neuroethology is the specific study of behaviors for the purposes of understanding the brain-behavior relationship and how it arises through natural selection [Ewert and Ewert, 1980]. Advances in experimental techniques and methods for monitoring behavior are making it possible to collect large amounts of precise data while animals engage in self-driven and unconstrained behavior [Datta et al., 2019]. Neuroethology tends to focus on innate behaviors (such as foraging) in order to characterize the computational principles of behavior that have emerged through evolution [Cisek, 2019]. This is particularly important to the Marr's levels approach as it has been argued that an evolutionary perspective is necessary when defining the computational level [Anderson, 2015].\nTinbergin's four questions, proposed by Niko Tinbergin in 1963 [Tinbergen, 1963b], provide a framework for studying ethology that captures the multi-faceted style of explanation one may want in order to 'understand' behavior. The questions include: Why is the animal performing the behaviour, i.e. what is its function? Through what historical stages did the behaviour evolve? What mechanism in the animal cause the behaviour to be performed? How does the behaviour develop during the lifetime of an individual? Neuroethology is primarily interested in the neural mechanisms that produce the behavior, but importantly is informed by answers to the other questions as well. While there is not a direct link between Marr's level and the different Tinbergin's questions, work has been done to try to integrate these two frameworks in psychology [Al-Shawaf, 2024]."}, {"title": "4.2 Psychophysics", "content": "Unlike neuroethology, psychophysics focuses on the study of very precise - and largely unnatural' - behaviors-in a laboratory setting. The use of tightly controlled, pared-down stimuli and responses is meant to help isolate specific capabilities, limitations, and mechanisms [Krakauer et al., 2017, Gescheider, 2013, Prins et al., 2016]. In a psychophysics experiment, a subject may, for example, sit in a dark room viewing grayscale images of simple lines or dots on a screen and respond when they perceive a particular stimulus. While psychophysics emerged as a means of measuring how physical stimuli give rise to perception, its methods are also readily applicable to motor and even 'cognitive' questions [Sanes and Evarts, 1984, Waskom et al., 2019].\nFindings Many visual behaviors have been thoroughly characterized through psychophysics. For example, in visual search, studies have shown what features of a stimulus will make it 'pop out' or not, how search behaviors depend on visual statistics, and how it interacts with both short and long-term memory [Wolfe, 2020]. These precise findings put strong constraints on the potential algorithms that could be used by the brain to achieve this behavior. The impacts of visual attention have been similarly characterized and used to explain and predict findings from neurophysiology [Carrasco et al., 2009].\nIn the case of sound localization, behavioral work from nearly 100 years ago established the importance of inter-aural time differences (that is the difference in time it takes for a sound to reach one ear versus the other, ITD) in the sound localization computation"}, {"title": "4.3 Bayesian Approaches to Cognition", "content": "Unlike the above two examples, this line of work does not advocate for a particular type of behavioral experiment but rather puts forth a computational framework that can encapsulate many behavioral findings. Specific, probabilistic models of cognition champion the idea that one can model cognition, from vision and motor control, to language and beyond, with probabilistic models [Chater et al., 2006, 2010]. This thread of research emphasizes that many aspects of human cognition can be captured by positing that humans build generative models of the world and conduct inference under uncertainty in and over models [Ullman and Tenenbaum, 2020, Tenenbaum et al., 2011]. This kind of explanation places principal importance on inductive biases, which may be helpful in understanding the kinds of problems artificial systems are equipped, or can be equipped, to solve [Tenenbaum et al., 2011, Lake et al., 2017, McCoy et al., 2023]. Probabilistic frameworks allow us to address computation level questions such as: What constraints on learning are necessary [Griffiths et al., 2015, Griffiths and Tenenbaum, 2006]? How do children learn to infer the meanings of new words based on just a few examples [Xu and Tenenbaum, 2007]? How do people infer the mental states underlying actions of others [Baker et al., 2009]? Computational questions, as a result, should be addressed with computational theories.\nFindings Neuroscientists have identified strong empirical support for the idea that humans employ Bayesian strategies that combine both knowledge of the statistical distribution of the tasks (prior knowledge) and sensory uncertainties when performing sensorimotor learning [K\u00f6rding and Wolpert, 2004]; similarly, human behavior when combining visual and haptic information is well-explained by a maximum-likelihood model [Ernst and Banks, 2002]. More broadly, researchers have successfully applied probabilistic models to account for many aspects of vision [Kersten and Yuille, 2003, Chater, 1996, Weiss et al., 2002, Tu et al., 2005], language [Xu and Tenenbaum, 2007, Chater and Manning, 2006, Goodman and Frank, 2016, Wong et al., 2023], navigation [Kaelbling et al., 1998, Stankiewicz et al., 2006], causal learning and inference [Pearl, 1988, Gopnik et al., 2004, Lagnado et al., 2013], concept learning [Tenenbaum, 1998, Tenenbaum and Griffiths, 2001], emotion prediction [Houlihan et al., 2023], reasoning about other minds [Baker et al., 2009, 2017, 2011, Ho et al., 2022], how people couple different sensory data to integrate perception with planning [Ernst and Banks, 2002, Clark and Yuille, 2013] and reason about physics [Ullman et al., 2017, Battaglia et al., 2013].\nProbabilistic models of cognition primarily sit at computational level description however, as with other computational level strategies, they can motivate the search for algorithms which underpin behaviour and ultimately be used to explain the roles of neurons and neural populations [Coen et al., 2023, Vaghi et al., 2017]. Indeed, researchers have been able to find evidence for the neural implementations and mechanisms that convey probabilistic information [Chater et al., 2006, Echeveste et al., 2020, Aitchison and Lengyel, 2016]."}, {"title": "4.4 Limitations and Considerations for the Computational Level", "content": "While it is common for researchers studying the brain to start at the computational level and work down through algorithm and implementation, the question of whether this is the best approach is actively debated [Gy\u00f6rgy Buzs\u00e1ki, 2019, Poeppel and Adolfi, 2020]. Specifically, in Gy\u00f6rgy Buzs\u00e1ki [2019], the author argues that instead of starting with psychological terms like 'memory', neuroscientists should focus on the 'hardware' first, by documenting features of neurons and their activity and then asking what function they could implement. This bottom-up approach is meant in part to help move hypotheses about brain function away from simple and intuitive ideas derived from folk psychology and towards more complex and surprising mechanisms [Gy\u00f6rgy Buzs\u00e1ki, 2019]. The debate around which approach is best highlights the ways in which characterizing behavior may inject biases or assumptions into the analysis of a neural system.\nAn important difference between ANNs and the brain with respect to the computational level is the fact that technically the computational aims of an ANN are known and even specified by the builder in the form of the objective function. However, in biological neural systems, a goal of our study is to reverse engineer something akin to the objective function for a brain region [Richards et al., 2019]. It is clear that the capabilities of large ANNs can be described in ways that go well beyond the explicit task they were trained for (e.g., next word prediction for LLMs), and so it may make sense to still try to break down the behavior of these large models into different computational capacities. This is akin to acknowledging from an evolutionary perspective the only 'objective function' of the brain is to support the production of more offspring; however we can still speak of different sub-types of behaviors and abilities."}, {"title": "5 Algorithmic & Representational Level of Analysis", "content": "At the algorithmic or representational level, researchers aim to figure out the question of \"how\" neural systems solve computational problems, in terms of algorithmic steps taken and latent representations used. At this level, the exact nature of the physical implementation becomes less relevant as it is abstracted away through an algorithmic description. Researchers operating on this level are concerned with how information is encoded (therefore \"representation\") and transformed (therefore \"algorithm\") within the system.\nExamples of questions to ask at this level:\n\u2022 How is the relevant information encoded in neural systems? [Quiroga et al., 2005, Nogueira et al., 2023, Stringer et al., 2019, Sucholutsky et al., 2023]\n\u2022 How are abstract concepts represented and manipulated in neural systems? [Dayan and Abbott, 2005, Bernardi et al., 2020, Johnston and Fusi, 2023]\n\u2022 How do neural representations change with learning to achieve computational goals? [Yasuda et al., 2006, Wojcik et al., 2023]\n\u2022 How is information transformed and combined in the neural system to produce intelligent behaviors? [Harris and Mrsic-Flogel, 2013]\n\u2022 What are some strategies neural systems can utilize to solve their computational problems? [Banich and Compton, 2018, van Opheusden et al., 2023]\nMany exciting recent advances in mechanistic interpretability aim to address Level 2 questions. These include the use of sparse autoencoders to separate out responses to different input features [Bricken et al., 2023, Templeton, 2024] and representation engineering approaches that track latent neural trajectories when processing different input types [Zou et al., 2023]. Here, we survey a variety of approaches, old and new, that can help make progress in understanding the algorithms and representations employed by a neural system to achieve its computational goals."}, {"title": "5.1 Reverse-engineering the Algorithm", "content": "Cognitive science has long tried to understand the principles of intelligence common across both minds and machines [Simon, 1980]. In doing so, much of the field inherently works at the algorithmic level, as it builds frameworks that are not meant to be specific to any one physical implementation. One approach to developing such algorithmic descriptions and frameworks is through a form of normative reverse engineering [Zednik and J\u00e4kel, 2016]. In this approach, a detailed characterization of behavior is undertaken and"}, {"title": "5.2 Decoding models (probes) and encoding models", "content": "Often researchers might have specific hypotheses about the features that a system needs to represent to achieve its goal. To test whether a specific neural population represents these features, they can design a mapping between neural responses and the features of interest. If the mapping predicts features of interest (e.g., deceptive vs. truthful behavior) from neural data, it is called a decoding model. If the mapping predicts neural responses from the features of interest, it is called an encoding model (Naselaris et al., 2011); note the difference between this terminology and the traditional use of the terms \"encoder\" and \"decoder\" in machine learning).\nDecoding models have a long history of use in neuroscience and have guided understanding of many different brain regions and systems [Abbott, 1994, Pouget et al., 2000, Ritchie et al., 2019, Kriegeskorte and Douglas, 2019]. The main criticisms of this approach, however, concern the potential lack of a causal connection between the decoded features and the system's function [Weichwald et al., 2015, Holdgraf et al., 2017, Kriegeskorte and Douglas, 2019]. This can happen for many reasons: first, the decoder might pick up on features that are simply correlated with the target features; second, constraining the decoder to be linear (as is commonly done in neuroscience) might be overly limiting (but providing no constraints at all is overly permissive as the decoder may come to implement computations not done by downstream brain regions) [Ivanova et al., 2022]; third, there is no guarantee that the decoded information is actually necessary for performing the task, unless causal manipulations are done in addition to decoding. Another reason for caution, particularly when trying to create an algorithmic level description, is that simply knowing what information is where doesn't tell you what is being done with that information. As said in Kriegeskorte and Douglas [2019], \"Decoding reveals the products, not the process of brain computation\". The ultimate goal at the algorithmic level is to understand not just how information is represented but also how it is processed and transformed.\nA popular alternative to standard linear decoders are information-theoretic approaches [Quiroga and Panzeri, 2009, Timme and Lapish, 2018]. These tools can be used to trace information flow from one brain region to the next, quantify the amount of redundant information between regions, and measure the rate of information loss.\nEncoding models are also ubiquitously used by neuroscientists [Kriegeskorte and Douglas, 2019]. They quantify the amount of variance in neural responses that can be explained by a particular set of features [Dupr\u00e9 la Tour et al., 2024]. They are also useful for mapping between two different systems for comparison, e.g. a brain region and a layer of an artificial NN (which represents a set of pre-processed features) [Yamins and DiCarlo, 2016, Schrimpf et al., 2018, Antonello et al., 2023].\nFindings In neuroimaging, multi-voxel pattern analysis (MVPA) has been used to decode task information from patterns of BOLD values [Norman et al., 2006, Mahmoudi et al., 2012]; this was designed to move fMRI analysis beyond simple questions of where activity goes up or down to questions of what information is present in this activity. MVPA studies have been used, for example, to demonstrate how spatial information is encoded in human hippocampus [Kim et al., 2017], to predict symptom severity for certain disorders [Coutanche et al., 2011], and to verify the frontoparietal network's role in domain-general computations [Woolgar et al., 2016].\nDecoders have also been used to identify the time course of information processing. Specifically, by measuring how well a decoder trained on activity from one time point in an experiment's trial generalizes to other time points, researchers can learn about the"}, {"title": "5.3 Neural Population Geometry", "content": "Neural population geometry studies the arrangement of high-dimensional neural representations during complex cognitive tasks using mathematical and computational tools [Chung and Abbott, 2021]. The goal is to understand how information is embedded and processed in neural population activity and how these properties reflect the algorithm implemented by the population. The main benefit of analyzing the geometry of neural representations is the ability to abstract away from the specific architecture of a given neural system with the goal of comparing their representational solutions to a given computational problem. Often these approaches data-driven rather than hypothesis-driven (in contrast to the approaches above).\nGeometry is typically defined by how the neural responses to different inputs or conditions relate to each other in neural activity space [Kriegeskorte and Kievit, 2013]. This geometry can be thought of as an emergent property of individual cell responses, but it is inherently defined at the population level [Kriegeskorte and Wei, 2021]. The way in which neural activity is organized puts constraints on how the information it encodes can be read out and transformed by downstream networks, which makes it important for understanding task performance [Chung and Abbott, 2021].\nA variety of methods can be used to study different aspects of neural geometry [Sucholutsky et al., 2023]. Representational similarity analysis measures the correlation between neural responses to a set of inputs or states; this indicates which features most affect this neural population's response patterns. Variants of this approach aim to relax the strict correlation measurement by fitting different weights to different neurons and/or creating linear combinations of original features, thus effectively performing affine transformations on the neural activation space [Khaligh-Razavi et al., 2017, ?].\nDimensionality reduction is a common strategy to project neural responses into a low-dimensional space and then track their clustering patterns for different inputs, or the evolution of response trajectories over time [Cunningham and Yu, 2014]. To elicit meaningful latent dimensions during dimensionality reduction, neuroscientists have used a variety of techniques, from the standard principal component analysis, PCA [Sohn et al., 2019] to independent component analysis, ICA [Norman-Haignere et al., 2015] to bespoke"}, {"title": "5.4 Limitations and Considerations for the Algorithmic/Representational Level", "content": "The algorithmic level can be heavily constrained by the implementation and computational levels. By many respects, the algo- rithmic/representational level is the least constrained and therefore most difficult level of description to pin down. Insofar as an algorithm is meant to describe a process that could be implemented by many different physical implementations, it is not itself physical. While the algorithm is meant to be independent of the implementation, for any given information processing system, the physical implementation will put constraints on what algorithms it can realize [Bechtel and Shagrir, 2015]. This, combined"}, {"title": "6 Implementation Level of Analysis", "content": "At the implementation level", "level": "n\u2022 Can a particular function be localized to certain components of the brain/ANN (where component refers to specific parts such as a neurons or a attention head) [Olsson et al., 2022, Rehman and Al Khalili, 2023, Anand and Dhikav, 2012, Huff et al., 2018"}]}