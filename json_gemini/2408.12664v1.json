{"title": "MULTILEVEL INTERPRETABILITY OF ARTIFICIAL NEURAL NETWORKS: LEVERAGING FRAMEWORK AND METHODS FROM NEUROSCIENCE", "authors": ["Zhonghao He", "Jascha Achterberg", "Katie Collins", "Kevin Nejad", "Danyal Akarca", "Yinzhu Yang", "Wes Gurnee", "Ilia Sucholutsky", "Yuhan Tang", "Rebeca Ianov", "George Ogden", "Chole Li", "Kai Sandbrink", "Stephen Casper", "Anna Ivanova", "Grace W. Lindsay"], "abstract": "As deep learning systems are scaled up to many billions of parameters, relating their internal structure to external\nbehaviors becomes very challenging. Although daunting, this problem is not new: neuroscientists and cognitive\nscientists have accumulated decades of experience analyzing a particularly complicated system - the brain.\nIn this work, we argue that interpreting both biological and artificial neural systems requires analyzing those\nsystems at multiple levels of analysis, with different analytic tools for each level. We first lay out a joint grand\nchallenge among scientists who study the brain and who study artificial neural networks: understanding how\ndistributed neural mechanisms give rise to complex cognition and behavior. We then present a series of analytical\ntools that can be used to analyze biological and artificial neural systems, organizing those tools according to\nMarr's three levels of analysis: implementation, algorithm/representation, and behavior/computation. Overall,\nthe multilevel interpretability framework provides a principled way to tackle neural system complexity; links\nstructure, computation, and behavior; clarifies assumptions and research priorities at each level; and paves the\nway toward a unified effort for understanding intelligent systems, may they be biological or artificial.", "sections": [{"title": "1 Introduction", "content": ""}, {"title": "1.1 Shared Goals and Joint Challenges", "content": "Interpretability research aims to provide a human-understandable explanation for model outputs and behaviors based on the input\nand model's internal structure [Doshi-Velez and Kim, 2017]. The field's goal is to generate mechanistic explanations of how neural\nnetworks perform computations and produce behaviors [Nanda et al., 2023, Olsson et al., 2022], which could help predict the\nbehavior of such networks across a wide range of scenarios and possibly solve notable problems of AI systems, such as hallucination\nand toxic output [Ji et al., 2023]. Being able to interpret AI systems is therefore a key capability to be able to understand whether\nmodels are appropriately fair, reliable, robust, and worthy of user trust [Doshi-Velez and Kim, 2017]."}, {"title": "1.2 Interpretability Today", "content": "There are several notable research agendas and approaches to interpretability today (see [?] for a review). Mechanistic Inter-\npretability (MI) is a bottom-up approach focused on understanding how the circuits within a network give rise to its behavior [Nanda\net al., 2023]. The main assumptions underlying MI are (1) that features 3 are the fundamental unit of analysis, (2) that features are\nconnected into computational circuits via network weights, and (3) that features and circuits are universal across models [Olah et al.,\n2020]. While mechanistic interpretability mainly focuses on analyzing trained neural networks, Developmental Interpretability is\nan emerging research direction that attempts to draw inferences on the computations of neural networks by analyzing their training\nprocess and learning dynamics [Nanda et al., 2023, Hoogland et al., 2024, Davies et al., 2023], often by applying tools from singular\nlearning theory [Watanabe, 2009]. Finally, Representation Engineering (RepE), inspired by Hopfieldian view in neuroscience\n[Barack and Krakauer, 2021], adopts a top-down approach to model transparency by focusing on localizing and editing latent\nrepresentations of concepts [Zou et al., 2023].\nHow do these approaches relate to one another? Should we focus on circuits, features, and/or representations? How do we draw\nprincipled links between the internal organization of a model and its behavior?\nBelow, we show that different interpretability tools can be put to work to address questions at different levels of analysis. Identifying\nthose levels can help interpretability researchers establish the questions that are most appropriate to answer with each tool. We also\nbelieve it can help organize and unify disparate approaches and spur new research questions."}, {"title": "1.3 Our Contributions", "content": "Building on key methods and findings from neuroscience research, we propose four ways to enhance the interpretability of machine\nlearning models: Marr's conceptual framework, methods, lessons learned, and promising research directions.\nAs an example, mapping the full set of neurons and their connections in a tiny worm C. elegans by itself was insufficient for\ndescribing how the nervous system gives rise to the worm's behavior [Krakauer et al., 2017]."}, {"title": "1.4 How this paper is organized", "content": "We first introduce Marr's levels of analysis, a conceptual multilevel framework for analyzing information processing systems, and\nexplain how this framework can improve our understanding of system behaviors.\nWe then survey methods from neuroscience/cognitive science that can yield insights at each level of analysis and compare these\ntechniques with techniques used in interpretability research. When describing each technique, we give explicit examples of some of\nwhat has been learned through such techniques at this level and discuss any limitations.\nIn the discussion section, we explore what we think may be necessary for understanding neuoral systems and their behaviors, notable\nlessons from neuroscience that may help interpretability, and acknowledge differences between biological and artificial neural\nsystems that may limit the connections between their studies."}, {"title": "2 Multilevel Analysis of Complex Neural Systems", "content": ""}, {"title": "2.1 Marr's Levels of Analysis: a framework for understanding neural systems", "content": "David Marr proposed Marr's three levels of analysis as a conceptual framework for understanding complex neural networks[Marr,\n2010]:\n1. Computational. What is the function of a neural system? What is the desirable behavior (outputs) as a function of inputs,\ncurrent system state, and time?\n2. Representational/Algorithmic. What is the series of computations that can achieve the computational function of the\nsystem? How should the relevant information be represented to implement these computations?\n3. Implementation. What is the neural substrate of the algorithm? Which physical components of the system realize the\nalgorithm/representation in question?\nFor example, we could describe what a cash register at the checkout counter in a supermarket does:\n\u2022 At the computational level we know that the cash register performs addition.\n\u2022 At the representation/algorithmic level we could choose from decimal, binary or Roman numerals as representations; for\ndecimal numerals the addition algorithm follows the steps of (i) adding least significant digits first (ii) carrying to more\nsignificant ones if the sum exceeds ten."}, {"title": "2.2 The benefits of multi-level analysis of neural systems", "content": "Marr's levels of analysis is a conceptual framework for studying computational systems. The benefits of multi-level analysis\nframeworks include:\n\u2022 Holistic understanding. A holistic understanding of cognitive behaviors would require analysis across all three levels\n[Griffiths et al., 2023]. In contrast, research at only one or two levels generates an incomplete understanding of the brain\nand interested behaviors [Krakauer et al., 2017]. Each of three levels offers distinctive and non-redundant understanding\nof neural systems. Missing computational analysis would risk being mislead by wrong understanding of task [Bechtel\nand Shagrir, 2015]; analysis at the representation/algorithm level allows us to understand how the task is done by the\nneural systems without committing to specific realized implementation [Sober, 1999, Krakauer et al., 2017]; and the\nimplementation level decpompose and localizes the computations in neural network by mapping them onto physical\ncomponents [Bechtel and Richardson, 2010], which paves the way for establishing causality between internal components\nto the final output.\n\u2022 Understanding Emergence. Because of emergence, lower-level descriptions say little about neural network behav-\niors[Anderson, 1972]. Behaviors and cognition are emergent from the organization of neurons and circuits, having effects\nthat are not apparent in any single neurons [Miller et al., 2024], and therefore should be firstly studied at the computation\nlevel [Krakauer et al., 2017, Marr, 2010]. As Marr put it, you should start with aerodynamics if you are interested in bird\nflight, rather than study the materials of feathers; flocking in birds should be studied at the 'group of birds' level, as some\nbehavioral rules such as \"steer to the average heading of your neighbors\" can never be found by studying an individual bird\n[Reynolds, 1987].\n\u2022 Consistency and Constraints. Once we delineate three levels of analysis, it should be straightforward to see that\ndifferent levels of analysis should be consistent with each other. Computational level identify problems to be solved in an\nenvironment and determines a computational function for them; through algorithmic and representational analysis we\nunderstand how information is encoded and transformed when solving the problems, and implementation identifies the\nphysical form to support such function, representations, and algorithms [Bechtel and Shagrir, 2015].\nThree levels of analysis also enables us to see the constraints applied across levels. Vilas et al. [2024] termed as \"mutual\nconstraints\". Taking a top-down approach, a computational explanation identifies how environmental information constrains\nthe functions performed by the systems; it therefore narrows down the search for possible algorithms and implementations\n[Griffiths et al., 2023]. With a bottom-up approach, knowledge of the physical system helps to specify what algorithms are\nimplementable, and therefore limit the range of functions possibly performed by the system [Bechtel and Shagrir, 2015].\n\u2022 Goals and questions at each level. One benefit of specifying the level of analysis is to understand what is the goal of the\nanalysis, what questions can be asked, what form the answer should take, and what other analysis can be complementary\nand therefore necessary for a more complete understanding."}, {"title": "3 The case of deception", "content": "Deception is a behavior that stimulates research interests from both cognitive and brain sciences [Ganis and Keenan, 2009] and AI\nresearchers [Hagendorff, 2024, Shevlane et al., 2023, Williamson and Prybutok, 2024]. Because deception is a cognitive behavior of\ncritical legal, moral and social implications, there is strong interest from the brain sciences community in studying deception [Karim\net al., 2010]. There is also growing evidence of AI systems exhibiting deceptive behaviors [Park et al., 2024], such as providing false\ninformation to players to win a game [, FAIR, Xu et al., 2023] or to a human worker for the completion of its task [Achiam et al.,\n2023]. More recently, the mechanistic interpretability community has developed interest in reverse engineering the mechanism for\ndecption in language models in order to control this behavior [Olah and Jermyn, 2023].\nWhat might we gain from a Marr's-levels analysis of AI deception? When investigating the basis of this behavior, we would ask\ndifferent research questions depending on the level of analysis.\nComputational Level: what is the function of deceptive behavior in the context of a model's overall behavior? What is considered\nsuccess vs. failure? What aspects of a model's input and internal states determine whether it is going to lie or respond truthfully?\nWhat is a formal definition of \"lying\" and \"deception\" when applied to an AI system and when is it theoretically possible for a model\nto possess such capabilities?\nAlgorithmic/Representational Level: what are possible algorithms underlying deceptive behavior and which of these algorithms is\nrealized in a particular AI model that has been shown to exhibit deceptive behavior? How are inputs combined in order to determine\nwhether deception or truth will be used? How does the model represent the true state of the world and the intention to deceive vs\nrespond truthfully? How do these representations combine to determine the output?\nImplementation Level: what are the physical circuits that implement the deception algorithm? Are representations of the true state of\nthe world and representations of the intention to deceive/respond truthfully localized to particular parts of the AI system or distributed\nthroughout? What is the role of individual layers/units/attention heads in supporting deceptive behavior?\nA reader familiar with the interpretability literature might recognize that much of the previous research has concentrated on\nimplementation questions, with some work also starting to tackle the algorithmic level. Yet the computational level, which is perhaps\nprimary, remains relatively underexplored."}, {"title": "4 Computational/Functional Level of Analysis", "content": "The computational level of analysis aims to study the information-processing system as a whole and to understand the goal of the\nsystem. The key at this level is to emphasize a high level description of what functions a system implements without committing to a\nparticular algorithmic strategy [Marr, 2010, Griffiths et al., 2023, Willems, 2011].\nExamples of questions to ask at the computational level:\n\u2022 What is the system capable of? What patterns exist in its behaviors? [Ewert and Ewert, 1980, Watson, 1913]\n\u2022 What are some objectives or computational constraints of the system that shape its functions? [Laughlin and Sejnowski,\n2003, Gershman et al., 2015, McCoy et al., 2023]\n\u2022 What are the rules governing different functions such as learning and adaptation? [Dayan and Abbott, 2005, Botvinick\net al., 2019]\n\u2022 Do any general principles in function emerge across many different behaviors (or different species)? [Hulse et al., 2018,\nDewsbury, 1978]\nIn brain studies, the computational level is frequently tackled with behavioral studies Tinbergen [1963a], Krakauer et al. [2017],\nsometimes accompanied by abstract mathematical models used to summarize a wide range of behavioral findings. Well-characterized\nbehavior gives insights to the computational goals of a neural system and can provide hints as to the algorithms used to achieve those\ngoals [Blokpoel, 2018]. Computational study also guide the search for algorithms and representations [Griffiths et al., 2010]\nExisting interpretability studies often do not intimately engage with the computational level (problems to be solved by the systems\n[McCoy et al., 2023], why they are solving those problems [Laughlin and Sejnowski, 2003], environmental constraints [Bechtel,\n2009], and, ideal solutions expressed by Bayesian models [Griffiths et al., 2015]) but rather focus on mapping functions to structures\n(we call implementation level of analysis). Where it exists, research at the computational level of ANNs usually focuses on the\ngeneral principles of deep learning systems (e.g., scaling law Kaplan et al. [2020], Hestness et al. [2017], Seung et al. [1992],\nmemorization Carlini et al. [2022], hallucination Zhang et al. [2023], inductive biases White and Cotterell [2021], rather than aligning\nwith the interpretability research agenda that focuses on specific behaviors of interest. We believe a synergy between the two can\npotentially advance our understanding of neural network behaviors.\nIn this section, following an approach from the cognitive science/neuroscience communities that takes computational analysis as the\nfirst step into investigating neural systems [Marr, 2010], we highlight the importance of studies at the computation level and the\nlinks between different levels. Under this mindset, we advocate an approach that first curates a list of behaviors of interest in the\ncomputational system under study (e.g, the mathematics ability of an LLM; see sections 2.2 and 2.4 in the survey paper [Anwar\net al., 2024] for a detailed discussion) and understands basic computational principles behind those behaviors, studying problems,\ntasks, and goals before they try to identify algorithms for it and locating circuits [Krakauer et al., 2017, Griffiths et al., 2010, McCoy\net al., 2023].\nA key takeaway of this section for the interpretability community is to reflect on the question of whether you are starting at the top\nor at the bottom. While the classic bottom-up approach of mechanistic interpretability can help generate explanations with high\ngranularity, a top-down approach may lead to more robust and general explanations of model behavior in some instances. As the\nneuroscientist Horace Barlow wrote in 1961, \"A wing would be a most mystifying structure if one did not know that birds flew\"\n[Barlow et al., 1961]."}, {"title": "4.1 Neuroethology", "content": "Ethology is the study of natural animal behavior. Neuroethology is the specific study of behaviors for the purposes of understanding\nthe brain-behavior relationship and how it arises through natural selection [Ewert and Ewert, 1980]. Advances in experimental\ntechniques and methods for monitoring behavior are making it possible to collect large amounts of precise data while animals\nengage in self-driven and unconstrained behavior [Datta et al., 2019]. Neuroethology tends to focus on innate behaviors (such as\nforaging) in order to characterize the computational principles of behavior that have emerged through evolution [Cisek, 2019]. This\nis particularly important to the Marr's levels approach as it has been argued that an evolutionary perspective is necessary when\ndefining the computational level [Anderson, 2015].\nTinbergin's four questions, proposed by Niko Tinbergin in 1963 [Tinbergen, 1963b], provide a framework for studying ethology\nthat captures the multi-faceted style of explanation one may want in order to 'understand' behavior. The questions include: Why\nis the animal performing the behaviour, i.e. what is its function? Through what historical stages did the behaviour evolve? What\nmechanism in the animal cause the behaviour to be performed? How does the behaviour develop during the lifetime of an individual?\nNeuroethology is primarily interested in the neural mechanisms that produce the behavior, but importantly is informed by answers to\nthe other questions as well. While there is not a direct link between Marr's level and the different Tinbergin's questions, work has\nbeen done to try to integrate these two frameworks in psychology [Al-Shawaf, 2024]."}, {"title": "4.2 Psychophysics", "content": "Unlike neuroethology, psychophysics focuses on the study of very precise-and largely unnatural' behaviors-in a laboratory setting.\nThe use of tightly controlled, pared-down stimuli and responses is meant to help isolate specific capabilities, limitations, and\nmechanisms [Krakauer et al., 2017, Gescheider, 2013, Prins et al., 2016]. In a psychophysics experiment, a subject may, for example,\nsit in a dark room viewing grayscale images of simple lines or dots on a screen and respond when they perceive a particular stimulus.\nWhile psychophysics emerged as a means of measuring how physical stimuli give rise to perception, its methods are also readily\napplicable to motor and even 'cognitive' questions [Sanes and Evarts, 1984, Waskom et al., 2019].\nFindings Many visual behaviors have been thoroughly characterized through psychophysics. For example, in visual search,\nstudies have shown what features of a stimulus will make it 'pop out' or not, how search behaviors depend on visual statistics, and\nhow it interacts with both short and long-term memory [Wolfe, 2020]. These precise findings put strong constraints on the potential\nalgorithms that could be used by the brain to achieve this behavior. The impacts of visual attention have been similarly characterized\nand used to explain and predict findings from neurophysiology [Carrasco et al., 2009].\nIn the case of sound localization, behavioral work from nearly 100 years ago established the importance of inter-aural time differences\n(that is the difference in time it takes for a sound to reach one ear versus the other, ITD) in the sound localization computation"}, {"title": "4.3 Bayesian Approaches to Cognition", "content": "Unlike the above two examples, this line of work does not advocate for a particular type of behavioral experiment but rather puts forth\na computational framework that can encapsulate many behavioral findings. Specific, probabilistic models of cognition champion\nthe idea that one can model cognition, from vision and motor control, to language and beyond, with probabilistic models [Chater\net al., 2006, 2010]. This thread of research emphasizes that many aspects of human cognition can be captured by positing that\nhumans build generative models of the world and conduct inference under uncertainty in and over models [Ullman and Tenenbaum,\n2020, Tenenbaum et al., 2011]. This kind of explanation places principal importance on inductive biases, which may be helpful in\nunderstanding the kinds of problems artificial systems are equipped, or can be equipped, to solve [Tenenbaum et al., 2011, Lake\net al., 2017, McCoy et al., 2023]. Probabilistic frameworks allow us to address computation level questions such as: What constraints\non learning are necessary [Griffiths et al., 2015, Griffiths and Tenenbaum, 2006]? How do children learn to infer the meanings of\nnew words based on just a few examples [Xu and Tenenbaum, 2007]? How do people infer the mental states underlying actions of\nothers [Baker et al., 2009]? Computational questions, as a result, should be addressed with computational theories.\nFindings Neuroscientists have identified strong empirical support for the idea that humans employ Bayesian strategies that\ncombine both knowledge of the statistical distribution of the tasks (prior knowledge) and sensory uncertainties when performing\nsensorimotor learning [K\u00f6rding and Wolpert, 2004]; similarly, human behavior when combining visual and haptic information is\nwell-explained by a maximum-likelihood model [Ernst and Banks, 2002]. More broadly, researchers have successfully applied\nprobabilistic models to account for many aspects of vision [Kersten and Yuille, 2003, Chater, 1996, Weiss et al., 2002, Tu et al.,\n2005], language [Xu and Tenenbaum, 2007, Chater and Manning, 2006, Goodman and Frank, 2016, Wong et al., 2023], navigation\n[Kaelbling et al., 1998, Stankiewicz et al., 2006], causal learning and inference [Pearl, 1988, Gopnik et al., 2004, Lagnado et al.,\n2013], concept learning [Tenenbaum, 1998, Tenenbaum and Griffiths, 2001], emotion prediction [Houlihan et al., 2023], reasoning\nabout other minds [Baker et al., 2009, 2017, 2011, Ho et al., 2022], how people couple different sensory data to integrate perception\nwith planning [Ernst and Banks, 2002, Clark and Yuille, 2013] and reason about physics [Ullman et al., 2017, Battaglia et al., 2013].\nProbabilistic models of cognition primarily sit at computational level description however, as with other computational level strategies,\nthey can motivate the search for algorithms which underpin behaviour and ultimately be used to explain the roles of neurons and\nneural populations [Coen et al., 2023, Vaghi et al., 2017]. Indeed, researchers have been able to find evidence for the neural\nimplementations and mechanisms that convey probabilistic information [Chater et al., 2006, Echeveste et al., 2020, Aitchison and\nLengyel, 2016]."}, {"title": "4.4 Limitations and Considerations for the Computational Level", "content": "While it is common for researchers studying the brain to start at the computational level and work down through algorithm and\nimplementation, the question of whether this is the best approach is actively debated [Gy\u00f6rgy Buzs\u00e1ki, 2019, Poeppel and Adolfi,\n2020]. Specifically, in Gy\u00f6rgy Buzs\u00e1ki [2019], the author argues that instead of starting with psychological terms like 'memory',\nneuroscientists should focus on the 'hardware' first, by documenting features of neurons and their activity and then asking what\nfunction they could implement. This bottom-up approach is meant in part to help move hypotheses about brain function away from\nsimple and intuitive ideas derived from folk psychology and towards more complex and surprising mechanisms [Gy\u00f6rgy Buzs\u00e1ki,\n2019]. The debate around which approach is best highlights the ways in which characterizing behavior may inject biases or\nassumptions into the analysis of a neural system.\nAn important difference between ANNs and the brain with respect to the computational level is the fact that technically the\ncomputational aims of an ANN are known and even specified by the builder in the form of the objective function. However, in\nbiological neural systems, a goal of our study is to reverse engineer something akin to the objective function for a brain region\n[Richards et al., 2019]. It is clear that the capabilities of large ANNs can be described in ways that go well beyond the explicit task\nthey were trained for (e.g., next word prediction for LLMs), and so it may make sense to still try to break down the behavior of\nthese large models into different computational capacities. This is akin to acknowledging from an evolutionary perspective the only\n'objective function' of the brain is to support the production of more offspring; however we can still speak of different sub-types of\nbehaviors and abilities."}, {"title": "5 Algorithmic & Representational Level of Analysis", "content": "At the algorithmic or representational level, researchers aim to figure out the question of \"how\" neural systems solve computational\nproblems, in terms of algorithmic steps taken and latent representations used. At this level, the exact nature of the physical\nimplementation becomes less relevant as it is abstracted away through an algorithmic description. Researchers operating on this\nlevel are concerned with how information is encoded (therefore \"representation\") and transformed (therefore \"algorithm\") within the\nsystem.\nExamples of questions to ask at this level:\n\u2022 How is the relevant information encoded in neural systems? [Quiroga et al., 2005, Nogueira et al., 2023, Stringer et al.,\n2019, Sucholutsky et al., 2023]\n\u2022 How are abstract concepts represented and manipulated in neural systems? [Dayan and Abbott, 2005, Bernardi et al., 2020,\nJohnston and Fusi, 2023]\n\u2022 How do neural representations change with learning to achieve computational goals? [Yasuda et al., 2006, Wojcik et al.,\n2023]\n\u2022 How is information transformed and combined in the neural system to produce intelligent behaviors? [Harris and\nMrsic-Flogel, 2013]\n\u2022 What are some strategies neural systems can utilize to solve their computational problems? [Banich and Compton, 2018,\nvan Opheusden et al., 2023]\nMany exciting recent advances in mechanistic interpretability aim to address Level 2 questions. These include the use of sparse\nautoencoders to separate out responses to different input features [Bricken et al., 2023, Templeton, 2024] and representation\nengineering approaches that track latent neural trajectories when processing different input types [Zou et al., 2023]. Here, we survey\na variety of approaches, old and new, that can help make progress in understanding the algorithms and representations employed by a\nneural system to achieve its computational goals."}, {"title": "5.1 Reverse-engineering the Algorithm", "content": "Cognitive science has long tried to understand the principles of intelligence common across both minds and machines [Simon, 1980].\nIn doing so, much of the field inherently works at the algorithmic level, as it builds frameworks that are not meant to be specific to\nany one physical implementation. One approach to developing such algorithmic descriptions and frameworks is through a form of\nnormative reverse engineering [Zednik and J\u00e4kel, 2016]. In this approach, a detailed characterization of behavior is undertaken and"}, {"title": "5.2 Decoding models (probes) and encoding models", "content": "Often researchers might have specific hypotheses about the features that a system needs to represent to achieve its goal. To test\nwhether a specific neural population represents these features, they can design a mapping between neural responses and the features\nof interest. If the mapping predicts features of interest (e.g., deceptive vs. truthful behavior) from neural data, it is called a decoding\nmodel. If the mapping predicts neural responses from the features of interest, it is called an encoding model (Naselaris et al., 2011);\nnote the difference between this terminology and the traditional use of the terms \"encoder\" and \"decoder\" in machine learning).\nDecoding models have a long history of use in neuroscience and have guided understanding of many different brain regions and\nsystems [Abbott, 1994, Pouget et al., 2000, Ritchie et al., 2019, Kriegeskorte and Douglas, 2019]. The main criticisms of this\napproach, however, concern the potential lack of a causal connection between the decoded features and the system's function\n[Weichwald et al., 2015, Holdgraf et al., 2017, Kriegeskorte and Douglas, 2019]. This can happen for many reasons: first, the decoder\nmight pick up on features that are simply correlated with the target features; second, constraining the decoder to be linear (as is\ncommonly done in neuroscience) might be overly limiting (but providing no constraints at all is overly permissive as the decoder\nmay come to implement computations not done by downstream brain regions) [Ivanova et al., 2022]; third, there is no guarantee\nthat the decoded information is actually necessary for performing the task, unless causal manipulations are done in addition to\ndecoding. Another reason for caution, particularly when trying to create an algorithmic level description, is that simply knowing\nwhat information is where doesn't tell you what is being done with that information. As said in Kriegeskorte and Douglas [2019],\n\"Decoding reveals the products, not the process of brain computation\". The ultimate goal at the algorithmic level is to understand not\njust how information is represented but also how it is processed and transformed.\nA popular alternative to standard linear decoders are information-theoretic approaches [Quiroga and Panzeri, 2009, Timme and\nLapish, 2018]. These tools can be used to trace information flow from one brain region to the next, quantify the amount of redundant\ninformation between regions, and measure the rate of information loss.\nEncoding models are also ubiquitously used by neuroscientists [Kriegeskorte and Douglas, 2019]. They quantify the amount of\nvariance in neural responses that can be explained by a particular set of features [Dupr\u00e9 la Tour et al., 2024]. They are also useful for\nmapping between two different systems for comparison, e.g. a brain region and a layer of an artificial NN (which represents a set of\npre-processed features) [Yamins and DiCarlo, 2016, Schrimpf et al., 2018, Antonello et al., 2023]."}, {"title": "5.3 Neural Population Geometry", "content": "Neural population geometry studies the arrangement of high-dimensional neural representations during complex cognitive tasks\nusing mathematical and computational tools [Chung and Abbott, 2021]. The goal is to understand how information is embedded and\nprocessed in neural population activity and how these properties reflect the algorithm implemented by the population. The main\nbenefit of analyzing the geometry of neural representations is the ability to abstract away from the specific architecture of a given\nneural system with the goal of comparing their representational solutions to a given computational problem. Often these approaches\ndata-driven rather than hypothesis-driven (in contrast to the approaches above).\nGeometry is typically defined by how the neural responses to different inputs or conditions relate to each other in neural activity\nspace [Kriegeskorte and Kievit, 2013]. This geometry can be thought of as an emergent property of individual cell responses, but\nit is inherently defined at the population level [Kriegeskorte and Wei, 2021]. The way in which neural activity is organized puts\nconstraints on how the information it encodes can be read out and transformed by downstream networks, which makes it important\nfor understanding task performance [Chung and Abbott, 2021].\nA variety of methods can be used to study different aspects of neural geometry [Sucholutsky et al., 2023]. Representational similarity\nanalysis measures the correlation between neural responses to a set of inputs or states; this indicates which features most affect\nthis neural population's response patterns. Variants of this approach aim to relax the strict correlation measurement by fitting\ndifferent weights to different neurons and/or creating linear combinations of original features, thus effectively performing affine\ntransformations on the neural activation space [Khaligh-Razavi et al., 2017, ?].\nDimensionality reduction is a common strategy to project neural responses into a low-dimensional space and then track their clustering\npatterns for different inputs, or the evolution of response trajectories over time [Cunningham and Yu, 2014]. To elicit meaningful\nlatent dimensions during dimensionality reduction, neuroscientists have used a variety of techniques, from the standard principal\ncomponent analysis, PCA [Sohn et al., 2019] to independent component analysis, ICA [Norman-Haignere et al., 2015] to bespoke"}, {"title": "5.4 Limitations and Considerations for the Algorithmic/Representational Level", "content": "The algorithmic level can be heavily constrained by the implementation and computational levels. By many respects, the algo-\nrithmic/representational level is the least constrained and therefore most difficult level of description to pin down. Insofar as an\nalgorithm is meant to describe a process that could be implemented by many different physical implementations, it is not itself\nphysical. While the algorithm is meant to be independent of the implementation, for any given information processing system,\nthe physical implementation will put constraints on what algorithms it can realize [Bechtel and Shagrir, 2015]. This, combined"}, {"title": "6 Implementation Level of Analysis", "content": "At the implementation level, researchers aim to attribute mechanistic functions to individual and independent parts of"}]}