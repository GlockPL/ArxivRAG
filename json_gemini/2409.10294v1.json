{"title": "MGSA: Multi-granularity Graph Structure Attention\nfor Knowledge Graph-to-Text Generation", "authors": ["Shanshan Wang", "Chun Zhang", "Ning Zhang"], "abstract": "The Knowledge Graph-to-Text Generation task aims\nto convert structured knowledge graphs into coherent and\nhuman-readable natural language text, a crucial step in making\ncomplex data accessible to non-expert users. Recent efforts\nin this field have focused on enhancing pre-trained language\nmodels (PLMs) by incorporating graph structure information\nto capture the intricate structure details of knowledge graphs.\nHowever, most of these approaches tend to capture only single-\ngranularity structure information, concentrating either on the\nrelationships between entities within the original graph or on\nthe relationships between words within the same entity or across\ndifferent entities. This narrow focus results in a significant\nlimitation: models that concentrate solely on entity-level structure\nfail to capture the nuanced semantic relationships between words,\nwhile those that focus only on word-level structure overlook\nthe broader relationships between original entire entities. To\novercome these limitations, this paper introduces the Multi-\ngranularity Graph Structure Attention (MGSA) model, which is\nbased on PLMs. The encoder of the model architecture features\nan entity-level structure encoding module, a word-level structure\nencoding module, and an aggregation module that synthesizes\ninformation from both structure. This multi-granularity structure\nencoding approach allows the model to simultaneously capture\nboth entity-level and word-level structure information, providing\na more comprehensive understanding of the knowledge graph's\nstructure information, thereby significantly improving the quality\nof the generated text. We conducted extensive evaluations of\nthe MGSA model using two widely recognized KG-to-Text\nGeneration benchmark datasets, WebNLG and EventNarrative,\nwhere it consistently outperformed models that rely solely on\nsingle-granularity structure information, demonstrating the ef-\nfectiveness of our approach.", "sections": [{"title": "I. INTRODUCTION", "content": "Knowledge Graphs (KGs) [1]\u2013[3] are structured data storage\nformats used to represent knowledge and information, with\nstrong capabilities in data integration and information retrieval.\nTheir graph-structured representation significantly enhances\nthe reasoning capabilities between pieces of knowledge. How-\never, the graph-structured nature of KGs differs greatly from\nnatural language text, making it more difficult for humans to\ndirectly comprehend the information they contain. Therefore,\nthe task of KG-to-text generation aims to transform structured\nKGs into human-readable natural language text, serving as a\nbridge between KGs and natural language. As illustrated in\nFig. 1, a given KG and its corresponding natural language\ndescription are presented. The task of KG-to-text has a wide\nrange of applications, such as question-answering systems [4],\n[5], dialogue generation or dialogue agents [6], and event\nnarration [7], among others.\nKG-to-text generation typically requires encoding the KG\nso that the model can understand the information it contains,\nthereby generating text that accurately describes the KG.\nUnlike AMR-to-Text Generation [8], which involves a more\nrestrictive space where the graph follows predefined dense\nconnection templates, the sparsity of KGs makes it difficult\nfor typical text generation models to align the relationships\nbetween the KG and the target generated text. Given that PLMs\nhave already learned rich linguistic knowledge, contextual\ninformation, and semantic relationships from large-scale pre-\ntraining corpora, recent works on KG-to-text generation [9]-\n[11] have achieved state-of-the-art (SOTA) results by lever-\naging PLMs. These models linearize the KG into a token\nsequence as input to the model, transforming the KG-to-text\ntask into a sequence-to-sequence (seq2seq) task by fine-tuning\nPLMs or adding additional pre-training tasks.\nAlthough fine-tuning PLMs has yielded encouraging results,\nseveral issues remain. First, linearizing the KG as input\nleads to the loss of graph structure information. Some works\n[10], [12]\u2013[14] have attempted to add additional embedding\ninformation to the linearized sequence or introduce graph-\naware modules into the model's encoder to capture the graph\nstructure. However, these methods either focus solely on\nentity-level or word-level structure, without simultaneously\nconsidering both granularities of structure information. Sec-"}, {"title": "II. RELATED WORK", "content": "A. KG-to-Text Generation\nKG-to-text generation is a subset of Data-to-Text generation\n[16] and a branch of Natural Language Generation (NLG) [17].\nUnlike traditional NLG tasks (seq2seq), where the input is a\nlinear sequence, a KG is structured as a graph rather than a\nlinear sequence. Therefore, models designed for conventional\nNLG tasks cannot be directly applied to KG-to-text generation.\nTo address this challenge, recent work has primarily focused\non two directions: one direction utilizes Graph Neural Net-\nworks (GNNs) [18]\u2013[22], which encode node information in\nthe graph through neighborhood aggregation and then decode\nthe encoded results to generate the corresponding textual\ndescription. Alternatively, some approaches directly modify\nthe encoder part of Transformer models [23], combining them\nwith Graph Attention Networks (GATs) [24], enabling the\nmodel to directly compute self-attention information for all\nnodes in the graph. The other direction leverages the success\nof PLMs in NLG tasks, such as BART [25], T5 [26], and\nGPT [27]. These methods fine-tune PLMs by linearizing the\nstructured KG into a token sequence, using the linearized KG\nnodes as input to generate sentences, thus transforming the\ngraph-to-text task into a seq2seq task. However, the drawback\nof this approach is that the original graph structure information\nis lost during the linearization process.\nAlthough explicitly encoding graph structure information\nwith GNNs has been shown to be effective, methods based on\nPLMs have demonstrated superior results compared to GNNs.\nTherefore, we also chose to explore the PLM-based approach\nfor the KG-to-text generation task.\nB. Structure Information for PLMs\nRecent work has focused on injecting the structure infor-\nmation of KGs into PLMs. Studies such as [28] and [29] in-\ncorporate additional embeddings into the linearized sequence,\nprimarily to capture token-level structure information in the\nKG. In [12] and [11], a relative distance matrix is designed\nbased on the relative distances between words or entities,\nand this matrix is integrated into the attention mechanism\nof the encoder to capture word-level or entity-level structure\ninformation from the KG. Meanwhile, [10], [13], [30] design\na graph structure-aware module within the encoder to capture\nentity-level structure information from the KG. Additionally,\n[31] and [32] combine GNNs with PLMs to align the entity-\nlevel graph structure of KGs with the token-level semantic\ninformation in the linear sequence.\nOverall, the aforementioned work either incorporates word-\nlevel structure information, entity-level structure information,\nor directly integrates entity-level structure information with the\nsemantic information of linearized sequences. However, these\napproaches do not simultaneously consider the structure infor-\nmation between entities and the structure information between\nwords within the same entity or across different entities in the\nKG. Inspired by [22] and [33], which explore multi-granularity\nstructure information in KGs using GNNs, we also consider\nboth entity-level and word-level structure information when\nincorporating graph structure into PLMs. Experimental results\ndemonstrate the effectiveness of our model."}, {"title": "III. METHODOLOGY", "content": "A. Problem Definition and Model Architecture\nKG-to-text generation aims to convert a set of triples into\nnatural language text. For a given KG $G = \\{(h_i,r_{ij}, t_j) |\\nh_i, t_j \\in V, r_{ij} \\in R\\}$, where V is the set of all nodes in the\ngraph and R is the set of all relation labels in the graph,\nthe triples are linearized into a token sequence $G_{linear} =$\n$(x_1,x_2,...,x_n)$ where $x_i$ represents the i-th token in the\nlinear sequence and n represents the number of tokens in the\nsequence. The linearized sequence is input into the model to\ngenerate the corresponding text sequence $T = (t_1, t_2,...,t_k)$,\nwhere $t_i$ represents the i-th token generated by the model and\nk represents the length of the text."}, {"title": "B. Entity-level Structure Encoding Module", "content": "This module encompasses the linearization of entity-level\nstructure in the KG, the design of a relative position matrix,\nand the incorporation of this relative position matrix into the\nself-attention weight calculations to effectively capture the\nstructure information.\n1) Entity-level Linearization: For a given KG in the form of\na set of triples, the linearization process follows the method in\nexisting work [13]. All triples are concatenated together, with\nspecial tokens , , and  used to denote the head,\nrelation, and tail of each triple, respectively.\n2) Linear Attention: The linearized sequence of the KG is\nthen fed into the module, where it first undergoes self-attention\ncomputation. This process captures the global information of\nthe linear sequence by calculating the global attention across\nall tokens in the sequence. Let the linearized sequence be\ndenoted as $X^E$, and the computation process of the linear\nself-attention is as follows:\n$X^E_{lin} = \\sigma(\\frac{(QK^T)}{\\sqrt{d}})V$,\n(1)\nwhere $Q = X^E W^Q, K = X^E W^K, V = X^E W^V$, and $W^Q,$\n$W^K, W^V$ are learnable weight parameters. d is the dimen-\nsionality of the vectors. $\\sigma(\\cdot)$ denotes the softmax function.\n3) Entity-level Graph Structure Attention: While linear\nattention primarily captures the global information of linear\nsequences, it is insufficient to obtain the structure informa-\ntion of entities and relations in the KG. To further capture\nthe structure information between entities and relations, the\nvectors $X_{lin} \\in \\mathbb{R}^{n \\times d}$ obtained from linear attention are first\ntransformed into entity and relation vectors $X_p \\in \\mathbb{R}^{m \\times d}$\nthrough a pooling layer.\n$X_p = pooling(X_{lin})$\n(2)\nHere, n and m represent the number of tokens in the linear\nsequence and the number of entities and relations in the KG,\nrespectively. The pooling operation employs mean pooling.\nTransforming the KG into a bipartite graph, as illustrated in\nFig. 3a. The nodes on the left side represent all entities from\nthe original KG, while the nodes on the right side represent the\nrelations. The relative position matrix $R^E$ is generated based\non the connection types between nodes in the bipartite graph:\n$R^E_{ij} = \\begin{cases}\n1, & \\text{if i and j are neighboring entities,} \\\\\n2, & \\text{if (i, j) is an (entity, edge) pair, }\\\\\n3, & \\text{if (i, j) is an (edge, entity) pair,}\n\\end{cases}$ (3)\nThe connection types are defined based on the neighboring\nentities or relations in the original KG. By assigning different\nweights to different connection types, the relative position\nmatrix $R^E$ is derived. Using this relative position matrix $R^E$,\nthe graph structure attention between entities is calculated:\n$X^g_a = \\sigma(\\frac{Q^P (K^P)^T}{\\sqrt{d}} + A + \\gamma(\\Gamma(R^E))V^P)$, (4)\nHere $Q^P, K^P, V^P$ are constructed from $X_p$ by multiplying\nit with their corresponding learnable parameter $W^{QP}, W^{KP},$"}, {"title": "C. Word-level Structure Encoding Module", "content": "1) Word-level Linearization: An entity or relation is usually\ncomposed of several words. In word-level linearization, these\nentities or relations are split into individual words, with each\nword marked by a special token [N]. The words are then\nconcatenated in the order of the original set of triples.\n2) Word-level Graph Structure Attention: In the entity-\nlevel bipartite graph, each node consists of several words.\nBased on this, the graph is transformed into a word-level\ngraph, as illustrated in Fig. 3b, where each node from the\noriginal bipartite graph is split into multiple nodes, with each\nnode containing only one word. When calculating word-level\nstructure attention, the word-level graph is used to generate the\nword-level relative position matrix $R^N$ based on the shortest\npath distances between words:\n$R^N_{ij} = \\begin{cases}\nencode(p), & \\text{if } d(n_i, n_j) = \\infty \\text{ and } d(n_j, n_i) = \\infty, \\\\\n\\delta(n_i, n_j), & \\text{if } d(n_i, n_j) \\in SAME_p, \\\\\n-\\delta(n_j, n_i), & \\text{if } d(n_i, n_j) \\leq \\delta(n_j, n_i), \\\\\n\\delta(n_i, n_j), & \\text{if } d(n_i, n_j) > \\delta(n_j, n_i),\n\\end{cases}$ (6)\nHere, $d(n_i, n_j)$ represents the shortest path distance from word\nnode $n_i$ to word node $n_j$. If $n_i$ to $n_j$ are unreachable from\neach other, their shortest path distance is set to $\\infty$. The term\n$SAME_p$ indicates words that belong to the same entity. The\nfunction encode($\\cdot$) maps the distance between words from the\nsame entity to a value outside the range of the $\\delta(\\cdot)$ function.\nThe encoding function is defined as $encode(p) := sgn(p) \\cdot$\n$d_{max} + p$, where $d_{max}$ is the diameter of the graph, representing\nthe maximum shortest path length between two words. Let the"}, {"title": "D. Aggregation Module", "content": "After encoding through the two granularity-specific module,\nwe obtain the entity-level and word-level graph structure\nencoding vectors, which are then concatenated:\n$c = [X^E || \\lambda X^W]$\n(8)\nHere, $\\lambda$ is a hyperparameter that controls the contribution of\nthe word-level structure encoding vector. For the vector c,\nthe overall attention is calculated to fuse the graph structure\ninformation at both granularities as follows:\n$X^c = \\sigma(\\frac{Q^c (K^c)^T}{\\sqrt{d}} V^c)$,\n(9)\nHere, $Q^c, K^c, V^c$ are constructed from $c$ by multiplying it\nwith their weight matrices. The representation is then enhanced\nthrough a residual connection, followed by a two-layer feed-\nforward network (FF) to adjust the dimensionality of the vector\nrepresentation. Finally, the output representation O, which\nfuses the information from both granularities, is obtained\nthrough another residual connection and layer normalization\n(LN) as follows:\n$O = LN(FF(X^c + c) + X^c)$"}, {"title": "E. Loss Function", "content": "The model's decoder adopts the standard Transformer de-\ncoder structure. For a given KG G, the loss for text generation\nis calculated using the negative log-likelihood (NLL) as fol-\nlows:\n$\\mathcal{L} = - \\sum_{i=1}^{k} log p (t_i | t_1,..., t_{i-1};G)$ (11)\nHere, p represents the generation probability of each token."}, {"title": "IV. EXPERIMENTS", "content": "A. Experiment Setup\n1) Dataset: The experiments were conducted on two\ndatasets designed for the KG-to-text generation task: WebNLG\nv2.0 and EventNarrative. WebNLG consists of crowdsourced\nRDF data manually created by human annotators. Each exam-\nple in the dataset contains up to seven triples and one or more\nreference texts. For comparison with previous work, version\n2.0 was used in our experimental analysis. EventNarrative\nextracts events from EventKG [35] and enriches each event\nwith additional data from Wikidata [3], including related\nattributes and objects. Detailed textual descriptions related to\nthese events are then obtained from Wikipedia. This process\nensures a close correspondence between the KG and the text,"}, {"title": "V. CONCLUSION", "content": "This paper proposes a KG-to-text generation model based\non multi-granularity graph structure attention, which simulta-\nneously considers both entity-level and word-level structure\ninformation in the knowledge graph. The model primarily\nconsists of three modules: an entity-level structure encoding\nmodule, a word-level structure encoding module, and an aggre-\ngation module that integrates structure encoding information\nfrom both granularities. Experimental evaluations conducted\non two KG-to-text benchmark datasets demonstrate that the\nproposed model consistently outperforms baseline models\n(which utilize single-granularity structure). An analysis of the\nfactors influencing the model further explains its effectiveness\nin generating text from knowledge graph."}]}