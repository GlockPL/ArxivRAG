{"title": "MultiQ&A: An Analysis in Measuring Robustness via Automated Crowdsourcing of Question Perturbations and Answers", "authors": ["Nicole Cho", "William Watson"], "abstract": "One critical challenge in the institutional adoption journey of Large Language Models (LLMs) stems from their propensity to hallucinate in generated responses. To address this, we propose MultiQ&A, a systematic approach for evaluating the robustness and consistency of LLM-generated answers. We demonstrate MultiQ&A's ability to crowdsource question perturbations and their respective answers through independent LLM agents at scale. Our experiments culminated in the examination of 1.9 million question perturbations and 2.3 million answers. Furthermore, MultiQ&A shows that ensembled LLMs, such as gpt-3.5-turbo, remain relatively robust and consistent under perturbations. MultiQ&A provides clarity in the response generation space, offering an effective method for inspecting disagreements and variability. Therefore, our system offers a potential framework for institutional LLM adoption with the ability to measure confidence, consistency, and the quantification of hallucinations.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) exhibit immense potential for diverse downstream applications; however, they often lack transparency regarding their reasoning processes (Liang et al. 2022; Wei et al. 2023; Kojima et al. 2023; Li et al. 2023) and the robustness of their generated answers. This challenge is further exacerbated by the limited accessibility into a models' training datasets, especially when deployed externally (Liang et al. 2022). Moreover, research has demonstrated that LLMs are highly sensitive to input perturbations (Zhang et al. 2022; Moradi and Samwald 2021). Therefore, the motivation for this study is to propose an evaluation method for LLMs that is undeterred by such limitations and operates on the external interface. We present MultiQ&A, an adversarial \"IQ test\" for language models, mainly gpt-3.5-turbo, that measures the robustness of answer generation by automating the crowdsourcing of questions and answers through independent agents. This cognitive evaluation method stands in contrast to the more commonly utilized context-based retrieval systems for hallucination mitigation (Reimers and Gurevych 2019; Johnson, Douze, and J\u00e9gou 2019; Nogueira and Cho"}, {"title": "2 Related Work", "content": "Large language models (LLMs), such as GPT-3, Instruct-GPT, and LLaMA (Brown et al. 2020; Ouyang et al. 2022; Touvron et al. 2023), have demonstrated remarkable capabilities but also face challenges, such as hallucinations and"}, {"title": "3 Methodology", "content": "We propose a robust multi-step pipeline for measuring question answering (QA) robustness, consisting of three components: the Query Rewriter, Answer Generator, and Aggregator. These components (gpt-3.5-turbo agents) work in tandem to perturb, answer, and consolidate diverse responses, enabling automatic crowdsourcing and robust QA evaluation mimicking real-world scenarios."}, {"title": "3.1 Query Rewriter", "content": "The Query Rewriter, powered by gpt-3.5-turbo, transforms the original query qo into a diverse set of v + 1 variations Q = {qo, q1,..., qv }, ensuring semantic consistency while introducing meaningful perturbations. These variations include the identity transformation To(90) = qo, ensuring that the original query is always preserved.\nPerturbation Process To encourage creativity and prevent duplicates, the perturbation process employs a high temperature setting ( = 1.0) in a single generation step. The resulting perturbations Q are generated as:\n\nQ = T(go) = \\begin{bmatrix}I(qo) \\\\ T_1 (90) \\\\ : \\\\ T_\u03c5 (90)\\end{bmatrix} = \\begin{bmatrix}9_0 \\\\ 9_1 \\\\ : \\\\ q_\u03c5\\end{bmatrix}\n\nwhere Tj represents perturbation functions in the transformation set T. The output is a set of v+1 semantically similar yet syntactically diverse questions, ready for answering.\nPrompt Design Each perturbed question is explicitly designed to preserve the semantic intent of qo while enabling evaluation of the system's robustness under diverse phrasings. A single prompt call generates all variations in one pass, leveraging the efficiency of LLMs to simulate creative crowdsourcing. The prompt is: Rewrite the question in n radically different ways."}, {"title": "3.2 Answer Generator", "content": "The Answer Generator employs |Q| = v + 1 independent gpt-3.5-turbo agents to generate answers A = {\u03b1\u03bf, \u03b11,..., \u03b1\u03c5} for each question qi \u2208 Q. This design ensures no contextual information is shared amongst agents, isolating the effect of each query perturbation.\nAnswering Process For each query qo \u2208 Q, the agent receives a specific prompt, tailored to the type of QA task:\n\u25ba Extractive QA: The question is presented with its corresponding context c\u2081, and the model extracts the answer.\n\u25ba Multiple Choice QA: The question is presented alongside candidate choices K = {ko,... km}, with the model selecting the most appropriate option(s).\n\u25ba Abstractive QA: The question is presented in isolation, and the model generates a free-form answer.\nThe prompt formats for each scenario are detailed in Table 2. All experiments use a high temperature setting (r = 1.0) to prioritize diversity and stress-test the answering agents."}, {"title": "3.3 Aggregator", "content": "The Aggregator consolidates and evaluates the generated answers A using semantic clustering, ranking, and evaluation metrics. It provides holistic insights into the system's robustness and reliability through the following mechanisms:\nClustering A semantic paraphrase model groups answers into coherent clusters based on similarity (Reimers and Gurevych 2019). This clustering provides an interpretable structure for analyzing cohort diversity and agreement.\nRe-ranking Within each cluster, an answer-critic model re-ranks responses to identify the most semantically aligned answer to the original query qo. This alignment is measured using cross-encoder techniques via Sentence-BERT models (Reimers and Gurevych 2019).\nEvaluation Metrics The Aggregator computes both supervised (S) and unsupervised (U) metrics to quantify robustness and agreement (as defined in \u00a75 and illustrated in Fig-"}, {"title": "4 Datasets", "content": "We evaluate MultiQ&A on 12 QA datasets spanning Extractive, Multiple Choice, and Abstractive paradigms, ensuring robust assessment across diverse knowledge domains."}, {"title": "4.1 Extractive QA", "content": "SQUADv2 (Rajpurkar et al. 2016; Rajpurkar, Jia, and Liang 2018): A reading comprehension dataset containing 86,821 answerable questions. We exclude non-answerable questions and evaluate on 71,802 training examples, 5,834 validation samples."}, {"title": "4.2 Multiple Choice QA", "content": "We utilize eight multiple-choice datasets to assess model reasoning, commonsense, and truthfulness:"}, {"title": "4.3 Abstractive QA", "content": "For Abstractive QA, we evaluate the ability of LLMs to generate free-text answers without explicit candidates:\n\u25ba SQUADv2 (Rajpurkar et al. 2016; Rajpurkar, Jia, and Liang 2018): Repurposed for abstractive QA by conditioning the model solely on the transformed question.\n\u25ba TruthfulQA (Lin, Hilton, and Evans 2022): The model generates free-text answers scored for semantic similarity with correct options.\n\u25ba WikiQA (Yang, Yih, and Meek 2015): Adapted for abstractive tasks, with correctness determined by a cosine similarity score greater than 60% between the generated answer and labeled passages.\n\u25ba SciQ (Johannes Welbl 2017): Evaluated without candidate choices, using approximate Levenshtein distance to match generated answers.\n\u25ba HotpotQA (Yang et al. 2018): A Wikipedia-based QA dataset with 57,711 train and 5,600 validation samples, emphasizing factual, diverse topics.\n\u25ba TriviaQA (Joshi et al. 2017): Contains 95,000 QA pairs authored by trivia experts, evaluated on 67,469 training and 11,313 validation samples."}, {"title": "5 Metrics", "content": "To comprehensively evaluate the performance of gpt-3.5-turbo, we analyze several key aspects: accuracy, robustness, and plurality-based voting under adversarial question perturbations. Since the original question qo is always included in our answer set A, we juxtapose the baseline accuracy with ensemble metrics that capture worst-case performance, best-case outcomes, and agreement across perturbed answers. Inspired by prior LLM evaluations (Liang et al. 2022), our measure of robustness considers the following:"}, {"title": "5.1 Accuracy, Robustness, & Plurality (S)", "content": "\u25ba Worst-case Robustness (\u03a9): Measures the lower bound, where at least one perturbed input is incorrect.\n\u25ba Best-case Robustness (O): Measures the upper bound, where at least one perturbed input is correct.\nFor consistency, let m denote an indicator function for accuracy, A represent the baseline accuracy over n samples, and Tj (x) describe perturbations applied to input xj. Let Y represent the ensemble answer generated by plurality voting over v + 1 raters. The metrics are defined as follows:\nm(f(x), y) = \\begin{cases} 1 & f(x)=y \\\\ 0 & f(x) \\neq y\\end{cases}\nA = \\frac{1}{n} \\sum_{j=1}^{n}m(f(x_j), y_j)\n\\Omega = \\frac{1}{n} \\sum_{j=1}^{n} \\min_i m (f(T_i(x_j)), y_j) \\le A\nO = \\frac{1}{n} \\sum_{j=1}^{n} \\max_i m(f(T_i(x_j)), y_j) \\ge A\nY = \\frac{1}{n} \\sum_{j=1}^{n} m \\Big(mode\\{f(T_i(x)) \\mid T_i\\in T \\Big\\}, y_j)\nWhere:\n\u25ba To(x) = I: the identity function representing the original unperturbed question.\n\u25ba f(x): the model's output for input x.\n\u25ba y: the ground truth answer.\nRelationships Between Metrics The relationship between accuracy A, robustness (\u03a9, \u039f), and plurality voting \u00dd depends on the interaction between the original query and its perturbed variations:\n\u03a9 \u2264 min(\u0176, A) \u2264 max(\u0176, A) \u2264 O\nThis hierarchy reflects that worst-case robustness (\u03a9) sets the lower bound, while best-case robustness (O) defines the upper bound. Perturbations can influence the model's performance and agreement in the following ways:\n\u25ba Perturbations Help Align Outputs: Then \u0176 > A as the mode benefits from consensus across the outputs.\n\u25ba Perturbations Introduce Noise: Then \u0176 < A as the mode is skewed by incorrect answers from variations.\n\u25ba Perturbations Are Neutral: In this case, the model performs consistently across all queries (\u0176 \u2248 A).\nRandom Guessing: If the raters randomly guess among k possible answer choices, the following behaviors are observed:\n\u25ba Accuracy A and plurality \u0176 are 1/k.\n\u25ba Worst-case robustness (\u03a9) asymptotically approaches:\n\n\\lim_{k\\to\\infty} {v+1 \\choose 1} (\\frac{1}{k})^{v+1} \\approx 0\n\u25ba Best-case robustness (0) approaches:\n1 - (1 - \\frac{1}{k})^{v+1}\nThe best-case probability represents the likelihood of at least one correct answer across v + 1 independent guesses."}, {"title": "5.2 Agreement", "content": "Item Difficulty (S) Item difficulty \u00b5D measures how challenging each question is for the LLM raters by computing the average correctness across all responses (Lord 1952):\n\u00b5D = \\frac{1}{n} \\sum_{j=1}^{n}  \\frac{1}{\\left|T\\right|} \\sum_{T\\in T}  m(f(T_i(x_j)), y_j)\nwhere n represents the number of samples, T is the set of perturbations, and m is the indicator function for correctness. For random guessing, the expected value follows a Bernoulli distribution, E [\u00b5D] = 1/k.\nMean Normalized Certainty (U) Entropy H quantifies uncertainty in the responses: higher entropy corresponds to greater uncertainty, while lower entropy reflects more consistent answers (Shannon 1948; Wilcox 1973). We normalize the rater entropy H by the maximum possible entropy Hmax, inverting the scale to reflect certainty: 1 represents high certainty and 0 indicates uncertainty:\n\\Eta_\\eta = 1 - \\frac{H}{H_{max}}\nH= \\frac{1}{n} \\sum_{j=1}^n\\left[1 +  \\frac{\\sum_{i=0}^{K_j} p_i \\log_b{(p_i)}}  {\\log_b (K_j)}  \\right]\nwhere \u0397\u03b7 \u2208 [0, 1], fi is the frequency of answer choice i, pi is the proportion of answer choice i across v + 1 raters, and Kj is the number of possible choices for question qj.\nGibbs' M2 Index (U) The M2 index quantifies the variance in rater's responses, assuming a multinomial distribution for the answer choices (Gibbs and Poston 1975). The index is standardized such that M2 = 1 indicates complete certainty (no variability), while M2 = 0 indicates a uniform distribution (maximum uncertainty):\nM_2 = 1 - \\frac{1}{n} \\sum_{j=1}^n\\left[\\frac{K_j}{K-1} \\left( 1-  \\sum_{i=0}^{K_j} p_i^2  \\right)\\right]\nFleiss's Generalized K (U) Fleiss' measures the degree of inter-rater agreement beyond what would be expected by random chance. A value of 1 indicates perfect agreement, while 0 indicates no agreement beyond chance (Cohen 1960; Fleiss 1971). Let fi represent the frequency of answer choice"}, {"title": "5.3 Reliability (S)", "content": "Cronbach's a measures the internal consistency and reliability of dichotomous responses (correct/incorrect) (Cronbach 1951). It is widely accepted in testing theory and is equivalent to the Kuder-Richardson Formula 20 (KR-20) for binary data (Kuder and Richardson 1937). The formula is:\n\\alpha = \\frac{n}{n-1} \\left( 1 - \\frac{\\sum_{j=1}^n \\sigma_{f_j}^2}{\\sigma_{y}^2} \\right)\nwhere n is the number of samples, \\sigma_{f_j}^2 is is the variance in scores across v + 1 raters for each sample, and \\sigma_{y}^2 is the variance in total correct responses per rater."}, {"title": "6 Analysis & Discussion", "content": "As shown in Table 4, the performance of the different question answering formats across perturbations is: Extractive > Multiple Choice > Abstractive (Table 5). This ranking suggests that Extractive tasks outperform the others, due to the inherent advantage of additional content, such as context or answer choices, improving model robustness in Retrieval-Augmented scenarios (Lewis et al. 2021). Multiple Choice tasks benefit from the fixed set of choices, which provide some constraints that help guide the model's decision-making process. In contrast, Abstractive tasks experience greater variability in rater responses under perturbations, possibly due to the model's increased likelihood of hallucination or misinterpretation when generating free-form answers."}, {"title": "6.1 Extractive QA", "content": "Performance When provided with context, MultiQ&A demonstrates that gpt-3.5-turbo performs strongly on SQUADv2. In particular, the baseline and mode accuracy for the model are nearly identical, indicating that the model is quite stable under perturbations. Despite this high accuracy, adversarial question generation still manages to cause a failure rate of 13% in the training set and 9.4% in the validation"}, {"title": "6.2 Multiple Choice QA", "content": "Robustness Most variations in the Multiple Choice format benefit from the presence of fixed answer choices to rely on, demonstrating significant robustness across five perturbations. The model's reliance on unaltered answer choices helps maintain consistency, even when the question is modified. We see that gpt-3.5-turbo exhibits strong performance across several benchmarks, achieving 67.6% accuracy on MMLU in a 0-shot setting, 60.4% accuracy on TruthfulQA, and 83.8% (C) and 92.8% (E) on the ARC Challenge and Easy sets, respectively. These results highlight the model's capability to handle a variety of multiple-choice questions with a solid level of accuracy. For other benchmarks, the model shows varied performance: 80.1% on PIQA, 71.5% on BoolQ, and 72.2% on TriviaQA in a 0-shot environment. While the model performs well on most multiple-choice tasks, the differences in its performance across datasets emphasize the importance of dataset characteristics and question types that play a significant role in influencing accuracy.\nEnsemble Accuracy & Internal Consistency Our experiments focused on evaluating robustness under perturbations rather than conducting few-shot ablation studies. For most datasets, gpt-3.5-turbo achieves high ensemble accuracy, consistent with the baseline, suggesting that perturbations do not significantly disrupt the most frequent response. Internal consistency across raters is also very high, likely due to the strong agreement and accuracy, indicating that the model remains stable even under slight changes in the question formulation.\nOutlier: MathQA However, MathQA stands out as an outlier in our analysis. The model demonstrates low accuracy, poor worst-case performance, and a low \u00b5D, suggesting that the questions in MathQA are particularly challenging for gpt-3.5-turbo. Furthermore, the agreement between raters is low at approximately 30%, indicating that the model struggles to provide consistent answers across different perturbations. This is corroborated by the low consistency scores (Cronbach's a \u2248 42.8% on the test), signaling to gpt-3.5-turbo's difficulties in performing arithmetic operations within a language modeling framework (Mirzadeh et al. 2024)."}, {"title": "6.3 Abstractive QA", "content": "Performance Without Context In the Abstractive format, gpt-3.5-turbo performs well even without the inclusion of additional context, demonstrating notable robustness at a temperature setting of T = 1.0 and under v = 5 perturbations. The model's ability to handle perturbations in the absence of explicit context suggests a flexible approach to generating answers. However, the accuracy still fluctuates depending on the difficulty of the question and the ground truth, indicating that while the model is adaptive, its performance is sensitive to task complexity.\nVariation Across Tasksets Significant variation is observed in accuracy across different tasksets. Specifically, tasksets like TriviaQA, SciQ, and WikiQA show an improvement of +20% in accuracy compared to SQuADv2, TruthfulQA, and HotpotQA. This highlights that simpler tasksets, with more straightforward ground truths, are easier for generative models like gpt-3.5-turbo to answer. The results suggest that the complexity of the task and the nature of the questions play a critical role in the model's performance, with simpler or more direct questions yielding better outcomes in Abstractive settings."}, {"title": "6.4 Abusive or Sensitive Content", "content": "We encountered 2,293 cases where gpt-3.5-turbo failed to generate responses due to content filtering, separate from standard service errors (APIError, ServiceUnavailableError, RateLimitError). These failures fell into two main categories:\n\u25ba AttributeError (1,081 cases): Triggered when generating violent or explicit content.\n\u25ba InvalidRequestError (1,212 cases): Occurs when prompt filtering flags violent or explicit terms in the input.\nThese cases can enrich adversarial datasets for content filtering. Below is the observed distribution across datasets:"}, {"title": "6.5 Token Usage and Statistics", "content": "Our evaluation spanned 376,201 questions, producing 1,881,005 variations and 2,257,206 total answers. The process utilized 717,530,842 tokens, including 115,834,262 for perturbations and 601,696,580 for answer generation."}, {"title": "7 Conclusion", "content": "MultiQ&A is a crowdsourcing-based method to assess the robustness and consistency of LLM-generated answers. By perturbing 376,201 questions into 1,881,005 lexical variations while preserving semantics, our experiments across 13 datasets quantified consistency, reliability, and robustness, providing valuable insights into LLM responses. We believe that MultiQ&A provides a promising infrastructure for institutions adopting LLMs with increased confidence."}]}