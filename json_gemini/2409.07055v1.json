{"title": "Legal Fact Prediction: Task Definition and Dataset Construction", "authors": ["Junkai Liu", "Yujie Tong", "Hui Huang", "Shuyuan Zheng", "Muyun Yang", "Peicheng Wu", "Makoto Onizuka", "Chuan Xiao"], "abstract": "Legal facts refer to the facts that can be proven by acknowledged evidence in a trial. They form the basis for the determination of court judgments. This paper introduces a novel NLP task: legal fact prediction, which aims to predict the legal fact based on a list of evidence. The predicted facts can instruct the parties and their lawyers involved in a trial to strengthen their submissions and optimize their strategies during the trial. Moreover, since real legal facts are difficult to obtain before the final judgment, the predicted facts also serve as an important basis for legal judgment prediction. We construct a benchmark dataset consisting of evidence lists and ground-truth legal facts for real civil loan cases, LFP-Loan. Our experiments on this dataset show that this task is non-trivial and requires further considerable research efforts.", "sections": [{"title": "Introduction", "content": "Legal Fact refers to the case facts determined by the judge during the litigation process, through the presentation and cross-examination of evidence by the parties involved (Korn, 1966). Only evidence-backed facts that adhere to legal criteria are acknowledged as legal facts. For example, evidence extracted through coercive interrogation by investigators, due to the lack of legality, cannot be recognized as legal facts. Precise legal fact-finding can help judges reconstruct the actual circumstances of a case, which is essential for ensuring the fairness of judicial rulings and safeguarding the legal rights of all parties concerned (Greenberg, 2004).\nPrevious research has dedicated a lot of efforts to automatic judicial decision-making, utilizing different methods such as neural models (Yang et al., 2019; Feng et al., 2019), label-attention mechanisms (Wang et al., 2019) and pre-trained language models (Chalkidis et al., 2020, 2019; Xiao et al., 2021; Niklaus et al., 2021; He et al., 2024a). However, most of them primarily focus on legal judgment prediction, directly bypassing the essential fact-finding process (Medvedeva and Mcbride, 2023). These works directly leverage the description of legal facts recorded in official court judgments to predict the final judgments. However, in practice, legal facts are not available before the final judgments are made by the court. Instead, the parties only know the evidence information before the trial, and the legal fact can only be determined after the evidence has undergone rigorous cross-examination during the trial. Therefore, the prediction of legal facts remains unexplored and is in urgent demand.\nThis paper introduces the task of Legal Fact Prediction. Without access to the court trial process, such as evidence examination or judicial questioning, this paper aims to automatically determine legal facts given evidence information as input. This task is valuable as it assists parties and lawyers in predicting fact-finding outcomes, enabling them to strengthen evidence and adjust strategies for a trial. It also aids judges by streamlining the fact-finding process, thus enhancing case adjudication efficiency. Most importantly, legal fact prediction is essential for legal judgment prediction, as fact-finding is a prerequisite for making accurate legal judgments.\nSince it is often difficult to obtain the list of evidence submitted by the parties to the court in bulk, constructing benchmark datasets for legal fact pre-"}, {"title": "Task Definition", "content": "As depicted in Figure 2, court trials typically unfold in two stages: fact-finding and the application of law. During the first stage, the court would determine the legal facts, based on the evidence presented and the arguments made by both the plaintiff and the defendant. Subsequently, the court would apply relevant laws to these facts, to assess the validity of the plaintiff's claims and make an appropriate judgment or ruling. Therefore, legal facts serve as the foundation for the application of law.\nAs discussed in Section 1, while most existing works on legal judgment prediction assume that legal facts are already known, in practice, legal facts are officially determined when the final judgment is issued. In other words, it is impractical to obtain the legal facts before judgment prediction. Therefore, to automate the legal judgment process, the first step must involve predicting the legal facts.\nFormally, the task of legal fact prediction can be defined as follows: Given the evidence list provided by both the plaintiff and the defendant, predict the legal fact that the court will determine. The evidence list is a document of great importance in legal proceedings, detailing all the evidence items that will be considered in a trial, including documentary evidence, physical evidence, witness testimonies, expert reports, etc. Since the evidence list is typically formed before the trial, in our task, we assume the evidence list as the input, which aligns with real-world legal practice. Due to the inaccessibility of court trial information, the task requires an internal reasoning step to perform evidence verification, which poses a new challenge for precise prediction especially when the conflicting parties present contradictory evidence."}, {"title": "Dataset Construction", "content": "To build a benchmark for our task, two types of data are required: evidence lists and legal facts. While legal facts can be readily extracted from publicly available court judgments, evidence lists provided by the plaintiff and defendant are typically not disclosed to the public. Nonetheless, we found that trial transcripts, which are written records of all arguments and decisions made in trials, also contain the evidence information presented by the parties.\nTherefore, we propose to extract the evidence information from trial transcripts to reconstruct evidence lists. Specifically, we obtain trial transcripts and legal facts from the LJP-MSJudge dataset (Ma et al., 2021) and annotate the evidence list recorded in the transcripts using the GPT-40 model. Since the challenge of predicting legal facts often lies in the contradictory arguments and evidence presented by the conflicting parties, we exclude all cases where the defendant does not provide any evidence.\nThe evaluation of fact prediction results can be conducted from two dimensions. Firstly, we can compare the predicted fact description and the ground truth written by the judge based on text similarity. However, text similarity in legal fact descriptions does not necessarily equate to the accuracy of the prediction. Therefore, we aim to conduct a more fine-grained comparison of key items within the fact descriptions, such as the loan amount, loan date, and repayment date. Note that the key elements are manually annotated to ensure the integrity of the labels regarding legal facts."}, {"title": "Experiment", "content": "In this section, we conduct experiments to showcase how challenging legal fact prediction is."}, {"title": "Setup", "content": "Baseline approaches. We evaluate the following LLM-powered approaches for legal fact prediction, which are commonly used as baselines in automated judicial decision-making.\n1. Question-answering (QA)-based method (Zhong et al., 2020): We present the list of evidence to an LLM agent, who acts as a judge, and ask it to determine the legal facts based on the evidence.\n2. Simulation-based method (Wu et al., 2023; He et al., 2024b): As shown in Figure 3, we simulate a legal trial using multiple LLM agents, with each agent role-playing as the judge, plaintiff, or defendant. During the simulated trial, the plaintiff and defendant clarify and debate the facts of the case based on the evidence they each possess. Ultimately, the judge summarizes the legal facts based on the content of the debate.\nAdditionally, we enhance the performance of the LLM judge in the baseline approaches via few-shot learning (Vinyals et al., 2016). That is, we present the real legal facts of three other cases to the LLM judge, which allows for learning the reasoning processes of real-life judges. We use GPT-40 to set up all LLM agents. All prompts and the parameter setting for LLM agents can be found in the appendix.\nMetrics. To evaluate the performance of the predicted fact descriptions, we convert them into embeddings using OpenAI's text-embedding-3-large model and calculate the cosine similarity between the predicted and ground-truth fact descriptions. For the key items in the legal fact, we measure their performances using the strict accuracy metric that calculates how often the predictions match the labels."}, {"title": "Experimental Results", "content": "Table 1 presents the performance of various methods in predicting fact descriptions and key items in legal facts. Overall, these methods, leveraging the intelligence of LLM agents, demonstrate non-trivial predictive capabilities, though there remains significant room for improvement. This indicates that, while it is possible for machines to predict legal facts only given the evidence, accurate predictions still require continued research efforts."}, {"title": "Discussion", "content": "Although the input for legal fact prediction is defined as the evidence list, other trial-related information could also serve as input for this task. As discussed by Medvedeva and Mcbride (2023), ideally, the input for legal judgment prediction should be any information that is available to the court or the parties before the judgment is made, such as complaints, defenses, and evidence submitted by the parties. This also stands true for legal fact prediction. However, the information available to the parties or the court depends on the stage of the trial. For example, prior to filing a lawsuit, the plaintiff and defendant may only have access to the evidence they personally possess as the basis for predicting legal facts. After filing, they gain access to each other's evidence and arguments regarding the legal facts. We choose the evidence list as the basic input for legal fact prediction because, at different stages of the trial, both parties have access to certain evidence. Note that the \"evidence list\" here does not necessarily correspond to the final list of evidence submitted to the court, but rather represents the set of evidence available to the parties at the current stage. Additionally, if the parties know other trial-related information, it can also be used as supplementary input to improve prediction accuracy. This suggests that in future work, we can adapt the legal fact prediction task to different trial stages by tailoring the input, thereby addressing various demands in legal practice."}, {"title": "Conclusions", "content": "This paper introduces a new legal task for NLP, namely legal fact prediction. This task takes the evidence held by the parties as the basis to predict legal facts, which then serve as inputs for legal judgment prediction. As a result, it addresses the recent concern that current legal judgment prediction technologies fail to meet practical needs (Medvedeva and Mcbride, 2023). To initiate this direction, this paper builds a legal fact prediction benchmark based on real civil loan cases. However, even for this relatively simple benchmark, baseline approaches still fail to predict legal facts with high precision, indicating that this task is non-trivial. Therefore, considerable effort is required to further investigate this task in the future."}, {"title": "Limitations", "content": "This paper still has some limitations. First, our current dataset only includes civil loan cases. In the future, we plan to expand the dataset to include other civil cases and criminal cases. Second, the baseline methods used in our experiments are relatively simple. In the future, we will employ more NLP techniques to improve the effectiveness of legal fact prediction."}, {"title": "Parameter Settings for LLM Agents", "content": "parameter\nvalue\nfrequency_penalty\t0\nlogprobs\tfalse\npresence_penalty\t0\ntemperature\t1\ncontext_window\t128,000\nmax_output_tokens\t4,096\ntop_p\t1\nAll parameters are set to the default values of\nthe OpenAI library."}, {"title": "Prompts for QA-Based Method", "content": "[Evidence list]\nPlease analyze the evidence list of the loan\ncase above and output the following two re-\nsponses:\nA faithful description of the basic facts of the\ncase; A list containing some basic informa-\ntion about the case, mainly including the fol-\nlowing items: (1) The specific amount of the\nloan; (2) Whether there is interest, 1 for yes,\n0 for no; (3) Interest rate, accurately tran-\nscribe the original number, if not mentioned,\nthen 0; (4) Loan time, fill in -1 if unknown;\n(5) Repayment time, fill in -1 if unknown; (6)\nThe specific amount of the repayment.\nThe two time points in the above list\nshould be written directly in the format of\nyear+month+day. For example, if the output\ntime is January 1, 2002, it should be output\nas 20020101.\n[Few-shot prompt (Prompt 2)]\nFinally, you need to output all your answers\nin a json format without adding any com-\nments. The specific format is as follows:\n{\"fact description\": \"[PLACEHOLDER]\",\n\"key items\": {\n\u201cloan amount\": \"[PLACEHOLDER]\",\n\u201cinterest applied\": \"[PLACEHOLDER]\",\n\u201cinterest amount\": \"[PLACEHOLDER]\",\n\"loan date\": \"[PLACEHOLDER]\",\n\u201crepayment date\": \"[PLACEHOLDER]\",\n\u201crepaid amount\": \"[PLACEHOLDER]\" }\n}"}, {"title": "Prompts for Simulation-Based Method", "content": "The simulation-based method generates legal facts\nin two stages. In the first stage, LLM agents follow\nPrompt 3 to conduct a simulated trial and make a\nsummary of the trial. In the second stage, we use\nPrompt 4 to generate the legal facts based on the\nsummary of the simulated trial and the evidence list."}, {"title": "Trial Script", "content": "Judge: The court is now open. First, please\npresent your evidence, both parties.\nPlaintiff: We have submitted a total of [num-\nber of plaintiff's evidence] pieces of evi-\ndence. Evidence 1 is [name of evidence 1];\nEvidence 1 states: [content of evidence 1];\nEvidence 1 is used to prove: [purpose of\nevidence 1]. (The same for Evidence 2, Evi-\ndence 3, etc.)\nDefendant: We have submitted a total of\n[number of defendant's evidence] pieces of\nevidence. Evidence 1 is [name of evidence\n1]; Evidence 1 states: [content of evidence\n1]; Evidence 1 is used to prove: [purpose\nof evidence 1]. (The same for Evidence 2,\nEvidence 3, etc.)\nJudge: Next, the court will investigate the\nfacts of the case according to its authority,\nand both parties can debate the facts of the\ncase.\nJudge: Plaintiff, please present evidence to\nillustrate whether the \"borrowing behavior\"\noccurred.\nPlaintiff: The [evidence number] we sub-\nmitted is an IOU, which can prove that the\nlending relationship between the plaintiff\nand the defendant has been established; The\n[evidence number] we submitted is a trans-\nfer record, which can prove the fact that we\nhave transferred money to the defendant.\nDefendant: [The defendant may present ev-\nidence or point out flaws in the plaintiff's\nevidence for rebuttal, considering aspects\nsuch as whether the lender and borrower in-\ndicated on the IOU correspond to the plain-\ntiff and defendant, whether there is a signa-\nture or fingerprint of the borrower on the\nIOU, and whether the defendant actually\nreceived the loan from the plaintiff. How-\never, it is not limited to these aspects.]\n...(Consider setting up 1-2 rounds of random\ndialogue here)\nJudge: Next, plaintiff, please present evi-\ndence to illustrate the amount of the loan.\nPlaintiff: The [evidence number] we sub-\nmitted is an IOU, which can prove that the\ndefendant borrowed [agreed loan amount]\nyuan from the plaintiff; The [evidence num-\nber] we submitted is a transfer record, which\ncan prove that we have lent [actual loan\namount] yuan to the defendant."}]}