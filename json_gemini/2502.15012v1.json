{"title": "Graph in the Vault: Protecting Edge GNN Inference with Trusted Execution Environment", "authors": ["Ruyi Ding", "Tianhong Xu", "Aidong Adam Ding", "Yunsi Fei"], "abstract": "Wide deployment of machine learning models on edge devices has rendered the model intellectual property (IP) and data privacy vulnerable. We propose GNNVault, the first secure Graph Neural Network (GNN) deployment strategy based on Trusted Execution Environment (TEE). GNNVault follows the design of \"partition-before-training\u201d and includes a private GNN rectifier to complement with a public backbone model. This way, both critical GNN model parameters and the private graph used during inference are protected within secure TEE compartments. Real-world implementations with Intel SGX demonstrate that GNNVault safeguards GNN inference against state-of-the-art link stealing attacks with a negligible accuracy degradation (<2%).", "sections": [{"title": "I. INTRODUCTION", "content": "On-device machine learning has emerged as an important paradigm for tasks requiring low latency and high privacy [1]. Applications like Apple's CoreML [2] and Huawei's Mind-Spore [3] enable user personalization on local devices. This trend has also extended to Graph Neural Networks (GNNs) [4], [5], ensuring the privacy of user data during inference for tasks such as community detection [6], e-commerce personaliza-tion [7], and recommender systems [8]. However, local GNN inference grants users significant privileges to local models and data, introducing additional security vulnerabilities [9].\nThere are two primary security threats to inference on local devices: model intellectual property (IP) theft and data privacy breach. Valuable model IP includes the hyper-parameters, trained parameters, and model's functionality [10]. Data pri-vacy issues involve attacks such as membership inference [11] and link stealing attack [12], which can exploit the white-box accessibility of local models. Existing defenses, including watermarking [13], non-transferable learning [14], and ho-momorphic encryption [15], are often passive, inaccurate, or computation-expensive. Recently, studies have begun leverag-ing Trusted Execution Environments (TEEs) to protect local models and data [16], [17], by partitioning them into slices and deploying critical components within secure compartments. However, current TEE-based secure deployment mainly targets datasets for tasks such as computer vision and large language models, which is not applicable to GNN inference. Specifically, GNN operations require updating the graph node representations (i.e., embeddings) based on information from their neighboring data points. Thus, the entire graph data, including node features and their connectivity (adjacency matrix), has to be stored locally during inference, posing additional challenges for data privacy. Moreover, TEEs have limited memory capacities, while large graphs require signifi-cant storage for node features. To bridge this gap, we propose a new method for local GNN deployment, GNNVault, which effectively preserves both critical model parameters and the most important structural information (i.e., edges) using TEE. GNNVault employs the design strategy known as partition-before-training\" strat-egy [16], where the GNN model is designed with a public backbone and a private GNN rectifier. Specifically, the public backbone is computation-intensive but inaccurate, which is trained with a substitute graph and deployed in the untrusted environment. The GNN rectifier leverages the real private graph structure to rectify the backbone's node embeddings. It is designed to be much smaller and resides within the TEE. Our experiments show that GNNVault achieves performance comparable to the original unprotected GNN while preventing information leakage of private parameters and edges. We also evaluate our design on a real system using Intel SGX, demonstrating the low overhead of GNNVault.\nOur Contributions: We propose GNNVault, the first secure deployment framework designed specifically for graph neural networks. Our work makes the following contributions:\n1) We propose various communication schemes between pub-lic backbones and private rectifiers in GNNVault, and eval-uate the performance and implementation cost of rectifiers.\n2) Software evaluation demonstrates that GNNVault achieves a high inference accuracy with a small rectifier inside TEE while protecting critical model parameters and edge data.\n3) We implement a real-world deployment using Intel SGX to run GNNs locally. Experimental results show that the introduction of the enclave increases a small inference overhead and negligible accuracy degradation (less than 2%), demonstrating the effectiveness of GNNVault.\n4) We conduct a security analysis with link stealing attacks on the intermediate data communicated from the public backbone to the rectifier, showing that no private edge information is leaked in the untrusted environment."}, {"title": "II. BACKGROUND", "content": "A. Graph Neural Network\nGNNs process graph-structured data, which contains rela-tional information between nodes (or edges). Without loss of generality, we follow the common GNN definition [18]: a graph G = (V, E) consists of the node set V with n nodes, each with a feature represented by a d-dimensional vector, denoted as X \u2208 Rn\u00d7d. The set of edges E is depicted by an adjacency matrix A \u2208 {0,1}n\u00d7n, indicating the existence of connections between nodes in G. A GNN model is a function f(X, A) to summarize global information in the graph for downstream tasks including node classification [19], community detection [6], and recommender systems [20]. The forward propagation of GNN layer k can be written as\n$$H^{(k)} = \\sigma(AH^{(k-1)}W^{(k)})$$\nwhere H(k) \u2208 RN\u00d7d(k) is the matrix of node embedding at layer k, A is the adjacency matrix with self-loops normalized with degree matrix, W(k) \u2208 Rd(k-1)\u00d7d(k) is the learnable weight matrix, and o is an activation function. Note that GNN operations require A through message passing during the inference process, which causes edge data privacy concerns.\nB. Model Deployment in Trusted Execution Environments\nA TEE, such as Intel SGX or ARM TrustZone, protects code and data within it, ensuring their confidentiality and integrity with hardware support against strong adversaries including a malicious host operating system. The increasing demand for edge AI applications, e.g., those that require real-time responses or rely on local data or private data, has led to a trend of using TEEs for IP protection and privacy preservation. Previous works focus on securely deploying DNNs by isolating the non-linear layers [21] or by partitioning the model and placing the final dense layers inside the TEE [22]. Recently, Zhang et.al [16] also consider the privacy of training data and propose a \u201cpartition-before-training\" strategy against membership inference attack [11]. Additionally, Liu et al. [17] design a deployment strategy that only allows single-directional data flow from the untrusted environment to the enclave. However, all previous works only focus on local DNN's IP and data privacy, which are not applicable to the secure deployment of a GNN on the edge. Our proposed method, GNNVault, bridges this gap by protecting the security of trained GNNs and private edge data using TEEs."}, {"title": "III. PROBLEM STATEMENT", "content": "A. Deploying GNN locally Causes Vulnerabilities\nDeploying GNNs on local devices requires access to graph data in addition to the trained model, which introduces unique security and privacy challenges. Similar to DNN deployment, the IP of the well-performed local model, including its trained weights and biases, is valuable asset that must be protected against model extraction attacks. Beyond the model IP, local GNN inference raises additional privacy concerns due to the nature of GNN architecture. Specifically, during the message-passing phase of GNN inference, target nodes aggregate infor-mation from neighboring nodes to update their embeddings. This process involves accessing sensitive edge data, such as user-product interactions in recommender systems. In our work, we will address the GNN IP infringement and edge data breach vulnerability during GNN deployment.\nB. Edge Privacy is Valuable\nMembership inference attack is the most common data privacy threat to machine learning models [11], where the goal is to determine whether a given data point belongs to the training set. However, in the context of GNNs, edge data raises additional privacy concerns. Link stealing attacks [12], [23] aim to infer the connectivity between any pair of given nodes. In this work, we focus on the adjacency information (edges), while considering the node features as public. A real-world example is illustrated in Fig. 1, where Alice (victim) deploys a recommender system (RS) on local edge devices. In such a product graph, the node features are public attributes of the products such as price, user reviews, or categories-that are available to any user. However, the internal relationships between products require intensive learning from user behavior data, which is valuable IP for the model vendor. Therefore, safeguarding the node connectivity information during GNN local inference is of great importance.\nC. TEE Has Memory Restrictions\nThe introduction of TEE greatly enhances data security and privacy with secure compartments. However, TEE platforms face significant memory limitations, a critical constraint for secure computation. For instance, for Intel SGX trusted en-claves, the physical reserved memory (PRM) is limited to 128MB, with 96 MB of it allocated to the Enclave Page Cache (EPC) [24]. Excessive memory allocation will lead to frequent page swapping between the unprotected main memory and the protected enclave, which can cause high overhead and additional encryption/decryption to ensure data integrity [25]. This memory constraint poses a significant challenge for deploying GNN models and the entire graph (including node features and adjacency information) within the secure enclave, which often far exceed the PRM limitation of enclaves."}, {"title": "IV. OUR APPROACH: GNNVAULT", "content": "A. Threat Model\nOur work aims to protect both GNN model parameters and the edge data during inference, specifically using the TEE of\nB. Overview of GNNVault\nGNNVault employs the design strategy known as partition-before-training [16], to completely avoid using the private edge data during training of the public backbone. To maintain the model performance, a GNN rectifier is designed to use the real adjacency matrix, which is lightweight and securely protected within the TEE. Furthermore, to prevent information leakage through intermediate data transfer, GNNVault allows only one-way communication from the untrusted environment to the enclave. As illustrated in Fig. 2, GNNVault can be summarized in four steps: (1) generate substitute graph(s); (2) train the public backbone; (3) train the private rectifier; (4) securely deploy the data and models.\nC. Public GNN Backbone\nThe backbone is trained with public data, aiming to pro-vide sufficient pre-computation during the inference process. Unlike the backbones for DNNs or LLMs in [16], which are open-source pre-trained models, GNNs are more dataset-specific and cannot directly leverage such models. Therefore, our GNNVault uses a pre-trained neural network backbone that utilizes public node features. We generate a substitute adjacency matrix based on node similarity and train this backbone for the target tasks (node classification).\nSubstitute Adjacency Matrix: To enable the backbone model to capture message passing between nodes, we design it as a GNN with a substitute adjacency matrix. This substitute adjacency is computed based on the similarity between nodes:\n$$A'(i, j) = \\begin{cases} 1 & \\text{if } F(i, j) \\geq \\tau \\text{ for each } j \\neq i \\\\ 0 & \\text{otherwise} \\end{cases}$$\nHere, F(i, j) = sim(xi, xj) represents the similarity measure computed between two node features, such as cosine similarity xiXj 2. Based on this similarity, we determine the connected XiXj edges using a threshold \u03c4. Alternatively, connectivity can be computed using the k-nearest neighbors, where we connect the top-k similar nodes to construct the adjacency matrix.\nTraining the Public Backbone: The public backbone consists of GCN layers trained with the substitute adjacency matrix A' and the public node features X. Each GCN layer k projects the high-dimensional features (embeddings) from RNxd(%) to a lower-dimensional space RNxd(k+1) through the message passing procedure. However, due to the loss of spatial infor-mation from the real adjacency matrix, this public backbone often exhibits lower accuracy. Therefore, the other component, a private rectifier, aims to recalibrate the embeddings in IR\u00d1\u00d7d(k+1) with real adjacency to achieve higher accuracy.\nD. Secure GNN Rectifier\nWith the public backbone trained using the substitute ad-jacency matrix, the node embeddings are passed from the\nPrivate Edge\nReal Graph\nTEE\nrectifier"}, {"title": "V. EXPERIMENTAL VALIDATION", "content": "A. Experiment Setup\nDatasets: We focus on semi-supervised learning tasks for\nnode classification with GNNs on multiple datasets at dif-ferent scales, as shown in Table I. Specifically, we evaluate GNNVault on three standard datasets in [26], two larger graphs Amazon Computer and Photo in [27], and CoraFull [28] which has many classes (70). Table I shows the memory requirement for a dense adjacency matrix. To train the GNN, we follow the common practice: using 20 labeled node per class [27], and the unlabeled nodes are used as the testing set. If not specified, we use KNN with k=2 as substitute graphs in backbones, which are chosen based on the ablation study in V-B4.\nModels: We evaluate GNNVault on three different GNN structures based on dataset features space size: M\u2081 has as a 3-layer GCN backbone with output channel size as (128, 32, C) and a rectifier (128, 32, C), which is designed for a smaller dataset including Cora, Citeseer, Pubmed, where C is the label space size. M2 has wider output channels (256) for both the backbone and the rectifier, which is used for the case when the model has a high-class number, such as CoraFull. We also test a larger and deeper design M3 with a backbone (256, 64, 32, 16, C) and rectifier (64, 32, C). Different rectifiers may have various input channel sizes, as shown in Fig. 3.\nMetrics: We first compute the accuracy of the original GNN model, which has the same architecture as the backbone but uses the real adjacency matrix and therefore has different parameters, denoted as Porg. The difference between the classification accuracy of the public backbone in the normal world, denoted as pbb, and the accuracy of the GNN rectifier, denoted as prec, measures the protection performance. We also compare the size of the public backbone and private rectifier in terms of the number of model parameters, denoted as bb, and Orec. To show accuracy degradation, we utilize the difference between prec and porg and the silhouette score to show the node's clustering performance [29].\nPlatform: We run the training procedure and the software evaluation process on Ubuntu 18.04.6 with NVIDIA TITAN RTX. We also follow a similar setting to [16] as a real-world case study on an SGX-enabled Intel Core i7-7700, running Ubuntu 20.04.6 and utilizing Intel SGX SDK and PSW."}, {"title": "VI. CONCLUSIONS AND FUTURE WORK", "content": "In this paper, we propose a novel secure GNN deploy-ment strategy called GNNVault. Our approach involves strict isolation, training a public backbone model using only pub-lic data, and training a rectifier on private data to rectify"}]}