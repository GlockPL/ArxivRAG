{"title": "DreamStory: Open-Domain Story Visualization by LLM-Guided Multi-Subject Consistent Diffusion", "authors": ["Huiguo He", "Huan Yang", "Zixi Tuo", "Yuan Zhou", "Qiuyue Wang", "Yuhang Zhang", "Zeyu Liu", "Wenhao Huang", "Hongyang Chao", "Jian Yin"], "abstract": "Story visualization aims to create visually compelling images or videos corresponding to textual narratives. Despite recent advances in diffusion models yielding promising results, existing methods still struggle to create a coherent sequence of subject-consistent frames based solely on a story. To this end, we propose DreamStory, an automatic open-domain story visualization framework by leveraging the LLMs and a novel multi-subject consistent diffusion model. DreamStory consists of (1) an LLM acting as a story director and (2) an innovative Multi-Subject consistent Diffusion model (MSD) for generating consistent multi-subject across the images. First, DreamStory employs the LLM to generate descriptive prompts for subjects and scenes aligned with the story, annotating each scene's subjects for subsequent subject-consistent generation. Second, DreamStory utilizes these detailed subject descriptions to create portraits of the subjects, with these portraits and their corresponding textual information serving as multimodal anchors (guidance). Finally, the MSD uses these multimodal anchors to generate story scenes with consistent multi-subject. Specifically, the MSD includes Masked Mutual Self-Attention (MMSA) and Masked Mutual Cross-Attention (MMCA) modules. MMSA module ensures detailed appearance consistency with reference images, while MMCA captures key attributes of subjects from their reference text to ensure semantic consistency. Both modules employ masking mechanisms to restrict each scene's subjects to referencing the multimodal information of the corresponding subject, effectively preventing blending between multiple subjects. To validate our approach and promote progress in story visualization, we established a benchmark, DS-500, which can assess the overall performance of the story visualization framework, subject-identification accuracy, and the consistency of the generation model. Extensive experiments validate the effectiveness of DreamStory in both subjective and objective evaluations. Please visit our project homepage at https://dream-xyz.github.io/dreamstory.", "sections": [{"title": "INTRODUCTION", "content": "Story visualization aims to create a visually captivating\nand coherent sequence of visual content (including images\nand videos) that aligns with a given story. This field has\nbecome increasingly important in entertainment [1] and\neducation [2], [3]. However, story visualization is particularly\ndaunting in open-domain contexts, subjectized by diverse"}, {"title": "RELATED WORKS", "content": ""}, {"title": "Visual Content Generation", "content": "Variational AutoEncoders (VAEs) [34] and Generative Ad-\nversarial Networks (GANs) [28], [35]\u2013[37] used to dominate\nvisual generation field. Despite the significant advancements\nmade by GAN, its optimization challenges persist [38]-\n40]. Later, diffusion-based generative models [4], [5], [41]\u2013\n[44] have emerged, achieving impressive image quality\nand diversity. Notably, Stable Diffusion (SD) [6] utilizes"}, {"title": "Dataset-based Story Visualization", "content": "Early methods [47]\u2013[53] for story visualization relied on col-\nlecting datasets, such as PororoSV [28] and FlintstonesSV [29].\nFor example, Rahman et al. [50] propose a novel autoregres-\nsive diffusion-based framework. This framework includes\na visual memory module that implicitly captures the ac-\ntor and background context across the generated frames.\nPan et al. [53] propose an auto-regressively diffusion model\nconditioned on history captions and generated images. It\nemploys multimodal guidance (a CLIP [54] text encoder\nand a BLIP [55], [56] multimodal encoder) to ensure the\ngeneration of relevant and coherent images. Liu et al. [25]\nfurther proposed the StorySalon dataset and achieved SOTA\nresults.\nHowever, these methods are constrained by the size and\nquality of existing datasets, limiting their performance in\nopen-domain tasks. In contrast, our approach is designed for\nopen-domain scenarios and is training-free, circumventing\nthe challenges of gathering high-quality story visualization\ndatasets."}, {"title": "Few-shot Finetuning Consistent Generation", "content": "Few-shot finetuning methods [10], [21], [23], [24], [57]\u2013[59]\nprimarily revolve around personalized image generation\nbased on a few subject images. The model is finetuned on\nthese images to learn their unique textual expressions. For\nexample, Dreambooth [23] first proposed fine-tuning SD with\nLORA [60] on several images to make the model remember\nspecific subject tokens for reference images. Sun et al. [10]\nfurther extend it in a never-ending manner, i.e., new concepts\nfrom the user are quickly learned without catastrophic\nforgetting. Jang et al. [24] proposed using a segmentation\nmodel to segment subjects for training and inference, effec-\ntively mitigating the influence of multi-subject blending. It\nhas achieved SOTA performance in the field of few-shot\nfinetuning for multi-subject consistent generation.\nHowever, these methods necessitate finetuning for each\nstory or subject, resulting in extra computational costs.\nBesides, this approach inevitably risks overfitting, leading\nto a decline in the aesthetic quality and diversity of the\ngenerated images [27]."}, {"title": "Encoder-Based Consistent Generation", "content": "In foundational T2I models, such as SD [6] and SDXL [8],\nthe text is typically encoded into an embedding vector\nand injected into a cross-attention mechanism to generate\nimages satisfying textual conditions. To achieve consistent\ngeneration, previous methods [22], [61]\u2013[65] attempt to\ndesign an image encoder for generation under the image\ncondition. Specifically, some studies [62], [64] tried to train a\nface encoder to ensure that the generated images maintain\nID consistency. Similarly, Ye et al. [22] tried to train an\nimage encoder that converts image conditions into a space\naligned with the original text embedding. However, these\nmethods can only handle a single subject. Therefore, they"}, {"title": "Training-free Consistent Generation", "content": "Training-free methods have gained widespread attention due\nto their efficiency. These methods [26], [27], [31] maintain\nsubject consistency by facilitating interaction between the\ntarget and reference images in the self-attention layer. For\nexample, MasaCtrl [26] introduced mutual self-attention,\nwhich replaces the key and value in self-attention with\nthose from the reference image. They also utilized a cross-\nattention map as a mask to ensure that mutual self-attention\nconcentrates on relevant subjects. ConsiStory [27] introduced\nSubject Driven Self-Attention (SDSA), which allows each\nframe to refer all subjects from multiple reference images in\na batch. They also implemented token dropout and blended\nVanilla Query techniques to increase layout diversity, and\nused DIFT [66] for feature injection in self-attention to\nenhance detail consistency.\nHowever, these methods still struggle to generate multi-\nple subjects because all subjects in the target image can refer\nto all reference images regardless of whether their roles are\nthe same. Furthermore, they failed to consider fine-grained\ndescriptions of subjects that contain rich information on\nattributes which are beneficial for maintaining consistency."}, {"title": "Large Language Model", "content": ""}, {"title": "LLM in Text Understanding", "content": "Large Language Model (LLM) has recently demonstrated\nimpressive capabilities in various NLP tasks, such as text\nsummarization [67], [68], and question answering [69], [70].\nMoreover, ChatGPT employs Reinforcement Learning from\nHuman Feedback (RLHF) [71] to align the model's output\nwith human preferences, demonstrating an impressive ability\nfor human interaction. Its remarkable In-Context Learning\n(ICL) [17] ability enables it to generate expected outputs\nby completing the input text's word sequence, without\nadditional fine-tuning. Furthermore, some studies [18]\u2013[20]\nhave revealed that carefully crafted Chain of Thought (CoT)\nstrategies can significantly improve the performance of LLM\nmodels in handling intricate and lengthy tasks. Though LLM\nmodels can summarize texts and answer human questions,\ntheir ability to generate suitable prompts that guide diffusion\nmodels for story visualization is less studied. In the story\nvisualization field, diffusion models are limited to recogniz-\ning subjects in novel visual vocabulary as they are usually\nreferred to by their names without visual descriptions. In\nthis paper, we study how to adjust the prompts generated by\nLLM models to better bootstrap diffusion models for story\nvisualization."}, {"title": "LLM in Image Generation", "content": "Advanced visual generation models struggle with low-\nquality descriptions, which impedes their comprehension of\nsubtle semantics. Many research [72]\u2013[76] efforts have aimed\nto enhance the capabilities of T2I models by refining datasets\nand modifying user prompts. Specifically, Segalis et al. [72] en-\nhanced generation performance by re-captioning the images\nusing a specialized LLM and retraining a text-to-image model."}, {"title": "OUR APPROACH", "content": ""}, {"title": "Overall Framework", "content": "In this subsection, we introduce our automated story vi-\nsualization framework (DreamStory), shown in Fig. 2. The\nframework operates as follows:\n1) Story comprehension and prompt generation. Given a\nstory (e.g., The Little Match Girl), a Large Language Model\n(LLM), such as GPT-4 [32], comprehends the narrative and\ngenerates concise yet detailed prompts for key subjects\nand scenes. These prompts serve as the foundation for\nsubsequent visual content generation.\n2) Prompt alignment and rewriting. The LLM identifies\nthe subjects within each scene and performs necessary\nrewrites, replacing names with descriptions that the dif-\nfusion model can understand, such as rewriting \"Kondo\"\nto \"towering gorilla.\" This enriches the scenes for visual\ncontent generation.\n3) Subject portraits generation. The Text-to-Image (T2I)\nmodel then utilizes these prompts to create subject por-\ntraits. By focusing on individual subjects, this approach\nensures alignment with the provided prompts.\n4) Multimodal anchors for scene generation. The subject\nportraits, accompanied by their textual descriptions, act as\nmultimodal anchors. The subsequent T2I model leverages\nthese multimodal anchors to maintain subject consistency.\nIt enriches the scenes with additional details, resulting in\nhigh-quality visual representations. These images can be\ntransformed into video clips using an Image-to-Video\n(I2V) model, such as SVD [74], ConsistI2V [79], and\nKling 1.\nOur comprehensive process enhances the final image quality,\nmaking DreamStory indispensable for vivid story visualiza-\ntion."}, {"title": "LLM Prompt Generation Model", "content": "The Chain of Thought (CoT) [80]\u2013[83] strategy has shown\npromising results in LLMs. The core idea of CoT is to break\ndown complex problems into a series of simpler, manageable\ntasks, which guides the model towards generating antici-\npated results and enhances overall performance [80], [81].\nInspired by these pioneering works, we designed a\nprompt generation model based on the CoT strategy for\nthe diffusion model. Our approach simplifies the entire\nprocess into a sequence of simple steps: generating prompts\nfor subjects or scenes, annotating whether subjects are\npresent in scenes, and making necessary revisions. Each of\nthese tasks (text understanding or rewriting) is considerably\neasier due to its widespread presence in the LLM's training\nsamples compared with that of directly obtaining a suitable\nprompt for the diffusion model to visualize stories. All the\ntasks prompts are designed with at least two in-context\nexamples to improve the performance and formatting of the\nresults [83].\nIn the process of annotating scenes, we utilize the LLM\nto determine if a subject is present in the scene's imagery,\ngiven the subject's name and detailed prompts. We have\nobserved that the LLM often generates scene prompts\nusing the subject's name, such as \"Kondo\". However, these\nprompts encounter difficulties when applied to the diffusion\nmodel, which often fails to recognize the names of subjects,\nparticularly when the subject is not well-known and is absent\nfrom the training data.\nTo address this issue, we propose rewriting the scene\nprompts. Specifically, we employ the LLM to create a concise\nprompt for the subject that encapsulates its key attributes.\nWe then instruct the LLM to rewrite the scene based on\nthis newly created short prompt. For example, the subject\n\"Kondo\" would be replaced with a description such as\n\"towering gorilla\". This method ensures a more accurate\nvisual representation of the subject within the scene and is\nmore suitable for the diffusion model.\nOur approach to LLM prompt generation presents a\nlogical sequence of steps that address the challenges of"}, {"title": "Multi-Subject Consistent Diffusion Model", "content": "Preserving subject consistency is a crucial objective in the\ngeneration of story images. Our MSD is specifically designed\nto provide a training-free solution for open-domain story vi-\nsualization, as shown in Fig. 3. This approach is necessitated\nby the considerable costs involved in obtaining high-quality\ndatasets for story visualization."}, {"title": "Existing Attention Mechanism", "content": "A standard attention layer in the popular diffusion model\n(e.g., SD [6], SD-XL [8], and Playground [9]) can be formulated\nas follows,\n$A_i = softmax (Q_iK_i/\\sqrt{d_k}),$\n(1)\n$O_i = convout(A_iV_i),$\n(2)\nwhere the $O_i$ represents the attention output and $A_i$ indicates\nthe attention weight for the i-th image. Q is the query features\nprojected from the spatial features, and K V are the key and\nvalue features projected from the spatial features (in self-\nattention layers) or the textual embedding (in cross-attention\nlayers) with corresponding projection matrices. A simple\nconvolution layer convout is finally applied to fuse the output\nfeatures. We omitted the residual connection and layer count\nto simplify our expression.\nExisting work has verified that self-attention can control\nthe appearance of the generated image [84], while cross-\nattention controls the layout and can be used to locate the\narea of the target subject [85].\nBased on these discoveries, recent works have verified\nthe appearance information of reference images can be"}, {"title": "Accurate Object Mask Generation", "content": "Accurate subject mask generation has been verified as a\ncrucial problem in image generation [26], [27] and edit-\ning [65], [86]. However, obtaining the subject mask in an\nunregenerated target image is difficult. Previous works\nutilize LLM to manage the layout of generated images\nfor editing [78] and accurate attribute binding [77]. This\npotentially leads to a lack of aesthetic layout and the\ngeneration of objects with unreasonable sizes, as LLM has not\nbeen optimized in this situation. Since diffusion models tend\nto generate similar layouts under close control conditions\nif the random seeds are the same [87], we adopt an open-\nvocabulary segmentation model, e.g., GroundingSAM [88],\nto obtain an accurate subject mask in rehearsal target images,\nwhich is pre-generate with the original diffusion model.\nThe detection phrases are marked by LLM, as mentioned\nabove. To improve accuracy, we contact tokens of all subjects\nseparated by periods as detection prompts for the target\nimage, such as \"man. girl.\". A simple post-processing is\nadopted to guarantee the non-overlapping of all masks,\nenhancing the robustness of our approach.\nDue to the imposition of a new control process, the\nimage may go beyond the SAM's mask during generation,\nespecially in the later steps. So, it is necessary to adjust the\nmask according to the features in the generation process.\nPrevious work [26], [27] mainly adopts a segmentation\nmask by averaging the cross-attention maps of the subject\ntoken. However, this strategy may create a holed and noisy\nmask [86]. Therefore, we obtain a segmentation mask by\nmultiplying the self-attention and cross-attention maps, with\nthe self-attention map serving as a completion of the cross-"}, {"title": "Masked Mutual Self-Attention", "content": "To alleviate the confusion between multiple subjects, we\npropose that only the appearances between the same roles\ncan be referenced, i.e., multiple subjects in the target image\ncan only refer to the same corresponding subject in other\nreference images. Given N subject portraits (reference image),\nwe aim to generate one corresponding scene image (target\nimage). By constructing the subject mask, the formalization\nof our self-attention layer is as follows,\n$K^+ = [K_1 \\oplus K_2 \\oplus ... \\oplus K_N \\oplus K_{TGT}],$\n(5)\n$V^+ = [V_1 \\oplus V_2 \\oplus ... \\oplus V_N \\oplus V_{TGT}],$\n(6)\n$M^+ = [M_1 \\oplus M_2 \\oplus ... \\oplus M_N \\oplus 1],$\n(7)\n$A^+ = softmax (Q_{TGT}K^+/\\sqrt{d_k}+log M^+),$\n(8)\n$O_{TGT} = convout(A^+ . V^+),$\n(9)\nwhere $M_i$ is the subject mask for i-th reference images,\nand $\\oplus$ indicates the concatenation operation. We assume the\nlast one to be the target image, denoted with the subscript\n\u2018TGT\u201d. The standard attention masking technique is adopted,\nwhich nullifies softmax's logits by assigning their scores to\n-\u221e based on the mask, followed by previous works [26],\n[27]. It should be noted that, unlike ConsiStory [27] and\nStory Diffusion [31], which allows all areas of the target image\nto reference the subject in the reference image, our method\nonly permits referencing information from the same subject."}, {"title": "Masked Mutual Cross-Attention", "content": "As mentioned above, the rich information about the subject\nis not only contained in the reference image but also in\nthe reference text. To fully utilize this information, we've\nimplemented a Masked Mutual Cross-Attention (MMCA)\nmechanism. Its core idea is to allow the subject in the\ntarget image to query their reference text embedding and\nobtain rich, detailed attributes. We replace K and V with"}, {"title": "EXPERIMENTS", "content": "This section will introduce the evaluation benchmark and\nmetrics, implementation details, and comprehensive experi-\nmental results. In Sec. 4.1, we will introduce the constructed\nbenchmark, which includes 100 stories and 400 synthetic\ncases. Sec. 4.2 will introduce the objective and subjective\nevaluation metrics. We present the specific implementation\ndetails in Sec. 4.3, including our LLM, diffusion backbone,\nand MSD module. In Sec. 4.4 and Sec. 4.5, we will respectively\npresent the comparative results with the current state-of-the-\nart (SOTA) methods and conduct an ablation study. Sec. 4.6\nbriefly discuss the limitations of our method."}, {"title": "Evaluation Benchmark", "content": "To our knowledge, few datasets can validate the proposed\nDreamStory's performance in open-domain story visualiza-\ntion. To address this issue, we constructed a benchmark\nDS-500, including 100 real stories and 400 synthetic cases.\nThe benchmark of 100 real stories assesses our framework's\nholistic performance. The additional 400 synthetic samples,\ndivided into four groups of 100 samples, each with 0, 1, 2,\nand 3 subjects, are utilized to evaluate the precision of the\nLLM in annotating subjects present in the scene and the\nefficacy of multi-subject consistent generation."}, {"title": "The 100 Stories Benchmark", "content": "To validate the effectiveness of our overall framework, we\nfirst constructed a dataset. This dataset consists of 50 real,\ncopyright-free English stories randomly downloaded from\nfree-short-stories 2, and 50 short stories generated by ChatGPT.\nThese data effectively simulate the distribution of real stories,\nthereby providing a robust validation of the performance of\nour DreamStory in open-domain story visualization."}, {"title": "The 400 Synthetic Benchmark", "content": "Firstly, we instruct GPT to generate a variety of non-repetitive\nsubjects, each accompanied by detailed portrait prompts.\nWe then employ GPT to annotate these subjects with type\nattributes (e.g., girl, man, dog), which are applicable for DINO\ndetection. Subsequently, a subset of subjects is randomly\nselected, and GPT is tasked to generate scene prompts that ex-\nclusively include the chosen subjects. To prevent performance\ndegradation of the diffusion model due to overly lengthy\noutput text, we limit GPT's output to approximately 40 words\n(roughly 50 tokens). These scene prompts, along with their\nassociated subject prompts and type attributes, constitute"}, {"title": "Evaluation Metrics", "content": "In story visualization, aesthetics and image-text alignment\nare commonly employed metrics. In addition, the consistency\nof subjects across multiple frames is another crucial metric,\nwhich is one of the main problems this paper aims to address.\nTherefore, we evaluate generated results using three criteria:\n1) aesthetics, 2) consistency between scene image and text,\nand 3) subject consistency between scene and reference\nimage. To ensure accuracy and reliability, each criterion is\nevaluated objectively and subjectively."}, {"title": "Objective Evaluation", "content": "Followed by previous works [8], [9], we utilize an aesthetic\npredictor 3 to determine aesthetic scores. The CLIP 4 score is\nadopted to evaluate the similarity between the scene text and\nscene image, denoted as CLIP-T. To better assess subject con-\nsistency, we employ DreamSim [90] to evaluate the similarity"}, {"title": "Subjective Evaluation", "content": "Due to the bias of existing metrics, a user study is conducted\nto assess subjective results. Given the variability of individual\nratings and the broad spectrum of scores across different\nevaluators, we employed a pairwise comparison in our\nuser study. For each evaluation, two sets of images were\nrandomly displayed, each generated by a different method\nand accompanied by their respective texts. Participants\nwere asked to judge each metric by selecting one of three\noptions: Image A is superior, Image B is superior, or both\nare comparable. We engaged 5 independent evaluators for\nthe assessment. Each evaluator conducted 100 reviews per\nbenchmark, culminating in 500 votes in total. The final results\nwere compiled and are presented as percentages."}, {"title": "Implementation Details", "content": "LLMs as Story Director. We utilize ChatGPT4 [32], currently\nthe most advanced large-scale language model, as our story\ndirector due to its powerful interactive and long context\ncapabilities. Interaction with the LLM is conducted via their\nAPI 5.\nDiffusion Model Backbone. We first conducted an\nablation study on two popular T2I backbones, Playground 6\nand SDXL 7. The results are presented in Tab. 1. We adopt\nPlayground as the final T2I backbone due to its excellent per-\nformance in aesthetics and subject consistency. We utilize the\ndefault scheduler (EDMDPMSolverMultistepScheduler [92])\nwith 50 inference steps to ensure optimal performance during\nthe inference phase. The guidance scale [93] is set to 7.0 in our\nexperiments. The weight of text feature injection, A, is fixed\nto 0.9 for a tradeoff between scene semantics and consistency\nof subjects. The evaluation of all our models focuses on\ngenerating visual content with dimensions of 1280 (width)\nby 768 (height).\nAttention Mechanisms in MSD. Our MSD is applied\nacross all diffusion steps to ensure multi-subject consistency.\nThe masked mutual self-attention is applied to all decoder\nlayers to maintain the appearance consistency, followed by\nprevious works [26], [27]. Inspired by previous work [30],\n87], the masked mutual cross-attention is applied to all\nlayers for better cross-attention fusion. The dropout [27]\nstrategy with a dropout rate of 0.5 is adopted to enhance\nlayout diversity. Furthermore, we adopt the open-vocabulary\nsegmentation model, GroundingSAM [88], to generate pre-\ncise masks for the subjects. This process begins with the\ndetection of the subject using the open-vocabulary detection\nmodel, GroundingDINO [91], followed by segmentation with\nthe powerful SAM [94]."}, {"title": "Comparison with SOTA Methods", "content": "To demonstrate the advantage of the proposed DreamStory,\nwe compare our DreamStory with the state-of-the-art ap-\nproaches. These methods fall into two main categories:\n(1) MuDI [24], fine-tuned using reference images; and\n(2) training-free methods, ConsiStory [27] and StoryDiffu-\nsion [31]. All the approaches are tested under the same setting\nfor a fair comparison."}, {"title": "Objective Comparison", "content": "The overall results are presented in Tab. 2. As can be seen\nfrom the table, our DreamStory outperforms other methods\nin terms of all metrics. Notably, the D&C-DS metric of ours\nis significantly surpassed other methods, exceeding MuDI,\nConsiStory, and StoryDiffusion by margins of 0.1034(23.4%),\n0.1293(25.1%), and 0.1080(24.7%) respectively in the 2-Subject\nof DS-500 benchmark. This pattern is mirrored in the 3-\nSubject benchmark, reinforcing the effectiveness of our\nmethod in maintaining multi-subject consistency. The high\nperformance of LLM in annotating subjects is also presented"}, {"title": "Subjective Comparison", "content": "The overall subjective comparison results are presented\nin Fig. 12(a)(b)(c). It can be seen from Fig. 12 that over\n80% evaluators believe that our DreamStory surpasses or is"}, {"title": "Ablation Studies", "content": "We also conduct ablation studies in our benchmark to\nverify the effectiveness of each of our modules in MSD.\nWe integrated these two modules, Masked Mutual Self-\nAttention (MMSA) and Masked Mutual Cross-Attention\n(MMCA), individually into the baseline, i.e., Playground. All\nthe settings are compared from both subjective and objective\nperspectives as described in Sec. 4.2."}, {"title": "Objective Comparison", "content": "All the objective results are presented in Tab. 3. It is evident\nfrom Tab. 3 that adding MMSA and MMCA improved\nsubject consistency, as indicated by an increase in DreamSim\nsimilarity (DS) and D&C-DS. However, a minor decline was\nobserved in the similarity between the scene and its text.\nThis is attributed to our generation process's emphasis on\nsubject consistency, which marginally affects the scene's\ncontent. Nonetheless, this impact is negligible, i.e., the\ndifference in CLIP-T similarity is less than 0.007. These results\nconfirm that our method maintains subject consistency while\npreserving the scene's semantics. Moreover, Tab. 3 reveals\nthat incorporating both MMSA and MMCA modules led to\nour DreamStory achieving optimal performance regarding\naesthetic scores and subject consistency. This conclusively\nvalidates the effectiveness of our approach."}, {"title": "Subjective Comparison", "content": "We present the user study result of our DreamStory compared\nto our different settings, baseline, with MMCA and with\nMMSA in Fig. 12(d)(e). It can be seen from Fig. 12 that the\nevaluators prefer DreamStory to the other settings. We also\nshow the visual results of ablation studies in Fig. 7 to show\nthe effectiveness of each component. As Fig. 7 illustrates, the\napproach without the MMSA module has the potential to\ngenerate images with blending subjects, particularly when\ntwo subjects are close within the image, as seen in the first\nrow with the man and woman, and the second row with the\npirate and boy. Furthermore, without the MMCA module,\nthere is a significant discrepancy in appearance between\nthe generated subject's portraits and the reference image,\nas demonstrated in the second row's pirate and boy and\nthe fourth row's boy and man. This discrepancy can be\nattributed to two factors. Firstly, the subject's text contains"}, {"title": "Limitations and Failure Cases", "content": "Our method relies on the abilities of both the LLM and\ndiffusion model. Firstly, LLM may have hallucinations when\nlabeling whether the subject is in the scene. As shown in\nFig. 10, the scene includes a boy, but the LLM failed to\nidentify the boy. In addition, the LLM may not effectively\ndistinguish between subjects with similar descriptions, such\nas the pirate and pirate captain in the second row of Fig. 10.\nFinally, diffusion models suffer from semantic under-\nstanding issues, which may be difficult in multi-subject and\nmulti-attribute generation [24]. As shown in Fig. 11, when\nthe diffusion model failed to generate three subjects for the\nfirst time, our method also failed to generate three subjects\nwith consistent appearances. Despite these limitations, our\nframework still has promising potential as individual models\nevolve and progress."}, {"title": "CONCLUSION", "content": "This paper introduced an automatic training-free open-\ndomain story visualization framework, DreamStory. It lever-\nages Language Models (LLMs) as a story director to generate\nconcise prompts for subjects and scenes, annotating the\nsubjects in each scene. This information guides diffusion\nmodels in creating visually consistent content that aligns\nwith the story narrative. We also developed a novel Multi-\nSubject consistent Diffusion model (MSD) that leverages both\nthe subject prompt and its corresponding portrait to maintain\nconsistency in multiple subjects across frames. To validate\nour approach and promote progress in story visualization, we\nestablished an evaluation benchmark, DS-500. Our method\noutperforms previous methods in aesthetics, image-text\nalignment, and subject consistency through objective and\nsubjective evaluations.\nIn conclusion, our DreamStory method represents a\nsignificant step forward as a framework for open-domain\nstory visualization. It does not require additional training\nand is poised to enhance its performance as the underlying\nmodels evolve. This positions our framework for promising\nadvancements in story visualization."}]}