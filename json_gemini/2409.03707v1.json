{"title": "A Different Level Text Protection Mechanism With Differential Privacy", "authors": ["Qingwen Fu"], "abstract": "With the widespread application of differential privacy in text protection, however, the current text cleaning mechanism based on metric local differential privacy (MLDP) is not applicable to non-metric semantic similarity measurement, and cannot achieve a good trade-off between privacy and practicality. And currently when we perform differential privacy on long text data, all text data will be perturbed. This method of perturbing all texts may be relatively effective for downstream tasks on some data sets, but if applied to long text data, it may have a great impact on the overall meaning of the text. Therefore, in this article, we propose to use the weights of different words in the pre-trained model to assign different weight parameters to words of different importance. Perform differential perturbations. In addition to conducting inference attacks, we also use large models to perform privacy and validity tests on our perturbed data.", "sections": [{"title": "Introduction", "content": "In many natural language processing (NLP) applications, input text often contains sensitive information that can infer the identity of a specific person Jegorova et al. (2022). In addition, legal restrictions such as CCPA and GDPR may further restrict the sharing of sensitive text data. This makes it difficult for NLP service providers to collect training data unless the privacy concerns of data owners (including individuals and institutions) are properly addressed.\nA lot of work has been done to address privacy issues Lyu et al. (2020); Anil et al. (2021); Dupuy et al. (2022); Li et al. (2021) to train language models using differential privacy (DP)Dwork et al. (2006), which is considered the standard for privacy-preserving computing. These methods protect the data source by adding noise to the gradient or training data. However, they require service providers to collect raw data for LM training, which may still cause privacy leakage.\nIn order to fundamentally solve the privacy leakage problem, data needs to be fundamentally protected. Typically, these privacy mechanisms Feyisetan et al. (2019, 2020); Yue et al. (2021) work by replacing the original tokens in the original document with new tokens extracted from the output token set. To generate a cleaned text document. Specifically, they adopt metric local differential privacy (MLDP, also known as dx-privacy) to provide privacy and practicality guarantees. MLDPChatzikokolakis et al. (2013) inherits the idea of DP and ensures that the output of any adjacent input tokens is indistinguishable to protect the original tokens from being inferred. On the other hand, MLDP preserves the utility of the purified text by assigning higher sampling probabilities to tokens that are semantically closer to the original tokens. In these mechanisms, any metric distance (such as Euclidean distance) can be used to measure the semantic similarity between tokens.\nIn the paper Chen et al. (2022)\", an MLDP-based concept is proposed to assign a smaller custom output set to each input token to achieve token-level privacy protection. This method is an improvement on the santextYue et al. (2021) method, which in-"}, {"title": "Related Work", "content": "When discussing privacy risks and protection measures in natural language processing (NLP), we can see three main research directions: research on privacy attacks on deep learning models, differential privacy (DP) and its application in NLP, and the application of local differential privacy (LDP).\nFirst, privacy attacks against deep learning models, especially language models (LMs), have become an important research area. For example, Song and Raghunathan (2020) proposed a classification for recovering sensitive attributes or parts of original text from text embeddings output by popular LMS without relying on the structure or pattern of the input text. Carlini et al. (2021) demonstrated a black-box attack against GPT-2, capable of extracting verbatim text of the training data. These studies show that privacy attacks on LMs are realistic and damaging, so it is crucial to develop defenses with strict safeguards.\nSecondly, in terms of Differential Privacy (DP) and its application in NLP, DP has become the de facto standard for statistical analysis. For example, some research attempts to inject high-dimensional DP noise into text representations Feyisetan et al. (2019, 2020); Xu et al. (2020) but these methods fail to achieve a good balance between privacy and utility, mainly because of the \"dimensionality Curse\". Another approach is to learn private text representations through adversarial training Xie et al. (2017); Coavoux et al. (2018), where the adversary model is trained to infer sensitive information together with the master model, while the master model is trained to maximize the adversary's loss and minimize the main learning objective.\nThird, the application of local differential privacy (LDP) also plays an important role in NLP. LDP allows data owners to sanitize data locally before sending it to the server. This means data owners can share information without revealing the content of their original data. In NLP applications, LDP is particularly valuable because it can collect and analyze text data while protecting user privacy. For example, the LDP mechanism can be used to generate sanitized text datasets that can be used to train machine learning models without exposing personal information. The challenge of LDP is to achieve privacy protection while maintaining data practicality, especially when dealing with text data with complex structure and high-dimensional features.\nTo sum up, the NLP field faces multiple challenges when dealing with privacy protection issues. On the one hand, effective defense strategies need to be developed against privacy attacks on LMs; on the other hand, differential privacy and local differential privacy provide a series of solutions to protect the pri-"}, {"title": "Preliminaries", "content": "Before we delve deeper into our CusText technique, let's first briefly review some fundamental concepts, including $\\epsilon$-differential privacy and the exponential mechanism.\nDefinition 1 ($\\epsilon$-differential privacy) Given a privacy parameter $\\epsilon \\geq 0$, for all adjacent input pairs $x,x' \\in X$, and for every possible output $y \\in Y$, a randomized mechanism M satisfies $\\epsilon$-differential privacy if it adheres to the following condition:\n$\\frac{Pr[M(x) = y]}{Pr[M(x') = y]} < e^{\\epsilon}$\nIn this definition, a smaller $\\epsilon$ indicates a higher level of privacy protection. Theoretically, $\\epsilon$-DP ensures that even adversaries with infinite computing power cannot distinguish between the probability distributions of two adjacent inputs, as their probabilities of producing the same output y are closely matched. In the context of Natural Language Processing (NLP), any pair of input tokens that produce the same output set Y are considered adjacent. This paper continues to use this definition for adjacent inputs.\nDefinition 2 (Exponential Mechanism). Given a scoring function $u : X \\times Y \\rightarrow \\mathbb{R}$, the exponential mechanism M(X, u, Y) achieves $\\epsilon$-differential privacy by randomly selecting an output token $y \\in Y$ to perturb the input token $x \\in X$ with a probability proportional to\n$e^{\\frac{\\epsilon u(x,y)}{2\\Delta u}}$\nHere, u(x, y) represents the score of the output token y for the input token x. Additionally, the sensitivity of u, denoted as $\\Delta u$, for the exponential mechanism (EM) is defined by\n$\\Delta u := max_{y\\in Y} max_{x,x'\\in X} |u(x, y) - u(x',y)|$\nAccording to the second definition, lower sensitivity makes it statistically more difficult to distinguish the original token from its adjacent tokens. In practice, we may standardize the scoring function u, normalizing its sensitivity $\\Delta u$ to a fixed value (e.g., 1), so that the selection probability for each output token y for an input token x is solely related to u(x, y), considering that $\\epsilon$ and $\\Delta u$ are predetermined, and a larger u(x, y) results in a higher sampling probability. In an NLP task, we assume each document D = (Ri)1 contains m records, and each record R = (t)=1 contains n tokens. We define the task of text sanitization as follows: Given an input document D containing sensitive information, a set of all possible input tokens X, a set of all possible output tokens Y, and a differential privacy mechanism M (e.g., the EM used in this work), it applies the mechanism M to each input token tj $\\in$ D, replacing it with an output token t'j $\\in$ Y if tj $\\in$ X. All tokens after replacement form the sanitized document, i.e., D' = (R) and R' = (t)=1\nFollowing previous studies Xu et al. (2020); Feyisetan et al. (2019); Yue et al. (2021); Chen et al. (2022); Qu et al. (2021), we still adopt a semi-honest threat model in the context of local differential privacy. In this model, the data owner only submits sanitized documents to the service provider. However, a malicious service provider may try to extract sensitive information from the received data. We assume that the adversary can only obtain the sanitized text and all algorithms and mechanisms are public and transparent. In addition, we also assume that the adversary has unlimited computing power."}, {"title": "Method", "content": "Our privacy perturbation method is based on the CusText mechanism. The difference is that we use the BERT pre-trained model to assign weights to different words in the same example. Then we average the weights of multiple heads and layers. We remove the weights of CLS and septoken and regularize the weights of other words. Use this weight value to represent the importance of different words. Then we combine the CusText mechanism to perform different degrees of perturbation for our words of different importance."}, {"title": "Experiment", "content": ". Experimental Setup,\n5.1 Experimental Setup Following Feyisetan et al. (2020); Yue et al. (2021) We selected two datasets from the GLUE benchmark Wang (2018) in our experiments, both of which contain In our experimental section, we aim to demonstrate the efficacy of using attention mechanism parameters to represent the importance of different words within a sample. This section is divided into two parts, each utilizing the public datasets SST-2 and QNLI to validate our method.\nDatasets Description:\n\u2022 SST-2: A widely-used movie review dataset for sentiment classification, consisting of 67,000 training samples and 1,800 test samples. The evaluation metric is accuracy.\n\u2022 QNLI: A dataset for sentence pair classification with 105,000 training samples and 5,200 test samples. Accuracy is also used as the evaluation metric here.\nIn our approach, for both the SST-2 and QNLI datasets, we first identify the most and least important words, quantified as the top and bottom 10%, 20%, 30%, 40%, 50%, and 60% based on the attention scores. These words are considered as the sensitive words that need to be perturbed. We record the number of words actually perturbed during training and compare it under similar total perturbation conditions to gauge the effectiveness of our method. We use the vocabulary from CounterFitting in GloVe, and apply both Euclidean distance and cosine similarity as measures for comparing GloVe vectors. The sensitive word list is derived from the probabilities associated with different words in the pre-trained model. For each downstream task, we set the maximum sequence length to 128 and limit the training to 3 epochs. On both SST-2 and QNLI datasets, the batch size is set to 64. We use bert-base-uncased as the pre-trained model with an increased learning rate of 2 \u00d7 10-5. The experiments are conducted on an A100 GPU.\nThe second part of our experimental analysis focuses on demonstrating the effectiveness of our approach. In this phase, we perturb words of varying degrees of importance specifically, 5%, 10%, and 20% of the words determined by our quantifier. We then evaluate both the privacy and effectiveness of the perturbed datasets using several established mechanisms.\n\u2022 Evaluation Mechanisms: We apply various metrics to assess the privacy levels and the utility of the datasets after perturbation.\n\u2022 Data Perturbation: We methodically perturb the words identified as having high, medium, and low importance to measure the impact on the dataset's utility and privacy."}, {"title": "Experiment Result", "content": "Below are some of my experimental results when e equals to 3."}, {"title": "result analysis", "content": "For the perturbation of data with different importance, we conducted experiments on the SST-2 and QNLI datasets. For fair comparison in the future, we chose Glove as the token embedding and controlled other variables to be the same. Table 1 shows the results of perturbing words of different importance on the SST-2 dataset while keeping the training set unchanged. For the same test set, words of different importance are perturbed while keeping $\\epsilon$ = 3 unchanged. As can be seen from the figure, when the test set data is perturbed with basically the same amount of data, the result of perturbing more important words is worse than that of perturbing less important words, which also proves that our vocabulary extraction method is correct. Figure 2 shows the results of perturbing the training data and test data at the same time. The results show that when perturbing the same number of words of different importance, perturbing more important words has a greater impact on the results, which also proves that our extraction strategy is correct. When we make a horizontal comparison, we find that when we use the perturbed training set for training, the matching effect with the test set is better, which also reflects the effectiveness of our method for words of different importance to a certain extent. When we observe the results of the QNLI dataset, we can also draw the above conclusions. Therefore, our Transformer-based extraction method is effective. When we perform differential privacy on the text, we can selectively perturb words of different importance. Of course, this"}, {"title": "Conclusion and limitation", "content": "Conclusion: This method proves that we can reflect the importance of different words in different sentences through multiple layers of Transformers and the attention weights between them, but more supplementary experiments are needed. Moreover, when we apply this method to long text data, our accuracy will be biased due to the limitation of the maximum length of Transformer and the long text length. This requires us to combine some other models and find a way to obtain longer length data at the same time. We need to do more work on this basis to improve its performance. We can do more experiments and research on this basis in combination with LLM."}, {"title": "Future work", "content": "With the recent emergence and development of LLM, I think we can combine the large oracle model with the discovery of sensitive data. Combined with prompts, LLM can identify important and sensitive information in the text. And we can combine LLM with this method to filter sensitive information in the text except for specific categories, because some other information in the text that is not classified may also contain some critical sensitive information. This is a direction worth exploring."}]}