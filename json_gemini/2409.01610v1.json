{"title": "DECOMPOSE THE MODEL: MECHANISTIC INTER- PRETABILITY IN IMAGE MODELS WITH GENERALIZED INTEGRATED GRADIENTS (GIG)", "authors": ["Yearim Kim", "Sangyu Han", "Sangbum Han", "Nojun Kwak"], "abstract": "In the field of eXplainable AI (XAI) in language models, the progression from local explanations of individual decisions to global explanations with high-level concepts has laid the groundwork for mechanistic interpretability, which aims to decode the exact operations. However, this paradigm has not been adequately explored in image models, where existing methods have primarily focused on class-specific interpretations. This paper introduces a novel approach to systematically trace the entire pathway from input through all intermediate layers to the final output within the whole dataset. We utilize Pointwise Feature Vectors (PFVs) and Effective Receptive Fields (ERFs) to decompose model embeddings into interpretable Concept Vectors. Then, we calculate the relevance between concept vectors with our Generalized Integrated Gradients (GIG), enabling a comprehensive, dataset-wide analysis of model behavior. We validate our method of concept extraction and concept attribution in both qualitative and quantitative evaluations. Our approach advances the understanding of semantic significance within image models, offering a holistic view of their operational mechanics.", "sections": [{"title": "INTRODUCTION", "content": "In the field of eXplainable AI (XAI), efforts have historically transitioned from Local explanation to Global explanation to Mechanistic Interpretability. While local explanation methods including Selvaraju et al. (2016); Montavon et al. (2017); Sundararajan et al. (2017); Han et al. (2024) have focused on explaining specific decisions for individual instances, global explanation methods seek to uncover overall patterns and behaviors applicable across the entire dataset (Wu et al., 2022; Xu- anyuan et al., 2023; Singh et al., 2024). One step further, mechanistic interpretability methods seek to analyze the fundamental components of the models and provide a holistic explanation of opera- tional mechanics across various layers.\nRecently, researchers in language models, especially in Large Language Models (LLMs), have ex- tensively studied mechanistic interpretability to reveal the causal relationships and precise mecha- nisms transforming inputs into outputs (Geva et al., 2022; Bricken et al., 2023; Gurnee et al., 2024). Researches on interpretability in image models, however, have typically focused on class-wise ex- planations (Fel et al., 2023b; Ghorbani et al., 2019) rather than dataset-wide explanations. Further- more, mechanistic interpretability, a comprehensive explanation dealing with every layer within the whole dataset, has not been applied to image models. This distinction arises because images consist of pixels that do not inherently represent concepts, unlike languages where each comprising word itself can be treated as a concept. Additionally, as meaningful structures in images are localized and only occupy small regions of the entire image, the embedding space in image datasets is far more sparse compared to that in language datasets.\nIn this paper, we present a novel approach to mechanistic interpretation in image models by system- atically decomposing and tracing the pathways from input to output across an entire dataset. Unlike"}, {"title": "RELATED WORKS", "content": "Local Explanation methods, such as LIME (Ribeiro et al., 2016) and SHAP (Lundberg & Lee, 2017), have been widely studied to provide an explanation for a specific instance. These techniques identify the most influential input features for specific instances, aiding in the understanding of particular decisions. However, they often lack generalizability, as they focus on isolated cases rather than providing insights into the model's overall behavior across different inputs.\nTo address these limitations, Global Explanation methods were introduced (Kim et al., 2018; Ghor- bani et al., 2019). Unlike local explanations, they aim to provide a broad understanding of a model's behavior by analyzing patterns and decision-making processes across the entire dataset. However, in vision models, global explanation methods, such as TCAV (Kim et al., 2018) and ACE (Ghorbani et al., 2019), remain class-specific. These methods quantify the importance of high-level concepts or identify concepts through multi-resolution segmentation and clustering techniques, but they often focus on understanding these concepts within individual classes. This class-specific focus limits their ability to provide a comprehensive understanding of the model's behavior across the entire dataset, which is a crucial component of true global explanation. Moreover, Fel et al. (2023a) extends var- ious local, attribution methods into global, Concept ATtribution (CAT) methods, but it primarily focuses on interpreting the output from a single layer without addressing the interpretation of inter- actions between different layers. This limitation underscores the importance of our approach, which provides a more comprehensive analysis by considering the relationships between layers.\nAs the need for deeper insights into model behavior grew, researchers turned to Mechanistic Inter- pretability. While global explanations provide a high-level understanding of a model's behavior by identifying patterns across a dataset, they do not analyze the internal workings of the model. Mech- anistic Interpretability addresses this gap by focusing on how specific components\u2014such as neu- rons, layers, or circuits-interact and contribute to the overall function. For instance, CRP (Achtibat"}, {"title": "METHOD", "content": "Fig. 2 shows the overview of method. In the figure, each process of our method is represented with the corresponding section number.\n3.1 ANALYSIS UNIT: PFV-ERF DATASET\nIn our study on mechanistic interpretability in image models, we utilize several key components essential for understanding and analyzing the network's behavior from the work of Han et al. (2024). Specifically, we use a Pointwise Feature Vector (PFV) as the unit of analysis and its corresponding Effective Receptive Field (ERF) as the visual label to effectively show and validate the knowledge encoded by the PFV.\nFirstly, a PFV is a vector of neurons along the channel axis within a hidden layer that share an identical receptive field. Given the embedding of layer l denoted as $A^l \\in \\mathbb{R}^{H^lW^l \\times C^l}$, where $C^l$ is the number of channels and $H^lW^l$ represents the spatial dimensions of the feature map, the PFV at position $p \\in \\{1,\\ldots\\ldots,\\ldots, H^lW^l\\}$ is represented as $x_p \\in \\mathbb{R}^{C^l}$. This vector encapsulates a localized feature representation at a specific point within the input image, providing a clear characterization of the features at that particular location. Unlike individual neurons, a PFV ensures monoseman- ticity, capturing a singular, coherent concept from the multi-channel features at a specific spatial location. Therefore, we decompose a layer in a network using PFVs. More specifically, a PFV in the preactivation space is linearly decomposed with the concept vectors.\nSecondly, we use the ERF as the PFV's label. Receptive Field (RF) denotes the region within the input image that influences the activation of a specific feature, defining the spatial extent over which the input pixels contribute to the feature's activation. Han et al. (2024) further refined this concept to Effective Receptive Field (ERF) to highlight the differential impact of individual pixels, identifying those that are most influential in the computation of the PFV. With ERF, we directly attribute a meaning (or a concept) to each hidden layer feature vector (PFV in our case), in contrast to other existing methods, which infer feature vector meaning through indirect techniques. Ghorbani et al. (2019) and Kowal et al. (2024) used global average pooling after masking, and Fel et al. (2023b) used bilinear interpolation on the masked feature maps to create a squared region to provide an indirect explanation of feature vectors by transforming segmented areas into representative vectors. Yet, with ERF, we explicitly assign meanings to the hidden layer feature vectors, treating them as representations of specific concepts so that we can offer a more straightforward interpretation of how particular features contribute to the model's decisions."}, {"title": "CONCEPT EXTRACTION", "content": "To determine the principal axis of the PFVs in each layer and find out the concept vectors, we utilize ImageNet validation dataset, consisting of 50,000 images. Even though there are $H^lW^l$ PFVs within a single layer, we take only one PFV and its corresponding ERF, resulting in 50,000 PFV-ERF pairs per layer for the dataset. In an image, we sample a PFV in a non-uniform sense to reflect its contribution to the output (logit), due to the foreground-background imbalance problem in images; If we sample PFVs randomly from an image, then the majority would capture the background, which would be irrelevant to the output class. For example, the sky in an image could be present across various classes, leading to an overrepresentation of the class-irrelevant feature, sky, rather than critical features like a bird's beak. This overrepresentation of irrelevant features within the PFVs could skew the identification of the principal axes of the PFVs. Thus, we probabilistically"}, {"title": "PFV DECOMPOSITION", "content": "Let there be k concept vectors in layer 1, denoted as $v_1,\\ldots,\\ldots, v_k$, discovered in the same C- dimensional vector space $V^l$ with PFVs. Then, each PFV $x^l_p$ can be expressed as a linear combi- nation of the concept vectors:\n$x^l_p = \\sum_{j=1}^k U_{pj}v_j + \\epsilon,$\n(1)\nwhere $U_{pj}$ is the coefficient representing the contribution of the j-th concept vector to PFV $x^l_p$ ($U_p = [U_{p1}, \\ldots,\\ldots, U_{pk}]^T$), and $\\epsilon$ is the residual error. To determine the coefficients $u_p$, we use Lasso"}, {"title": "INTER-LAYER CONCEPT ATTRIBUTION", "content": "In this paper, we leveraged Integrated Gradients (IG) (Sundararajan et al., 2017) to calculate the inter-layer concept attribution. Among other attribution methods, we utilized IG due to its superiority across various reliability metrics, such as C-Deletion, C-Insertion, and C-uFidelity, which are crucial in ensuring the robustness and accuracy of concept-based explanations (Fel et al., 2023a).\nBased on IG, we propose a novel method, Generalized Integrated Gradients (GIG), which extends the integrated gradients to quantify the contribution of a specific concept vector in a layer to both the final class output and the concept vectors of subsequent layers.\nLet a and b denote the preceding and target layer, and $X^l(l \\in \\{a,b\\})$ be the embeddings of the corresponding layer. In this work, we want to measure the influence of a query concept vector in layer a, $v_a$, on the target concept vector in layer b, $v_b^*$. To compute the attribution for the target concept vector, we first compute the output embeddings\n$\\Omega^b(\\alpha) = F_{ab}(\\alpha U_aV_a^T)$\n(3)\nin layer b by varying the embeddings in layer a from 0 to $X^a = U_aV_a^T$, i.e, $\\alpha \\in [0, 1]$ in Eq. (3). Here, $F_{ab}$ represents the nonlinear function from layer a to b and $U_aV_a^T$ is the approximation of $X^a$ obtained in Sec. 3.2.2. Then, we project $\\Omega^b(\\alpha)$ onto the target concept vector $v_b^*$ and obtain the projected vectors. These projected vectors are spatially aggregated and we compute the integrated gradient for the q-th element of the coefficient vector, $u_{pq}$, which is the component of $v^a_q$ in the PFV $x_p^a$ at position p as follows:\nGIG($v^a_q|p \\rightarrow v_b^*) = u^a_{pq} \\sum_{i=1}^{H^bW^b} \\int_{\\alpha=0}^1 \\langle \\frac{\\partial \\Omega^b(\\alpha)}{\\partial u_{pq}}, v_b^* \\rangle d\\alpha.$\n(4)\nHere, $v_b^*$ is the normalized version of $v_b^*$, $\\langle \\cdot, \\cdot \\rangle$ is the inner product operation and $w_i^b$ is the embedding of $U^b$ at the i-th position.\nNote that the above GIG measures the attribution of the query concept vector at position p to the target concept vector in a subsequent layer. To measure the attribution of a query concept vector, $v^a_q$, to the target concept vector, $v_b^*$, we sum up all the attributions of $v^a_q$ at different positions as follows:\nGIG($v_q^a \\rightarrow v_b^*) := \\sum_{p=1}^{H^aW^a} GIG(v^a_q|p \\rightarrow v_b^*) = \\sum_{p=1}^{H^aW^a} u^a_{pq} \\int_{\\alpha=0}^1 \\frac{\\partial}{\\partial u_{pq}} \\sum_{i=1}^{H^bW^b} \\langle w^b_i(\\alpha), v_b^* \\rangle d\\alpha.$\n(5)\nClass Concept Relevance To quantify the class importance score of a query concept vector in layer a, $v_a$, for the final output (contribution of the concept vector to the given class), we treat each class label c as an independent concept. Thus, we convert the class index into a one-hot vector, $e_c \\in \\{0, 1\\}^N$, where N is the number of classes:\n$(e_c)_i = \\begin{cases} 1 & \\text{if } i = c \\\\ 0 & \\text{if } i \\neq c. \\end{cases}$\n(6)"}, {"title": "EXPERIMENT", "content": "To demonstrate the effectiveness of our method, we provide two kinds of qualitative analysis includ- ing one-class explanation (Fig. 3) and two-class explanation (Fig. 1). Furthermore, we validate our method of concept extraction and concept attribution with comprehensive experiments in Sec. 4.2.\nSettings. Following Bricken et al. (2023), we selected the concept size of each layer as 8 times the number of channels in that layer, making overcomplete linear basis. For classic dictionary learning, we utilized the Least Angle Regression (LARS) algorithm and the Lasso LARS algorithm for PFV decomposition. For sparse autoencoder, we followed the setting of Templeton et al. (2024). For both methods, we extend them by decomposing PFVs directly into coefficients and concept vectors without relying on global average pooling, as they have been applied either at the token level within Transformer architecture, or on the global average pooled outputs of ResNet50 architecture."}, {"title": "QUALITATIVE ANALYSIS", "content": "As seen in Fig. 3, we can explain how the concept components are constructed through layers. Moreover, as shown in Fig. 1, we can even find out the shared concepts, since we analyze the models within the whole dataset, not a specific class."}, {"title": "VALIDATION OF OUR METHOD", "content": "Since our method involves two main steps, we validate the steps of our method with both qualita- tive and quantitative experiments: Sec. 4.2.1 for Concept Extraction, and Sec. 4.2.2 for Inter-layer Concept Attribution."}, {"title": "VALIDATION OF CONCEPT EXTRACTION", "content": "To validate our method, we assess its fidelity using the C-Deletion and C-Insertion metrics, as pro- posed by Fel et al. (2023a). These methods provide a robust framework for evaluating the alignment between our explanation model and the original model's behavior by systematically modifying con- cept activations and observing the resulting impact on model predictions.\nIn the C-Deletion and C-Insertion metrics, concept vectors are removed or inserted in the order of their importance, and the Area Under the Curve (AUC) of the accuracy drop graph is measured. The importance score of a concept is calculated with Eq. (7), as it is the most reliable CAT method (Fel et al., 2023a). For C-Deletion, a lower AUC indicates a more effective extraction method, as it signifies a greater impact on model performance when key concepts are removed. Conversely, in C-Insertion, a higher AUC is preferable, reflecting a more accurate prediction when important concepts are introduced. Finally, we measure the AUC difference to see the overall trends in every layer.\nResults As shown in the top part of Fig. 4, ours with Bisecting Clustering demonstrated consis- tently strong performance across most layers in both C-Deletion and C-Insertion. Considering the"}, {"title": "VALIDATION OF INTER-LAYER CONCEPT ATTRIBUTION", "content": "To validate the effectiveness of our concept attribution method, Generalized Integrated Gradient (GIG), we adapted the concept insertion and deletion strategies typically used in evaluating Concept ATtribution (CAT) methods. The original C-Insertion and C-Deletion metrics quantify the relation- ship between the identified concept vectors and the target class. By systematically inserting or delet- ing concept vectors according to their attribution scores and observing changes in the target class score, we assess the validity of the concept vectors of CAT methods.\nWe extended this metric to validate the relationship between concept vectors in different layers. As derived in Sec. 3.3, the class label can be seen as the one-hot concept vector of the last layer after the fully-connected layer. Therefore, we validated the efficacy of our inter-layer concept attribution method, observing the changes in the direction of the target concept vector in the subsequent layer by inserting or deleting concept vectors from a preceding layer. Specifically, we deleted or inserted concept vectors from the source layer one by one and observed the changes in the output of the target layer (with dimensions $H^b \\times W^b \\times C^b$). For instance, if we delete concept vectors related to the target concept \"dog nose\" from the source layer in order of their GIG attribution, the output in the target layer corresponding to the \"dog nose\" direction should decrease accordingly. However, given that the actual region of \"dog nose\" in the image may constitute only a small portion (e.g, less than 10%) of the total image, removing the most relevant concept from the source layer will likely affect only 1-2 PFVs in the target layer. Therefore, by deleting or inserting concepts in the source layer that most strongly contribute to the \"dog nose,\u201d and observing the change in the projection magnitude of the one PFV in the target layer that has the largest projection onto the \"dog nose\" direction, we can determine whether the attribution computed by GIG is valid.\nTo this end, we plotted the curve of the normalized maximum projection values of the PFVs in the target layer onto the target concept vector direction. Specifically, during the Insertion/Deletion processes, the maximum projection values at each step were normalized by the original maximum projection value prior to any Insertion or Deletion. We refer to this normalized value as the projection score, and this metric as Interlayer Insertion/Deletion."}, {"title": "CONCLUSION", "content": "In this paper, we firstly present a novel approach for extracting and attributing concepts within image models, enhancing interpretability through a comprehensive layer-wise analysis. Unlike ex- isting methods that often confine their explanations to specific classes, our approach provides a comprehensive understanding by analyzing shared concepts throughout the dataset. The shift from class-specific to dataset-wide explanations represents a significant advancement in the field of XAI in image models, allowing for a more holistic understanding of model behavior.\nWith the dataset of PFV and ERF, we propose a pipeline that systematically decompose PFVs into meaningful concept vectors, and further attribute these concepts across layers using the Generalized Integrated Gradient (GIG) method. With our method, we can reveal how concepts evolve and influ- ence decisions across different layers of the network. Through extensive qualitative and quantitative analyses, we demonstrate the effectiveness of our method in both accurately capturing and utilizing essential features.\nGiven its potential for broad applicability, we can extend our method to other deep learning architec- tures, such as Transformer models. Additionally, the implications of analyzing entire datasets rather than focusing solely on class-specific explanations could be more thoroughly investigated. We be- lieve that our approach opens a new avenues for interpretability in image models, broadening the perspective of XAI."}, {"title": "AUTHOR CONTRIBUTIONS", "content": "If you'd like to, you may include a section for author contributions as is done in many journals. This is optional and at the discretion of the authors."}]}