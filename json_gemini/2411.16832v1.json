{"title": "Edit Away and My Face Will not Stay:\nPersonal Biometric Defense against Malicious Generative Editing", "authors": ["Hanhui Wang", "Yihua Zhang", "Ruizheng Bai", "Yue Zhao", "Sijia Liu", "Zhengzhong Tu"], "abstract": "Recent advancements in diffusion models have made gener-\native image editing more accessible than ever. While these\ndevelopments allow users to generate creative edits with\nease, they also raise significant ethical concerns, partic-\nularly regarding malicious edits to human portraits that\nthreaten individuals' privacy and identity security. Existing\ngeneral-purpose image protection methods primarily focus\non generating adversarial perturbations to nullify edit effects.\nHowever, these approaches often exhibit instability to protect\nagainst diverse editing requests. In this work, we introduce\na novel perspective to personal human portrait protection\nagainst malicious editing. Unlike traditional methods aiming\nto prevent edits from taking effect, our method, FACELOCK,\noptimizes adversarial perturbations to ensure that original\nbiometric information\u2014such as facial features\u2014is either\ndestroyed or substantially altered post-editing, rendering the\nsubject in the edited output biometrically unrecognizable.\nOur approach innovatively integrates facial recognition and\nvisual perception factors into the perturbation optimization\nprocess, ensuring robust protection against a variety of edit-\ning attempts. Besides, we shed light on several critical issues\nwith commonly used evaluation metrics in image editing and\nreveal cheating methods by which they can be easily ma-\nnipulated, leading to deceptive assessments of protection.\nThrough extensive experiments, we demonstrate that FACE-\nLOCK significantly outperforms all baselines in defense per-\nformance against a wide range of malicious edits. Moreover,\nour method also exhibits strong robustness against purifi-", "sections": [{"title": "1. Introduction", "content": "Image editing has advanced at an unprecedented rate due to\nthe rise of diffusion-based techniques, making it possible to\nproduce edits that are indistinguishable from reality [3-14].\nThis rapid development has led to tools capable of seamlessly\nmodifying visual content, with edits so convincing that they\nare often impossible to differentiate from the original image.\nWhile this progress opens up creative possibilities, it also\nbrings significant ethical and societal challenges.\nThe power of these editing techniques has led to severe\nethical implications [15-17]. Recent incidents, such as the\nwidely discussed manipulation of Taylor Swift's images [18]\nand the proliferation of pornographic content affecting Ko-\nrean schools [19], underscore the urgent need to address the\nrisks associated with malicious image editing. These inci-\ndents have highlighted growing concerns about how personal\nimages, particularly those depicting individuals' faces, can\nbe misused once they are posted online [13, 14, 20]. Protect-\ning such images from unauthorized and malicious edits has\nthus become an important topic of research [21-23].\nTo address this challenge, several recent attempts [1,\n2, 24-28] have focused on using adversarial perturbations,\nwhich are imperceptible to human eyes but are intended to\nnegate the effects of editing when such images are used as\ninputs to diffusion-based editing algorithms. These perturba-\ntions aim to protect personal images by preventing the suc-\ncess of the intended edits (see Fig. 1 for an illustration). How-\never, current methods suffer from instability [1, 2, 27, 28]\nand simple purification methods. Specifically, while they\nare effective for certain types of editing instructions, they\nfail against others, largely due to the inherent diversity and\nversatility of editing prompts. The underlying issue is that\nas long as existing methods continue to focus on 'canceling\noff editing effects', the inconsistency of results is inevitable.\nThe diversity in editing prompts and the complexity of gener-\native diffusion models make it difficult for such approaches\nto generalize effectively.\nThe rationale behind current defense methods is to en-\nsure that the edited image does not meet the requirements\nof a successful image editing task. To understand this more\ndeeply, we first revisit what constitutes a successful image\nediting task: it should accurately reflect the editing instruc-\ntion while preserving the original, irrelevant visual features,\nsuch as those related to the subject's identity, including facial\nfeatures. The latter requirement, which has been largely over-\nlooked, provides an opportunity for a new defense strategy.\nInstead of attempting to cancel out edits, here, we ask:\n(Q) Can we design adversarial perturbations that\ncause edited images to lose their biometric informa-\ntion, making the edited image biometrically unrecog-\nnizable and thereby causing the edit to fail?"}, {"title": "2. Related Work", "content": "Generative editing models. Recent advances in latent diffu-\nsion models [29] have demonstrated superior image editing\ncapabilities through instructions and prompt editing [3-6].\nMost recent methods [7, 8] combine diffusion models with\nlarge language models for understanding text prompts. In-\nstructPix2Pix [9] leverages a fine-tuned version of GPT-3\nand images generated from SD and achieves on-the-fly im-\nage editing without further per-sample finetuning. On the\nother hand, many such models also allow personalized image\nediting [10, 11]. DreamBooth [12] learns a unique iden-\ntifier and class type of an object by finetuning a pretrained\ntext-to-image model with a few images. SwapAnything [14]\nand Photoswap [13] allow for personal content editing by\nswapping faces and objects between two images. In the gen-"}, {"title": "3. FACELOCK: Adversarial Perturbations for\nBiometrics Erasure", "content": "What defines a successful image editing task? Before\nintroducing our proposed method for safeguarding human\nportrait images from malicious edits, we revisit the criteria\nfor a successful image editing outcome. Specifically, we pro-\npose that a successful text-guided image editing hinges on\ntwo critical requirements: \u25cf prompt fidelity, and image\nintegrity. Prompt fidelity requires that the edit accurately\nreflects the instructions provided in the prompt. For instance,\nas shown in Fig. 2, a successful edit replaces the person's\nclothing with a police uniform as instructed by the prompt.\nMeanwhile, image integrity requires that other elements\nin the image remain intact after editing. Although this re-\nquirement is less explicit than prompt fidelity, it defines the\nessence of image editing and differentiates it from general\ntext-to-image generation tasks. As illustrated in Fig. 2, aside\nfrom the change in attire, the edited image should retain\nas much of the subject's original appearance as possible,\nincluding facial features, poses, and other details. While\nprompt fidelity has been emphasized and extensively stud-\nied [7, 9, 12, 29], image integrity remains long-overlooked\nand underexplored in literature. Next, we will demonstrate\nhow this holistic view of image editing can provide new in-\nsights into protecting human portraits from malicious edits.\nA new direction for defending against malicious editing.\nAs discussed above, to safeguard personal images from ma-\nlicious editing, the defender must ensure that at least one\nof the two requirements is not met. Previous works have"}, {"title": "4. Pitfalls in The Widely-Used Quantitative\nEvaluation Metrics for Image Editing Tasks", "content": "In this section, we begin by providing a critical analysis of\nexisting quantitative evaluation metrics for image editing\ntasks [45-47]. For the first time, we highlight potential\npitfalls in these widely accepted metrics, particularly how\nthey can be easily manipulated to achieve deceptively high\nscores. Finally, we introduce two new, more robust metrics\nfor evaluating human portrait editing. Detailed mathematical\ndescriptions of the quantitative evaluation metrics discussed\nin this section can be found in Appendix A.\nExisting quantitative metrics suffer from pitfalls and can\nbe manipulated for misleading performance. As discussed\nin \u00a73, the evaluation of general image editing tasks should\nconsider two aspects: prompt fidelity and image integrity.\nHowever, all existing quantitative metrics, including CLIP\nscores [46], SSIM, and PSNR primarily focus on the former,\nnamely how well the editing instruction is reflected in the\nedited image. In the following, we revisit each of these\nmetrics and demonstrate the intrinsic pitfalls in their design.\nCLIP-based scores overemphasize the presence of ele-\nments from the editing instructions, often prioritizing\nover-editing. CLIP-based scores are widely used to assess\nprompt fidelity by measuring the cosine similarity between\nthe CLIP text embedding of the editing prompt and the vi-\nsual embedding difference between the edited and source\nimages. While this metric effectively indicates whether the\nedit has taken effect, it tends to overemphasize the presence\nof specific elements in the edited image. Fig. 4 shows a\ncontradictory CLIP score ranking compared to the visual\nediting quality. Although Fig. 4(b) demonstrates a visually\nbalanced outcome between the editing effect 'turn the hair\npink' and preserving other irrelevant (especially facial) fea-\ntures, the CLIP-based score still assigns higher values to\nFig. 4(c) and (d) simply because they show stronger 'pink\nhair' effects, even if the subject's identity has been com-\npletely altered. Therefore, CLIP-based scores can easily\nprioritize over-editing and be manipulated by replicating\nelements from the editing instructions.\nSSIM and PSNR over-rely on differences between the\nedited image and the undefended source, potentially lead-\ning to a false sense of successful defense. Unlike CLIP-\nbased scores, metrics such as SSIM and PSNR evaluate\nwhether a defense against editing is successful by compar-\ning the pixel-level statistical differences between the edited\nimages with and without defense. While comparing against\nthe edited image without defense can be effective in some\nscenarios, concluding that a defense is successful simply\nbecause the defended image differs from the undefended one\nis premature. For example, in Fig. 5, Fig. 5(b) demonstrates"}, {"title": "5. Experiments", "content": "5.1. Experiment Setup\nModels and dataset. We adopt the widely accepted Instruct-\nPix2Pix [9] as our primary target model for prompt-based\nimage editing. In our experiments, we utilize a filtered subset\nof the CelebA-HQ dataset [49], a high-quality human face\nattribute dataset widely used in the facial analysis commu-\nnity. The dataset consists of 2,000 human portrait images\nspanning diverse race, age, and gender groups. For edit-\ning prompts, we manually selected 25 prompts across three\ncategories: facial feature modifications (e.g., hair, nose mod-\nification), accessory adjustments (e.g., clothing, eyewear),\nand background alterations.\nBaselines. We evaluate FACELOCK against two established\ntext-guided image editing protection methods: PhotoGuard\n[1] and EditShield [2], both designed for general image pro-\ntection. Additionally, we also compare against a variety of\nwidely used methods [50-56] in adversarial machine learn-\ning field, including untargeted encoder attack, CW attack,\nand VAE attack as other baseline methods. Full details on\nthese baselines are provided in Appx. A.\nEvaluation metrics. We adopt quantitative evaluation met-\nrics across two categories: prompt fidelity and image in-\ntegrity. For prompt fidelity, we report PSNR, SSIM, and\nLPIPS scores between edits on protected and unprotected\nimages, as well as the CLIP similarity score (CLIP-S), which\ncaptures the alignment between the edit-source image embed-\nding shift and the text embedding. For image integrity, we\nreport the CLIP image similarity score (CLIP-I) and facial\nrecognition similarity score (FR). CLIP-I captures overall\nvisual similarity, while FR specifically measures similarity\nin biometric information.\nImplementation details For a fair comparison, we set the\nperturbation budget to 0.02 and the number of iterations\nto 100 for all methods, except EditShield, which does not\nhave a default perturbation budget. Additionally, we include\nthe untargeted latent-wise loss from EditShield as a regu-\nlarization term to stabilize the protection results. Further\nexperimental details are provided in Appx. A."}, {"title": "5.2. Experiment Results", "content": "Superior performance of FACELOCK in human portrait\nimage protection: quantitative and qualitative evaluation.\nBuilding upon our analysis of comprehensive evaluation met-\nrics for image editing and protection, we present a quanti-\ntative evaluation of various protection methods in Tab. 1.\nOur proposed method, FACELOCK, demonstrates remark-\nable protection effectiveness across both prompt fidelity and\nimage integrity metrics. Regarding prompt fidelity, FACE-\nLOCK achieves competitive results in multiple metrics. It ties\nthe lowest SSIM score and maintains a competitive CLIP-S\nscore, and more notably, it excels in the LPIPS metric with\nthe highest score. This aligns with our discussion on the\nimportance of perceptual measures over pixel-based metrics.\nFor image integrity, FACELOCK outperforms all baselines\nsignificantly, especially in FR scores. This underscores its\nunparalleled efficacy in protecting the subject's biometric\ninformation against malicious editing. In Fig. 6, we present\nqualitative results of the three editing types. As we can see,\nOur approach demonstrates the most pronounced alteration\nof biometric details between the edited and source images."}, {"title": "6. Conclusion, Limitation, and Discussion", "content": "In this paper, we present FACELOCK, an innovative method\nto protect human portrait images from malicious editing by\noptimizing adversarial perturbations that prevent biometric\nrecognition post-editing. FACELOCK effectively disrupts\nidentifiable facial features, breaking the biometric link be-\ntween original and edited images. Experiments show its\nsuperior performance over existing defenses and robustness\nagainst purification techniques. While FACELOCK is tai-\nlored for single portraits, extending its efficacy to images\nwith multiple individuals remains a challenge. Additionally,\nemerging generative models like rectified flows [59, 60] may\nrequire further adaptations to sustain robustness. Address-\ning these challenges can enhance privacy protection at the\nforefront of generative models."}, {"title": "Broader Impact and Ethics Statement", "content": "Broader Impact Statement. Advancements in diffusion-\nbased image editing enable creative expression but also pose\nsignificant risks to privacy and identity security. Our work,\nFACELOCK, addresses these risks by providing a robust\ndefense mechanism that renders biometric information un-\nrecognizable after edits. By demonstrating the potential\npitfalls in current evaluation metrics, we aim to encourage\nthe development of more reliable and effective solutions in\nthis domain.\nEthics Statement. We believe our work sets a precedent for\nprivacy-preserving AI research, especially in image synthesis\nand editing. By showing that privacy protection can be\nachieved through targeting biometric integrity, we hope to\ninspire more robust and innovative approaches to privacy in\nthe broader context of Generative AI systems. This research\nalso contributes to the ongoing dialogue about responsible\nAI development and highlights the importance of addressing\nprivacy concerns as these technologies continue to advance."}, {"title": "A. Detailed Experiment Setups", "content": "A.1. Implementation Details of FACELOCK\nFACELOCK optimizes perturbation on facial disruption and feature embedding disparity that prevent biometric recognition\npost-editing. The pseudocode of FACELOCK is presented in Algorithm 1. More specifically, the facial recognition loss function\n$f_{FR}$ is defined as the negative of the similarity score between the input images computed by the CVLFACE model, and the\nfeature disparity loss function $f_{FE}$ is computed as the weighted sum of the layer-wise feature embedding distances across the\nfeature extractor network. As mentioned in Sec 5, we also include the untargeted latent-wise loss from EditShield[2] as a\nregularization term to stabilize the protection results. The hyper-parameters used in our implementation are summarized in\nTab A1.\nAlgorithm 1 FACELOCK\nInput: Input image x, VAE E, D in the diffusion model, step size a, number of steps N, overall perturbation budget \u03f5,\nregularization weight \u03bb, facial recognition loss function $f_{Fr}$, feature disparity loss function $f_{FE}$\n1: Initialize perturbation \u03b4 \u2190 N(0, I), and the protected image x' \u2190 x + \u03b4\n2: Compute the latent embedding of the input image z \u2190 E(x)\n3: for n = 1 to N do\nCompute the latent embedding of the protected image z' \u2190 E(x')\nCompute the decoded image from the latent embedding $x_d$ \u2190 D(z')\nCompute the facial recognition loss $l_{FR}$ \u2190 $f_{FR}$($x_d$, x)\nCompute the feature disparity loss $l_{FE}$ \u2190 $f_{FE}$($x_d$, x)\nCompute the latent loss (regularization term) l\u2081 \u2190 $||z' - z||_2$\nUpdate the perturbation \u03b4 \u2190 \u03b4 + \u03b1 \u00b7 sign(\u2207x'(lFr + lFE + \u03bb \u00b7 l\u2081))\n\u03b4 - clip(\u03b4, -\u03f5, \u03f5)\nUpdate the protected image: x' \u2190 x + \u03b4\nend for\nReturn: The protected image x'\nA.2. Implementation Details of Baselines\nIn addition to using previous methods [1, 2] as baselines, we also compare our FACELOCK approach against several widely\nused techniques in the adversarial machine learning field. These methods are summarized in Algorithms 2, 3, and 4. To ensure\na fair comparison, we use the same hyper-parameters settings in Tab A1.\nA.3. Image Editing Details\nModels. For image editing, we use the open-source instruction-guided diffusion model InstructPix2Pix [9] hosted on Hugging\nFace2 as our primary target model. We use the hyper-parameters presented in Tab A2. We use the same seed setting when\ncomparing edits on the unprotected images and the images protected by different methods to ensure that the edit images are\nare modified in the same way and that the different editing effects are due to the protection methods instead of random seeds.\nDataset. For the human portrait images used in our experiments, we utilize a filtered subset of the CelebA-HQ dataset\u00b3,\na high-quality human face attribute dataset widely used in the facial analysis community. The dataset consists of 2,000\nhuman portrait images ensuring diversity across various demographic groups, including race, age, and gender, to enhance"}]}