{"title": "Physics Augmented Tuple Transformer for Autism Severity Level Detection", "authors": ["Chinthaka Ranasingha", "Harshala Gammulle", "Tharindu Fernando", "Sridha Sridharan", "Clinton Fookes"], "abstract": "Early diagnosis of Autism Spectrum Disorder (ASD) is an effective and favorable step towards enhancing the health and well-being of children with ASD. Manual ASD diagnosis testing is labor-intensive, complex, and prone to human error due to several factors contaminating the results. This paper proposes a novel framework that exploits the laws of physics for ASD severity recognition. The proposed physics-informed neural network architecture encodes the behaviour of the subject extracted by observing a part of the skeleton-based motion trajectory in a higher dimensional latent space. Two decoders, namely physics-based and non-physics-based decoder, use this latent embedding and predict the future motion patterns. The physics branch leverages the laws of physics that apply to a skeleton sequence in the prediction process while the non-physics-based branch is optimised to minimise the difference between the predicted and actual motion of the subject. A classifier also leverages the same latent space embeddings to recognise the ASD severity. This dual generative objective explicitly forces the network to compare the actual behaviour of the subject with the general normal behaviour of children that are governed by the laws of physics, aiding the ASD recognition task. The proposed method attains state-of-the-art performance on multiple ASD diagnosis benchmarks. To illustrate the utility of the proposed framework beyond the task ASD diagnosis, we conduct a third experiment using a publicly available benchmark for the task of fall prediction and demonstrate the superiority of our model.", "sections": [{"title": "I. INTRODUCTION", "content": "Autism Spectrum Disorder (ASD), also known as autism, is a neurodevelopmental condition that poses underlying communication and behavioral challenges and commonly co-occurs with cognitive conditions of patients [1], [2]. Because of the significant variations of the symptoms and types between individuals, ASD is referred to as a spectrum disease. Learning, thinking, and problem-solving skills in people with ASD can range from exceptionally gifted to severely challenged. They may also behave, speak, and learn differently from most other people [3]. As such, early diagnosis of autism symptoms is vital and the Autism Diagnostic Observation Schedule (ADOS) is one of the widely used metrics to measure the severity of autism [4].\nEarly diagnosis of ASD is a feasible and useful step toward enhancing the health of those with autism and can help to reduce the stress on the families who are supporting them [5], [6]. Some autism diagnostic studies have examined the possible advantages of addressing the social and communication aspect with the help of therapeutic interventions while some others focused on certain medications for core symptoms [7]-[9]. One of the most widely researched and clinically supported therapeutic interventions for individuals with ASD is Applied behavior analysis (ABA) which aims to improve the behaviour of the by using systematic approaches [10].\nA specific ABA approach named discrete trial training (DTT) has been widely used in autism interventions in which a child is given a specific instruction or discriminative stimulus by a therapist [11]. If the child performs the expected behavior, they receive a reward. However, if the child does not respond appropriately, the therapist could use cues or demonstrations to change the behavior. This structured approach helps reinforce desired behaviors and provides guidance when necessary. In recent years, robots have been used to assist the child during DTT. During this alternative form of therapy known as Robot Enhanced Therapy (RET) [12], the robot actively assists the child through a game-like activity, and at the same time the therapist observes and guides the child by observing the interaction of the child with the robot. Information gathered during these sessions could be used to assess the ADOS level. However, manual analysis of ADOS is a challenging task that requires specialised knowledge and is prone to human error. As a solution, a number of machine learning-based methods [13]-[19] have been proposed to assess ADOS levels automatically.\nDespite the existence of numerous architectures for automated ADOS-level classification, these methods do not achieve the required levels of robustness or efficiency for practical application. Firstly, facial expression [20], [21], and eye gaze [22] features have been utilised in most of the prior works which make their applicability highly constrained due to privacy concerns and challenges with respect to collecting those data. Secondly, the limited number of skeleton-based methods [15], have complex architectures and require substantial computational power to execute these algorithms, which is not available in clinical settings. Moreover, existing algorithms [15] do not generalise well to diverse ADOS levels.\nTo address these limitations this paper proposes a novel physics-informed neural network architecture for skeleton-based ASD severity recognition by predicting ADOS scores.\nTo be more precise, our architecture analyses the laws of physics that relate to the skeleton sequences in conjunction with the tasks that the subject is expected to do, to distinguish between various ADOS levels. The resultant architecture (See Figure 1) is lightweight, interpretable, robust, and generalis-"}, {"title": "II. RELATED WORK", "content": "When considering the literature on automated ADOS level identification, eye-gaze patterns, and gestures have been the most commonly used factors for the analysis.\nUsing gaze behaviours it has been experimentally validated that there is a decrease of attention in children with ASD compared to TD children [26]. Motivated by this observation, Li et al. [27] proposed a computational framework that leverages gaze from a raw video to estimate autism levels. Liu et al. [28] used face-scanning patterns to identify children with ASD by adopting classification machine learning techniques. Similarly, Wang et al. [29] proposed to train a model to classify individuals with ASD based on the eye gaze patterns during image-viewing. Jiang and Zhao [30] extended the work in [29] by adopting eye-tracing and deep neural networks for ASD screening. However, in a clinical setting it is difficult to obtain eye gaze patterns as eye tracking requires specialised hardware.\nIn another line of work, gesture patterns have been analysed to discriminate between children with ASD and TD children. For instance, Anzulewicz et al. [31] has computationally assessed autism based on the recorded motor patterns during smart device gameplay and concluded that there is a wide difference between the hand gesture patterns of ASD and TD children. Another work that utilises motor function to discriminate between ASD and TD is the recurrent deep neural network-based approach proposed by Zunino et al. [19] in which they have analysed the grasping action in videos.\nOur work is inspired by the recent success of spatio-temporal behaviour analysis models that leverage the full body motion of the subject to identify motion cues that are indicative of ASD. Negin et al. [32] used the Bag-of-Visual-Words approach to develop several action recognition-based frameworks to identify ASD characteristics in children. Tian et al. [17] proposed an end-to-end deep architecture for video-based early ASD detection. They have utilized a temporal pyramid network to capture high-level semantics temporal feature maps at all scales and a discriminator to detect high-risk repetitive behaviours. Similarly in [18], state-of-the-art human action recognition architectures have been adapted for the task of recognising ASD. Marinoiu et al. [16] have used 3d human pose data for action and emotion recognition in children with ASD. Specifically, 2D and 3D pose features and interactions between the child and therapist have been leveraged as motion features. A hierarchical bi-directional recurrent neural network has been used to model the pose-related features.\nMore recently, [33] proposed a guided weak supervision architecture to address the lack of large-scale annotated datasets when performing automated video-based detection of ASD using supervised machine learning. The authors propose to match the similarities between the actions in publicly available large-scale video action datasets and the actions performed in datasets with children that have ASD symptoms. A multimodal approach for screening ASD subjects is proposed in [14] in which the authors have leveraged both behavioural patterns extracted from video and eye-gaze patterns. Two ResNet feature extractors are employed for extracting features from individual modalities and the temporal relationships across the extracted features are learned through modality-specific LSTM networks. In [15], the authors propose to use gesture and gait information for identifying autism-related behaviour. A Graph Convolution Neural Network (GCN) is used to embed the skeleton information in the latent space. To further augment the learning capabilities the authors propose to encode the skeleton as a skeleton picture element such that a vision transformer can be used to learn the relationships across skeleton joints.\nIn contrast to the existing studies on behaviour-based identification of ASD, our work leverages the laws of physics that could be applied to skeleton motion to identify anomalous A. Automated ADOS Prediction Approaches"}, {"title": "B. Transformer-based Skeleton Data Analysis", "content": "Transformers have revolutionised the field of deep learning and are being used as the backbone of many state-of-the-art applications ranging from image classification and recognition [34], [35], action recognition [36] to object detection [37]. The attention mechanism [38] leverages to extract salient information from sequences has enabled it to better capture context and handle long-range dependencies.\nThe success of transformers has seeped into the domain of action recognition as well. Video Action Transformer [39] and Recurrent Vision Transformer [40] are some notable works in the video-based action recognition domain. In the biomedical behaviour analysis literature, skeleton data is used widely due to the numerous unique advantages over videos such as compactness and privacy preservation. In the past few years, the GCN [41], [42] has been widely used as the backbone for extracting useful features from the input skeleton data. The skeleton data itself forms a graph structure, as such the graph neural networks are a natural choice for processing skeleton data. Furthermore, GCNs are highly effective in learning relationships across adjacent nodes enabling the learning of re-lationships among adjacent joints. Processing skeletal data for behavior modeling with transformers is a relatively new adaptation. Among the limited number of works on transformer-based skeleton data modelling, we would like to compare [43], [44], and [45] together with the proposed transformer architecture. Ibh et al. [43] proposed a transformer-based framework for fine-grained modelling of interactions among multiple players in team sports. In [44], Higher-order Transformer embeddings are used to model higher-order dynamics in the skeleton for action recognition. Moreover, in [45] Spatio-Temporal Tuples Transformer (STTFormer) model is proposed which proposes a spatiotemporal tuples self-attention module for efficient modelling of skeleton sequences. Specifically, the proposed attention scheme is capable of capturing the relationship of all joints across multiple consecutive frames at the same time, making it an efficient backbone for modelling skeleton data. Inspired by this we modify the tuple-based encoder architecture in the proposed framework. However, in contrast to [43]\u2013[45] we incorporate two decoder branches in the proposed method, one of which is driven by the physical laws to aid the recognition task. We further enhance the ability to learn informative features with the aid of proposed discriminative loss."}, {"title": "III. METHODOLOGY", "content": "This section first describes the overall framework of our proposed physics-augmented tuple transformer (PATT). Our proposed model consists of an encoder followed by two decoders. We introduce the architecture of our encoder in Sec. III-B, and the physics and non-physics-based decoders in Sec. III-C and III-D, respectively. We discuss the training loss functions in Sec. III-F.\nThe overall framework of the proposed Physics-Augmented Tuple Transformer (PATT) model is shown in Figure 2. Our architecture follows the encoder-decoder transformer architecture. The inputs for the model are a skeleton sequence, which is composed of human joints, and the corresponding action class.\nInspired by the sequence division step of the STTFormer [45] we adapt the encoder of STTFormer for encoding human skeleton information. This encoding step is computationally efficient and effective in capturing dependencies between different joints among different frames. However instead of directly using the encoding steps of STTFormer, we have simplified the encoding process by further reducing the complexity, which makes the encoding efficient even with a higher number of frames and reduces the overfitting of the model in situations where the data is scarce. Specifically, the final encoding layer of the STTFormer [45] encoding module has been removed and the expansion of the input channels was handled by the input mapping layer.\nOur encoder generates the normalized coordinates, the joint positions, and their corresponding forces leveraging the skeleton sequence and the action class as input. This encoded information is utilised by two decoder streams, namely the physics-based decoder and the non-physics-based decoder. The non-physics decoder generates a skeleton sequence of future frames, given only the generalized joint positions. On the other hand, our physics-based decoder predicts the future sequence by using both joint positions and forces. This input difference enforces the separation of the latent embedding into positions and forces where the non-physics branch is purely driven by the coordinates while the physics branch utilises complementary information regarding forces. The predictions generated by physics-based decoder is purely driven by physics-based theories regarding the skeleton structure and the forces. During the inference, both decoders are discarded and a Feed-Forward Neural Network performs autism severity level prediction using the positions and forces as the input. The following subsections illustrate the details of our encoder, decoders, and autism-severity-level classifier in detail.\nA crucial aspect of skeleton-based analysis tasks is capturing the inter-dependencies among joints and sequential-dependencies between frames. In addition, we need our encoder to learn the conditional relationship between the encoded sequential dependencies among frames and the action that the A. Overall framework\nB. Encoding the Skeleton Sequence and Action Information"}, {"title": "C. Physics-based decoder", "content": "The physics-based decoder utilise laws of physics to generate the future skeleton sequences and is mainly formed with two parts, a physics solver followed by a transformer decoder. Specifically, we aim to utilise the physics-based decoder to model the TD behaviour where it generates the future skeleton sequences by utilising the encoded joint positions, forces and physics-based theories (which is encoded in a physics solver). As such, the generated predictions follow the behaviour of TD subjects.\nIn classical mechanics, a set of parameters could be defined to fully describe the state of a physical system in terms of dynamics so that it guarantees the output representations of the decoder are plausible physical positions and forces. The functionality of the physics solver can be written as,\n$E(q_t, \\dot{q_t}, f, \\mu) = [q_{t+1}, \\dot{q_{t+1}}],$ (5)\nwhere $q_t, \\dot{q_t}, f, \\mu$ are the current position, velocity, control forces, and inertial properties, respectively. The inertial properties ($\\mu$), and velocity ($\\dot{q_t}$) are gained by fitting joint data to a general skeleton model.\nThen the predicted next position, $q_{t+1}$, is generated by integration using,\n$\\dot{q_{t+1}} = q_t + \\Delta t \\dot{q_t},$ (6)\nwhere the scrutinized time interval is denoted by $\\Delta t$.\nThe Lagrangian dynamic equation is solved in the generalized coordinates by the decoder to solve $\\dot{q_{t+1}}$.\n$M(q_t, \\mu)\\dot{q_{t+1}} = M(q_t, \\mu)\\dot{q_t} \u2013 \\Delta t (c(q_t, \\dot{q_t}, \\mu) -f) + J^T (q_t) \\tau,$(7)\nwhere $M, c, \\tau$ are the mass matrix, Coriolis and gravitational force, contact force in the generalized coordinate system respectively, with Jacobian matrix $J$. $\\tau$ is gained by solving the linear complementarity problem (LCP) [46]:\nfind $\\tau, v_{t+1}$ such that\n$\\tau > 0, v_{t+1} > 0, \\tau^T v_{t+1} = 0$ (8)\nThe velocity $v_{t+1}$ can be written as a linear function of $\\tau$:\n$v_{t+1} = J \\dot{q_{t+1}} = JM^{-1} (M\\dot{q_t} \u2013 \\Delta t (c \u2013 f) + J^T \\tau) = A \\tau + b,$ (9)\nwhere $A = JM^{-1}J^T$ and $b = J (\\dot{q_t} + \\Delta t M^{-1}(f - c))$.\nThen $A, b$ are mapped to the contact force f.\n$f_{LCP}(A(q_t, \\mu), b(q_t, \\dot{q_t}, f, \\mu)) = \\tau$ (10)\nThen the output from the physics solver, the generalized positions of joints of the sequence is converted again to Cartesian coordinates and re-scaled to the input scale by the proposed transformer decoder network, $f_{phy}$, where\n$q_{t+1} = f_{phy}(q_{t+1}),$ (11)\nand $f_{phy}$ is the physics-based decoder.\nThese predictions of the decoder are bound to be physically realistic future poses as we utilise a differentiable physics solver within it."}, {"title": "D. Non-Physics-based Decoder", "content": "The non-physics-based decoder is trained to generate the future skeleton sequence of the subject by only considering the encoder's positional output. Therefore, this branch does not consider the physical feasibility of the predicted future poses. The predictions are purely driven by the learned mapping between the current position, $q_t$, and the predicted future position, $q_{t+1}$, where the non-physics-based decoder network try to minimise the error between the predicted future poses of the subject and his or her actual poses. This could be written as,\n$q_{t+1} = f_{non-phy}(q_t),$ (12)\nwhere $f_{non-phy}$ is the on-physics-based decoder."}, {"title": "E. ADOS-classifier", "content": "Using the encoded generalized position and force sequences, the classifier aims to predict the ADOS score for each sequence. A feed-forward network with softmax output is adopted as the classifier. Specifically, the joint positions $q_t \\in R^D$ and forces $f_t \\in R^D$ at each frame are concatenated to form the feature vector $W_t \\in R^{2D}$ and is fed to the classifier. Formally, let the classifier be denoted as $f_{cls}$, then the ADOS prediction can be generated as,\n$Y_{ADOS} = f_{cls}(W_t).$ (13)"}, {"title": "F. Loss Functions", "content": "We utilise three losses to govern the training of the proposed framework.\nFor training the physics branch, we leverage the mean-square-error between the predicted future positions of ASD subjects and the actual future positions of TD subjects. Non-physics branch is trained using the mean-square-error loss between the predicted future positions of that branch and the actual future positions of the subject. These losses could be written as,\n$L_{phys} = \\sum_t MSE(q_{t+1}, q_{t+1}),$ (14)\nand\n$L_{Nonphys} = \\sum_t MSE(q_{t+1}, q_{t+1}),$ (15)\nwhere $q_{t+1}$ is the actual ground truth position of the subject in frame $t + 1$.\nA third loss is used to discriminate between the predictions from the physics-based branch and the non-physics-based branch. Specifically,\n$L_{phys-Non Phys} = \\sum_t MSE(q_{t+1}, \\dot{q_{t+1}}),$ (16)\ncalculates the total discrepancy between the physics-informed predictions and non-physics-based predictions. The motivation is to maximise this discrepancy such that the network will be able to better understand natural human behaviour and autism-related behaviour. Then the total loss is calculated by,\n$L_{total} = L_{ADOS} + L_{phys} + L_{Nonphys} + L_{phys-Non Phys},$ (17)\nwhere $L_{ADOS}$ is the binary classification loss of the ADOS classifier."}, {"title": "IV. EXPERIMENTS", "content": "Multimodal Dataset for Autism Intervention Analysis (MMASD) [23] is a dataset that includes 1315 video clips of 32 different children (27 males and 5 females) aged between 5 and 12 years, diagnosed with autism of different severity levels. It comprises more than 108 hours of video in total. The data set has been categorized into eleven activity classes depending on the conducted activity. The demographic information and the Auism Evaluation Scores (ADOS) of all participating children have been reported along with the date of birth, motor functioning score, and severity of autism. According to the documentation of the MMASD dataset the ADOS comparison scores rage from 5 to 10. Score < 5 falls to the TD category, 5-7 as moderate ASD level and 8-10 as severe ASD level. In this paper, we use the 2D skeleton sequence with the ADOS comparison scores ranging from 6-10 in the MMASD.\nDevelopment of Robot-Enhanced therapy for children with Autism spectrum disorders (DREAM) [24] is another dataset constituting behavioral data recorded from 61 children aged between 3 to 6 years, diagnosed with ASD. The developers have collected the samples of the subjects whose ADOS scores range from 7-20, performing three different tasks: imitation, joint attention, and turn-taking in a therapy environment where they interact with either a human therapist (SHT) or a robot (RET). With a median length of 32 minutes, each session ranged in length from 3 to 87 minutes.\nTo further illustrate the utility of the proposed framework in additional application domains we have conducted a third evaluation using the UR Fall Detection Dataset (URFD) dataset [25] for the fall prediction task. Details of this dataset and the results are provided in supplementary materials.\nWe used the 2D-Openpose data provided in the MMASD dataset [23] and the raw 3D skeleton data provided in the DREAM dataset [24] as the input to the proposed model for the respective datasets. Each skeleton sequence is meanly normalised and the frames without skeletons are replaced with the skeleton from the immediately preceding frame. Then all the frames within a data sample were re-scaled with respect to the first frame of that data sample. The scale of ADOS levels differs with the dataset, however, we utilise the ground truth ADOS score levels provided by the authors of the dataset and do not change these levels.\nThe MMASD dataset consists of three skeletons per frame, namely the skeleton of the child, the skeleton of the therapist, and the skeleton of the interaction partner. Out of the three skeletons, the skeleton sequences of the child and the interaction partner were leveraged in our work. Therefore, the input A. Datasets\nB. Implementation Details"}, {"title": "C. Evaluation Metrics", "content": "As evaluation metrics, we use precision, recall, accuracy, and F1-Score. Prior works [15], [19] have only used accuracy as the performance metric. While accuracy indicates how often the model is correct on average, this measure could be biased toward the majority class. In contrast, the precision metric indicates how well the model could avoid False Positives (FP) and detect True Positives (TP). In addition, recall indicates how many true positive cases the model has been able to detect. Avoiding False Negative (FN) and FP classifications is extremely important in biomedical applications, as such precision, recall, and F1-score are used as additional evaluation metrics. These metrics can be calculated as follows:\nPrecision = $\\frac{TP}{TP + FP},$ (18)\nRecall = $\\frac{TP}{TP+FN},$ (19)\nAccuracy = $\\frac{TP+TN}{TP+TN+FP + FN},$ (20)\nF1 Score = $2 \\frac{Precision . Recall}{Precision + Recall}$ (21)"}, {"title": "D. Results", "content": "In this section, we provide evaluation results of the proposed models together with the state-of-the-art methods in the MMASD and DREAM datasets. The evaluations of our model for the fall prediction task using the UR Fall Detection (URFD) dataset [25] are provided in supplementary materials.\nThe quantitative comparisons between the proposed model and existing state-of-the-art methods on the MMASD dataset are provided in Tab. II. Due to the unavailability of existing baseline models that have been evaluated using the MMASD dataset, we referred to the literature on behaviour-based autism disorder detection and evaluated these methods on the MMASD dataset for baseline comparisons. Recently, in [18] the authors propose the use of LSTM and MSTCN models for detecting the behavioural traits in videos that are indicative of autism disorder. We adapted these models to skeleton data by changing the input shapes of those models. In addition, to provide comparisons to the off-the-shelf transformer models that have been proposed for processing skeleton data we use STTFormer [45] baseline.\nWhen analysing the results in Tab. II we observe that the proposed method has been able to achieve significant performance gain compared to the baselines. Specifically, we observe more than 35% increase in accuracy compared to the LSTM-based model proposed in [18] and approximately 14% gain in accuracy compared to the existing state-of-the-art transformer for modelling skeleton sequences. Figure 5 compares the classification accuracy per ADOS score level on the MMASD dataset achieved by the proposed PATT model with the baseline LSTM model of [18]. We would like to note the superior recognition of the proposed model in all the ADOS score levels compared to the baseline.\nFor comparisons on the DREAM dataset we use the LSTM and MSTCN models of [18], STTFormer [45] model and the model recently proposed in [15] for autism detection. These evaluations are provided in Tab. III. When analysing the results we could clearly see that the proposed method has been able to achieve competitive results compared to all the baselines. Figure 6 compares the classification accuracy per ADOS scores"}, {"title": "V. ABLATION EXPERIMENTS", "content": "To gain insights into the relative contributions of the components of the proposed innovations including the novel encoder-decoder framework for ADOS score regression, the physics-based decoder, and the innovative loss function that helps discriminate between the predictions of the two decoders, we conduct a series of ablation experiments using the following models.\n1) PATT-Enc: In this ablation variant we remove the two decoder branches from our PATT framework. The encoded skeleton sequence is directly classified using the ADOS classifier.\n2) PATT-Phy Dec: We add a physics-based decoder to the PATT-Enc model.\n3) PATT-Non-Phy Dec: We add a non-physics-based decoder to the PATT-Enc model.\n4) PATT-w.o Dis Loss: This is the proposed PATT model without the proposed discriminative loss which discriminates the predictions between the physics and non-physics branches.\nAblation experiments are conducted using the MMASD dataset and the results are provided in Tab. IV.\nWhen comparing the results of PATT-Enc with the proposed PATT model's results we can clearly see the impact of the proposed encoder-decoder architecture. We believe a significant 10.45% gain in accuracy when the two decoder branches are added to the model. We believe this is because of the ability that the decoding branches provide to the overall framework to compare and contrast behaviour of children with and without autism disorder and identify distinguishable features which could support the classification.\nIn addition, using the PATT-Phy Dec ablation variant we demonstrate the utility of the proposed physics-based decoder branch. We would like to compare the performance gain that the PATT-Phy Dec model achieves compared to the PATT-Enc model with the simple addition of the physics-driven decoder. Similarly, a notable performance gain is achieved with the addition of a non-physics-based decoder (i.e. PATT-Non-Phy Dec branch). With these decoder branches, we force the encoded latent embedding to carry information regarding the subject's behaviour, which helps the identification of ADOS levels.\nMoreover, with the addition of the proposed discriminative loss which enables the two branches to compare and contrast their predictions we further improve our performance gain. This is clearly indicated by the performance drop we observe in the PATT-w.o Dis Loss when the discriminating loss is removed from our framework. This validates our hypothesis that there exist discriminative behaviour patterns between children with and without autism disorder and explicitly comparing them helps further augment the learned features of the encoder.\nTo further illustrate the discriminative power of the proposed framework we visualise the 3-dimensional representation of the embedding spaces extracted from the 2nd last layer of the physics and non-physics decoders of the proposed model for the MMASD [23] dataset. This visualisation is presented in Fig. 7. To generate a 3D illustration of the higher dimensional latent space we used Principal Component Analysis (PCA).\nWhen analysing the visualisation in Fig. 7 we observe that the physics branch has more extensive embedding space, which is expected as it captures the distinct motion patterns of TD subjects. In contrast, a more compact latent space is observed in the non-physics-based branch, which is directly optimised to match the behaviour of the subjects with an autism spectrum disorder. Therefore, these results clearly validate the discriminative learning capability of the proposed framework."}, {"title": "VI. CONCLUSION, LIMITATIONS, AND FUTURE WORK", "content": "In this paper, we have introduced a physics-informed novel deep learning framework for Autism Spectrum Disorder (ASD) severity recognition. We show how a discriminative architecture can be developed by incorporating laws of Conclusion."}]}