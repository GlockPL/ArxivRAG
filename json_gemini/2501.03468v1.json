{"title": "MTRAG: A Multi-Turn Conversational Benchmark for Evaluating Retrieval-Augmented Generation Systems", "authors": ["Yannis Katsis", "Sara Rosenthal", "Kshitij Fadnis", "Chulaka Gunasekara", "Young-Suk Lee", "Lucian Popa", "Vraj Shah", "Huaiyu Zhu", "Danish Contractor", "Marina Danilevsky"], "abstract": "Retrieval-augmented generation (RAG) has recently become a very popular task for Large Language Models (LLMs). Evaluating them on multi-turn RAG conversations, where the system is asked to generate a response to a question in the context of a preceding conversation is an important and often overlooked task with several additional challenges. We present MTRAG: an end-to-end human-generated multi-turn RAG benchmark that reflects several real-world properties across diverse dimensions for evaluating the full RAG pipeline. MTRAG contains 110 conversations averaging 7.7 turns each across four domains for a total of 842 tasks. We also explore automation paths via synthetic data and LLM-as-a-Judge evaluation. Our human and automatic evaluations show that even state-of-the-art LLM RAG systems struggle on MTRAG. We demonstrate the need for strong retrieval and generation systems that can handle later turns, unanswerable questions, non-standalone questions, and multiple domains. MTRAG is available at https://github.com/ibm/mt-rag-benchmark.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) play an important role as chat-based assistants (Lin et al., 2023). Relying on knowledge-sources during the conversation is an important task that helps improve answer reliability and trust, and hence Retrieval-augmented generation (RAG) has become an important and popular field in recent years (Lewis et al., 2020; Chen et al., 2024). The primary focus of RAG benchmarks has been on single turn (Friel et al., 2024; Niu et al., 2024; Yang et al., 2024) which LLMs have become proficient at (Kuo et al., 2024), however multi-turn RAG, where a turn is defined as a question-response pair, has been largely overlooked as a benchmark, and presents additional challenges not covered in single-turn RAG. A multi-turn conversation benchmark should sufficiently cover several challenging aspects to effect a holistic evaluation of the full RAG pipeline:\nRetrieval The relevant passages should change during the conversation causing repeated retrieval.\nGeneration The generator should struggle to answer many of the questions correctly, particularly questions that refer to and rely on previous turns.\nWe present MTRAG, a diverse and representative multi-turn RAG benchmark of human-generated conversations across 4 different domains that vary in style, topic and source. Our conversations comprise turns that vary along the dimensions of question type, multi-turn, and answerability.\nOur benchmark is constructed using a novel process where human annotators simulate a real-world conversation, by actually interacting with a live RAG agent via a custom chat application and improving the output in real time. Annotators took care to diversify their questions across the aforementioned different dimensions, including referencing earlier turns, while ensuring a flowing and natural conversation. At every turn, after issuing their questions, annotators checked the passages retrieved by the RAG system and modified the passage set to improve relevance and diversity. Next, they reviewed and repaired the generated response to improve its quality. Figure 1 shows part of a conversation from the benchmark to illustrate the output of our data creation process (described in detail in Section 4). The resulting conversations average 7-8 turns in length and 16.9 unique relevant passages per conversation.\nWe evaluate our MTRAG benchmark on the retrieval and generation components of RAG systems. We examine retrieval performance of lexical, sparse and dense retrieval under two settings (last turn and query rewrite) and analyze generative performance of 9 LLMs under three retrieval settings (reference, reference+RAG, and full RAG). Our rigorous human evaluation of model responses demonstrates that all models struggle on our tasks, especially on unanswerable questions, and in later turns.\nAs human data generation and evaluation does not scale well, we systematically explore automation paths. We identify several automated evaluation metrics that correlate with human scores (and many that do not), and demonstrate the need for more work in automatic evaluation. We also construct a companion benchmark, MTRAG-S, of synthetically generated conversations. By providing both human-generated and synthetic conversations over the same corpora, we aim to help the community analyze and understand the relative advantages of the two types of data."}, {"title": "2 Related Work", "content": "Question Answering (QA) and Information Retrieval (IR) have been popular tasks for many years. Prior focus has been on Extractive QA (Rajpurkar et al., 2016; Kwiatkowski et al., 2019), Long Form Question Answering (Fan et al., 2019; Stelmakh et al., 2022; Rosenthal et al., 2024), and Open Domain QA (Lee et al., 2019). There has also been work on multi-turn conversations, primarily through datasets like MT-Bench (Zheng et al., 2023), which focuses on tasks without retrieval, and Wizard of Wikipedia (Dinan et al., 2019), which is conversational rather than information seeking. Several surveys exist which summarize prior approaches and datasets available to the community (Bai and Wang, 2021; Cambazoglu et al., 2021; Wang, 2022; Rogers et al., 2023).\nWe compare our benchmark to existing multi-turn RAG datasets in Table 1. Except for FaithDial (Dziri et al., 2022), prior work keeps the retrieval component fixed. Retrieval is either performed once at the beginning, restricting the entire conversation to the initial passages; or retrieval is only deployed to find evidence for an existing conversation, which can result in strange mismatches between answers and verifiability. In contrast, we perform active retrieval (Jiang et al., 2023), where ongoing passage retrieval influences both follow-up questions and provided answers, more closely reflecting real life scenarios. Excluding RAD-Bench (Kuo et al., 2024) and iKAT (Aliannejadi et al., 2024), most prior datasets focus on extractive or short answers (1-2 sentences) limiting the kind of questions that can be asked. Further, many existing datasets ignore unanswerable questions - a ripe source of hallucinations in LLMs (Song et al., 2024). Finally, most datasets focus on a single domain/topic, while we explore several domains of different types. Our MTRAG benchmark reflects real-world properties of multi-turn conversations, including active retrieval, long-form answers, unanswerable questions, and multiple domains."}, {"title": "3 MTRAG Benchmark", "content": "We now describe MTRAG's characteristics, before delving into how it was created in Section 4."}, {"title": "3.1 Dimensions", "content": "To ensure that MTRAG is representative of real RAG use cases, we designed it to be diverse across several important dimensions. For the detailed definitions see Appendix A.\nQuestion types: Similar to contemporaneous work on datasets for RAG (Yang et al., 2024; Lee et al., 2024) conversations in MTRAG contain questions of diverse types, including factoid, comparison, explanation, keyword questions, and others. Each question has one or more question type labels.\nMulti-turn: In addition to question types that apply to individual questions regardless of the surrounding conversation, there are different types of multi-turn questions. A question can be a follow-up or clarification. All questions in MTRAG beyond the first turn are labeled with one multi-turn type.\nAnswerability: Models often struggle when confronted with questions or problems that cannot be answered (Rajpurkar et al., 2018; Sun et al., 2024). Based on this observation, we include answerable, partially answerable, unanswerable based on the corpora, and conversational statements (e.g., \u201cHi\", \"That's interesting\", \"Thank you\").\nDomain: To test RAG systems over different types of documents, MTRAG is created over four domains and corresponding document corpora (discussed in more detail in Section 4.2)."}, {"title": "3.2 Conversation Properties", "content": "To ensure that MTRAG is a challenging benchmark, the following properties were incorporated during conversation creation.\nHigh Quality Responses: Following and extending prior work (Rosenthal et al., 2024; Es et al., 2024; Aliannejadi et al., 2024), each reference answer is written to satisfy the following properties: (a) Faithfulness: The answer is faithful to the passages or earlier turns, (b) Appropriateness: It is appropriate/relevant to the question, (c) Naturalness: The answer sounds natural, and (d) Completeness: Includes all information in the passages relevant to the question. We refer to these properties as FANC.\nPassage Diversity: Conversations contain questions that are diverse enough that the relevant passages do not remain static through the conversation. Our conversations have on average 16.9 unique relevant passages and 20.9 relevant passages in total.\nAnswer Repair: During conversation creation we employ an LLM to create the initial response, which is then repaired by the annotator as needed. A response that does not require repair can be considered an indication that the question is not challenging for the LLM. Our conversations contain repairs on 92% of the turns.\nNon-standalone: The questions occasionally rely on prior turns in the conversation (e.g., by using co-references to prior questions or answers or by clarifying previous turns). On average 1.3 questions per conversation include co-references."}, {"title": "4 Benchmark Creation", "content": "We use MTRAG to evaluate both the retrieval and generation components of RAG systems. Retrieval evaluation is described in this section, while generator evaluation is presented in the next section. Note, that this is not intended to be a comprehensive evaluation of all retrievers and generators. Instead our goal is to demonstrate the challenging nature of MTRAG and how it can help evaluate and surface issues in state-of-the-art RAG systems."}, {"title": "4.1 Annotators", "content": "The annotators that contributed to this work are highly skilled individuals hired solely to perform language annotation tasks and paid well above minimum wage. Unless otherwise noted, the annotators were used for all annotation tasks in this paper. Great care was taken to ensure random assignment."}, {"title": "4.2 Document Corpora", "content": "The first step in creating the benchmark was to assemble the document corpora over which the conversations would be built. MTRAG consists of four document corpora/domains:\nCLAPNQ (Rosenthal et al., 2024): a subset of Wikipedia pages,\nFiQA (Maia et al., 2018): a set of StackExchange posts discussing financial advice,\nGovt: the crawled contents of select web-pages under the .gov and .mil domains, and\nCloud: the crawled contents of select technical documentation pages of a major cloud provider.\nCLAPNQ and FiQA are existing corpora from QA/IR datasets, while Govt and Cloud are new corpora assembled specifically for this benchmark. To ensure that the new corpora are well suited for generating diverse conversations (i.e., conversations that touch several different passages), both the Govt and Cloud corpora were designed to contain sets of inter-connected pages (see Appendix B.1).\nEach corpus was indexed using Elser from ElasticSearch.\u00b9 During ingestion, documents were split into passages of 512 tokens with an overlap stride of 100 tokens. The indexes are used to perform retrieval during conversation creation and all RAG experiments. Finally, for each corpus we also assembled a set of seed questions to help human annotators bootstrap the conversation generation process, described in Section 4.3. For CLAPNQ and FiQA we leveraged questions from the corresponding QA datasets, while for Govt and Cloud we selected a set of seed documents from each corpus and asked annotators to write seed questions based on them."}, {"title": "4.3 Human-Generated Conversations", "content": "Annotators were asked to create multi-turn conversations over the four corpora. To aid in this process, we developed a custom chat application allowing them to interact with a live RAG agent consisting of an ELSERv1 (ElasticSearch 8.10) retriever and Mixtral 8X7b Instruct (Jiang et al., 2024) generator and correcting the retriever and generator outputs as needed. Using the application, annotators created multi-turn conversations by performing the following actions at every turn: write a question, adjust the set of retrieved passages, edit agent response and enrich with question dimensions. Annotators were directed to write 6-9 (or more) turns per conversation.\n(i) Training: Prior to the main creation task we provided annotators with interactive training sessions, extensive documentation with examples, and pilot tasks with feedback. Once they were comfortable with the task and producing conversations that met our criteria, we began the full annotation task.\n(ii) Creation: Annotators started a conversation using a seed question. For subsequent turns, they were encouraged to create questions that naturally extended the preceding conversation while varying in answerability types, question types, and multi-turn patterns (Section 3.1), which were noted during annotation time. Once annotators wrote a question, the chat application queried the retriever for potentially relevant passages, ran the generator to produce an agent response based on the retrieved passages, and presented everything to the user for repair. Annotators were guided to ensure passage diversity and answer quality (Section 3.2) using the application's custom passage search and edit agent response functionalities. In the final benchmark, the similarity between the original and repaired response based on Rouge-L (denoted as the edit score) is 60.7 indicating significant amount of repair. This novel process simulates real-time conversations, which is missing in prior work (Section 2).\n(iii) Review: The resulting 126 conversations were then reviewed. Annotators could accept or reject conversations, and repair responses, passage relevance, and dimensions as needed. They were not allowed to edit the questions or passages as such changes could negatively affect the conversation flow. During this phase, most conversations were kept (see review process details in Appendix B.3). Our human evaluation (Section 7) shows that the reference responses are preferred by humans."}, {"title": "4.4 Data Statistics", "content": "This process yielded a benchmark of 110 conversations (29 ClapNQ, 27 FiQA, 28 Govt, 26 Cloud) with an average 7.7 turns per conversation, leading to 842 tasks. A task is a conversation turn containing all previous turns together with the last user question (e.g., the task created for turn k includes all user and agent questions/responses for the first k - 1 turns plus the user question for turn k). All evaluations described next are performed at the task level. Figure 2 shows the distribution of tasks in MTRAG on the benchmark dimensions."}, {"title": "5 Retrieval", "content": "https://www.elastic.co/search-labs/blog/elasticsearch-elser-relevance-mteb-comparison\nWe use MTRAG to evaluate both the retrieval and generation components of RAG systems. Retrieval evaluation is described in this section, while generator evaluation is presented in the next section. Note, that this is not intended to be a comprehensive evaluation of all retrievers and generators. Instead our goal is to demonstrate the challenging nature of MTRAG and how it can help evaluate and surface issues in state-of-the-art RAG systems."}, {"title": "5.1 Experimental Setup", "content": "We use MTRAG with the indexed corpora from the creation stage (Section 4.2) to evaluate lexical (BM25), dense (BGE-base 1.5 (Xiao et al., 2023)) and sparse (Elser) retrieval. The competitiveness of these models is shown on the MTEB leaderboard.\u00b2\nWe report the commonly used Recall and nDCG metrics (Thakur et al., 2021) @1, 3, 5, 10. Since we use Elser for retrieval during data creation, there may be some biases towards Elser.\nThe retrieval task is performed on the reference passages (i.e., those marked as relevant during the creation process). It is therefore only computed on answerable and partially answerable tasks where reference passages exist."}, {"title": "5.2 Retrieval strategies", "content": "We experimented with several strategies to query the retriever for relevant passages, including sending the full conversation up to the current user turn, all the user turns without the responses, subsets of the conversation, and only the last user turn. Using the full conversation, or even just a few turns from it consistently under-performed, often causing the retriever to bring back the same passages over and over. The most effective strategy was just using the last user turn. A particular challenge in multi-turn conversations is that a user turn may be non-standalone, employing shortcuts to express intent, or referencing entities or concepts from earlier in the conversation. To alleviate this we used a query rewrite strategy, also known as contextual query rewriting (Zhou et al., 2023; Sun et al., 2023) to rewrite the user turn using an LLM so that it incorporates all necessary parts from the context into an unambiguous, standalone question (see Appendix C.1 for implementation details). An example of query rewrite is shown below:\nUser: Who is the CEO of Apple Inc.?\nAgent: The CEO of Apple Inc. is TIM COOK.\nUser: its address?\n[Rewriting] What is the address of Apple Inc?"}, {"title": "5.3 Retrieval Results", "content": "We highlight the results in Tables 3 and 4. Table 3 shows that the query rewriting strategy consistently outperforms using only the last turn (without rewriting), across all metrics, for all models. Elser outperforms BM25 and BGE-base 1.5. Table 4 shows the Elser results with query rewriting broken down by domain, first turn vs later turn, and whether the question is standalone. Retrieval performance is significantly lower for later turns than for the first turn, and non-standalone questions continue to pose a challenge (though query rewriting helps). These results highlight two key areas of improvement for retrieval components: 1) multi-turn retrieval, and 2) non-standalone questions."}, {"title": "6 Generation", "content": "We next present the generator experiments. We start with the experimental setup, followed by the results, using automated metrics including LLM judges. Section 7 will complement this with a human evaluation on a subset of MTRAG. Given a task, we send to the model the following information: the question, preceding turns, N passages, and instructions. We choose N=5 passages because it achieves considerable improvement compared to top 3, while remaining a manageable amount of passages (Section 5). For more generation format details, see Appendix D.2."}, {"title": "6.1 Retrieval Settings", "content": "We evaluate how LLMs perform under three retrieval settings, simulating ideal/noisy retrieval.\nReference (\u2022): Generation using reference passages or no passages if unanswerable/conversational. No retrieval is performed in this setting; it simulates a perfect retriever.\nReference + RAG (o): Partial retrieval followed by generation, where the reference passages are supplemented by the top retrieved passages (using Elser with rewrite) to yield a total of 5 passages. We restrict this to the 426 tasks that have at most two reference passages to ensure all passages needed for the reference are included. This can be considered an upper bound, where the retrieval is successful but there are additional noisy passages.\nFull RAG (0): Retrieval using Elser with rewrite followed by generation, where the top N=5 passages are retrieved (the standard RAG setting)."}, {"title": "6.2 Models", "content": "We evaluate the following auto-regressive models.\nLlama 3.1 Models (Dubey et al., 2024): The Llama 3.1 family of models are instruction-tuned models that support up to 128K tokens. We evaluate the 8B, 70B and 405B models.\nMixtral Mixture-of-Expert Models (Jiang et al., 2024): The instruction fine-tuned Mixtral 8x22B model that supports up to 32K tokens.\nGPT-40 Models: We use the GPT-4o and GPT-4o-mini model in our experiments. These support context lengths of up to 128K tokens.\nCommand R+5: This is a 104B parameter multilingual model optimized for RAG and tool use."}, {"title": "6.3 Metrics", "content": "We use three metrics, described below, to evaluate the quality of RAG systems and understand if a model response exhibits the desirable FANC properties outlined in Section 3.2. We use these metrics to provide insights into the trends of LLMs on multi-turn RAG rather than declare winners. See Section 8 for a discussion on how we picked the metrics and the challenges associated with finding good evaluation metrics for multi-turn RAG. Moreover, for implementation details, see Appendix F."}, {"title": "6.3.1 Aggregate performance metrics", "content": "The first two metrics, $R_{Balg}$ and $R_{B1lm}$, are both reference-based (RB) metrics. They both measure the aggregate performance of model responses by comparing the model response (MR) to the reference answer (RA) utilizing different techniques:\n$R_{Balg}$ is the harmonic mean of three algorithmic metrics (Adlakha et al., 2024): Bert-Recall (Bert-Rec), Bert-K-Precision (Bert-K-Prec), and Rouge-L. The intuition is as follows: Bert-Rec is an approximation for completeness as it measures the semantic overlap between MR and RA, tending to prefer longer answers. Bert-K-Prec compares MR to the passages, $P$, and is an approximation for faithfulness and completeness. Rouge-L measures whether phrases from RA are in MR and is an approximation for appropriateness.\n$R_{B1lm}$ is an LLM judge inspired by RAD-Bench (Kuo et al., 2024). We adapt RAD-Bench's approach of comparing MR to RA but modify the prompt to add P and anchor the evaluation on the metrics of faithfulness, appropriateness, and completeness. To minimize model biases and improve evaluation reliability, we use several models as judges and use the median as the final score."}, {"title": "6.3.2 Faithfulness metric", "content": "We also use a metric specifically for faithfulness, which is important for RAG applications and a challenge for LLMs (see Section 7). $R_{LF}$, the Faithfulness LLM judge from RAGAS (Es et al., 2024), appears to be a good judge for faithfulness (see Section 8). In contrast to the other two metrics, this is a reference-less (RL) metric, as it does not rely on the reference answer."}, {"title": "6.3.3 Conditioning metrics on answerability", "content": "Prior to computing the metrics we employ an IDK (\"I Don't Know\") judge to detect whether the response has a full or partial answer. It is important to first determine whether a response is IDK because intuitively, words used to indicate not knowing the answer may not match the context; this is also reflected in the metrics, which were not designed to measure IDK correctly. Our IDK judge achieves an accuracy of over 97% (see Appendix F.3). We condition the metric score, $\\Phi \\in {R_{Balg}, R_{B1lm}, R_{LF}}$, on answerability value A and IDK value IDK as follows:\n| IDK = no, partial | IDK = yes |\n|--------------|-----------|\n| $\\Phi$           | 0         |\n| 0              | 1         |\nWe define answerability accuracy (Ans. Acc.) of a model as the accuracy of IDK correctly pre-"}, {"title": "6.4 Generation: Evaluation Results", "content": "The overall results across the different retrieval settings are shown in Table 5.6 Within model families, the larger model does as well or better than its smaller counterparts across metrics. In general, GPT-40 and Llama 3.1 405B Instruct perform the best across metrics and settings. All models score significantly lower than the reference answer, indicating there is still room for improvement in MTRAG for all LLMs. Comparing performance across retrieval settings, we see that the results degrade as the setting gets more challenging: \u2022 > o > 0, indicating the noise has an impact on generation. Interestingly, Qwen 2.5 72B and Command-R+ are more competitive and closer to GPT-40 and Llama models in noisy settings.\nFocusing next on the Reference (\u2022) retrieval setting, we further explore how LLMs perform on the different dimensions of MTRAG. Figure 3 shows the results. For space reasons we only report the $R_{Balg}$ metric. We also leave out the breakdown by question types and multi-turn types, where we did not find interesting patterns. The additional metrics and breakdowns can be found in Appendix I.\nAnswerable vs unanswerable questions. As shown in Figure 3a, model performance generally drops when questions become partially answerable or unanswerable. In particular, models experience a dramatic drop in performance on unanswerables, struggling to declare they do not know the answer when the answer is not included in the input passages. Interestingly, performance on unanswerables differs widely between model families. While GPT-40 and Llama 405B score low for unanswerables, they still perform much better than models in other families. It is interesting to note that Llama 70B and 8B perform better on the unanswerables, because they say \"I don't know\" too often. This can also be seen by their low answerability accuracy in Table 5.\nFirst turn vs subsequent turns. As shown in Figure 3b, in almost all cases the models perform better on first turn vs subsequent turn questions. This proves our conjecture that answering questions in a multi-turn setting is more challenging, as a model has to interpret a question in the context of the preceding conversation, which was one of the main motivations for this work.\nDomains. Figure 3c shows that model performance is similar across domains except for FiQA where the results tend to be lower. We suspect this is due to the nature of the corpus; it contains posts from a financial discussion forum, which are typically short, very informal in style, and often subjective."}, {"title": "7 Human Evaluation", "content": "We also performed a human evaluation on a subset of the benchmark. This serves several purposes: 1) Verifying that the reference responses are of high quality, 2) correlating our metrics with human judgment, and 3) analyzing frontier models."}, {"title": "8 Automatic Evaluation", "content": "Human evaluation is not feasible as a long term solution for evaluating models as it does not scale easily. We explore reference-less and reference-based automated evaluation via algorithmic metrics and LLM judges. Table 7 shows the correlation of these metrics with the human evaluation's win-rate and Figure 4 shows the correlation of metrics that correlate positively with win-rate for the individual properties of FANC. We employed a weighted correlation on human judgements to correct for data imbalance. We exclude the reference responses from Figure 4 because by design the ranking of the reference is 1 for the Reference-Based metrics.\nThese findings drive our decision to report $R_{Balg}$, $R_{B1lm}$, and RLF as our main metrics in Section 6.\nReference-Based Metrics. We consider an adapted version of RAD-Bench (Kuo et al., 2024), denoted as $R_{B1lm}$, as well as the harmonic mean of algorithmic metrics, denoted as $R_{Balg}$, as described in Section 6.3. As shown in Table 7, these metrics correlate reasonably well with win-rate; in particular $R_{B1lm}$. Figure 4 shows that they also tend to correlate well with our desired properties; in particular $R_{Balg}$.\nReference-Less Metrics. We investigate popular metrics in literature: the Faithfulness (RLF) and Answer Relevance (RLR) metrics from RAGAS (Es et al., 2024), an adapted version of MT-Bench"}, {"title": "9 Synthetic Conversations", "content": "Manually creating data is an expensive and time-consuming process that does not scale well. Automating this process has become popular via synthetic data generation (Soudani et al., 2024) and can serve as useful evaluation. To explore this direction, we construct a companion benchmark, MTRAG-S of synthetically-generated conversations. We extend the recently proposed framework of (Lee et al., 2024) to automatically generate multi-turn conversations (see Appendix G.1 for details). To ensure that the two benchmarks are comparable, we utilize the same corpora, question types, multi-turn patterns, and answerability types (Section 3.1).\nIn Table 8, we see that synthetic conversations are typically shorter (averaging 5.9 vs 7.7 turns) and exhibit a lower passage diversity (4.6 vs 16.9 unique passages per conversation). Question and response lengths also differ with synthetic data having longer (potentially more detailed) questions but shorter answers. We also found that mimicking several important characteristics posed challenges: our attempts to synthetically generate unanswerable questions were not very successful as the model would often create questions with at least a partial answer. Moreover, increasing the number of turns tended to lead to repetitive user questions and a higher likelihood of hallucinated agent responses.\nSince the aggregate metrics ($R_{Balg}$ and $R_{B1lm}$) rely on a reference answer, which does not exist for synthetic data, we employ the reference-less RLF and Bert-K-Prec to evaluate faithfulness on MTRAG-S. Using either metric, we see that models across the board receive a higher faithfulness score on the synthetic than on the human-generated data (see Appendix G.2). There are multiple potential explanations for this, ranging from potential idiosyncrasies of synthetic data generation approaches, to the reliability of the presence of desired characteristics, to the quality and biases of automatic evaluation metrics. More work is needed to compare human-generated and synthetic data and we hope that our companion synthetic benchmark serves as a valuable asset towards that goal."}, {"title": "10 Conclusions and Future Work", "content": "We present MTRAG, a comprehensive and diverse benchmark of 110 multi-turn human-generated conversations averaging 7.7 turns for a total of 842 tasks. 8 These tasks are used to test the full RAG pipeline. MTRAG is the first end-to-end human-generated multi-turn RAG benchmark that reflects real-world properties of multi-turn conversations.\nOur experimental results, employing both automated metrics and a human evaluation, highlight the quality of our benchmark and outline several trends and challenges related to multi-turn RAG systems that state-of-the-art retrievers and LLMs face during retrieval and generation. Our findings encourage future research on improving retrieval and generation performance, especially in longer multi-turn conversations, unanswerable questions, and non-standalone user questions. In addition, the ability to scale indicates a clear need for i) more accurate reference-less automatic evaluation metrics, which align more closely with human judgement and can better differentiate model performance; and ii) synthetic data to obtain more conversations. We are also motivated to extend MTRAG in the future to include adversarial turns, additional domains and multilingual conversations."}, {"title": "A Dimension definitions", "content": "This section provides a detailed list of the MTRAG's dimensions, introduced in Section 3.1. In particular, Tables 9, 10, and 11 show the definitions of the question types, multi-turn types, and answerability types included in the benchmark."}, {"title": "B Benchmark creation details", "content": "The first step in creating the benchmark was to assemble the document corpora over which the conversations would be built. MTRAG consists of four document corpora/domains:\nCLAPNQ (Rosenthal et al., 2024): a subset of Wikipedia pages,\nFIQA (Maia et al., 2018): a set of StackExchange posts discussing financial advice,\nGovt: the crawled contents of select web-pages under the .gov and .mil domains, and\nCloud: the crawled contents of select technical documentation pages of a major cloud provider.\nCLAPNQ and FiQA are existing corpora from QA/IR datasets, while Govt and Cloud are new corpora assembled specifically for this benchmark. To ensure that the new corpora are well suited for generating diverse conversations (i.e., conversations that touch several different passages), both the Govt and Cloud corpora were designed to contain sets of inter-connected pages (see Appendix B.1).\nEach corpus was indexed using Elser from ElasticSearch.\u00b9 During ingestion, documents were split into passages of 512 tokens with an overlap stride of 100 tokens. The indexes are used to perform retrieval during conversation creation and all RAG experiments. Finally, for each corpus we also assembled a set of seed questions to help human annotators bootstrap the conversation generation process, described in Section 4.3. For CLAPNQ and FiQA we leveraged questions from the corresponding QA datasets, while for Govt and Cloud we selected a set of seed documents from each corpus and asked annotators to write seed questions based on them."}, {"title": "C Retrieval experiment details", "content": "We implemented\nquery rewriting by sending the prompt of Figure 5\nto Mixtral 8x7B Instruct. This is only a reference\nimplementation, and better ones could be obtained\nwith larger models (e.g., Mixtral 8x22B Instruct,\nLlama 3.1 405B, etc.), few-shot prompts with in-\ncontext-learning examples, or via fine-tuning. How-\never, this implementation is sufficient to make the\npoint in Section 5 that query rewriting is both nec-\nessary and effective in mitigating the challenges of\nnon-standalone questions in multi-turn data."}, {"title": "D Generation experiment details", "content": "To pick the models used in the generation experi-\nments, we selected a suite of state-of-the-art mod-\nels of varying sizes. It is not an exhaustive list, but\nshows the common trends in frontier models. For\nOpenAI models, we also experimented with the\nlatest models in the o1 9 family. However, initial\nexperiments showed that o1 did not perform as well\nas GPT-40. We suspect that this is due to ol's focus\non reasoning and multimodal rather than on RAG."}, {"title": "E Annotator agreement", "content": "We next describe the computation of the annotator agreement reported in the human evaluation (Section 7). For each property, the annotator agreement was computed as the percentage of tasks on which the 3 annotators had either an absolute agreement (i.e., they all agreed on the score) or high agreement (i.e., two of the annotators agreed on the score and the third annotator gave a score that was a single point away from the score given by the majority)."}, {"title": "F Metrics details", "content": "To adapt the RAD-Bench judge (Kuo et al.", "modifications": 1}]}