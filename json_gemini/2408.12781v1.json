{"title": "The Model Mastery Lifecycle: A Framework for Designing Human-Al Interaction", "authors": ["Mark Chignell", "Mu-Huan Chung", "Khilan Jerath", "Jaturong Kongmanee", "Abhay Raman"], "abstract": "The utilization of Al in an increasing number of fields is the lat-\nest iteration of a long process, where machines and systems have\nbeen replacing humans, or changing the roles that they play, in\nvarious tasks. Although humans are often resistant to technologi-\ncal innovation, especially in workplaces, there is a general trend\ntowards increasing automation, and more recently, AI. AI is now\ncapable of carrying out, or assisting with, many tasks that used to\nbe regarded as exclusively requiring human expertise. In this paper\nwe consider the case of tasks that could be performed either by\nhuman experts or by AI and locate them on a continuum running\nfrom exclusively human task performance at one end to Al auton-\nomy on the other, with a variety of forms of human-AI interaction\nbetween those extremes. Implementation of AI is constrained by\nthe context of the systems and workflows that it will be embedded\nwithin. There is an urgent need for methods to determine how AI\nshould be used in different situations and to develop appropriate\nmethods of human-AI interaction so that humans and AI can work\ntogether effectively to perform tasks. In response to the evolving\nlandscape of AI progress and increasing mastery, we introduce\nan AI Mastery Lifecycle framework and discuss its implications\nfor human-Al interaction. The framework provides guidance on\nhuman-Al task allocation and how human-AI interfaces need to\nadapt to improvements in AI task performance over time. Within\nthe framework we identify a zone of uncertainty where the issues\nof human-AI task allocation and user interface design are likely to\nbe most challenging.", "sections": [{"title": "1 INTRODUCTION", "content": "We live in an age of model-based prediction and classification. A\nchess playing model, for instance, seeks to predict what the best\nmove should be in each situation encountered during a chess game\n[4]. In recent decades models have begun to outperform humans\nin chess as well as many other games and tasks. In chess, an ex-\npert player is referred to as a master. Humans have always been\nfamiliar with the idea of mastery. In many occupations, such as\nblacksmithing, masters would pass on their skills to apprentices\nwho might, with the passage of time, become more skillful than\nthe masters who taught them. The same process of mastery can\nbe observed in a wide range of occupations and pursuits including\nartistic, sporting, and technical endeavours. The concept of mas-\ntery is a common way of recognizing human expertise in many\nfields. Thus people in professions such as the law, engineering and\nmedicine have to go through a training and licensing process before\nthey can practice in their chosen profession.\nOne problem with AI models is that the necessary licensing and\ncertification processes are not yet in place. There has been a focus\non making Al models increasingly smart, but with AI capable of\nperforming more and more tasks, the problem of AI governance\nbecomes increasingly salient. We need to ensure that AI works in\nthe service of human needs and values [31], rather than as a com-\npeting form of intelligence. We have not reached the stage where AI\nmodels or agents can be licensed as radiologists or pharmacists, for\nexample, yet Al techniques are being applied to an ever-expanding\nset of tasks and the issues of how to implement newly available AI\nexpertise into task workflows is becoming increasingly pressing.\nWhen AI models can beat the best human players in complex games\nlike chess and Go, and can perform many tasks autonomously, the\nissues of Al governance, licensing, and human-AI coordination\nbecome unavoidable.\nAs with human-human interactions, different forms of human-AI\ninteraction (HAII) can be distinguished [5]. AI can be \"in charge\u201d (or\nautonomous), it can be an equal partner, where decision making is\nmade jointly and collaboratively by human and AI agents, or it may\nbe subservient, carrying out the requests made by humans who\nact as supervisory controllers [37, 38]. In this paper we examine\nwhen and how AI should be in charge, and how we should adapt\nto increasing levels of progress towards AI mastery. Our sugges-\ntions relating to the integration of AI into tasks and workflows\nsupplement approaches that argue for human priority [27], with\ntechnology that is human-centred, controlled and managed [31].\nAnother factor that complicates AI implementation is resistance\nto recognizing AI mastery [8] when it occurs. This may be partly due\nto a Dunning-Kruger effect [18] where experts may overestimate\""}, {"title": "2 MODEL MASTERY", "content": "Models become competitive with humans in performing tasks when\nthey outperform the practitioners in a field (e.g., doctors making\ndiagnostic decisions), and they achieve mastery when their superi-\nority to human experts performing a particular task is undisputed.\nFor instance, AI chess programs have achieved model mastery and\nthe best human chess players in the world are consistently beaten\nby them. It is easy to demonstrate mastery in a game like chess,\nwhere ratings are based on the quality of player that a human or\nAl agent can beat. In other areas the measurement of mastery may\nbe more controversial. For instance, an Al system that makes diag-\nnoses or predicts health outcomes may have to deal with a wide\nrange of patients and may sometimes make errors, thus making it\nharder to demonstrate that the model's decision making is overall\nsuperior to the human experts currently doing the task [25].\nEarly demonstrations of model mastery using simple statistical\nmodels (e.g., linear regression) had relatively little impact on prac-\ntice. However, the second decade of the twenty-first century saw\nincreasing acceptance of machine learning and AI approaches in\nfields such as healthcare [10, 32, 34], with models exceeding human\nperformance in a number of domains."}, {"title": "3 ADAPTATION OF HUMAN-AI INTERACTION\nTO INCREASING MODEL MASTERY", "content": "Human-human interaction is a complex topic that is relevant to a\nwide range of fields (each with their own theories and approaches)\nincluding the law, education, business, healthcare, psychology and\nsociology. Adding AI to the mix increases this complexity even\nmore [3, 23]. People with varying levels of expertise (relative to the\ntasks being performed) may be interacting with Al models (also\nwith varying levels of expertise on different types of task), as can\nbe seen in Figure 2 below.\nIn cases where human users are experts, we can consider cases\nwhere the model is error prone vs. high performing (top two quad-\nrants of Figure 2). For cases where the model is error prone, experts\ncan train the model (typically using active learning [28] to ensure\nthat the training process is efficient). For high performing models\nwhere users are experts there is a choice to be made. If model mas-\ntery is accepted, then the model may be provided with a high degree\nof autonomy, with ultimate autonomy being automatic implemen-\ntation of the AI model's decisions. However, if the model mastery is\nnot fully accepted then a supervisory control approach can be used,\nwhere one or more experts oversee the model performance and\nmay make the final decision on whether to accept the AI model's\nadvice or not.\nFor common (non-expert) users (the situation in the lower two\nquadrants of Figure 2), supervisory control will generally be prob-\nlematic because the user may not have enough expertise to make"}, {"title": "4 INERTIA IN ACCEPTING MODEL MASTERY", "content": "Imagine that a society had a privileged group of people who were\nassigned tasks, not because they were good at those tasks, but\nbecause of their status. Wars would be fought by incompetent gen-\nerals, operations would be carried out by incompetent surgeons,\nand banks would fail because of the incompetence of their execu-\ntives. Clearly this society would be less successful than one where\nthe most competent people were put in charge of key tasks. As mod-\nels achieve mastery, failure to fully implement them may be costly.\nHowever, experience with clinical decision making have shown that\nexperts may be resistant to accepting model mastery [13] and may\nbase their resistance on a number of arguments that have varying\ndegrees of legitimacy. One argument can be characterized as the\n\"good people on both sides argument\".\nThe argument here is that the model might work well in some\ncases but that the expert should still be able to be involved in the\ndecision and over-rule the model in the cases where the expert\n\"knows\" that the model is wrong. Sometimes this argument might\nbe effective. For instance, human advice may usefully overrule a\nmodel that is brittle, and the expert can provide a better decision in\nan edge case that the model is unprepared for. However, if experts\nget to pick and choose when they can intervene, there is a danger\nthat in many cases overall decision making outcomes may be worse,\nsince experts may not really know when they are in a better position\nthan the model to make the decision.\nAnother argument (in a clinical setting) can be characterized as", "could": "This argument conflates\ngeneral knowledge about a person with the specific knowledge\nneeded to make a decision. Thus deciding whether or not to order\nan appendectomy after complaints of severe stomach pains, or do\na brain scan after a head injury, should likely be based on a very\nspecific set of criteria. Thus, a clinician's sense of superiority about\nbeing more knowledgeable about a broader set of features that\nrelate to the patient may be misplaced."}, {"title": "5 CONCLUSION", "content": "The \"passing of the torch\" between generations is often challenging.\nA father may be unwilling to continue playing after he has taught\nhis child to play chess only to find that he now loses every game.\nGovernments may feel the need to regulate the use of AI models\nto minimize job losses, and they will need to ensure that there\nis accountability, transparency, and fairness in the deployment of\nthose models.\nProgress towards model mastery is occurring across a wide range\nof applications. We need to recognize where Al systems are in the\nmodel mastery lifecycle for various applications and design task\nallocations and HAII accordingly. There should always be a degree\nof healthy skepticism when considering the possibility of model\nmastery. Models should be tested for brittleness. Are there edge\ncases that they do not handle well? Models should also be tested\nfor sensitivity to drift. Is it possible that the characteristics of the\ndecision problem are changing over time so that a model that was\npreviously successful may no longer fit the data as well?\nWe can use a statistical analogy to characterize the dilemma\nof when to accept Model Mastery and give AI models relative au-\ntonomy. If we implement model autonomy but overall task per-\nformance worsens, we have committed a Type 1 error where we\nrejected the status quo where the human remains \"in-the-loop\".\nOn the other hand, if we fail to implement model autonomy for a\nmodel that is demonstrably better and task performance suffers as a\nresult, then we have committed a Type II error, accepting the status\nquo when we should have rejected it. In the traditional statistical\napproach you have to be very sure before rejecting the status quo\n(null hypothesis) and this level of caution is likely required in the\ndesign of Al systems as well. Just as a pilot has to earn her wings,\nso too Al models should prove that they have achieved mastery\nand can act more autonomously.\nWhile we advise caution in accepting model mastery, based on\nobjective analyses of task performance, we should also be cautious\nabout accepting persuasive arguments against model mastery that\nare not backed up by data, since efforts to avoid Type I errors should\nnot lead to too many Type II errors.\nModel mastery seems easier to accept when the model is creating\nnew capabilities that were not previously available, e.g. leveraging\nbig data to speed up the process of drug discovery. However, when\nmodel mastery occurs in a domain that has been dominated by\nskilled human experts, then the implementation of successful mod-\nels becomes more problematic. Although model mastery appears\nto be relatively prevalent in clinical decision making tasks [13],\nuptake of automated methods has been relatively slow. One reason\nfor slow uptake of automated clinical decision making may be the\nview that clinicians have privileged expertise and are better aware\nof the decision making context. In this view, it might be better for\nAl to make suggestions and perhaps physician decisions can be\nimproved through feedback. However, studies that have looked\nat this issue have not yielded encouraging results. For instance,\nGoldberg [12] gave judges immediate feedback on the accuracy of\ntheir judgments. In spite of the feedback provided, the clinicians\nwere outperformed by a four-variable equally weighted regression\nequation.\nAs AI models progress towards mastery across a wide range\nof domains we need more rational allocation of human/machine\nexpertise where HAII appropriate to the level of progress towards\nmodel mastery can be implemented, and with sufficient safeguards\nagainst factors like brittleness, model drift, and violations of ethics\nand fairness.\nIn conclusion, position on the model mastery lifecycle is an\nimportant factor in the design of HAII. While AI may outperform\nhumans in specific tasks, human engagement still remains crucial in\nthe vast majority of applications and thus HAII is also crucial. Our\nmodel mastery lifecycle framework seeks to provide guidance for\nintegrating Al systems into work practices while accommodating\nthe changing requirements for HAII as model expertise increases.\nHealthcare is a domain of particular interest and concern as many\napplications appear to be in or approaching the zone of uncertainty\nand because decisions have significant ethical and safety implica-\ntions. While model mastery is an important issue, there will be\nmany tasks where human judgment, advice, and interpretation will\nremain necessary and where HAII will be a critical determinant of\noverall system performance."}]}