{"title": "The Model Mastery Lifecycle: A Framework for Designing Human-Al Interaction", "authors": ["Mark Chignell", "Mu-Huan Chung", "Khilan Jerath", "Jaturong Kongmanee", "Abhay Raman"], "abstract": "The utilization of Al in an increasing number of fields is the lat-est iteration of a long process, where machines and systems have been replacing humans, or changing the roles that they play, in various tasks. Although humans are often resistant to technologi-cal innovation, especially in workplaces, there is a general trend towards increasing automation, and more recently, AI. AI is now capable of carrying out, or assisting with, many tasks that used to be regarded as exclusively requiring human expertise. In this paper we consider the case of tasks that could be performed either by human experts or by AI and locate them on a continuum running from exclusively human task performance at one end to Al auton-omy on the other, with a variety of forms of human-AI interaction between those extremes. Implementation of AI is constrained by the context of the systems and workflows that it will be embedded within. There is an urgent need for methods to determine how AI should be used in different situations and to develop appropriate methods of human-AI interaction so that humans and AI can work together effectively to perform tasks. In response to the evolving landscape of AI progress and increasing mastery, we introduce an AI Mastery Lifecycle framework and discuss its implications for human-Al interaction. The framework provides guidance on human-Al task allocation and how human-AI interfaces need to adapt to improvements in AI task performance over time. Within the framework we identify a zone of uncertainty where the issues of human-AI task allocation and user interface design are likely to be most challenging.", "sections": [{"title": "1 INTRODUCTION", "content": "We live in an age of model-based prediction and classification. A chess playing model, for instance, seeks to predict what the best move should be in each situation encountered during a chess game [4]. In recent decades models have begun to outperform humans in chess as well as many other games and tasks. In chess, an expert player is referred to as a master. Humans have always been familiar with the idea of mastery. In many occupations, such as blacksmithing, masters would pass on their skills to apprentices who might, with the passage of time, become more skillful than the masters who taught them. The same process of mastery can be observed in a wide range of occupations and pursuits including artistic, sporting, and technical endeavours. The concept of mas-tery is a common way of recognizing human expertise in many fields. Thus people in professions such as the law, engineering and medicine have to go through a training and licensing process before they can practice in their chosen profession.\nOne problem with AI models is that the necessary licensing and certification processes are not yet in place. There has been a focus on making Al models increasingly smart, but with AI capable of performing more and more tasks, the problem of AI governance becomes increasingly salient. We need to ensure that AI works in the service of human needs and values [31], rather than as a com-peting form of intelligence. We have not reached the stage where AI models or agents can be licensed as radiologists or pharmacists, for example, yet Al techniques are being applied to an ever-expanding set of tasks and the issues of how to implement newly available AI expertise into task workflows is becoming increasingly pressing.\nWhen AI models can beat the best human players in complex games like chess and Go, and can perform many tasks autonomously, the issues of Al governance, licensing, and human-AI coordination become unavoidable.\nAs with human-human interactions, different forms of human-AI interaction (HAII) can be distinguished [5]. AI can be \"in charge\u201d (or autonomous), it can be an equal partner, where decision making is made jointly and collaboratively by human and AI agents, or it may be subservient, carrying out the requests made by humans who act as supervisory controllers [37, 38]. In this paper we examine when and how AI should be in charge, and how we should adapt to increasing levels of progress towards AI mastery. Our sugges-tions relating to the integration of AI into tasks and workflows supplement approaches that argue for human priority [27], with technology that is human-centred, controlled and managed [31].\nAnother factor that complicates AI implementation is resistance to recognizing AI mastery [8] when it occurs. This may be partly due to a Dunning-Kruger effect [18] where experts may overestimate"}, {"title": "2 MODEL MASTERY", "content": "Models become competitive with humans in performing tasks when they outperform the practitioners in a field (e.g., doctors making diagnostic decisions), and they achieve mastery when their superi-ority to human experts performing a particular task is undisputed. For instance, AI chess programs have achieved model mastery and the best human chess players in the world are consistently beaten by them. It is easy to demonstrate mastery in a game like chess, where ratings are based on the quality of player that a human or Al agent can beat. In other areas the measurement of mastery may be more controversial. For instance, an Al system that makes diag-noses or predicts health outcomes may have to deal with a wide range of patients and may sometimes make errors, thus making it harder to demonstrate that the model's decision making is overall superior to the human experts currently doing the task [25].\nEarly demonstrations of model mastery using simple statistical models (e.g., linear regression) had relatively little impact on prac-tice. However, the second decade of the twenty-first century saw increasing acceptance of machine learning and AI approaches in fields such as healthcare [10, 32, 34], with models exceeding human performance in a number of domains.\nA key reason why even simple models often outperform well-trained human experts is their consistency. Unlike humans, who can vary greatly in their predictions due to factors like mood, illness, cognitive biases [36], and sometimes forgetfulness, models main-tain a uniform approach in their predictions. This raises important questions about the future role of algorithms in decision-making, suggesting that any shift towards more autonomous systems will need to be supported by careful management of trust and oversight in these technologies.\nModel mastery is easy to demonstrate in situations where there are not enough human experts to carry out a task on a large scale, or there is too much data for them to take account of. An example of this kind of task is screening. In recent years it has become custom-ary for machine learning models to outperform human screeners (e.g., recognizing faces of incoming passengers at airports) and this first type of model competitiveness may be regarded as unsur-prising, as the models typically have access to large amounts of data (e.g., databases of face images) that can be easily processed computationally, but that are difficult for humans to deal with. As further examples, language understanding, machine translation, and question answering are areas where the problem is too vast for an apprenticeship style of training and where algorithms are used, along with vast amounts of data and text, to infer the knowledge needed.\nTraditional ML evaluation metrics such as AUROC (Area under the Receiver Operating Characteristic Curve) and F1 (a weighted combination of precision and recall) can be used to compare model and human performance to quantify the degree of model mastery, but the problem here is that not all errors are created equal, and thus controversy over the relative merits of human and AI performance may still remain in spite of evaluations based on ground truth data, because the costs of different kinds of errors are controversial and humans and AI models tend to be differentially susceptible to different types of error.\nAl superiority, often attributed to its vast data access and comput-ing power, is less surprising than its ability to outperform human experts even in domains where it has been trained by those ex-perts [6]. There is a kind of paradox where experts may recognize model mastery too late, while more naive users may assume model mastery too early. An example of this latter situation is \"algorithm appreciation\" [22] where non-experts sometimes trust algorithms more than human advice even if they are not provided with evi-dence of model mastery."}, {"title": "3 ADAPTATION OF HUMAN-AI INTERACTION TO INCREASING MODEL MASTERY", "content": "Human-human interaction is a complex topic that is relevant to a wide range of fields (each with their own theories and approaches) including the law, education, business, healthcare, psychology and sociology. Adding AI to the mix increases this complexity even more [3, 23]. People with varying levels of expertise (relative to the tasks being performed) may be interacting with Al models (also with varying levels of expertise on different types of task), as can be seen in Figure 2 below.\nIn cases where human users are experts, we can consider cases where the model is error prone vs. high performing (top two quad-rants of Figure 2). For cases where the model is error prone, experts can train the model (typically using active learning [28] to ensure that the training process is efficient). For high performing models, where users are experts there is a choice to be made. If model mas-tery is accepted, then the model may be provided with a high degree of autonomy, with ultimate autonomy being automatic implemen-tation of the AI model's decisions. However, if the model mastery is not fully accepted then a supervisory control approach can be used, where one or more experts oversee the model performance and may make the final decision on whether to accept the AI model's advice or not.\nFor common (non-expert) users (the situation in the lower two quadrants of Figure 2), supervisory control will generally be prob-lematic because the user may not have enough expertise to make a suitable judgement of whether or not the model's advice is ac-ceptable or should be acted upon. If the model is high performing but users are relatively unskilled (lower right quadrant of Figure 2), then there may be more willingness to accept model mastery. When both Al models and humans are unskilled (lower left quad-rant of Figure 2) crowdsourcing may be a workable strategy. An overall labeling process that applies consensus labels may lead to higher quality final labels than the individual user-assigned labels on which the consensus is based.\nFigure 2 shows a cross-sectional view of human vs. Al expertise. While individual people may transition from the lower to upper quadrants through training, the general pools of experts and users will remain in the upper and lower quadrants respectively. In con-trast, Al model performance (expertise) for a particular task will generally move from left to right in the figure as the model is trained and enhanced.\nGrowing Al/automation expertise changes the role of the hu-man, while also creating new human-AI interaction challenges, with automated driving being an example of this process. There is a strong business case for changing the role of the human and increasing the role of automation and AI in driving. Many people don't like driving or are not particularly good at it, and as people get older and more physically and cognitively frail, autonomous driving might be able to help them maintain the mobility that they would otherwise lose.\nHancock et al. [14] explored the evolution of vehicle automation from fully human-driven to completely autonomous systems. They cited the ASME model of levels of automation (LOA), from Level 0, where the driver controls all tasks, to Level 5, where no human intervention is required. Key transitional phases were highlighted [14], especially Level 3 (Conditional Automation) where the driver must be ready to take over when requested, and Level 4 (High Automation) where the vehicle handles all driving tasks but may allow for human override. Figure 3 shows a schematic overview of the progression towards mastery in the case of automated driving.\nTrust and HAII are crucial concerns in increasingly automated vehicles. In lower levels (level 0-2), the driver is actively engaged and must constantly monitor vehicle operations. At mid-level au-tomation (level 3), trust issues become prominent as drivers rely on the vehicle for full environmental monitoring but must remain prepared to intervene. At higher levels (level 4-5), the vehicle gains full control, requiring absolute trust from passengers as their role shifts completely from driver to observer. The transition to fully autonomous vehicles necessitates robust trust calibration and clear communication about the vehicle's capabilities and limitations. Reg-ulatory frameworks and independent third-party testing are essen-tial to ensure safety and efficacy as these technologies are increas-ingly implemented in vehicles across a wider range of roadways. This example shows how the type of HAII, and related issues such as trust, changes as AI/automation progresses towards mastery.\nAs automated driving technology transitions from lower to higher levels of automation, the identification of who is the co-pilot [27] is likely to change. At low levels of automation the human driver is the \"pilot\u201d and the automation is the co-pilot. However, as automation progresses to higher levels more of the driving is car-ried out by the automation and the human transitions to a co-pilot role. Thus, rather than automatically assigning the co-pilot task to the AI model/agent, we need to consider which agent (human or AI) is naturally in charge and then assign the co-pilot to the remaining agent (human or machine). Perhaps, in the future, the etiquette of HAII across different levels of humans vs. Al expertise will be well understood. However, at present people in general are not well-trained, or experienced, in terms of how to interact with Al models. Human ability will tend to change little over the short term, so in most tasks AI ability will be increasing relative to human ability over time. This effect is captured schematically in Figure 4, where we see that the green line (human ability in a given task given an appropriate level of training) is relatively flat, whereas model performance is increasing over time.\nWe hypothesize that there will generally be a four-stage process, each stage requiring different types of HAII. As model performance improves, it goes from being clearly worse than a human (where if"}, {"title": "4 INERTIA IN ACCEPTING MODEL MASTERY", "content": "Imagine that a society had a privileged group of people who were assigned tasks, not because they were good at those tasks, but because of their status. Wars would be fought by incompetent gen-erals, operations would be carried out by incompetent surgeons, and banks would fail because of the incompetence of their execu-tives. Clearly this society would be less successful than one where the most competent people were put in charge of key tasks. As mod-els achieve mastery, failure to fully implement them may be costly. However, experience with clinical decision making have shown that experts may be resistant to accepting model mastery [13] and may base their resistance on a number of arguments that have varying degrees of legitimacy. One argument can be characterized as the \"good people on both sides argument\".\nThe argument here is that the model might work well in some cases but that the expert should still be able to be involved in the decision and over-rule the model in the cases where the expert \"knows\" that the model is wrong. Sometimes this argument might be effective. For instance, human advice may usefully overrule a model that is brittle, and the expert can provide a better decision in an edge case that the model is unprepared for. However, if experts get to pick and choose when they can intervene, there is a danger that in many cases overall decision making outcomes may be worse, since experts may not really know when they are in a better position than the model to make the decision.\nAnother argument (in a clinical setting) can be characterized as \"models may be good at dealing with cases in general, but I know my patients better than any model could\". This argument conflates general knowledge about a person with the specific knowledge needed to make a decision. Thus deciding whether or not to order an appendectomy after complaints of severe stomach pains, or do a brain scan after a head injury, should likely be based on a very specific set of criteria. Thus, a clinician's sense of superiority about being more knowledgeable about a broader set of features that relate to the patient may be misplaced.\nGrove and Meehl [13] gave the example of how physicians are sometimes more willing to accept risk than patients are because they feel that their knowledge of individual cases over-rules the general statistical relationships that apply in a set of data. A patient may have an annoying medical condition but be unwilling to accept a 5% risk of dying during an operation to treat the condition. The surgeon on the other hand may feel like his knowledge of the case indicates that the risk is lower. \"Personalizing\" expert assessments in this way may often be wrong due to cognitive biases and heuristics such as the availability and representativeness heuristics [36]."}, {"title": "5 CONCLUSION", "content": "The \"passing of the torch\" between generations is often challenging. A father may be unwilling to continue playing after he has taught his child to play chess only to find that he now loses every game. Governments may feel the need to regulate the use of AI models to minimize job losses, and they will need to ensure that there is accountability, transparency, and fairness in the deployment of those models.\nProgress towards model mastery is occurring across a wide range of applications. We need to recognize where Al systems are in the model mastery lifecycle for various applications and design task allocations and HAII accordingly. There should always be a degree of healthy skepticism when considering the possibility of model mastery. Models should be tested for brittleness. Are there edge cases that they do not handle well? Models should also be tested for sensitivity to drift. Is it possible that the characteristics of the decision problem are changing over time so that a model that was previously successful may no longer fit the data as well?\nWe can use a statistical analogy to characterize the dilemma of when to accept Model Mastery and give AI models relative au-tonomy. If we implement model autonomy but overall task per-formance worsens, we have committed a Type 1 error where we rejected the status quo where the human remains \"in-the-loop\". On the other hand, if we fail to implement model autonomy for a model that is demonstrably better and task performance suffers as a result, then we have committed a Type II error, accepting the status quo when we should have rejected it. In the traditional statistical approach you have to be very sure before rejecting the status quo (null hypothesis) and this level of caution is likely required in the design of Al systems as well. Just as a pilot has to earn her wings, so too Al models should prove that they have achieved mastery and can act more autonomously.\nWhile we advise caution in accepting model mastery, based on objective analyses of task performance, we should also be cautious about accepting persuasive arguments against model mastery that are not backed up by data, since efforts to avoid Type I errors should not lead to too many Type II errors.\nModel mastery seems easier to accept when the model is creating new capabilities that were not previously available, e.g. leveraging big data to speed up the process of drug discovery. However, when model mastery occurs in a domain that has been dominated by skilled human experts, then the implementation of successful mod-els becomes more problematic. Although model mastery appears to be relatively prevalent in clinical decision making tasks [13], uptake of automated methods has been relatively slow. One reason for slow uptake of automated clinical decision making may be the view that clinicians have privileged expertise and are better aware of the decision making context. In this view, it might be better for Al to make suggestions and perhaps physician decisions can be improved through feedback. However, studies that have looked at this issue have not yielded encouraging results. For instance, Goldberg [12] gave judges immediate feedback on the accuracy of their judgments. In spite of the feedback provided, the clinicians were outperformed by a four-variable equally weighted regression equation.\nAs AI models progress towards mastery across a wide range of domains we need more rational allocation of human/machine expertise where HAII appropriate to the level of progress towards model mastery can be implemented, and with sufficient safeguards against factors like brittleness, model drift, and violations of ethics and fairness.\nIn conclusion, position on the model mastery lifecycle is an important factor in the design of HAII. While AI may outperform humans in specific tasks, human engagement still remains crucial in the vast majority of applications and thus HAII is also crucial. Our model mastery lifecycle framework seeks to provide guidance for integrating Al systems into work practices while accommodating the changing requirements for HAII as model expertise increases. Healthcare is a domain of particular interest and concern as many applications appear to be in or approaching the zone of uncertainty and because decisions have significant ethical and safety implica-tions. While model mastery is an important issue, there will be many tasks where human judgment, advice, and interpretation will remain necessary and where HAII will be a critical determinant of overall system performance."}]}