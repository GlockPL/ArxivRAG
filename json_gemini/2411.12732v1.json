{"title": "Benchmarking Positional Encodings for GNNs and Graph Transformers", "authors": ["Florian Gr\u00f6tschla", "Jiaqing Xie", "Roger Wattenhofer"], "abstract": "Recent advances in Graph Neural Networks (GNNs) and Graph Transformers (GTs) have been driven by innovations in architectures and Positional Encodings (PEs), which are critical for augmenting node features and capturing graph topology. PEs are essential for GTs, where topological information would otherwise be lost without message-passing. However, PEs are often tested alongside novel architectures, making it difficult to isolate their effect on established models. To address this, we present a comprehensive benchmark of PEs in a unified framework that includes both message-passing GNNs and GTs. We also establish theoretical connections between MPNNs and GTs and introduce a sparsified GRIT attention mechanism to examine the influence of global connectivity. Our findings demonstrate that previously untested combinations of GNN architectures and PEs can outperform existing methods and offer a more comprehensive picture of the state-of-the-art. To support future research and experimentation in our framework, we make the code publicly available.", "sections": [{"title": "INTRODUCTION", "content": "Graph machine learning has traditionally relied on message-passing neural networks (MPNNs), which work through iterative rounds of neighborhood aggregation (Kipf & Welling, 2016). In each round, nodes update their states by incorporating information from their neighbors along with their own current states. While effective in capturing local graph structures, this approach can struggle with modeling long-range dependencies. Graph Transformer (GT) architectures utilize full attention mechanisms to circumvent this, but necessitate new methods to integrate graph topology information (Dwivedi & Bresson, 2020). This is similar to how positional encodings (PEs) in Natural Language Processing (NLP) represent token positions within sequences (Vaswani et al., 2017). However, encoding positional information in graphs is more complex than in sequences. Ideally, positional encodings should allow the reconstruction of the graph's topology from node features and provide useful inductive biases to improve performance (Black et al., 2024). Despite the growing number of new graph transformer architectures and positional encodings, there has been a lack of systematic evaluation comparing these encodings across different GT architectures. This makes it difficult to determine whether observed performance improvements are due to novel encodings or architectural innovations.\nIn this paper, we conduct a comprehensive evaluation of various positional encodings for both message-passing and transformer frameworks. Our goal is to understand the impact of positional encodings on model performance and identify the best combinations of encodings and architectures. By benchmarking state-of-the-art graph transformers with a variety of positional encodings, we provide a clear picture of the current state of the field and offer guidance for future research. Additionally, we further strengthen the theoretical connection between MPNNs and GTs. Although GTs are generally considered fundamentally different due to their use of attention mechanisms, we show that under certain conditions, MPNNs and GTs can be equally expressive, with additional results that extend the scope of previous analyses (Veli\u010dkovi\u0107, 2023; M\u00fcller & Morris, 2024). Specifically, MPNNs can be applied to fully-connected graphs and operate like a GT, while attention mechanisms can also be adapted for local message-passing. Our theoretical analysis demonstrates that both MPNNs and GTs can have the same expressiveness when the underlying topology of the MPNN"}, {"title": "RELATED WORK", "content": "Message-Passing Neural Networks (MPNNs). Earlier graph neural networks (GNNs), including models like GCN (Kipf & Welling, 2016), GAT (Veli\u010dkovi\u0107 et al., 2017), GraphSAGE (Hamilton et al., 2017), and GIN (Xu et al., 2018), have paved the way for various advancements. Some convolutional filtering variants incorporate edge attributes into their architecture. GatedGCN (Bresson & Laurent, 2017) employs gates as sparse attention mechanisms, while GINE (Hu et al., 2019) augments features with local edges. Recent efforts aim to enhance the expressive power of GNNs, addressing the limitations imposed by the 1-WL test. For instance, Principal Neighborhood Aggregation (PNA) (Corso et al., 2020) combines different aggregators with degree-scalers to tackle isomorphism tasks in continuous feature spaces. Higher-order GNNs, like k-GNN (Morris et al., 2019; Maron et al., 2019), build on the k-WL algorithm, a more generalized version of the WL test, offering increased expressive power. Other approaches, such as GSN (Bouritsas et al., 2022) and GIN-AK+ (Zhao et al., 2021), utilize substructures (subgraphs) for message passing, while methods like CIN (Bodnar et al., 2021) operate on regular cell complexes, although they remain less powerful than the 3-WL test. Importantly, these models serve as baselines in some graph transformers, demonstrating comparable performance with certain GTs, as cited in GraphGPS (Ramp\u00e1\u0161ek et al., 2022), GRIT (Ma et al., 2023), and Exphormer (Shirzad et al., 2023).\nGraph Transformers (GTs). Graph Transformers (GT) were popularized in recent years (Ramp\u00e1\u0161ek et al., 2022; Liu et al., 2023; Mao et al., 2024; Zhang et al., 2023). Modules including positional or structural encodings, global attention, and local message passing are considered as mainstream design components for a standard graph transformer model, which successfully solved the problem of in-scalability (Ramp\u00e1\u0161ek et al., 2022; Shirzad et al., 2023) in large graphs, lack of graph inductive bias (Ma et al., 2023), and over-smoothing problems (Chen et al., 2022b). Apart from its maturity in some machine learning fields such as natural language processing, computer vision, or bioinformatics that many previous GT papers have mentioned, GTs have also demonstrated their strength by extending their application to scientific domains such as differential equations (Bryutkin et al., 2024; Choromanski et al., 2022), quantum physics (Wang et al., 2022a), and symbolic regression (Zhong & Meidani). Some recent works are theoretical analysis in graph transformers regarding the theoretical expressive power of GT (Zhou et al., 2024), and the analytical relationship between positional encodings in GT (Keriven & Vaiter, 2024; Black et al., 2024). However, there is currently a lack of a practical benchmark that compares different types of positional encodings. MPNNs and GTs have been compared extensively in the literature, with early work observing that these models can simulate one another (Veli\u010dkovi\u0107, 2023). A more rigorous theoretical analysis has demonstrated that GTs can be related to MPNNs when a virtual node is employed (Cai et al., 2023). Furthermore, it has been established that GTs can simulate MPNNs, provided that the positional encodings are sufficiently strong (M\u00fcller & Morris, 2024). In contrast, our findings show"}, {"title": "THEORETICAL FOUNDATIONS", "content": "conditions under which MPNNs operating on fully connected graphs can achieve equal expressiveness to that of GTs, without requiring additional positional encodings or architectural modifications.\nGTs traditionally make use of positional encodings to encode the graph topology, especially when full attention is used. We provide an in-depth review of positional encodings and benchmarking in Section 3.1 and Appendix A.1."}, {"title": "POSITIONAL ENCODINGS", "content": "Numerous positional encodings for graph-based models have been discussed in recent research, but they are often scattered across various ablation studies with no unified framework. In this paper, we categorize and streamline the formal definition of existing graph-based positional encodings into three main categories: Laplacian-based, Random walk-based, and others.\nWe start with some fundamental definitions related to graphs. Let the input graph be G = (V, E, \u03a7), where X \u2208 R^{|V|} represents the node features. For any graph G, essential properties include the degree matrix D and the adjacency matrix A. The graph Laplacian matrix L is defined as L = D-A. A normalized graph Laplacian is given by L = I \u2013 D^{-\u00bd} A D^{-\u00bd} = U \\Tau U, where the i-th row of U corresponds to the graph's i-th eigenvector u\u1d62, and \u039b is a diagonal matrix containing the eigenvalues of L. We define a graph neural network model f(\u00b7) parameterized by \u0398. We denote X_{PE}^{k} as the positional encoding for node K.\nLaplacian-based methods utilize functions of the k-th eigenvector U_{k,:}, \u039b, and parameters \u0398. Examples include Laplacian Positional Encoding (LapPE) (Ramp\u00e1\u0161ek et al., 2022) and Sign-Invariant Networks (SignNet) (Lim et al., 2022).\nX_{PE}^{k} = f (U_{k,:}, \u039b, \u0398, {\u00b7})\nRandom walk-based methods are derived from polynomial function p(\u00b7) of D and A. Examples are Random-Walk Structural Encoding RWSE (Ramp\u00e1\u0161ek et al., 2022), Random-Walk Diffusion (RWDIFF/LSPE) (Dwivedi et al., 2021), and Relative Random Walk Probability Based (RRWP) (Ma et al., 2023).\nX_{PE}^{k} = p (D, A, {})\nOther methods rely on different procedures, such as colors obtained by mapping 1-WL to higher dimensions. We thus use this umbrella class for all remaining PEs. Examples include the WL-based Positional Encoding (WLPE) (Dwivedi & Bresson, 2020) and Graph Convolution Kernel Networks (GCKN) (Mialon et al., 2021). We aim to succinctly summarize and unify these positional encoding methods for better accessibility and comparison. The Appendix contains more specific details (including equations) for each positional encoding."}, {"title": "MESSAGE-PASSING NETWORKS", "content": "MPNNs comprise multiple layers that repeatedly apply neighborhood aggregation and combine functions to learn a representation vector for each node in the graph. For an input graph G = (V, E, X), the i-th layer of a MPNN can be written as\nc_{v}^{(i)} = COMBINE^{(i)} \\Big(c_{v}^{(i-1)}, AGGREGATE^{(i)} \\big({c_{w}^{(i-1)} : w \u2208 N(v)}\\big)\\Big),\nwhere c_{v}^{(i-1)} represents the state of node v after layer (i \u2013 1)."}, {"title": "GRAPH TRANSFORMERS", "content": "Transformer models have been widely used in modeling sequence-to-sequence data in different domains (Vaswani et al., 2017). Although the attention mechanism has commonly been used to learn on graph-structured data (Veli\u010dkovi\u0107 et al., 2017), the use of transformers is relatively recent. A GT layer relies on a self-attention module that lets nodes attend to a set of \u201cneighbors\u201d, effectively resulting in a dependency graph G of nodes that can attend to each other. We will refer to the nodes"}, {"title": "BRIDGING GTS AND WL", "content": "In the literature, various attempts have been made to bridge the gap between Graph Transformers (GTs) and the WL test (M\u00fcller & Morris, 2024; Cai et al., 2023). This is usually done by defining new variants of the WL algorithm that apply to the GT of interest (Kim et al., 2022). However, we argue that such extensions are not necessary. Instead, we can interpret the execution of a GT on G as an MPNN on a new topology G' = (V, E') corresponding to the dependency graph, representing the information flow in the attention layer (Veli\u010dkovi\u0107, 2023). For example, a GT with full attention can be seen as an MPNN on the fully connected graph, with E' = V \u00d7 V. Relative positional encodings can be added to the MPNN as edge labels. This means we can use the (edge-augmented) 1-WL algorithm on G' to upper bound the expressive power of a GT on G. While it is perhaps not surprising that GT expressivity can be upper bounded in this way, we also show that GTs can attain this upper bound under some reasonable assumptions. To facilitate this proof, we use the same idea as Xu et al. (2018) to show the equivalence between the GIN architecture and 1-WL.\nLemma 3.1 (Adapted from Corollary 6 by Xu et al. (2018)). Assume X is a countable set. There exists a function f : X \u2192 R\u207f so that for infinitely many choices of \u2208, including all irrational numbers, h(c,X) = f(c) + \u03a3_{x\u2208X} f(x) is unique for each pair (c, X), where c \u2208 X and X \u2286 X is a multiset of bounded size. Moreover, any function g over such pairs can be decomposed as g(c, X) = \u03c6 ((f(c) + (1 + \u2208) \u03a3_{x\u2208X} f(x)) for some function \u03c6.\nTo complete the proof for GTs, we adapt Corollary 6 by moving the use of the multiplicative factor \u2208 R from f (c) to the aggregation \u03a3_{x\u2208X} f(x). This is because the GT can multiply the aggregation by \u2208 using the attention coefficients while it cannot transform c_{h(a-1)} directly. The is used in the proof to differentiate between embeddings from neighbors and a node's own embedding.\nWith the adapted Lemma, we can prove the following:\nTheorem 3.2. Let G = (V, E) be a graph with node embeddings c\u1d65 for nodes v \u2208 V. A GT layer on the dependency graph G' = (V, E') can map nodes v\u2081, v\u2082 \u2208 V to different embeddings only if the 1-WL algorithm using E' assigns different labels to nodes v\u2081 and v\u2082. For equivalence, we need d (in the definition of GTs) to be injective and a_{u,v} = c for a given constant c \u2208 R and all (u, v) \u2208 \u0395', making the GT as expressive as the 1-WL algorithm."}, {"title": "SPARSE GRIT MESSAGE-PASSING CONVOLUTION", "content": "GRIT introduces two main innovations: (1) A new attention mechanism that updates edge labels on a fully connected graph and (2) RRWP as a positional encoding. While it is relatively easy to use RRWP with both other message-passing and Graph Transformer architectures, we need some adaptions to use the GRIT attention mechanism with message-passing GNNs on sparse graphs. As motivated earlier in Section 3.4, a Graph Transformer can be seen as message-passing on a fully-connected graph. Therefore, we generalize the GRIT attention mechanism designed for fully-connected graphs to a message-passing convolution that works with any neighborhood. We call the resulting convolution Sparse GRIT, as it can attend to local neighborhoods on sparse graphs and does not suffer from the quadratic computational overhead that the original GRIT mechanism has. This makes sparse GRIT more efficient and scalable, as we further underline in our empirical evaluation in Section 5.2.\nSparse GRIT utilizes the same updating edge labels \u00ea_{i,j} as the original, but only for edges that exist in the original graph. This further distinguishes the convolution from other popular local attention mechanisms like GAT. The main difference to GRIT lies in the update function x\u1d62 for nodes, which now attend to their local neighborhood instead of all nodes in the graph. It becomes:\nx_{i} = \\sum_{j \u2208 N(i)} \\frac{e^{w_j e_{i,j}}}{\\sum_{k \u2208 N(i)} e^{w_k e_{i,k}}} (W_v x_j + W_E v e_{i,j}),\nwhere w\u2c7c is the attention weight, W\u1d65 and W_{E} are weight matrices. In contrast to GRIT, the summation is taken only over a node's local neighborhood using the implementation of a sparse softmax. With these changes, sparse GRIT works the same as GRIT on a fully connected graph. We, therefore, effectively transform the GRIT GT into an MPNN, which enables us to isolate and analyze what impact the graph that is used for message-passing (fully connected vs. local) has. Results and empirical analysis of the sparse GRIT and GRIT are provided in Section 5."}, {"title": "BENCHMARKING POSITIONAL ENCODINGS", "content": "The general GNN framework we consider for our evaluation is depicted in Figure 1. More information on the employed datasets can be found in Appendix A.4. We describe the main components here and give an overview over what methods were tested."}, {"title": "GENERAL FRAMEWORK", "content": "Design Space 1: Positional Encoding. As specified in Section 3.1, we test three types of graph-based positional encodings, treating them as node feature augmentations. More background for the different encodings is given in Appendix 3.1\nDesign Space 2: Connection Type. In most real-world graph datasets, graphs tend to be sparse. This means that message-passing on the original topology can potentially lead to a lack of global information exchange that is necessary for the task. To mitigate this issue, GTs usually employ full-attention on the complete, fully-connected graph (as discussed in Section 3.4). To change the topology that is used for the following GNN layer, we apply a classic MPNN to the fully-connected graph to compare it to GTs and adapt the currently best-performing GT to run on the original graph with our SparseGRIT convolution. While Exphormer implicitly applies some degree of rewiring, we do not consider further rewiring approaches in this work to keep the number of comparisons at a reasonable level.\nDesign Space 3: GNN Layers. Based on the chosen topology, we apply several GNN layers and benchmark their performance. On the MPNN side, we consider GINE, GatedGCN, and SparseGRIT, while we use GraphGPS, Exphormer, and GRIT as classical GTs. The architectures were chosen due to the fact that they are widely used and currently perform best in leaderboards for the tasks we consider. Other convolutions and transformer layers can easily be tested in our general framework.\nDesign Space 4: Prediction Heads. Lastly, we need task-specific prediction heads that decode to either link level, node level, or graph level tasks for the datasets we consider. We use the same setup as popularized by GraphGPS (Ramp\u00e1\u0161ek et al., 2022) and do not undertake further testing here."}, {"title": "BENCHMARKING FRAMEWORK", "content": "To enable the evaluation of models and future research for measuring the impact of positional encodings, we provide a unified codebase that includes the implementation of all tested models and the respective positional encodings. We base the code off GraphGPS Ramp\u00e1\u0161ek et al. (2022) and integrate all missing implementations. This makes for reproducible results and easy extensibility for new datasets, models, or positional encodings. Our codebase further provides readily available implementations for NodeFormer (Wu et al., 2022), Difformer (Wu et al., 2023), GOAT (Kong et al., 2023), GraphTrans (Wu et al., 2021), GraphiT (Mialon et al., 2021), and SAT (Chen et al., 2022a) that are based on the respective original codebases. The code is publicly available at https://github.com/ETH-DISCO/Benchmarking-PEs.\nIn our experiments, we use five different random seeds for the BENCHMARKINGGNN (Dwivedi et al., 2023) datasets and four for the others. The train-test split settings adhere to those established previously, employing a standard split ratio of 8:1:1. All experiments can be executed on either a single Nvidia RTX 3090 (24GB) or a single RTX A6000 (40GB). To avoid out-of-memory (OOM) issues on LRGB and OGB datasets, we ensure that 100GB of reserved CPU cluster memory is available when pre-transforming positional encodings. Configurations that did not fit into this"}, {"title": "EVALUATION", "content": "Based on the framework we established in Section 4.2, we benchmark the performance of different PEs on the BENCHMARKINGGNN (Dwivedi et al., 2023) and LRGB (Dwivedi et al., 2022) datasets. Results for ogbg-molhiv and ogbg-molpcba can be found in Appendix A.7."}, {"title": "BENCHMARKINGGNN DATASETS", "content": "We benchmark state-of-the-art models with commonly used PEs in-depth to identify the best configurations. This analysis is often overlooked when new PEs are introduced alongside new architectures without being evaluated with existing models. Our approach decouples the architecture from the PE, allowing us to measure the full range of possible combinations. Our experimental evaluation starts with a dataset-centric approach, examining the effect of various PEs on model performance. Figure 2 illustrates the range of values for the respective target metrics achieved by different PEs. These values are aggregated over all models in our analysis, while more detailed, unaggregated results are available in Appendix A.7. Notably, while we could reproduce most results of previously tested model and PE combinations, we consistently observed slightly worse values for GRIT. This was the case even when using the official codebase and the most up-to-date commit at the time of writing, with provided configuration files intended to reproduce the results stated in the original paper.\nOur findings reveal that PEs can significantly influence model performance, with the best choice of PE varying depending on the dataset and task. However, PEs can also negatively impact performance in some cases. For instance, while RRWP performs best on the CIFAR10 dataset and ZINC, there are not always clear winners. Sometimes, good performance can be achieved even without any positional encoding (e.g., for PATTERN). This is also evident when examining the best-performing configurations for each model and PE. While the complete results for all runs are"}, {"title": "LONG-RANGE GRAPH BENCHMARK", "content": "We extend our evaluation to the LRGB datasets and use hyperparameter configurations based on those by T\u00f6nshoff et al. (2023), with results presented in Table 2. In these datasets, Laplacian-based encodings generally outperform others (except for the Peptides variations), likely due to their ability to capture more global structure in the slightly larger graphs. This might also be reflected in the fact that transformer-based architectures or models that facilitate global information exchange consistently perform better. Our findings largely align with previous rankings, except for PCQM-Contact, where we achieve a new state-of-the-art with Exphormer, which underscores the importance of thorough benchmarking of existing models. Figure 3 further analyzes the performance of the employed PEs. It is noteworthy that RRWP could not be utilized for larger datasets due to its significant memory footprint and computational complexity, similar to models employing full attention mechanisms. The results align with our previous analysis and show that on datasets like Peptides-func, the PE has"}, {"title": "RUNNING TIME AND MEMORY COMPLEXITY FOR PES", "content": "The computational cost of positional encodings (PEs) is a critical consideration, particularly for large graphs where methods with high complexity quickly become infeasible. We evaluated the running time and memory usage for various PEs, and the full results are presented in Appendix A.8.\nRRWP is the most memory-intensive PE, but maintains reasonable running times. RWSE and RWDIFF, on the other hand, tend to have significantly longer running times but are relatively more memory-efficient. Laplacian-based methods, such as LapPE and ESLapPE, offer a good balance between computational speed and memory usage, making them practical even for larger datasets. PPR and GCKN come with high computational demands in both time and memory, making them less suited for large-scale graphs. In contrast, Laplacian-based encodings like ESLapPE strike a better trade-off, making them practical for a broader range of graph sizes while still offering competitive performance."}, {"title": "GUIDELINES FOR PRACTICIONERS", "content": "For superpixel graph datasets, such as PascalVOC-SP, COCO-SP, MNIST, and CIFAR10, we found that the inclusion of positional encodings generally does not result in substantial performance improvements. In particular, larger superpixel graphs like PascalVOC-SP and COCO-SP showed minimal to no gains from adding PEs, while MNIST similarly exhibited negligible benefits. An exception to this trend is CIFAR10, where RRWP demonstrated potential for enhancing model performance. This suggests that while superpixel graphs may not typically benefit from positional encodings, RRWP could be considered as a candidate for improvement. However, the gains observed may not always justify the increased computational complexity associated with RRWP for such datasets.\nIn contrast, molecular datasets, such as ZINC and the Peptides variations, displayed a strong dependency on the choice of positional encoding, with significant variations in model performance based on the PE used. For instance, ZINC consistently showed the best results with PPR. On the other hand, the Peptides datasets revealed task-specific preferences: Peptides-func benefited the most from RRWP, while Peptides-struct achieved optimal performance with WLPE. Interestingly, despite using identical graph structures, the two Peptides tasks favored different PEs, which indicates that the nature of the prediction target (functional vs. structural) plays a significant role. Thus, when dealing with molecular datasets, practitioners are advised to experiment with various PEs, as the optimal choice may depend more on the specific task than on the graph structure itself. Still, the optimal PE for a given dataset is generally consistent across different models, which, in combination with the fact that we test on commonly used datasets provides practitioners with a strong starting point for their experiments. This distinction is further highlighted when comparing random-walk-based encodings with Laplacian encodings, where one typically emerges as the clear winner depending on the dataset and task."}, {"title": "CONCLUSIONS", "content": "This study underscores the critical role of positional encodings in enhancing the performance of Graph Neural Networks (GNNs), particularly within Graph Transformer architectures. We conducted a thorough comparison of various positional encodings across a wide array of state-of-the-art models and identify the optimal configurations for diverse datasets, as well as offer valuable insights into the relationship between positional encodings and model performance. While we consolidated much of the current state-of-the-art, we also identified new configurations that surpass the performance of existing best models, such as Exphormer on PCQM-Contact. This underscores the necessity of in-depth comparisons to provide a fair and accurate ranking. Our theoretical considerations have led to the development of the SparseGRIT model. This model shows competitive performance across multiple benchmarks, while maintaining scalability to larger graphs. It shows that sparse message-passing together with the right positional encodings is a viable option on many datasets.\nFurthermore, we provide a comprehensive overview of the current state-of-the-art in graph learning and highlight the importance of selecting appropriate positional encodings to achieve optimal results. Our unified codebase includes implementations of all tested models and encodings, which serves as a valuable resource for future research. The framework ensures reproducible results and supports the integration of new datasets, models, and positional encodings, thereby facilitating further experimentation.\nLimitations. Due to computational constraints, we could not explore all possible hyperparameter configurations and there might be slightly better performing ones that we did not catch. Additionally, although we tested a wide range of models and encodings, it is infeasible to test every model and PE. This is why we focused on current state-of-the-art for both. Further, our evaluations are based on a specific set of benchmark datasets, which may not fully represent the diversity of real-world graph structures. Thus, performance on these benchmarks may not generalize to all types of graph data. Nevertheless, our unified codebase serves as a robust foundation for further testing and development, and enables researchers to overcome these limitations by facilitating the inclusion of new datasets, models, and positional encodings."}, {"title": "APPENDIX", "content": "A.1 EXTENDED RELATED WORK\nPositional Encodings for Graphs. Positional encodings are traditionally used in natural language processing to capture the absolute position of a token within a sentence (Vaswani et al., 2017) or the relative distance between pairs of tokens (Shaw et al., 2018; Ke et al., 2020; Chen, 2021). Similarly, positional encoding in graphs aims to learn both local topology and global structural information of nodes efficiently. This approach has been successfully implemented with the introduction of the graph transformer (Dwivedi & Bresson, 2020). With the advent of graph transformers in the field of graph representation learning, many traditional graph theory methods have been revitalized. Graph signal processing techniques have been employed such as Laplacian decomposition and finite hop random walks (Ramp\u00e1\u0161ek et al., 2022; Dwivedi et al., 2023; Ma et al., 2023; Beaini et al., 2021; Dwivedi & Bresson, 2020; Kreuzer et al., 2021; Dwivedi et al., 2021; Lim et al., 2022; Wang et al., 2022b) as absolute or relative positional encoding. Node properties such as degree centrality (Ying et al., 2021) and personalized PageRank (PPR) (Gasteiger et al., 2018; Fu et al., 2024) could be mapped and expanded into higher dimensions for absolute positional encoding, while the shortest distance between nodes could be used for relative positional encoding (Li et al., 2020; Ying et al., 2021). Recent studies have focused on developing learnable positional encodings for graphs (Ying et al., 2021) and exploring their expressiveness and stability as well (Wang et al., 2022b; Ma et al., 2023; Huang et al., 2023). Additionally, graph rewiring combined with layout optimization to coarsen graphs has been proposed as a form of positional encoding (Gr\u00f6tschla et al., 2024).\nGNN Benchmarking. One of the first GNN benchmarking papers compared architectures with and without positional encodings (PEs) (Dwivedi et al., 2023), where their PE mainly refers to Laplacian positional encoding (LapPE). Their study was limited to the GatedGCN model and discussed the expressive power, robustness, and efficiency of state-of-the-art message-passing methods. Additionally, several surveys have benchmarked the complexity, specific tasks, unified message-passing frameworks (Wu et al., 2020; Zhou et al., 2020), robustness, and privacy (Ju et al., 2024). The LRGB dataset (Dwivedi et al., 2022) has been tested in both GNNs and transformers to demonstrate the superiority of Graph Transformers (GTs) over Message Passing Neural Networks (MPNNs). Many state-of-the-art GTs have included this benchmark in their experiments (Ramp\u00e1\u0161ek et al., 2022; Shirzad et al., 2023; Ma et al., 2023). One limitation of the LRGB benchmark is that LRGB only considers LapPE and random walk structural encodings (RWSE). One notable work benchmarked using LRGB by fine-tuning the architectures of GraphGPS and pre-processing (T\u00f6nshoff et al., 2023). We adopt their settings but place greater emphasis on the effect of positional encodings."}, {"title": "PROOFS", "content": "Lemma 3.1 (Adapted from Corollary 6 by Xu et al. (2018)). Assume X is a countable set. There exists a function f : X \u2192 R\u207f so that for infinitely many choices of , including all irrational numbers, h(c,X) = f(c) + \u03a3_{x\u2208X} f(x) is unique for each pair (c, X), where c \u2208 X and X \u2286 X is a multiset of bounded size. Moreover, any function g over such pairs can be decomposed as g(c, X) = \u03c6 ((f(c) + (1 + ) \u03a3_{x\u2208X} f(x)) for some function \u03c6.\nProof of Lemma 3.1. We slightly tightly follow the proof by Xu et al. (2018) for Corollary 6, but define h as h(c, X) = f(c) + (1 + \u0454) \u03a3_{x\u2208X} f(x) (with f defined as in the original proof). We then want to show that for any (c', X') \u2260 (c, X) with c, c' \u2208 X and X, X' \u2282 X, h(c, X) \u2260 h(c', X') holds, if e is an irrational number. We show the same contradiction as Xu et al. (2018): For any (c, X), suppose there exists (c', X') such that (c', X') \u2260 (c, X) but h(c, X) = h(c', X') holds. We consider the following two cases: (1) c' = c but X' \u2260 X, and (2) c' \u2260 c. For the first case, h(c, X) = h(c, X') implies \u03a3_{x\u2208X} f(x) = \u03a3_{x\u2208X'} f(x). By Lemma 5 from Xu et al. (2018) it follows that equality will not hold. For the second case, we can rewrite h(c, X) = h(c', X') as the following equation:\nf(c) + (1+e) \\sum_{x \u2208 X}f(x) = f(c\u2019) + (1+e) \\sum_{x \u2208 X\u2019}f(x)\nTheorem 3.2. Let G = (V, E) be a graph with node embeddings c\u1d65 for nodes v \u2208 V. A GT layer on the dependency graph G' = (V, E') can map nodes v\u2081, v\u2082 \u2208 V to different embeddings only if the 1-WL algorithm using E' assigns different labels to nodes v\u2081 and v\u2082. For equivalence, we need d (in the definition of GTs) to be injective and A_{u,v} = c for a given constant c \u2208 R and all (u, v) \u2208 \u0395', making the GT as expressive as the 1-WL algorithm.\nProof of Theorem 3.2. First, we show that a GT is bounded by 1-WL on the same topology by showing that 1-WL is at least as powerful as a graph transformer. As 1-WL hashes all neighbor states with an injective function, we can observe states from all nodes in the graph in the aggregated multiset at node v, including possible edge labels. This information is sufficient to compute the result of the attention module at every node.\nFor the other direction, we can make use of Lemma 3.1 by setting c to the desired and follow the same proof as (Xu et al., 2018). Note that and 8 have to be powerful enough such that we can apply the universal approximation theorem."}, {"title": "TESTED GNN ARCHITECTURES", "content": "Message Passing Neural Networks (MPNN). For message passing neural networks", "follows": "nx_i^{(l+1)} = f_{G-GCNN} \\big("}]}