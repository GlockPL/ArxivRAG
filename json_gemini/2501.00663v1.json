{"title": "Titans: Learning to Memorize at Test Time", "authors": ["Ali Behrouz", "Peilin Zhong", "Vahab Mirrokni"], "abstract": "Over more than a decade there has been an extensive research effort of how effectively utilize recurrent models and\nattentions. While recurrent models aim to compress the data into a fixed-size memory (called hidden state), attention allows\nattending to the entire context window, capturing the direct dependencies of all tokens. This more accurate modeling\nof dependencies, however, comes with a quadratic cost, limiting the model to a fixed-length context. We present a new\nneural long-term memory module that learns to memorize historical context and helps an attention to attend to the\ncurrent context while utilizing long past information. We show that this neural memory has the advantage of a fast\nparallelizable training while maintaining a fast inference. From a memory perspective, we argue that attention due to its\nlimited context but accurate dependency modeling performs as a short-term memory, while neural memory due to its\nability to memorize the data, acts as a long-term, more persistent, memory. Based on these two modules, we introduce\na new family of architectures, called Titans, and present three variants to address how one can effectively incorporate\nmemory into this architecture. Our experimental results on language modeling, common-sense reasoning, genomics,\nand time series tasks show that Titans are more effective than Transformers and recent modern linear recurrent models.\nThey further can effectively scale to larger than 2M context window size with higher accuracy in needle-in-haystack tasks\ncompared to baselines.", "sections": [{"title": "Introduction", "content": "ransformers, pure attention-based architectures (Vaswani et al. 2017), have been firmly established as state-of-\nthe-art models in sequence modeling, mainly due to their in-context learning and ability to learn at scale (Kaplan\net al. 2020). The primary building blocks of Transformers-attention modules-function as associative memory\nblocks (Bietti et al. 2024), where they learn to store key-value associations and retrieve them by computing pairwise\nsimilarity between queries (i.e., search signals) and keys (i.e., contexts). Accordingly, by design, the output of a Transformer\nis exclusively conditioned on the direct dependencies of tokens in the current context window. This accurate modeling of\ndependencies, however, comes with quadratic time and memory complexity in terms of the context length. In complex\nreal-world tasks (e.g., language modeling (N. F. Liu et al. 2024), video understanding (C.-Y. Wu et al. 2019), long-term time\nseries forecasting (H. Zhou et al. 2021)), the context window can become extremely large, making the applicability of\nTransformers challenging in these downstream tasks.\nTo overcome the scalability issue of Transformers, recent studies aim to design different variants of linear Transform-\ners (Kacham, Mirrokni, and P. Zhong 2024; Katharopoulos et al. 2020; S. Yang, B. Wang, Shen, et al. 2024), where softmax is\nreplaced by a kernel function in the attention (see \u00a72.1 for details), resulting in a significant drop in memory consumption.\nDespite efficiency and the ability to scale to longer context, linear Transformers do not show competitive performance\ncompared to Transformers as the kernel trick makes the model a linear recurrent network, in which the data is compressed\ninto a matrix-valued states (Katharopoulos et al. 2020). This, however, brings a contradictory fact about linear recurrent (or\nlinear Transformers) models: On one hand, we use these linear models to enhance scalability and efficiency (linear vs.\nquadratic complexity), whose advantages is appeared for very long context; On the other hand, a very long context cannot\nbe properly compressed in a small vector-valued or matrix-valued states (S. Wang 2024)."}, {"title": "Memory Perspective", "content": "Memory is a fundamental mental process and is an inseparable component of human learning (Terry 2017). Without\na properly functioning memory system, humans and animals would be restricted to basic reflexes and stereotyped\nbehaviors. Accordingly, memory has been the inspiration for many seminal research in machine learning literature; e.g.,\nHopfield Networks (Hopfield 1982), LSTMs (J\u00fcrgen Schmidhuber and Hochreiter 1997), and Transformers (Vaswani et al.\n2017).\nTaking inspiration from the common definitions of memory and learning in neuropsychology literature (Okano, Hirano,\nand Balaban 2000), most existing architectures consider memory as a neural update caused by an input, and define learning\nas a process for acquiring effective and useful memory, given an objective. In this perspective, Recurrent Neural Networks\n(RNNs) (Williams and Zipser 1989) can be defined as models with a vector-valued memory module M (also called hidden\nstate) with two main steps: Given a new input x\u2081 at time t, the model (1) updates the memory using a function f (Mt-1, xt)\n(with compression); and (2) retrieves the corresponding memory of input using a function g(Mt, xt) (see \u00a72.1 for details).\nSimilarly, Transformers can be seen as architectures with a growing memory and two similar steps. That is, the pair of key\nand value matrices acts as the model's memory, and the model: (1) updates the memory by appending the key and value to\nthe memory (without compression), and (2) retrieves query vectors' corresponding memory by finding the similarity of\nquery and key vectors, which is then used to weight the value vectors for the output.\nThis perspective, can help us better understand existing paradigms, their critical differences, and design more effective\narchitectures. For example, the main difference between Transformers (Vaswani et al. 2017) and linear Transform-\ners (Katharopoulos et al. 2020) is the memory structure as well as the memory updating step, in which linear Transformers\ncompress the historical data into a fixed-size matrix-valued memory while Transformers keep all historical data (within\nthe context length) without any compression. While both linear Transformers and linear RNNs (including state space\nmodels) compress the information in memory update step, the critical difference lies in the structure of the memory,\nwhere linear RNNs (vs. linear Transformers) use a vector-valued memory (vs. matrix-valued memory). Therefore, this\nperspective motivates us to ask: (Q1) What constitute a good structure for the memory? (Q2) What is a proper memory\nupdate mechanism? and (Q3) What is a good memory retrieval process?\nRevisiting our understanding of human memory, it is neither a unitary process nor it serves a single function (Cowan\n2008). In fact, memory is a confederation of systems-e.g., short-term, working, and long-term memory-each serving a\ndifferent function with different neural structures, and each capable of operating independently (Willingham 1997). This\nfact motivates us to ask: (Q4) How to design an efficient architecture that incorporates different interconnected memory\nmodules. Finally, storing a memory is a neural process that requires to encode and store the abstraction of the past. It can\nbe over-simplification to assume a single vector or a matrix, whose parameters are encoding the data in a linear manner,\nare enough for storing long-term history. (Q5) Is a deep memory module needed to effectively store/remember long\npast?"}, {"title": "Contributions and Roadmap", "content": "In this paper, we aim to answer the above five questions by designing a long-term neural memory module, that can\nefficiently and effectively learn to memorize at test time. Building upon its design, we discuss how it can be incorporated\ninto an architecture.\nNeural Memory (\u00a73). We present a (deep) neural long-term memory that (as a meta in-context model) learns how to\nmemorize/store the data into its parameters at test time. Inspired by human long-term memory system (Mandler 2014),"}, {"title": "Learning to Memorize at Test Time", "content": "o overcome the lack of long-term memory and to enable the model to learn, forget, and retrieve information, in\nthis section, we present a neural long-term memory module, which is a meta models that learns to memorize at\ntest time. In Section 3.1, we first discuss the motivation and the design of the neural memory. In Section 3.2, we\ndiscuss how our architecture design can benefit from a fast and parallelizable training. Finally, in Section 3.3, we augment\nour architecture using persistent memory module, in which we use learnable but data-independent parameters to learn\nmeta information about the task."}, {"title": "Long-term Memory", "content": "To design a neural long-term memory module, we need a model that can encode the abstraction of the past history into its\nparameters. An example of this can be LLMs that are shown to be memorizing their training data (Leybzon and Kervadec\n2024; Schwarzschild et al. 2024; Staab et al. 2024). Therefore, a simple idea is to train a neural network and expect it to\nmemorize its training data. Memorization, however, has almost always been known as an undesirable phenomena in\nneural networks as it limits the model generalization (Bayat et al. 2024), causes privacy concerns (Staab et al. 2024), and\nso results in poor performance at test time. Moreover, the memorization of the training data might not be helpful at test\ntime, in which the data might be out-of-distribution. We argue that, we need an online meta-model that learns how to\nmemorize/forget the data at test time. In this setup, the model is learning a function that is capable of memorization, but it\nis not overfitting to the training data, resulting in a better generalization at test time.\nLearning Process and Surprise Metric. The key idea to train a long-term memory is to treat its training as an online\nlearning problem, in which we aim to compress the past information x1, . . ., Xt\u22121 into the parameters of our long-term\nneural memory module Mt. As discussed earlier, an event that violates the expectations (i.e., is surprising) is more\nmemorable for humans (Mandler 2014). Inspired by this, a simple definition of surprise for a model can be its gradient with\nrespect to the input. The larger the gradient is, the more different the input data is from the past data. Accordingly, using\nthis surprise score, we can update the memory as:\n\u039c\u2081 = Mt\u22121\u22120t \\nabla l(Mt-1;xt).", "latex": "\\\u039c\u2081 = Mt\u22121\u22120t \\nabla l(Mt-1;xt)."}, {"title": "How to Parallelize the Long-term Memory Training", "content": "As discussed above, the design of our long-term memory module is equivalent to training a meta model by optimizing\nassociative memory loss function l(Mt\u22121;xt) = ||Mt-1 (kt) \u2013 vt||2 using gradient descent with momentum and weight\ndecay. Therefore, in theory, the training of long-term memory module requires O (N) FLOPs, where N is the sequence\nlength. However, in practice, we need to parallelize the training process and to fully take advantage of hardware accelerators\n(e.g., TPUs, GPUs), we need to tensorize the process and use more matmuls.\nNext, we show that calculating the weights in the inner loop with mini-batch gradient descent, data-dependent learning\nrate, and weight decay can be reformulated so that it uses only matmuls and sum. We build upon the work of Yu Sun et al.\n(2024) that shows forward pass of a model optimizing with the mini-batch gradient descent (with constant learning rate)\ncan be calculated using matmuls. We can split the sequence into chunks of size b \u2265 1, and write the mini-batch gradient\ndescent as:\nMt = (1 \u2013 \u03b1\u03c4) Mt-1 \u2013 Ot\u2207l(Mt\u22121;xt) = Bt Mo \u2212 \u03a3\u03b8 Pte(Mr; xi),\nwhere t' = t \u2013 mod(t, b), and \u03b2\u2081 = \u03a0=1(1 \u2212 aj). For the sake of simplicity, we focus on the first chunk, i.e., t = b and so\nt' = 0. Also, we explain the process for the case that M\u2081 = W\u2081 is linear. The process for MLPs with Np \u2265 2 is similar. Using\nour loss function, we have:\n\u2207l(Wo; xt) = (Woxt - xt)x+ \u21d2 SoPe (Wo; xi) = \u0398\u266dB\u266d(W\u2030X \u2013 X)XT,\nwhere O\u2081 = diag ([01 ... Ob]) and Bb is defined analogously on Ps. Note that, we do not need to store all Orb and\nBkb for k = 1, . . ., N/b, instead, we store these matrices for each chunk, resulting in using less memory. Next, we extend\nthis representation so we can also incorporate the momentum term. In a chunk wise gradient descent with momentum, if\nwe look at the momentum term, we have:\nSt = ntSt-1-0 Ut,\nwhere ut Vl (M\u2081; xt). Note that, we can compute all ut at the same time, and so Equation 18 is a linear recurrence\nwith ut as an input, St as the hidden state, and nt as input-dependent transition value. Accordingly, we can use parallel\nassociative scan (J. T. Smith, Warrington, and Linderman 2023) to calculate Sts in this chunk.\nParameters as the Function of Chunks. Instead of making parameters like a\u2081, \u03b8t, and nt input-dependent (i.e., a function\nof token xt), we can make them functions of their chunk. Despite losing expressive power, this formulation can help to\nmake the training even faster. In this case, we are using the same value for each of a, \u03b8, and \u03b7 in each chunk. Accordingly,\nin Equation 17, we can store using a single scaler. Similarly we can make Equation 18 faster. That is, when \u03b7 and \u03b8 are\nlearnable but time-invariant inside each chunk, this equation becomes a linear time-invariant system (LTI), which can be\ncomputed by a global convolution (Gu, Goel, and Re 2022). In our experiments, we make these parameters as the functions\nof tokens. However, such simplifications (i.e., as the function of chunks) can be the interest of future work to training\nlarger models in more efficient manner."}, {"title": "Persistent Memory", "content": "Our long-term memory can also be seen as a contextual memory, meaning that the output is fully depend on the context.\nTherefore, in addition to our long-term memory, we also use a set of learnable but input-independent parameters to act as\ntask-related memory. This type of memory has been referred to as persistent or meta-memory in the literature (X. Dong\net al. 2024; Sukhbaatar, Grave, et al. 2019). Given Np \u2265 1, we use learnable parameters P = [P1 P2 ... PN] and\nappend it to the start of our sequence: i.e., given a context window size of N, we modify the input as:\nXnew = [P1 P2 ... PNp] || x,\nwhere || is concatenation. Next, we discuss the motivation of persistent memory from three perspective:\nMemory Perspective. As discussed earlier, our neural long-term memory is a contextual memory, in which all parameters\nare input-dependent. An effective memory system, however, also needs input-independent parameters to store the\nabstraction of the task knowledge. That is, mastering a task requires the memorization of the knowledge that how the task\ncan be done, and these parameters are responsible for storing such knowledge.\nFeedforward Network Perspective. In the Transformer architectures, there are fully connected layers after the attention\nmodule, which are shown to be similar to attention weights but with data-independent parameters. That is, Sukhbaatar,\nGrave, et al. (2019) showed that replacing the ReLU in fully connected layers with Softmax can results in an attention-like\nweights, in which weights are data-independent:\nFFN(x) = Wy Softmax (WKx).\nIn fact, WK and Wy are acting similar to K and V matrices in attention module when they are input-independent. The\npersistent memory weights are expected to have the same functionality, meaning that using them in the first part of the\nsequence leads to having input-independent attention weights (Sukhbaatar, Grave, et al. 2019).\nTechnical Perspective. Attention with causal mask has implicit bias toward initial tokens in the sequence, and so attention\nweights are almost always highly active for initial tokens, resulting in performance damage. From the technical perspective,\nthese learnable parameters at the start of the sequence can mitigate such effect by redistributing the attention weights\nmore effectively (Han et al. 2024; Xiao et al. 2024)."}, {"title": "How to Incorporate Memory?", "content": "n important question that remained unanswered is: How one can effectively and efficiently incorporate the\ndesigned neural memory into a deep learning architecture? As discussed earlier, from a memory perspective,\nthe pair of K and V matrices in transformers can be interpreted as an associative memory block. Due to their\naccurate modeling of dependencies and so their limited context window, we interpret them as short-term memory modules,\nattending to the current context window size. On the other hand, our neural memory with the ability to continuously\nlearn from data and store it in its weights can play the role of a a long-term memory. In this section, we aim to answer\nthe above question by proposing three different variants of Titans. Later in our experiments, we show that each of these\nvariants has its own advantages/disadvantages and also can show a trade-off between the efficiency and effectiveness in\nvery long-contexts."}, {"title": "Memory as a Context", "content": "In the first architecture design (see Figure 2), we treat the memory as a context to the current information. That is, given\na long sequence x \u2208 RN\u00d7din, we first chunk the sequence into fixed-size segments S(i) for i = 1,...,N/C. Given the\nincoming segment S(t), we consider it as the current context and its past segment as the historical information. Therefore,\nlet Mt\u22121 be the state of long-term memory before segment S(t), we use the input context as the query to the memory\nMt-1 to retrieve the corresponding information from the long-term memory. That is, we retrieve the past information that\ncorresponds to S(t) as:\nht = M-1(qt),\nwhere qt = S(t) Wo. Next, we use this historical information along with our persistent memory parameters as the input\nsequence to the attention module:\n5(t) = [P1 P2 ... PNp] || ht || S(t),\nYt = Attn (5).\nThe structure of the attention map over the entire sequence is shown in Figure 3a. We then use yt to update the long-term\nmemory module for the next segment and the final output:\nMt = Mt-1 (Yt),\nOt = Yt M (yt).\nNote that, in the above, we are updating the weight of Mt-1 through forward pass.\nThis architecture has two key advantages: (1) Attention by having both historical and current context, has the ability to\ndecides whether given the current data, the long-term memory information is needed. (2) The attention module helps", "latex": "ht = M-1(qt).\\\\\n5(t) = [P1 P2 ... PNp] || ht || S(t).\\\\\nYt = Attn (5).\\\\\nMt = Mt-1 (Yt).\\\\\nOt = Yt M (yt)."}, {"title": "Gated Memory", "content": "In the next variant (see Figure 4), in one branch, we directly use the input data to update the long-term memory, and in the\nsecond branch, we use a sliding window attention (SWA):\nx = [P1 P2 ... PNp] || x,\ny = SW-Attn* (x),\no = y \u2297 M(x),\nwhere SW-Attn* is sliding window attention with prefix (see Figure 3b). Note that, contrary to the previous design, we are\nnot segmenting the input data. Also, we abuse the notation and use M(x) to refer to the final output of the memory after\nall recursion over the tokens of the sequence. In the above equation, \u2297 can be any non-linear gating. In our experiments,\nwe normalize the outputs y and M(x) using learnable vector-valued weights, followed by a non-linearity \u03c3(.).\nThe overall attention mask of this design is shown in Figure 3b. In this design, sliding window attention is act as a precise\nshort-term memory, while the neural memory module is acting as a fading memory for the model. This architecture design\ncan also be seen as a multi-head architecture where the structure of heads are different (X. Dong et al. 2024)."}, {"title": "Memory as a Layer", "content": "The last variant uses the neural Memory As a Layer (MAL) of a deep neural network (see Figure 5). This architecture\ndesign is more common in the literature, where the hybrid models stack recurrent models with full or sliding window\nattentions. Given input x, we have:\nx = [P1 P2 ... PNp] || x,\ny = M(x),\no = SW-Attn (y),", "latex": "x = [P1 P2 ... PNp] || x,\\\\\ny = M(x),\\\\\no = SW-Attn (y),"}, {"title": "Architectural Details", "content": "For the sake of simplicity and presentation, we avoid discussing the implementation details like using residual connection,\ngating with linear layer, and normalization. In all blocks, we use residual connections. In our implementation, we use\nSiLU(.) activation (Elfwing, Uchibe, and Doya 2018) as the non-linear activation for computing query, key, and values and\nnormalize queries and keys using l2-norm.\nConvolution. Following the recent modern linear recurrent models (Gu and Dao 2024; S. Yang, Kautz, and Hatamizadeh\n2024), we incorporate a 1D depthwise-separable convolution layer after each of the query, key, and value projections.\nWhile not significantly affect the performance, these 1D convolutions have shown performance improvement and are also\ncomputationally efficient.\nGating. We also follow the recent architectures that use normalization and gating with a linear layer before the final\noutput projection (Mehta et al. 2023).\nTheorem 4.1. Contrary to Transformers, diagonal linear recurrent models, and DeltaNet, all of which are limited to TC\u00ba (Merrill,\nPetty, and Sabharwal 2024), Titans are capable of solving problems beyond TC \u00ba, meaning that Titans are theoretically more\nexpressive than Transformers and most modern linear recurrent models in state tracking tasks."}, {"title": "Experiments", "content": "ext, we evaluate the performance of Titans and its variants in language modeling, commonsense reasoning, needle\nin haystack, DNA modeling, and time series forecasting tasks\u00b9. In more details, in this section, we answer the\nfollowing empirical questions: (1) How do Titans perform compared to baselines in downstream tasks? (see \u00a75.2,\n\u00b9In the first version of the work, we aim to provide insights/evidences about why the learning paradigms of Titans are effective. We are working on\nfinalizing the results of larger models and will report them in the next version."}, {"title": "Experimental Setup", "content": "Models. In our experiments, we focus on the three variants of Titans, which we refer to as: Titans with (1) Memory as a\nContext (MAC), (2) Memory as a Gate (MAG), and (3) Memory as a Layer (MAL) as well as (4) neural memory module\nalone. The reason behind using our long-term memory as a separate module is based on our definition of learning. As\ndiscussed in Section 1, we define learning a process for acquiring effective and useful memory. Accordingly, we expect our\nlong-term memory to effectively learn from data, even without attention. For each of these models, we consider four scales\nwith: (i) 170M, (ii) 340M, (iii) 400M, and (iv) 760M parameters. While the first three are trained on 15B tokens sampled\nfrom FineWeb-Edu dataset (Penedo et al. 2024), the last one is trained on 30B tokens from the same dataset.\nBaselines. We compare our models with the state-of-the-art linear recurrent models, Transformers, and hybrid models\n(recurrent + attention). More specifically in language tasks, we compare with Transformer++ (Touvron et al. 2023),\nRetNet (Yutao Sun et al. 2023), Gated Linear Attention (GLA) (S. Yang, B. Wang, Shen, et al. 2024), Mamba (Gu and Dao\n2024), Mamba2 (Dao and Gu 2024), DeltaNet (S. Yang, B. Wang, Yu Zhang, et al. 2024), TTT (Yu Sun et al. 2024), and Gated\nDeltaNet (S. Yang, Kautz, and Hatamizadeh 2024). In needle in haystack tasks, we also compare with GPT4 (Achiam et al.\n2023), Llama3 with RAG (Touvron et al. 2023), RecurrentGemma2-9B (Botev et al. 2024), and Mistral (Jiang et al. 2023)\nmodels, all of which are provided in the benchmark (Yuri Kuratov et al. 2024). In time series tasks, we compare with\nMamba-based (Behrouz, Santacatterina, and Zabih 2024), Transformer-based (Y. Liu et al. 2023; Nie et al. 2022; Yunhao\nZhang and Yan 2023), and linear models (Das et al. 2023; Z. Li et al. 2023; H. Wu et al. 2023; Zeng et al. 2023).\nTraining. In the training, we follow the training procedure of S. Yang, Kautz, and Hatamizadeh (2024), and use LLama 2\ntokenizer with a vocabulary size of 32K and use training length of 4K tokens. We employ AdamW optimizer with learning\nrate of 4e-4 with cosine annealing schedule with batch size of 0.5M tokens, and weight decay of 0.1."}, {"title": "Language Modeling", "content": "We first focus on the perplexity in language modeling and also commonsense reasoning tasks. The results for Titans'\nvariants and also baselines with three different sizes of 340M, 400M, and 760M are reported in Table 1. Among non-hybrid\nmodels, including Transformer++, our neural memory module achieves the best performance in both perplexity and\naccuracy measures. Comparing our neural memory module and TTT, which is also a gradient-based recurrent model can\nshow us the importance of our weight decay as well as the momentum. As discussed earlier, the weight decay can be\ninterpreted as a gating mechanism to forget the past data, when it is needed. Also, momentum can help us better manage\nthe memory by providing additional memory for the surprise metric. While some baselines also take advantage of gating\nmechanism, e.g., Mamba, Mamba2, and Gated DeltaNet, the superior performance of our neural memory module shows\nthe importance of both our surprise mechanism and having deep and non-linear memory. We further discuss the later in\nSection 5.5.\nComparing the hybrid models, we found that all three variants of Titans (MAC, MAG, and MAL) outperform both Samba\n(Mamba + attention) and Gated DeltaNet-H2 (Gated DeltaNet + atttention). We attribute the superior performance of Titans\n(MAL) to the power of neural memory module as the architecture design and used attention are all the same. Comparing\nTitans (MAG) and (MAC), we find that while their performance are close, MAC performs better when dealing with longer\ndependencies in the data. Interestingly, both MAG and MAC outperform MAL variant, which due to using the same\nmodules, we attribute this to the architecture design of these models. This finding is particularly important as the current\nhybrid models (except Hymba (X. Dong et al. 2024)) in the literature are using MAL-style combination of recurrent models\nand attention."}, {"title": "Needle in a Haystack", "content": "Scaling a model to longer context window is not always equivalent to being effective for very long sequences (Hsieh\net al. 2024). The needle-in-a-haystack (NIAH) task is designed to measure the actual effective context length of models.\nIn this task, we evaluate the model on retrieving a piece of information (i.e., the \u201cneedle\u201d) from long distractor texts (i.e.,"}, {"title": "BABILong Benchmark", "content": "In the previous section we discussed the results on a simple NIAH tasks where a single needle needs to be retrieved.\nAlthough Titans showed better performance compared to baselines, their true advantage over very long sequences is still\nhidden. To this end, in this section, we use a harder task from BABILong benchmark (Yuri Kuratov et al. 2024), in which\nthe model needs to reason across facts distributed in extremely long documents. We follow the original experimental setup\nand training process in the benchmark. There are two settings: (1) Few-shot setting, in which we use large pre-trained\nmodels, and (2) fine-tuning setting, where we fine-tune the MAC variant of Titans to compare it with other fine-tuned\nbaselines. The results for few-shot setting are reported in Figure 6a. In this setup, we can see Titans outperform all\nbaselines-i.e., Mamba2.8B (Gu and Dao 2024), RWKV-6-7B (Peng, Goldstein, et al. 2024), RecurrentGemma-9B (Botev et al.\n2024), Gemma-9B (Team et al. 2024), Llama3.1-8B (Touvron et al. 2023), GPT-4, and GPT40-mini (Achiam et al. 2023). These\nresults are achieved while Titans (MAC) is having much less number of parameters than baselines.\nIn the fine-tuning setup, we compare the small fine-tuned version of Titans (MAC) with: (i) the fine-tuned version of small\nmodels (almost the same number of parameters as Titans) such as Mamba (Gu and Dao 2024), RMT (Bulatov, Yury Kuratov,\nand Burtsev 2022), (ii) large models with Retrieval-Augmented Generation (RAG) (P. Lewis et al. 2020) such as Llama3.1-\n8B (Touvron et al. 2023), and (iii) extremely large models such as GPT-4 (Achiam et al. 2023), GPT40-mini, Qwen2.5-72B (A.\nYang et al. 2024), and Llama3.1-70B (Touvron et al. 2023). Baseline results are reported by (Yuri Kuratov et al. 2024). The\nresults of Titans and baselines are reported in Figure 6b. Titans outperform all models even extremely large models like\nGPT4. Also, compared to Transformer-based with memory models like RMT, Titans show better performance mainly due\nto their powerful memory. That is, RMT compress the historical data into 16 size vector-valued memory, while Titans with\nin-context online memory learner are capable of encoding the past into the parameters of the model. Interestingly, even"}, {"title": "The Effect of Deep Memory", "content": "In this section, we evaluate the effect of deep memory in both wall-clock training time and model performance\u00b2. To this\nend, we focus on different variants of our neural memory module, where LM = 1, 2, 3, 4. We also use Mamba as a baseline\nfor the model performance. For a fair comparison, we use the same training process for all models and train them on a\nsubset of the Pile dataset (L. Gao et al. 2020).\nWe report the perplexity of our models and baselines as the function of the sequence length in Figure 7. Interestingly, with\nthe increase of memory depth, LM, the model can achieve better perplexity over all sequence length. Also, deeper memory\nmodules are more robust to the sequence length when the model has less number of parameters. With the increase of the\nnumber of parameters, all models show better performance on longer sequences.\nWe also evaluate the effect of memory depth (LM = 1, 2, 3, 4) on the training\nthroughput. We report the training throughput (the number of tokens per\nsecond) as the function of sequence length in Figure 8. All models scale linearly\nwith respect to the context length (i.e., constant trend in the number of tokens\nper second with respect to sequence length). Also, by increasing the memory\ndepth, as expected, we can see a linear trend that a deeper memory results in\na slower training. Therefore, it is not always efficient to use deeper memory\nmodules, showing a trade-off between effectiveness and efficiency."}, {"title": "Time Series Forecasting", "content": "To show the effectiveness of our memory module in a broader tasks, we also evaluate its performance in time series\nforecasting tasks. To this end, we use Simba framework (Patro and Agneeswaran 2024) for time series forecasting, and"}, {"title": "DNA Modeling", "content": "In order to understand the capability of Titans beyond natural language, we further evaluate the performance of our\nneural memory module on DNA modeling tasks. To this end, we evaluate pre-trained models on the downstream tasks\nin GenomicsBenchmarks (Gre\u0161ov\u00e1 et al. 2023). We follow the same experimental setups from Nguyen et al. (2024), and\nre-use the reported results of baselines by Arora et al. (2024). The performance of Titans (LMM"}]}