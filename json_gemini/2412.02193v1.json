{"title": "LAYOUTVLM: Differentiable Optimization of 3D Layout via Vision-Language Models", "authors": ["Fan-Yun Sun", "Weiyu Liu", "Siyi Gu", "Dylan Lim", "Goutam Bhat", "Federico Tombari", "Mangling Li", "Nick Haber", "Jiajun Wu"], "abstract": "Open-universe 3D layout generation arranges unlabeled 3D assets conditioned on language instruction. Large language models (LLMs) struggle with generating physically plausible 3D scenes and adherence to input instructions, particularly in dense scenes. We introduce LAYOUTVLM, a framework and scene layout representation that exploits the semantic knowledge of Vision-Language Models (VLMs) and supports differentiable optimization to ensure physical plausibility. LAYOUTVLM employs VLMs to generate two mutually re-inforcing representations from visually marked images, and a self-consistent decoding process to improve VLMs spatial planning. Our experiments show that LAYOUTVLM addresses the limitations of existing LLM and constraint-based approaches, producing physically plausible 3D layouts better aligned with the semantic intent of input language instructions. We also demonstrate that fine-tuning VLMS with the proposed scene layout representation extracted from existing scene datasets can improve performance.", "sections": [{"title": "1. Introduction", "content": "Spatial reasoning and planning involve understanding, arranging, and manipulating objects in 3D space within the constraints of the physical world. These skills are essential for autonomous agents to navigate, plan tasks, and physically interact with objects in complex environments. Automatically generating diverse, realistic scenes in simulation has become crucial for scaling up data to train autonomous agents with enhanced spatial reasoning abilities. In this paper, we advance this goal by addressing open-universe layout generation, which involves creating diverse layouts based on unlabeled 3D assets and free-form language instructions.\nTraditional scene synthesis and layout generation methods are often constrained by predefined object categories and patterns of object placements learned from synthetic scene datasets [1-3], preventing them from achieving the diversity of scene layouts seen in the real world. Recent methods leverage Large Language Models (LLMs) for open-universe layout generation by utilizing the spatial common-sense embedded in language and program code. However, a key challenge is to achieve both physical plausibility and semantic coherence, as illustrated in Fig. 1. Methods that predict numerical object poses (e.g., LayoutGPT [4]) often produce layouts with object collisions or out-of-bound placements. Other methods, such as Holodeck [5], attempt to improve physical plausibility by predicting spatial relations between assets and solving a constraint optimization problem. However, these approaches either struggle to find feasible solutions for scenes with large numbers of objects or output layouts that lack the semantic nuances specified in the language instructions.\nIn this paper, we introduce LAYOUTVLM, an open-universe layout generation method that effectively achieves both physical plausibility and semantic alignment. Our approach leverages the complementary nature of numerical object poses and spatial relations, combining them within a differentiable optimization framework to enable robust layout generation. More specifically, LAYOUTVLM first predicts numerical object poses as initialization for the optimization process. Then, LAYOUTVLM jointly optimizes for physics-based objectives and spatial relations, each with their corresponding differentiable objectives. The physics-based objectives ensure physical plausibility, while the spatial relations preserve the overall layout semantics during the optimization process. To improve VLM capabilities for spatial grounding, LAYOUTVLM uses visually marked images, allowing accurate estimation of object placements within the scene, especially when existing objects exist. We also introduce a self-consistency decoding process that allows LAYOUTVLM to focus on the semantically meaningful spatial relations during optimization. The synergy between numerical values and spatial relations, regulated through self-consistent decoding, ensures accurate and coherent scene layouts.\nOur contributions are as follows: first, we introduce a novel scene layout representation that can be combined with differentiable optimization to generate diverse layouts. The scene representation builds on two complementary representations-numerical pose estimates and spatial relations with matching differentiable objectives. Second, we show that we can use VLMs and a self-consistency decoding process to generate our scene layout representation using visually marked scene and asset renderings. Third, through systematic evaluation across 11 room types, we achieved significant improvements when compared to the current best-performing method. Fourth, we show that fine-tuning open-source models on our scene representation with synthetic data yields substantial performance improvements, even for models that struggle with 3D layout generation."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Layout Generation & Indoor Scene Synthesis", "content": "Recent advances in indoor scene synthesis have explored two main directions. One line of work leverages the strong generative priors of image generation models, often using Neural Radiance Fields (NeRFs) or Gaussian splats as the output representation [6-9]. However, these generated scenes lack separable, manipulable objects and surfaces, rendering them unsuitable for robotics applications where precise object interactions are required. Another line of research focuses on generating scenes using intermediate representations, such as scene graphs or scene layouts, combined with an asset repository [4, 5, 10-12]. The advent of Large Multimodal Models (LMMs) has enabled open-vocabulary 3D scene synthesis, supporting the flexible generation of scenes without dependence on predefined labels or categories [13, 14]. For example, LayoutGPT [4] prompt language models to directly generate 3D Layouts for indoor scenes. Holodeck [5] uses LLMs to generate spatial scene graphs and then uses the specified scene graph to optimize object placements. LAYOUTVLM also falls into this line of work, but instead of using LLMs, we generate scene layout representations from image and text inputs using VLMs. Additionally, we introduce a differential optimization process as opposed to solving a constraint satisfaction problem with search [5]."}, {"title": "2.2. Vision-Language Models for 3D Reasoning", "content": "Recent works have explored the spatial reasoning capabilities of vision-language models (VLMs). Some studies have trained 3D visual encoders on representations like point clouds and meshes to improve tasks such as 3D scene understanding, question answering, navigation, and planning [15-21]. Other research has adapted 2D VLMs for spatial reasoning by fine-tuning them on visual question-answering datasets involving metric and qualitative spatial relations grounded in 3D environments [22, 23]. A related direction is to reconstruct the 3D scene based on 2D images by training on large synthetic data [24]. However, these approaches primarily focus on perception tasks and do not extend to generating 3D structures. In contrast, our work uses 2D VLMS for the task of 3D layout generation, utilizing techniques originally developed for visual reasoning\u2014such as leveraging images of a scene from different viewing angles [25] and visual markers [26-28]-repurposed here for spatial planning. We also investigate fine-tuning VLMs for 3D layout generation tasks and observe significant improvements in the performance of open-source models."}, {"title": "3. Problem Formulation", "content": "The problem of open-universe 3D layout generation is to arrange any assets within a 3D environment based on natural language instructions. Formally, given a layout criterion $l_{layout}$ in natural language, a space defined by four walls oriented along the cardinal directions ${w_1,...,w_4}$, and a set of N 3D meshes ${m_1,...,m_N}$, the goal is to create a 3D scene that faithfully represents the provided textual description. Following prior work [5, 14], we assume that the input 3D objects are upright and an off-the-shelf vision-language model (VLM) (i.e., GPT-40 [29]) is employed to determine their front-facing orientations. The VLM also annotates each object with a short textual description $s_i$, and the dimensions of its axis-aligned bounding box after rotating to face the +x axis will be represented as $b_i \\in \\mathbb{R}^3$. The target output of layout generation is each object's pose $P_i = (X_i, Y_i, Z_i, \\theta_i)$, including the object's 3D position and rotation about the z-axis."}, {"title": "4. LAYOUTVLM", "content": "In this paper, we propose LAYOUTVLM, a method to generate physically plausible arrangements of unlabeled 3D assets based on open-vocabulary natural language descriptions of the desired layouts outlined in Figure 3. Our approach employs vision-language models (VLMs) to generate code for our proposed scene layout representation that specifies both an initial layout ${p_i}_{i=1}^N$, as well as a set of spatial relations between assets (and walls). This representation is then used to produce the final object placements through optimization:\n$\\arg \\min\\limits_{\\{P_i\\}_{i=1}^N} (\\mathcal{L}_{semantic} + \\mathcal{L}_{physics}),$\ninitial solution ${p_i}_{i=1}^N$.\nwhere $\\mathcal{L}_{semantic}$ is the objective function decoded from the scene layout representation and $\\mathcal{L}_{physics}$ is the objective func-"}, {"title": "4.1. Scene Layout Representation", "content": "To generate layouts from open-ended language instructions and 3D assets, a desirable representation must be semantically expressive for diverse language specifications and physically precise to ensure plausible 3D layouts. Our representation includes (a) numerical estimates of object poses ${p_i}_{i=1}^N$ and (b) spatial relations with differentiable objectives. Initial estimates provide a starting point for optimization, while differentiable objectives guide the process, addressing the challenge of directly predicting physically plausible layouts.\nThe initial layout is key in the optimization process, as poor initialization can lead to a suboptimal layout (e.g., a table separating the room into two halves where each chair is placed on opposite sides, when the language instruction instructs them to be placed on the same side). The spatial relations are crucial to ensure that the layout semantics are maintained while the layout is adjusted for physical plausibility. In Figure 2, we show an example of our scene representation.\nDifferentiable Spatial Relations. The goal of these spatial relations is twofold: (a) to capture the semantics of the input language instruction and (b) to preserve these semantics during optimization for physical plausibility. For example, consider the instruction, \u201cset up a dining table.\" A vision-language model might initially generate poses where the chairs overlap with the table. Our differentiable spatial relations are designed to adjust these poses during optimization-preventing overlaps-while maintaining the essential semantics, such as \"chairs should be positioned near the dining table in a dining setup.\" To design a set of spatial relations that can capture a wide range of semantics for indoor scenes, we devise five expressive spatial relations: two positional objectives (i.e., distance, on_top_of), two orientational objectives (i.e., align_with, point_towards), and one wall-related objective (i.e., against_wall) that pertains to both the position and orientation of an asset. Note that our spatial relations do not rely on a fixed reference frame, unlike classic spatial relations like in front_of and left_of. Each spatial relation corresponds to a differentiable objective function with optional parameters that VLMs can choose to impose on object poses; for example, a distance imposes a higher loss if objects are outside a specified distance range, and the VLMs can decide the lower and upper bound for this function."}, {"title": "4.2. Generating Scene Layout Representation with Vision-Language Models", "content": "Our approach utilizes the generalization and commonsense reasoning abilities of Vision-Language Models (VLMs) to generate the scene representation outlined above based on the objects, 3D scene, and language instructions. To improve the accuracy of the generated scene representation, we employ two techniques: visual prompting with coordinates and self-consistent decoding.\nVisual Prompting. Figure 3 illustrates our VLM-based scene representation generation process. The VLM's input includes rendered images of the 3D scene and individual asset views. Prior research has shown that visual cues can improve VLMs' object recognition and spatial reasoning [26]. We provide two types of visual annotations for layout generation: coordinate points in the 3D space spaced 2 meters apart to help the VLM gauge dimensions and scale and visualizations of coordinate frames to maintain consistent spatial references. We also annotate the front-facing orientations of each object with directional arrows, which is essential for generating rotational constraints like aligned_with or point_towards. In practice, we first use an LLM to group the assets given the textual descriptions $s_i$ of all the input assets to address context length limitations when handling many 3D assets. Then, we place assets in groups, one group at a time. Before generating each group, we re-render the 3D scene to help the VLM identify unoccupied areas, thus facilitating the physically valid placement of remaining assets.\nSelf-Consistent Decoding. One key challenge is that VLMs struggle with spatial planning; while they may successfully generate spatial relations for pairs of objects, they tend to fail at accounting for overall layout coherence. We hypothesize that self-consistent spatial relations (i.e., \"the spatial relations that are also satisfied in the estimated numerical poses of the objects\") represent the most critical semantics to preserve during optimization when the object poses are adjusted for better physical plausibility. Thus, we introduce self-consistent decoding for our scene layout representation. Different from standard self-consistency [30], which selects the most consistent answer from multiple reasoning paths following the same format, we require the two distinct but mutually reinforcing representations predicted by the VLM to self-consistent. That is, we only retain the spatial relations satisfied with the predicted poses. After self-consistent decoding, we can formally describe the semantic part of the optimization loss as:\n$\\mathcal{L}_{semantic} = \\sum\\limits_{\\mathcal{L} \\in \\mathcal{R}} 1 [\\mathcal{L}_i (p_i, p_j, \\lambda) \\leq \\epsilon] \\cdot \\mathcal{L}_i(p_i, p_j, \\lambda), (1)$\nwhere $p_i$ and $p_j$ are the initial estimated poses, $\\Lambda$ represents the additional parameters in the functions (refer to Table 1), and $\\epsilon$ is a threshold value for determining whether the differentiable spatial relation $\\mathcal{L}$ is satisfied in the initial estimates."}, {"title": "4.3. Differentiable Optimization", "content": "To generate a scene from the estimated object poses and differentiable constraints, we jointly optimize all objects according to the specified constraints over a set number of iterations. In addition to the spatial relational functions generated by the VLM, we impose Distance-IoU loss on objects' 3D oriented bounding box [31, 32] for collision avoidance:\n$\\mathcal{L}_{physics} = \\sum\\limits_{i=1}^N \\sum\\limits_{j=1 \\atop j\\neq i}^N \\mathcal{L}_{D I O U} (p_i, p_j, b_i, b_j). (2)$\nEq. 1 and Eq. 2 form the final objective function for the optimization problem. We use projected gradient descent (PGD) to optimize for this objective, projecting assets within the physical boundary every fixed number of iterations during the optimization. With the objective function, we can confine objects within the boundary and avoid unwanted intersections, ensuring physically valid layouts."}, {"title": "4.4. Finetuning VLMs with Scene Datasets", "content": "Our scene representation can model a wide variety of physically valid and semantically meaningful 3D layouts. Additionally, we can fine-tune VLMs to quickly adapt to this representation, enabling the generation of specific types of layouts. This scene representation can be automatically extracted from scene layout datasets without requiring manual annotations. Specifically, given a set of posed objects in a 3D scene, we apply the preprocessing procedure outlined in Section 3 to obtain both textual descriptions and oriented bounding boxes for each object. After canonicalizing the objects, we compute cost values for our defined spatial relations based on the ground-truth positions and orientations of the objects, using heuristic thresholds to determine whether each spatial relation is satisfied. The resulting scene representation includes both raw object poses and the satisfied spatial relations, which we then use to fine-tune VLMs to generate these scene representations from input objects and scene renderings. In our implementation, we use the 3D-Front dataset to extract training data for around 9000 rooms. Our approach is capable of identifying layout patterns in 3D scenes, such as a variable number of chairs around a table, nightstands positioned beside a bed, or an entertainment area comprising a TV, coffee table, and sofa. We investigate fine-tuning two VLMs for the layout generation task: the closed-source GPT-40 [29] and the open-source LLaVA-NeXT-Interleave [33]."}, {"title": "5. Experiments", "content": "In our experiments, we aim to answer the following three questions: Q1: Does LAYOUTVLM outperform existing methods on open-universe 3D layout generation? Q2: Are the proposed scene layout representation and VLM-based framework essential for generating prompt-aligned 3D layouts that are physically plausible? Q3: With limited data, can we improve upon pre-trained VLMs' ability to generate the proposed scene layout representation?"}, {"title": "5.1. Experimental Setup", "content": "Evaluation. In our evaluation, we assess models' abilities in 1) open-vocabulary language instructions, 2) predicting layouts for objects beyond predefined categories, and 3) generating accurate object placements within boundaries while avoiding collisions. We create test cases across 11 room types, with three rooms per type and up to 80 assets per room. Each test case includes human-verified, pre-processed 3D assets sourced from Objaverse [34], language instructions generated by GPT-4, and a floor plan defining space dimensions. All methods use the same pre-processed assets. Details on test case generation are in the appendix.\nEvaluation Metrics. We evaluate generated 3D layouts on physical plausibility and semantic coherence with respect to the provided language instructions. Physical plausibility is measured using the Collision-Free Score (CF) and In-Boundary Score (IB). All assets are enforced to be placed, with remaining assets randomly placed if a method fails. Semantic coherence is assessed using Positional Coherency (Pos.) and Rotational Coherency (Rot.), measuring alignment with the input prompt. To evaluate semantic coherence across layouts without groundtruth, we use GPT-40 to score layouts based on top-down and side-view renderings and the language instructions, leveraging its effectiveness as a human-aligned evaluator in text-to-3D generation [35]. We also introduce the Physically-Grounded Semantic Alignment Score (PSA) to assess physical plausibility and semantic alignment, assigning 0 if assets cannot be feasibly placed. Scores range from 0 to 100, with higher scores indicating better performance.\nBaselines. We evaluate our method against the following baselines: LayoutGPT [4], Holodeck [5], and I-Design [13], representing existing methods for open-universe layout generation."}, {"title": "5.2. Benchmark Performance", "content": "Our method achieves significantly improved performance over existing methods, as shown in Table 2. Averaging over 11 room types, it improves the PSA score by 40.8 compared to the best-performing baseline, I-Design. LayoutGPT generates layouts with high semantic coherence by having LLMs predict precise object placements but often produces physically infeasible layouts. Holodeck struggles to generate valid and accurate scenes due to its search-based approach, which becomes ineffective with the large number of assets and rigid constraints that fail to accommodate diverse language instructions. Notably, our approach significantly reduces objects placed outside room boundaries while maintaining high positional and rotational coherence."}, {"title": "5.3. Ablation Study", "content": "We conduct an ablation study to assess the key components of our approach. In w/o Visual, the VLM was replaced with an LLM. The ablation w/o Self-Consistency removes the step of validating predicted constraints with raw poses, while w/o Const. placed objects based solely on predicted poses, without performing optimization. As shown in Table 3, the results confirm the effectiveness of our method's design. Although w/o Visual still uses the same optimization process, which improves physical feasibility, we observe a clear advantage in using a VLM. This likely stems from the VLM's ability to leverage rendered scenes and asset views to enhance spatial planning (e.g., arranging groups of objects in distinct regions). The comparison with w/o Const. confirms that VLMs alone were insufficient; our scene representation-consisting of both numerical object poses and spatial relations-is essential for generating practical layouts. Finally, we observe that self-consistency takes advantage of our scene representation to help ensure that placements met both physical and semantic requirements."}, {"title": "5.4. Finetuning LayoutVLM", "content": "In this experiment, we investigate whether fine-tuning pretrained VLMs with our scene layout representation improves physical feasibility and semantic coherence. Specifically, we compare this approach with fine-tuning VLMs to directly predict numerical poses. Since the 3D-Front training data consist of typical household room types, we evaluated the fine-tuned models on test cases within the residential category (i.e., bedroom, living room, and dining room). As these test cases include unseen 3D assets from Objaverse and even new object categories, this experiment also assessed generalization capability.\nThe results in Table 4 show that: 1) fine-tuning with our scene representation is more effective than using direct numerical values, and 2) our approach significantly improves the performance of the open-source model. The relatively limited improvement in the closed-source model likely stems from its existing ability to leverage our scene representation through prompt instructions, as shown in previous experiments. In contrast, the open-source VLM struggles with zero-shot layout generation. Fine-tuning with our scene representation enables it to generate better layouts for residential room types with unseen objects."}, {"title": "6. Conclusion", "content": "In this paper, we present LAYOUTVLM, a comprehensive framework for open-universe 3D layout generation, including a novel scene layout representation that builds on two representations of layout from visually marked images: numerical pose estimates and spatial relations. With the scene layout representation combined with self-consistent decoding, differentiable optimization, and visual prompting, we demonstrated significant improvements over existing LLM and constraint-based approaches. While LAYOUTVLM shows promise, it has limitations, including occasional failures in generating valid layouts due to suboptimal VLM initializations. We hope our work paves the way for future research to explore more complex scenes and address these challenges, pushing the boundaries of 3D layout generation with improved semantic and physical reasoning."}]}