{"title": "Metron: Holistic Performance Evaluation Framework for LLM Inference Systems", "authors": ["Amey Agrawal", "Anmol Agarwal", "Nitin Kedia", "Jayashree Mohan", "Souvik Kundu", "Nipun Kwatra", "Ramachandran Ramjee", "Alexey Tumanov"], "abstract": "Serving large language models (LLMs) in production can incur substantial costs, which has prompted recent advances in inference system optimizations. Today, these systems are evaluated against conventional latency and throughput metrics (eg. TTFT, TBT, Normalised Latency and TPOT). However, these metrics fail to fully capture the nuances of LLM inference, leading to an incomplete assessment of user-facing performance crucial for real-time applications such as chat and translation. In this paper, we first identify the pitfalls of current performance metrics in evaluating LLM inference systems. We then propose Metron, a comprehensive performance evaluation framework that includes $fluidity-index$- a novel metric designed to reflect the intricacies of the LLM inference process and its impact on real-time user experience. Finally, we evaluate various existing open-source platforms and model-as-a-service offerings using Metron, discussing their strengths and weaknesses. Metron is available at github.com/project-metron/metron.", "sections": [{"title": "1 Introduction", "content": "The surge in popularity of LLMs has resulted in the proliferation of both proprietary model-as-a-service offerings [10, 2, 4, 5, 1] and active open-source developments aimed at optimizing LLM inference [14, 12, 6, 16]. Given the vast array of available options, a systematic comparison of these frameworks becomes critical to ensure good user experience and cost-effective deployment.\nCurrent evaluation metrics for LLM serving frameworks, such as TTFT (Time To First Token), TBT (Time Between Tokens), normalized latency, and TPOT (Time Per Output Token), fail to capture the full essence of the user experience in real-time LLM interactions. This paper demonstrates that these conventional performance metrics, while valuable, are inadequate and potentially misleading when applied to the dynamic, streaming nature of LLM inference. We argue for a more nuanced approach that considers the temporal aspects of token generation and their impact on perceived responsiveness and overall user satisfaction.\nFine-grained metrics like TTFT and TBT effectively capture latency for individual tokens and tail latency characteristics, however, they fail to represent the overall end-to-end token generation throughput. Conversely, normalized metrics such as TPOT and normalized latency attempt to measure token throughput, but fall short in identifying specific sources of user experience degradation, such as inter-token jitter or scheduling delays \u2013 which are similar to buffering time in conventional media streaming settings. This dichotomy highlights the need for a more comprehensive evaluation framework that concisely captures the overall user experience.\nTo address the limitations of existing metrics, we introduce Metron, a comprehensive framework for evaluating user-facing performance in LLM inference. At its core are two novel metrics: $fluidity-$"}, {"title": "2 Background", "content": "In this section, we describe the typical LLM inference process, commonly used metrics to characterize inference performance, and an overview of open-source and proprietary inference solutions."}, {"title": "2.1 LLM Inference Process", "content": "There are two distinct phases in LLM inference a prefill phase followed by a decode phase. During prefill phase, the user's input prompt is processed and first output token is produced. Next, during the decode phase, output tokens are generated one at a time, where the token generated in one step is passed through the model to generate a new token in the next step until a special end-of-sequence token is generated. Decode phase also requires access to KV (key and value) pairs associated with all previously processed tokens during its attention phase. Contemporary LLM inference systems store activations in KV-cache to avoid repeated re-computation during each step [30, 3, 25]."}, {"title": "2.2 Performance Metrics for LLM Inference", "content": "Conventional performance metrics for LLM inference performance are the following:\n\u2022 TTFT: Time To First Token (TTFT) [32, 7] is the latency between the request arrival and the first output token generated by the system for the request. It includes the scheduling delay (time elapsed from request arrival to start of prompt processing) and the prompt processing time. Minimizing TTFT is crucial for real-time interactions to maintain a responsive user experience. In contrast, longer TTFT is acceptable in offline or batch processing contexts.\n\u2022 TBT : Time Between Tokens (TBT) [17] is the latency of every subsequent token generation in the decode phase. This metric directly influences the perceived speed of the model by users. If we assume the average English reading speed is 250 words per minute then a TBT of roughly 6 tokens per second is required. Optimizing TBT enhances the user experience by ensuring rapid and fluid response generation.\n\u2022 TPOT : Time Per Output Token (TPOT) [32, 7] is closely related to TBT. It is the average time to generate an output token in the decode phase. It is calculated as the total decode time of a request normalized by the number of decode tokens generated."}, {"title": "2.3 LLM Inference Framework Evaluation", "content": "We now discuss what it means to evaluate the user-facing performance for LLM serving frameworks, in open-source [14, 17, 6, 13] as well as public model offerings [10, 4, 5, 2].\nOpen-source frameworks. Evaluating the performance of open-source frameworks like vLLM [14], Sarathi-Serve [17], LightLLM [6], Text-Generation-Inference [13], etc. is challenging due to their numerous configurable parameters. At the same time, accurate performance assessment is crucial during deployment to determine the maximum sustainable load for a given cluster while meeting specific latency targets (SLOs).\nProprietary model service offerings. Companies like OpenAI [10], Azure AI Studio [2], Fireworks AI [4], and Groq [5] provide model-as-a-service solutions that typically restrict end-user configurability regarding system performance. Consequently, users and developers are limited to passive performance evaluations based on their specific workload. In this constrained environment, performance comparisons across different services rely primarily on observable metrics such as latency and cost. This highlights the need for methods that can effectively guide users in selecting the most efficient and cost-effective service for their specific applications."}, {"title": "3 Motivation", "content": ""}, {"title": "3.1 Pitfalls of Existing Metrics", "content": "While the conventional latency and througput metrics described in \u00a72.2 appear adequate in evaluating the performance of LLM inference systems, they fail to provide a comprehensive view of the user experience. Below, we discuss specific shortcomings identified in the current metrics.\nTime To First Token (TTFT) is oblivious of prompt length. TTFT, which measures prefill efficiency, includes both the scheduling delay (which depends on the system load, routing policy, batching policy [30, 25, 17], etc.) and the actual prompt processing time which depends on the prompt length. Naively comparing two systems on their TTFT does not reveal the individual contribution of these components to the prefill time. Moreover, since TTFT is highly dependent on the prompt length (quadratic), as shown in Figure 1; defining a static Service Level Objective (SLO) on TTFT as a measure for user-facing responsiveness of the system is not practical. A naive alternative would be to normalize TTFT by the prompt length; but this normalizes the scheduling delay as well and would penalize shorter input requests disproportionately compared to longer ones.\nNormalized latency hides scheduling delay. Normalized latency normalizes the request end-to-end time by the total number of decode tokens (which is the visible output of the system). However, this ends up hiding specifics about metrics such as scheduling delay. For example, consider the example illustrated in Figure 2. Here, the scheduling delay is above 25s for almost 60% of the requests in VLLM, compared to Sarathi-Serve which has a maximum scheduling delay of 15s. However, the normalized latency for these systems differs only be a few hundred milliseconds! This is a result of the normalization by decode tokens (the median decode tokens in Arxiv-Summarization [18] is 228).\nTime Per Output Token (TPOT) and normalised latency hides jitters in token generation. Both these metrics normalise the latency by the number of decode tokens in the request. This normalization can mask the jitters that occur as intermittent stalls during token generation. As shown in Figure 3a, VLLM suffers a long stall of 10s (this can happen due to a long prefill request which is onboarded into the ongoing batch). While this will result in a very bad user experience, the impact of this stall on the TPOT or normalized latency metric will be numerically small due to the normalization by"}, {"title": "3.2 Desirable Properties of Evaluation Framework", "content": "Having identified the pitfalls of existing metrics in evaluating LLM serving frameworks, we articulate the essential attributes of an ideal evaluation metric. First, we need an evaluation framework that is blackbox (can evaluate any API endpoint), and workload agnostic (e.g., not impacted by variance in prompt lengths in the workload). Second, given the complexity of inferring system performance from a collection of metrics, there is a pressing need for a unified metric that not only simplifies analysis but also accurately reflects the user-facing performance of LLM serving systems, while incorporating the unique dynamics of the inference process. Lastly, the metric should comprehensively capture the frequency, duration, and timing of stalls within the system, addressing one of the most critical aspects affecting user experience."}, {"title": "4 Metron: Design and Implementation", "content": "Let us assume that we have the ideal TTFT and TBT for a given application based on some expectation on user behavior. The current TBT based SLO metrics treats each token generation independently, which may not capture the user experience well. For example, take a concrete example where the desired TBT is 100ms. Then, a system which produces 10 tokens at TBT of 10ms and the 11th token at TBT of 150ms, will see the same TBT miss rate as a system which produces 10 tokens at TBT of 100ms and the 11th token at TBT of 150ms. Clearly the first system is much more superior than the second, but the TBT miss rate itself does not capture that. What we propose instead is to incorporate a notion of deadline for each token's generation. Let us analyze our example in more detail. For the first system, if the token generation started at $t = 0s$, the first 10 tokens would then have been generated by $t = 100ms$ while the 11th token would have been generated at $t = 250ms$. If the reading speed of the user is say one token per 100ms, they would have ample time by the time they reach to the 11th token (at $t = 1000ms$). Thus, the extra delay in the 11th token generation would not be perceived by the user. In the second system, however, the user will actually perceive a delay while reading the 11th token. This is very similar to the case of video streaming, where TTFT corresponds to the initial delay in video playback (this includes the load times, buffering, etc.), while TBT corresponds to the delay in generation of each frame and should be below 1/fps, where fps is the video's frames per second. In the case of video playback, even if a frame is available earlier than desired, the client actively delays the frame playback to 1/fps. Although this is not required in the case of LLM decode, the client may decide do display the tokens at TBT rate for a consistent experience even if they are available earlier. Based on this motivation, we propose a deadline based TBT acceptance rate metric, which we call fluidity-index."}, {"title": "4.1 fluidity-index metric", "content": "Let the desired TTFT and TBT for a given application be $D_p$ and $D_d$, respectively. Note that $D_p$ will be a function of the number of prompt/prefill tokens. We then define the deadline for the generation of the ith token as $D_i = D_p + i \u00d7 D_d$. As long as all tokens are generated within their deadline, $D_i$, the user will not perceive any delay or generation stall, even if some individual tokens see a delay of more than $D_d$ between consecutive token generation. We then define a deadline-miss as an event when the actual token generation time of the ith token exceeds $D_i$. Note that in the event of a deadline-miss, depending on the length of the stall, many subsequent tokens may miss their deadlines defined as above. This can be misleading as a single stall can amount to 10s of deadline-misses. To account for this, if there was a deadline-miss at the sth token, we reset the deadlines for all subsequent"}, {"title": "5 Evaluation", "content": "In this section we demonstrate the effectiveness of Metron in holistically evaluating the performance of different LLM inference systems both open source frameworks and proprietary offerings."}, {"title": "5.1 Evaluating Public APIS", "content": "In this section, we demonstrate the effectiveness of Metron to benchmark public API endpoints. Metron performs black-box analysis on these systems to characterize their performance under various configurations. We evaluate three proprietary systems with API-only access: Anyscale [1], Groq [5], and Fireworks [4], across two models a dense model LLaMA3-70B[9], and a MoE model Mixtral-8x7B [24]. We use a custom workload with varying prefill length (between 256 and 8k and maximum tokens to generate set to 256. Since the performance of public APIs can change throughout the day depending on request traffic and other factors, we run Metron once every hour for 24 hours to accommodate varying nature of traffic throughout the day.\nResults. Figure 5a plots the throughput of the three systems using three metrics \u2013 TPOT, tail TBT, and fluidity-index. For the first two, we plot the inverse of the observed mean TPOT across all requests, and the inverse of the 99th percentile TBT. For fluid token generation rate, we find the minimum TBT SLO $D_d$ such that 99% percent of the requests have fluidity-index of at-least 0.9. The inverse of this is the fluid token generation rate."}, {"title": "5.2 Evaluating Open Source Systems", "content": "We now demonstrate the effectiveness of Metron in setting SLOs for deployment operators and capacity planning. We evaluate vLLM [14] and Sarathi-Serve [17] (via vLLM with chunked-prefill feature turned on). on LLaMA3-8B [9, 29] on a H100. We use rope-scaling to support prompt token lengths longer than 8192. Requests are randomly sampled from the Arxiv-Summarization [18] dataset that represents long context workloads.\nResults. Earlier in \u00a73.1 we compared the vLLM and Sarathi-Serve at a high load, where we observed that the tail TBT-based throughput for vLLM is 3\u00d7 worse than Sarathi due to huge generation stalls, while the TPOT based throughput shows these systems at par. fluidity-index shows the true difference in throughput between these systems. Next, we compare these systems under a strict TBT SLO. We use fluidity-index to define the service SLO as \u2013 99% of requests should have less than 10% deadline miss rate with 25ms target TBT. Metron then finds the maximum request load (QPS) at which the SLOs can be maintained. We also consider TPOT and P99 TBT based SLOs as baselines with the target latency of 25ms. Here, Sarathi has a 2x lower token throughput compared to vLLM as"}, {"title": "6 Discussion", "content": "Metron provides an evaluation framework for LLM inference using fluidity-index metric that tracks the missed deadlines per request. We now discuss the challenges that we leave to future work.\nThe fluidity-index metric requires setting a deadline for every token \u2013 for the first token of every request, this is the target latency for the prefill phase. In this paper we discuss a potential mechanism for selecting prefill latency target based on observing the prefill processing curve across varying prompt sizes. However, picking a deadline for a given prompt length is challenging for proprietary systems as we cannot accurately characterize their prefill performance; the observed prefill time can include scheduling delays which may offset the expected trends in prefill processing. We leave it to future work to explore alternate ways of prefill latency target selection for propreitery system evaluation.\nNext, we observe that we need to provide a small scheduling slack in deadline computations as discussed in \u00a74.1. We pick this value based on our empirical observations; we leave it to future work to systematically set a scheduling slack. Finally, open-source systems have various performance tuning knobs; for e.g., chunk size in Sarathi-Serve, block size in vLLM etc. Metron currently does not explore or auto-tune such parameters; it expects the users to set the configuration parameters while evaluating across two systems."}, {"title": "7 Related Work", "content": "Machine learning inference systems have been studied extensively over the last decade. TensorFlow-Serving [27], Clipper [19], BatchMaker [21], and Clockwork [22] propose various caching, placement, and batching strategies to improve general model serving. More recently works including Orca [30], VLLM [25], Sarathi [16] primarily addresses the dedicated challenges faced in auto-regressive transformer inference using efficient memory management and scheduling. SplitWise, DistServe and TetriInfer [28, 32, 23] have presented options to disaggregate the prefill and decode phases to eliminate the interference between them."}, {"title": "8 Conclusion", "content": "Evaluating LLM inference systems is a challenging problem due to the unique characteristics of autoregressive decode process. We presented a detailed analysis of existing evaluation metrics and their pitfalls. To address their shortcomings, we introduce Metron- a holistic LLM evaluation framework that instantiates a novel approach comprised of a novel fluidity-index based approach to evaluating LLM inference systems in a user-facing manner. We then show how Metron can be leveraged to evaluate both open-source and proprietary model serving systems. Metron is aimed to serve as a standard evaluation suite for LLM inference systems."}]}