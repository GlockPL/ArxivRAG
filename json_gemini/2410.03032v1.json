{"title": "Counter Quill: Investigating the Potential of Human-Al Collaboration in Online Counterspeech Writing", "authors": ["XIAOHAN DING", "KAIKE PING", "UMA SUSHMITHA GUNTURI", "BUSE CARIK", "SOPHIA STIL", "LANCE T WILHELM", "TAUFIQ DARYANTO", "JAMES HAWDON", "SANG WON LEE", "EUGENIA H RHO"], "abstract": "Online hate speech has become increasingly prevalent on social media platforms, causing harm to individuals and society. While efforts have been made to combat this issue through content moderation, the potential of user-driven counterspeech as an alternative solution remains underexplored. Existing counterspeech methods often face challenges such as fear of retaliation and skill-related barriers. To address these challenges, we introduce COUNTERQUILL, an AI-mediated system that assists users in composing effective and empathetic counterspeech. COUNTERQUILL provides a three-step process: (1) a learning session to help users understand hate speech and counterspeech; (2) a brainstorming session that guides users in identifying key elements of hate speech and exploring counterspeech strategies; and (3) a co-writing session that enables users to draft and refine their counterspeech with COUNTERQUILL. We conducted a within-subjects user study with 20 participants to evaluate COUNTERQUILL in comparison to ChatGPT. Results show that COUNTERQUILL'S guidance and collaborative writing process provided users a stronger sense of ownership over their co-authored counterspeech. Users perceived COUNTERQUILL as a writing partner and thus were more willing to post the co-written counterspeech online compared to the one written with ChatGPT.", "sections": [{"title": "1 INTRODUCTION", "content": "The rapid growth of online communication platforms has led to an increase in the prevalence of hate speech, causing significant harm to individuals and society [39]. A study by the Pew Research Center found that 41% of U.S. adults have experienced online hate speech, with 18% subjected to severe forms such as physical threats, persistent harassment, or sexual harassment [21]. Youth and young adults are even more likely to be exposed to online hate. One cross-national study of online hate in Finland, France, Poland, Spain, the United Kingdom, and the United States found that approximately 70% of youth and young adults ages 18 to 25 had seen online hate materials. Exposure rates ranged from 65% in France to almost 79% in Finland, with 73% of Americans being exposed [74]. To reduce online hate, technology companies have employed human moderators and automated moderation systems. However, prior research suggests that these efforts, particularly the practice of deplatforming \u2013 banning users who express hateful views \u2013 may not effectively eliminate hate speech and often drives users to less monitored online environments [40]. The limitations of traditional moderation methods have prompted scholars to explore the potential of user-driven counterspeech as an alternative solution to combat online hate.\nCounterspeech is \u201ccommunication that tries to counteract potential harm brought about by other speech\" [10, 77]. With respect to hate speech, counterspeech is a direct response to derogatory or harmful content, intended to undermine or refute the hateful message with the intention of stopping it, reducing its impact, or providing support to those it targeted [77]. However, individuals often face significant challenges and barriers when engaging in this practice, such as fear of retaliation as well as emotional and skill-related barriers [63, 70]. Previous research has identified such barriers as crucial factors deterring people from engaging in counterspeech [70]. Individuals with these barriers were found to be less satisfied with their own counterspeech, experienced more difficulty in writing it, and were less likely to perceive their counterspeech as effective [70]."}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 Online Hate and its Dangers", "content": "Online hate \u2013 or cyberhate \u2013 is a type of cyberviolence that uses computer technology to express hatred toward a group based on the group's race, ethnicity, gender, gender identity, sexual orienta-tion, national origin, religion, or other group characteristic [68, 72]. It differs from cyberbullying because of this collective focus. Like terrorism, online hate can advocate the hatred of one group or the hatred of numerous groups. Exposure to online hate has increased in the U.S. [16], with over 60 percent of Americans ages 18 to 25 being exposed to it in 2023 [18, 35]. But it is not limited to the U.S., it is a global phenomenon [15, 74]. Although not everyone who sees online hate suffers negative consequences, exposure to it is correlated with both short-term and long-term emotional distress, such as mood swings, anger, feelings of isolation, decreased levels of trust, and fear [16, 25, 64, 65]. Because they are still forming their identities, young people are especially vulnerable to negative effects of online hate [6]. Given the amount of online hate that can be found online and the dangers it can pose, it is imperative that we find effective tools for detecting it and combatting it. Recent advances in automated detection have been made, but detecting it is not enough. We also need to find ways to lessen its impact. Combatting online hate with counterspeech is one such strategy."}, {"title": "2.2 Human-Al Interaction with Generative Al", "content": "Recent advancements in Generative AI like GPT-4, Bard, and Llama have enabled their integration into various application domains [1, 71, 85], causing interest in how users interact, collaborate, and co-write with these AI systems [5, 22, 48, 49, 84, 87, 89]. In the field of co-writing with AI, researchers have shed light on aspects of user engagement, adaptation, and reliance on AI-generated content [48, 51, 89]. These studies clarify activities ranging from using AI suggestions for conceptualization and content generation to evaluating [19, 50], editing, and refining machine-generated text [5, 14, 90, 94], such as creative writing [14, 90], technical documentation [22, 94], and online communication [42]. Despite such advancements, there are limitations in AI-assisted writing, such as incorporating user instructions [88, 91], handling biased AI-generated content [42], and navigating challenges related to AI's understanding of user intent or desired tone [48, 70]. For instance, Osone et al. (2021) identified challenges in collaborating with AI in creative writing, where experienced writers exhibited dissatisfaction with AI-generated content due to misalignment with expected storylines [67]. Meanwhile, as Zhou et al. (2023) revealed that generative AI creates disinformation by both reshuffling human-made data and manipulating language to produce content that seems credible to targeted audiences.[95].\nAs the integration of AI into writing platforms becomes more prevalent, not only writers and researchers but also social media developers and users will incorporate AI tools into their workflows to enhance the co-creation of social media content [63, 70]. Particularly, the collaboration between humans and AI in generating counterspeech involves a nuanced understanding of language, tone, and context to counteract harmful narratives online [63, 70, 73]. For instance, Ray (2023) and Ping et al. (2024) highlighted that researchers, while crafting counterspeech, find AI tools beneficial"}, {"title": "2.3 Counterspeech Generation", "content": "Text generation, especially counterspeech generation, is one of the many open challenges in NLP research that has made breakthroughs in recent years [12, 13, 79, 96]. Researchers have explored various aspects of counterspeech generation using LLMs, such as generating contextually relevant responses [34, 96], knowledge-grounded counterspeech generation [12], and ensuring that the generated counterspeech adheres to ethical and societal norms [62]. For instance, Chung et al. (2022) explored whether adding relevant external information could improve LLMs' ability to generate counterspeech to hate speech [12]. By extracting keyphrases from the hate speech and finding related outside knowledge, they were able to generate more suitable and informative counterspeech. Meanwhile, Hassan et al. (2023) have developed a novel framework based on theories of discourse to study the inferential links connecting counterspeech to hateful comments [34]. They proposed a taxonomy of counterspeech derived from discourse frameworks and discourse-informed prompting strategies to generate contextually-grounded counterspeech [34].\nDespite LLMs' ability to generate counterspeech, we do not know whether machine-generated counterspeech aligns with people's preferences or whether people would actually use it to counter hate online [63, 70]. Prior research shows that people are hesitant to use counterspeech entirely generated by AI because it strips away their personal voice and raises concerns about authenticity [70]. On the other hand, some believe AI assistance in counterspeech-writing can reduce the emotional burden of responding to online hate and help them articulate their thoughts better [70]. Researchers in HCI and CSS emphasize the importance of a human-centered approach in understanding the use of AI-generated content. Likewise, we argue that effectively using LLMs to counter hate speech in the real world requires attention to human involvement, preferences, and concerns. Our work adopts this approach to improve LLMs' ability to generate human-centered counterspeech and to explore how humans can co-create counterspeech with AI in alignment with their preferences, personal voices, and ideas."}, {"title": "2.4 Ownership in Human-Al Collaboration", "content": "Previous research has demonstrated that ownership extends beyond physical or tangible objects, encompassing everything from people to text to ideas [38, 60]. Regarding how interaction with AI systems affects the human sense of ownership, past studies have found that the more people engage in their interactions with AI, the more likely they are to develop a strong sense of ownership over the outcomes of their co-creation with the AI [53, 55, 69]. For instance, in Louie et al.\u02bcs (2021) study of AI music co-creation, participants indicated that they felt the final product did not belong to them due to their minimal control over the AI-generated suggestions [55]. Furthermore, Draxler et al.'s study of AI-assisted writing suggested that users' perceived control over AI-generated content and their feeling of leadership in the interaction significantly impact their sense of ownership [20]."}, {"title": "3 BACKGROUND AND DESIGN GOALS", "content": "We take a human-centered approach in the design of CounterQuill as an interactive human-AI system that empowers users with relevant knowledge (DG1), ideation space (DG2), and collaborative writing assistance (DG3) rather than simply generating counterspeech. By doing so, our system aims to help users engage in the process of crafting effective and empathetic counterspeech that incorporates their preferences and thought-process in countering online hate, rather than soleley delivering a machine-generated output. Our system is designed to achieve the following design goals:\nDG1. Help users understand hate speech and counterspeech and their impact.\nDG2. Help users ideate counterspeech ideas through a brainstorming process.\nDG3. Help users draft and refine their counterspeech with AI assistance."}, {"title": "3.1 Help Users Learn About Hate Speech & Counterspeech and Their Impact (DG1)", "content": ""}, {"title": "3.1.1 Understanding Hate Speech and its Impact", "content": "Previous research found that many internet users, especially younger generations, may not fully understand what constitutes hate speech or its harmful effects [2, 72]. A survey conducted by the Anti-Defamation League revealed that 65% of American youth were exposed to online hate speech, but only 49% of them recognized and reported hateful content to social media companies [2]. This lack of awareness underscores the need for educational initiatives to help users identify hate speech effectively. To address this issue, NGOs have launched campaigns aimed at raising awareness about hate speech and teaching people how to recognize it [45, 82].\nPrevious research has found that hate speech can be expressed both explicitly and implicitly [24, 31]. Explicit hate speech directly attacks social groups or individuals through overt language, such as profanity [41, 74]. In contrast, implicit hate speech is more subtle [24], often relying on coded language [24, 66], neologisms [75], sarcasm [66], or microaggressions [31] to convey prejudice or discrimination. Thus, same words or phrases can have different meanings depending on the specific situation, the speaker's intent, and the audience's perception [31]. For such reasons, it can be particularly difficult for humans to recognize implicit hate speech."}, {"title": "3.1.2 Understanding counterspeech and its Impact", "content": "Furthermore, previous research found that the lack of clear guidelines and practices for crafting effective counterspeech is another factor contributing to people's reluctance to respond to hate speech [70]. Baider (2020) observed that individuals often struggle to determine what constitutes impactful counterspeech and how to formulate responses that can make a difference [3].\nResearch has highlighted the potential for unintended consequences when counterspeech is not delivered carefully [41]. As Howard's (2019) study found, counterspeech perceived as aggressive or"}, {"title": "3.2 Help Users Ideate Counterspeech Ideas Through a Brainstorming Process (DG2)", "content": "Previous research has found that while many people recognize the harmful effects of hate speech, they may feel uninformed or unable to engage in counterspeech effectively. Studies by Mun et al. (2024), Buerger et al. (2021), and Ping et al. (2024) revealed that due to cognitive load, users often hesitate or struggle to create effective counterspeech [7, 63, 70] . This cognitive load arises from the difficulty in accurately identifying the key hate elements in hate speech, such as aspects of an individual or group's identity and actions or perceptions that dehumanize that person or group [33, 86]. Furthermore, many people may lack the skills to effectively counter hate speech [70]. A study by Costello et al. (2017) found that although young adults (from an online survey, N = 963) recognized the importance of countering online hate speech, they were often uncertain about how to effectively counter the key hate elements of hate speech [17].\nTo address these challenges, we adopted the traditional brainstorming approach (TBS) [26, 93] and extended it by drawing on prior HCI work on AI technologies that facilitate brainstorming [9, 46, 93, 94]. Researchers in these studies use LLMs to help people generate [93, 94], refine [28, 93], and evaluate ideas [9]. For example, Gero et al. developed Metaphoria, an LLM-powered system that assists writers in brainstorming creative ideas by generating metaphorical associations and encouraging diverse creative outputs [29]. In addition, Zhang et al. created Visar, a human-AI collaborative writing assistant that helps writers brainstorm and revise hierarchical goals within their argumentative writing context [94].\nLubart (2005) and Chiu et al. (2023) [56, 93], identified four key components in HCI brainstorming: (1) AI engaging in communication and guidance with humans during the thinking process; (2) AI providing targeted feedback; (3) AI and humans collaborating to organize brainstorming results; and (4) AI encouraging reflective behavior by integrating human-machine collaboration into\nthe creative process. Built on previous research, we developed two AI agents:\nBrainstorming Guidance Agent (BGA), which facilitates communication, guidance, and encouragement, and\nBrainstorming Feedback Agent (BFA), which provides feedback and collaborates with users to organize the outputs of the brainstorming process. Overall, our brainstorming session aims to help users ideate counterspeech ideas through a brainstorming process assisted by these AI agents."}, {"title": "3.3 Help Users Draft and Refine Their Counterspeech With Al Assistance (DG3)", "content": "Previous research has highlighted the difficulties of writing effective counterspeech [41, 70]. Studies by Mathew et al. found that a significant portion of counterspeech on Twitter were ineffective or even counterproductive, sometimes escalating conflict or using inappropriate language [58, 59]. Ping et al. (2024) reveal the various challenges individuals face when writing online counterspeech, such as the emotional burden, time commitment, and perceived lack of skill in crafting effective"}, {"title": "4 SYSTEM IMPLEMENTATION", "content": "CounterQuill was developed using React for the front end, and Python + MySQL for the back end, which was served through Flask. The system's interface was designed with a minimal aesthetic using the Tailwind CSS framework to create clear indicators of system features."}, {"title": "4.1 CounterQuill Learning Session", "content": "As shown in Figure 2, the Learning session has three main sections: 1 What is Hate Speech?\n2 Learning about Counterspeech and 3 Testing your Understanding . On the left side of the\naccordions is our CounterQuill guidelines message, which summarizes the main tasks during the\nLearning session as well as the basic steps to follow. We designed the interface of Learning session\nusing classic CSS accordions, a common user interface pattern that allows users to interactively"}, {"title": "4.2 CounterQuill Brainstorming Session", "content": "To help users ideate counterspeech ideas, we designed the Brainstorming session with two key sections: (a) a section for users to highlight key elements of hate speech, as shown in Figure 5, and (b) a section to encourage users to engage in thinking about hate speech by considering the negative assumptions suggested by the statement and the feelings of those targeted (Figure 6). The brainstorming session includes: 1 Tutorial on highlighting practice, 2 Highlighing area, 3 Brainstorming through Q&A and 4 CounterQuill suggestions on brainstorming results."}, {"title": "4.2.1 Highlighting Practice", "content": "For 1, the tutorial features a\nBGA that communicates the core brainstorming tasks and steps. It dynamically demonstrates how to highlight aspects of an individual or group's identity in yellow and actions that dehumanize them in green. The tutorial, developed by the first author, two sociologists and an HCI scholar, uses a sample hate speech statement (\u201cI saw a random black man jogging nearby our house today and now I feel unsafe walking around my own neighborhood\u201d).\nIn 2 Highlighing area, users manually select and highlight the identity aspects in yellow and the dehumanizing actions in green. Once users have completed highlighting the content, they can click the \"Done\" button, triggering BFA to semantically compare the user-highlighted content with the system's predefined correct answer [57]. BFA captures the user-highlighted text and sends it to the backend service, where it is compared using the GPT-3.5-turbo model with the following prompt:\n{ \"role\": \"assistant\",\n\"content\":\n\"context\": \"The correct individual or a group's identity and dehumanizing action\nin the speech\ntext '{hatespeech}' is: \u2018{identity}' and '{action}' \",\n\"query\": If the user selects \u2018{user selection_1}' as identity and \u2018{user selection_2}'\nas dehumanizing\naction, are they semantically equivalent to the correct answer?\",\n\"responseOptions\": [\"Yes\", \"No\"] }\nFor example, in the hate speech statement \u201cI saw a random black man jogging nearby our house today and now I feel unsafe walking around my own neighborhood\u201d, the correct semantic"}, {"title": "4.2.2 Question Answering", "content": "For 3 Brainstorming through Q&A, the first author collaborated with two sociologists and an HCI scholar to design two thought-provoking questions that encourage users to provide thoughtful written responses (Figure 6). Question 1 asks, \u201cWhat negative stereotypes or assumptions about the targeted group or individual are suggested by the statement?\u201d This question aims to let users think of harmful assumptions implied about the targeted group or individual. Question 2 asks, \u201cConsider the feelings and experiences of someone who identifies with the group mentioned in the statement. How might this comment affect their sense of safety, belonging, or self-esteem?\" This question encourages users to reflect on the impact of identity-based dehumanization. Users can input their responses in the provided text box. For example, in Figure 6, the user has written, \u201cThe statement implies harmful stereotypes: the presence of black people in a neighborhood leads to decreased property values and a decline in the overall quality of the area\u201d.\nFor 4 CounterQuill suggestions on brainstorming results, after completing their input for both questions, users can click the \u201cDone\u201d button, prompting CounterQuill to provide suggestions based on the user's responses. These suggestions include (1) feedback on the user's understanding and (2) suggestions on strategies for crafting their counterspeech. To implement this step, clicking the \"Done\" button triggers the BFA, which captures the user's input text and sends it to the backend service. The backend service then uses the GPT-3.5-turbo model with the following prompt:\n{ \"role\": \"assistant\",\n\"content\":"}, {"title": "4.3 CounterQuill Co-Writing Session", "content": "To help users draft and refine their counterspeech, we designed a co-writing session with Coun-terQuill. This session includes four main sections: 1 Tutorial about CounterQuill writing assistant, 2 Brainstorming notes section for users to review, 3 Text area for users to write counterspeech, and 4 CounterQuill writing assistance tool.\nIn section 1, the WGA interacts with users to introduce the writing tasks they need to complete during the session, including the hate speech they need to counter and a video tutorial on co-writing with CounterQuill. The tutorial video demonstrates how users can obtain writing suggestions and refine their counterspeech with the help of CounterQuill.\n2 Brainstorming notes section displays the notes that users have taken by clicking the \u201cTake Notes\" button during the brainstorming session. These notes are the ones users find noteworthy and helpful for writing their counterspeech. To implement this feature, the WGA accesses our backend database, where user-selected content is stored (see \u00a73.2.2). After indexing the user's notes, the WGA sends them back to the frontend and displays them to users, allowing users to make initial self-revisions based on their notes. In 3 Counterspeech writing area, the WGA first guides users, indicating that this section is for writing their counterspeech. The WGA then uses the user's reflections on hate speech from the brainstorming session (i.e., their responses to the two questions considering the negative assumptions suggested by the statement and the feelings of those targeted) as an initial draft for their counterspeech. This reduces the user's psychological pressure when drafting counterspeech from scratch. Users can then make initial additions, deletions, and modifications to the content based on their brainstorming notes. As users begin inputting content, our system automatically saves the input to the backend database. Once users complete"}, {"title": "5 USER STUDY", "content": "We conducted a user study with 20 participants to evaluate CounterQuill. The study was approved by an Institutional Review Board (IRB), and designed to take 70 minutes for each participant. Our user study examined the following research questions:\nRQ1. How does the learning session influence users' understanding of hate speech and the strategies and empathy-based approaches for counterspeech?\nRQ2. How does the brainstorming session guide users through the process of identifying and responding to identity-based dehumanization in online hate speech impact users' confidence, and perceived ability to engage in counterspeech?"}, {"title": "5.1 Participants", "content": "We recruited 20 native English-speaking participants (9F11M), whose ages ranged from 20 to 44 (mean = 26, SD = 5.7), through word-of-mouth, LinkedIn, Reddit, and X (Twitter). Five of them participated in the study in person at a usability lab and fifteen participated virtually through Zoom. Participants were required to complete a pre-screening survey to test whether they were required to attend our user study (See Appendix A) and collect their basic demographic information, including age, gender, sexual orientation, and race/ethnicity. We established two key criteria for participation: first, participants needed to have prior experience with generative AI tools, such as ChatGPT, Google Gemini, or Claude, though none had used CounterQuill before the study. Second, participants were required to have encountered hate speech online and have previously engaged in counterspeech. Additionally, we tried to diversify the participant group as much as possible. The demographic information of our 20 participants is shown in Table 1. Participants were informed that their responses to surveys, as well as video and audio from the interview may be used for data analysis. After the study, participants were compensated $20 in gift card credit for their time.\nRegarding their experience with AI-supported writing tools or GenAI tools, all 20 participants had prior experience with AI-supported writing tools, such as ChatGPT and Claude. Every participant was familiar with conversational interactions using generative AI tools, with 18 participants having"}, {"title": "5.2 Study Protocol", "content": "Each study session lasted approximately 70 minutes and employed a within-subjects design. Before the study, participants received a brief introductory email about the system and a tutorial video that provided a comprehensive guide on using CounterQuill's features. During the study, participants experienced two conditions: a 10-minute period of writing counterspeech with ChatGPT, and a 50-minute period of co-writing counterspeech with CounterQuill. The detailed study procedure is described in \u00a75.3."}, {"title": "5.2.1 Conditions", "content": "In our user study, we implemented two distinct conditions to evaluate the effectiveness of AI-assisted counterspeech writing, drawing on previous research that used GPT-based systems [52, 76, 94] such as GPT-3.5 as baseline:\nChatGPT (Baseline): Participants used ChatGPT to write counterspeech for the hateful content. ChatGPT allowed users to input prompts and receive generated responses from the GPT-3.5-turbo language model, which is the same model used in CounterQuill.\nCounterQuill: Participants used the full version of CounterQuill, which includes a learning session (help users understand hatespeech and counterspeech), a brainstorming session (step-by-step counterspeech ideation support), and a writing session (co-writing with CounterQuill for counterspeech).\nTo minimize potential biases, we used a Latin Square experimental design [78] to ensure a balanced sequence of task topics and conditions for each study session: with half the participants starting with our system and the other half beginning with ChatGPT. To assess the potential impact of the"}, {"title": "5.3 Study Procedure", "content": "At the beginning of each study session, the experimenters collected informed consent from the participants. For the user study, each participant was presented with 20 hate speech examples and asked to select one of them. The hate speech data was sourced from Ping's (2024) research [70], which included a total of 400 hate speech instances evenly distributed across five hate speech themes: race, gender, sexual orientation, disability, and religion. To ensure a balanced representation of themes, each participant was randomly assigned 20 hate speech examples, with an average of 4 examples per theme. Table A1 (Appendix F) lists the hate speech instances used by each of the 20 participants.\nThe study coordinators introduced participants to the ChatGPT counterspeech writing session and provided a 3-minute tutorial on how to use CounterQuill, followed by an introduction to a CounterQuill co-writing session. Each participant completed the two writing sessions, with the order counterbalanced between participants to minimize potential biases [11, 30].\nIn the ChatGPT (Baseline) condition, participants used ChatGPT to write counterspeech for the hateful content. Then they completed a post-study questionnaire including NASA-TLX questions and customized questions (see Appendix B for post-study questionnaire). In the CounterQuill condition, participants went through a learning session to understand hatespeech and counterspeech, a brainstorming session for step-by-step ideation support, and used the CounterQuill co-writing assistant. They then completed a post-study questionnaire. The study ended with a 20-minute semi-structured interview, during which the experimenter asked questions to understand participants' experiences, feedback on CounterQuill features, and reflections on the two conditions. The purpose of the semi-structured interview was to explore participants' overall experiences, elicit their specific"}, {"title": "6 RESULT", "content": ""}, {"title": "6.1 Quantitative Analysis", "content": ""}, {"title": "6.1.1 Statistical Analysis of ChatGPT and CounterQuill with NASA TLX", "content": "Table 3 summarizes the ratings for each dimension of the NASA Task Load Index (TLX) across two conditions: using ChatGPT (Condition A) and using CounterQuill (Condition B). We evaluated the differences between these conditions on each dimension using paired-sample t-tests [61]. To further contextualize these findings, Table 2 provides a comparison of the average time spent on counterspeech composition in each condition."}, {"title": "6.1.2 Quantitatively Analyze User Perceptions and Experiences with ChatGPT and CounterQuill", "content": "Our study further investigated participants' experiences and perceptions when using ChatGPT and CounterQuill for counterspeech writing. To gain deeper insights, we designed six customized questions (see Appendix B for detailed post-study questionnaire) that focused on specific aspects of the participants' experience, such as confidence in identifying hate speech, effectiveness of the tools in providing guidance, and satisfaction with the final counter-speech produced. The analysis of the six customized questions revealed several notable findings (Table 4):\nOur result found statistically significant differences (p < 0.001) between CounterQuill and Chat-GPT across various measures. Participants reported higher confidence in identifying online hate speech, found CounterQuill more effective for brainstorming and increasing self-efficacy in writing counterspeech, felt more engaged and motivated when using CounterQuill, were more satisfied with the counterspeech created using CounterQuill, and were more likely to post the co-written counterspeech online when using CounterQuill compared to ChatGPT.\nOverall, the violin plot (Figure 11) visually represents the score distributions across the two conditions based on our six customized questions. Participants found CounterQuill more effective in enhancing their ability to identify hate speech, providing brainstorming assistance, increasing self-efficacy, fostering engagement, and leading to satisfaction with the final counterspeech. Moreover, participants were more likely to post the counterspeech co-created with CounterQuill on social media than ChatGPT."}, {"title": "6.2 Qualitative Findings of Learning with CounterQuill", "content": ""}, {"title": "6.2.1 Evolving Perceptions of Hate Speech: From Explicit to Implicit Forms", "content": "The learning session played a crucial role in shaping participants' understanding of hate speech, particularly distinguish-ing between explicit and implicit forms. Many participants (19 | P1 - 13, P15 - P20) came into the study with assumptions about what does and does not constitute hate speech. While most (15 | P1-P5, P7, P8, P10 - P13, P15 - P20) found it easy to recognize explicit hate speech, identifying implicit hate speech proved more challenging.\nPrior to the learning session, participants had a general understanding of hate speech, especially explicit forms, but many (13 | P1 - P3, P7, P10 - P13, P16 - P20) struggled to articulate the nuances between explicit and implicit hate speech. As P7 mentioned, \u201cBefore the session, I knew [explicit] hate speech was bad, but I didn't really think this (implicit hate speech) could also be hateful.\u201d After completing the learning session, all participants (20 | P1 - P20) consistently characterized explicit hate speech as direct, obvious, and easily recognizable, similar to what P4 relates here: (\u201cExplicit hate speech is when someone directly calls another person names or uses slurs, and they are direct and easy to recognize.\u201d). Most Participants (15 | P1 - P5, P7, P8, P10 - P13, P15 - P20) were surprised by"}, {"title": "6.2.2 Shifting Perceptions: Recognizing the Lasting Impact of Implicit Hate Speech", "content": "Prior to the learning session, participants had varying levels of awareness about the impact of hate speech. Some, like P19, initially believed that implicit hate speech was less harmful than explicit hate speech, stating, \u201cI think (implicit hate speech) is less harmful (than explicit hate speech), it's not direct, probably doesn't hurt too many people.\u201d.\nHowever, the learning session, which included explaining the impact of different types of hate speech and testing participants' knowledge, helped to create a unified understanding and changed many participants' perceptions. After completing the session, most participants (17 | P1 - P5, P7, P8, P10 - P13, P15 - P20) recognized that despite its subtlety, implicit hate speech could have a more lasting and harmful impact than explicit hate speech. This shift in perspective is exemplified by P5's reflections following the learning session:"}, {"title": "6.2.3 Familiarizing with Counterspeech: From Awareness to Understanding", "content": "Our learning session introduced participants to the concept of counterspeech and its impact. Prior to the session, participants had varying levels of familiarity with counterspeech. Some participants (12 | P1 - P3, P8, P10 - P12, P14 - P17, P19) had a general understanding of counterspeech as a way to respond to hate speech. Two participants (P4, P13) had even posted responses to online hate speech prior to the session, but they were unaware that their behavior was considered counterspeech. Others (6 | P5, P6, P7, P9, P18, P20) were less familiar with the concept of counterspeech.\nAfter completing the learning session, participants defined counterspeech as a respectful (18 | P2-P12, P14 - P20) and non-escalating (8 | P1, P3, P6, P7, P9, P13, P18, P20) way to respond to hate speech. P4 provided a clear example of this understanding:"}, {"title": "6.2.4 Empathy-Based Counterspeech: Recognizing Its Effectiveness", "content": "Our learning session also in-troduced participants to the concept of empathy-based counterspeech. Prior to the session, few"}, {"title": "6.3 Qualitative Findings of Brainstorming with Counter Quill", "content": ""}, {"title": "6.3.1 Developing Hate Speech Identification Skills through Highlighting Practice", "content": "Our brainstorming session introduced participants to a color-coded highlighting practice", "speech": 1}]}