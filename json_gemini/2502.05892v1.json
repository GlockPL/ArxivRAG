{"title": "A Distributional Perspective on Word Learning in Neural Language Models", "authors": ["Filippo Ficarra", "Ryan Cotterell", "Alex Warstadt"], "abstract": "Language models (LMs) are increasingly being studied as models of human language learners. Due to the nascency of the field, it is not well-established whether LMs exhibit similar learning dynamics to humans, and there are few direct comparisons between learning trajectories in humans and models. Word learning trajectories for children are relatively well-documented, and recent work has tried to extend these investigations to language models. However, there are no widely agreed-upon metrics for word learning in language models. We take a distributional approach to this problem, defining lexical knowledge in terms of properties of the learned distribution for a target word. We argue that distributional signatures studied in prior work fail to capture key distributional information. Thus, we propose an array of signatures that improve on earlier approaches by capturing knowledge of both where the target word can and cannot occur as well as gradient preferences about the word's appropriateness. We obtain learning trajectories for a selection of small language models we train from scratch, study the relationship between different distributional signatures, compare how well they align with human word learning trajectories and interpretable lexical features, and address basic methodological questions about estimating these distributional signatures. Our metrics largely capture complementary information, suggesting that it is important not to rely on a single metric. However, across all metrics, language models' learning trajectories fail to correlate with those of children.", "sections": [{"title": "1 Introduction", "content": "There is a long tradition of characterizing words in terms of their statistical properties (Wittgenstein, 1953). The distributional hypothesis (Harris, 1954; Lenci, 2008), which characterizes knowledge of a word in terms of \u201cthe company it keeps\" (Firth, 1957), has proven surprisingly prescient. Indeed, such a characterization is the idea behind static word representations (Deerwester et al., 1990; Landauer and Dumais, 1997; Hofmann, 1999; Mikolov et al., 2013; Pennington et al., 2014) estimated from data, as well as modern (large) language models (OpenAI, 2022; Meta, 2024). While such a distributional approach to training language models (LMs) is now well-established, only recently has distributional information been explored as a tool for evaluating lexical knowledge in LMs.\nOver the last few years, there has been a growing interest in studying word learning in language models (Nikolaus and Fourtassi, 2021a; Chang and Bergen, 2022; Portelance et al., 2024, 2023; Vong et al., 2024; Zhuang et al., 2024b,a; Ma et al., 2024). Most of these studies are part of a larger research program to use LMs to inform the study of human language acquisition by serving as convenient, controllable, and effective models of human development (Dupoux, 2018; Linzen, 2019; Warstadt and Bowman, 2022; Constantinescu et al., 2024). From this perspective, it is desirable to have LMs with human-like learning trajectories, as they can better serve as generalizable models of human learners. Word learning has a potentially important role in the success of this research program because it is one of the best proving grounds for comparing the learning trajectories of humans and LMs head to head. While some studies (e.g., Choshen et al., 2022) have tracked syntax learning in LMs using benchmarks like BLiMP (Warstadt et al., 2020), corresponding data for children is more limited in scope (Evanson et al., 2023). There is also child data on phonological learning (Lavechin et al., 2022) which can be explored further as audio-based LMs improve.\nFortuitously, word learning trajectories in text-based LMs can be easily compared against a wealth of child data in multiple languages thanks to the massive efforts of caregivers and scholars who report and curate child word learning data in the Wordbank database (Frank et al., 2017). Unfortunately, the caregiver reporting approach (Fenson et al., 2013) used in Wordbank is not immediately applicable to LMs, and there is no consensus on how to benchmark word learning in LMs. Zhuang et al. (2024b) explored word learning through different methods, including comparing LMs' word similarity scores to humans' (Finkelstein et al., 2001; Bruni et al., 2012; Hill et al., 2015; Gerz et al., 2016), classifying lexical entailment relations (Santus et al., 2016), predicting semantic features (Buchanan et al., 2019) and using minimal pairs to measure LM preferences for appropriate word usage (Marvin and Linzen, 2018). Other articles rely on visual stimuli to ground evaluations for multimodal models (Nikolaus and Fourtassi, 2021a; Berger et al., 2022; Vong et al., 2024). Notably, Chang and Bergen (2022) and Portelance et al. (2023) take a distributional approach, characterizing lexical knowledge in terms of the LM's surprisal, an information-theoretic quantity that has been widely studied in psycholinguistics (Hale, 2001; Levy, 2008).\nIn this study, we take inspiration from Chang and Bergen's (2022) approach to tracking the model's distributional knowledge about a particular word throughout training. We formalize their approach and improve on it in several respects. While Chang and Bergen only consider the surprisal under an LM of a word in a context where the target word is appropriate and (implicitly) rely on a trivial approximation of the ground truth distribution in evaluating the quality of lexical knowledge, in contrast, we propose a family of distributional signatures that allow for the consideration of the LM's learned distribution in both appropriate and inappropriate contexts. We also introduce distributional signatures that are truly intrinsic to the model itself, as well as strongly reference signatures that compare the learned distribution to a non-trivial ground truth, which we approximate using a large pretrained LM.\nIn our experiments, we train language models from scratch on three datasets resembling the input to children to varying degrees. We record the distributional signatures for a set of common words throughout training, and following Chang and Bergen, we apply a threshold to the measured learning trajectories to obtain an age-of-acquisition (AoA) for each word. We then conduct analyses to answer the following questions:"}, {"title": "2 Preliminaries", "content": "Let \u2211 be an alphabet, a finite, non-empty set of characters, e.g., Unicode symbols. A string is a finite sequence of characters drawn from an alphabet \u03a3. The set \u03a3*, the Kleene closure of \u03a3, is the set of all strings with characters drawn from \u03a3-including the empty string \u025b. We consider two distinguished types of strings. First, we define a word as a character string w \u2208 \u03a3*, which is believed to operate as a lexical item. Second, we refer to an arbitrary character string that precedes a word as a context. We denote a context as c \u2208 \u03a3*.\nA language model p is a probability distribution over \u03a3*. A language model's prefix probability is defined as the following sum\n\ndef\np(y | c) = \u2211 p(yy').\ny'\u2208\u03a3*\n                                                                                                         (1)"}, {"title": "3 Defining Lexical Knowledge", "content": "Our goal is to evaluate word learning in LMs by following the trajectory of a distributional signature for each target word throughout LM training.\nThroughout the paper, we will primarily be interested in a specific ratio of p's prefix probabilities, which we use to define the probability of a word in a context as follows\n\ndef\np(w | c) = (cw)/p(c) (2)\nWe are also interested in the surprisal of a word in a context, denoted \u2013 log p(w | c).\nNow, we derive a language model p's context distribution using Bayes' rule as follows\n\np(c | w) = p(w | c) p (c)/\u2211c\u2208\u03a3* p(w | c) p (c) (3)\nUnder the assumption that p is of finite expected length, then \u2211c\u2208\u03a3* p(w | c) p (c) is always finite (Opedal et al., 2024, \u00a72.1). In contrast to p(w | c), px(c | w) is a distribution over \u2211* due to the normalization present in Eq. (3). Complementarily, we define a word w's negative context distribution as\n\npx(c | \u00acw) = (1 \u2212 p (w | c)) p (c)/\u2211c\u2208\u03a3* (1 \u2212 p (w | c)) p(c) (4)\nThe probability 1 \u2013 p (w | c) can be thought of as follows. Given that p (w | c) is the probability of the event that a string sampled from p( | c) has w as a prefix, 1 \u2013 p (w | c) is the complement of that event, i.e., it is the probability that a string sampled from p( | c) does not have w as a prefix.\nIn the remainder of this paper, we will distinguish three LMs: p, the underlying distribution assumed to have generated the observed strings; q, a parameterized model whose parameters we estimate; and r, a pre-trained reference LM, potentially larger and trained on more data. A standard method of constructing a language model p that approximates q is maximum-likelihood estimation. Suppose we observe a bag of N samples (y(n)) SN-1 where y(n) ~p, then we choose a model q that minimizes the following cross-entropy: - \u2211n=1N log q(y(n))."}, {"title": "4 Analyzing Trajectories", "content": "Given our goal of studying the word acquisition process in LMs, we aim to study the trajectory of a signature \u03c3 for various words throughout the training of the target LM. However, an entire trajectory may contain too much information for some analyses. In this section, we consider a family of statistics that can be extracted from the trajectory and review the main choice points in doing so.\nDetermining AoA by Thresholding While many statistics are possible, we focus on age of acquisition (AoA), which is a single number that should be interpreted as the point at which learning has advanced to a satisfactory degree. For human learners, Braginsky et al. (2016) define AoA as the age at which 50% of children are such that their caregivers report them as understanding the word. Chang and Bergen (2022) apply this thresholding approach to LMs. Given a trajectory, they define the AoA to be the first time step at which the signature reaches a threshold defined as 7% of the way between some initial value representing the beginning of learning and some final value representing the endpoint of learning. Unfortunately, thresholding in this way is only suitable when exhibits (roughly) monotonic change over time. While this is true of some signatures we consider, we find empirically that 6+, \u00d4I\u00b1, \u00d41+, and 1 are exceptions. Thus, we adopt a different approach to extracting AoAs based on the notion of a Cauchy sequence. Intuitively, we say that the target word is learned at the point in the trajectory where the value of the signature becomes close to its neighboring points in the trajectory. Our approach is defined formally in App. D. For the sake of uniformity, we apply this approach to all signatures and leave an exploration of thresholding approaches for suitable signatures to future work.\nSmoothing the Trajectory Empirical trajectories may be noisy due to estimation errors or local instabilities during training (Datta et al., 2023). Thus, we consider several techniques for smoothing the trajectory. One approach to smoothing is parametric curve fitting; Braginsky et al. (2019) and Chang and Bergen (2022) employ such an approach and assume trajectories follow the form of a sigmoid curve. However, parametric curve fitting requires the modeler to assume the functional form of the curve. If the functional form of the curve is unknown, one can instead smooth the curve using a non-parametric method, e.g., a moving average or a generalized additive model (Hastie and Tibshirani, 1986), as done by Chang et al. (2024). For simplicity, we opt for a moving average to smooth the trajectories in this paper."}, {"title": "5 Methods", "content": "We find that the learning trajectories for different distributional signatures are indeed different from each other, suggesting that earlier approaches failed to capture some aspects of word learning. While many signatures, like Chang and Bergen's (2022), give trajectories that are highly correlated with simple features like lexical frequency, other signatures are harder to predict and therefore may capture more nontrivial information. However, we find that learning trajectories for some distributional signatures fail to converge, making AoAs difficult to infer. Finally, no signature yields AoA scores that are strongly correlated with children's AoA, supporting the conclusion that with current methods, LMs' learning patterns are poorly aligned with humans' and underscoring a limitation of current LMs as models of human development. We, therefore, call for future work to evaluate and improve the human-likeness of LMs' learning trajectories using the distributional signatures we propose.\n5.1 Language Models\nWe train several language models to explore our proposed distributional signatures.\nTraining Data. We use three datasets previously released with train/test splits for training and evaluating our LMs. (i) Unified: This dataset was compiled by Constantinescu et al. (2024). It consists of approximately 600M words sampled from a combination of three corpora: Project Gutenberg, Wikipedia, and OpenSubtitles (Lison and Tiedemann, 2016). Given that a typical 13-year-old person may be exposed to around 100M words (Gilkerson et al., 2017), this dataset is not fully representative of the actual input to children, although it contains a large proportion of spoken language. (ii) BabyLM: This is the 100M text-only corpus from the second BabyLM Challenge (Choshen et al., 2024). The dataset is designed to be relatively developmentally plausible while also containing the amount of input that a typical adolescent is exposed to. It includes child-directed speech from CHILDES (MacWhinney, 2000) and children's stories from Project Gutenberg (Gerlach and Font-Clos, 2020), as well as dialogue such as BNC and the Switchboard Corpus (Stolcke et al., 2000), along with Simple English Wikipedia and Open Subtitles. (iii) CHILDES: This is the CHILDES subset taken from BabyLM, consisting of 29M tokens of child-directed speech. These datasets constitute an attempt to balance developmental plausibility against quantity. Our motivation for training on datasets such as BabyLM and CHILDES is to observe whether more developmentally plausible training distributions result in more human-like word learning trajectories.\nSignature Estimation. To estimate the signatures for each word, we sample 100 positive and 100 negative contexts from the BabyLM test set. To ensure fair cross-model comparisons, we use the same test contexts for all models, regardless of training data.\nModels. We train GPT-2 from scratch following the training procedure described by Radford et al. (2019). To reduce variance in performance due to random initialization, we train three variations of each model using different random seeds. To compute the reference signatures (\u00ceR+, \u00d4R\u2212, \u00d4R\u00b1) we use Llama-3.1-8B as the reference distribution r. Full details regarding the hyperparameters, training duration, and loss curves are provided in App. C. As we are interested in analyzing the learning trajectories for models, it is important that they are trained for a reasonable duration. For models trained on BabyLM and CHILDES we apply early stopping, i.e., we choose the best model on a held-out development set, as we found that models eventually overfit. For models trained on Unified we train for 30,000 steps, or 12 epochs, following (Constantinescu et al., 2024). We estimate that Chang and Bergen (2022) trained"}, {"title": "6 Examining LM Learning Trajectories", "content": "Before quantitatively comparing LM and child word learning trajectories in \u00a77, we conduct several analyses focusing solely on LM trajectories.\n6.1 Case studies\nWe perform several case studies by inspecting the learning trajectories and AoA scores for humans and each distributional signature from \u00a73. We analyze the trajectories and AoA scores for LMs trained on the Unified dataset for a sample of 8 words: two FUNCTION WORDS, two NOUNS, two ADJECTIVES, and two VERBS. For each category, one word is chosen from the 10 most and 10 least frequent (for the Unified dataset).\nFig. 1 shows the trajectories for these words, and Table A5 gives the AoA scores. For most signatures, we observe that the higher-frequency word from a category has an earlier AoA than the corresponding lower-frequency word. We also observe that most signatures yield a wide range of AoA scores, but others-particularly \u00d4-show very similar (and late) AoAs for all words we inspect. Table A6 shows the first and last learned words for each signature. Generally, we find that high-frequency words and function words are learned first.\n6.2 Convergence behavior\nAs we rely on the Cauchy criterion to extract AoA scores, we now examine how different signatures converge. Fig. 1 shows that the shape of the learning trajectories varies between signatures. Within a given signature, trajectory shapes are internally consistent to varying degrees. As expected, the reference signatures are mostly monotonically decreasing, indicating that the probability of the word of interest under the LM becomes closer to the ground truth after more iterations. Furthermore, for the corpus-based signatures, \u2642+ trajectories are decreasing, whereas \u2013 are increasing. On the other hand, the intrinsic signatures and \u2642+ are not consistently increasing or decreasing.\nWe compute the AoA scores for a given signature using a range of values for e. Fig. A4 shows how many words failed to converge under different thresholds \u20ac. We find that the vast majority of trajectories converge with \u20ac = 0.15. For lower values of e, we see as many as half of all word trajectories failing to converge on the CHILDES dataset. However, on the larger datasets BabyLM and Unified, we observe high rates of convergence across the board. Finally, it is the intrinsic signatures and \u00f4+ that show the lowest rates of convergence. As discussed above, these are precisely the same signatures that do not have an internally consistent shape. Furthermore, the figures in App. I show correlations for each pair of thresholds. With a few exceptions for extreme values, different thresholds still yield AoA scores that are highly correlated. Therefore, in all our results (including those discussed above), we apply an intermediate value of e = 0.07.\n6.3 Comparing Signatures\nAnother important question is whether different signatures give similar AoA scores to each other. App. J shows the correlation matrix of AoA scores"}, {"title": "7 Human vs. LM Learning Trajectories", "content": "We now examine the similarities and differences between word learning in LMs and humans.\n7.1 Comparing Human and LM AoAs\nWe begin simply by measuring the Pearson correlation between human AoAs and the LM AoAs from each signature. These values are plotted in Fig. 2. Overall, we observe very weak or negative correlations. We find that the signature that correlates most changes depending on the datasets, but no correlation exceeds 0.31 (either positive or negative). The strongest positive correlations occur in BabyLM for \u00d4I+ and \u00f4r-, while the strongest negative correlations are from Unified and BabyLM for the \u00f4\u2081 signature.\n7.2 Predicting AoAs from Features\nWe now investigate which factors predict human and LM AoAs, and compare whether these factors have similar effects. Braginsky et al. (2016) identified several interpretable features that predict human AoAs. Chang and Bergen (2022) previously fit linear models to predict LM AoAs using these features. We extend this analysis to our set of signatures. Specifically, we take the AoA scores of children and of each of our signatures as our dependent variables, and, additionally, consider the following predictors that Braginsky et al. (2016) studied: (i) log frequency with respect to each LM's training dataset for LMs and with respect to CHILDES for children, (ii) number of characters, i.e., the number of symbols from \u2211 in the word, (iii) concreteness judgments, collected from human subjects by Brysbaert et al. (2014), which indicates the extent to which a word is concrete, measured on a scale from 1 (very abstract) to 5 (very concrete), (iv) mean length of the utterances (MLU) with respect to each LM's training dataset for LMs and with respect to CHILDES for children, and (v) lexical category NOUN, PREDICATE, FUNCTION WORD, and OTHER, annotated by Frank et al. (2017, 2021).\nDo similar factors influence LMs and Children AoAs? Regression analyses for LMs trained on Unified and for children are presented in Table 2. For children, the adjusted R2 with all predictors is 0.417. The strongest individual predictors of children's AoA are concreteness and lexical category. Log frequency is a notably weak predictor on its own, though it does still contribute significant predictive power when added to a model including all other predictors. These results largely reproduce those of (Braginsky et al., 2016) and Chang and Bergen (2022), the latter of whom reported an adjusted R2 of 0.43 for predicting child AoA from all features using a larger vocabulary of 571 words. In predicting LMs' AoA, we identify two main patterns. First, the signatures 61+ and \u00f4 exhibit negligible relationships with any predictor. Second, among the other signatures, log frequency is consistently the most predictive factor, mirroring the findings of Chang and Bergen (2022). The next most predictive factors are concreteness and lexical category. See the figures in App. G or visualizations of AoAs against each predictor.\nFor brevity, we focus on results from the Unified dataset. Across all signatures and datasets, language models (LMs) exhibit the opposite pattern from children when it comes to log frequency and concreteness, with more frequent words having a lower AoA. While children tend to acquire concrete words earlier, language models appear to struggle more with processing concrete words and perform better with abstract ones. Furthermore, although children's AoA does not display a significant correlation with number of characters, most LM signatures reveal positive correlations. The exceptions are \u00d4R- and 81+ (for BabyLM and CHILDES), which show slightly negative correlations. Lastly, MLU follows a similar pattern in both children and language models.\nDoes more developmentally plausible training data result in more human-like learning patterns? From Fig. 2, we observe that the models trained on BabyLM tend to exhibit the most human-like learning trajectories according to some signatures; however, as stated above, LM trajectories are far from human-like across the board. This finding is surprising given that CHILDES, which comes from discourses between caregivers and young children, most closely resembles the input received by the young children studied in Wordbank. However, since all three datasets differ greatly in size, we cannot determine whether this result is due to data domain or dataset size. By analyzing Table A8, we find that there is a positive effect of the training set size on the predictability of AoA. We speculate that this may explain why Chang and Bergen"}, {"title": "8 Discussion and Conclusion", "content": "Our main objective was to explore the space of distributional signatures of words more comprehensively, with an application to understanding word learning in LMs. We showed that the distributional test adopted by Chang and Bergen (2022) and Portelance et al. (2023) can be viewed as an estimator of a more general distributional signature. This insight also enabled us to define a broader family of signatures that follow a clear typology. However, the question remains: which of these evaluations should be the focus of researchers interested in studying word learning in LMs? One of our key findings in \u00a76.3 is that many of these signatures are complementary. This is true with respect to children's AoAs as well as in comparison to each other. Arguably, considering both positive and negative contexts provides a more complete picture of the LM's distributional knowledge, and comparing the LM's distribution against an LLM allows the signature to better reflect the gradient of the ground truth distribution, which is not directly observable. Nonetheless, each signature we propose has a clear interpretation and may be useful for specific applications, though extracting usable AoA scores is not always feasible.\nIn \u00a77 we found that we could not predict children's AoAs well from any of our proposed distributional signatures. This result might seem somewhat surprising in light of Portelance et al.'s (2023) finding that LM surprisal improves predictions of children's AoAs. However, we note that that work uses + at the end of training as a predictor of AoA, rather than the AoA of the model as determined by a specific distributional signature. Our results do further corroborate Chang and Bergen's (2022) conclusions on this question, and significantly expand them to a wider variety of signatures. Additionally, our findings contribute to a growing body of work finding specific differences between the language learning patterns of humans and LMs in other domains (e.g., Evanson et al., 2023; Constantinescu et al., 2024). On the other hand, Zhuang et al. (2024b,a) show that multimodal LMs can exhibit more human-like learning trajectories and also introduce a novel training objective that further improves human-likeness. Future work should apply our distributional tests to these and other potentially more human-like training procedures. Besides learning in a world grounded in sensory experience, children also learn through interaction both with the physical world and with other agents (Clark, 2018; Nikolaus and Fourtassi, 2023). Moreover, unlike LMs, children have constraints on production, going through one-word and two-word utterance phases (Bloom, 1970). These factors no doubt influence the kinds of words children use early in development and may account for the precedence of concrete words. There are only a few examples of training regimes for LMs inspired by interaction (Lazaridou et al., 2020; Nikolaus and Fourtassi, 2021b; Ma et al., 2024). Furthermore, the reliance on stochastic gradient descent and cross-entropy loss likely skew learning trajectories in LMs in ways that are not entirely human-like. There are many opportunities to explore more human-like LM training, and we expect that word learning will be an important evaluation of human likeness as these are explored.\nHaving better mapped out the space of distributional signatures of lexical knowledge, our work paves the way for comparing trajectories of language models and humans. Our findings provide strong empirical support that there are large differences between how human and artificial language learners develop throughout learning and draw attention to the fact that there is significant work to be done to explore pre-training methods and datasets that result in more developmentally plausible language models."}, {"title": "9 Limitations", "content": "Our study has several limitations. First, while we are interested in the possibility that LMs can be used as cognitive models and we attempt to use developmentally plausible data, our LMs are not trained in a way that is maximally similar to how humans learn. They lack exposure to speech, grounding, and interaction with other agents, all of which may significantly influence word learning. Second, while our proposed true and reference signatures are weighted by a distribution pr, we only estimate this distribution using Monte Carlo estimation. Future work should explore whether alternative estimation techniques, such as those based on LLMs, yield qualitatively different results. Third, the specifics of our findings may be sensitive to our training setup. Future work should examine whether different pre-training settings yield qualitatively different results, i.e., whether our findings are robust across various setups. Finally, our study focuses on extracting AoAs from learning trajectories, but AoA is just one statistic that can be extracted from the learning trajectory."}, {"title": "A The Intrinsic Signature", "content": "We now overview the intrinsic signature in more detail, which was omitted from \u00a73.\nA.1 An Intrinsic Metric\nWe develop an intrinsic metric, i.e., a metric that does not relay, in expectation, on the true language model p. Thus, we consider the following information-theoretic quantity that resembles Eq. (6), but where the expectation is taken with respect to the model itself:\n\ndef\n\u03c3I+ = \u2211 px(c | w) log q(w | c). (1)\nc\u2208\u03a3*\nIn contrast to Chang and Bergen's (2022) distribution signature, Eq. (1) is not grounded in an external language model. Thus, it measures a notion of knowledge internal to the language model itself. We can also, by analogy to Eq. (9), define an intrinsic metric that considers just negative contexts\n\ndef\n\u03c3\u03b9- = \u2211 px(c | \u00acw) log q(w | c), (2)\nc\u2208\u03a3*\nand one that considers all contexts\n\ndef\n\u03c3\u0399 = \u2211 px(c) log q(w | c). (3)\nc\u2208\u03a3*\nA.2 A Practical Estimator\nWe now discuss a scheme to estimate Eq. (1). First, we note that, by Bayes' rule, we have\npx(c | w) = q(w | c) q(c)/\u2211c\u2208\u03a3* q(w | c)q(c) (4)\nInstead, we consider the following approximation. Given a bag of contexts C = (c(m)) SM-1 that proceed a word w, we construct the following empirical approximation\nIk (c | w) = M1 {c \u2208 C} q(w | c) qk(c)/\u2211m=1M q(w | c(m)) qk(c(m)) (5)\nPlugging Eq. (5) into Eq. (1), we arrive at\n\ndef\n\u00d4I+ = \u2211 Ik(c(m) | w) log q(w | c(m)), (6)\nm=1\nIn the limiting case, i.e., when C includes all of \u2211*, we have 81+ \u2192 01+. Note that Eq. (6) is not a standard Monte Carlo estimator as the contexts c(m) may not have been drawn from \uac12(\u00b7 | w), but it is still consistent. An analogous estimator can be derived for Eq. (2) and Eq. (3)."}, {"title": "B The Signature \u03c3R+, GR\u2212 and oR+ are Distance Metrics", "content": "The reference signatures as introduced in \u00a73 can be easily shown to be distance metrics. Let Xc def logpx(c | w) and Yc def log(w | c), we can rewrite the signatures as follows:\n\u03c3R+ = \u2211 px(c | w) |xc - Yc| (7a)\nc\u2208\u03a3*\n\u2211 px(c | \u00acw) |xc - Yc| (7b)\nc\u2208\u03a3*\n\u2211 px(c) |xc - Yc. (7c)\nc\u2208\u03a3*\nOR- =\nOR =\nBecause pr(c | w), px(c | \u00acw) and pr(c) are all greater than zero, the expressions above represent weighted Manhattan distances, which is a known distance metric."}, {"title": "C Training Details", "content": "The training was conducted in parallel across 8 GPUs, with gradient accumulation (Hermans et al., 2017) set to 16 and a batch size per device of 4. As a result, our model was trained with an effective batch size of 512."}, {"title": "D Convergence with the Cauchy Criterion", "content": "To judge the convergence of learning trajectories that are non-monotonic and may not have a well-formed shape, we develop a novel technique based on the idea of a Cauchy sequence. Let \u03c3(w, t) the value that the signature of the word w assumes at time-step t. For a fixed tolerance parameter \u20ac > 0, the age of acquisition AoA is defined as\n\nAoA = \u03b1 (\u03c3, w) = argmin ( max |\u03c3(w, s) \u2212 \u03c3(w, s')| < \u03b5) . (8)\nte{1,...,T} \ns,s'\u2208{t,...,T}\nThis definition mirrors the definition of the convergence of a Cauchy sequence, albeit for a finite sequence. However, because T is finite, for small enough e, we do not, in general, observe true convergence in the analytic sense. Thus, the tolerance parameter e is best viewed as a hyperparameter, and our findings are dependent on the choice of e. However, given that nearly all learning algorithms are analyzed by letting T\u2192 \u221e, there is a sense in which our definition of AoA is well-founded. Specifically, if we assume the convergence of the learning algorithm as T \u2192 \u221e implies the convergence of o, then, for every \u20ac > 0, there exists a number of epochs such that we will achieve \u03b1 (\u03c3, \u03c9)."}, {"title": "H Regression analysis", "content": "In the following regressions, we denote log frequency as LF, concreteness as Co, number of characters as NC, mean length of utterances as MLU, and lexical category as LC. No VIF value exceeds 5, indicating that while some multicollinearity may be present, it is not severe.\nH.1 Regression on Child Learning Data"}, {"title": "H.2 Regression on Language Model Learning Data", "content": "The table below reports the Adjusted R2 values for each predictor across all datasets and signatures, as introduced in App. G. Consistent with previous analyses, the AoA values were computed using a e = 0.07. The table shows the number of words included in the regression analysis, counting only those that remained after outlier removal and successfully achieving convergence across all three seeds. Among all the signatures, 6+ is the one that demonstrates the strongest predictive power across nearly all predictors."}, {"title": "I The Impact of Convergence Thresholds on AoA Extraction", "content": "The Pearson correlations reported in Fig. A5 illustrate how the AoA values extracted using varying e correlate. This analysis aims to determine whether the choice of e significantly impacts the results. As discussed in \u00a76.2, with few exceptions, the results across different e values show high correlations. Therefore, our analysis will remain consistent regardless of the choice of e."}, {"title": "J Correlation of Various AoA", "content": "The figures in this section show how each signature's AoA values correlate with one another across each model trained with different datasets. As a result, the dataset itself influences the correlation patterns among the different signatures. For example, the Unified dataset displays only positive correlations, whereas Childes and BabyLM datasets exhibit negative correlations for 81+ and \u00d4R-. The AoA values were extracted using \u20ac = 0.07."}]}