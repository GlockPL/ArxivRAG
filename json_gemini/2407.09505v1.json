{"title": "1-Lipschitz Neural Distance Fields", "authors": ["Guillaume Coiffier", "Louis B\u00e9thune"], "abstract": "Neural implicit surfaces are a promising tool for geometry processing that represent a solid object as the zero level set of a neural network. Usually trained to approximate a signed distance function of the considered object, these methods exhibit great visual fidelity and quality near the surface, yet their properties tend to degrade with distance, making geometrical queries hard to perform without the help of complex range analysis techniques. Based on recent advancements in Lipschitz neural networks, we introduce a new method for approximating the signed distance function of a given object. As our neural function is made 1-Lipschitz by construction, it cannot overestimate the distance, which guarantees robustness even far from the surface. Moreover, the 1-Lipschitz constraint allows us to use a different loss function, called the hinge-Kantorovitch-Rubinstein loss, which pushes the gradient as close to unit-norm as possible, thus reducing computation costs in iterative queries. As this loss function only needs a rough estimate of occupancy to be optimized, this means that the true distance function need not to be known. We are therefore able to compute neural implicit representations of even bad quality geometry such as noisy point clouds or triangle soups. We demonstrate that our methods is able to approximate the distance function of any closed or open surfaces or curves in the plane or in space, while still allowing sphere tracing or closest point projections to be performed robustly.", "sections": [{"title": "1. Introduction", "content": "Implicit surfaces [BB97] are a powerful tool for geometric model-ing and computer graphics, with direct applications in constructive solid geometry, rendering or surface reconstruction. Unlike explicit representations like point clouds, surface meshes or voxel grids, which rely on a discretization of space, an implicit representation involves defining an object as the zero level set of a continuous function. In the last few years, this idea have received a lot of atten-tion with the introduction of neural implicit surfaces, which encode the function as the parameters of a neural network, allowing such representations to be computed for arbitrary input shapes.\nInfinitely many implicit functions can correspond to the same ge-ometry, yet not all of them are created equal: properties of the func-tion sometimes need to also be preserved far from the zero level set. A useful implicit representation to consider in these contexts is a signed distance function (SDF), which outputs the distance to"}, {"title": "2. Background and Related Work", "content": "the boundary of its underlying object, counted negatively for points that are inside. A SDF has a unit-norm gradient almost everywhere, making it a 1-Lipschitz function. Having a 1-Lipschitz implicit rep-resentation is a necessary condition for applications like ray march-ing [Har95], numerical simulation [SS03, SC20] or geometrical queries like surface projection to be performed easily. When com-puting an approximated SDF, it is indeed crucial to never overesti-mate the true distance otherwise correctness of the queries cannot be guaranteed. In practice, this means that the function's Lipschitz constant should never exceed 1. However, this Lipschitz property is often overlooked by neural implicit methods, which rather focus on surface fidelity and detail preservation, making them unusable in these contexts without relying on careful range analysis [SJ22].\nIn this work, we propose a method to approximate the signed distance function of an object by using neural network architec-tures that are 1-Lipschitz by construction [AHD*23], thus guaran-teeing correctness of geometrical queries even during training. The Lipschitz constraint of these neural architectures allow us to utilize a different loss function, called the hinge-Kantorovitch-Rubinstein (hKR) loss [SMG*21]. This loss has two important effects. Firstly, we prove that any minimizer over all possible Lipschitz functions is close to the SDF of the considered object, which makes our trained neural network a very good approximation of the true distance, as illustrated in Figure 1. Secondly, using the hKR loss makes us ap-proach the problem of learning a signed distance field not from the usual supervised regression point of view but from a semi-supervised classification point of view: instead of fitting a neural network's output to precomputed distances over a dataset of points, we instead try to maximize the distance between points from in-side the shape and points from outside while remaining 1-Lipschitz. This means in particular that the only information required for training is knowing in which category (inside or outside of the input shape) a point is, an information that can be robustly extracted even for point clouds or triangle soups [BDS* 18]. As a consequence, we are able to approximate the SDF of an object without access to the ground truth distance, enabling training for a wide range of inputs including triangle soups and point clouds, even noisy, sparse or in-complete.\nTo summarize, our contributions are as follows:\n\u2022 We apply the known method of minimizing the hKR loss on some 1-Lipschitz neural network to the problem of approximat-ing the signed distance field of an object.\n\u2022 We demonstrate that such an approach solves the usual robust-ness issues of similar methods, as it outputs a function that is a good approximation of the true signed distance function while being guaranteed to never overestimate it.\n\u2022 As the hKR loss does not need ground truth distances but only occupancy labels, we show that we are able to compute signed or unsigned distance fields of noisy, incomplete or sparse represen-tations of objects of any topology, including open surfaces and curves.\n\u2022 We apply our method to a variety of geometry processing tasks, like surface sampling, medial axis estimation, constructive solid geometry and ray marching."}, {"title": "2.1. Signed Distance Function", "content": "In all of this work, we will denote by \u03a9 some solid object R", "latex": ["S_{\\Omega}(x) = \\begin{cases}\n  \\\\ d(x,\\partial \\Omega) & x \\in \\Omega \\\\\n  -d(x,\\partial \\Omega) & x \\not \\in \\Omega\n\\end{cases}"]}, {"title": "2.2. Lipschitz Implicit Representations", "content": "Although the SDF of an object is easy to compute in closed form for simple shapes (see for instance [Quia] for a list), the same can-not be said of objects found in the wild. Representing those objects via a general implicit surface or even an approximated SDF can still achieve high visual fidelity but comes at a cost. In such con-texts, there is indeed no direct strategy for closest point queries or ray intersections: one has to rely to iterative methods, where a key"}, {"title": "2.3. Neural Implicit Surfaces", "content": "Approximating a distance function has historically been achieved using basis functions like blobs or blended balls [Bli82, WMW86]. In the last few years, an ongoing trend has proposed to encode it into the parameters of a multilayer perceptron (MLP) fe. This idea of a neural field has sparked many applications in computer graphics and learning, which are not restricted to distance fields. We refer to Xie et al. [XTS*22] for a survey.\nPerhaps the most simple neural implicit representation is to rep-resent the shape as a binary occupancy field, predicting 1 for points inside the shape and 0 otherwise [CZ19]. This can be seen as a bi-nary classification problem and treated as such [MON*19]. While enabling total surface reconstruction, such neural fields give no in-formation far from the surface and are therefore hard to query geo-metrically.\nThe DeepSDF [PFS*19] algorithm is the first proposition of a neural SDF. It is setup as a single large neural network optimized over a collection of objects, where a given object is represented via a latent vector fed as an input along the query point. This popular setup allows shape interpolation [LWJ*22], classification as well as shape segmentation [PGMK23]. In contrast, training one net-work per object has also been performed [DNJ21] for shape com-pression purposes. Learning an unsigned distance field has also been attempted either directly [CmP20] or using a sign-agnostic loss [AL20a], thus extending the application of neural distance fields to open surfaces and curves. All of these methods are su-pervised, meaning that they are optimized to make the network's"}, {"title": "3. Robust Learning of a Signed Distance Function", "content": "As current neural implicit representations cannot provide guaran-tees on geometrical queries from the implicit function nor theoret-ical bounds on its Lipschitz constant, we propose to directly inte-grate the constraint of being 1-Lipschitz directly into the neural ar-chitecture. As shown experimentally in Figure 3 (c), this will result in an implicit function that cannot overestimate the true distance by construction, even during training.\nOur method takes as an input any curve or surface \u03a9 from R", "latex": ["x \\rightarrow x - \\frac{2W^T \\sigma(W^T x + b)}{\\sqrt{\\Sigma_{j=1}^k (W^T W)_{ij} \\exp{(q_j - q_i)}}}"]}, {"title": "3.1. 1-Lipschitz Neural Architecture", "content": "Classically, a neural network fo has its parameters arranged in a series of layers f\u00b9, ..., f\u00b9 so that the final function is the composition of all layers in order. As the Lipschitz constant of a composition is upper bounded by the product of all Lipschitz constants, designing a 1-Lipschitz architectures boils down to defining some 1-Lipschitz layers to be chained together. To this end, Araujo et al. [AHD*23] propose the Semi-definite Programming Lipschitz Layer (SLL). Us-ing a square matrix W \u2208 R", "latex": ["T_{ii} = \\Sigma_{j=1}^k (WW^T)_{ij} \\exp(q_j-q_i)"]}, {"title": "3.2. The hinge-Kantorovitch-Rubinstein Loss Function", "content": "Given a dataset (X, Y) of points with associated signed distance ground truth, a straightforward approach to learn a neural signed distance field guaranteed to always underestimate the true distance would be to minimize a fitting loss over a 1-Lipschitz architecture as defined above. While it solves the problem of robustness of neu-ral signed distance fields, this approach fails short of our goal for"}, {"title": "3.3. Inside/Outside Partitionning", "content": "Let us first consider the case where the input represents a closed curve in the plane or a closed surface. If the input geometry is clean enough, like for instance a manifold surface mesh, determin-ing its inside from its outside is a task that can be solved in a ro-bust way [SSS74]. When the geometry presents holes, defects, or is simply made of points, a notion of \"insideness\" can still be recov-ered by computing the generalized winding number [JKS13]. Intu-itively, the winding number wo of a surface \u2202 at point x is the sum of signed solid angles between x and surface patches on \u2202. For a closed smooth manifold, the values amounts at how many times the surface \"winds around\" x, yielding an integer value. When computed on imperfect geometries, wo becomes a continuous function (see Figure 5). Through careful thresholding, it is still possible to determine points that are inside or outside the shape with high con-fidence.\nGoing back to our problem of learning a signed distance func-tion from surface data d\u03a9, we first sample points X uniformly in"}, {"title": "3.4. Distance field of shapes without interior", "content": "If the input object \u03a9 has no interior, like the case of a curve or an open surface, we can still learn a distance function to \u03a9 by min-imizing the hKR loss without relying on the generalized winding number. We simply sample Xin as being points on the surface or curve, either by taking directly the point cloud as input or sampling on triangles. Xout is then a uniform distribution over the domain D. Up to the margin parameter m in the loss, this still results in an approximation of the distance function of the considered mani-fold. But since level sets of the neural function can only be closed surfaces, the optimization results in an object having a \"thickness\". This property is directly controlled by the margin parameter m in the loss, as shown in Figure 6: a larger margin leads to reconstruc-tion of the data with a non-negligible thickness, whereas a margin too small leads to instabilities on the final result. Figure 12 further demonstrates the SDF reconstruction ability of our method in three dimensions in the context of open surfaces or curves. Note that this setup is also suitable for closed surfaces in order to learn an un-signed distance field, as it is the case in Figure 4."}, {"title": "4. Results and Applications", "content": "We implement our method in python using the Pytorch library for neural network training. All our experiments were performed on a Ubuntu 22 workstation using a Nvidia 4070Ti GPU. For the gen-eralized winding number, we use the original implementation of Barill et al. [BDS*18] as provided in libigl [JP*18].\nWe use a neural network architecture of 20 SLL layers of size k = 128. In total, this amounts for 330K floating point parameters for a total size of approximately 1.4MB. For reference, the hand mesh used in Figure 7 is described by 150K floating point numbers for vertices and 300K integers for faces, for a total size of 2.2MB when compressed.\nAll inputs are normalized in a bounding box [-1; 2] before any processing. We set D = [-1,1] as our sampling domain. With the exception of Figure 6, all our experiments were performed with m = 10-2 and \u03bb = 100."}, {"title": "4.2. Gradient Robustness", "content": "As a first experiment, we demonstrate the robustness of our ap-proach in comparison with neural distance field methods from the state of the art. These methods are trained on a dataset of points (xi, So(xi)) with So being precomputed from the input mesh. They minimize a least-square fitting loss:"}, {"title": "4.3. Signed vs Unsigned Distance Field", "content": "As our method provides stable distance estimation far from the zero level set, we are also able to extract high quality isosurfaces for dif-ferent values of the function. This is illustrated on Figure 1 for the signed case (using the generalized winding number) and on Fig-ure 12 in the unsigned case, for a curve and an open surface with holes. Additional results are available in supplemental material.\nWhile fitting an unsigned distance also works perfectly fine for a closed object, partitioning the points first with generalized winding number enables our method to benefit from its robustness to faulty and noisy input. As a result, some defects of the zero level set can be repaired, as it is the case with the corrupted botijo model shown in Figure 4."}, {"title": "4.4. Underestimation of the true distance field", "content": "To further justify our claim that our learned signed distance field never overestimates the true distance, we train different neural dis-tance fields on a handle model (Figure 8). We then extract the zero level set of each method using marching cubes [LC87] and sample 100K points uniformly in a sphere of radius 30 centered at zero. We plot the difference between the true distance S(x) to the zero level set mesh and the predicted value of the neural network, in func-tion of S(x). In this experiment, a perfect network would output a straight line, but all methods present some deviation in practice. However, we observe that only our method provide only negative values, meaning that the network's output is always smaller than the true distance."}, {"title": "4.5. Geometrical Queries", "content": "Having a robust SDF far from the surface enables efficient and ro-bust geometrical queries on any of its level sets. In this section, we demonstrate experimentally that it is indeed possible to do on our Lipschitz network trained with the hKR loss, without requiring neither range analysis nor an estimation of the Lipschitz constant."}, {"title": "4.6. Constructive Solid Geometry", "content": "A key feature of implicit geometries is the ease with which we can apply boolean operations on their function to represent inter-sections, unions or differences of shapes [Ric73], thus building more complex shapes for simple ones. We illustrate this property on our neural distance field on Figure 10: given two SDFs f\u2081 and f2, we can compute the surface of their union as the 0-level set of min(f1, f2). Similarly, their intersection coincides with the 0-level set of max(f1, f2). These operations preserve the Lipschitz constant so the guarantee of being 1-Lipschitz still holds for the composition. However these point-wise minimum and maximum are not necessarily SDFs themselves [Quib, MSLJ23]."}, {"title": "4.7. Robustness to noise and sparsity of input", "content": "Finally, approaching the neural distance field problem with the hKR loss means that we are still able to recover a good approxi-mation of the SDF of an object in contexts where the ground truth distance cannot be computed or is not available. We illustrate this"}, {"title": "5. Discussion and conclusion", "content": "Neural distance fields are a promising technique for representing arbitrary geometry without relying on a discretization of space. While previous methods have demonstrated outstanding results in surface reconstruction and visual fidelity, we demonstrated that these neural functions could also be made 1-Lipschitz and support geometrical queries without the need of range analysis. Having this 1-Lipschitz constraint also allowed to learn a distance field with-out needing any distance information from the input shape, thus enabling neural fields applications for a broader set of geometry representations, even including bad quality point clouds or triangle soups.\nYet, this built-in robustness did not come without drawbacks. The hKR loss can only be successfully minimized up to a mar-gin m > 0, which prevents a perfectly accurate surface reconstruc-tion. Smaller margin allows for more marginally more fine-grained details to be captured, but the resulting functions are usually bi-ased towards low frequencies. Some recent neural methods solve this problem by using positional encoding at the start of the net-work [Lip21], like the Fourier features [TSM*20]. A network us-ing such an encoding could still be made 1-Lipschitz: as the Lip-schitz constant K of the embedding can be known in closed form (it depends on the frequencies of the harmonic functions), it suf-fices to build a 1/K-Lipschitz network by scaling the output. Yet, as the embedding does not preserve the gradient norm, so will the hKR-trained final network, which could lead to significant under-estimation of the true distance. Improving reconstruction quality while being almost gradient preserving thus remains a challenge for future works.\nOverall, for a fixed number of parameters, there seems to be a clear trade-off between quality of the zero level set and qual-ity of the gradient far from it, an observation that has already been made in classification contexts regarding 1-Lipschitz net-works [BBS*22]. For a network of bounded depth, the zero level set is made of affine pieces whose maximum number grows as a polynomial of the width [Tel16, Yar18, BHLM19, PKH23], which would require a similar increase in numbers of parameters.\nFinally, as Lipschitz networks are computationally heavier than their classical counterpart, future works could also focus on im-proving speed of convergence of the method, for example by re-ducing the total number of points needed in the dataset. This could"}, {"title": "1 Implementation Details", "content": "We implement our method in python using the Pytorch library for neural network training. All our experiments were performed on a Ubuntu 22 workstation using a Nvidia 4070Ti GPU. For the gen-eralized winding number, we use the original implementation of Barill et al. [BDS*18] as provided in libigl [JP*18].\nWe use a neural network architecture of 20 SLL layers of size k = 128. In total, this amounts for 330K floating point parameters for a total size of approximately 1.4MB. For reference, the hand mesh used in Figure 7 is described by 150K floating point numbers for vertices and 300K integers for faces, for a total size of 2.2MB when compressed.\nAll inputs are normalized in a bounding box [-1; 2]3 before any processing. We set D = [-1,1]3 as our sampling domain. With the exception of Figure 6, all our experiments were performed with m = 10\u22122 and \u03bb = 100."}]}