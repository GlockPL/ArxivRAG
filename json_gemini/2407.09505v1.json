{"title": "1-Lipschitz Neural Distance Fields", "authors": ["Guillaume Coiffier", "Louis B\u00e9thune"], "abstract": "Neural implicit surfaces are a promising tool for geometry processing that represent a solid object as the zero level set of a neural network. Usually trained to approximate a signed distance function of the considered object, these methods exhibit great visual fidelity and quality near the surface, yet their properties tend to degrade with distance, making geometrical queries hard to perform without the help of complex range analysis techniques. Based on recent advancements in Lipschitz neural networks, we introduce a new method for approximating the signed distance function of a given object. As our neural function is made 1-Lipschitz by construction, it cannot overestimate the distance, which guarantees robustness even far from the surface. Moreover, the 1-Lipschitz constraint allows us to use a different loss function, called the hinge-Kantorovitch-Rubinstein loss, which pushes the gradient as close to unit-norm as possible, thus reducing computation costs in iterative queries. As this loss function only needs a rough estimate of occupancy to be optimized, this means that the true distance function need not to be known. We are therefore able to compute neural implicit representations of even bad quality geometry such as noisy point clouds or triangle soups. We demonstrate that our methods is able to approximate the distance function of any closed or open surfaces or curves in the plane or in space, while still allowing sphere tracing or closest point projections to be performed robustly.", "sections": [{"title": "1. Introduction", "content": "Implicit surfaces [BB97] are a powerful tool for geometric modeling and computer graphics, with direct applications in constructive solid geometry, rendering or surface reconstruction. Unlike explicit representations like point clouds, surface meshes or voxel grids, which rely on a discretization of space, an implicit representation involves defining an object as the zero level set of a continuous function. In the last few years, this idea have received a lot of atten- tion with the introduction of neural implicit surfaces, which encode the function as the parameters of a neural network, allowing such representations to be computed for arbitrary input shapes.\nInfinitely many implicit functions can correspond to the same geometry, yet not all of them are created equal: properties of the function sometimes need to also be preserved far from the zero level set. A useful implicit representation to consider in these contexts is a signed distance function (SDF), which outputs the distance to"}, {"title": "2. Background and Related Work", "content": "In all of this work, we will denote by \u03a9 some solid object $R^n$, where n = 2 or 3. The signed distance function (SDF) of 2 is the function\n$S_\u03a9$ defined over $R^n$ as:\n$$S_\u03a9(x) = (1_{R^n \\\u03a9}(x) - 1_\u03a9(x)) \\min_{p \u2208 \u2202\u03a9} ||x-p||$$\nwhere $\u2202\u03a9$ is the boundary of \u03a9 and the distance considered in the Euclidean distance.\nSigned distance functions have mainly been studied in computer graphics for the ease with which they enable certain operations like boolean composition [Ric73], smooth blending [Bli82], surface offset [FP06] or deformation [SP86] while still allowing an explicit representation, like a surface mesh, to be extracted for instance using the marching cubes algorithm [LC87, dLJ*15]. In essence, the SDF value at point x gives two pieces of information: its sign directly tells if the query point is inside or outside the object, while its magnitude gives the radius of the largest sphere centered at x that does not intersect the boundary of the object. This observation is the starting point of the sphere tracing algorithm [Har95] which enables direct rendering of SDFs. Additionally, the gradient of the SDF is aligned with the normal vector field of the object on its boundary and gives the direction to the closest point on the boundary. Evaluating the function and its gradient at a point therefore gives a simple strategy for projecting onto the zero level set."}, {"title": "2.2. Lipschitz Implicit Representations", "content": "Although the SDF of an object is easy to compute in closed form for simple shapes (see for instance [Quia] for a list), the same cannot be said of objects found in the wild. Representing those objects via a general implicit surface or even an approximated SDF can still achieve high visual fidelity but comes at a cost. In such contexts, there is indeed no direct strategy for closest point queries or ray intersections: one has to rely to iterative methods, where a key"}, {"title": "2.3. Neural Implicit Surfaces", "content": "Approximating a distance function has historically been achieved using basis functions like blobs or blended balls [Bli82, WMW86]. In the last few years, an ongoing trend has proposed to encode it into the parameters of a multilayer perceptron (MLP) $f_\u03b8$. This idea of a neural field has sparked many applications in computer graphics and learning, which are not restricted to distance fields. We refer to Xie et al. [XTS*22] for a survey.\nPerhaps the most simple neural implicit representation is to represent the shape as a binary occupancy field, predicting 1 for points inside the shape and 0 otherwise [CZ19]. This can be seen as a binary classification problem and treated as such [MON*19]. While enabling total surface reconstruction, such neural fields give no information far from the surface and are therefore hard to query geometrically.\nThe DeepSDF [PFS*19] algorithm is the first proposition of a neural SDF. It is setup as a single large neural network optimized over a collection of objects, where a given object is represented via a latent vector fed as an input along the query point. This popular setup allows shape interpolation [LWJ*22], classification as well as shape segmentation [PGMK23]. In contrast, training one network per object has also been performed [DNJ21] for shape compression purposes. Learning an unsigned distance field has also been attempted either directly [CmP20] or using a sign-agnostic loss [AL20a], thus extending the application of neural distance fields to open surfaces and curves. All of these methods are supervised, meaning that they are optimized to make the network's"}, {"title": "2.4. Lipschitz Neural Networks", "content": "At its core, a neural network is nothing more than a function $f_\u03b8$ where the parameters (or weights) $\u03b8 \u2208 R^K$ are arranged in a pre- determined pattern. Specifying such an architecture defines a functional space:\n$$F = \\{f_\u03b8 | \u03b8 \u2208 R^K \\}$$\nover which learning algorithms optimize the weights to find the function $f_\u03b8$ that minimizes some user-defined loss criterion. Knowing exactly the extent of the functional space F for a fixed architecture is an open problem in deep learning, but recent works have managed to define some F as a subset of L-Lipschitz functions [SMG*21].\nThe investigation of Lipschitz architecture in deep learning is primarily motivated by the robustness of such networks against adversarial attacks and overfitting. Early attempts focused on regularizing the weight matrices by controlling their largest singular value [YM17]. To prevent the gradient from vanishing, other efforts focused on having singular values all close to one, namely regularizing weight matrices to be orthogonal [CBG*17, TK20] and the network to be gradient preserving. Since this hindered expressiveness overall when using usual component-wise activation functions like the sigmoid or ReLU, Anil et al. [ALG19] propose a sort as a non-linearity. Lispchitz network have since been shown to have comparable results with classical neural networks on a variety of tasks [BBS*22].\nMore recently, other constructions of Lipschitz neural layers decreasing the computational cost of previous approaches have been proposed. Instead of relying on iterative projections of weight matrices to orthogonal ones, Prach and Lampert [PL22] introduce an almost orthogonal layer where singular values of the matrices are updated directly during training. Other Lipschitz layers have then been defined, such as the Convex Potential Layer [MDAA22] or the Semi-definite Programming Lipschitz Layer (SLL) [AHD*23]. Neural architectures used in this work are based on the latter."}, {"title": "3. Robust Learning of a Signed Distance Function", "content": "As current neural implicit representations cannot provide guarantees on geometrical queries from the implicit function nor theoretical bounds on its Lipschitz constant, we propose to directly integrate the constraint of being 1-Lipschitz directly into the neural architecture. As shown experimentally in Figure 3 (c), this will result in an implicit function that cannot overestimate the true distance by construction, even during training.\nOur method takes as an input any curve or surface $\u2202\u03a9$ from $R^n$, represented either by a point cloud with normals or a triangle soup. The first step is to define some 1-Lipschitz neural architecture, for which we use the SLL architecture of Araujo et al. [AHD*23] (Section 3.1). As having a Lipschitz constant strictly smaller than 1 can induce greater computation times for geometrical queries to converge, it is desired to not only be 1-Lipschitz but to have gradient as close as possible to unit norm everywhere. In our case,"}, {"title": "3.1. 1-Lipschitz Neural Architecture", "content": "Classically, a neural network $f_\u03b8$ has its parameters $\u03b8$ arranged in a series of layers $f^1, ..., f^L$ so that the final function is the composition of all layers in order. As the Lipschitz constant of a composition is upper bounded by the product of all Lipschitz constants, designing a 1-Lipschitz architectures boils down to defining some 1-Lipschitz layers to be chained together. To this end, Araujo et al. [AHD*23] propose the Semi-definite Programming Lipschitz Layer (SLL). Using a square matrix $W \u2208 R^{k\u00d7k}$, a bias vector $b \u2208 R^k$ and an additional vector $q \u2208 R^k$ as parameters, it is defined as:\n$$x \u2192 x\u22122W T^{-1} \u03c3(W^Tx+b)$$\nwhere T is a diagonal matrix of size $R^{k\u00d7k}$.\n$$T_{ii} = \u2211_j (WW^T)_{ij} exp(q_j-q_i)$$\nand $\u03c3(x) = max(0,x)$ is the rectified linear unit (ReLU) function. In comparison to the classical multilayer perceptron layer $x \u2192 \u03c3(W^Tx + b)$, the SLL layer only adds a small amount of parameters in the form of the vector q and a $O(k^2)$ operations, which makes it more efficient than previous Lipschitz architectures.\nThe SLL function is a residual layer, meaning that its computation is added to its input. As a consequence, the layer can only be defined for matching input and output dimensions. In our case, the input of the network is a point in $R^n$ with n = 2 or 3 and its output is a single real number. The input of the network is therefore first padded with zeros to match the size k of the SLL layers. To retrieve a single number as output, the network ends with an affine layer defined as:\n$$x \u2192 \\frac{w^Tx}{||w||} +b$$\nwhere $w \u2208 R^k$ and $b \u2208 R$. Dividing by the euclidean norm of w in the computation ensures that the operation is 1-Lipschitz."}, {"title": "3.2. The hinge-Kantorovitch-Rubinstein Loss Function", "content": "Given a dataset (X, Y) of points with associated signed distance ground truth, a straightforward approach to learn a neural signed distance field guaranteed to always underestimate the true distance would be to minimize a fitting loss over a 1-Lipschitz architecture as defined above. While it solves the problem of robustness of neural signed distance fields, this approach falls short of our goal for"}, {"title": "3.3. Inside/Outside Partitionning", "content": "Let us first consider the case where the input \u2202\u03a9 represents a closed curve in the plane or a closed surface. If the input geometry is clean enough, like for instance a manifold surface mesh, determining its inside from its outside is a task that can be solved in a robust way [SSS74]. When the geometry presents holes, defects, or is simply made of points, a notion of \"insideness\" can still be recovered by computing the generalized winding number [JKS13]. Intuitively, the winding number $w_\u03a9$ of a surface $\u2202\u03a9$ at point x is the sum of signed solid angles between x and surface patches on $\u2202\u03a9$. For a closed smooth manifold, the values amounts at how many times the surface \"winds around\" x, yielding an integer value. When computed on imperfect geometries, $w_\u03a9$ becomes a continuous function (see Figure 5). Through careful thresholding, it is still possible to determine points that are inside or outside the shape with high confidence.\nGoing back to our problem of learning a signed distance function from surface data $\u2202\u03a9$, we first sample points X uniformly in-"}, {"title": "3.4. Distance field of shapes without interior", "content": "If the input object \u03a9 has no interior, like the case of a curve or an open surface, we can still learn a distance function to \u03a9 by minimizing the hKR loss without relying on the generalized winding number. We simply sample $X_{in}$ as being points on the surface or curve, either by taking directly the point cloud as input or sampling on triangles. $X_{out}$ is then a uniform distribution over the domain D. Up to the margin parameter m in the loss, this still results in an approximation of the distance function of the considered manifold. But since level sets of the neural function can only be closed surfaces, the optimization results in an object having a \"thickness\". This property is directly controlled by the margin parameter m in the loss, as shown in Figure 6: a larger margin leads to reconstruction of the data with a non-negligible thickness, whereas a margin too small leads to instabilities on the final result. Figure 12 further demonstrates the SDF reconstruction ability of our method in three dimensions in the context of open surfaces or curves. Note that this setup is also suitable for closed surfaces in order to learn an unsigned distance field, as it is the case in Figure 4."}, {"title": "4. Results and Applications", "content": "We implement our method in python using the Pytorch library for neural network training. All our experiments were performed on a Ubuntu 22 workstation using a Nvidia 4070Ti GPU. For the generalized winding number, we use the original implementation of Barill et al. [BDS*18] as provided in libigl [JP*18].\nWe use a neural network architecture of 20 SLL layers of size k = 128. In total, this amounts for 330K floating point parameters for a total size of approximately 1.4MB. For reference, the hand mesh used in Figure 7 is described by 150K floating point numbers for vertices and 300K integers for faces, for a total size of 2.2MB when compressed.\nAll inputs are normalized in a bounding box $[-1; 2]^n$ before any processing. We set D = [-1,1]\" as our sampling domain. With the exception of Figure 6, all our experiments were performed with m = 10-2 and \u03bb = 100."}, {"title": "4.2. Gradient Robustness", "content": "As a first experiment, we demonstrate the robustness of our approach in comparison with neural distance field methods from the state of the art. These methods are trained on a dataset of points $(x_i, S_\u03a9(x_i))$ with $S_\u03a9$ being precomputed from the input mesh. They minimize a least-square fitting loss:\n$$L_{fit} = \u2211_i[f_\u03b8(x_i) \u2212 S_\u03a9(x_i)]^2$$\nto match the true distance function, as well as an eikonal loss:\n$$L_{eikonal} = \u222b(||\u2207f_\u03b8(x)|| \u2212 1)^2 dx$$"}, {"title": "4.3. Signed vs Unsigned Distance Field", "content": "As our method provides stable distance estimation far from the zero level set, we are also able to extract high quality isosurfaces for different values of the function. This is illustrated on Figure 1 for the signed case (using the generalized winding number) and on Figure 12 in the unsigned case, for a curve and an open surface with holes. Additional results are available in supplemental material.\nWhile fitting an unsigned distance also works perfectly fine for a closed object, partitioning the points first with generalized winding number enables our method to benefit from its robustness to faulty and noisy input. As a result, some defects of the zero level set can be repaired, as it is the case with the corrupted botijo model shown in Figure 4."}, {"title": "4.4. Underestimation of the true distance field", "content": "To further justify our claim that our learned signed distance field never overestimates the true distance, we train different neural distance fields on a handle model (Figure 8). We then extract the zero level set of each method using marching cubes [LC87] and sample 100K points uniformly in a sphere of radius 30 centered at zero. We plot the difference between the true distance S(x) to the zero level set mesh and the predicted value of the neural network, in function of S(x). In this experiment, a perfect network would output a straight line, but all methods present some deviation in practice. However, we observe that only our method provide only negative values, meaning that the network's output is always smaller than the true distance."}, {"title": "4.5. Geometrical Queries", "content": "Having a robust SDF far from the surface enables efficient and robust geometrical queries on any of its level sets. In this section, we demonstrate experimentally that it is indeed possible to do on our Lipschitz network trained with the hKR loss, without requiring neither range analysis nor an estimation of the Lipschitz constant.\nGiven an exact SDF S and a point x in space, the closest"}, {"title": "4.6. Constructive Solid Geometry", "content": "A key feature of implicit geometries is the ease with which we can apply boolean operations on their function to represent intersections, unions or differences of shapes [Ric73], thus building more complex shapes for simple ones. We illustrate this property on our neural distance field on Figure 10: given two SDFs $f_1$ and $f_2$, we can compute the surface of their union as the 0-level set of min($f_1, f_2$). Similarly, their intersection coincides with the 0-level set of max($f_1, f_2$). These operations preserve the Lipschitz constant so the guarantee of being 1-Lipschitz still holds for the composition. However these point-wise minimum and maximum are not necessarily SDFs themselves [Quib, MSLJ23]."}, {"title": "4.7. Robustness to noise and sparsity of input", "content": "Finally, approaching the neural distance field problem with the hKR loss means that we are still able to recover a good approximation of the SDF of an object in contexts where the ground truth distance cannot be computed or is not available. We illustrate this"}, {"title": "Appendix A: Proof of Theorem 1", "content": "Let f* be a 1-Lipschitz minimizer of $L_{KR}$, under the constraint that $L'_{hinge}(f^*, y) = 0$. f* is shown to exist by Serrurier et al. [SMG*21, Theorem 1]. Define:\n$$\u2202\u03a9^m = \\{x \u2208 D, |S_\u03a9(x)| <m\\}$$\nas the \"shell\" of width 2m centered around $\u2202\u03a9$. Given the as-"}]}