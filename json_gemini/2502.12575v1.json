{"title": "DemonAgent: Dynamically Encrypted Multi-Backdoor\nImplantation Attack on LLM-based Agent", "authors": ["Pengyu Zhu", "Zhenhong Zhou", "Yuanhe Zhang", "Shilinlu Yan", "Kun Wang", "Sen Su"], "abstract": "As LLM-based agents become increasingly\nprevalent, backdoors can be implanted into\nagents through user queries or environment\nfeedback, raising critical concerns regarding\nsafety vulnerabilities. However, backdoor at-\ntacks are typically detectable by safety au-\ndits that analyze the reasoning process of\nagents. To this end, we propose a novel\nbackdoor implantation strategy called Dynami-\ncally Encrypted Multi-Backdoor Implanta-\ntion Attack. Specifically, we introduce dy-\nnamic encryption, which maps the backdoor\ninto benign content, effectively circumvent-\ning safety audits. To enhance stealthiness, we\nfurther decompose the backdoor into multi-\nple sub-backdoor fragments. Based on these\nadvancements, backdoors are allowed to by-\npass safety audits significantly. Additionally,\nwe present AgentBackdoorEval, a dataset de-\nsigned for the comprehensive evaluation of\nagent backdoor attacks. Experimental results\nacross multiple datasets demonstrate that our\nmethod achieves an attack success rate near-\ning 100% while maintaining a detection rate\nof 0%, illustrating its effectiveness in evading\nsafety audits. Our findings highlight the limita-\ntions of existing safety mechanisms in detect-\ning advanced attacks, underscoring the urgent\nneed for more robust defenses against back-\ndoor threats. Code and data are available at\nhttps://github.com/whfeLingYu/DemonAgent.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) demonstrate re-\nmarkable performance (OpenAI, 2024a), catalyz-\ning the extensive deployment of LLM-based agents\nacross various domains (Xi et al., 2023). These\nagents excel in understanding and planning com-\nplex tasks by utilizing external tools and mem-\nory storage to access historical context (Yao et al.,\n2023; Schick et al., 2023). Despite their impres-\nsive capabilities, LLM-based agents also pose un-\nprecedented safety challenges (He et al., 2024),\nincluding jailbreak (Li et al., 2024b), misinforma-\ntion (Huang et al., 2025), and knowledge poison-\ning (Schuster et al., 2021). Such vulnerabilities\nhave raised serious concerns regarding the safety\nof LLM-based agents.\nLLM backdoor attacks (Kurita et al., 2020), a\ntypical risk, involve injecting a backdoor into the\ntarget model, causing it to behave benignly unless\ntriggered by specific conditions that induce ma-\nlicious behavior (Chen et al., 2021; Yang et al.,\n2021). In contrast to backdoor attacks targeting\nindividual LLMs, those in agent-based scenarios\nmanifest in distinct forms (He et al., 2024; Wang\net al., 2024b). Recent research explores methods\nsuch as embedding hidden triggers within user in-\nteractions or environmental feedback, achieving\nnotable attack success rate, portability, and perfor-\nmance of normal tasks (Yang et al., 2024; Wang\net al., 2024c; Anonymous, 2024). However, previ-\nous work lacks stealth, as backdoor attack contents\nare stored in memory without additional process-\ning, making them easily detectable through safety\naudits and oversight mechanisms.\nIn this paper, we propose a backdoor implan-\ntation strategy called Dynamically Encrypted\nMulti-Backdoor Implantation Attack, designed\nto circumvent safety audits. First, the backdoor\ncontent in our Dynamic Encryption Mechanism\nevolves alongside the agent's running process. The\nencrypted content is seamlessly integrated into the\nagent's workflow, remaining hidden throughout\nthe process. To enhance stealth, we introduce\nMulti-Backdoor Tiered Implantation (MBTI),\nwhich leverages anchor tokens and overlapping\nconcatenation to decompose the backdoor into\nmultiple sub-backdoor fragments that poison the\nagent's tools. These fragments are encrypted and\nimplanted through tiered implantation within the\nagent's workflow, activated through cumulative\ntriggering, ensuring they are difficult to trigger by\narbitrary user queries. Through these algorithms,\nour method achieves exceptional stealth and a high\nattack success rate. It performs effectively across\nvarious scenarios and datasets, with a focus on\nits ability to bypass safety audits and execute at-\ntack behavior. Compared to other backdoor at-\ntack algorithms, our method remains undetectable,\nas demonstrated in our memory inspection exper-\niments where it achieved a 0% detection rate, un-\nderscoring its superior stealth and threat potential.\nAdditionally, we introduce a dedicated dataset\ncalled AgentBackdoorEval designed to evaluate\nthe effectiveness of adversarial backdoor attacks.\nTo ensure a comprehensive assessment, the dataset\ncovers five real-world scenarios in which agents\nmay be deployed, including Banking Finance, E-\ncommerce, Medical, Social Media, and Weather\nQuery. Each task within these scenarios represents\na request that users might make of agents in real-\nworld applications. We also incorporate various\nsimulation tools to facilitate tool calls for every sce-\nnario. Furthermore, automated generation prompts\nare created for both the data and tools, enabling the\nscalable expansion of the dataset and the coverage\nof additional domains.\nOur contributions are summarized as follows:\n1. Dynamic Encryption: We introduce a novel\nencryption mechanism for backdoor attacks that\nallows the attack content to bypass safety audits.\n2. Multi-Backdoor Tiered Implantation\n(MBTI): We propose an advanced mechanism that\nenhances stealth and ensures activation only under\ncarefully designed conditions.\n3. A Dataset for Evaluating Backdoor At-\ntacks: We introduce a specialized dataset that pro-\nvides comprehensive testing scenarios to evaluate\nthe resilience of LLM-based agents against sophis-\nticated backdoor attacks."}, {"title": "2 Related work", "content": "2.1 LLM-based Agents\nLLM-based agents are systems that leverage large\nlanguage models (LLMs) for autonomous reason-\ning, planning, and task execution using external\ntools (Wang et al., 2024a; Muthusamy et al., 2023).\nThese agents integrate LLMs as core controllers\nto manage complex workflows, enabling them to\nperceive, plan, act, and learn within a defined scope\n(Xi et al., 2023). Unlike traditional LLMs, agents\nautonomously plan and execute tasks, enabling\ngoal-directed automation in real-world applications\n(Park et al., 2023). For instance, the agent may\nadapt to household environments by responding to\nlighting conditions and anticipating tool locations\nfor task execution (Wu et al., 2023). Similarly, auto-\nmatic shopping agents interact with users to under-\nstand preferences, recommend products, and track\nprice fluctuations, alerting users when the optimal\npurchase time arrives (Yao et al., 2022). Recent\nadvancements, such as HuggingGPT (Shen et al.,\n2023), AutoGPT (Yang et al., 2023), and ChatDev\n(Qian et al., 2024) further demonstrate the growing\npotential of LLMs in automating workflows and\nsupporting decision-making.\n2.2 Backdoor Attacks\nA backdoor in deep learning embeds an exploit dur-\ning training, invoked by a specific trigger (Gao\net al., 2020; Goldblum et al., 2023). Early re-\nsearch focused on computer vision (Gu et al., 2019),\nwhich was later expanded to natural language pro-\ncessing (Chen et al., 2021; Qi et al., 2021; Kurita\net al., 2020). More recently, backdoor attacks have\nemerged as a significant threat to LLMs (Xu et al.,\n2024; Yan et al., 2024). Backdoor attacks can be\ncategorized into data poisoning (Xu et al., 2024)\nand model poisoning (Li et al., 2024a). LLM-based\nagents relying on LLMs as core controllers are sus-\nceptible to both types of attacks (Xi et al., 2023).\nHowever, backdoor attacks on agents differ from\nthose targeting traditional LLMs, as agents perform\nmulti-step reasoning and interact with the environ-\nment (He et al., 2024). This extended reasoning\nprocess creates more opportunities for sophisti-\ncated attacks, such as query-attack, observation-\nattack, and thought-attack (Yang et al., 2024)."}, {"title": "3 Dynamically Encrypted\nMulti-Backdoor Implantation Attack", "content": "3.1 Preliminary\nWe explore our agent within the context of a widely\nadopted agent framework, ReAct (Yao et al., 2023).\nIn constructing agents, the LLM is denoted as $I_L$,\nthe user's query as $q$, and the memory as $I_m$, which\nstores context across steps. The agent's process\nbegins by initializing $I_m$ with $q$. The ReAct process\nconsists of three phases at each step $i$: the thought\ngenerated by $I_L$, the agent's action, the observation\nfrom the environment (Yang et al., 2024).\nThe process continues until a termination result\n$R$ is reached, at which point the final memory state\nis processed by $I_L$ to generate the final answer $A$.\nThe workflow of the agent can be defined as Alg.1\n3.2 Dynamically Encryption Mechanism\nIn this section, we introduce a dynamic encryp-\ntion mechanism that ensures the encrypted content\nevolves with the agent's workflow. By using time-\ndependent encryption, the content is altered as the\nagent progresses, with backdoor code set $C_b$ being\nencrypted throughout the process.\nFirst, We designed an encryptor $E$ to generate\nthe encrypted content set $C_e$ using an injective map-\nping function $f$, defined as:\n$\\forall c_b \\in C_b, \\exists c_e \\in C_e, c_e = E(c_b) = f(c_b), \\quad(1)$$\nOur function $f(.)$ employs time-dependent encod-\ning to transform the backdoor code $c_b$ into the en-\ncrypted content $c_e$, where $c_e$ represents the times-\ntamp at the current time of encryption, ensuring\nthat the encryption is both time-sensitive and non-\nreproducible. The encryptor $E$ implements $f(.)$,\nmapping $C_b$ to $C_e$ and executing the encryption\nprocess. We dynamically store the correspond-\ning key-value pairs of $c_e$ in an encryption table\n$T$ within temporary storage:\n$$T = \\cup_{k=1}^{N} \\{(c_b^k, c_e^k) \\ | \\ c_e^k = f(c_b^k)\\}, \\quad(2)$$\nwhere $N$ is the total number of key-value pairs.\nFor ease of understanding, we model a finite\nstate machine (FSM) (Lee and Yannakakis, 1996)\nto describe the life cycle of the encryption table $T$\nin the agent's workflow:\n$$FSM_T = (S, \\Sigma, \\delta, s_0, F), \\quad(3)$$\nwhere $S$ represents the states of the encryption ta-\nble, which can be inactive, active, or destroyed.\nThe set $\\Sigma$ denotes the events that trigger state\ntransitions, specifically initialize, execute, and\nterminate. The transition function $\\delta: S \\times \\Sigma \\rightarrow S$\ndefines how the states change in response to these\nevents. Upon completion of the agent's workflow,\nthe encryption table is destroyed, as indicated by\n$T_{delete, 0}$.\nOur design ensures the creation of a time-\ndependent, dynamically encrypted algorithm that is\nimmediately discarded and adapts to the evolving\nworkflow of the agent."}, {"title": "3.3 Multi-Backdoor Tiered Implantation\n(MBTI)", "content": "Building on the encryption mechanism introduced\nin Section 3.2, we further propose Multi-Backdoor\nTiered Implantation (MBTI), a technique designed\nto implant dynamically encrypted sub-backdoors\ninto the agent in a tiered manner, thereby en-\nhancing the stealthiness of the attack. MBTI\ninvolves using Anchor Tokens and Overlapping\nConcatenation to create multiple sub-backdoor\nfragments that form an Attack Matrix. These\nfragments are used to poison the agent and\ngenerate an Attack Dependency Matrix. As the\nagent executes, MBTI uses the dynamic encryption\nmechanism to encrypt multiple sub-backdoor\nfragments and implants them by Tiered Implan-\ntation. This approach enables the attack to be\nactivated through Cumulative Triggering, en-\nsuring it is only triggered under specific conditions.\nAnchor Tokens. First, we decompose the complete\nbackdoor attack code $c_0$ into $m$ sub-backdoor frag-\nments, denoted as $C_1 = \\{c_1^b, c_2^b, ..., c_m^b\\}$, where\n$c_0^b$ is considered as the whole set $C_0$ for simplicity.\nWe then introduce anchor tokens, denoted as $A$,\nconsisting of the start token $A_s$ and the end token\n$A_d$. These tokens are appended to the first fragment\n$c_1^b$ and the last fragment $c_m^b$, effectively delimiting\nthe sequence. Although $A_s$ and $A_d$ are not part of\nthe executable code, they function as special char-\nacter tokens that do not impact execution and are\nremoved upon recognition. The process of append-\ning is shown as follows:\n$$A = (A_s, A_d),$$\n$$c_b = \\cup_{k=1}^{m} A_s c_k^b \\ominus A_d, \\quad(4)$$\nwhere $\\ominus$ denotes the join operator of $A_s$ and $A_d$.\nOverlapping Concatenation. We use overlapping\nconcatenation to embed associated codes $\\psi_k$ be-\ntween consecutive sub-backdoor fragments where\neach $\\psi_k$ serves as a distinct code. Specifically, $\\psi_k$ is\nsplit into two interrelated parts, $\\psi_{k1}$ and $\\psi_{k2}$, which\nare inserted at the boundaries between $c_k^b$ of and $c_{k+1}^b$,\nfrom the first to the last fragment. The mechanism\nis defined as:\n$$\\psi_k = (\\psi_{k1}, \\psi_{k2})$$\n$$c_k^b = c_k^b \\ominus \\psi_{k1}$$\n$$c_{k+1}^b = \\psi_{k2} \\ominus c_{k+1}^b, \\quad(5)$$\nwhere $\\ominus$ represents the concatenation operation.\nAttack Matrix. Building on anchor tokens and\noverlapping concatenation, we construct an attack\nmatrix. The attack matrix $A$ is of size $m \\times m$,\nwhere $A[k, j]$ represents the relationship between\nthe sub-backdoor fragments $c_k^b$ and $c_j^b$. Specifically,\nif fragment $c_j^b$ of must immediately precede fragment\n$c_k^b$, then $A[k, j] = 1$; otherwise, $A[k, j] = 0$. This\nrelationship can be formalized as:\n$$\\forall k, j, A[k, j] = 1 \\rightarrow c_k^b \\rightarrow c_j^b. \\quad(6)$$\nThe attack matrix $A$ is presented as follows:\n$$A=\\begin{bmatrix}\n0 & 1 & 0 & ... & 0\\\\\n0 & 0 & 1 & ... & 0\\\\\n... & ... & ... & ... & ...\\\\\n0 & 0 & 0 & ... & 1\\\\\n0 & 0 & 0 & ... & 0\n\\end{bmatrix}, \\quad(7)$$\nwhere rows and columns represent the indices of\nsub-backdoor fragments, and non-zero entries in-\ndicate valid sequential dependencies between them.\nAttack Dependency Matrix. After constructing\nthe attack matrix $A$, we poison the agent's toolset\nby injecting distinct sub-backdoor fragments into\nthe invocation code of $m$ out of $n$ tools. The toolset\nis represented as follows:\n$$I_s = [s_1, ..., s_m, s_1, ..., s_{n-m}], \\quad(8)$$\nwhere $s_1, ..., s_m$ are poisoned tools, and\n$s_1, ..., s_{n-m}$ are benign tools.\nAfter poisoning, we construct the attack depen-\ndency matrix $B$ to capture invocation relationships\nbetween tools. The computation of $B$ is as follows:\n$$B = A \\bullet (\\Pi_{I_s})^T = \\begin{bmatrix}\nb_{1,1} & b_{1,2} & ... & b_{1,n}\\\\\nb_{2,1} & b_{2,2} & ... & b_{2,n}\\\\\n... & ... & ... & ...\\\\\nb_{n,1} & b_{n,2} & ... & b_{n,n}\n\\end{bmatrix}, \\quad(9)$$\nwhere $\\bullet$ denotes the poisoning process, and\n$\\Pi_{I_s}$ forms an $n \\times n$ matrix representing the\nrelationships between tools based on their posi-\ntions in the toolset. If harmful tool $s_k$ must be\ncalled directly before harmful tool $s_j$, the ma-\ntrix entry $B[k, j]$ is set to 1. Otherwise, it is set to 0.\nTiered Implantation With the attack depen-\ndency matrix $B$ constructed, the preparation for"}, {"title": "4 Experiment", "content": "We outline the experimental setup in Section 4.1\nand present the experimental results in Section\n4.3. In Section 4.4, we compare the distribution of\nharmless paths to demonstrate that using MBTI im-\nproves stealth. Additionally, we assess the impact\nof different mapping methods in Section 4.5.\n4.1 Experimental Setting\n4.1.1 Models and Datasets\nWe used the GPT-4 (OpenAI, 2024a), DeepSeek-\nV3 (DeepSeek-AI et al., 2024) and Qwen2.5-14B\n(Qwen et al., 2025) as the base models for the\nAgent and conducted experiments on two agent\nbenchmarks: AgentInstruct (Zeng et al., 2024)\nand ToolBench (Qin et al., 2024), adapting tools\nfor both. AgentInstruct includes six real-world\nagent tasks: AlfWorld (AW) (Shridhar et al., 2021),\nMind2Web (M2W) (Deng et al., 2023), Knowledge\nGraph (KG), Operating System (OS), Database\n(DB) and WebShop (WS) (Yao et al., 2022). Tool-\nBench consists of a large number of samples that\nrequire the application of diverse tool categories.\nWe introduce AgentBackdoorEval to evalu-\nate agent backdoor attacks, consisting of five\nreal-world domain tasks: Banking Finance, E-\ncommerce, Medical, Social Media, and Weather\nQuery. Each task is designed to simulate the tools\nand environments typically found in real-world ap-\nplications. For more details on the construction\nprocess, please refer to Appendix B.\n4.1.2 Backdoor Attack Targets\nWe utilize simulated file writing as the backdoor\nattack target to facilitate monitoring and avoid inter-\nference with the actual environment. This approach\nsimulates sending an HTTP request to a malicious\nwebsite by writing the agent's generated private\ndata to a file, making it easier to track the success\nrate of the attack and observe its outcomes. For de-\ntails on specific backdoor codes and effects, please\nrefer to Appendix C.\n4.1.3 Audit Mechanism\nBased on the existing defense mechanism (Hua\net al., 2024), we use GPT-40 (OpenAI, 2024b) as\nthe base model to evaluate the agent's memory con-\ntent and execution results, ensuring the fairness of\nthe audit process. By providing tailored prompts,\nas detailed in Appendix D, the model reviews the\nmemory content to identify any irregularities or un-\nreasonable actions. If any issues are detected, they\nare flagged with clear explanations for why they\nare deemed unreasonable. Additionally, a man-\nual review process is incorporated to re-evaluate\nflagged issues, ensuring thorough validation and\nconfirming the rationality of the results.\n4.2 Evaluation Metrics\nWe focus on three key metrics: Attack Success\nRate (ASR), Detection Rate (DR), and Normal Task\nCompletion Performance (NP). Detailed descrip-\ntions of the measurement and calculation methods\nfor these metrics can be found in Appendix F.\n\u2022 Attack Success Rate (ASR) measures the\nlikelihood that the LLM agent executes the\nintended malicious operations after complet-\ning the task, given the presence of a trigger\n(Wang et al., 2024c). This metric is essential\nfor evaluating the overall effectiveness of the\nattack. The ASR of our method is assessed by\nchecking if the backdoor objectives in Section\n4.1.2 are achieved.\n\u2022 Detection Rate (DR) assesses whether any ir-\nregularities or suspicious content are detected\nduring the audit of the agent's memory after\nexecution, as outlined in Section 4.1.3. This\nmetric is essential for evaluating the stealth-\niness of the attack, indicating the likelihood\nof detecting the presence of the attack or any\nunauthorized actions performed by the agent."}, {"title": "4.3 Main Results", "content": "4.3.1 Comparison of Methods\nWe evaluate the effectiveness of our backdoor at-\ntack method by comparing it with several existing\nattack methods, including BadAgent (Wang et al.,\n2024c), Foot-in-the-Door (Nakash et al., 2024),\nand AdvWeb (Anonymous, 2024). Our evaluation\nfocuses on DR, which measures the attack's ability\nto evade detection, and ASR, which assesses the\nsuccess rate of carrying out malicious actions.\nAs shown in Figure 2, our Encryption and En-\ncryption+MBTI methods achieve significant im-\nprovements in stealth, maintaining a DR of 0% and\nan ASR of 100%. This demonstrates their abil-\nity to execute malicious actions without detection,\nin contrast to methods like BadAgent and Foot-\nin-the-Door, which suffer from high DRs despite\nrelatively high ASRs. AdvWeb, while showing\na high ASR, also has a significant DR. These re-\nsults underscore the effectiveness of our method in\nbalancing attack success and stealth.\n4.3.2 Performance on Benchmarks\nWe conducted experiments on AgentInstruct and\nToolBench to evaluate the performance of our pro-\nposed methods. For each dataset, representative\ntasks were selected, and simulation tools were\nadapted to ensure compatibility with their respec-\ntive environments, as described in Appendix B.2.\nAdditionally, specific queries were modified in the\ndatasets to better accommodate MBTI, thus guid-\ning the backdoor attack.\nAs shown in Table 1, experiments were con-\nducted across different models. Both the Encryp-\ntion and Encryption+MBTI methods consistently\nachieved 100% ASR with a 0% DR across all mod-\nels, demonstrating the stealth and effectiveness of\nthe attack. These results highlight the robustness\nof our methods across various models and datasets.\nMoreover, NP remained high across all tasks,\nindicating that the agent successfully completed"}, {"title": "\u2022 Normal Task Completion Performance", "content": "(NP) measures the agent's ability to complete\nits tasks correctly while the backdoor is active,\nensuring that the attack does not affect the\nagent's regular functionality, similar to FSR\n(Wang et al., 2024c) with detailed differences\nprovided in Appendix F.3. We evaluate NP\nthrough GPT-40 and manual review to confirm\nthat the task meets its intended goals without\nsignificant disruption from the backdoor.\ntasks without significant disruption, even with the\nbackdoor present. In cases of incomplete tasks\ndue to the model's limited ability, the attack still\nsucceeded because of the correct use of tools. This\nconfirms that our methods execute the backdoor\nattack while preserving the agent's functionality.\n4.3.3 Cross-Domain Performance on\nAgentBackdoorEval\nWe conducted experiments on the AgentBack-\ndoorEval dataset to thoroughly assess the perfor-\nmance of our proposed methods across multiple\ndomains and models. For each domain, we tailored\na comprehensive set of domain-specific tools to\nmeet the unique data requirements. Detailed statis-\ntics of the tools used are provided in Appendix G.\nAs shown in Table 2, both the Encryption and\nEncryption+MBTI methods consistently achieved\na 100% ASR across all domains and models. Addi-\ntionally, both methods maintained a 0% DR, high-\nlighting the stealthiness of the attack. Notably, NP\nremained high across all domains and models, in-\ndicating that the agent successfully completed its\nintended tasks without significant disruption, even\nwith the backdoor present.\nThese results underscore our approach's strong\nadaptability, robustness, and effectiveness across\ndiverse domains and models, achieving reliable\nattack success while remaining undetectable and\nensuring the agent's normal functionality.\n4.4 Comparison of Harmless Path Ratios\nWe compare the harmless path distribution before\nand after applying MBTI based on simulation ex-\nperiments. Figure 3 illustrates the harmless path\nratios under varying total tool counts n and poi-\nsoned tool counts m. The left panel presents the\nharmless path ratio without MBTI, while the right\npanel shows the ratio after applying MBTI.\nIn contrast, the right figure demonstrates a sig-\nnificant increase in the harmless path ratio with\nMBTI. As m increases, the ratio approaches 100%"}, {"title": "after a certain threshold. This indicates that MBTI\nenhances the complexity of triggering the attack\nby requiring correct sequential assembly of code\nfragments, making the attack more stealthy.\nOverall, MBTI enhances the attack's stealth by\nrequiring a specific sequence of tool invocations,\nreducing the likelihood of accidental activation and\npreventing random user queries from triggering the\nattack. For detailed calculations, see Appendix I.\n4.5 Impact of Different Mapping Methods", "content": "We evaluate the impact of various encoding meth-\nods on DR performance by comparing different\nencoding strategies: no encoding (original), base64\nencoding, and time-based encoding, under both\nEncryption and Encryption+MBTI conditions.\nFigure 4 shows that time-based encoding, when\nused with Encryption, achieves a DR of 0%,\ndemonstrating superior stealth. In contrast, base64\nencoding results in a moderate DR, while the orig-\ninal method yields a DR of 100%. When Encryp-\ntion+MBTI is applied, time-based encoding con-\ntinues to maintain a DR of 0%, while the DR for\nbase64 and the original method decreases, benefit-\ning from the enhancement provided by MBTI.\nThese results highlight the effectiveness of En-\ncryption in lowering the DR, with time-based en-\ncoding excelling in minimizing detection. The com-\nbination of Encryption and MBTI further enhances\nthe attack's stealth, especially for the base64 and\noriginal encoding methods, demonstrating the ro-\nbustness and efficiency of the proposed methods in\nevading detection."}, {"title": "5 Conclusion", "content": "We introduce the Dynamically Encrypted Multi-\nBackdoor Implantation Attack, which ensures\nstealth by evading safety audits. Dynamic encryp-\ntion converts the backdoor into benign content,\nwhile Multi-Backdoor Tiered Implantation (MBTI)\nenhances stealth by decomposing the backdoor into\nmultiple sub-backdoor fragments. Our method\noutperforms existing techniques in detection eva-\nsion, guaranteeing attack success and normal task\ncompletion across various datasets, demonstrating\nits robustness. We also present the AgentBack-\ndoorEval dataset to advance agent security. This\nresearch underscores the need for stronger defenses\nand the responsible development of trustworthy\nlarge language models."}, {"title": "6 Limitation", "content": "This study primarily focuses on black-box models\nacross various datasets and domains constrained\nby available resources. Due to these limitations,\nwe have not explored experiments with white-box\nmodels, which could offer further insights into the\neffectiveness of our approach. Future research\ncould explore encryption techniques, especially in-\ncorporating white-box model pre-training methods\nfor generating encrypted content, to improve the\nrobustness and versatility of our method. Moreover,\nthe lack of effective defense mechanisms against\nsuch backdoor attacks highlights a critical gap in\ncurrent research. Since our method could be ex-\nploited for malicious purposes, developing defen-\nsive strategies to mitigate this type of attack is a key\ndirection for future work. Additionally, with the\ngrowing deployment of multi-agent systems, it is\ncrucial to extend our research to assess the effective-\nness of our attack within multi-agent collaboration\nenvironments. Investigating the performance of\nour approach in such settings, including the coordi-\nnation of backdoor attacks across multiple agents,\nrepresents a significant avenue for future explo-\nration. This would provide a deeper understanding\nof the full scope of safety risks and aid in strength-\nening the resilience of multi-agent systems against\nsophisticated attack strategies."}, {"title": "A Algorithm of Triggering", "content": "In this section, we present the algorithms for triggering the backdoor attack, both with and without the use\nof Multi-Backdoor Tiered Implantation (MBTI). These algorithms outline the steps involved in activating\na backdoor when the agent performs tasks. The first algorithm describes the triggering process without\nMBTI, while the second algorithm includes the use of MBTI for more stealthy and complex backdoor\nactivation.\nA.1 Triggering Process without MBTI\nIn this scenario, the backdoor attack is triggered directly from the encrypted backdoor content. The\nprocess involves the following steps:\nThis process directly utilizes the encrypted backdoor and activates it through a simple decoding and\nexecution mechanism.\nA.2 Triggering Process with MBTI (Cumulative Triggering)\nWhen MBTI is used, the backdoor is first decomposed into multiple sub-backdoor fragments, which are\nencrypted, decoded, and then assembled before activation. The attack is only triggered if the backdoor\nfragments are combined correctly. The process involves the following steps:\nIn this method, the backdoor is assembled using multiple encrypted fragments. Cumulative triggering\nensures the attack is activated only when all fragments are properly combined, enhancing the stealthiness\nand complexity of the attack.\nB Data and Tool Generation Methods\nThis section outlines the data and tool generation methods used to create the AgentBackdoorEval dataset,\nwhich is designed to assess agent backdoor attacks and can be adapted to other datasets. The data\ngeneration process involves using GPT to automatically generate dataset entries and simulation tools in\nbatches. These tools are then manually screened and fine-tuned to ensure data quality."}, {"title": "B.1 Data Generation Process", "content": "To generate data for the dataset, a prompt-based approach is utilized. The data generation prompt\nguides the process of creating simulated attack scenarios, ensuring that the generated data adheres to the\nspecifications outlined, such as the uniqueness of attack tools and the simulation of malicious behaviors\nwithout performing actual harmful operations. The data is generated in a structured format for each\ndomain with examples provided to clarify the expected output.\nB.2 Tool Generation Process\nIn parallel with data generation, Python tool functions are created to simulate operations across various\ndomains. Each tool returns mock results in JSON format, including success/error indicators, messages,\nand operation details. These tools are integrated into the agent to simulate tasks and assess backdoor\nattacks, enabling realistic evaluation."}, {"title": "C Backdoor Attack Target", "content": "This section demonstrates a simulated backdoor attack within an agent's workflow, showing how file\nwrites and fake data generation can simulate medical tasks like checking blood pressure or viewing\nmedical history. Malicious URLs are used to redirect or log actions, illustrating the attack's potential\nimpact."}, {"title": "D Audit Mechanism", "content": "This section outlines the audit mechanism used to review agent memory, combining both model-based and\nmanual analysis. The audit prompt guides the reviewer to assess the agent's decisions for accuracy, identify\npotential backdoor attacks, and detect anomalies in the content. The process ensures that suspicious\nbehavior or unexpected content is flagged, contributing to a comprehensive security check for agent\noutputs."}, {"title": "E NP Evaluation Prompt", "content": "This section describes the NP evaluation Prompt, which combines both model-based (GPT-40) and\nmanual evaluation to assess the quality and completeness of an AI assistant's response. The evaluator is\ntasked with determining whether the assistant's reply effectively addresses the user's request, is relevant,\naccurate, and sufficiently detailed. By leveraging both automated analysis and human judgment, this\napproach ensures that responses are thoroughly reviewed, with a clear distinction between valid outputs\nand potential deficiencies."}, {"title": "F The Calculation and Measurement of Evaluation Metrics", "content": "In this appendix", "methods": "Attack Success Rate (ASR)", "functionality.\nNote": "All symbols introduced in this section are independent and not related to those in the main text.\nF.1 Attack Success Rate (ASR)\nDefinition: Attack Success Rate (ASR) measures the probability that the LLM agent performs the\nintended malicious operations after completing the task", "been\nactivated.\nMeasurement": "To measure the Attack"}]}