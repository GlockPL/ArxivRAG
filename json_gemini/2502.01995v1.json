{"title": "THEORETICAL AND PRACTICAL ANALYSIS OF FR\u00c9CHET REGRESSION VIA COMPARISON GEOMETRY", "authors": ["Masanari Kimura", "Howard Bondell"], "abstract": "Fr\u00e9chet regression extends classical regression methods to non-Euclidean metric spaces, enabling the analysis of data relationships on complex structures such as manifolds and graphs. This work establishes a rigorous theoretical analysis for Fr\u00e9chet regression through the lens of comparison geometry which leads to important considerations for its use in practice. The analysis provides key results on the existence, uniqueness, and stability of the Fr\u00e9chet mean, along with statistical guarantees for nonparametric regression, including exponential concentration bounds and convergence rates. Additionally, insights into angle stability reveal the interplay between curvature of the manifold and the behavior of the regression estimator in these non-Euclidean contexts. Empirical experiments validate the theoretical findings, demonstrating the effectiveness of proposed hyperbolic mappings, particularly for data with heteroscedasticity, and highlighting the practical usefulness of these results.", "sections": [{"title": "Introduction", "content": "Fr\u00e9chet regression (Petersen & M\u00fcller, 2019) is a powerful statistical tool for analyzing relationships between variables when the response or predictor lies in a non-Euclidean space. It generalizes classical regression to settings where the response variable Y resides in a metric space M. Given predictors X, Fr\u00e9chet regression seeks to estimate the conditional Fr\u00e9chet mean.\n$\\\\mu(x) = \\\\arg\\\\min_{m \\\\in M} E [d\u00b2 (Y, m) | X = x],$\nwhere d is the metric on M. This approach accommodates data in various non-Euclidean spaces, such as manifolds, trees, and graphs (Lin & M\u00fcller, 2021; Ferguson & Meyer, 2022; Ghosal, 2023; Qiu et al., 2024; Chen & M\u00fcller, 2022). In recent years, several variants of Fr\u00e9chet regression have been proposed (Tucker et al., 2023; Bhattacharjee & M\u00fcller, 2023; Song & Han, 2023; Ghosal et al., 2023; Zhang et al., 2024; Yan et al., 2024), each addressing different aspects such as variable selection, error modeling, and high-dimensional data handling. However, most existing studies primarily focus on specific geometric settings or lack a comprehensive theoretical framework that accounts for varying curvature bounds. This study fills this gap by leveraging comparison geometry to provide a unified theoretical analysis of Fr\u00e9chet regression across CAT(K) spaces with diverse curvature properties.\nFr\u00e9chet regression allows the assumption of a non-Euclidean space in the space of the data, so one can expect that its behavior can be described depending on the geometrical properties of the space. To investigate this, this study utilizes comparison geometry, which is a fundamental branch of differential geometry that investigates the geometric properties of a given space by comparing it to model spaces of constant curvature (Cheeger et al., 1975; Grove & Petersen, 1997; Cheeger & Grove, 2007; Wei & Wylie, 2009). Unlike information geometry (Amari, 2016; Ay et al., 2017; Nielsen, 2020; Amari & Nagaoka, 2000; Kimura & Hino, 2021, 2022), which focuses on general statistical manifolds, this framework leverages classical comparison theorems to derive insights about the structure and behavior of more complex or less regular spaces. By establishing inequalities and structural similarities between a target space and well-understood model spaces (e.g., Euclidean, spherical, or hyperbolic geometries), comparison geometry enables the extension of geometric and topological results to broader contexts, including spaces that may lack smoothness"}, {"title": "Notation", "content": "In this section, the notations and definitions required for the following analysis are organized. Let M be a metric space and d be the metric on M. Here, the metric space (M, d) is geodesic space if every pair of points in M can be connected by a geodesic, a curve whose length equals the distance between the points.\nDefinition 2.1 (CAT(K) space). Let (M, d) be a geodesic metric space and let K\u2208 R. The space M is said to be a CAT(K) space if it satisfies the following curvature condition: for any geodesic triangle \u25b3pqr in M with perimeter less than 2DK (where DK = \u03c0/\u221aK if K > 0, and DK = \u221e otherwise), and for any points x, y on the edges [pq] and [qr] respectively, the distance between x and y in M does not exceed the distance between the corresponding points  and y on the comparison triangle Ap\u0101r in the model space of constant curvature K:\n$d(x, y) \\\\leq d_{M_K^2}(x, y),$\nwhere the comparison triangle Ap\u0101r is a triangle in the simply connected, complete 2-dimensional Riemannian manifold M\u00b2 of constant curvature K that preserves the side lengths as dm\u00b2 (p, q) = d(p, q), dm\u00b2 (\u0101, r) = d(q, r), and dm\u00b2 (r,p) = d(r, p).\nDefinition 2.2 (Geodesic convexity). A function f : M \u2192 R is geodesically convex if for every geodesic \u03b3: [0, 1] \u2192 M, f(x(t)) \u2264 (1 \u2212 t)f(y(0)) + tf(y(1)), for all t \u2208 [0, 1].\nDefinition 2.3 (A-strong geodesic convexity). A function f: M \u2192 R is A-strongly geodesically convex around p\u2208 M if there exists a constant > > 0 depending only on K and diam(M) such that\n$f(x) \u2212 f(p) > \\\\lambda d\u00b2(x,p),$\nfor every x \u2208 \u041c.\nDefinition 2.4 (Lower semicontinuity). A functional F : M \u2192 RU {+\u221e} is lower semicontinuous at a point x \u2208 \u039c if for every sequence {Xn} converging to x, it satisfies\n$F(x) < \\\\liminf_{n\\\\to+\\\\infty} F(x_n).$\nDefinition 2.5 (Weak convergence in metric space). A sequence of probability measures {vn} on M is said to converge weakly to a probability measure v (denoted by vn \u21d2 v) if for every bounded continuous function f : M \u2192 R,\n$\\lim_{n\\\\to+\\\\infty} \\\\int_M f(y)dv_n(y) = \\\\int_M f(y)dv(y).$\nDefinition 2.6 (Alexandrov angle). The Alexandrov angle \u2220x(y, z) is defined as the limit of secular angles between short sub-segments. Concretely, if y' is a point on [xy] with d(x, y') \u2192 0 and z' is a point on [xz] with d(x, z') \u2192 0. Then,\n$\\\\angle_x(y, z) := \\\\lim_{y'\\\\to x,z'\\\\to x} \\\\angle^{(sec)}_{y'x,z'x},$\nwhere \u2220(sec) (y'z') is the ordinary angle in the comparison triangle for \u2206xy'z' in the model space.\nDefinition 2.7 (Riemannian exponential map). Let TM be the tangent space of M at a point z \u2208 M. For a fixed point z, the Riemannian exponential map at z, denoted by exp\u2082 is a map from the tangent space at z to the manifold M: expz: TzM \u2192 M. Here, the Riemannian exponential map is constructed as\ni) Choose a tangent vector v \u2208 TzM.\nii) Consider the unique geodesic (t) emanating from z with initial velocity v. Formally, y\u028a(t) satisfies Y\u028a (0) = z and y'(0) = v.\niii) The exponential map sends the tangent vector v to the point on the manifold reached by traveling along the geodesic Yu for unit time, expz(v) = Yu(1)."}, {"title": "Theory", "content": "See Appendix B for complete proofs of all statements.\n3.1 Existence and Uniqueness of the Fr\u00e9chet Mean\nFirst, it can be shown that in CAT(K) spaces with K < 0, the convexity properties ensure the existence and uniqueness of the Fr\u00e9chet mean under mild conditions. For CAT(K) spaces with K > 0, additional constraints on the diameter of the space may be necessary to ensure uniqueness due to potential multiple minima arising from positive curvature.\nLemma 3.1. Let (M, d) be a CAT(K) space for K < 0. For any fixed point p \u2208 M, the function f : M \u2192 R defined by f(x) = d\u00b2(p, x) is geodesically convex.\nLemma 3.1 establishes that the squared distance function retains geodesic convexity in CAT(K) spaces with non- positive curvature. This property is fundamental because it ensures that the Fr\u00e9chet functional, which aggregates squared distances, inherits convexity. Consequently, optimization procedures to find the Fr\u00e9chet mean are well-behaved, avoiding local minima and guaranteeing global optimality under the given conditions.\nLemma 3.2. Let (M, d) be a complete CAT(K) space. For any probability measure v on M with compact support, there exists at least one minimizer m \u2208 M of the Fr\u00e9chet functional:\n$m = \\\\arg \\\\min_{x\\\\in M} \\\\int_M d\u00b2 (y,x)dv(y).$\nLemma 3.3. Let (M, d) be a CAT(K) space with K < 0 that is strictly geodesically convex, meaning that the squared distance function f(x) = d\u00b2(p, x) is strictly geodesically convex for any fixed point p \u2208 M. Then, for any probability measure v on M with compact support, the Fr\u00e9chet mean m is unique.\nBased on Lemma 3.1, which ensures geodesic convexity of the squared distance function in non-positively curved CAT(K) spaces, and Lemma 3.2, which guarantees the existence of a Fr\u00e9chet mean under compact support, one can establish the stability of the Fr\u00e9chet mean under measure perturbations. Furthermore, Lemma 3.3 ensures uniqueness under strict geodesic convexity, thereby enabling Proposition 3.4 to assert the convergence of Fr\u00e9chet means in non-positively curved spaces.\nProposition 3.4. Let (M, d) be a CAT(K) space with K < 0. Suppose {vn} is a sequence of probability measures on M that converges weakly to a probability measure v. Assume that for each n, the measure vn has a unique Fr\u00e9chet mean mn, and v also has a unique Fr\u00e9chet mean m. Then, the sequence of Fr\u00e9chet means {mn} converges to m \u2208 M.\nProposition 3.4 claims that the CAT(K) condition with K < 0 ensures that the space is non-positively curved, which imbues the space with strict convexity properties crucial for the uniqueness and stability of minimizers. This geometric structure prevents the existence of multiple local minima, thereby facilitating the continuity of minimizers under perturbations of the measure. Here, the stability of the Fr\u00e9chet mean under measure perturbations is foundational for Fr\u00e9chet regression. It ensures that as predictors vary and induce changes in the conditional distributions of responses, the conditional Fr\u00e9chet means (regression estimates) behave predictably and converge appropriately as sample size increases.\nProposition 3.5. Let (M, d) be a CAT(K) space with positive curvature bound K > 0. If the diameter of the support of the probability measure v, denoted by diam(supp(v)), satisfies $diam(supp(v)) < \\\\frac{\\\\pi}{2\\\\sqrt{K}}$, then the Fr\u00e9chet mean m of v is unique.\nIn Proposition 3.5, the diameter constraint ensures that all points in the support of v lie within a geodesic ball of radius R = \u03c0/2\u221aK. In CAT(K) spaces with K > 0, such balls are geodesically convex, meaning any geodesic between two points within the ball lies entirely inside the ball. This local convexity is crucial for preserving strict convexity properties of the Fr\u00e9chet functional. Here, the strict convexity implies that the Fr\u00e9chet functional cannot have multiple minimizers within the convex neighborhood defined by the diameter constraint. If two distinct minimizers existed, the functional would attain a strictly lower value at intermediate points along the geodesic connecting them, violating their minimality. One can see that exceeding this bound could allow the support to span regions where the curvature induces multiple local minima of the Fr\u00e9chet functional.\nIn addition, applying Lemmas 3.2 and 3.3, the following theorem can be obtained.\nTheorem 3.6. Let (M, d) be a complete CAT(K) space and consider a conditional distribution v\u2081 of Y given X = x. If for each x, the support of ve satisfies\n$diam(supp(\\nu_x)) < D_K = \\\\begin{cases}\\\\+ \\\\infty & \\\\text{if } K \\\\leq 0, \\\\\\frac{\\\\pi}{\\\\sqrt{K}} & \\\\text{if } K > 0, \\\\end{cases}$"}, {"title": "Convergence Rates and Concentration", "content": "Let \u00fb denote a nonparametric Fr\u00e9chet regression estimator (e.g., Nadaraya\u2013Watson-type kernel smoothing (Nadaraya, 1964; Watson, 1964; Bierens, 1988) on the predictor space). Then, the following statements for the concentration results, the pointwise consistency, and rates of convergence can be obtained. The important point is that one has to rely on exponential concentration inequalities valid in CAT(K) spaces (e.g., specific versions of concentration of measure or deviation bounds for Fr\u00e9chet means).\nTheorem 3.7 (Concentration for the sample Fr\u00e9chet mean). Let (M, d) be a complete CAT (K) space of diameter at most D. Suppose that Y1, Y2, ..., Yn are independent and identically distributed random points in M, and let \u03bc and un be the population and sample Fr\u00e9chet mean.\n$\\mu := \\\\arg \\\\min_{z\\\\in M} E[d\u00b2 (Y, z)],$\n$\\hat{\\\\mu} := \\\\arg \\\\min_{z\\\\in M} \\\\frac{1}{n} \\\\sum_{i=1}^n d\u00b2 (Y_i, z).$\nAssume further that each d\u00b2(Yi, z) is essentially bounded by D\u00b2, or more generally that d\u00b2(Yi, z) has sub-Gaussian tails uniformly in z. Then there exists \u03b4 > 0 such that for every \u0454 > 0,\n$P [d(\\\\mu, \\\\hat{\\\\mu}) > \\\\epsilon] <2 \\\\left( \\\\frac{\\\\alpha(K, D)D}{\\\\delta} \\\\right)^m e^{-\\\\frac{n (\\\\alpha(K, D) \\\\epsilon^2)^2}{8 D^2}},$\nwhere m is the dimension of the manifold, and a(K, D) is the strong convexity constant.\nSketch of Proof. i) The key is that in a CAT(K) space, with small diameter (or global non-positive curvature), the map z \u2192 d(Y, z) is geodesically convex (or strictly convex in the sense of comparison). ii) One then applies concentration-of- measure arguments akin to those used for vector-valued means, taking advantage of the fact that variance-like functionals have a unique minimizer and that small fluctuations in the empirical mean lead to exponential tail bounds.\nIn addition to the concentration for the sample Fr\u00e9chet mean in the standard sense, the following proposition gives the concentration in Lp sense.\nProposition 3.8. Under the hypotheses of Theorem 3.7, there exist explicit constants Cp(K, D) such that for any integer n \u2265 1 and p \u2265 1,\n$E[d^p (\\\\mu_n, \\\\mu)] \\\\leq C_p(K, D)(n^{-p/2}).$\nThat is, d(\u03bc\u03b7, \u03bc) converges to 0 in Lp at a rate on the order of n-p/2.\nSketch of Proof. This follows from integrating the exponential tail bound in Theorem 3.7. The boundedness of M (or sub-Gaussian tails for Y) is used to control moments of the distance.\nMoreover, the following theorem gives the pointwise consistency of nonparametric Fr\u00e9chet regression in a CAT(K) space. The main idea parallels classical kernel-based regression arguments in Rd, but replaces ordinary arithmetic means by Fr\u00e9chet means in the metric space (M, d).\nAssumption 3.9 (Kernel LLN condition). For any bounded (or square-integrable) function f : M \u2192 R, nonnegative weights {Wn,i(x)}=1 satisfies\n$\\lim_{n\\\\to\\\\infty} \\\\sum_{i=1}^n w_{n,i}(x)f(y_i) \\\\approx E[f(x) | X = x].$\nTheorem 3.10 (Pointwise consistency of nonparametric Fr\u00e9chet regression). Let {(Xi, Yi)}=1 be i.i.d. sample with Xi \u2208 Rd and Yi \u2208 M, where (M,d) is a complete CAT(K) space with diameter diam(M) < D. Define the population Fr\u00e9chet regression function:\n$\\mu^*(x) := \\\\arg \\\\min_{z\\\\in M} E[d\u00b2 (Y, z) | X = x].$"}, {"title": "Angle Stability for Conditional Fr\u00e9chet Means", "content": "Understanding not just the position but also the directional relationships around the Fr\u00e9chet mean is crucial for capturing the local geometry of the data distribution. Angle stability ensures that small perturbations in the underlying probability measures or data configurations do not lead to significant distortions in the angular relationships among points relative to the Fr\u00e9chet mean. This property is particularly valuable when analyzing directional data or when the regression function's local behavior depends on angular relationships, such as shape analysis or directional statistics.\nFirst, the following lemma for the angle comparison in CAT(K) spaces is provided.\nLemma 3.12. Let (M, d) be a CAT(K) space, and let \u2206xyz C M be a geodesic triangle of perimeter < \u03c0/\u221aK when K > 0. Let \u2206xy\u017e be its comparison triangle in the simply connected model space of constant curvature K. Then for each vertex x and the corresponding comparison vertex x, \u2220x(y, z) \u2264 \u2220\u5143(y, z), where \u2220x(y, z) is the Alexandrov angle (or geodesic angle) at x formed by the geodesic segments [xy] and [xz].\nNote the assumption that the perimeter of \u2206xyz is < \u03c0/\u221aK (when K > 0) is used to ensure\ni) The geodesics [xy], [yz], [zx] are short enough so that the entire triangle Axyz (and sub-triangles \u2206xy'z') can be compared in the standard simply connected model space (the sphere of radius 1/\u221aK if K > 0).\nii) One avoids the potential degeneracy where side lengths might exceed \u03c0/\u221aK, which could cause the model triangle in spherical geometry to become ambiguous or wrap around the sphere.\nIn the case K < 0, there is no maximum perimeter restriction because the simply connected model space (Euclidean or hyperbolic) is unbounded in diameter.\nNext, the lemma for the angle continuity under small perturbation is provided.\nLemma 3.13. Let Apqr and \u25b3p'q'r' be two geodesic triangles in a CAT(K) space (M, d). Suppose each has a perimeter < \u03c0/\u221aK when K > 0 (no restriction is needed if K < 0). Also assume d(p, p') + d(q, q') + d(r, r') is small. Then, for the angles at p in Apqr and at p' in Ap'q'r',:\n$|\\\\angle_p(q,r) - \\\\angle_{p'}(q',r')| \\\\leq C \\\\delta_{pp' qq'rr'},$\nwhere C > 0 is a constant depending only on K and the maximum side length (or perimeter) constraints, and\n$\\delta_{pp'qq'rr'} := d(p, p') + d(q, q') + d(r,r').$\nBased on the above lemmas, the following statements are obtained.\nProposition 3.14 (Angle perturbation via conditional measures). Let {v} be a family of probability measures on a CAT(K) space (M, d), each supported in a geodesic ball of diameter < D = \u03c0/2\u221aK when K > 0. Let \u03bc*(x) be the unique Fr\u00e9chet mean of vx. Suppose \u03bd\u03b1 and v\u30a3are close in the Wasserstein metric on measures: dw(vx,vx') \u2264 \u20ac. Then, for any fixed u, v \u2208 M, one has\n$|\\\\angle_{\\\\mu^*(x)} (u, v) \u2013 \\\\angle_{\\\\mu^* (x')} (u, v)| \\\\leq C\\\\epsilon,$\nwhere the constant C > 0 depends on the strong-convexity modulus a(K, D). In particular, smaller e implies the angles at \u03bc* (x) and \u00b5* (x') to points u, v differ by at most O(\u20ac)."}, {"title": "Local Jet Expansion of Fr\u00e9chet Functionals", "content": "Lemma 3.16. Let z \u2208 M and let exp\u2082 : TzM \u2192 M be the Riemannian exponential map (in a local sense if M is a manifold, or a suitable geodesic parameterization if M is just a geodesic metric space). Then for points u, v sufficiently close to z, define U := exp-1(u) and V := exp-1(v). Then,\n$\\\\angle_z(u, v) = \\\\angle_0(U, V) + O(||exp^{-1}(u)||^2 + ||exp^{-1}(v) ||\u00b2),$\nwhere \u22200(U, V) is the standard Euclidean angle in T\u2082M \u2248 Rm, and the big-Oh term depends on curvature bounds near z.\nProposition 3.17 (Local Jet expansion of Fr\u00e9chet functionals). Let v be a probability measure on a sufficiently regular CAT(K) space (M, d). Suppose that p(x) is the Fr\u00e9chet mean of v\u30a7: \u03bc(x) := arg minzem \u222b d\u00b2(y, z)dvx(y), and consider the Fr\u00e9chet functional F\u30a7(z) = \u222bd\u00b2(y,z)dvx(y). Then, in a sufficiently small neighborhood of \u00b5, the functional F can be expanded in the tangent space TuM via the exponential map. Specifically, using local coordinates exp : T\u00b5M\u2283 B\u2084(0) \u2192 M, for a vector v with ||v|| small, define z = exp\u300f(v). The expansion is given by\n$F(exp_{\\\\mu}(v)) = F_x(\\\\mu) + (\\\\nabla F_x(\\\\mu), v) + \\\\frac{1}{2} (H_x v, v) + R(v),$\nwhere \u2207Fx(\u00b5) is the gradient (which is zero if \u00b5 is the unique minimizer), H\u30a7 is the Hessian (a linear operator on T\u00b5M), and the remainder term R(v) satisfies |R(v)| = O(||v||\u00b3)."}, {"title": "Auxiliary Statements", "content": "Here, a couple of auxiliary propositions that facilitate a deeper understanding of the structural properties of the Fr\u00e9chet functional within CAT(K) spaces are introduced in this section. These propositions decompose the Fr\u00e9chet functional into radial and angular components, enabling a more nuanced analysis of variance and stability around the Fr\u00e9chet mean.\nProposition 3.18 (Angle Splitting in Distance Sums). Consider the Fr\u00e9chet functional F(z) = \u222b d\u00b2(y, z)dv(y). For z near \u00b5*, decompose:\n$d\u00b2(y, z) = d\u00b2(y, \\\\mu^*) + \\\\Pi_d(y, z, \\\\mu^*) + \\\\Pi_{\\\\angle}(y, z, \\\\mu^*),$\nwhere IIa captures radial changes in distances II\u2220 represents angular corrections around \u03bc*. If \u2220\u03bc* (y, z) remains small near \u00b5*, then II\u2220 is of order (\u2220\u00b5* (y, z))d(\u03bc*, z).\nProposition 3.19 (Angle-Distance Decomposition of Conditional Variance). Let vx be the conditional distribution of Y given X = x on a sufficiently smooth CAT(K) space (M, d). Suppose \u03bc* (x) is the unique Fr\u00e9chet mean of vx. Around \u03bc*(x), let\n$R_x(y) := d(y, \\\\mu^*(x)), \\\\Phi_x(y) := \\\\angle_{\\\\mu^*(x)}(u_0, y),$\nfor a fixed reference point uo \u2208 M. Then the conditional variance can be partially decomposed into a radial variance term, an angle-radial covariance term, and higher-order corrections:\n$\\text{Var}_{\\\\nu_x} [d\u00b2 (Y, \\\\mu^* (x))] = \\\\text{Var}[A_x(Y)] + \\\\text{Cov} (\\\\Phi_x(Y), R_x(Y)\u00b2) + \\\\beta,$\nwhere Ax is the radial part and \u1e9e is the higher-order term."}, {"title": "Experiments", "content": "From the discussion in Section 3, it can be seen that the negative curvature space has better properties in terms of estimation than the positive curvature space with broader support. To confirm these results, this section considers numerical experiments. See Appendix A for the intuitive understanding of the following hyperbolic mapping.\n4.1 Illustrative Example\nA point on the unit sphere is parameterized as\nx = sin() cos(0), y = sin() sin(0), z = cos(),\nwhere \u03c6 \u2208 [0, \u03c0] is the polar angle and \u03b8 \u2208 [0,2\u03c0] is the azimuthal angle. Let R be the radius of the sphere. Here, consider the stereographic projection: The plane is tangent to the sphere at the south pole (0, 0, -R) and is defined z = -R, and the north pole N = (0,0, R) serves as the projection point. For a point p = (x, y, z), the stereographic projection \u03c0(p) = (u, v) on the plane is given by\nu = \\\\frac{Rx}{R + z} \\\\upsilon = \\\\frac{Ry}{R + z}"}, {"title": "Conclusion", "content": "This study provides a comprehensive theoretical analysis of Fr\u00e9chet regression within the framework of comparison geometry, focusing on CAT(K) spaces. It establishes foundational results on the existence, uniqueness, and stability of the Fr\u00e9chet mean under varying curvature conditions. Notably, the analysis demonstrates how curvature properties influence statistical estimation, with non-positive curvature spaces offering advantageous stability and convergence properties. The paper also extends statistical guarantees to nonparametric Fr\u00e9chet regression, including exponential concentration bounds and convergence rates, which align with classical Euclidean results. Angle stability and local jet expansion further highlight the behavior of Fr\u00e9chet functionals, offering geometric insights of regression in non- Euclidean spaces. Experimental results support the theoretical findings, showing that hyperbolic mappings often improve performance under heteroscedasticity assumption.\nLimitations: While this study provides a robust theoretical foundation for Fr\u00e9chet regression in CAT(K) spaces, several limitations exist. Firstly, the analysis predominantly focuses on spaces with constant curvature bounds, which may not encompass all practical scenarios where data resides in more heterogeneous geometric contexts. Additionally, the reliance on strong convexity conditions and diameter constraints in positively curved spaces may restrict the applicability of the results. As has been done in the information geometry framework (Akaho, 2004; Peter & Rangarajan, 2008; Carter et al., 2011; Kimura, 2021; Kimura & Bondell, 2024; Murata et al., 2004; Amari, 1998), future work could explore relaxing assumptions, extending the framework to broader classes of metric spaces, and developing efficient algorithms."}, {"title": "Intuitive Understanding for Hyperbolic Mapping", "content": "In regression analysis, transforming the response variable can often lead to improved model performance by stabilizing variance, normalizing distributions, or linearizing relationships. A classical example is the logarithmic transformation Y log(Y) which can enhance the performance of a linear regression model under certain conditions. Similarly, mapping spherical responses into hyperbolic space can offer analogous benefits, particularly in scenarios where the data exhibits inherent geometric or hierarchical structures.\nLog Transformation in Linear Regression Consider the simple linear regression model:\nY  \u03b2\u03a7 + \u2208,\nwhere Y is the response variable, X is the predictor, \u03b2 is the regression coefficient, and e is the error term with E[e] = 0 and Var(\u20ac) = \u03c3\u00b2. Applying a logarithmic transformation to Y yields\nlog(Y) = X + \u0454,\nY = exp(\u03b2X + \u20ac) = exp(3X) exp(\u20ac).\nAssuming e is small and approximately normally distributed, exp(e) introduces multiplicative noise to Y effectively stabilizing variance across different levels of X. This transformation often reduces heteroscedasticity in the residuals, leading to improved regression performance. Here, the heteroscedasticity refers to the phenomenon where the variability of the errors (or residuals) in a regression model is not constant across the range of predictor variables.\nDefinition A.1 (Heteroscedasticity). Consider a regression model:\nYi = \u1e9eXi + ei,\nwhere \u20ac\u00bf ~ \u039d(0, \u03c3\u00b2(X\u2081)). Here, the variance of the error term \u03c3\u00b2(X) depends on X. In a heteroscedastic model, the variance of er is a function of the predictors Xi:\nVar(\u20aci | Xi) = \u03c3\u00b2(\u03a7\u2081).\nIn contrast, for homoscedasticity, the variance of e\u00bf is constant.\nHyperbolic Mapping via Stereographic Projection Analogous to the log transformation, hyperbolic mapping trans- forms the response variable into a space where the geometric structure can lead to improved regression characteristics. The procedure involves mapping points from a spherical representation to a hyperbolic plane using stereographic projection. A point on the unit sphere of radius R is parameterized using spherical coordinates:\nX Rsin() cos(0),\ny Rsin() sin(0),\n2  Rcos(),\nwhere \u03c6 \u2208 [0, \u03c0] is the polar angle and \u03b8 \u2208 [0, 2\u03c0) is the azimuthal angle. The stereographic projection maps a point p = (x, y, z) on the sphere to a point p \u2192 \u03c8(p) = (u, v) on the plane tangent to the sphere at the south pole (0, 0, -R) and defined by z = \u2212R. The north pole N = (0,0, R) serves as the projection point. The projection formulas are\nu = \\\\frac{Rx}{R+z}\n\\upsilon = \\\\frac{Ry}{R+z}\nThis plane can be interpreted as a model of hyperbolic space, specifically visualized as a pseudosphere, which inherently possesses properties conducive to handling hierarchical or tree-like data structures.\nBoth the logarithmic transformation and hyperbolic mapping aim to stabilize variance and linearize relationships, through different geometric transformations. To understand the benefits of hyperbolic mapping, consider the effect of each transformation on the variance of the response variable. Starting with Y = BX +\u20ac, applying the log transformation yields\nlog Y = X + \u20ac.\nAssuming \u2208 ~ N(0, \u03c3\u00b2), The variance of log Y remains \u03c3\u00b2 which can be advantageous if the original Y exhibits multiplicative noise:\nVar(Y) = Var(exp(\u1e9eX + \u0454)) = exp(2\u1e9eX) \u00b7 (exp(02) \u2013 1) ."}, {"title": "Theoretical and Practical Analysis of Fr\u00e9chet Regression via Comparison Geometry", "content": "The transformation effectively decouples the variance from X stabilizing it across different predictor values.\nFor hyperbolic mapping, consider a response variable represented as a point on the sphere. The stereographic projection transforms this spherical representation into the hyperbolic plane. Let Y be the original response mapped to a point p = (x, y, z) on the sphere, and \u03c8(p) = (u, v) its hyperbolic projection. Assuming small deviations around a mean direction, the hyperbolic mapping can linearize angular variations similarly to how the log transformation linearizes multiplicative variations. Specifically, fluctuations in Y around the mean direction correspond to additive noise in the hyperbolic plane, potentially reducing variance in a manner akin to the log transformation. Formally, if Y is modeled on the sphere with\nY = Rp+\u20ac,\nwhere e represents angular noise, the hyperbolic projection yields\n\\psi(Y) = \\\\begin{pmatrix} \\\\frac{Rx}{R+z} \\\\\\\\frac{Ry}{R+z} \\\\end{pmatrix} + \\\\epsilon',\nwhre e' is the transformed noise. Under specific conditions (e.g., small angular deviations), e' exhibits reduced variance compared to e, analogous to the variance stabilization achieved by the log transformation.\nExample A.2 (Stabilizing Variance in Hierarchical Data). Consider a dataset where the response variable Y represents hierarchical relationships, such as the popularity of topics in a taxonomy. The inherent tree-like structure implies that differences between nodes (topics) grow exponentially with depth. Direct regression on Y would face increasing variance as depth increases. By mapping Y into hyperbolic space via stereographic projection, the exponential growth inherent in hierarchical data is linearized. This transformation stabilizes variance across different levels of the hierarchy, enabling more effective regression modeling. Specifically, the hyperbolic mapping aligns the geometric properties of the data with the regression framework, similar to how the log transformation aligns multiplicative relationships with additive modeling.\nLet Y be mapped to hyperbolic space via stereographic projection:\nu = \\\\frac{Rx}{R+z}\n\\upsilon = \\\\frac{Ry}{R+z}\nAssuming Y lies close to the north pole N = (0, 0, R), small perturbations e around N imply\n$2 = Rcos(\\phi) \\\\approx R (1-\\frac{\\\\phi^2}{2}),$\nx Rsin() cos(0) \u2248 R cos(0),\ny Rsin() sin(0) \u2248 R\u00f8 sin(0).\nSubstituting into the projection formulas,\n$u \\\\approx \\\\frac{R \\\\cdot R\\\\phi cos(0)}{R + R(1-\\frac{\\\\phi^2}{2})} = \\\\frac{R^2\\\\phi cos(0)}{2R-\\frac{R\\\\phi^2}{2}} \\\\approx \\\\frac{R\\\\phi cos(0)}{2},$\nv \u2248  .\\nThus, small angular deviations & result in approximately linear changes in u and v, effectively reducing the variance from multiplicative to additive in the hyperbolic plane:\n$Var(u,v) \\\\approx (\\\\frac{R}{2}) Var(\\phi).$\nCompared to the original spherical variance Var(), the hyperbolic mapping scales and linearizes the variance, analogous to the stabilizing effect of the log transformation."}, {"title": "Details of Experiments", "content": "This section describes the details of experiments in Section 4.\nModel Details Throughout the experiment, we use an implementation of Fr\u00e9chet regression based on the Nadaraya- Watson estimator (Davis et al., 2010; Hein, 2009; Steinke & Hein, 2008).\n$\\mu^*(x) = \\\\arg \\\\min_{z\\\\in M} \\\\frac{1}{n} \\\\sum_{i=1}^n K_h(X_i - x)d\u00b2(Y_i, z),$\nwhere Kh is a smoothing kernel that corresponds to a probability density with Kh(\u00b7) = h\u00ae\u00b9K(./h). For the optimization, we use Limited-memory BFGS (Liu & Nocedal, 1989).\nData Generating Process To assess the performance of the Fr\u00e9chet regression estimator, consider to generate simulated data. The regression function is\n$\\mu(x)(\\\\cdot) = ((1 - x^2)^{1/2}cos(x), (1 - x^2)^{1/2} sin(x),x),$\nx\u2208 (0,1),\nwhich maps a spiral on the sphere. To generate a random sample {(X, Y)}_1, let X\u00bf ~ U(0, 1) followed by a bivariate normal random vector Ui, and\nY\u2081 = cos(||U||)\u03bc(Xi"}]}