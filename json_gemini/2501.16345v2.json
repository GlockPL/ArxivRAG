{"title": "SELF-CLUSTERING GRAPH TRANSFORMER APPROACH TO\nMODEL RESTING STATE FUNCTIONAL BRAIN ACTIVITY", "authors": ["Bishal Thapaliya", "Esra Akbas", "Ram Sapkota", "Bhaskar Ray", "Vince Calhoun", "Jingyu Liu"], "abstract": "Resting-state functional magnetic resonance imaging (rs-fMRI) offers valuable insights into the human brain's\nfunctional organization and is a powerful tool for investigating\nthe relationship between brain function and cognitive processes\nas it allows for the functional organization of the brain to be\ncaptured without relying on a specific task or stimuli. In this\nstudy, we introduce a novel attention mechanism for graphs\nwith subnetworks, named Self Clustering Graph Transformer\n(SCGT), designed to handle the issue of uniform node updates\nin graph transformers. By using static functional connectivity\n(FC) correlation features as input to the transformer model,\nSCGT effectively captures the sub-network structure of the\nbrain by performing cluster-specific updates to the nodes unlike\nuniform node updates like vanilla graph transformers, further\nallowing us to learn and interpret the subclusters. We validate\nour approach on the Adolescent Brain Cognitive Develop-\nment (ABCD) dataset, comprising 7,957 participants, for the\nprediction of total cognitive score and gender classification.\nOur results demonstrate that SCGT outperforms the vanilla\ngraph transformer method, and other recent models, offering\na promising tool for modeling brain functional connectivity and\ninterpreting the underlying subnetwork structures.", "sections": [{"title": "I. INTRODUCTION", "content": "Resting-state functional magnetic resonance imaging (rs-\nfMRI) provides an unprecedented opportunity to delve into\nthe human brain's functional organization [1]. By mea-\nsuring spontaneous fluctuations in the blood-oxygen-level-\ndependent (BOLD) signal when the brain is at rest, rs-\nfMRI reveals intrinsic functional connectivity patterns among\ndifferent brain regions. Although traditional MRI studies\nin the literature focused on structural brain measures for\nvarious phenotypes ([2], [3], [4]), rapidly growing studies\nhave emerged to investigate the prediction of intelligence\nbased on brain functional features ([5], [6], [7], [8]). Func-\ntional connectivity (FC) is defined as the degree of temporal\ncorrelation between regions of the brain computed using\ntime series of BOLD signals, and rs-fMRI FC provides a\ncomprehensive view of the brain's intrinsic organization.\nGraph neural networks (GNNs) have emerged as powerful\ntools for modeling complex relational data, such as brain\nnetworks ([9], [10]). However, most existing GNNs assume"}, {"title": "II. MATERIALS AND METHODS", "content": "The Adolescent Brain Cognitive Development (ABCD)\nstudy is a large ongoing longitudinal study following youths\nfrom age 9-10 into late adolescence to understand factors that\ninfluence individual brain development. Participants were"}, {"title": "B. Proposed Architecture", "content": "The architecture of SCGT is designed to effectively model\nthe subnetwork structures within the brain by introducing a\nnovel attention mechanism tailored for subclustered graphs.\nOur model uses static FC correlation features as input to\nthe transformer model. The input graph is constructed with\nnodes representing brain regions of interest (ROIs) and edge\nweights representing the correlation coefficients between the\nROIs. Positional encodings are incorporated using Laplacian\neigenvectors to capture the structural information of the\ngraph.\nThe core component of SCGT is the self-clustering graph\nattention block, which enables nodes belonging to different\nclusters of the brain to learn embeddings in a customized\nmanner, addressing the issue of uniform node updates in\ntraditional GNNs and GTs. Unlike standard transformer\nmodels that apply the same attention mechanism across\nall nodes, our approach allows for specialized attention\nmechanisms for each subnetwork or cluster within the graph.\nThe output node representations from the attention blocks\nare then aggregated using a concatenation readout function,\nwhich combines the features from all nodes to produce a\ngraph-level representation. This representation is then passed\nthrough a fully connected layer for the final prediction or\nclassification task."}, {"title": "C. Functional connectivity graph creation", "content": "The initial phase involves the processing of input from\nfMRI time series data to create a static FC matrix. Using\nROIs time series, Pearson's correlation between ROIs is\nused to create a FNC matrix. The resulting FNC matrix\nis used to construct an undirected graph denoted by the\ntuple $G = (V,E)$ with an adjacency matrix $A \\in R^{N*N}$. Each"}, {"title": "", "content": "An edge set $E$ represents the functional connections between\nROIs with each edge $e_{ij}$ linking two nodes $(i, j) \\in E$.\nThe Adjacency matrix $A_{k} \\in R^{N \\times N}$ is formed as in Equation\n1, where $e_{i,j}$ is the thresholded element of FNC matrix to\nachieve either fully connected or sparse graph.\n$A=\\begin{cases}\n0, & i = j \\\\\ne_{i,j}, & otherwise\n\\end{cases}$                                                                       (1)\n1) Laplacian Eigenvectors as Positional Encodings: To\ninclude positional encodings (PE), we make use of Laplacian\neigenvectors, a fundamental concept in spectral graph theory,\nto represent the underlying graph structure [14]. Laplacian\neigenvectors capture the global structure of the graph and\nprovide meaningful position information for each node. The\nnormalized graph Laplacian is computed as:\n$L=I-D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}$\nwhere $D$ is the degree matrix, and $A$ is the adjacency\nmatrix. The eigenvectors $\\lambda_{i}$ corresponding to the smallest\nnon-trivial eigenvalues are used as positional encodings for\nnode $i$.\n2) Self-Clustering Graph Attention Block: The self-\nclustering graph attention block is a pivotal component of our\narchitecture, specifically designed for brain analysis. Unlike\nconventional GT models that apply uniform processing to\nall nodes, this block incorporates specialized mechanisms"}, {"title": "", "content": "where $h_{i}$ is the node feature, and $\\lambda_{i}$ is the positional\nencoding for node $i$.\nb) Self-Clustering Attention: The attention mechanism\nwithin this block is tailored to accommodate the brain's sub-\nclustered structure. The parameters for the key, query, and\nvalue matrices in layer $l$ are defined as functions of the\npositional encoding $\\lambda_{i}$:\n$w^{(k)^{(l)}} = \\sigma (\\theta_{1}^{(l)}.\\lambda_{i})+b^{k^{(l)}}),$\n$w^{(q)^{(l)}} = \\sigma (\\theta_{2}^{(l)}.\\lambda_{i})+b^{q^{(l)}}),$\n$w^{(v)^{(l)}} = \\sigma (\\theta_{3}^{(l)}.\\lambda_{i})+b^{v^{(l)}}),$\nwhere $\\theta^{(l)}_{i} \\in R^{N \\times k_{r}}$ is a shared parameter matrix across all\nkey, query, and value computations, facilitating the learning\nof clustering assignments for the nodes. Here, $k_{r}$ denotes the\nnumber of clustered communities.\nUsing these parameters, the key, query, and value matrices\nare computed as:\n$K_{ROI} = W^{(K)^{(l)}} . \\hat{H},$\n$Q_{ROI} = W^{(Q)^{(l)}} . \\hat{H},$\n$V_{ROI} = W^{(V)^{(l)}} . \\hat{H},$\nwhere $\\hat{H}$ is the matrix of node features with positional\nencodings.\nc) Layer Update Equations: The update mechanism\nfor node features in layer $l$ is governed by the following\nequation:\n$h_{i}^{(l+1)} = \\sigma\\left[ h_{i}^{(l)} + \\left[ \\sum_{j \\in N_{i}} w_{ij}^{(k,l)} V_{ROI}^{(l)} \\right]\\right] ,$\nwhere:\n$\\bullet \\sigma$ is an output transformation function for node\nfeatures, typically a linear layer followed by a non-linear\nactivation.\n$\\bullet K$ denotes the number of attention heads or clusters.\n$\\bullet N_{i}$ is the set of neighboring nodes connected to node $i$.\n$\\bullet w_{ij}^{(k,l)}$ are the attention weights for the $k$-th head at layer\n$l$, computed as:\n$w_{ij}^{(k,l)} = \\text{softmax} \\left( \\frac{Q_{ik}^{(l)} K_{jk}^{(l)}}{\\sqrt{d_{k}}} \\right)$                                                                      (10)"}, {"title": "", "content": "ik\njk\nwhere $Q_{ik}^{(l)}$ and $K_{jk}^{(l)}$ are the elements of the query and\nkey matrices for nodes $i$ and $j$ in head $k$ at layer\n$l$, respectively, and $d_{k}$ is the dimensionality of the\nkey/query vectors."}, {"title": "D. Readout", "content": "After passing through the self-clustering graph attention\nblocks, the node representations are aggregated using a\nconcatenation readout function. This function combines the\nfeatures from all nodes into a single graph-level representa-\ntion:\n$h_{G} = \\text{Concat}(h_{1},h_{2},...,h_{N})$                                                          (11)\nThis graph-level representation $h_{G}$ is then passed through a\nfully connected layer for the final prediction or classification\ntask."}, {"title": "III. EXPERIMENTAL SETUP AND RESULTS", "content": "The neural network architecture depicted in Fig. 1 was\nimplemented using PyTorch [15] and PyTorch Geometric\n[16] for the specific graph neural network components. The\nnumber of nodes was 100 (corresponding to the Neuromark\nICA components). We used 100 eigenvectors as positional\nencodings $\\lambda_{i}$ for each node $i$. We used two layers of self-\nclustering graph attention blocks, each with dimension $D =$\n64 for output features. The number of clustered communities\n$(k_{r})$ was set to 7, inspired by the seven functional networks\ndefined by Yeo et al. [17].\nOur approach employed a 5-fold stratified cross-validation\nprocedure on the ABCD dataset. In each fold, we divided\nthe dataset into training, validation, and test sets, with the\nvalidation and test sets comprising 20% of the original data.\nThe neural network training was performed for 100 epochs,\nutilizing the Adam optimizer [18] and the SmoothL1Loss\nloss function. An early stopping mechanism was incor-\nporated, terminating training if the validation loss failed\nto decrease over 30 consecutive epochs. Additionally, the\nlearning rate was reduced by a factor of 0.3 with a patience\nsetting of 30. We utilized a batch size of 32.\nWe compared our proposed model with some baseline\nmodels, which included BrainGNN [11], BrainNetCNN [19],\nBrainRGIN [12], Graph Transformer (GT) [14], and Support\nVector Regression (SVR)."}, {"title": "B. Results", "content": "In this section, we present the results of evaluating the per-\nformance of our proposed (SCGT) against several baseline\nmodels on two key tasks: predicting total cognitive scores\nand classifying gender based on static FC data from the\nABCD dataset. The comparison focused on demonstrating\nthe superiority of our novel attention mechanism over the\ntraditional Graph Transformer (GT) model by highlighting\nimproved performance metrics."}, {"title": "IV. INTERPRETATION OF LEARNED SUBCLUSTERS", "content": "A notable advantage of our SCGT is its ability to learn\nand interpret subclusters within the brain's functional con-\nnectivity network. This interpretability is facilitated by the\nshared parameter matrix $\\theta_{i}^{(l)}$, which captures the association\nbetween each of the 100 brain regions (nodes) and the\n7 predefined functional communities. To identify clusters\nfrom the $(100 \\times 7) \\theta_{i}^{(l)}$ matrix, we assign each node to\nthe community with the highest association score in its"}, {"title": "", "content": "corresponding row. Mathematically, for each node $i$, the\nassigned cluster $c_{i}$ is given by:\n$C_{i} = \\text{arg max}_{c \\in \\{1,...,7\\}} \\theta^{(1)}_{[i,c]}$\nThis approach ensures that each brain region is uniquely\nassigned to the most strongly associated functional com-\nmunity, facilitating clear and interpretable clustering of the\nbrain's functional networks."}, {"title": "V. DISCUSSION AND CONCLUSION", "content": "In this research study, we developed a novel technique\ncalled SCGT, which enhances the attention mechanism of the\ntransformer by using self-clustering graph attention which\nenables nodes belonging to distinct clusters representing\ndifferent brain networks to learn embeddings in a customized\nmanner, addressing the issue of uniform node updates in tra-\nditional GNNs. Our preliminary experiments on the ABCD\ndataset demonstrate that SCGT outperforms existing models\nin predicting total cognitive scores and gender classification,\nindicating its potential for modeling brain functional connec-\ntivity and interpreting underlying subnetwork structures.\nThis is preliminary work, and in the future, we aim to\nperform more extensive experiments on other phenotypes and\nexplore the interpretability of the learned subclusters from\nSCGT. Furthermore, this approach can be extended to make\nit usable for any kind of graphs with subnetwork, and we\nplan to test on general graph benchmark datasets to see if\nsubclustered updates are useful in overall scenarios."}]}