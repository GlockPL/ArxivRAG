{"title": "dafny-annotator:\nAI-Assisted Verification of Dafny Programs", "authors": ["Gabriel Poesia", "Chloe Loughridge", "Nada Amin"], "abstract": "Formal verification has the potential to drastically reduce\nsoftware bugs, but its high additional cost has hindered large-\nscale adoption. While Dafny presents a promise to signifi-\ncantly reduce the effort to write verified programs, users are\noften required to provide logical annotations to aid the veri-\nfier. Here, we explore using a combination of Large Language\nModels and search to build dafny-annotator: a tool that\nadds logical annotations to a Dafny method until the verifier\ncan prove it correct. On a test set from the DafnyBench col-\nlection of programs, greedy search guided by LLaMa 3.1 8B\nsuccessfully annotates only 15.7% of the methods. Since this\ndata-driven approach is hindered by the lack of large-scale\ntraining data, we propose a method for open-ended synthesis\nof new Dafny programs in a flexible pipeline where LLMs for-\nmulate high-level ideas, implement them, and incrementally\npropose changes to existing programs, which Dafny vali-\ndates. This gives us a synthetic dataset, DafnySynth, which\nwe use to augment DafnyBench for training. Fine-tuning\non both datasets boosts LLaMa 8B's success rate to 50.6%\nsignificantly better than the base model, or training on either\ndataset alone. Our results suggest a path towards capable\nAI assistants for languages that don't yet have large-scale\nhuman-generated examples. In turn, such assistants might\nreduce friction for users and ultimately drive adoption.", "sections": [{"title": "1 Introduction", "content": "Software bugs remain a persistent and costly problem in\nthe technology industry, with estimates that poor software\nquality cost the US economy $2.08 trillion in 2020 alone\n[5]. Formal verification has the potential to drastically im-\nprove software quality: in verified software, the error surface\n(modulo performance considerations) is reduced to its spec-\nification, while the implementation is proved to adhere to\nits specified behavior. However, practical adoption of for-\nmal software verification has still been limited to specialized\nprojects where errors are exceptionally costly, due to the\nnon-trivial cost added by verification itself.\nThis extra cost stems from both (a) formalizing the speci-\nfication of the program and (b) creating verification artifacts\n(e.g., formal proofs) that a formal verifier needs to validate\nthe program's implementation. In many widely used theorem\nprovers, like Coq [1] and Isabelle [12], the cost of verifying\nthe program is particularly high, as users must write cor-\nrectness proofs in addition to the program itself. Though\nusers can partially automate this process using tactics\nreusable proof producing procedures -, the proof scripts\nneeded to verify realistic programs are still often larger than\nthe program itself. In contrast, Dafny's design [7] proposes to\nlargely automate verification, relying on SMT (Satisfiability\nModulo Theory) encodings and solvers. Instead of complete\nproofs, users only need to provide logical annotations (e.g.,\nassertions, or loop invariants) to guide the verifier when it\ndoesn't succeed, and sometimes prove helper lemmas. While\nthis already substantially reduces user input, adding these\nannotations is still non-trivial, requiring knowledge of for-\nmal semantics and logic than most software developers have\nnot been exposed to, as well as bespoke intuition on what\ntriggers the automated verification.\nIn this work, we introduce dafny-annotator\u00b9, a tool that\nleverages Large Language Models (LLMs) and search to au-\ntomatically add logical annotations to Dafny methods. Con-\ncretely, given a Dafny method, with its formal specification\nand implementation, dafny-annotator will use an LLM to\nmake proposals of annotations to add to the program, au-\ntomatically localize where to insert the annotations, and\niterate this procedure until Dafny can prove the method's\nspecification (or we reach a maximum number of trials).\nWe use DafnyBench [10], a recent benchmark of 1326\nstandalone Dafny programs collected from Github, to evalu-\nate dafny-annotator. Specifically, we remove the human-\nwritten annotations from a held-out test set of methods, and\ntest whether dafny-annotatorcan annotate each resulting\nmethod until Dafny verifies it. We use LLMs from the LLaMa\nfamily to make annotation proposals, and evaluate their suc-\ncess rate at eventually arriving at fully verified methods. The\nbase LLaMa 8B model only obtains a success rate of 15.7% at\nthis task. We then describe a simple method for fine-tuning a\nbase LLM using existing annotated programs for this task of\nguiding an annotator: fine-tuning on a subset of DafnyBench\nboosts success rate to 20.5%.\nWhile data-driven methods improve given more data, the\nnumber of Dafny programs available for training is extremely\nsmall compared to popular languages like Python, which are"}, {"title": "2 Methods", "content": "dafny-annotator aims to automate the addition of logical\nannotations to Dafny programs using a combination of Large\nLanguage Models (LLMs) and search strategies. We divide\nour method into three main components: an LLM-guided\nsearch to annotate methods, a fine-tuning procedure using\nan existing training set of annotated programs, and a pipeline\nto synthesize a potentially unbounded number of programs\nfor training."}, {"title": "2.1\nLLM-Guided Greedy Search", "content": "To automatically annotate Dafny methods, we employ a\nsimple greedy search for annotations that the Dafny verifier\naccepts. Given a Dafny program with its implementation and\nspecification but lacking necessary annotations, we proceed\nas follows:\n1. Annotation Proposal: We prompt the LLM to gener-\nate K candidate annotations. The prompt includes the\ncurrent program, up to the method we are annotating,\nwith its existing annotations. We only require the LLM\nto output the annotation, but not the location at which\nit should be inserted.\n2. Insertion: For each candidate annotation, we attempt\nto insert it at all syntactically valid locations within\nthe method (e.g., for loop invariants, we try adding\nit to each of the loops; for assertions, we add it after\nevery statement). We try these options in parallel for\nefficiency. This makes the LLM's task easier, since it\nhas less to predict.\n3. Greedy Selection: We follow a greedy strategy by\nkeeping the first annotation and program location that\nDafny accepts. If none of the candidates verify in any\nlocation in the current program, we keep the existing\nprogram at the end of this step.\ndafny-annotator repeats these steps up to a maximum\nnumber of iterations, stopping early if Dafny can prove the\nmethod's post-condition after adding an annotation."}, {"title": "2.2 Fine-Tuning the LLM for Annotation Generation", "content": "While general-purpose LLMs like LLaMA [2] can generate\nplausible annotations, their performance improves signifi-\ncantly when fine-tuned on task-specific data. To this end, we\ncreate a fine-tuning dataset for this task of proposing anno-\ntations for a given program by leveraging existing annotated\nDafny programs.\nConcretely, in the search algorithm above, we sample an-\nnotations from the LLM given the current program. Thus,\nour fine-tuning dataset should consist of pairs of (pi, ai) pro-\ngrams and an annotation to be added to that program. Given\nan annotated program p, we thus construct these training\npairs by removing the last annotation a (e.g., assertion, in-\nvariant, or decreases clause) from the program p, yielding\na program p-a missing one annotation. The pair (p-a, a)\nthus becomes a training example. We repeat this procedure\nwith program p-a until there are no more annotations. Each\nof these pairs becomes one training example, as shown in\nFigure 1.\nWe collect a training set for this annotation prediction\nproblem from DafnyBench [10], the largest existing dataset\nof Dafny programs. DafnyBench consists of 1326 stand-alone\nDafny programs collected from Github, deduplicated and fil-\ntered for validity (i.e., Dafny verifies all of them). For each\nprogram, we systematically remove all annotations, and sim-\nulate their reinsertion into the program one by one, as de-\nscribed above. These examples are formatted as strings as"}, {"title": "2.3 Edit Graph for Synthetic Dataset Generation", "content": "As we show in Section 3, even modest fine-tuning can drasti-\ncally improve the performance of dafny-annotator. How-\never, the scarcity of large-scale datasets of Dafny programs\nposes a significant limitation for training data-intensive mod-\nels like LLMs. We note that this presents a challenge to the\nProgramming Languages community to propose new tools\nmore broadly: as experienced and new developers alike be-\ncome used to AI assistance, new languages, even if they\nmight contain significant design advantages over existing\nones, can become less appealing due to existing Als being\nunfamiliar with them. To overcome this challenge, we need\nto be able to train Als to assist human users even for new lan-\nguages, without relying on decades of accumulated human-\nwritten programs for training (as is the case for languages\nlike Python and Java).\nTo address this, we develop a general methodology for\nsynthesizing an arbitrarily large number of valid programs,\nbootstrapping from LLMs imperfect knowledge of Dafny and\ntheir ability to do in-context learning. We first instantiate\nthis method in Dafny, relying on the Dafny verifier to ensure\ncorrectness.\nOur method is based on the simple observation that large\nprograms are written as a series of much smaller edits to\nexisting programs. An initial code repository might start with\na simple prototype of some core functionality, and evolve\nfrom there, diff by diff over time, these changes compound\ntowards a large project. We will use LLMs to likewise start\nby writing a simple program, then iteratively propose small\ndiffs to this program. At each step, we can rely on Dafny\nto ensure that the changes keep programs syntactically and\nsemantically valid.\nConcretely, we propose to create an Edit Graph, where\nnodes have a type and some content, which can be turned\ninto new nodes by a series of Editors. Edges represent the\nrelationship that a node was derived from another. When\nan Editor is scheduled to run, it (a) selects nodes from the\nexisting graph that it is able to edit, and (b) proposes new\nnodes based on the existing ones. We initialize the graph\nwith a single node, of type root, and empty content. Then,\nwe run a pipeline composed from the following editors:\nIdea Proposer: Takes the initial root node and exist-\ning nodes of type idea, and uses an LLM to sample\na set of N new \u201cideas\u201d (high-level natural language\ndescriptions of a simple verified program one could"}, {"title": "3 Experiments", "content": "We implement dafny-annotator using the PyTorch [16]\nand vLLM libraries [6], and use it in combination with two\nrecent open-weights LLMs hosted on Hugging Face [15]:\nLLaMa 3.1 8B [2] and CodeLlama 7B [14]. We experiment\nboth with the base models, as well as versions fine-tuned on\nvarious datasets we obtain from combinations of DafnySynth\nand DafnyBench.\nTo evaluate dafny-annotator, we split DafnyBench into\na training set of 1000 Dafny methods, holding out the re-\nmaining 326 for test. We then strip annotations from the test\nmethods, and measure the success rate of dafny-annotator\nat recovering annotations until the Dafny verifier accepts the\nprogram. Out of the 326 test methods, only 83 failed to verify\nafter we removed the original annotations. We thus exclude\nthe remaining programs from the evaluation, and focus on\nthe 83 where Dafny needs hints to prove the specification.\nFor fine-tuning dafny-annotator, we use Low-Rank Adap-\ntation (LoRA [4]), a lightweight fine-tuning method that re-\nquires significantly less GPU memory than full fine-tuning.\nAll experiments were ran on a machine with one NVIDIA\nA100 80GB GPU, and 30 cores, used to parallelize calls to\nDafny during greedy search. We use r = a = 128, and train\nall models (regardless of their training set size) for 1500 gra-\ndient steps (thus, all fine-tuning runs use the same amount\nof compute: we only vary their training dataset).\nTo construct training sets, we take subsamples of 0%, 25%,\n50% and 100% of the 1000 training programs from Dafny-\nBench (DB), and also vary whether we augment each of these\nDafnyBench subsamples with DafnySynth. This gives 8 vari-\nations of the training set (one of which is the \u201ctrivial\u201d empty\ntraining set, corresponding to no fine-tuning). We then eval-\nuate the success rate of dafny-annotator, when guided by\na model fine-tuned in each of these datasets, with up to 5\niterations of the greedy search procedure from Section 2.\nTable 1 shows the results with both LLaMA 3.1 8B and\nCodeLlama 7B. We make the following observations:\nEven small-scale fine-tuning can help dafny-annotator\nsubstantially. Even with the order of a few thousand train-\ning examples in our datasets \u2013 much less than typical for\nLLM training, an LLM-guided tool like dafny-annotator\ncan enjoy substantial performance gains. The base LLAMA\n3.1 8B model manages to verify 15.7% of the programs in\nthe test set after fine-tuning on DafnySynth alone, this\nnumber improves to 27.7%. Similarly, for CodeLlama 7B, the\nbase performance improves from only 6% to 20.5% after fine-\ntuning on DafnySynth. Moreover, we generally see gains as\nwe increase the percentage of DafnyBench used for train-\ning (a trend that only breaks when not using DafnySynth in\ncombination with it when training LLaMa 3.1 8B).\nDafnySynth provides high-quality training data. Our\nbest result is obtained in a combination of DafnySynth and\nDafnyBench, and using LLaMA 3.1 8B: using all of the avail-\nable training data in both datasets yields a model with a\nsuccess rate of 50.6%. Interestingly, for LLaMA 3.1 8B, us-\ning DafnySynth alone is better than using DafnyBench for\ntraining, while for CodeLlama, DafnySynth leads to the same\nimprovement as training on 250 programs from DafnyBench.\nGeneral code pre-training might not always help lan-\nguages with smaller representation. We do not see gains\nfrom using CodeLlama compared to the base Llama 3.1 8B\nmodel, despite it having been trained with a focus on code.\nMainstream languages like Python will have a much larger\nrepresentation in CodeLlama's training set, which possibly\naccounts for this fact. Pre-training on those languages does\nnot seem to necessarily help with verifying Dafny programs,\nsince the main feature we're interested in - verification an-\nnotations - is not present in popular languages. We note that\nthis ability is still likely helpful in generating DafnySynth:"}, {"title": "4 Related Work", "content": "Our work is most closely related to recent efforts using LLMs\nin program verification, in Dafny and other languages. Re-\ncent work in languages such as Isabelle (e.g. Baldur [3]),\nCoq (e.g. PALM [11]), and Lean (e.g. miniCodeProps [9])\nfocus on proof synthesis using LLMs, since proofs must be\nexplicitly constructed in these languages. As is typically the\ncase in deep learning, fine-tuning on domain-specific data\ndramatically boosts performance of these systems [3]. Coq\nand Isabelle have had a history of larger developments that\nallows data-driven approaches to benefit from already exist-\ning training data. For example, Isabelle's Archive of Formal\nproofs has more than 100k proofs contributed by users. While\nstill relatively small compared to the most popular program-\nming languages, these examples can already support a range\nof data-driven approaches for proving properties in these\nlanguages.\nWe build on DafnyBench [10], a recent dataset of Dafny\nprograms collected from Github. DafnyBench is the largest\nDafny dataset so far, and yet it has only 1326 programs.\nWhile we show that even small-scale fine-tuning on Dafny-\nBench can already boost LLM performance, we believe that\nsynthetic data generation, as has been applied in other do-"}, {"title": "5 Conclusion", "content": "We introduced dafny-annotator, an LLM-guided tool for\nautomatically annotating Dafny methods. While our initial\nfocus was in generating annotations, we note that these alone\nare often not the main difficulty of writing verified programs\nin Dafny. dafny-annotator assumes that programs come\nwith formal specifications, while a challenge for users is\noften in formally describing the desired behavior. We believe\nthat LLMs have a great potential to help in that step, as\nwell, due to their understanding of natural language and of\ncoding patterns that are common in human-written code"}]}