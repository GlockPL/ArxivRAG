{"title": "Cross Domain Adaptation using Adversarial networks\nwith Cyclic loss", "authors": ["Manpreet Kaur", "Ankur Tomar", "Srijan Mishra", "Shashwat Verma"], "abstract": "Deep Learning methods are highly local and sensitive to the domain of data they are\ntrained with. Even a slight deviation from the domain distribution affects prediction\naccuracy of deep networks significantly. In this work, we have investigated a set\nof techniques aimed at increasing accuracy of generator networks which perform\ntranslation from one domain to the other in an adversarial setting. In particular,\nwe experimented with activations, the encoder-decoder network architectures, and\nintroduced a Loss called cyclic loss to constrain the Generator network so that\nit learns effective source-target translation. This machine learning problem is\nmotivated by myriad applications that can be derived from domain adaptation\nnetworks like generating labeled data from synthetic inputs in an unsupervised\nfashion, and using these translation network in conjunction with the original domain\nnetwork to generalize deep learning networks across domains.", "sections": [{"title": "Introduction", "content": "The problem of input distribution bias has been a long standing problem in deep networks, which is a\nnatural outcome of the MLE framework to arrive at optimized weights. Based on the fact that data\ncollected in real world is often localized in its distribution and entails huge efforts with respect to\nlabeling; the concept of Domains for input space is often used. Domains in an NLP context could\nrefer to the source that the input comes from, for e.g. Wall Street Journal forms one domain of input\nspace while MEDLINE, a medical journal, forms another. In the context of vision problems, we\ncould have MNIST (handwritten digits) as one domain, and SVHN (digits written on real world\nobjects) as another domain. A network trained on a particular task in one domain doesn't give a\nsimilar accuracy on other domain.\nSome interesting problem statements naturally arise from this aspect of deep learning. Is it possible\nto generalize one single network to generalize well across different domains? Many Machine\nLearning techniques try to answer this question, for example, auto-encoders which share the same\nlatent space. Other techniques work on an associated problem, which is to learn mappings from one\ndomain(often called source domain) to another(often called target domain) and use a combination of\ntranslation and the learned task to generalize better. Solving the domain translation problem also\nleads to an interesting application apart from improved cross-domain generalization of deep networks,\nthat is, to generate labeled data for a particular domain using this translation in an unsupervised\nfashion. In what follows for this report, we define the following terms. Source domain refers to\nthe domain of input space where we have labels and a corresponding task that we want to gen-"}, {"title": "Related Work", "content": "Convolutional neural networks when trained on large-scale datasets, act as very powerful tools\nwhich can be used across a wide variety of tasks and visual domains. However, sometimes due\nto the phenomenon of dataset bias or domain shift the models which are trained along with these\nrepresentations do not generalise well when they see new dataset. This problem can be solved\nin many different ways. The the group of models that are used for such applications are called\nTransfer Learning. In order to go ahead and define the different kinds of related work that has been\ndone in this field, we would like to define transfer learning in a detailed way. Transfer learning\nor inductive transfer is a research problem in machine learning that focuses on storing knowledge\ngained while solving one problem and applying it to a different but related problem. Given source\nand target domains $D_s$ and $D_t$ where $D = X, P(X)$ and source and target tasks $T_s$ and $T_t$ where\n$T = Y, P(Y|X)$ there are four ways in which the scenarios can differ:"}, {"title": "Methodology", "content": "Our experiments are based on modifying aspects of GAN training which would allow the discriminator\nto not win excessively and render the Generator network mute. Along with that, we proposed certain\nmodifications and used a symmetric GAN network which performs a translation from source to target\ndomain and reinforces the Generator networks to preserve ordinality of target to source transformation\non the regression task. We define below the architectures for different component networks of our\ncomposite architecture, and then move on to describe phase-wise the methodology to train these\nnetworks."}, {"title": "Component Networks", "content": "We have 5 networks which we are training to learn effective Target to source transformations."}, {"title": "Steering Regression Network $R_{steering}$", "content": "This network is responsible for the core learning task, which is to predict the steering angle given an\nimage. To perform this, we built a convolutional neural network with 3 convolution block, and one\nfully connected block. The non-linearities we used were ReLU and we used the least squares loss\nfor regression output. Each convolution was batch normalized, and each block down-sampled the\nimage by a factor of half using Max Pooling. Our input size is 80X160 pixels with 3 channels. The\nconvolution layers have 5X5, 5X5, and 3X3 windows, with 24, 48 and 64 channels after their outputs\nrespectively and are batch normalized. These blocks are followed by a linear layer with one output\nfor the steering angle."}, {"title": "Source to Target and Target to Source Domain Translation networks $G_{S\u2192T}$ & $G_{T\u2192S}$", "content": "This domain translation network is inspired by autoencoder architectures, mainly based on the hy-\npothesis that such architectures would force the network to distill latent features that are semantically\nimportant from which the source domain image can be synthesized. Specifically, the domain transla-\ntion network consists of 6 layer blocks. The first 3 layers perform convolution operations and the later\n3 perform deconvolution operations along with un-pooling. The image while passing through these\nlayers has 24, 48, 64, 48, 24, and finally 3 channels respectively. The window sizes vary as 3X3,\n3X3, 5X5, 5X5, 3X3, and 3X3 through the layers. Each block is batch normalized, and two Max\nPooling and Unpooling schemes have been used. One is a non-overlapping 2 stride, 2X2 window\nMax Pooling, and other is the 1 stride, 2X2 window Max Pooling.\nWe also considered adding skip connections between the second and fourth layers, but decided against\nit as experimentation was taking a long time. All layers had a leaky ReLU to improve GAN training\nand prevent gradient vanishing."}, {"title": "Discriminator Networks $D_{S\u2192T}$ & $D_{T\u2192S}$", "content": "This network essentially mimicked the encoder part of the auto-encoder inspiration we took for the\ndomain translation network and consisted of 3 convolution layer blocks with Leaky ReLUs. These\nconvolutional layers were followed by two fully connected layers. The inner FC layer had 100 units,\nand the final FC layer had two units, each representing a domain.\nThe architectures for all these networks are explained in detail in Table 1. We next elaborate on our\nloss functions, modifications specific to GAN training and training routines."}, {"title": "Loss Functions and Overall Training Methodology", "content": "In this section, we define the overarching process we followed to ensure our Domain Translation\nnetworks had robust learning, and this section entails the most important components of our proposed\nhypotheses, the reconstruction loss. In essence, we are going to train simultaneously all the networks\nin this setup using a composite that concurrently tries to align all the goals for our different learning\ntasks. As initialization to this process, we will use pretrained individual networks $G_{S\u2192T}, G_{T\u2192S}$ and\n$R_{Steering}$. Therefore, our process is a 3-phase method, where Phase 1 trains a regression network on\nSource Domain (the Udacity Dataset) and Phase 2 trains the GAN setup networks. In the final phase\n3, we train all the 5 networks in a concurrent fashion to achieve goals important for all the tasks that\nare taking place in each of these networks. We describe in detail the Learning framework used for\neach of the 3 phases."}, {"title": "Phase 1 : Training Steering Angle Regression", "content": "This 4 layer network was trained using the Empirical Risk Minimization framework, which translated\nto minimizing the mean squared error for predicted steering angle. We used the loss\n$L_{Steering}(X_s, Y_s, f_s) = E_{x\u00a7~X_5}[||f_s(x_s) \u2013 Y_s||_2]$\n$= (1/n) \\sum_{i=1}^n (f_s(x_i) \u2013 Y_i)^2 $\nThis was a fairly standard network to train, and we used Xavier Initialization for weight initializations,\na learning rate of 1e-4 and second order Adam optimizer for optimization."}, {"title": "Phase 2 : Training the Domain Translation and Discriminators in GAN setting", "content": "This part of the training process required significant amount of engineering which was not driven\nstrictly by strong mathematical background. Our Loss for the domain translation network $G_{S\u2192T}$ and\n$D_{S T}$ was"}, {"title": "Phase 3 : Training all the networks together", "content": "This part of the training entailed loading the networks trained in previous two phases as pre-\ntrained networks for the third phase. To enforce semantic retention, it should be the case that\n$GTS(GST(x_s)) ~ x_s$, and similarly $Gs\u2192T(GT\u2192S(x_t)) ~ Xt$. While we want to keep on\nrefining the $Gs\u2192T,GT\u2192s, Ds\u2192T, DT\u2192s$, and $R_{steering}$ simultaneously so that they achieve their\nindividual goals we would like to individual pieces of this network group to reinforce (or optimally\nplay against) each other. One way of doing this is to use the idea mentioned above, and introduce a\nloss which reinforces this reconstruction aspect of the domain translation networks. We will call this\nthe reconstruction loss, i.e.\n$L_{Rec}(G_{ST}, G_{T\u2192S},X_S,X_T) =E_{xs~X_S}[||G_{T\u2192s}(G_{S\u2192T}(x_s)) - X_s||_1]$\n$+ E_{xt~X_T} [||G_{S\u2192T}(G_{T\u2192S}(x_t)) - X_t||_1]$\nAnother way we are trying to enforce the semantic retention in the domain translation networks is by\ntraining the regression task network to predict from both the source images, as well as the generated\nimages from these source images, labeled using the source label. Therefore, our objective function\nlooked like\n$L_{combined}(X_S, X_T,Y_s,G_{S\u2192T},G_{T\u2192s},D_{S\u2192T},D_{T\u2192S}, R_{Steering}) =$\n$L_{GAN}(G_{T\u2192S}, D_{T\u2192S},X_S,X_T)+$\n$L_{GAN}(G_{T\u2192S}, D_{T\u2192S},X_S,X_T) + L_{Rec}(G_{S\u2192T},G_{T\u2192s},X_S,X_T)+$\n$L_{Regression}(fs,X_5,X_T,Y_s,G_{S\u2192T})$\nWith parameters shared between different losses which form $L_{combined}$, our expectation was a more\nstable training of the GAN setting and a more generalized $R_{steering}$. However, this was a very\nexpensive and tricky training process with many possibilities for sequential training, much like the\nsequential training often followed in GANs. In essence, our objective optimization problem was\n$S^* = \\underset{f_s}{\\operatorname{min}} \\underset{G_{ST}}{\\operatorname{min}} \\underset{D_{S\u2192T}}{\\operatorname{max}} (L_{combined})$\n$\\underset{G_{TS}} {min}  \\underset{D_{TS}}{\\operatorname{max}}$\nWe implemented the gradient step for this optimization problem in modular manner, much similar to\na single GAN training, where we use a complement of the objective function to solve for the max\nproblem. This forces the training to cycle between different data pieces and networks, and reduces\nthe chance of going in the true gradient direction of the objective function, which in itself is not a\nguarantee on reaching the saddle point we wish to find for $L_{Combined}$. So hoping for the best, we\ntook mini-batches which cycled through 1. Source Images, 2. Synthesized Source Images from\nTarget Images, 3. Target Images, and 4. Synthesized target images from Source Images, where we\nupdated the steering angle regression only during the mini-batches 1 and 4 (attaching the source label"}, {"title": "Datasets", "content": "1) Dataset for which we have trained the labels(source)- Udacity self driving dataset\n2) Dataset for which we have done the domain adaptation- comma.ai real world dataset(target)"}, {"title": "Udacity self-driving data:", "content": "The Udacity self-driving data was created for there Machine Learning nanodegree program. It\nwas done in collaboration with Google and the objective was to initiate more research in the\nspace of self-driving cars. The dataset consists of 8032 images which are extracted from a\nsimulator in a gameplay environment. The download link is: https://github.com/ymshao/\nEnd-to-End-Learning-for-Self-Driving-Cars. Some of the images of the dataset are:\nThe images have been generated through the simulator that Udacity has created and images can be\ngenerated by any run of the simulator which simulates it like a driving environment and the vehicle\ncan be controlled through the four arrow keys on the keyboard(like any other simple game). The\ndataset gives different values of throttle, driving angle through 3 different camera views i.e. centre,\nleft and right. We have chosen the central frame and have used the images in size 80X160. We chose\nthis dataset because of the ease at which any number of examples can be generated with high amount\ndetails concerning the steering angle. In comparison to this, any physical simulator would require\nphysical measurement equipment to measure steering angle and also a different pipeline to translate\nthrottle data to computer comprehensible data. Also, would require a US driving license which none"}, {"title": "Comma.ai self-driving data:", "content": "The comma.ai dataset is a real-world dataset that is also made by central camera mount over the\ndriver's dashboard. It is also publicly available for academic use and is intended to enhance research\nin the self-driving space. Comma.ai is a company which was actively involved in the space of\nself-driving cars. Some of the sample images of the comma.ai dataset are as follows. Also, note that\nthe dataset consists of images both during the day and the night time."}, {"title": "Experiments", "content": "We build our network as described in the methodology. The training process takes place in three\nphases. The first phase trains the convolution network on source data. This is a fully supervised\ntask as we train source images with their steering angle labels. The second phase trains the domain\ntranslation networks and the corresponding discriminators using GAN loss and reconstruction loss.\nThe third phase uses pre-trained networks from Phase 1 and 2 with the final aim of learning $R_{steering}$\nfor both source and target domains. In the third phase, all the network parameters of $G_{S\u2192T}, G_{T\u2192s},$\n$D_{S\u2192T}, D_{T\u2192s}, R_{Steering}$ are trained using a composite loss of GAN loss, reconstruction loss and\nsemantic loss. We now describe the hypothesis we formed to train our networks and the experiments\ndesigned to validate or refute them. The discriminators are binary classification networks which\ntry to distinguish the input between source and target domains. The discriminators minimize their\ncross-entropy loss $L_{discrim}$ while generators try to maximize the discriminator loss."}, {"title": "Phase 1: Training the regressor", "content": "We used a convolutional neural network to train the regressor. There were multiple experimentations\nboth on the total number of layers used as well as the normalization of the image."}, {"title": "Phase 2: Training domain translation network", "content": "We use Adversarial learning approach to train $G_{S\u2192T}, G_{T\u2192s}$. We introduce two discriminator\nnetworks $D_{S\u2192T}$ and $D_{T\u2192s}$ which will be trained adversarially with respect to $G_{S\u2192T}, G_{T\u2192S}$.\nIn all our experiments described below, we normalized the data using mean and\nstandard deviation of 128.0, 47.0 and 70.0, 44.6 for source and target data respectively. The mean\nand std deviation values were calculated over all the channels of training data. This decision was\ninspired by works of [Xiang and Li, 2017], [Radford et al., 2015], etc."}, {"title": "Phase 3: Training the regressor", "content": "The final regressor is trained using all components of the network. We want to generalize the source\nregressor trained in first phase, over both source and target domains. Note once again, that we don't\nuse labels from target data. We transform source to target distribution and feed generated target\nimage to the network so that it learns to output correct steering angles for target images. Similarly,\nwe reconstruct the source image from generated target image to preserve semantic labels of the data.\nOverall, we train the $G_{S\u2192T}, G_{T\u2192s}$ and the regressor R in the final stage of training. We use the\ncomposite loss function of adversarial losses, reconstruction losses and the MSELoss. The results are\ndiscussed in Results section."}, {"title": "Results", "content": ""}, {"title": "Results for phase 1", "content": "The loss values on the training data started from 0.75 and after 5000 iterations with a batch-size of 32\nended up being 0.02. Also on the validation set a similar trend was observed where the validation\nloss went from 1.2 radians average to 0.04."}, {"title": "Results for phase 2", "content": ""}, {"title": "Results for phase 3", "content": "In Figure 15 is the plot of validation loss of a target video of 1000 frames. The video sequence was\npassed through the regressor at every tenth iteration of training. We plotted the loss of regressor\non validation set as the regressor progressed through Phase 3 training. The loss decreases almost\nconsistently from 0.687 to 0.082. While this loss is not as low as the training loss on source images\nwhich was 0.02, we certainly feel that this method has potential to perform better given more time"}, {"title": "Discussion and Conclusions", "content": "We refer to the final graph of Experiments section. Note that a loss 0.09 means that the steering\nangle could be as wrong as 0.09 radians on an average. To put this to perspective, 0.09 radians is\napproximately 4.5 degrees which could prove fatal if errors compound. Hence, our system cannot be"}]}