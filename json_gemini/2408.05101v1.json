{"title": "MooER: LLM-based Speech Recognition and Translation Models from Moore Threads", "authors": ["Zhenlin Liang", "Junhao Xu", "Yi Liu", "Yichao Hu", "Jian Li", "Yajun Zheng", "Meng Cai", "Hua Wang"], "abstract": "In this paper, we present MooER, a LLM-based large-scale automatic speech recognition (ASR) / automatic speech translation (AST) model of Moore Threads. A 5000h pseudo labeled dataset containing open source and self collected speech data is used for training. We achieve performance comparable to other open source models trained with up to hundreds of thousands of hours of labeled speech data. Meanwhile, experiments conducted on Covost2 Zh2en testset [1] suggest that our model outperforms other open source Speech LLMs. A BLEU score of 25.2 can be obtained. The main contributions of this paper are summarized as follows. First, this paper presents a training strategy for encoders and LLMs on speech related tasks (including ASR and AST) using a small size of pseudo labeled data without any extra manual annotation and selection. Second, we release our ASR and AST models and plan to open-source our training code and strategy in the near future. Moreover, a model trained on 8wh scale training data is planned to be released later on.", "sections": [{"title": "1 Motivation", "content": "In May 2024, OpenAI released GPT-40, which supports end-to-end speech inputs and outputs, pioneering the end-to-end speech interation technology based on LLMs. From then on, speech, the most natural human-machine interaction modality, has entered a new age of 'the GPT Moment'. To this end, researchers have been continuously exploring large scale speech models. However, there are still uncertainties in the model structure (such as the selection of LLMs, the selection of speech encoders, and the connection relationship between speech encoders and LLMs), the training methods (such as which stages of training are divided, which parameters are adjusted separately, the required data size, computing resources, and costs), and so on. Especially in the open source community, existing work related to speech large models can be mainly divided into two categories: the first one uses open source training datasets to validate their performance on academic benchmarks, such as Salmonn [2]; The second type uses a large amount of data and training resources to train models for multiple speech-related tasks, such as whisper [3], SeamlessM4T [4], Qwen-audio [5], SenseVoice [6], SpeechLlama [7], etc. At present, there are few large-scale speech models that can realize industrial-scale applications for specific vertical classes under the constraints of resources.\nOur released work applies the speech large model technology to the tasks of speech ASR and AST in the following two aspects. Firstly, in terms of model structure and training, we used the open-source Qwen2-7B-instruct model [8] and open-source Paraformer [9] model encoder for model initialization. During the training procedure, only the speech adapter and LLM Lora [10]"}, {"title": "2 Method", "content": "MooER has been greatly inspired by the following amazing works and teams: SLAM-LLM [12], we thank all the contributors for open-sourcing. We concentrate more on the tasks of ASR and AST, and have put forward some corespondibg optimization methods for the model structure, as well as the training strategy on large-scale industrial data. For example, DeepSpeed, Dataloader acceleration, gradient checkpoint, gradient accumulation, BF16 training, are combined during the fine-tuning procedure under 5000 hours of training data. The proposed model consists of encoder, adapter, and decoder (LLM) as in figure1. Encoder implements feature extraction and embedding on audio, adapter performs down-sampling of audio modality and fusion of text modality. LLM performs corresponding tasks based on the input audio and text prompts, such as ASR, AST, etc."}, {"title": "3 Dataset", "content": "We constructed the MT5K training dataset containing a total of 5000 hours speech data from the following source in table 1:\nThe data in the open-source dataset is randomly selected from the entire dataset. In house data is collected internally, and its ASR pseudo labels are obtained through the recording file recognition interface of a third-party cloud service. We will call the third-party translation interface with the corresponding ASR pseudo label to obtain the corresponding AST pseudo label. We did not use any data filtering methods to select crawled data and pseudo labels, which may potentially reduce manual processes in industrial production environments. We also present the performance of our ASR model trained on 80000 hours of internal data on the test set."}, {"title": "4 Experiments", "content": "Our model structure is as follows. We use a Paraformer large encoder as our audio encoder. Paraformer has only 158M parameters, which is lighter and has no padding to 30s limit (whisper), making both training and inference more efficient. The output of the audio encoder will be further down sampled by the adapter, thereby reducing the density of fusion with text embedding. The adapter will downsample the audio by 2 and pass it through two linear layers to obtain the final audio prompt embedding. Our audio feature frames are shifted by 10ms, and the front-end uses LFR for downsampling with a downsampling rate of 6. So, the modeling granularity of audio is 120ms per embedding. The audio embedding will be directly spliced before the text prompt embedding and sent to LLM for training. We use Qwen2-7B instruction as our LLM Decoder.\nThe final input form of LLM is:\n<Speech Embedding><|im_start|>system\\n You are a helpful assistant.\n<|im_end|>\\n<\\im_start\\>user\\nTranslate speech to english text.<|im_end|>\\n<|im_start|>assistant\\n\\u4f60\\u53eb\\u4ec0\\u4e48\\u540d\\u5b57? \\nWhat's your name?<\\im_end\\>\nDuring the training process, we will freeze the parameters of the encoder and train the adapter and LLM (lora). Our experiment found that if LLM is involved in the training process, its semantic understanding ability can be utilized to improve the final audio understanding effect. The configuration for training Lora is as in Appendix A. Ultimately, 2% of LLM parameters will be involved in training. Our model has the following parameter scale as in table 2.\nTo enhance training speed and reduce GPU memory usage, we train using the Zero2 optimization within the DeepSpeed framework. We have observed that the Paraformer tends to experience upward"}, {"title": "5 Discussion", "content": "We selected different encoders on our in house validation test set. We found that if Semi-Supervised Learning (SSL) encoder is used, the encoder needs to participate in training, otherwise the loss will be difficult to converge. Considering the performance, parameter size, and efficiency, we ultimately chose Paraformer as our encoder. In this experiment, we fixed LLM and only the adapter participated in training. (In addition to W2v-Bert2.0 being an encoder, the encoder also participates in training).\nWe attempted to model the granularity of 240ms, 180ms, and 120ms, and found that this parameter is crucial for the fusion of audio and text. In the end, we chose to output an audio embedding every 120ms. In this experiment, we fixed LLM and only the adapter participated in training as shown in table 6.\nWe train based on a Paraformer large encoder. We used approximately 140 ~ 150 hours of English data and achieved better results on the English test set as in table 7. At the same time, we attempted to migrate to other tasks, such as AST, and achieved a BLEU score of 25.2 on the Covost2 Zh2en translation test set. We believe that such an approach can also be applied to other low-resource audio understanding task domains, such as minority languages, dialects, etc as shown in table 8."}, {"title": "5.4 Fully utilize the capabilities of large models", "content": "We found that incorporating LLM into audio understanding training can lead to faster and more stable convergence and ultimately achieve better results. Moreover, the final effect will increase as the LLM effect improves as in table 9."}, {"title": "5.5 Acceleration method", "content": "We optimized the dataloader section, which can increase training speed by 4-5 times under the same configuration. At the same time, we optimized Deepspeed's training strategy based on 5000h of training and reused it in our 8wh internal data training. For training that requires unfrozening the encoder, we use gradient checkpoint to reduce the memory usage. We use the KUAE platform based on Moore Threads for accelerated training of large models."}, {"title": "6 Demo", "content": "Our demo is built on the domestically produced S4000 GPUs by Moore Threads. https://mooer-speech.mthreads.com:10077/"}, {"title": "7 Models", "content": "Github: https://github.com/MooreThreads/MooER\nModelScope: https://modelscope.cn/models/MooreThreadsSpeech/MooER-MTL-5K\nHuggingface: https://huggingface.co/mtspeech/MooER-MTL-5K"}]}