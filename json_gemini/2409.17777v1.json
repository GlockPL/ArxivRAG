{"title": "Harnessing Shared Relations via Multimodal Mixup Contrastive Learning for Multimodal Classification", "authors": ["Raja Kumar", "Raghav Singhal", "Pranamya Kulkarni", "Deval Mehta", "Kshitij Jadhav"], "abstract": "Deep multimodal learning has shown remarkable success by leveraging contrastive learning to capture explicit one-to-one relations across modalities. However, real-world data often exhibits shared relations beyond simple pairwise associations. We propose M3CoL, a Multimodal Mixup Contrastive Learning approach to capture nuanced shared relations inherent in multimodal data. Our key contribution is a Mixup-based contrastive loss that learns robust representations by aligning mixed samples from one modality with their corresponding samples from other modalities thereby capturing shared relations between them. For multimodal classification tasks, we introduce a framework that integrates a fusion module with unimodal prediction modules for auxiliary supervision during training, complemented by our proposed Mixup-based contrastive loss. Through extensive experiments on diverse datasets (N24News, ROSMAP, BRCA, and Food-101), we demonstrate that M3CoL effectively captures shared multimodal relations and generalizes across domains. It outperforms state-of-the-art methods on N24News, ROSMAP, and BRCA, while achieving comparable performance on Food-101. Our work highlights the significance of learning shared relations for robust multimodal learning, opening up promising avenues for future research.", "sections": [{"title": "Introduction", "content": "The way we perceive the world is shaped by various modalities, such as language, vision, audio, and more. In the era of abundant and accessible multimodal data, it is increasingly crucial to equip artificial intelligence with multimodal capabilities [1]. At the heart of advancements in multimodal learning is contrastive learning, which maximizes similarity for positive pairs and minimizes it for negative pairs, making it practical for multimodal representation learning. CLIP [2] is a prominent example that employs contrastive learning to understand the direct link between paired modalities and seamlessly maps images and text into a shared space for cross-modal understanding, which can be later utilized for tasks such as retrieval and classification. However, traditional contrastive learning methods often overlook shared relationships between samples across different modalities, which can result in the learning of representations that are not fully optimized for capturing the underlying connections between diverse data modalities. These methods primarily focus on distinguishing between positive and negative pairs of samples, typically treating each instance as an independent entity. They tend to disregard the rich, shared relational information that could exist between samples within and across modalities. This limited focus can prevent the model from leveraging valuable contextual information, such as semantic similarities or complementary patterns, which can enhance"}, {"title": "Methodology", "content": "Pipeline Overview: Figure 2 depicts our framework, which comprises of three components: uni-modal prediction modules, a fusion module, and a Mixup-based contrastive loss. We obtain latent representations (using learnable modality specific encoders $f^{(1)}$ and $f^{(2)}$) of individual modalities and fuse them (denoted by concatenation symbol '+') to generate a joint multimodal representation, which is optimized using a supervised objective (through classifier 3). The unimodal prediction modules provide additional supervision during training (via classifier 1 and 2). These strategies enable deeper integration of modalities and allow the models to compensate for the weaknesses of one modality with the strengths of another. The Mixup-based contrastive loss (denoted by $\\mathcal{L}_{M3Co}$) continuously updates the representations by capturing shared relations inherent in the multimodal data. This comprehensive approach enhances the understanding of multimodal data, improving accuracy and model robustness.\nMultimodal Mixup Contrastive Learning: Given a batch of N multimodal samples, let $x_i^{(1)}$ and $x_i^{(2)}$ denote the i-th samples for the first and second modalities, respectively. The modality encoders, $f^{(1)}$ and $f^{(2)}$, generate the corresponding embeddings $p_i^{(1)}$ and $p_i^{(2)}$:"}, {"title": "Experiments", "content": "Datasets. We evaluate our approach on four diverse publicly available multimodal classification datasets: N24News [25], Food-101 [26], ROSMAP [27], and BRCA [27]. N24News and Food-101 are both bimodal image-text classification datasets. Food-101 is a food classification dataset, where each sample is linked with a recipe description gathered from web pages and an associated image. N24News is a news classification dataset consisting of four text types (Abstract, Caption, Heading, and Body) along with the corresponding images. Following other works [28], we use the"}, {"title": "Results", "content": "Comparison with Baselines\nThe results are reported as the average and standard deviation over three runs on Food-101/N24News, and five runs on ROSMAP/BRCA. The best score is highlighted in bold, while the second-best score is underlined. The classification accuracy on N24News and Food-101 are displayed in Table 1 and 3 respectively. In the result tables, ALI denotes alignment (indicating if the method employs a contrastive component), while AGG specifies whether aggregation is early (combining unimodal feature) or late fusion (combining unimodal decisions).\nThe experimental results from Table 1, 2, 3, reveal the following findings: (i) M3CoL consistently outperforms all SOTA methods across all text sources on N24News when using the same encoders, beats SOTA on all evaluation metrics on ROSMAP and BRCA, and also achieves competitive results on Food-101; (ii) contrastive-based methods with any form of alignment demonstrate superior performance compared to other multimodal methods; (iii) our proposed M3CoL method, which employs a contrastive-based approach with shared alignment, improves over the traditional contrastive-based models and the latest SOTA multimodal methods. We visualize the unimodal and combined\nEffect of Vanilla Mixup. Mixup involves two main components: the random convex combination of raw inputs and the corresponding convex combination of one-hot label encodings. To assess the performance of our M3CoL method in comparison to this Mixup strategy, we conduct experiments on Food-101 and N24News (text source: abstract). We remove the contrastive loss from our framework (Eq. 16) while keeping the rest of the modules unchanged. Table 4 shows that the Mixup technique underperforms relative to our proposed M3CoL approach (Testing accuracy curves shown in Figure 6a). The observed accuracy gap can be attributed to excessive noise introduced by label mixing,\nEffect of Loss & Unimodality Supervision. To assess the necessity of each component in the framework, we investigate several design choices: (i) the framework's performance without the supervision of unimodal modules during training, and (ii) the performance differences between using only MultiSClip and only M3Co loss during end-to-end training. The M3CoL (No Unimodal Supervision) result indicates that excluding the unimodal prediction module results in a decline in performance as shown in Table 4 and Figure 6a, highlighting its importance as it allows the model to compensate for the weaknesses of one modality with the strengths of another. Additionally, the M3Co loss (only M3Co) outperforms the MultiSClip loss (only MultiSClip) by learning more robust representations through Mixup-based techniques, which prevent trivial discrimination of positive\nTesting on Random Data and Single-Corrupt Modal-ities. To showcase the benefits of our framework over traditional contrastive methods, we evaluate the impact of incorporating Mixup-based contrastive loss (M3Co) during training, highlighting its improvements over standard approaches. It is well-established that deep networks tend to exhibit overconfidence, particularly when making pre-dictions on random or adversarial inputs [50]. Previous research has demonstrated that Mixup can mitigate this context [12]. We evaluate the confidence scores produced using M3CoL (0.33 M3Co + 0.67 Mul- tiSClip) loss in comparison to only MultiSClip loss when predicting on random noise images and\nError Analysis. To evaluate the efficacy of our multimodal approach in integrating and leveraging image and text features, we performed a comprehensive error analysis, comparing it with image-only (ViT) and text-only (RoBERTa) models using the N24News dataset (see Table 9 in Appendix A.5). The analysis reveals that our method excels when both modalities are correctly classified (42.71:0.03 correct-to-incorrect ratio). This demonstrates that our model can learn valuable insights from the\nUMAP Plots\nWe generate UMAP plots on embeddings derived from the N24News and Food-101 datasets to visualize the clustering performance of our M3CoL model. For each dataset, we randomly select"}, {"title": "Related Work", "content": "Contrastive Learning. Contrastive learning has driven significant progress in unimodal and multi-modal representation learning by distinguishing between similar (positive) and dissimilar (negative) pairs. In multimodal contexts, cross-modal contrastive techniques align representations from different modalities [2, 51, 52], with approaches like CrossCLR [53] and GMC [54] focusing on global and"}, {"title": "Discussion and Limitations", "content": "Discussion and Conlusions. Aligning representations across modalities presents significant chal-lenges due to the complex, often non-bijective relationships in real-world multimodal data [3]. These relationships can involve many-to-many mappings or even lack clear associations, as exemplified by linguistic ambiguities and synonymy in vision-language tasks. We propose M3Co, a novel contrastive-based alignment method that captures shared relations beyond explicit pairwise associa-tions by aligning mixed samples from one modality with corresponding samples from others. Our approach incorporates Mixup-based contrastive learning, introducing controlled noise that mirrors the inherent variability in multimodal data, thus enhancing robustness and generalizability. The M3Co loss, combined with an architecture leveraging unimodal and fusion modules, enables continuous updating of representations necessary for accurate predictions and deeper integration of modalities.\nLimitations and Future Work. M3CoL demonstrates promising results, yet faces optimization challenges due to the inherent limitations of multimodal frameworks, particularly extended training times on large-scale datasets like Food-101. The method's modality-agnostic nature and effective use of mixup augmentation suggest its potential adaptability to various multimodal tasks, especially where data augmentation and learning real-world inter-modal relationships are crucial. Future work should focus on investigating domain adaptation strategies, validating M3CoL's utility on"}, {"title": "Appendix", "content": "Experimental Details\nThe models were trained on either an NVIDIA RTX A6000 or an NVIDIA A100-SXM4-80GB GPU. The results are reported as the average and standard deviation over three runs on Food-101 and N24News, and five runs on ROSMAP and BRCA. We use a grid search on the validation set to search for optimal hyperparameters. The temperature parameter for the M3Co and MultiSClip losses is set to 0.1. The corresponding loss coefficient $\\beta$ is 0.1 to keep the loss value in the same range as the other losses. We use the Adam optimizer [68] for all datasets. For Food-101 and N24News, the learning rate scheduler is ReduceLROnPlateau with validation accuracy as the monitored metric, lr factor of 0.2, and lr patience of 2. For ROSMAP and BRCA, we use the StepLR scheduler with a step size of 250. For Food-101 and N24News, the maximum token length of the text input for the BERT/ROBERTa encoders is 512. Other hyperparameter details are provided in Table 6.\nDataset Information and Splits\nThe datasets used in our experiments can be downloaded from the following sources: Food-101 from https://visiir.isir.upmc.fr, N24News from https://github.com/billywzh717/ N24News, and BRCA and ROSMAP from https://github.com/txWang/MOGONET.\nAnalysis under Different Model Variations\nIn addition to the ACC scores presented in Table 4, we also report the performance of other metrics,\nAblation Studies on the N24News Dataset\nError Analysis\nWe provide an in-depth error analysis on the N24News dataset in Section 4.2. As shown in Table 9,\nUMAP Plots\nWe generate UMAP plots on embeddings derived from the N24News and Food-101 datasets to\nAdditional Visualization Attention Heatmaps\nFollowing Section 4.2, we generate heatmaps using class embeddings and patch embeddings for\nBaseline Details\nThe baselines used in our comparsions are described in details as follows:\nUse of Generative AI Models"}]}