{"title": "Empirical evidence of Large Language Model's influence on human spoken communication", "authors": ["Hiromu Yakura", "Ezequiel Lopez-Lopez", "Levin Brinkmann", "Ignacio Serna", "Prateek Gupta", "Iyad Rahwan"], "abstract": "Artificial Intelligence (AI) agents now interact with billions of humans in natural language, thanks to advances in Large Language Models (LLMs) like ChatGPT. This raises the question of whether AI has the potential to shape a fundamental aspect of human culture: the way we speak. Recent analyses revealed that scientific publications already exhibit evidence of AI-specific language. But this evidence is inconclusive, since scientists may simply be using AI to copy-edit their writing. To explore whether AI has influenced human spoken communication, we transcribed and analyzed about 280,000 English-language videos of presentations, talks, and speeches from more than 20,000 YouTube channels of academic institutions. We find a significant shift in the trend of word usage specific to words distinctively associated with ChatGPT following its release. These findings provide the first empirical evidence that humans increasingly imitate LLMs in their spoken language. Our results raise societal and policy-relevant concerns about the potential of AI to unintentionally reduce linguistic diversity, or to be deliberately misused for mass manipulation. They also highlight the need for further investigation into the feedback loops between machine behavior and human culture.", "sections": [{"title": "1 Introduction", "content": "Language, as a social phenomenon, is constantly evolving through iterative processes of perception, internalization, and reproduction [1]. Individuals unconsciously adjust their language to align with their social environment, aiding in maintaining mutual understanding, reducing social distance, and assimilating into social groups [2-4]. These linguistic changes contribute to cultural evolution\u2014a process long influenced by media and emerging technologies, which shape the transmission of spoken and written language and, consequently, how we speak [5].\nOur digital environment now incorporates a new actor: Large Language Models (LLMs). LLM-based applications, such as ChatGPT, have become a part of our daily activities, particularly in the academic context [6], where they are widely used for writing (e.g., polishing, summarizing, rephrasing, and proof-reading content), bridging the gap between initial drafts and polished formal communication. Recent studies on preprint abstracts [7-10] have shown a marked increase in the usage of specific words like delve, underscore, realm, and intricate, suggesting a shift in linguistic preferences directly attributed to the use of ChatGPT. This phenomenon raises the intriguing possibility that our spoken communication might also be evolving under the influence of these models, either through direct use for text editing or by indirect exposure to LLM-generated content. If confirmed, this would highlight the role of LLMs not only as tools for producing text but also as sources of influence from which humans learn and adapt their language."}, {"title": "2 Results", "content": ""}, {"title": "2.1 Changing trend in word frequencies", "content": "Fig. 1 shows the prevalence of videos containing words that previous studies have identified as distinctive of ChatGPT-edited texts [8, 10]. We modeled the temporal evolution of this frequency, $y_w$, as the following continuous piecewise linear regression:\n$\\log (y_w) = \\alpha + \\beta_{w,natural} \\times t + \\beta_{w, GPT} \\times t_{post} + \\epsilon$\n$t_{post} = \\begin{cases}\nt - T_{event} & \\text{if } t > T_{event} \\\\\n0 & \\text{otherwise}\n\\end{cases} \\quad \\text{for } t \\in [T_{start}, T_{end}] $           (1)\nThereby, the coefficient $\\beta_{w, GPT}$ captures the change in trend observed at $T_{event}$ (marked by a red line; in Fig. 1A, corresponding to the release of ChatGPT in November 2022). We set $T_{end}$ at 18 months after $T_{event}$, reflecting the extent of the available data after ChatGPT's release (as of May 2024), and $T_{start}$ at 36 months prior. The posterior medians (95% highest density interval in brackets) for the trend changes ($\\beta_{w,GPT}$) following $T_{event}$ = 2022-11-30 are 0.11 [0.04, 0.18] for 'delve', 0.09 [0.03, 0.14] for 'realm', 0.10 [0.02, 0.17] for 'meticulous', and 0.12 [0.02, 0.22] for 'adept', all indicating accelerated adoption post-ChatGPT. This suggest an increase of 48%, 35%, 40% and 51%, respectively, over the 18 months observed since ChatGPT's introduction.\nIn Fig. 1B and C, we report models with alternative change points set one and two years prior to ChatGPT's release, demonstrating no comparable trend change, thereby indicating that such an increase is not commonly observed."}, {"title": "2.2 Relationship between ChatGPT's word preferences and human adoption", "content": "To extrapolate the previous analysis to a wider spectrum of words used in the collected videos, we examined the magnitude of trend changes associated with words peculiar to ChatGPT's output (GPT words). To this end, we use the dataset provided by Liang et al. [8], which comprises 10,000 human-written abstracts of preprint papers and their corresponding ChatGPT-edited versions. We assessed the distinctiveness of each word in ChatGPT-edited abstracts by calculating the relative log frequency of its usage by ChatGPT compared to its usage by humans, $\\log(\\frac{P_{w,GPT}}{P_{w,human}})$. Based on this metric, we identified the top $N$ words as GPT words and calculated the mean of their $\\beta_{w,GPT}$ across varying $N$, as shown in Fig. 2. The results indicate a significant association of the words most distinctive to ChatGPT and an accelerated adoption of these words by humans. As $N$ increases, the trend changes following ChatGPT's release become less pronounced, suggesting that ChatGPT's influence is not uniform across all words. In contrast, when selecting the bottom $N$ words as control words for comparison, no substantial trend changes with a large magnitude of $\\beta_{w,GPT}$ were observed, suggesting the specificity of this effect to GPT words.\nFig. 3A presents a scatter plot of the 20 words identified as most distinctive to ChatGPT-edited texts. The plot reveals a correlation between the distinctiveness of these words to ChatGPT and a greater acceleration in their usage in spoken communication over time. Consistent with Fig. 2, Fig. 3B indicates that this relationship is most pronounced when considering the top 20 words, suggesting a specific effect on those words most characteristic of ChatGPT. In addition, Fig. 3C shows that the trend change became significant only after several months had passed since the release of ChatGPT."}, {"title": "3 Discussion", "content": "Our findings show that the widespread use of LLMs like ChatGPT is influencing human linguistic patterns, with humans increasingly adopting the language favored by these models. While intensive research focuses on machines' alignment with human behavior, our study suggests that the reverse may also be occurring. This mirrors previous findings where humans adopt strategies in games like chess and Go from machines [14], indicating that machines may now be taking on the role of cultural models in an increasing number of domains.\nOur study is focused on academic communication, yet we anticipate that similar patterns may extend to other communicative contexts. The mechanisms driving the accelerated adoption of certain words remain an open question for future research. A qualitative analysis of videos featuring the word 'delve' suggests its usage occurs both in spontaneous conversations and during the reading of prepared manuscripts (details in Appendix D). While we observe a strong correlation between the specificity of certain words to ChatGPT and their accelerated usage, this trend is not universal. For example, words like 'groundbreaking' and 'underscore' are common in written abstracts edited by ChatGPT but did not significantly accelerate in usage in spoken language. This variability indicates that multiple factors may influence the adoption process, warranting further investigation.\nThis study illustrates how AI systems can, within a relatively short period of time, influence key aspects of human culture, such as language. Researchers have examined the challenges posed by training future LLMs on the outputs of current models, with particular concern over the potential decline in cultural diversity [15]. If the effects observed in this work continue, especially with the growing use of LLMs in education, our findings challenge the notion that humans can continue to provide unbiased, novel data to counteract this decline. Instead, our study suggests that humans themselves may face a reduction in linguistic diversity, emphasizing the complex, bidirectional relationship in which humans and machines influence each other within a shared cultural environment."}, {"title": "4 Methods", "content": "We constructed a dataset of YouTube videos released by academic institutions and obtained 280,000 transcripts in a systematic manner. We modeled the frequency trend of individual words and analyzed its changes following the release of ChatGPT as well as the correlation between the trend changes and the prominence of each word in ChatGPT-generated texts. See Appendix A to D for more details."}, {"title": "A Dataset construction", "content": "We first cataloged 20,622 research institutes from the Research Organization Registry [16] as of May 13, 2024. Institutes not identified as active educational entities were omitted to minimize the inclusion of non-educational content, such as videos from corporate or inactive channels. Subsequently, we queried the YouTube API\u00b9 with each institute name and its country name to list relevant channels and provided the results to gpt-3.5-turbo-0125 as input to pick the most plausible channel using the prompt presented in Fig. 4. The channels identified were then used to compile a list of 2,958,103 videos through a subsequent YouTube API query. From these, we retained 1,613,839 videos, recognized as having an English title by a language detection library [17], for further analysis, excluding those less likely to have a conversation in English. Additionally, 1,248,913 videos under 20 minutes were removed, as they tend to feature content other than lecture or presentation videos, such as promotional materials. Furthermore, we excluded videos exceeding the duration of the 99th percentile (3h07\u203216\u2033) among the remaining 364,916 videos, noting that excessively long videos often diverge from academic discourse, exemplified by a 5-hour graduation ceremony broadcast. This exclusion was strategic to avoid both data noise and unnecessary GPU usage for transcription. Also, we encountered several errors while downloading (e.g., videos deleted during the dataset construction), resulting in a dataset of 360,445 videos (Fig. 5).\nThe transcription of the collected data was performed using the large-v3 model of whisper [18]. Here, we opted to run the transcription process ourselves rather than using the pre-existing transcription data from YouTube due to the possibility that YouTube has switched transcription models across different videos. Specifically, we found an unnatural increase in the frequency of the filler word \"um\" starting around May 2020, which we found difficult to attribute to an actual increase in speakers' usage of the word. It is more plausible that YouTube switched to a transcription model that transcribes fillers verbatim, and thus, we conducted the transcription process to avoid a potential source of bias. We then applied the language detection library [17] again to the transcriptions to filter out those from videos that were judged as non-English by whisper, resulting in the transcript data from 279,480 videos. The final dataset of word occurrence was obtained by applying the Porter stemmer preprocessing [19] to the transcripts.\nGiven the above, we acknowledge that our dataset is not all-encompassing. Here, the data collection procedure was designed to construct a dataset that can capture the influence of large language models on spoken language within the academic context in a systematic manner. While recognizing the potential presence"}, {"title": "B Linear regression model analysis", "content": "We employed a hierarchical Bayesian Gaussian regression to model the frequency transition of videos containing a specific word in their transcriptions. The model is represented in Eqn. 2. In this model, $\\log(y_w)$ represents the log frequency of videos containing the word in a specific time period $t$ (sampled on a monthly basis). We used log frequency with Laplace smoothing [20] because word frequency is empirically known to follow a long-tail, exponential (Zipfian) distribution [21]. Here, $\\alpha$ is the intercept, $\\beta_{w,natural}$ is the coefficient for the natural transition across all time periods from $T_{start}$ to $T_{end}$, and $\\beta_{w, GPT}$ is the coefficient for the transition observed between $T_{event}$ and $T_{end}$. Note that $t$ is normalized such that $\\beta_{w, GPT} = 1$ indicates the corresponding word increased its frequency by 10 times over a year after $T_{event}$. Also, while we set $T_{end}$ to be 18 months after $T_{event}$, $T_{start}$ was designated to be 36 months prior due to the relatively low number of videos uploaded in the earlier years. For example, the number of videos uploaded in 2015 in our dataset is less than half of those uploaded in 2023. The error term $\\epsilon$ is normally distributed with a mean of zero and variance $\\sigma$. We used a half-Cauchy prior for $\\sigma$ while using normal distribution priors for all other parameters. Sampling was conducted across four chains using STAN's no-U-turn sampler, with 1,000 samples per chain."}, {"title": "C Relationship between ChatGPT's word preferences and human adoption", "content": ""}, {"title": "C.1 Selection of GPT words", "content": "We selected a set of GPT words based on the dataset by Liang et al. [8], which consists of 10,000 human-written abstracts from preprint papers (denoted as Dhuman) and their ChatGPT-edited versions (DGPT). For each word presented in both Dhuman and DGPT, we calculated the occurrence frequencies Pw,human and Pw,GPT, respectively. Here, our goal was to identify GPT words that are prominent in texts generated by ChatGPT but less common in human-written texts. To achieve this, we ranked the words based on the difference\n$\\log (\\frac{P_{w,GPT}}{P_{w, human}})$, selecting the top $N$ words as GPT word. In contrast, we selected the bottom $N$ words with\n$\\log (\\frac{P_{w,GPT}}{P_{w, human}})$ as control words for Fig. 2."}, {"title": "C.2 Sensitivity analysis of the correlation", "content": "For the correlation analysis, we also performed a sensitivity analysis by varying $N$, the number of words included into GPT words and control words, and $T_{event}$, the change point for fitting $\\beta_{w, GPT}$. Note that, when we set $T_{event}$ one year prior to the release of ChatGPT, $\\beta_{w,GPT}$ is still influenced by the trend change following ChatGPT's release because we set $T_{end}$ at 18 months subsequent to $T_{event}$, meaning that $\\beta_{w, GPT}$ captures data from 6 months post-release. Conversely, if $T_{end}$ is before the release of ChatGPT, then $\\beta_{w,GPT}$ (and the correlation coefficient $r$) are unaffected by data following the release. To clarify this point, we have employed $T_{end}$ for the X-axis of Fig. 3C."}, {"title": "D Manual analysis of videos containing 'delve'", "content": "We conducted a manual review of a random sample of 50 YouTube videos, focusing on instances where the word \"delve\" was mentioned. Our review aimed to identify various setups and presentation styles in which these mentions of \"delve\" occurred. The analysis revealed that 32% of the videos involved potential signs of reading, indicating that the speaker may have been following a script or prepared notes. Conversely, 58% of the videos showed no signs of reading, suggesting a more spontaneous or conversational style. About 30% of the videos featured multiple speakers, often in a Q&A format or dialogue. Given the noticeable increase in the use of \"delve\" as discussed in the main text, this finding suggests that the adoption of certain linguistic patterns potentially influenced by tools like ChatGPT extends beyond scripted speech."}]}