{"title": "An Adversarial Approach to Register Extreme Resolution Tissue Cleared 3D Brain Images", "authors": ["Abdullah Nazib", "Clinton Fookes", "Dimitri Perrin"], "abstract": "We developed a generative patch based 3D image registration model that can register very high resolution images obtained from a biochemical process name tissue clearing. Tissue clearing process removes lipids and fats from the tissue and make the tissue transparent. When cleared tissues are imaged with Light-sheet fluorescent microscopy, the resulting images give a clear window to the cellular activities and dynamics inside the tissue. Thus the images obtained are very rich with cellular information and hence their resolution is extremely high (eg.2560 x 2160 x 676). Analyzing images with such high resolution is a difficult task for any image analysis pipeline.Image registration is a common step in image analysis pipeline when comparison between images are required. Traditional image registration methods fail to register images with such extant. In this paper we addressed this very high resolution image registration issue by proposing a patch-based generative network named InvGAN. Our proposed network can register very high resolution tissue cleared images. The tissue cleared dataset used in this paper are obtained from a tissue clearing protocol named CUBIC. We compared our method both with traditional and deep-learning based registration methods. Two different versions of CUBIC dataset are used, representing two different resolutions 25% and 100% respectively. Experiments on two different resolutions clearly show the impact of resolution on the registration quality. At 25% resolution, our method achieves comparable registration accuracy with very short time (7 minutes approximately). At 100% resolution, most of the traditional registration methods fail except Elastix registration tool. Elastix takes 28 hours to register where proposed InvGAN takes only 10 minutes.", "sections": [{"title": "1. Introduction", "content": "In medical applications, image registration is a fundamental step to check correspondence between images. Voxel-to-voxel or pixel-to-pixel correspondence is required to analyze tissue differences, changes of tissues, organs or tumors over time, etc. Image registration is traditionally addressed as an optimization problem. However, iteratively optimizing alignment parameters by an objective function requires lot of computation specially when deformable registration is required. The number of parameters and the associated computational cost increase with the resolution and dimensions of the images. This makes such approaches impractical for high-resolution images.\nRecently, a biochemical process called tissue clearing has emerged. It is used to remove light-obstructing elements from soft tissues and enable biologists to take 3D single-cell resolution images without sectioning. A number of tissue clearing methods have been developed, including BABB (Dodt et al., 2007), Scale (Hama et al., 2011), SeeDB (Ke et al., 2013), CLARITY (Chung and Deisseroth, 2013), iDISCO (Renier et al., 2014) and CUBIC (Susaki et al., 2014). Irrespective of the clearing protocol, light-sheet fluorescence microscopy (LSFM) is then used to produce images of entire organs with a resolution of a few micrometres. For instance, the images from the CUBIC dataset used in this paper are about three orders of magnitude larger than typical MRI images ($6.45 \\times 6.45 \\times 10\\mu m^3$ as opposed to ($0.86 \\times 0.86 \\times 1.5mm^3$).\nWhile this level of detail is crucial from a biologi-cal point of view, it makes it difficult to use conventional registration methods for these images, due to their very long computation time and consumption of large compute resources. Recently, deep-learning (DL) based methods has achieved remarkable success in image registration (Balakrishnan et al., 2018; Eppenhof and Pluim, 2019; Cao et al., 2018). Very high computational efficiency and accuracy makes these methods a favorable choice over the iterative methods. However, training a DL-based registration method requires large amount of data. In our case, having large amount of tissue cleared data (similar to Balakrishnan et al. (2018)) is practically impossible. Another limitation is that deep-learning based methods have to estimate a large num-ber of deformable parameters, and this number of registra-tion parameters increases with the resolution. This makes such approaches very demanding, for both memory and computating power.\nBased on our observation, we set a list of criteria for a DL-based registration method suited to the tissue-clearing context:\n\u2022 The learning framework has to be trainable with small amount of data.\n\u2022 It has to be able to take high-resolution images with limited computational resources."}, {"title": "1.1. Related Work", "content": "Deep neural networks have recently been successfully used in medical image registration, based on supervised (Yang et al., 2016; Roh\u00e9 et al., 2017; Sokooti et al., 2017) or unsupervised methods (Li and Fan, 2018; Balakrishnan et al., 2018). The development of these deep learning based registration methods open the door to apply adversarial training for medical image registration. Yan et al. (2018) appear to be the first to propose an adversarial method for medical image registration. In that paper, two similar convolution networks are used as generator and discriminator. The last layer of both networks is a fully connected layer. In the generator network, the last fully-connected layer regress the transformation parameters while the last fully connected layer in discriminator provides the similarity score. The generator network takes magnetic resonance (MR) and transrectal ultrasound (TRUS) images as input and estimates transformation parameters which are used by an image resampler to resample the moving image. The discriminator takes two pairs of images to discriminate the quality of the registration. For the discriminator training, a well-aligned image pair is used as ground-truth alignment and the discriminator determines the quality of the alignment/transformation parameters estimated by the generator. In this work, Wasserstein loss (Arjovsky et al., 2017) is used to train the discriminator network while the generator network is trained with adversarial loss along with $l_1$ loss between estimated flow and a randomly generated flow. The GAN framework is trained and tested on a MR/TRUS dataset with 763 pairs of images. The performance is measured using Dice score and no baseline method is used to compare the performances. Though, the performance of the GAN framework proposed in this paper is not compared to other tools, and it is therefore difficult to asses it, the method demonstrates the applicability of adversarial training for image registration. Again, the dependence on an already registered image pair for the discriminator training is a major limitation of this method.\nMahapatra et al. (2018) proposes another GAN-based image registration framework. In this GAN framework, the training strategy of cycleGAN (Zhu et al., 2017) is used, with cyclic loss. The generator network takes the reference and moving images as input and generates a transformed image and transformation parameters. The discriminator differentiates between the transformed image and reference image along with the difference between reference flow and estimated flow by generator network. There is no use of Spatial Transformer Network (STN) like the previous method, since the generator network directly generates the moving image. During the training, the method uses normalized mutual information loss (NMI), structured similarity index (SSIM) loss, and VGG loss to train the generator network. The NMI loss maintains the intensity distribution of the transformed image similar to moving image while SSIM loss maintains the structural similarity between the transformed image and reference image. To make the deformation field consistent and to maintain its reversibility the cyclic loss similar to cycleGAN (Zhu et al., 2017) is used. Training this network begins with the training of RESNET. The weights from the trained RESNET are used to initialize the generator network. A retinal fundus dataset and a cardiac dataset are used to evaluate performance and compared with two base line methods, DIRNET (de Vos et al., 2017) and Elastix (Klein et al., 2010). For both datasets, it is found that the GAN based model without cyclic loss achieved the best performance. However, the method still has limitations. Firstly, the dependence on VGG layers is a weakness and authors did"}, {"title": "2. Materials and Method", "content": "The architecture of our proposed network is shown in Figure 1. The network consists of an encoder like Voxel-Morph (VM) U-net (Ronneberger et al., 2015), and two de-coders. The input to the network is a concatenation of source and target images. The encoder branch has four layers, each of which is a convolution layer of 3\u00d73\u00d73 kernel with stride 2 and 32 output channel. Among the two decoder branches, one branch is responsible for forward flow computation and the other one is for backward flow computation. The forward-decoder has one simple convolution layer, three forward computation blocks (red blocks in Figure 1) with 32, 32 and 8 channels, and two additional convolution layers with 8 and 3 channels. Each forward computation block contains an upsampling layer, an addition layer and concate-nation layer. The addition layers adds the same sized features from the encoder branch and then concatenates them. The architecture of the backward-decoder is exactly the same as the forward-decoder with the exception of subtraction layers in the backward computation blocks (blue blocks in Figure 1).\nEach discriminator has six 3D convolution layers with stride 2 and LeakyRelu activation. The last layer of convolu-tion has 1 \u00d71 \u00d71 convolution filters. Each convolution layer reduces the resolution of the input image patch by half. The last layer of the discriminators produces a single unit output which represents the similarity between input images."}, {"title": "2.2. Discriminator Loss", "content": "In our GAN framework, the two discriminators are termed as $D_s$ and $D_T$ respectively. During training $D_T$ takes the target image as the real image and the transformed source image (with forward transformation) produced by generator network for comparison. The forward branch of the generator network produces a forward transformation which is used to warp the source image patch. The target discriminator $D_T$ compares the difference between the orig-inal target image and the transformed source image using binary cross entropy loss. $D_T$ tries to establish whether the image is from generator or it is real. The objective function for the target discriminator network is given by:\n$L_{D_{T}} = \\frac{1}{N} \\sum_{i=1}^N L_{D_{T}}(P_{r}, 1) + \\frac{1}{N} \\sum_{i=1}^N L_{D_{T}}(P_{s^\\phi}, 0)$  (1)\nwhere $L_{D_{T}}$ is the binary-cross entropy loss. To optimize the generator network to learn a more accurate forward flow, the adversarial loss from the discriminator is used. For the generator, the forward adversarial loss is:\n$L_{INVadv_{D_{T}}} = \\frac{1}{N} \\sum L_{D_{T}}(P_{S^\\phi}, 1)$  (2)\nSimilar to the target discriminator $D_T$, the source discriminator $D_S$ is also trained with binary cross entropy loss. The objective of the source discriminator is to differentiate the difference between the original source image and the transformed target image, which is in the source image space. Therefore, the loss for the source discriminator is:\n$L_{D_{S}} = \\frac{1}{N} \\sum L_{D_{S}}(P_{S}, 1) + \\frac{1}{N} \\sum L_{D_{S}}(P_{T^\\phi}, 0)$ (3)\nAgain, the generator needs to be optimized to generate better a backward flow with the adversarial gain learned from the source discriminator. The adversarial loss in this case is:\n$L_{INVadv_{D_{S}}} = \\frac{1}{N} \\sum L_{D_{S}}(P_{T^\\phi}, 1)$ (4)"}, {"title": "2.3. Generator Loss", "content": "The generator network is optimized with the two ad-versarial losses. The adversarial losses help the generator network learn the distribution of the flow vectors from the input images. At the beginning of the adversarial training, the generator sometimes fails to win over the discriminator, and ongoing failing of the generator network makes the training unstable. To avoid unstable training, the generator needs an independent loss function that will slowly drive the generator to the Nash equilibrium of the minmax game. To drive the generator, we use cross-correlation loss as image similarity metric and cycle loss to ensure reversibility. The overall loss function is then given by:\n$L_{Loss} = L_{similarity} + L_{cycle} + \\lambda(L_{INVadv_{D_{S}}} + L_{INVadv_{D_{T}})$ (5)\nwhere\n$L_{similarity} = -CC(S \\circ \\phi_{ST}, T) - CC(T \\circ \\phi_{TS}, S)$ (6)\nand\n$L_{cycle} = ||((T \\circ \\phi_{TS}) \\circ \\phi_{ST})-T||_1 + ||((S \\circ \\phi_{ST}) \\circ \\phi_{TS})-S||_1$ (7)"}, {"title": "2.4. Data Preprocessing", "content": "In this paper, we use data from Susaki et al. (2014, 2015). Whole brains from Arc-dVenus mice were sampled, cleared using the CUBIC protocol, and imaged using LSFM. The mouse brains were imaged from two different direction, Dorsal-to-Ventral (D-V) and Ventral-to-Dorsal (V-D). Here, we use two different resolutions for this data, 25% and 100%. For the 25% resolution data, we use the CUBIC informatics protocol (Susaki et al., 2015), in which images are down-sampled to 25% resolution and then D-V and V-D sides are merged. 20 merged CUBIC brains at 25% resolution are used to train the network and 3 merged brains are used"}, {"title": "2.5. Training Patch Selection", "content": "Since training with such high-resolution 3D volumes is difficult due to resource constraints, we employ a patch-based training. The patch selection itself is a critical step. The performance of the network depends on the regions from which the patches are selected. Previously, we used mean intensity of the patches as the selection criteria. We used 0.2 of the mean patch intensity as the threshold value to decide whether a patch is to be selected or not. We found that using a specific patch intensity introduced a bias, with most patches selected from only a few specific regions. This limits the ability of the network to learn deformation parameters for other regions. To avoid a biased patch selection, we present here a probabilistic patch selection procedure. We use a step function, shown in Eq.8, that gives a weight to a randomly selected patch, based on the mean patch intensity. The function sets the probability 1 if the mean patch intensity is between 0.1 to 0.35, and if it is more than the given range the probability decreases exponentially. The mean intensity range given in Eq.8 is decided empirically.\n$pdf(x) = \\begin{cases} 1 & 0.1 <= x <= 0.35\\\\ 10 e^{K\\mu} & x > 0.2, K = 6.6 \\\\ 0 & otherwise \\end{cases}$ (8)"}, {"title": "2.6. Training", "content": "The proposed network is trained and tested at 25% and 100% resolution. For both cases, there are 20 brains for training and 3 brains for testing. We use N \u00d7 (N \u2212 1) combinations of training pairs to train the network. At 25% resolution, the image dimension is 640 \u00d7 540 \u00d7 169 and the training patch extraction can therefore be done on the fly. For each pair of images, 2500 patches are extracted using the $pdf$ defined in Eq.8. Since the image dimension at 100% resolution is very high (2560 x 2160 \u00d7 676), loading two images is difficult. Patch extraction at this resolution (10,000 patches from each pair of images) is done before the training starts. All training images are intensity normalized as well as affine registered to the selected brain (brain-3 of test dataset at both scale) before the training process is applied.\nThe network is developed using tensorflow. At 25% resolution, the network is trained and tested in a High-Performance Computing environment with 64 GB RAM, 12 GB Video RAM in Tesla K40m GPU and a single core 2.66GHz 64bit Intel Xeon processor. The same configuration is used to train the network at 100% resolution. The data pre-processing and training patch extraction at this resolution is done in a Big-data machine with 4TB memory."}, {"title": "2.7. Competing Methods", "content": "Five state-of-the art image registration tools have been used in this investigation. Each of these registration tools are tuned and optimized for best performance. The registration tools selected in our evaluation are based on previous studies like (Klein et al., 2010) and (Xu et al., 2016). We select tools which are automated, easy to use, developed for volume registration and showed consistent performance in previous studies. We exclude tools like FreeSurfer. The FreeSurfer tool is primarily developed for surface registration not for volume. Moreover, the FreeSurfer has a brain labeling al-gorithm attached with its own labelled atlas which is not suitable for our CUBIC data (Klein et al., 2010). A brief discussion on each of these tools is as follows:\n1) IRTK: One of the early image registration tools by (Rueckert et al., 1999) for breast MRI images, using voxelized mutual information similarity and a free-form deformation model. Before starting registration, IRTK ap-plies contrast enhancement to make the similarity measure insensitive to intensity change. A hierarchical transformation model is applied to capture global and local motion of the volume data where global motion captured by affine model and local motion is by a non-linear, free-form deformation model. Voxel-based Normalized mutual information is used as the similarity measure. In our comparison, we followed the same settings as (Xu et al., 2016) except for the B-spline control points. Since the pixel spacing of CUBIC dataset is very small, the control point spacing is set to 5mm, which is the highest possible value for this method. The IRTK codes are available in https://github.com/BioMedIA/IRTK.\n2) Elastix: One of the popular registration tool de-veloped by (Klein et al., 2010), for CT and MRI images with large set of common registration algorithms. This tool consists of many algorithms for similarity, optimization, reg-ularization, interpolation, transformation etc. For the sim-ilarity measure, Elastix includes mutual information (MI), Normalized mutual information (NMI), Cross-correlation (CC), mean squared difference (MSD) etc. The transforma-tion models included in the Elastix library are rigid; affine with different degree of freedom; B-spline with physics based control points in uniform and non-uniform grids; a set of optimization methods namely gradient descent, quasi-Newton, nonlinear conjugate gradient (with several vari-ants); and a number of stochastic gradient descent methods. All these options add flexibility to choose required compo-nents whenever necessary. We consider the elastix parameter settings used in (Hammelrath et al., 2016) for rigid, affine and nonlinear registration. The Elastix codes are available in https://github.com/SuperElastix/elastix.\n3) ANTS: Advanced Normalization is developed by (Avants et al., 2008), (Avants et al., 2011). ANTS use sym-metric diffeomorphic normalization method for non-linear transformation. In ANTS, cross-correlation is maximized in a symmetric diffeomorphic map and uses Eular-Lagrange equations for optimization. The diffeomorphic map pre-serves the topology map along with invertible transforma-tion parameters and gives sub-pixel accuracy. The parameter settings for the ANTS tool is derived from ANTS example script. In evaluation by (Xu et al., 2016), two different setups of ANTS tool were used with two different similarity metric (Cross-Correlation and Mutual Information), which they considered as two separate methods. In our settings, only cross-correlation is used as a similarity measure; the number of resolution levels is three, with 100 iterations in each sampling level. The ANTS codes are available in https://github.com/ANTsX.\n4) NiftyReg: A promising registration tool developed by (Modat et al., 2010). It's an extended version of IRTK, based on free form deformation. In this method, the gra-dient of normalized mutual information of each B-spline control point is calculated and used in gradient descent-based optimization method. The algorithm was implemented with parallel processing, but in our evaluation only a CPU version is used. The parameter setting of this tool for CUBIC evaluation is exactly the same as settings mentioned by (Xu et al., 2016). The number of iterations is 1000 for free-form deformation and 500 intensity threshold for both source and target image. The NiftyReg binary files are downloaded from https://github.com/KCL-BMEIS/niftyreg/wiki.\n5) VoxelMorph: This is the first deep-learning based registration method (Balakrishnan et al., 2018). The Vox-elMorph integrates a fully convolutional U-net architecture with a Spatial transformer and train them simultaneously. Unlike deep-learning based approaches developed before, this architecture directly takes fixed and moving images in 3D form instead of taking image patches. Using cross-correlation as objective function to estimate dissimilarity between fixed and instantaneously moved moving image, the network is trained in unsupervised manner. A diffusion regularizer is used to prevent training over-fitting of the network.\nFor our experiment, ANTS, Elastix and IRTK are built from the source code given in their respective code reposi-tory while binary files of NiftyReg is downloaded."}, {"title": "3. Results", "content": "The performance of the proposed method is evaluated on two different resolution scales, 25% and 100% respectively. We compared the quantitative performance of the registra-tion methods by normalized cross-correlation and mutual information for test brains. We have only three CUBIC brains to test and for the convenience we note them as brain-1, brain-2 and brain-3. The performance scores showed in tables are measured by making brain-3 as target and other two brains as moving image. For qualitative evaluation, we show the same brain slice extracted from all test brains and overlaid on the target brain with different color map. In all of our experiments, the target brain is mapped with red color and registered brains are mapped with green color. The dissimilarity in the cerebellum (posterior part) region is common due to high variability of that part from brain to brain. The visual registration quality is assessed by con-sidering regions like hippocampal formation, dentate gyrus to be similar and aligned. Again, to facilitate the qualitative evaluation process at 25%, cropped and zoomed patches from hippocampal formation and dentate gyrus regions are also presented."}, {"title": "3.1. Results at 25% Resolution", "content": "In Table 1, proposed InvGAN achieves the highest scores in both test brains in terms of cross-correlation score with scores 0.8877 and 0.9627. In mutual-information, our In-VGAN still remains one second best with tiny difference with ANTS tool. Elaxtix tool achieves third best results in both metrics. VoxelMorph, the only deep learning based competitor performs better than conventional NiftyReg and IRTK. The IRTK tool become the least performing tool in this experiment.\nTo further explore the capability of of InvGAN architec-ture, a non-generative version is trained and tested without discriminator training. The results of the model without adversarial training is also presented in the last row of Table 1. The quantitative results of non-adversarial training of is extremely high in our context. To verify the registration quality of non-adversarial training, registered images are checked. In Figure 2 a comparison between training with and without adversarial loss is presented. It is found that without adversarial loss the image quality drastically affected by the box artifacts from the patch-based training strategy.The qualitative comparison of the proposed method and other baseline methods are shown in Figure 3 and 4. We select the brain-1 registered by each each registration method and overlay on the target brain (brain-3). In Figures 3 and 4, proposed InvGAN and other methods are compared side-by-side. The alignment in the registered images are visible in their color difference. When the registered image (green channel) perfectly aligns with reference image (red chan-nel), perfect alignments are represented by yellow channel. Regions where alignments are not perfect, red and green channels are visible separately. To further extend the visuali-sation in more detail, we extract patches from three selected regions (Dentate Gyrus, left and right Hippocampal) from both registered image and reference image and calculate the difference between the patches. The difference image contains intensity values in the range +1 to -1 (since all brain volumes are intensity normalized from 0 to 1). For accurate alignment the intensity difference should be 0 in the differ-ence image. For proper visualization of the difference image, we transform the difference image intensities into the range 0 to 255 using a linear triangular function centered at zero. The resulting transformed images thus contain intensity values 255 (0 in difference image) or white, in accurately aligned regions and 0 or dark in non-aligned regions.\nIn Figures 3 and 4, proposed InvGAN is compared with ANTS, Elastix,NiftyReg, IRTK and VoxelMorph. Between ANTS and InvGAN, it is difficult to differentiate which one is better based on overlay-ed image. But clear differences are found in the transformed difference images from three selected regions. In three regions, the number of dark spots in InvGAN registered brain is much smaller than the ANTS registered brain. This clearly indicates that InvGAN has better registration accuracy than ANTS. In comparison to Elastix, the registration difference is clearly visible from the ovarlayed images. In Elastix registration, the red and green channels are not aligned perfectly and hence they are separate while in InvGAN registration they are aligned much accurately.The difference images further verifies the superior registration accuracy of InvGAN over Elastix. Sim-ilar pattern observed in case of NiftyReg and IRTK where alignment mismatches are clearly visible from the overlay images and further confirmed by local patches. The deep learning-based VoxelMorph on the other hand shows very similar pattern with ANTS. From the overlay image, regis-tration performance of VoxelMorph is difficult to evaluate but the patches from three local regions clearly indicates that InvGAN beats VoxelMorph with large margin."}, {"title": "3.2. Results at 100% Resolution", "content": "The quantitative performances of InvGAN architecture and Elastix tool at 100% resolution of the CUBIC data are presented in Table 2 and the qualitative results are shown in Figure 5. Since no other conventional tools are able to register images to such extent, the results presented here only contain performance achieved by proposed InvGAN architecture and Elastix.In Table 2, the CC and MI scores are presented in different iterations of InvGAN training at 100% resolution. The score before the registration is also provided to compare the registration performance. It takes a long time to improve the registration quality by InvGAN. At 10,000 iterations, the CC and MI scores improved slightly and started to degrade afterwards. For the CC score on brain-1, the elastix achieves the best scores with 0.8412 and 0.8676. For MI score, InvGAN is always higher than the elastix in brain-1. For brain-2, the CC score of InvGAN is smaller than Elastix while the MI score is slightly better. At 20,000 iterations, the performance becomes almost similar to the before-registration state for both brain. Slowly but gradually the accuracy continues to improve.\nAt 88,000 iteration, InvGAN again improves. For brain-1, the CC score is reaches to 0.8076 and for brain-2 its reaches to 0.6278. In terms of mutual information, it achieves 0.9234 and 0.7801 respectively. At 100% resolution, we trained and tested our method with only V-D side of CUBIC brains and the quantitative score represents that fact.\nThe qualitative performances at 100% resolution are pre-sented in Figure 5 with different iterations. At each iteration, the color mapped overlay image shows the alignment quality and the gray-scale images present the image quality after registration. At 10,000 iterations, the alignment between brain-1 and brain-3 is not as expected but the image quality (shown in gray image) is good and has no box artifacts. In left and right hippocampal regions, the red and green channels of both brains are visible, which means the alignment in this region is not perfect. In the dentate-gyrus region, the differences between brains are still visible. At 60,000 itera-tions, with careful inspection it is found that the alignment in the hippocampal region improves slightly and it also improved in the dentate-gyrus region. At this iteration, the box artifacts start appearing. At iteration 80,000 and 88,000, the improvement of visual alignment between the brains is difficult to identify, while the image quality improves noticeably with no patch artifacts.\nAt 100% resolution, the affine registration by ANTS tool takes around eight hours to align one pair. ANTS fails to apply deformable registration at 100% resolution. The Elastix takes 27.5 hours of time to register one pair of 100% resolution image. In contrary, proposed InvGan is extremely fast. It takes 6 mins to generate warp patches in the HPC environment and combining those patches in the big-data machine takes 2/3 mins per image, which takes in total 9 to 10 mins to register an image with 100% resolution.\nTo illustrate how our method is performing at 100% in comparison to Elastix, Figure 6 is added. In Figure 6 the qualitative performance of two methods are compared side-by-side. The clear difference between reference image and Elastix registered image is visible in dentate-gyrus,left and right hippocampal regions. In InvGAN registered image, there are clear difference in left hippocampal but in right hippocampal the red and green channels are almost aligned. Since we trained our model with affine aligned V-D side only, the misalignment on the other part of the brain is expected. In Dentate Gyrus, the Elastix completely fails to align while our method aligns two brain more perfectly. The high quantitative score of Elastix is due to the fact that it aligns the Cerebellum region, therefore, the lower part of the brains more accurately than the dentate-gyrus and both hippocampal regions. The alignment in this region does not provide any reasonable conclusion since this region extensively varies from brain to brain.\nThe qualitative and quantitative results at 100% reso-lution indicates that the proposed method is applicable to very high resolution images. Considering the fact that only V-D side is used for training, its consistent performance in dentete gyrus and hippocampal regions compared to Elastix clearly shows its potential for very high resolution image registration."}, {"title": "3.3. Landmark Validation", "content": "To validate the registration performance of the proposed methods and comparing the baseline methods in a more objective manner, a landmark registration test is conducted.In the CUBIC dataset, three brains are used to test the registration performance. In the landmark test, the same dataset is used for the performance validation. 12 landmarks are selected and all of these landmarks are selected where their positions vary in all three axis. 3D slicer tool is used to select the landmarks for this experiment from the CUBIC brains. A set of selected landmarks are shown in Figure 7. Table 3 shows the results of the 3D landmark registration by proposed InvGAN method and other baseline methods. The Euclidean distance between the registered landmarks and reference landmarks are presented in mm. For optimisation-based ANTS and Elastix tools, the same parameter sets are used to register landmarks selected from moving image and fixed image. After the registration, the output point locations are compared with reference point locations in the fixed image. For the deep-learning-based VoxelMorph and InvGAN, the deformation values in X,Y and Z are extracted from the same voxel location of the selected landmark's voxel location. After applying deformation to the landmarks, the new position is compared with that of corresponding reference points in the fixed image.\nIn Table 3, distances between fixed image landmarks and registered landmarks are presented in columns. In this test, NiftyReg and IRTK both are excluded due to the lack of tech-nical documentation provided for landmark registration. In Table 3 brainwise scores performed by each registration tool and the average of all 12 landmarks as well as their standard deviation are presented. The smaller the scores, the better the registration performance is. For brain-1 proposed InvGAN achieves least average distance with 0.1547mm. The Vox-elMorph achieves second best score with 0.3362mm while ANTS become third in position with 0.3685mm. Elastix be-come the lest performer with 4.4555mm. In brain-2, ANTS achieves the best result with 0.1938mm while proposed InvGAN achieves 0.2926mm. The VoxelMorph improves its performance with 0.2693mm. The Elastix tool remains again the least performer in brain-2 and its performance in brain-2 is even worse than brain-1 with more than 6mm average distance."}, {"title": "3.4. Computation Time", "content": "The side-by-side comparisons of computation time at 25% and 100% resolutions are presented in Table4. The com-putation time plays as a significant indicator of performance, efficiency and applicability of the comparing methods on tissue cleared data. In Table4, the registration time at 25% of all the traditional tools takes significantly longer time for both affine and deformable registration than the deep-learning based tools. Proposed InGAN and its deep-learning based counterpart VoxelMorph take approximately 1 min for deformable registration. The VoxelMorph takes only 55s which is slightly better than proposed InvGAN but with the cost of registration accuracy (see Tablel). The performance difference more significantly evident when the resolution in-creases. Most of the traditional tools fails to register at 100% resolution. The ANTS only performs affine registration and fails for the deformable.For affine only, it takes around 8 hours of time for a single pair registration. Elastix, on the other hand performs both affine and deformable registration with the exponential increase in registration time. Registra-tion time for both deep learning based methods are signif-icantly smaller than their traditional counterparts at 100%. A similar pattern in registration time at 100% resolution is shown by both deep-learning based methods. Like 25%, at 100% the proposed InvGAN takes slightly longer time (approximately 10 mins) than VoxelMorph (approximately 7 mins). By considering the performance measures in Table 2 and landmark results in Table 3 it is evident that the fast registration time of VoxelMorph is achieved with cost of registration accuracy which is not the case for proposed InvGAN network. Considering high registration accuracy of InvGAN, its slight longer registration time is acceptable for analysis pipeline like CUBIC."}, {"title": "4. Discussion", "content": "In this paper we proposed a patch-based deep learn-ing method for registration of very high resolution tissue cleared images. The proposed method is unsupervised and produced high registration accuracy on the tissue cleared dataset. Experimentation on two different resolution and on the anatomical landmarks indicates its ability as a general purpose registration method for large images in a resource constrained environment"}]}