{"title": "MultiPruner: Balanced Structure Removal in Foundation Models", "authors": ["J. Pablo Mu\u00f1oz", "Jinjie Yuan", "Nilesh Jain"], "abstract": "Recently, state-of-the-art approaches for pruning large pre-trained models (LPMs) have demonstrated that the training-free removal of non-critical residual blocks in Transformers is viable for reducing model size, achieving results that outperform previous training-free pruning approaches. Motivated by these findings, we extend BlockPruner (Zhong et al., 2024) and propose MultiPruner, a pruning approach that surpasses recent training-free pruning methods by adopting a multidimensional, iterative, fine-grained pruning strategy. In MultiPruner, multidimensional pruning reinstates the structural balance in block-pruned models by sequentially compressing along three dimensions: i) residual blocks, ii) channels of multilayer perceptrons (MLP), and iii) attention heads. This solution enhances zero-shot accuracy on downstream tasks compared to other techniques while improving model compression ratios, producing compressed models with fewer computing and memory requirements. Extensive experiments demonstrate the advantages of the proposed method across various large pre-trained models. The code and pruning configurations are available at https://github.com/IntelLabs/Hardware-Aware-Automated-Machine-Learning.", "sections": [{"title": "Introduction", "content": "Large pre-trained models (LPMs), also known as foundation models (FMs) (Bommasani et al., 2021), such as GPT-4 (Achiam et al., 2024), are producing outstanding results across a variety of domains, and have motivated an increase in investment in artificial intelligence (AI)-related ventures. State-of-the-art AI models often have billions of parameters and require large clusters of graphics processing units (GPUs) to train. Pre-trained models are usually subject to a less resource-intensive subsequent stage in which the model is adapted or fine-tuned for a downstream task. Beyond the challenges of training and fine-tuning, deploying these models requires complex systems with significant computing and memory capacity. Two delimited stages are in play during inference: prefill and decode. In the prefill stage, all the required caches are created, which tends to be compute-bound. In the decode stage, the model uses the existing caches to generate new tokens, which tends to be memory-bound.\nGiven the substantial resource requirements for training, fine-tuning, and deploying these models, model compression techniques have become increasingly important. Pruning and quantizing LPMs have been proposed to reduce resource requirements, improve model performance, and enable deployment to more limited environments. However, choosing a particular compression algorithm requires considering the large parameter space of these models and the cost in time and resources that this process will take. For instance, when utilizing pruning, the algorithm should be capable of efficiently analyzing which model components might be removed, guaranteeing a sustained performance, e.g., with a minor tolerable drop in accuracy.\nRecently, ShortGPT (Men et al., 2024) demonstrated that removing Transformer blocks based on their relative importance is a viable approach for pruning large pre-trained models. BlockPruner (Zhong et al., 2024) took this idea further to demonstrate that Transformer blocks can be partitioned into their two sub-components based on the residual connections, i.e., minimal residual blocks and showed that this more fine-grained pruning approach results in models with higher a pruning ratio and accuracy. However, these works assume that structured pruning should be applied to the Transformer's depth dimension, which can easily lead to over-pruning in a single dimension, removing necessary layers or blocks. This paper considers these challenges and extends BlockPruner (Zhong et al., 2024), resulting in a method to advance the state-of-the-art in structured pruning of large pre-trained models. MultiPruner removes this assumption and demonstrate that block pruning can be complemented with additional pruning in other dimensions, maintaining the target efficiency. Our approach, MultiPruner, is a training-free approach that extends the benefits of state-of-the-art training-free block pruning approaches and produces smaller models with higher accuracy. Specifically, MultiPruner operates in three pruning stages. First, it prunes the least important residual blocks, leveraging the insights from BlockPruner (Zhong et al., 2024) to identify minimal residual blocks that can be removed without significantly impacting performance. Next, it applies a fine-grained pruning strategy to the MLP channels, followed by the attention heads, aiming to prune the model more accurately. This sequential pruning process ensures that each dimension is optimally compressed, resulting in a smaller and more efficient model.\nIn summary, MultiPruner advances training-free model compression and includes the following contributions:\n1. A pruning algorithm, MultiPruner, that extends BlockPruner (Zhong et al., 2024) and provides an iterative fine-grained pruning strategy across multiple dimensions, leading to enhanced zero-shot accuracy on downstream tasks and improved model compression rates.\n2. Studies to explore the challenges in multidimensional pruning, such as the pruning order and ratios for different types of structures.\n3. Extensive experiments demonstrating the effectiveness of MultiPruner across various pruning ratios and models.\nThe following sections provide a comprehensive overview of the benefits and applications of MultiPruner, organized as follows: Section 2 discusses the proposed method followed by Section 3 with experimental results. Related work is discussed in Section 4. We conclude with our thoughts on the impact of our research."}, {"title": "Methodology", "content": "MultiPruner is motivated by recent block pruning algorithms, e.g., Zhong et al. (2024); Men et al. (2024), that remove elements on a single dimension. Based on their importance, these methods prune Transformer blocks or their sub-components, like self-attention or multilayer perceptron (MLP) blocks. However, focusing solely on these coarse residual blocks might leave pruning opportunities unrealized in other dimensions. MultiPruner adopts a fine-grained approach to structural pruning while exploring the structural balance of the given architecture, since by removing complete residual blocks in Transformers, we are altering the original network design and the choices that might have been determined after expensive exploration and experimentation by the model's creators. For instance, a pruned network becomes shallower after applying block pruning, but its overall width remains unchanged. By pruning in other orthogonal dimensions, MultiPruner attempts to reinstate this balance, resulting in compressed high-performing models that closely follow the original design considerations. The benefits of MultiPruner are demonstrated by experiments results in Section 3."}, {"title": "Multidimensional Pruning with Fixed Thresholds per Pruning Target", "content": "Input: Set of minimal residual blocks M from a model m, s.t. $M = \\{M_i | M_i\\in M, \\text{type} (M_i) \\in \\{\\text{MLP}, \\text{ATTN}\\}\\}$, Calibration dataset C, Metric $\\phi$, Target pruning ratios $T_1$, $T_2$, and $T_3$ for each pruning modality, MLP channel group size $g_{MLP}$, ATTN channel group size $g_{ATTN}$.\nOutput: Pruned model $m^*$\n1: $t \\gets 0$\n2: while $T < T_1$ do\n3: for all $M_i \\in M$ do\n4: $S_i \\gets \\text{BlockImportance} (M_i, m, C, \\phi)$\n5: end for\n6: $M_{\\min} \\gets \\arg \\min_{M_i \\in M} S_i$\n7: $M \\gets M \\setminus \\{M_{\\min}\\}$\n8: $T \\gets \\text{PruningRatio}(m)$\n9: end while\n10: WeightReordering(m)\n11: for all $t \\in \\{\\text{MLP}, \\text{ATTN}\\}$, $T_t \\in \\{T_2, T_3\\}$, $g_t \\in \\{g_{MLP}, g_{ATTN}\\}$ do\n12: $M_t = \\{M_i | M_i \\in M, \\text{type}(M) == t\\}$\n13: while $T < T_t$ do\n14: for all $M_i \\in M_t$ do\n15: $S_i \\gets \\text{WidthImportance} (M_i[:, :-g_t], m, C, \\phi)$\n16: end for\n17: $M_{\\min} = \\arg \\min_{M_i \\in M_t} S_i$\n18: $M_{\\min} = M_{\\min}[:, :-g_t]$\n19: $T \\gets \\text{PruningRatio}(m)$\n20: end while\n21: end for\n22: return $m^*$ with the remaining and altered blocks in M\nGiven a dense model m, e.g., Llama-2-7B (Touvron et al., 2023), associated with a set of minimum residual blocks M from each Transformer block (self-attention or MLP), and a target pruning ratio \u03c4, MultiPruner takes a finer-grained pruning approach compared to other pruning solutions to obtain a model m* with an associated subset of altered blocks from M. MultiPruner's objective is to find a high-performing pruning configuration that results in a similar pruning ratio to the competing state-of-the-art block pruning while maintaining a structural balance and improving its zero-shot performance on downstream tasks."}, {"title": "Multidimensional Fine-Grained Pruning", "content": "MultiPruner targets three main elements for structure removal:\n\u2022 Residual Transformer Blocks (Depth)\n\u2022 MLP Channels (Width)\n\u2022 Attention Heads (Width)\nIn the depth dimension, MultiPruner removes iteratively the least important minimal residual blocks as in BlockPruner (Zhong et al., 2024). In the width dimension, MultiPruner removes groups of channels from the MLP and attention heads. When exploring various architectural dimensions of the model for pruning and considering that a full search of architecture configurations is not practical for large pre-trained models, even when using training-free approaches and zero-shot evaluation, a natural research question arises:\n(1) Should MultiPruner prune the model's depth and width in parallel or sequentially to obtain a high-performing pruned model?\nThe parallel strategy adds complexity and does not provide insights regarding the contributions of pruning in each dimension. As detailed in Section 3, experimentally, we have observed that following sequential steps to prune different model's dimensions yields the best pruned models. However, if the decision is to prune each dimension sequentially, we are confronted with a second research question:\n(2) What is the recommended order to sequentially explore the removal of structures in each dimension?\nIntuitively, a coarse-to-fine-grained order, i.e., from blocks to MLP channels to attention heads, will result in a more precise pruned model. This intuition is supported by our experimental results in Section 3, which demonstrate that MultiPruner achieves better performance when it first removes structures along the depth of the model. Once this stage is completed, it focuses on structures along the width of selected components. As shown in Figure 1, MultiPruner begins by pruning the least important residual Transformer blocks, reducing the model's depth. Following this, it targets the MLP Channels, and finally, it prunes the attention heads, which raises an additional research question:\n(3) When must MultiPruner stop pruning in each dimension or type of component?\nTo answer this last question, we assign each of the three pruning targets with a pruning ratio threshold, $T_1$, $T_2$, and $T_3$ (i.e., the target ratio \u03c4). To discover values for these thresholds that yield high-performing models, we utilize two search strategies:\n\u2022 Fixed thresholds per pruning target.\n\u2022 Fixed threshold for the depth dimension and evolutionary search to discover Pareto-optimal configurations on the width dimension.\nNext, we discuss these search variants in more detail."}, {"title": "Sequential Pruning with Fixed Targets", "content": "Experimentally, we discover the value for hyperparameters, $T_1$, $T_2$, and $T_3$ that determine the pruning ratio of each type of component: complete residual blocks, MLP channels, and attention heads, respectively. The entire process is detailed in Algorithm 1 and Figure 1.\nThe algorithm begins by pruning the depth dimension, where MultiPruner removes the least important residual Transformer blocks iteratively until the pruning ratio $T_1$ is reached (lines 2-9 in Algorithm 1). The importance metric used for all pruning steps is the perplexity (PPL) on the calibration dataset. This step ensures that the model's depth is reduced in a controlled manner, preserving the most critical blocks based on their importance scores.\nAfter completing the depth pruning, MultiPruner performs a weight reordering step (line 10 in Algorithm 1). This step aims to prioritize the less important channels and heads for pruning. By re-ordering the weights based on an importance metric, the channels and heads are reordered such that the least important ones are positioned last, making them the primary candidates for pruning. In our main experiment, we employed the L1 Norm as the reorder metric, and more details can be found in Appendix A.\nOnce the weight reordering is complete, MultiPruner shifts focus to the width dimension. It sequentially prunes non-essential groups of channels in the MLP and attention heads, adhering to the pruning ratios $T_2$, and $T_3$, respectively (lines 11-21 in Algorithm 1). The algorithm calculates the importance scores for each component type and removes the least important channel group or head iteratively until the target pruning ratio is achieved.\nThis sequential approach ensures that each dimension is pruned effectively, balancing depth and width reduction. By following this method, MultiPruner achieves a high-performing pruned model that closely aligns with the original design considerations while significantly reducing the model's size."}, {"title": "Fixed Threshold for Depth and Evolutionary Search for Width", "content": "After removing residual blocks in the depth pruning stage, we have experimentally observed that stopping at half of the pruning ratio, i.e., \u03c4/2, provides an effective heuristic to initiate the pruning using evolutionary search in the width dimension. This stopping point opens a significant opportunity to remove elements in the width dimension, resulting in a better balance in the model."}, {"title": "Multidimensional Pruning with Fixed Delimitation and Evolutionary Search.", "content": "Input: Set of minimal residual blocks M from a model m, s.t. $M = \\{M_i | M_i\\in M, \\text{type}(M_i) \\in \\{\\text{MLP}, \\text{ATTN}\\}\\}$, Calibration dataset C, Metric $\\phi$, Target pruning ratio \u03c4, Number of evaluations in evolutionary search N, Search Space S.\nOutput: Pruned model $m^*$\n1: $t \\gets 0$\n2: while $t < \\frac{\\tau}{2}$ do\n3: for all $M_i \\in M$ do\n4: $S_i \\gets \\text{BlockImportance} (M_i, m, C, \\phi)$\n5: end for\n6: $M_{\\min} \\gets \\arg \\min_{M_i \\in M} S_i$\n7: $M\\gets M \\setminus \\{M_{\\min}\\}$\n8: $t \\gets \\text{PruningRatio}(m)$\n9: end while\n10: WeightReordering(m)\n11: $\\{(S,M)\\} \\gets \\text{EvolutionarySearch}(M_i, m, C, \\phi, S)$\n12: $M \\gets \\arg \\max_{i} \\{S_i | PruningRatio(M) == \u03c4\\}\\_0$\n13: return $m^*$\nThe algorithm begins by pruning residual blocks"}, {"title": "Experiments", "content": "To demonstrate the broad applicability of MultiPruner, we conducted experiments using the following models: Llama3.2-3B, Llama3.1-8B, Llama3-8B (Dubey et al., 2024), Llama2-7B, Llama2-13B (Touvron et al., 2023), Qwen2.5-7B (Yang et al., 2024a), Qwen1.5-7B, Qwen1.5-14B (Bai et al., 2023), Baichuan2-7B and Baichuan2-13B (Yang et al., 2023). These models share similar architectures, and their Transformer blocks are composed of self-attention (MHA, GQA (Ainslie et al., 2023), etc.) and multilayer perceptrons (MLP).\nBuilding on the analysis by Zhong et al. (2024), we include a comparison of MultiPruner with other recent approaches, i.e. SliceGPT (Ashkboos et al., 2024), LaCo (Yang et al., 2024b), Short-"}, {"title": "Main Results", "content": "The results provide a comprehensive comparison of various pruning methods applied to different large language models, focusing on zero-shot downstream task performance. The metrics considered include perplexity"}, {"title": "Block Pruning in Multiple Dimensions", "content": "Notably, MultiPruner outperforms the block-level pruning method, BlockPruner, by employing a more sophisticated approach, block pruning in multiple dimensions. Specifically, MultiPruner subdivides the pruning units into MLP channels and attention heads, allowing for more precise pruning than the coarse-grained block pruning used by BlockPruner or other layer pruning methods. The experimental results in the tables demonstrate that this finer granularity leads to more effective model pruning."}, {"title": "Exploration of More Pruning Ratios", "content": "As we explore the performance of the Llama2-7B model using BlockPruner and our proposed method MultiPruner across various pruning ratios (22%, 24%, 27%, and 31%). Across all pruning ratios, MultiPruner consistently achieves lower perplexity and higher average scores than BlockPruner. For instance, as the pruning ratio reaches 27%, MultiPruner demonstrates superior performance with a perplexity of 11.59 and an average accuracy score of 60.20. At this pruning ratio, MultiPruner reduces the perplexity of BlockPruner by 2.48 and improves the average score by 4.39%. The consistent outperformance of MultiPruner can be attributed to its finer-grained pruning strategy, which allows for more precise and effective pruning, as evidenced by the superior results across various metrics and pruning ratios."}, {"title": "Comparison with Multidimensional Pruning via Evolutionary Search", "content": "In addition to comparing BlockPruner and MultiPruner, we also compare the zero-shot downstream task performance of MultiPruner and MultiPruner-Evol. Both MultiPruner and MultiPruner-Evol outperform BlockPruner, demonstrating the effectiveness of our multidimensional pruning approach. When comparing MultiPruner to MultiPruner-Evol, we observe that MultiPruner-Evol often achieves slightly higher average scores but slightly higher perplexity than MultiPruner. For instance, at a 24% pruning ratio, MultiPruner-Evol achieves a higher average score (63.15) compared to MultiPruner (61.99) but a slightly higher perplexity (10.61 vs. 10.01). It is essential to note that MultiPruner-Evol incurs higher computational costs and requires more prun-"}, {"title": "Ablation Studies", "content": "we present ablation studies on the Llama2-7B model using MultiPruner with a pruning ratio of 22%. The table examines the impact of pruning dimensions and their pruning order on model performance, considering both perplexity and accuracy scores.\nAs shown in the table, the results demonstrate that the default order of Block, MLP Channel, and Attention Head pruning achieves optimal performance. When the order of the pruning stages is altered, we observe variations in performance. For example, starting with MLP Channel or Attention Head generally leads to higher perplexity and lower average scores, confirming that the default order, which moves from coarse to fine granularity progressively, is more effective for precise pruning.\nThe existence variations further validate the necessity of each pruning stage. Removing any single stage results in degraded performance. For instance, omitting the MLP channel pruning stage increases perplexity to 11.04 and decreases the average score to 61.69. Removing two stages, such as MLP channel and attention head pruning (i.e., BlockPruner), leads to even more significant performance drops, with perplexity rising to 11.51 and the average score falling to 60.17. Relying solely on attention head pruning results in a drastic increase in perplexity to 142.20 and a significant drop in the average score to 40.60, highlighting that most attention heads are essential and cannot be excessively pruned."}, {"title": "Sensitivity Exploration of Self-Attention and MLP", "content": "What happens when increasing/decreasing the pruning of MLP channels or attention heads (the weight of the overall target pruning ratio)? This section explores the sensitivity of MLP channel and attention head pruning. As shown in Figure 4, the four plots illustrate the impact of varying the weight of the target pruning ratio allocated to MLP Channels and Attention Heads on model performance.\nThe first two plots show the effect of changing the MLP channel ratio weight while keeping the overall target pruning ratio constant. As the MLP ratio weight increases from 20% to 80%, the Wikitext2 PPL decreases, but the average score initially rises, reaching a maximum of around 50%, and then declines. This indicates that excessive pruning of MLP channels can negatively impact the model's overall performance. The latter two plots depict varying weights' influence on the attention head ratio. As this ratio weight increases from 0% to 25%, the Wikitext2 PPL consistently rises, particularly when the ratio weight exceeds 5%. Similarly, the average score decreases as the head ratio weight increases after 5%, with a notable drop beyond the 15% mark. This suggests that the attention heads are more sensitive to pruning, and even a slight increase in the pruning ratio can lead to substantial performance loss."}, {"title": "Recovery Tuning of the Pruned Model", "content": "Following most of the work (Ma et al., 2023; Zhong et al., 2024), we also conducted post-training on the pruned model using the cleaned version of Alpaca. The results, shown in Table 5, indicate significant performance improvements after just two epochs of recovery tuning. Specifically, MultiPruner w/tune achieves a reduced perplexity of 8.08 and an increased average score of 64.18. The recovery tuning phase effectively enhances the performance of the pruned model, making it closer to the original dense model while requiring less computational resources."}, {"title": "Inference Speedup", "content": "The inference speedup results for Llama2-7B model, as demonstrate the efficacy of the MultiPruner method in enhancing inference performance on an Intel\u00ae Xeon\u00ae Platinum 8480+ processor. The pruned model, with a 22% reduction in parameters, consistently outperforms the dense model. Specifically, the prefill phase exhibits a speedup of 1.32\u00d7, while the decode phase achieves a speedup of approximately 1.28x. These results underscore the potential of model pruning techniques to significantly reduce inference latency, facilitating more efficient deployment of large language models in real-world applications."}, {"title": "Example of the Pruned Model", "content": "illustrates the pruning results for Llama2-7B obtained using MultiPruner. We observe that the latter part of the model (layers 16 to 31) is pruned more extensively. Notably, the intermediate size of three MLP modules is reduced to 1792, and block pruning is concentrated in the blocks of layers 16 to 31, suggesting that the latter part of the model has more parameter redundancy, while the earlier layers might be more critical. Overall, by selectively reducing channel sizes and removing entire blocks where necessary via MultiPruner, the pruned model achieves a more compact architecture without compromising its ability to perform downstream tasks effectively."}, {"title": "Related Work", "content": "The increasing size of large pre-trained models (LPMs) has motivated the development of cost-effective compression techniques to reduce these models' footprint and enable deployment in a broader range of devices. Many methods have been proposed, e.g., pruning (LeCun et al., 1989), quantization (Gholami et al., 2021), and knowledge distillation (Hinton et al., 2015), that overcome the high cost of previous generations of compression algorithms that had fewer resources and time constraints for their execution. Since the paper focuses on pruning, we discuss the evolution of pruning approaches and the latest developments emphasizing efficiency in the compression of LPMs."}, {"title": "Pruning Large Pre-trained Models", "content": "Pruning is a popular model compression technique that targets removing or masking redundant elements in a neural network. Pruning methods utilize a pruning criteria to accomplish this removal, combined with several strategies to detect the least critical model components efficiently. However, special considerations are required when attempting to prune large pre-trained models. Previous approaches that were successful in pruning small Transformer-based models are not practical for large models, e.g., Movement (Sanh et al., 2020) or Block pruning (Lagunas et al., 2021), because they require expensive weights updates."}, {"title": "Unstructured Pruning", "content": "Unstructured pruning approaches remove or mask individual weights without any pre-determined pattern. Wanda (Sun et al., 2023) prunes weights utilizing an unstructured or semistructured strategy by applying a pruning criterion based on the weight's magnitude and the norm of the input activation. BESA (Xu et al., 2024) employs a reconstruction loss per block to sparsify the model. Once sparsity has been induced in the model using any of the mentioned techniques, parameter-efficient fine-tuning techniques (PEFT), e.g., Hu et al. (2022); Mu\u00f1oz et al. (2024), can be applied to recover the accuracy for a downstream task.\nAlthough unstructured pruning can achieve high levels of sparsity, it faces limitations due to the requirement of complex decompression algorithms."}, {"title": "Structured Pruning", "content": "Structured pruning focuses on removing elements at a higher granularity than unstructured pruning. For instance, in the case of Transformer blocks, these algorithms might remove attention heads or groups of channels in linear layers of the multilayer perceptron (MLP) component. The benefits of structured pruning are more straightforward to realize than its unstructured counterpart since one can extract the pruned model as a smaller version of the original pre-trained model and utilize the same runtime used by the dense model to realize the benefits and acceleration.\nLoRAPrune (Zhang et al., 2024) proposes a structured pruning approach for LPMs that is guided by analyzing the weights and gradients of low-rank adapters (LoRA) (Hu et al., 2022) to determine the importance of components of the Transformer block. Recently, several algorithms have been proposed to perform structural removal in a neural network efficiently but without incurring the cost of updating the weights of LPMs. Hence, most state-of-the-art approaches take the training-free path. LLMPruner (Ma et al., 2023) removes network structures using gradient information and recovers any accuracy drops utilizing parameter-efficient fine-tuning (PEFT) techniques. ShortGPT (Men et al., 2024) exploits block redundancy in Transformer-based models and proposes a Block Influence (BI) metric to decide which blocks to prune. BI is a local metric based on the evolution of hidden stages in each block. BlockPruner (Zhong et al., 2024) improves over ShortGPT by proposing a global metric, e.g., the model's perplexity, that is computed by masking the candidate block and assessing its impact if removed. The candidate that results in the most minor drop in performance is removed from the model, which is iteratively conducted until reaching the target pruning ratio. BlockPruner also increases the pruning granularity by focusing on the multi-head attention (MHA) and multilayer perceptrons (MLP), i.e., the minimal residual blocks."}, {"title": "Conclusion", "content": "Pruning large pre-trained models requires efficient algorithms that consider their immense resource requirements. This paper presents MultiPruner, an efficient training-free structured pruning approach that outperforms other pruning methods. Thanks to their smaller size, the pruned models from MultiPruner accelerate inference and extend the range of devices where machine learning practitioners can deploy these models."}, {"title": "Limitations", "content": "Due to the complexity of foundation models, the search spaces utilized in our experiments attempt to obtain a good balance between efficiency and efficacy. With a larger computing budget, a finer-grained search might lead to even better results and optimal balancing of the pruning dimensions. Although accuracy is a good indicator of the performance of the pruned models compared to the dense and baseline models, it does not capture the intricacies of these large models. For instance, compressed models are more efficient and have accuracy similar to the base foundation model but might behave differently under certain conditions. Our research focuses on improving the efficiency of large models. Still, additional investigations are required from the larger research community to understand better the impact of the different methods for model compression on the quality of the output from these models."}, {"title": "Inference Speedup", "content": "The inference speedup results for Llama2-7B model, as shown demonstrate the efficacy of the MultiPruner method in enhancing inference performance on an Intel\u00ae Xeon\u00ae Platinum 8480+ processor. The pruned model, with a 22% reduction in parameters, consistently outperforms the dense model. Specifically, the prefill phase exhibits a speedup of 1.32\u00d7, while the decode phase achieves a speedup of approximately 1.28x."}]}