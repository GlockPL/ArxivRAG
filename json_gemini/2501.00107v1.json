{"title": "An Unsupervised Anomaly Detection in Electricity Consumption Using Reinforcement Learning and Time Series Forest Based Framework", "authors": ["Jihan Ghanim", "Mariette Awad"], "abstract": "Anomaly detection (AD) plays a crucial role in time series applications, primarily because time series data is employed across real-world scenarios. Detecting anomalies poses significant challenges since anomalies take diverse forms making them hard to pinpoint accurately. Previous research has explored different AD models, making specific assumptions with varying sensitivity toward particular anomaly types. To address this issue, we propose a novel model selection for unsupervised AD using a combination of time series forest (TSF) and reinforcement learning (RL) approaches that dynamically chooses an AD technique. Our approach allows for effective AD without explicitly depending on ground truth labels that are often scarce and expensive to obtain. Results from the real-time series dataset demonstrate that the proposed model selection approach outperforms all other AD models in terms of the F1 score metric. For the synthetic dataset, our proposed model surpasses all other AD models except for KNN, with an impressive F1 score of 0.989. The proposed model selection framework also exceeded the performance of GPT-4 when prompted to act as an anomaly detector on the synthetic dataset. Exploring different reward functions revealed that the original reward function in our proposed AD model selection approach yielded the best overall scores. We evaluated the performance of the six AD models on an additional three datasets, having global, local, and clustered anomalies respectively, showing that each AD model exhibited distinct performance depending on the type of anomalies. This emphasizes the significance of our proposed AD model selection framework, maintaining high performance across all datasets, and showcasing superior performance across different anomaly types.", "sections": [{"title": "1 Introduction", "content": "Anomaly detection (AD) finds widespread use in various applications [1], including smart grids [2], Internet of Things (IoT) [3], medical systems [4], financial fraud detection systems [5], and blockchain networks [6]. With the prevalence of cloud infrastructure and IoT-enabled devices, there is a vast availability of time series data [7]. Consequently, AD for time series data is gaining attraction, particularly in data-driven applications [8]. Anomalies, also called outliers, represent observations that deviate from the rest of the dataset [9] or exhibit deviations from most of the sequence's distributions or patterns [10]. For example, anomalies in power grid meter measurements may indicate potential cyber-attacks or malfunctions [11, 12]. Similarly, anomalies in time series power consumption patterns can lead to errors in power consumption predictions, resulting in underproduction or overproduction of electricity [13]. In finance, outliers in time series can signify \"illegal activities like fraud, risk, network intrusions, account takeover, and money laundering\" [5]. Thus, time series AD plays a crucial role in ensuring operational efficiency, reliability, and security in real-world applications [8].\nHowever, AD often falls short of achieving expected performance in data-driven real-world systems. This is primarily due to the scarcity, cost, and impracticality of obtaining labeled data in large quantities [8]. In real-world applications, a limited quantity of labeled anomalies can be provided through methods like active learning or identification by domain experts [14, 15, 16]. Therefore, it is preferable to employ unsupervised AD techniques that do not rely on labeled data [8].\nFurthermore, given the complexity of data in real-world systems, data abnormalities manifest in various forms [17]. AD techniques that assume specific abnormalities may tend to rely on certain aspects more than others, making them sensitive to specific types of anomalies while potentially overlooking others. Consequently, there is a need for a universal AD model that outperforms other AD models on different types of data [1]. To address this, leveraging multiple AD models at different time steps can be beneficial when applied to a particular time series data. This can be achieved by designing an AD selection model that chooses the optimal AD model from a pool of candidate models at each time step [18]. The predicted label (normal or anomalous) at each time step depends on the output of the currently selected anomaly detector. Reinforcement learning (RL), a machine learning domain that maximizes numerical rewards by mapping situations to actions at each time step, can be employed for this purpose [19].\nIn a RL setting, an agent aims to learn the optimal decision-making policy to maximize the overall reward. RL has demonstrated success in various model selection applications, such as RL model selection for wind speed prediction [20] and RL-based model combination for time series forecasting [21]. This study proposes an RL-based model selection approach for AD (RLAD) in time series datasets. To address the challenge of gathering labeled data which is both expensive and time-consuming [22], we focus on six unsupervised AD techniques that do not require labeled data for training. To implement the RL agent without explicit reliance on ground truth labels in its reward function, we utilize six time series forest (TSF) models. Each TSF is trained to classify a specific AD model's incorrect and correct predictions. Our proposed AD model selection framework utilizes the classifications from these six classifiers for the majority of data points and ground truth labels for the remaining data points [23]. In other words, our approach requires a portion of ground truth labels instead of explicit reliance on all ground truth labels. We compare our RL-based"}, {"title": "2 Related Work", "content": "Due to the emerging spread of time series problems [24], the field of AD has garnered substantial attention for the analysis of time series data [25]. It can be categorized into three main groups: unsupervised, semi-supervised, and supervised methods [26]. While supervised or semi-supervised methods can utilize previously identified abnormalities to label instances as normal or abnormal, the practicality of supervised techniques is limited by the need for a large amount of labeled data containing both normal and abnormal instances, which is often not feasible in real-world settings [27]. Semi-supervised AD techniques typically stem from supervised algorithms by incorporating a bias term to account for unlabeled data [28, 29]. In light of this, unsupervised AD categories, such as distance-based, statistical, ensemble-based, and reconstruction-based approaches, have recently gained favor.\nIn this section, we comprehensively understand these prominent categories of unsupervised AD techniques and their applicability in various scenarios. After exploring the strengths and limitations of these techniques, we discuss the recent advancements in this field, with the goal of tackling the existing AD challenges effectively."}, {"title": "2.1 Distance-based Approaches", "content": "Among the distance-based AD techniques, [30] considers a point (p) as an $(\\pi,\\varepsilon)$-anomaly if at most $(\\pi)\\%$ of the entire data points are away from p by a distance less than $(\\varepsilon)$. This was further improved in [31] by proposing an outlier score which is obtained by calculating the distance to the kth nearest neighbor (KNN) of a data point. After that, the data point is classified as normal or anomalous based on a threshold. [32] suggested a better variant with a KNN-based method where the outlier score represents the aggregate distance of the data point (p) to the k nearest neighbors of p. Another distance-based AD algorithm, the Local Outlier Factor (LOF), considered the ratio of the average of the k nearest neighbors densities to the data point density itself [33]. While LOF and KNN have both been shown to outperform state-of-the-art (SOTA) AD techniques, they yet failed to preserve a high performance with large high-dimensional and seasonal datasets [34]."}, {"title": "2.2 Statistical Approaches", "content": "Numerous seminal probabilistic AD methods compute the anomaly score relying on the estimates of the marginal likelihood $P_{\\theta}(X)$ derived from the data generation model [35, 36, 37]. One of the probabilistic techniques that fits a number of Gaussians on the data is Gaussian Mixture Model (GMM). To estimate the models' parameters, the expectation-maximization algorithm (EM) is utilized [38]. Support vector machine (SVM) has been extensively used for novelty detection tasks [39]. One-class SVM (OSVM) is a support-vector-based approach that assumes a probability density (like GMMs fitting) to learn a hyper-plane that separates the low-density region from the region with a high-density of data [40, 41]. One limitation of generative models lies in the complexity of obtaining marginal likelihoods, which necessitates computing integrals with exceedingly high dimensions.\n$P_{\\theta}(X) = \\int P_{\\theta}(z)P_{\\theta}(X | z)dz$ [8]. More recent non-parametric statistical methods for AD are Empirical Cumulative Distribution for Outlier Detection (ECOD) [42] and Copula-Based Outlier Detection (COPOD) [43]. The ECOD approach involves computing an empirical cumulative distribution to predict tail probabilities for individual data points, which are then used to calculate the outlier score. In contrast, COPOD utilizes an empirical copula to estimate tail probabilities for the entire set of data instances, enabling the computation of the outlier score. Although these novel techniques provide valuable contributions, their scalability is constrained by their dependence on the data's shape [44]."}, {"title": "2.3 Ensemble-based Approaches", "content": "In [45], an ensemble-based AD technique considered Local Outlier Factor (LOF) with multiple hyper-parameter sets and combined the results to obtain outlier scores. Another ensemble-based technique, known as Isolation Forest (IForest), was proposed in [46]. IForest consists of a forest with random binary trees in such a way that outliers are relatively near the tree's root (within a shallow depth of the tree). The underlying principle of IForest is based on the notion that outliers are more prone to isolation."}, {"title": "2.4 Reconstruction-based Approaches", "content": "Several contemporary AD algorithms capitalize on using synthetic data reconstruction. These algorithms are based on the insight that outliers, when projected into a lower-dimensional space, tend to lose information, leading to less efficient reconstruction and an increased reconstruction error for anomalous instances [8]. Principal Component Analysis (PCA) is a popular approach for data reconstruction that permits linear reconstruction only [47]. The non-linear variant of PCA, known as Kernel PCA, applies the kernel trick to map the data into a feature space [48]. Deep learning-based AD algorithms, such as those employing Recurrent Neural Networks (RNN), have garnered significant attention, especially in time series applications [49, 50]. However, these techniques can be computationally expensive and time-consuming [51, 52]. A more recent approach is Unsupervised AD on Multivariate Time Series (USAD) [53]. It relies on an encoder-decoder pair that is trained in an adversarial manner. The reconstruction error in the testing phase represents the anomaly score."}, {"title": "2.5 Large Language Models-based Approaches", "content": "Large Language Models (LLMs) have lately shown remarkable success in the AD task. LLMs are capable of processing and interpreting huge amounts of data having temporal dependencies which provide more complicated and precise AD compared to traditional statistical methods. Motivated by the fact that anomaly detection of time series data resembles text classification and that BERT [54] has proven effective in handling text classification, Dang et al. [55] suggested a time series anomaly detection technique relying on BERT model. Their model has the same architecture as the BERT model, however, the main distinctions are the input representation and additional output layer. Simulation results reveal that this technique only requires a small amount of labeled data for training the BERT model to achieve better results than the SOTA methods. Yet, the model scores on the KPI dataset are still not high enough recording an F1 score of 0.634. Recently, Dong et al. [56] studied the performance of zero-shot learning of LLMs in time series AD problems and their explanatory abilities. They also introduced a synthesized dataset to fine-tune LLMs and improve their performance in time series AD. Their work states that LLMs exhibit encouraging potential for time series AD, while customized prompts and instructions remain necessary. Despite that GPT-4 showed good results with minimal instructions, the present absence of public fine-tuning capabilities on GPT-4 stopped them from investigating whether fine-tuning on GPT-4 could record SOTA performance.\nLeveraging LLMs for AD holds significant promise but still faces several critical challenges. One main concern is their dependence on historical datasets, which can be a matter of concern related to data availability, quality, and model bias. Moreover, these models often have trouble with generalizability, making it hard for them to use learned patterns with novel scenarios. LLMs may also generate inaccurate or misleading outputs under certain conditions, due to hallucinations, which question their reliability. At last, the computational efficiency of these large models imposes a barrier, as their resource-intensive nature can limit scalability and accessibility [57]."}, {"title": "2.6 Model Selection for AD", "content": "Most work in AD has attempted to build novel and enhanced models to perform outlier detection on distinct data types. However, very limited work exists on the AD model selection task. Research work in [58] proposed an automated outlier detection method that applied neural architecture search to find the best neural network model. The limitation of this technique is that it is restricted to auto-encoder-based AD methods and relies on annotated data for evaluation. Another set of automated-based AD model selection techniques are TODS [59] and PyODDS [60] which also rely on labeled data. A more recent automated model selection for unsupervised outlier detection proposed a meta-learning approach without needing ground truth labels in the testing phase [61]. Yet, their approach requires ground truth labels in the meta-train datasets. These model selection methods tend to choose a single AD model after evaluation. However, a recent model selection approach applied to the Secure Water Treatment (SWaT) dataset chooses dynamically an unsupervised AD technique among five different AD models using RL [1]. However, their AD pool of models does not contain a distance-based AD model. Additionally, their RL framework does not consider the anomalous dataset in its state space. Another drawback is that their RL approach relies explicitly on ground truth labels in its reward function. Due to the difficulty of obtaining a huge amount of labeled data in real-world applications, we propose a dynamic model selection for time series AD without excessively relying on ground truth labels. Our approach uses a time series forest and RL-based framework relying on a limited amount of ground truth labels to select an AD algorithm from six unsupervised AD techniques at each time step. Specifically, we will consider six various and popular AD algorithms from the above-mentioned categories: KNN, COPOD, ECOD, OSVM, IForest, and USAD."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Unsupervised AD for Time Series", "content": "This research focuses on building a model selection for time series unsupervised AD. Six distinct unsupervised AD techniques are considered in our framework; KNN [32], COPOD [43], ECOD [42], OSVM [40], IForest [46], and USAD [53]. First, the training set $D_{normal}$, containing a time series of only normal data points, is separately fed to each unsupervised AD model. Every AD model is separately trained on the training set $D_{normal}$. Then, every trained model is tested on the testing set $D_{anomaly}$ having anomalous data points. Each model will output a set of anomaly scores; a score for each instance of the testing set $D_{anomaly}$. For every AD model, an anomaly threshold is empirically computed. Depending on the produced anomaly scores and thresholds, each AD model yields a predicted anomaly label for every instance of $D_{anomaly}$."}, {"title": "3.2 The Confidence Scores", "content": "Two confidence scores, initially used by [1], were considered in our proposed model selection framework for AD techniques. The first confidence score is derived from the concept that the greater the anomaly score, the more likely the corresponding instance is anomalous. AD models generate anomaly scores when tested on new data. The higher the predicted anomaly score exceeds the model's threshold, the more likely it is that the tested data point is an anomaly. Therefore, the extent to which the anomaly score surpasses its threshold serves as a reasonable indicator of the prediction's reliability for the given AD model. The Distance-to-Threshold Confidence Score is equivalent to the following:\n$\\frac{\\text{Anomaly score} - \\text{Threshold}}{\\text{max(Anomaly score)} - \\text{min(Anomaly score)}}$  (1)\nIt is worth mentioning that the anomaly scores are scaled using the min-max scale to a range between zero and one before computing the Distance-to-Threshold Confidence score. Inspired by the principle of majority voting in ensemble learning, the Prediction Consensus Confidence Score indicates that as more AD models in the pool produce the same prediction (predicted label), the likelihood of the current prediction being correct increases. Precisely, the Prediction Consensus Confidence Score is determined as follows:\n$\\frac{\\text{Number of models predicted the same label}}{\\text{Total number of models}}$ (2)"}, {"title": "3.3 Evaluation Metrics", "content": "We annotated the normal data as \u201cnegative\" and the anomalous one as \u201cpositive\u201d. To evaluate the performance of our proposed approach, we apply the well-known precision, recall, and F1score metrics.\n$\\text{Precision} = \\frac{TP}{TP+FP}$ (3)\n$\\text{Recall} = \\frac{TP}{TP+FN}$ (4)\n$\\text{F1score} = \\frac{2 \\times \\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}}$ (5)"}, {"title": "3.4 Time Series Classification Using TSF", "content": "Six TSFs are implemented; a TSF for the predicted anomaly labels of KNN, COPOD, ECOD, OSVM, IForest, and USAD, respectively. Each TSF classifies the predicted anomaly labels of a specific trained AD model (from the previous step) into two classes, $C=\\{0,1\\}$ where C=0 indicates wrongly predicted anomaly labels and C=1 indicates correctly predicted anomaly labels. In other words, if the anomaly-predicted label of a certain AD model is equivalent to the ground truth label, then it is classified as 1 otherwise it is classified as 0."}, {"title": "3.5 AD Model Selection Using Deep Q Network (DQN)", "content": "To tackle our model selection challenge AD, we adopt an RL framework. RL is a type of machine learning that involves an agent learning to make decisions by interacting with an environment. The agent receives feedback in the form of rewards or penalties based on its actions and uses this information to optimize its decision-making strategy over time. RL is particularly suitable for problems where the agent must learn from trial and error to maximize cumulative rewards, such as in robotics, game playing, and autonomous systems. RL can thus be conceptualized as a Markov Decision Process (MDP) where the RL agent maximizes the total expected return by learning the decision policy. MDP contains five major elements; an action-space consisting of the actions set, a state-space containing the states set, a transition matrix representing the probability of state-transition, a reward function, and a discount factor that refers to a value between 0 and 1 that is used in calculating the reward. The future return in an MDP consists of the immediate reward along with the discounted future rewards. The policy represents a probability distribution that maps the present state to the possibility of choosing a particular action.\nIn this work, the DQN algorithm of RL is applied. DQN is a form of RL that utilizes deep neural networks to approximate the action-value function, known as Q-function, in a high-dimensional state-action space. It combines the power of deep learning with Q-learning to enable the efficient handling of complex environments. The agent uses experience replay, storing past experiences in a memory buffer, and updates the neural network using batches of experiences, breaking the correlation between consecutive experiences. This technique allows for more stable and effective learning, enabling the agent to make informed decisions by selecting actions that yield the maximum expected future rewards. In our MDP modeling, the DQN agent was trained to learn a policy to select the suitable AD model among the pool at each time step. At each time step, the agent was rewarded based on its action (selection)."}, {"title": "Reward Function (R)", "content": "Given the context of a time series application, the transitions between states adhere to the chronological order of the time series dataset. As a result, the state transition is considered deterministic, eliminating the necessity to incorporate a transition matrix. Moreover, we set the discount factor to one to appropriately handle the time series nature of the problem. The remaining components of the MDP are described below;\nState-space: The entire testing set $D_{anomaly}$, the scaled predicted anomaly scores from all the considered AD models, the obtained scaled empirical thresholds, and the predicted anomaly labels for these six AD models, as well as the two confidence scores of these AD models.\nAction-space: The action-space is a discrete action-space. The DQN agent can select one AD model from the pool of the six AD models at each time step. Thus, the size of the action-space is the same as the number of candidate AD models in the pool.\nReward Function (R): The reward function is obtained by comparing the predicted anomaly label obtained from the selected AD model in the pool of models to the predicted class labels obtained the corresponding TSF in most of the testing set $D_{anomaly}$ (80% of $D_{anomaly}$). This represents the class-based portion of the reward. In the remaining 20%, the reward function is obtained by comparing the predicted anomaly label obtained from the selected AD model in the pool of models and its corresponding ground truth labels. This refers to the ground truth-based portion of the reward. It should be mentioned that the same 20% of ground truth labels used during the training of the TSFs, are used here in the reward function.\nThe reward function of equation 6 is used as the original reward, where a TP is given a higher reward compared to a TN reward ($r_{Tp} > r_{Tn}$). A TP is an instance where the agent selects an AD model that correctly predicts an anomaly. However, TN means that the AD model selected by the agent at this time step correctly predicts this data point as normal. Since the majority of instances are usually normal while the anomalous instances are rare and we are interested in detecting anomalies, thus the reward for true positive should be set to be greater than the TN reward $r_{Tp} > r_{Tn}$. On the other hand, a higher penalty should be applied to FN compared to a FP. A FN represents a case where the RL agent selects an AD model that predicted this data point to be normal when this point is actually anomalous. A FP refers to a case where the agent chooses an AD model that predicts this instance as anomalous when it is, in reality, normal. In real-world applications, neglecting anomalies (FN case) is more detrimental than giving false alarms (FP case), so $|r_{FN}| > |r_{Fp}|$. As such, we reflected it in equation 6 a reward function that consists of $r_{rn}$ that is set to 0.5, $r_{Tp}$ is set to 1, $r_{fP}$ is set to-1.5, and $r_{FN}$ is set to -3.\n$R=\\begin{cases} r_{tn} = 0.5 & \\text{TN} \\\\ r_{FN} = -3 & \\text{FN} \\\\ r_{Tp} = 1 & \\text{TP} \\\\ r_{fp} =-1.5 & \\text{FP} \\end{cases}$ (6)"}, {"title": "4 Dataset Description and Experimental Settings", "content": ""}, {"title": "4.1 Datasets Description", "content": "In this work, we considered two datasets; one real and one synthetic dataset. Each of these datasets consists of the hourly time-steps, the electricity consumption, and the ground truth labels. The real dataset is an annotated subset of the Large-scale Energy AD dataset (LEAD1.0) [62], and it is made up of hourly meter readings obtained from 1,413 smart electricity meters over one year and gathered from sixteen distinct sites around the world. The LEAD1.0 dataset contains point and sequential (collective) anomaly types. For the sake of this work, we used a subset of LEAD1.0 containing hourly meter readings from twenty-two smart electricity meters. We split this real data into a normal dataset consisting of only normal instances and an anomalous dataset with normal and anomalous instances. The normal dataset consisting of hourly electricity meter readings (electricity consumptions) has a total of 18696 time-steps. However, the anomalous hourly electricity meter readings dataset has readings of 20424 time-steps with around 5.7% of anomalies and their corresponding ground truth labels.\nThe second dataset considered in this work is a synthetic dataset in which anomalies are injected randomly into the dataset. We used the time series of hourly electricity consumption data from the residential areas of Germany provided by ENTSO-E Transparency, a European electricity data platform [63]. Similarly, this dataset was divided into normal and anomalous datasets. The normal dataset has hourly power-consumption data from the year 2015 until 2018 making up a total of 35064 time-steps. The anomalous dataset consists of 1008 hourly power-consumption data points of 2019 with 5% of randomly injected anomalies and the corresponding ground truth labels."}, {"title": "4.2 Data Preprocessing", "content": "Initially, data preprocessing was performed on the datasets. First, any missing values in the data were removed. Then, the electricity consumption data points in the normal and anomalous datasets (synthetic and real datasets) were rescaled. In the synthetic normal and anomalous datasets, we applied min-max scaling to scale the electricity consumption data into values between zero and one. However, in the real datasets, we applied logarithmic scaling similar to the original paper of LEAD1.0 [62]. After that, each normal and anomalous dataset was transformed into sliding windows of size six with a step size of one. The chosen size of the sliding window and the step size were found the most suitable after heuristically trying sliding sizes six, twelve, and twenty-four with different step sizes of one, two, three, and zero overlaps."}, {"title": "4.3 Settings of the AD Candidate Models in the Pool", "content": "We considered six different AD models in our pool of models, which are KNN [32], COPOD [43], ECOD [42], OSVM [40], IForest [46], and USAD [53]. OSVM and IForest are implemented from the Scikit-Learn library [64]. COPOD, ECOD, and KNN are imported from the PyOD toolbox [65]. As for the USAD model, it was implemented from the GitHub repository of USAD [66].\nFor each of KNN, OSVM, and IForest, we performed hyperparameter optimization using Hyperopt library [67] to tune their hyperparameters and select the best configuration of hyperparameters. The list of hyperparameter choices for each of KNN, OSVM, USAD, and IForest AD models are available in the Appendix. COPOD and ECOD do not require hyperparameter tuning since they are deterministic and have no hyperparameters. As for USAD, it has two hyperparameters, alpha and beta, such that their sum must be equal to one. They are utilized for parameterizing the trade-off between true and false positives, which varies the model's sensitivity. We used the original implementation of USAD having neutral detection sensitivity where alpha equals beta equals 0.5 to avoid having a sensitive model biased towards true positives or false positives.\nA specific criterion is necessary to compute the empirical threshold for each AD model. Given that we know the percentage of anomalies in both datasets, we set the criterion to be equal to this percentage (5%). To elaborate further, for each AD model, if a data point's predicted anomaly score ranks among the highest 5%, within all the predicted anomaly scores generated by that model, the data point is classified as an anomaly."}, {"title": "4.4 Settings of the Proposed RL Framework for AD Model Selection", "content": "In the implementation of the RL framework, we used the DQN model imported from Stable-Baseline3. Stable-Baseline3 is a RL toolbox based on PyTorch [68]. The default settings of DQN provided by [68], having a decaying exploration epsilon, were utilized. However, the exploration fraction of the decaying exploration epsilon was set to 0.7 instead of the default value of 0.1 to allow the DQN agent to explore more, and the seed number was set to 1. In the synthetic anomalous dataset case, the RL agent was trained for 600,000 time steps. The agent was trained for 3,000,000 time steps in the real anomalous dataset case. This is because the real dataset is larger than the synthetic one. During the evaluation phase of the DQN agent, we set the agent's actions to be deterministic."}, {"title": "5 Experiments and Results", "content": ""}, {"title": "5.1 AD Using the Candidate AD Models in the Pool", "content": "To perform AD using the six unsupervised AD techniques, each AD model was trained on the normal sliding windows having normal electricity consumption instances. After that, each of these pre-trained AD models was tested on the sliding windows of the anomalous electricity consumption instances. The predicted anomaly label (anomalous or normal instance) was compared to the ground truth labels."}, {"title": "5.2 Time Series Classification Using TSF", "content": "To avoid relying on large quantities of ground truth labels that are not usually available in real-world applications, TSFs were used to classify the predicted anomaly labels into correct or wrong labels. Six TSFs were implemented corresponding to the six AD models. Each TSF was fed with a different dataset corresponding to the AD model."}, {"title": "5.3 AD Model Selection Based on an RL Framework", "content": "In this work, we proposed an AD model selection using a RL framework. The original reward function, shown in equation 6, was used in our proposed RLAD model selection framework. The performance of the proposed model selection framework for AD the synthetic dataset is stated in Table 1. Comparing the results of the six AD models in Table 1; shows that the proposed AD model selection provided a precision of 1, the same as KNN outperforming all the remaining AD models. In terms of the F1score, our suggested AD model selection recorded a value of 0.989 higher than the entire candidate AD models except for KNN having a slightly greater F1score of 1. In the case of the real dataset, results in Table 2 show that the proposed AD model selection, having a precision of 0.977 and an F1score of 0.727, outperformed all the candidate AD techniques of Table in terms of the precision and F1score."}, {"title": "5.4 Comparison of AD Model Selection Based on an RL Framework to an LLM-based AD model", "content": "Considering the synthetic dataset, which is a relatively small dataset, the proposed RLAD model selection framework was compared to an LLM-based AD model. Leveraging Chain-of-Thought (CoT) prompting, as illustrated in Figure 2, we motivated the ChatGPT4o-mini model to perform an AD task and to break down the complex thoughts or \"processes\" into smaller steps before providing a response on any detected anomalies. The LLM model yielded encouraging results on the AD task scoring an F1 score of 0.913 and a recall of 0.84 as shown in Table 5. Yet, our proposed AD model framework, recording an F1 score of 0.989 and recall of 0.977 still outperforms the LLM-based AD model."}, {"title": "5.5 Comparison of Different RL Frameworks for AD Model Selection", "content": "The proposed RLAD model selection framework, which uses a small portion of ground truth labels, was compared with two other RLAD model selection frameworks; The first framework $RLAD_{Gtruth}$ totally relies on ground truth labels in its reward function. This means that, in all the instances, the reward was obtained by comparing the predicted anomaly label with the ground truth label. On the contrary, the second framework named $RLAD_{class}$ relies totally on the predicted class labels obtained from TSFs. It should be noted that the $RLAD_{class}$ framework also requires a portion of ground truth labels (used for TSFs training). The state-space and action-space remain the same in all frameworks. The original reward of equation 6 was applied in all of the three frameworks ($r_{rn}$= 0.5, $r_{FN}$= -3, $r_{Tp}$= 1, $r_{Fp}$= -1.5).\nThe precision, recall, and F1score results of these two AD model selection frameworks based on RL are also present in Tables 6 and 7 using the synthetic and real datasets respectively. In both datasets, all the frameworks showed nearly the same high performance. In the synthetic data, the three frameworks recorded a precision of 1; however, our proposed framework recorded the highest F1score of 0.989. In the real data, our proposed framework, having a 0.977 precision and an F1score of 0.727, showed a slightly higher performance than $RLAD_{class}$ but slightly lower than $RLAD_{Gtruth}$. It can be said that, in the case of the real dataset, the proposed AD model selection offered a compromise between $RLAD_{Gtruth}$ that explicitly uses ground truth labels and $RLAD_{class}$ that relies totally on class labels in its reward function."}, {"title": "5.6 Different Constant Reward Functions with a Decaying Epsilon", "content": "To study the effect of the different reward values on the proposed RL framework, we considered two extreme scenarios of reward functions R1 and R2 (Table 8) and evaluated them on the synthetic dataset. It can be observed that in comparison with the original reward function ($r_{Tn}$= 0.5, $r_{FN}$= -3, $r_{Tp}$= 1, $r_{fp}$= -1.5), the TN reward in R1 was decreased from 0.5 to 0.15. The FP reward in the original reward which represented a high penalty of -3 was changed into a low positive reward of 0.1. However, the FN reward and TP rewards in both functions remained the same ($r_{FN}$= -3 and $r_{Tp}$= 1). Applying a lower positive reward on TN and a low positive reward on FP instead of a high penalty has led to a decrease in the model's performance in the case of R1. In this regard, the precision decreased sharply from 1 to 0.636, the recall decreased from 0.977 to 0.88, and the F1score also decreased from 0.989 to 0.738 as shown in Table 9.\nAs shown in Table 8, another extreme scenario was investigated in the R2 function; in which the TN reward in R2 was set to be greater than the TN reward in the original function. Similarly, the FP penalty in R2 was set to be greater than the FP penalty in the original function. However, the penalty applied on FN in the original reward function was modified to a low reward in the case of R2. The TP reward in R2 was set to be less than the TP reward in the original function. The results of the original reward function in comparison to R2 reward function, in Table 9, reflected a slight decrease in the precision from 1 to 0.987 but a significant decrease in the recall from 0.977 to 0.564, and in the F1score from 0.989 to 0.718. This is due to decreasing the TP reward and giving the agent a low reward in case of a FN instead of penalizing it with a negative reward. As a result, the reward functions play a significant role in the performance of the proposed AD model selection approach in which the original reward function offered the most suitable and logical formulation of the AD model selection problem."}, {"title": "5.7 Adaptive Rewards with Different Epsilons", "content": "We also inspected the effect of two adaptive reward functions; an increasing adaptive reward AdapInc and a decreasing adaptive reward AdapDec. In the AdapInc, we designed a reward function that increases iteratively with the time steps of the anomalous data. In this regard, the TN and TP rewards were intended to increase in a step-wise parabolic manner as a function"}]}