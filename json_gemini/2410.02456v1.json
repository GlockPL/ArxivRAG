{"title": "Recurrent Few-Shot model for Document Verification", "authors": ["Maxime Talarmain", "Carlos Boned", "Sanket Biswas", "Oriol Ramos Terrades"], "abstract": "General-purpose ID, or travel, document image- and video-based verification systems have yet to achieve good enough performance to be considered a solved problem. There are several factors that negatively impact their performance, including low-resolution images and videos and a lack of sufficient data to train the models. This task is particularly challenging when dealing with unseen class of ID, or travel, documents. In this paper we address this task by proposing a recurrent-based model able to detect forged documents in a few-shot scenario. The recurrent architecture makes the model robust to document resolution variability. Moreover, the few-shot approach allow the model to perform well even for unseen class of documents. Preliminary results on the SIDTD and Findit datasets show good performance of this model for this task.", "sections": [{"title": "Introduction", "content": "The increased of remote identity authentication systems, incorporating biometrics and the verification of ID and travel documents, has surged and become widespread in the wake of the COVID-19 pandemic. These authentication systems have empowered citizens to engage in work and business activities outside traditional office settings. Public administration, banks, productive industries, and numerous services have integrated these systems seamlessly into their routine workflows. These services provide an online enrollment option, eliminating the need for users to physically attend by requesting a selfie and an image of their ID document for authentication. Nevertheless, cybercrime has exploited societal vulnerabilities, evolving towards increasingly sophisticated threats. Therefore, it is necessary to develop techniques that allow us to detect this type of fraudulent actions that expose citizens' data and their privacy to the general public.\nA current trend is to detect ID documents, passports or driving licenses that have physically or digitally been modified from images acquired from mobile devices. In [4], the authors applied texture descriptors to image patches and then applied BoW followed by an SVM classifier to classify each patch as genuine or fake. In that work, the most performant CNN architectures of that time (AlexNet, VGG and Inception) were also used as general descriptor extractors. The results obtained in terms of F1-Score were very good, obtaining results between 0.99 and 1 in many cases. The main problem of"}, {"title": "Methodology", "content": "As we mentioned in the Introduction, the proposed model is relatively simple and, to some extent, unoriginal as it combines components that are well known in the community. Fig. 1 shows the architecture used to train the model. Essentially, it is a recurring many-to-one network. The input of each recurring unit, in addition to the previous state of the network, is a feature vector calculated in patches of the image. The output of the network is an m-dimensional vector that is used to classify the document image as genuine or fake.\nDocument images have been partitioned into W-dimensional square patches that overlap to avoid contour effects. Each patch is processed by a pre-trained backbone that is used as an universal feature extractor. The output of the backbone is a vector of dimension $n > m$ that feeds the Recurrent Unit (RU)."}, {"title": "Related Work", "content": "Document verification is essentially a binary classification task. We have seen in the Introduction that in a classical supervised context almost any classification method would perform well if it were not for the lack of sufficient quality data to provide these models enough generalization capacity. In this section, we will review the key architecture components that closely related to the proposed model. Therefore, we will briefly review the main convolution architectures that can be used as backbones in our model, the usual recurring units, and the main FSL strategies."}, {"title": "Experiments and discussion", "content": "As we have pointed from the beginning of this paper, the biggest weakness of document verification methods in general is their unreliability when they have to process document classes not seen in training. Consequently, the goal of the experiments carried out was to evaluate as much as possible the generalization capacity of the proposed model. To this end, we have repeated 10 times the same experiments with randomly chosen metaclasses, queries sets and support sets. We reported the performances of the evaluated models in terms of the the accuracy rate and the area under the curve, respectively denoted by Accuracy and AUC in the results tables. Together with the mean of the values of these metrics, we have computed their standard deviation. Below, we briefly describe the datasets used and the experiments set up. We conducted an ablation study to assess the impact of the backbone and the recurrent units in the model performance. Then, we analyse and discuss the generalization capacity of the proposed models under harder conditions."}, {"title": "Conclusion", "content": "In this paper, we have presented a recurrent network model combined with FSL strategies to verify whether document images are genuine or fake. Despite of not introducing any new component in this architecture, the proposed architecture, by itself, is original and has not been applied to this task and similars. Moreover, the results obtained, as we have already discussed in the previous section, support the proposed strategy.\nThe proposed model seems to be able to learn good document representations. That representations are what would allow our proposed model to generalize well. However, there are still elements that deserve further study. FSL models still require few examples of fake and genuine documents and it is knows that in practice is not always feasible. Therefore, it is necessary to develop models that allow us to move towards zero-shot models."}]}