{"title": "SleepCoT: A Lightweight Personalized Sleep Health Model via Chain-of-Thought Distillation", "authors": ["huimin zheng", "xiaofeng xing", "xiangmin xu"], "abstract": "We present a novel approach to personalized sleep health management using few-shot Chain- of-Thought (CoT) distillation, enabling small-scale language models (< 2B parameters) to rival the performance of large language models (LLMs) in specialized health domains. Our method simultaneously distills problem-solving strategies, long-tail expert knowledge, and personalized recommendation capabilities from larger models into more efficient, compact models. Unlike ex- isting systems, our approach offers three key functionalities: generating personalized sleep health recommendations, supporting user-specific follow-up inquiries, and providing responses to domain- specific knowledge questions. We focus on sleep health due to its measurability via wearable devices and its impact on overall well-being. Our experimental setup, involving GPT-40 for data synthesis, Qwen-max for instruction set creation, and Qwen2.5 1.5B for model distillation, demonstrates sig- nificant improvements over baseline small-scale models in penalization, reasoning, and knowledge application. Experiments using 100 simulated sleep reports and 1,000 domain-specific questions shows our model achieves comparable performance to larger models while maintaining efficiency for real-world deployment. This research not only advances AI-driven health management but also provides a novel approach to leveraging LLM capabilities in resource-constrained environments, potentially enhancing the accessibility of personalized healthcare solutions.", "sections": [{"title": "1 Introduction", "content": "The rapid proliferation of wearable devices has ushered in a new era of personal health data collec- tion. These devices, ranging from smartwatches to fitness trackers, continuously gather a wealth of physiological data that serve as external indicators of an individual's health status. Heart rate vari- ability, sleep patterns, physical activity levels, and other metrics provide a comprehensive picture of one's well-being, offering unprecedented insights into personal health trends and potential issues. The volume and variety of data generated by these devices present both opportunities and challenges in the field of personalized healthcare. On one hand, this data deluge allows for a more nuanced and individualized understanding of health patterns. By analyzing these extensive datasets, it becomes possible to discern subtle changes in health status, potentially identifying early warning signs of various conditions or tracking the effectiveness of lifestyle changes.\nThe rapid evolution of large language models (LLMs), such as GPT-40, Claude 3.5 Sonnet, and Qwen-max[2], has significantly advanced the handling of personalized health management and domain- specific knowledge applications. These state-of-the-art models are highly capable of generating person- alized lifestyle recommendations based on physiological signals from wearable devices, such as heart rate variability and sleep patterns. By leveraging complex reasoning and expert knowledge, LLMs can offer tailored advice, enhancing users' health management in various contexts. However, despite their outstanding performance, these models face significant limitations in real-world applications, particularly in resource-constrained environments.\nFirstly, the sheer size of these models poses a considerable challenge. With billions of parameters, large models demand substantial computational resources, making them impractical for deployment on edge devices like smartphones or wearable products. This restricts their usability in everyday scenar- ios, where on-device, real-time interactions are crucial for effective personalized health management. Furthermore, the high computational complexity associated with these models leads to increased en- ergy consumption and high hardware costs, which hinder the widespread adoption of such advanced"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 LLM for Health applications", "content": "Large language models exhibit the remarkable capability to provide proficient responses to free- text queries, demonstrating a nuanced understanding of professional medical knowledge [11, 20, 25]. Through the provision of personalised recommendations [1, 29, 14], customised treatment strategies, and continual monitoring of patients' advancements throughout their medical journeys, LLMs offer the promise of revolutionizing healthcare delivery. LLMs are also widely used in healthcare-related question-answering tasks[38, 24], where they demonstrate the ability to understand complex medical queries, provide evidence-based answers [26], and offer personalized insights [22, 21], making them valu- able tools in clinical decision support[28], patient education [36], and health management scenarios[34].\nThe two works most closely related to ours are: one is PH-LLM[5], a fine-tuned model for contextual- izing physiological data and producing personalized insights, the other is PhysioLLM[7], an interactive system that leverages large language models (LLMs) to provide personalized health understanding and exploration by integrating physiological data from wearables with contextual information. Unlike their direct use of large models like GPT-3.5 or Gemini, our work focuses primarily on smaller large language models (LLMs) with parameters less than 2 billion. This approach aims to balance the performance benefits of LLMs with the practical constraints of deployment, such as computational efficiency, lower latency, and suitability for real-time, on-device applications. By concentrating on smaller models, our work addresses the challenges of making advanced AI-driven personalized health management acces- sible and deployable in everyday scenarios, where larger models are often impractical due to their resource demands."}, {"title": "2.2 LLM with wearable data", "content": "Personal health data, often derived from personal devices such as wearables, are distinguished by their multi-dimensional, continuous and longitudinal measurements that capture granular observations of physiology and behavior in-situ rather than in a clinical setting[19]. Research on LLMs based on wearable data is still in its early stages, exploring the integration of physiological data such as heart rate, sleep patterns, and physical activity into AI-driven models [5]. These initial studies aim to harness the potential of LLMs to provide personalized health insights and recommendations. However, as mentioned in [19], large-scale wearable data remains a significant barrier to the application of LLMs in personalized health scenarios. [19] proposes a framework for data synthesis, aiming to overcome the challenges posed by the variability, privacy concerns, and volume of wearable data. This framework seeks to generate realistic and diverse synthetic datasets that can be used to train LLMs, thereby facilitating their adaptation to personalized health applications without the need for massive real- world data collection. Derived from the aforementioned approach, but distinct in its focus, this paper specifically addresses sleep scenarios by synthesizing heart rate variability (HRV) data. Unlike prior methods, our approach generates HRV datasets that encompass a wide range of health conditions and sleep states, capturing the intricate variations in physiological responses associated with different sleep qualities. This targeted synthesis of HRV data is crucial for accurately reflecting the complex interactions between sleep and overall health, enhancing our model's capacity to deliver personalized recommendations and insights tailored to individual sleep health management."}, {"title": "2.3 LLM distillation", "content": "Due to resource constraints and real-time requirements [10], many studies have focused on distilling large language models, which involves transferring the knowledge[18, 37], reasoning capabilities[16, 12], and specialized domain expertise[39] from these expansive models into smaller, more efficient models. [27] proposes an alternative reasoning scheme, SOCRATIC COT that learns a decomposition of the original problem into a sequence of subproblems and uses it to guide the intermediate reasoning steps. [12] proposes Knowledge-Augmented Reasoning Distillation (KARD), a novel method that fine-tunes small LMs to generate rationales obtained from LLMs with augmented knowledge retrieved from an external knowledge base. However, these methods are primarily oriented towards numerical reasoning and factual judgment, focusing on interpreting data points and deriving logical conclusions. They excel in processing quantitative information and making evidence-based decisions but often lack the depth"}, {"title": "3 MOTIVATION", "content": "As demonstrated in [7, 5], state-of-the-art large models, such as GPT-40 and Qwen-max, already possess capabilities for personalized question-answering and data interpretation specifically tailored to the sleep health domain. These advanced models can understand complex sleep-related data and provide customized health advice directly. However, their substantial size and computational demands make them unsuitable for deployment in resource-constrained environments or applications with high real-time requirements. Simultaneously, a substantial body of research [8, 32, 31] has shown that in specific domains, smaller models can attain performance levels comparable to those of larger models through the process of distillation. Distillation enables these smaller models to effectively inherit the specialized knowledge, complex reasoning capabilities, and contextual understanding from their larger counterparts, thereby enhancing their efficiency and effectiveness despite having significantly fewer parameters. These findings validate the potential of using distilled smaller models in specialized applications, where they can provide high-quality outcomes while addressing practical constraints such as limited computational resources and stringent real-time requirements, as shown in 1 This motivation drives our design goals: to create a model that not only inherits the advanced understanding and personalized response capabilities of larger models but also addresses the practical constraints of deployment. By achieving a balance between performance and deployability, our work seeks to bring advanced, AI-driven personalized sleep health management closer to everyday use, offering the potential for real-time, personalized insights directly at the user's fingertips."}, {"title": "4 SleepCoT ARCHITECTURE AND IMPLEMENTATION", "content": "Figure 2 illustrates the overall framework of SleepCoT, which consists of three main components: data acquisition, personalized recommendation generation, and user question-answering (including both personalized and domain-specific knowledge inquiries)."}, {"title": "4.1 Data acquisition", "content": "In real-world scenarios, wearable devices such as smartwatches or sensor-embedded mattresses can capture electrocardiogram (ECG) signals during the user's sleep. From these signals, various heart rate variability (HRV) parameters can be derived, such as SDNN (Standard Deviation of NN intervals) and RMSSD (Root Mean Square of Successive Differences). These HRV metrics provide valuable insights into the autonomic nervous system's activity and overall sleep quality, serving as critical indicators for personalized sleep health analysis and recommendation generation. [5] has demonstrated the significant potential of Large Language Models (LLMs) in synthesizing wearable data. This highlights their capability to generate realistic and diverse datasets, which can be effectively utilized for training and evaluating models in personalized health applications.\nFigure illustrates the overall framework of data synthesis. The framework outlines the process of generating synthetic datasets, starting from the collection and processing of instance data, followed by the application of physiological parameter constraints. This approach ensures that the synthesized data closely mimics real-world conditions. The framework also includes steps for generating personalized recommendations and potential user questions based on the synthesized sleep reports, thereby creating a comprehensive and realistic dataset for model training and evaluation. Using GPT-40, we synthesized 100 samples derived from real-world examples(i) and predefined physiological rules(R). Each sample consists of HRV parameters collected over six consecutive nights, including key metrics such as SDNN and RMSSD, alongside average sleep duration and sleep staging data, such as time spent in light, deep, and REM sleep stages. To assess users' cardiac health, stress resilience, and other related conditions, we employed predefined algorithms analogous to those described in prior studies[13, 33, 30]. These algorithms facilitated the generation of comprehensive sleep state assessments, which were subsequently integrated with the corresponding sample data. Leveraging GPT-40, personalized recommendations were then generated based on these combined assessments, providing tailored guidance specific to the evaluated sleep profiles. The above process can be represented by the following equations:\nwearable_data = GPT \u2013 4o(Pr1(I + R))\nsleep_description = D_A(wearabledata)\npersonal_suggestion = GPT \u2013 4o(Pr2(wearable + sleep_description))\nwhere D_A represents the predefined evaluation algorithm, Pr represents the predefined prompt template, while Table 1 and Table 2 is the example prompt of Pr1 and Pr2. I represents a sample obtained from wearable data."}, {"title": "4.2 Experiment Setting", "content": "Before distillation, the teacher model Qwen-max was tested on the public dataset SleepQA[4] to val- idate the richness of its domain-specific expertise in the field of sleep. In the distillation experiment, we set up three distillation tasks: personalized recommendation generation, personalized question- answering, and domain-specific knowledge question-answering. These three tasks were trained jointly. In addition, experiments were conducted to investigate the impact of varying the proportions of instruc- tion sets among the three tasks on model performance. The experiment was designed to understand how different ratios of task-specific instruction sets affect the overall effectiveness of the model. By adjusting the proportions of instruction sets for tasks such as personalized question-answering, domain- specific knowledge question-answering, and recommendation generation, the goal was to identify which task data most significantly enhances model performance. This approach allows for the optimization of instruction set design and data allocation, improving the model's performance in real-world appli- cations. In resource-constrained environments, these findings enable the more efficient utilization of data to enhance the model's capabilities."}, {"title": "4.2.1 Instruction Set", "content": "The training set comprises 80 suggestion generation samples, 12000 personalized question-answering samples, and 600 knowledge-based question-answering samples. The test set includes 20 suggestion generation samples, 3000 personalized question-answering samples, and 200 knowledge-based question- answering samples. Additionally, the test set contains a separate set of 100 personalized questions generated by Claude-Sonnet 3.5."}, {"title": "4.2.2 Few shot Chain-of-Thought Prompt", "content": "In this study, the model is guided to answer questions using a Chain-of-Thought (CoT) approach, where the reasoning process mimics human thought patterns. The CoT prompts are designed to first extract relevant information from the context before generating a response. In open-ended question- answering scenarios, questions are categorized into three types to enhance the accuracy of the answers: those where information can be directly found from the context, those that require a global summary, and those where relevant information is not available in the context. Each category is illustrated with"}, {"title": "4.2.3 model and Training Parameter setting", "content": "Qwen-max was selected as the teacher model, and Qwen2.5.5-1.5B was chosen as the student model. The models were fine-tuned using the LORA (Low-Rank Adaptation) [9] approach. The fine-tuning process employed the LORA (Low-Rank Adaptation) approach with specific hyper-parameters set as follows: the learning rate was configured at 1.0e-5, the batch size was set to 1, the LoRA rank was set to 8, and the number of epochs was set to 10. These settings were chosen to ensure efficient training and effective adaptation of the student model (Qwen2.5-1.5B) and (Qwen2.5-0.5B) from the teacher model (Qwen-max) while maintaining computational efficiency. In addition, experiments were conducted directly using GPT-40\u00b9, Claude-Sonnet 3.52, Baichuan43, GLM-44, Gemini 1.5 Pro5, Qwen2.5-7B, and Qwen2.5-1.5B to enhance the richness of the experiments. Finally, we tested the distilled model, SleepCoT, on the SleepQA dataset to evaluate its performance in the sleep domain and assess its ability to effectively handle sleep-related questions while retaining sufficient domain knowledge and reasoning capabilities after the distillation process."}, {"title": "4.2.4 Evaluation method", "content": "Since traditional evaluation methods such as BLEU, ROUGE, and BERTScore struggle to effectively differentiate model performance in this scenario, GPT-40[3] is employed as the evaluator. Inspired by RAGAS[6], the models are assessed based on the following four dimensions:\n\u2022 Penalization: Evaluates how well the generated recommendations and answers are tailored to the individual user's data and specific needs.\n\u2022 Relevance: Measures the alignment of the responses with the user's context and the specific questions asked, ensuring that the information provided is pertinent.\n\u2022 Completeness: Assesses whether the responses comprehensively cover all necessary aspects of the query, ensuring no critical details are left out.\n\u2022 Accuracy: Evaluates the correctness of the information provided, focusing on domain-specific knowledge and the validity of personalized advice.\nEach dimension was scored on a scale of 1 to 5, with five levels of assessment."}, {"title": "4.3 Main Results", "content": "The results of the evaluation of the teacher model Qwen-max regarding the adequacy of its domain- specific expertise in the field of sleep are shown in table 5. As demonstrated in Table 5, Qwen-max's domain-specific expertise in the field of sleep is not only adequate but also performs well across various evaluation metrics. This suggests that the model has a strong understanding of sleep-related knowledge and is capable of answering questions with a high degree of accuracy. The robustness of Qwen-max in this specialized domain indicates that it can serve as a reliable teacher model for downstream tasks, such as knowledge distillation, where capturing and transferring this expertise to smaller, more efficient models is crucial. After the distillation process, the distilled model retained the same level of domain expertise, showing no decline in the quality of specialized knowledge. This validates both the potential of Qwen-max to provide accurate and comprehensive insights into sleep-related queries and the effectiveness of the distillation process in preserving essential knowledge, making the distilled model a suitable candidate for further application in sleep-related QA systems.\nThe evaluation results of SleepCoT are presented in table 6. It shows that proposed method SleepCoT improves task performance across all baselines. It can be observed that SleepCoT-1.5B performs on par with Qwen-max across all four evaluation dimensions, while SleepCoT-0.5B a slight performance gap compared to SleepCoT-1.5B. An example is shown in Figure 3. From this example, it can be observed that open-source small models like Qwen2.5-7B lack sufficient penalization in their responses. From the performance gap observed between the 0.5B and 1.5B distilled small models, it is evident that the 0.5B model lags behind in the accuracy dimension compared to the 1.5B model. This indicates that even after fine-tuning, the 0.5B model's ability to utilize long-tail knowledge is still not as strong as that of the 1.5B model.\nThe results of the experiment on adjusting data proportions are presented in Figure 4. As the number of personalized question-answering instructions increases from 4000 to 12000, there is a no- ticeable improvement in the scores across all four dimensions-Penalization, Relevance, Completeness, and Accuracy.\n\u2022 Penalization: Starting at a score of 4.3 with 4000 instructions, it steadily improves, reaching 4.8 at 8000 instructions, and slightly increases to 4.85 as the data volume hits 12000. This suggests that increased personalized data significantly enhances the model's ability to tailor responses effectively.\n\u2022 Relevance: Initially scoring 4.1 at 4000 instructions, relevance improves to 4.7 by 8000 instruc-"}, {"title": "4.4 Ablation Study", "content": "To demonstrate the necessity of each component in SleepCoT, we take a series of ablation study by remov- ing the following parts:(1) examples in prompt: the instruction prompt contains only Chain of"}, {"title": "5 Conclusion", "content": "This work explores the effectiveness of few-shot chain-of-thought prompting for distilling complex reasoning abilities and domain-specific knowledge from large language models to specialized smaller models. The approach was validated in the sleep domain, where a 1.5B-sized model was shown to be easily deployable on edge devices while meeting specific demands in vertical domains, such as personalized sleep recommendations. As with other specialized fields, this method can be extended to similar domains, demonstrating its versatility and potential for broader applications."}, {"title": "6 Limitations", "content": "One limitation of this work is the potential lack of generalization to other domains, as the success in the sleep domain may not translate directly to more complex or less structured fields. Additionally, the method's effectiveness is highly dependent on the quality and diversity of the data, making it vulnerable to bias or reduced performance in cases where domain-specific data is limited. While smaller models like the 1.5B model are easier to deploy on edge devices, there remains a trade-off between efficiency and performance, particularly in more complex tasks. Moreover, few-shot learning techniques, though effective for specialized tasks, may struggle to scale to broader applications that require a more extensive knowledge base. The ability of the model to adapt in real-time to new data or evolving user needs is another potential limitation, as its personalized recommendations may lose relevance without frequent updates. Furthermore, traditional evaluation metrics such as BLEU or ROUGE may not fully capture the model's deeper reasoning capabilities, limiting the assessment of its true effectiveness. Finally, while edge deployment is feasible, optimizing the model to function effectively within the strict resource constraints of edge devices, without compromising performance, remains a challenge."}, {"title": "7 Appendix", "content": ""}]}