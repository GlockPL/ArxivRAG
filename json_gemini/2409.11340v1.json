{"title": "OmniGen: Unified Image Generation", "authors": ["Shitao Xiao", "Yueze Wang", "Junjie Zhou", "Huaying Yuan", "Xingrun Xing", "Ruiran Yan", "Shuting Wang", "Tiejun Huang", "Zheng Liu"], "abstract": "The emergence of Large Language Models (LLMs) has unified language generation\ntasks and revolutionized human-machine interaction. However, in the realm of\nimage generation, a unified model capable of handling various tasks within a single\nframework remains largely unexplored. In this work, we introduce OmniGen,\na new diffusion model for unified image generation. Unlike popular diffusion\nmodels (e.g., Stable Diffusion), OmniGen no longer requires additional modules\nsuch as ControlNet or IP-Adapter to process diverse control conditions. OmniGen\nis characterized by the following features: 1) Unification: OmniGen not only\ndemonstrates text-to-image generation capabilities but also inherently supports\nvarious downstream tasks, such as image editing, subject-driven generation, and\nvisual-conditional generation. Additionally, OmniGen can handle classic computer\nvision tasks by transforming them into image generation tasks, such as edge\ndetection and human pose recognition. 2) Simplicity: The architecture of OmniGen\nis highly simplified, eliminating the need for additional text encoders. Moreover,\nit is more user-friendly compared to existing diffusion models, enabling complex\ntasks to be accomplished through instructions without the need and cost for extra\npreprocessing steps (e.g., human pose estimation), thereby significantly simplifying\nthe workflow of image generation. 3) Knowledge Transfer: Benefit from learning\nin a unified format, OmniGen effectively transfers knowledge across different\ntasks, manages unseen tasks and domains, and exhibits novel capabilities. We also\nexplore the model's reasoning capabilities and potential applications of chain-of-\nthought mechanism. This work represents the first attempt at a general-purpose\nimage generation model, and there remain several unresolved issues. We will\nopen-source the related resources at https://github.com/VectorSpaceLab/OmniGen\nto foster advancements in this field.", "sections": [{"title": "Introduction", "content": "The pursuit of Artificial General Intelligence (AGI) has intensified the demand for generative founda-\ntion models capable of handling a wide variety of tasks within a single framework. In the field of\nNatural Language Processing (NLP), Large Language Models (LLMs) have become exemplary in\nachieving this goal, demonstrating remarkable versatility across numerous language tasks such as\nquestion answering, text summarization, and code generation.\nHowever, the field of visual generation has yet to reveal a counterpart that mirrors the universality of\nLLMs. Current image generation models have demonstrated proficiency in specialized tasks. For\ninstance, in the text-to-image generation filed, state-of-the-art models such as the Stable Diffusion\nseries [56; 52; 13], DALL-E [55], and Imagen [26] have made significant strides. Meanwhile, many\nefforts have been proposed to extend and optimize the capabilities of diffusion models for specific\ntasks. Models like ControlNet [73] and T2i-Adapter [45] design an additional network plugged into\nthe text-to-image diffusion model to support visual conditions. InstructPix2Pix [4] is trained on a\ncomprehensive dataset tailored for image editing tasks. Despite their strengths, those models are\nlimited by their task-specific nature and do not exhibit the comprehensive perceptual understanding\nand generative capabilities required for a universal model in visual generation.\nIs it possible to address various image generation tasks, such as text-to-image, image editing,\ncontrollable generation, and image restoration, within a single diffusion framework, akin to\nhow GPT handles language tasks? If a universal model is available, the need for training additional\nmodules (e.g., ControlNet, IP-Adapter, T2I-Adapter) in practical applications can be eliminated.\nMotivated by this potential, we explore a unified framework for image generation, named OmniGen.\nUnlike popular diffusion models, OmniGen features a very concise structure, comprising only two\nmain components: a VAE and a transformer model, without any additional encoders. OmniGen\nsupports arbitrarily interleaved text and image inputs as conditions to guide image generation,\nrather than text-only or image-only conditions. To train a robust unified model, we construct the\nfirst large-scale unified image generation dataset X2I, which unifies various tasks into one format.\nAdditionally, we incorporate several classic computer vision tasks such as human pose estimation,\nedge detection, and image deblurring, thereby extending the model's capability boundaries and"}, {"title": "OmniGen", "content": "In this section, we present the details of OmniGen framework, including the model architecture and\ntraining method."}, {"title": "Model Design", "content": "Principles. Current diffusion models are typically limited to common text-to-image tasks and can not\nperform a broader range of downstream image-generation tasks. To achieve real-world applications,\nusers often need to design and integrate additional network structures to extend the capabilities of\ndiffusion models, making the models highly cumbersome. Even worse, these additional parameter\nnetworks are usually task-specific and can not be reused for other tasks, unless more networks\nare designed and trained for different functions. To circumvent these issues, the design principles\nof OmniGen are as follows: 1). Universality: accepting any form of image and text inputs for\nvarious tasks; 2). Conciseness, avoiding overly complex structural designs and numerous additional\ncomponents.\nNetwork Architecture. As illustrated in Figure 2, the OmniGen framework adopts an architecture\ncomprised of a Variational Autoencoder (VAE) [28] and a pre-trained large transformer model.\nSpecifically, VAE extracts continuous visual features from images, while the transformer model\ngenerates images based on input conditions. In this paper, we use the VAE from SDXL [52] and freeze\nit during training. We use Phi-3 [1] to initialize the transformer model, inheriting its excellent text\nprocessing capabilities. Unlike state-of-the-art diffusion models that require additional encoders to pre-\nprocess conditional information (such as clip text encoder and image encoder), OmniGen inherently\nencodes conditional information by itself, significantly simplifying the pipeline. Furthermore,\nOmniGen jointly models text and images within a single model, rather than independently modeling\ndifferent input conditions with separate encoders as in existing works [67; 68; 70; 63; 9] which lacks\ninteraction between different modality conditions.\nInput Format. The input to the model can be multimodal interleaved text and images in free form.\nWe utilize the tokenizer of Phi-3 to process text without any modifications. For images, we firstly\nemploy a VAE with a simple linear layer to extract latent representations. Then, they are flattened"}, {"title": "Attention Mechanism", "content": "Different from text, which can be decomposed into discrete tokens to model,\nwe argue that images should be modeled as a whole. Therefore, we modify the common causal\nattention mechanism in LLM, integrating it with the bidirectional attention as illustrated in Figure 2.\nSpecifically, we apply causal attention to each element in the sequence, but apply bidirectional\nattention within each image sequence. This allows each patch to pay attention to other patches within\nthe same image, while ensuring that each image can only attend to other images or text sequences\nthat have appeared previously.."}, {"title": "Inference", "content": "During inference, we randomly sample a Gaussian noise and then apply the flow matching\nmethod to predict the target velocity, iterating multiple steps to obtain the final latent representation.\nFinally, we use a VAE to decode the latent representation into the predicted image. The default\ninference step is set to 50. Thanks to the attention mechanism, OmniGen can accelerate inference like\nLLMs by using kv-cache: storing previous and current key and value states of the input conditions on\nthe GPU to compute attention without redundant computations."}, {"title": "Training Strategy", "content": "Train objective. In this work, we use rectified flow [41] to optimize the parameters of model.\nDifferent from DDPM [25], flow matching conducts the forward process by linearly interpolating\nbetween noise and data in a straight line. At the step t, $x_t$ is defined as\n$x_t = tx + (1 - t)\\epsilon,$\nwhere x is the original data, and $\\epsilon \\sim N(0,1)$ is the Gaussian noise. The model is trained to\ndirectly regress the target velocity given the noised data $x_t$, timestep t, and condition information c.\nSpecifically, the objective is to minimize the mean squared error loss:\n$L = E [||(x \u2013 \\epsilon) \u2013 v_\\theta (x_t, t, c)||^2].$\nFor image editing tasks, the objective is to modify specific regions of the input image while keeping\nother areas unchanged. Therefore, the difference between the input image and the target image is\noften small, which allows the model to learn an unexpected shortcut: simply copying the input image\nas the output to make the related training loss very low. To mitigate this phenomenon, we amplify\nthe loss in the regions of the image where changes occur. More specifically, we calculate the loss\nweights for each region based on these latent representations of input image x' and target image x:\n$W_{i,j} = \\begin{cases}\n    \\frac{1}{\\frac{1}{1}} & \\text{if } x_{i,j} = x'_{i,j} \\\\\n    \\frac{1}{\\|x-x'\\|^2} & \\text{if } x_{i,j} \\neq x'_{i,j}\n\\end{cases}$"}, {"title": "Training Pipeline", "content": "Following previous work [13; 18; 6], we gradually increase the image resolution\nduring the training process. Low resolution is data-efficient, while high resolution can enhance the\naesthetic quality of the generated images. Detailed information for each training stage is presented in\nTable 2.2. We adopt the AdamW [42] with \u03b2 = (0.9, 0.999) as the optimizer. All experiments are\nconducted on 104 A800 GPUs."}, {"title": "X21 Dataset", "content": "To achieve robust multi-task processing capabilities, it is essential to train models on large-scale and\ndiverse datasets. However, in the field of image generation, a readily available large-scale and diverse\ndataset has yet to emerge. In this work, we have constructed a large-scale unified image generation\ndataset for the first time, which we refer to as the X2I dataset, meaning \"anything to image\". We\nhave converted these data into a unified format, and Figure 3 presents some examples from the X2I\ndataset. The entire dataset comprises approximately 0.1 billion images. We will provide a detailed\ndescription of the composition of this dataset in the following sections."}, {"title": "Text to Image", "content": "The input for this subset of data is plain text. We have obtained multiple open-source datasets from\nvarious sources: Recap-DataComp [35](a subset of 56M images), SAM-LLaVA [6], ShareGPT4V [7],\nLAION-Aesthetic [58](a subset of 4M images), ALLaVA-4V [5], DOCCI [47], DenseFusion [36]\nand JourneyDB [60]. While these datasets are large in quantity, their image quality is not always high\nenough. In the early stages of training, we use them to learn a broad range of image-text matching\nrelationships and diverse knowledge. After stage 3, we utilize our internal collection of 16 million\nhigh-quality images to enhance the aesthetic quality of the generated images. A lot of studies [13; 6]\nhave demonstrated that synthetic detailed captions can greatly improve text-to-image models trained\nat scale. Therefore, we use the InternVL2 [11] to create synthetic annotations for internal data and\nLAION-Aesthetic (the other datasets come with detailed text descriptions and do not require further\nannotation)."}, {"title": "Multi-modal to Image", "content": "Different from most existing diffusion models, our model can accept more general and flexible\nmultimodal instruction as conditions to guide the generation of images."}, {"title": "Common Mixed-modal Prompts", "content": "The input of this portion of data is arbitrarily interleaved text and images. We collect the data\nfrom multiple tasks and sources: image editing (SEED-Data-Edit [19], MagicBrush [72], and\nInstructPix2Pix [4]), human motion (Something-Something [23]), virtual try-on (HR-VITON [31]\nand FashionTryon [75]), and style transfer (stylebooth [24]). We standardize all tasks into the\ninput-output pair format as shown in Figure 3-(b).\nThe issue of utilizing additional visual conditions for finer-grained spatial control has garnered\nwidespread attention[73; 33]. We employ the MultiGen [53] dataset to learn this function, and select\nsix representative visual conditions: Canny, HED, Depth, Skeleton, Bounding Box, and segmentation."}, {"title": "Subject-driven Image Generation", "content": "We constructed both a large-scale foundational dataset (GRIT-Entity dataset) and a high-quality\nadvanced dataset (Web Images dataset) for subject-driven image generation. For the GRIT-Entity\ndataset, we leveraged the GRIT dataset [51], which annotates object names within images. Using these\nannotations, we applied the Grounding DINO model [40] for text-to-bounding-box grounding. Based\non the bounding boxes, we employed SAM [29] to segment the cropped images, obtaining object\nmasks. We further used the MS-Diffusion model [64] to repaint the object images, enhancing data\nquality. The process of data construction and the final instruction format are illustrated in Figure 4-(a).\nThrough this method, we acquired 6 million pairs.\nAlthough the GRIT-based approach provides a substantial amount of data, the input data extracted\ndirectly from original images can lead the model to fall into simple copy-paste patterns. To fully\nunleash the subject-driven image generation capability of OmniGen, we constructed a high-quality\nweb images training dataset using natural images of well-known individuals. First, we sampled 20\nmillion Alt-text entries from the Datacomp dataset [14] and used spaCy\u00b3 for named entity recognition.\nWe selected the most frequently occurring names and employed GPT-40 to filter out real, notable\nindividuals, resulting in 2,000 names. Furthermore, we expanded the initial 2,000 names by including\nclosely related individuals, resulting in approximately 10,000 name pairs. We then scraped images\nof these individuals and pairs from search engines. Due to the noise in web images, where scraped\nimages may not contain the specified individuals, we designed a cross-verification strategy using\nInternVL [11] to filter single and group images, as detailed in Figure 4-(b). The retained single\nand group images were then captioned with details such as attire and actions. Through additional"}, {"title": "Computer Vision Tasks", "content": "We introduce classic computer vision tasks to enhance the image generation capabilities of the\nmodel. For low-level vision tasks (low-light image enhancement [66], deraining [71], deblurring [46],\ninpainting [53], outpainting [53] and colorization [58]), where the annotation itself is an image, we\nonly add text instructions, which were randomly sampled from instructions generated by GPT-40.\nFor high-level tasks, we choose to represent all annotations as images. We used LAION [58] as the\nsource image and annotations from [53] as the target to construct image pairs (such as source image\nand its human pose mapping). The annotations include human pose, depth mapping, canny, and\nsegmentation. Additionally, we also use several datasets for referring image segmentation, including\nRefCOCO [27], ADE20k [76], and ReasonSeg [30]. As shown in Figure 3-(c), the input is the\nsource image and a natural language expression, the output is an image with the corresponding object\nhighlighted in blue.\nThe purpose of constructing these datasets is not merely to endow the model with these functionalities.\nWe also aim to transfer the knowledge acquired from these traditional computer vision tasks to\nimage generation tasks, thereby achieving more sophisticated image generation capabilities. Our\nexperiments have also demonstrated that multi-task learning enables the model to exhibit emergent\nabilities."}, {"title": "Few-shot to Image", "content": "We constructed a few-shot to image dataset to stimulate the model's in-context learning capabilities.\nSpecifically, for each task described in the preceding sections, we randomly selected a few examples\nand combined the original input with these examples to form new inputs. The specific data format\ncan be referenced in Figure 3-(e). Due to limitations in training resources, we opted to use only one\nexample to enhance training efficiency."}, {"title": "Experimental Results", "content": "In this section, we show the results of OmniGen in image generation tasks and traditional vision\ntasks."}, {"title": "Image Generation", "content": null}, {"title": "Qualitative Results", "content": "Figure 5 shows the results of the text-to-image task. It can be observed that OmniGen effectively\nfollows the textual descriptions to generate images with arbitrary aspect ratios.\nFigure 6 presents the outcomes of the subject-driven generation task. Our model can extract the\nrequired objects from the given reference images and generate new images accordingly. Furthermore,\nwhen the reference image contains multiple objects, the model can directly select the needed objects\nbased on textual instructions (e.g., the cat in the figure) without requiring additional preprocessing\nsteps such as image cropping or face recognition.\nFigure 7 summarizes the results of other image generation tasks, demonstrating that the model can\nhandle various downstream tasks based on multi-modal instructions."}, {"title": "Text to Image", "content": "Following [13], we evaluate text-to-image generation capability of OmniGen on the GenEval [22]\nbenchmark. We compared the performance of our model with the reported results of other popular\nimage generation models, as summarized in Table 2. Surprisingly, our model achieved similar perfor-\nmance compared to the current state-of-the-art diffusion models, such as SD3, which underscores\nthe effectiveness of our framework. The GenEval benchmark does not reflect the aesthetic quality of\nimages, we will leave this aspect for future evaluation."}, {"title": "Image Edit", "content": "We compare OmniGen with other state-of-the-art image editing models on EMU-Edit [59] dataset,\nwhich includes seven different operations: background alteration, comprehensive image changes,\nstyle alteration, object removal, object addition, localized modifications, and color/texture alterations.\nWe measure three metrics: 1) CLIP-I: CLIP image similarity between the source image and output\nimage; 2) DINO: DINO similarity between the source image and output image; and 3) CLIP-T: CLIP\ntext-image similarity between edited image and target caption. DINO and CLIP-I similarity scores\nmeasure the model's ability to preserve elements from the source image, while CLIP-T measures how\nwell the model followed the instructions. As shown in Table 2, our model significantly outperforms\nInstructPix2Pix [4], and exhibits comparable performance to the current state-of-the-art model:\nEMU-Edit [59]."}, {"title": "DreamBooth", "content": "We evaluate the single-entity subject-driven generation capability on DreamBench [57]. The Dream-\nBench contains 750 prompts for 30 subjects (e.g., dog and toy). For each prompt, we generate 4\nimages, resulting in a comprehensive evaluation set of 3,000 images. Following Kosmos-G [49], we\nonly select one image as input from the 4-7 provided images for each subject. We adopted DINO\nand CLIP-I from DreamBooth to assess subject fidelity, and CLIP-T for text fidelity. All results\nare summarized in Table 4. Compared to methods based on fine-tuning, our approach maintains a\ncomparable level of text fidelity while better preserving the subject from the source image. Compared\nwith models without fine-tuning, OmniGen significantly outperforms both Re-Imagen and Kosmos-G,\nand demonstrates superior subject fidelity relative to SuTI."}, {"title": "Visual Conditional Controls", "content": "Image-based prompts can provide detailed spatial conditioning controls for diffusion models. To\nevaluate this ability of OmniGen, we use the dataset and script from [33]. This benchmark includes\nADE20K test dataset for segmentation mask condition, and evaluation split of MultiGen-20M for\ncanny edge map, hed edge map, and depth map condition. For each condition, the controllability is\nevaluated by measuring the similarity between the input conditions and the extracted conditions from\ngenerated images of diffusion models. The experimental results are shown in Table 5. We can find\nthat our model achieves optimal results on segmentation mask and hed edge map conditions, and\nobtains competitive results for canny edge map and depth map conditions."}, {"title": "Computer Vision Tasks", "content": "We present several qualitative results of computer vision tasks in Figure 8. OmniGen can handle\nvarious low-level vision tasks such as deraining, deblurring, and inpainting. In the bottom of Figure 8,\nwe can see that OmniGen is also able to handle high-level tasks, such as human pose recognition and\ndepth estimation.\nSo far, we have demonstrated that our model can generate images well based on visual conditions\nwhile also extracting visual conditions from raw images. This motivates us to ponder: can we directly\nuse the model to generate new images based on a reference image in only one step, instead of first\nusing a processor to extract spatial condition information and then inputting it into the model for\ngeneration? Surprisingly, even without having encountered such a task before, OmniGen handles it\nadmirably. As shown in Figure 9, the existing workflow for ControlNet involves using a detector to\nextract spatial condition information from the reference image, and then loading the corresponding\ncontrol module to model the spatial condition information for image generation, which requires\nmultiple network components and operations. Now, only based on our model, we can directly input\nthe reference image and text instruction (e.g., Follow the depth mapping of this image <reference-\nimage> to generate new image. The text description for new image is \"...\") to generate an image\nin only one step without any additional intermediate procedures. It can be observed that the model\ncomprehends the instruction well; when tasked with using the human pose from the reference image,"}, {"title": "Further Analysis", "content": "LLMs demonstrate remarkable generalization capabilities, achieving impressive performance in\npreviously unseen tasks and domains. Furthermore, they can boost performance through mechanisms\nsuch as in-context learning and chain of thought. We observe similar functionalities in OmniGen as\nwell, and present our findings in this section."}, {"title": "Emgerging Capabilities", "content": "By standardizing all tasks into a unified format and training on X2I dataset, OmniGen can acquire\nuniversal knowledge and allow knowledge transfer across different scenarios and tasks, thus enabling\nthe generation capabilities on unseen tasks and domains. We illustrate several emerging capabilities\nusing the following examples.\nTask Composition. In real-world applications, user requirements often involve combinations of tasks.\nAs shown in Figure 10-(a), our model is capable of simultaneously processing multiple instructions,\nincluding those for different tasks (Image inpainting and change the color of hair to white) as well as\nmultiple instructions for the same task (Add a sunglasses to the man's face, and change the color of\nclothes to blue). These results highlight our model's versatility and potential for widespread adoption\nin the wild.\nImplicit Combination of Tasks. In addition to explicit task combinations, our model is capable of\nperforming multiple tasks implicitly through a single instruction. As demonstrated in Figure 9, upon\nreceiving the input like \"follow the human pose/depth mapping to generate the image: ...\", our model"}, {"title": "Reasoning Ability", "content": "We have explored the reasoning capabilities of the model and presented the results in Figure 11. As\nshown in the left half of Figure 11, when given an instruction without explicitly specifying the object,\nsuch as \"Where can I wash my hands? Please help me find the right place in \\image_1|\", the model\ncan recognize image contents and infer that a sink is needed. Consequently, the model identifies and\nindicates the area of the sink in the image. This functionality creates potential applications in the field\nof embodied intelligence, assisting intelligent agents in comprehending multi-modal instructions,\nlocating necessary objects and planning subsequent actions. Moreover, the right half of Figure 11\ndemonstrates that after inferring the target object, the model can also perform editing operations on it.\nIf no object matches, the model will refrain from editing any unrelated objects."}, {"title": "Chain of Thought", "content": "The Chain-of-Thought (CoT) method can significantly boost the performance of LLMs by decompos-\ning the task into multiple steps and sequentially solving each step to obtain an accurate final answer.\nWe consider whether a similar alternative can be applied to image generation. Inspired by the basic\nway of human drawing, we hope to mimic the step-by-step drawing process, iteratively refining the"}, {"title": "Limitations and Discussions", "content": "Figure 13 illustrates several typical failure cases of the current model. We summarize the limitations\nof the current model as follows:\n\u2022 Similar to existing diffusion models, OmniGen is sensitive to text prompts. Typically,\ndetailed text descriptions result in higher-quality images.\n\u2022 The current model's text rendering capabilities are limited; it can handle short text segments\nbut fails to accurately generate longer texts. Additionally, due to resource constraints, the\nnumber of input images during training is limited to a maximum of three, preventing the\nmodel from handling long image sequences.\n\u2022 The generated images may contain erroneous details, especially small and delicate parts. In\nsubject-driven generation tasks, facial features occasionally do not fully align. OmniGen\nalso sometimes generates incorrect depictions of hands.\n\u2022 OmniGen cannot process unseen image types (e.g., image for surface normal estimation).\nWe believe that most limitations can be addressed by training the model on more related data.\nMoreover, compared to most models, fine-tuning OmniGen for downstream tasks is simpler, as it"}, {"title": "Related Work", "content": "inherently supports various image generation tasks without the need for extensive efforts and costs to\nbuild additional networks."}, {"title": "Generative Foundation Models", "content": "The generative foundation model serves as the core of many contemporary artificial intelligence\nsystems, revolutionizing the way machines interact with humans. The GPT series [54; 48] have\ndemonstrated that language models can learn numerous tasks via training on a large-scale dataset.\nFollowing this trend, the rise of large language models (LLMs) [43; 3; 1] has further showcased\ntheir versatility, adeptly performing various tasks such as question answering, text summarization,\nand code generation within a single framework. Beyond language, multimodal large language\nmodels [39; 12] have been proposed to integrate vision and language capabilities. For example,\nas a typical architecture and popular trend, LLaVA [39] equips the LLM with visual perception\nand understanding capabilities by linking the vision encoder to the LLM through a connector layer.\nThese models have shown impressive performance in vision-language understanding tasks. However,\ndespite their ability to handle mixed text and image inputs, they lack the capability to generate\nimages. The construction of a universal foundation model for image generation remains unclear\nand has not been fully explored. In this work, we propose a universal generative model that accepts\narbitrary interleaved multimodal inputs and generates images, marking a significant stride towards a\ngeneral-purpose image generation foundation model.\nRecently, some works have explored unified models that support both text and image generation.\nIn Chameleon [61], images and texts are both tokenized into the token sequence and modeled via\ndiscrete autoregressive modeling. Concurrent works such as TransFusion [77] and Show-O [69]\nunify diffusion and autoregressive methods into a single model, generating text autoregressively\nand images through diffusion. Nonetheless, like most existing diffusion models, they can only\nperform text-to-image tasks and cannot handle more complex and various visual generation tasks.\nThe unification of tasks in visual generation remains unexplored. Unlike these efforts, our current\nfocus is on the unification of diverse visual generation tasks. The model is capable of performing\nvarious tasks, including text-to-image generation, image editing, subject-driven generation, virtual\ntry-on, image deblurring, human pose recognition, and more. To the best of our knowledge, this is\nthe first model capable of unifying such a wide range of visual generation tasks. Building on this\nfoundation, further expansion into text generation is planned as the next step in the research agenda."}, {"title": "Diffusion Model", "content": "Recent advancements in diffusion models have been remarkable, with notable contributions from the\nStable Diffusion series [56; 52; 13], DALL-E [55], and Imagen [26]. These models are predominantly\ndesigned for text-to-image generation tasks. To facilitate visual-conditioned generation, approaches\nsuch as ControlNet [73] and T2i-Adapter [45] introduce supplementary networks integrated into\nexisting text-to-image models, thereby enabling them to accommodate image-based conditions.\nStyleShot [17] incorporates a style-sensitive encoder to manipulate the style feature of the output\nimages. InstructPix2Pix [4] addresses image editing by augmenting the model with additional input\nchannels. SEED-X [20] and Kosmos-G [49] employ an MLLM to replace the CLIP encoder in\nSD, improve the performance on the specific downstream task. However, these methods are task-\nspecific, extending the capabilities of SD by modifying the model architecture. In contrast, OmniGen\nis a model that natively supports various image generative tasks, unifying all tasks into a single\nframework. Multi-task learning enhances the model's capabilities and also leads to the emergence of\nnew abilities. Furthermore, when addressing various real-world tasks, OmniGen no longer requires\nany preprocessing steps or assistance from other models.\nThere is some work exploring the unification of computer vision (CV) tasks [2; 65; 16; 21]. However,\nthese efforts primarily focus on classic vision tasks and do not support general image generation tasks.\nAdditionally, current models often underperform compared to those specifically designed and trained\nfor corresponding tasks, limiting their practical applications in real-world scenarios. In our work,\nthe introduction of CV tasks plays a crucial role in enabling the model to learn general knowledge,\nthereby enhancing its image-generation capabilities and fostering the emergence of new abilities. For\ninstance, incorporating the human pose estimation task has led to the model's ability to generate new"}]}