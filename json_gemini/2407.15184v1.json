{"title": "Decoding Multilingual Moral Preferences:\nUnveiling LLM's Biases Through the Moral Machine Experiment", "authors": ["Karina Vida", "Fabian Damken", "Anne Lauscher"], "abstract": "Large language models (LLMs) increasingly find their way\ninto the most diverse areas of our everyday lives. They indi-\nrectly influence people's decisions or opinions through their\ndaily use. Therefore, understanding how and which moral\njudgements these LLMs make is crucial. However, morality\nis not universal and depends on the cultural background. This\nraises the question of whether these cultural preferences are\nalso reflected in LLMs when prompted in different languages\nor whether moral decision-making is consistent across differ-\nent languages. So far, most research has focused on investi-\ngating the inherent values of LLMs in English. While a few\nworks conduct multilingual analyses of moral bias in LLMs in\na multilingual setting, these analyses do not go beyond atomic\nactions. To the best of our knowledge, a multilingual analysis\nof moral bias in dilemmas has not yet been conducted.\nTo address this, our paper builds on the moral machine ex-\nperiment (MME) to investigate the moral preferences of five\nLLMs, Falcon, Gemini, Llama, GPT, and MPT, in a multilin-\ngual setting and compares them with the preferences collected\nfrom humans belonging to different cultures. To accomplish\nthis, we generate 6500 scenarios of the MME and prompt the\nmodels in ten languages on which action to take. Our analysis\nreveals that all LLMs inhibit different moral biases to some de-\ngree and that they not only differ from the human preferences\nbut also across multiple languages within the models them-\nselves. Moreover, we find that almost all models, particularly\nLlama 3, divert greatly from human values and, for instance,\nprefer saving fewer people over saving more.", "sections": [{"title": "Introduction", "content": "Morality and the question of the right action have ac-\ncompanied humanity throughout history (Aristotle ca. 350\nB.C.E/2020; Hursthouse and Pettigrove 2022). With the emer-\ngence of large language models (LLMs), the topic is now of\nparticular interest to the natural language processing (NLP)\ncommunity and is becoming increasingly popular (Vida, Si-\nmon, and Lauscher 2023).\nHumans engage with LLMs in several ways in discussions\nabout morality. For example, models can make moral judge-\nments about situations (e.g., Alhassan, Zhang, and Schlegel\n2022), provide advice on moral issues (e.g., Zhao et al. 2021),\nand extract moral beliefs from texts (e.g., Botzer, Gu, and\nWeninger 2023; Pavan et al. 2023). LLMs have long since\nfound their way into our daily lives\u00b9 and various domains\n(Zhao et al. 2023), particularly through easily accessible and\nwidely used chat models such as ChatGPT (Brown et al.\n2020) or Gemini (Google 2024). Since LLMs are (also)\ntrained on human-generated data such as books and newspa-\nper articles (Zhao et al. 2023) which contain moral values and\njudgements, it can be assumed that these models also have a\nmoral bias. Consequently, LLMs can directly influence peo-\nple morally (e.g., by advising them in moral situations), and\ntheir intrinsic underlying moral bias leads to the possibil-\nity that they can also indirectly influence people outside of\nexplicit moral issues (Kr\u00fcgel, Ostermaier, and Uhl 2023)."}, {"title": "Related Work", "content": "Most of the work based on the MME focuses on the ethi-\ncal and social implications of autonomous driving vehicles\n(e.g., Bigman and Gray 2020; Mill\u00e1n-Blanquel, Veres, and\nPurshouse 2020). Closest to our paper is the work of Take-\nmoto (2024), who also applies the MME to LLMs. Unlike\nour work, however, they concentrate on fewer models and\nonly take English into account.\nIn the realm of NLP, so far, several studies have focused\non the moral bias of models. Some works analyse the moral\ndimensions of BERT in detail using atomic actions in both\nEnglish (Haemmerl et al. 2023; Schramowski et al. 2019)\nand multilingual context (H\u00e4mmerl et al. 2022). Furthermore,\nScherrer et al. (2023) investigate the moral beliefs of LLMs in\nspecially created moral scenarios, which, similar to the MME,\ngive the models two choices. Benkler et al. (2023) base their\nassessment of models on the World Value Survey and also\ncompare different cultural identities of LLMs. Other works\ndeal with the prominent Delphi Model (Jiang et al. 2021)\nand examine in detail the underlying moral dispositions and\npreferences (e.g., Fraser, Kiritchenko, and Balkir 2022; Talat\net al. 2022-07, 2021).\nAnother series of works is concerned with investigating\ncultural differences of LLMs. Arora, Kaffee, and Augenstein\n(2023) systematically investigate the extent to which social,\npolitical, and cultural values in pre-trained language models\nvary between cultures (Arora, Kaffee, and Augenstein 2023).\nA detailed analysis of the inherent cultural values that char-\nacterise ChatGPT was carried out by Cao et al. (2023) and\nfound that ChatGPT is very strongly oriented towards West-\nern (American) values. Multilingual studies focussing on the\nArabic language were carried out by Naous et al. (2024).\nThey were able to show that the tested language models were\nnot able to culturally detach themselves from Western values."}, {"title": "On Morality and Machines", "content": "Assessing the moral bias of LLMs is an essential part of\nmachine and AI ethics. As such, it is a subfield of applied\nethics and deals both with the possibility of designing al-\ngorithms and machines that \"mimic, simulate, generate, or\ninstantiate ethical sensitivity, learning, reasoning, argument,\nor action\" (Guarini 2013), as well as the concerns associated\nwith such technological artefacts (M\u00fcller 2020). One chal-\nlenge facing developers and researchers of such algorithms is\nthe lack of ground truth in moral judgements (Vida, Simon,\nand Lauscher 2023). It is, therefore, unclear which values\nshould influence and be incorporated into the models.\nThe Moral Machine Experiment. To address these con-\ncerns regarding the question of correct behaviour for au-\ntonomous vehicles, Awad et al. designed the MME, which is\nbased on a modification of the trolley problem (Foot 1967).\nOn an online platform, called the moral machine (MM),\nusers are presented with 13 randomly generated scenarios,\neach composed of distinct outcomes (profiles), and asked\nabout their moral preferences. The MM generates randomised scenar-\nios using the nine factors: inaction versus action (inaction\nfactor), sparing pedestrians versus passengers (pedestrian\nfactor), sparing women versus men (gender factor), sparing\nthe fit versus the less fit (fitness factor), sparing the lawful\nversus jaywalking (lawful factor), sparing those with higher\nsocial status versus those with lower social status (social sta-\ntus factor; e.g., female and male executives versus criminals,\nhomeless people, and women and men without a particu-\nlar role), sparing the young versus the elderly (age factor),\nsparing more lives versus fewer lives (count factor), sparing\nhumans versus pets (species factor). In their paper, Awad"}, {"title": "Methodology", "content": "In this section, we cover how we obtain the data that we\nanalyse, as well as how we perform the analysis.\nTo assess the moral bias of different LLMs and how it\ndiffers from actual human moral preferences, we prompt\nmultiple different LLMs with typical scenarios presented by\nthe MM in different languages. Concretely, we perform the\nfollowing steps to attain the relevant data: first, we generate\nthe scenarios, we then translate the instruction prompt into all\nused languages before we prompt the models which action to\ntake. Finally, we perform an analysis following the work of\nAwad et al. (2018).\nIn the following sections, we describe each of these steps\nin greater detail as well as how we selected the models and\nlanguages to evaluate.\nWe evaluate the MME on the following models: Falcon (7B- Instruct, 40B-Instruct, 180B-Chat) (Almazrouei et al. 2023), Gemini 1.0 Pro (Google 2024), Llama 2 (7B-Chat, 13B- Chat, 70B-Chat) (Touvron et al. 2023), Llama 3 (8B-Instruct, 70B-Instruct), GPT 3.5 Turbo, and MPT (7B-Chat, 30B- Chat) (Team 2023b,a). We chose these models as they are widely used, have reported usability in multilingual settings (Holtermann et al. 2024), and are easily accessible.\nIn terms of languages, we prompt all models in Arabic (ar), German (de), English (en), Spanish (es), French (fr), Japanese (ka), Korean (ko), Portuguese (pt), Russian (ru), and Chinese (zh). These are exactly the languages officially supported by the MM website. As we reuse their translations, this support is crucial.\nIn the MM, the user faces 13 moral dilemmas in a row (one such row of scenarios is called a session). Each session con- sists of two possible outcomes (called profiles; see section 3 for a more detailed description of such scenarios). These scenarios are randomly generated (where 12 scenarios are generated to check for specific preferences, e.g., male versus female, and an additional scenario is generated completely randomly). For best comparability, we leverage the MM's code to generate these scenarios. Hence, we can guarantee to have the same data distribution. We generate 500 sessions with 13 scenarios each, resulting in 6500 scenarios.\nSince we want to evaluate the moral biases for the tested LLMs in a multilingual setting, we need multilingual scenar- ios. For this, we use the translations from the MM as they would be presented to a user in the previously mentioned languages. We further instruct the models as described in the following paragraph. To translate this instruction, native speakers translated our initial English instruction into their native language (see table 7 in appendix B). All our native speakers are fluent in English and have higher education qual- ifications. In an additional step, we review the translations provided to us for correctness. For this, we use a common machine translator."}, {"title": "Results", "content": "Overall Results. Generally, the tested models react very\ndifferently to the experiments. Table 3 shows the proportion\nof invalid responses per model across all languages. Due to\nthe high non-evaluable response rate (up to 100% for all\nlanguages for Llama-2), we excluded Falcon 40B-Instruct,\nFalcon 180B-Chat, all Llama 2 models from all subsequent\nanalyses. The two Falcon models did not comply with the\nsystem prompt (e.g., they answered with Chinese characters\ninstead of \"1\" or \"2\"). With Llama 2, on the other hand, the\nimplemented safety measures blocked all of our prompts. For\ninstance, one of the responses generated by Llama 2 was\nI cannot make decisions that result in harm or death\nto any living being, including pedestrians, animals, or\ncriminals. [...]\nAcross all models, all Llama 3 models, Gemini 1.0 Pro, and\nGPT 3.5 Turbo show the lowest invalid session rate.\nWe also report the invalid session proportions for the indi-\nvidual languages across all models (table 2). We observe the\nhighest invalid session proportions in Arabic (38%), Korean\n(30%), Portuguese (30%) and Russian (28%). For example,\nGemini 1.0 Pro blocks many of the Arabic prompts as they\nare considered dangerous. We hypothesise that this finding\nis tied to the quality of the language-specific representation\nspaces. Furthermore, the models often responded with the\nrespective characters in Korean and Chinese scenarios rather\nthan \"1\" or \"2\u201d. Corresponding scenarios are also labelled"}, {"title": "Discussion", "content": "We now discuss our findings, answer our research questions\nand discuss further implications. We start with a general\ndiscussion of behaviour that is consistent across all models.\nFirst, we found that all models are slightly biased towards\nsaving men over saving women across all clusters. However,\nwe must note that this bias is slight.\nSecond, all models except Llama 8B-Instruct seem not to\nconsider whether a character is fit or unfit or whether they\nare elderly or young.\nThird, all models seem to prefer sparing the passengers\nover pedestrians, which differs from the MME results where\nhumans would rather spare pedestrians. One reason for this\nmight be that humans consider the deaths their fault and\nwould rather sacrifice themselves for their mistakes rather\nthan running over others. Conversely, an autonomous car is\nat fault for both cases and would instead save its passengers.\nInterestingly, no models in any language show a preference\nfor action versus inaction. This is similar to the MME results\nand suggests that the common issue of the trolley experiment\n(\"If I change lanes, I am actively running over people, and\nthus I do nothing.\") is not as prevalent as usually thought.\nGiven our results, we can now go back to our research ques-\ntions and formulate answers for them.\n(RQ1) Do LLMs exhibit biases reflected through their\npreferences when faced with moral dilemmas in au-\ntonomous driving scenarios? Yes. As the various radar\ncharts from section 5 and table 4 show, the models have a\nmoral bias to varying degrees (except for Falcon 7B-Instruct,\nMPT 7B-Chat, and MPT 30B-Chat which show no moral\nbias).\nLlama 3 70B-Instruct is the model with the most pro-\nnounced preferences. While Gemini 1.0 Pro, GPT 3.5 Turbo,\nand Llama 3 8B-Instruct have a similar moral bias, which\nis differently enunciated in each case, Llama 3 70B-Instruct\nclearly stands out regarding the reported preferences. This\nis unexpected regarding the other Llama models: despite\nthe same training data, the moral bias of Llama 3 70B-\nInstruct and Llama 3 8B-Instruct differs significantly across\nthe various factors. Llama 3 70B-Instruct's bias is particu-\nlarly surprising since Llama 2's safety mechanisms blocks\nthe prompts as it focuses on safety and was designed with a\n\u201cno danger or harm\u201d-policy in mind. In our analysis, however,\nLlama 3 is the model with the most concise moral preferences.\nInterestingly, Llama 3 70B-Instruct shows no utilitarian pref-\nerences across all three culture clusters and tends to run over\nmore people rather than fewer when given the choice, as well\nas rather running over humans than pets. The model shows\na marginal tendency to prefer deontological behaviour and\nsave rule-following individuals in the Western and Eastern\nclusters.\nThe remaining three models (Gemini 1.0 Pro, GPT 3.5\nTurbo, and Llama 3 8B-Instruct) are similar in their moral\npreferences but have different degrees of preference in each\ncase. Gemini 1.0 Pro favours deontological preferences, es-\npecially in the Southern and Western clusters. In addition, it\nis the only model that slightly favours humans rather than\nanimals. This suggests that this may be an intrinsically hard-\ncoded value. Otherwise, the model is balanced in its bias."}, {"title": "Conclusion and Future Work", "content": "In this paper, we investigated whether (RQ1) LLMs ex-\nhibit moral preferences concerning the behaviour of an au-\ntonomous car, (RQ2) whether the moral bias depends on\nthe prompted language, and (RQ3) whether the moral bias\nreflects the respective cultural moral disposition of people\nspeaking the language. We conclude that the answers to these\nquestions are yes, yes, and no, respectively. Moreover, we\ndefine the term moral bias for LLMs and define moral con-\nsistency. We conclude that LLMs turn out not to be morally\nconsistent in that they have different moral preferences de-\npending on the prompted language.\nWhile most models possess moral preferences and culture-\ndependent moral bias are eminent, they do not align with\nhuman biases found in the MME. Strikingly, we found that\nsome models, in particular Llama 3 70B-Instruct, exhibit\nimmoral behaviour such as running over as many characters\nas possible or saving pets over humans.\nTo summarise, we can say that one shall not entrust an\nLLM with decisions that could result in harm. In particular,\nLlama 3 70B-Instruct shows a stark preference towards action\nthat is against widespread ethical considerations. Moreover,\none shall not expect the same moral bias of an LLM in differ-\nent languages and neither expect the moral bias of an LLM\nto align with a culture's beliefs.\nThere are a couple of possible extension points for future\nwork. It would be interesting to see how well an LLM can\nadapt to a different culture by changing the system prompt,\ne.g., to \"You are a self-driving car in Portugal [...].\" This\ncould reveal further biases present in the model that are not\nrevealed by language alone. Furthermore, comparing the lan-\nguage clusters (fig. 2) to linguistic features (e.g., language\nfamilies, left-to-right text, etc.) could reveal interesting pat-\nterns."}, {"title": "Limitations", "content": "As our experiments are based on the MME, our work heavily\ndepends on it. This results in limitations for our paper. Since,\nunlike in the MME, we only look at languages and not de-\nmographic backgrounds, the Southern cluster only consists\nof the Spanish language. In general, by clustering differ-\nent languages into one large culture (Western, Eastern, and\nSouthern), individual subtleties of the various subordinate\ncultures and languages can also be lost, as with generalisa-\ntion. Consequently, the clustering might be noisy. Further\nresearch should incorporate the various cultural aspects into\nthe prompts for the LLMs at a more granular level and in-\nvestigate how responses and moral bias behave. Moreover,\nrepeating the same experiment with different system prompt\nformulations may reveal biases that we did not account for."}]}