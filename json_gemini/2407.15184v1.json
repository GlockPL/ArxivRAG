{"title": "Decoding Multilingual Moral Preferences: Unveiling LLM's Biases Through the Moral Machine Experiment", "authors": ["Karina Vida", "Fabian Damken", "Anne Lauscher"], "abstract": "Large language models (LLMs) increasingly find their way into the most diverse areas of our everyday lives. They indirectly influence people's decisions or opinions through their daily use. Therefore, understanding how and which moral judgements these LLMs make is crucial. However, morality is not universal and depends on the cultural background. This raises the question of whether these cultural preferences are also reflected in LLMs when prompted in different languages or whether moral decision-making is consistent across different languages. So far, most research has focused on investigating the inherent values of LLMs in English. While a few works conduct multilingual analyses of moral bias in LLMs in a multilingual setting, these analyses do not go beyond atomic actions. To the best of our knowledge, a multilingual analysis of moral bias in dilemmas has not yet been conducted.\nTo address this, our paper builds on the moral machine experiment (MME) to investigate the moral preferences of five LLMs, Falcon, Gemini, Llama, GPT, and MPT, in a multilingual setting and compares them with the preferences collected from humans belonging to different cultures. To accomplish this, we generate 6500 scenarios of the MME and prompt the models in ten languages on which action to take. Our analysis reveals that all LLMs inhibit different moral biases to some degree and that they not only differ from the human preferences but also across multiple languages within the models themselves. Moreover, we find that almost all models, particularly Llama 3, divert greatly from human values and, for instance, prefer saving fewer people over saving more.", "sections": [{"title": "Introduction", "content": "Morality and the question of the right action have accompanied humanity throughout history (Aristotle ca. 350 B.C.E/2020; Hursthouse and Pettigrove 2022). With the emergence of large language models (LLMs), the topic is now of particular interest to the natural language processing (NLP) community and is becoming increasingly popular (Vida, Simon, and Lauscher 2023).\nHumans engage with LLMs in several ways in discussions about morality. For example, models can make moral judgements about situations (e.g., Alhassan, Zhang, and Schlegel 2022), provide advice on moral issues (e.g., Zhao et al. 2021), and extract moral beliefs from texts (e.g., Botzer, Gu, and Weninger 2023; Pavan et al. 2023). LLMs have long since found their way into our daily lives\u00b9 and various domains (Zhao et al. 2023), particularly through easily accessible and widely used chat models such as ChatGPT (Brown et al. 2020) or Gemini (Google 2024). Since LLMs are (also) trained on human-generated data such as books and newspaper articles (Zhao et al. 2023) which contain moral values and judgements, it can be assumed that these models also have a moral bias. Consequently, LLMs can directly influence people morally (e.g., by advising them in moral situations), and their intrinsic underlying moral bias leads to the possibility that they can also indirectly influence people outside of explicit moral issues (Kr\u00fcgel, Ostermaier, and Uhl 2023)."}, {"title": "Related Work", "content": "Most of the work based on the MME focuses on the ethical and social implications of autonomous driving vehicles (e.g., Bigman and Gray 2020; Mill\u00e1n-Blanquel, Veres, and Purshouse 2020). Closest to our paper is the work of Takemoto (2024), who also applies the MME to LLMs. Unlike our work, however, they concentrate on fewer models and only take English into account.\nIn the realm of NLP, so far, several studies have focused on the moral bias of models. Some works analyse the moral dimensions of BERT in detail using atomic actions in both English (Haemmerl et al. 2023; Schramowski et al. 2019) and multilingual context (H\u00e4mmerl et al. 2022). Furthermore, Scherrer et al. (2023) investigate the moral beliefs of LLMs in specially created moral scenarios, which, similar to the MME, give the models two choices. Benkler et al. (2023) base their assessment of models on the World Value Survey and also compare different cultural identities of LLMs. Other works deal with the prominent Delphi Model (Jiang et al. 2021) and examine in detail the underlying moral dispositions and preferences (e.g., Fraser, Kiritchenko, and Balkir 2022; Talat et al. 2022-07, 2021).\nAnother series of works is concerned with investigating cultural differences of LLMs. Arora, Kaffee, and Augenstein (2023) systematically investigate the extent to which social, political, and cultural values in pre-trained language models vary between cultures (Arora, Kaffee, and Augenstein 2023). A detailed analysis of the inherent cultural values that characterise ChatGPT was carried out by Cao et al. (2023) and found that ChatGPT is very strongly oriented towards Western (American) values. Multilingual studies focussing on the Arabic language were carried out by Naous et al. (2024). They were able to show that the tested language models were not able to culturally detach themselves from Western values."}, {"title": "On Morality and Machines", "content": "Assessing the moral bias of LLMs is an essential part of machine and AI ethics. As such, it is a subfield of applied ethics and deals both with the possibility of designing algorithms and machines that \"mimic, simulate, generate, or instantiate ethical sensitivity, learning, reasoning, argument, or action\" (Guarini 2013), as well as the concerns associated with such technological artefacts (M\u00fcller 2020). One challenge facing developers and researchers of such algorithms is the lack of ground truth in moral judgements (Vida, Simon, and Lauscher 2023). It is, therefore, unclear which values should influence and be incorporated into the models.\nThe Moral Machine Experiment. To address these concerns regarding the question of correct behaviour for autonomous vehicles, Awad et al. designed the MME, which is based on a modification of the trolley problem (Foot 1967). On an online platform, called the moral machine (MM), users are presented with 13 randomly generated scenarios, each composed of distinct outcomes (profiles), and asked about their moral preferences. The MM generates randomised scenarios using the nine factors: inaction versus action (inaction factor), sparing pedestrians versus passengers (pedestrian factor), sparing women versus men (gender factor), sparing the fit versus the less fit (fitness factor), sparing the lawful versus jaywalking (lawful factor), sparing those with higher social status versus those with lower social status (social status factor; e.g., female and male executives versus criminals, homeless people, and women and men without a particular role), sparing the young versus the elderly (age factor), sparing more lives versus fewer lives (count factor), sparing humans versus pets (species factor). In their paper, Awad"}, {"title": "Methodology", "content": "In this section, we cover how we obtain the data that we analyse, as well as how we perform the analysis.\nTo assess the moral bias of different LLMs and how it differs from actual human moral preferences, we prompt multiple different LLMs with typical scenarios presented by the MM in different languages. Concretely, we perform the following steps to attain the relevant data: first, we generate the scenarios, we then translate the instruction prompt into all used languages before we prompt the models which action to take. Finally, we perform an analysis following the work of Awad et al. (2018).\nIn the following sections, we describe each of these steps in greater detail as well as how we selected the models and languages to evaluate.\nWe evaluate the MME on the following models: Falcon (7B-Instruct, 40B-Instruct, 180B-Chat) (Almazrouei et al. 2023), Gemini 1.0 Pro (Google 2024), Llama 2 (7B-Chat, 13B-Chat, 70B-Chat) (Touvron et al. 2023), Llama 3 (8B-Instruct, 70B-Instruct), GPT 3.5 Turbo, and MPT (7B-Chat, 30B-Chat) (Team 2023b,a). We chose these models as they are widely used, have reported usability in multilingual settings (Holtermann et al. 2024), and are easily accessible.\nIn terms of languages, we prompt all models in Arabic (ar), German (de), English (en), Spanish (es), French (fr), Japanese (ka), Korean (ko), Portuguese (pt), Russian (ru), and Chinese (zh). These are exactly the languages officially supported by the MM website. As we reuse their translations, this support is crucial.\nIn the MM, the user faces 13 moral dilemmas in a row (one such row of scenarios is called a session). Each session consists of two possible outcomes (called profiles; see section 3 for a more detailed description of such scenarios). These scenarios are randomly generated (where 12 scenarios are generated to check for specific preferences, e.g., male versus female, and an additional scenario is generated completely randomly). For best comparability, we leverage the MM's code to generate these scenarios. Hence, we can guarantee to have the same data distribution. We generate 500 sessions with 13 scenarios each, resulting in 6500 scenarios.\nSince we want to evaluate the moral biases for the tested LLMs in a multilingual setting, we need multilingual scenarios. For this, we use the translations from the MM as they would be presented to a user in the previously mentioned languages. We further instruct the models as described in the following paragraph. To translate this instruction, native speakers translated our initial English instruction into their native language (see table 7 in appendix B). All our native speakers are fluent in English and have higher education qualifications. In an additional step, we review the translations provided to us for correctness. For this, we use a common machine translator."}, {"title": "Results", "content": "Generally, the tested models react very differently to the experiments. Table 3 shows the proportion of invalid responses per model across all languages. Due to the high non-evaluable response rate (up to 100% for all languages for Llama-2), we excluded Falcon 40B-Instruct, Falcon 180B-Chat, all Llama 2 models from all subsequent analyses. The two Falcon models did not comply with the system prompt (e.g., they answered with Chinese characters instead of \"1\" or \"2\"). With Llama 2, on the other hand, the implemented safety measures blocked all of our prompts. For instance, one of the responses generated by Llama 2 was\nAcross all models, all Llama 3 models, Gemini 1.0 Pro, and GPT 3.5 Turbo show the lowest invalid session rate.\nWe also report the invalid session proportions for the individual languages across all models (table 2). We observe the highest invalid session proportions in Arabic (38%), Korean (30%), Portuguese (30%) and Russian (28%). For example, Gemini 1.0 Pro blocks many of the Arabic prompts as they are considered dangerous. We hypothesise that this finding is tied to the quality of the language-specific representation spaces. Furthermore, the models often responded with the respective characters in Korean and Chinese scenarios rather than \"1\" or \"2\u201d. Corresponding scenarios are also labelled"}, {"title": "Discussion", "content": "We now discuss our findings, answer our research questions and discuss further implications. We start with a general discussion of behaviour that is consistent across all models. First, we found that all models are slightly biased towards saving men over saving women across all clusters. However, we must note that this bias is slight.\nSecond, all models except Llama 8B-Instruct seem not to consider whether a character is fit or unfit or whether they are elderly or young.\nThird, all models seem to prefer sparing the passengers over pedestrians, which differs from the MME results where humans would rather spare pedestrians. One reason for this might be that humans consider the deaths their fault and would rather sacrifice themselves for their mistakes rather than running over others. Conversely, an autonomous car is at fault for both cases and would instead save its passengers. Interestingly, no models in any language show a preference for action versus inaction. This is similar to the MME results and suggests that the common issue of the trolley experiment (\"If I change lanes, I am actively running over people, and thus I do nothing.\") is not as prevalent as usually thought.\n(RQ1) Do LLMs exhibit biases reflected through their preferences when faced with moral dilemmas in autonomous driving scenarios? Yes. As the various radar charts from section 5 and table 4 show, the models have a moral bias to varying degrees (except for Falcon 7B-Instruct, MPT 7B-Chat, and MPT 30B-Chat which show no moral bias).\nLlama 3 70B-Instruct is the model with the most pronounced preferences. While Gemini 1.0 Pro, GPT 3.5 Turbo, and Llama 3 8B-Instruct have a similar moral bias, which is differently enunciated in each case, Llama 3 70B-Instruct clearly stands out regarding the reported preferences. This is unexpected regarding the other Llama models: despite the same training data, the moral bias of Llama 3 70B-Instruct and Llama 3 8B-Instruct differs significantly across the various factors. Llama 3 70B-Instruct's bias is particularly surprising since Llama 2's safety mechanisms blocks the prompts as it focuses on safety and was designed with a \u201cno danger or harm\u201d-policy in mind. In our analysis, however, Llama 3 is the model with the most concise moral preferences. Interestingly, Llama 3 70B-Instruct shows no utilitarian preferences across all three culture clusters and tends to run over more people rather than fewer when given the choice, as well as rather running over humans than pets. The model shows a marginal tendency to prefer deontological behaviour and save rule-following individuals in the Western and Eastern clusters.\nThe remaining three models (Gemini 1.0 Pro, GPT 3.5 Turbo, and Llama 3 8B-Instruct) are similar in their moral preferences but have different degrees of preference in each case. Gemini 1.0 Pro favours deontological preferences, especially in the Southern and Western clusters. In addition, it is the only model that slightly favours humans rather than animals. This suggests that this may be an intrinsically hard-coded value. Otherwise, the model is balanced in its bias."}, {"title": "Conclusion and Future Work", "content": "In this paper, we investigated whether (RQ1) LLMs exhibit moral preferences concerning the behaviour of an autonomous car, (RQ2) whether the moral bias depends on the prompted language, and (RQ3) whether the moral bias reflects the respective cultural moral disposition of people speaking the language. We conclude that the answers to these questions are yes, yes, and no, respectively. Moreover, we define the term moral bias for LLMs and define moral consistency. We conclude that LLMs turn out not to be morally consistent in that they have different moral preferences depending on the prompted language.\nWhile most models possess moral preferences and culture-dependent moral bias are eminent, they do not align with human biases found in the MME. Strikingly, we found that some models, in particular Llama 3 70B-Instruct, exhibit immoral behaviour such as running over as many characters as possible or saving pets over humans.\nTo summarise, we can say that one shall not entrust an LLM with decisions that could result in harm. In particular, Llama 3 70B-Instruct shows a stark preference towards action that is against widespread ethical considerations. Moreover, one shall not expect the same moral bias of an LLM in different languages and neither expect the moral bias of an LLM to align with a culture's beliefs.\nThere are a couple of possible extension points for future work. It would be interesting to see how well an LLM can adapt to a different culture by changing the system prompt, e.g., to \"You are a self-driving car in Portugal [...].\" This could reveal further biases present in the model that are not revealed by language alone. Furthermore, comparing the language clusters (fig. 2) to linguistic features (e.g., language families, left-to-right text, etc.) could reveal interesting patterns."}, {"title": "Limitations", "content": "As our experiments are based on the MME, our work heavily depends on it. This results in limitations for our paper. Since, unlike in the MME, we only look at languages and not demographic backgrounds, the Southern cluster only consists of the Spanish language. In general, by clustering different languages into one large culture (Western, Eastern, and Southern), individual subtleties of the various subordinate cultures and languages can also be lost, as with generalisation. Consequently, the clustering might be noisy. Further research should incorporate the various cultural aspects into the prompts for the LLMs at a more granular level and investigate how responses and moral bias behave. Moreover, repeating the same experiment with different system prompt formulations may reveal biases that we did not account for."}]}