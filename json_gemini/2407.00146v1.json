{"title": "The Qiyas Benchmark: Measuring ChatGPT Mathematical and Language Understanding in Arabic", "authors": ["Shahad Al-Khalifa", "Hend Al-Khalifa"], "abstract": "Despite the growing importance of Arabic as a global language, there is a notable lack of language models pre-trained exclusively on Arabic data. This shortage has led to limited benchmarks available for assessing language model performance in Arabic. To address this gap, we introduce two novel benchmarks designed to evaluate models' mathematical reasoning and language understanding abilities in Arabic. These benchmarks are derived from a General Aptitude Test (GAT) called Qiyas exam, a standardized test widely used for university admissions in Saudi Arabia. For validation purposes, we assess the performance of ChatGPT-3.5-trubo and ChatGPT-4 on our benchmarks. Our findings reveal that these benchmarks pose a significant challenge, with ChatGPT-4 achieving an overall average accuracy of 64%, while ChatGPT-3.5-trubo achieved an overall accuracy of 49% across the various question types in the Qiyas benchmark. We believe the release of these benchmarks will pave the way for enhancing the mathematical reasoning and language understanding capabilities of future models tailored for the low-resource Arabic language.", "sections": [{"title": "I. INTRODUCTION", "content": "Evaluating the capabilities of large language models (LLMs) across different tasks like mathematical reasoning and natural language understanding is a critical area of research as these general-purpose AI systems become more widely used. Developing comprehensive evaluation benchmarks, especially for languages beyond English, is crucial for driving model improvement.\nArabic, a Semitic language with complex morphology and written from right-to-left, is spoken by over 400 million people across the Arab world [1]. Despite its status as a global language of importance, Arabic is considered low-resource in the field of natural language processing [2]. There is currently a shortage of LLMs pre-trained exclusively on large Arabic datasets. This has resulted in limited benchmarks available for robustly assessing Arabic LLM performance. While some prior work has translated English benchmarks, there is a need for high-quality, natively developed Arabic evaluation resources.\nThe lack of robust, Arabic-native benchmarks focused on key capabilities like math reasoning and language understanding represents a significant gap in the field. Having professionally designed evaluation resources in this domain could accelerate the development of higher- performing Arabic language models. This work makes two key contributions to address this need:\n1) It introduces Qiyas, a benchmark suite for comprehensively evaluating LLM performance on mathematical and language tasks in the Arabic language. Qiyas consists of two components a quantitative section assessing math skills and a verbal section evaluating Arabic language understanding abilities.\n2) Using the Qiyas benchmark, the performance of the latest ChatGPT models (versions 3.5-turbo and 4) is extensively evaluated across zero-shot, one-shot, and few- shot settings to establish strong Arabic language baselines.\nThese benchmarks are derived from the Qiyas exam, a standardized exam widely used for university admissions in Saudi Arabia, ensuring their quality has been validated by educational experts. The results shed light on current LLM limitations for the Arabic language and highlight the impact of varied training data and prompting approaches. The release of Qiyas paves the way for advancing Arabic LLMs' mathematical reasoning and language understanding capabilities on these challenging, nationally representative Arabic tasks.\nThe remainder of the paper is structured as follows: The 'Background and Literature Review' section provides an overview of the Qiyas exam and discusses previous efforts in evaluating large language models (LLMs) in standardized exams. The 'Dataset Description' section describes the dataset used in the study. The 'Evaluation Approach' section outlines the methodology used to evaluate the performance of ChatGPT on the quantitative section assessing math skills and the verbal section evaluating Arabic language understanding abilities. The 'Results and Discussion' section delves into the benchmark results, analyzing the performance of both ChatGPT-3.5-turbo and ChatGPT-4 models. Finally, the 'Conclusion' section summarizes the key findings of the paper and provides an outlook on future research."}, {"title": "II. BACKGROUND AND LITERATURE REVIEW", "content": "The National Centre for Assessment (aka Qiyas) is a significant institution in education and assessment. It is responsible for conducting standardized tests to assess the scholastic achievement of students applying for universities [3].\nQiyas is responsible for developing and implementing over 90 standardized and professional tests for the public and private sectors. It has over 1,500 test models and an item bank of over 230,000 questions. The tests consist of two sections: the verbal and the quantitative, focusing on students' analytical and deductive skills, helping them assess their learning capacity. The center also provides linguistic tests, including the English language efficiency test and the Arabic language test for non-native speakers. Additionally, it presents an assessment test for talented and creative students, as well as vocational tests, the most important of which is the Vocational Standards Test for Teachers [4].\nRecent studies have evaluated the performance of large language models like ChatGPT on standardized exams across different domains. In the medical domain, ChatGPT has shown promising results, with studies indicating that it has reached the standard of passing third-year medical student exams [5]. Furthermore, research has demonstrated ChatGPT's success in passing the gold-standard US medical exam, suggesting significant potential applications in medicine [6]. Additionally, ChatGPT has been compared to other Al models, such as Bard, demonstrating the potential of Al models to match or even exceed human standards in tasks like processing and applying medical knowledge at a postgraduate level [7].\nIn the educational domain, ChatGPT has excelled in standardized tests such as the Test of Understanding in College Economics, scoring in the 91st to 99th percentile [8]. Furthermore, studies have highlighted ChatGPT's proficiency in various standardized admission tests in the UK, showcasing its potential as an innovative tool for education and test preparation [9]. The model has also shown capabilities in history exams and has been compared to students' scores, indicating a commendable level of proficiency in the subject [10].\nAs for the Arabic language, Alkaoud [11] introduces a new benchmark for evaluating large language models in English and Arabic. The author built an evaluation dataset based on the General Aptitude Test (GAT) to measure the linguistic capabilities of LLMs. The study demonstrates that ChatGPT-4's Arabic capabilities are significantly better than ChatGPT's.\nIn summary, while large language models show promising results on various exams, there remains a need for robust, natively developed Arabic benchmarks to rigorously evaluate mathematical reasoning and language understanding abilities tailored for the Arabic context."}, {"title": "III. DATASET DESCRIPTION", "content": "The Qiyas exam includes two sections: quantitative and verbal, as mentioned in the previous section. All questions in both sections are of multiple choice with four choices for each question. The quantitative section comprises of questions to test students' intellectual abilities in math, geometry, algebra, and data analysis. The verbal section comprises of questions to test students' linguistic abilities in semantic relations, linguistic structures, and comprehension [3].\nIn the quantitative section, there are four types of questions, as outlined by [12]:\n1) Math: Transforming verbal statements into solvable equations that involves basic arithmetic operations such as addition, subtraction, multiplication, and division.\n2) Geometry: Applying geometric formulas and principles encompassing properties of triangles, area computations, angle measurements, and related concepts.\n3) Algebra: Analyzing and resolving a set of algebraic equations or expressions to find the numerical value of an unknown variable, discern numerical sequences and patterns, among other related concepts.\n4) Statistics: Applying fundamental principles in probability theory and statistics that involves utilizing mathematical concepts to analyze, interpret data, and make predictions.\nIn the verbal section, there are five types of questions:\n1) Reading Comprehension: Comprehending reading passages and responding to questions that pertain to the content of the passage.\n2) Sentence Completion: Extracting the appropriate word from the choices to complete a sentence with a missing word.\n3) Contextual Error: Identifying the contextual discrepancy in the sentence and pinpointing the word whose meaning contradicts the overall meaning of the sentence (The error is not a spelling or grammar error).\n4) Verbal Analogy: Recognizing the connection between the two words in the question, then evaluating them based on analogous choices provided.\n5) Anomalous Word: Detecting the distinct word that is not related to the connected choices by a particular association.\nThe appendix shows an example of each question type with its translation to English. The questions used in the evaluation were written by domain experts experienced in designing and grading Qiyas exams. Figure 1 shows the distribution of quantitative and verbal questions in the dataset, resulting in a total of 2,407 questions. In the quantitative section, the number of questions related to math and algebra surpasses those in geometry and statistics. The reason is that math and algebra questions do not necessitate reliance on charts or plots for answering. Unlike geometry and statistics questions, which often involve visual representations. We focused on questions that do not rely on visual representations, as indicated by a previous study [13], which revealed that ChatGPT-4 struggled to retain and process visual information, highlighting the necessity of adding image descriptions in the evaluation of ChatGPT-4. It is important to note that the Qiyas exam does not include image descriptions, emphasizing that the objective of the evaluation aims to mirror the examination process of students. On the other hand, the verbal section exhibits a relatively balanced distribution of question types, except for reading comprehension, which demonstrates a lower prevalence."}, {"title": "IV. EVALUATION APPROACH", "content": "Our evaluation approach starts by formulating a prompt for each question within our dataset. The prompts used were the exact prompts in Arabic utilized in the official Qiyas exam as provided by authorized guides [12]. This approach also aligns with the examination methodology experienced by students, mitigating the risk of injecting our own subjective influences into the prompts. Due to ChatGPT's tendency to generate lengthy explanations for questions, which complicates the process of extracting the answer, we have introduced the instruction \"Write the answer only\" in the prompt. This measure is intended to ensure that only the answer is provided without additional explanation. While ChatGPT-4 complied with this directive, ChatGPT-3.5 persisted in including explanations in most answers, thus failing to adhere to the specified command.\nIn the evaluation phase, we employed OpenAI's API (ChatGPT-3.5-turbo and ChatGPT-4) to prompt and extract corresponding answers [14]. We initiated the evaluation of the models by employing zero-shot prompts, but we subsequently extended it by incorporating one-shot and 3-shot prompts. This adjustment was made to investigate the impact of varying prompt complexities on the model's performance and to explore how providing additional context influences the model's responses. The examples used in both the one-shot and 3-shot prompts remained consistent across all questions. Figure 2 provides an example of the prompt methodology used, noting that the prompts were originally in Arabic but translated to English for clarity purposes."}, {"title": "V. RESULTS AND DISCUSSION", "content": "Table I displays the results of our experiments on ChatGPT- 4 and ChatGPT-3.5-turbo. The evaluation metric used is the accuracy (See equation 1).\n\\documentclass{article}\n\\usepackage{amsmath}\n\\[\\begin{aligned}\n\\text{Accuracy of Question Type A} = \\frac{\\text{Number of Correct Answers in A}}{\\text{Total Number of Questions in A}} \\times 100 \\tag{1}\n\\end{aligned}\\]\n**A. Quantitative Section**\nIn the quantitative section, ChatGPT-4 excelled in math and statistics with zero-shot prompts, indicating its strong capability in answering these types of questions without providing additional context. However, in geometry and algebra, ChatGPT-4 exhibited its peak performance with an accuracy of 81% and 63% respectively when presented with 3-shot prompts, suggesting that its capabilities were optimized when provided with more context, enabling it to leverage additional information to enhance its proficiency in these areas.\nOn the other hand, ChatGPT-3.5-turbo's accuracy did not surpass 65% across all prompt settings. Its peak performance of 65% accuracy was attained in geometry with the 3-shot prompt. Notably, ChatGPT-3.5-turbo achieved its best accuracy in statistics questions when provided with 3-shot prompts, contradicting ChatGPT-4's results for the same question type, where it excelled with zero-shot prompts. The contradictory results observed could be attributed to differences in their training data and model architectures. ChatGPT-4's larger training corpus and advanced architecture might enable it to leverage patterns and context more effectively in zero-shot settings, while ChatGPT-3.5- turbo benefits from additional contextual information provided in few-shot prompts.\n**B. Verval Section**\nIn the verbal section, ChatGPT-4 demonstrated notable proficiency in reading comprehension, achieving an accuracy peak of 80% with one-shot and 3-shot prompts. This showcases that providing context and examples can positively influence the results in language-related tasks. However, it is worth noting that ChatGPT-4 also excelled in reading comprehension with zero-shot prompts, indicating its strong language understanding capabilities even without supplementary examples. We believe that these exceptional results were achieved due to the nature of the question and its dependency on the passage to draw the connections required to answer questions accurately. Following closely, sentence completion yielded an accuracy of 74% with the 3-shot prompt.\nConversely, ChatGPT-3.5-turbo exhibited its highest accuracy across most question types when employing one- shot prompts, except for anomalous word, where it performed best with 3-shot prompt, and contextual error where it achieved the same accuracy for all prompt settings.\n**C. Summary**\nThe overall results show that ChatGPT-4 outperforms ChatGPT-3.5-turbo in a wide variety of linguistic and mathematical domains with a total average accuracy of 64%, whereas GPT-3.5-turbo achieved an average total accuracy of 49% in all prompt settings.\nCompared to Alkaoud benchmark study on the Arabic language [11], our dataset size surpasses his study's dataset, which comprised of only 468 Arabic verbal questions, whereas our dataset comprised of 2,407 both quantitative and verbal questions. This larger dataset enables a more robust evaluation of these models' capabilities across different question types and prompts. Alkaoud followed a comparison approach between the Arabic and English language with zero-shot prompt setting, whereas we focused solely on the Arabic language with different prompt settings to evaluate the models' performance. In line with Alkaoud's findings, ChatGPT-4 demonstrated superior performance in reading comprehension, achieving 74% accuracy, while our results achieved 77% accuracy in the same question type with zero- shot prompts. In our experiments in the verbal section, the lowest accuracy achieved with zero-shot prompts was in the contextual error question type, reaching an accuracy of 56%. In contrast, Alkaoud's research achieved a higher accuracy of 63.37% in the same question type. We suspect that this variance in results could be attributed to Alkaoud utilizing a publicly accessible dataset, suggesting that ChatGPT might have been trained on it, while our dataset remains non-public."}, {"title": "VI. ERROR ANALYSIS", "content": "To gain a deeper understanding of the errors made by both models and to identify any patterns or common error types, we conducted a comprehensive error analysis on the zero- shot results for both ChatGPT-4 and ChatGPT-3.5-turbo. The analysis, generated with the assistance of ChatGPT-40 [14], aimed to categorize the errors and provide detailed insights into the specific challenges faced by each model. We also evaluated the performance of another language model, Gemini-pro by Google [15], on the questions that were incorrectly answered by ChatGPT-4 and ChatGPT-3.5-turbo to determine if alternative models could perform better. Table II showcases the zero-shots error analysis results with the most common error types for each section.\n**A. Quantitative Section Error Analysis**\nIn the quantitative section, Algebra questions resulted in the highest error rate for both models. Both models exhibited difficulty in solving complex equations for the missing variable. Additionally, they struggled with identifying the correct relationship (>, <, or =) between various algebraic expressions. This highlights the need for further development and incorporation of more diverse training data encompassing complex algebraic equations. Conversely, the models performed exceptionally well on problems involving simple and direct equations, suggesting that both models are adept at handling straightforward scenarios that lack complex transformations.\nThe analysis of statistical tasks revealed a more nuanced picture. ChatGPT-4 achieved a significantly lower error rate compared to ChatGPT-3.5-turbo. Notably, ChatGPT-3.5- turbo encountered specific difficulties with probability questions and problems involving combinatorics.\n**B. Verbal Section Error Analysis**\nIn the verbal section, Contextual Error questions resulted in the highest error rate for ChatGPT-4. Both models faced difficulty in differentiating between synonymous answer choices and comprehending the deeper context of the sentence. Our assumption suggests that synonyms might share similar statistical properties that may challenge the model in distinguishing the correct word in a specific context.\nOn the other hand, Verbal Analogy questions resulted in the highest error rate for ChatGPT-3.5-turbo. These questions require identifying the closest relationship between two given words from a set of answer choices. The difficulty appears to stem from the inherent ambiguity within the answer choices themselves. Since each answer choice likely shares some form of connection to the original word pair, the model struggles to pinpoint the most precise analogy. Further research is needed to explore how LLMs can be better equipped to handle tasks that require reasoning about subtle semantic relationships between words.\n**C. Gemini-pro Results**\nTo evaluate the performance of other language models on the incorrectly answered questions by ChatGPT-4 and ChatGPT-3.5-turbo, we have provided the same questions to Gemeni-pro by Google [15] and its response were compared to the originally incorrect outputs from ChatGPT-4 and ChatGPT-3.5-turbo.\nTable III summarizes the evaluation results. We can see that Gemini-pro demonstrated promising performance in correctly answering most questions. Notably, Gemini-pro excelled in the Reading Comprehension question type, suggesting a strong capability for leveraging relevant background information for response generation.\nHowever, Gemini-pro's performance on verbal analogy questions was lower. Verbal analogy questions demand the model to grasp the relationship between word pairs and identify another pair with a similar connection. This task can be challenging for LLMs, as it necessitates not only understanding individual word meanings but also the intricate ways words can relate to each other. Interestingly, both ChatGPT-4 and ChatGPT-3.5-turbo also exhibited lower performance on this question type, potentially indicating a general limitation in current LLM technology.\nIt is worth noting that Gemini-pro showed strength in following instructions by responding with the answer only without explanation or additional context. Unlike ChatGPT- 4 and ChatGPT-3.5-turbo, which occasionally included extraneous information in their responses, Gemini-pro consistently provided only the answer to the question, as instructed."}, {"title": "VII. CONCLUSION AND FUTURE WORK", "content": "This research paper introduces the Qiyas benchmark, a novel evaluation framework developed to comprehensively assess the mathematical reasoning and language understanding capabilities of large language models (LLMs) in the Arabic language. The Qiyas benchmark is a standardized General Aptitude Test (GAT) used for university admissions in Saudi Arabia, ensuring its quality and relevance to real-world assessment.\nThe key findings of this paper are:\n1) ChatGPT-4 outperformed ChatGPT-3.5-turbo across both the quantitative (math) and verbal (language) sections of the benchmark. This suggests that the newer, more advanced model has made notable progress in Arabic language understanding and mathematical reasoning compared to its predecessor.\n2) The performance of the models varied depending on the prompt setting (zero-shot, one-shot, 3-shot). In general, providing more contextual information through one-shot and 3-shot prompts improved the models' accuracy, particularly in the verbal section tasks like reading comprehension.\n3) The results highlight the current limitations of state-of- the-art LLMs in handling the complexities of the Arabic language, including its unique morphology and writing system. This underlines the need for more Arabic- focused training data and model development efforts to enhance the mathematical and linguistic capabilities of future Arabic LLMs.\nThe release of the Qiyas benchmark represents a significant contribution to the field, as it provides a robust, standardized evaluation framework for assessing the capabilities of Arabic language models. This resource can drive the development of more capable Arabic LLMs by serving as a benchmark for progress and identifying specific areas requiring further research and improvement. Future work includes expanding the dataset to include image-based questions, enabling the evaluation of multimodal models' ability to integrate language and visual understanding for Arabic-based tasks. Additionally, assessing a wider range of state-of-the-art LLMs on the Qiyas benchmark will provide a more comprehensive understanding of the current capabilities and limitations of Arabic language AI systems. Overall, this work lays the foundation for advancing the state-of-the-art in Arabic language understanding and reasoning for large language models."}]}