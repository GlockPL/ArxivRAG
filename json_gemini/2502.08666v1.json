{"title": "Hallucination, Monofacts, and Miscalibration: An Empirical Investigation", "authors": ["Muqing Miao", "Michael Kearns"], "abstract": "Recent theoretical work [Kalai and Vempala, 2024] proves that a particular notion of hallucination rate in LLMs must be lower bounded by the training data monofact rate (related to the classical Good-Turing missing mass estimator) minus model miscalibration. Through systematic experiments with n-gram models and in-context learning with LLMs, we empirically investigate and validate this theory by examining how different underlying data distributions affect the monofact rate and a model's tendency to hallucinate. We then vary model miscalibration through controlled upweighting of training sample while holding monofact rates constant, allowing us to isolate miscalibration's reduction effect on hallucination. These findings suggest that both the distribution of fact frequencies in training data and the calibration-hallucination trade-off are inherent to probabilistic language generation. Our results also suggest that current practices of aggressive deduplication in training data may need to be reconsidered, as selective duplication could serve as a principled mechanism for reducing hallucination.", "sections": [{"title": "Introduction", "content": "Consider using a language model to write a biography of a living person. The model may confidently state that \"John Smith was born in Seattle in 1982, earned his PhD from Stanford in 2008, and now leads AI research at Tech Corp,\" but these \"facts\" could be completely fabricated. Such hallucinations (plausible but verifiably false statements) are a critical issue for language models. When lawyers have submitted hallucinated legal cases or doctors have received incorrect medical advice, the consequences can be severe [McCoy et al., 2024, Shin, 2023].\nA natural question is: Why do modern language models hallucinate? While issues like outdated training data or adversarial prompts contribute to hallucination, recent theoretical work demonstrates a more fundamental cause: calibrated language models must hallucinate at a rate related to the prevalence of rare facts in their training data [Kalai and Vempala, 2024]. Specifically, they prove that for arbitrary facts whose veracity cannot be systematically determined from training data, the hallucination rate has a statistical lower bound tied to the fraction of facts that appear exactly once in training (the monofact rate) minus model miscalibration (a measure of overfitting). Importantly, this theoretical relationship suggests a practical approach for reducing hallucination that does not require knowledge of the true distribution: controlled duplication of training examples can induce beneficial miscalibration while maintaining model performance.\nIn this paper, we provide the first systematic empirical investigation of this theoretical framework. Using carefully controlled experiments with bigram models and in-context learning with language models, we examine how different distributions over fact frequencies influence monofact and hallucination rates. We then develop an algorithm to induce varying levels of miscalibration while holding monofact constant in order to isolate miscalibration's impact on hallucination. Our study provides three main contributions:\nFirst, we show that training data constructed using Pareto or Zipf distributions create patterns where certain facts appear much more frequently than others. These patterns can produce widely varying monofact rates by adjusting underlying parameters that influence such frequency, and we demonstrate that these variations directly drive corresponding changes in hallucination rates. In contrast, data frequencies constructed using Gaussian and Poisson distributions tend to produce more uniform fact frequencies, resulting in a narrower range of monofact rates and limiting their usefulness for exploring this relationship.\nSecond, we introduce an upweighting algorithm that induces controlled miscalibration by selectively duplicating transition counts for subsets of training examples in our n-gram models. For a chosen number k of training examples, our algorithm incrementally adds one additional count to both their initial state probabilities and bigram transition frequencies, effectively amplifying their contribution to the model. By systematically varying k while maintaining constant monofact rates, we demonstrate that hallucination reduction is associated with increased levels of miscalibration. Our experimental results show that even selective upweighting of a small fraction of training examples can substantially reduce hallucination rates, providing a practical mechanism for controlling the calibration-hallucination trade-off.\nThird, we extend these findings to LLMs through in-context learning experiments, showing that the relationship between monofact rates and hallucination persists even in real world LLMs.\nOur results have meaningful implications for addressing hallucination in language models. They demonstrate that two fundamental factors drive a model's tendency to hallucinate: the distribution of fact frequencies in the training data (which directly affects monofact rate) and the model calibration (the model's ability to generalize).\nMore importantly, our findings reveal a fundamental tension in language model optimization: while traditional machine learning approaches emphasize maintaining calibration, our results indicate that perfect calibration may actually impede a model's ability to reliably generate factual content. Some degree of miscalibration appears necessary to reduce hallucination rates effectively.\nThis insight has implications for training data preparation. While current practices emphasize aggressive deduplication of training data [Lee et al., 2021], our experiments demonstrate that controlled upweighting of training examples can induce beneficial miscalibration and reduce hallucination rates. This provides theoretical justification for selective reintroduction of duplicates during post-training and suggests a new direction for model development: strategically introducing duplication where it most effectively reduces hallucination while maintaining overall model performance."}, {"title": "Related Work", "content": "An important line of theoretical research shows that when training data contains many rare examples, memorization becomes indispensable for achieving low error. Feldman [2020] demonstrates that for such distributions with frequent rare occurrences, any algorithm aiming for small error must memorize a substantial fraction of these uncommon instances. This observation connects directly to the theoretical lower bound on hallucination, which shows that well-calibrated language models must hallucinate at least at the rate of facts that appear only once in training [Kalai and Vempala, 2024]. In other words, when many facts occur infrequently, models face a tension between avoiding hallucinations and maintaining calibration: the scarceness of certain facts forces either deeper memorization or an increased risk of fabricated outputs.\nRecent empirical work reinforces the link between unfamiliar examples and hallucination. Kang et al. [2023] introduce a \"familiarity score\" that quantifies how closely a model's training data matches a test example, and they observe that hallucination rates grow almost linearly with unfamiliarity. Their findings echo the notion of \"monofacts\" by highlighting how rare or singly observed statements pose the greatest challenge for accurate prediction. Gekhman et al. [2024] similarly show that novel facts are learned more slowly by language models, and that integrating these facts tends to elevate hallucination rates in a linear fashion. Both studies underscore the inherent difficulty models face when handling sparse knowledge or atypical data.\nCollectively, these works suggest that hallucination arises in part from the basic statistical structure of real-world datasets, especially when they include many low-frequency facts. The more the underlying distribution skews toward these rare observations, the higher the baseline for hallucinations becomes."}, {"title": "n-Gram Model Methodology", "content": "Our experimental framework utilizes n-gram language models with a focus on bigrams. We craft a simplified but rigorous setting while closely following the assumptions described by Kalai and Vempala [2024] in Section 4.1 Factoid Assumptions."}, {"title": "Experimental Methodology", "content": "We employ bigram models as our primary experimental framework for several reasons. First, bigram models provide a simplified yet theoretically grounded setting. Second, they represent one of the most fundamental statistical approaches to language modeling [Shannon, 1948], capturing local word dependencies through conditional probabilities. While modern LLMs are significantly more complex, bigram models share the core characteristic of learning probability distributions over sequences [Bengio et al., 2003, Jurafsky and Martin, 2000], making them ideal for studying the fundamental tension between monofact and hallucination.\nWe have experimented with higher-order n-grams and found that increased model capacity generally leads to lower hallucination rates while holding other factors constant, and the fundamental relationships between monofact rate, miscalibration, and hallucination remain consistent across different orders. We chose bigrams as our primary model as they provide sufficient model capacity for our experimental setup which involves just 6-word sequences, while offering the clearest demonstration of these relationships without the added complexity of higher-order dependencies.\nIn addition, we design our data to be comma-separated sequences without filler words to test bigram behavior in its purest form. This simplified setting helps isolate the core statistical phenomena, as low-order n-grams typically generate gibberish when trained on natural language with unstructured filler words.\nWhile this controlled setting may appear artificial, we later demonstrate in Section 4 and 5 that our key findings about the relationships between monofact rates and hallucination generalize to experiments with real-world LLMs on more complicated statements."}, {"title": "Definitions and Main Theorem", "content": "To investigate hallucination in a controlled setting, we need to carefully define what constitutes a fact versus a hallucination. Consider the statement \"Timoth\u00e9e Chalamet starred in Dune: Part Two directed by Denis Villeneuve.\" This is a true movie fact. However, if our model generates \"Timoth\u00e9e Chalamet starred in Dune: Part Two directed by Jack Black,\" this would be a hallucination a plausible but false statement. Our framework formalizes this distinction and allows us to measure how often such hallucinations occur.\nIn our setting, we consider U the universe of all possible comma-separated movie-related statements. Within this universe, some statements are true movie facts (our set T), while the remaining statements are false (our set F), allowing us to precisely identify when the model generates hallucinations. Following the theoretical framework\u00b9, we define:\n\u2022 U: The universe of all possible statements, consisting of 6-entity comma-separated sequences in the form \"Actor, Costar, Movie, Director, Genre, Year\". For example, any combination of these entities forms a possible statement in our universe\n\u2022 $T \\subset U$: The set of true movie statements (facts) from our universe these are the sequences that represent actual movies with their correct actors, directors, genres and years\n\u2022 $F \\subset U$: The set of false statements (hallucinations), where $F = U \\setminus T$\n\u2022 $S \\subset T$: The training dataset. A subset of T that the model is exposed to during training\n\u2022 $H = U \\setminus S$: The set of unobserved statements that could be true or false\n\u2022 G: The set of generated statements by the bigram model post-training\n\u2022 p: The true probability distribution over T, which we vary as a part of our experiment\n\u2022 g: The bigram model's probability distribution over T\nThe key theoretical result establishes a lower bound on hallucination [Kalai and Vempala, 2024]:\n$f_{gen} \\geq MF - Mis(g, p) - \\frac{3e^{-m}}{n} - \\frac{6ln(6/\\delta)}{\\delta}$"}, {"title": "Dataset Construction", "content": "We construct our T dataset through a multi-stage process using the IMDb's non-commercial datasets [IMDb, 2024].\n1. First, we utilize 20,000 unique movie facts from IMDb's datasets to create our base set of true facts. Each fact is a 6-entity comma-separated sequence as defined in 2.2. One example sequence is: \"Timoth\u00e9e Chalamet, Zendaya, Dune: Part Two, Denis Villeneuve, Science Fiction, 2024\"\n2. We then construct p, the distribution over true facts T, by controlling how many times each fact appears in our dataset. For each fact, we draw a repetition count from one of the following distributions and replicate the fact accordingly:\n\u2022 Pareto distribution with probability density function:\n$f(x; \\gamma, x_m) = \\frac{\\gamma x_m^\\gamma}{x^{\\gamma+1}}, x \\geq x_m$\nwhere $\\gamma$ is the shape parameter and $x_m$ is the scale parameter\n\u2022 Zipf's distribution with probability mass function:\n$p(k; \\rho, N) = \\frac{1/k^\\rho}{\\sum_{n=1}^{N} 1/n^\\rho}$\nwhere $\\rho$ is the skewness parameter and N is the number of elements\n\u2022 Gaussian distribution with probability density function:\n$f(x; \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$\nwhere $\\mu$ is the mean and $\\sigma$ is the standard deviation\n\u2022 Poisson distribution with probability mass function:\n$p(k; \\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!}$\nwhere $\\lambda$ is both the mean and variance parameter\n3. Finally, we create training set S by sampling with replacement from this expanded dataset, maintaining a fixed size of 5,000 samples."}, {"title": "Statement Generation Process", "content": "Let g be a distribution over factual statements. For our movie dataset, each statement is a structured 6-tuple where each position can only contain tokens of a specific type (actors in position 1, costars in position 2, etc.). To build our bigram model, we first convert each 6-tuple in our training data into a set of adjacent pairs: (actor, costar), (costar, movie), (movie, director), (director, genre), and (genre, year). This creates a dataset of token pairs that we use to train a single bigram model, effectively treating our structured tuples as a bag of adjacent pairs. For any such pair of tokens ($t_1, t_2$), the bigram model learns:\n$g(t_1, t_2) = g(t_1) g(t_2 | t_1)$\nwhere $g(t_1)$ represents the marginal distribution over first tokens and $g(t_2 | t_1)$ represents the conditional distribution over second tokens given the first token. To generate a new 6-tuple statement, we apply this model sequentially:\n$g(x) = g(t_1) \\prod_{i=1}^{5} g(t_{i+1} | t_i)$\nThe generation proceeds as follows:\n1. Sample an initial token $t_1$ according to $g(t_1)$\n2. For i from 1 to 5:\n\u2022 Sample token $t_{i+1}$ according to $g(t_{i+1} | t_i)$\nNote that while this approach is similar to standard bigram language modeling, it does not exploit the full structure of our movie data where each position has its own distinct token space (i.e., \u201cTimoth\u00e9e Chalamet\u201d can only appear in the Actor slot). Instead, we treat all position pairs equivalently in building our transition probabilities. For evaluation, each model is trained on |S| = 5,000 statements and generates an equal number of new statements."}, {"title": "Hallucination Evaluation", "content": "Our hallucination evaluation process is simple. We define hallucinated generation $F_{gen}$ as any generation that does not belong in T. In other words:\n$F_{gen} := G \\cap F$\nThe hallucination rate is simply computed as:\n$f_{gen} = \\frac{|F_{gen}|}{|G|}$"}, {"title": "Miscalibration Measurement", "content": "Before diving into technical details, let's understand calibration through a simple example. Consider a weather forecaster who predicts a 30% chance of rain on multiple days. If they are well-calibrated, it should actually rain on approximately 30% of the days where they made this prediction. More generally, a model is considered calibrated if among events it assigns a certain probability to, that probability matches their true frequency of occurrence.\nIn our context, calibration means that for statements where our bigram model assigns similar probabilities, these probabilities should match the average true probability p across those statements a weaker requirement than matching the exact probability of each individual statement.\nWe employ a logarithmic binning strategy following Kalai and Vempala [2024]. For a given parameter $\\epsilon \\in [0, 1]$, we created bins with boundaries:\n$B_i = [(1 - \\epsilon)^{i+1}, (1 - \\epsilon)^{i}]$\nwhere $i \\geq 0$ indexes the bins. A statement x in F is assigned to bin $B_i$ if g(x) fell within its boundaries. This logarithmic binning ensures appropriate granularity across different orders of magnitude of probabilities.\nThe experiments in this paper utilize an $\\epsilon$ of 0.1 unless otherwise indicated.\nThe total variation distance between p and g for each bin is computed as follows. First, let B(g) be an arbitrary partition of the distribution g, where for each statement x, there exists exactly one bin B\u2208 B(g) such that x belongs to B if and only if g(x) falls within that bin's probability range. For any such partition B(g), we define $p^{B(g)}$ to be the B(g)-coarsening of p, where for all x \u2208 B:\n$p^{B(g)}(x) = \\frac{p(B)}{|B|}$"}, {"title": "Controlled Miscalibration and Hallucination Reduction", "content": "To systematically investigate how miscalibration affects hallucination while holding monofact rates constant, we introduce a training sample upweighting algorithm. Our approach allows us to induce varying levels of miscalibration by selectively reinforcing observed transitions, without altering the underlying monofact rate of the training data.\nSpecifically, after initial model training, we upweight transition probabilities derived from a subset of the training data. For a given size k, we select k training examples and amplify their contribution to the model's transition matrix by adding their token count one more time in the bigram model, effectively duplicating them. This process increases the model's confidence in those particular transitions while maintaining the same set of observed facts, allowing us to isolate the effects of miscalibration from changes in the monofact rate.\nThe procedure is detailed in Algorithm 1. For each selected training example, we increment both the initial state probabilities and the bigram transition counts. After each upweighting session, we call Algorithm 2 to maintain normalization of the bigram model's initial and transition probabilities.\nAs we increase the number of upweighted examples, we should observe that upweighting of a subset of the training data leads to higher miscalibration as the model becomes overconfident in certain transitions. As the upweighted subset approaches the full training set, both miscalibration and hallucination rates should return to their baseline levels prior to any interventions, since the effect of double counting all tokens is eliminated through normalization as detailed in Algorithm 2."}, {"title": "LLM Methodology", "content": "To extend our investigation beyond n-gram models, we conduct similar p distribution-based experiments using LLMs. We first create a synthetic universe of facts U where we can fully control and verify the true distribution. These experiments are conducted via in-context learning using Gemini [Team et al., 2024], selected for its accessibility and context window of 1 million tokens, which enables the input of over 1,000 statements per prompt."}, {"title": "Synthetic Statement Construction", "content": "We generate a synthetic dataset of 1 million statements, each containing 6 distinct entities to ensure the data would be novel to the LLM (not observed during pre-training). Each entity type is carefully designed to be plausible yet previously unseen:\n1. Entity 1: Random 10-digit numbers\n2. Entity 2: Fictional names generated using phonetic rules\n3. Entity 3: Randomly formed ASCII letter sequences\n4. Entity 4: Procedurally generated city names\n5. Entity 5: Generated language names\n6. Entity 6: Base64-encoded 16-byte random strings\nThese entities are combined into coherent statements following a fixed template: \u201cOn day [Entity 1], [Entity 2] convened with [Entity 3] to debate the secret message they found in an obscure cave in [Entity 4], while whispering in [Entity 5], which reads [Entity 6].\""}, {"title": "Distribution Construction and Sampling", "content": "Similar to our bigram experiments, we construct different fact frequency distributions of p over the true facts T. This allows us to systematically vary the monofact rate in our sampled training data S. In this setting, we test the Pareto and Gaussian distributions."}, {"title": "Evaluation", "content": "Following the same methodology as our bigram experiments, we define hallucination as any statement not in T. This controlled setting allows us to precisely quantify hallucination rates and evaluate how different p distributions affect LLM behavior."}, {"title": "LLM Results", "content": "Our LLM experiments provide additional empirical support for Kalai and Vempala's theoretical upper bound on hallucination rates using in-context learning. As illustrated in Figure 8, we evaluate how monofact rates change as we vary how often facts repeat (controlled by the Pareto shape parameter $\\gamma$), and how this influences hallucination across 100 rounds of in-context learning. Two main insights emerge from these results.\nWe observe a direct, positive relationship between monofact rate and hallucination rate. As monofact rate increases from 10% to 90%, hallucination rates consistently rise across all Pareto shape parameters. This trend aligns with the theoretical claim that the monofact rate directly raises the theoretical bound on hallucinations.\nWhen we conduct the same experiment using Gaussian distribution as shown in Figure 9 the underlying distribution is unable to yield a wide range of monofact rates. For the range of monofact rates (70% to 90%) that we are able to produce, we find no relationship between hallucination and monofact rates.\nOverall, these experiments highlight the interplay between monofact rates and repetition patterns in governing the prevalence of hallucinations in the context of LLMs."}, {"title": "Conclusion and Future Work", "content": "Through systematic experiments with n-gram models and LLM in-context learning, we studied how monofact rates, miscalibration, and hallucination interact. Our results validate Kalai and Vempala's lower bound for hallucination, showing that the true data distribution critically influences monofact rates patterns where facts appear with varying frequency affect both monofact rates and hallucination. Specifically, when certain facts appear much more frequently than others, we observe lower monofact rates and reduced hallucination.\nWe also found a trade-off between model calibration and factual reliability: controlled miscalibration can lower hallucination at a fixed monofact rate. These findings highlight the importance of fact repetition patterns and strategic calibration management. Notably, our upweighting experiments suggest selective data duplication can manage this trade-off, challenging aggressive deduplication and offering insights for building more reliable language models.\nFuture research could focus on developing more sophisticated techniques for mitigating hallucination in LLMs. A crucial direction will be to investigate how the observed relationships between monofact rates, hallucination, and miscalibration translate to more complex model architectures. Post-training techniques to strategically introduce duplications to reduce hallucination could be an interesting direction."}]}