{"title": "Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?", "authors": ["Nemika Tyagi", "Mihir Parmar", "Mohith Kulkarni", "Aswin RRV", "Nisarg Patel", "Mutsumi Nakamura", "Arindam Mitra", "Chitta Baral"], "abstract": "Solving grid puzzles involves a significant amount of logical reasoning. Hence, it is a good domain to evaluate the reasoning capability of a model which can then guide us to improve the reasoning ability of models. However, most existing works evaluate only the final predicted answer of a puzzle, without delving into an in-depth analysis of the LLMs' reasoning chains (such as where they falter) or providing any finer metrics to evaluate them. Since LLMs may rely on simple heuristics or artifacts to predict the final answer, it is crucial to evaluate the generated reasoning chain beyond overall correctness measures, for accurately evaluating the reasoning abilities of LLMs. To this end, we first develop GridPuzzle, an evaluation dataset comprising 274 grid-based puzzles with different complexities. Second, we propose a new error taxonomy derived from manual analysis of reasoning chains from LLMS including GPT-4, Claude-3, Gemini, Mistral, and Llama-2. Then, we develop an LLM-based framework for large-scale subjective evaluation (i.e., identifying errors) and an objective metric, PuzzleEval, to evaluate the correctness of reasoning chains. Evaluating reasoning chains from LLMs leads to several interesting findings. We further show that existing prompting methods used for enhancing models' reasoning abilities do not improve performance on GridPuzzle. This highlights the importance of understanding fine-grained errors and presents a challenge for future research to enhance LLMs' puzzle-solving abilities by developing methods that address these errors\u00b9.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in LLMs such as GPT-4, Gemini, Claude-3 (Anthropic, 2024), Llama-2 (Touvron et al., 2023), and Mistral (Jiang et al., 2023) have achieved remarkable performance on a wide range of Natural Language Understanding (NLU) tasks previously thought to be exclusive to human. Beyond NLU, exploring LLMs' logical reasoning abilities (Liu et al., 2021; Saparov and He, 2022; Parmar et al., 2024; Patel et al., 2024) on complex tasks such as puzzle-solving is under-explored. Past attempts have been made to evaluate models on logic-intensive grid-based puzzle-solving. However, they either do not focus on evaluating LLMs (Mitra and Baral, 2015; Jabrayilzade and Tekir, 2020) or do not evaluate LLMs independently, but rather use neuro-symbolic approaches (Ishay et al., 2023) that use external specialized solvers on LLM outputs. Here, we aim to evaluate the puzzle-solving abilities of LLMs by themselves, without the use of any external logic solvers.\nTo understand the reasoning capabilities of LLMs, it is important to evaluate reasoning chains, rather than the final predicted answer. There have been works that evaluate reasoning chains using objective metrics such as ROSCOE (Golovneva et al., 2022), CTC (Deng et al., 2021), and BARTScore (Yuan et al., 2021), however, they do not focus specifically on evaluating reasoning. Some prior works propose metrics for specific reasoning tasks, such as FOLIO (Han et al., 2022) and ProntoQA (Saparov and He, 2022). However, these methods rely on reference-based evaluation, do not focus on puzzle-solving, and do not aim to identify fine-grained errors in reasoning chains. To address these limitations, we propose a reference-free manual and automated subjective evaluation of reasoning chains to understand various fine-grained errors in reasoning chains for grid-based puzzle-solving.\nMotivated by Mitra and Baral (2015), we first develop GridPuzzle, a comprehensive evaluation dataset consisting of grid-based puzzles with grid-size of 3 \u00d7 4, 3 \u00d7 5, 4 \u00d7 4, 4 \u00d7 5, and 4 \u00d7 6 with three levels of difficulty (easy, medium, and hard). Then, we evaluate LLMs including GPT-"}, {"title": "2 Related Work", "content": "Puzzle-solving Task Puzzle-solving task provides detailed insights into LLMs' logical reasoning. Giadikiaroglou et al. (2024) categorize puzzles into (1) rule-based and (2) rule-less puzzles."}, {"title": "3 Evaluation of Reasoning Chains", "content": "To explore where exactly these LLMs falter in performing reasoning, we conduct a detailed manual analysis of the reasoning chains generated by them while solving grid-based puzzles. Our manual analysis process consists of three steps. First, we begin by segmenting the reasoning chains into individual sentences, allowing us to categorize errors more precisely. Second, we identify the premise and conclusion for each sentence and determine their respective correctness. We refrain from subdividing sentences into multiple premises or conclusions to maintain simplicity for finding errors. At last, each sentence is categorized as either containing a single premise and conclusion or being a declarative statement without a conclusion. Then, we begin assessing potential issues or errors in the reasoning chains. Now, we follow an exhaustive approach to create fine-grained error categories. We begin with 30 reasoning chains (6 puzzles x 5 reasoning chains from LLMs) to manually identify potential errors. Next, we categorize these errors in a structured format. We then add another 30 reasoning chains to see if any new types of errors emerge. If new errors are identified, we refine our categories accordingly."}, {"title": "3.1 GridPuzzle", "content": "To develop this dataset, we extract logic grid puzzles of various grid sizes from Puzzle Baron's Logic Puzzles\u00b2. Specifically, we compile logic grid puzzles of size 3 \u00d7 4, 3 \u00d7 5, 4 \u00d7 4, 4 \u00d7 5, and 4 \u00d7 6. Each grid size has three levels of difficulty (easy, medium, and hard) except 4 \u00d7 6. This particular grid size has only two difficulty levels (Easy and Medium). Statistics corresponding to each grid size are presented in Figure 1 (top left)."}, {"title": "3.2 Manual Evaluation", "content": "To explore where exactly these LLMs falter in performing reasoning, we conduct a detailed manual analysis of the reasoning chains generated by them while solving grid-based puzzles. Our manual analysis process consists of three steps. First, we begin by segmenting the reasoning chains into individual sentences, allowing us to categorize errors more precisely. Second, we identify the premise and conclusion for each sentence and determine their respective correctness. We refrain from subdividing sentences into multiple premises or conclusions to maintain simplicity for finding errors. At last, each sentence is categorized as either containing a single premise and conclusion or being a declarative statement without a conclusion. Then, we begin assessing potential issues or errors in the reasoning chains. Now, we follow an exhaustive approach to create fine-grained error categories. We begin with 30 reasoning chains (6 puzzles x 5 reasoning chains from LLMs) to manually identify potential errors. Next, we categorize these errors in a structured format. We then add another 30 reasoning chains to see if any new types of errors emerge. If new errors are identified, we refine our categories accordingly."}, {"title": "3.3 Proposed Error Taxonomy", "content": "As shown in Table 1, we present five main categories: \u201cWW\u201d Wrong Premise Wrong Conclusion, \u201cWR\u201d Wrong Premise Right Conclusion, \u201cRW\u201d - Right Premise Wrong Conclusion, \u201cRR\u201d - Right Premise Right Conclusion, and \"NC\" - No Conclusion. These acronyms of broad categories are self-explanatory. For instance, the category \"WW\u201d comprises sentences where the sentence consists of a wrong premise as well as a wrong conclusion. Interestingly, we also find the \u201cWR\u201d category consists of instances where a wrong premise still leads to a correct conclusion. Additionally, sentences containing only information from clues or premises from previous steps fall under \"NC\u201d. We conduct further investigation as to why the premises and conclusions become incorrect.\nAs shown in Table 2, we identified the source of the premise to determine the origin of errors: (i) \u2018From Clues' where the premise is directly borrowed from one of the clues without any further reasoning, and (ii) 'Derived\u2019 \u2013 where the premise is inferred from either the clues or the previous conclusions. From Table 2, there are six possible reasons associated with two different sources for the wrong premise. When the premise originates from the source (i), we find three types of errors: Hallucination \u2013 When some factual information from the clues is distorted or completely made up; Incomplete information \u2013 When the information is correctly borrowed from the clues but it is not sufficient to make a particular conclusion; Assumptions \u2013 This is a special category where the premise is not derived but also not given exactly in the clues. It is often related to one of the clues and is of the form, \"Let's assume\" or \"Assuming that.\" When source is derived, we find three different errors: Error Propagation \u2013 This occurs when a previously incorrect conclusion becomes the basis for a flawed premise, thereby extending the error from one conclusion to the next; Incomplete information \u2013 When the derived premise is not sufficient to make a particular conclusion; and lastly, Wrong Assumption \u2013 When the LLM reasoner clearly states that a premise was an assumption but it was incorrectly derived.\nAs shown in Table 2 (source), conclusions are always logically derived from a fixed set of premises. For having a wrong conclusion in any reasoning step, we find 3 errors responsible: Error Propagation \u2013 When a conclusion is wrong strictly due to some error in the preceding premise; Wrong Elimination \u2013 When the conclusion is wrong because the LLM reasoner failed to eliminate all the unfit choices correctly. This case is specific to the grid-based puzzle task but is inherently an erroneous deduction on the LLM's end; Wrong reasoning \u2013 The remaining incorrect conclusions that did not fit in the above categories are classified under this label."}, {"title": "3.4 Automated Evaluation", "content": "Manual analysis of reasoning chains provides a detailed categorization of errors; however, it is te-"}, {"title": "4 Experimental Steup", "content": ""}, {"title": "4.1 Experiments", "content": "We evaluate a range of closed-source LLMs including GPT-4-Turbo, Claude-3-Opus, and Gemini-Pro, and open-source models Llama-2-13B, and Mistral-7B-Instruct on GridPuzzle in the Zero-shot-CoT setting (Kojima et al., 2022). Our GridPuzzle dataset consists of a set of instances denoted as \\(P = < p_{n_{i\u00d7j}}, a_{n} >\\), where \\(p_{n_{i\u00d7j}}\\) is nth puzzle instance with grid size of i \u00d7 j and \\(a_{n}\\) as a gold answer. We prompt each LLM to generate a reasoning chain before predicting answer \u00e2. To evaluate each model in the Zero-shot-CoT setting, we provide < I, \\(p_{n_{i\u00d7j}} \\) > as input to the model and predict an answer a where I is a natural language instruction. The evaluation is conducted on the OpenAI, Google, and Anthropic model versions released in April 2024 with temperature setting 0 for deterministic predictions. NVIDIA A100 GPUs are used for conducting the inference of open-source models with a batch size of 4. The example prompts used for these experiments are provided in Appendix A."}, {"title": "4.2 Metrics", "content": "We use accuracy to demonstrate the capability of LLMs in solving grid-based puzzles based on their ability to predict the final answer. To calculate this metric, we use the LLM-generated final answers and compare them with the available gold solution. The predicted answers and the gold solution are in the form of tables with the number of rows and columns equal to the grid size of the puzzle. We perform an Exact Match (EM) to com-"}, {"title": "5 Results and Analysis", "content": ""}, {"title": "5.1 Objective Evaluation", "content": "To evaluate the performance of LLMs when solving grid-based puzzles, we assess the outputs of 5 LLMs using the accuracy and PuzzleEval. As shown in Figure 3, we found that all the models have low performance on the GridPuzzle dataset in terms of accuracy. The smaller open-source LLMs completely failed at the puzzle-solving task, with LLama-2 solving only one puzzle correctly. Close-source models with significantly larger parameter sizes also exhibited poor performance. GPT-4 had the highest accuracy at only 5.11% (14 puzzles out of 274). Despite the overall low performance of all LLMs, the closed-source models perform marginally better. We evaluate the quality of the reasoning chains using PuzzleEval. Table 4 provides the ACS for each grid size available in the GridPuzzle. Surprisingly, compared to the accuracy, the performance of the models with PuzzleEval was significantly better as shown in Table 4. The ACS lie in the range of 0.26 to 0.64 across all grid sizes. This higher score can be attributed to the partial correctness of reasoning chains when solving the grid-puzzle task. The disparity between metrics shows that evaluating only final answers doesn't fully capture LLMs' effectiveness in complex logical tasks like grid puzzles.\nWith the increase in the sizes of the grids, the complexity of the puzzles also rises, leading to a depreciating performance by the LLMs with larger grids. Overall the performance of larger LLMs was much better than the small open-source models. Mistral-7B performed the worst in PuzzleEval which is in accordance with its low accuracy score. GPT-4 and Gemini models surprisingly have similar PuzzleEval scores (0.59 and 0.58 respectively) despite their large difference in accuracy. This difference in PuzzleEval could be attributed to the relatively shorter reasoning chains (fewer reasoning steps) produced by Gemini (an average of 14.91 steps) compared to GPT-4 (an average of 20.66 steps). Shorter reasoning chains may reduce the number of errors that occur while solving the puzzle. It is interesting to note that the smaller LLMs have consistently low performance with the increase in the grid size of the puzzles but the larger LLMs have mixed performance."}, {"title": "5.2 Reasoning Chain Evaluation", "content": "The relative distribution of the broad error categories over the collective reasoning steps for each model is given in Figure 4. It is important to note that, despite using the same zero-shot-CoT setting, the GPT-4 and Llama-2 used significantly more reasoning steps (> 5.5k steps) to solve the 274 puzzles compared to the other three models (~4k steps). The distribution of error sub-categories for each model is presented as heatmaps in the first five sub-figures in Figure 5. Here, we present several findings based on the evaluation of different error category distributions across GridPuzzle.\nFigure 4 shows that most reasoning steps for each model fall into the \u201cNC\u201d error category, indicating that many steps reiterate the facts or clues from the initial puzzle rather than focusing on reasoning. Over 55% of Gemini-Pro's reasoning steps fall into"}, {"title": "6 Conclusion", "content": "In this work, we evaluated the logical reasoning abilities of LLMs through the lens of a grid-based puzzle-solving task. We introduced GridPuzzle, an evaluation dataset of 274 puzzles with various grid sizes. From a manual evaluation of reasoning chains generated by five different LLMs on GridPuzzle, we developed a fine-grained error taxonomy with five broad categories and nine sub-categories. We then created an Auto-evaluator to automate the identification of error categories, providing broader insights into error distributions across the dataset. Additionally, we proposed PuzzleEval, a reference-free metric to objectively evaluate the correctness of reasoning chains for grid-based puzzles. Our analysis of error distributions in GridPuzzle revealed several interesting findings and insights into the logical reasoning abilities of different LLMs. We further evaluated existing reasoning-specific prompting methods, such as self-discover and self-correct, finding that they do not improve results on GridPuzzle. We believe our work offers a challenging dataset, highlights where these LLMs make mistakes, and provides insights to develop better logical reasoning systems for complex tasks such as grid puzzle-solving."}, {"title": "Limitations", "content": "While GridPuzzle facilitates the evaluation of LLMs' logical reasoning abilities, the complexity of the puzzles can be enhanced by incorporating further complex grid sizes beyond 4x6. Additionally, this study can be extended to different types of puzzles, such as Sudoku, Game of 24, and commonsense puzzles. Though our study provides fine-grained error categories, it can be further refined by mapping to formal logic to identify more detailed and atomic errors, offering a deeper understanding of LLMs' reasoning failures. Although we propose an effective automatic method for error identification to reduce manual analysis, exploring other automated methods using smaller-scale supervised learning could be a promising future research direction. We also note that this research is currently limited to the English language and can be extended to multilingual scenarios to evaluate the logical reasoning abilities of LLMs."}, {"title": "Ethics Statement", "content": "We have used AI assistants (Grammarly and ChatGPT) to address the grammatical errors and rephrase the sentences."}]}