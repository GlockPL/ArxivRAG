{"title": "Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?", "authors": ["Nemika Tyagi", "Mihir Parmar", "Mohith Kulkarni", "Mutsumi Nakamura", "Arindam Mitra", "Aswin RRV", "Nisarg Patel", "Chitta Baral"], "abstract": "Solving grid puzzles involves a significant\namount of logical reasoning. Hence, it is a good\ndomain to evaluate the reasoning capability of\na model which can then guide us to improve\nthe reasoning ability of models. However, most\nexisting works evaluate only the final predicted\nanswer of a puzzle, without delving into an in-\ndepth analysis of the LLMs' reasoning chains\n(such as where they falter) or providing any\nfiner metrics to evaluate them. Since LLMs\nmay rely on simple heuristics or artifacts to pre-\ndict the final answer, it is crucial to evaluate\nthe generated reasoning chain beyond overall\ncorrectness measures, for accurately evaluat-\ning the reasoning abilities of LLMs. To this\nend, we first develop GridPuzzle, an evalua-\ntion dataset comprising 274 grid-based puzzles\nwith different complexities. Second, we pro-\npose a new error taxonomy derived from man-\nual analysis of reasoning chains from LLMS\nincluding GPT-4, Claude-3, Gemini, Mistral,\nand Llama-2. Then, we develop an LLM-based\nframework for large-scale subjective evaluation\n(i.e., identifying errors) and an objective met-\nric, PuzzleEval, to evaluate the correctness of\nreasoning chains. Evaluating reasoning chains\nfrom LLMs leads to several interesting find-\nings. We further show that existing prompting\nmethods used for enhancing models' reasoning\nabilities do not improve performance on Grid-\nPuzzle. This highlights the importance of un-\nderstanding fine-grained errors and presents a\nchallenge for future research to enhance LLMs'\npuzzle-solving abilities by developing methods\nthat address these errors\u00b9.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in LLMs such as GPT-4,\nGemini, Claude-3 (Anthropic, 2024), Llama-2\n(Touvron et al., 2023), and Mistral (Jiang et al.,\n2023) have achieved remarkable performance on\na wide range of Natural Language Understanding\n(NLU) tasks previously thought to be exclusive to\nhuman. Beyond NLU, exploring LLMs' logical\nreasoning abilities (Liu et al., 2021; Saparov and\nHe, 2022; Parmar et al., 2024; Patel et al., 2024)\non complex tasks such as puzzle-solving is under-\nexplored. Past attempts have been made to eval-\nuate models on logic-intensive grid-based puzzle-\nsolving. However, they either do not focus on eval-\nuating LLMs (Mitra and Baral, 2015; Jabrayilzade\nand Tekir, 2020) or do not evaluate LLMs indepen-\ndently, but rather use neuro-symbolic approaches\n(Ishay et al., 2023) that use external specialized\nsolvers on LLM outputs. Here, we aim to evaluate\nthe puzzle-solving abilities of LLMs by themselves,\nwithout the use of any external logic solvers.\nTo understand the reasoning capabilities of\nLLMs, it is important to evaluate reasoning chains,\nrather than the final predicted answer. There have\nbeen works that evaluate reasoning chains using ob-\njective metrics such as ROSCOE (Golovneva et al.,\n2022), CTC (Deng et al., 2021), and BARTScore\n(Yuan et al., 2021), however, they do not focus\nspecifically on evaluating reasoning. Some prior\nworks propose metrics for specific reasoning tasks,\nsuch as FOLIO (Han et al., 2022) and ProntoQA\n(Saparov and He, 2022). However, these methods\nrely on reference-based evaluation, do not focus\non puzzle-solving, and do not aim to identify fine-\ngrained errors in reasoning chains. To address these\nlimitations, we propose a reference-free manual\nand automated subjective evaluation of reasoning\nchains to understand various fine-grained errors in\nreasoning chains for grid-based puzzle-solving.\nMotivated by Mitra and Baral (2015), we first\ndevelop GridPuzzle (Figure 1), a comprehensive\nevaluation dataset consisting of grid-based puzzles\nwith grid-size of 3 \u00d7 4, 3 \u00d7 5, 4 \u00d7 4, 4 \u00d7 5, and 4 \u00d7 6\nwith three levels of difficulty (easy, medium, and\nhard). Then, we evaluate LLMs including GPT-"}, {"title": "2 Related Work", "content": "Puzzle-solving Task Puzzle-solving task pro-\nvides detailed insights into LLMs' logical reason-\ning. Giadikiaroglou et al. (2024) categorize puz-\nzles into (1) rule-based and (2) rule-less puzzles."}, {"title": "3 Evaluation of Reasoning Chains", "content": "To develop this dataset, we extract logic grid puz-\nzles of various grid sizes from Puzzle Baron's\nLogic Puzzles\u00b2. Specifically, we compile logic\ngrid puzzles of size 3 \u00d7 4, 3 \u00d7 5, 4 \u00d7 4, 4 \u00d7 5, and\n4 \u00d7 6. Each grid size has three levels of difficulty\n(easy, medium, and hard) except 4 \u00d7 6. This partic-\nular grid size has only two difficulty levels (Easy\nand Medium). Statistics corresponding to each grid\nsize are presented in Figure 1 (top left).\nTo explore where exactly these LLMs falter in per-\nforming reasoning, we conduct a detailed manual\nanalysis of the reasoning chains generated by them\nwhile solving grid-based puzzles. Our manual anal-\nysis process consists of three steps. First, we begin\nby segmenting the reasoning chains into individual\nsentences, allowing us to categorize errors more\nprecisely. Second, we identify the premise and con-\nclusion for each sentence and determine their re-\nspective correctness. We refrain from subdividing\nsentences into multiple premises or conclusions to\nmaintain simplicity for finding errors. At last, each\nsentence is categorized as either containing a sin-\ngle premise and conclusion or being a declarative\nstatement without a conclusion. Then, we begin\nassessing potential issues or errors in the reasoning\nchains. Now, we follow an exhaustive approach to\ncreate fine-grained error categories. We begin with\n30 reasoning chains (6 puzzles x 5 reasoning chains\nfrom LLMs) to manually identify potential errors.\nNext, we categorize these errors in a structured for-\nmat. We then add another 30 reasoning chains to\nsee if any new types of errors emerge. If new errors\nare identified, we refine our categories accordingly."}, {"title": "3.3 Proposed Error Taxonomy", "content": "Broad Categories As shown in Table 1, we\npresent five main categories: \u201cWW\" Wrong\nPremise Wrong Conclusion, \u201cWR\u201d Wrong\nPremise Right Conclusion, \u201cRW\u201d - Right Premise\nWrong Conclusion, \u201cRR\u201d - Right Premise Right\nConclusion, and \"NC\" - No Conclusion. These\nacronyms of broad categories are self-explanatory.\nFor instance, the category \"WW\u201d comprises sen-\ntences where the sentence consists of a wrong\npremise as well as a wrong conclusion. Interest-\ningly, we also find the \u201cWR\u201d category consists of\ninstances where a wrong premise still leads to a\ncorrect conclusion. Additionally, sentences con-\ntaining only information from clues or premises\nfrom previous steps fall under \"NC\u201d. We conduct\nfurther investigation as to why the premises and\nconclusions become incorrect.\nSub-categories: Wrong Premise As shown in\nTable 2, we identified the source of the premise to\ndetermine the origin of errors: (i) \u2018From Clues'\nwhere the premise is directly borrowed from one\nof the clues without any further reasoning, and (ii)\n'Derived' \u2013 where the premise is inferred from ei-\nther the clues or the previous conclusions. From\nTable 2, there are six possible reasons associated\nwith two different sources for the wrong premise.\nWhen the premise originates from the source (i), we\nfind three types of errors: Hallucination \u2013 When\nsome factual information from the clues is distorted\nor completely made up; Incomplete information\n\u2013 When the information is correctly borrowed from\nthe clues but it is not sufficient to make a partic-\nular conclusion; Assumptions \u2013 This is a special\ncategory where the premise is not derived but also\nnot given exactly in the clues. It is often related to\none of the clues and is of the form, \"Let's assume\"\nor \"Assuming that.\" When source is derived, we\nfind three different errors: Error Propagation \u2013\nThis occurs when a previously incorrect conclusion\nbecomes the basis for a flawed premise, thereby\nextending the error from one conclusion to the\nnext; Incomplete information \u2013 When the derived\npremise is not sufficient to make a particular con-\nclusion; and lastly, Wrong Assumption \u2013 When\nthe LLM reasoner clearly states that a premise was\nan assumption but it was incorrectly derived.\nSub-categories: Wrong Conclusion As shown\nin Table 2 (source), conclusions are always log-\nically derived from a fixed set of premises. For\nhaving a wrong conclusion in any reasoning step,\nwe find 3 errors responsible: Error Propagation \u2013\nWhen a conclusion is wrong strictly due to some er-\nror in the preceding premise; Wrong Elimination\n\u2013 When the conclusion is wrong because the LLM\nreasoner failed to eliminate all the unfit choices cor-\nrectly. This case is specific to the grid-based puzzle\ntask but is inherently an erroneous deduction on the\nLLM's end; Wrong reasoning \u2013 The remaining\nincorrect conclusions that did not fit in the above\ncategories are classified under this label."}, {"title": "3.4 Automated Evaluation", "content": "Manual analysis of reasoning chains provides a\ndetailed categorization of errors; however, it is te-"}, {"title": "4 Experimental Steup", "content": "We evaluate a range of closed-source LLMs in-\ncluding GPT-4-Turbo, Claude-3-Opus, and Gemini-\nPro, and open-source models Llama-2-13B, and\nMistral-7B-Instruct on GridPuzzle in the Zero-shot-\nCoT setting (Kojima et al., 2022). Our GridPuz-\nzle dataset consists of a set of instances denoted\nas $P = < p_n^{n \\times j}, a_n >$, where $p_n^{n \\times j}$ is nth puzzle\ninstance with grid size of i \u00d7 j and $a_n$ as a gold\nanswer. We prompt each LLM to generate a reason-\ning chain before predicting answer \u00e2. To evaluate\neach model in the Zero-shot-CoT setting, we pro-\nvide < I, $p_n^{n \\times j}$ > as input to the model and predict\nan answer a where I is a natural language instruc-\ntion. The evaluation is conducted on the OpenAI,\nGoogle, and Anthropic model versions released in\nApril 2024 with temperature setting 0 for determin-\nistic predictions. NVIDIA A100 GPUs are used for\nconducting the inference of open-source models\nwith a batch size of 4. The example prompts used\nfor these experiments are provided in Appendix A."}, {"title": "4.2 Metrics", "content": "We use accuracy to demonstrate the\ncapability of LLMs in solving grid-based puzzles\nbased on their ability to predict the final answer. To\ncalculate this metric, we use the LLM-generated\nfinal answers and compare them with the available\ngold solution. The predicted answers and the gold\nsolution are in the form of tables with the number\nof rows and columns equal to the grid size of the\npuzzle. We perform an Exact Match (EM) to com-"}, {"title": "5 Results and Analysis", "content": "To evaluate the performance of LLMs when solv-\ning grid-based puzzles, we assess the outputs of\n5 LLMs using the accuracy and PuzzleEval. As\nshown in Figure 3, we found that all the models\nhave low performance on the GridPuzzle dataset in\nterms of accuracy. The smaller open-source LLMs\ncompletely failed at the puzzle-solving task, with\nLLama-2 solving only one puzzle correctly. Close-\nsource models with significantly larger parameter\nsizes also exhibited poor performance. GPT-4 had\nthe highest accuracy at only 5.11% (14 puzzles\nout of 274). Despite the overall low performance\nof all LLMs, the closed-source models perform\nmarginally better. We evaluate the quality of the\nreasoning chains using PuzzleEval. Table 4 pro-\nvides the ACS for each grid size available in the\nGridPuzzle. Surprisingly, compared to the accu-\nracy, the performance of the models with PuzzleE-\nval was significantly better as shown in Table 4.\nThe ACS lie in the range of 0.26 to 0.64 across all\ngrid sizes. This higher score can be attributed to\nthe partial correctness of reasoning chains when\nsolving the grid-puzzle task. The disparity between\nmetrics shows that evaluating only final answers\ndoesn't fully capture LLMs' effectiveness in com-\nplex logical tasks like grid puzzles.\nWith the increase in the sizes of the grids, the\ncomplexity of the puzzles also rises, leading to a\ndepreciating performance by the LLMs with larger\ngrids. Overall the performance of larger LLMs\nwas much better than the small open-source mod-\nels. Mistral-7B performed the worst in PuzzleE-"}, {"title": "5.2 Reasoning Chain Evaluation", "content": "The relative distribution of the broad error cate-\ngories over the collective reasoning steps for each\nmodel is given in Figure 4. It is important to note\nthat, despite using the same zero-shot-CoT setting,\nthe GPT-4 and Llama-2 used significantly more\nreasoning steps (> 5.5k steps) to solve the 274\npuzzles compared to the other three models (~4k\nsteps). The distribution of error sub-categories for\neach model is presented as heatmaps in the first five\nsub-figures in Figure 5. Here, we present several\nfindings based on the evaluation of different error\ncategory distributions across GridPuzzle.\nMajority of reasoning steps are error-free. Fig-\nure 4 shows that most reasoning steps for each\nmodel fall into the \u201cNC\u201d error category, indicating\nthat many steps reiterate the facts or clues from\nthe initial puzzle rather than focusing on reasoning.\nOver 55% of Gemini-Pro's reasoning steps fall into"}, {"title": "6 Conclusion", "content": "In this work, we evaluated the logical reasoning\nabilities of LLMs through the lens of a grid-based\npuzzle-solving task. We introduced GridPuzzle,\nan evaluation dataset of 274 puzzles with various\ngrid sizes. From a manual evaluation of reason-\ning chains generated by five different LLMs on\nGridPuzzle, we developed a fine-grained error tax-\nonomy with five broad categories and nine sub-\ncategories. We then created an Auto-evaluator\nto automate the identification of error categories,\nproviding broader insights into error distributions\nacross the dataset. Additionally, we proposed Puz-\nzleEval, a reference-free metric to objectively eval-\nuate the correctness of reasoning chains for grid-\nbased puzzles. Our analysis of error distributions\nin GridPuzzle revealed several interesting findings\nand insights into the logical reasoning abilities\nof different LLMs. We further evaluated exist-\ning reasoning-specific prompting methods, such\nas self-discover and self-correct, finding that they\ndo not improve results on GridPuzzle. We believe\nour work offers a challenging dataset, highlights\nwhere these LLMs make mistakes, and provides\ninsights to develop better logical reasoning systems\nfor complex tasks such as grid puzzle-solving."}, {"title": "Limitations", "content": "While GridPuzzle facilitates the evaluation of\nLLMs' logical reasoning abilities, the complexity\nof the puzzles can be enhanced by incorporating\nfurther complex grid sizes beyond 4x6. Addition-\nally, this study can be extended to different types\nof puzzles, such as Sudoku, Game of 24, and com-\nmonsense puzzles. Though our study provides fine-\ngrained error categories, it can be further refined by\nmapping to formal logic to identify more detailed\nand atomic errors, offering a deeper understand-\ning of LLMs' reasoning failures. Although we\npropose an effective automatic method for error\nidentification to reduce manual analysis, explor-\ning other automated methods using smaller-scale\nsupervised learning could be a promising future\nresearch direction. We also note that this research\nis currently limited to the English language and can\nbe extended to multilingual scenarios to evaluate\nthe logical reasoning abilities of LLMs."}, {"title": "C Evaluation of Reasoning Chains", "content": "In order to identify the error categories from the\nerroneous reasoning chains we conducted manual\nand auto-evaluation of the reasoning chains. The\nprocess of manual evaluation has been described\nin figure 8 and the process of auto-evaluation using\nGPT-40 has been described in figure 9."}, {"title": "D Mitigation Strategy Prompts", "content": "We conducted a study on the 60, 3x4 puzzles\npresent in GridPuzzle dataset to try and improve the\nreasoning abilities of LLMs when solving the grid-\npuzzle task. We used prompt-based methods, such\nas the Plan-and-Solve technique, which divides\npuzzle-solving into planning and solving steps. We\nalso enhanced the solver with insights from our\nerror taxonomy. The prompt structure for this tech-"}]}