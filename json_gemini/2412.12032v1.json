{"title": "FSFM: A Generalizable Face Security Foundation Model via Self-Supervised Facial Representation Learning", "authors": ["Gaojian Wang", "Feng Lin", "Tong Wu", "Zhenguang Liu", "Zhongjie Ba", "Kui Ren"], "abstract": "This work asks: with abundant, unlabeled real faces, how to learn a robust and transferable facial representation that boosts various face security tasks with respect to generalization performance? We make the first attempt and propose a self-supervised pretraining framework to learn fundamental representations of real face images, FSFM, that leverages the synergy between masked image modeling (MIM) and instance discrimination (ID). We explore various facial masking strategies for MIM and present a simple yet powerful CRFR-P masking, which explicitly forces the model to capture meaningful intra-region Consistency and challenging inter-region Coherency. Furthermore, we devise the ID network that naturally couples with MIM to establish underlying local-to-global Correspondence via tailored self-distillation. These three learning objectives, namely 3C, empower encoding both local features and global semantics of real faces. After pretraining, a vanilla ViT serves as a universal vision Foundation Model for downstream Face Security tasks: cross-dataset deepfake detection, cross-domain face anti-spoofing, and unseen diffusion facial forgery detection. Extensive experiments on 10 public datasets demonstrate that our model transfers better than supervised pretraining, visual and facial self-supervised learning arts, and even outperforms task-specialized SOTA methods.", "sections": [{"title": "1. Introduction", "content": "The human face plays a crucial role in daily life and com-puter vision. In parallel, the facial security landscape suf-fers from diverse digital and physical manipulations, no-tably face forgery and presentation attacks. Face forgery ed-its digital pixels while keeping realistic visual quality. Withsignificant advances in generative models , the"}, {"title": "3.1. CRFR-P masking", "content": "The mask sampling strategy plays a key role in MIM forboth representation quality and downstream performance,and also provides the local view for our ID network.Motivation Random masking with a high mask ratio iswidely used in natural [45, 114] and facial [103, 130] MIM,but it lacks domain-specific knowledge, limiting pretrainedfacial models. Given that human faces, our sole focus, com-prise well-defined parts with distinct textures, we turn to ex-plicitly segment the facial semantics for reasonable and ef-ficient mask sampling rather than additional learning mod-ules . Similarly, MARLIN [8] uses an off-the-shelf face parser to divide facial parts and introduces theFasking mask for face video MIM. We adapt Fasking to im-ages as Fasking-I in Fig. 2 (b): it divides facial parts into{left-eye, right-eye, nose, mouth, hair, skin, background},prioritizing masking non-skin and non-background regions.However, Fasking-I struggles to capture sufficient detailsolely from background and skin patches, rendering it un-suitable for face security tasks. Motivated by FACS [29]and psychological studies [42, 85], we explore more effec-tive masking strategies to learn local facial representations.Intuition of intra-region consistency and inter-region co-herency. As shown in Fig. 2, random masking and Fasking-I are prone to fully mask small but informative regions (e.g.,eyes), hindering accurate learning of textures therein (e.g.,pupils, iris). For intra-region consistency, we introduce FRP(Facial Region Proportional) masking that randomly masksan equal portion of patches in each region. FRP ensures allregions have visible patches, thus enabling attention to thesame region when restoring masked patches. However, apotential shortcut, restoring a region\u2019s masked patches di-rectly from its unmasked patches, may lead to a trivial re-"}, {"title": "Algorithm 1 CRFR-P Masking Strategy", "content": "Input: Real face image I, Masking ratio r\nOutput: Image mask M, Facial region mask Mfr\n1: PM \u2190 Face_Parser(I)\n2: Ppm \u2208 RN \u2190 patchify(PM)\n3: M, Mfr\u2190 [0] \u2208 RN, [0] \u2208RN\n4: FR {eyebrows [right eyebrow, left eyebrow], eyes [right eye,\nleft eye], mouth [upper lip, inner mouth, lower lip], face boundary\n[skin background, skin hair], nose, hair, skin, background}\n5: Randomly select a fr \u2208 {FR - {skin, background}}\n6: Mfr [Ppmfr] \u2190 1 Covering a Random Facial Region\n7: if Mfr > Nr then Extreme-case\n8: Randomly unmask (Mfr \u2212 \u039dr) patches in Mfr\n9: M\u2190 Mfr\n10: break\n11: end if\n12: M\u2190 Mfr\n13: for pr \u2208 {FR - {fr}} do Proportional masking in other regions\n14: r = (N\u00b7r \u2212 \u03a3\u039c) / (N \u2212 \u03a3\u039c)\n15: M[(Ppmpr) r] \u2190 1\n16: end for\n17: Return: M, Mfr"}, {"title": "3.2. MIM for facial region perception", "content": "In a nutshell, the MIM network (ED) in FSFM is anMAE [45] model guided by our CRFR-P masking strat-egy. Below, let x f = {x}N denote the full set of Nnon-overlapping patches split from the input face image I.Online Encoder E only operates on the visible patchesxv\u2190 Mxf and maps xv into latent features zo, wheremeans the element-wise product for masking and \u2190 se-lects the visible ones. The online encoder first embeds xvby a linear projection as patch embeddings, and adds cor-responding positional embeddings pr. It then processes thefused embeddings through a series of transformer blocks toobtain: zu = Eo(xv + pv).\nOnline Decoder Do reconstructs the input image pixels. Itfirst combines encoded visible tokens zu with mask tokenszm, and adds relative positional embeddings to form thefull tokens set zf. The online decoder consists of anothersequence of transformer blocks that receives zf as input,followed by a linear layer to restore the masked patches:z = \u0395\u03bf(\u03c7\u03c5 + pv).\nMIM Objective Following [45], we take normalized pixelsas the reconstruction target and minimize the mean squarederror (MSE) loss on masked patches between the predictionI'm and the original one $I_m \\leftarrow (1 \u2212 M) \u2299 I$:\n$L_{rmec} = \\frac{1}{N_m} \\sum_{i=1}^{N_m} (I'^{(i)}_m- I^{(i)}_m)^2$, (1)\nwhere Nm =N\u00d7r=\u2211M is the number of masked patches.Our CRFR-P masking provides an additional mask Mfr,covering all patches in a randomly selected facial regionIm\u2190 (1 \u2013 Mfr) \u2299 I. To enhance inter-region coherencyand prevent trivial solutions, we add another reconstruction"}, {"title": "3.3. ID for local-to-global self-distillation", "content": "In a nutshell, the ID network in FSFM employs symmet-ric designs between the online and target branches w.r.t. theencoder and representation decoder, as well as asymmet-ric designs w.r.t. the input view, projection head, negative-free loss, and model update rate. These designs focus onmore precise and reliable semantic alignment for face secu-rity tasks, distinguishing our method from previous worksthat also integrate ID into MIM or degraded image input.Target Encoder Et takes full patches xf = {x}=1 as thetarget view to produce target latent features zf that guide"}]}