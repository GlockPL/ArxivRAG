{"title": "EyeBench: A Call for More Rigorous Evaluation of Retinal Image Enhancement", "authors": ["Wenhui Zhu", "Xuanzhao Dong", "Xin Li", "Yujian Xiong", "Xiwen Chen", "Peijie Qiu", "Vamsi Krishna Vasa", "Zhangsihao Yang", "Yi Su", "Oana Dumitrascu", "Yalin Wang"], "abstract": "Over the past decade, generative models have achieved significant success in enhancement fundus images. However, the evaluation of these models still presents a considerable challenge. A comprehensive evaluation benchmark for fundus image enhancement is indispensable for three main reasons: 1) The existing denoising metrics (e.g., PSNR, SSIM) are hardly to extend to downstream real-world clinical research (e.g., lesion preserving, Vessel morphology consistency). 2) There is a lack of comprehensive evaluation for both paired and unpaired enhancement methods, along with the need for expert protocols to accurately assess clinical value. 3) An ideal evaluation system should provide insights to inform future developments of fundus image enhancement. To this end, we propose a novel comprehensive benchmark, EyeBench, to provide insights that align enhancement models with clinical needs, offering a foundation for future work to improve the clinical relevance and applicability of generative models for fundus image enhancement. EyeBench has three appealing properties: 1) multi-dimensional clinical alignment downstream evaluation: In addition to evaluating the enhancement task, we provide several clinically significant downstream tasks for fundus images, including vessel segmentation, DR grading, denoising generalization, and lesion segmentation. 2) Medical expert-guided evaluation design: We introduce a novel dataset that facilitates comprehensive and fair comparisons between paired and unpaired methods and includes a manual evaluation protocol by medical experts (e.g., the ratio of lesion structure changed, background-color changed, and extra structures generated). 3) Valuable insights: Our benchmark study provides a comprehensive and rigorous evaluation of existing methods across different downstream tasks, assisting medical experts in making informed choices. Additionally, we offer analysis of the challenges faced by existing methods, which would shine a light for the further design of generative models for fundus image enhancement.The code is available at https://github.com/Retinal-Research/EyeBench", "sections": [{"title": "1. Introduction", "content": "Non-mydriatic retinal color fundus photography (CFP) is widely used in various fundus disease analyses due to the advantage of not requiring pupillary dilation [8, 21, 32, 37, 40, 41]. However, it commonly suffers low quality due to artifacts, uneven illumination, deficient ocular media transparency, poor focus, or inappropriate imaging [9, 24]. Recently, fundus image enhancement has witnessed significant advancements with the rapid development of the generative model. Since these models are not constrained by the need for paired data, a growing number of unpaired image enhancement models have been developed, showing performance comparable to paired methods [5, 30, 33, 38, 39]. At the same time, due to the difficulty of collecting paired noisy and high-quality images, medical experts now show a stronger preference for these unpaired methods. However, the existing evaluation metrics for fundus image enhancement still comply with the supervised denoising task where the low-high quality fundus image pair synthesis by adding known noises (e.g., Gaussian blur, white noise) into real-world high-quality images. This evaluation heavily relies on conventional metrics such as SSIM and PSNR, which fall short of thoroughly assessing the denoising capabilities and similarity between the latent representations of enhanced images and real high-quality images. Moreover, enhancement evaluation alone does not meet clinical requirements, and a rigorous evaluation framework is needed for both unpaired and paired methods to ensure comprehensive assessment. In this paper, we introduce EyeBench, a comprehensive and rigorous benchmark for evaluating fundus image enhancement methods, which includes the multi-dimensional clinical alignment downstream evaluation, medical expert-guided evaluation design, and valuable insights.\nFirst, our benchmark introduces a set of downstream tasks to assess enhanced fundus images, breaking down enhancement quality into clinical preferences, specifically focusing on preserving vessels, disease grading, and lesion structures. We train existing enhancement methods within a standardized framework and apply them to improve fundus image quality for each downstream task. These enhanced images are then processed through respective evaluation workflows for further analysis. As shown in Fig. 1, the downstream tasks include enhancement generalization, vessel segmentation, lesion segmentation, representation, and diabetic retinopathy grading (DR grading). These tasks assess the discrepancies between the generated masks, labels, or representations of the enhanced images and the ground truth or high-quality images. This allows us to determine if vessel structures remain intact and lesion areas are preserved. In addition, the proposed evaluation assesses the enhancement performance and improves the credibility of different enhancement methods for clinical applications."}, {"title": "2. Existing Methods", "content": "We aim to explore current image-denoising methods, focusing on paired and unpaired training approaches. To facilitate this discussion, we let X\u017c, and Yi represent low-quality and high-quality images, respectively, with corresponding distribution Px, and PY, where the disjoint set index i \u2208 {1, 2}. For all paired methods outlined in Sec. 2.1, we focus on data pairs (x1, y1) such that x1 ~ Px\u2081 and y1 ~ PY1. In contrast, for unpaired methods discussed in Sec. 2.2, the data is represented as x1 ~ PX1, y2 ~ PY\u2082, ensuring that no paired information is available."}, {"title": "2.1. Paired Methods", "content": "Leveraging pairs of degraded and clean images, denoted as (x1, y1), paired methods in retinal fundus image enhancement can be uniformly expressed as:\n$\\begin{equation}\n\\hat{y}_1 = f_\\theta(x_1)\n\\end{equation}$\nHere, f\u03b8 represents the denoising network, which utilizes a degradation model to simulate noise in fundus images and applies various neural network architectures to restore image quality. In methods such as SCR-Net [14], Cofe-Net [24], PCE-Net [16] and GFE-Net [15], \u0177\u2081 is modeled using a Variational autoencoder (VAE), incorporating additional information (e.g., high-frequency details, latent retinal structure, artifacts, and the Laplacian Pyramid"}, {"title": "2.2. Unpaied Methods", "content": "Unpaired methods in retinal image denoising can be broadly categorized into two main approaches: GAN-based and SDE-based methods, employing the generative models (e.g., GAN [10], Diffusion [12, 25], Gradient flow [26]). Since collecting paired clean and noisy images from the real world is challenging, the prevailing approach frames the denoising task as a style transfer problem.\nGAN-based model. The adversarial learning strategy enhances GAN-based models in generating realistic retinal images with detailed structures. The typical adversarial objective is formulated as follows:\n$\\begin{equation}\n\\begin{aligned}\n\\min _{G_{X_1}} \\max _{D_{Y_2}} \\mathcal{L} :=\\mathbb{E}_{y_2}\\left[\\log D_{Y_2}\\left(y_2\\right)\\right] \\\\\n+\\mathbb{E}_{x_1}\\left[\\log\\left(1-D_{Y_2}\\left(G_{X_1}\\left(x_1\\right)\\right)\\right)\\right]\n\\end{aligned}\n\\end{equation}$\nHere, the generator Gx\u2081 and discriminator Dy\u2082 work in opposition, seeking to converge toward a Nash Equilibrium. CycleGAN [36] addresses the limitation of requiring paired data by duplicating the GAN structure. It incorporates cycle consistency and identity regularization to support two-way transformations. Specifically, the cycle consistency loss enforces bidirectional mapping, improving alignment and coherence in the generated images. However, these additional structures increase computational complexity and may result in suboptimal performance (e.g., mode collapse, artifacts), especially when dealing with images that exhibit multimodal distributions [23].\nWasserstein-GAN (WGAN) [1, 11] is a widely recognized model rooted in OT theory. Instead of solving the primal OT problem directly, WGAN leverages the Kantorovich-Rubinstein duality [31] to approximate the Wasserstein distance in a computationally feasible manner. The objective function is given by:\n$\\begin{equation}\n\\min _{G_{X_1}} \\max _{D_{Y_2}} \\mathcal{L} :=\\mathbb{E}_{y_2}\\left[D_{Y_2}\\left(y_2\\right)\\right]-\\mathbb{E}_{x_1}\\left[D_{Y_2}\\left(G_{X_1}\\left(x_1\\right)\\right)\\right]\n\\end{equation}$\nHere, the generator Gx\u2081 learns to map Px\u2081 to PY2 by minimizing the Wasserstein distance between them. The discriminator Dy\u2082 maximizes the difference in continuous"}, {"title": "3. Clinic Experts Guided Data Annotation", "content": "Our dataset was sourced from the EyePACS initiative [7], with quality annotations derived from the EyeQ dataset [9]. We collected 28,791 color fundus images, which were categorized into three quality levels: good, usable, and reject. Additionally, each image was annotated with diabetic retinopathy (DR) severity labels across five levels (0, 1, 2, 3, and 4), where higher values indicate greater severity of DR. Analyses of attribute distributions (i.e., brightness,"}, {"title": "4. Experiments", "content": "For full-reference assessment, we used the previously synthesized Full-Reference Assessment Dataset. We strictly followed the training configurations for paired and unpaired methods. For the unpaired method, synthetic low-quality images from training set A (i.e., A*) were used as input images, while high-quality images from training set B served as the clean reference images. For the paired method, we performed supervised training using low-high-quality image pairs from the training set A (i.e., A* and A). All models were trained with the parameter report in the original paper. For two segmentation tasks, we trained a vanilla U-Net [22] model from scratch. The following baselines were considered in this evaluation: Paired algorithms: SCR-Net [14], Cofe-Net [24], PCE-Net [16], GFE-Net [15], RFormer [4], Unpaired algorithms: I-SECRET [3], Cycle-GAN [36], WGAN [11], OTTGAN [33], OTEGAN [39], Context-aware OT [30], CUNSB-RFIE [5]. We generated enhanced images for the trained models separately for the downstream evaluations. Refer to the Appendix A for more details.\nDenoising Evaluation. We input the noisy images from the Full-Reference testing set into the trained models to generate enhanced images. These enhanced low-quality images were then evaluated using the Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM).\nDenoising generalization Evaluation. To evaluate denoising generalization, we degraded high-quality images following the same degradation algorithm to synthesize the"}, {"title": "4.1. Full-Reference Quality Assessment Experiments", "content": null}, {"title": "4.2. No-Reference Quality Assessment Experiments", "content": "Evaluating enhancement quality without ground-truth clean images presents a particular challenge for paired methods. Therefore, we focused on unpaired methods to assess real-world denoising capabilities. For this evaluation, we trained the unpaired method using the No-Reference Assessment Dataset, processing 2,434 low-quality testing images to generate enhanced images. These enhanced images were then used in various downstream evaluations, including DR grading, representation feature analysis, and expert assessment by medical professionals. A no-reference quality assessment was conducted on the following baselines: Cycle-GAN [36], WGAN [11], OTTGAN [33], OTEGAN [39], Context-aware OT [30], and CUNSB-RFIE [5]. All models were trained using the parameters followed in the original papers. Refer to the Appendix B for more details.\nDR grading. We trained an NN-MobileNet model [41] for the DR grading task using real-world high-quality images. The enhanced test images are used with the trained NN-MobileNet to infer DR grading classification. Enhancement performance is evaluated based on classification accuracy (ACC), kappa score, F1 score, and AUC. This evaluation primarily aims to assess whether the denoising model disrupts lesion distribution, potentially leading to inconsistencies with the original DR grading labels.\nRepresentation Feature Evaluation. We employed two fundus image-based foundation models (Retfound [35] and Ret-clip [6]) to calculate the Fr\u00e9chet inception distance (FID) between enhanced and real-world high-quality image feature representation, referred to as FID-Retfound and FID-Clip. FID-Retfound measures the preservation of disease-related information, while FID-Clip assesses the similarity of spatial structures and continuous features.\nExperts Annotation Evaluation. To better align with clinical preferences, we evaluated the enhanced images following protocols provided by medical experts. This evaluation includes the Background Preserving Ratio (BPR), Lesion Preserving Ratio (LPR), and Structure Preserving Ratio (SPR), each used to calculate the proportion of changes in the enhanced images. Importantly, we did not use all 2,434 testing images; instead, we selected 159 images with more prominent lesions, specifically those at DR grading levels 2, 3, and 4. These protocols are shown in Fig. 7, which evaluate whether the denoised images maintain consistency with the original images regarding background, lesion, and structural integrity, helping to evaluate the practical applicability of these unpaired denoising models in real-world medical settings."}, {"title": "4.3. Experiment Results", "content": "Overall, paired methods outperform unpaired methods. As shown in Tab. 1, paired methods, particularly GFE-Net, effectively leverage frequency information, achieving higher SSIM (0.9554, 0.7935) and PSNR (29.719, 25.012) on EyeQ and IDRID, respectively. Among unpaired methods, CycleGAN and OTEGAN demonstrate competitive performance, especially on IDRID and DRIVE, where CycleGAN leads in SSIM (0.7668, 0.6681) and PSNR (22.511, 22.696), indicating robust noise reduction and generalization on unseen datasets. In segmentation tasks (Tab. 2), SCR-Net achieves the highest AUC (0.9227), PR (0.7783), and F1 scores (0.7) in vessel segmentation among paired methods. Unpaired models CUNSB-RFIE and OTEGAN also perform comparably, with CUNSB-RFIE achieving the highest AUC (0.9163). For lesion segmentation, GFE-Net excels in HE lesions, while unpaired models CUNSB-RFIE and OTEGAN attain high F1 scores for EX lesions, demonstrating their effectiveness in lesion preservation.\nSince collecting paired noisy and clean images is challenging in real-world settings, unpaired methods are increasingly prioritized by medical experts. Notably, some methods excel in noise reduction but face challenges in segmentation (e.g., CycleGAN), whereas SDE-based approaches like CUNSB-RFIE show strong generalization and excel in downstream segmentation. This highlights the need for multidimensional evaluation, as high noise reduction performance does not ensure the preservation of small, clinically significant structures."}, {"title": "5. Further Analysis", "content": "The necessity of multi-dimensional evaluation. To further analyze the importance of the multi-dimensional evaluation of Eyebench, we visualized the correlation between medical experts guiding the protocol evaluation and other evaluations, including single-dimension and our multi-dimension evaluation (Full-Reference and No-Reference),"}, {"title": "6. Conclusion", "content": "With the rapid development of generative models, aligning future methods for denoising fundus images with clinical needs has become essential. In this paper, we propose a new benchmark designed to provide more rigorous, clinically relevant evaluations of enhanced images, enabling broader access for medical experts. Furthermore, these multi-dimensional evaluations have demonstrated a strong correlation with manual expert evaluations, helping to bridge the gap in applying generative model-based denoising methods to real-world clinical requirements.\nLimitations and Future Work. Currently, our evaluations are primarily based on deep learning methods. In the future, we plan to expand our work to include more unsupervised traditional algorithms and apply these methods to MRI enhancement tasks."}, {"title": "Ethics Statement", "content": "This retrospective study used open- access human subject data and did not require ethical approval, as confirmed by their license [7, 9, 20, 27]."}]}