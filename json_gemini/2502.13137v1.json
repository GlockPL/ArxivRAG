{"title": "Theorem Prover as a Judge for Synthetic Data Generation", "authors": ["Joshua Ong Jun Leang", "Giwon Hong", "Wenda Li", "Shay B. Cohen"], "abstract": "The demand for synthetic data in mathematical reasoning has increased due to its potential to enhance the mathematical capabilities of large language models (LLMs). However, ensuring the validity of intermediate reasoning steps remains a significant challenge, affecting data quality. While formal verification via theorem provers effectively validates LLM reasoning, the autoformalisation of mathematical proofs remains error-prone. In response, we introduce iterative autoformalisation, an approach that iteratively refines theorem prover formalisation to mitigate errors, thereby increasing the execution rate on the Lean prover from 60% to 87%. Building upon that, we introduce Theorem Prover as a Judge (TP-as-a-Judge), a method that employs theorem prover formalisation to rigorously assess LLM intermediate reasoning, effectively integrating autoformalisation with synthetic data generation. Finally, we present Reinforcement Learning from Theorem Prover Feedback (RLTPF), a framework that replaces human annotation with theorem prover feedback in Reinforcement Learning from Human Feedback (RLHF). Across multiple LLMs, applying TP-as-a-Judge and RLTPF improves benchmarks with only 3,508 samples, achieving 5.56% accuracy gain on Mistral-7B for MultiArith, 6.00% on Llama-2-7B for SVAMP, and 3.55% on Llama-3.1-8B for AQUA.", "sections": [{"title": "1 Introduction", "content": "Recent studies have shown that large language models (LLMs) are increasingly capable of tackling mathematical problems (Dubey et al., 2024; Jaech et al., 2024; Yang et al., 2024a; Guo et al., 2025). Techniques such as Chain of Thought (CoT) aim to enhance the reasoning process by breaking complex problems into intermediate steps (Wei et al., 2022; Kojima et al., 2022). Although these methods have achieved success on benchmarks like GSM8K (Cobbe et al., 2021), they are prone to critical flaws. Specifically, LLM-generated CoT reasoning steps may often include errors, leading to incorrect solutions even when the final answer appears plausible (Leang et al., 2024).\nTo address these flaws, coding-based tools such as Python (Chen et al., 2022; Lyu et al., 2023) and SMT Solvers like the Z3 solver (De Moura and Bj\u00f8rner, 2008) have been widely used to improve reasoning capabilities, as well as theorem provers such as Lean (De Moura et al., 2015) and Isabelle (Paulson, 1994). These systems provide formal verification on the logical validity of LLM reasoning steps. Nevertheless, they are not without challenges; autoformalisation errors often lead to execution failures during theorem prover verification in complex tasks (Xu et al., 2024; Quan et al., 2024; Gou et al., 2023a; Olausson et al., 2023), including Python's struggles with Olympiad-level problems (Leang et al., 2024).\nParallel to these strategies, synthetic data generation emerged as a promising approach to enhance mathematical reasoning (Xin et al., 2024; Lu et al., 2024b). A commonly used verification method is LLM-as-a-Judge (Zheng et al., 2023; Stephan et al., 2024; Saha et al., 2025), wherein LLMs evaluate and generate reasoning data. However, this method often introduces bias (Guan et al., 2025). Generating labelled training data for step-by-step reasoning remains resource-intensive as well, requiring substantial human effort for annotation, and automatic annotation methods have shown limited success, as noisy reward scores often impede their efficacy (Luo et al., 2024; Wang et al., 2024; Chen et al., 2024). Recently, iterative techniques like Monte Carlo Tree Search (MCTS) have been applied to refine reasoning steps and achieve accurate solutions (Guan et al., 2025; Park et al., 2024). While MCTS enhances reasoning quality, it is computationally expensive due to extensive rollouts, limiting its scalability for large-scale applications.\nTo overcome these challenges, we first address execution failures in autoformalisation by proposing iterative autoformalisation, a method that integrates autoformalisation errors as feedback, thereby ensuring that each intermediate reasoning step is verified. Specifically, we use the Lean prover as our autoformalisation verifier. Unlike Python-based approaches (Bi et al., 2024), which refine execution traces, iterative autoformalisation validates individual proof tactics, theorems, and logical steps within the formal system. As problem complexity increases, additional iterations are required for accurate autoformalisation. Simply discarding questions after a single failed attempt risks introducing biases toward simpler problems (Lu et al., 2024a).\nBuilding on this foundation, we introduce Theorem Prover as a Judge (TP-as-a-Judge), a novel framework for generating synthetic data in mathematical reasoning tasks. TP-as-a-Judge is integrated with a theorem prover, leveraging its step-by-step verification to ensure logical correctness. We then propose Reinforcement Learning from Theorem Prover Feedback (RLTPF), a novel approach in which the theorem prover replaces human annotators by providing formalised feedback. By applying TP-as-a-Judge and RLTPF, we achieve competitive performance compared to models fine-tuned on substantially larger datasets in GSM8K and AIME 2024 dataset. This demonstrates the significance of intermediate reasoning in data quality and enhances the data efficiency of model training."}, {"title": "2 Theorem Prover as a Judge", "content": "As shown in Figure 2, TP-as-a-Judge consists of two key stages: LLM Data Generation, where LLMs generate questions and answers; Theorem Prover Verification, where a theorem prover assesses the validity of the model's reasoning on the generated data and proceeds to RLTPF, leveraging verification results to refine the model through Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) (Rafailov et al., 2024)."}, {"title": "2.1 LLM Data Generation", "content": "Building on the work of Balepur et al. (2024), we apply the Reverse Question-Answering method to the mathematical domain for constructing our initial synthetic datasets. This technique involves providing the model with a few example solutions and instructing it to generate corresponding questions that align with those solutions. The datasets we use are sourced from two primary sources: the GSM8K dataset (Cobbe et al., 2021) and the MATH dataset (Hendrycks et al., 2021). This approach enables us to cover a broad array of mathematical topics, spanning domains such as simple problem solving, algebra, number theory, geometry, counting and probability, and calculus. In our setup, we primarily focus on three core areas: problem solving, algebra, and counting and probability."}, {"title": "2.2 Theorem Prover Verification", "content": "After generating the samples, we use gpt-4o and chatgpt-4o-latest (Hurst et al., 2024) as two primary models to generate solutions for the synthetic questions. Subsequently, gpt-4o is used for autoformalisation, after which the outputs are passed to the theorem prover for validation. The Theorem Prover Verification process is divided into two stages: Question and Answer formalisation."}, {"title": "2.2.1 Question Formalisation", "content": "In the question formalisation stage, each question is formalised and verified by a theorem prover, serving as a filter for questions lacking clarity.\nThis process follows a four-phase approach, starting with the original question ($s_0$). In the first phase ($s_1$), we apply the CoMAT method (Leang et al., 2024) to convert $s_0$ into a symbolic representation. The second phase ($s_2$) involves autoformalisation, where the symbolic representation is formalised and verified using a theorem prover.\nTo address the formalisation errors raised by Yang et al. (2024b), which highlight the inherent difficulty of question autoformalisation due to the absence of definitive metrics for assessing correctness, we introduce the third phase ($s_3$), Auto-Informalisation, where the formalised statement is translated back into natural language via GPT-4o. This is followed by the final phase ($s_4$), Alignment Check, which employs a second gpt-4o verification step to ensure alignment between $s_0$ and $s_3$. This serves as a consistency check, confirming the correctness of the autoformalisation process.\nLet $Q$ denote the set of all generated questions. The autoformalisation function $s$ maps each question $q \\in Q$ to its formal representation $s(q)$:\n$s: Q \\rightarrow S, \\quad s(q) = s_4(q)$,\nwhere $S$ is the set of successfully validated questions and answers, and $s_4(q)$ is the final representation after the four-phase process. Questions that do not fulfil the requirements, where $s(q) \\neq s_4(q)$, are flagged as logically inconsistent or poorly formalised, and excluded from answer formalisation."}, {"title": "2.2.2 Answer Formalisation", "content": "During the answer formalisation stage, the model transforms the model reasoning $a(q)$ for each valid question $q \\in Q_{\\text{valid}}$ into a formal representation:\n$a'(q) = \\text{Formalise}(a(q)).$\nIt then verifies each step of the reasoning $a'(q)$ using a theorem prover, confirming the validity of each step. For instance, the model may perform calculations or generate specific tactics to justify its progression; these tactics are formally validated by the theorem prover to ensure adherence to mathematical principles, as shown in Figure 3.\nLet $\\{ a_i(q) \\\\}$ denote the sequence of proof steps in $a'(q)$. The verification process yields one of three outcomes for each answer, $a(q)$: (i) Verified: if all steps are validated by the theorem prover. (ii) False: if any step is refuted by the theorem prover. (iii) error: if the autoformalisation failed to execute through the theorem prover. Formally:\n$\\Lambda(q) = \\begin{cases} 1 & \\text{if } \\forall i, \\text{TP}(a_i(q)) = \\text{verified}, \\\\ 0 & \\text{if } \\exists i, \\text{TP}(a_i(q)) = \\text{false}, \\\\ \\text{error} & \\text{if } \\exists i, \\text{TP}(a_i(q)) = \\text{error}. \\end{cases}$"}, {"title": "2.2.3 Iterative Autoformalisation", "content": "While the formalisation process may introduce errors in initial iterations, we propose iterative autoformalisation, a novel approach in which the model learns from its errors during the autoformalisation process, shown in Algorithm 1. Our approach concentrates on mathematical autoformalisation, correcting each individual step, prompting the model to revise and reverify the autoformalisation using theorem prover. This iterative process reflects the nature of formalisation, as even human experts require multiple attempts to correctly formalise complex problems. The model may need to experiment with different definitions, refine assumptions, and even attempt partial proofs to identify missing conditions. Our results show that iterative refinement improves the execution rate of the Lean prover formalisation code from 60% to 87% after up to five iterations. This result demonstrates the efficacy of iterative refinement in overcoming challenges associated with theorem prover formalisation."}, {"title": "2.3 Reinforcement Learning from Theorem Prover Feedback (RLTPF)", "content": "In conventional RLHF, human evaluators provide structured feedback to help the model distinguish between valid and invalid reasoning. By contrast, RLTPF replaces human evaluators with a theorem prover, which verifies each formalised reasoning step. In our framework, the theorem prover categorises training data based on verification outcomes, ensuring the model learns from both validated reasoning patterns and cases requiring refinement. Specifically, when both the LLMs generate answers verified by the theorem prover, the corresponding dataset is used for Supervised Fine-Tuning (SFT). When one LLM produces a correct response while the other generates an incorrect one, the paired dataset is utilised for Direct Preference Optimisation (DPO). The model then trained with SFT on verified data before undergoing further optimisation with DPO using the paired dataset. Details for the dataset allocation is shown in Table 1."}, {"title": "3 Experimental Setup", "content": "SIMQA Our initial training dataset, Synthetic Induced Mathematical Question Answering (SIMQA) comprises 3,508 samples, of which 1,876 are synthetically generated from GSM8K and 1,632 from MATH. Details on the dataset breakdown and configurations are in Appendix A, and few-shot examples in Appendix G.\nEvaluation Datasets. We evaluate on a diverse set of mathematical benchmarks, including SVAMP (Patel et al., 2021), GSM8K (Cobbe et al., 2021), MultiArith (Roy and Roth, 2016) GSMSymbolic-p1 (Mirzadeh et al., 2024), MATH-500 (Hendrycks et al., 2021), AQUA (Ling et al., 2017), AIME (2024), OlympiadBench (He et al., 2024) datasets. Due to computational constraints, we evaluate only the first 500 samples from GSMSymbolic-p1.\nBase Models and Setup. To demonstrate the effectiveness and generalisability of our approach, we apply it across a variety of LLMs, including Llama-2-7b-chat-hf (Touvron et al., 2023), Llama-3.2-3B-Instruct, Llama-3.1-8B-Instruct (Dubey et al., 2024), OLMo-2-1124-7B-Instruct (OLMO et al., 2024), and Mistral-7B-Instruct-v0.3 (Jiang et al., 2023). These models were selected as representative general-purpose single-task language models (SLMs) without specific specialisation in mathematical reasoning tasks. As part of our training pipeline, we employ Low-Rank Adaptation (LORA; Hu et al. 2021) as our fine-tuning baseline to improve computational efficiency. Details on hyperparameters are provided in Appendix B.\nBaselines Since the model focuses on mathematical reasoning, we compare it against three baselines using the same decoding strategy: greedy decoding, where the most probable next token is selected. The baselines include: (1) Standard Prompting, which follows a similar prompt to that in the Holistic Evaluation of Language Models (HELM; Liang et al., 2022), where the model is instructed to provide the answer without any reasoning; (2) COT Prompting (Wei et al., 2022), and (3) Model Training, which involves performing RLTPF training on 3,508 samples from the synthetic dataset. The prompts for (3) follow the same template as those in (2) to ensure a fair comparison.\nBenchmarks We compare GSM8K and AIME dataset across existing models, including DeepSeekMath (Shao et al., 2024), fine-tuned on an extensive dataset comprising 120B mathematical tokens; NuminaMath (Li et al., 2024), fine-tuned on over 860k mathematical problem-solution pairs using the DeepSeek-Math-7B-Base model; OpenMath2 (Toshniwal et al., 2024), fine-tuned Llama-3.1-8B-Base with a significantly larger corpus of 14M problem-solution pairs; and Mathstral-7B (The Mistral AI Team, 2024).\nEvaluation Metrics We report Pass@1 accuracy as the primary evaluation metric for all baselines. The answers were parsed based on an exact match. For answers where exact matching failed, we parsed the numerical values from the model's final response and compared them against the ground truth. For the MATH-500, AIME, and Olympiad-Bench datasets, we adopt a similar evaluation strategy to Leang et al. (2024). We use gpt-4o-mini to assess answer alignment by comparing the last two output sentences to the ground truth, generating a numerical score without revealing the question. Further details can be found in Appendix D."}, {"title": "4 Experimental Results", "content": "4.1 Main Results\nTable 2 compares the performance of TP-as-a-Judge, CoT prompting, and the standard baseline across various mathematical benchmarks. TP-as-a-Judge consistently outperforms traditional CoT on most datasets, demonstrating the effectiveness of the synthetic data generation method.\nAmong Llama models (Llama-2-7B, Llama-3.2-3B, and Llama-3.1-8B), we observe that while CoT improves accuracy over the standard baseline, TP-as-a-Judge further enhances performance. In particular, Llama-3.1-8B shows a notable increase in GSM-Symbolic (55.60% \u2192 58.20%) and AQUA (53.54% \u2192 57.09%). Llama-2-7B exhibits substantial improvements in MultiArith (39.44% \u2192 59.44%), SVAMP (38.00% \u2192 44.00%), and GSM8K (10.33% \u2192 16.70%), while Llama-3.2-3B demonstrates an improvement in GSM-Symbolic (55.80% \u2192 59.60%). Our empirical findings suggest that the incremental gains from additional model training diminish as models progress from Llama-2 to Llama-3.1 to Llama-3.2. Nevertheless, these results indicate that synthetic data derived solely from GSM8K and MATH generalises well across a range of benchmarks. Notably, even on AQUA, a multiple-choice task performance is enhanced through the inclusion of synthetic data.\nFor Mistral-7B and OLMO-2-7B, we observe consistent improvements across most benchmarks. In particular, OLMo-2-7B demonstrates progress across all evaluated tasks, whereas Mistral-7B exhibits a 0.84% decrease on GSM8K and 1.2% decrease on MATH; however, it achieves an improvement of 3.2% on GSM-Symbolic."}, {"title": "4.2 Analysis", "content": "We conduct an analysis on the effectiveness of TP-as-a-Judge in verifying model solution. We analyse the effectiveness of TP-as-a-Judge in verifying model solutions by extracting 2,000 samples from the SIMQA dataset, using GSM8K as the few-shot example. Our analysis revealed that 30% of these samples failed during the question formalisation stage, potentially classified as poor question clarity. Among the remaining 70%, approximately 5% exhibited execution errors from the Lean prover during the answer formalisation stage (Figure 4).\nWe then randomly sampled 54 successfully executed cases for manual annotation and evaluate the performance of GPT-4o, o1, and o3-mini on these annotated questions. As shown in Table 5, while the models achieve high accuracy on the GSM8K dataset, their performance declines significantly when answering synthetic questions. This highlights a significant gap in the models' ability to handle self-generated questions, demonstrating the critical need of theorem prover for verification, and the potential gap of existing LLMs in mathematical reasoning. Detailed examples of manual annotations can be found in Appendix H.\nTo further investigate, we examine a fundamental question: How effective is theorem prover formalisation in verifying reasoning chains? As shown in Table 4, TP-as-a-Judge achieves an F1 score of 0.87 and a Recall of 0.91 in correctly verifying reasoning steps. In contrast, using o1-mini as an LLM-based judge without a theorem prover, while achieving 94.80% accuracy on GSM8K, results in a significantly lower F1 score of 0.72 and a Recall of 0.78. This leads to a 2x increase in False Positives, potentially degrading dataset quality. These results demonstrate the robustness of iterative autoformalisation within the Lean prover and highlight TP-as-a-Judge 's effectiveness in validating intermediate reasoning steps."}, {"title": "4.3 Robustness to Iterative Autoformalisation", "content": "Given the strong performance of iterative autoformalisation, we explore another key aspect: How many iterations are required for the model to improve its autoformalisation? To do this, we analyse 356 samples from the dataset. We find that approximately 40% require iterative refinement to enhance formalisation Appendix E. Notably, most successful autoformalisations that require iterative autoformalisation are achieved by the third iteration, indicating that multiple rounds of error correction significantly benefit the model. In a minority of cases, more than three iterations are necessary to reach a correct autoformalisation. Instances requiring more iterations often fail, even after seven attempts. By leveraging verification-based self-correction, the model improves its autoformalisation through an iterative cycle. This demonstrates the importance of structured feedback in enhancing the robustness of formal proofs, ensuring that logical errors are systematically identified and resolved."}, {"title": "4.4 Effectiveness of RLTPF in TP-as-a-Judge", "content": "We conduct a more detailed analysis to determine which components of the proposed framework contribute to performance improvement and to evaluate the effectiveness of theorem prover in this process.\nAs shown in Table 6, performing SFT using all possible training instances achieves the highest accuracy among SFT, as it utilises a large number of samples. Meanwhile, using only rejected instances results in lower accuracy compared to using only verified instances. This implies that the Theorem Prover provides an effective inductive bias for learning, but SFT fails to fully leverage it.\nTo address this issue, the proposed RLTPF (\u00a72.3) enables more efficient training by assigning different preferences to verified instances and rejected instances, achieving the highest accuracy among the tested approaches. Interestingly, SFT with all instances showed degraded performance when combined with RLTPF. This is likely due to a conflict between SFT, which was trained with a focus on all instances including rejected ones, and RLTPF, which learns preferences between rejected and verified instances. This finding validates the dataset allocation criteria established in Table 1 and \u00a72.3."}, {"title": "5 Related Work", "content": "Math Data Synthesis Advancements in mathematical reasoning for LLMs increasingly rely on high-quality CoT datasets, often distilled from frontier models (Guo et al., 2025; Wang et al., 2023; Gou et al., 2023b; Luo et al., 2024; Yu et al., 2023; Tang et al., 2024), such as NuminaMath (Li et al., 2024) and OpenMathInstruct (Toshniwal et al., 2024). Nevertheless, even solvable problems may contain error-prone intermediate reasoning steps, which are inherently challenging to detect. Although rejection sampling methods (Yuan et al., 2023; Brown et al., 2024) can improve data quality by filtering out less reliable outputs, they do not guarantee the correctness of intermediate steps. Consequently, the benefits of scaling CoT datasets exhibit diminishing returns, with performance gains nearing saturation. For instance, OpenMathInstruct reported only a 3.9% improvement on the MATH dataset despite an 8x increase in dataset size. Recently, STaR (Zelikman et al., 2022), Lean-STaR (Lin et al., 2024) and rStar-Math (Guan et al., 2025), have been proposed. These approaches rely on generating multiple roll-outs and trajectories for verification and synthesis.\nTheorem Proving & Autoformalisation Modern formal mathematics environments typically centre on theorem provers, such as Lean (Jiang et al., 2024; Lin et al., 2024), Isabelle (Zhou et al., 2024; Xin et al., 2023) and Coq (Huet et al., 1997). These systems have been widely used to verify complex mathematical results, including the Liquid Tensor Experiment (Castelvecchi, 2021), the formalisation of the PER conjecture (Gowers et al., 2023), and efforts to formalise graduate-level number theory (Eberl et al., 2024). The Draft-Sketch-Prove (Jiang et al., 2022) approach enhances language models' formal proving abilities by generating informal proofs, translating them into formal sketches, and completing them with symbolic tools."}, {"title": "6 Conclusion", "content": "We propose Theorem Prover as a Judge (TP-as-a-Judge), a framework for synthetic data generation that ensures the correctness of intermediate reasoning steps. TP-as-a-Judge comprises three main stages: synthetic data generation, theorem prover verification, and Reinforcement Learning from theorem Prover Feedback (RLTPF). Synthetic data generation includes question and answer formalisation, a process that is inherently challenging and often leads to execution errors in the theorem prover. To address this, we introduce iterative autoformalisation, which incrementally refines the model's autoformalisation until a correct execution is achieved. We mitigate question formalisation errors by incorporating Auto-Informalisation and Alignment Check, reducing autoformalisation errors within the synthetic dataset generation and improving verification consistency. RLTPF involves TP-as-a-Judge separating the dataset into two subsets: one for SFT and another for DPO. The model is then trained sequentially through SFT followed by DPO. TP-as-a-Judge with RLTPF enhances model accuracy using only 3,508 samples, achieving results comparable to those obtained with large samples. Despite its simplicity, TP-as-a-Judge presents a promising direction using autoformalisation for improving mathematical reasoning in LLMs."}, {"title": "Limitations", "content": "While TP-as-a-Judge demonstrates strong capabilities in verifying intermediate reasoning for synthetic data generation, several limitations persist. Firstly, theorem prover verification is highly effective in mathematical reasoning but remains challenging in other domains, making its extension beyond mathematics an open research question. While formalisation enhances verification compared to standard methods, it is not flawless, requiring further refinement for broader applicability. Secondly, our current implementation primarily focuses on algebra, counting, probability, and problem-solving datasets. Expanding to additional mathematical areas could improve generalisation and model performance. Thirdly, dataset complexity is limited by the LLM's ability to reliably solve problems. When generating more challenging problems, the model often fails, restricting the complexity of synthetic data and preventing the creation of more advanced mathematical questions. Next, computational constraints limit scalability, particularly for models exceeding 8 billion parameters, requiring substantial resources for large-scale verification and training. Finally, the iterative nature of formalisation incurs a high computational cost, as multiple refinement cycles are needed to ensure correct execution. Addressing these challenges would enable more efficient and scalable synthetic data generation."}]}