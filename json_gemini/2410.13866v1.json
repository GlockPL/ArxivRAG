{"title": "Associative memory and dead neurons", "authors": ["Vladimir Fanaskov", "Ivan Oseledets"], "abstract": "In \"Large Associative Memory Problem in Neurobiology and Machine Learning,\" Dmitry Krotov and John Hopfield introduced a general technique for the systematic construction of neural ordinary differential equations with non-increasing energy or Lyapunov function. We study this energy function and identify that it is vulnerable to the problem of dead neurons. Each point in the state space where the neuron dies is contained in a non-compact region with constant energy. In these flat regions, energy function alone does not completely determine all degrees of freedom and, as a consequence, can not be used to analyze stability or find steady states or basins of attraction. We perform a direct analysis of the dynamical system and show how to resolve problems caused by flat directions corresponding to dead neurons: (i) all information about the state vector at a fixed point can be extracted from the energy and Hessian matrix (of Lagrange function), (ii) it is enough to analyze stability in the range of Hessian matrix, (iii) if steady state touching flat region is stable the whole flat region is the basin of attraction. The analysis of the Hessian matrix can be complicated for realistic architectures, so we show that for a slightly altered dynamical system (with the same structure of steady states), one can derive a diverse family of Lyapunov functions that do not have flat regions corresponding to dead neurons. In addition, these energy functions allow one to use Lagrange functions with Hessian matrices that are not necessarily positive definite and even consider architectures with non-symmetric feedforward and feedback connections.", "sections": [{"title": "1 Introduction", "content": "Associative or content-addressable memory is a system that retrieves the most appropriate stored pattern based on a partially known or distorted input pattern. One particularly influential realization of associative memory was proposed by John Hopfield in [12] for discrete variables and in [13] for continuous variables. Both models are distinguished by their biological plausibility, autonomy, asynchronous operations of constituent parts, robustness to noise, and strong theoretical guarantees. Later in [17], it was shown that one could develop a general biologically plausible model that unites many previously known models and allows building novel associative memory systems [16].\nThe model in [17] is based on the nonlinear dynamical system that evolves in time from a given initial state. Nonlinear dynamical systems show an exceptionally diverse set of behavior [29], so one needs to select an appropriate class of ordinary differential equations suitable to model associative memory. The most cluical requirement is the ability of the system to evolve to a single state from many initial conditions that are close according to some problem-specific metrics. This fact suggests one should use a dynamical system with many stable steady states. If one selects such a system, each stable steady state corresponds to a particular memory and basin of attraction - all initial conditions that evolve to a selected state - defines a measure of similarity between states.\nThe technique of choice to study the stability of steady state is to construct energy or Lyapunov function [22]. This function, defined on the state of a dynamical system, is non-increasing on trajectories of a dynamical system. If it is possible to find such a function, its isolated local minima will correspond to steady states. Different variants of energy functions with this property are available in all previous works"}, {"title": "2 Dead neurons", "content": "In [17] Krotov and Hopfield proposed the following set of equations to model associative memory\n\\dot{y}(t) = Wg(y(t)) \u2013 y(t) + b, y(0) = y_0,  (1)\n\\begin{equation*}g(y) = \\frac{\\partial L(y)}{\\partial y}, W = W^T,  (2)\\end{equation*}\n\\Lambda(y) \\geq 0,  (3)\n\\begin{equation*}\\Lambda_{ij}(y) = \\frac{\\partial^2 L(y)}{\\partial y_i \\partial y_j}\\end{equation*}\nE(y) = (y - b)^T g(y) \u2013 L(y) \u2013 \\frac{1}{2} (g(y))^T Wg(y). (4)\nEquation (1) describes a temporal dynamics of feature vector y starting from y0. The equation contains weights W, bias b and activation function g(y). Structural assumptions (2) ensure that the energy func- tion (4) exists, and restriction on the Hessian \u039b of Lagrange function L(y) (3) guarantees that energy (4) is non-increasing on trajectory of dynamical system (1).\nDynamical system (1) has several favorable properties: (i) steady states can be used as memory vectors, (ii) dynamics in the basins of attraction naturally model memory recovery process, (iii) energy function can be used to ensure the existence of steady states and basins of attraction, (iv) memory is related to neural ODEs so can be trained end-to-end, (v) with a special choice of W steady states resemble activation pattern of classical deep learning architectures. The example below illustrates the last point.\nExample 1 (MLP with feedback connections). For simplicity, we consider four layers, extensions to a larger number of layers are straightforward. Weights and state vectors are partitioned on blocks of conformable size\n\\begin{equation*}W =\\begin{pmatrix}\n0 & W_{12} & 0 & 0\\\\\nW_{21} & 0 & W_{23} & 0\\\\\n0 & W_{32} & 0 & W_{34}\\\\\n0 & 0 & W_{43} & 0\n\\end{pmatrix}, y =\\begin{pmatrix}\ny_1 \\\\\ny_2 \\\\\ny_3 \\\\\ny_4\n\\end{pmatrix}, b =\\begin{pmatrix}\nb_1 \\\\\nb_2 \\\\\nb_3 \\\\\nb_4\n\\end{pmatrix}.\\end{equation*}\nNote that W is symmetric so $W_{12} = W_{21}^T$. With that choice dynamical system (1) becomes\n\\begin{equation*}\n\\dot{y_i} = W_{ii-1}g(y_{i-1}) + W_{ii+1}g(y_{i+1}) - y_i + b_i,\n\\end{equation*}\nso steady-state indeed resembles MLP but with feedback connections that are symmetric."}, {"title": "3 Consequences of dead neurons", "content": "Proposition 1 implies that for most architectures used in practice, there are large regions of state space with flat energy functions. This has several negative consequences that we somewhat arbitrarily classify as problems with sensitivity and stability.\n3.1 Sensitivity\nAs evident from Figure 1 energy is flat in directions V corresponding to dead neurons. This can be formalised\n\\begin{equation*}\ny_d = VV^T y, y_a = (I-VV^T) y\\end{equation*}\ny corresponding to dead and alive neurons.\nSince y = yd + ya, the energy (4) becomes\n\\begin{equation*}\nE(y_d, y_a) = (y_d + y_a - b) g(y_a) \u2013 L(y_a + y_d) \u2013 \\frac{1}{2}(g(y_a))^T Wg(y_a)\n\\\\\\\\\\\\\n= (y_a - b) g(y_a) \u2013 L(y_a) \u2013 \\frac{1}{2} (g(y_a))^T Wg(y_a),\n\\end{equation*}\nwhere we used g(yd + ya) = g(ya) and L(yd + ya) = L(ya) + g(ya)Tyd.\nWe see that energy does not depend on variables ya corresponding to dead neurons, which means the effective number of degrees of freedom is decreased from N to N-k where k is the number of dead neurons. If only energy is available it is not possible to recover values on ya and steady states can not be found as in examples from Figure 1.\nVariables ya are from the kernel of g which means the activation function becomes effectively invertible. In [13], the energy function for this situation has already been introduced. It is instructive to compare a new energy function [17] with the old one. Lyapunov function from the 1984 paper, in the notation used in this article, reads\n\\begin{equation*}\nE(u) = \\sum_i \\int_0^{g_i(u_i)} drg_i^{-1}(r) - b^T g(u) - \\frac{1}{2} (g(u))^T Wg(u),\n\\end{equation*}\nand dynamical system is precisely the same as (1). Activation functions gi are invertible and monotone by assumption. The first term of (5) is seemingly distinct from any term of (4). To show that this is an illusion we simplify integrals as follows\n\\begin{equation*}\n\\int_0^{g(u)} drg^{-1}(r) = \\int_{g^{-1}(0)}^{u} dg(p)g'(p)p = - \\int_{g^{-1}(0)}^{u} dg(p) + pg(p)|_{g^{-1}(0)}^{u} = \\int_{g^{-1}(0)}^{u} dg(p) + ug(u),\n\\end{equation*}\nwhere in the first step we used \u03c4 = g(\u03c1). Using this simplification and additional definition of Lagrange function $L(u): g(u) = \\frac{\\partial L(u)}{\\partial u}$ we immediately recognize that energy function from [17] is precisely the same as from [13] but without the assumption that g should be invertible. New energy from [17] does not formally contain the inverse of the activation function, but as we see still implies that g is invertible.\nBesides insensitivity to values of dead neurons, there is another problematic fact. Proposition 1 implies that, when at least one neuron is dead, the energy function has invariant transformation E(y) = E(y+Vc). The steady state of dynamical system (1) does not have this symmetry, so, when one transforms y \u2192 y+Vc and preserve energy, this corresponds to a new dynamical system with b \u2192 b \u2013 Vc. In other words, in the regions when dead neurons are present, energy is not sensitive to the changes in the bias term.\nWe summarise arguments made in this section in the proposition below"}, {"title": "3.2 Stability of dynamical system", "content": "One of the main applications of the Lyapunov function is to perform stability analysis [7]. This part is relevant for predictive coding and related approaches [33], [25] to ensure the existence of at least some steady states. Besides, when associative memory is considered, one can only recover stable steady states, so stability directly influences memory capacity. Lyapunov function also characterizes a basin of attraction for a given steady state. In the context of associative memory, basins of attraction define a measure of similarity between inputs, since inputs from the same basin result in the recovery of the same memory. Below, we will show that in the regions with dead neurons, Lyapunov function (4) alone fails to help in stability analysis.\nStability analysis with the Lyapunov function boils down to the study of the energy landscape near the steady state. Using the Taylor series, we find\n\\begin{equation*}\nE(y^* + \\delta) \u2013 E(y^*) = \\delta^T \\frac{\\partial E(u)}{\\partial u}|_{u=y^*} + \\delta^T \\frac{\\partial^2 E(u)}{\\partial u^2}|_{u=y^*+t\\delta} \\delta, (\\delta, y^*) \\in (0, 1),\n\\end{equation*}\nwhere y*: Wg(y*) \u2013 y* + b = 0. Computing derivatives, we find\n\\begin{equation*}\nE(y^* + \\delta) \u2013 E(y^*) = \\delta^T (\\Lambda(y^*) \u2013 \\Lambda(y^*)WA(y^*)) \\delta, ||\\delta||_2 \\ll 1\n\\end{equation*}\nFor stability in the vicinity of a steady state, it is sufficient that the matrix above is positive definite, and for instability, it is sufficient that at least one eigenvalue of this matrix is negative. In situations such as in Figure 1 when energy has a flat direction nothing can be said about the dynamics in the region with flat energy. It is easy to show that if dead neurons are present at point u, matrix V lays in zero eigenspace of \u039b(u). \u03a4\u03bf show this we consider the Taylor series\n\\begin{equation*}\ng(u + Vc) = g(u) + \\Lambda(u + tVc)Vc, t \\in (0, 1),\n\\end{equation*}\nand since g(u + Vc) = g(u) we confirm that \u039bV = 0, i.e., zero eigenspace is always present\u00b9\nSince energy is insufficient to analyze stability, we need to use a dynamical system (1) directly. We suppose that in the region of interest V, matrix that describes dead neurons, does not change and use yd, ya defined in the previous section. With that, one can show that dynamical system becomes\n\\dot{y_a}(t) = P_aWg(y_a(t)) \u2013 y_a + P_ab, \\dot{y_a}(t) = P_dWg(y_a(t)) \u2013 y_a + P_db,  (8)\nwhere Pd = VVT and Pa = I \u2013 Pd. One can immediately see that dead neurons do not influence stability and, when steady state y is reached, one can find y\u2081 = PaWg(y) + Pab. Moreover, when ya(t) reaches steady state ya(t) converges exponentially from the arbitrary starting point, i.e., the whole flat region is a basin of attraction.\nThe sufficient condition for stability/instability of ya can be obtained from (8) with linearisation. More specifically, stability is defined by matrix S = V\u22a5T WV\u22a5 \u2013 V\u22a5T \u039bV\u22a5 = V\u22a5T WV\u22a5 - I = W\u22a5A\u22a5 \u2212 I \u2208 R^(N\u2212k)\u00d7(N\u2212k), where V\u22a5 \u2208 R^(N\u00d7N-k) is a matrix with orthogonal columns such that $P_a = V_{\\perp} V_{\\perp}^T$. When matrix S does not have eigenvalues with a positive real part, the steady state is stable; in case there is at least one eigenvalue with a positive real part, the steady state is unstable (see [7, Theorem 3.19]).\nThe structure of matrix S suggests that, in fact, dynamic is stable when energy function indicates that. Indeed, A1/2 W A1/2 has the same spectrum as W\u039b\u22a5A\u22a5, so sufficient condition for S is equivalent to sufficient condition for $A_{\\perp}^{1/2} W A_{\\perp}^{1/2} - I$. Finally since A\u22a5 is full rank we find that if \u039bWA \u2013 \u039b\u22a5 is negatively stable (has non-positive eigenvalues), the dynamics of ya is stable. Since this matrix is a restriction of \u039bWA - \u039b on the range of Hessian we conclude that one can analyze the stability of steady-state using energy if projector on the range of Hessian is known. It is important to note that matrix AWA - A can have other flat directions not corresponding to nullspace of Hessian, these directions will cause instability, so one need to distinguish them from directions corresponding to dead neurons."}, {"title": "4 Associative memories without dead neurons", "content": "We have seen that dead neurons present certain problems for energy function (4): (i) flat regions make optimization and analysis of energy cumbersome; (ii) one needs to distinguish between harmless flat direction of Hessian and other flat direction of energy that can compromise stability; (iii) energy do not contain full information and one need to build a projector on the nullspace of Hessian to restore the whole state. The last point implies that one needs Hessian even when stability analysis is not performed and one merely wants to compute a local minimum of energy which can often be done successfully with first-order methods.\nTo avoid these difficulties we will slightly modify the dynamics of associative memory (1) to minimally alter steady states and get a better energy function. The resulting system that we propose reads:\n\\dot{u}(t) = R(u) (g(Wu(t) + b) \u2013 u(t)), u(0) = u_0,  (9)\nwhere R(u) is a matrix-valued function to be specified later.\nIf one assumes that it is possible to select R(u) \u2260 0 for all u, the condition that we will verify later, steady states of the newly introduced system (9) follows from the old one with affine transformation y = Wu + b. This means all architectures possible with the old model (1) are also possible with the new one (9).\nDynamical systems (9) and (1) have equivalent steady states, but (9) allows to construct of a large set of Lyapunov functions with good properties in a systematic manner."}, {"title": "5 Conclusions", "content": "We describe the effect of dead neurons on the energy landscape of associative memory, analyze the con- sequences for stability, and provide several remedies, including new dynamical systems with good energy functions. We think it is appropriate to discuss the overall significance of the Lyapunov function for asso- ciative memory. Is it necessary to have this function at all? How is this function used in practice?\nOne may observe that, currently, the Lyapunov function is underutilized. As a rule, one uses several steps or even a single step of temporal dynamics of ODE during training and inference [10], [9], [24], [27], [18]. The role of Lyapunov's function is merely to provide comfort that some steady states may exist somewhere. As we show in this article, it is often a false comfort.\nIt is important to note, that the Lyapunov function in itself does not ensure stability: (i) it may be the case that no steady state exists, (ii) limit cycles may still be present, (iii) all steady states may be unstable. Besides that, for any steady state, the Lyapunov function can be constructed (by solving the Lyapunov equation) even when the \"global\" Lyapunov function is unavailable.\nThe Lyapunov function, as it is currently considered, is not of huge help. We can suggest several appropriate use cases: (i) model parameters may be adjusted on the training stage to make the Lyapunov function unstable for particular states \u2013 something that can not be easily done by integrating the dynamical system alone. This may help to learn from negative and adversarial examples [32], [6]; (ii) Lyapunov function may help in theoretical understanding of memory capacity, e.g., in the present article we have already shown that a whole flat direction corresponds to the basin of attraction, and can not support more than a single memory; (iii) Lapynov function may be directly used to compute and manipulate basins of attraction, potentially speeding up learning and making memory more robust to adversarial attacks.\nAll in all, we think that the Lyapunov function is a powerful tool that can lead to novel theoretical results and practical learning techniques in the field of associative memory. We hope that our results will inspire further research in this direction."}]}