{"title": "ASTRA: EFFICIENT AND MONEY-SAVING AUTOMATIC PARALLEL STRATEGIES SEARCH ON HETEROGENEOUS GPUS", "authors": ["Peiran Wang", "Haibing Li", "Haohan Fu", "Shiyong Li", "Yanpeng Wang", "Dou Shen"], "abstract": "In this paper, we introduce an efficient and money-saving automatic parallel strategies search framework on both homogeneous and heterogeneous GPUs: Astra. First, Astra searches for the efficiency-optimal parallel strategy in both GPU configurations search space and parallel parameters search space. Then, Astra also provides the solution on heterogeneous GPUs by mathematically modeling the time consumption of heterogeneous training. At last, Astra is the first to propose the automatic parallel strategy search on money-saving. The experiment results demonstrate that Astra can achieve better throughput than expert-designed strategies. The search time cost for Astra can also be limited to 1.27 seconds in a single-GPU setting and less than 1.35 minutes in a heterogeneous-GPU setting on average with an accuracy of over 95%.", "sections": [{"title": "INTRODUCTION", "content": "The large language models like GPT-4 (Achiam et al., 2023), Llama (Touvron et al., 2023a), Llama2 (Touvron et al., 2023b), Mistral (Jiang et al., 2023), etc., are consisted of billions of parameters and has achieved remarkable capa- bilities in natural language processing and other domains. Training such large models requires the parallel computing of numerous GPUs, and numerous GPU parallel training methods have emerged, such as data parallel, tensor par- allel, pipeline parallel, etc. These methods have different advantages and disadvantages, for instance, tensor parallel saves memory consumption but raises the training latency. There is not a single parallel method that is better than other parallel methods at each scene.\nThus, in real-world practice, LLM developers mainly com- bine those parallel methods into a hybrid parallel strat- egy to satisfy their needs. Many industry developer teams rely on human experts to manually design hybrid parallel strategies based on their experience. Other than relying on manual expert-designed hybrid parallel strategies, previous researchers have also proposed automatical paralleliza- tion, such as Alpa(Zheng et al., 2022), Aceso (Liu et al., 2024) and Galvatron(Miao et al., 2022), which automati- cally searches throughput-optimal hybrid parallel strategies based on cost models and optimization algorithms.\nHowever, previous methods still leave several realistic and important questions to answer. Now, assume you are a developer of a GPU training cloud service:\nRQ 1-Large search space: The GPU training cloud plat- form will have many customers with different needs. Differ- ent customers have different requirements for model type, model scale, GPU scale (tens of cards to tens of thousands of cards), and GPU type. Most customers of the GPU train- ing cloud platform are novice users (otherwise they would build their private cloud). When designing training strate- gies, they need a fast, accurate, and low-cost tool to obtain the best GPU configuration (GPU number) and the corre- sponding splitting parameters to guide training. However, previous methods only focus on the scenes of fixed GPU numbers and GPU types.\nRQ 2-Heterogeneous GPUs: In pursuit of extreme cost, many customers of cloud platform providers purchase differ- ent types of GPUs for heterogeneous training. The current method focuses on the scenario of a single GPU type and cannot solve the parallel strategy generation in this hetero- geneous GPU heterogeneous training scenario.\nRQ 3-Money saving: In addition to focusing on training"}, {"title": "RELATED WORK", "content": "In this section, we discuss the related works of Astra. First, we first discuss current large-scale distributed deep learning model training in \u00a76.1. Then, we introduce different parallel methods and the large search space of hybrid parallel in \u00a76.2, which drives the need to build auto parallel for large language model training."}, {"title": "Large-scale Distributed DL Training", "content": "In recent years, large-scale Transformer models have demon- strated exceptional performance across various application domains, pushing the frontiers of deep learning research. For instance, models like BERT (Devlin et al., 2018), GPT-3 (Achiam et al., 2023), and T5 (Ni et al., 2021) have achieved state-of-the-art results in natural language processing tasks such as machine translation, text classification, and question answering. These advancements underscore the capabilities of large-scale models in capturing complex patterns and generating high-quality outputs that surpass those of smaller models.\nHowever, the sheer size of these models poses significant computational challenges. GPT-3, for example, consists of 175 billion parameters (Achiam et al., 2023), making it impractical to train on a single GPU due to memory and processing constraints (Hoffmann et al., 2022). Single-GPU setups cannot handle the extensive computations and data storage required by such massive models (Hoffmann et al., 2022). As model sizes increase, the demand for compu- tational power and memory scales exponentially, further exacerbating the limitations of single-GPU environments (Geiping & Goldstein, 2023).\nTo address these challenges, the training of large-scale Transformer models necessitates the use of distributed train- ing across multiple GPUs (Narayanan et al., 2021b; Li et al., 2020; Narayanan et al., 2019). This approach leverages several GPUs' combined computational power and memory capacity, enabling the efficient handling of large models."}, {"title": "Different Parallel Methods", "content": "Existing parallel methods for training large-scale models include tensor parallelism (Narayanan et al., 2021b), data parallelism (Hillis & Steele Jr, 1986; Li et al., 2020; Agarwal et al., 2012), and pipeline parallelism (Li et al., 2021; Kim et al., 2023; Yang et al., 2021; Narayanan et al., 2021a; Zhao et al., 2021), each offering distinct advantages and catering to different aspects of the training process (see Fig. 1).\nTensor parallelism involves partitioning the model's ten- sors (such as weight matrices) across multiple GPUs, al- lowing parallel computation of operations like matrix multi- plications. This approach effectively reduces the memory footprint on each GPU by distributing the model parameters and computations, thereby accommodating larger models that would otherwise exceed the memory capacity of a sin- gle GPU (Narayanan et al., 2021b).\nData parallelism, on the other hand, splits the training data across multiple GPUs (Hillis & Steele Jr, 1986; Li et al., 2020; Agarwal et al., 2012). Each GPU maintains a com- plete copy of the model and processes a subset of the data. The gradients calculated by each GPU are then averaged and synchronized across all GPUs to ensure consistent model updates. This method is particularly effective in leveraging the computational power of multiple GPUs to handle large datasets and improve training speed.\nPipeline parallelism divides the model into sequential stages, each assigned to a different GPU (Li et al., 2021; Kim et al., 2023; Yang et al., 2021; Narayanan et al., 2021a; Zhao et al., 2021). As data flows through the pipeline, each GPU processes its assigned stage sequentially. This"}, {"title": "PROPOSED SCHEME", "content": "Astra uses MegatronLM as the runtime backend due to various advantages of it (\u00a73.1). Astra works as follows:\n1) Input Preprocess (\u00a73.2): Astra first extracts the parameter set of MegatronLM as the parameter search space of Astra. Then, the training model architecture is parsed. Next, the GPU selector of Astra iterates the GPU pool to generate a diverse set of GPU configurations, including the GPU type, model, number, etc.\n2) Parallel Strategy Search (\u00a73.3): Based on the three input information (GPU configurations, parameter set, model ar- chitecture), Astra runs its search space generator to generate a diverse set of parallel strategies. Then, the rule-based filter reads users' crafted rules to filter out the strategies not permitted by user-crafted rules. Next, the memory-based filter computes the memory allocated to each stage's GPU. If the memory is outside the upper boundary, the strategy is filtered.\n3) Heterogeneous GPU Search (\u00a73.4): We re-modeled the time cost of heterogeneous training so that Astra can search for heterogeneous training strategies. We also performed a computational complexity analysis on the search overhead of heterogeneous training.\n3) Cost Simulation (\u00a73.5): Then, given the filtered parallel strategies, the performance simulator computes the commu- nication cost and computation cost to get the overall time cost for each parallel strategy. Each operator's time cost is predicted by a trained XGBoost model.\n4) Money Calculation (\u00a73.6): At last, Astra calculates each parallel strategy's money cost for the training models. The money computation calculator calculates the money cost for each parallel strategy due to the overall time cost and GPU configurations."}, {"title": "MegatronLM-based Runtime", "content": "Astra leverages MegatronLM as the runtime backend for the parallel strategy execution. Below, we describe Mega- tronLM's key features and benefits, substantiating its selec- tion as our system's core runtime.\nOverview of MegatronLM. MegatronLM is a highly op- timized, scalable framework for training large-scale Trans- former models. Developed by NVIDIA, it has been widely adopted in research and industry for its robust performance and flexibility in handling diverse parallelism techniques. We utilize MegatronLM as the Astra's backend for the rea- sons below:\n[1] Support for diverse parallelism methods. Mega- tronLM is designed to handle a variety of parallelism strategies, including data parallelism, tensor parallelism, and pipeline parallelism. This versatility in parallelism methods allows MegatronLM to optimize training for different model architectures and hardware configura- tions, ensuring maximum efficiency.\n[2] Industry-validated reliability. MegatronLM's reliabil- ity has been extensively validated in industrial applica- tions. It is widely used in the industry for training some of the largest language models, such as GPT-3, demon- strating its robustness and stability. Furthermore, as an open-source framework, continuous contributions from both the research community and industry practitioners ensure that MegatronLM stays up-to-date with the latest advancements and best practices in deep learning."}, {"title": "Input Preprocess", "content": "The input preprocess stage in Astra involves several key steps to prepare and organize the data necessary for the subsequent parallel strategy search. This stage ensures the system can effectively explore various parallelization strate- gies and configurations to optimize model training. Here's a detailed breakdown of the process:\nGPU pool. Astra supports three search modes, and gener- ates the corresponding GPU pool:\nMode 1: homogeneous: Specify a GPU type and the num- ber of cluster GPUs, such as specifying A800 and 32768 GPUs, to find the best strategy:\n$C_{gpu} = (A800, 32768)$ \nMode 2: heterogeneous: Specify multiple GPU types and the number of cluster GPUs, such as specifying the number of cluster GPUs to be 8192, and the cluster uses two types of GPUs, A800 and H100 (the maximum number of these two GPUs can be specified at the same time), to find the best heterogeneous training strategy:\n$C_{gpu} = 8192, (A800 : 2048), (H100 : 7168)$ \nwhere maximum number of A800 is 2048, maximum num- ber of H100 is 7168.\nMode 3: cost: Specify a GPU type(for example, H100) the maximum number of cluster GPUs(for example, 4096) and the maximum money limit to find the best strategy:\n$C_{gpu} = \\{[(H100, 2)], [(H100, 4)], ...[(H100, 4096)]\\}$ \nEach configurations defines a runnable GPU collections for Astra.\nParameter set extraction. Astra extracts a detailed pa- rameter set P from the MegatronLM framework, including micro-batch size, recompute activations, pipeline model par- allel size, sequence parallel, tensor model parallel size, data model parallel size, off-load compute, recompute layers, etc. These parameters define the search space for possible parallelization parameter set:\n$P = \\{P_{1},P_{2},...,P_{n}\\}$\nModel architecture parsing. The system parses the archi- tecture of the training model, denoted as M. This includes details such as:\n$M = \\{model type, number of layers, hidden size, $\\$\nattention heads, intermediate size, vocabulary size$\\$\nIntegration of input information. The selected GPU con- figurations $C_{gpu}$, parameter set P, and parsed model archi- tecture M are integrated to form the foundational input for the parallel strategy search:\n$I = (C_{gpu}, P, M)$"}, {"title": "Parallel Strategies Search", "content": "The parallel strategy search stage generates, filters, and iden- tifies the most efficient parallelization strategies for training large-scale Transformer models. This stage consists of three components: (1) The search space generator generates diverse parallel strategies, (2) the rule-based filter filters il- legal parallel strategies according to user-defined rules, and (3) the memory-based filter filters out parallel strategies that are out of memory bounds.\nSearch space generator. The search space generator iter- ates all GPU configurations $C_{gpu}$ according to the user input, all potential parallel parameters value denoted as $f (P)$ with the fixed model architecture M to get all potential $s_{i} \\in S$. Each strategy is defined as\n$S_{i} = \\{C_{gpu}, P', M\\}, C_{gpu} \\in C_{gpu}, P' \\in f(P)$ \nThe total number of strategies S is determined by the prod- uct of all parameter options and the number of all GPU configurations:\n$S =  \\prod_{P' \\in f(P)} |P'| \\times |C_{gpu}|$\nRule-based filter. The Rule-Based Filter applies user- defined rules to filter out strategies that meet specific rules. Let $r_{1}, r_{2},..., r_{k}$ represent the rules. A strategy $s_{i} \\in S$ is valid if:\n$r_{j}(s_{i}) = False \\forall j\\in \\{1,2,...,k\\}$\nwhich suggests that if the strategy meets any of the rules, it is dropped.\nEach rule is a logical expression involving the strategy pa- rameters, such as:\n$r_{j} = ($use\\_flash\\_attn \\neq None$)^{\\$\n($recompute\\_granularity = selective$)\nIf any rule is met, the strategy is filtered out.\nTo illustrate the design of our rule-based filters, we provide three examples of rules:\n1. Flash attention rule:\n$use\\_flash\\_attn \\neq None &&$$\nrecompute\\_granularity = selective$\nThis rule ensures that the flash attention is being used, and the recompute granularity can not be selective.\n2. Layer recomputation rule:\n$recompute\\_num\\_layers >$$\npipeline\\_model\\_parallel\\_size$\nThis rule filters out the strategies where the number of layers to be recomputed exceeds the number of pipeline parallel stages.\n3. GPU division rule:\n$num\\_gpus%($pipeline\\_model\\_parallel\\_sizex$$\ntensor\\_model\\_parallel\\_size) \\neq 0$\nThis rule ensures that the total number of GPUs is divisible by the product of the pipeline model parallel size and tensor model parallel size.\nThese rules are written in a format:\nexpression &&/|| expression &&/|| expression... \nwhere && has a higher precedence than ||, and the expres- sions are evaluated from left to right.\nMemory-based filter. The Memory-Based Filter calculates the memory usage for each stage of a strategy. Let $M_{i}(s_{j})$ denote the memory required for stage i of strategy $s_{j}$. The strategy is filtered out if:\n$M_{i}(s_{j}) > M_{gpu}, \\forall i, s_{j}$\nOnly strategies that satisfy the memory constraints for all stages are retained for further evaluation:\n$S_{valid} = \\{s_{j} \\in S | M_{i}(s_{j}) \\leq M_{gpu}, \\forall i\\}$"}, {"title": "Heterogeneous GPUs Strategies Search", "content": "Compared with homogeneous GPU strategies search, het- erogeneous GPU strategies search has several problems:\n[1] In a heterogeneous GPU scene, the time cost of each pipeline stage varies, and each pipeline's correspond- ing bubble time also differs. The classic total dura- tion formula: $T_{total} = T_{comp}+T_{comm} + T_{bubble}$ with $T_{bubble} = \\frac{PP-1}{m} \\times (T_{comp}+T_{comm})$ is no longer appli- cable.\n[2] Each pipeline stage can be a different GPU type. The pipeline stages of different GPUs may have different numbers of model layers. The number of such combi- nations increases exponentially with the GPU type and the model number layers.\nDefinition. Assume there are M types of GPUs, with the computational capability of the i-th type denoted as $c_{i}$, and a maximum quantity of $l_{i}$ for each type. Set the following parameters: pipeline parallel size = P, data parallel = D, tensor parallel = T, number of micro-batches = K, model layers = N.\nConsider deploying P pipeline stages across M types of GPUs. The partition of P stages across M types of GPUs is denoted by $\\{p_{1},p_{2},...,p_{P}\\}$, where each $p_{i}$ is a value representing the GPU type it is deployed on, ranging from 1 to M. Thus, there are $O(M^{P})$ possible configurations. An example of a simple pipeline parallelization plan (assuming four pipeline stages deployed on four types of GPUs, each stage containing a number of model layers $n_{1}, n_{2}, N_{3}, n_{4}$ is shown in Fig. 3. Each pipeline stage has a different compu- tation latency, and the bubble time varies across stages, so the total latency cannot simply be the sum of the latencies and bubble times across all pipeline stages.\nHeterogeneous GPU cost model. Consider any given parti- tion where the P pipeline stages are divided across M types of GPUs. Let $t_{p_{i}}$ represent the computation latency of the i-th pipeline stage, and $h_{p_{i}}$ represent the peer-to-peer com- munication latency. The forward latency for this partition (the backward latency is similar) can be derived as follows:\n$\\sum_{1<i<P} (t_{p_{i}}+h_{p_{i}}) + (K - 1) \\times max(t_{p_{1}}+h_{p_{i}})$\nHere, $t_{p_{i}}$ depends only on the number of model layers in the i-th pipeline stage and the GPU it is deployed on, while $h_{p_{i}}$ depends solely on the tensor shape during communica- tion. Therefore, for any configuration, the order of $p_{i}$ can be rearranged to place identical values of $p_{i}$ in consecutive posi- tions (i.e., each segment of P pipeline stages is deployed on the same GPU type), reducing the total number of partitions from $O(M^{P})$ to $C_{P}^{M-1} \\times (M - 1)! \\approx O(P^{M-1})$.\nIn summary, when the model is deployed in a cluster with M types of GPUs, the mathematical model of its strategy search space is equivalent to solving the following equation for $m_{i}, n_{i}, and \\hat{n_{i}}$ (assuming the number of model layers in the pipeline stage on GPU type i is $n_{i}$):\n$\\begin{cases}\n  m_{i}, n_{i}  \\sum_{1<i<M} M_{i} = P, \\\\\n \\sum_{1<i<M} M_{i} \\cdot N_{i} = N\\\\\n  m_{i} \\leq \\frac{l_{i}}{D \\cdot T}\n\\end{cases}$\nSearch complexity analysis. Dividing P pipeline stages into M sequential segments results in $C_{P-1}^{M-1} \\times (M-1)! \\approx O(P^{M-1})$ possible configurations. Solving Equa- tion (23) for all combinations of $m_{i}$ has a time complex- ity of $O(P^{M-1})$, while for any fixed combination of $m_{i}$, solving Equation (23) for all $n_{i}$ has a time complexity of $<O(\\frac{N^{M-1}}{\\Pi m_{i}!})$.\nThus, the total time complexity for all combinations of $m_{i}$ and $n_{i}$ for Equation (23) is $O(\\frac{N^{M-1}}{\\Pi m_{i}!}) \\times O(P^{M-1})$."}, {"title": "Cost Simulation", "content": "The Cost Simulation stage is essential for evaluating the per- formance of the filtered parallel strategies from the Parallel Strategy Search stage. This stage estimates the overall train- ing time by considering communication and computation costs.\nInput: parallel strategy set. The parallel strategies that pass through the Rule-Based Filter and Memory-Based Fil- ter, denoted as $S_{valid}$, are considered for cost simulation. These strategies represent feasible configurations that satisfy all the constraints and are now evaluated for their compu- tational and communication efficiency. Let $S_{cost}$ represent the set of strategies selected for cost evaluation:\n$S_{cost} = \\{s_{1}, s_{2}, ..., s_{m}\\}, s_{i} \\in S_{valid}$\nXGBoost-based cost model. The computation time for each operator is estimated as follows:\n$T_{comp}(s_{i}) = \\frac{\\Theta_{comp}}{\\Phi_{comp} \\times \\Omega_{comp}}$\nwhere $\\Theta_{comp}$ is the theoretical computing overhead, $\\Phi_{comp}$ is the theoretical computing power, $\\Omega_{comp}$ represents the GPU efficiency, a parameter estimated via XGBoost, with values constrained within the range (0, 1].\nSimilarly, the communication time for each operation is calculated as:\n$T_{comm}(s_{i}) = \\frac{\\Theta_{comm}}{\\Phi_{comm} \\times \\Omega_{comm}}$\nwhere $\\Theta_{comm}$ is the theoretical communication overhead, $\\Phi_{comm}$ is the theoretical bandwidth, $\\Omega_{comm}$ represents the"}, {"title": "Money-limit Search", "content": "In this section, we present the process for calculating the monetary cost of parallel strategies in the Astra framework. The method involves traversing all possible strategies, select- ing the optimal strategies based on throughput and cost, and finally computing the financial cost for each strategy. This approach builds upon the principles outlined in Cost-based Astra Framework, where the highest throughput strategy is first identified without considering cost limitations.\nOptimal strategy pool. Once all possible valid strategies have been generated, we aim to create an optimal pool of strategies. Let $P_{i}$ represent the throughput of strategy i and $C_{i}$ its corresponding cost. We define the optimal pool such that for any strategy $(P_{i}, C_{i})$ in the pool, no other strategy $(P_{j}, C_{j})$ exists such that:\n$P_{j} > P_{i} and C_{j} < C_{i}$\nThe goal is to find the set of strategies $S_{opt}$ where:\n$S_{opt} = \\{(P_{i}, C_{i}) | \\nexists(P_{j}, C_{j})\\$\nsuch that $P_{j} > P_{i} > C_{j} < C_{i}\\$\nMoney cost calculation. The monetary cost for each strat- egy in the optimal pool is calculated based on the GPU usage and the total training time. Let $T_{i}$ be the total time required for strategy i, and let $g_{i}$ denote the cost per hour for using GPU g. The total cost $M_{i}$ for strategy i is given by:\n$M_{i} = T_{i} \\times N_{g_{i}} \\times F_{g_{i}}$\nwhere $F_{g_{i}}$ is the fee for GPU $g_{i}$ per second, $N_{g_{i}}$ is the number of GPUs used by strategy.\nUsing this formula, we can compute the monetary cost for all strategies in the optimal pool. The strategy with the highest throughput that meets the user's money constraints is selected as the final strategy.\nSorting strategies by cost and throughput. To sort the strategies, we first rank them by throughput in descending order. If two strategies have the same throughput, we then compare them by cost in ascending order. Formally, the sorting function S can be expressed as:\n$S(P, C) =  P_{i} > P_{j}, if P_{i} \\neq P_{j}$$$C_{i} <C_{j}, if P_{i} = P_{j}$\nThis ensures that the strategies are sorted optimally for both performance and cost."}, {"title": "IMPLEMENTATION", "content": "Searchable Parameters. We implemented Astra's searched parallel strategy on a MegatronLM backend. Since we need to search out different parameters for different parallel strate- gies, we listed our parameter search space in Appendix Table 3.\nSearchable GPUs. We implemented our system on Nvidia A800. Our hardware configurations enable each node to"}, {"title": "EXPERIMENTS", "content": "To prove Astra's optimal search ability on MegatronLM, we did a comparative analysis between Astra and experts on MegatronLM in \u00a75.1. Finally, we evaluate Astra to search for the finance-optimal plan under different settings in \u00a75.3."}, {"title": "Mode-1: Comparison with Expert Plans", "content": "Method. To prove the Astra's ability to search the optimal strategy on MegatronLM, we compared Astra with an ex- pert. We first selected three models with different parameter sizes (7 model settings in total): Llama-2 (7B, 13B, and 70B), Llama-3 (8B, 70B), and GLM (67B, 130B). Then, we offer 4 GPU number settings: 32, 128, 256, and 1024. Next, we asked six experts to craft a parallel strategy for each setting (different models and different GPU settings, overall 7 \u00d7 4 = 28 settings) based on their expert experience. Each participant has over six years of industry machine learning service or training experience. Then, we ran each of the six participants' parallel strategies for each setting on Mega- tronLM and picked the optimal one (one with the largest throughput) among the six expert-crated strategies as the expert-optimal strategy. At last, we run Astra to search the optimal parallel strategy automatically and compare the As- tra's parallel strategy's throughput with the expert-optimal parallel strategy's throughput.\nResults. As shown in Fig. 5, Astra demonstrates its ability to automatically generate parallel strategies that match or exceed expert-tuned plans across various model configura- tions. This highlights Astra's capability to generalize and optimize without manual intervention.\nA key finding is that Astra consistently matches or outper- forms manually designed strategies, showing that its auto- mated search can achieve results on par with domain experts. This adaptability extends across diverse hardware and model types, while specific setups often constrain expert-tuned plans. Astra dynamically adjusts to different configurations, optimizing parallel strategies based on the specific training environment.\nAnother important observation is Astra 's flexibility in com- bining different parallelism techniques-data, tensor, and pipeline. While expert strategies often focus on one type of parallelism, Astra optimally balances multiple forms, leading to superior performance, especially for large-scale models. This hybrid approach is likely the key to future parallelism strategies, where flexibility and adaptation are critical."}, {"title": "Mode-2: Heterogeneous GPU Search", "content": "Method. To evaluate Astra's performance in heterogeneous GPU environments, we conducted a comprehensive com- parison of Astra-searched strategies and expert-designed strategies under heterogeneous GPU configurations. We use Astra in the two GPU-heterogeneous environments with Nvidia H100 and A800 activated for search. Also, we follow the design of \u00a75.1, we recruit six experts to craft a hetero- geneous parallel strategy for each setting, and we picked the optimal one as the expert-designed strategy. We offer 4 GPU number settings: 64, 256, 1024, and 4096.\nBesides that, we also compared the heterogeneous GPU setting with single GPU setting in the same GPU number setting (1024). We compare the throughput between the dif- ferent settings (only A100, H100, H800, and heterogeneous settings)\nResults. As shown in Fig. 6, our experiments reveal that Astra consistently achieves higher throughput than expert- tuned configurations, particularly with larger models. Astra 's approach dynamically balances data, tensor, and pipeline parallelism across heterogeneous GPUs, a task often chal- lenging for manual tuning. This adaptability highlights the efficiency of automated strategies, especially in cloud-based or distributed environments where GPU types may vary.\nOverall, Astra 's heterogeneous GPU search framework of- fers a scalable, cost-effective solution for optimizing model training in heterogeneous hardware contexts.\nTable 2 shows the heterogeneous GPU setting compared with a single GPU setting. Though a heterogeneous GPU setting strategy can not beat the performance of a single- GPU setting strategy, Astra's searched strategy can nearly match with them."}, {"title": "Mode-3: Evaluation Performance on Financial Cost", "content": "Search pools for GPU. To comprehensively evaluate the financial cost performance of Astra, we incorporate a vari- ety of GPU types commonly used by major cloud service providers. Our search pools include the following GPU models: NVIDIA H100, A800 and H800.\nThese GPUs represent a range of performance capabilities and costs, providing a realistic and comprehensive basis for evaluating the financial efficiency of our system. By including these diverse GPU options, we can simulate the decision-making process of users who leverage cloud-based GPU resources, allowing us to optimize for both time and financial cost under various configurations."}, {"title": "RELATED WORK", "content": "In this section, we discuss the related works of Astra. First, we first discuss current large-scale distributed deep learning model training in \u00a76.1. Then, we introduce different parallel"}, {"title": "Large-scale Distributed DL Training", "content": "In recent years, large-scale Transformer models have demon- strated exceptional performance across various application domains, pushing the frontiers of deep learning research. For instance, models like BERT (Devlin et al., 2018), GPT-3 (Achiam et al., 2023), and T5 (Ni et al., 2021) have achieved state-of-the-art results in natural language processing tasks such as machine translation, text classification, and question answering. These advancements underscore the capabilities of large-scale models in capturing complex patterns and generating high-quality outputs that surpass those of smaller models.\nHowever, the sheer size of these models poses significant"}, {"title": "Different Parallel Methods", "content": "Existing parallel methods for training large-scale models include tensor parallelism (Narayanan et al., 2021b), data parallelism (Hillis & Steele Jr, 1986; Li et al., 2020; Agarwal et al., 2012), and pipeline parallelism (Li et al., 2021; Kim et al., 2023; Yang et al., 2021; Narayanan et al., 2021a; Zhao et al., 2021), each offering distinct advantages and catering to different aspects of the training process (see Fig. 1).\nTensor parallelism involves partitioning the model's ten- sors (such as weight matrices) across multiple GPUs, al- lowing parallel computation of operations like matrix multi- plications. This approach effectively reduces the memory footprint on each GPU by distributing the model parameters and computations, thereby accommodating larger models that would otherwise exceed the memory capacity of a sin- gle GPU (Narayanan et al., 2021b).\nData parallelism, on the other hand, splits the training data across multiple GPUs (Hillis & Steele Jr, 1986; Li et al., 2020; Agarwal et al., 2012). Each GPU maintains a com- plete copy of the model and processes a subset of the data. The gradients calculated by each GPU are then averaged and synchronized across all GPUs to ensure consistent model updates. This method is particularly effective in leveraging the computational power of multiple GPUs to handle large datasets and improve training speed.\nPipeline parallelism divides the model into sequential stages, each assigned to a different GPU (Li et al., 2021; Kim et al., 2023; Yang et al., 2021; Narayanan et al., 2021a; Zhao et al., 2021). As data flows through the pipeline, each GPU processes its assigned stage sequentially. This"}, {"title": "Manual hybrid parallelism", "content": "In practice, experts often combine these parallelism techniques to create hybrid par- allelism strategies tailored to the specific requirements of their models and hardware configurations (Song et al., 2019). This expert-crafted hybrid parallelism involves significant manual effort and domain knowledge to balance the trade- offs between computation, memory usage, and communi- cation overheads. For instance, systems like DeepSpeed (Rasley et al., 2020) and Megatron-LM (Narayanan et al., 2021b) employ a mix of data parallelism, tensor parallelism, and pipeline parallelism to optimize the training of massive models like GPT-3 (Achiam et al., 2023)."}, {"title": "Automatic parallelism", "content": "However, manual hybrid paral- lelism has its limitations. The complexity and diversity of modern deep learning models and hardware environments make it increasingly difficult for experts to design optimal parallelism strategies. This is where automatic parallelism comes into play. Automatic parallelism systems, such as Alpa (Zheng et al., 2022), Metis (Um et al., 2024), and Galvatron (Miao et al., 2022), automate finding efficient par- allelism plans by exploring a vast search space of potential strategies using advanced algorithms and heuristics."}, {"title": "DISCUSSION", "content": "Adaptation to heterogeneous environments. Astra's abil- ity to adapt to dynamic and heterogeneous GPU clusters pro- vides significant advantages for real-world deployments. In contrast to other frameworks that may be optimized for spe- cific hardware setups, Astra can adjust its strategies based on the available resources, making it particularly suited for cloud-based or distributed environments where hardware configurations may vary.\nScalability and future directions. The scalability demon- strated by Astra, particularly in handling large models like Llama-70B, is a testament to its robustness. However, as models and GPU configurations continue to grow in com- plexity, further optimizations in the simulation phase could be crucial for maintaining performance. This suggests fu- ture work may focus on improving the efficiency of the simulation component to prevent bottlenecks as the search space expands."}, {"title": "CONCLUSION"}]}