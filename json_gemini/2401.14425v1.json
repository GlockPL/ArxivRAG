{"title": "No Longer Trending on Artstation: Prompt Analysis of Generative AI Art", "authors": ["Jon McCormack", "Maria Teresa Llano", "Stephen James Krol", "Nina Rajcic"], "abstract": "Image generation using generative AI is rapidly becoming a major new source of visual media, with billions of AI generated images created using diffusion models such as Stable Diffusion and Midjourney over the last few years. In this paper we collect and analyse over 3 million prompts and the images they generate. Using natural language processing, topic analysis and visualisation methods we aim to understand collectively how people are using text prompts, the impact of these systems on artists, and more broadly on the visual cultures they promote. Our study shows that prompting focuses largely on surface aesthetics, reinforcing cultural norms, popular conventional representations and imagery. We also find that many users focus on popular topics (such as making colouring books, fantasy art, or Christmas cards), suggesting that the dominant use for the systems analysed is recreational rather than artistic.", "sections": [{"title": "Introduction", "content": "In just a few years, generative AI has become a dominant new paradigm for the creation of high-quality digital media, including images, video and text. One recent estimate put the number of AI generated images created to date at over 15 billion - exceeding the number of photographs taken in the first 150 years of photography [37]. Text-to-image (TTI) systems, such as Stable Diffusion, Midjourney or DALL-E allow the creation of high-quality images or illustrations just by supplying a short text description (prompt) of the desired image. These systems have become so popular, that \"prompting\" is now being considered as a part of the skillset of successful commercial art production.\nHowever, as is the case with most recent generative AI advances, these systems raise many cultural, ethical and conceptual issues. Exploring and understanding these issues is increasingly important, given the rapid update and normalisation of generative AI imagery into mainstream software tools\u00b9."}, {"title": "Background and Related Work", "content": "concerns raised by generative AI include bias and the reinforcement of cultural stereotypes, concerns regarding censorship, issues of data laundering [2] and training on copyrighted data, \"style theft\" and the automation of specific artistic styles, questions of authenticity, \"hallucinations\" and \"deep-fake\" imagery, loss of traditional human skills, cultural and aesthetic homogenisation [11,23,35]. At the centre of many of these issues are the prompts themselves, the language they cultivate, and the type of images they produce. By understanding how collectively - people are expressing their ideas through prompts, we can improve our understanding of how generative AI is impacting visual art and culture.\nIn this paper we examine several large prompting datasets, using Natural Language Processing (NLP) and data visualisation to draw a broad understanding of how users of TTI systems utilise language in generative AI systems. Our overall aim is to improve understanding of generative AI's influence on how we think when making a prompt and in particular, our use of language in relation to art-making and creativity. Hence, we focus on how artistic concepts are represented in prompt language; tracing their evolution as TTI systems continue to become more prolific and technically sophisticated. To this end, we compare prompting datasets from mid-2022\u00b2 to November 2023, by which point, TTI systems had become widely adopted. We also explore how TTI systems have and continue to impact human artists and illustrators."}, {"title": "Text-to-Image Generation", "content": "The introduction of diffusion models marked a significant leap in image generation quality over previous methods such as Generative Adversarial Networks (GANs) [13]. The fundamental innovation of recent prompt-based image generators is the fusion of two independently powerful models \u2013 CLIP (Contrastive Language-Image Pre-training) [29] for text-based understanding, and Diffusion Models for image synthesis \u2013 into a cohesive system capable of generating images from textual prompts.\nCLIP was trained on dataset of 400 million images and their associated textual descriptions. Unlike predecessor models that focused on direct object recognition, CLIP also set out to understanding the context, style, and other abstract concepts conveyed through language. The training set was scraped from online sources: social media posts, image-caption pairs, alt text descriptions, among others. For TTI models, CLIP serves as a mechanism to interpreting prompts. When a user inputs a textual description, CLIP analyses this text to construct the visual elements and themes that should be present in the output image."}, {"title": "Prompt and Image analysis", "content": "With the introduction of TTI systems, in addition to being a creative tool, generative AI has become a social phenomenon [30]. Writing prompts is primarily an iterative, trial and error process; an exploratory practice [40]. More importantly, specific terms that coax the model into generating high-quality results have become a significant aspect of successful prompting.\nExisting literature in prompt analysis has focused on aiding prompt engineering and making improvements to the underlying models [41]; for instance, by identifying popular or \"most useful\" keywords and model parameters that would allow users to generate higher quality images [21,42,28]. Xie at al [40] further propose using higher rating prompts \u2013 in which their analysis pointed at longer prompts, and prompts including artists' names as a way to further train the models, feeding back to the systems what users want. These approaches to prompt analysis pay less attention to large-scale analysis from a cultural perspective, and often favour feedback approaches that tend to homogenise content and reinforce popular clich\u00e9s, amplifying concerns about biases, stereotypes, authorship and authenticity. Additionally, they reinforce the artistic styles of just a handful of specific artists (see our analysis below).\nA recent study undertook a more exploratory approach to prompt analysis [30] using the DiffusionDB database [39]. Applying topic analysis, the author identified a taxonomy of prompt specifiers and developed a model for identifying the categories from this taxonomy from TTI prompts. Although the main goal of this study was the development of an interactive tool to assist with prompting, the results from this work open up possibilities for better understanding of what topics are being expressed in prompts. We build upon this work, performing topic analysis on a more recent dataset, and examining how current trends in prompting may inform thinking about art and visual culture.\nVisual analysis of AI-generated images has also become a topic of significant interest, with current studies focused on identifying biases (e.g. under-representing certain race groups [3,26]), cultural gaps (e.g. over-representing specific nations [26]), and the reinforcement of stereotypes (e.g. \u201ca photo of a lawyer\" consistently showing a white male) [5]). An analysis of 3,000 AI images depicting national identities also highlighted these tendencies towards bias and stereotypes of TTI systems (e.g. New Delhi's streets were mostly portrayed as polluted and littered) [35]. This perpetuates cultural norms that are prevalent in training datasets while under-representing less stereotypical and non-Western aspects of culture, art and society. Although some researchers have proposed ways to mitigate these effects, such as adding specific phrases (e.g. \"irrespective of gender\" [3]) or through the use of multilingual prompts (e.g. \"a photo of a king \" appending a Russian character to the prompt [38]), these mitigation strategies are often ineffective (e.g. despite explicitly mentioning words such as \"white\", \"wealthy\u201d or \u201cmansion\", the authors in [5] report that Stable Diffusion continues to associate poverty with people of colour). In this paper we follow a different perspective from the aforementioned studies and perform the first\""}, {"title": "Datasets", "content": "To perform our analysis we used three different datasets:\n1. DiffusionDB: a text-to-image prompt dataset containing 14 million images generated by Stable Diffusion, 1.8 million unique prompts, and hyperparameters [39]. Collected over a 2 week period in August, 2022, the dataset is publicly available at: https://poloclub.github.io/diffusiondb;\n2. Midjourney 2022 Discord dataset: 248k prompts and their associated generated images obtained by scraping ten public Discord channels over 28 days in June 2022 [34];\n3. Midjourney 2023 dataset: 2.84M prompts and associated generated images obtained by scraping public Discord channels over 16 days in October-November 2023. This dataset was created by the authors of this paper and is publicly available [24].\nDue to the stochastic nature of the image generation process, it is common for prompting interfaces to produce a number of image variants from a single user supplied prompt. In Midjourney for example, the default is to generate four images, from which the user has options to regenerate, upscale, or produce four new variants from one of the generated images. For our prompt analysis we only consider the initial generation, removing identical prompts that appear in up-scaling or variant generation from our analysis. For the DiffusionDB dataset, tiled images were automatically split into separate images by the dataset's authors (one of the reasons why the number of images is much greater than the number of unique prompts).\nBoth the DiffusionDB and Midjourney 2022 Discord dataset (hereafter MJ-2022) were generated in the early stages of development and public release of these systems (around mid 2022). DiffusionDB was obtained by scraping public Discord channels when Stable Diffusion was in public beta testing. Midjourney has always used Discord as its interface for prompt generation.\nFollowing an initial analysis of the DiffusionDB and MJ2022 datasets, we opted to create a new, more recent dataset to compare how prompting might have changed over the course of one year\u00b3.\nAll the datasets include the full prompt text, user id, generated image URL and timestamp, the DiffusionDB dataset also includes other metadata, including NSFW scores for images and prompts, configuration parameters specific to stable diffusion and image dimensions. These additional fields were not used in our analysis."}, {"title": "Initial Processing", "content": "Before analysing the datasets we performed some basic cleanups to help ensure the reliability of the analysis. This included removing any records with missing data or empty prompts, incomplete requests, and non-image generating prompts. Prompts without validated users were removed from user statistics."}, {"title": "Dataset Statistics and Initial Analysis", "content": "The most obvious change in the overall dataset comparisons shown is the growth in the number of users Midjourney data from 2022 has less than 1.7k unique users, whereas just over a year later there are over 34k users, a twenty-fold increase. While there are more people using these TTI systems, they are not using the system as often or as much: the distribution of users vs. prompts shows a small number of \u201cpower users\" (people spending a large majority of their time on the system), and a large number of casual users, with 75% of users entering less than 15 prompts over the collection period in 2023.\nOur analysis was performed using language tools based on English. However, we analysed the datasets to determine the language composition of prompts, using the fastText model [16] to automatically classify each prompt by language. As can be seen, English is by far the dominant language used in prompting within these systems, with over 99% of prompts in each dataset identified as English\u2074. However, we note a significant increase in non-English prompts in languages such as French and Spanish between 2022 and 2023.\nPrompts were divided into components (specifiers), separated by commas or periods, ignoring letter case and removing a small set of stopwords, system commands and punctuation (similar to [30]). We also ensured that terms utilising"}, {"title": "Use of Artist Names", "content": "\"Stability has a music generator that only uses royalty free music in their dataset. Their words: \"\"Because diffusion models are prone to memorisation and overfitting, releasing a model trained on copyrighted data could potentially result in legal issues.\"\" Why is the work of visual artists being treated differently?\u201d[1] (Lois Van Baarle - #3 artist in DiffusionDB).\nIt has become a common practice to use artist's names in prompts as a way of generating images in the style of that artist - what has been called \"style theft\" [23]. We used the SpaCy6 NLP library to perform identification of people's names in the datasets. We also looked at specifiers that contain the term \"style\", as often desired styles reference artworks or organisations rather than creators (e.g. \"Banana Fish anime style\" or \"Pixar style\" - Table 4)."}, {"title": "Topic Analysis", "content": "Building on a previous topic analysis of the DiffusionDB dataset [30], we conducted a topic analysis on prompt specifiers in the MJ2023 dataset. Prompt specifiers with 100 or more uses were included in the analysis, resulting in 1700 individual prompt specifiers. The topic modelling adhered to the approach outlined in [14] and involved first utilising the MPnet (Masked and Permuted Pre-training for Language Understanding) model [33] to encode prompt specifiers into vector representations. MPnet, a language model specialising in language understanding, is capable of embedding phrases into a latent space that proves effective for diverse language tasks, including clustering and sentence similarity. As in [30], we utilised the all-mpnet-base-v2 pre-trained weights and the sentence-transformer library [31] for encoding. The UMAP [25] dimensionality algorithm was then applied to the vector embeddings to reduce them to a 5-dimensional space in order to mitigate the negative effect of high dimensionality on clustering performance [7]. We utilised, the hierarchical density-based spatial clustering (HDBSCAN) algorithm [8] to identify topic clusters within the embedding space resulting in an initial collection of 40 topics."}, {"title": "Coda: What are all these images actually of?", "content": "So far we have looked at prompting and its implications through the lens of the prompts themselves, where language is used as a proxy for the user's intent. A prompt writer must craft the prompt to get the image they want and the success (or otherwise) of that prompt is evaluated visually rather than linguistically. As we have discussed, prompting is a rather convoluted process, requiring specific keywords around style and surface aesthetics, which tend to dominate our analysis of over 3M prompts from the three datasets."}, {"title": "Summary and Conclusion", "content": "Our datasets represent results only for two systems: Stable Diffusion and Midjourney. Other popular systems, such as DALL-E, Leonardo, Crayion, Firefly are all propriety, making access to prompts difficult. As Stable Diffusion is an open-source model, it is no longer run via Discord and users are free to download their own versions. One possibility to address this issue would be to scrape data from the many \"prompt showcase\" websites that communities of people can post"}, {"title": "Limitations of the study", "content": "AI-generated images and their associated prompts (e.g. sites such as Prompt Hero https://prompthero.com), we leave this as future work."}, {"title": "Findings", "content": "As shown, Midjourney's user base has grown significantly between 2022 and 2023. The median prompt length is reducing, probably due to changes in the software that require less specialist prompting (hinting at the tool's accessibility). While there is a significant increase in users, the majority of those users do not prompt very much, suggesting that for many people, the use of TTI systems is largely recreational (something confirmed informally by a poll in [17]). This observation is further reinforced from our topic analysis of both prompts and images, with topics such as \"colouring books\" and \"Christmas\" being popular (MJ2023 data was collected in November, leading to the Christmas period).\nOur analysis shows that prompting emphasises and privileges popular styles and surface aesthetic appearances (\"cinematic lighting\", \"photorealistic\", \"ultra detailed\"). It forces a fixation on surface aesthetics, potentially at the expense of other important factors in art making, including narrative, realism, authenticity and individuality.\nWhile we identified a wide variety of artistic styles, TTI systems tend towards the popular, reinforcing stylistic norms and aesthetic \"sameness\". Our image analysis showed what seems like common knowledge when viewing websites such as the Midjourney Showcase or various \"Prompt Art\" sites: that most of the images are closeups or medium shots of young women. Genres of fantasy art, game art and comic or anime illustration dominate the specifiers used in prompting. As prior research has demonstrated, TTI systems continue to promote and reinforce racial, gender and other cultural biases. Despite the vast volume of new synthetic images generative AI has brought us, its seems that most people using these systems are after \"more of the same\". In the process of working with these datasets and being exposed to many thousands of images, our view is that few if any AI images are memorable in the way that human art is, and its difficult to see how they can be aesthetically unique in the way that human art can.\nThe agency exerted by TTI systems onto the human user, while difficult to quantify, is an important topic for future investigation. When viewing TTI systems as a creative medium, their inherent properties will inevitably shape and contribute to the future of image production. The precise way in which this occurs is multiple and nuanced. In this paper, we set out to trace this effect through a qualitative interpretation of a statistical analysis of the language employed in TTI systems and the images they produce. This leads us to conclude that generative AI imagery, at least in its current form, is probably not a serious threat to human art. After all, surface mimicry of a popular style does not constitute an artistic innovation or practice. A physical painting will always embody the act and intention of a human artist, something that no synthetic AI image can ever do."}]}