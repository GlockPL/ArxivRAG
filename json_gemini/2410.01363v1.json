{"title": "PCQPR: Proactive Conversational Question Planning with Reflection", "authors": ["Shasha Guo", "Lizi Liao", "Jing Zhang", "Cuiping Li", "Hong Chen"], "abstract": "Conversational Question Generation (CQG) enhances the interactivity of conversational question-answering systems in fields such as education, customer service, and entertainment. However, traditional CQG, focusing primarily on the immediate context, lacks the conversational foresight necessary to guide conversations toward specified conclusions. This limitation significantly restricts their ability to achieve conclusion-oriented conversational outcomes. In this work, we redefine the CQG task as Conclusion-driven Conversational Question Generation (CCQG) by focusing on proactivity, not merely reacting to the unfolding conversation but actively steering it towards a conclusion-oriented question-answer pair. To address this, we propose a novel approach, called Proactive Conversational Question Planning with self-Refining (PCQPR). Concretely, by integrating a planning algorithm inspired by Monte Carlo Tree Search (MCTS) with the analytical capabilities of large language models (LLMs), PCQPR predicts future conversation turns and continuously refines its questioning strategies. This iterative self-refining mechanism ensures the generation of contextually relevant questions strategically devised to reach a specified outcome. Our extensive evaluations demonstrate that PCQPR significantly surpasses existing CQG methods, marking a paradigm shift towards conclusion-oriented conversational question-answering systems.", "sections": [{"title": "1 Introduction", "content": "Conversational Question Generation (CQG) has significantly enhanced the capabilities of conversational question-answering (QA) systems, bringing a level of dynamism and intelligence that was previously unattainable. In various fields such as educational technology, customer service, and interactive entertainment, CQG has become a crucial component for enhancing user interactions, enabling more natural and responsive conversations between humans and machines. By generating contextually appropriate questions, CQG systems have improved the interactive experience, making conversations with AI more engaging and informative.\nDespite these advancements, current CQG methods are predominantly reactive, generating questions based solely on the immediate context without strategically considering the specified conversational outcome. This reactivity is evident in various existing approaches: from choosing the most rel-"}, {"title": "2 Related Work", "content": "Question Generation. Question Generation (QG) aims to generate questions from diverse inputs, including text , knowledge base , and so on. Traditionally, QG has played a pivotal role in numerous practical applications, focused on producing contextually appropriate and meaningful questions .\nRecently, research has increasingly focused on Conversational Question Generation (CQG), which emphasizes multi-turn interactions to better simulate dynamic and natural conversational scenarios. In this domain, frameworks such as that proposed by Gu et al. (2021) incorporate answer encoders and QG modules to learn from each conversational turn and generate subsequent questions. Similarly, Do et al. (2022) apply top-p strategies to select pertinent conversation history, thereby enhancing question relevance. Other notable approaches include Ling et al. (2023)'s context-enhanced neural model for identifying key contexts in question generation, and Zeng et al. (2023)'s Zero-shot CQG, which utilizes transfer learning for multi-turn scenarios from single-turn QG instances. Furthermore, the 'what to ask' and 'how to ask' modules of Do et al. (2023)'s two-stage framework further high-"}, {"title": "3 Problem Formulation", "content": "A conversational question generation dataset\u00b9 is denoted as D = {Ci, Hi, Ti, Gi} with N as the total number of conversations. Ci = {$1, $2,..., Sm} with m sentences represents the context related to the i-th conversation. Hi = {(91, ani), (q2, an\u2082), ..., (qt, ant)} with t question-answer pairs represents the conversational history in the i-th conversation. Ti denotes the predefined conclusion consisting of a question-answer pair (qn, ann). Gi = {(qt+1, ant+1), (qt+2, ant+2), ..., (qn\u22121, ann\u22121)} represents the ground-truth responses to achieve the specified outcome T\u2081. The task of CCQG is formalized as follows: given a predefined outcome Ti, a context Ci, and a conversation history Hi, the objective of CCQG is to proactively guide the conversation to generate desired responses\u00b2 \u011ci, thereby reaching the predefined outcome Ti."}, {"title": "4 Methodology", "content": ""}, {"title": "4.1 Overview", "content": "To address the CCQG task, we propose a novel framework called PCQPR, designed to proactively steer the conversation towards achieving the specified outcome. The framework initially employs the Monte Carlo Tree Search-like (MCTS-like) algorithm for planning, performing a lookahead search to generate the desired response. Subsequently, it"}, {"title": "4.2 Planning with Monte Carlo Tree Search", "content": "We explain how to (1) formulate the CCQG task as a Markov decision process (MDP) and (2) solve it by MCTS-like planning algorithm."}, {"title": "4.2.1 MDP", "content": "For the CCQG task, we utilize the LLM (e.g., ChatGPT) to generate an action for the current state, thereby obtaining the next state. Specifically, a state s is the concatenation of the context Ci, the conversation history Hi, and the partially or completely generated response Gi, where the complete response signifies the attainment of the designated outcome Ti. An action a comprises a question-answer pair that forms the basis of the complete response. The terminal action matches or closely mirrors the predefined outcome, indicating the achievement of the desired goal. The transition function systematically combines a state s with an action a. An episode concludes once the LLM executes the terminal action. To accurately evaluate the state s, the reward for s is determined by how closely the responses reach a specified outcome, given that s matches or closely resembles the predefined outcome 3, i.e., REWARD(response) (line 17 in Algorithm 1). If the response fails to meet the specified outcome, the reward is set to 0.\nTo determine the optimal policy in an MDP, we explore a tree search-based planning algorithm, drawing inspiration from MCTS . More concretely, the tree search algorithm, inherently structured like a tree, assigns nodes to represent various states and uses edges to indicate actions. This algorithm begins at the root node, which is the initial state, and it explores the state space to identify terminal states with high rewards. Each node in the tree search algorithm encompasses two essential elements: the visited number and a value function. The visited number indicates how many times each node has been visited, while the value function represents the maximum reward obtained by starting from the node (or state) 4 s and executing action a. The algorithm effectively maintains a proper balance between exploration and exploitation. In the subsequent portion of this section, we detail the method of incorporating the tree search algorithm into the planning procedure."}, {"title": "4.2.2 MCTS-like Planning", "content": "To effectively guide the conversation toward a specified outcome, we develop an LLMs-based plan-"}, {"title": "4.3 Comparable Reflection", "content": "Although the MCTS-like planning algorithm demonstrates commendable performance in the CCQG task, there is significant potential for further improvement. Notably, some initial planning paths fail to achieve the desired outcomes, underscoring the need for a more refined approach to enhance the algorithm's effectiveness. Inspired by human cognitive strategies, which learn from past successes and failures to improve future performance, we propose an enhancement for the MCTS-like planning algorithm through a comparable reflection mechanism that summarizes both errors and successful experiences. This mechanism provides the algorithm with detailed verbal feedback on previous planning paths, enabling it to optimize future decision-making processes.\nTo implement this enhancement, we integrate a self-reflection mechanism powered by advanced LLMs (e.g., ChatGPT). This marks a significant advancement in the MCTS-like planning algorithm's effectiveness for the CCQG task. The sophisticated analytical capabilities of LLMs enable comprehensive evaluations of planning paths, generating detailed verbal reflections for each sequence of actions (i.e., COMP_REFINE(response), see line 22 in Alogrithm 1). These reflections serve as a semantic guidance signal, providing the algorithm with concrete directions for improvement by pinpointing failure points and highlighting successful strategies. This balanced view of what works and what doesn't fosters a deeper comprehension of the decision-making dynamics, enabling the detection of patterns that lead to both suboptimal results and successful outcomes. The algorithm can be finely tuned by systematically analyzing these patterns to anticipate and circumvent similar pitfalls while replicating successful strategies in future iterations. This reflective process provides valuable insights that optimize the algorithm's behavior, enhancing its ability to solve complex scenarios through iterative trials and self-reflection.\nFor example, in planning, there are two initial planning paths: a failed path P1 = (a1,..., an) where the terminal action an diverges from the outcome Ti, and a successful path p2 = (a1, ..., an) where the terminal action am matches the outcome Ti. The comparable reflection mechanism provides fine-grained feedback for two planning paths. For the failed path p1, it identifies the points of divergence and reasons for failure, providing critical feedback on what went wrong. Conversely, for the successful path p2, it recognizes the key decisions that led to a positive outcome, reinforcing effective strategies. This dual analysis ensures that the algorithm not only learns from its mistakes but also builds on its successes, leading to continuous improvement and more reliable planning outcomes. This comparable reflection mechanism ensures that future iterations of the planning algorithm are better equipped to handle similar situations, ultimately enhancing performance and decision-making capabilities in complex scenarios. To the best of our knowledge, we are the first to combine a tree search algorithm with a comparable reflection strategy to enhance the performance of the novel CCQG task."}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Experimental Settings", "content": "Datasets. As the novel task setting of CCQG has not been investigated in prior research, no direct dataset is available. Hence, we modify two popular conversational datasets (i.e., CoQA and QuAC ) to meet the requirements of the CCQG task. For the CoQA dataset, we standardize the number of turns in all conversations to 10 because some have too many turns. For the QuAC dataset, we include only conversations where the turn is labeled with 'followup: y'or \u2018followup: m\u2019, resulting in a training set of 2,749 conversations and a validation set of 221 conversations. We use the last three question-answer pairs as the target pairs. The two datasets comprise conversations across a wide range of domains, where each conversation consists of a relevant context and several question-answer pairs. Following prior studies , we use the validation sets from two datasets as our test sets since the original test sets are unavailable.\nAutomatic Evaluation Metrics. We employ widely used evaluation metrics for text generation, including BLEU and METEOR , to evaluate the generated response. Concretely, BLEU evaluates the precision of the generated text compared to the reference text. METEOR is a detailed evaluation considering exact words, synonyms, and similar phrases. Additionally, we employ SimCSE to measure conversational coherence. This method regards the generated response as the premise, while the previous conversational history serves as the hypothesis. It then calculates a similarity score between them to assess topic coherence. In our experiments, we assess the conversation coherence score by focusing on the last one and last two prior turns, referred to as Conv-last1 and Conv-last2, respectively. Furthermore, the most pivotal metric is the Success Rate, i.e., Success Rate =  Number of Successful Outcomes / Total Number of Instances which is employed to measure the capacity to achieve the intended conversational outcome.\nHuman Evaluation Metrics. Due to the extensive critique of automatic metrics for their poor alignment with human assessments , we incorporate two human evaluation metrics: Coherence and Effectiveness. The former met-"}, {"title": "5.3 Human Evaluation", "content": "To comprehensively validate the effectiveness of our approach, we conduct a rigorous human evaluation. This involves providing scorers with detailed examples, designed to guide them towards objective and fair assessments of generated responses. The results presented in Table 3 demonstrate the superiority of our method compared to traditional baselines. Notably, our approach shows superiority in terms of conversational coherence (i.e., \"Coherence\") and in successfully reaching the designated conversational outcome (i.e., \u201cEffectiveness\"). These two aspects are crucial in assessing the quality of conversational question-answering systems, as they directly reflect the system's ability to maintain a natural and purposeful conversation flow. To ensure the reliability of these evaluation results, we further employ Fleiss's kappa, a statistical measure designed to assess the consistency of agreement among multiple scorers. The kappa values range between 0.41 and 0.60, indicating moderate agreement among the three scorers. This range suggests a reasonable consensus, lending further credibility to our results."}, {"title": "5.4 Ablation Studies", "content": "To verify the effectiveness of PCQPR, we conduct extensive ablation experiments."}, {"title": "5.4.1 Effect of MCTS-based Planner", "content": "To investigate the effectiveness of our proposed MCTS-based planner, we conduct an experiment where the MCTS-based planner is removed, denoted as \"w/o MCTS\u201d. Table 4 reports the results on Conv-last1 and Success Rate. We observe that removing the MCTS-based planner results in a reduction of 8.36% in Conv-last1 score and 20.4% in Success Rate for our method, PCQPR(GPT-4-Turbo). A reasonable explanation is that the MCTS-based planner performs a lookahead search and finds high-quality responses toward the predefined outcome. Consequently, the MCTS-based planner we designed plays a crucial role in our framework."}, {"title": "5.4.2 Effect of Reflection Strategy", "content": "We evaluate the impact of the reflection strategy on our proposed approach. This involves a contrastive analysis, termed \u201cw/o Reflection\", specifically designed to measure the contribution of this strategy. As shown in Table 4, the results indicate a significant performance decrease in the absence of reflection: a 10.4% drop in Success Rate for PCQPR(Mixtral-8x7B). This decline underscores the reflection strategy's vital role, in which verbal feedback integration facilitates an iterative learning process. This process enables the model to refine its response generation based on previous outcomes, improving relevance and accuracy. Therefore, the reflection strategy elevates performance and marks a significant advancement in conversational question-answering systems. It enhances the model's adaptability and effectiveness, particularly in dynamic and evolving conversational contexts, thereby underscoring its importance in developing more sophisticated, responsive systems.\""}, {"title": "6 Conclusion", "content": "We present a novel task setting, CCQG, designed to generate subsequent questions that proactively guide conversations toward the specified outcome. To address this task, we propose an innovative framework, PCQPR, which uniquely combines the MCTS-like planning algorithm with LLMs to enhance planning capabilities. This approach conducts a lookahead search to explore multiple potential paths. Furthermore, we introduce a novel reflection mechanism that provides insightful verbal feedback for each action along the entire planning path. Extensive experiments demonstrate the superior performance of PCQPR over closed-source and open-source LLMs. We believe this effort could be inspiring for future research in AI-driven conversational question-answering systems."}, {"title": "Limitations", "content": "Despite the effectiveness of the comparable self-reflection strategy, it still exhibits certain limitations. This paper primarily focuses on leveraging large language models (LLMs) to provide valuable verbal feedback and refine each action within the entire planning path. However, we recognize that the inherent capabilities of LLMs may constrain their effectiveness. In particular, for some paths that initially succeed in reaching the specific outcome, the reflection strategy may not yield further improvements. In future work, we will explore optimal methods for integrating human feedback with verbal reflection generated by LLMs to address this challenge above effectively."}]}