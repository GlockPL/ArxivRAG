{"title": "MLLM-Search: A Zero-Shot Approach to Finding People using Multimodal Large Language Models", "authors": ["Angus Fung", "Aaron Hao Tan", "Haitong Wang", "Beno Benhabib", "Goldie Nejat"], "abstract": "Robotic search of people in human-centered environments, including healthcare settings, is challenging as autonomous robots need to locate people without complete or any prior knowledge of their schedules, plans or locations. Furthermore, robots need to be able to adapt to real-time events that can influence a person's plan in an environment. In this paper, we present MLLM-Search, a novel zero-shot person search architecture that leverages multimodal large language models (MLLM) to address the mobile robot problem of searching for a person under event-driven scenarios with varying user schedules. Our approach introduces a novel visual prompting method to provide robots with spatial understanding of the environment by generating a spatially grounded waypoint map, representing navigable waypoints by a topological graph and regions by semantic labels. This is incorporated into a MLLM with a region planner that selects the next search region based on the semantic relevance to the search scenario, and a waypoint planner which generates a search path by considering the semantically relevant objects and the local spatial context through our unique spatial chain-of-thought prompting approach. Extensive 3D photorealistic experiments were conducted to validate the performance of MLLM-Search in searching for a person with a changing schedule in different environments. An ablation study was also conducted to validate the main design choices of MLLM-Search. Furthermore, a comparison study with state-of-the art search methods demonstrated that MLLM-Search outperforms existing methods with respect to search efficiency. Real-world experiments with a mobile robot in a multi-room floor of a building showed that MLLM-Search was able to generalize to finding a person in a new unseen environment.", "sections": [{"title": "I. INTRODUCTION", "content": "Autonomous mobile robots can be used to search for specific people in human-centered environments to engage in human-robot interactions. For example, robots need to locate people in: 1) multi-room homes to assist with daily tasks such as meal preparation and exercise [1]-[3], 2) office and university buildings to deliver packages or messages [4]-[6], and 3) public venues such as shopping malls and amusement parks to locate lost people [7]-[11]. In healthcare settings, robots search in long-term homes to find residents to remind them of meal-times or appointments [12], [13] and in hospitals to find medical professionals to deliver supplies [4], [14], [15], or to guide visitors to their destinations [16].\nExisting person search methods such as Hidden Markov Model (HMM)-based [3]-[5], [17] and Markov Decision"}, {"title": "II. RELATED WORKS", "content": "In this section, we present and discuss existing: 1) person search methods developed for robotic search of a dynamic person in human-centered environments, and 2) visual prompting methods used to improve spatial reasoning."}, {"title": "A. Person Search by Robots", "content": "Existing person search methods for robotic applications have consisted of: 1) lookahead [3]-[5], [17], 2) MDP-based [18]\u2013[21], or 3) graph-based [1], [2] planners."}, {"title": "1) Lookahead Planners", "content": "Lookahead planners have either used HMMs [3], [4] or predefined likelihood functions [5], [17] to navigate a robot to the most probable user locations. In particular, HMMs identify user locations based on past location and activity data. For example, in [3], the Casper robot used an HMM to find static known people in a single-floor home. The HMM predicted a person's location based on their past activity sequence. In [4], an HMM-based person search method was used by a robot to find a dynamic person in an indoor office setting. The HMM predicted the person's movements in the environment based on past observed locations.\nIn [5] and [17], lookahead planners used likelihood functions defined by human-experts [5] or by past user locations [17] to determine the next search region. Namely, in [5], a robot searched for static people in an indoor laboratory, navigating to locations with the highest likelihoods assigned by human-experts. In [17], a robot searched for a dynamic resident in an apartment by navigating to the highest likelihood location based on past user location frequencies."}, {"title": "2) MDP-based Planners", "content": "MDP-based planners [18]\u2013[21] have optimized robotic search actions by maximizing the likelihood of finding a person. For example, in [18], an MDP-based method was used by a robot to search for static people on a floor of a building. A sequence of search regions was determined based on the expected proportion of people in each region to minimize search time. In [20], an MDP-based method was also used for a mobile robot to find an elderly person in a home. The MDP determined the next location to visit based on the probability of the person's current location. In [21], an MDP search method generated a search plan to maximize the number of residents found within a retirement home in a specific time frame. The search plan consisted of actions such as moving to different regions. Likelihood functions, based on residents' daily activity schedules, were used to compute the reward.\nIn [19], a partially observable MDP approach was used for a robot to find a dynamic person in a multi-room home"}, {"title": "3) Graph-based Planners", "content": "Graph-based planners have used activity probability density functions (APDF) to plan search paths based on user schedules. For example, in [1], a people search method was proposed for the assistive robot Blueberry to search for dynamic people in long-term care homes. APDF was utilized to predict user locations by considering their complete schedules. These schedules included activity types and duration, time of day, and specific regions. The work in [1] was extended in [2] to consider multiple robots searching."}, {"title": "B. Visual Prompting Methods", "content": "Visual prompting methods [31]\u2013[34] for MLLMs improve spatial reasoning over standard prompting methods by overlaying visual coordinates onto RGB images of a scene. In [31]\u2013[33], visual prompting methods were introduced for visual Q&A tasks to infer object positions [31], to identify regions based on text [32], or to answer queries related to size and distance of objects in a scene [33]. For example, in [31], the Scaffold method placed visual coordinates evenly across an RGB image of a scene, allowing MLLMs to associate visual data with textual data. Similarly, both [32] and [33], placed visual coordinates on segmented objects within an RGB image of a scene to associate these objects with their corresponding visual coordinates. Visual prompting methods have also been used in robot manipulation [34]. In [34], the PIVOT method applied visual prompting to robot manipulation tasks by placing coordinates within an RGB image of a scene corresponding to potential robot manipulation actions. The MLLM was then used to select the next robot action based on the coordinates from the image."}, {"title": "C. Summary of Limitations", "content": "Existing lookahead planners [3]\u2013[5], [17] prioritize the next user region to search with the highest likelihood. However, this can result in increased search times as search plans may select further away regions [1]. On the other hand, MDP-based planners [18]-[21] rely on the Markov assumption that search decisions are based solely on the current region. This can result in redundant searches, where a robot revisits a recently searched region. Graph-based planners along with MDP-based and lookahead planners, all require complete user schedules [1], [3], [19], [21], past observed user locations [17], [18], and/or last known user locations [20]. However, in real-world scenarios, such user information may be unavailable or incomplete due to insufficient knowledge about the user. Furthermore, user behaviors can deviate from expected schedules due to real-time events such as emergency situations or schedule changes, availability of locations, etc. Thus, existing robotic person search methods cannot generalize to these real-world scenarios.\nMLLMs have the potential to infer the region a person is in from incomplete or changing information, by leveraging knowledge learned from extensive internet data [26]. They can also consider additional search information beyond user"}, {"title": "III. PERSON SEARCH PROBLEM UNDER EVENT-DRIVEN SCENARIOS WITH VARYING USER SCHEDULES", "content": null}, {"title": "A. Problem Definition", "content": "The robot problem of person search under event-driven scenarios requires a mobile robot to search for a dynamic person in a known environment without complete, partial, or any a priori knowledge of their schedules. A search query qs, provided by the search operator to the robot, includes natural language instructions containing the person to search for and their physical description qa. The search query can optionally contain information such as their name, role, tasks, last known location, etc. The robot has access to an information database Qdb which consists of: 1) the user schedule Qu, if available, and 2) the system database Qs, if available, containing textual data relevant to the search, such as building/room schedules, visitor logs, EHRs (Electronic Health Records), etc. During the search at time t, images xt are obtained from the robot's camera. The function f is defined to output a sequence of robot actions ut given the robot position pi, metric map M, search query qs, and information database Qdb:\n$u_t = f(x_t, p_i, M, q_s, Q_{db} ).$ (1)\nThe objective is to minimize the expected distance traveled d between the robot start location ps and the target location ptg:\n$\\min E [d(p_s, p_{tg})].$ (2)"}, {"title": "IV. MLLM-SEARCH ARCHITECTURE", "content": "The proposed MLLM-Search architecture, Fig. 1, consists of two subsystems: 1) Map Generation Subsystem (MGS), and 2) Person Search Subsystem (PSS). The goal of the MGS is to generate both a semantic metric map Msem and waypoint metric map Mwp of the environment. Once the environment is mapped, the PSS leverages MLLMs to search for the user."}, {"title": "A. Map Generation Subsystem (MGS)", "content": "The MGS consists of two main modules: the Semantic Map Generation (SMG) module and the Waypoint Map Generation (WMG) module."}, {"title": "1) Semantic Map Generation (SMG)", "content": "The SMG module builds a semantic map of the environment. It consists of three modules: 1) Object Discovery VLM (OD-VLM), 2) Open Segmentation (OS), and 3) Semantic Simultaneous Localization and Mapping (S-SLAM). The OD-VLM module utilizes an MLLM to identify the objects in the environment, Namely, it takes an input image XRGB and generates a list of detected objects labels Lo in the image. The OS module takes these object labels Lo and uses the Grounded Segment Anything Model (Grounded SAM) [35] to generate corresponding segmentation masks Mseg for each object. The S-SLAM module [36] takes an RGB image XRGB and depth image XD, and produces a semantic map Msem, Fig. 2(a). Specifically, it takes the segmented portions of XRGB and XD, and projects them into a 3D point cloud XSEG-PCL using the pinhole camera model [37], [38]. The point cloud XSEG-PCL is then converted into a voxel representation and summed over"}, {"title": "2) Waypoint Map Generation (WMG)", "content": "The WMG module generates a semantically and spatially grounded waypoint map Mwp and consists of three sub-modules: 1) Occupancy Grid SLAM (OG-SLAM), 2) Topological Map Generation (TMG), and 3) Waypoint Visual Prompt Generation (WVPG). The OG-SLAM sub-module creates an occupancy map Mocc using odometry p and point clouds XPCL with particle filters [39]. The TMG sub-module uses the occupancy map Mocc to generate a topological map, represented as a graph Mtop = (V,E), where nodes V represent navigable waypoints, and edges E represent the traversable paths between waypoints. The nodes V are obtained based on free space in Mocc. First, the distance transform D is computed, which measures the distance from each point p on Mocc to the nearest obstacle o\u2208 O. For each point p in Mocc, the distance transform is computed:\n$D(p) = \\min | p - o|.$\n(3)\nSafe points psafe = {p | D(p) \u2265 \u03c3min} are identified as those a distance \u03c3min away from obstacles. K-means clustering is used to generate waypoints wi from these safe points. Any wi such that D(wi) < \u03c3min are updated to the nearest waypoint:\n$w_i = \\arg \\min |p - w_i|.$\n(4)\nEdges E are determined using a KDTree to find neighboring nodes within a distance \u03c3max. Bresenham's algorithm [40] is used to check if the path between waypoints is obstacle-free.\nThe WVPG sub-module uses both Mocc and Mtop to generate a waypoint map Mwp, Fig. 2(b), where navigation waypoints wi and high-level region labels Ly are directly overlaid on top of the occupancy map. Waypoints wi are represented as numbered markers (i.e., w\u2081 is labelled as \"1\"),"}, {"title": "B. Person Search Subsystem", "content": "The Person Search Subsystem is used to search for individuals within a dynamic environment using the semantic and waypoint maps generated by MGS. It contains the following modules: MLLM Region Planner, MLLM Waypoint Planner, the Target Tracking, and Navigation."}, {"title": "1) Multimodal LLM Region Planner", "content": "The MLLM Region Planner (MLLMRp) module determines the region rt+1 \u2208 R to search by considering the semantic relevance of each search region with respect to the search scenario. The inputs to this module include the robot's waypoint position wt, the search query qs, the information database Qdb, and the robot memory H. H consists of previous search histories for regions visited H\u2081 = {ri}i=1 and waypoints visited Hw = {wi}i=1, namely, H = H\u2081 \u222a Hw Region-to-object assignments are obtained from the semantic map Msem by assigning each object to the closest region: Or = {(ri, {0j}jeji)}i Ji is the index set of objects of assigned to region ri. The contextual database Q'db, representing semantically relevant information for the search, is retrieved through Retrieval Augmented Generation (RAG) [41]. RAG uses the cosine similarity between the search query embeddings e(qs) and database embeddings e(qab), where the database is divided into chunks qab \u2208 Qab:\n$Q_{db}' = \\arg \\max \\frac{e(q_s) \\cdot e(q_{ab})}{||e(q_s) || \\cdot ||e(q_{ab})||}.$\n(5)\nWe use region-based score prompting to assign semantic scores, St+1 = {(si, sp, s)}, to potential search regions ri. MLLMRP outputs the semantic scores for each region ri:\n$S_{t+1} = MLLM_{RP}(q_s, Q_{db}', w_t, H_t, O_r, M_{wp}).$ (6)\nIn particular, St+1 consists of: 1) the likelihood score si, 2) the proximity score sp, and 3) the recency score sr shown in Fig. 3. Furthermore, we use Chain-of-Thought (CoT) prompting [42] to provide explicit and sequential step-by-step reasoning. The text and visual prompts are also presented in Fig. 3. The robot selects the region with the highest sum of semantic scores as the next region rt+1 to search:\n$r_{t+1} = \\arg \\max s_i + s_p + s_r.$\n(7)"}, {"title": "2) Multimodal LLM Waypoint Planner", "content": "The MLLM Waypoint Planner (MLLMwp) module plans a sequence of waypoints pt+1 to a search region rt+1, while prioritizing the likelihood of encountering the person along the path. For example, when searching for a student, it may plan a route near tables where students work. A* [43] is used to generate several paths pi from the current waypoint wt to the destination waypoint wt+1. Waypoint-to-object assignments are obtained from Msem by associating each object to the closest waypoint: Ow = {(wi, {0j}jesi)}i. This allows MLLMwp to consider semantically relevant objects during planning. We have developed a novel spatial CoT (SCoT) prompting method which improves spatial awareness of MLLMs over standard prompting methods by decomposing the planning task into sequential steps, with each step uniquely considering the semantically relevant objects (the parameter \u201cobjects\u201d in Fig. 4) as well as the local spatial context (the parameter \u201cnext_waypoints\u201d in Fig. 4).\nMLLMwp finds pt+1 by optimizing the path to region rt+1:\n$p_{t+1} = MLLM_{WP}(q_s, Q_{db}', w_t, r_{t+1}, p_i, H_t, M_{wp}, O_w).$\n(8)\nThe output waypoint sequence pt+1 is checked for feasibility against the topological map Mtop:\n$\\forall (w_i, w_{i+1}) \\in p_{t+1}, (w_i, w_{i+1}) \\in M_{top}.$\n(9)\nIf constraints are violated, the above steps are repeated."}, {"title": "3) Target Tracking", "content": "The Target Tracking module identifies and tracks the target person in the environment. It takes as input an RGB image XRGB, and a text description qa of the person's appearance. The Person Tracker, which runs throughout, detects and tracks individuals using LDTrack [6] that we have developed. LDTrack leverages diffusion models [44]to capture temporal embeddings of people. When a person is identified, the Open Detection (OpenDet) module using G-DINO [45] detects the individual based on their description qa. Each box bi is associated with a label ci and confidence score p(ci), where ci \u2208 qa. An MLLM is then used to evaluate whether the detected individual matches the search target by comparing ci with qa. If it matches, a target waypoint wt+1is passed to the Navigation module."}, {"title": "4) Navigation", "content": "The Navigation module converts a target waypoint wt+1 from the Target Tracking module or a sequence of waypoints pt+1 from the MLLMwp module into robot velocities (\u03bd, \u03c9) for navigation. The A* algorithm [43] and the TEB planner [46] were used as the global and local planners, respectively. AMCL [47] was used to localize the robot within Mocc"}, {"title": "V. SIMULATED EXPERIMENTS", "content": "We conducted extensive simulated experiments to evaluate the performance of our MLLM-Search architecture for robotic person search under event-driven scenarios with varying user schedules. Namely, we first conducted an ablation study to investigate the contributions of the design choices of our MLLM-Search architecture, and then performed a benchmark comparison study with state-of-the-art (SOTA) person search methods. GPT-4o [28] was used as the MLLM. These experiments were conducted using a Clearpath Jackal robot."}, {"title": "A. Environments", "content": "Two 3D photorealistic environments from AWS RoboMaker [48] were used in the ROS Gazebo 3D simulator: 1) a hospital environment, and 2) an office environment. The hospital is 25m x 55m in size and includes 11 regions such as patient rooms, intensive care units, and patient wards, with objects such as beds, chairs, and medical equipment, Fig 5(a). The office is 22m x 48m in area and includes 14 regions such as rooms, cubicles, and conference rooms, with objects such as tables, chairs, and TVs, Fig. 5(b). Both environments include dynamic people, whose movements have been modeled using the social force model [49]."}, {"title": "B. Performance Metrics", "content": "We use three metrics to evaluate robot search performance:\n1. Mean Success Rate (SR): the proportion of searches where the robot successfully locates the target user.\n2. Success weighted by Path Length (SPL): the efficiency of the search method: $\\frac{1}{N} \\sum_{i=1}^N S_i \\frac{l_i}{\\max(p_i, l_i)}$, where N is the total number of search trials, Si represents whether the search was successful, li is the shortest path, and pi is the robot path.\n3. Mean Search Time (ST): the average time to complete the search and locate the target person across all trials."}, {"title": "C. Search Scenarios", "content": "For each environment type, we generated three types of scenarios based on people's schedules: 1) Complete Schedules: scenarios involving schedules with all"}, {"title": "D. Ablation Methods", "content": "We considered the following for our ablation study: 1) MLLM-Search: our proposed method, and 2) MLLM-Search with (w/) Single Stage (SS): a variant of MLLM-Search using a single MLLM to perform both region and waypoint planning. We also ablated the score variables of the region planner MLLMRP to determine their influence: 3) MLLM-Search without (w/o) Likelihood s\u2081: a variant with no likelihood score s\u2081; 4) MLLM-Search w/o Proximity sp: a variant with no proximity score sp; 5) MLLM-Search w/o Recency s: a variant with no recency score s; and 6) MLLM-Search w/o Scores \u03a3s a variant with no scores. Furthermore, we ablated the waypoint planner MLLMwp: 7) MLLM-Search w/o SCoT: a variant with no SCoT prompting."}, {"title": "E. SOTA Methods", "content": "The SOTA methods we compared our MLLM-Search method with were: 1) MDP-based Planner [20], [21]: this planner selects a region to search based on the expected reward which is determined using transition probabilities between regions and the user location PDF. It was selected as a representative decision-making approach, 2) HMM-based Planner [3]: this planner predicts the target person's region by modeling movement as a sequence of hidden states with transition probabilities between regions. It was selected as a representative probabilistic inference approach, and 3) Random Walk Planner (RW): RW selects a region to search uniformly at random. It was selected as a na\u00efve baseline. For all methods, GPT-40 is used to generate transition and user location probabilities for each scenario."}, {"title": "F. Simulation Results", "content": "Table I presents the results of both the ablation study and the SOTA comparison. In general, MLLM-Search outperformed all methods across all metrics and scenarios. In particular, the ablation study showed that our MLLM-Search with two stages of region and waypoint planning performed better than the single stage variant (MLLM-Search w/ SS) for both environments. The degradation in performance metrics for the single stage variant is due to the selection of suboptimal waypoints as a result from longer context windows [50]. Namely, the MLLM must simultaneously process more information when combining the planning stages.\nAblating the design choices of the region planner, we observed that removing each score component resulted in performance degradation. The most significant degradation was observed for the MLLM-Search w/o \u03a3s variant. This variant does not account for the travel distances between regions nor how regions were visited recently, resulting in frequent travel to faraway regions and revisiting the same regions repeatedly. Similarly, MLLM-Search w/o sr resulted in the robot frequently revisiting the same regions, leading to longer search times (up to 20.7 min) and up to 50% longer search paths. MLLM-Search w/o sp resulted in the robot selecting regions that were farthest away from its current location, leading to inefficient searches as noted by a longer ST of up to 18.1 min as well as a degradation of up to 40% in SR and 48% in SPL. MLLM-Search w/o s prioritizes the closest region rather than the most probable, which resulted in degradations of up to 30% in SR, 36% in SPL, and up to 9.9 min longer ST across all scenarios in both environments.\nAblating the design choice of the waypoint planner, MLLM-Search w/o SCoT, resulted in comparable SR and SPL values with MLLM-Search, however, up to 22.7 min longer ST. Without SCoT, many of the generated paths were infeasible due to a lack of spatial understanding, thus, requiring more time for GPT-4o to replan.\nThe comparison results showed that our MLLM-Search method outperformed the SOTA planners. For both MDP-based and HMM-based planners, search degradation became more significant as user schedules became less available. The SOTA methods can only perform optimally when a user's past schedule directly matches their actual location, as they rely on historical data to derive transition probabilities (to predict movement patterns) and user location PDFs (to estimate user likelihoods in different regions). However, even"}, {"title": "VII. CONCLUSION", "content": "In this paper, we present MLLM-Search, a novel person search architecture developed to address the challenge of locating a person under event-driven scenarios with varying user schedules. MLLM-Search is the first approach to incorporate MLLMs for search, using a unique visual prompting method that generates a semantically and spatially grounded waypoint map to provide robots with spatial understanding of the global environment. MLLM-Search includes a region planner that selects regions based on semantic relevance, and a waypoint planner which considers semantically relevant objects and spatial context using our novel SCOT prompting method in order to plan a robot search path. An extensive ablation study validated the design choices of MLLM-Search, while a comparison study with SOTA methods demonstrated that MLLM-Search achieves higher search efficiency in 3D photorealistic environments under event-driven scenarios with varying user schedules. A real-world experiment highlight the generalizability of MLLM-Search to be applied to a new unseen environment and scenario. Future work will extend MLLM-Search to consider human-robot interactions during the search for the robot to obtain additional search evidence or clues."}]}