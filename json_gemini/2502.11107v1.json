{"title": "Revisiting Weak-to-Strong Generalization in Theory and Practice: Reverse KL vs. Forward KL", "authors": ["Wei Yao", "Wenkai Yang", "Ziqiao Wang", "Yankai Lin", "Yong Liu"], "abstract": "As large language models advance toward superhuman performance, ensuring their alignment with human values and abilities grows increasingly complex. Weak-to-strong generalization offers a promising approach by leveraging predictions from weaker models to guide stronger systems, but its effectiveness could be constrained by the inherent noise and inaccuracies in these weak predictions. To address this, we propose a theoretically grounded approach that replaces forward KL divergence whose mass-covering behavior risks overfitting to imperfect weak signals with reverse KL divergence. Reverse KL divergence's zero-forcing effect prioritizes high-confidence predictions, effectively mitigating the influence of unreliable weak supervision. Theoretically, we extend existing bounds and derive tighter lower bounds for both forward and reverse KL divergence, establishing that reverse KL achieves at least comparable guarantees to forward KL. Notably, when a sufficiently pre-trained strong model is fine-tuned on the last layer, reverse KL uniquely guarantees that it outperforms its weak supervisor by the magnitude of their disagreement a guarantee that forward KL cannot provide. Empirically, we demonstrate that reverse KL and reverse cross-entropy enable strong models to successfully outperform those trained with forward KL and standard cross-entropy across most settings, highlighting the practical advantages of these reverse losses.", "sections": [{"title": "1 Introduction", "content": "Human supervision is indispensable to align Large Language Models (LLMs) with human values (Bai et al., 2022a; OpenAI, 2023a). However, as LLMs approach superhuman capabilities, their behaviors may exceed human ability to reliably manage (OpenAI, 2023b). To address this challenge, Weak-to-Strong Generalization (WTSG) (Burns et al., 2023) emerges as a promising approach, leveraging weaker models to guide and control more advanced systems, thereby bridging the gap between human oversight and superhuman AI capabilities.\nIn particular, WTSG demonstrates that strong pre-trained LLMs, when fine-tuned under weak model supervision, can achieve performance surpassing that of their weak supervisors. However, this approach is fundamentally constrained by the inherent imperfections of weak model supervision, which may introduce inaccuracies and noise (Burns et al., 2023). Blindly fitting the strong model to these imperfect signals can lead to a significant discrepancy between the ground truth and the model's predictions, ultimately undermining the effectiveness of WTSG (Yao et al., 2025). This raises a critical question: How to effectively leverage weak supervision to guide strong models while mitigating the impact of noisy or inaccurate signals?\nTo answer this question, we propose a theoretically principled approach, supported by fine-grained analysis and a simple yet effective solution. Our motivation stems from an insightful comparison with Knowledge Distillation (KD) (Hinton, 2015) in classification, where strong teachers provide informative soft labels to guide weak students. In KD, the forward KL divergence loss plays a crucial role as it encourages students to learn not only the target class probabilities but also the relative relationships among non-target classes encoded in the teacher's soft labels. For instance, in the image classification scenario, a strong teacher might assign higher probabilities to \"tiger\u201d than to \u201cdog\u201d when the input image is a \u201ccat\", reflecting the semantic similarity between cats and tigers in the feature space. However, this advantageous property of forward KL in KD becomes a limitation in the WTSG paradigm. The fundamental distinction lies in the quality of supervision: while strong teachers in KD provide reliable and informative soft labels, weak teachers in WTSG often generate noisy and potentially misleading signals for non-target classes (Burns et al., 2023). Thus, the mass-covering nature of forward KL (Jerfel et al., 2021; Sun and van der Schaar, 2024), which forces the student to match the entire probability distribution of the teacher's predictions, becomes detrimental in WTSG as it may lead the strong model to overfit to the weak teacher's unreliable supervision. This observation motivates our investigation of reverse KL divergence as a more suitable alternative for WTSG.\nAs shown in Figure 1, the key advantage of reverse KL lies in its mode-seeking behavior (Minka et al., 2005; Ji et al., 2024a), which enables the strong model to focus on the weak teacher's high-confidence predictions while being less sensitive to potentially noisy low-probability regions. This property aligns better with the WTSG setting, as it allows the strong model to extract reliable patterns from weak supervision without being overly constrained by its imperfections.\nBuilding on the intuitive motivation above, we first conduct a theoretical analysis to compare forward losses and reverse losses in the context of WTSG. Inspired by the lower and upper bounds established for the strong model in WTSG (Yao et al., 2025), we extend these results and derive tighter lower bounds for both forward and reverse losses, demonstrating that reverse losses achieves at least equivalent theoretical guarantees to forward losses. Furthermore, we identify a unique advantage of reverse KL: when an adequately pre-trained strong model undergoes last-layer fine-tuning, reverse KL guarantees that the strong student will outperform its weak teacher by at least the magnitude of their disagreement. Notably, this performance guarantee fails to hold for forward KL without additional assumptions, underscoring the theoretical advantage of reverse losses. In our experiments, we empirically demonstrate that employing reverse KL divergence and reverse Cross-Entropy (CE) as loss functions enables the strong model to achieve superior performance compared to using forward KL divergence and standard CE. We also extend the analysis to an improved algorithm discussed in Burns et al. (2023), where the optimization objective incorporates an additional regularization term. It further demonstrates the practical advantages of reverse CE over standard CE in the context of WTSG."}, {"title": "2 Related Work", "content": "Weak-to-Strong Generalization. The weak-to-strong paradigm (Burns et al., 2023) emerges as a promising framework to address the challenges of AI alignment, particularly in the context of superalignment (OpenAI, 2023b)\u2014where future AI systems may surpass human capabilities, rendering human supervision weak or insufficient. It leverages weaker models to guide stronger models, potentially unlocking their full capabilities while maintaining alignment with human values. It has been extensively studied through algorithms (Zhu et al., 2024; Agrawal et al., 2024; Sang et al., 2024; Guo and Yang, 2024), empirical analyses (Yang et al., 2024; Ye et al., 2024), and theoretical frameworks (Lang et al., 2024; Somerstep et al., 2024; Wu and Sahai, 2024; Charikar et al., 2024; Yao et al., 2025), these works primarily focus on WTSG with forward KL divergence and CE losses. However, to the best of our knowledge, the potential of reverse KL and reverse CE losses in classification under the WTSG framework remains unexplored.\nForward KL and Reverse KL. Forward KL and Reverse KL are employed in distinct applications, each offering unique advantages. Forward KL is widely utilized in standard classification tasks (Goodfellow, 2016), often appearing in the form of CE loss to align predicted and true label distributions. Its mass-covering behavior (Jerfel et al., 2021; Sun and van der Schaar, 2024) ensures that the model comprehensively captures all high-probability regions of the target distribution, making it particularly effective in knowledge distillation (Hinton, 2015) for classification tasks. In such tasks, the teacher model's soft labels provide informative guidance, enabling the student model to learn a representative distribution (Yang et al., 2025). In contrast, reverse KL is frequently adopted in variational inference (Kingma and Welling, 2014; Pinheiro Cinelli et al., 2021), where it exhibits zero-forcing behavior (Minka et al., 2005). By focusing on high-confidence predictions while disregarding low-probability regions, reverse KL prioritizes precision over diversity. In the context of WTSG, the choice of divergence is especially significant. Weak teachers in WTSG provide imperfect supervision signals (Burns et al., 2023; Yang et al., 2024; Yao et al., 2025), and using forward KL divergence as the loss function may lead to overfitting to these noisy or incomplete guidance. Reverse KL, on the other hand, allows the strong model to extract reliable patterns from weak supervision without being overly constrained by its imperfections. This property aligns well with the goal of WTSG, where the focus is on leveraging weak supervision while avoiding its pitfalls.\nFurthermore, reverse KL divergence has recently gained increasing attention in related fields such as domain adaptation (Nguyen et al., 2022) and KL-regularized reinforcement learning (Rafailov et al., 2024; Wang et al., 2024; Ji et al., 2024b). These applications share a conceptual similarity with WTSG, as they all involve transferring knowledge across domains or models under imperfect or constrained conditions. Moreover, beyond classification tasks, reverse KL divergence has been increasingly utilized in generation tasks within knowledge distillation (Gu et al., 2024; Agarwal et al., 2024; Wu et al., 2025), owing to its mode-seeking properties. Given these developments, it is natural to investigate the role of reverse KL within the WTSG framework. To the best of our knowledge, no prior work has systematically explored this direction, leaving a significant gap in understanding its potential applications and implications."}, {"title": "3 Preliminaries", "content": "We consider k-classification tasks. Given the data domain \\$\\mathcal{X} \\subset \\mathbb{R}^d\\$ and output domain \\$\\mathcal{Y} \\subseteq \\mathbb{R}^k\\$, let the model space be \\$\\mathcal{F} : \\mathcal{X} \\rightarrow \\mathcal{Y}\\$. Consider the model equipped with a softmax module, which ensures that its outputs form a valid probability distribution, i.e., \\$\\forall y = (y_1,\\dots, y_k)^T \\in \\mathcal{Y}\\$, there holds \\$\\sum_{i=1}^k Y_i = 1\\$ and \\$0 < Yi < 1\\$. The forward and reverse KL divergence losses are defined below.\nDefinition 1 (KL divergence losses). Given the data distribution P and two models g, h \u2208 F, the forward KL divergence loss is defined as:\n\\$\\text{KL}(g, h) = \\mathbb{E}_{x \\sim P} [D_{KL}(g(x)||h(x))]\\$,\n\\$\\qquad \\qquad= \\mathbb{E}_{x \\sim P} \\left[\\sum_{i=1}^k [g(x)]_i \\log \\frac{[g(x)]_i}{[h(x)]_i}\\right]\\$\nwhere \\$\\left[g(x)\\right]_i\\$, \\$\\left[h(x)\\right]_i\\$ represent the i-th elements of g(x), h(x), respectively. Thus, the reverse KL divergence loss is KL(h, g).\nAs illustrated in Figure 1, forward KL promotes full coverage of the target distribution, whereas reverse KL focuses on capturing the dominant mode. Additionally, the difference between KL divergence and CE is an entropy term:\nDefinition 2 (Cross-entropy losses). Given the data distribution P and two models g, h \u2208 F, define the forward cross-entropy divergence loss:\n\\$\\text{CE}(g, h) = - \\mathbb{E}_{x \\sim P} \\left[\\sum_{i=1}^k [g(x)]_i \\log({[h(x)]_i})\\right]\\$,\n\\$\\qquad \\qquad= \\text{KL}(g, h) + \\mathbb{E}_{x \\sim p} H(g(x)),\\$\nwhere H(\u00b7) is the Shannon entropy. Thus, the reverse cross-entropy loss is CE(h, g).\nConsequently, note that when minimizing forward losses, the model g is fixed to provide supervision signals. Thus, minimizing forward KL divergence loss is equivalent to minimizing standard CE loss as \\$\\mathbb{E}_{x \\sim p} H(g(x))\\$ is a constant."}, {"title": "3.1 Classification", "content": null}, {"title": "3.2 Weak-to-Strong Generalization", "content": "Consider WTSG in the context of k-classification tasks. We focus on the fine-tuning phase after pre-training. The labeling function F* maps data x to its label F*(x). The strong model aims to learn \\$\\mathbb{F}_{sw} = f \\circ h_s\\$, where hs is a fixed strong model representation and f \u2208 F is a task-specific function from a hypothesis class Fs. In the convention setting of AI alignment (Ouyang et al., 2022), the model is fine-tuned through ground truth data:\n\\$\\$f_s = \\underset{f \\in \\mathcal{F}}{\\text{argmin}} \\mathcal{L}(F^*, f \\circ h_s),\\$\\$\n(1)\nwhere the loss \\$\\mathcal{L}(\\cdot, \\cdot)\\$ can be KL(\u00b7, \u00b7) or CE(\u00b7, \u00b7). However, it is humans who provide weak supervision in the super-alignment scenario (OpenAI, 2023b). To explore this, the WTSG framework (Burns et al., 2023) leverages a weak model's predictions to supervise the strong model:\n\\$\\$f_{sw} = \\underset{f \\in \\mathcal{F}}{\\text{argmin}} \\mathcal{L}(F_w, f \\circ h_s),\\$\\$\n(2)\nwhere Fw is a given weak model, and \\$\\mathcal{L}(\\cdot, \\cdot)\\$ is originally the standard CE loss. If we employ reverse losses, the objective transforms into\n\\$\\$f_{sw} = \\underset{f \\in \\mathcal{F}}{\\text{argmin}} \\mathcal{L}(f \\circ h_s, F_w).\\$\\$\n(3)\nRegardless of the choice of loss function, the core objective is replacing ground truth data with weak supervision. Thus, while minimizing forward losses \\$\\mathcal{L}(F_w, F_{sw})\\$ or reverse losses \\$\\mathcal{L}(F_{sw}, F_w)\\$, we simultaneously strive to achieve an Fsw with a small generalization error \\$\\mathcal{L}(F^*, F_{sw})\\$."}, {"title": "4 Theoretical Analysis: Justifying Reverse KL in WTSG", "content": "In Sections 4.1, we establish that both reverse and forward losses offer comparable generalization guarantees for the strong model, indicating that reverse losses is at least as favorable as forward losses in terms of theoretical properties. However, our analysis in Section 4.2 uncovers a key distinction: with reverse KL divergence loss employed in WTSG, the strong model is theoretically guaranteed to outperform the weak model by at least the magnitude of their disagreement under reasonable assumptions. Notably, this performance guarantee does not hold for forward KL, highlighting the theoretical advantage of reverse losses in WTSG."}, {"title": "4.1 Generalization Analysis of Both Losses", "content": "We establish that both reverse and forward losses yield comparable generalization guarantees by deriving upper and lower bounds for their respective generalization errors. We begin with a universal result for both forward and reverse losses.\nUpper and lower bounds. We extend Yao et al. (2025) and establish bounds of strong model's performance. Unlike previous work that focuses only on forward KL loss, we comprehensively examine all four loss variants: forward KL, reverse KL, forward CE, and reverse CE.\nLemma 1 (Proved in Appendix A.1). Let \\$\\mathcal{L}(\\cdot, \\cdot)\\$ be KL(\u00b7, \u00b7) or CE(\u00b7,\u00b7). Given the data domain X, output domain Y and models Fw, F* defined above. For any strong model Fsw, there holds\n\\$\\mathcal{L}(F^*, F_w) - C_1 d(F_w, F_{sw})\\$\n\\$\\qquad \\leq \\mathcal{L}(F^*, F_{sw}) \\leq\\$\n\\$\\qquad \\mathcal{L}(F^*, F_w) + C_1 d(F_w, F_{sw}),\\$\nwhere C1 is a positive constant, d(Fw, Fsw) can be \\$\\sqrt{KL(F_w, F_{sw})}\\$ or \\$\\sqrt{KL(F_{sw}, F_w)}\\$, and \\$\\mathcal{L}(F^*, F_{sw})\\$ and \\$\\mathcal{L}(F^*, F_w)\\$ represent the error of strong model and weak model, respectively.\nNote that d(Fw, Fsw) captures the disagreement between the strong and weak models, which serves as the minimization objective in WTSG. Lemma 1 quantifies the difference between the weak and strong models' performance from two perspectives: a lower bound and an upper bound, which is similar to Yao et al. (2025). The lower bound indicates that strong model's performance cannot be arbitrarily improved using weak supervision. Improving the strong model depends critically on ensuring \\$\\mathcal{L} (F^*, F_w)\\$ is small, underscoring the importance of weak model's performance. Also, whether we choose forward or reverse loss, the student-supervisor disagreement d(Fw, Fsw) is minimized. While reducing \\$\\mathcal{L} (F^*, F_{sw})\\$ requires increasing d(Fw, Fsw), the lower bound also implies that strong model's performance gain may be inherently constrained by WTSG's own optimization objective (Yao et al., 2025). In other words, achieving the minimal optimization objective limits the strong model's ability to significantly outperform its weak supervisor. The upper bound ensures that strong model's error \\$\\mathcal{L}(F^*, F_{sw})\\$ remains bounded and do not be arbitrarily large. It shows that a better weak model is also crucial to improve strong model's"}, {"title": "4.2 Unique Advantage of Reverse Losses", "content": "To achieve a tighter upper bound, our theoretical analysis below yields an intriguing insight: when using reverse KL in WTSG, an adequate pre-training and subsequent last layer fine-tuning guarantees that the strong student can outperform its weak teacher by at least the magnitude of their disagreement (i.e., R \u2265 0 in Proposition 1).\nTheorem 2 (Proved in Appendix A.4). Consider WTSG using reverse KL divergence loss:\n\\$\\$f_{sw} = \\underset{f \\in \\mathcal{F}}{\\text{argmin}} KL(f \\circ h_s, F_w).\\$\\$\nDenote \\$\\mathbb{F}_{sw} = f_{sw} \\circ h_s\\$. Assume that the function class \\$\\mathcal{F}_s\\$ is a convex set and \\$\\exists f_s \\in \\mathcal{F}_s\\$ such that \\$\\$f_s h_s = F^*\\$$. Then:\n\\$\\$KL (F^*, F_{sw}) < KL (F^*, F_w) \u2013 KL (F_{sw}, F_w) .\\$\\$\nRemark. Similar result can be naturally extended to reverse CE loss. Furthermore, note that our proof leverages Bregman divergence, a generalization of both squared loss and KL divergence. This approach not only broadens the applicability of our results but also demonstrates how our framework naturally recovers the regression analysis of Charikar et al. (2024). On the contrary,"}, {"title": "5 Empirical Validation", "content": "In this section, we empirically compare reverse KL, forward KL, reverse CE, and standard CE losses in the context of WTSG. Our experiments directly support the claim that reverse losses outperform forward losses in most experimental settings."}, {"title": "5.1 Experimental Settings", "content": "Datasets. We follow previous studies (Burns et al., 2023; Yang et al., 2024) to conduct experiments mainly in the reward modeling task in two settings: enabling a weak model to effectively guide a strong model in achieving either harmlessness or helpfulness. To achieve harmlessness, we follow (Yang et al., 2024) to leverage CAI-Harmless (Bai et al., 2022b), a widely adopted benchmark for single-turn harmless dialogue tasks. To achieve helpfulness, we utilize HH-RLHF (Bai et al., 2022a), a benchmark designed to guide models toward producing responses that are helpful, informative, and contextually relevant. We use a subset of single-turn helpful data of HH-RLHF.\nEach dataset includes three subsets: (1) Ground truth set: 4K samples with ground truth labels, used to fine-tune the base models to create strong ceiling models. (2) Weak supervision set: 4K held-out samples, where the weak model generates predicted labels to guide the training of the strong model. (3) Test set: The extra 4K samples, reserved for evaluating the generalization performance of all strong ceiling and weak-to-strong models. Each sample is formatted as"}, {"title": "5.2 Main Results", "content": "The experimental results of the GPT-2 series on the CAI-Harmless and HH-RLHF datasets are presented in Figure 2. Due to space limitation, we put the detailed results for the Pythia series in Ap-"}, {"title": "5.3 Ablation Study", "content": "We notice that Burns et al. (2023) investigates an improved strategy: incorporating an additional regularization term aimed at boosting the strong model's confidence in its predictions, while utilizing the standard CE loss as the primary objective. This naturally raises the question of whether combining reverse CE loss with such regularization can further improve the strong model's performance compared to standard CE loss with regularization. To explore this question, we conduct experiments using the GPT-2 series on the CAI-Harmless dataset as a representative case. Due to space limitation, we only put the results when GPT-2-Base acts as the weak model to supervise GPT-2-Medium, GPT-2-Large, and GPT-2-XL here in Fig-"}, {"title": "6 Conclusion", "content": "In this work, we propose a theoretically principled approach by rethinking the loss function in WTSG. Unlike the mass-covering nature of forward KL, reverse KL exhibits a mode-seeking behavior that focuses on high-confidence predictions from the weak supervisor, thereby reducing the influence of noisy signals. Theoretically, we derive both upper and lower bounds for forward and reverse losses, demonstrating that reverse losses provide at least comparable guarantees to forward losses. Notably, when fine-tuning a pre-trained strong model on its last layer, reverse KL theoretically ensures that the strong model outperforms its weak supervisor by the magnitude of their disagreement\u2014a guarantee forward KL cannot provide. Empirically, we show that reverse losses successfully outperform forward losses in most settings, highlighting the practical benefits of reverse KL and CE losses in WTSG."}, {"title": "Limitations", "content": "While our study provides theoretical insights and empirical validation for the advantages of reverse losses in WTSG, several limitations remain. First, our analysis mainly assumes relatively reliable weak supervision from pre-trained and fine-tuned models. However, real-world applications often involve noisy weak supervision, and reverse KL's mode-seeking nature may amplify extreme noise. Further research is needed to assess its suitability in such cases. Second, while the theoretical results in Section 4.1 provide broad insights, the assumptions in Section 4.2 may not hold for complex LLMs. This limitation is shared by all related work on theoretical understanding of WTSG, which relies on simplifying assumptions. Nonetheless, these foundations offer valuable guidance and a starting point for future research on advancing WTSG theory in LLMs. Third, our experiments are conducted on two well-known alignment-focused binary classification datasets with relatively smaller model sizes. While these results offer valuable insights, it remains an open question whether they can be generalized to more diverse datasets and larger-scale models. Exploring this aspect in future work will help further validate the broader applicability of our approach."}, {"title": "Ethics Statement", "content": "Our intention is to highlight the positive impact of reverse losses in improving weak-to-strong generalization, ensuring more robust and reliable model performance while minimizing the influence of potentially imperfect weak supervision. However, the potential amplification of biases from weak models remains a concern, particularly in sensitive applications where fairness is a critical issue. While reverse KL mitigates overfitting to unreliable supervision, its mode-seeking nature may amplify the biases present in the weak model's predictions. Additionally, stronger AI models trained using WTSG could be misused if deployed without appropriate safeguards, emphasizing the need for responsible development and oversight."}, {"title": "A Main Proof", "content": "We first state some preliminaries for the proof.\nLemma 2 (Donsker and Varadhan's variational formula (Donsker and Varadhan, 1983)). Let Q, P be probability measures on X, for any bounded measurable function f : X \u2192 R, we have\n\\$\\$D_{KL}(Q||P) = \\sup_f \\mathbb{E}_{x\\sim Q}[f(x)] - \\log \\mathbb{E}_{x\\sim P}[\\exp f(x)].\\$\\$\nLemma 3 (Hoeffding's lemma). Let X \u2208 R such that a < X < b. Then, for all \u03bb \u2208 R,\n\\$\\mathbb{E} [e^{\\lambda (X-\\mathbb{E}X)}] \\leq \\exp (\\frac{\\lambda^2 (b-a)^2}{8})\\$\\$\nDefinition 3 (Subgaussian random variable). A random variable X \u2208 R is \u03c3-subgaussian if for any \u03c1,\n\\$\\log \\mathbb{E} \\exp(\\rho (X - \\mathbb{E}X)) \\leq \\frac{\\rho^2 \\sigma^2}{2}.\\$\\$\nWe define the corresponding probability distributions for prediction of Fsw and Fw. \u2200x \u2208 X, we know that \\$\\sum_{j=1}^k [F_w(x)]_j = 1\\$. Therefore, given the class space Ck = {1,\u2026\u2026\u2026, k}, we define a probability distribution Pw(x) with the probability density function pw, where j \u2208 Ck and\n\\$\\$P_w(j) = [F_w(x)]_j.\\$\\$\n(6)\nUsing this method, we also define the probability distribution Psw(x) for Fw(x).\nLemma 4 (Yao et al. (2025)). Given the probability distributions Pw(x) and Psw(x) above. For any x \u2208 X, j \u2208 Ck, g: Ck \u2192 R and assume that g is \u03c3-subgaussian. Let f = t \u00b7 g for any t \u2208 R, then\n\\$\\$D_{KL} (F_w(x)||F_{sw}(x)) \\geq \\sup_t (\\mathbb{E}_{j'\\sim P_w(x)} [g (j')] \u2013 \\mathbb{E}_{j\\sim P_{su}(x)}[g(j)]) \u2013 \\frac{t^2 \\sigma^2}{2}.\\$\\$\nNow we start the proof.\nProof. By taking expectations of x on both sides of the inequality in Lemma 4, we obtain\n\\$\\$KL(F_w, F_{sw}) = \\mathbb{E}_x D_{KL} (F_w(x)||F_{sw}(x))\\$\\$\n\\$\\qquad \\geq \\sup_t (\\mathbb{E}_x\\mathbb{E}_{j'\\sim P_w(x)} [g (j')] \u2013 \\mathbb{E}_x\\mathbb{E}_{j\\sim P_{sw}(x)}[g(j)]) \u2013 \\frac{t^2 \\sigma^2}{2}.\\$\\$\n\\$\\$\\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\phi(t)\\$\\$\nNote that \u03c6(t) is a quadratic function of t. Therefore, by AM-GM inequality, we find the maximum of this quadratic function:\n\\$\\phi(t) \\leq \\frac{1}{2\\sigma^2} (\\mathbb{E}_x\\mathbb{E}_{j'\\sim P_w(x)} [g (j')] - \\mathbb{E}_x\\mathbb{E}_{j\\sim P_{sw}(x)}[g(j)])^2 = \\sup_t \\phi(t) < KL(F_w, F_{sw}).\\$\\$\nSubsequently, there holds\n\\$\\$|\\mathbb{E}_x\\mathbb{E}_{j'\\sim P_w(x)} [g (j')] \u2013 \\mathbb{E}_x\\mathbb{E}_{j\\sim P_{sw}(x)}[g(j)]| \\leq \\sqrt{2\\sigma^2 KL(F_w, F_{sw})}.\\$\\$\n(7)\nLikewise, according to Lemma 4, we have\n\\$\\$D_{KL} (F_{sw}(x)||F_{w}(x)) \\geq \\sup_t (\\mathbb{E}_{j\\sim P_{sw}(x)} [g (j')] \u2013 \\mathbb{E}_{j'\\sim P_w(x)}[g(j)]) \u2013 \\frac{t^2 \\sigma^2}{2}.\\$\\$\n(8)"}, {"title": "A.1 Proof of Lemma 1", "content": null}, {"title": "A.2 Proof of Theorem 1", "content": "Total variation distance is introduced for our proof.\nDefinition 4 (Total Variation Distance). Given two probability distributions P and Q, the Total Variation (TV) distance between P and Q is\n\\$\\$D_{TV}(P||Q) = \\frac{1}{2} \\int_{\\chi \\in \\mathcal{X}}|P(x) - Q(x)|dx.\\$\\$\nNote that DTV(P||Q) \u2208 [0,1]. Also, DTV(P||Q) = 0 if and only if P and Q coincides, and DTV(P||Q) = 1 if and only if P and Q are disjoint.\nProof. We have\n\\$\\mathcal{L}(F^*, F_{sw}) = \\mathbb{E}_x \\left[\\sum_{i=1}^k [F^*(x)]_i \\log \\frac{[F^*(x)]_i}{[F_{sw}(x)]_i}\\right]\\$\n\\$\\qquad = \\mathbb{E}_x \\left[\\sum_{i=1}^k [F^*(x)]_i \\log \\frac{[F^*(x)]_i [F_{sw}(x)]_i}{[F_{sw}(x)]_i [F_{w}(x)]_i}\\right]\\$\n\\$\\qquad = \\mathbb{E}_x \\left[\\sum_{i=1}^k [F^*(x)]_i \\log \\frac{[F^*(x)]_i}{[F_{w}(x)]_i}\\right] + \\mathbb{E}_x \\left[\\sum_{i=1}^k [F^*(x)]_i \\log \\frac{[F_{w}(x)]_i}{[F_{sw}(x)]_i}\\right]\\$\n\\$\\qquad = \\mathcal{L}(F^*, F_w) - \\mathbb{E} \\left< F^*, \\log \\frac{F_{sw}}{F_{w}} \\right>_E\\$\nRearranging terms and we know that:\n\\$\\mathcal{L}(F^*, F_{sw}) = \\mathcal{L}(F^*, F_w) \u2013 \\left< F^*, \\log \\frac{F_{sw}}{F_{w}} \\right>_E\\$\n(12)\n(13)\nRecall that the output domain Y \u2286 Rk, where \u2200y = (y1,\u2026, yk)T \u2208 Y, there holds \\$\\sum_{i=1}^k Y_i = 1\\$ and 0 < yi \u2264 1. In other words, \u2203\u04af > 0 such that 0 < \u03b3 < Yi \u2264 1. Firstly, we know that F*(x) is element-wise non-negative. Denote 1 = (1,1,\u2026\u2026,1)T. We know that there is a positive constant  \u2265 (mini[Fw(x)]i)\u00af\u00b9. We use element-wise addition, subtraction, multiplication, division and absolute value in the proof. Note that\n\\$\\left< F^*, \\log \\frac{F_{sw}}{F_{w}} \\right>_E \\leq \\left< F^*, \\frac{F_{sw}}{F_{w}} - 1 \\right>_E\\$\n\\$\\qquad = \\frac{1}{\\gamma} \\left< F^*, \\frac{F_{sw}}{F_{w}} - 1 \\right>_E\\$\n\\$\\qquad = \\frac{1}{\\gamma} \\left< F^*, \\frac{F_{sw}}{F_{w}} - \\frac{F_{w}}{F_{w}} \\right>_E = \\frac{1}{\\gamma} \\left< F^*, \\frac{F_{sw} - F_{w}}{F_{w}} \\right>_E\\$\n\\$\\qquad \\leq \\frac{1}{\\gamma} \\left< F^*, |F_{sw} - F_{w}| \\right>_E,\\$\n(log x \u2264 x - 1)\n\\$\\$\\qquad(F_{w} \\geq 1 (element-wise))\\$\\$\nand\n\\$\\left< F^*, |F_{sw} - F_{w}| \\right>_E = \\mathbb{E}_x [(F^*(x))^T (|F_{sw}(x) \u2013 F_{w}(x)|)]\\$\n\\$\\qquad \\leq \\mathbb{E}_x [||F^*(x)|| \u00b7 ||F_{sw}(x) \u2013 F_{w}(x)||_1]\\$\n(Holder's inequality for vector-valued functions)\n\\$\\qquad \\leq \\mathbb{E}_x [||F_{sw}(x) \u2013 F_{w}(x)||_1]\\$\n([F*(x)]i \u2264 1)\n\\$\\qquad = 2\\mathbb{E}_x D_{TV}(F_w(x), F_{sw}(x))\\$\n(Definition of TV distance)\n\\$\\qquad \\leq 2\\sqrt{ \\frac{1}{\\gamma}}\\sqrt{\\mathbb{E}_x D_{TV}(F_w(x), F_{sw}(x))}\\$\n(Jensen's inequality)\n\\$\\qquad \\leq 2\\sqrt{ \\frac{1}{\\gamma}}\\sqrt{\\frac{1}{2}\\mathbb{E}_x D_{KL}(F_w(x), F_{sw}(x))}\\$\n(Pinsker's inequality)"}, {"title": "A.3 Proof of Proposition 1", "content": "Proof. We have\n\\$\\mathcal{L}(F^*, F_{w}) = \\mathbb{E}_x \\left[\\sum_{i=1}^k [F^*(x)", "frac{[F^*(x)": "i}{[F_{w}(x)", "i}\\right": "n\\$\\qquad = \\mathbb{E}_x \\left[\\sum_{i=1}^k [F^*(x)"}, {"frac{[F^*(x)": "i [F_{sw}(x)", "i}{[F_{sw}(x)": "i [F_{w}(x)", "i}\\right": "n\\$\\qquad = \\mathbb{E}_x \\left[\\sum_{i=1}^k [F^*(x)"}, {"frac{[F^*(x)": "i}{[F_{sw}(x)", "i}\\right": "mathbb{E}_x"}]}