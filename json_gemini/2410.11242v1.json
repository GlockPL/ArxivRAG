{"title": "Automatically Generating Visual Hallucination Test Cases for Multimodal Large Language Models", "authors": ["Zhongye Liu", "Hongbin Liu", "Yuepeng Hu", "Zedian Shao", "Neil Zhenqiang Gong"], "abstract": "Visual hallucination (VH) occurs when a multimodal large language model (MLLM) generates responses with incorrect visual details for prompts. Existing methods for generating VH test cases primarily rely on human annotations, typically in the form of triples: (image, question, answer). In this paper, we introduce VHExpansion, the first automated method for expanding VH test cases for MLLMs. Given an initial VH test case, VHExpansion automatically expands it by perturbing the question and answer through negation as well as modifying the image using both common and adversarial perturbations. Additionally, we propose a new evaluation metric, symmetric accuracy, which measures the proportion of correctly answered VH test-case pairs. Each pair consists of a test case and its negated counterpart. Our theoretical analysis shows that symmetric accuracy is an unbiased evaluation metric that remains unaffected by the imbalance of VH testing cases with varying answers when an MLLM is randomly guessing the answers, whereas traditional accuracy is prone to such imbalance. We apply VHExpansion to expand three VH datasets annotated manually and use these expanded datasets to benchmark seven MLLMs. Our evaluation shows that VHExpansion effectively identifies more VH test cases. Moreover, symmetric accuracy, being unbiased, leads to different conclusions about the vulnerability of MLLMs to VH compared to traditional accuracy metric. Finally, we show that fine-tuning MLLMs on the expanded VH dataset generated by VHExpansion mitigates VH more effectively than fine-tuning on the original, manually annotated dataset.", "sections": [{"title": "1 Introduction", "content": "Given a prompt containing both an image and a question, multimodal large language models (MLLMs) [1, 2, 3, 4, 5, 6] generate a text response. MLLMs extend the capabilities of large language models (LLMs) [7, 8, 9, 10, 11] by enabling them to understand visual inputs. An MLLM typically comprises three main components: a vision encoder, a vision-language connector, and an LLM. Specifically, the vision encoder extracts visual embedding vectors from the image in the prompt, while the vision-language connector aligns these visual embedding vectors with the token-based input used by the LLM. The LLM then generates the text response based on the outputs of the vision-language connector and the text in the prompt. This integration allows MLLMs to tackle complex tasks like Visual Question Answering (VQA) [12, 13, 14, 15].\nDespite significant advancements, MLLMs are prone to a critical flaw known as visual hallucination (VH) [13, 16], where the model generates responses containing incorrect or misleading visual information. For example, Figure 1 illustrates a VH case where the MLLM provides an incorrect response regarding the number of spots on a butterfly's wings. VH can lead to catastrophic outcomes, especially in high-stakes applications such as autonomous driving [17, 18], medical diagnostics [19], and content moderation [20]. Therefore, VH poses significant obstacles to the safe deployments of MLLMs. This concern is highlighted in the U.S. Executive Order on Trustworthy AI [21], which emphasized rigorous testing of AI systems to identify and mitigate their potential harms. Therefore, developing methods to test and mitigate VH in MLLMs is crucial for ensuring their safety.\nExisting VH testing relies on either manual [14] or semi-automated [13, 12] methods to construct test cases, both of which require extensive human annotations. As MLLMs evolve rapidly, these methods struggle to scale up VH testing, limiting the number of test cases and thus hindering comprehensive testing of MLLMs' vulnerability to VH. Furthermore, existing VH testing methods do not consider adversarial testing [22, 23, 24] in a white-box setting, where an adversary with full knowledge of the target MLLM can craft adversarial examples to trigger VH through adding human-imperceptible perturbations to"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 MLLMS", "content": "MLLMs [1, 2, 3, 4, 5, 6] have revolutionized the ability of LLMs to respond to prompts containing images and questions. Recall that MLLMs typically comprise three components: a vision encoder, a vision-language connector, and an LLM. Vision encoders are often pre-trained via self-supervised learning [27, 28] on large datasets of unlabeled images or image-text pairs. Among the widely used vision encoders are those from the CLIP family [27], including CLIP-ViT-L/14 [27], EVA-CLIP ViT-g/14 [29], and OpenCLIP ConvNeXt-XXL [30, 31]. Cambrian-1 [5] also incorporates other vision encoders, including DINOv2 ViT-L/14 [28] and SigLIP ViT-SO400M/14 [32]. Recently, several types of vision-language connectors have been introduced, such as 2-layer multilayer perceptrons (MLPs), Q-Former [33], 1-layer cross-attention mechanisms [8], and Spatial Visual Aggregator [5]. The backbone LLMs used in MLLMs can be models like Llama2 [9], Llama3 [7], Vicuna [11], and Qwen [3]."}, {"title": "2.2 Methods to Generate VH Test Cases", "content": "To detect and mitigate VH in MLLMs, several methods to generate VH test cases [14, 13, 12, 34] have been proposed. These methods can be categorized into two types: manual and semi-automatic. Manual methods [14,"}, {"title": "2.3 Mitigating VH via Fine-tuning", "content": "With VH datasets constructed by these methods, MLLMs can be fine-tuned on them to mitigate VH [13]. This approach enables the MLLMs to learn from instances of VH, allowing them to distinguish between accurate visual representations and hallucinated content. By exposing the models to diverse VH instances during fine-tuning, they can better generalize and reduce the occurrence of hallucinations [13]."}, {"title": "3 Our VHExpansion", "content": "Figure 2 shows an overview of our VHExpansion. Given an initial VH test case, VHExpansion automatically generates additional VH test cases by modifying the question and answer through negation, as well as modifying the image through common and adversarial image perturbations. We denote a VH test case as ${x_I, x_Q, y_A}$, where $x_I$ and $x_Q$ are respectively the image and text question in the prompt, while $y_A$ is the ground-truth answer. To support automated evaluation, we focus on binary questions in this work, i.e., $y_A$ is either \"yes\" or \"no\". Note that non-binary question-answer pairs ($x_Q$, $y_A$) can be rewritten as binary counterparts."}, {"title": "3.1 Modifying Question $x_Q$ and Answer $y_A$ via Negation", "content": "Given a VH test case ${x_I, x_Q, y_A}$, the goal of negation is to transform it into ${x_I, \\neg x_Q, \\neg y_A}$. Our VHExpansion automates this process using an LLM with a custom prompt (showed in Figure 3). This prompt takes $x_Q$ as input and instructs the LLM to output a negated question using predefined transformation rules, such as adding negation prefixes or modifying key words to reverse the meaning of $x_Q$.\nThe primary intuition behind negation is that an MLLM may simply guess the answer (i.e., \"yes\" or \"no\") correctly for binary questions without really understanding the image. In particular, some MLLMs such as LLaVA-1.5 tend to answer \"yes\" for binary questions [2]. Therefore, if the VH test cases are imbalanced and a majority of them have \"yes\" as ground-truth answers, such MLLMs would have high accuracy without understanding the images, misleading developers to think that the MLLMs are not vulnerable to visual hallucination. However, such MLLMS would be likely to answer incorrectly for the negated questions, leading to low accuracy on them. Thus,"}, {"title": "3.2 Modifying Image $X_I$", "content": ""}, {"title": "Common image perturbations:", "content": "In real-world scenarios, images often undergo standard editing operations for various purposes. For example, images are frequently compressed using formats like JPEG to reduce transmission costs over the Internet. These image edits are known as common image perturbations [37]. Our VHExpansion uses these perturbations to generate additional VH test cases. Given a VH test case ${x_I,x_Q, y_A}$, we apply a common perturbation method T to the image $x_I$, creating a new VH test case ${T(x_I), x_Q, y_A}$. The intuition is that for a slightly perturbed image $T(x_I)$, the ground-truth answer $y_A$ should remain unchanged for the same question $x_Q$. However, this subtle alteration may trigger VH in an MLLM. We focus on four common image perturbations: Gaussian Noise, Brightness Adjustments, Defocus Blur, and JPEG Compression. Further details on these common perturbations are provided in Section C of the Appendix."}, {"title": "Adversarial image perturbations:", "content": "In the context of adversarial image perturbations, we consider a white-box setting where an adversary, with full knowledge of the target MLLM's model parameters, crafts nearly-imperceptible adversarial perturbations to generate VH test cases. Given an original VH test case ${x_I, x_Q, y_A}$, the adversarial image perturbation generates a new test case ${x_I + \\delta^*, x_q, y_A}$, where $\\delta^*$ is the adversarial perturbation. Our intuition is that for a VH test case that does not trigger VH in an MLLM M, VHExpansion creates perturbations that cause the projected visual embedding vector from the vision-language connector to differ from the original. Conversely, if the test case already triggers VH in M, VHExpansion generates perturbations that make the projected visual embedding vector similar to the original. Formally, for an MLLM M with vision encoder $M_E$ and vision-language connector $M_C$, we formulate finding $\\delta^*$ as the solution to the following constrained optimization problem:\n$$\\delta^* = \\begin{cases}\n\\arg\\min_\\delta (-cos (M_E \\circ M_C(x_I), M_E \\circ M_C(x_I + \\delta)))), & \\text{if } x_I \\text{ does not trigger VH,}\\\\\n\\arg\\min_\\delta (cos (M_E \\circ M_C(x_I), M_E \\circ M_C(x_I + \\delta))), & \\text{if } x_I \\text{ triggers VH,}\\\\\ns.t. ||\\delta||_\\infty \\leq \\epsilon,\n\\end{cases}$$\nwhere $M_E \\circ M_C$ denotes the concatenation of the vision encoder and the vision-language connector, $cos$ denotes cosine similarity, and $ \\epsilon$ is the $l_\\infty$-norm constraint on the perturbation $\\delta$ added to the image $x_I$. Note that when the VH test case already triggers VH, we initialize $\\delta$ to be a non-zero vector with random value and apply early stopping to avoid the optimization result to be identical with the original image input $x_I$; and when the VH test case does not trigger VH, we initialize $\\delta$ to be zero. Our algorithm solves the optimization problem in Equation 1 using either Projected Gradient Descent (PGD) [25] or the iterative Fast Gradient Sign Method (I-FGSM) [26]. PGD iteratively updates $\\delta$ via gradient ascent: $\\delta = \\delta - \\gamma \\cdot \\nabla_s l$, where $l = cos (M_E \\circ M_C(x_I), M_E \\circ M_C(x_I + \\delta))$, followed by projecting $\\delta$ onto the feasible region using $\\delta = clip(\\delta, -\\epsilon, \\epsilon)$. I-FGSM differs from PGD by using the sign of the gradient instead: $\\delta = \\delta - \\gamma \\cdot sign(\\nabla_s l)$."}, {"title": "4 Theoretical Analysis", "content": "In this section, we theoretically analyze the standard accuracy metric and our proposed symmetric accuracy metric for evaluating an MLLM model's performance when the model is making random guessing. Suppose we are given a VH test case $t = {x_I, x_Q, y_A}$, sampled from the distribution T of VH test cases, i.e., $t \\sim T$. Our analysis focuses on binary questions, i.e., $y_A$ is either \u201cyes\u201d or \u201cno\u201d. Specifically, we denote by $q$ the probability that a randomly sampled $t$ has a ground-truth answer \"yes\". In other words, a randomly sampled $t$ has a ground-truth answer \"no\" with probability 1 \u2212 $q$. $q$ quantifies the imbalance of the VH test cases with answers \u201cyes\u201d and \u201cno\u201d.\nWe denote by $f$ an MLLM model and $f(x_I,x_Q)$ the MLLM's answer for the VH test case. $f(x_I,x_Q) \\neq y_A$ indicates that the MLLM hallucinates. When the MLLM model makes random guessing to answer the test case without understanding the image $x_I$ and question $x_Q$, it outputs an answer \u201cyes\u201d or \u201cno\u201d randomly. Suppose the MLLM model guesses \u201cyes\u201d with probability $p$ and \u201cno\u201d with probability 1 \u2212 $p$.\nAn evaluation metric measures the performance of an MLLM model $f$ on the VH test cases whose distribution is T. Specifically, an evaluation metric takes T and $f$ as input and outputs a number (e.g., between 0 and 1), with a smaller number indicating that $f$ is more vulnerable to VH test cases from the distribution T. An evaluation metric is unbiased if it does not depend on the imbalance of the VH test cases when the model $f$ makes random guessing, i.e., it does not depend on $q$. Otherwise, the evaluation metric is biased. Formally, we have the following definition."}, {"title": "Definition 1 (Unbiased Evaluation Metric).", "content": "An evaluation metric is said to be unbiased if does not depend on $q$ when the MLLM model makes random guessing."}, {"title": "Definition 2 (Accuracy).", "content": "Accuracy is the probability that an MLLM model $f$ correctly answers a VH test case $t = {x_I,x_Q, y_A}$ sampled from T. Formally, we have: $accuracy = Pr_{t \\sim T}(f(x_I,x_Q) = y_A)$."}, {"title": "Theorem 1.", "content": "Accuracy is a biased evaluation metric when $p \\neq \\frac{1}{2}$, where $p$ is the probability that the MLLM model guesses answer \"yes\"."}, {"title": "Definition 3 (Symmetric Accuracy).", "content": "Symmetric accuracy is the probability that an MLLM model $f$ correctly answers a VH test case $t = {x_I,x_Q, y_A}$ sampled from T and its negated version. Formally, we have: $symmetric accuracy = Pr_{t \\sim T}(f(x_I,x_Q) = y_A \\wedge f(x_I,\\neg x_Q) = \\neg y_A)$.\nWe prove that symmetric accuracy is an unbiased evaluation metric in the following theorem:"}, {"title": "Theorem 2.", "content": "Symmetric accuracy is an unbiased evaluation metric."}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Experimental Setup", "content": "VH datasets: We use three popular VH datasets: MMVP [12], VHTest [13], and POPE [14]. MMVP and VHTest consist of VH test cases across various object properties in images, such as color, counting, and position."}, {"title": "Evaluation metrics:", "content": "We use accuracy and symmetric accuracy as our evaluation metrics, both of which are formally defined in Section 4. Our theoretical analysis demonstrates that symmetric accuracy is an unbiased evaluation metric, whereas traditional accuracy is biased. In our experiments, we illustrate how symmetric accuracy leads to different conclusions about the vulnerability of MLLMs to VH compared to the traditional accuracy metric. Subsequently, we use symmetric accuracy as our default evaluation metric unless otherwise mentioned. We also report the number of successful VH test cases generated by our VHExpansion."}, {"title": "Parameter settings:", "content": "Unless otherwise mentioned, we use LLaVA-1.5 on MMVP dataset by default. We use GPT-40 as the LLM to negate all questions in VH test cases due to its state-of-the-art performance. We use the default parameter settings for all MLLMs. For common image perturbations, the parameters are set as follows: Gaussian Noise standard deviation $\\sigma = 0.08$, Brightness Hue-Saturation -Value space constant $c = 0.5$, Defocus"}, {"title": "5.2 Experimental Results", "content": "Symmetric accuracy v.s. accuracy: Table 3 shows accuracy and symmetric accuracy of the seven MLLMs across the three datasets MMVP, VHTest, and POPE. We have three main observations. First, symmetric accuracy reveals different conclusions about MLLM vulnerability to VH compared to traditional accuracy. For example, on the POPE dataset, Cambrian-1 has higher traditional accuracy than LLaVA-NeXT (0.887 vs. 0.879) but performs worse in symmetric accuracy (0.745 vs. 0.798). Second, when comparing symmetric accuracy across MLLMs,"}, {"title": "Manual verification for negation:", "content": "The correctness of our proposed symmetric accuracy metric relies on the validity of the negated questions, which are generated by LLMs. Since LLMs may exhibit hallucinations, these negated questions might not always be the negated counterparts of the original questions. Thus, it is necessary to verify whether the negated questions generated by the LLM are correct.\nTo validate the correctness of these negated questions, we randomly sampled 200 VQA triples (100 original-negation pairs) from each of the MMVP, VHTest, and POPE datasets, which were evaluated by four independent annotators. The task of the annotators was to judge whether each negated question was the correct negation of the corresponding original question generated by the LLM. The annotators unanimously agreed that all negated questions were correctly generated by the LLM. This result demonstrates the reliability of the LLM in generating valid negations."}, {"title": "5.3 Ablation Study", "content": "We conduct a comprehensive ablation study on adversarial image perturbation using I-FGSM, since it is the most effective method to generate successful VH test cases in our VHExpansion.\nImpact of $l_\\infty$-norm constraint $ \\epsilon$ : Recall that I-FGSM projects the perturbation into the feasible region defined by the $l_\\infty$-norm constraint $ \\epsilon$ at each iteration. Table 6a shows the effect of varying $ \\epsilon$ on symmetric accuracy. We observe that symmetric accuracy initially decreases and then stabilizes as the $l_\\infty$-norm constraint $ \\epsilon$ increases. For example, at $ \\epsilon = 4/255$, symmetric accuracy is 0.080, dropping to 0.051 at $ \\epsilon = 8/255$, after which it converges. This trend occurs because larger perturbations changes the visual embedding vector more significantly of an image for a non-hallucinated VH test case, which is more likely to trigger VH and thereby reducing symmetric accuracy.\nImpact of perturbation step size $ \\gamma$ : The perturbation step size $ \\gamma$ controls the update in every iteration of I-FGSM. Table 6b shows the impact of $ \\gamma$ on symmetric accuracy. We observe that symmetric accuracy is relatively insensitive to different small perturbation step size $ \\gamma$.\nImpact of iterations: Since I-FGSM solves the optimization problem in Equation 1 iteratively, we study the impact of the number of iterations and present the results in Table 6c and Table 6d for hallucinated and non-hallucinated VH test cases, respectively. For hallucinated VH test cases, we observe that symmetric accuracy remains consistently low as the number of iterations increases from 50 to 150. This is because I-FGSM updates the adversarial perturbations to increase the cosine similarity between the original and perturbed images for hallucinated VH test cases, maintaining the effectiveness of VH test cases. In non-hallucinated VH test cases, symmetric accuracy initially decreases and then stabilizes as the number of iterations increases from 100 to 900.\nImpact of repetition of evaluation: Due to the inherent randomness in the decoding algorithm of MLLMs, we repeat the evaluation and report the average symmetric accuracy in Table 6e, varying the number of repetitions. We observe that symmetric accuracy remains consistent across different repetition counts, ranging from 0.040 to 0.051. This suggests that symmetric accuracy stabilizes after only a few repetitions, with even a single evaluation providing reliable results, thus avoiding unnecessary computational overhead.\nImpact of MLLM's temperature: Temperature controls the randomness of MLLMs' responses, with higher temperatures typically leading to more diverse outputs. Table 6f shows the impact of temperature on LLaVA-1.5's"}, {"title": "5.4 Mitigating VH via Fine-tuning", "content": "[13] demonstrate that fine-tuning MLLMs on VH datasets constructed using VH test case generation methods can help mitigate VH. In this section, we compare the symmetric accuracy across three scenarios: 1) before fine-tuning, 2) fine-tuning on original VH test cases generated by other methods, and 3) fine-tuning on original VH test cases generated by other methods combined with expanded VH test cases from our VHExpansion.\nExperimental settings: We use LLaVA-1.5 as the fine-tuning MLLM. For fine-tuning on the original VH test cases generated by other methods, we randomly sample 200 VH test cases from each of the MMVP, VHTest, and POPE datasets, along with 4,000 randomly sampled VQA triples from the LLaVA-1.5 fine-tuning data [2]. For fine-tuning on our expanded VH test cases, we expand the previously sampled 200 VH test cases from each of the three datasets using negation and adversarial image perturbations, resulting in 800 VH test cases. To further increase data diversity, we use GPT-40 to rephrase the questions four times for each VH test case, generating four additional versions of each. Consequently, our expanded fine-tuning set contains 4,000 VH test cases and the sampled 4,000 VQA triples from the fine-tuning data of LLaVA-1.5. All remaining VH test cases from the three VH datasets, along with their adversarially perturbed versions, are used as evaluation data.\nFollowing LLaVA-1.5 [2], we fine-tune LLaVA-1.5 using LoRA [39] with a learning rate of 1.8 \u00d7 10-6 for one epoch. All other parameters are set to the default fine-tuning settings of LLaVA-1.5.\nExperimental results: The comparison results of fine-tuning are shown in Table 7 and Table 8. Our findings demonstrate that fine-tuning on our expanded VH test cases significantly improves symmetric accuracy across the three VH datasets. For instance, on the POPE dataset, symmetric accuracy increases slightly from 0.180 to 0.189 after fine-tuning on the original VH test cases, but rises substantially to 0.711 after fine-tuning on our expanded VH test cases. This highlights the effectiveness of using VH test cases generated by our VHExpansion to mitigate VH in MLLMs. Moreover, Table 8 shows that fine-tuning on our expanded VH test cases maintains the model's performance on other general-purpose VQA datasets, MME Perception and MME Recognition [15]."}, {"title": "6 Conclusion", "content": "In this paper, we introduce VHExpansion, an automated framework to generate VH test cases for MLLMs. VHExpansion significantly advances VH testing by automating the generation of test cases through techniques such as negation and image perturbations, both common and adversarial. We also propose an unbiased evaluation metric, symmetric accuracy, to measure the consistency of MLLMs in answering VH test cases and their negated counterparts. Our experiments demonstrate that, given VH test cases, VHExpansion can find more successful VH test cases. Importantly, fine-tuning MLLMs on the expanded VH test cases generated by VHExpansion significantly mitigates VH, while maintaining general performance on standard VQA tasks."}]}