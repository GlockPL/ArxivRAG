{"title": "FLAIR: Feeding via Long-horizon AcquIsition of Realistic dishes", "authors": ["Rajat Kumar Jenamani", "Priya Sundaresan", "Maram Sakr", "Tapomayukh Bhattacharjee", "Dorsa Sadigh"], "abstract": "Robot-assisted feeding has the potential to improve the quality of life for individuals with mobility limitations who are unable to feed themselves independently. However, there exists a large gap between the homogeneous, curated plates existing feeding systems can handle, and truly in-the-wild meals. Feeding realistic plates is immensely challenging due to the sheer range of food items that a robot may encounter, each requiring specialized manipulation strategies which must be sequenced over a long horizon to feed an entire meal. An assistive feeding system should not only be able to sequence different strategies efficiently in order to feed an entire meal, but also be mindful of user preferences given the personalized nature of the task. We address this with FLAIR, a system for long-horizon feeding which leverages the commonsense and few-shot reasoning capabilities of foundation models, along with a library of parameterized skills, to plan and execute user-preferred and efficient bite sequences. In real-world evaluations across 6 realistic plates, we find that FLAIR can effectively tap into a varied library of skills for efficient food pickup, while adhering to the diverse preferences of 42 participants without mobility limitations as evaluated in a user study. We demonstrate the seamless integration of FLAIR with existing bite transfer methods [19, 28], and deploy it across 2 institutions and 3 robots, illustrating its adaptability. Finally, we illustrate the real-world efficacy of our system by successfully feeding a care recipient with severe mobility limitations.", "sections": [{"title": "I. INTRODUCTION", "content": "Eating is a vital part of everyday life, yet millions worldwide struggle to feed themselves independently due to mobility limitations caused by conditions such as neurological disorders, injuries, the effects of aging, or other health complications [54]. These individuals often rely on caregivers for meal assistance, which impacts their sense of independence, daily routines, and the social experience of dining [27, 41, 50]. Moreover, feeding is one of the most time-consuming Activities of Daily Living (ADL) for caregivers [14]. A system for autonomous mealtime assistance holds promise for improving the quality of life for those requiring assistance [9], and reducing the physical workload on caregivers [6, 28].\nRobot-assisted feeding entails first performing bite acquisition [18, 21-24, 32, 51, 52], where the robot must manipulate a utensil to pick up a bite of food, followed by bite transfer [5, 19, 28, 43, 48], or bringing a bite of food to the mouth for consumption. In this paper, we primarily focus on bite acquisition. Several prior works in bite acquisition develop individual skills targeting specific food groups. This includes policies for skewering firm foods [18, 21-23, 51], scooping soft foods [24, 53], or rearranging and twirling noodles [52]. These works, however, mostly operate over a single bite horizon or consider plates with a homogeneous type of food, such as only noodles or only bite-sized fruits and vegetables. However, the challenge of achieving reliable bite acquisition for dishes encountered in-the-wild, which contain multiple different food types within the same meal and require strategic skill sequencing over many timesteps, persists.\nConsider a robot tasked with feeding a meal with a fruit appetizer-bananas, celery, and watermelon with chocolate sauce and ranch dressing\u2014and spaghetti and meatballs for the main course. The robot must not only execute specialized strategies, such as cutting bananas, skewering fruits, dipping in sauces, and grouping and twirling noodles, but also infer how to sequence them over a long horizon, considering:\nEfficiency: For the main course, if meatballs incidentally rest on top of the spaghetti, the robot should prioritize efficiency by serving the meatball first. This sequencing exposes the spaghetti for subsequent bites, avoiding the inefficiency of pushing the meatball aside to access the spaghetti initially.\nUser Preferences: However, if the user prefers to not eat meatballs, the robot must adjust the bite sequence accordingly.\nCommonsense Reasoning: In the absence of explicit user preferences, the robot must employ commonsense reasoning to correctly order / combine bites for human-like feeding. For the appetizer, it should pair celery with ranch, bananas with chocolate, and feed watermelon standalone, reflecting typical food pairings. For the main course, it should vary the serving order between spaghetti and meatballs to avoid repetitions.\nWe desire a system that considers all these criteria to achieve long-horizon bite acquisition via a library of skills, and finally integrates with frameworks for bite transfer [5, 19, 28, 43, 48] to effectively feed complete meals.\nIn this work, we introduce FLAIR (Feeding via Long-horizon Acquisition of Realistic dishes), a robot-assisted feeding system capable of feeding a complete meal to a care recipient. Given a plate image, and an optional user-provided natural language preference specifying their desired feeding strategy (i.e. 'I prefer to alternate bites of X and Y' or 'Don't feed me X'), FLAIR executes a sequence of actions that efficiently feeds the items on the plate while adhering to the preference. The framework starts by detecting food items and their semantic labels (i.e. 'spaghetti') via Vision-Language Models (VLMs). We then pass the visual state estimate and semantic label for all items to a hierarchical task planner, which outputs per-item efficiencies by proxy of inferring a sequence of skills to achieve acquisition for each item. Finally, we pass all of this context \u2013 the food item labels, the optional user's preference, and per-item efficiences \u2013 to a Large Language Model (LLM)-based planner which outputs the next bite to feed. The few-shot reasoning capabilities of LLMs allows for reasoning about the available context in a chain-of-thought manner, and planning sequences of bites that cater to both preference and efficiency. We carry out these action sequences via a library of parameterized food manipulation skills implemented on custom hardware. Finally, FLAIR's modular approach to long-horizon bite acquisition enables seamless integration with existing outside-mouth bite transfer [19] and inside-mouth bite transfer [28] frameworks.\nWe deploy FLAIR across two institutions and three robots: a Kinova 6-DoF at Cornell University and a Franka Emika Panda and a Kinova 7-DoF at Stanford University, demonstrating its adaptability to various robotic platforms. We validate FLAIR for long-horizon food pickup across six diverse plates, ranging from DoorDash orders and prepared grocery store meals to homemade meals. In a user study across 42 individuals without mobility limitations, we use FLAIR to demonstrate the necessity of balancing between both preferences and efficiency for feeding complete, realistic meals, as compared to an efficiency-only or preference-only approach. Moreover, we compare FLAIR's hierarchical task planner against three state-of-the-art baselines [3, 39, 52] on two different datasets, demonstrating that it significantly outperforms these baselines. Finally, we demonstrate the real-world effectiveness of our system in feeding a care recipient with Multiple Sclerosis a meal consisting of various fruits and dips.\nOverall, our contributions include:\n\u2022 FLAIR: A system for long-horizon feeding which leverages foundation models to sequence a library of diverse skills towards in-the-wild long-horizon bite acquisition.\n\u2022 Deployment of FLAIR across two institutions and three different robots, demonstrating its versatility.\n\u2022 A user study with 42 individuals without mobility limitations across 6 diverse plates validating the effectiveness of considering both preferences and efficiency for feeding.\n\u2022 Demonstration of the real-world efficacy of our system by feeding a care recipient with mobility limitations."}, {"title": "II. RELATED WORK", "content": "Robot-Assisted Feeding. While various commercial robot-assisted feeding systems [1, 2] have been introduced, they typically rely on pre-programmed trajectories or user teleoperation. This limited autonomy has hindered their widespread adoption and retention, and inspired autonomous methods for bite acquisition and transfer. Prior work in bite acquisition has focused on developing individual food manipulation skills for specific food types. Various works [18, 21, 22, 51] tackle acquisition of solid bite-sized foods, and demonstrate effective skewering strategies based on the food item's pose and material properties. Sundaresan et al. [52] propose visually parameterized primitives for twirling and grouping noodle-like dishes, and show generalization to unseen noodles. Beyond fork-based manipulation, Grannen et al. [24] plan bimanual scooping actions with two custom utensils, while Tai et al. [53] and Zhang et al. [58] develop specialized strategies for scooping with a spoon and cutting with a knife, respectively. However, no prior work in robot-assisted feeding considers complete, in-the-wild meals containing various food types (noodles, semisolids, sauces, cuttable food items, etc.) within the same plate, as typically encountered in everyday scenarios. In this work, we leverage insights from the aforementioned state-of-the-art food manipulation works to develop a large library of bite acquisition skills, and use foundation models to sequence these skills for efficiently feeding realistic dishes while obeying user preferences. To the best of our knowledge, FLAIR is the first of any autonomous feeding system to tackle in-the-wild meals containing various food types, and incorporate bite sequencing preferences for long-horizon feeding.\nVarious works have shown joint bite acquisition with transfer [6, 19, 28]. However, they typically consider bite acquisition actions over a single timestep and not over the complete meal. In contrast, we illustrate that our long-horizon bite acquisition framework can seamlessly integrate with existing methods for bite transfer [19, 28], and demonstrate feeding of a full meal to a care recipient.\nFoundation Models for Robotic Manipulation. Two of the most challenging aspects associated with feeding are planning over available skills, and developing a library of food manipulation skills themselves. To this end, several recent works in robotic manipulation use foundation models such as visionlanguage models (VLMs) [3, 31, 35, 46] or large language"}, {"title": "III. FLAIR: FEEDING VIA LONG-HORIZON ACQUISITION OF REALISTIC DISHES", "content": "In this section, we present FLAIR, a system for feeding complete meals which combines existing foundation models in a novel way towards personalized and efficient bite sequencing. We first give an overview of our custom system hardware, then outline our approach to long-horizon bite acquisition, and finally discuss integration of our method with existing bite transfer frameworks [19, 28] for feeding of in-the-wild dishes.\nA. Hardware System\nWe tackle a wide range of food categories in this work such as fruits, vegetables, noodles, meat, soft foods, dipping sauces, and non-bite-sized items that require cutting. Many of these foods require specialized, dynamic manipulation strategies that typical 6 or 7-DoF robots struggle with due to their limited workspace. We thus implement FLAIR on Kinova and Franka robot arms equipped with a motorized feeding utensil mounted at the end-effector, adapting the design from [48]. The utensil contains a fork attachment and has two degrees of freedom corresponding to the orientation of the fork tines and the tilt angle. This allows for directly controlling the utensil to perform dynamic movements like twirling and scooping, while the robot handles moving between waypoints in the workspace via Cartesian position control. We also use a wrist-mounted RGB-D Realsense camera with a known end-effector to camera transformation. This enables perceiving plates of food and localizing food items in the 3D workspace. We note that the same hardware was replicated on two different Kinova arms and one Franka Emika Panda, each with their separate fork attachment and sensors across two different institutions (detailed in Appendix), demonstrating the reproducibilty of our method and hardware (Fig. 2).\nB. Long-Horizon Bite Acquisition Framework\nWith access to a hardware platform that supports dexterous food manipulation strategies, our goal is to plan and execute long-horizon bite sequences that cater to a user's preference while efficiently feeding a meal.\nProblem Formulation. We assume access to an RGB-D plate image observation \\(o_t \\in \\mathcal{O} = \\mathbb{R}^{4\\times W\\times H}\\) of width W and height H, and an optional natural language instruction \\(l_{pref}\\) from the user, representing their preferred feeding strategy at a high-level (i.e., \\(l_{pref}\\) = \"Feed me alternating bites of X and Y\u201d or \"Only feed me X\"). X and Y can denote an arbitrary food item semantic label (i.e., \u201cspaghetti\u201d, \u201cstrawberry\u201d, \u201ccaramel\u201d) or category (i.e. \u201cnoodles\u201d, \u201cfruit\u201d, \u201csauce\u201d).\nWe further assume access to a library \\(\\mathcal{L} = {\\phi_1,...,\\phi_N}\\) of N skills that the robot can use to manipulate food items. Each skill \\(\\phi(p)\\) represents a parameterized manipulation primitive that takes in parameters p and outputs low-level motor commands. We represent a low-level action at time t by \\(a_t = (x, y, z, \\beta, \\gamma, \\psi)\\), where (x, y, z) denotes the position of the feeding utensil tip, \\(\\beta\\) and \\(\\gamma\\) denote pitch and roll of the utensil respectively, and \\(\\psi\\) denotes the robot's end effector roll angle. Thus, the output of any skill is a sequence of T actions \\({a_t, a_{t+1},...,a_{t+T}}\\) that the robot takes to execute the particular strategy. For instance, a skewering skill may take the position and orientation of a desired food item as input, and output a trajectory that skewers the item of choice. Our goal is to plan and execute a sequence of parameterized skills \\({\\phi_1(p_1), \\phi_2(p_2),...,\\phi_H(p_H)}\\) which results in efficient and user-preferred bite acquisition, where H is the total number of skills to execute to complete feeding a plate and \\(p_h\\) refers to the parameters of skill \\(\\phi_h \\in \\mathcal{L}\\).\nState Representations for Food. Our approach addresses the main challenges in long-horizon bite acquisition-parameterizing low-level skills and sequencing them-by integrating state-of-the-art visual-language models. We use visual state estimates and semantic features of food items to guide skill parameterization and sequencing.\nFor a given plate observation \\(o_t\\) at time t, we first query GPT-4V [3] in a few-shot manner to recognize which food items are present. We prompt the model with a few in-context examples of plate images and their corresponding ground truth food item semantic labels, and ask the model to complete the prompt for the test image \\(o_t\\). GPT-4V outputs a list of semantic labels \\(l_t\\) that are present, (i.e., \\(l_t\\) = ['fettuccine', 'chicken', 'broccoli']) along with their corresponding categories \\(C_t\\) (i.e., \\(C_t\\) = ['noodles', 'meat/seafood','vegetable','cuttable']). These categories are relevant for associating the appropriate skill to each food item for bite acquisition. We then pass the recognized semantic labels to GroundingDINO [38], an open-vocabulary VLM, for bounding box detection. For each bounding box, we use SegmentAnything (SAM) [33] to refine these bounding boxes into segmentation masks \\({m_1,m_2,\u2026\u2026\u2026,m_D}\\) for all D items detected.\nSkill Library The segmented representations of food we obtain from VLMs provide a useful way to parameterize food manipulation skills, which we split into acquisition and pre-acquisition skills. Fig. 3 visualizes all skill parameterizations.\n1) Acquisition skills: Acquisition skills refer to those that pick up food, such as skewering a food item, twirling a pile of noodles, scooping a soft pile of food, or dipping an item to coat it in sauce. We parameterize them as follows, assuming access to a segmentation mask \\(m_i\\) for the item of interest:\n\u2022 skewer\\((x_c, y_c, z_c, \\gamma)\\): We detect the centroid of \\(m_i\\) and deproject this 2D pixel coordinate to a 3D coordinate \\((x_c, y_c, z_c)\\) representing the center of a food item in the robot's frame of reference. We also estimate the major axis orientation \\(\\theta\\) of an item from \\(m_i\\) analytically. Following [18, 51], we bring the utensil above the the food item center with \\(\\gamma = 90^\\circ + \\theta\\) and execute a swift downward trajectory skewering perpendicular to the main axis of the item. This encourages the tines of the fork to pierce the item. If the tines align parallel to the item's major axis, they may run along its longer length and miss the shorter breadth due to slight calibration challenges, leading to unsuccessful skewering.\n\u2022 twirl\\((x_d, y_d, z_d, \\gamma)\\): We adopt the parameterization from VAPORS [52], a long-horizon system for noodle acquisition. Specifically, we twirl noodles by bringing the fork to the sensed densest pile \\((x_d, y_d, z_d)\\) on the plate, estimated via 2D Gaussian filtering on \\(m_i\\), and with \\(\\gamma\\) identical to the parameterization for skewering (orthogonal to the major axis of the noodle pile sensed via a pose estimation network from [52]). We actuate the roll joint of the fork to complete two full twirls, wrapping noodles on the fork.\n\u2022 scoop\\((x_s, y_s, z_s, x_d, y_d, z_d)\\): The fork starts with tines horizontal to the plate and scoops from the sparsest region \\((x_s, y_s, z_s)\\) to the densest region \\((x_d, y_d, z_d)\\) on the plate, up to a pre-defined maximum distance empirically selected to pick up a bite-sized amount. We define the sparsest region as the point on the boundary of the food item mask \\(m_i\\) that is furthest from the densest region, with the condition that the line connecting these points is not intersected by other food items, such as toppings.\n\u2022 dip\\((x_c, y_c, z_c)\\): Finally, dipping entails bringing a fork containing a food item into the center \\((x_c, y_c, z_c)\\) of a small dish containing sauce. We initially orient the fork with tines horizontal to the plate to avoid the food item slipping off the utensil during dipping.\nImmediately following each of these actions, the robot moves the fork tines in a scooping motion by actuating the utensil's pitch joint. The resulting horizontal fork helps prevent items from slipping off the fork after being picked up.\n2) Pre-acquisition skills: When the above acquisition skills are not immediately feasible due to occlusion from other items or the anticipated amount of food to be picked up being insufficient, we employ a number of auxiliary strategies which we refer to as pre-acquisition skills. These actions do not directly pick up food but rearrange or manipulate items to"}, {"title": "IV. EXPERIMENTS", "content": "We evaluate the effectiveness of FLAIR for feeding diverse plates each containing various types of food items. We first conduct a user study to assess FLAIR's ability to perform long-horizon bite acquisition of in-the-wild plates, while adhering to user preferences and efficiently feeding bites. For all acquisition experiments, we interchangeably use 2 Kinova Gen3 arms (one 6-DoF, and another 7-DoF), and a 7-DoF Franka Emika Panda. We then ablate our hierarchical task planner T against various state-of-the-art baselines [3, 39, 52]. Finally, we evaluate the real-world efficacy of our system for feeding a complete plate to a care recipient with mobility limitations.\nA. Bite Acquisition Experiments\nBaselines: FLAIR presents a unique approach of taking into account both preference and efficiency considerations for bite sequencing. This naturally begs the question of how an Efficiency-Only or Preference-Only approach would compare. We implement an Efficiency-Only baseline which greedily selects the next bite as the item which requires the least number of pre-acquisition and acquisition skills for pickup in the current instant, as dictated by the task planner |T(\\u00b7, \\u00b7)|. The Preference-Only baseline is identical to FLAIR in implementation, but notably omits efficiency scores when prompting the LLM to generate a next bite. This encourages the LLM to only respect a user's preference without consideration for how efficient a particular bite may be. In the case that a user has no preference for feeding, we refer to the Preference-Only baseline as Commonsense-Only.\nEvaluation Plates: We consider an evaluation suite of 6 diverse plates of food spanning a wide range of food categories, visualized in Fig. 4. We include 2 in-the-wild noodle dishes: a spaghetti and meatballs plate which is a prepared frozen meal from a grocery store, and a fettuccine alfredo dish with chicken and broccoli ordered from Applebee's via Doordash. We also consider 2 homemade semisolid dishes: mashed potatoes with sausage, and oatmeal with strawberries. Lastly, we evaluate an appetizer plate of strawberry, watermelon, celery, ranch, and chocolate dipping sauce, as well as a dessert plate of a whole banana, brownie bites, and chocolate dipping sauce.\nUser Study Design: We evaluate FLAIR's ability to cater to user preferences via a two-phase user study across 42 individuals without mobility limitations (Ages: 19-64, Genders: 22F, 20M). In the first phase, we present participants with a survey showing images of all 6 evaluation plates, and solicit their natural language preference over how they would prefer to be fed each plate. In the survey, we specify the capabilities of our skill library to the participants of our user study, and ask them to note preferences over their preferred order of bites, or pairings of food items with sauces. Details on the reported user preferences are provided in the Appendix. Since evaluating each submitted preference across all of the plates and baselines is not scalable, we cluster the submitted preferences into common shared responses via LLM summarization (GPT-4V). We focus on cases where users have either no preference or strong preferences, as slight preferences are not informative for comparing method behaviors. Thus, we specifically prompt GPT to filter for strong preferences (i.e. 'Always feed me alternating bites of X and Y' or \u2018Please do not feed me X') and group them accordingly. For each of the six plates, we then evaluate our system on the 2 most popular strong preferences summarized per plate, as well as a \u2018I have no preference' setting for completeness.\nWe hypothesized the following:\n\u2022 H1: Compared to the Preference-Only baseline, FLAIR's consideration of efficiency in bite sequencing will lead to more number of bites across all settings.\n\u2022 H2: Compared to the Efficiency-Only baseline, FLAIR's consideration of user preferences in bite sequencing in presence of strong preferences will lead to more perceived adherence to preferences, and more human-like feeding, rated based on the statement \"This method is similar to the strategy I would use to feed myself.\"\n\u2022 H3: Compared to the Efficiency-Only baseline, FLAIR's consideration of commonsense reasoning in bite sequencing in the absence of preferences will lead to more perceived adherence to bite variety and common food item pairings, and more human-like feeding.\nThis user study was approved by the Institutional Review Boards of both Cornell University and Stanford University.\nFood Pickup Results: Fig. 7 displays the results of food pickup over time across methods in the no preference scenario. We provide additional results for food pickup efficiency for all methods averaged across all plates (both strong preferences and no preferences) in Appendix, noting a similar trend. Due to its consideration of efficiency in bite sequencing, FLAIR executes a greater number of pickup skills compared to Preference-Only, validating H1. This is because when faced with multiple valid candidate bites, FLAIR, informed with efficiency scores for each bite, is able to choose the bite that optimizes for efficiency. In contrast, Preference-Only randomly selects one bite from this set, often leading to inefficient acquisition trajectories (Fig. 5). The efficiency disparity between Efficiency-Only and FLAIR can be linked to settings where bite variety or strong preferences require the robot to pickup bites that are less efficient than those of a method which does not take such preferences into account. For instance, in a scenario where a robot is instructed to feed a bed of spaghetti hidden beneath multiple meatballs, methods that consider preferences must undertake multiple pre-acquisition skills to push away the meatballs.\nUser Evaluation: Fig. 6 presents average participant ratings comparing FLAIR with baseline approaches for settings with strong user preferences. We conduct a Mann-Whitney U test for statistical significance, and indicate pairs of methods for which the average participant ratings were significant (p-value < 0.05). This is a non-parametric test compatible with ordinal Likert data, and without specific assumptions on the normality or variance of the data distributions. By integrating user preferences into task planning, FLAIR substantially surpasses the Efficiency-Only baseline in terms of adherence to user preferences and human-like feeding across various settings, as hypothesized (H2). The exceptions, where the performance difference between Efficiency-Only and FLAIR is not significant, occur in settings where the bite sequence, generated based solely on efficiency, inadvertently matches the user's preferences. Fig. 7 shows participant ratings that compare FLAIR with baseline approaches in settings where no user preferences were specified. By leveraging commonsense reasoning, FLAIR significantly outperforms the Efficiency-Only baseline across most plates by ensuring bite variety and appropriately pairing food items with dips, resulting in a more human-like feeding experience (H3).\nB. Comparisons with Task Planning Baselines\nFor evaluating necessity of pre-acquisition actions, FLAIR first estimates the distribution of food items by sensing density and entropy metrics from segmented observations, and then uses a hierarchical decision-tree style approach. In this section, we compare this approach against other task planning baselines. The closest relevant work, VAPORS [52], concentrates on noodle dishes and employs physics-based simulations for decision-making between twirling and grouping noodles. Our work, however, encompasses a broader spectrum of food textures and types (such as solids like meatballs, semi-solids like mashed potatoes, and noodle-like items like spaghetti), making direct adaptation of VAPORS challenging due to the complex physics simulations required for accurately representing their varied interactions. Taking this gap into account, we compare FLAIR's task planning accuracy against 3 established baselines: (i) VAPORS, (ii) VLM-TaskPlanner, which queries a VLM (GPT-4V [3]) using 10 in-context examples from the training set to decide between candidate actions, and (iii)"}, {"title": "V. DISCUSSION", "content": "FLAIR is a first step towards robot-assisted feeding in real-world scenarios, adeptly handling various in-the-wild meals composed of diverse food items. We deploy FLAIR across 2 institutions and 3 different embodiments with a library of 7 dexterous skills. Our evaluations include both bite acquisition and bite transfer, along with a demonstration feeding a complete plate to a care recipient. FLAIR showcases the ability to abide by preferences across 42 individuals and a range of diverse plates, without compromising on efficient food pickup. Through our extensive evaluations, we identify the following limitations to guide future work in robot-assisted feeding.\nLimitations of Food Perception using VLMs. While current VLMs are capable of identifying food items on a plate, using these generated identifiers with open-set object detectors can sometimes lead to inaccuracies. FLAIR addresses this challenge by enriching the identifiers with a set of hand-coded descriptors tailored to the typical type of the food item, for example, specifying 'banana' as ['banana piece', 'sliced banana'] and 'fettuccine' as ['fettuccine pasta', 'fettuccine noodles']. In the future, advancements in open-set object detection may eventually make such specific enhancements unnecessary.\nLimitations of Food Manipulation Skills. FLAIR leverages a library of skills inspired by state-of-the-art food manipulation methods, but open challenges that occasionally occur include: slippage during skewering, failing to twirl noodles or scoop mashed potatoes into reasonable bite sizes, failing to cut tough items, and errors due to perception (erroneous depth sensing or imprecise food detection) which can cause manipulation imprecision. Although some of these failures can be addressed by re-trying (as long as the item is re-detected), these challenges can be mitigated in the future by making the skills themselves reactive, enabling adaptive utensil trajectories that adjust to food slippage or deformation on the fly.\nLimitations of Bite Sequencing. We harness LLMs to plan efficient bite sequences that adhere to user preferences. However, today's language models can sometimes generate unrealistic or irrelevant outputs (\"hallucinations\"). In FLAIR, we reduce hallucinated artifacts in bite sequencing by using prompt-engineering strategies which we detail in the Appendix. However, even with templated prompts, FLAIR is still limited by the tendency of language models to occasionally neglect context, such as the manipulation efficiency of food items and their remaining portions. In the future, we are excited by structured prompting strategies [17] and incorporation of real-time corrections from the user [49] to address these challenges.\nAlthough these are current limitations, FLAIR's modular system design allows for easy interchange of the perception/planning stacks or even skills themselves. Thus, it will be able to take full advantage of future advances in VLMs or better low-level skill policies that are learned or engineered."}]}