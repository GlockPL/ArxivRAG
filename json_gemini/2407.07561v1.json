{"title": "FLAIR: Feeding via Long-horizon AcquIsition of Realistic dishes", "authors": ["Rajat Kumar Jenamani", "Priya Sundaresan", "Maram Sakr", "Tapomayukh Bhattacharjee", "Dorsa Sadigh"], "abstract": "Abstract-Robot-assisted feeding has the potential to improve\nthe quality of life for individuals with mobility limitations who are\nunable to feed themselves independently. However, there exists\na large gap between the homogeneous, curated plates existing\nfeeding systems can handle, and truly in-the-wild meals. Feeding\nrealistic plates is immensely challenging due to the sheer range of\nfood items that a robot may encounter, each requiring specialized\nmanipulation strategies which must be sequenced over a long\nhorizon to feed an entire meal. An assistive feeding system should\nnot only be able to sequence different strategies efficiently in order\nto feed an entire meal, but also be mindful of user preferences\ngiven the personalized nature of the task. We address this with\nFLAIR, a system for long-horizon feeding which leverages the\ncommonsense and few-shot reasoning capabilities of foundation\nmodels, along with a library of parameterized skills, to plan and\nexecute user-preferred and efficient bite sequences. In real-world\nevaluations across 6 realistic plates, we find that FLAIR can\neffectively tap into a varied library of skills for efficient food\npickup, while adhering to the diverse preferences of 42 partici-\npants without mobility limitations as evaluated in a user study.\nWe demonstrate the seamless integration of FLAIR with existing\nbite transfer methods [19, 28], and deploy it across 2 institutions\nand 3 robots, illustrating its adaptability. Finally, we illustrate\nthe real-world efficacy of our system by successfully feeding a\ncare recipient with severe mobility limitations. Supplementary\nmaterials and videos can be found at: emprise.cs.cornell.edu/flair.", "sections": [{"title": "I. INTRODUCTION", "content": "Eating is a vital part of everyday life, yet millions worldwide\nstruggle to feed themselves independently due to mobility\nlimitations caused by conditions such as neurological disor-\nders, injuries, the effects of aging, or other health compli-\ncations [54]. These individuals often rely on caregivers for\nmeal assistance, which impacts their sense of independence,\ndaily routines, and the social experience of dining [27, 41, 50].\nMoreover, feeding is one of the most time-consuming Activi-"}, {"title": "II. RELATED WORK", "content": "Robot-Assisted Feeding. While various commercial robot-\nassisted feeding systems [1, 2] have been introduced, they\ntypically rely on pre-programmed trajectories or user teleop-\neration. This limited autonomy has hindered their widespread\nadoption and retention, and inspired autonomous methods for\nbite acquisition and transfer. Prior work in bite acquisition\nhas focused on developing individual food manipulation skills\nfor specific food types. Various works [18, 21, 22, 51] tackle\nacquisition of solid bite-sized foods, and demonstrate effec-\ntive skewering strategies based on the food item's pose and\nmaterial properties. Sundaresan et al. [52] propose visually\nparameterized primitives for twirling and grouping noodle-like\ndishes, and show generalization to unseen noodles. Beyond\nfork-based manipulation, Grannen et al. [24] plan bimanual\nscooping actions with two custom utensils, while Tai et al.\n[53] and Zhang et al. [58] develop specialized strategies for\nscooping with a spoon and cutting with a knife, respectively.\nHowever, no prior work in robot-assisted feeding considers\ncomplete, in-the-wild meals containing various food types\n(noodles, semisolids, sauces, cuttable food items, etc.) within\nthe same plate, as typically encountered in everyday scenarios.\nIn this work, we leverage insights from the aforementioned\nstate-of-the-art food manipulation works to develop a large\nlibrary of bite acquisition skills, and use foundation models\nto sequence these skills for efficiently feeding realistic dishes\nwhile obeying user preferences. To the best of our knowledge,\nFLAIR is the first of any autonomous feeding system to tackle\nin-the-wild meals containing various food types, and incorpo-\nrate bite sequencing preferences for long-horizon feeding.\nVarious works have shown joint bite acquisition with trans-\nfer [6, 19, 28]. However, they typically consider bite acquisi-\ntion actions over a single timestep and not over the complete\nmeal. In contrast, we illustrate that our long-horizon bite\nacquisition framework can seamlessly integrate with existing\nmethods for bite transfer [19, 28], and demonstrate feeding of\na full meal to a care recipient.\nFoundation Models for Robotic Manipulation. Two of the\nmost challenging aspects associated with feeding are planning\nover available skills, and developing a library of food manip-\nulation skills themselves. To this end, several recent works in\nrobotic manipulation use foundation models such as vision-\nlanguage models (VLMs) [3, 31, 35, 46] or large language"}, {"title": "III. FLAIR: FEEDING VIA LONG-HORIZON ACQUISITION\nOF REALISTIC DISHES", "content": "In this section, we present FLAIR, a system for feeding\ncomplete meals which combines existing foundation models"}, {"title": "A. Hardware System", "content": "We tackle a wide range of food categories in this work such\nas fruits, vegetables, noodles, meat, soft foods, dipping sauces,\nand non-bite-sized items that require cutting. Many of these\nfoods require specialized, dynamic manipulation strategies\nthat typical 6 or 7-DoF robots struggle with due to their\nlimited workspace. We thus implement FLAIR on Kinova and\nFranka robot arms equipped with a motorized feeding utensil\nmounted at the end-effector, adapting the design from [48].\nThe utensil contains a fork attachment and has two degrees of\nfreedom corresponding to the orientation of the fork tines and\nthe tilt angle. This allows for directly controlling the utensil\nto perform dynamic movements like twirling and scooping,\nwhile the robot handles moving between waypoints in the\nworkspace via Cartesian position control. We also use a wrist-\nmounted RGB-D Realsense camera with a known end-effector\nto camera transformation. This enables perceiving plates of\nfood and localizing food items in the 3D workspace. We note\nthat the same hardware was replicated on two different Kinova\narms and one Franka Emika Panda, each with their separate\nfork attachment and sensors across two different institutions\n(detailed in Appendix), demonstrating the reproducibilty of\nour method and hardware (Fig. 2)."}, {"title": "B. Long-Horizon Bite Acquisition Framework", "content": "With access to a hardware platform that supports dexterous\nfood manipulation strategies, our goal is to plan and execute\nlong-horizon bite sequences that cater to a user's preference\nwhile efficiently feeding a meal.\nProblem Formulation. We assume access to an RGB-D plate\nimage observation $o_t \\in O = \\mathbb{R}^{W \\times H \\times 4}$ of width W and height\nH, and an optional natural language instruction $l_{pref}$ from the\nuser, representing their preferred feeding strategy at a high-\nlevel (i.e., $l_{pref}$ = \"Feed me alternating bites of X and Y\"\nor \"Only feed me X\"). X and Y can denote an arbitrary food\nitem semantic label (i.e. \u201cspaghetti\u201d, \u201cstrawberry\u201d, \u201ccaramel\u201d)\nor category (i.e. \u201cnoodles\u201d, \u201cfruit\u201d, \u201csauce\u201d).\nWe further assume access to a library $L = {\\phi_1,...,\\phi_N}$\nof N skills that the robot can use to manipulate food items.\nEach skill $\\phi_i(p)$ represents a parameterized manipulation\nprimitive that takes in parameters p and outputs low-level\nmotor commands. We represent a low-level action at time t\nby $a_t = (x, y, z, \\beta, \\gamma, \\psi)$, where (x, y, z) denotes the position\nof the feeding utensil tip, $\\beta$ and $\\gamma$ denote pitch and roll of\nthe utensil respectively, and $\\psi$ denotes the robot's end effector\nroll angle. Thus, the output of any skill is a sequence of T\nactions ${a_t, a_{t+1},...,a_{t+T}}$ that the robot takes to execute\nthe particular strategy. For instance, a skewering skill may take\nthe position and orientation of a desired food item as input,\nand output a trajectory that skewers the item of choice. Our\ngoal is to plan and execute a sequence of parameterized skills\n${\\phi_1(p_1), \\phi_2(p_2),...,\\phi_H(p_H)}$ which results in efficient and\nuser-preferred bite acquisition, where H is the total number\nof skills to execute to complete feeding a plate and $p_h$ refers\nto the parameters of skill $\\phi_h \\in L$.\nState Representations for Food. Our approach ad-\ndresses the main challenges in long-horizon bite ac-\nquisition-parameterizing low-level skills and sequencing\nthem-by integrating state-of-the-art visual-language models.\nWe use visual state estimates and semantic features of food\nitems to guide skill parameterization and sequencing.\nFor a given plate observation $o_t$ at time t, we first\nquery GPT-4V [3] in a few-shot manner to recognize\nwhich food items are present. We prompt the model\nwith a few in-context examples of plate images and their\ncorresponding ground truth food item semantic labels, and\nask the model to complete the prompt for the test image $o_t$.\nGPT-4V outputs a list of semantic labels $l_t$ that are present,\n(i.e., $l_t$=['fettuccine', 'chicken', 'broccoli'])\nalong with their corresponding categories $C_t$ (i.e., $C_t$\n= ['noodles', 'meat/seafood',\u2018vegetable\u2019,\u2018cuttable\u2019]).\nThese categories are relevant for associating the appropriate\nskill to each food item for bite acquisition. We then pass\nthe recognized semantic labels to GroundingDINO [38],\nan open-vocabulary VLM, for bounding box detection. For\neach bounding box, we use SegmentAnything (SAM) [33]\nto refine these bounding boxes into segmentation masks\n${m_1,m_2,\u2026\u2026\u2026,m_D}$ for all D items detected.\nSkill Library The segmented representations of food we"}, {"title": "C. Bite Sequencing via Foundation Models", "content": "We introduce a unified framework for planning and exe-\ncuting bite sequences that are efficient and adhere to user\npreferences. With access to a library of skills $L$, task planner\n$T$, and user preference $l_{pref}$, we show how the commonsense-\nreasoning capabilities of LLMs enable them to act as few-shot\nplanners for bite sequencing, inherently balancing preference\nand efficiency.\nWe prompt an LLM, in our case GPT-4V with relevant\ncontext about the meal. This includes the semantic food\nitem labels $l_t$ and the user's preference $l_{pref}$. We augment\nthis context with: (i) a history of bites taken so far, (ii)\nan estimate of the portions of each food type remaining,\nand (iii) the per-item efficiencies which correspond to the"}, {"title": "D. Integration of Acquisition and Transfer", "content": "The self-contained nature of our bite acquisition frame-\nwork allows for straightforward integration with bite transfer\nframeworks, and is agnostic to the exact approach used. We\ndemonstrate this easy combination with two existing methods:\n(i) an outside-mouth bite transfer method [19] that features\nvisual servoing capabilities, and (ii) a recent method for inside-\nmouth transfer [28] that leverages robust mouth tracking and\nphysical interaction-aware control."}, {"title": "IV. EXPERIMENTS", "content": "We evaluate the effectiveness of FLAIR for feeding diverse\nplates each containing various types of food items. We first\nconduct a user study to assess FLAIR's ability to perform long-\nhorizon bite acquisition of in-the-wild plates, while adhering\nto user preferences and efficiently feeding bites. For all ac-\nquisition experiments, we interchangeably use 2 Kinova Gen3\narms (one 6-DoF, and another 7-DoF), and a 7-DoF Franka\nEmika Panda. We then ablate our hierarchical task planner $T$\nagainst various state-of-the-art baselines [3, 39, 52]. Finally,\nwe evaluate the real-world efficacy of our system for feeding\na complete plate to a care recipient with mobility limitations."}, {"title": "A. Bite Acquisition Experiments", "content": "Baselines: FLAIR presents a unique approach of taking into\naccount both preference and efficiency considerations for\nbite sequencing. This naturally begs the question of how an\nEfficiency-Only or Preference-Only approach would compare.\nWe implement an Efficiency-Only baseline which greedily\nselects the next bite as the item which requires the least\nnumber of pre-acquisition and acquisition skills for pickup\nin the current instant, as dictated by the task planner |T(\u00b7,\u00b7)|.\nThe Preference-Only baseline is identical to FLAIR in imple-\nmentation, but notably omits efficiency scores when prompting\nthe LLM to generate a next bite. This encourages the LLM\nto only respect a user's preference without consideration for\nhow efficient a particular bite may be. In the case that a user\nhas no preference for feeding, we refer to the Preference-Only\nbaseline as Commonsense-Only.\nEvaluation Plates: We consider an evaluation suite of 6 di-\nverse plates of food spanning a wide range of food categories,\nvisualized in Fig. 4. We include 2 in-the-wild noodle dishes: a\nspaghetti and meatballs plate which is a prepared frozen meal\nfrom a grocery store, and a fettuccine alfredo dish with chicken\nand broccoli ordered from Applebee's via Doordash. We also\nconsider 2 homemade semisolid dishes: mashed potatoes with\nsausage, and oatmeal with strawberries. Lastly, we evaluate an\nappetizer plate of strawberry, watermelon, celery, ranch, and"}, {"title": "B. Comparisons with Task Planning Baselines", "content": "For evaluating necessity of pre-acquisition actions, FLAIR\nfirst estimates the distribution of food items by sensing density\nand entropy metrics from segmented observations, and then\nuses a hierarchical decision-tree style approach. In this section,\nwe compare this approach against other task planning base-\nlines. The closest relevant work, VAPORS [52], concentrates\non noodle dishes and employs physics-based simulations for\ndecision-making between twirling and grouping noodles. Our\nwork, however, encompasses a broader spectrum of food\ntextures and types (such as solids like meatballs, semi-solids\nlike mashed potatoes, and noodle-like items like spaghetti),\nmaking direct adaptation of VAPORS challenging due to the\ncomplex physics simulations required for accurately represent-\ning their varied interactions. Taking this gap into account, we\ncompare FLAIR's task planning accuracy against 3 established\nbaselines: (i) VAPORS, (ii) VLM-TaskPlanner, which queries\na VLM (GPT-4V [3]) using 10 in-context examples from the\ntraining set to decide between candidate actions, and (iii)"}, {"title": "C. Demonstration of Real-World Feeding", "content": "We demonstrate FLAIR's effectiveness in helping a care\nrecipient with severe mobility restrictions eat an entree dish\ncomprising boiled baby carrots, watermelon, strawberries,\nranch dressing, and chocolate sauce. The care recipient, a\n44-year-old Caucasian/White female with Multiple Sclerosis\nfor 19 years, has a severely limited range of motion in\ntheir head and neck. Consequently, they require inside-mouth\ntransfer [28] of acquired bites for successful feeding.\nIn the pre-study questionnaire, the care recipient mentioned\nthat they typically have a preferred order in which they like to\neat their meal. They convey this preference to their caregivers"}, {"title": "V. DISCUSSION", "content": "FLAIR is a first step towards robot-assisted feeding in real-\nworld scenarios, adeptly handling various in-the-wild meals\ncomposed of diverse food items. We deploy FLAIR across 2\ninstitutions and 3 different embodiments with a library of 7\ndexterous skills. Our evaluations include both bite acquisition\nand bite transfer, along with a demonstration feeding a com-\nplete plate to a care recipient. FLAIR showcases the ability\nto abide by preferences across 42 individuals and a range of\ndiverse plates, without compromising on efficient food pickup."}, {"title": "VI. ACKNOWLEDGEMENT", "content": "This work was partly funded by NSF IIS #2132846,\nCAREER #2238792, and DARPA under Contract\nHR001120C0107. It was additionally supported by\nNSF awards #2132847, #2218760, the Office of Naval\nResearch award #N00014-21-1-2298, and AFOSR YIP. Priya\nSundaresan is supported by an NSF GRFP and Maram Sakr is\nsupported by the Natural Sciences and Engineering Research\nCouncil of Canada (NSERC). We would like to acknowledge\nAnthony Song, Pranav Thakkar, Eric Hu, and Karan Jha for\ntheir assistance with user studies and annotations for our task\nplanning experiments."}]}