{"title": "Assessing Brittleness of Image-Text Retrieval Benchmarks from Vision-Language Models Perspective", "authors": ["Mariya Hendriksen", "Shuo Zhang", "Ridho Reinanda", "Mohamed Yahya", "Edgar Meij", "Maarten de Rijke"], "abstract": "Image-text retrieval (ITR), an important task in information retrieval (IR), is powered by pretrained vision-language models (VLMs) that consistently achieve state-of-the-art performance. However, a significant challenge lies in the brittleness of existing ITR benchmarks. In standard datasets for the task, captions often provide broad summaries of scenes, neglecting detailed information about specific concepts. Additionally, the current evaluation setup assumes simplistic binary matches between images and texts and focuses on intra-modality rather than cross-modal relationships, which can lead to misinterpretations of model performance. Motivated by this gap, in this study, we focus on examining the brittleness of the ITR evaluation pipeline with a focus on concept granularity. We start by analyzing two common benchmarks, MS-COCO and Flickr30k, and compare them with their augmented versions, MS-COCO-FG and Flickr30k-FG, given a specified set of linguistic features capturing concept granularity. We discover that Flickr30k-FG and MS COCO-FG consistently achieve higher scores across all the selected features. To investigate the performance of VLMs on coarse and fine-grained datasets, we introduce a taxonomy of perturbations. We apply these perturbations to the selected datasets. We evaluate four state-of-the-art models - ALIGN, AltCLIP, CLIP, and GroupViT - on both the standard and fine-grained datasets under zero-shot conditions, with and without the applied perturbations. The results demonstrate that although perturbations generally degrade model performance, the fine-grained datasets exhibit a smaller performance drop than their standard counterparts. Moreover, the relative performance drop across all setups is consistent across all models and datasets, indicating that the issue lies within the benchmarks themselves. We conclude the paper by providing an agenda for improving ITR evaluation pipelines.", "sections": [{"title": "1 INTRODUCTION", "content": "Image-text retrieval (ITR) is a bidirectional retrieval task that concerns retrieving top-k images or texts/captions, given a query in a different modality [3]. The task bridges the gap between visual and textual information, making search results richer, more relevant, and easier to access [8]. By considering both images and text, ITR systems offer a more intuitive way for users to explore retrieved information. While vision-language models (VLMs) have achieved state-of-the-art (SOTA) performance on the task [15, 41, 45, 53, 61, 80], there is a need to refine the datasets and evaluation methods used to assess their performance. This need arises from two key challenges that we explain next.\nChallenge 1: Concept granularity. The first challenge is related to the level of detail (granularity) within ITR datasets as existing benchmarks often suffer from coarse-grained textual descriptions [12, 21, 36]. In the context of VL datasets, granularity refers to the specificity of the relationship between images and their corresponding textual descriptions. Fine-grained datasets provide detailed and specific captions for each image, capturing subtle nuances and intricate details.\nConversely, coarse-grained datasets offer more general descriptions, focusing on broader aspects of the images. Popular benchmarks in the field, such as MS-COCO [13, 48] and Flickr30k [85], typically feature coarse captions. They contain images of complex scenes with captions that represent high-level overviews of corresponding scenes. This lack of granularity makes it challenging to evaluate if models can learn to identify specific objects or aspects within a scene based on detailed attributes. Consequently, it hinders the ability to evaluate models' performance on the ITR task.\nSome of the recent work in the domain aims to mitigate the problem by introducing augmentations of datasets. For instance, Chen et al. [12] propose to augment captions with additional contextual details extracted directly from associated images, thereby introducing augmented versions of MS-COCO and Flickr30k, namely MS-COCO-FG and Flickr30k-FG.\nChallenge 2: Evaluation metrics. The second challenge concerns the limitations of current ITR evaluation metrics. These metrics have several shortcomings:\n(1) Binary Matching Assumption: They assume a binary match be-tween images and texts based on pre-defined image-caption"}, {"title": "2 PRELIMINARIES", "content": "Notation. We follow notation from prior work [5, 7]. Let D be a dataset of N image-caption tuples: D = {(x', {x}-1)}1. Each\nN\ntuple i \u2208 N contains one image x, and k captions x, where\n1 \u2264 j \u2264 k. All captions in tuple i \u2208 N are considered matching\ncaptions w.r.t. image x, in the tuple i. In the context of our work,\nqueries, and documents are sampled from the image-caption tuples.\nSpecifically, we denote query as q, and i-th retrieved document\nas di. We further denote q and d are vectors representing a query\nand a document respectively; reli denotes the relevance for the\ni-th retrieved document. We further denote frel(.) as the relevance\nscore function and fsim(.) as the similarity function.\nTask. The image-text retrieval (ITR) task is defined analogously to\nthe standard information retrieval task: given a query and a set\nof candidates, we rank all candidates w.r.t. their relevance to the\nquery. The query can be either a caption or an image. Similarly, the\nset of candidate items can contain either images or captions. ITR\nis performed across modalities, therefore, if the query is a caption\nthen the set of candidates are images, and vice versa. Hence, the task\ncomprises two subtasks: (i) text-to-image retrieval: retrieving images\nrelevant to a caption query, and (ii) image-to-text retrieval: retrieving\nrelevant captions that describe an image query. The performance is\ntypically evaluated bidirectionally using R@k where k = {1, 5, 10},\nand sum of recall (rsum)."}, {"title": "3 CONCEPT GRANULARITY IN IMAGE-TEXT RETRIEVAL DATASETS", "content": "In this section, we outline the features for analyzing the granularity\nof concepts in ITR datasets. We will also describe the datasets se-\nlected for evaluation and perform an analysis based on the provided\ndefinition of granularity."}, {"title": "3.1 Granularity Features in Image-Text Retrieval", "content": "In this part of our study, we focus on the features that contribute\nto defining the granularity of ITR datasets, specifically on Noun\nPhrase (NP) level and Caption-level features."}, {"title": "3.1.1 NP-level Granularity.", "content": "We start by discussing the linguistic\nfeatures that contribute to the granularity of NPs in captions.\nModifiers of the Noun. Adjectives and Complement Phrases (CPs)\nprovide nuanced details about identified objects in images [58, 91].\nQuantifying these modifiers helps measure the level of detail and\ngranularity associated with the objects [46]. To do this, we count the\nnumber of adjectives and CPs per identified noun in the captions.\nSemantics: Concept Depth. The semantic feature of concept depth\nprovides insight into the richness of the conceptual information"}, {"title": "3.1.2 Caption-level Granularity.", "content": "We continue our overview of the\nlinguistic features that contribute to the granularity of captions by\nconsidering the caption-level features.\nCaption Length. The number of characters in a sentence reflects\nthe amount of information conveyed [38]. Longer captions are likely\nto include more details, contributing to a finer level of granularity in\nthe image-caption relationship. To quantify the aspect, we measure\nthe total word count in each caption.\nNumber of Words. The total word count in a caption contributes\nto its richness [38]. A higher word count suggests a more elaborate\ndescription, indicating a finer granularity in conveying the content\nand context associated with the image. To quantify the feature, we\ncount each caption's total number of words.\nSemantics Diversity of Concepts per Caption. The semantic\nfeature of concept diversity is crucial for analyzing the granularity\nof concepts within ITR datasets [30]. Concept diversity evaluates\nthe variety and richness of concepts expressed within a caption,\ncapturing the range of ideas and semantic complexity present in the\ndataset. To quantify this feature, we compute the ratio of unique\nsynonyms to the total number of words in the given caption."}, {"title": "3.2 Granularity Analysis", "content": "Next, we analyze the selected datasets in terms of granularity versus\ncoarseness, with a focus on various linguistic aspects at both the\nNP and caption levels."}, {"title": "3.2.1 Datasets.", "content": "In this work, we use the following datasets:\nMS-COCO [48] is a large-scale object detection, segmentation,\nand captioning dataset that consists of 123,287 images and 616,435\ncaptions, each image is annotated with 5 captions.\nFlickr30k [85] is an image caption corpus consisting of 158,915\ncrowd-sourced captions describing 31,783 images. Each image is\nannotated with 5 captions.\nMS-COCO-FG and Flickr30k-FG [12] are augmented variants\nof Flickr30k and MS-COCO, respectively, these datasets captions\ncontain additional contextual details extracted from the associated\nimages.\nFor all the datasets, we use the training, validation, and test splits\nfrom [31]."}, {"title": "4 EVALUATION FRAMEWORK", "content": "4.1 Perturbations\nWe describe the perturbations we designed for evaluation of the\nrobustness and performance of VLMs in the context of ITR. The\nperturbations are categorized into word-level and sentence-level\nand focus on evaluating the model's response to typos, synonyms,\ndistractions, and challenges related to compositionality and sen-\nsitivity to word order in the context of ITR task. The overview of\nperturbations is shown in Figure 1."}, {"title": "4.1.1 Word-level Perturbations.", "content": "Word-level perturbations are ap-\nplied at the level of individual words within a caption. The focus\nis on investigating the model's robustness to typos and synonyms.\nThe perturbations types include:\nTypos. Typos are common in real-world scenarios, and evaluating\na model's response to such errors is crucial for ensuring its practical\nusability in information retrieval (IR) [65, 94] and on the image-\ncaption retrieval (ICR) task in particular [76]. This perturbation\nassesses the model's ability to handle input variations introduced\nby typographical mistakes, providing insights into its robustness in\nretrieving images given textual descriptions. Typos perturbations\naim to assess the model's resilience to typographical errors. This\ntype has been previously tested on sentiment analysis, duplicate\nquestion detection, and natural language inference [40, 71]. How-\never, it has not been applied in the context of evaluating VLMs on\nthe ITR task. The subtypes are as follows.\n\u2022 Character Swap: Swaps two random adjacent word characters\nin a caption, simulating the introduction of a typo through char-\nacter transposition. This perturbation allows us to evaluate the\nmodel's ability to recognize and correct character-level errors.\n\u2022 Missing Character: Removes a randomly selected character\nfrom the input text, mimicking the effect of a typo where a char-\nacter is omitted. This perturbation tests the model's robustness"}, {"title": "4.1.2 Synonyms.", "content": "Synonym-based perturbations aim to assess the\nmodel's adaptability and robustness to variations in language, specif-\nically focusing on the substitution of nouns and adjectives with\ntheir synonyms. This perturbation type is motivated by the need\nto evaluate VLMs capacity to comprehend and retrieve images and\ncaptions when faced with lexical variations that convey similar\nmeanings [19, 29]. Specifically, we focus on testing the models' ca-\npacity to retrieve the right image using semantically similar nouns\nand adjectives. The subtypes are as follows.\n\u2022 Synonym Noun: This perturbation involves replacing k nouns\nin a given caption with their synonyms. The motivation behind\nthis perturbation is to examine how well the model handles vari-\nations in nouns, which is important for accurate and descriptive\nimage-caption associations.\n\u2022 Synonym Adjective: This perturbation implies replacing k ad-\njectives in a given caption with their synonyms. Adjectives play\na vital role in expressing characteristics and qualities associated\nwith visual elements in an image. Introducing synonym substi-\ntutions in adjectives aims to assess the model's proficiency in\nmaintaining the descriptive quality of captions when faced with\nlexical variations."}, {"title": "4.1.3 Sentence-level Perturbations.", "content": "Sentence-level perturbations\nare applied at the level of sentences in a caption. The focus is on\nevaluating the model's resilience to distracting elements, composi-\ntionality-related challenges, and sensitivity to word order.\nDistraction-Based Perturbations. Distraction-based perturba-\ntions aim to evaluate the model's robustness to distracting elements\nwithin captions. Specifically, we focus on the statements that are\nalways true and do not add any meaningful content to the caption.\nThe motivation is to understand how well the model can filter out\nrelevant information from distractors, a critical skill for accurate\nimage-caption retrieval in the presence of additional context [68]."}, {"title": "4.1.4 Compositionality-Related Perturbations.", "content": "Compositionality-\nrelated perturbations assess the model's ability in the context of\ncompositionality [57, 87], focusing on its sensitivity to word order\nchanges within sentences.\nSensitivity to Word Order. This category of perturbations tests\nthe model's sensitivity to word order changes within sentences.\n\u2022 Shuffle Nouns and Adjectives: This subtype involves shuffling\nthe order of nouns and adjectives within the input sentence. The\nmotivation is to examine how well the model can handle changes\nin the arrangement of descriptive elements, crucial for capturing\nthe visual details of an image accurately.\n\u2022 Shuffle All Words: Randomly shuffling the order of all words\nin the input sentence to assess the model's general flexibility in\nunderstanding and generating coherent captions despite drastic\nchanges in word order. This perturbation aims to reveal the\nmodel's adaptability to varied sentence structures.\n\u2022 Shuffle All Words But Nouns and Adjectives: Shuffling all\nwords except for nouns and adjectives tests the model's ability to\nmaintain the key descriptive elements in their original positions,\nexamining its proficiency in preserving the essential details while\nundergoing significant rearrangement. In practice, it implies\nkeeping the nouns and adjectives in fixed positions and randomly\nshuffling all the other words.\n\u2022 Shuffle within Trigrams: Dividing the input sentence into\ntrigrams and shuffling the order of words within each trigram\nevaluates the model's response to localized word rearrangements.\nThis perturbation offers insights into the model's sensitivity\nto changes in smaller, contextually relevant segments of the\nsentence.\n\u2022 Shuffle Trigrams: Dividing the input sentence into trigrams\nand shuffling the order of entire trigrams assesses the model's\nability to comprehend and generate captions when faced with\nlarger-scale rearrangements. This perturbation provides a broader\nperspective on the model's understanding of sentence composi-\ntion and structure in diverse contexts."}, {"title": "4.2 Evaluation Metric", "content": "The current evaluation framework for ITR faces challenges due\nto the binary match assumption, the focus on intra-modality com-\nparisons, and the disregard of cross-modal relationships across\nimage-caption tuples [10, 28, 28, 32, 52, 72]. Such limitations hinder\nthe comprehensive assessment of model performance, failing to\ncapture the relationships between visual and textual content. To\naddress these shortcomings, we propose a novel evaluation metric\nthat uses similarity functions to estimate relevance scores across\nmodalities and image-caption tuples.\nGiven a query q, and a ranked list of top-k retrieved results\nK = [d\u00b9, ..., dk], we want to obtain a list of the relevance scores"}, {"title": "5 EXPERIMENTS", "content": "5.1 Models\nFor our experiments, we select four pre-trained large vision-language\n(VL) models that demonstrate SOTA performance on a variety of\nVL tasks and exhibit good performance on ITR in particular.\nALIGN [27] is a VLMs that addresses the challenge of costly cura-\ntion processes in VL representation learning by leveraging a noisy\ndataset of over one billion image alt-text pairs from the Concep-\ntual Captions dataset. Employing a simple contrastive dual-encoder\narchitecture, ALIGN learns to align visual and language represen-\ntations effectively. The model achieves SOTA results on a variety\nof VL tasks, outperforming more complex cross-attention models.\nThe learned representations enable zero-shot image classification\nand support cross-modality search with complex text and image\nqueries, showcasing the effectiveness and scalability of the ALIGN\nmodel in large-scale VL tasks.\nAltCLIP [15] is a multilingual VLM built upon CLIP [61]. It en-\nhances CLIP's capabilities by incorporating a pre-trained multi-\nlingual text encoder XLMR and employing a two-stage training\nschema. In the first stage, knowledge distillation from CLIP is con-\nducted through Teacher Learning, followed by Contrastive Learning\nin the second stage, where the model is trained on a small set of\nChinese and English text-image pairs. AltCLIP achieves SOTA per-\nformances on a variety of VL tasks. Furthermore, AltCLIP closely\nmatches CLIP's performance, indicating that simple alterations to\nCLIP's text encoder can lead to extended capabilities in handling\nmultilingual tasks.\nCLIP [61] is a dual encoder pre-trained on a dataset of 400 mil-\nlion (image, text) pairs collected from the internet. Its pre-training"}, {"title": "5.2 Three Experiments", "content": "To answer our RQs, we run the following experiments:\nIn Experiment 1, we assess the impact of refined text on VLMs\nperformance on the ITR task and validate the proposed evaluation\nframework (RQ1). We compare our experimental results with those\nreported in a prior study [12]. This study is chosen as our reference\ndue to its alignment with our concerns regarding the limitations of\ncurrent ITR benchmarks, along with its suggestions for enhance-\nments to evaluate models' abilities in fine-grained cross-modal\nsemantic matching. Additionally, Chen et al. [12] introduce aug-\nmented benchmarks, MS-COCO-FG and Flickr30K-FG, which we\nincorporate into our research. Our evaluation involves measuring\nthe recall at 1 for both image-to-text (i2t) and text-to-image (t2i)\nretrieval tasks.\nIn Experiment 2, we investigate the impact of dataset granularity\non ITR task performance (RQ2). We compare the performance of\nvision-language models on standard image-caption datasets (MS-\nCOCO and Flickr30k) and their more fine-grained counterparts\n(MS-COCO-FG and Flickr30k-FG). We evaluate the models on i2t\nand t2i tasks.\nIn Experiment 3, we further evaluate the effectiveness of our\nproposed evaluation framework and analyze the performance of\nstate-of-the-art vision-language models on the ITR task (RQ3). We\nconduct experiments where perturbations are applied to the four\nselected datasets. We examine the performance drop of models after\nperturbation and assess the robustness of models to changes in the\ninput data."}, {"title": "5.3 Results", "content": "Experiment 1: Refined Text Impact Evaluation. To address\nRQ1, we evaluate models R@1 performance for both i2t and t2i\nretrieval and compare the results obtained with original (MS-COCO\nand Flicker30k) and refined (MS-COCO-FG and Flickr30k-FG) cap-\ntions. The main findings from Table 2 underscore the impact of\nadding refined texts on the performance of ITR, with overall im-\nprovements in R@1 scores observed across datasets. The exceptions"}, {"title": "6 RELATED WORK", "content": "Cross-Modal Retrieval. Cross-modal retrieval (CMR) methods\ncreate a multimodal representation space, where the similarity of\nconcepts from different modalities can be measured using a distance\nmetric such as cosine or Euclidean distance. Some of the earliest ap-\nproaches in CMR used canonical correlation analysis [22, 34]. This\nwas later followed by the emergence of a dual encoder architecture\nthat combined recurrent and convolutional components, gaining\nprominence in the field and often employing a hinge loss [20, 74].\nFurther advancements have increased effectiveness through tech-\nniques like hard-negative mining [18]. Subsequently, the incorpora-\ntion of attention mechanisms, such as dual attention [54], stacked\ncross-attention [37], and bidirectional focal attention [49], further\nimproved performance. Other work aims to improve CMR perfor-\nmance through modality-specific graphs [73], or image and text\ngeneration modules [23], or learning sparse multimodal representa-\ntions [55]. And there is domain-specific research focusing on CMR\nin various fields such as fashion [21, 36], e-commerce [25], cultural\nheritage [63], and cooking [73].\nRecent methods use transformer-based dual encoders trained\non extensive data. ALBEF [41] aligns unimodal representations\nbefore fusion, X-VLM [89] adds a cross-modal encoder for fine-\ngrained VL representations. Florence [86] uses adaptation mod-\nels for object-level representations, and CLIP [61] predicts image-\ncaption pairs. ALIGN [41] uses a dual encoder on image alt-text\npairs. FILIP [84] features late multimodal interaction, and SLIP\n[53] combines language and image self-supervision. DeCLIP [47]\nimproves CLIP pretraining via self-supervision and cross-modal su-\npervision. AltCLIP [15] uses a pre-trained multilingual text encoder\nand a two-stage training schema. GroupViT [80] reintroduces the\ngrouping mechanism to vision transformers, dynamically forming\nvisual segments for various images.\nAnother line of work adopts transformer encoders [69] for the\nITR task [52], adapting models like BERT [17]. ViLBERT [50] and\nLXMERT [67] introduce a two-stream architecture, while B2T2 [2],\nVisualBERT [44], Unicoder-VL [39], VL-BERT [66], and UNITER\n[14] propose single-stream architectures. Oscar [45] incorporates\ncaption object tags with region features, and BEIT-3 [75] adapts\nmultiway transformers. This work focuses on transformer-based\ndual encoder models due to their performance on various VL tasks.\nWe select four SOTA methods and provide a comparative analysis\nof their performance on the ITR task.\nVision-Language Model Evaluation. The evaluation of VLMs as-\nsesses their performance across various tasks and datasets. Standard\nbenchmarks are MS-COCO [13, 48] and Flickr30k [85] for tasks like"}, {"title": "7 CONCLUSIONS", "content": "In this work, we focus on addressing the problem of brittleness of\nevaluation pipeline of the ITR task. We highlight two main con-\ncerns: the granularity of existing benchmarks and the limitations of\ncurrent evaluation metrics. We investigate the granularity of exist-\ning benchmarks, MS-COCO and Flickr30k, by comparing them with\ntheir augmented counterparts, MS-COCO-FG and Flickr30k-FG. We\npropose an evaluation framework that comprises a taxonomy of\nperturbations and an evaluation metric. For the experiments, we\nselect four SOTA VLMs, AltCLIP, ALIGN, CLIP, and GroupViT. We\nconduct three experiments. First, we focus on the reproducibility\nassessment of results obtained by integrating refined texts into"}]}