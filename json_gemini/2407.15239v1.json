{"title": "Assessing Brittleness of Image-Text Retrieval Benchmarks from Vision-Language Models Perspective", "authors": ["Mariya Hendriksen", "Shuo Zhang", "Ridho Reinanda", "Mohamed Yahya", "Edgar Meij", "Maarten de Rijke"], "abstract": "Image-text retrieval (ITR), an important task in information retrieval (IR), is powered by pretrained vision-language models (VLMs) that consistently achieve state-of-the-art performance. However, a significant challenge lies in the brittleness of existing ITR benchmarks. In standard datasets for the task, captions often provide broad summaries of scenes, neglecting detailed information about specific concepts. Additionally, the current evaluation setup assumes simplistic binary matches between images and texts and focuses on intra-modality rather than cross-modal relationships, which can lead to misinterpretations of model performance. Motivated by this gap, in this study, we focus on examining the brittleness of the ITR evaluation pipeline with a focus on concept granularity. We start by analyzing two common benchmarks, MS-COCO and Flickr30k, and compare them with their augmented versions, MS-COCO-FG and Flickr30k-FG, given a specified set of linguistic features capturing concept granularity. We discover that Flickr30k-FG and MS COCO-FG consistently achieve higher scores across all the selected features. To investigate the performance of VLMs on coarse and fine-grained datasets, we introduce a taxonomy of perturbations. We apply these perturbations to the selected datasets. We evaluate four state-of-the-art models - ALIGN, AltCLIP, CLIP, and GroupViT - on both the standard and fine-grained datasets under zero-shot conditions, with and without the applied perturbations. The results demonstrate that although perturbations generally degrade model performance, the fine-grained datasets exhibit a smaller performance drop than their standard counterparts. Moreover, the relative performance drop across all setups is consistent across all models and datasets, indicating that the issue lies within the benchmarks themselves. We conclude the paper by providing an agenda for improving ITR evaluation pipelines.", "sections": [{"title": "1 INTRODUCTION", "content": "Image-text retrieval (ITR) is a bidirectional retrieval task that concerns retrieving top-k images or texts/captions, given a query in a different modality [3]. The task bridges the gap between visual and textual information, making search results richer, more relevant, and easier to access [8]. By considering both images and text, ITR systems offer a more intuitive way for users to explore retrieved information. While vision-language models (VLMs) have achieved state-of-the-art (SOTA) performance on the task [15, 41, 45, 53, 61, 80], there is a need to refine the datasets and evaluation methods used to assess their performance. This need arises from two key challenges that we explain next.\nChallenge 1: Concept granularity. The first challenge is related to the level of detail (granularity) within ITR datasets as existing benchmarks often suffer from coarse-grained textual descriptions [12, 21, 36]. In the context of VL datasets, granularity refers to the specificity of the relationship between images and their corresponding textual descriptions. Fine-grained datasets provide detailed and specific captions for each image, capturing subtle nuances and intricate details.\nConversely, coarse-grained datasets offer more general descriptions, focusing on broader aspects of the images. Popular benchmarks in the field, such as MS-COCO [13, 48] and Flickr30k [85], typically feature coarse captions. They contain images of complex scenes with captions that represent high-level overviews of corresponding scenes. This lack of granularity makes it challenging to evaluate if models can learn to identify specific objects or aspects within a scene based on detailed attributes. Consequently, it hinders the ability to evaluate models' performance on the ITR task.\nSome of the recent work in the domain aims to mitigate the problem by introducing augmentations of datasets. For instance, Chen et al. [12] propose to augment captions with additional contextual details extracted directly from associated images, thereby introducing augmented versions of MS-COCO and Flickr30k, namely MS-COCO-FG and Flickr30k-FG.\nChallenge 2: Evaluation metrics. The second challenge concerns the limitations of current ITR evaluation metrics. These metrics have several shortcomings:\n(1) Binary Matching Assumption: They assume a binary match between images and texts based on pre-defined image-caption"}, {"title": "2 PRELIMINARIES", "content": "Notation. We follow notation from prior work [5, 7]. Let D be a dataset of N image-caption tuples: D = {(x', {x}-1)}1. Each\nN\ntuple i \u2208 N contains one image x, and k captions x, where\n1 \u2264 j \u2264 k. All captions in tuple i \u2208 N are considered matching\ncaptions w.r.t. image x, in the tuple i. In the context of our work,\nqueries, and documents are sampled from the image-caption tuples.\nSpecifically, we denote query as q, and i-th retrieved document\nas di. We further denote q and d are vectors representing a query\nand a document respectively; reli denotes the relevance for the\ni-th retrieved document. We further denote frel() as the relevance\nscore function and fsim (.) as the similarity function.\nTask. The image-text retrieval (ITR) task is defined analogously to\nthe standard information retrieval task: given a query and a set\nof candidates, we rank all candidates w.r.t. their relevance to the\nquery. The query can be either a caption or an image. Similarly, the\nset of candidate items can contain either images or captions. ITR\nis performed across modalities, therefore, if the query is a caption\nthen the set of candidates are images, and vice versa. Hence, the task\ncomprises two subtasks: (i) text-to-image retrieval: retrieving images\nrelevant to a caption query, and (ii) image-to-text retrieval: retrieving\nrelevant captions that describe an image query. The performance is\ntypically evaluated bidirectionally using R@k where k = {1, 5, 10},\nand sum of recall (rsum)."}, {"title": "3 CONCEPT GRANULARITY IN IMAGE-TEXT RETRIEVAL DATASETS", "content": "In this section, we outline the features for analyzing the granularity of concepts in ITR datasets. We will also describe the datasets selected for evaluation and perform an analysis based on the provided definition of granularity."}, {"title": "3.1 Granularity Features in Image-Text Retrieval", "content": "In this part of our study, we focus on the features that contribute to defining the granularity of ITR datasets, specifically on Noun Phrase (NP) level and Caption-level features."}, {"title": "3.1.1 NP-level Granularity", "content": "We start by discussing the linguistic features that contribute to the granularity of NPs in captions.\nModifiers of the Noun. Adjectives and Complement Phrases (CPs) provide nuanced details about identified objects in images [58, 91]. Quantifying these modifiers helps measure the level of detail and granularity associated with the objects [46]. To do this, we count the number of adjectives and CPs per identified noun in the captions.\nSemantics: Concept Depth. The semantic feature of concept depth"}, {"title": "3.1.2 Caption-level Granularity", "content": "We continue our overview of the linguistic features that contribute to the granularity of captions by considering the caption-level features.\nCaption Length. The number of characters in a sentence reflects the amount of information conveyed [38]. Longer captions are likely to include more details, contributing to a finer level of granularity in the image-caption relationship. To quantify the aspect, we measure the total word count in each caption.\nNumber of Words. The total word count in a caption contributes to its richness [38]. A higher word count suggests a more elaborate description, indicating a finer granularity in conveying the content and context associated with the image. To quantify the feature, we count each caption's total number of words.\nSemantics Diversity of Concepts per Caption. The semantic feature of concept diversity is crucial for analyzing the granularity of concepts within ITR datasets [30]. Concept diversity evaluates the variety and richness of concepts expressed within a caption, capturing the range of ideas and semantic complexity present in the dataset. To quantify this feature, we compute the ratio of unique synonyms to the total number of words in the given caption."}, {"title": "3.2 Granularity Analysis", "content": "Next, we analyze the selected datasets in terms of granularity versus coarseness, with a focus on various linguistic aspects at both the NP and caption levels."}, {"title": "3.2.1 Datasets", "content": "In this work, we use the following datasets:\nMS-COCO [48] is a large-scale object detection, segmentation, and captioning dataset that consists of 123,287 images and 616,435 captions, each image is annotated with 5 captions.\nFlickr30k [85] is an image caption corpus consisting of 158,915 crowd-sourced captions describing 31,783 images. Each image is annotated with 5 captions.\nMS-COCO-FG and Flickr30k-FG [12] are augmented variants of Flickr30k and MS-COCO, respectively, these datasets captions contain additional contextual details extracted from the associated images.\nFor all the datasets, we use the training, validation, and test splits from [31]."}, {"title": "4 EVALUATION FRAMEWORK", "content": "We describe the perturbations we designed for evaluation of the robustness and performance of VLMs in the context of ITR. The perturbations are categorized into word-level and sentence-level and focus on evaluating the model's response to typos, synonyms, distractions, and challenges related to compositionality and sensitivity to word order in the context of ITR task. The overview of perturbations is shown in Figure 1."}, {"title": "4.1.1 Word-level Perturbations", "content": "Word-level perturbations are applied at the level of individual words within a caption. The focus is on investigating the model's robustness to typos and synonyms. The perturbations types include:\nTypos. Typos are common in real-world scenarios, and evaluating a model's response to such errors is crucial for ensuring its practical usability in information retrieval (IR) [65, 94] and on the image-caption retrieval (ICR) task in particular [76]. This perturbation assesses the model's ability to handle input variations introduced by typographical mistakes, providing insights into its robustness in retrieving images given textual descriptions. Typos perturbations aim to assess the model's resilience to typographical errors. This type has been previously tested on sentiment analysis, duplicate question detection, and natural language inference [40, 71]. However, it has not been applied in the context of evaluating VLMs on the ITR task. The subtypes are as follows.\n\u2022 Character Swap: Swaps two random adjacent word characters in a caption, simulating the introduction of a typo through character transposition. This perturbation allows us to evaluate the model's ability to recognize and correct character-level errors.\n\u2022 Missing Character: Removes a randomly selected character from the input text, mimicking the effect of a typo where a character is omitted. This perturbation tests the model's robustness"}, {"title": "4.1.2 Synonyms", "content": "Synonym-based perturbations aim to assess the model's adaptability and robustness to variations in language, specifically focusing on the substitution of nouns and adjectives with their synonyms. This perturbation type is motivated by the need to evaluate VLMs capacity to comprehend and retrieve images and captions when faced with lexical variations that convey similar meanings [19, 29]. Specifically, we focus on testing the models' capacity to retrieve the right image using semantically similar nouns and adjectives. The subtypes are as follows.\n\u2022 Synonym Noun: This perturbation involves replacing k nouns in a given caption with their synonyms. The motivation behind this perturbation is to examine how well the model handles variations in nouns, which is important for accurate and descriptive image-caption associations.\n\u2022 Synonym Adjective: This perturbation implies replacing k adjectives in a given caption with their synonyms. Adjectives play a vital role in expressing characteristics and qualities associated with visual elements in an image. Introducing synonym substitutions in adjectives aims to assess the model's proficiency in maintaining the descriptive quality of captions when faced with lexical variations."}, {"title": "4.1.3 Sentence-level Perturbations", "content": "Sentence-level perturbations are applied at the level of sentences in a caption. The focus is on evaluating the model's resilience to distracting elements, compositionality-related challenges, and sensitivity to word order.\nDistraction-Based Perturbations. Distraction-based perturbations aim to evaluate the model's robustness to distracting elements within captions. Specifically, we focus on the statements that are always true and do not add any meaningful content to the caption. The motivation is to understand how well the model can filter out relevant information from distractors, a critical skill for accurate image-caption retrieval in the presence of additional context [68]."}, {"title": "4.1.4 Compositionality-Related Perturbations", "content": "Compositionality-related perturbations assess the model's ability in the context of compositionality [57, 87], focusing on its sensitivity to word order changes within sentences.\nSensitivity to Word Order. This category of perturbations tests the model's sensitivity to word order changes within sentences.\n\u2022 Shuffle Nouns and Adjectives: This subtype involves shuffling the order of nouns and adjectives within the input sentence. The motivation is to examine how well the model can handle changes in the arrangement of descriptive elements, crucial for capturing the visual details of an image accurately.\n\u2022 Shuffle All Words: Randomly shuffling the order of all words in the input sentence to assess the model's general flexibility in understanding and generating coherent captions despite drastic changes in word order. This perturbation aims to reveal the model's adaptability to varied sentence structures.\n\u2022 Shuffle All Words But Nouns and Adjectives: Shuffling all words except for nouns and adjectives tests the model's ability to maintain the key descriptive elements in their original positions, examining its proficiency in preserving the essential details while undergoing significant rearrangement. In practice, it implies keeping the nouns and adjectives in fixed positions and randomly shuffling all the other words.\n\u2022 Shuffle within Trigrams: Dividing the input sentence into trigrams and shuffling the order of words within each trigram evaluates the model's response to localized word rearrangements. This perturbation offers insights into the model's sensitivity to changes in smaller, contextually relevant segments of the sentence.\n\u2022 Shuffle Trigrams: Dividing the input sentence into trigrams and shuffling the order of entire trigrams assesses the model's ability to comprehend and generate captions when faced with larger-scale rearrangements. This perturbation provides a broader perspective on the model's understanding of sentence composition and structure in diverse contexts."}, {"title": "4.2 Evaluation Metric", "content": "The current evaluation framework for ITR faces challenges due to the binary match assumption, the focus on intra-modality comparisons, and the disregard of cross-modal relationships across image-caption tuples [10, 28, 28, 32, 52, 72]. Such limitations hinder the comprehensive assessment of model performance, failing to capture the relationships between visual and textual content. To address these shortcomings, we propose a novel evaluation metric that uses similarity functions to estimate relevance scores across modalities and image-caption tuples.\nGiven a query q, and a ranked list of top-k retrieved results K = [d\u00b9, ..., dk], we want to obtain a list of the relevance scores [rel\u00b9,..., relk] where reli denotes the relevance for the i-th retrieved document.\nTo estimate relevance scores across modalities and image-caption tuples we use a relevance score function. Formally, let q \u2208 Rd, and d\u2208 Rd be vectors that represent a query and a document. Let frel() be a relevance score function that takes a query q vector and a document vector di as input and returns a relevance score reli, i.e., a numerical value representing the relevance of the document w.r.t the query: frel (q, di) = reli. The relevance score function frel() is defined as follows:\n$$f_{rel}(q, d^{i}) = \\begin{cases}\n1 & \\text{if } i \\in N \\text{ s. th. } q = x^{i} \\text{ and } d^{i} \\in {x^{i}_{cj}}_{j=1}^{k}\n\\\\frac{1}{k} \\sum_{j=1}^{k} f_{sim}(q, d^{i}) & \\text{otherwise,}\n\\end{cases}$$\nwhere fsim() is a similarity function that takes a query and document vectors as input and returns a numerical value representing their similarity: fsim (q, d): Q\u00d7 D \u2192 R. In our work we use the cosine similarity function: fsim (q, d) = ||qd||. Therefore, we define the metric as follows:\n$${DCG^{CM}_{q,d}} = \\sum_{i=1}^{p} \\frac{reli}{log_{2}(i + 1)},$$\nwhere p denotes the position up to which the score is computed."}, {"title": "5 EXPERIMENTS", "content": "For our experiments, we select four pre-trained large vision-language (VL) models that demonstrate SOTA performance on a variety of VL tasks and exhibit good performance on ITR in particular.\nALIGN [27] is a VLMs that addresses the challenge of costly curation processes in VL representation learning by leveraging a noisy dataset of over one billion image alt-text pairs from the Conceptual Captions dataset. Employing a simple contrastive dual-encoder architecture, ALIGN learns to align visual and language representations effectively. The model achieves SOTA results on a variety of VL tasks, outperforming more complex cross-attention models. The learned representations enable zero-shot image classification and support cross-modality search with complex text and image queries, showcasing the effectiveness and scalability of the ALIGN model in large-scale VL tasks.\nAltCLIP [15] is a multilingual VLM built upon CLIP [61]. It enhances CLIP's capabilities by incorporating a pre-trained multilingual text encoder XLMR and employing a two-stage training schema. In the first stage, knowledge distillation from CLIP is conducted through Teacher Learning, followed by Contrastive Learning in the second stage, where the model is trained on a small set of Chinese and English text-image pairs. AltCLIP achieves SOTA performances on a variety of VL tasks. Furthermore, AltCLIP closely matches CLIP's performance, indicating that simple alterations to CLIP's text encoder can lead to extended capabilities in handling multilingual tasks.\nCLIP [61] is a dual encoder pre-trained on a dataset of 400 million (image, text) pairs collected from the internet. Its pre-training"}, {"title": "5.2 Three Experiments", "content": "To answer our RQs, we run the following experiments:\nIn Experiment 1, we assess the impact of refined text on VLMs performance on the ITR task and validate the proposed evaluation framework (RQ1). We compare our experimental results with those reported in a prior study [12]. This study is chosen as our reference due to its alignment with our concerns regarding the limitations of current ITR benchmarks, along with its suggestions for enhancements to evaluate models' abilities in fine-grained cross-modal semantic matching. Additionally, Chen et al. [12] introduce augmented benchmarks, MS-COCO-FG and Flickr30K-FG, which we incorporate into our research. Our evaluation involves measuring the recall at 1 for both image-to-text (i2t) and text-to-image (t2i) retrieval tasks.\nIn Experiment 2, we investigate the impact of dataset granularity on ITR task performance (RQ2). We compare the performance of vision-language models on standard image-caption datasets (MS-COCO and Flickr30k) and their more fine-grained counterparts (MS-COCO-FG and Flickr30k-FG). We evaluate the models on i2t and t2i tasks.\nIn Experiment 3, we further evaluate the effectiveness of our proposed evaluation framework and analyze the performance of state-of-the-art vision-language models on the ITR task (RQ3). We conduct experiments where perturbations are applied to the four selected datasets. We examine the performance drop of models after perturbation and assess the robustness of models to changes in the input data."}, {"title": "5.3 Results", "content": "Experiment 1: Refined Text Impact Evaluation. To address RQ1, we evaluate models R@1 performance for both i2t and t2i retrieval and compare the results obtained with original (MS-COCO and Flicker30k) and refined (MS-COCO-FG and Flickr30k-FG) captions. The main findings from Table 2 underscore the impact of adding refined texts on the performance of ITR, with overall improvements in R@1 scores observed across datasets. The exceptions"}, {"title": "5.4 Model Input Analysis and Jaccard Similarity", "content": "We further investigate the robustness of the models under different types of caption perturbations. For each model, we collect a sets of collect perturbed captions and their corresponding rsums. We categorize all the perturbed captions into three groups based on their impact on model performance: (i) perturbed captions causing performance decrease, (ii) perturbed captions causing performance increase, and (iii) perturbed captions with no change in performance. We proceed by calculating Jaccard similarity for each category (increased, decreased, no change) across all image-caption pairs within a dataset. This analysis helps identify patterns in how each model's performance is affected by different perturbations. The results are shown in Table 5. The highest Jaccard similarity"}, {"title": "6 RELATED WORK", "content": "Cross-Modal Retrieval. Cross-modal retrieval (CMR) methods create a multimodal representation space, where the similarity of concepts from different modalities can be measured using a distance metric such as cosine or Euclidean distance. Some of the earliest approaches in CMR used canonical correlation analysis [22, 34]. This was later followed by the emergence of a dual encoder architecture that combined recurrent and convolutional components, gaining prominence in the field and often employing a hinge loss [20, 74]. Further advancements have increased effectiveness through techniques like hard-negative mining [18]. Subsequently, the incorporation of attention mechanisms, such as dual attention [54], stacked cross-attention [37], and bidirectional focal attention [49], further improved performance. Other work aims to improve CMR performance through modality-specific graphs [73], or image and text generation modules [23], or learning sparse multimodal representations [55]. And there is domain-specific research focusing on CMR in various fields such as fashion [21, 36], e-commerce [25], cultural heritage [63], and cooking [73].\nRecent methods use transformer-based dual encoders trained on extensive data. ALBEF [41] aligns unimodal representations before fusion, X-VLM [89] adds a cross-modal encoder for fine-grained VL representations. Florence [86] uses adaptation models for object-level representations, and CLIP [61] predicts image-caption pairs. ALIGN [41] uses a dual encoder on image alt-text pairs. FILIP [84] features late multimodal interaction, and SLIP [53] combines language and image self-supervision. DeCLIP [47] improves CLIP pretraining via self-supervision and cross-modal supervision. AltCLIP [15] uses a pre-trained multilingual text encoder and a two-stage training schema. GroupViT [80] reintroduces the grouping mechanism to vision transformers, dynamically forming visual segments for various images.\nAnother line of work adopts transformer encoders [69] for the ITR task [52], adapting models like BERT [17]. ViLBERT [50] and LXMERT [67] introduce a two-stream architecture, while B2T2 [2], VisualBERT [44], Unicoder-VL [39], VL-BERT [66], and UNITER [14] propose single-stream architectures. Oscar [45] incorporates caption object tags with region features, and BEIT-3 [75] adapts multiway transformers. This work focuses on transformer-based dual encoder models due to their performance on various VL tasks. We select four SOTA methods and provide a comparative analysis of their performance on the ITR task.\nVision-Language Model Evaluation. The evaluation of VLMs assesses their performance across various tasks and datasets. Standard benchmarks are MS-COCO [13, 48] and Flickr30k [85] for tasks like"}, {"title": "7 CONCLUSIONS", "content": "In this work, we focus on addressing the problem of brittleness of evaluation pipeline of the ITR task. We highlight two main concerns: the granularity of existing benchmarks and the limitations of current evaluation metrics. We investigate the granularity of existing benchmarks, MS-COCO and Flickr30k, by comparing them with their augmented counterparts, MS-COCO-FG and Flickr30k-FG. We propose an evaluation framework that comprises a taxonomy of perturbations and an evaluation metric. For the experiments, we select four SOTA VLMs, AltCLIP, ALIGN, CLIP, and GroupViT. We conduct three experiments. First, we focus on the reproducibility assessment of results obtained by integrating refined texts into"}]}