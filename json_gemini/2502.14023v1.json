{"title": "Dynamic Activation with Knowledge Distillation\nfor Energy-Efficient Spiking NN Ensembles", "authors": ["Orestis Konstantaropoulos", "Theodoris Mallios", "Maria Papadopouli"], "abstract": "Abstract-While foundation AI models excel at tasks like\nclassification and decision-making, their high energy consump-\ntion makes them unsuitable for energy-constrained applications.\nInspired by the brain's efficiency, spiking neural networks (SNNs)\nhave emerged as a viable alternative due to their event-driven\nnature and compatibility with neuromorphic chips. This work\nintroduces a novel system that combines knowledge distillation\nand ensemble learning to bridge the performance gap between\nartificial neural networks (ANNs) and SNNs. A foundation AI\nmodel acts as a teacher network, guiding smaller student SNNs\norganized into an ensemble, called Spiking Neural Ensemble\n(SNE). SNE enables the disentanglement of the teacher's knowl-\nedge, allowing each student to specialize in predicting a distinct\naspect of it, while processing the same input. The core innovation\nof SNE is the adaptive activation of a subset of SNN models of\nan ensemble, leveraging knowledge-distillation, enhanced with an\ninformed-partitioning (disentanglement) of the teacher's feature\nspace. By dynamically activating only a subset of these student\nSNNs, the system balances accuracy and energy efficiency,\nachieving substantial energy savings with minimal accuracy loss.\nMoreover, SNE is significantly more efficient than the teacher\nnetwork, reducing computational requirements by up to 20x with\nonly a 2% drop in accuracy on the CIFAR-10 dataset. This\ndisentanglement procedure achieves an accuracy improvement\nof up to 2.4% on the CIFAR-10 dataset compared to other\npartitioning schemes. Finally, we comparatively analyze SNE\nperformance under noisy conditions, demonstrating enhanced\nrobustness compared to its ANN teacher. In summary, SNE offers\na promising new direction for energy-constrained applications.\nIndex Terms-Spiking Neural Networks, Dynamic Neural Net-\nworks, Convolutional Neural Networks.", "sections": [{"title": "I. INTRODUCTION", "content": "Foundation AI is repeatedly breaking ground in computer\nvision and machine learning [1], [2], with advancements at\ndramatic speed across various domains, including image and\nvideo classification, semantic segmentation, depth estimation,\nimage captioning, and decision-making. However, training\nand deploying foundation AI models demands extraordinary\namounts of energy and data [3]. Even with efforts to reduce\nmodel sizes, inference still requires hundreds of Watts, making\nthese models impractical for deployment at the edge, where\nenergy efficiency is crucial. On the other hand, the human\nbrain is remarkably energy-efficient, consuming about 20 watts\nof power, impressive given its computational capabilities,\ninvolving billions of neurons firing in complex patterns to\nprocess sensory input, control movement, and enable cogni-\ntion. Spiking neural networks (SNNs), inspired by biologi-\ncal neuronal networks, provide a promising alternative for\nachieving energy-efficient intelligence [4]. They use binary\nspiking signals (0 for no activity and 1 for a spiking event) to\nfacilitate communication between neurons. SNNs can operate\nefficiently on neuromorphic chips by performing spike-based\naccumulate (AC) operations, eliminating the need to process\nzero values in inputs or activations (i.e., they are event-driven).\nThis enables SNNs to consume significantly less power com-\npared to artificial neural networks (ANNs), which rely on\nenergy-intensive multiply-and-accumulate (MAC) operations\ntypically performed on dense computing hardware like GPUs.\nWith the advent of neuromorphic chips [5] the integration of\nneuromorphic processors into everyday devices is becoming\nincreasingly plausible.\nIn our research, we envision a system that harnesses the\ntransferability and generalization capabilities of state-of-the-art\nAI foundation models to enhance neuromorphic architectures\nthat combine energy efficiency with high accuracy. To achieve\nthis, we adopt a knowledge distillation approach, where the\nknowledge from foundation AI models is transferred to neu-\nromorphic architectures. In this framework, large-scale foun-\ndation AI architectures serve as teachers that guide smaller,\nneuromorphic, or more broadly neuroscience-inspired, student\narchitectures enabling them to emulate teacher's performance.\nSpecifically, we propose an innovative neuromorphic archi-\ntecture trained using knowledge-distillation: a powerful ANN\nacts as a single-teacher model distilling its knowledge to\nan ensemble of small student SNNs, called Spiking Neural\nEnsemble (SNE). This combination of knowledge distillation\nand ensemble learning can significantly improve the energy\nefficiency and accuracy of SNNs, aiming to reduce the perfor-\nmance gap between ANNs and SNNs. Our approach leverages\nthe Single-Teacher, Multiple-Student paradigm, where each\nstudent learns to mimic a distinct subset of features from the\nteacher network. Specifically, the teacher's feature space-the\nfinal layer just before the classification head-is partitioned\ninto distinct subsets, with each student responsible for repli-\ncating one of them.\nEach student processes the same input image, generating a\nfeature vector. These feature vectors are concatenated to form"}, {"title": "II. BACKGROUND", "content": "Training SNNs is challenging due to the non-differentiable\nnature of spikes. Current mainstream approaches to SNN\ntraining can be classified into two broad categories, namely\nANN-to-SNN conversion and direct SNN training, described\nbelow. A common approach for training SNNs is to leverage\nthe well-established techniques used in ANNs by converting\na high-performing ANN into an equivalent SNN with similar\naccuracy [6, 7, 8, 9, 10, 11, 12]. This process typically involves\nreplacing the ANN's ReLU activation functions with spiking\nneurons and fine-tuning the SNN to approximate the ANN's\noutputs. Although this approach can achieve high accuracy, it\noften requires a large number of time steps for the SNN to\nreplicate the ANN's outputs effectively [7, 9, 10]. This reliance\non extended simulation time limits its applicability in energy-\nconstrained environments or latency-critical applications, such\nas real-time inference. The direct SNN training using surrogate\ngradients (SG) [13, 14, 15, 16, 17, 18, 19] addresses the non-\ndifferentiability of spikes by approximating the gradient of the\nspiking function with a smooth surrogate function during the\nbackward pass. Combined with backpropagation through time\n(BPTT), it enables weight updates across multiple time steps.\nWhile direct training provides more freedom in the design\nof SNNs, it often leads to worse performance compared to\nANNs, due to the intrinsic complexity of spiking dynamics\nand limitations of surrogate approximations."}, {"title": "III. SINGLE-TEACHER MULTIPLE-STUDENT SNN\nENSEMBLE (SNE)", "content": "We introduce a novel training framework that combines re-\ncent advancements in Spiking Neural Network (SNN) training\nwith the knowledge distillation paradigm, employing a single-\nteacher model and multiple student models. The teacher model\nis a standard deep convolutional neural network (CNN), while\nthe student models are SNNs, with shallower architectures\nthan the teacher, trained to mimic the feature representations\ngenerated by the teacher. Each student learns a distinct subset\nof the features produced by the teacher's final layer, just before\nthe classification head. The partitioning of the feature set,\nand thus its allocation to students, could be performed using\ndifferent techniques, as discussed in Section III-C.\nThe features extracted by the student SNNs are then con-\ncatenated and fed into a classification head for downstream\ntasks. This ensemble-based approach achieves comparable\nperformance to a single-student SNN with the same total\ncomputational cost. Furthermore, the method offers flexibility\nin balancing computational cost and model performance by\nselecting the number and depth of the used student models."}, {"title": "A. Knowledge Distillation for SNN Models", "content": "The neurons of the student SNNs follow the Leaky Integrate\nand Fire (LIF) model [28, 29]. Each neuron has a state\ndivided into three processes, namely, charging, discharging,\nand resetting. The charging process can be expressed as in Eq.\n1, where $V[t]$ denotes the membrane potential of the neuron"}, {"title": "B. Student Ensemble", "content": "Rather than distilling knowledge from the teacher model\ninto a single-student network, our proposed method employs\nan ensemble of student models. Each student is assigned the\ntask of learning a specific subset of the teacher's feature vector,\nwhich is partitioned into N distinct (non-overlapping) sub-\nvectors, where N corresponds to the number of students. For\ninstance, in the case of two students and a feature vector of\nsize 100, the first student processes features corresponding to\nindexes [1, 50], while the second processes indexes [51, 100].\nThe teacher's feature space $S = {1, .., D}$ is partitioned\ninto N distinct subsets $S_i$ and each subset is assigned to a\ndifferent student, in an ensemble of N students. We will name\nthe partitioning of random equal-sized subsets fixed (with no\nfeature disentanglement).\nDuring the inference phase, the teacher generates feature\nvector v and each student generates a predicted feature sub-\nvector $s_i$. The Mean Square Error (MSE) is calculated between\nthese predicted sub-vectors and their corresponding teacher\nsub-vectors $v_i = {v_k : k \\in S_i}$. The outputs of all students\nare subsequently concatenated to form a comprehensive repre-\nsentation, which is then input into a linear classification head\nCH to produce the final prediction: $s = s_1 s_2 ... s_N$;"}, {"title": "C. Computation-Performance Trade-off Using Dropout", "content": "The student ensemble offers the advantage of dynamically\nadjusting the number of students active depending on the\nneed for more accuracy or less energy consumption. In order\nto utilize the flexibility of the ensemble we propose two\nmethodologies.\nThe first methodology utilizes the complete ensemble\nthroughout the training process. During the evaluation phase,\nwe employ a stochastic selection mechanism that operates\nby randomly sampling K indexes from the range [1, N]\nwithout replacement, where N represents the total number\nof student models in the ensemble and K the target number\nof active models. These sampled indexes determine which\nstudent models will be activated for the current batch, while\nthe remaining N-K models remain dormant. The input is\nfed only into the selected students, generating their respec-\ntive output features, while zero vectors are assigned to the\noutput feature spaces of the inactive models. This dynamic\nactivation pattern is regenerated for each subsequent batch,\nensuring a diverse mix of active student models throughout\nthe evaluation process. This stochastic approach provides a key\nadvantage: by randomly selecting subsets from the full-range\nof possible student-model combinations, it generally produces\nperformance levels that approximate the mean of all possible\ncombinations."}, {"title": "D. Performance Under Noise", "content": "We examine the performance of the SNE in the presence\nof noise added in the input dataset, during the testing phase."}]}