{"title": "LEARNING IN LOG-DOMAIN: SUBTHRESHOLD ANALOG AI ACCELERATOR BASED ON STOCHASTIC GRADIENT DESCENT", "authors": ["Momen K Tageldeen", "Yacine Belgaid", "Vivek Mohan", "Zhou Wang", "Emmanuel M Drakakis"], "abstract": "The rapid proliferation of AI models, coupled with growing demand for edge deployment, necessitates the development of AI hardware that is both high-performance and energy-efficient. In this paper, we propose a novel analog accelerator architecture designed for AI/ML training workloads using stochastic gradient descent with L2 regularization (SGDr). The architecture leverages log-domain circuits in subthreshold MOS and incorporates volatile memory. We establish a mathematical framework for solving SGDr in the continuous time domain and detail the mapping of SGDr learning equations to log-domain circuits. By operating in the analog domain and utilizing weak inversion, the proposed design achieves significant reductions in transistor area and power consumption compared to digital implementations. Experimental results demonstrate that the architecture closely approximates ideal behavior, with a mean square error below 0.87% and precision as low as 8 bits. Furthermore, the architecture supports a wide range of hyperparameters. This work paves the way for energy-efficient analog AI hardware with on-chip training capabilities.", "sections": [{"title": "1 Introduction", "content": "In recent years, artificial intelligence (AI) has become an integral part of daily life, serving as a transformative tool across various professional domains [1] and driving personal applications through advancements in transformer models that power large language models (LLMs) [2]. However, both training and inference of AI models demand substantial computational and energy resources, which are becoming increasingly challenging to access [3, 4]. While server-class GPUs are effective for training, their energy inefficiency [5] and high costs present significant barriers [6]. Additionally, the environmental impact of energy-intensive AI systems has raised critical concerns about their role in exacerbating climate change [4].\nAmdahl's law predicts that performance and efficiency gains are best achieved through innovative application-specific accelerator architectures rather than scaling up multi-core general-purpose processors [7]. Consequently, application- specific integrated circuits (ASICs), both digital and analog, have emerged as critical solutions for enabling high- efficiency training and inference of artificial neural networks [7, 8, 9].\nDigital accelerators are widely adopted for training workloads. Notable examples include the Brainwave Neural Processing Unit (NPU) [10], Google's Tensor Processing Unit (TPU) [11], and low-precision inference accelerators such as YodaNN [5], the Unified Neural Processing Unit (UNPU) [12], and BRein Memory [13]. These accelerators leverage specialized architectures to optimize the execution of artificial neural networks, achieving significant performance gains by reducing computational overheads and improving throughput. For instance, the first-generation TPU achieves a peak throughput of 92 tera-operations per second (TOPS) with a thermal design power of 28\u201340 watts [11].\nHowever, despite their energy efficiency at the architectural level, digital accelerators still consume considerable amounts of power due to their reliance on high-speed clocking, frequent memory access, and large-scale digital circuit implementations. For example, training a large language model like GPT-3, which has 175 billion parameters, has been estimated to consume approximately 1,287 megawatt-hours (MWh) of electricity [14]. These high energy demands raise"}, {"title": "2 Background", "content": "concerns about their environmental impact and make server-class digital accelerators unsuitable for energy-constrained environments, such as edge devices [6, 4].\nAnalog accelerators, which enable in-memory computation, address the power consumption challenges of digital accelerators by offering significant energy efficiency gains [7]. These accelerators, operating in the analog domain, perform operations directly within memory, reducing the need for data movement and associated energy costs. However, many existing analog accelerators are built on emerging memristive devices, such as Resistive RAM (ReRAM) and Phase Change Memory (PCM), which introduce challenges due to their non-idealities and non-linear characteristics. These issues make them difficult to model, characterize, and integrate into robust systems [7]. Hybrid approaches, such as the Programmable Ultra-efficient Memristor-based Accelerator (PUMA), have demonstrated superior performance for inference tasks, but they remain limited to inference and are not yet suitable for training workloads [15].\nThis paper presents a novel analog AI accelerator for training machine learning models, leveraging ultra-low power subthreshold MOS circuits. The proposed accelerator supports stochastic gradient descent with L2 regularization (SGDr) by formulating its computations in continuous time (SGDr-CT). Unlike conventional digital accelerators that require frequent memory access during training, our design uses volatile memory, requiring memory access only at the conclusion of training. Additionally, unlike analog accelerators based on emerging memristive devices, the proposed design utilizes mature CMOS technology. The performance of the accelerator was simulated and validated using the AMS 0.35 \u00b5m process, demonstrating its potential for efficient and scalable on-device training."}, {"title": "3 Theory", "content": "This section introduces the mathematical formulation for solving stochastic gradient descent with L2 regularization (SGDr) in continuous time, establishing the theoretical foundation and guiding principles of our proposed analog accelerator. The SGDr can be expressed as the following discrete difference equation:\n$w[n + 1] = w[n] \u2013 \u03b1\\delta[\u03b7]x[n] \u2013 \u03b1\\lambda w[\u03b7]$,\nwhere n denotes the discrete time step, w represents the weight, x is the input feature, $\\delta[n]$ is the error term, a is the learning rate, and $\\lambda$ is the regularization coefficient. Let m denote the number of samples in the dataset; thus, one epoch is completed after processing m samples.\nTo approximate the difference equation (5) using a continuous differential equation, we represent x[n] as a step function x(t), defined as:\n$x(t) = \\sum_{n=0}^{\\infty} x[n] [u(t \u2212 (n + 1)\\Delta s) \u2013 u(t \u2013 n\\Delta s)]$\nwhere $\\Delta s$ denotes the hold time of each sample, and u(t) is the unit step function. Figure 2 illustrates the relationship between the discrete signal x[n] and its continuous counterpart x(t). To model the behavior of a practical digital-to- analog converter (DAC), the rise time of each step is set to %0.5 of $\\Delta s$ for all results presented in this paper."}, {"title": "4 Analog Circuits Realisation", "content": "This section details the architecture and implementation of the proposed analog accelerator, which is based on well- characterized MOS log-domain circuits. The accelerator leverages the continuous-time stochastic gradient descent with L2-regularization (SGDr-CT) algorithm, described earlier, to enable efficient training of machine learning models in the continuous time domain.\nThe architecture of the proposed accelerator is illustrated in Figure 3. Similar to other analog accelerators in the literature, the design employs a crossbar array as its primary computational block. The crossbar is composed of weight-learning nodes, where each node is responsible for tuning or updating, in memory, a single weight in the neural network model being trained. The crossbar depicted in the figure represents a layer k, which contains $r_k$ inputs connected to $r_{k+1}$ outputs via the crossbar of the weight-learning nodes. These outputs subsequently serve as the inputs for the next layer (k + 1). Nodes in the same row are associated with the same feature (input), while nodes in the same column correspond to the same output. In smaller models, only a subset of these nodes is utilized; for example, a simple linear regression model may use only a single column.\nIn addition to the crossbar, the accelerator integrates analog blocks to realize the activation function (f) and its gradients (f'), both of which are essential for forward pass and back-propagation during training. The implementation of these blocks is particularly straightforward for the ReLU activation function, as detailed in section 6. Figure Figure 3 highlights the datapath used for the forward pass and backward pass, respectively, highlighting the seamless integration of operations within the same structure.\nAs shown in Figure 3, the core of the architecture is the weight-learning node, which constitutes the fundamental building block of the crossbar. To better understand its functionality, we present its transistor-level implementation. The weight-learning node consists of two identical unidirectional cells that implement the positive (11) and negative (12) learning equations, respectively. These cells, referred to as the positive learning cell and negative learning cell, are illustrated in Figure 3.\nSince the two cells are identical in design and operation, we focus our discussion on the positive learning cell. The mathematical representation of the positive learning cell is expressed as:\n$\\dot{I_w^+(t)} + \\frac{\u03b1}{nCV_T}I_w^+(t) = \\frac{I_q}{I_u} \\frac{I_{\\delta}(t) I_x(t)}{I_u}$,\nwhere $\\dot{I_w^+(t)}$ is the time derivative of the positive weight current, u is the unit constant, n is the subthreshold slope factor, C is the capacitance, $V_T$ is the thermal voltage, $I_q$ is the reference current, $I_{\\delta}(t)$ is the negative error current, $I_x(t)$ is the input current, and $I_{\\bar{u}}$ is the normalizing current. The full derivation of this equation is provided in Appendix A.2 and relies on the Log-Domain State-Space approach and the Bernoulli Cell (BC) proposed in [16].\nTo realize the dynamics of the positive learning equation (11), the mathematical variables are mapped into electrical currents within the circuit, as detailed in Table 1. The bidirectional weight w(t), mapped to $I_w(t)$, is computed as the difference between the currents $I_w^+(t)$ and $I_w^-(t)$, generated by the positive and negative learning cells, respectively.\nNotably, the mapping relationship between the hyperparameters (e.g., $\\Delta s$ and $\\lambda$) the circuit parameters exhibits a one-to-many cardinality. For instance, a desired learning rate (a) can be obtained by adjusting the values of $I_q$ and/or $\\Delta s$. Similarly, the regularisation coefficient ($\\lambda$) can be tuned by adjusting $I_q$ and/or u. The primary advantage of this cardinality is that it enables using the same learning rate (a) with smaller capacitance C, and hence smaller circuit footprint, simply by scaling down $\\Delta s$.\nIn addition to the learning cells, the weight-learning node includes two multipliers, both implemented using translinear log-domain circuits. The first multiplier computes the product of the weight and input currents during the forward pass, while the second multiplier second multiplier is employed during back-propagation to compute the product of the error and the weight currents. Finally, to support differential signaling of the error $\\delta(t)$ in back-propagation, the accelerator incorporates a geometric mean splitter (GMS) [17], which generates the differential current-mode error signals ($I_{\\delta}^+(t)$ and $I_{\\delta}^-(t)$).\nIn summary, similar to other analog accelerators in the literature, the proposed architecture employs a crossbar as its primary computational block. However, unlike traditional designs that utilize memristive elements, our approach introduces a novel weight-learning node based on volatile memory and translinear MOS circuits. This design leverages the energy efficiency of analog computation while utilizing well-characterized and mature circuit elements, ensuring both reliability and scalability."}, {"title": "5 Results", "content": "In this section, the performance of the proposed analog accelerator is evaluated through two distinct experiments. First, we compare the accuracy of the circuit implementation (SGDr-CT) against the ideal SGDr algorithm on single-feature datasets. Second, we assess the ability of the circuit to fit a regression model, demonstrating its effectiveness in practical applications.\nFor all experiments presented in this section, the SGDr-CT circuit was implemented in AMS 0.35 \u00b5m CMOS technology and simulated using Spectre in Cadence Virtuoso. To provide a baseline for comparison, an ideal model implementing the SGDr algorithm was developed in Python."}, {"title": "5.1 Single Neuron", "content": "We evaluated our proposed analog AI accelerator architecture using five univariate datasets. Each dataset consisted of a randomly generated feature vector and weight pairings, sampled from a uniform distribution over (\u22121, 1). Gaussian noise, scaled by 0.1, was added to simulate realistic data. The distributions of the datasets are shown in Figure 4.\nBoth the circuit implementation (SGDr-CT) and the ideal mathematical implementation were used to fit the datasets. The regularization coefficient ($\\lambda$) and learning rate (a) were set to 1 \u00d7 10-3 and 0.1, respectively, while the sampling time ($\\Delta s$) was set to 0.01 milliseconds. Table 2 provides the nominal values of the circuit parameters used to map the hyperparameters.\nFigure 4 compares the results, presenting the mean squared error (MSE) loss curve and the estimated weight over 200 epochs. The curves from the circuit and ideal model appear nearly identical, with maximum absolute errors of 0.71% for the estimated weights and 0.87% for the MSE values. Table 3 provides a more detailed comparison of the results for both methods.\nNext, we examine the accuracy of the circuit in supporting a wide range of hyperparameter values ($\\lambda$ and a). Starting with the previous nominal values, the learning rate was set to a to 1e\u22122, 1e-3, and 1e-4 by varying $\\Delta s$. Simulations were repeated for $\\lambda$ values of 0.2, 0.1, and 0.05. Figure 5 illustrates the results.\nFor $\\lambda$ = 0.2 and $\\lambda$ = 0.05, slight deviations were observed between the circuit and the ideal behavior. These deviations were corrected by tuning the mapped currents in the circuit to 0.27 and 0.03, respectively. The deviations occurred"}, {"title": "5.2 Model Fitting", "content": "because the transistor dimensions were optimized for $\\lambda$ = 0.1. This tuning process aligns with the standard practice of hyperparameter tuning in the machine learning workflow, and further discussion is provided in Section 6.\nIn summary, these results demonstrate that the performance of the circuit implementation of SGDr-CT closely matches that of the ideal SGDr model. The negligible differences confirm the accuracy and reliability of the proposed analog accelerator for stochastic gradient descent-based training.\nThe proposed analog AI accelerator was evaluated by training a linear regression model on the Boston Housing dataset [18], a widely utilized benchmark for assessing machine learning algorithms [21, 20]. The dataset comprises 506 samples, with 13 features representing various socioeconomic and geographical factors, used to predict the median home value in neighborhoods of Boston, Massachusetts. The Boston Housing dataset has been critiqued for embedding biases, particularly through the inclusion of a variable engineered under the assumption that racial self-segregation positively impacts house prices [19, 20]. While this dataset was chosen for historical benchmarking purposes [21, 20], future work will explore alternative datasets that promote fairness in AI model evaluation.\nPrior to training, the dataset was preprocessed to ensure uniform scaling and improve model training. Features were normalized using max normalization, and their means were shifted to 0.8 through the application of appropriate offsets. The dataset was subsequently divided into training and testing subsets, with 80% of the data allocated for training and the remaining 20% reserved for testing.\nFigure 6 illustrates the predicted weights obtained from the circuit implementation and the ideal mathematical model. The figure also presents the training and test loss curves. Overall, the predictions from the circuit closely align with those of the ideal model. A quantitative summary of the results at the final epoch is provided in Table 4. As shown, the maximum error, expressed as the absolute difference between the circuit and ideal predictions, is 0.527%. This error corresponds to an 8-bit resolution for weight values with a full-scale range of [-1,1]. The accuracy in terms of the number of bits is determined using the following equation:\n$\\#Bits = - log_2(\\frac{Max_{error}}{Full\\ scale})$"}, {"title": "6 Discussion and Future Work", "content": "Notably, the training and test loss curves align more closely with the ideal behavior compared to the parameter curves, with percentage errors remaining below 1%. This observation suggests that the accelerator effectively compensates for circuit non-idealities, converging to a locally optimal solution with a similar loss to the ideal model, albeit with slight differences in the parameter values.\nThis paper has focused on validating the core building block of the proposed analog accelerator: the weight-learning node. While the results demonstrate the feasibility and accuracy of the weight update mechanism, several design considerations remain critical for the complete realization of the accelerator. These considerations are discussed below."}, {"title": "6.1 Inclusion of Bias Term", "content": "The proposed design effectively implements the weight update rule; however, the realization of the bias term has not been addressed in this work. The inclusion of a bias term is essential for training machine learning models, as it enhances their ability to fit data accurately by shifting the decision boundary.\nThe continuous-time differential equation governing the update of the bias term is expressed as:\n$\\dot{w_0(t)} = \\frac{\u03b1}{\\Delta s} \\delta(t)$,\nwhere $w_0(t)$ denotes the bias term, a is the learning rate, $\\Delta s$ is the sampling time, and $\\delta(t)$ represents the error signal.\nThe update rule for the bias term, as described in (15), is straightforward and can be efficiently implemented using a Seevinck integrator. The Seevinck integrator [22, 23] is a fully differential log-domain integrator that aligns well with the analog nature of the proposed accelerator. Its capability to handle differential signals and its compatibility with log-domain circuits make it a suitable choice for realizing the bias term update.\nThe integration of the bias term into the accelerator design will enable a more comprehensive evaluation of the accelerator performance across diverse machine learning tasks, particularly those requiring complex decision boundaries. This enhancement represents a natural extension of the current work and will be a focus of future investigations."}, {"title": "6.2 Regularization Coefficient", "content": "The proposed accelerator was shown to support regularization coefficients ($\\lambda$) as small as 0.05. However, since $\\lambda$ is represented as the ratio of two currents (u/$I_q$), smaller values of $\\lambda$ values would result in transistors operating outside the weak inversion region.\nTo address this limitation, the translinear loop can be extended by stacking additional transistors, as illustrated in green in Figure 7. This modification ensures that the circuit operates entirely within the subthreshold region, even for very small $\\lambda$ values. The updated mathematical equation governing the extended circuit is expressed as:\n$\\dot{I_w^+(t)} + \\frac{\u03b1}{\\Pi_i I_{ri} nCV_T}I_w^+(t) = \\frac{\\Pi_j I_{li}}{\\Pi_i I_{ri}} \\frac{I_q}{I_u} \\frac{I_{\\delta}(t) I_x(t)}{I_u}$,\nwhere $I_{li}$ and $I_{ri}$ denote the left and right currents in the extended translinear loop, respectively.\nThe hyperparameters can be mapped to electrical currents as detailed in Table 5. To support regularization coefficients two orders of magnitude smaller (i.e., 0.05 \u00d7 e\u00af\u00b2), two additional transistors can be added to each side of the translinear loop, with the bias currents set as $I_{ro}$ = $I_{r1}$ = 10 \u00d7 $I_{to}$ = 10 \u00d7 $I_{l1}$."}, {"title": "6.3 ReLU Activation Function", "content": "The ReLU activation function is widely employed in AI/ML applications due to its simplicity and effectiveness. Conveniently, both the ReLU function and its derivative can be efficiently implemented in silicon using analog circuits. The ReLU function can be realized with a current mirror, where the diode-connected transistor inherently permits only positive values (unidirectional currents) to pass. Similarly, the derivative of the ReLU function can be effectively implemented using a current comparator based on the Winner-Take-All (WTA) topology [24]."}, {"title": "6.4 Process Nodes", "content": "As discussed earlier, a key advantage of the one-to-many mapping between hyperparameters and circuit parameters is the ability to scale the chip area by using smaller capacitance (C) through the reduction of $\\Delta s$. This not only decreases the circuit area but also increases throughput and reduces training time. Therefore, the proposed architecture scales effectively with faster process nodes. Future work should focus on realizing the accelerator using advanced fabrication technologies."}, {"title": "A Mathematical Proof", "content": ""}, {"title": "A.1 SGDr-CT Formulation", "content": "Proof. Consider the first-order ordinary differential equation:\n$\\dot{w(t)} + \\frac{\u03b1}{\\Delta s} \u03bb w(t) + \\frac{\u03b1}{\\Delta s} \\delta(t) x(t) = 0$\nTo derive the discrete-time formulation, integrate (17) with respect to t over the interval [n$\\Delta s$, n$\\Delta s$ + $\\Delta s$]:\n$\\int_{n\\Delta s}^{(n+1)\\Delta s} \\dot{w(t)} - w(n\\Delta s) = \\frac{\u03b1}{\\Delta s} \\int_{n\\Delta s}^{(n+1)\\Delta s} [\\lambda w(t) + \\delta(t) x(t)] dt$\nAssuming that $\\delta(t)$ and w(t) vary minimally over the interval $\\Delta s$, and applying Euler's method, we approximate:\n$\\frac{\u03b1}{\\Delta s} w((n + 1)\\Delta s) \u2248 w(n\\Delta s) - \\Delta s [\\lambda w(n \\Delta s) + \\delta(n \\Delta s) x(n \\Delta s)]$\n$w((n+1)\\Delta s) = w(n\\Delta s) - \u03b1 [\u03bb w(n \\Delta s) + \u03b4(n \\Delta s) x(n \\Delta s)]$\nSince x(n$\\Delta s$) corresponds to the discrete-time variable x[n] (as illustrated in Figure 2), (20) can be rewritten in its discrete-time form as the stochastic gradient descent (SGD) difference equation:\n$w[n + 1] = w[n] \u2013 \u03b1\u03b4[\u03b7]x[n] \u2013 \u03b1\u03bb\u03c9[\u03b7]$\nThis establishes the equivalence between the continuous-time SGDr-CT formulation and the discrete-time SGDr difference equation."}, {"title": "A.2 Positive Learning Cell Expression", "content": "Proof. We begin by analyzing the Bernoulli Cell (BC) in the positive learning cell, followed by an examination of its translinear loop.\nThe drain current of the NMOS transistor within the Bernoulli Cell (BC) of the positive learning cell can be expressed as follows:"}]}