{"title": "Value Internalization: Learning and Generalizing from Social Reward", "authors": ["Frieda Rong", "Max Kleiman-Weiner"], "abstract": "Social rewards shape human behavior. During development, a caregiver guides a learner's behavior towards culturally aligned goals and values. How do these behaviors persist and generalize when the caregiver is no longer present, and the learner must continue autonomously? Here, we propose a model of value internalization where social feedback trains an internal social reward (ISR) model that generates internal rewards when social rewards are unavailable. Through empirical simulations, we show that an ISR model prevents agents from unlearning socialized behaviors and enables generalization in out-of-distribution tasks. We characterize the implications of incomplete internalization, akin to \"reward hacking\" on the ISR. Additionally, we show that our model internalizes prosocial behavior in a multi-agent environment. Our work provides a foundation for understanding how humans acquire and generalize values and offers insights for aligning AI with human values.", "sections": [{"title": "1 Introduction", "content": "Why do we want what we want? Some goals we pursue are responses to the extrinsic rewards and punishments of the environment. We pursue food when hungry, shelter when cold, and sleep when tired. Money can motivate us to work harder, and the threat of punishment can incentivize us to follow the law. Other goals are intrinsically self-motivated and do not require external reinforcement. We play and explore, feel a warm glow when altruistic, and may take pride in our work even when no one is watching (Andreoni, 1990; Ryan & Deci, 2000). Both extrinsic and intrinsic rewards have likely been shaped by natural selection to enable adaptive behavior across many environments (Singh et al., 2009). They have also played a key role in building reinforcement learning agents that can learn in an open-ended fashion across a lifetime of experiences and tasks without hand-crafting reward functions for each one (Singh et al., 2010; Schmidhuber, 2010; Mohamed & Jimenez Rezende, 2015; Kulkarni et al., 2016; Jaques et al., 2019). While some pursue a quest for a universal reward function that generates the full suite of human-like intelligent behavior (Silver et al., 2021), we aim to study how values might be acquired through social and cultural learning and then leveraged for open-ended autonomy.\nOur approach can explain some key challenges for understanding the source of values. First, although many aspects of desire are innate, and any acquisition process itself requires some degree of innate motivation and machinery, there must be a substantial role for learning in determining what humans find rewarding. Different cultures across time and space have varied substantially in terms of what people in those societies find rewarding (Henrich et al., 2001; 2006; Medvedev et al., 2024). In some places, spicy food can cause physical pain, while in others, food without spice is considered bland and tasteless (Billing & Sherman, 1998). Different individuals chase meaning and reward in different ways: maximizing money, power, artistic expression, knowledge, fame, the probability of reaching an afterlife, and many others (Maslow, 1958). Moral values, such as how different individuals trade off the welfare of different groups, vary as well; some might weight family members highly, while others strive for impartiality (Kleiman-Weiner et al., 2017; McManus et al., 2020). This logic applies"}, {"title": "1.1 Related Computational Work", "content": "Our work takes inspiration from reinforcement learning from human feedback (RLHF), a technique currently used to train agents and align models to judgments made by human annotators. Christiano et al. (2017) train a reward model from pairwise preference judgments of an agent's behavior and show that the reward model can be used to train a deep reinforcement learning agent on simple tasks. Tien et al. (2022) study generalization in reward models and show that reward modeling from pairwise judgment data often fails to generalize because the reward models can learn spurious correlations rather than capturing the underlying causal process. Similar to our work here, Colas et al. (2020) train a goal generator from the language of a social partner and show that this goal generator can imagine new goals to improve generalization and exploration. Finally, Kleiman-Weiner"}, {"title": "2 MDPs With Social Rewards", "content": "We study the process of value internalization in a two-agent Markov decision process (MDP) with a learner and a caregiver. In our setup, the caregiver only interacts with the learner by giving social rewards. Social reward is a single continuous number corresponding to the degree to which the feedback is intended to be rewarding (positive) or punishing (negative). Finally, in some trials, the caregiver is absent, so there is no social reward in those trials.\nFormally, an MDP with social rewards (MDP-SR) is a tuple $(S, A, T, \\gamma, R_e, P, R_s)$: a set of states $S$, a set of actions for each state $A(s)$, a transition function that maps states and actions to future states $T(s, a) \\rightarrow s'$, a discount factor $\\gamma\\in [0,1)$, an extrinsic reward function that maps actions and outcomes to environmentally given rewards $R_e(s,a,s') \\rightarrow R$. We extend these terms to account for social reward by augmenting the MDP with $P(s) \\in {0,1}$ that indicates whether the caregiver is present (1) or absent (0) and the social reward $R_s(s, a, s') \\rightarrow R$ which is available only when $P(s) = 1$. We assume that learners are socially motivated and have a utility function $U = R_e + R_s$ that integrates environmental and social rewards (Dweck, 2017). The learner aims to find a policy that maximizes expected cumulative discounted utility.\nOur experiments are divided into two phases. First is a socialization phase where the caregiver is present (p = 1). Second is an autonomous phase where the caregiver is absent (p = 0). Our framework allows for more complex dynamics (e.g., slowly reducing the probability of the caregiver's presence over time), but we use a simple two-phase approach to simplify the analyses. The MDP-SR framework enables us to ask questions about how computational learners will handle the transition between these two phases.\nWe developed a procedurally generated set of navigation tasks using the Minigrid Learning Environment (Chevalier-Boisvert et al., 2023). Figure 1 shows some examples. In each episode, we generate a 5x5 grid with the agent denoted as a red arrow that can face any of the cardinal directions, a green square, and three blocks that create obstacles for navigation. The green square, three blocks, starting position, and agent orientation are uniformly randomly sampled. The agent can turn 90 degrees in place or move forward one square. Going forward has a small negative cost, $R_e = \\frac{-0.9}{max(steps)}$, where $max(steps)$ is the maximum number of steps in an episode. There were 20 steps in each episode, so $R_e = -0.045$. This small cost incentivizes efficient action and is the only extrinsic reward in our setting. The discount rate $\\gamma = 0.99$. The grid and starting location are randomly resampled if the agent reaches the green square. During the first phase (socialization), when the caregiver is present, the caregiver provides a large reward ($R_s = 0.4$) when the agent reaches the green square."}, {"title": "3 Modeling Value Internalization", "content": "Our baseline agent is a deep reinforcement learner trained with PPO from Stable Baselines 3 (Schulman et al., 2017; Raffin et al., 2021). We use a learning rate of 1e-4 and otherwise use the default hyperparameters from Stable Baselines 3 for all models tested. The top of Figure 2 shows an abstracted version of the typical loop between the environment and the agent where the environment provides the state and an extrinsic reward, and the agent produces actions based on its learned policy.\nFigure 1 shows the performance of the baseline agent on our environment distribution. We contrast what happens when the caregiver is present (blue) versus when the caregiver leaves at the halfway point (green). When the caregiver remains, the agent steadily improves its performance until eventually plateauing near an optimal level. In contrast, when the caregiver leaves at the halfway point, performance rapidly drops to zero. This confirms our initial hypothesis: when the social rewards provided by the caregiver are the primary source of rewards that define the task, a typical reinforcement learner will not be able to continue learning and exploring autonomously when the caregiver is no longer present.\nWe hypothesize that human learners address this problem by internalizing the values of others. Here, we formalize this hypothesis by augmenting our baseline agent with an internalized social reward model (ISR) that learns to model the social rewards given by the caregiver and creates internal rewards (Ri) when the caregiver is absent (Figure 2). Thus we can write the full utility function of an agent with an ISR as $U = R_e + P \\cdot R_s + (1 - P) \\cdot R_i$. It receives a non-zero social reward Rs when the caregiver is present P = 1 and a non-zero internalized reward Ri when the caregiver is absent P = 0.\nThe ISR model is a deep neural network using the same architecture as the policy network. The network takes in the state and action and predicts reward. During the socialization phase, the agent stores the social rewards received, and those stored rewards are used to train the ISR model. The model was trained to minimize mean square error (MSE, $||R_s - R_i||$) since rewards are continuous. Finally, since the distribution of social rewards is imbalanced - positive rewards are more sparse than zero rewards - rewards were sampled such that each training batch had a balanced sample of reward magnitudes.\nWhen the task or distribution of tasks changes, deep RL policies often fail to generalize (Kansky et al., 2017). This failure results partly from the challenge of needing to predict an entire sequence of actions that optimize the expected cumulative discounted rewards. In contrast, the ISR module only needs to predict the reward for a particular action in a particular state without considering future actions. If the ISR module generalizes to new environments before the policy does, the agent could continue learning in those new environments even in the total absence of extrinsic reward. On"}, {"title": "4 Results", "content": "We first analyze the training of the ISR model. We then test whether a reinforcement learner augmented with ISR can solve the challenge posed in Figure 1 and analyze whether the ISR enables generalization by allowing for additional learning even without any social reward. Finally, we introduce a multi-agent scenario where the caregiver rewards altruistic behavior and show that our model extends to prosocial value internalization."}, {"title": "4.1 Training the ISR model", "content": "Figure 3 shows learning curves for the ISR. With sufficient data, the model achieves minimal test loss. The final test loss was an exponential function of the amount of social rewards observed, where each doubling of the number of rewards yielded an order of magnitude reduction in MSE loss."}, {"title": "4.2 Continual Learning and Generalization", "content": "We next test whether augmenting a reinforcement learner with the ISR module is sufficient to enable continual learning (Thrun, 1998; Hadsell et al., 2020). Figure 4 updates Figure 1 and shows how a model with internalized reward (shown in red) performs when the social rewards from the caregiver are removed. The model with ISR continues to do the task at the same rate as one that continues receiving social rewards. Thus, for this context, the ISR model fully internalized the social rewards of the caregiver. This enables the agent to continue autonomously without dependence on the caregiver's social rewards to maintain its behavior.\nThe right panel of Figure 4 shows a test of generalization. During the socialization period, when the caregiver was present, the agent was trained with just one block in the environment for 6K episodes. We then tested how well the agent could learn directly from the ISR in environments with five blocks (another 6K episodes). Performance was evaluated by calculating the total reward that would have been obtained if the caregiver was present in a held-out set of 100 episodes, i.e., a proxy for how well the caregiver's values have been internalized and generalized. This is labeled Reward (OOD) on Figure 4b. We compared ISR performance to a baseline (\"frozen\") and an upper bound (\"oracle\"). The frozen baseline corresponds to testing a model right after the one-block socialization period on the five-block test without additional learning (all weights are frozen)."}, {"title": "4.3 Internalization Failure: Reward Hacking", "content": "When value internalization is incomplete, problems can arise when the internalized rewards are prematurely optimized. Generalization failures in the ISR model will propagate into errors in the agent's policy during the autonomous period when the ISR model is the target for learning. To study this empirically, we undertrained the ISR model on 1/12th of the data as before. Figure 6 shows that while the agent correctly optimizes its internal reward from the ISR model, it is less likely to reach the caregiver's goal. This failure can be considered an instance of reward hacking:"}, {"title": "4.4 Internalization of Prosocial Values", "content": "Up to this point, our empirical investigation focused on a single agent operating alone in an environment. However, many of the most important culturally acquired values are interpersonal and relate to how we should treat others. We investigate this phenomenon in a procedurally generated set of two-player scenarios shown in Figure 7 inspired by Ullman et al. (2009). Instead of the red agent being socially rewarded when it reaches the green square, the caregiver rewards it when the green player reaches the green square. However, a blue boulder blocks the green arrow's path in each generated grid. The red agent has two additional actions \"pick up\" and \"drop\" which allow it to pick up and move the boulder, clearing the path. The state is also augmented to include a binary indicator of whether or not the player is carrying the boulder. If possible, the green agent always moves toward the goal using a depth-first search. If no path is found, it remains in place.\nTo create solvable tasks procedurally, we generated 5x5 grids with seven blocks subject to the constraint that the remaining open tiles form a single connected component, which results in a tree structure. The boulder is placed in the location with the maximum degree in that tree, and the green player and green goal are placed on opposite components of the resulting disconnected graph at the tree's leaf nodes (endpoints). The red agent is placed at another leaf node. Thus, in each starting configuration, the green player's path is blocked by the boulder, and the only way for that player to reach the goal is if the red agent picks up and moves the boulder away. The caregiver gives a social reward to the red agent when the green player reaches the green square and otherwise gives no reward. Thus, we can study how a prosocial reward that is dependent on the behavior of another is internalized by the ISR module.\nFigure 7 shows the results from this experiment. Overall, we observe similar phenomena to those seen in previous experiments. During the socialization period, the agent learns the task. When the caregiver leaves, the agent without ISR unlearns the behavior. However, with ISR, the agent continues helping the other agent in new environments, having internalized the prosocial value. This is reminiscent of the human feeling of a \"warm glow\" when behaving altruistically (Andreoni, 1990)."}, {"title": "5 Discussion", "content": "We develop a new computational cognitive model for studying how values can be socially acquired and maintained during learning. We proposed a process called value internalization, where, during a socialization period, a caregiver socially rewards a learning agent based on the correctness of their behavior. The learner models these rewards internally, and once the caregiver leaves and the learner must continue independently, the internal model of reward prevents unlearning the socially acquired behaviors and enables further learning and generalization. Together, these results shed light on some of the features and challenges of value acquisition. In the following, we discuss some implications that arise from this view and describe opportunities for future study on computational value internalization.\nHere, we only considered the simplest kind of social feedback, directly rewarding the desired outcome. However, human social feedback is far richer and often requires some computation on the side of the receiver to be interpreted correctly. For instance, when people teach with rewards and punishments, their actions have a communicative goal rather than just shaping a policy (Ho et al., 2017; 2019). In the prosocial environments shown in Figure 7, rather than giving a reward when the green agent reaches the green square, it might be more natural to give a positive reward when the red agent picks up the boulder and moves it out of the way. Once the boulder has been moved, the red agent no longer has a role to play as a helper, so it might make sense to deliver the reward then. However, without additional inferential machinery, the agent will learn that moving boulders is the goal rather than seeing moving the boulder as a means to an end. A sophisticated learning agent should learn to disambiguate between approval of instrumentally valuable actions and intrinsically valuable actions when interpreting an approval signal provided by a caregiver. Other forms of social feedback, such as observation, demonstrations, language, or corrections, may need their own inferential machinery to distill into an ISR model (Colas et al., 2020; Jeon et al., 2020; Kleiman-Weiner et al., 2020).\nWhy internalize social rewards instead of learning from scratch? Internalization is useful for any boundedly rational agent (including humans) that is unable to perform the (very) long-horizon planning required for survival in a complex world. Instead, a resource-rational strategy is to learn to intrinsically value the goals that have already been acquired by previous learners. These goals may end up somewhat decorrelated from the original environmental rewards, but may still lead to the acquisition of a wide variety of skills relevant to survival without the computational cost. Similar explanations have been offered for why it's adaptive for humans to internalize social norms (Gintis, 2003).\nOur computational approach to value internalization gives a novel view on a developmental question: when is an agent or organism ready to seek independence instead of further care? From the perspective of value internalization, the more time an agent spends with their caregiver, the more accurate their internal rewards will be (as we showed in Figure 3). If we assume that the caregiver's social rewards transmit a culturally evolved set of values, then accurately representing those values will be of benefit to the learner (Henrich, 2015). While a learner can only weakly estimate the benefit of a more accurate ISR model because of the uncertain future, an outer optimization loop of cultural evolution could at least estimate the average value of a given ISR accuracy (Sorg et al., 2010). Let B(n) be the benefit to a particular ISR and n be the amount of social feedback that the ISR module was trained with. Furthermore, providing feedback is costly to the caregiver and may delay the productivity of the learner during independence. Let C(n) be these costs, which are also a function of the amount of social rewards. Applying the logic of marginal utility, an agent is ready for independence when:\n$\\frac{\\partial B}{\\partial n} \\geq \\frac{\\partial C}{\\partial n}$\nor when the marginal benefit of improving the ISR is less than the marginal cost of the next additional social reward.\nWhile this work used a deep neural network to model social reward, the framework we presented applies more generally to a wide range of representations. More structured models, such as hierarchical Bayesian models or probabilistic programs, may be better suited to capture people's inductive biases when learning what kinds of states and actions are likely to be rewarding (Kleiman-Weiner et al., 2017). These inductive biases give up some flexibility for greater sample efficiency. However, from an evolutionary perspective, flexibility might be highly valuable the range of possible cultural values cannot be easily anticipated (e.g., non-intuitive complex rituals) over the time span of biological evolution, and it may be worth spending more time and energy in a socialization phase to allow for a wider range of possible values (Piantadosi & Kidd, 2016). For instance, what kind of ISR model could accurately recognize and reward values like curiosity and exploration? We hope to study this question empirically in future work.\nValue internalization may have implications for aligning artificial intelligence with human values. Today, large language models (LLMs) are made more helpful and ethical and less biased and harmful through a process called reinforcement learning from human feedback (RLHF) that shares a resemblance with the ISR model (Ouyang et al., 2022). Acting in a caregiver-like role, human annotators rate pairs of model outputs. Those ratings are used to train a reward model, which tunes the language model toward the preferences of the human annotators. In our work, we attempted to reverse engineer how human learners might internalize their caregiver's feedback, aligning their wants and desires to those of the previous generation. Now, we are faced with engineering these internalization mechanisms into AI agents in order to build safe, intelligent machines."}]}