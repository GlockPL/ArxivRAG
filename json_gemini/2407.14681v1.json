{"title": "Value Internalization: Learning and Generalizing from Social Reward", "authors": ["Frieda Rong", "Max Kleiman-Weiner"], "abstract": "Social rewards shape human behavior. During development, a caregiver guides a learner's behavior towards culturally aligned goals and values. How do these be- haviors persist and generalize when the caregiver is no longer present, and the learner must continue autonomously? Here, we propose a model of value inter- nalization where social feedback trains an internal social reward (ISR) model that generates internal rewards when social rewards are unavailable. Through empirical simulations, we show that an ISR model prevents agents from unlearning socialized behaviors and enables generalization in out-of-distribution tasks. We character- ize the implications of incomplete internalization, akin to \"reward hacking\" on the ISR. Additionally, we show that our model internalizes prosocial behavior in a multi- agent environment. Our work provides a foundation for understanding how humans acquire and generalize values and offers insights for aligning AI with human values.", "sections": [{"title": "1 Introduction", "content": "Why do we want what we want? Some goals we pursue are responses to the extrinsic rewards and punishments of the environment. We pursue food when hungry, shelter when cold, and sleep when tired. Money can motivate us to work harder, and the threat of punishment can incentivize us to follow the law. Other goals are intrinsically self-motivated and do not require external reinforcement. We play and explore, feel a warm glow when altruistic, and may take pride in our work even when no one is watching (Andreoni, 1990; Ryan & Deci, 2000). Both extrinsic and intrinsic rewards have likely been shaped by natural selection to enable adaptive behavior across many environments (Singh et al., 2009). They have also played a key role in building reinforcement learning agents that can learn in an open-ended fashion across a lifetime of experiences and tasks without hand-crafting reward functions for each one (Singh et al., 2010; Schmidhuber, 2010; Mohamed & Jimenez Rezende, 2015; Kulkarni et al., 2016; Jaques et al., 2019). While some pursue a quest for a universal reward function that generates the full suite of human-like intelligent behavior (Silver et al., 2021), we aim to study how values might be acquired through social and cultural learning and then leveraged for open-ended autonomy.\nOur approach can explain some key challenges for understanding the source of values. First, although many aspects of desire are innate, and any acquisition process itself requires some degree of innate motivation and machinery, there must be a substantial role for learning in determining what humans find rewarding. Different cultures across time and space have varied substantially in terms of what people in those societies find rewarding (Henrich et al., 2001; 2006; Medvedev et al., 2024). In some places, spicy food can cause physical pain, while in others, food without spice is considered bland and tasteless (Billing & Sherman, 1998). Different individuals chase meaning and reward in different ways: maximizing money, power, artistic expression, knowledge, fame, the probability of reaching an afterlife, and many others (Maslow, 1958). Moral values, such as how different individuals trade off the welfare of different groups, vary as well; some might weight family members highly, while others strive for impartiality (Kleiman-Weiner et al., 2017; McManus et al., 2020). This logic applies"}, {"title": "1.1 Related Computational Work", "content": "Our work takes inspiration from reinforcement learning from human feedback (RLHF), a technique currently used to train agents and align models to judgments made by human annotators. Christiano et al. (2017) train a reward model from pairwise preference judgments of an agent's behavior and show that the reward model can be used to train a deep reinforcement learning agent on simple tasks. Tien et al. (2022) study generalization in reward models and show that reward modeling from pairwise judgment data often fails to generalize because the reward models can learn spurious correlations rather than capturing the underlying causal process. Similar to our work here, Colas et al. (2020) train a goal generator from the language of a social partner and show that this goal generator can imagine new goals to improve generalization and exploration. Finally, Kleiman-Weiner"}, {"title": "2 MDPs With Social Rewards", "content": "We study the process of value internalization in a two-agent Markov decision process (MDP) with a learner and a caregiver. In our setup, the caregiver only interacts with the learner by giving social rewards. Social reward is a single continuous number corresponding to the degree to which the feedback is intended to be rewarding (positive) or punishing (negative). Finally, in some trials, the caregiver is absent, so there is no social reward in those trials.\nFormally, an MDP with social rewards (MDP-SR) is a tuple $(S, A, T, \\gamma, R_e, P, R_s)$: a set of states $S$, a set of actions for each state $A(s)$, a transition function that maps states and actions to future states $T(s, a) \\rightarrow s'$, a discount factor $\\gamma\\in [0,1)$, an extrinsic reward function that maps actions and outcomes to environmentally given rewards $R(s,a,s')_e \\rightarrow \\mathbb{R}$. We extend these terms to account for social reward by augmenting the MDP with $P(s) \\in \\{0,1\\}$ that indicates whether the caregiver is present (1) or absent (0) and the social reward $R_s(s, a, s') \\rightarrow \\mathbb{R}$ which is available only when $P(s) = 1$. We assume that learners are socially motivated and have a utility function $U = R_e + R_s$ that integrates environmental and social rewards (Dweck, 2017). The learner aims to find a policy that maximizes expected cumulative discounted utility.\nOur experiments are divided into two phases. First is a socialization phase where the caregiver is present (p = 1). Second is an autonomous phase where the caregiver is absent (p = 0). Our framework allows for more complex dynamics (e.g., slowly reducing the probability of the caregiver's presence over time), but we use a simple two-phase approach to simplify the analyses. The MDP-SR framework enables us to ask questions about how computational learners will handle the transition between these two phases.\nWe developed a procedurally generated set of navigation tasks using the Minigrid Learning Environ- ment (Chevalier-Boisvert et al., 2023). Figure 1 shows some examples. In each episode, we generate a 5x5 grid with the agent denoted as a red arrow that can face any of the cardinal directions, a green square, and three blocks that create obstacles for navigation. The green square, three blocks, starting position, and agent orientation are uniformly randomly sampled. The agent can turn 90 degrees in place or move forward one square. Going forward has a small negative cost, $R_e = \\frac{-0.9}{\\text{max(steps)}}$, where $\\text{max(steps)}$ is the maximum number of steps in an episode. There were 20 steps in each episode, so $R_e = -0.045$. This small cost incentivizes efficient action and is the only extrinsic reward in our setting. The discount rate $\\gamma = 0.99$. The grid and starting location are randomly resampled if the agent reaches the green square. During the first phase (socialization), when the caregiver is present, the caregiver provides a large reward ($R_s = 0.4$) when the agent reaches the green square."}, {"title": "3 Modeling Value Internalization", "content": "Our baseline agent is a deep reinforcement learner trained with PPO from Stable Baselines 3 (Schul- man et al., 2017; Raffin et al., 2021). We use a learning rate of 1e-4 and otherwise use the default hyperparameters from Stable Baselines 3 for all models tested. The top of Figure 2 shows an ab- stracted version of the typical loop between the environment and the agent where the environment provides the state and an extrinsic reward, and the agent produces actions based on its learned policy.\nFigure 1 shows the performance of the baseline agent on our environment distribution. We contrast what happens when the caregiver is present (blue) versus when the caregiver leaves at the halfway point (green). When the caregiver remains, the agent steadily improves its performance until even- tually plateauing near an optimal level. In contrast, when the caregiver leaves at the halfway point, performance rapidly drops to zero. This confirms our initial hypothesis: when the social rewards provided by the caregiver are the primary source of rewards that define the task, a typical reinforce- ment learner will not be able to continue learning and exploring autonomously when the caregiver is no longer present.\nWe hypothesize that human learners address this problem by internalizing the values of others. Here, we formalize this hypothesis by augmenting our baseline agent with an internalized social reward model (ISR) that learns to model the social rewards given by the caregiver and creates internal rewards ($R_i$) when the caregiver is absent (Figure 2). Thus we can write the full utility function of an agent with an ISR as $U = R_e + P \\cdot R_s + (1 - P) \\cdot R_i$. It receives a non-zero social reward $R_s$ when the caregiver is present $P = 1$ and a non-zero internalized reward $R_i$ when the caregiver is absent $P = 0$.\nThe ISR model is a deep neural network using the same architecture as the policy network. The network takes in the state and action and predicts reward. During the socialization phase, the agent stores the social rewards received, and those stored rewards are used to train the ISR model. The model was trained to minimize mean square error (MSE, $||R_s - R_i||$) since rewards are continuous. Finally, since the distribution of social rewards is imbalanced - positive rewards are more sparse than zero rewards - rewards were sampled such that each training batch had a balanced sample of reward magnitudes.\nWhen the task or distribution of tasks changes, deep RL policies often fail to generalize (Kansky et al., 2017). This failure results partly from the challenge of needing to predict an entire sequence of actions that optimize the expected cumulative discounted rewards. In contrast, the ISR module only needs to predict the reward for a particular action in a particular state without considering future actions. If the ISR module generalizes to new environments before the policy does, the agent could continue learning in those new environments even in the total absence of extrinsic reward. On"}, {"title": "4 Results", "content": "We first analyze the training of the ISR model. We then test whether a reinforcement learner aug- mented with ISR can solve the challenge posed in Figure 1 and analyze whether the ISR enables generalization by allowing for additional learning even without any social reward. Finally, we in- troduce a multi-agent scenario where the caregiver rewards altruistic behavior and show that our model extends to prosocial value internalization."}, {"title": "4.1 Training the ISR model", "content": "Figure 3 shows learning curves for the ISR. With sufficient data, the model achieves minimal test loss. The final test loss was an exponential function of the amount of social rewards observed, where each doubling of the number of rewards yielded an order of magnitude reduction in MSE loss."}, {"title": "4.2 Continual Learning and Generalization", "content": "We next test whether augmenting a reinforcement learner with the ISR module is sufficient to enable continual learning (Thrun, 1998; Hadsell et al., 2020). Figure 4 updates Figure 1 and shows how a model with internalized reward (shown in red) performs when the social rewards from the caregiver are removed. The model with ISR continues to do the task at the same rate as one that continues receiving social rewards. Thus, for this context, the ISR model fully internalized the social rewards of the caregiver. This enables the agent to continue autonomously without dependence on the caregiver's social rewards to maintain its behavior.\nThe right panel of Figure 4 shows a test of generalization. During the socialization period, when the caregiver was present, the agent was trained with just one block in the environment for 6K episodes. We then tested how well the agent could learn directly from the ISR in environments with five blocks (another 6K episodes). Performance was evaluated by calculating the total reward that would have been obtained if the caregiver was present in a held-out set of 100 episodes, i.e., a proxy for how well the caregiver's values have been internalized and generalized. This is labeled Reward (OOD) on Figure 4b. We compared ISR performance to a baseline (\"frozen\") and an upper bound (\u201coracle\u201d). The frozen baseline corresponds to testing a model right after the one-block socialization period on the five-block test without additional learning (all weights are frozen)."}, {"title": "4.3 Internalization Failure: Reward Hacking", "content": "When value internalization is incomplete, problems can arise when the internalized rewards are prematurely optimized. Generalization failures in the ISR model will propagate into errors in the agent's policy during the autonomous period when the ISR model is the target for learning. To study this empirically, we undertrained the ISR model on 1/12th of the data as before. Figure 6 shows that while the agent correctly optimizes its internal reward from the ISR model, it is less likely to reach the caregiver's goal. This failure can be considered an instance of reward hacking:"}, {"title": "4.4 Internalization of Prosocial Values", "content": "Up to this point, our empirical investigation focused on a single agent operating alone in an envi- ronment. However, many of the most important culturally acquired values are interpersonal and relate to how we should treat others. We investigate this phenomenon in a procedurally generated set of two-player scenarios shown in Figure 7 inspired by Ullman et al. (2009). Instead of the red agent being socially rewarded when it reaches the green square, the caregiver rewards it when the green player reaches the green square. However, a blue boulder blocks the green arrow's path in each generated grid. The red agent has two additional actions \"pick up\" and \"drop\" which allow it to pick up and move the boulder, clearing the path. The state is also augmented to include a binary indicator of whether or not the player is carrying the boulder. If possible, the green agent always moves toward the goal using a depth-first search. If no path is found, it remains in place.\nTo create solvable tasks procedurally, we generated 5x5 grids with seven blocks subject to the constraint that the remaining open tiles form a single connected component, which results in a tree structure. The boulder is placed in the location with the maximum degree in that tree, and the green player and green goal are placed on opposite components of the resulting disconnected graph at the tree's leaf nodes (endpoints). The red agent is placed at another leaf node. Thus, in each starting configuration, the green player's path is blocked by the boulder, and the only way for that player to reach the goal is if the red agent picks up and moves the boulder away. The caregiver gives a social reward to the red agent when the green player reaches the green square and otherwise gives no reward. Thus, we can study how a prosocial reward that is dependent on the behavior of another is internalized by the ISR module.\nFigure 7 shows the results from this experiment. Overall, we observe similar phenomena to those seen in previous experiments. During the socialization period, the agent learns the task. When the caregiver leaves, the agent without ISR unlearns the behavior. However, with ISR, the agent continues helping the other agent in new environments, having internalized the prosocial value. This is reminiscent of the human feeling of a \"warm glow\" when behaving altruistically (Andreoni, 1990)."}, {"title": "5 Discussion", "content": "We develop a new computational cognitive model for studying how values can be socially acquired and maintained during learning. We proposed a process called value internalization, where, during a socialization period, a caregiver socially rewards a learning agent based on the correctness of"}, {"title": "5.1 Source Code", "content": "https://github.com/friedeggs/social-play"}]}