{"title": "PGB: One-Shot Pruning for BERT via Weight Grouping and Permutation", "authors": ["Hyemin Lim", "Jaeyeon Lee", "Dong-Wan Choi"], "abstract": "Large pretrained language models such as BERT suffer from slow inference and high memory usage, due to their huge size. Recent approaches to compressing BERT rely on iterative pruning and knowledge distillation, which, however, are often too complicated and computationally intensive. This paper proposes a novel semi-structured one-shot pruning method for BERT, called Permutation and Grouping for BERT (PGB), which achieves high compression efficiency and sparsity while preserving accuracy. To this end, PGB identifies important groups of individual weights by permutation and prunes all other weights as a structure in both multi-head attention and feed-forward layers. Furthermore, if no important group is formed in a particular layer, PGB drops the entire layer to produce an even more compact model. Our experimental results on BERTBASE demonstrate that PGB outperforms the state-of-the-art structured pruning methods in terms of computational cost and accuracy preservation.", "sections": [{"title": "1. Introduction", "content": "Transformer-based models [1] including BERT [2], ROBERTa [3] and GPT-3 [4] have achieved great performance on natural language processing (NLP) tasks. However, all these models suffer from a large number of"}, {"title": "2.1. Transformer Architecture", "content": "The typical transformer architecture consists of encoder and decoder layers, each of which commonly contains two main components: multi-head attention (MHA) and feed-forward network (FFN). In an MHA layer, there are $N_H$ self-attention heads, and each head h \u2208 [1, $N_H$] involves the following weight matrices, $W_h^Q, W_h^K, W_h^V, W_h^O \\in R^{d^2}$. Then, the final output of the MHA layer is computed as follows:\n\n$MHA(X) = \\sum_{h=1}^{N_H} Attnn(X)$,\n\nwhere $Attnn(X)$ is the output of the standard self-attention unit.\nThe output of MHA is then fed into the corresponding FFN layer, which consists of weight matrices $W^{(1)} \\in R^{d \\times d_{ffn}}, W^{(2)} \\in R^{d_{ffn} \\times d}, b^{(1)} \\in R^{d_{ffn}}$ and $b^{(2)} \\in R^d$, where $d_{ffn}$ (usually 4 \u00d7 d) represents the dimension of the intermediate hidden features. The output of the FFN layer can be computed as follows:\n\n$FFN(A) = \\sum_{i=1}^{d_{ffn}} GELU (AW^{(1)}+b^{(1)})W^{(2)} + b^{(2)}$,\n\nwhere A = \u041c\u041d\u0410(X).\nIn transformer-based models, this same structure is repeatedly defined across multiples layers (e.g., 12 layers in BERTBASE [2]) and each layer has another multiple heads (e.g., 12 heads in BERTBASE [2]). Consequently, they have numerous trainable parameters, which motivates the NLP community to develop various compression methods for these models."}, {"title": "2.2. Distillation", "content": "Knowledge distillation (KD) [6] is a compression technique that trains a lightweight student model to mimic the soft output of a large teacher model, leading to competitive performance on downstream tasks. There are some studies where KD methods have been applied to transformer-based models. For example, DistilBERT [13] transfers knowledge to a student model of a fixed-size through a pre-training phase and an optional fine-tuning process. MiniLM [21] deeply describes the self-attention information of a task-agnostic teacher model by providing a detailed analysis of the self-attention matrices. Both TinyBERT [14] and MobileBERT [22] transfer knowledge during"}, {"title": "2.3. Pruning", "content": "Pruning [5] is another popular compression scheme that removes unimportant weights or structures from the target neural network. Following a massive volume of pruning techniques on convolutional neural networks (CNNs), pruning for transformer family has also been studied, falling into either unstructured or structured pruning.\nUnstructured pruning. In unstructured pruning, we remove individual weights that are not important, often aiming to reduce the memory storage itself for the target model while maintaining performance, such as the methods based on lottery ticket hypothesis [9] and movement pruning [23]. However, in these unstructured pruning methods, it is difficult and complicated to make satisfactory speedup at inference time, often requiring a special hardware.\nStructured pruning. Structured pruning is a simpler and more efficient way to reduce the model size, where we eliminate some structures of parameters that are considered less significant. In the case of transformer architectures, we can remove redundant attention heads [24, 25, 26], entire layers of either MHA or FFN [27, 28], or neurons along with all their connecting weights of FFNs [29]. However, such a coarse-grained pruning scheme inevitably suffers from severe drop in accuracy. Consequently, recent studies [10, 11, 12] have explored the combination of pruning with KD to further enhance the performance of compressed networks. Despite higher performance of these combined approaches, they sacrifice efficiency and simplicity in the compression process itself, as KD typically takes lengthy training times and involves complicated matching procedures."}, {"title": "3. Method", "content": "This section presents our one-shot semi-structured pruning method, called Permutation and Grouping for BERT (PGB), which applies a grouping procedure to weight matrices of each structure of a given task-specific BERT, as illustrated in Figure 2."}, {"title": "3.1. Overall Process", "content": "Unlike existing grouped transformer architectures [17, 18] that are designed to be trained from scratch, our goal is to find the adaptive grouping for minimizing information loss in the original task-specific BERT. Therefore, as shown in Figure 2, our PGB method performs the grouped pruning process for each individual weight matrix, rather than partitioning every part of the model into the fixed number of groups as in grouped transformers. In our pruned BERT architectures, the resulting number of groups for each layer can be different, and even a particular layer can be dropped as a whole when no important group is formed in the layer. After the pruning process, we apply the re-permutation step to every pruned weight matrix W to restore the original positions of the weights.\nProblem formulation. We first formulate our problem of grouped pruning on BERT. Consider a task-specific BERT $[\u0398_1, ..., \u04e8_L]$ with L layers, where $\u04e8_i$ consists of weight matrices $W^Q, W^K, W^V, W^O$ in its MHA sub-layers, and $W^{(1)}, W^{(2)}$ in its FFN sub-layers. Given a target compression rate \u03b3 and the task-specific dataset D, our goal is to find the pruned architecture $[\u0472_1, ..., \u04e8_L]$ such that:\n\n$\\min -\\frac{1}{L} ||F(D; O_i) \u2013 F(D; \\tilde{O_i})||\\\\\n  s.t. \\sum_{i=1}^L C(\\tilde{O_i}) \u2248 \u03b3 \u00b7 \\sum_{i=1}^L C(\u0398_i)$,\n\nwhere F(D; O) denotes the output of the model parameterized by \u0472 for the input data D and C(.) is the number of parameters (or FLOPs). As mentioned above, each $\u0472_i$ can have different number of groups unlike the equally grouped transformer architectures [17, 18].\nPGB outline. Algorithm 1 presents how our PGB method finds the group-based pruned architecture for a given task-specific BERT. The algorithm consists of two main phases, namely MHA pruning and FFN pruning. In accordance with previous studies [11, 12], our approach focuses on pruning less weights in the MHA sub-layers, compared to those of the FFN sub-layers. Furthermore, since it is crucial for one-shot pruning to deal with a more challenging scenario of pruning a larger number of weights in either MHA or FFN sub-layers, we attempt to minimize the information loss in MHA sub-layers than in FFN sub-layers. To this end, we first take a more conservative approach for pruning weights in MHA sub-layers (Lines 2-5) by trying to find an optimal grouped architecture for each weight matrix"}, {"title": "3.2. Grouped Weight Pruning", "content": "In order to find the optimal grouping for each weight matrix of BERT, we adapt the technique of group convolution pruning [15, 16], which is originally intended to prune filters in CNNs, not individual weights as in our problem setting.\nThe process of our grouped weight pruning is presented in Algorithm 2. For each weight matrix W \u2208 $R^{M\\times N}$ in BERT, our method first adaptively"}, {"title": "Finding the optimal permutation.", "content": "The permutation procedure (Line 6 in Algorithm 2) returns the optimal arrangement W of individual weights within the given matrix W \u2208 $R^{M\u00d7N}$. The objective is to cluster more important weights together, forming a group located at the top-left corner block of W. To this end, we determine the optimal pair of permutation vectors \u03c0, and $\u03c0_c$ for the rows and columns of W, respectively, that are used to rearrange the"}, {"title": "Heuristic solution for permutation.", "content": "Since weight matrices in BERT are typically high-dimensional, we employ an efficient heuristic algorithm [16] that finds sub-optimal permutation vectors. This algorithm alternatively sorts rows or columns based on the summation of importance scores corresponding to the weights of either row vectors within the region W[1:,$G \\frac{M}{NG}$] or column vectors within the region W[$\\frac{M}{NG}$,1:G]. Since each sorting process for the rows or the columns changes the arrangement of the corresponding columns or rows, respectively, we repeat this pairwise sorting process a few times (e.g., 6 times in our experiments using BERTBASE).\nAdaptive group numbers. The key property of our method is the adaptive determination of the number G of groups (Line 1 in Algorithm 2), based on the importance of weights within W \u2208 $R^{M\u00d7N}$. Basically, as the number of important weights in W increases, we decrease G and prune a smaller number of weights, whereas if W contains fewer important weights, we increase G to prune a greater number of weights. To this end, we devise the following strategy to adjust G based on the count of weights whose important scores exceed a specified threshold \u03c4:\n1. Determine the count $n_\u03c4$ of weights in W with importance scores higher than T."}, {"title": "", "content": "2. If $\\frac{n_\u03c4}{M X N} > G_{max}$, then prune the entire W.\n3. Otherwise, set G to a value less than $\\frac{M X N}{n_\u03c4}$.\nThe term $\\frac{M X N}{G}$ is derived from the fact that the number of parameters of a G-grouped matrix W is equal to that of W divided by G, i.e., $\\frac{M X N}{G}$. Therefore, to ideally cover all $n_\u03c4$ weights, W should have at most $\\frac{M X N}{G}$ groups. Also, we introduce the hyperparameter $G_{max}$ to prevent the formation of excessive groups for non-critical weight matrices (e.g., $G_{max}$ = 6 in our experiments)."}, {"title": "3.3. Re-Permutation", "content": "PGB finally performs the re-permutation procedure on every pruned weight matrix W in all the layers of the model to identify the positions of the weights that correspond to the original model. This yields a re-permuted weight matrix W*, wherein each weight is returned to its original positions. In this process, we utilize the permutation vectors \u03c0, and $\u03c0_c$ that have been stored, and proceed the following operation:\n$\\tilde{W}^* = \\tilde{W} \\underset{argsort(\\pi c)}{argsort(\\pi r)}$,\nwhere argsort(\u03c0) returns the corresponding re-permutation vectors that re-arrange the shuffled weights back to their original positions. Note that the final W* after re-permutation is in the same form resulting from fine-grained unstructured pruning, but actual computation at inference time is efficiently performed only with each $g^{(i)} \\subset \\tilde{W}$ as in grouped transformer architectures [17, 18].\nWeight compensation. To further restore the performance of the original task-specific BERT model, we update each unpruned weight in every W* by minimizing the following reconstruction error:\n$\\min -\\frac{}{2}||F(X;\\tilde{W^*}) \u2013 F(X;W)||^2$,\nwhere F(X; W) denotes the outputs for a sample dataset X.\nRe-Finetuning. After all these steps, we perform re-finetuning in the same way as in the original BERT [2] to recover the performance that is lost due to the pruning process."}, {"title": "4. Cost Analysis for Inference with PGB", "content": "As mentioned above, we make inference using only G groups of each pruned weight matrix W\u2208 $R^{M\u00d7N}$ for each linear operation $XW^*$, where X is the S-length input sequence. By using this inference module, we ensure G times faster efficiency by reducing the previous time cost S \u00b7 M \u00b7 N to S \u00b7 G \u00b7 $\\frac{M N}{G}$.\nAlgorithm 3 provides a detailed outline of the linear operation process used at inference time with a pruned model resulting from the GROUP-WEIGHT-PRUNING procedure in Algorithm 2. It takes the input sequence X \u2208 $R^{S\u00d7M}$, the grouped weight matrix W obtained after PGB pruning, and the row and"}, {"title": "5. Experiments", "content": "5.1. Environment\nDatasets. We conduct our experiments using two benchmark datasets, SQUAD [20, 35] and seven tasks (QNLI, QQP, SST-2, COLA, STS-B, MRPC, and RTE) in GLUE [19] on BERTBASE [2]. In our experiments, we use 2K samples from the training data for each benchmark dataset. Also, we perform re-finetuning on the pruned model using the training data of each NLP downstream task and evaluate on the corresponding dev dataset for each task. For detailed information on the benchmarks, please refer to Appendix A.3.\nCompared methods. In order to evaluate the performance of PGB, we conduct comparative experiments with the state-of-the-art structured pruning methods for BERT, including EBERT [38], DynaBERT [10], and CoFi [12]. In order to examine their pruning performance, we train each method without using distillation and data augmentation, both of which can commonly be applied to each pruning method.\nImplementation details. Our PGB method is implemented using Python 3.7.15, PyTorch [36] and CUDA 11.6, along with the Huggingface library [37], which incorporates the latest NLP techniques. We set the hyperparmeter $N_{perm}$ that controls the number of sorting operations in the permutation step, and $G_{max}$ that indicates the maximum number of groups, ensuring that the size of the grouped model is within the given budget capacity. All the experiments are conducted on a PC with NVIDIA GeForce RTX A6000. We report that all experimental results are the average of 5 random seeds within a range between 0 and 10.\nAs our target model, we use fine-tuned BERT [2] for specific tasks. Even though we mainly compare the performance using BERTBASE, we also conduct experiments using RoBERTABASE and DistilBERTBASE [13] (refer to 5.3 and 5.3). Full experimental details can be found at Appendix A."}, {"title": "5.2. Main Experimental Results", "content": "Performance comparison. Table 1 and Figure 3 show the performance comparison results of PGB with prior structured methods, using the seven tasks of GLUE and SQUAD. To equalize the resulting size of pruned models, we re-implement the released code of the compared methods by ourselves. For CoFi and DynaBERT, since there is no publicly available code for SQUAD, comparative experiments for these two methods were conducted exclusively"}, {"title": "Comparison with other SOTA methods.", "content": "Table 2 presents additional comparisons between PGB and other state-of-the-art (SOTA) pruning methods, using the reported results from their respective experiments. The compared methods include: hybrid-based block movement pruning (BMP) [11], Layer-Drop that drops certain layers of the model [28], and SNIP that prunes redundant mappings in residual modules [39]. Similar to the findings in Table 1,"}, {"title": "5.4. Ablation Studies", "content": "Performance of PGB without re-finetuning. Table 3 presents the performance comparison between vanilla PGB, which does not perform re-finetuning, and PGB with re-finetuning. We measure the accuracy of the compressed model with 40% and 60% of reduced FLOPs before and after re-finetuning on task-specific BERT models. We can observe that vanilla PGB itself is already effective to maintain high performance after pruning. For the SST-2 and QNLI tasks, the performance of vanilla PGB is either matches or surpasses that of other pruning methods shown in Figure 3. This demonstrates the capability of our grouped pruning operation to maintain the original performance of large transformer-based models with just a single round of pruning.\nHyperparameter Sensitivity. We conduct ablation studies to investigate the sensitivity to the hyperparameter utilized in the proposed PGB method. In Figures 6 and 7, we present visualization of the accuracy of the compressed model as we vary hyperparameter values, $G_{max}$ and 7. Figure 6 indicates that when the number of groups is limited to a small number (i.e., $G_{max}$ = 3),"}, {"title": "6. Conclusion", "content": "This paper introduced PGB, one-shot semi-structured pruning with a grouping strategy, as a fast and simple compression approach for transformer-"}, {"title": "Appendix A. Experimental Details", "content": "Appendix A.1. Details for Comparison Methods\nWe conduct comparative experiments on structured pruning for BERT: CoFi [12], BMP (based Hybrid Filled) [11], DynaBERT [10] and EBERT [38]. All these methods belong to the iterative pruning approach, which involves performing pruning during training to improve performance. CoFi [12] consists of 2 stages: pruning and final finetuning. Each stage involves 20 epochs of training with layer-wise distillation. BMP [11] selects appropriate components to prune at the block level over 20 epochs training stages. Subsequently, prediction layer distillation is performed on the pruned model. DynaBERT [10] utilizes a 2-stage training process with knowledge distillation to dynamically prune the width and depth size, followed by a final finetuning stage. The number of training epochs varies depending on the data size. For large datasets, the width-adaptive and width- and depth-adaptive stages are performed for 1 epoch of training each, while for small datasets, each stage is performed for 3 epochs of training. Additionally, in all cases, 3 additional epochs of finetuning are conducted. EBERT [38] involves 3 epochs of joint training during the pruning stage, followed by 3 epochs of final finetuning on the pruned model.\nAppendix A.2. Hyperparameters\nThe detailed experimental setup for PGB is provided in Table A.2. During the PGB pruning process, we utilize two hyperparameters, namely $N_{perm}$ and $G_{max}$. Also, we use the hyperparameters of the original BERT model and those of prior pruning methods for BERT.\nAppendix A.3. Details About Benchmark Datasets\nWe perform experiments using the GLUE [19] and SQUAD [20, 35] benchmark datasets. Each task of GLUE and SQuAD datasets can fall into the following categories:"}, {"title": "Appendix A.4. Model Configurations", "content": "The experimental models used in this paper include BERTBASE, DistilBERTBASE and ROBERTabase architectures, each of which has its respective parameter"}, {"title": "Appendix A.5. Rows and Columns for each Weight Matrix in BERT", "content": "Our PGB method performs pruning on individual parameters of $W^Q$, $W^K$, $W^V$, $W^O$ in the MHA sub-layers, as well as $W^{(1)}$ and $W^{(2)}$ in the FFN sub-layers. The number of rows and columns for each weight matrix is as follows:\n\u2022 (d, NH \u00d7 $\\frac{}{}$) : $W^Q$, $W^K$, $W^V$, $W^O$ in MHA\n\u2022 (d, $d_{ffn}$), ($d_{ffn}$, d) : $W^{(1)}$, $W^{(2)}$ in FFN"}]}