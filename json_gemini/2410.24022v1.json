{"title": "SFM-Protein: Integrative Co-evolutionary Pre-training for Advanced Protein Sequence Representation", "authors": ["Liang He", "Peiran Jin", "Yaosen Min", "Shufang Xie", "Lijun Wu", "Tao Qin", "Xiaozhuan Liang", "Kaiyuan Gao", "Yuliang Jiang", "Tie-Yan Liu"], "abstract": "Proteins, essential to biological systems, perform functions intricately linked to their three-dimensional structures. Understanding the relationship between protein structures and their amino acid sequences remains a core challenge in protein modeling. While traditional protein foundation models benefit from pre-training on vast unlabeled datasets, they often struggle to capture critical co-evolutionary information, which evolutionary-based methods excel at. In this study, we introduce a novel pre-training strategy for protein foundation models that emphasizes the interactions among amino acid residues to enhance the extraction of both short- and long-range co-evolutionary features from sequence data. Trained on a large-scale protein sequence dataset, our model demonstrates superior generalization ability, outperforming established baselines of similar size, including the ESM model, across diverse downstream tasks. Experimental results confirm the model's effectiveness in integrating co-evolutionary information, marking a significant step forward in protein sequence-based modeling.", "sections": [{"title": "1. Introduction", "content": "Proteins are the molecular machines of life, orchestrating essential biological functions that drive survival and growth. Their functionality stems from complex three-dimensional structures, which are determined by the unique sequences of amino acids that make up each protein (Jumper et al., 2021; Yang et al., 2020). Understanding how amino acid sequences define protein structure, and in turn function, remains a key challenge in biology. Solving this puzzle is crucial for designing targeted interventions in diseases and engineering proteins with new, desirable functions (Whitford, 2013).\nProteins are far more than simple chains of amino acids. Their behavior cannot be fully understood by examining individual residues or their linear arrangement. Instead, proteins fold into intricate tertiary structures where residues far apart in sequence can be spatially adjacent. These interactions, both short- and long-range, shape the protein's functional roles, mediating both static and dynamic biological processes (Sinai et al., 2017; Riesselman et al., 2017; Ding et al., 2019; Hsu et al., 2021).\nCo-evolutionary interactions, both within local sequences and across distant residues, are critical for maintaining the structural and functional integrity of proteins. These interactions ensure that amino acid co-variations preserve the stability of the protein's structure. This co-evolution reflects the collective interactions that underpin protein dynamics, influencing their folding and function. Following Anfinsen's landmark discovery that protein sequences inherently contain the information required to fold into their native structures (Anfinsen et al., 1961), Levinthal proposed his famous paradox. He argued that the number of possible protein conformations is so vast that a random search would take an impractical amount of time to find the correct structure (Levinthal, 1968; 1969). The paradox hinted at the existence of predetermined folding pathways, though their mechanisms remained unclear. To address this, researchers proposed the concept of \u201cfoldons\u201d, small cooperative folding units consisting of multiple residues, which fold sequentially and guide the protein to its final structure (Englander & Mayne, 2014). In silico studies have revealed that the formation of local structures often precedes the establishment of distant, non-local contacts (Lindorff-Larsen et al., 2011). These early-forming interactions are key to efficiently achieving the protein's final, functional form without the need for backtracking (Voelz & Dill, 2007). Understanding these co-evolutionary interactions, both short-range, which influence secondary structures, and long-range, which define tertiary structures, is vital for decoding the sequence-structure-function relationship. Insights into this relationship are crucial for applications such as drug design and protein engineering (Phillips, 2009a;b; Tang & Kaneko, 2020; Xu et al., 2021).\nProtein foundation models (PFMs), particularly those pre-trained on vast datasets of unlabeled sequences, have emerged as powerful tools for representing protein sequences (Alley et al., 2019; Bepler & Berger, 2019; Rives et al., 2021; Madani et al., 2020). These models achieve remarkable accuracy in protein-related tasks through fine-tuning. However, while models based on multiple sequence alignments (MSAs) effectively capture co-evolutionary relationships between homologous proteins, single sequence-based models often fall short in this area (Remmert et al., 2012; Mirabello & Wallner, 2019). Despite utilizing the same sequence data, single sequence models underperform compared to MSA-based approaches. We hypothesize that this performance gap arises from the insufficient capture of residue interactions in conventional pre-training techniques, which MSAs handle more effectively by incorporating evolutionary information.\nInspired by the folding hypothesis and existing research, we categorize co-evolutionary interactions into two types: short-range (local) interactions, which are crucial for forming secondary structures, and long-range (global) interactions, which shape the protein's overall tertiary structure. This distinction is illustrated in Figure 1, where local interactions are highlighted in red and global interactions in cyan.\nThis paper introduces a novel framework for protein foundation models that effectively captures both local and global co-evolutionary information within protein sequences. Leveraging large protein sequence databases, our model employs an integrative loss function designed to better reflect the complex interplay of interactions that govern protein folding and function. We detail the model's architecture, pre-training strategies, and the innovative loss functions used to capture both short- and long-range co-evolutionary information. Through extensive validation against empirical data and benchmarking against existing models, our approach demonstrates superior performance in predicting protein properties and functions.\nThe remainder of this paper is structured as follows: Section 2 reviews related work on protein foundation models. Section 3 outlines our model's architecture, loss function, and training algorithm. Section 4 presents our experimental results across a variety of downstream tasks. Section 5 concludes with a discussion of future directions and potential extensions of this work."}, {"title": "2. Related Work", "content": "Protein representation learning has become a pivotal area of research within computational biology, with approaches generally classified into two main categories: sequence-based and structure-based methods.\nSequence-based Approaches. Traditional sequence-based methods primarily rely on sequence alignment to capture evolutionary information. Tools like PSI-BLAST (Altschul et al., 1997) and HMMER (Finn et al., 2011) have been instrumental in identifying evolutionary relationships between protein sequences, which aid in predicting protein structure and function. With the advent of deep learning, protein language models (PLMs) have emerged, leveraging neural networks to encode protein sequences. For example, Bepler et al. (Bepler & Berger, 2019) used a BiLSTM to learn protein representations that capture sequential similarity and function. Inspired by advances in natural language processing, Rives et al. introduced ESM-1b (Rives et al., 2021), which employed self-supervised learning to create general-purpose sequence representations for downstream tasks. Subsequent models in the ESM series, such as ESM-1v (Meier et al., 2021), ESM-IF1 (Hsu et al., 2022), and ESM-2 (Lin et al., 2022), have progressively improved the performance of PLMs on large-scale protein sequence data. Several methods have integrated multiple sequence alignment (MSA) data to enhance PLMs' representation capacity. Examples include ESM-MSA-1b (Rao et al., 2021) and Tranception (Notin et al., 2022), which fuse evolutionary information to improve predictions. More recently, models like ESMFold (Lin et al., 2023), AminoBERT (Chowdhury et al., 2022), and OmegaFold (Wu et al., 2022) have explored the potential of single-sequence models for protein structure prediction, showing promising results in bypassing the need for MSA.\nStructure-based Approaches. With the success of advanced protein structure prediction models, such as AlphaFold (Varadi et al., 2024), and the availability of large structure databases, there has been increasing interest in developing PLMs that incorporate structural information. One approach encodes protein structures as graphs, where atoms or residues are represented as nodes, and structural contacts define the edges. Graph neural networks (GNNs), such as those used by Hermosilla et al.(Hermosilla & Ropinski, 2022), Zhang et al.(Zhang et al., 2022), and others (Van Kempen et al., 2024; Yang et al., 2023), have shown promise in capturing geometric structure for downstream tasks. However, GNN-based methods face scalability challenges compared to the transformer-based architectures used in large language models. Another emerging direction involves tokenizing structural information and integrating it into PLMs. Approaches like UniMol (Zhou et al., 2023), SAPROT (Su et al., 2023), and structural pre-training methods by Chen et al. (Chen et al., 2023) have fused structural data with sequence-based models, achieving improved performance in tasks requiring structural understanding. Notably, the recently introduced ESM-3 model (Hayes et al., 2024), designed specifically for protein design, incorporates structural information directly as input, which differentiates it from purely sequence-based models like ours. Our work distinguishes itself by developing a novel pre-training framework for protein foundation models that enhances the capture of both local and global co-evolutionary information from sequences, without requiring MSA or explicit structural input. This approach aims to close the gap between single-sequence models and MSA-based methods by better modeling residue interactions and co-evolutionary signals through a dedicated loss function tailored to protein folding and structure prediction."}, {"title": "3. Methods", "content": "SFM-Protein is a protein sequence representation model based on Transformer encoder (Vaswani, 2017), which leverages bi-directional self-attention to capture the interactions among the residues pre-trained using a masked language model with both pairwise loss and span loss. The model details will be introduced in the following section."}, {"title": "3.1. Preliminaries", "content": "A Transformer encoder layer consists of a multi-headed self-attention mechanism and a feed-forward network, both incorporating skip connections and pre-layer normalization (Xiong et al., 2020).\nx = x + MHA(LAYERNORM(x)),\nx = x + FFN(LAYERNORM(x)),\nThe multi-head attention can be written as:\nMHA(Q, K, V) = CONCAT(head1, ..., headh) WO\nhead\u2081 = ATTN(QWKWK,VWV)\nATTN(Q, K, V) = SOFTMAX($\\frac{QKT}{\\sqrt{dk}}$) V\nSFM-Protein also exploits a positional embedding to distinguish the tokens in different positions, i.e., RoPE (Rotary"}, {"title": "3.2. Sequence-based Pre-training", "content": "The initial phase of our model training involves pre-training on protein sequences. This pre-training is done in a self-supervised fashion using a unique masked language modeling objective. The model has two output heads: a pairwise prediction head which is used to model pairwise interactions between residues, and a span prediction head which predicts the byte-pair encoding (BPE) tokens of the protein sequences.\nOur model's backbone is a deep transformer encoder architecture that is made up of several layers of multi-head self-attention (MHA) and feed-forward networks. To infuse positional information into the self-attention layers, we use relative positional encodings (RoPE).\nIn our approach, the masked language modeling objective is used to randomly mask a subset of residues for prediction. Notably, instead of masking residues independently, we mask consequential spans of residues with a certain probability x%. This unique span-based masking strategy is designed to encourage the model to learn higher-order residue interactions and patterns. Furthermore, it aims to model the secondary structure in the proteins, which is a significant aspect of understanding and predicting protein function and structure."}, {"title": "3.3. Global Co-evolution: Pair Label Recovering", "content": "The pairwise prediction head utilizes the output embedding from the transformer to predict a distribution over all possible pairs of residues for each residue in the sequence. This output is then transformed into a pair-wise feature representation as defined by the equation:\nFglobal(x) = W\uff61 * ((Wq \u2022 x) & (Wk.x))\nwhere stands for the outer product, i.e., [xy]ij = Xi\u00b7Yj, and is the element-wise product of two vectors.\nFollowing this transformation, the pair-wise feature is then projected into a pair-wise dictionary with the help of a Multilayer Perceptron (MLP). The primary aim of this pairwise"}, {"title": "3.4. Local Co-evolution: BPE Label Recovering", "content": "The span prediction head operates by predicting the BPE tokens based on the contextual representations derived from the model. A BPE tokenizer, trained on the protein sequences from the training data, is used to tokenize each protein sequence into BPE tokens.\nThen, each residue is assigned a BPE label that corresponds to the BPE token it is grouped into. If the tokenizer encodes a consecutive span of residues (Xi, Xi+1, ..., Xi+k) into a single BPE token b, then all these residue positions are assigned the same BPE label b: xi = b, xi+1 = b, ... Xi+k = b.\nThis labeling methodology serves a dual purpose. Firstly, it allows the model to learn representations at the individual residue level via the pairwise prediction head. This allows the model to understand the interactions between different residues. Secondly, it enables learning at the BPE token level through the span prediction head, which helps the model to identify and understand patterns across multiple consecutive residues, providing a more comprehensive understanding of the protein sequences. This dual-level learning is a key differentiator of our model, making it uniquely positioned to capture both local and global contextual information in protein sequences."}, {"title": "3.5. Training Objectives", "content": "To effectively encapsulate the co-evolutionary information within a comprehensive framework, we propose a composite loss function:\nL = a. Lglobal + (1 \u2212 a) \u00b7 Llocal\nThis function is composed of two distinctive components. The global loss, also known as pairwise loss, is specifically designed to capture the co-evolutionary relationships between masked pairs within the sequence. It does this by calculating a cumulative loss across these pairs, with its influence on the overall loss function dictated by the coefficient a:\nLglobal = \u03a3\u03a3 log P(xi, xj | X/M)\nXED i,jEM\nOn the other hand, the local loss, also referred to as span loss, is employed to account for the contiguous masked spans within the sequence. The Byte Pair Encoding (BPE) labels serve as the ground truth in this calculation. The contribution of local loss to the total loss function is regulated by the coefficient (1 \u2013 a):\nLlocal = \u03a3 \u03a3 log P(bi,..., bi+k | X/M)\nXED i..i+k\u2208M\nwhere bi is the BPE label of xi."}, {"title": "3.6. Downstream Task Fine-tuning", "content": "A simple multi-layer perceptron (MLP) was exploited to fine-tune the pre-trained encoder for various downstream protein prediction tasks, including classification, regression, and structured prediction problems. For sequence-level tasks, e.g., solubility prediction or protein function classification, the [CLS] token embedding can be used to represent the entire input protein sequence. This embedding is fed into the MLP head to make the sequence-level prediction. For residue-level tasks, e.g., secondary structure prediction or residue property annotation, the output embedding vector corresponding to each residue position can be extracted and fed into the MLP for making per-residue predictions. Uniquely, in addition to learning single residue representations, our pairwise prediction head allows the model to generate rich pairwise residue-residue representations by definition. This enables fine-tuning the model for tasks involving residue-residue relationships, such as residue contact prediction for modeling tertiary protein structure. The pairwise residue embeddings can be used as input features to the MLP for this task. Overall, this flexible output representation learning allows our model to be effectively transferred to a diverse array of protein analysis tasks at varying granularities - sequence-level, residue-level, and residue pair-level."}, {"title": "4. Experiment Results", "content": "SFM-Protein is pre-trained on the UniRef50 database of clustered protein sequences at 50% identity. Our model operates on sequences of amino acid residues, comprising over 62 million protein sequences and 17 billion residues from UniRef50. For the masked token prediction, we randomly corrupt 30% of the amino acid residues and then train the model to recover the original residues. This high mask ratio of 30% allows the model to learn more better representations compared to lower mask ratios. To efficiently utilize compute resources and enable data-parallel pre-training across multiple GPUs, we develop a novel data packing strategy. Protein sequences are packed into chunks of 8192 tokens, with padding to uniform lengths. This allows batching and parallel processing of sequences during pre-training. Our masked token prediction and data packing approaches allow scaling pre-training to the full UniRef50 dataset in a computationally efficient manner. We evaluate the pre-trained model on various downstream protein prediction tasks.\nWe compare the performance of different models across various understanding tasks and metrics. The datasets for GO term prediction and EC number prediction are selected from DeepFRI (Gligorijevi\u0107 et al., 2021) which are widely used, while the others are primarily adopted from the PEER benchmark (Xu et al., 2022). Two model sizes are chosen, namely, 650 million parameters (650M) and 3 billion parameters (3B). The models evaluated include ESM-1b, ESM2, and our model SFM-Protein. Each model's performance is assessed using specific metrics that are relevant to the particular understanding task. Below, we will analyze these results in detail. The choice of metric aligns well with the nature of each task, providing a nuanced view of model performance in terms of ranking (Spearman), binary classification accuracy, and multi-class classification accuracy (F1 max, AUPRC). Our model consistently shows superior performance across nearly all tasks and metrics, highlighting its effectiveness in various biological understanding tasks."}, {"title": "4.1. SFM-Protein Exhibits Favorable Scaling Properties", "content": "During the pre-training phase, we observe a consistent improvement in the model's performance that correlates with increased computational resources, specifically floating-point operations per second (FLOPS), as depicted in Figure 3. In downstream tasks, we note a general enhancement in performance when the number of parameters is augmented from 650 million to 3 billion. This trend holds true for both the ESM2 and our proposed models, indicating that larger models are more adept at capturing intricate patterns and relationships within biological datasets."}, {"title": "4.2. SFM-Protein Enables Accurate Protein Function Prediction via Fine-Tuning", "content": "The models are compared on a diverse set of tasks from various benchmark datasets related to protein function annotation, including Gene Ontology (GO) term prediction and Enzyme Commission (EC) number prediction. Specifically, the tasks include predicting GO terms across the three branches - Molecular Function (GO-MF), Biological Process (GO-BP), and Cellular Component (GO-CC). The EC number prediction task involves assigning EC numbers, which provide a hierarchical classification of enzymes based on the metabolic reactions they catalyze. These function prediction tasks use a combination of metrics for evaluation - the maximum F1 score (F1 max) across multiple thresholds, as well as the Area Under the Precision-Recall Curve (AUPRC). F1 max measures the maximum harmonic mean of precision and recall, while AUPRC provides a threshold-independent assessment of the precision-recall trade-off. The results are listed in Table 1.\nOur model consistently outperforms other state-of-the-art methods across all categories - GO-MF, GO-BP, GO-CC, and EC number prediction - both in terms of the F1 max and AUPRC metrics. Notably, it achieves top scores of 0.855 F1 max and 0.884 AUPRC on the challenging EC number prediction task, surpassing previous models by a significant margin. Even at the larger 3 billion parameter scale, our model exhibits top performance across all GO categories and EC number prediction. It demonstrates the highest F1 max and AUPRC scores compared to other large language models. This highlights the model's robustness in handling the highly complex and diverse biological data involved in protein function annotation. Overall, the strong results validate the effectiveness of our self-supervised pre-training approach and pairwise residue modeling for learning powerful protein representations transferable to a wide range of function prediction tasks."}, {"title": "4.3. SFM-Protein Enables Data-Driven Protein Fitness Landscape Modeling via Fine-Tuning", "content": "The tasks include quantitative regression tasks like fluorescence prediction and stability prediction for proteins. These regression tasks use different evaluation metrics compared to the classification tasks. Specifically, Spearman's rank correlation coefficient is employed to assess the correlation between the predicted and ground truth real-valued outputs for fluorescence and stability. The performance are compared in Table 2."}, {"title": "4.4. SFM-Protein Fine-Tuning Enables Accurate Soluble Protein Identification", "content": "The task is solubility prediction, which involves classifying whether a given protein sequence is soluble or insoluble. For this binary classification task, the accuracy score is used as the evaluation metric. The results are presented in Table 1."}, {"title": "4.5. SFM-Protein Facilitates Rational Antibody Design via Sequence Completion", "content": "The task of rational antibody CDR-H3 design focuses on constructing the Complementarity-Determining Region H3 (CDR-H3) of an antibody, which plays a critical role in antigen binding due to its high variability and potential for optimization in antibody design. For this study, we employ the RAbD benchmark dataset (Adolf-Bryfogle et al., 2018), comprising 60 antibody sequences. The task is framed as designing a CDR-H3 sequence that maximally matches the corresponding residues in the native antibody.\nThe evaluation metric is Amino Acid Recovery (AAR), which quantifies the percentage of residues in the generated CDR-H3 sequence that match those in the ground truth sequence. The results of the CDR-H3 design task are summarized in Table 4. GNN-based models, such as AR-GNN, RefineGNN (Jin et al., 2021), and ABGNN (Gao et al., 2023), demonstrate moderate performance. Sequence-based models outperform GNN-based approaches, with English-Bert (Melnyk et al., 2023) achieving the highest performance. SFM-Protein delivers comparable results."}, {"title": "5. Discussion", "content": "The detailed comparison shows that, across a range of protein understanding tasks, our models, particularly those with 3 billion parameters, demonstrate high effectiveness, largely due to the integrated loss design. These results also suggest that novel approaches are needed for pre-training protein foundation models on pure sequences. Future work should focus on exploring specific features or techniques that contribute to the improved performance of protein foundation models."}]}