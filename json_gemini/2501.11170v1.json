{"title": "AIMA at SemEval-2024 Task 3: Simple Yet Powerful Emotion Cause Pair Analysis", "authors": ["Alireza Ghahramani Kure", "Mahshid Dehghani", "Mohammad Mahdi Abootorabi", "Nona Ghazizadeh", "Seyed Arshan Dalili", "Ehsaneddin Asgari"], "abstract": "The SemEval-2024 Task 3 presents two subtasks focusing on emotion-cause pair extraction within conversational contexts. Subtask 1 revolves around the extraction of textual emotion-cause pairs, where causes are defined and annotated as textual spans within the conversation. Conversely, Subtask 2 extends the analysis to encompass multimodal cues, including language, audio, and vision, acknowledging instances where causes may not be exclusively represented in the textual data. Despite this, our model addresses Subtask 2 using the same architecture as Subtask 1, focusing solely on textual and linguistic cues. Our architecture is organized into three main segments: (i) embedding extraction, (ii) cause-pair extraction & emotion classification, and (iii) post-pair-extraction cause analysis using QA. Our approach, utilizing advanced techniques and task-specific fine-tuning, unravels complex conversational dynamics and identifies causality in emotions. Our team, AIMA (MotoMoto at the leaderboard), demonstrated strong performance in the SemEval-2024 Task 3 competition ranked as the 10th rank in subtask 1 and the 6th in subtask 2 out of 23 teams. The code for our model implementation is available on https://github.com/language-ml/SemEval2024-Task3.", "sections": [{"title": "1 Introduction", "content": "The task of Emotion-Cause Pair Extraction in Conversations holds significant importance in advancing the field of emotion analysis. Unlike previous endeavors that primarily focused on recognizing emotions, this task delves deeper into understanding the underlying causes behind emotional expressions within conversational contexts (Wang et al., 2023). Recognizing that emotions are conveyed not only through words but also through vocal intonations and facial expressions, the field has shifted towards multimodal emotion recognition. This move aims to understand how emotions are interwoven with text, sound, and visual cues in dialogue (Wang et al., 2023).\nThe SemEval-2024 Task 3 (Wang et al., 2024, 2023; Xia and Ding, 2019) encompasses two subtasks aimed at extracting emotion-cause pairs in conversational contexts. Subtask 1 focuses on textual emotion-cause pair extraction, where causes are defined and annotated as textual spans within the conversation. In contrast, Subtask 2 broadens the analysis to incorporate multimodal cues, including language, audio, and vision. The task is based on the multimodal conversational emotion cause dataset ECF (Wang et al., 2023).\nIn this paper, we introduce an approach based on a model architecture consisting of three key components: (i) embedding extraction, (ii) cause-pair extraction & emotion classification, and (iii) cause extraction via QA post-pair detection. Utilizing advanced techniques and fine-tuning on specific datasets, our goal is to dissect complex conversational dynamics and pinpoint nuances that indicate emotional causality.\nAlthough our architecture supports multimodal data-including text, audio, and video through concatenations of the embeddings of these modalities using pretrained models this study specifically harnesses textual data, as our primary focus is on addressing subtask 1."}, {"title": "2 Related Work", "content": "This section provides an overview of two key areas in the field of emotion analysis: Emotion Recognition in Conversation and Emotion-Cause Pair Extraction in Conversations.\nEmotion Recognition in Conversation: Emotion recognition in conversation, a burgeoning field, aims to decipher and understand the complex interplay of emotions within dialogues. ERC has seen significant advancements in recent years (Kim and Vossen, 2021; Zheng et al., 2023). These approaches have shown promising results on popular datasets such as IEMOCAP (Busso et al., 2008) and MELD (Poria et al., 2019).\nEmoBERTa (Kim and Vossen, 2021) enhances ROBERTa (Liu et al., 2019) for emotion recognition in conversation (ERC) on datasets IEMOCAP (Busso et al., 2008) and MELD (Poria et al., 2019), by incorporating speaker information and dialogue context. It preprocesses dialogues, representing them as sequences with speaker annotations and context segments. EmoBERTa extends ROBERTa to handle multiple segments and utilizes a linear layer with softmax nonlinearity for sequence classification.\nThe FacialMMT (Zheng et al., 2023) framework comprises two key stages. Initially, a pipeline method is employed to isolate the face sequence of the real speaker within each utterance. Following this, a multi-modal facial expression-aware emotion recognition model is applied. This model utilizes frame-level facial emotion distributions and incorporates multi-task learning to improve utterance-level emotion recognition. Experimental evaluations conducted on the MELD (Poria et al., 2019) dataset validate the effectiveness of FacialMMT.\nEmotion-Cause Pair Extraction in Conversations: The task of Emotion-Cause Pair Extraction in Conversations is pivotal for advancing our understanding of the nuanced interplay between emotions and their underlying triggers within dialogues, offering insights into human communication, cognition, and interpersonal dynamics.\nThe paper (Wang et al., 2023) introduces a baseline system, MC-ECPE-2steps, comprising two steps. Firstly, it employs multi-task learning to extract emotions and causes separately, utilizing word-level encoding and utterance-level encoders to derive representations specific to each. Secondly, it combines the predicted emotions and causes into pairs and employs BiLSTM and attention mechanisms to obtain pair representations. Subsequently, non-causal pairs are filtered out using a feed-forward neural network. Additionally, the system incorporates multimodal features from text, audio, and video modalities to enhance the extraction process. In addition to this approach, there exist other methodologies for Emotion-Cause Pair Extraction in Conversations (Xia and Ding, 2019; Zheng et al., 2022), some of which leverage question-answering techniques (Nguyen and Nguyen, 2023)."}, {"title": "3 System Overview", "content": "Our model architecture, illustrated in Figure 2, is designed with the capacity to incorporate a diverse set of inputs from various sources such as text, video, and audio to perform emotion-cause analysis within conversational contexts. However, for the purpose of addressing subtask 1, we specifically utilized textual data.\nEmbedding Extraction and Emotion Classification: In the Embedding Extraction phase, we leverage the EmoBERTa (Kim and Vossen, 2021) model specifically designed for text embedding. EmoBERTa's selection is based on its proven effectiveness in capturing the nuanced emotional dynamics inherent in conversational data, thereby facilitating precise emotion classification of the utterances."}, {"title": "Causality Matrix Extraction Process:", "content": "1. Initial combination of embeddings and logits:\ncombined = [81, 82, 83, logits] (1)\nwhere 81, 82, and s3 are embeddings for an utterance, and logits are the output from\nthe classification model Mc, computed as\nlogits = Mc([81, 82, 83]).\n2. Application of dropout and addition of positional embeddings:\ninput = dropout(combined) + epos (2)\nHere, epos represents the positional embeddings, which are added to the dropout-modified combined inputs to incorporate positional information into the sequence representation. Specifically, epos encodes the position of each utterance within the conversation, enriching the model's understanding of dialogue structure and the sequential context of each utterance.\n3. Generation of the causality matrix through the transformer encoder layers:\nCm = AN(lencoder (input))\n(3)\nHere, lencoder denotes the i-th transformer encoder layer, with N 1 indicating that the input sequentially passes through all layers up to the N \u2013 1-th layer. AN refers to the attention weights from of the N-th (last) encoder layer. The causality matrix, Cm, is specifically derived from these attention weights applied to the output of the N \u2013 1-th layer, which has been processed by all preceding encoder layers and enhanced with positional embeddings. This matrix captures the causal interactions within the dialogue, as inferred from the attention mechanism of the transformer's final layer."}, {"title": "Question Generation for Causality Pairs:", "content": "Following the emotion classification task, where emotions within the dialogue are identified, a causality matrix is created. For each emotion-cause pair detected in this matrix, the system generates a structured query to facilitate the extraction of the causal text segment. The prompt, constructed only for these detected pairs, follows the template:\n\"Which part of the text {target_utterance} is the reason for {speaker}'s feeling of {emotion} when {main_utterance} is said?\"\nThe Cause Extraction After Finding Pairs phase utilizes a question-answering model to interrogate the text, pinpointing exact sub-texts that substantiate the identified emotional triggers. This study undertook a thorough evaluation of various question-answering (QA) models, uncovering areas where each model could be enhanced. Among the models examined, DistilBERT (Sanh et al., 2019) and BERT (Devlin et al., 2018) showed considerable promise for application within our research framework. Ultimately, we selected the deepset/deberta-v3-base-squad2, a pre-trained QA model, for our specific task requirements. This choice was informed by the model's foundation on the DeBERTa-v3-base architecture (He et al., 2021) and its prior fine-tuning on the SQuAD2 dataset (Rajpurkar et al., 2016), which includes both answerable and unanswerable questions. By further fine-tuning this model on our dataset, we ensured its proficiency in accurately extracting causal text segments from conversational contexts, a critical capability for our emotion-cause analysis."}, {"title": "4 Experimental Setup", "content": "4.1 Dataset Preparation\nDataset Preparation for Attention Model: The dataset preparation for cause pair extraction and emotion classification procedure commenced with the loading of conversation data and emotion-cause pairs, accompanied by preprocessing steps tailored for model training. A custom dataset class facilitated the loading and processing of data, extracting essential details like conversation ID, utterances, and emotion-cause pairs. Subsequently, a collate function was employed to organize individual samples into batches suitable for model input, focusing solely on text and generating attention targets based on the presence of cause pairs within the textual data.\nDataset Preparation for QA Model: The dataset preparation for subtext emotion cause extraction using question answering involved constructing samples for question answering by generating questions and contexts solely from text data. Each sample comprised a question formulated with a predefined prompt, the context concatenating all utterances from the conversation, and the answer containing the cause subtext. The dataset then underwent preprocessing to train the question-answering model, utilizing a pre-trained tokenizer to align tokenized inputs with the original text and determine the start and end positions of the answers within the textual context.\n4.2 Training\nTraining the Attention Model: The attention model was optimized using mean squared error loss and the AdamW optimizer with a learning rate of le-4.\nTraining the QA Model: The QA model was trained over 25 epochs with a batch size of 8.\n4.3 Evaluation Metrics\nOur models' performance was gauged using F1 scores across the six primary emotion categories, with additional emphasis on weighted averages to account for class imbalances. Subtask 1 evaluations incorporated both Strict Match and Proportional Match metrics to assess the accuracy of textual span identification for emotional causes."}, {"title": "5 Results", "content": "5.1 Quantitative Findings\nOur team, MotoMoto, participated in the SemEval-2024 Task 3 competition and secured the 10th rank in Subtask 1 and 5th rank in Subtask 2. The official metrics for our team's performance are as shown in Table 1 To explore the effectiveness of our approach, we compare it with the MC-ECPE-2steps (Wang et al., 2023) method, which represents our baseline. The comparison is based on the weighted average F1 scores achieved by both approaches, as presented in Table 2.\n5.2 Error Analysis\nOur investigation into the discrepancies between our system's predictions and the ground truth leveraged the detailed insights from the confusion matrix (Table 3). The analysis underscores our emotion classification module's exceptional performance, notably in accurately identifying 'Neutral' and 'Joy' emotions with 4400 and 1576 correct instances, respectively. This substantiates our model's adeptness at recognizing emotions within conversations. Despite these strengths, the emotion-cause pair extraction component displayed variations, such as over or under-identification of causes compared to the ground truth annotations. Nevertheless, the precision of our model in identifying correct causes, as highlighted by specific successes in the confusion matrix, confirms its effectiveness in discerning emotions. These observations suggest that while our model excels in accurately identifying emotions, there is a valuable opportunity to refine the identification of causal factors within conversations for further improvement."}, {"title": "6 Conclusion", "content": "Our investigation into emotion-cause pair extraction presents a paradigm shift towards simplicity and efficiency without compromising performance. By adopting a streamlined approach, we have demonstrated that high-impact emotion analysis does not necessarily require heavy computational resources or complex multimodal data integration. Our participation in the SemEval-2024 Task 3 competition has validated our methodology, securing commendable rankings and highlighting the efficacy of our model. The results underscore the potential of cost-effective solutions in the realm of emotion analysis, opening doors to wider applicability in resource-constrained environments. Looking forward, we aim to further optimize our model's efficiency and explore the integration of lightweight multimodal data processing techniques. This endeavor not only reinforces the viability of minimalist approaches but also sets a new benchmark for future research in emotion-cause analysis."}]}