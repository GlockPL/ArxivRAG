{"title": "Explaining an Agent's Future Beliefs through Temporally Decomposing Future Reward Estimators", "authors": ["Mark Towers", "Yali Du", "Christopher Freeman", "Timothy J. Norman"], "abstract": "Future reward estimation is a core component of reinforcement learning agents; i.e., Q-value and state-value functions, predicting an agent's sum of future rewards. Their scalar output, however, obfuscates when or what individual future rewards an agent may expect to receive. We address this by modifying an agent's future reward estimator to predict their next N expected rewards, referred to as Temporal Reward Decomposition (TRD). This unlocks novel explanations of agent behaviour. Through TRD we can: estimate when an agent may expect to receive a reward, the value of the reward and the agent's confidence in receiving it; measure an input feature's temporal importance to the agent's action decisions; and predict the influence of different actions on future rewards. Furthermore, we show that DQN agents trained on Atari environments can be efficiently retrained to incorporate TRD with minimal impact on performance.", "sections": [{"title": "1 Introduction", "content": "With reinforcement learning (RL) agents exceeding human performance in complex and challenging game environments (e.g., Atari 2600 [3], DotA 2 [6], and Go [22]), there is significant interest in applying these methods to address practical problems, often in support of human judgement. There are several barriers to realising this vision, however, with the need for agents to be able to explain their decisions one of the most important [18]; agents need to be able to work with people [7], and so we need effective Explainable Reinforcement Learning (XRL) mechanisms.\nCentral to RL agents is a future reward estimator (Q-value or state-value function) predicting the sum of future rewards for a given state. These functions are used either explicitly in the policy itself (e.g., DQN [16]) or for learning with a critic (e.g., PPO [19] and TD3 [8]). However, few XRL algorithms have devised methods to explain these functions directly. One problem is that their scalar outputs provide no information on its composition (i.e., when and what future rewards the agent believes it will receive), just its expected cumulative sum.\nAn example of this problem is illustrated in Figure 1 where a drone has two paths: up or down. Depending on the path taken, the drone can receive coins for 1 point each or the treasure chest for 4 points. Using a discount factor of 0.95, the drone's Q-value for moving up is 3.26 while moving down is 3.52. Despite this small difference in Q-values, the quantity and temporal immediacy of their expected rewards are radically different; moving up, the drone receives a single large reward, while moving down receives many smaller rewards. A fact unknown from observing the Q-values alone although critical to agent behaviour in selecting whether to move up or down.\nWe propose a novel future reward estimator that predicts the agent's next N expected rewards for a given state, referred to as Temporal Reward Decomposition (TRD) (Section 4). We prove that TRD is equivalent to the Q-value and state-value functions. In this way, TRD can report the temporal immediacy and quantity of future rewards for different action choices, enabling decisions to be explained and contrasted. For example, using Figure 1, the agent's TRD Q-value for moving down is [0, 0.95, 0.90, 0.86, 0.81] and moving up [0, 0, 0, 0, 3.26], enabling us to produce explanations such as \u201cwhile the sum of actual rewards is equal, taking the route down has more immediate rewards, which are preferred by the drone due to its discount factor\".\nImplementing TRD requires only two changes to a deep RL agent's future reward estimator: increase the network output by N+1 for predicting the future rewards and a novel element-wise loss function of future rewards (Section 4). Importantly, TRD can achieve similar performance as DQN [16] agents for Atari environments [4]"}, {"title": "2 Related Work", "content": "In this brief review we focus on existing reward-based explanation mechanisms and algorithms for understanding an agent's decision-making in similar domains (see Qing et al. [18] for a survey).\nPrior work in XRL has explored decomposing the Q-value into reward components and by future states. Juozapitis et al. [11] proposed decomposing the future rewards into components. In Figure 1, for example, we have two components (or reward sources), the treasure chest and the coins. An explanation would then contrast the coin(s) versus treasure chest(s) along different trajectories, but not when any rewards are expected. Further work has incorporated policy summary [21] and abstract action spaces for robotics [12] into the explanations. Alternatively, Tsuchiya et al. [26] proposed decomposing the Q-value into the importance of future states and Yau et al. [28] into the probabilities of state transitions. However, neither state decomposition proposal has been shown to scale to complex environments where explanations are most important for understanding agents. Importantly, all these decomposition approaches differ from our work as they require decomposing the reward estimator into components or states rather than over time, although future work could explore combining these approaches.\nAn alternative approach to understanding an agent's rewards is to modify the environment's reward function. Mirachandani and Karamcheti [15] proposed a reward-shaping approach using natural language to convert long-horizon task descriptions to lower-level dense rewards. This allows the higher-level descriptions to be used to explain the agent policy however this relies on correctly interpreting these complex descriptors while our work requires no modification to the environment setup.\nFor approaches that contribute similar applications to TRD, Madumal et al. [13] proposed a text-based explanation using a hand-crafted causal model of a Starcraft 2 agent to explain why (or why not) to take an action with respect to environmental state variables from the causal model. Our explanation similarly illustrates the future reasoning of an agent, but does so in terms of rewards not changes to the environment's state. Most importantly, our approach requires neither a hand-crafted causal model nor explicitly identified environment features. To explain the agent's focus within an observation, Greydanus et al. [9] proposed a perturbation-based saliency map that uses the changes in the policy output for noise applied to an area of the observation to understand a region's importance to the agent. This is limited to only visualising a region's importance for all future rewards, whereas combining TRD with saliency map algorithms can explain a particular future reward's regions of importance in decision-making.\nOutside XRL, researchers have explored non-scalar variants of the Q-value, primarily for improving performance. Bellemare et al. [5] proposed C51, a training algorithm that learns the distribution of cumulative rewards rather than just the expectation, achieving state-of-the-art performance in Atari. Our work differs as we propose decompose the Q-value into the expected reward for future timesteps rather than the probability distribution over all future rewards. Furthermore, we propose new explanatory applications that are facilitated by TRD."}, {"title": "3 Preliminaries", "content": "Before we present TRD, we provide sufficient technical detail on methods we build upon: Markov Decision Processes to mathematically describe TRD; Deep Q-learning for learning Q-value functions in complex environments; QDagger for learning with pretrained agents; and GradCAM for creating saliency map explanations.\nTo model a reinforcement learning environment, we use a Markov Decision Process [17] described by the tuple (S, A, R, P, T). These variables denote the set of possible states and actions (S and A respectively), the reward function (R(s, a)) given a state action s, a that is bounded to finite values, the transition probability (P(s'|s, a)) of the next state (s') given the prior state-action (s, a) and the termination condition (T(s)) that returns True if the state (s) is a goal state. For simplicity, following Sutton and Barto [24], we denote Si, Ai and Ri as the state, action and reward received for timestep i.\nGiven an environment, we wish to learn a policy \u03c0 that maximises its cumulative rewards over an episode. Furthermore, to incentivise the agent to collect rewards sooner, we apply an exponential discount factor (\u03b3\u2208 [0, 1)). For a policy, \u03c0, we may define the expected sum of future rewards in terms of the Q-value, $q_{\\pi}(s, a)$, or the state-value, $v_{\\pi}(s)$, functions, Eqs. (1) and (2) respectively.\n$\\begin{aligned} q_{\\pi}(s, a) & = \\mathbb{E}\\left[\\sum_{n=0}^{\\infty} \\gamma^{n} R_{t+n} \\mid S_{t}=s, A_{t}=a\\right] \\\\ v_{\\pi}(s) & = \\mathbb{E}\\left[\\sum_{n=0}^{\\infty} \\gamma^{n} R_{t+n} \\mid S_{t}=s\\right] \\end{aligned}$     (1)\n     (2)\nTo learn an optimal policy, agents can select actions that maximise the Q-value for a given state. Using this, Watkins and Dayan [27] proposed iteratively minimising the error between the predicted Q-value for a state-action and a bootstrapped target Q-value using the state-action's resultant reward plus the maximum Q-value in the next timestep (Eq. (3)), referred to as Q-learning. Importantly, given initial conditions and an infinite number of iterations, [27] proved Q-learning would converge to the optimal policy.\n$\\mathcal{L}_{Q}(D)=\\mathbb{E}_{(s, a, R, s') \\sim D}\\left[\\left(q_{\\pi}(s, a)-\\Upsilon_{\\text {target}}\\right)^{2}\\right]$   (3)\n$\\Upsilon_{\\text {target}}=R+\\gamma \\max _{a' \\in A} \\hat{q}\\left(s', a'\\right)$   (4)\nThis was extended by Mnih et al. [16] to use neural networks, referred to as Deep Q-learning (DQN) for a general RL algorithm that achieved state-of-the-art performance in image-based environments."}, {"title": "4 Temporal Reward Decomposition", "content": "As described in Section 1, due to the scalar output of future reward estimators (i.e., Q-value and state-value functions), their reward composition cannot be known, preventing understanding when and what future rewards the agent expects to receive. We, therefore, propose a novel future reward estimator (Eqs. (6)), referred to as Temporal Reward Decomposition (TRD) that predicts an agent's next N expected rewards. Furthermore, we prove its equivalence to scalar future reward estimators and provide a bootstrap-based loss function to learn the estimator (Eq. (15)). For consistency, all equations in this section are for the Q-value with state-value equations in Appendix A.\nBefore defining our TRD-based future reward estimators, to prove their equivalence to scalar future reward estimators (Eq. (7)), we first prove that the expected sum of future rewards is equivalent to the sum of expected future rewards enabling the decomposition of rewards in Eq. (6): Theorem 1.\u00b9\n$\\mathbf{q}_{(\\pi)}^{TRD}(s, a)=\\begin{bmatrix} \\mathbb{E}_{\\pi}[R_{t} \\mid S_{t}=s, A_{t}=a] \\\\ \\mathbb{E}_{\\pi}[R_{t+1} \\mid S_{t}=s, A_{t}=a] \\\\ \\vdots \\\\ \\mathbb{E}_{\\pi}[\\gamma^{N-1} R_{t+N-1} \\mid S_{t}=s, A_{t}=a] \\\\ \\mathbb{E}_{\\pi}[\\sum_{i=N}^{\\infty} \\gamma^{i} R_{t+i} \\mid S_{t}=s, A_{t}=a] \\end{bmatrix}$  (6)\n$\\sum_{i=0}^{N} \\mathbf{q}_{(\\pi)_{i}}^{T R D}(s, a)=q_{\\pi}(s, a) \\qquad \\forall s \\in S, \\forall a \\in A$  (7)\nUsing the notation in Section 3, we propose Eq. (6) that outputs a vector of the next N expected rewards with the last element being equal to the cumulative sum of expected rewards from N to \u221e. Each element i refers to the expected reward in t + i timesteps with the final element being the sum of rewards beyond N timesteps. Using Theorem 1, Eq. (6) is provably equivalent to the scalar Q-value by summing over the array elements (Eq. (7)) through expanding Eq. (11) with N+1 expectations. Critically, this equivalence is not reversible such that given a scalar Q-value, Eq. (6) cannot be known.\nTheorem 1. Given a states and action a, the expected sum of rewards is equal to the sum of expected rewards, more precisely $\\mathbb{E}_{\\pi}[\\sum_{i=0}^{\\infty} \\gamma^{i} R_{t+i} \\mid S_{t}=s, A_{t}=a]=\\sum_{i=0}^{\\infty} \\mathbb{E}_{\\pi}[\\gamma^{i} R_{t+i} \\mid S_{t}=s, A_{t}=a]$ for all $s \\in S$ and $a \\in A$.\nProof.\n$\\begin{aligned} & \\mathbb{E}_{\\pi}\\left[\\sum_{i=0}^{\\infty} \\gamma^{i} R_{t+i} \\mid S_{t}=s, A_{t}=a\\right] \\quad \\text { (8) } \\\\ = & \\mathbb{E}_{\\pi}\\left[R_{t}+\\sum_{i=1}^{\\infty} \\gamma^{i} R_{t+i} \\mid S_{t}=s, A_{t}=a\\right] \\quad \\text { (9) } \\\\ = & \\mathbb{E}_{\\pi}\\left[R_{t} \\mid S_{t}=s, A_{t}=a\\right]+\\mathbb{E}_{\\pi}\\left[\\sum_{i=1}^{\\infty} \\gamma^{i} R_{t+i} \\mid S_{t}=s, A_{t}=a\\right] \\quad \\text { (given LoE) }\\text { (10) } \\\\ = & \\mathbb{E}\\left[R_{t} \\mid S_{t}=s, A_{t}=a\\right]+\\mathbb{E}_{\\pi}\\left[\\gamma R_{t+1} \\mid S_{t}=s, A_{t}=a\\right]+\\mathbb{E}_{\\pi}\\left[\\sum_{i=2}^{\\infty} \\gamma^{i} R_{t+i} \\mid S_{t}=s, A_{t}=a\\right] \\quad \\text { (11) } \\\\ = & \\sum_{i=0}^{\\infty} \\mathbb{E}_{\\pi}\\left[\\gamma^{i} R_{t+i} \\mid S_{t}=s, A_{t}=a\\right] \\quad \\text { (12) } \\end{aligned}$\nImplementing TRD within a deep RL agent's future reward estimator requires two primary changes. The first is increasing the neural network output by N+1; i.e., the size of Eq. (6) for predicting the next N future rewards. The second is the loss function (Eq. (15)) for the network to learn Eq. (6). Additionally, as the network now outputs a vector of future rewards rather than a scalar, for action selection and other applications, $q_{\\pi}$ can be recovered by summing across vector elements before being applied as normal (Eq. (7)). Appendix B includes pseudocode for implementing the loss function, and the associated GitHub repository\u00b2 contains the implementation of a TRD-modified DQN training algorithm using Gymnasium [25].\nFor long-horizon environments where an agent may take hundreds or thousands of actions, TRD is limited in scale as the number of predicted rewards scales linearly with the number of output neurons. We therefore propose an alternative approach to preserve the temporal distance that can be explained using a fixed number of output neurons. Rather than each vector element predicting an individual reward, Eq. (13) groups rewards in each vector element; e.g., for pair grouping $[R_{t}+R_{t+1}, R_{t+2}+R_{t+3}, ...]$. This approach, denoted $\\mathbf{q}_{(\\pi)}^{T R D}\\omega$ for the reward grouping size, scales linearly with the number of future rewards by w for a fixed N such that the total number of predicted rewards is N \u00b7 w. Importantly, like Eq. (6), Eq. (13) is equivalent to the Q-value by summing across elements (Eq. (14)) using N \u00b7 w + 1 expansions of Eq. (11). Additionally, for w = 1, Eq. (13) is equivalent to Eq. (6) and implementation only requires utilising an N-step [24] experience replay buffer to compute the sum of the first w rewards and the next observation in w timesteps.\nAs a result, N and w present a trade-off between the reward vector size (N) and precise knowledge of each timestep's expected reward (w). For example using Figure 1, if w = 2 and N = 2 then the $q^{T R D}$ for moving up is [0, 0, 3.26] as [0+0, 0+0, 3.26] and moving down is [0.95, 1.76, 0.81] as [0+ 0.95, 0.90 + 0.86, 0.81]. Furthermore, to predict, for example, the next 30 rewards, N = 30, w = 1 and N = 6, w = 5 are both valid parameters. We explore the impact of these parameters on training in Section 5.\nThrough experiment, we found that converting $q^{T R D}$ to $q_{\\pi}$ by summing over elements (Eq. (7)), then using the scalar loss function (Eq. (3)) does not converge to $q^{T RD}$. Therefore, based on the Q-learning loss function (Eq. (3)), we define a novel element-wise mean squared error of reward vectors (Eq. (15)) where $a'$ denotes the optimal next action (arg maxae\u0391\u03a3$q^{T R D}$(st+w, a)) and we use the following notation to index an element of the reward vector:\n$\\begin{aligned} & \\mathbf{q}_{(\\pi)}^{T R D} \\omega(s, a)=\\begin{bmatrix} \\mathbb{E}_{\\pi}\\left[R_{t} \\mid S_{t}=s, A_{t}=a\\right]+\\ldots+\\mathbb{E}_{\\pi}\\left[\\gamma^{\\omega-1} R_{t+\\omega-1} \\mid S_{t}=s, A_{t}=a\\right] \\\\ \\mathbb{E}_{\\pi}\\left[\\gamma^{\\omega} R_{t+\\omega} \\mid S_{t}=s, A_{t}=a\\right]+\\ldots+\\mathbb{E}_{\\pi}\\left[\\gamma^{2 \\omega-1} R_{t+2 \\omega-1} \\mid S_{t}=s, A_{t}=a\\right] \\\\ \\vdots \\\\ \\sum_{i=(N-1) \\omega}^{N \\omega} \\mathbb{E}_{\\pi}\\left[\\gamma^{i} R_{t+i} \\mid S_{t}=s, A_{t}=a\\right] \\\\ \\mathbb{E}_{\\pi}\\left[\\sum_{i=N}^{\\infty} \\gamma^{i} R_{t+i} \\mid S_{t}=s, A_{t}=a\\right] \\end{bmatrix} \\quad(13) \\\\\\ & \\sum_{i=0}^{N \\omega} \\mathbf{q}_{(\\pi)_{i}}^{T R D \\omega}(s, a)=q_{\\pi}(s, a) \\qquad \\forall s \\in S, \\forall a \\in A(14) \\\\\\ & \\mathcal{L}_{T R D}=\\mathbb{E}_{\\left(s_{t}, a_{t}, R_{t+i}, s_{t+\\omega}\\right) \\sim D}\\left[\\begin{array}{c} \\left(q_{\\pi_{0}}^{T R D}\\left(s_{t}, a_{t}\\right)-\\sum_{i=0}^{\\omega} R_{t+i}\\right)^{2} \\\\ \\left(q_{\\pi_{1}}^{T R D}\\left(s_{t}, a_{t}\\right)-\\gamma^{\\omega} q_{\\pi_{0}}^{T R D}\\left(s_{t+\\omega}, a^{\\prime}\\right)\\right)^{2} \\\\\\ \\left(q_{\\pi_{2}}^{T R D}\\left(s_{t}, a_{t}\\right)-\\gamma^{\\omega} q_{\\pi_{1}}^{T R D}\\left(s_{t+\\omega}, a^{\\prime}\\right)\\right)^{2} \\\\\\ \\vdots \\\\\\ \\left(q_{\\pi_{N}}^{T R D}\\left(s_{t}, a_{t}\\right)-\\gamma^{\\omega} q_{\\pi_{N-1}}^{T R D}\\left(s_{t+\\omega}, a^{\\prime}\\right)\\right)^{2} \\\\\\ \\left(q_{\\pi_{N+1}}^{T R D}\\left(s_{t}, a_{t}\\right)-\\left(q_{\\pi_{N}}^{T R D}\\left(s_{t+\\omega}, a^{\\prime}\\right)+q_{\\pi_{N+1}}^{T R D}\\left(s_{t+\\omega}, a^{\\prime}\\right)\\right)\\right)^{2} \\end{array}\\right]  (15) \\\\\\ & \\begin{array}{l} q_{\\pi_{0}}^{T R D}(s, a)=\\mathbb{E}_{\\pi}\\left[R_{t} \\mid S_{t}=s, A_{t}=a\\right] \\\\ q_{\\pi_{1}}^{T R D}(s, a)=\\mathbb{E}_{\\pi}\\left[\\gamma R_{t+1} \\mid S_{t}=s, A_{t}=a\\right] \\\\\\ q_{\\pi_{N}}^{T R D}(s, a)=\\mathbb{E}_{\\pi}\\left[\\gamma^{N-1} R_{t+N-1} \\mid S_{t}=s, A_{t}=a\\right] \\\\\\ q_{\\pi_{N+1}}^{T R D}(s, a)=\\mathbb{E}_{\\pi}\\left[\\sum_{i=N}^{\\infty} \\gamma^{i} R_{t+i} \\mid S_{t}=s, A_{t}=a\\right] \\end{array}  (16) \\\\  (17) \\\\\\  (18) \\\\\\  (19 ) \\end{aligned}"}, {"title": "5 Retraining Pretrained Agents for TRD", "content": "The goal of Temporal Reward Decomposition (TRD) is to provide information about an agent's expected future rewards over time so that we can use this information to better understand its behaviour. For this to be practically effective, TRD agents should be capable of achieving performance similar to their associated base RL agent. In this section, therefore, we evaluate the performance of DQN agents [16] that incorporate TRD for a range of Atari environments [4] and assess the impact of TRD's two hyperparameters on training: reward vector size, N; and reward grouping, w.\nWe conduct hyperparameter sweeps across each independently, varying N, w, and Nw, across three Atari environments (Breakout, Space Invaders and Ms. Pacman), each containing different reward functions. To account for variability in training, we repeat our hyperparameter sweeps three times. The training hyperparameters and hardware used in training, along with the agent's final scores, are presented in Appendix B. Training scripts and final neural network weights for all runs are provided in the associated GitHub repository.\nRather than training agents from scratch for these environments, we use open-sourced pretrained Atari agents [10] and the QDagger training workflow [2], described in Section 3.\nUsing periodic evaluation on the same ten seeds, Figure 2 plots the teacher normalised interquartile mean [1] of the episodic reward. We find that all three hyperparameter sweeps enable the agent to approach the pretrained (teacher) agent's score with neither parameter having a significant detrimental impact. Only the offline training for a constant temporal distance (N. w = 24) do the agents with smaller values of w showcase greater initial performance, but this difference is resolved during the online training stage.\nInterestingly, for the sweep of N, we found no degradation in performance, which was unexpected as we believed that larger values of N would require more training to reach the same performance. As a result, in Section 6, we trained agents with N=40, w=1. Further work is required to understand if these performance curves hold for larger values of N and for more complex environments or agents.\nTo verify that our TRD loss function (Eq. (15)) converges to a policy that is similar to the pretrained agent's scalar Q-value. Figure 3 plots the mean squared error of the Q-values for both pretrained DQN agents and TRD agents during training. We find all parameters get close to the pretrained agent's Q-value with w=1 being an important factor.\nRegarding the computation impact of incorporating TRD, we found that our QDagger+TRD DQN agents took \u2248 10% fewer steps per second than our base DQN agents, 248 to 274 steps per second, respectively. This performance will be jointly caused by QDagger requiring an additional forward pass from the teacher agent and TRD using a larger network output and a more complex loss function."}, {"title": "6 Explaining an Agent's Future Beliefs and Decision-Making", "content": "We now present three novel explanation mechanisms using future expected rewards: understanding what rewards the agent expects to receive and when, and their confidence in this prediction; visualising an observation feature's importance for predicting rewards at near and far timesteps; and a contrastive explanation using the difference in future rewards to understand the impact of different actions choices (Sections 6.1, 6.2, and 6.3 respectively). We showcase these applications using three different Atari environments with more examples in Appendices C, D, and E. All agents were retrained DQN agents incorporating TRD using N=40 and w=1.\n6.1 What Rewards to Expect and When?\nFor environments with complex transition dynamics or reward functions such as Atari, understanding how an agent maximises its future rewards or predicting what rewards it will receive and when is not possible, unlike with the toy example illustrated in Figure 1. We show here how a TRD agent's predicted future rewards supply this information, presenting an important explanatory perspective for understanding agent decisions. Furthermore, for environments with binary reward functions (i.e., where the rewards are either zero or a constant value) the agent's expected reward can be further decomposed into the probability of the reward function components. Atari uses integer rewards and DQN agents clip rewards to -1 to 1, and so for these examples the agent's probability of collecting a reward is equivalent to the reward's expectation.\nFigures 4 and 5 plot the agent's expected rewards over the next 40 timesteps for the observation on the left. As w=1, the discount factor is constant for each predicted timestep, and so we factor it out, leaving just the expected reward. Without domain knowledge of each environment and its reward functions, we can observe from the expected rewards plots that the agent expects periodic non-zero rewards every 8 to 9 timesteps for Space invaders and every 15 timesteps for Breakout. Additionally, considering that the expected rewards (for these environments) are equivalent to the agent's confidence (probability) in receiving a reward for a particular timestep, users can infer that the agent's confidence reduces over time for the specific timestep that the agent will receive a reward. As such, for space invaders, the agent has high confidence for the close timesteps (t+6 and t+15) with the expected rewards for the third and fourth rewards being distributed across several timesteps (t+23 to t+24 and t+30 to t+40).\nFurther, utilising domain knowledge of each environment, Figures 4 and 5 correlate with our understanding as agents can only shoot aliens or break bricks periodically. Additionally, as the policy is stochastic due to epsilon-greedy action selection and with randomness in the environment, the uncertainty of timesteps far in the future is unsurprising and matches with human expectations.\nBuilding on the two figures, we can generate videos of the agent's expected rewards across entire episodes plotting the expected reward for each observation. Example videos are provided in the associated"}, {"title": "6.2 What Observation Features are Important?", "content": "Understanding the areas of an input that have the greatest impact on a neural network is a popular technique for generating explanations, called saliency maps. These allow users to visualise what features of an observation most influence an agent's decision. With access to an agent's beliefs about its future expected rewards, TRD provides novel saliency map opportunities to understand how the agent's focus with respect to an observation varies.\nUtilising GradCAM [20] (a popular saliency map algorithm described in Section 3), we can select individual expected rewards as the output to discover its feature importance. Figure 6 plots an Atari Breakout observation and the normalised feature importance for the expected reward of the next timestep (t+1) and the most distance expected reward (t+40) along with their normalised difference. The feature importance plots highlight areas of focus (red), influencing its decision and ignored areas (blue). We find that the agent's focus on the ball and bricks vary depending on how far in the future a reward is predicted. For the t+1 feature importance, the agent is highly focused (shown in red) on the ball in the centre. In comparison, for t+40, the agent has a greater focus on the bricks than the ball. Using domain knowledge of the environment validates human expectations as the number of bricks left and their position will have greater long-term importance to the agent than the ball. This difference is highlighted when subtracting the feature importance of t+1 from t+40 such that the ball's importance is significantly lower (shown in blue) and the bricks have relatively greater importance (shown in red).\nTo help visualise this change in an observation feature's importance across each predicted future reward, we provide a video of Figure 6 within the associated GitHub repository. Additionally, we provide a video of an episode plotting the first and last predicted reward's feature importance for each timestep. Like visualising an agent's expected reward in Section 6.1, Figure 6 and videos can help researchers and developers understand in what context a feature has importance for an agent. Previously, it was only possible to understand a feature's importance to predict the agent's total reward, whereas TRD provides us with the ability to investigate the importance of features in a more granular way."}, {"title": "6.3 What is the Impact of an Action Choice?", "content": "Within an environment, there are often multiple (possibly similar) paths to complete a goal with humans interested in understanding the differences between them (e.g., Figure 1). Contrastive explanations are a popular approach to understanding the reasons for taking one decision over another. In our case, this is the choice between two alternative actions in some state [14]. With the future expected rewards, TRD provides additional information to compare and contrast states and actions using what rewards the agent expects to receive and when along different paths. In this section, we show how simple explanations only using the timestep-wise difference in expected rewards can help understand an action's impact on an agent's future rewards.\nFigure 7 shows the expected reward for taking no action (noop) and firing and the differences between the expected reward for noop and firing in the Atari Seaquest environment. The right-hand side figure shows that the difference in future rewards produces a positive and negative spike after which the expected rewards converge. We can infer from these spikes that if the agent fires rather than noop then there is a more immediate reward, whereas if the agent waits, taking no action, the reward is delayed resulting in a later spike. Crucially, this difference in reward outcomes is resolved afterwards causing no long-term difference in the agent's expected rewards. Using domain knowledge, we can assume that this means if the agent doesn't fire in this timestep, it will most likely fire in the following timestep or soon after, thus receiving a slightly delayed reward.\nCollectively, with the explanations from Sections 6.1 and 6.2, contrastive explanations highlight the consequences of different actions on an agent's future rewards."}, {"title": "7 Conclusion", "content": "Temporal Reward Decomposition (TRD) is a novel reward estimator that is equivalent to scalar future reward estimators that can uniquely reveal additional information about deep reinforcement learning agents. We have shown that pretrained Atari agents can be efficiently retrained to incorporate TRD with minimal impact on performance. Furthermore, we have showcased three novel explanatory mechanisms enabled by TRD, demonstrating how these can aid researchers and developers understanding agent behaviour in complex environments such as the three Atari environments considered here.\nWe can ask \"What rewards to expect and when?\" by predicting rewards numerous timesteps into the future and the confidence the agent has in their prediction (Section 6.1). We can ask \"What observation features are important?\" by revealing how an agent's focus changes depending on the immediacy of the reward predicted (Section 6.2). Lastly, we can as \"What is the impact of an action choice?\" by revealing the difference in future expected rewards for two alternative actions (Section 6.3).\nTRD can be extended in various ways to better explain an agent's future rewards. Incorporating prior decomposition approaches such as Juozapaitis et al. [11] to explain the future expected rewards of different reward components is a clear option for future research. Further, each reward could be modelled as a probability distribution, decomposing the expectation of a reward for a timestep [5]. A further avenue for future research is that the linear relationship between future reward estimators and TRD (Eq. (7)) may be exploited for more efficient training."}, {"title": "A State-value based Temporal Reward Decomposition", "content": "Section 4 outlines the Temporal Reward Decomposition for the Q-value only for consistency. Therefore, in this Appendix, we provide the equivalent Temporal Reward Decomposition for the state-value with theorem 2 providing the equivalent given only a state s, Eqs. (25) and (26) for the state-value based reward vector and Eq. (28) for the loss function.\nTheorem 2. Given a states, the expected sum of rewards is equal to the sum of expected rewards, more precisely $\\mathbb{E}_{\\pi}\\left[\\sum_{i=0}^{\\infty} \\gamma^{i} R_{t+i} \\mid S_{t}=s\\right", "S_{t}=s\\right": "."}, {"S_{t}=s\\right": "quad(20) \\\\\\ = & \\mathbb{E}_{\\pi}\\left[R_{t}+\\sum_{i=1}^{\\infty} \\gamma^{i} R_{t+i} \\mid S_{t}=s\\right"}, {"S_{t}=s\\right": "mathbb{E}_{\\pi}\\left[\\sum_{"}]}