{"title": "GPD-1: Generative Pre-training for Driving", "authors": ["Zixun Xie", "Sicheng Zuo", "Wenzhao Zheng", "Yunpeng Zhang", "Dalong Du", "Jie Zhou", "Jiwen Lu", "Shanghang Zhang"], "abstract": "Modeling the evolutions of driving scenarios is important\nfor the evaluation and decision-making of autonomous driv-\ning systems. Most existing methods focus on one aspect of\nscene evolution such as map generation, motion prediction,\nand trajectory planning. In this paper, we propose a unified\nGenerative Pre-training for Driving (GPD) model to ac-\ncomplish all these tasks altogether without additional fine-\ntuning. We represent each scene with ego, agent, and map\ntokens and formulate autonomous driving as a unified to-\nken generation problem. We adopt the autoregressive trans-\nformer architecture and use a scene-level attention mask to\nenable intra-scene bi-directional interactions. For the ego\nand agent tokens, we propose a hierarchical positional tok-\nenizer to effectively encode both 2D positions and headings.\nFor the map tokens, we train a map vector-quantized au-\ntoencoder to efficiently compress ego-centric semantic maps\ninto discrete tokens. We pre-train our GPD on the large-\nscale nuPlan dataset and conduct extensive experiments to\nevaluate its effectiveness. With different prompts, our GPD\nsuccessfully generalizes to various tasks without finetuning,\nincluding scene generation, traffic simulation, closed-loop\nsimulation, map prediction, and motion planning.", "sections": [{"title": "1. Introduction", "content": "Autonomous driving simulators [1, 4, 9, 11, 27, 28, 34] play\na crucial role in developing and validating driving systems,\nenabling safe testing across various driving scenarios, in-\ncluding perception [22, 30, 36], motion prediction [15, 39,\n49], and trajectory planning [6, 7, 21, 24, 31, 46, 47].\nTypical components of the driving simulators can in-\nclude scene generation, traffic simulation, closed-loop sim-\nulation, and motion planning. Particularly, recent ad-\nvancements in BEV (bird's eye view) representations have\ndemonstrated the feasibility of using simulators to replicate\nreal-world driving conditions and challenges [8]. Such sim-\nulators have become essential for testing complex behav-\niors, understanding interaction dynamics, and ensuring ro-\nbustness against potential failures, thus contributing to safe\nand reliable autonomous driving systems. However, exist-\ning methods for scene evolution in autonomous driving are\ngenerally specialized and limited to specific aspects of the\nsimulator, such as map generation [14, 18, 29], motion pre-\ndiction [8, 10, 40, 45], or trajectory planning [8]. Consid-\nering these approaches typically focus on one isolated task,\nthere exists no unified framework that integrates these as-\npects into a cohesive model for holistic simulation. For ex-\nample, the recent method SLEDGE [8] is only trained to\nreconstruct single frames and lacks control interfaces, lim-\niting its ability to support various downstream tasks. They\ncannot fully leverage the scene-level information including\nthe temporal evolution across scene elements and the inter-\nactions between dynamic agents and map elements, making\nit challenging to generalize to different downstream tasks.\nIn this paper, we propose to unify these elements with\na Generative Pre-training for Driving (GPD-1) model. We\nencode the map, agents, and ego vehicle as a unified set of\ntokens, enabling us to formulate the scene evolution as the\ngenerative prediction of the scene tokens. We adopt an au-\ntoregressive transformer architecture with a scene-level at-\ntention mask that enables bi-directional interactions within\nthe scene, allowing the model to efficiently capture depen-\ndencies among the ego, agent, and map tokens. For ego\nand agent tokens, we propose a hierarchical positional to-\nkenizer, which effectively encodes the BEV positions and\nheadings. The positional tokenizer transforms the continu-\nous agent positions into discrete tokens, which significantly\nreduces the noise in feature space. For map tokens, we\nleverage a vector-quantized autoencoder (VQ-VAE) [42] to\ncompress ego-centric semantic maps into discrete tokens.\nBy representing map information through discrete tokens,\nwe eliminate the complexity of predicting continuous map\ncoordinates, simplifying the learning process and enhanc-\ning generalization. To demonstrate the effectiveness of our\nGPD-1 model, we conduct a series of challenging experi-\nments across diverse tasks. Our model, as shown in Fig-\nure 1, without any fine-tuning, is capable of performing\nscene generation, traffic simulation, closed-loop simulation,\nand motion planning. Specifically, scene generation in-\nvolves initializing a scene and allowing the model to gen-\nerate agent, map, and ego information smoothly. Traffic\nsimulation provides the ground-truth map and initial agent\nstates, with the model predicting the evolution of subse-\nquent frames. Closed-loop simulation, given a ground-truth\nmap and ego trajectory, allows the model to dynamically\nadapt agent trajectories in response to ego movements. Fi-\nnally, for motion planning, the model generates ego trajec-\ntories in response to the provided agent and map informa-\ntion. With further fine-tuning, GPD-1 can achieve state-of-"}, {"title": "2. Related Work", "content": "Discrete Tokens for Autonomous Driving. Tokenized dis-\ncrete representations have become popular for capturing\ncomplex spatial layouts with efficiency and interpretability.\nVQ-VAE [42] introduced a codebook mechanism to con-\nstruct an encoder-decoder architecture within a tokenized\ndiscrete latent space, enabling richer, more compact rep-\nresentations of high-dimensional data. VQ-VAE-2 [38]\nfurther enhanced the framework with hierarchical quan-\ntized codes and autoregressive priors. Following this direc-\ntion, models like VQ-GAN [12], DALL-E [37], and VQ-\nDiffusion [16] map inputs into discrete tokens correspond-\ning to codebook entries, allowing simplified yet expres-\nsive representations. Recent works in visual pre-training\n[2, 35] employ similar tokenization strategies, using tokens\nto represent image patches and predicting masked tokens\nas a proxy task to enhance model robustness and versatil-\nity. To represent the map elements, recent methods on map\nreconstruction [32, 33] and end-to-end driving [24] encod-\ning each map element into a vectorized representation for\nmodeling, which ignores the scene-level structures.\nWe apply tokenizing to BEV-based autonomous driving\nscenarios and encode map features into discrete tokens. Our\nmethod addresses common issues in BEV modeling, such\nas computational inefficiencies and inconsistency in repre-\nsentations, by minimizing spatial noise and providing a uni-\nfied structure for map and agent information.\nData-Driven Autonomous Driving Simulation. Tradi-\ntional simulation techniques often involve replaying logged\ndriving data to emulate various driving conditions [4, 13,\n17, 25]. For instance, conventional simulators like nuPlan\n[4] rely heavily extensive driving logs to cover diverse sce-\nnarios. However, these simulations demand massive storage\ncapacities, making them resource-intensive and challenging\nfor broader accessibility. Also, these model-driven simula-\ntors require complicated rule-based modules for scene gen-\neration, agent behaviors, and rendering. To this end, data-\ndriven simulation methods are proposed for sensor render-\ning [23, 43, 44, 48], road network generation [14, 18, 29],\nand agent behavior prediction [8, 10, 40, 45]. For ex-\nample, SLEDGE [8] leverages generative models to simu-\nlate scenes with compact vectorized data, enabling efficient\nuse of storage without compromising on scenario diver-\nsity or complexity. While effective, they lack adaptability\nin dynamically modeling interactions between agents and\nthe surrounding map, limiting their application for reactive\ntasks. Differently, our framework aims to bridge this gap by\nincorporating a generative model capable of scene evolution\nand thus allows for interactive and flexible scene generation\nthat supports various downstream tasks."}, {"title": "3. Proposed Approach", "content": "3.1. 2D Map Scene Tokenizer\nA key aspect of autonomous driving is capturing spatial in-\nformation about the environment accurately and efficiently.\nTo achieve this, we employ a 2D Map Scene Tokenizer that\ntransforms complex, vector-based map representations into\ndiscrete tokens, which can be effectively modeled within a\ngenerative framework. This tokenizer is designed to sim-\nplify the continuous spatial features into a structured, dis-\ncrete format, enabling our model to incorporate map infor-\nmation seamlessly alongside agent and ego tokens.\nMap Vector Rasterization. The map data consists of\nvector representations of lines, each defined by multiple\npoints. Directly encoding these vectors poses challenges\ndue to the lack of spatial relationships within the vector for-\nmats. To resolve this, we rasterize the map vectors into a 2D\ncanvas centered at the ego vehicle and only represent the im-\nmediately visible region. This rasterized map is represented\nas a binary image $I \\in \\mathbb{R}^{H \\times W}$, where the interpolated line\nsegments and background regions are marked as 1 and 0.\nFeature Extraction and Quantization. To efficiently\nrepresent the map data, we use a vector-quantized autoen-\ncoder (VQ-VAE) [42] that converts continuous map fea-\ntures into discrete tokens. The rasterized map $I$ is first\nencoded by ResNet-50 [20] into compact features $Z_c\n\\in \\mathbb{R}^{H/d \\times W/d \\times C}$, where $H = W = 256$, $d$ is the downsam-\npling factor, and $C$ is the feature dimension. For quanti-\nzation, we introduce a codebook $V \\in \\mathbb{R}^{K \\times D}$ with $K$ dis-\ncrete codes, each capturing a high-level feature of the scene.\nEach map feature $\\tilde{z}_{ij}$ in $Z_c$ is quantized by mapping it to the\nnearest code in $V$:\n$Q(z_c) = \\underset{v_k \\in V}{\\text{arg min}} ||z_{ij} - v_k||_2$,\nwhere $||.||_2$ denotes the L2 norm. Here, $Q(z_c)$ represents the\nquantization function that maps the continuous latent vector\n$z_c$ to its nearest neighbor in the codebook $V$, resulting in the\ndiscrete representation $z_q$. These tokens provide a compact\nand consistent representation of the map information and\nencode spatial structure while reducing model complexity.\nReconstruction with Discrete Queries. We follow the\nDETR [5] decoding approach defined in SLEDGE [8] to\ndecode the quantized map tokens into the Vector Lane Rep-\nresentation as outlined in SLEDGE. For aligning the gen-\nerated and ground-truth map lines, we also adopt the Hun-\ngarian algorithm for matching, using the same supervision\nloss setup as SLEDGE to ensure accurate map reconstruc-\ntion. The map tokenizer transforms vector-based maps into\ncompact discrete space, encoding essential spatial relation-\nships. This representation facilitates the modeling of dy-\nnamic scene elements within the generative framework."}, {"title": "3.2. Agent Tokenizer", "content": "In autonomous driving simulations, accurately representing\ndynamic agents within the scene is essential for realistic and\ncoherent scene generation. To efficiently encode agent data,\nwe introduce a hierarchical positional tokenizer to capture\nboth spatial (2D position) and angular (heading) informa-\ntion. This tokenizer enables the model to represent complex\nagent dynamics while reducing the feature space, making\nthe generative process more manageable.\nMulti-Level Quantization. Each agent coordinate, de-\nnoted as a general variable p (e.g., x, y, or heading), under-\ngoes multi-level quantization across N hierarchical levels,\nrepresented by a set of thresholds {$s_1,s_2,...,s_N$}, where\neach $s_i$ denotes a specific scale of granularity.\nFor the first level, the quantized value $q_1$ is calculated as:\n$q_1 = floor(\\frac{p}{s_1})$\nFor levels i > 1, the quantization is performed on the\nresidual after accounting for the previous levels:\n$q_i = floor(\\frac{p - \\sum_{j=1}^{i-1}q_js_j}{s_i})$\nThis iterative quantization ensures that each level cap-\ntures progressively finer details by focusing on the resid-\nual not captured by previous levels. The result is a set of\nN quantized values {$q_1,q_2,...,q_N$}, each representing the\ncoordinate at different levels of precision.\nPositional Embedding. After quantization, we incorpo-\nrate a fixed sinusoidal embedding to each quantized level,\ncapturing its relative position within the feature space. This\nsinusoidal encoding is based on the classic positional en-\ncoding introduced in Transformers [26], which provides\nspatial context and preserves positional relationships within"}, {"title": "3.3. Generative Transformer for Scene Modeling", "content": "In autonomous driving, the ability to model the evolution of\nan entire scene is essential for predicting dynamic interac-\ntions among agents and understanding future outcomes. We\nemploy an autoregressive transformer architecture to han-\ndle scene modeling, inspired by the sequential generation\nframework of GPT [3]. Our approach incorporates a scene-\nlevel attention mask that enables bi-directional interactions\namong tokens within each frame, allowing for a compre-\nhensive understanding of both spatial and temporal relation-\nships, illustrated in Figure 3.\nEach scene, corresponding to a single frame, consists of\na fixed number of map tokens and agent tokens. The map\ntokens originate from the 2D Map Scene Tokenizer as dis-\ncrete latent representations $z_q$ obtained via VQ-VAE, and\ntheir quantity is determined by the dimensionality of the\nlatent space. The agent tokens, produced by the agent to-\nkenizer, represent individual agents within the scene, with a\nfixed number assigned to each frame.\nSpatial and Temporal Embeddings. To provide the\nmodel with structured information about the spatial layout\nand temporal progression, we add learnable spatial and tem-\nporal embeddings. The spatial embedding associates each\ntoken with its role as either a map or agent token, ensuring\nthat the model understands the distinct functions of each el-\nement within the scene. The temporal embedding encodes\nthe sequence order across frames, capturing the progression\nof events over time. These embeddings allow the model to\nmaintain a consistent structure, where each frame is com-"}, {"title": "3.4. GPD-1: Generative Pre-training for Driving", "content": "Our Generative Pre-training for Driving (GPD-1) model\nuses a two-stage training process to build a robust founda-\ntion for autonomous driving simulations and planning tasks.\nWe first train the Map VQ-VAE latent tokenizer, adopt-\ning the L1 error for map line position and binary cross-\nentropy (BCE) to assess map line visibility, as defined in\nSLEDGE [8]. Additionally, to improve codebook stability\nand precision, we include the mean squared error (MSE)\nloss to encourage accurate quantization. This stage creates\na high-fidelity map latent space that accurately encodes spa-\ntial structure, forming a solid base for scene generation.\nIn the second stage, the trained map tokenizer is frozen\nand used to extract latent representations of the map for each\nframe, which serve as both inputs and ground truth for fur-\nther training. Cross-entropy (CE) loss is used to match gen-\nerated tokens with their correct codebook entries, ensuring\naccurate map reconstruction. We treat both ego and agent\ntokens equally, using smooth L1 loss to calculate positional\nerrors and BCE loss for binary classification of presence.\nThis structured training allows the model to capture both\nspatial and temporal scene dynamics, enabling consistent\nscene modeling across diverse scenarios.\nGPD-1 allows it to perform a wide array of downstream\ntasks without additional fine-tuning, demonstrating flexibil-\nity across critical autonomous driving applications.\nScene Generation: GPD-1 autonomously generates\ncomplete scenes by initializing a scene setup and predict-\ning the spatial and temporal evolution of agents, the ego\nvehicle, and map features. This task is essential for creating\ndiverse driving scenarios from minimal initial inputs.\nTraffic Simulation: By initializing the model with a\nground-truth map and initial agent states, GPD-1 accurately\npredicts how traffic evolves across frames. This simula-\ntion capability is crucial for evaluating and training au-\ntonomous driving models in dynamic environments, where\nunderstanding the flow of traffic is fundamental.\nClosed-Loop Simulation: Given a ground-truth map\nand ego trajectory, the model can dynamically adapt agent\nbehaviors in response to the ego vehicle's movements. This\nsetup aligns closely with the closed-loop interactive settings\nin the nuPlan Challenge [4], where agent reactions to the\nego behavior are generated through the model rather than\nrelying on conventional rule-based algorithms.\nMotion Planning: GPD-1 supports ego trajectory plan-\nning by generating routes in response to a given set of agent\nand map information. This planning capacity closely aligns\nwith practical autonomous driving needs, offering a data-\ndriven alternative to conventional planning methods.\nConditional Generation: GPD-1 can also handle con-\nditional generation, allowing users to define specific condi-\ntions such as initial agent trajectories, the number of agents,\nor vector-based map features. With these constraints, GPD-\n1 autonomously generates compatible scene evolutions, en-\nabling simulation of targeted, scenario-specific driving con-\nditions for fine-grained control.\nEnhanced Performance with Fine-Tuning. Fine-\ntuning on specialized datasets or specific task scenarios\nfurther enhances the performance of GPD-1, especially in\ncomplex planning tasks. Fine-tuning enables GPD-1 to gen-\nerate extended, precise trajectories that meet the rigorous\nstandards of challenges such as the nuPlan Planning Chal-\nlenge, where both closed-loop and open-loop performance\nare critical for accurate trajectory prediction.\nThe generative pre-training equips GPD-1 with a flexi-"}, {"title": "4. Experiments", "content": "4.1. Datasets\nWe conducted extensive experiments on the nuPlan [4]\ndataset. nuPlan is a large-scale closed-loop planning bench-\nmark designed for long-term decision-making evaluation\nfor autonomous driving. It provides 1300 hours of driv-\ning data recorded from four different urban areas, which are\ndivided into 75 distinct scenario types with automated la-\nbeling tools. The data is collected with a vehicle with eight\ncameras providing a full 360\u00b0 horizontal field of view and a\nLiDAR sensor to obtain point cloud scans of the scenes.\n4.2. Experimental Settings\nWe employ the official evaluation metrics [4] to evalu-\nate the planning performance of our GPD-1, including"}, {"title": "5. Conclusion", "content": "In this paper, we have introduced Generative Pre-training\nfor Driving (GPD-1) for autonomous driving which models\nthe joint evolution of ego movements, surrounding agents,\nand scene elements. We employ a hierarchical agent to-\nkenizer and a vector-quantized map tokenizer to capture\nhigh-level spatial and temporal information, while an au-\ntoregressive transformer with scene-level attention predicts\nfuture scenarios across multiple driving tasks. Extensive re-\nsults demonstrate that GPD-1 effectively generalizes to di-\nverse tasks, such as scene generation, traffic simulation, and\nmotion planning, without additional fine-tuning. We believe\nthat GPD-1 represents a foundational step toward a fully in-\ntegrated, interpretable framework for autonomous driving."}, {"title": "A. Additional Implementation Details", "content": "Our training process consists of two stages: first, train-\ning a Map Tokenizer to encode single-frame images, and\nthen training the overall Generative Pre-training for Driv-\ning (GPD-1) model.\nMap Vector Rasterization. We model the map center-\nline within a 64 \u00d7 64 m rectangular region centered on the\nego vehicle. The map lines in this region are rasterized onto\na 256 \u00d7 256 pixel canvas, resulting in a binary (0/1) image.\nWe employ ResNet-50 [20] as the image encoder. For the\nVector Quantized Variational Autoencoder (VQVAE) [38],\nthe codebook size is set to 128, and the latent channel di-\nmension is also 128. This encodes the map image into to-\nkens of shape H \u00d7 W \u00d7 C = 8 \u00d7 8 \u00d7 128. The decoder\nand ground truth (GT) design follow the encoding strategy\nof SLEDGE [8].\nWe use the 100M-scene dataset from PlanTF [7] for both\ntraining and validation. During Map Tokenizer training, a\nrandom frame is sampled from each scene. Training is per-\nformed on 24 NVIDIA A800 GPUs with 80 GB memory\nover 41 hours, for 1000 epochs. The batch size is set to 64.\nThe AdamW optimizer is employed with a weight decay of\n0. The learning rate for the VQVAE codebook vectors is set\nto 1.5\u00d710-3, while the rest of the parameters use a learning\nrate of 3 \u00d7 10-4. A cosine annealing schedule is applied,\nwith a warmup period of 50 epochs.\nAgent Multi-Level Quantization Tokenization. For\nthe position component, we set the quantization intervals\n{$1,$2,...,SN} to {1, 0.01}. For the heading component,\nthe quantization intervals are set to {20, 1}.\nFor the Transformer decoder, we set the dimension to"}, {"title": "B. Additional Evaluation Metric Details", "content": "In this paper, we evaluate the performance of agent and ego\ngeneration using three metrics: Average Displacement Er-\nror (ADE), Final Displacement Error (FDE), and Colli-\nsion Rate (Coll.). For map generation, we adopt F1 Score\n(F1), Lateral L2 Distance (Lat.), and Chamfer Distance\n(Ch.)."}, {"title": "C. Additional Results", "content": "In Section 4, we present the results of different settings un-\nder the Test14-random [7] scenarios. For the Test14-hard\nscenarios, we report the results of Scene Generation, Traffic\nSimulation, Close-Loop Simulation, and Motion Planning,\nas shown in Table 5. We observe that the metrics across\ndifferent settings show a slight decrease, demonstrating the\nstrong generalization ability of our model.\nAs shown in Table 6, we present the results of map pre-\ndiction under the Test14-hard scenarios. It can be observed\nthat using agents and the ego as GT inputs achieves better\noverall performance compared to using only the ego while\nother agents remain invisible. This observation is contrary\nto the conclusion drawn in Test14-random. We believe this\nis because the hard scenario introduces more complex envi-\nronments, such as curves and intersections, where the map\nprediction benefits from the positional information of other\nagents. However, in simpler scenarios, such as Test14-\nrandom, the information from other agents might interfere\nwith map generation."}, {"title": "D. Video Demonstration", "content": "Figures 6 shows sampled images from the video demo il-\nlustrating the application of the GPD-1 model on the nu-\nPlan [4] validation set. In the accompanying video, we\ndemonstrate the performance of GPD-1 across five differ-\nent tasks, highlighting the effectiveness of our proposed\nmodel."}]}