{"title": "HUMT DUMT: Measuring and controlling human-like language in LLMs", "authors": ["Myra Cheng", "Sunny Yu", "Dan Jurafsky"], "abstract": "Should LLMs generate language that makes them seem human? Human-like language might improve user experience, but might also lead to overreliance and stereotyping. Assessing these potential impacts requires a systematic way to measure human-like tone in LLM outputs. We introduce HUMT and SOCIOT, metrics for human-like tone and other dimensions of social perceptions in text data based on relative probabilities from an LLM. By measuring HUMT across preference and usage datasets, we find that users prefer less human-like outputs from LLMs. HUMT also offers insights into the impacts of anthropomorphism: human-like LLM outputs are highly correlated with warmth, social closeness, femininity, and low status, which are closely linked to the aforementioned harms. We introduce DUMT, a method using HUMT to systematically control and reduce the degree of human-like tone while preserving model performance. DUMT offers a practical approach for mitigating risks associated with anthropomorphic language generation.", "sections": [{"title": "1 Introduction", "content": "Shneiderman (1993) famously critiqued \"the Humpty Dumpty syndrome\"-language that attributes human-like qualities to technology\u2014as it can mislead users, create confusion, and undermine their sense of humanity. These concerns have reverberated as anthropomorphism of LLMs, or the perception and framing of large language models (LLMs) as human-like entities, has become increasingly prevalent (Cheng et al., 2024). Anthropomorphism of LLMs has been linked to harms such as overtrust and overreliance (Salles et al., 2020; Chiesurin et al., 2023), reinforcing stereotypes (Bender, 2024), and problematic user behaviors like disclosing private information (Ischen et al., 2020) or forming emotional dependencies (Shteynberg et al., 2024)."}, {"title": "2 Related work on human-like LLMs", "content": "Decades of work in human-computer interaction have studied anthropomorphic machine-generated language, from Quintanar (1982); Shneiderman (1993) to more recent meta-analyses spanning responsiveness, agency, and social action (Emnett et al., 2024; Araujo, 2018). We similarly focus on linguistic anthropomorphism in system outputs. But prior work on measuring linguistic anthropomorphism is limited to a few features, such as personal pronouns or specific phrases (Shneiderman, 1993; Quintanar, 1982; Cohn et al., 2024). We build on works identifying different facets of human-likeness in LLMs (Glaese et al., 2022; Abercrombie et al., 2023; Li et al., 2024) to present a quantitative metric of human-like language."}, {"title": "3 Methods", "content": ""}, {"title": "3.1 HUMT", "content": "HUMT scores a text by comparing its probability when it is embedded as the complement of the verb \"say\" with animate versus inanimate subjects. For string s (representing an output from an LLM, or any text), with D = human-like tone, we measure the relative probability of D+ = {He said, She said} versus D\u00af = {It said} as the extent to which s is human-like. (D+ and D\u00af are phrase sets that align positively and negatively respectively with D.) We use an autoregressive LLM (GPT-2) to compute the relative log probability\n\\[TD(s) = log \\frac{PD+(s)}{PD-(s)}, where\\]\n\\[PD+(s) = \\frac{1}{n} \\sum_{i=1}^{n} \\sum_{w \\in D+}P(w + s[: 300]),\\]\nand similarly for PD-. That is, to account for noise in the computing of output probabilities, we compute each probability n = 100 times and take the average score as P(s). We also truncate the string s to 300 characters to avoid tokenization issues, a sufficient length for assessing implicit frames.\nFor example, consider s = \"Hello!\" We run GPT-2 n times to get the average probability of \"He said Hello!\", \"She said Hello!\" and \"It said Hello!\". Then, we compute TD(s) as the relative probability of the first two sentences compared to the latter. The resulting TD(s) > 0 reflects that, based on the training data of GPT-2, it is much more likely that a person says \u201cHello!\u201d rather than a non-human entity. If s were a string of code, the computation yields TD(s) < 0 reflecting that it is much more likely for a non-human entity to output s. More examples are in Table A1.\nBy comparing probabilities of in/animate pronouns, HUMT captures human-like tone by measuring whether the language is typical of a specific individual versus a more general or abstract source, i.e., human-like tone. This builds on theories distinguishing linguistic immediacy (orality) from distance (literacy) (Koch and Oesterreicher, 1985, 2012) and Biber (1991)'s broad differentiation of speech (more intimate, emotional) from writing (more abstract, impersonal).\nMethodologically, HUMT builds on previous work using masked language models (MLMs) to measure implicit frames (Card et al., 2022; Cheng et al., 2024). Note that while these works require specifying an entity (e.g. in the sentence \"The AI is my best friend\", Cheng et al. (2024) measure anthropomorphism for the term \u201cAI\u201d specifically) and thus are limited to measuring framing of explicit entities, our method is much more general because it captures implicit human-like tone of any text based on the inferred speaker without considering mentioned entities. Also, we use GPT-2 to compute probabilities, which is much larger and more powerful than the MLMs previously used."}, {"title": "3.2 SOCIOT", "content": "To test our first hypothesis about human-like language being associated with dimensions of social perception, we construct SOCIOT, a generalized form of HUMT, to measure these dimensions."}, {"title": "3.2.1 Dimensions of social perception", "content": "Scholars have linked dimensions of social perception such as social closeness (Quintanar, 1982; Glaese et al., 2022; Li et al., 2024) and warmth (Zhou et al., 2025) to downstream harms of anthropomorphism like overreliance/overtrust (Kim et al., 2024; Inie et al., 2024), emotional dependence (Contro and Brand\u00e3o, 2024; Laestadius et al., 2022), and challenging the sincerity of human expression (Porra et al., 2020). Others highlight that anthropomorphism reinforces gender stereotypes since human-like LLMs simultaneously reflect stereotypically feminine qualities (such as feminine names) and low-status, subservient roles (such as assistant personas) (Gruber and Benedikter, 2021; Yeon et al., 2023; Bender, 2024; Abercrombie et al., 2023). Building on the social psychological literature summarized below, we investigate these four potentially problematic social variables: warmth, status, social distance, and gender.\nWarmth is considered the primary dimension of the two-dimensional stereotype content model (SCM) on which stereotypes can be mapped (Fiske et al., 2002; Russell and Fiske, 2008). Warmth includes perceptions of sociability and morality. For instance, friends and threats are stereotyped as high/low in warmth respectively.\nCompetence, the secondary dimension of the SCM, relates to perceived ability or skill. Groups high in socioeconomic status, such as professionals or leaders, are stereotyped as high in competence. We do not provide a direct measure for competence because we find that it could not be distinguished from prompt topic; instead, we measure status, which is a reliable predictor for competence (Fiske et al., 2002) and strongly correlates to competence (average r = 0.9 in Durante et al. (2017)'s meta-analysis). Status can be conveyed through language, like via verbs that confer high or low-power to the subject (Sap et al., 2017).\nGoffman (1949) proposes social distance, the perceived closeness or detachment between interactors, as a key component of social perception. Linguistic markers like politeness and formality can signal unfamiliarity, while slang and casual language signal more closeness (Stephan et al., 2010).\nGender is a widely-studied component of social perception that is co-created by language: linguistic norms, such as expectations of what women say and how women are typically discussed, contribute to the social construction of gender by reifying gender hierarchies (Lakoff, 1973; Bigler and Leaper, 2015; Hoyle et al., 2019). West (1984) find that gender shapes perceptions more than other identity markers like class, and O'Barr and Atkins (2005) explore how stereotypically feminine language is synonymous with powerless language."}, {"title": "3.2.2 Computing SOCIOT", "content": "We compute TD for each of the dimensions D = warmth, status, social distance, and gender using Eqn. 1 with different phrase sets D+ and D\u00ae for each dimension (Table 2). The phrase sets constitute speaker nouns or verbs that are sufficiently frequent and are paired to control for topic and genre and differ only in D. For gender, we use the pair of pronouns \"she\u201d versus \u201che\u201d to represent speakers, as they are extremely common words that differ only in expressing gender. For social distance, we surface the most common implicit speakers of LLM outputs (i.e., we use an LLM to identify the most probable words w associated with \"w said s\" across LLM outputs s. Full details in App. A) and then choose terms that reflect varying levels of distance. Similarly, for warmth, for each common implicit speaker that reflects warmth in D+, we pair it by adding to D\u00af an implicit speaker that is similar in topic and genre but differs in warmth, like \"idol\" vs \"dictator.\" For status, we find that the implicit speakers are too topic-specific, such as different occupations. We instead use pairs from the lexicon of high- versus low-power verbs developed by Sap et al. (2017) that are similar in meaning but differ in status, like \"demand\" vs. \"ask.\""}, {"title": "3.3 Datasets", "content": "We measure HUMT and SOCIOT on various datasets that reflect how language models are trained and deployed. To compare human-written with LLM-generated texts, we use the Human ChatGPT Comparison Corpus (HC3) (Guo et al., 2023) and the Anthropic Persuasion Dataset (Durmus et al., 2024) (Persuasion), which contain human-written and LLM-generated responses to the same prompts. To assess the generalizability of HUMT and SOCIOT, we apply them to web data (the English C4 dataset (Soldaini et al., 2024)), which captures a variety of contexts beyond LLM use settings. Since LLMs are pre-trained on web data (Touvron et al., 2023), this also informs whether human-like tone emerges from pre-training or post-training.\nTo understand human-like tone and social perceptions in LLM outputs, we use five preference datasets spanning diverse use cases: three RLHF datasets and two usage datasets. The RLHF datasets-Stanford Human Preferences (SHP) (Ethayarajh et al., 2022), HH-RLHF (Bai et al., 2022), and UltraFeedback (UF) (Cui et al., 2024)-reveal preferences embedded in post-training processes. The usage datasets capture broader real-world preferences: the LMSys Chatbot Arena Conversations Dataset (LMSys) (Zheng et al., 2023) collects preferences from 14K IP addresses, and PRISM (Kirk et al., 2025b) from 1.5K participants across 75 countries, both comparing 20+ LLMs. Data examples are in Table 4."}, {"title": "3.4 Construct Validity of HUMT and SOCIOT", "content": "We validate that HUMT and SOCIOT capture human-like tone and social dimensions of perception using a human annotation study with four annotators (students who had familiarity with the project) to annotate 600 texts each: a random sample of 60 LLM outputs and 60 texts from the C4 dataset for each dimension (see Sec. 3.3), stratified by TD. For each text, annotators independently"}, {"title": "3.5 Experiments", "content": "To test H1, i.e., understand how linguistic dimensions of social perception align with human-like tone, we compute Pearson's correlation r between HUMT and SOCIOT on each dataset. Since multiple hypotheses are being tested, we apply the Benjamini-Hochberg procedure to adjust the p-values, using a = 0.001. To test H2, i.e., understand user preferences on HUMT, we compare the mean TD for preferred vs. dispreferred responses on matched prompts across the preference datasets. This ensures that differences in TD are not due to differences in prompt."}, {"title": "4 Results", "content": ""}, {"title": "Human-like tone: Humans > LLM Outputs > Web Data", "content": "As a sanity check, we compare the HUMT and SOCIOT scores of text written by humans versus LLMs (Figure 3). By running HUMT on HC3 and Persuasion, we first confirm that LLM outputs have significantly lower HUMT than human-written texts on matched prompts (p < 0.001). Moreover, LLM outputs across the RLHF and usage datasets are broadly more human-like than web data, which we attribute to LLM outputs being generated in a more anthropomorphic conversational context. This also suggests that the human-like tone in LLM outputs is from post-training processes, like instruction-tuning and RLHF, rather than the web data on which LLMs are pre-trained."}, {"title": "Linguistic Correlates", "content": "Having validated that HUMT and SOCIOT capture dominant notions of their respective construct, we next examine the more specific features they capture. Using LIWC-22, a well-validated lexicon of linguistic features for different psychological constructs (Pennebaker, 2011; Boyd et al., 2022), we compute t-test statistics comparing LIWC scores for texts in the highest versus lowest quartiles of TD scores (Table 3). For HUMT, the top associated features are function words including pronouns (especially \u201cI\u201d) and linguistic markers of warmth. For SOCIOT, the top features are emblematic of each dimension: texts that are warmer, more feminine, and more socially close (based on SOCIOT scores) have higher rates of social language, personal pronouns, and affect. Analytical language and language complexity are negatively associated with both HUMT and these dimensions of SOCIOT. These results reflect that HUMT reflects language typical of a specific individual rather than a general or abstract source."}, {"title": "4.1 H1: Correlations with social dimensions", "content": ""}, {"title": "4.2 H2: Users prefer lower HUMT", "content": "Our second hypothesis is that users may prefer less human-like tone because it is less deceptive and communicates information more clearly. We report our findings based on comparing mean HUMT.\nFirst, we find that less human-like responses are preferred overall. Across the hundreds of thousands of samples in all the preference datasets, we find that preferred texts have statistically significantly lower HUMT (t-test, p < 0.05) (Fig. 3 top right). In line with the correlations we discuss in the previous section, the preferred responses are also more socially distant, less feminine, less warm, and higher in status (Fig. 3 bottom right).\nTo interpret the difference in scores, recall that HUMT is a log likelihood. For instance, in PRISM, mean score 0.08 and 0.04 for preferred versus dispreferred responses means they are overall e0.08 ~ 1.08 and e0.04 ~ 1.04 times more likely to seem human-like respectively. Thus, the preferred responses are (1.08 \u2013 1.04)/1.04 = 4% less human-like. Fig. 3 is similarly labeled with the percent change between the dis/preferred responses. These differences are both statistically and conceptually significant: despite the many other factors that inform preference labels (quality, safety, clarity, etc.), preferred responses consistently exhibit lower human-like tone.\nPRISM in particular has the largest difference in HUMT. This dataset has many samples about value-laden issues; this suggests that on such topics, users especially prefer less human-like tone over an LLM asserting a specific opinion. More broadly, we find that preferences are somewhat topic-dependent. Mean HUMT scores vary based on the topic of the prompt. Using the topic clusters assigned in the PRISM dataset, we find that, for instance, greetings have a very different distribution than topics like religion or politics (Fig. 3 center). Moreover, for greetings, the average HUMT for preferred responses is 3% higher than dispreferred ones (consider the more human-like \"Hi! How can I help you?\" versus the less human-like \u201cState your query.\u201d). For LMSys, we also performed automatic topic modeling. While we found that some topics had higher HUMT overall, e.g. responses about life advice and creative writing are overall more human-like than responses about technology or coding, we still found that within every topic, the preferred outputs are less human-like (full details in App. D). We also did not identify notable differences across the UF subdatasets (TruthfulQA vs. FLAN, etc.).\nNext, we explore whether preferences vary based on user demographics, e.g, might women overall prefer warmer language? We find that preferences are not explained by demographics. Using PRISM metadata (race, gender, age, and familiarity with LLMs), we do not find statistically significant differences based on any demographic markers."}, {"title": "5 DUMT: Reducing human-like tone", "content": "Based on our findings that human-like tone is overall dispreferred, and its potential harms, we present DUMT, a method to systematically decrease human-like tone in LLMs using HUMT and DPO (a method to fine-tune directly on a preference dataset (Rafailov et al., 2024)). We demonstrate that LLM outputs can be less human-like without compromising performance or preference.\nMethod To train DUMT, we first de-duplicate and exclude unsafe data from the PRISM, UF, and LMSys datasets using GPT-4's moderation filter (\u00a73.3 - we do not use SHP since it is not phrased as prompts to an LLM, and HH-RLHF prompts are primarily unsafe). Then, we split the data using a 90-10 train-test split. From the 90%, to construct the preference dataset used in DPO, we randomly sampled n = 500 pairs of outputs (s, s') where s is preferred over s' and HUMT(s') \u2013 HUMT(s) > t = 0 (Table 4). The other 10% (3565 prompts) are the test set for evaluation. We use Meta-Llama-3-8B-Instruct as the base model B.\nEvaluation: DUMT reduces mean HUMT without sacrificing performance. To evaluate model performance, we compare this DUMT model both to the original baseline B and to a baseline BDPO-R, which is fine-tuned in the same way as DUMT except without considering HUMT at all. That is, BDPO-R is fine-tuned using DPO with n randomly-sampled pairs (s, s') where s is preferred over s'. We compute mean HUMT from DUMT and the two baselines over the test set. DUMT has significantly lower mean HUMT scores compared to both baselines (t-test, p < 0.001) (Figure 4 left). Interestingly, BDPO-R also has lower HUMT than B, likely because the training dataset reflects a preference for lower HUMT.\nTo assess model performance, we run the DUMT model and both baselines on RewardBench (Lambert et al., 2024), a standard evaluation set for instruction-tuned models (Longpre et al., 2024; Liu et al., 2024, 2025). Overall, DUMT improves performance compared to B and is similar to BDPO-R (Figure 4 center). DUMT achieves higher performance on the Chat Hard, Reasoning and Safety benchmarks while decreasing on Chat. The benchmarks on which DUMT im/deproves are those where less/more human-like tone is implicitly rewarded, e.g., in the Math-PRM benchmark (subset of Reasoning), the pronoun \u201cI\u201d is in 94% of the wrong answers and \u2248 0% of the correct ones; see App. F.1 for more details. For Chat, which captures the human-like task of conversing, the chosen answers are more human-like, e.g., a response starting with \"Sure, I can help with that!\u201d is chosen over a response that directly answers the user's query. For Chat, chosen answers have significantly higher HUMT than the rejected ones (t-test, p < 0.05).\nEvaluation: Human annotation To understand human preferences on DUMT, we conducted a small-scale study where participants compared outputs from DUMT and B. We randomly selected 500 prompts p in the test set where DUMT 's output is substantively less human-like than B's (HUMT(DUMT(p)) \u2013 HUMT(B(p)) > \u03b5, with \u03b5 = 0.02, covering ~ 30% of the test set). For each prompt, annotators chose between the outputs and optionally added comments on their choice. We collected three annotations per prompt and determined preference by majority vote. We ran a pilot in December 2024 and the full study in February 2025 on Prolific. Full details are in App. G. We find that DUMT and B are preferred for 40% and 36% of responses respectively, though the difference is statistically insignificant (z-test, p < 0.05) (Figure 4 right). Corroborating our previous finding that the preference for less human-like tone is more distinct in the PRISM dataset (\u00a74.2), we find that the preference for DUMT over B is higher for PRISM (44% vs. 35%) than LMSys (40% vs. 35%) or UF (38% vs. 37%).\nReasons for dis/preferring DUMT Annotators' comments provide some insight into reasons for preferences: we find that annotators preferred B for being more \"friendly,\u201d \u201cpersonable\u201d, \u201ccasual\u201d, and \u201cengaging", "too friendly.\"\nWe outline two reasons that annotators preferred DUMT: First, the less human-like responses from DUMT are more information-dense: they deliver more relevant information in fewer words. Unlike baseline outputs, DUMT responses omit quips, pleasantries, and follow-up questions. Annotators found this clearer and more concise (\u201cless wordy\", \"more to the point": "The information provided is also more thorough (\u201cgives [more] detail", "formal and informative.": "econd, they are more authentic to LLMs' capabilities. One annotator said \u201cI really don't like the AI implying that it's sorry since they do not feel things", "when AI is patronizing and pretend to care about things they can't physically care about\" (full examples in Table A3).\"\n    },\n    {\n      \"title\": \"6 Discussion and Future Work\",\n      \"content\": \"We find that human-like tone may be linked to adverse impacts (H1) and is overall dispreferred (H2). While it is preferred in certain settings (e.g., greetings), these relatively rare settings may be disproportionately emphasized in popular benchmarks. For instance, Chat constitutes 1/4 of the overall RewardBench score. Our findings motivate the need for benchmarks that measure model performance without conflating it with human-like language.\nMoreover, to better align LLMs, we must develop a deeper understanding of when and how LLM outputs should be human-like (AI Safety Institute, 2024). This sociotechnical question requires balancing immediate preference versus long-term benefits (Kirk et al., 2025a) and company profit motives versus societal wellbeing (Schanke et al., 2021; Suresh et al., 2024). HUMT, SOCIOT, and DUMT enable future work on measuring and navigating appropriate levels of human-like tone and other social perceptions across different settings (Bhattacharjee et al., 2024). These metrics can support multi-turn evaluations (Ibrahim et al., 2025) and human-subject experiments to rigorously assess the impacts of human-like language across the growing landscape of LLM applications.\"\n    },\n    {\n      \"title\": \"7 Limitations\",\n      \"content\": \"HUMT measures human-like tone in language specifically. There are other important dimensions of human-likeness beyond tone, especially since human-likeness is a construct whose definition shifts based on cultural contexts and other factors (Heyselaar, 2023; DeVrio et al., 2025). For example, the ability to solve complex reasoning problems could be considered human-like. These other facets of anthropomorphism are out of the scope of this work.\nThe datasets we use focus on general-purpose LLMs and do not exhaustively cover the various applications in which LLMs are used. Research shows that preferences vary by context, e.g., people favor more empathetic language in emotional support use cases (Bhattacharjee et al., 2024). Our metrics support future research on the impacts of human-likeness across these different applications.\nOur analysis is only on English data and reflect the WEIRD (Western, Educated, Industrialized, Rich, and Democratic) norms inherent to our chosen LLM (GPT-2), which is rooted in American English (Bender et al., 2021; Jha et al., 2023; Bianchi et al., 2023; Cheng et al., 2023b). This limits the generalizability of our findings to other cultures where social perceptions may differ significantly (Wetzel, 1988). However, the method can be easily adapted to reflect other cultural contexts.\nAlso, our metric is not intended to make essentialist claims about how specific individuals speak, as language can reinforce norms around who is considered as fully \\\"human\\\" by society, while preventing other individuals from achieving that status (Wynter, 2003; Butler, 2004). Work in disability studies also challenges the popular conceptions of human-like tone that we focus on in this paper (Henner and Robinson, 2023). Rather, our metrics are tools to capture aggregate societal patterns that further our understanding of anthropomorphism in LLM outputs. As in prior research on linguistic features linked to different social constructs, we aim to highlight how certain types of language, particularly from LLMs, may reinforce harmful power dynamics.\"\n    },\n    {\n      \"title\": \"A Unsupervised Implicit Speaker Discovery\",\n      \"content\": \"To construct D\u207a and D\u00af, we conducted a deductive thematic analysis to identify the prevalent implicit speakers of LLM outputs. Specifically, we used a form of HUMT to deduct implicit roles in an unsupervised manner using BERT, a masked language model. We identified the set of words\n\\[rs = argmax_{w1,w2,..., w15} (P(w said s)),\\]\ni.e., the words that had the highest probability to fill the masked position w in \u201cw said s": "or s \u2208 S, where S is a dataset of LLM outputs. Then, we identified the top 200 words w that occurred most frequently in {rs | s \u2208 S}. We then clustered the words using K-means clustering.\nLike the personas identified by Zheng et al. (2024), we find that many of the words are occupations due to the topics of the prompts rather than the underlying tone or implicit framing. We focus not on these occupation-specific words but instead on more generic words that reflect broader language ideologies."}, {"title": "B Related Work on Measuring Social Perceptions", "content": "Our SOCIOT metrics build on previous work measuring dimensions such as intimacy (Pei and Jurgens, 2020), warmth and competence (Fraser et al., 2021) and gender (Cheng et al., 2023a). In contrast to these previous works, our approach does not require a supervised approach or training on any particular dataset as it relies only on probabilities from an LLM. Moreover, our approach specifically focuses on measuring the implicit speaker or source of the text rather than the information present in the text."}, {"title": "C Human Annotation for Validating HUMT and SOCIOT", "content": "Examples of the annotation interface shown to the student volunteers are in Fig. A1. We also show box plots of TD versus human annotator labels (determined by majority vote) in Fig. A2."}, {"title": "Statistical Analysis", "content": "We perform two statistical tests to evaluate two different aspects of the annotations. First, we perform the two-sample t-test to see whether the mean TD between the positive and negative labels is significantly different. Additionally, we use the x2 test to confirm that the binary labels match the sign of TD.\nFor the t-test, we compare the mean TD between the positive and negative majority vote labels. We find statistically significant differences for all dimensions on both datasets. All the p-values are < 0.001, except for on status in the LLM outputs, which has p < 0.01. Next, we compute x\u00b2 between the majority vote labels and the sign of TD and find statistically significant differences in TD sign, with all the p- values are < 0.001, except for gender (p = 0.006, 0.001 on C4 and LLM outputs respectively) and status (p = 0.014 on both). Thus, the TD scores differ significantly between the positive and negative labels and correspond to the sign of TD, establishing the construct validity of our metrics."}, {"title": "Annotator Information", "content": "Our approach (4 annotators annotating 600 texts each) exceeds standards in similar work (Cheng et al., 2024; Rao et al., 2025; Su et al., 2025). Cheng et al. (2024) validate their metric using two authors annotating 400 sentences; Su et al. (2025) have 3 students each annotate 120 texts to validate their automatic evaluator; and Rao et al. (2025) have 300 data points annotated by 3 workers each for validating their dataset and 480 datapoints annotated by 2 students each for validating the model evaluation setup. Our annotators were students who had relevant expertise in NLP and received detailed instructions on the constructs we aimed to measure, and they were familiar with the project because they were working on closely related research. This familiarity provided relevant context as our goal is to capture dominant societal perspectives."}, {"title": "Power Analysis", "content": "We conducted a power analysis to justify the sample dataset size: Among the dimensions, the smallest effect size (calculated as Cohen's d) is 0.78. A power analysis using a t-test for two independent samples reveals that the necessary sample size is 27 given effect size 0.78, alpha 0.05, and desired power 0.8 (as is standard). Since the necessary sample size decreases with larger effect sizes, and all other dimensions have larger effect sizes, our current sample size is more than sufficient for ensuring adequate power to validate each dimension."}, {"title": "D Topic Modeling for LM-Sys", "content": "To understand differences in HUMT among different topics for LMSys, we first performed topic modeling for LMSys. We used the sentence embedding model all-mpnet-base-v2 (Reimers and Gurevych, 2019) to generate embeddings for each user prompt. We then clustered these embeddings into 10 topics using K-means via the BERTopic Python package (Grootendorst, 2022). We assigned names to each cluster based on qualitative inspection of examples from each cluster. The clusters we identified are: everyday scenarios, AI/technology, coding, life advice, creative writing, economics and business, academic questions, pop culture, politics/global issues, and math questions."}, {"title": "E Other comparisons using HUMT and SOCIOT", "content": ""}, {"title": "E.1 Differences between Models", "content": "We can also compare different models, such as GPT-2 vs Claude. Claude's responses are implicitly more human-like (Figure A4. We show this to demonstrate how HUMT and SOCIOT can be used to quantify qualitative or subtler differences in how different models respond to user prompts. In contrast to prompt-based methods to explore implicit characteristics of text (Dunlap et al., 2024; Eloundou et al., 2025), our method directly uses probabilities from an LLM, offering a more cost-effective, interpretable, and reliable approach. Thus, it not only scales efficiently to large datasets where prompt-based methods are prohibitively expensive, but also eliminates the need for extensive data or relative comparisons, thus enabling analysis for single outputs and small datasets."}, {"title": "E.2 Downstream Impacts", "content": "We further examine how TD relate to downstream impacts such as persuasiveness. We examine the data from Costello et al. (2024), which show that LLMs can be used to debunk conspiracy theories, to see how implicit framings play a role in the effectiveness of such language. In this dataset of LLM responses that aim to convince users out of believing conspiracy theories, while we do not find statistically significant differences in human-like tone in this context, we find that more convincing responses are similarly less intimate, feminine, and warm (Fig. A4).\nNote that these findings reflect information about comparisons among LLM outputs only, and do not hold for human writing. We find that human writing has higher Thumanlike, Twarmth, and Tintimacy (Fig. A4), and yet is on average more persuasive than LLM-generated responses (Durmus et al., 2024). This suggests that while LLMs' approximations of humanness, wamrth, and intimacy are not effective for persuasion, human-written content which may have the same surface-level markers of human-like tone can be more effective due to other features (Song et al., 2025)."}, {"title": "F DUMT Model Training and Evaluation Details", "content": "We train the model using the default hyperparameters of: learning rate of 2e-4, batch size 1, and 10 epochs.\nModel size We experimented with two different models, Qwen2-0.5B-Instruct and Meta-Llama-3-8B-Instruct. While the smaller model also has a significant decrease in HUMT scores"}]}