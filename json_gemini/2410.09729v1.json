{"title": "MIRAGE: Multimodal Identification and Recognition of Annotations in Indian General Prescriptions", "authors": ["Tavish Mankash", "V.S. Chaithanya Kota", "Anish De", "Praveen Prakash", "Kshitij Jadhav"], "abstract": "Hospitals generate thousands of handwritten prescriptions, a practice that remains prevalent despite the availability of Electronic Medical Records (EMR). This method of record-keeping hinders the examination of long-term medication effects, impedes statistical analysis, and makes the retrieval of records challenging. Handwritten prescriptions pose a unique challenge, requiring specialized data for training models to recognize medications and their patterns of recommendation. While current handwriting recognition approaches typically employ 2-D LSTMs, recent studies have explored the use of Large Language Models (LLMs) for Optical Character Recognition (OCR). Building on this approach, we focus on extracting medication names from medical records. Our methodology MIRAGE (Multimodal Identification and Recognition of Annotations in Indian General Prescriptions) involves fine-tuning the LLaVA 1.6 and Idefics2 models. Our research utilizes a dataset provided by Medyug Technology, consisting of 743,118 fully annotated high-resolution simulated medical records from 1,133 doctors across India. We demonstrate that our methodology exhibits 82% accuracy in medication name and dosage extraction. We provide a detailed account of our research methodology and results, notes about HWR with Multimodal LLMs, and release a small dataset of 100 medical records with labels.", "sections": [{"title": "1 Introduction", "content": "Handwritten prescriptions remain the predominant form of medical records in India. Despite widespread awareness of the high rate of errors associated with them, this practice persists. Once a prescription is written, it often becomes nearly impossible for an untrained individual to decipher it without the assistance of a pharmacist, who receives specialized training for this purpose.\nA study highlights that the inability to comprehend doctors' handwriting is a significant barrier to accessing effective healthcare services in Bangladesh, a chal-\nlenge mirrored in many developing countries, including India [1]. A South African study found that doctors, nurses, and pharmacists read medicine prescriptions with a median accuracy of 87.8%, 81.8%, and 75%, respectively [2]. Notably, pharmacists made errors in medication names in 5% and dosage in 12% of all medical records. Similar studies for India are lacking, but we don't expect the same results.\nAddressing the challenge of accurately reading handwritten prescriptions is complex and cannot be solved effectively using unspecialized models (see Figure 1). Our research contributes by employing a rare and extensive dataset while leveraging Multimodal LLMs to tackle this task."}, {"title": "1.1 Dataset", "content": "The novelty of our work lies in the utilization of a unique simulated dataset and the application of Multi-"}, {"title": "2 Literature Review", "content": ""}, {"title": "2.1 Handwriting Recognition with LLMS", "content": "Fadeeva and Schlattner et al. explored the application of LLMs for online handwriting recognition, achieving state-of-the-art accuracy through various innovative methods of representing handwriting data [4]. Their investigation included employing color coding to indicate the size and duration of each step (from one point to the next), effectively representing speed. However, their study focuses on online recognition, which is not applicable to paper-written medical records. Consequently, their research is not utilized in this work, but we recommend that future studies investigate online recognition of medical records using LLMs.\nA study that investigates the performance of LLMs in OCR related tasks notes semantic reliance and HWR as the first 2 points in their discussion of the weakness of LLMs in said tasks [5]. They do not properly analyze the reasons behind the poor performance in HWR, one of which we will explore in Section 5."}, {"title": "2.2 Reading Prescriptions with AI", "content": "Handwritten prescription reading is a challenging task due to several limitations in existing models. A critical issue in this process is the lack of a comprehensive and diverse dataset of handwritten medical prescriptions. Most models have been trained on the IAM handwriting dataset, which is later fine-tuned on a small dataset of handwritten prescriptions. However, this small dataset often fails to replicate the com-"}, {"title": "3 Methodology", "content": "We commenced our methodology by fine-tuning the LLaVA 1.6 model [10, 11, 12]. The specific model we used integrates the CLIP-ViT-Large-Patch14-336 vision transformer by OpenAI, connected to the Mistral 7B language model via a trainable projector [13, 14]. This projector is a Multilayer Perceptron. However, other approaches for the projector have also been tested by other works [15, 16, 17, 18]. Research indicates that CLIP-like models produce rough embeddings for images, which are subsequently aligned for the LLM by the projector [19, 13, 12]. To handle images exceeding CLIP's 336x336 pixel limit, LLaVA processes four 336x336 patches of the image and a single down-scaled version. LLaVA's maximum supported resolution is thus 672x672 pixels."}, {"title": "3.1 Accuracy Metrics", "content": "We measure accuracy with AWP (Accuracy w.r.t. Predicted), AWI (Accuracy w.r.t. Ideal), and their harmonic mean, HIP (Harmonic mean of Ideal and Predicted). Intuitively, AWP (precision) is the fraction of correctly predicted medicine names out of all predicted names, while AWI (recall) is the fraction of correctly predicted names out of all expected names.\nLet $P_e$ be the set of predicted medicine names, $E_e$ the set of expected names, and $C_e = P_e \\cap E_e$. Let $P = |P_e|$, $E = |E_e|$, and $C = |C_e|$.\n$AWP = \\frac{C}{P}$ (1)\n$AWI = \\frac{C}{E}$ (2)\n$HIP = \\frac{2 \\cdot AWP \\cdot AWI}{AWP + AWI} = \\frac{2C}{E+P}$ (3)\nIf $E + P = 0$, AWP, AWI, and HIP are 1. If E = 0 or P = 0, then AWP, AWI, and HIP are 0. Accuracy in our models refers to HIP."}, {"title": "4 Results", "content": ""}, {"title": "4.1 Fine-tuning to Extract All Details From Medical Record", "content": "We aimed to extract comprehensive information from simulated medical records, including simulated PII (age, gender, weight), vitals (blood pressure, temperature), medication names with schedules, diagnostics (lab tests), and diagnoses. We fine-tuned the QWEN VL and LLaVA models [21, 10]. On LLaVA, our highest average HIP reached 40%, with medication name extraction peaking at only 49%. We also evaluated the effect of dataset size and applied alphabet spacing in the targets to reduce semantic dependency, following [4]. Results are shown in Figure 9. QWEN VL, with a maximum resolution of 448x448 pixels, yielded a low 7% HIP after five epochs, rendering it impractical. All of this training took 9 days using 7 A6000 GPUs."}, {"title": "4.2 Fine-tuning LLaVA", "content": "In our experiments, the learning rate declined sharply during the 3rd epoch. Training and validation accuracy are illustrated in Figure 7. Training details are in Appendix. We achieved a final accuracy of 79.76%. We trained on 7 A6000s for 3.5 days. Percentage of occupation of various medicines in prediction data versus target data has been plotted in Figure 8. We suspect that this model shows poor performance because of its use of CLIP as its vision encoder. Following our analysis in Section 5, we proceeded to use Idefics2 because of its use of SigLIP [20]."}, {"title": "4.3 Fine-tuning Idefics2", "content": "In the first epoch, we achieved a HIP of 82%. Training details are in Appendix. Despite attempts with DDP, FSDP, DeepSpeed Zero (all 3 stages, with and without offload), and various libraries, Idefics2 was limited to a batch size of 1 per GPU, likely due to higher resolution images. This led to long training times per epoch (2.5-4 days), so we prioritized multiple experiments over multiple epochs. We trained on 6 A6000s for 13 days. Results are shown in Figure 10."}, {"title": "5 Discussion", "content": "Our 82% accuracy, while the best yet for real-world use, is still below practical deployment standards, despite using a large dataset and advanced AI models. A key limitation requires further discussion.\nIt is stated that,\n\"In training, we keep both the visual en-coder and LLM weights frozen.... In this way,the image features Hu can be aligned withthe pre-trained LLM word embedding. Thisstage can be understood as training a com-patible visual tokenizer for the frozen LLM.\"[12]\nThis explains that the projector, which connects the vision encoder to the LLM, realigns embeddings from CLIP for the LLM's interpretation. It is therefore reasonable to infer that handwriting recognition is largely completed at the CLIP stage. Consequently, if CLIP's handwriting performance is inadequate, the LLM has limited capacity to rectify these deficiencies during fine-tuning. We recommend future research to quantitatively assess the impact of the vision encoder in Multimodal LLMS for HWR tasks."}, {"title": "5.1 Challenges with CLIP and Handwriting Recognition", "content": "The CLIP model, developed by OpenAI, functions as the core vision encoder for many leading open-source Multimodal LLMs [13]. There are notable signs of its limited performance in HWR:\n1. Although zero-shot CLIP generally outperforms fully supervised linear classifiers on ResNet-50, its performance is notably deficient on the MNIST handwritten digits dataset [13]. To qualitatively understand the problem, refer to Table 2.\n2. For our analysis, we evaluated several prominent Multimodal LLMs, including GPT-40 Mini, Gemini 1.5 Flash, Llama 3 LLaVA 1.6 (CLIP), Intern VL 26B (custom vision encoder), mPLUG-Owl3 (SigLIP), and Idefics2 (SigLIP) using the IAM Line Handwriting Dataset [22, 23, 24, 25, 26, 17]. The non-transformer state-of-the-art model is OrigamiNet [27]. The error rates of each model and the non-transformer state-of-the-art model is plotted in Figure 11. While general Multimodal LLMs have demonstrated somewhat comparable performance against non-transformer state-of-the-art methods in OCR tasks, our findings reveal a substantial performance gap in HWR [5, 28]. We must highlight that HWR is generally more challenging than OCR. Notably, models incorporating CLIP, as shown in Table 2 and Figure 11, exhibited the lowest performance. We recommend further research to conduct a more comprehensive analysis of this."}, {"title": "5.2 Negative Impacts and Limitations", "content": "Firstly, we recognize that our model's current accuracy is not ready for deployment in hospitals. Our aim is to advance this field of study, with the ultimate goal of creating a model that is more accurate and dependable than pharmacists. These models could assist in hospitals by automating part of the data entry process, allowing pharmacists to correct any inaccuracies rather than inputting every detail manually. However, there is a risk that as these models become more accurate, pharmacists might develop overconfidence and fail to thoroughly examine entries, which could be dangerous. We acknowledge that this approach has significant risks, and extensive studies comparing this method to traditional practices are necessary. If this technology is deployed in hospitals without proper safety studies and measures, it could have serious repercussions.\nThe most valuable application of this model lies in data analysis. If we have access to a substantial, un-labelled dataset of medical records, our model can be employed to generate approximate labels. This capability would facilitate large-scale data analysis for researchers."}, {"title": "6 Conclusion", "content": "Our study shows promise and demonstrates a clear scope for improvement. Achieving an 82% accuracy, our approach stands out as the most accurate in real-world scenarios compared to existing methods. Our ablation studies, which analyze the impact of various components in the prompt and the influence of dataset size on accuracy, may be valuable to others. Additionally, our work highlights the current state of HWR using Multimodal LLMs. We hope others will continue to build on our work, with a particular focus on enhancing the vision encoder as a direction for future research."}, {"title": "Appendix", "content": ""}, {"title": "A Learning Graphs and Fine-tuning Specifics for LLaVA", "content": "We employed Low-Rank Adaptation (LoRA) for training with a rank of 128 and an alpha of 256 to enable extensive fine-tuning. The initial learning rate was set at 2e-5, with a warm-up ratio of 0.03. We also used DeepSpeed's ZeRO Stage 3. Loss and learning rate are illustrated in figure 13 and figure 14, respectively."}, {"title": "B Learning Graphs and Fine-tuning Specifics for Idefics2", "content": "For fine-tuning Idefics2, we used QLoRA and DDP. We trained with a rank of 128 and an alpha of 256 to enable extensive fine-tuning. Training loss is plotted in figure 15."}]}