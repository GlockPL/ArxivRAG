{"title": "AN ACTIONABLE FRAMEWORK FOR ASSESSING BIAS AND FAIRNESS IN LARGE LANGUAGE MODEL USE CASES", "authors": ["Dylan Bouchard"], "abstract": "Large language models (LLMs) can exhibit bias in a variety of ways. Such biases can create or exacerbate unfair outcomes for certain groups within a protected attribute, including, but not limited to sex, race, sexual orientation, or age. This paper aims to provide a technical guide for practitioners to assess bias and fairness risks in LLM use cases. The main contribution of this work is a decision framework that allows practitioners to determine which metrics to use for a specific LLM use case. To achieve this, this study categorizes LLM bias and fairness risks, maps those risks to a taxonomy of LLM use cases, and then formally defines various metrics to assess each type of risk. As part of this work, several new bias and fairness metrics are introduced, including innovative counterfactual metrics as well as metrics based on stereotype classifiers. Instead of focusing solely on the model itself, the sensitivity of both prompt-risk and model-risk are taken into account by defining evaluations at the level of an LLM use case, characterized by a model and a population of prompts. Furthermore, because all of the evaluation metrics are calculated solely using the LLM output, the proposed framework is highly practical and easily actionable for practitioners.", "sections": [{"title": "Introduction", "content": "The versatility of current Large Language Models (LLMs) in handling various tasks [Minaee et al., 2024, Liu et al., 2023, Ray, 2023] presents challenges when it comes to evaluating bias and fairness at the model level. Existing approaches primarily focus on assessing risk using benchmark data sets containing predefined prompts [Gehman et al., 2020, Dhamala et al., 2021, Nozza et al., 2021, Smith et al., 2022, Parrish et al., 2021, Li et al., 2020], masked tokens [Zhao et al., 2018, Rudinger et al., 2018, Nadeem et al., 2021, Levy et al., 2021], or unmasked sentences [Nangia et al., 2020, Barikeri et al., 2021, Jiao et al., 2023, Felkner et al., 2023], assuming that these adequately capture specific bias or fairness risks [Gallegos et al., 2023]. However, these assessments are likely to overestimate the risk for use cases where the population of prompts is low risk [Wang et al., 2024]. Moreover, to the best of the author's knowledge, the current literature does not provide a framework for effectively aligning LLM use cases with suitable metrics for evaluating bias and fairness.\nThis work aims to address these limitations by developing an actionable LLM bias and fairness evaluation framework defined at the use case level. Drawing inspiration from the classification fairness framework proposed by Saleiro et al. [2018], the framework proposed in this work enables practitioners to map an LLM use case to an appropriate set of bias and fairness evaluation metrics by considering relevant characteristics of the use case and stakeholder values. This evaluation approach is unique in that it incorporates actual prompts from the practitioner's use case, taking into account the prompt-specific risks that have been demonstrated to significantly increase the likelihood of biased and unfair outcomes [Wang et al., 2024]. By constraining the scope to focused use cases, where prompts are derived from a known population and the task is well-defined, this framework is specifically designed to customize the risk assessment for a specific application.\nTo introduce the framework, this study first provides formal definitions of bias and fairness desiderata for LLMs from the literature and segment these definitions by risk category. Subsequently, those risks are mapped to a taxonomy of use cases focused on large-scale applications, where human-in-the-loop may be infeasible. Lastly, for each risk category, various bias and fairness evaluation metrics are detailed, with discussions provided on their input requirements, calculation methods, the risks they assess, and circumstances under which they should be applied. As part of this work, a variety of novel bias and fairness metrics are introduced. This includes innovative counterfactual adaptations of recall-oriented understudy for gisting evaluation (ROUGE) [Lin, 2004], bilingual evaluation understudy (BLEU) [Papineni et al., 2002], and cosine similarity [Singhal and Google, 2001], as well a set of stereotype classifier-based metrics that are adapted from analogous toxicity classifier-based metrics.\nFor practical reasons, this study limits the selection of LLM bias and fairness metrics to those requiring only LLM generated output as inputs. This includes 1) generated text metrics, which take a generated set of tokens as input [Gallegos et al., 2023], 2) recommendation fairness metrics, calculated on a set of LLM-provided recommendations [Zhang et al., 2023], and 3) classification fairness metrics, which are already well-established in the machine learning fairness literature [Bellamy et al., 2018, Saleiro et al., 2018, Weerts et al., 2023, Hardt et al., 2016, Feldman et al., 2014, Mehrabi et al., 2019]. Due to practical limitations of the input requirements, the framework omits both embedding-based metrics, which are computed using an LLM's hidden vector representations of words or sentences [Islam et al., 2016, May et al., 2019, Guo and Caliskan, 2020], and probability-based metrics, which leverage predicted token probabilities from an LLM [Webster et al., 2020, Kurita et al., 2019, Ahn and Oh, 2021, Kaneko and Bollegala, 2021, Salazar et al., 2019, Barikeri et al., 2021, Nangia et al., 2020, Nadeem et al., 2021]. To further support the rationale behind the selection of evaluation metrics, it is important to note that metrics focused on the downstream task, consistent with the metrics incorporated in this framework, have been shown to be more reliable than metrics derived from embeddings or token probabilities [Goldfarb-Tarrant et al., 2020, Delobelle et al., 2021].\nThe remainder of this paper is organized as follows. Section 2 formally defines various notions of bias and fairness in LLMs. Section 3 discusses various techniques for conducting bias and fairness assessments in LLMs. Section 4 offers a framework for choosing among LLM bias and fairness metrics based on use case characteristics and stakeholder values. Finally, Section 5 offers concluding remarks."}, {"title": "Bias and Fairness Risks for LLM Use Cases", "content": "This section formally defines several pre-requisite terms and concepts upon which subsequent sections rely, several of which are adapted from those provided by Gallegos et al. [2023]. The discussions in this paper explore concepts of\nLarge Language Model (LLM). An LLM \\(M : X \\rightarrow Y\\) is a pre-trained, transformer-based model that maps a text sequence \\(X \\in X\\) to an output \\(Y \\in Y\\), where \\(X\\) denotes the set of all possible text inputs (i.e. prompts) and the form of \\(\\hat{Y}\\) is specific to the LLM and the use case [Gallegos et al., 2023]. Let \\(\\theta\\) parameterize \\(M\\), such that \\(\\hat{Y} = M(X;\\theta)\\).\nPopulation of Prompts. A population of prompts, denoted \\(P_X\\), is a collection of LLM inputs. To characterize well-defined use cases, subsequent sections refer to a \u2018known population of prompts', indicating that practitioners possess information about the prompt domain and are able draw representative samples from \\(P_X\\). For instance, a population of prompts might consist of clinical notes, where each individual prompt includes a collection of notes, accompanied by specific instructions for the LLM to generate a summary [Chuang et al., 2024].\nLarge Language Model Use Case. An LLM use case is characterized by an LLM \\(M(X; \\theta)\\) and a population of prompts \\(P_X\\). In the interest of concise notation, LLM use cases will be hereafter denoted as \\((M, P_X)\\). An LLM use case is evaluated on a finite set of responses generated by \\(M(X; \\theta)\\) from a sample of \\(N\\) prompts \\(X_1, ..., X_N\\), drawn from the population \\(P_X\\).\nProtected Attribute Groups. A protected attribute group \\(G \\in G\\) represents a subset of people characterized by a shared identity trait, where \\(G\\) is a partition [Gallegos et al., 2023].\nProtected Attribute Group Lexicon. A protected attribute group lexicon \\(A \\in A\\) is a collection of words that correspond to protected attribute group \\(G \\in G\\).\nCounterfactual Input Pair. A counterfactual input pair is a pair of prompts, \\(X' \\) and \\(X'' \\), which are identical in every way except the former mentions protected attribute group \\(G' \\) and the latter mentions protected attribute group \\(G'' \\) [Gallegos et al., 2023]. For an LLM use case \\((M, P_X)\\), an evaluation set of counterfactual input pairs is denoted \\((X'_1, X''_1), ..., (X'_N, X''_N)\\). To create each pair, a prompt is drawn from the subset of prompts containing words from the protected attribute lexicon \\(A\\), i.e. \\(P_{X|A} = \\{ X : X \\in P_X, X \\cap A \\neq \\emptyset \\}\\), and counterfactual variations are obtained via counterfactual substitution.\nFairness Through Unawareness (FTU). Given a protected attribute lexicon \\(A\\), an LLM use case \\((M, P_X)\\) satisfies FTU if for each \\(X \\in P_X, X \\cap A = (\\emptyset)\\). In simpler terms, FTU implies none of the prompts for an LLM use case include any mention of a protected attribute word [Gallegos et al., 2023].\nThis section presents formal definitions of various notions of bias and fairness applicable to LLMs. A taxonomy for these definitions is proposed, organized by risk category. Specifically, the risk categories included in this taxonomy are toxicity, stereotyping, counterfactual fairness, and allocational harms."}, {"title": "LLM Bias and Fairness Risks", "content": "Drawing from the definitions of toxicity and derogatory language outlined in the survey conducted by Gallegos et al. [2023], let the definition of toxic text encompass any offensive language that 1) launches attacks, issues threats, or incites hate or violence against a social group, or 2) includes the usage of pejorative slurs, insults, or any other forms of expression that specifically target and belittle a social group. To formalize this, a corresponding fairness desideratum is introduced below.\nNon-Toxicity. Let \\(T\\) denote the set of all toxic phrases. An LLM use case \\((M, P_X)\\) exhibits non-toxicity if \\(M(X; \\theta) \\cap T = \\emptyset\\) for each \\(X \\in P_X\\).\nStereotyping is an important type of social bias that should be considered in the context of LLMs [Bommasani et al., 2023, Bordia and Bowman, 2019, Zekun et al., 2023]. This study follows the work by Gallegos et al. [2023], in which stereotypes are defined as negative generalizations about a protected attribute group, often reflected by differences in frequency with which various groups are linked to stereotyped terms [Bommasani et al., 2023]. To formalize this notion, two fairness desiderata are considered, which are also proposed by Gallegos et al. [2023]: equal group associations (EGA) and equal neutral associations (ENA).\nEqual Group Associations [Gallegos et al., 2023]. For two protected attribute groups \\(G', G''\\), and a set of neutral words \\(W\\), an LLM use case \\((M, P_X)\\) satisfies equal group associations if, for each \\(w \\in W, P(w \\in \\hat{Y} |\\hat{Y} \\cap A' \\neq \\emptyset) = P(w \\in \\hat{Y} |\\hat{Y} \\cap A'' \\neq \\emptyset)\\). Put simply, equal group associations requires that each neutral word in \\(W\\) is equally likely to be contained in the output of \\(M\\) regardless of which protected attribute group is mentioned.\nEqual Neutral Associations [Gallegos et al., 2023]. For two protected attribute groups \\(G', G''\\), with respective asso- ciated lexicons \\(A', A''\\), and a set of neutral words \\(W\\), an LLM use case \\((M, P_X)\\) satisfies Equal Neutral Associations if, \\(P(A' \\cap \\hat{Y} \\neq \\emptyset)|\\hat{Y} \\cap W \\neq \\emptyset) = P(A \\cap \\hat{Y} \\neq \\emptyset)|\\hat{Y} \\cap W \\neq \\emptyset)\\). In other words, Equal Neutral Associations requires that co-occurrence of protected attribute words with a set of neutral words \\(W\\) is equally probable for both protected attribute groups.\nIn many contexts, it is undesirable for an LLM to generate substantially different output as a result of different protected attribute words contained in the input prompts, all else equal [Huang et al., 2020, Nozza et al., 2021, Wang et al., 2024]. Following previous work [Huang et al., 2020, Garg et al., 2019], this concept is hereafter referred to as (lack of) counterfactual fairness. Depending on context and stakeholder values, the practitioner may wish to assess an LLM for differences in overall content or sentiment resulting from inclusion of different protected attribute words in a prompt. Below, this section provides a formal definition of the corresponding fairness desiderata known as counterfactual invariance, adapted from Gallegos et al. [2023].\nCounterfactual Invariance. For two protected attribute groups \\(G', G''\\), an LLM use case \\((M, P_X)\\) satisfies counter- factual invariance if, for a specified invariance metric \\(v(\\cdot,\\cdot)\\), expected value of the invariance metric is less than some tolerance level \\(\\epsilon\\):\n\\[\\mathbb{E}[v(M(X'; \\theta), M(X'';\\theta))] \\leq \\epsilon,\\]\nwhere \\((X', X'')\\) is a counterfactual input pair corresponding to \\(G', G'' \\) [Gallegos et al., 2023]."}, {"title": "Allocational Harms", "content": "Allocational harms, which Gallegos et al. [2023] define as an unequal distribution of resources or opportunities among different protected attribute groups, have been widely studied in the machine learning fairness literature [Saleiro et al., 2018, Bellamy et al., 2018, Weerts et al., 2023, Kamishima et al., 2012, Zhang et al., 2018, Hardt et al., 2016, Feldman et al., 2014, Pleiss et al., 2017, Kamiran et al., 2012, Agarwal et al., 2018, Kamiran and Calders, 2011, Chouldechova, 2016]. The corresponding fairness desideratum, is known as group fairness, defined formally below.\nGroup Fairness. Given two protected attribute groups \\(G', G''\\), and a tolerance level \\(\\epsilon\\), an LLM use case \\((M, P_X)\\) satisfies group fairness if\n\\[|B(M(X;\\theta)|G') \u2013 B(M(X;\\theta)|G'')| \\leq \\epsilon,\\]\nwhere \\(B\\) is a statistical performance metric (e.g. false negative rate) applied to \\(M\\), conditioned on membership in a protected attribute group [Gallegos et al., 2023]. Here, conditioning on \\(G\\) implies calculating \\(B\\) on the subset of input prompts that either contain a direct mention of group \\(G\\) or, in the case of person-level prompt granularity, correspond to individuals belonging to group \\(G\\). Note that the choice of \\(B\\) will depend on context and stakeholder values."}, {"title": "Mapping Bias and Fairness Risks to LLM Use Cases", "content": "Section 2.1 establishes the characterization of a well-defined LLM use case based on a model and a known population of prompts. To set up the decision framework, this section segments use cases by task, according to the following three categories: 1) text generation and summarization, 2) classification, and 3) recommendation.\nThe bias and fairness evaluation framework proposed in this work is intended for large-scale applications, in which the volume of generated responses makes exhaustive human review impractical. It is important to note that, for scenarios in which the practitioner manually evaluates each generated output, the evaluations proposed here may be unnecessary if concerns related to bias and fairness can be effectively addressed by the individual who is reviewing the outputs.\nText Generation and Summarization. First, consider use cases where an LLM generates text outputs that are not constrained to a predefined set of classes (e.g., positive vs. negative) or list elements (e.g., products to recommend). For the sake of brevity, this group of use cases will be hereafter referred to as \u201ctext generation and summarization,\" acknowledging that this category can encompass additional use cases including, but not limited to, machine translation, retrieval augmented generation (RAG), and question-answering. An example of a text generation use case could be utilizing an LLM to compose personalized messages for customer outreach. Similarly, a summarization use case may involve employing an LLM to extract pertinent information or provide summaries from clinical notes. These use cases carry the potential risk of generating toxic text in their outputs. Moreover, if these use cases fail to meet the criteria of FTU, meaning that the prompts include references to protected attributes, they also pose the risk of perpetuating stereotypes or exhibiting counterfactual unfairness.\nClassification. LLMs have been widely used for text classification [Sun et al., 2023, Widmann and Wich, 2022, Bonikowski et al., 2022, Howard and Ruder, 2018, Sun et al., 2019, Chai et al., 2020, Chen et al., 2020, Lin et al., 2021]. In the context of bias and fairness, it is important to distinguish whether the text inputs can be mapped to a protected attribute, either by containing direct mentions of a protected attribute group, or in the case of person-level prompt granularity, corresponding to individuals belonging to certain protected attribute groups. For instance, one example of a person-level classification use case could involve utilizing an LLM to classify customer feedback as positive or negative in order to assign appropriate follow-ups. Similar to traditional person-level classification challenges in machine learning, these use cases present the risk of allocational harms. On the other hand, classification use cases that do not involve person-level data and satisfy FTU are not subject to these bias and fairness risks.\nRecommendation. Recommendation is another potential application of LLMs [Bao et al., 2023, Gao et al., 2023], such as using an LLM to recommend products to customers. Zhang et al. [2023] show that LLMs used as recommendation engines can discriminate when exposed to protected attribute information. Given this concern, it follows that LLM recommendation use cases pose the risk of counterfactual unfairness if they do not satisfy FTU."}, {"title": "Bias and Fairness Evaluation Metrics", "content": "This section introduces a range of evaluation metrics segmented by the use case task and its applicable bias and fairness risks. The proposed framework encompasses three distinct use case categories: 1) text generation and summarization, 2) classification, and 3) recommendation. For each category, bias and fairness evaluation metrics are included that address the applicable risks.\nAccounting for prompt-specific risk is important to accurately reflect the risk of a specific use case, as Wang et al. [2024] find in their evaluations that toxicity probability is 26 to 101 times higher when dealing with toxic prompts compared to non-toxic prompts. Moreover, Goldfarb-Tarrant et al. [2020], Delobelle et al. [2021] have demonstrated that evaluation metrics which take into account the LLM's task provide a more accurate reflection of the associated risk compared to metrics based on embeddings or token probabilities. Accordingly, the objective of this work is to assess the risks associated with a particular use case, taking into consideration not only the LLM used but also the task at hand and the prompt population. Following the characterization of a use case based on a model and a known population of prompts, as established in Section 2.1, each metric definition presented below is contextualized within an evaluation sample of size \\(N\\) drawn from a known population of prompts \\(P_X\\)."}, {"title": "Bias Metrics for Text Generation and Summarization Use Cases", "content": "Gallegos et al. [2023] propose a detailed taxonomy of evaluation metrics for bias evaluations in LLMs. They partition these metrics into three main categories: embedding-based metrics, probability-based metrics, and generated-text metrics. While embedding- and probability-based metrics require access to an LLM's upstream architecture, generated- text metrics instead treat an LLM like a black box and can be easily calculated from LLM output alone. Hence, given their ease of application, only generated-text metrics are included in the framework proposed here.\nWhile Gallegos et al. [2023] further segment generated-text metrics into three subcategories (namely distribution metrics, classifier metrics, and lexicon metrics), this work segments these metrics according to the risk taxonomy outlined in 2.2. In particular, generated-text bias and fairness metrics are segmented into the following categories: toxicity metrics, stereotype metrics, and counterfactual fairness metrics. Toxicity metrics leverage a pre-trained toxicity classifier, such as Perspective API, to assign a toxicity score to an LLM's output [Chowdhery et al., 2022, Lees et al., 2022, Wang et al., 2024, Bommasani et al., 2023, Gehman et al., 2020]. Stereotype metrics assess the relative co-occurrence of stereotype words with protected attribute words [Bordia and Bowman, 2019, Bommasani et al., 2023] or leverage a pre-trained stereotype classifier to assign a stereotype score to an LLM's output [Zekun et al., 2023]. Lastly, counterfactual fairness metrics assess fairness through a causal lens by exploiting variation in mentions of protected attributes in otherwise-identical prompts [Huang et al., 2020]. Select metrics are detailed in each category below."}, {"title": "Toxicity Metrics", "content": "Toxicity metrics leverage a pre-trained toxicity classifier to assess general toxicity that is not specific to a particular protected attribute. Given a toxicity classifier \\(T : Y \\rightarrow [0, 1]\\), this section defines toxicity metrics in the context of an LLM use case \\((M, P_X)\\). Specifically, these metrics are evaluated on a set of \\(m\\) responses per prompt, \\(\\lbrace \\hat{Y}_{ij}\\rbrace_{j=1}^m\\), generated by \\(M\\), from a sample of prompts \\(X_1, ..., X_N\\) drawn from \\(P_X\\). In practice, variation in responses for the same prompt can be achieved by choosing a non-zero temperature for the LLM or by utilizing various random seeds [Wang et al., 2024]. Below, formal definitions of three toxicity metrics are provided: Expected Maximum Toxicity, Toxicity Probability, and Toxic Fraction. All three metrics range in values from 0 to 1, with smaller values indicating a greater degree of fairness.\nExpected Maximum Toxicity (EMT) [Gehman et al., 2020]. EMT estimates the maximum predicted toxicity probability among the top \\(m\\) generations:\n\\[EMT = \\frac{1}{N} \\sum_{i=1}^N \\max_{1<j<m} T(\\hat{Y}_{ij}).\\]\nNote that the standard choice of \\(m\\) for this metric is \\(m = 25\\) [Gehman et al., 2020, Wang et al., 2024].\nToxicity Probability (TP) [Gehman et al., 2020]. TP is calculated as the empirical probability of having at least one toxic prediction \\((T(\\hat{Y}) \\geq 0.5)\\) among the top \\(m\\) generations:\n\\[TP = \\frac{1}{N} \\sum_{i=1}^N \\mathbb{I}(\\max_{1<j<m} T(\\hat{Y}_{ij}) \\geq 0.5).\\]\nAs with EMT, the standard choice of \\(m\\) for this metric is \\(m = 25\\) [Gehman et al., 2020, Wang et al., 2024].\nToxicity Fraction (TF) [Bommasani et al., 2023]. TF is defined as the fraction of generations that are classified as toxic:\n\\[TF = \\frac{1}{N} \\sum_{i=1}^N \\sum_{j=1}^m \\frac{1}{m} \\mathbb{I} (T(\\hat{Y}_{ij}) \\geq 0.5),\\]\nThis metric effectively estimates the likelihood that responses generated by \\(M\\) on prompts from \\(P_X\\) contain toxic text [Bommasani et al., 2023]. Note that while the standard choice of \\(m\\) for this metric is \\(m = 1\\) [Bommasani et al., 2023], a larger value of \\(m\\) may be preferred in practice if sampling a large \\(N\\) is infeasible."}, {"title": "Stereotype Metrics", "content": "Stereotype metrics aim to identify harmful stereotypes specific to protected attributes that might be present in an LLM's output. Because these metrics rely on mentions of protected attribute groups, these metrics may be unnecessary if FTU is satisfied for an LLM use case. Among stereotype metrics, this work distinguishes between metrics based on co-occurrence of protected attribute words and stereotypical words, and metrics that leverage a stereotype classifier."}, {"title": "Co-occurrence-Based Metrics", "content": "This section outlines a set of metrics that assess stereotype risk based on relative co-occurrence of protected attribute words with neutral words of interest. In particular, formal definitions of two co-occurrence-based stereotype metrics are provided: Co-Occurrence Bias Score and Stereotypical Associations.\nCo-Occurence Bias Score (COBS) [Bordia and Bowman, 2019]. Given two protected attribute groups \\(G', G''\\) with associated sets of protected attribute words \\(A', A''\\), a set of stereotypical words \\(W\\), and an LLM use case \\((M, P_X)\\), the full calculation of COBS is as follows:\n\\[P(W|A) = \\frac{\\sum_{i=1}^N \\sum_{w \\in W} cooccur(w, A|Y_i) / \\sum_{i=1}^N \\sum_{w \\in W}  \\sum_{\\hat{Y_i} }cooccur(w, A|\\hat{Y_i})}{\\sum_{i=1}^N \\sum_{\\alpha \\in A} C(\\alpha, Y_i)/ \\sum_{i=1}^N  \\sum_{\\hat{Y_i} } \\sum_{\\alpha \\in A} C(\\alpha, \\hat{Y_i})}\\]\n\\[COBS = \\frac{1}{|W|} \\sum_{w \\in W} log \\frac{P(w|A')}{P(w|A'')}\\]\nAbove, \\(C(x, Y_i)\\) denotes the count of \\(x\\) in \\(Y_i\\) and \\(Y_i\\) represents the LLM output \\(Y_i\\), with words from \\(A' \\cup A''\\) and stop words excluded. The co-occurrence function \\(cooccur(w, A|\\hat{Y})\\) computes a weighted count of words from \\(A\\) that are found within a context window centered around \\(w\\), each time \\(w\\) appears in \\(\\hat{Y}\\). In words, COBS computes the relative likelihood that an LLM \\(M\\) generates output having co-occurrence of \\(w \\in W\\) with \\(A'\\) versus \\(A''\\). This metric has a range of possible values of \\([-\\infty, \\infty]\\), with values closer to 0 signifying a greater degree of fairness.\nStereotypical Associations (SA) [Bommasani et al., 2023]. Consider a set of protected attribute groups \\(G\\), an associated set of protected attribute lexicons \\(A\\), and an associated set of stereotypical words \\(W\\). Additionally, let \\(C(x, Y)\\) denote the number of times that the word \\(x\\) appears in the output \\(Y\\), \\(\\mathbb{I}(\\cdot)\\) denote the indicator function, \\(P_{ref}\\) denote a reference distribution, and \\(TVD\\) denote total variation difference. For a given LLM \\(M(X; \\theta)\\) and a sample of prompts \\(X_1, ..., X_N\\) drawn from \\(P_X\\), the full computation of SA is as follows:\n\\[\\gamma(w|A') = \\sum_{\\alpha \\in A'} \\sum_{i=1}^N C(\\alpha, Y_i) \\mathbb{I}(C(w, Y_i) > 0)\\]\n\\[\\pi(w|A') = \\frac{\\gamma(w|A')}{\\sum_{A' \\in A} \\gamma(w|A')}\\]\n\\[P(w) = \\{\\pi(w|A') : A' \\in A\\}\\]\n\\[SA = \\frac{1}{|W|} \\sum_{w \\in W} TVD(P(w), P_{ref}).\\]\nIn words, SA measures the relative co-occurrence of a set of stereotypically associated words across protected attribute groups. SA ranges in value from 0 to 1, where smaller values indicate greater fairness."}, {"title": "Metrics Leveraging a Stereotype Classifier", "content": "It has been shown that stereotype classifiers can be an effective tool for assessing stereotype risk in LLM use cases [Zekun et al., 2023]. This section introduces three new metrics by extending the toxicity metrics outlined in Section 3.1.1, leveraging a pre-trained stereotype classifier \\(St : Y \\rightarrow [0,1]\\) rather than a toxicity classifier. Namely, these metrics include: Expected Maximum Stereotype, Stereotype Probability, and Stereotype Fraction. All three metrics range in values from 0 to 1, with smaller values indicating a greater degree of fairness.\nExpected Maximum Stereotype (EMS). EMS, analogous to EMT, estimates the maximum predicted stereotype probability among the top \\(m\\) generations:\n\\[EMS = \\frac{1}{N} \\sum_{i=1}^N \\max_{1<j<m} St(\\hat{Y}_{ij}).\\]\nFollowing the convention of EMT, practitioners may wish to use \\(m = 25\\) for this metric."}, {"title": "Stereotype Probability (SP)", "content": "Analogous to TP, SP measures as the empirical probability of having at least one stereotype prediction \\((St(\\hat{Y}) \\geq 0.5)\\), among the top \\(m\\) generations:\n\\[SP = \\frac{1}{N} \\sum_{i=1}^N \\mathbb{I}(\\max_{1<j<m} St(\\hat{Y}_{ij}) \\geq 0.5),\\]\nTo be consistent with the convention of TP, practitioners may wish to use \\(m = 25\\) for this metric."}, {"title": "Stereotype Fraction (SF)", "content": "SF, presented as an extension of TF, measures as the fraction of generations that are predicted to contain a stereotype:\n\\[SF = \\frac{1}{N} \\sum_{i=1}^N \\sum_{j=1}^m \\frac{1}{m} \\mathbb{I}(St(\\hat{Y}_{ij}) \\geq 0.5),\\]\neffectively estimating the likelihood that responses generated by \\(M\\) on prompts from \\(P_X\\) contain stereotypes. Note that while the standard choice of \\(m\\) for the analogous toxicity metric, EMT, is \\(m = 1\\) [Bommasani et al., 2023], a larger value of \\(m\\) may be preferred in practice if sampling a large \\(N\\) is infeasible."}, {"title": "Counterfactual Fairness Metrics", "content": "Counterfactual metrics aim to assess differences in LLM output when different protected attributes are mentioned in input prompts, all else equal. Given two protected attribute groups \\(G', G''\\), these metrics are defined in the context of an LLM use case \\((M, P_X)\\). In particular, these metrics are evaluated on a sample of counterfactual response pairs \\((Y'_1, Y''_1), ..., (Y'_N, Y''_N)\\) generated by \\(M\\), from a sample of counterfactual input pairs \\((X_1, X'_1), ..., (X'_N, X''_N)\\) drawn from \\(P_{X|A}\\). Note that, in scenarios where a large \\(N\\) is infeasible, practitioners may opt to generate multiple response pairs per counterfactual input pair.\nThese metrics, which are categorized into counterfactual similarity metrics and counterfactual sentiment metrics, respectively quantify the differences in text similarity and sentiment by leveraging the variations in LLM output observed across counterfactual input pairs. Due to their reliance on mentions of protected attributes in input prompts, if FTU is satisfied for an LLM use case, these metrics need not be used."}, {"title": "Counterfactual Similarity", "content": "Counterfactual similarity metrics measure the similarity in outputs generated from counterfactual input pairs according to a specified invariance metric \\(v\\), i.e. \\(v(M(X'; \\theta), M(X'';\\theta))\\). These metrics effectively assess whether the LLM use case satisfies the counterfactual invariance property defined in 2.2. One such example of \\(v\\) is exact match [Rajpurkar et al., 2016], but [Gallegos et al., 2023] argue that this metric is too strict. This work introduces three, less stringent, counterfactual similarity metrics: Counterfactual ROUGE-L, Counterfactual BLEU, and Counterfactual Cosine Similarity, which are extensions of state-of-the-art text similarity metrics [Minaee et al., 2024, Lin, 2004, Papineni et al., 2002, Singhal and Google, 2001, Gomaa and Fahmy, 2013]. While the first two assess similarity using token-sequence overlap, the the third assesses similarity using sentence embeddings. All three metrics range in values from 0 to 1, with larger values indicating a greater degree of fairness.\nCounterfactual ROUGE-L (CROUGE-L). This work introduces CROUGE-L, defined as the average ROUGE-L score [Lin, 2004] over counterfactually generated output pairs. The full calculation of CROUGE-L is as follows:\n\\[r = \\frac{LCS(\\hat{Y}, \\hat{Y}!", "hat{Y}!": ""}, {"len(Y!": ""}, "n\\[CROUGE-L = \\frac{1}{N} \\sum_{i=1}^N \\frac{2rr'}{r+r'},\\]\nwhere \\(LCS(., .)\\) denotes the longest common subsequence of tokens between two LLM outputs, and \\(len(\\hat{Y})\\) denotes the number of tokens in an LLM output. The CROUGE-L metric effectively uses ROUGE-L to assess similarity as the longest common subsequence (LCS) relative to generated text length.\nGiven its reliance on matching token sequences, practitioners should mask protected attribute words in coun- terfactual output pairs before computing CROUGE-L. For instance, suppose, for the counterfactual input pair \\((X', X'')\\) = ('What did he do next', \u2018What did she do next'), an LLM generates the output pair \\((Y',\\hat{Y}", "then he drove his car to work", "then she drove her car to work", ".", "In this context, these two responses are effectively identical. Masking the tokens {\u2018he', \u2018she', \u2018his\u2019, \u2018her\u2019} accomplishes this computationally.\nCounterfactual BLEU (CBLEU). This work introduces CBLEU, defined as the average BLEU score [Papineni et al., 2002] over counterfactually generated output pairs. The full calculation of CBLEU is as follows:\n\\[precision(\\hat{Y}, \\hat{Y}", "frac{\\sum_{snt \\in \\hat{Y'}} \\sum_{b-gram \\in snt} min(C'(b-gram, \\hat{Y'}|\\hat{Y}", "C(b-gram, \\hat{Y}", {"hat{Y}": "min(1, exp\\{1 - \\frac{len(\\hat{Y}"}, {"hat{Y}": {"hat{Y}": "BLEU(\\hat{Y}", "hat{Y'}|\\hat{Y}": "denotes the number of times b-gram appears in \\(\\hat{Y'}\\) given that it also appears in \\(\\hat{Y}", "V": "Y \\rightarrow \\mathbb{R}^d\\), this work defines CCS as:\n\\[CCS = \\frac{1}{N} \\sum_{i=1}^N \\frac{V(\\hat{Y}) \\cdot V(\\hat{Y}')}{\\|V(\\hat{Y})\\| \\|V(\\hat{Y}')\\|},\\]\ni.e. the average cosine similarity [Singhal and Google, 2001] between counterfactually generated output pairs for an LLM use case."}, "title": "Counterfactual Sentiment Bias", "content": "Counterfactual sentiment metrics measure the sentiment consistency across counterfactually generated pairs of output. To achieve this, these metrics leverage a pre-trained sentiment classifier \\(Sm : Y \\rightarrow [0, 1]\\). This section outlines two counterfactual sentiment metrics: Strict Counterfactual Sentiment Parity, proposed by Huang et al. [2020], and an extension of this metric introduced in this work called Weak Counterfactual Sentiment Parity. Both metrics have a range of values of [0, 1], with smaller values indicating a higher degree of fairness.\nStrict Counterfactual Sentiment Parity (SCSP) [Huang et al., 2020]. SCSP calculates Wasserstein-1 distance [Jiang et al., 2019] between the output distributions of a sentiment classifier applied to counterfactually generated LLM outputs:\n\\[W_1 = \\mathbb{E}_{r \\sim U(0,1)} |P(Sm(\\hat{Y}') > r) \u2013 P(Sm(\\hat{Y}\") > r)|,\\]\nwhere U(0, 1) denotes the uniform distribution. Above, \\(\\mathbb{E}_{r \\sim U(0,1)}\\) is calculated empirically on a sample of coun- terfactual response pairs \\((Y'_1, Y''_1), ..., (Y'_N, Y''_N)\\) generated by \\(M\\), from a sample of counterfactual input pairs \\((X_1, X'_1), ..., (X'_N, X''_N)\\) drawn from \\(P_{X|A}\\).\""}, {"title": "Fairness Metrics for Classification Use Cases", "content": "It is well-established that classification models can produce unfair outcomes for certain protected attribute groups [Saleiro et al., 2018, Bellamy et al., 2018, Weerts et al., 2023, Feldman et al., 2014, Hardt et al., 2016, Mehrabi et al., 2019]. Let a classification LLM use case be defined as an LLM tasked with classification, denoted as \\(M^{(c)}\\), and a population of prompts \\(P_X\\). Note that the metrics introduced in this section are confined to binary classification use cases, where \\(M^{(c)} : X \\rightarrow \\{0,1\\}\\), with the understanding that evaluating fairness for multiclass classification is a straightforward extension from the binary case [Rouzot et al., 2023].\nFor the remainder of this section, assume each prompt in \\(P_X\\) for a given classification LLM use case corresponds to a protected attribute group. Under this assumption, traditional machine learning fairness metrics [Bellamy et al., 2018, Saleiro et al., 2018, Weerts et al., 2023] may be applied [Czarnowska et al., 2021]. Accordingly, these metrics are defined on binary predictions \\(\\hat{Y}_1, ..., \\hat{Y}_N\\), generated from a sample of prompts \\(X_1, ..., X_N \\in P_X\\), with some metrics also incorporating corresponding ground truth values \\(Y_1, ..., Y_N\\). These metrics effectively assess group fairness (see Section 2.2), with choice of statistical outcome measure \\(B\\) depending on stakeholder values (e.g. the relative cost of false negatives vs. false positives).\nThis section distinguishes between representation fairness metrics, calculated using only predictions, and error-based fairness metrics, calculated using both predictions and ground truth values. The formulation of each fairness metric involves calculating the absolute difference between a pair of group-level metrics. This calculation yields a range of values between 0 and 1, where smaller values signify a higher level of fairness."}, {"title": "Representation Fairness Metrics for Binary Classification", "content": "Representation fairness metrics aim to determine whether protected attribute groups are adequately represented in the positive predictions generated by classifier. It is recommended that practitioners reserve this set of metrics for classification LLM use cases for which group-level predicted prevalence rates, i.e. the proportion of predictions belonging to the positive class, should be approximately equal. A single representation fairness metric is defined below: Demographic Parity.\nDemographic Parity (DP) [Dwork et al., 2011]. DP calculates the absolute difference in group-level predicted prevalence rates:\n\\[DP = |P(\\hat{Y} = 1|G = G') \u2013 P(\\hat{Y} = 1|G = G'')|,\\]\nwhere \\(\\hat{Y}\\) denotes a model prediction and \\(P(\\cdot)\\) denotes the empirical probability based on predictions generated from a sample prompts drawn from \\(P_X\\)."}, {"title": "Error-Based Fairness Metrics for Binary Classification", "content": "Error-based fairness metrics aim to determine whether disparities in model performance exist across protected attribute groups. For error-based fairness, two metrics focused on false negatives, False Negative Rate Difference and False Omission Rate Difference, and two metrics focused on false positives, False Positive Rate Difference and False Discovery Rate Difference, are introduced. Following Saleiro et al. [2018], it is recommended that the choice between these two types of error-based fairness metrics be informed by whether the interventions assigned by the model are assistive (meaning false negatives are undesirable) or punitive (meaning false positives are undesirable) in nature.\nFalse Negative Rate Difference (FNRD) [Bellamy et al., 2018]. FNRD measures the absolute difference in group- level false negative rates:\n\\[FNRD = |P(\\hat{Y} = 0|Y = 1, G = G') \u2013 P(\\hat{Y} = 0|Y = 1,G = G'')|,\\]\nwhere \\(Y\\) denotes ground truth value corresponding to \\(\\hat{Y}\\) and \\(P(\\cdot)\\) denotes the empirical probability based on predictions generated from a sample of prompts drawn from \\(P_X\\). Note that false negative rate measures the proportion of actual positives (Y = 1) that are falsely classified as negative (Y = 0). FNRD is equivalent to the equal opportunity difference metric proposed by Hardt et al. [2016].\nFalse Omission Rate Difference (FORD) [Bellamy et al., 2018]. FORD measures the absolute difference in group-level false omission rates:\n\\[FORD = |P(Y = 1|\\hat{Y} = 0, G = G') \u2013 P(Y = 1|\\hat{Y} = 0, G = G'')|,\\]\nwhere \\(Y\\) denotes ground truth value corresponding to \\(\\hat{Y}\\) and \\(P(\\cdot)\\) denotes the empirical probability based predictions generated from on a sample of prompts drawn from \\(P_X\\). Instead of concentrating on actual positives, false omission rate calculates the percentage of predicted negatives \\(\\hat{Y} = 0\\) that are misclassified. Thus, similar to the FNRD, a higher FORD indicates a greater difference in the likelihood of false negatives across groups.\nFalse Positive Rate Difference (FPRD) [Bellamy et al., 2018]. FPRD measures the absolute difference in group-level false positive rates:\n\\[FPRD = |P(\\hat{Y} = 1|Y = 0, G = G') \u2013 P(\\hat{Y} = 1|Y = 0, G = G'')|,\\]\nwhere \\(Y\\) denotes ground truth value corresponding to \\(\\hat{Y}\\) and \\(P(\\cdot)\\) denotes the empirical probability based on predictions generated from a sample of prompts drawn from \\(P_X\\). Note that false positive rate measures the percentage of actual negatives (Y = 0) being incorrectly predicted as positive \\(\\hat{Y} = 1\\).\nFalse Discovery Rate Difference (FDRD) [Bellamy et al., 2018]. FDRD measures the absolute difference in group-level false discovery rates:\n\\[FDRD = |P(Y = 0|\\hat{Y} = 1, G = G') \u2013 P(Y = 0|\\hat{Y} = 1,G = G'')|,\\]\nwhere \\(Y\\) denotes ground truth value corresponding to \\(\\hat{Y}\\) and \\(P(\\cdot)\\) denotes the empirical probability based on predictions generated from a sample of prompts drawn from \\(P_X\\). Rather than considering actual negatives, false discovery rate calculates the proportion of predicted positives \\(\\hat{Y} = 1\\) that are incorrectly classified. Hence, as with FPRD, a higher FDRD indicates a larger disparity in the likelihood of false positives across groups."}, {"title": "Multiclass Fairness Metrics", "content": "For multiclass classifiers, the framework proposed here follows the fairness guidelines provided by Rouzot et al. [2023] and hence recommends conducting class-wise fairness assessments using the appropriate binary classification fairness metrics, as per sections 3.2.1, 3.2.2, on each of the 'sensitive' classes. In particular, Rouzot et al. [2023] characterize sensitive classes as outcomes having significant impact on the lives of individuals to whom the model is applied."}, {"title": "Fairness Metrics for Recommendation Use Cases", "content": "Zhang et al. [2023] have shown that LLMs tasked with recommendation can exhibit discrimination when exposed to protected attribute information in the input prompts. Let a recommendation LLM use case be defined as an LLM tasked with recommendation, denoted as \\(M^{(R)}\\), and a population of prompts \\(P_X\\). Specifically, \\(M^{(R)} : X \\rightarrow R^K\\) maps a prompt \\(X \\in X\\) to an ordered K-tuple \\(R \\in R^K\\) of distinct recommendations from a set of possible recommendations \\(R\\).\nThis section outlines a set of fairness metrics for recommendation LLM use cases, as proposed by Zhang et al. [2023]. To ensure consistency with the metrics discussed in Section 3.1.3, this section presents modified versions of these metrics to be pairwise in nature, rather than attribute-wise. Given two protected attribute groups \\(G', G''\\), and an LLM use case \\((M^{(R)}, P_X)\\), these metrics assess similarity in counterfactually generated recommendation lists. Below, each metric is defined according to responses generated from a sample of counterfactual input pairs \\((X_1, X'_1), ..., (X'_N, X''_N)\\) which are drawn from \\(P_{X|A}\\). In particular, three metrics are introduced: Jaccard Similarity, Search Result Page Misinformation Score at K, and Pairwise Ranking Accuracy Gap at K. Each of these metrics ranges in value from 0 to 1, with larger values indicating a greater degree of fairness."}, {"title": "Jaccard Similarity at K (Jaccard-K)", "content": "Below, a pairwise version of Jaccard-K is introduced. This metric calculates the average Jaccard Similarity [Han et al., 2011]\u2014the ratio of the intersection cardinality to the union cardinality\u2014among pairs of counterfactually generated recommendation lists. Formally, this metric is computed as follows:\n\\[Jaccard-K = \\frac{1}{N} \\sum_{i=1}^N \\frac{|R'_i \\cap R''_i|}{|R'_i \\cup R''_i|},\\]\nwhere \\(R', R''\\) respectively denote the generated lists of recommendations by \\(M(X; \\theta)\\) from the counterfactual input pair \\((X', X'')\\). Note that this metric does not account for ranking differences between the two lists [Zhang et al., 2023].\nSearch Result Page Misinformation Score at K (SERP-K) [Zhang et al., 2023]. Adapted from Tomlein et al. [2021], SERP-K reflects the similarity of two lists, considering both overlap and ranks. A modified version of SERP-K, adapted for pairwise application, is introduced as follows:\n\\[\\psi(X', X'') = \\frac{\\sum_{v \\in R'} \\mathbb{I}(v \\in R'') * (K - rank(v, R) + 1)}{K * (K+1)/2}\\]\n\\[SERP-K = \\frac{1}{N} \\sum_{i=1}^N min(\\psi(X', X'_i), \\psi(X'', X')),\\)\\]\nwhere \\(R', R''\\) respectively denote the generated lists of recommendations by \\(M(X; \\theta)\\) from the counterfactual input pair \\((X', X'')\\), \\(v\\) is a recommendation from \\(R'\\), and \\(rank(v, R)\\) denotes the rank of \\(v\\) in \\(R\\). Note that the use of min(, ) is included to achieve symmetry.\nPairwise Ranking Accuracy Gap at K (PRAG-K) [Zhang et al., 2023]. Adapted from Beutel et al. [2019], PRAG-K reflects the similarity in pairwise ranking between two recommendation results. A pairwise version of PRAG-K is presented as follows:\n\\[rankmatch_i(v_1, v_2) = \\mathbb{I}(rank(v_1, R') < rank(v_2, R')) * \\mathbb{I}(rank(v_1, R'') < rank(v_2, R''))\\]\n\\[\\eta(X', X'') = \\frac{\\sum_{v_1, v_2 \\in R' v_1 \\neq v_2} \\mathbb{I}(v_1 \\in R'') * rankmatch_i(v_1, v_2)}{K* (K + 1)}\\]\n\\[PRAG-K = \\frac{1}{N} \\sum_{i=1}^N min(\\eta(X', X'_i), \\eta(X'', X')),\\)\\]\nwhere \\(R', R''\\) respectively denote the generated lists of recommendations by \\(M(X; \\theta)\\) from the counterfactual input pair \\((X', X'')\\), \\(v_1, v_2\\) are recommendations from \\(R'\\), and \\(rank(v, R_1)\\) denotes the rank of \\(v\\) in \\(R_1\\). As with SERP-K, min(, ) is used to achieve symmetry."}, {"title": "A Unified Framework for Bias and Fairness Assessments of LLM Use Cases", "content": "In general, bias and fairness assessments of LLM use cases do not require satisfying all possible evaluation metrics. Instead, practitioners should prioritize and concentrate on a relevant subset of metrics that align with their use case. To demystify metric choice for these assessments, this section introduces a decision framework that enables practitioners to determine suitable choices of bias and fairness evaluation metrics, drawing inspiration from the classification fairness framework proposed by Saleiro et al. [2018].\nIn defining the proposed decision framework for selecting LLM bias and fairness evaluation metrics, the scope is restricted to use cases for which prompts can be sampled from a known population and the task is well-defined. Use cases are categorized into three distinct groups based on task: 1) text generation and summarization, 2) classification, and 3) recommendation. The framework includes suitable metrics for each category to assess the potential bias and fairness risks that align with the specific characteristics of the use case."}, {"title": "Conclusion", "content": "This paper proposes an actionable decision framework for selecting bias and fairness evaluation metrics for LLM use cases, introducing several new evaluation metrics as part of the framework. This work addresses two gaps in the current literature. First, to the best of the author's knowledge, the current literature does not offer a framework for selecting bias and fairness evaluation metrics for LLM use cases. To fill this gap, the proposed framework draws inspiration from Saleiro et al. [2018] and incorporates use case characteristics and stakeholder values to guide the selection of evaluation metrics. Second, this framework tackles limitations of existing LLM bias and fairness evaluation approaches that rely on benchmark data sets containing predefined prompts. Instead, the approach outlined in this work involves using actual prompts from the practitioner's use case. By considering both prompt-risk and the assigned task of the LLM, this approach provides a more customized risk assessment for the practitioner's specific use case. Furthermore, the proposed framework aims to enhance practicality and ease of implementation, as all evaluation metrics are computed solely from the LLM output."}]}