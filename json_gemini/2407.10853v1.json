[{"title": "Bias and Fairness Risks for LLM Use Cases", "authors": ["Dylan Bouchard"], "abstract": "Large language models (LLMs) can exhibit bias in a variety of ways. Such biases can create or exacerbate unfair outcomes for certain groups within a protected attribute, including, but not limited to sex, race, sexual orientation, or age. This paper aims to provide a technical guide for practitioners to assess bias and fairness risks in LLM use cases. The main contribution of this work is a decision framework that allows practitioners to determine which metrics to use for a specific LLM use case. To achieve this, this study categorizes LLM bias and fairness risks, maps those risks to a taxonomy of LLM use cases, and then formally defines various metrics to assess each type of risk. As part of this work, several new bias and fairness metrics are introduced, including innovative counterfactual metrics as well as metrics based on stereotype classifiers. Instead of focusing solely on the model itself, the sensitivity of both prompt-risk and model-risk are taken into account by defining evaluations at the level of an LLM use case, characterized by a model and a population of prompts. Furthermore, because all of the evaluation metrics are calculated solely using the LLM output, the proposed framework is highly practical and easily actionable for practitioners.", "sections": [{"title": "Introduction", "content": "The versatility of current Large Language Models (LLMs) in handling various tasks [Minaee et al., 2024, Liu et al., 2023, Ray, 2023] presents challenges when it comes to evaluating bias and fairness at the model level. Existing approaches primarily focus on assessing risk using benchmark data sets containing predefined prompts [Gehman et al., 2020, Dhamala et al., 2021, Nozza et al., 2021, Smith et al., 2022, Parrish et al., 2021, Li et al., 2020], masked tokens [Zhao et al., 2018, Rudinger et al., 2018, Nadeem et al., 2021, Levy et al., 2021], or unmasked sentences [Nangia et al., 2020, Barikeri et al., 2021, Jiao et al., 2023, Felkner et al., 2023], assuming that these adequately capture specific bias or fairness risks [Gallegos et al., 2023]. However, these assessments are likely to overestimate the risk for use cases where the population of prompts is low risk [Wang et al., 2024]. Moreover, to the best of the author's knowledge, the current literature does not provide a framework for effectively aligning LLM use cases with suitable metrics for evaluating bias and fairness.\nThis work aims to address these limitations by developing an actionable LLM bias and fairness evaluation framework defined at the use case level. Drawing inspiration from the classification fairness framework proposed by Saleiro et al. [2018], the framework proposed in this work enables practitioners to map an LLM use case to an appropriate set of bias and fairness evaluation metrics by considering relevant characteristics of the use case and stakeholder values. This evaluation approach is unique in that it incorporates actual prompts from the practitioner's use case, taking into account the prompt-specific risks that have been demonstrated to significantly increase the likelihood of biased and unfair outcomes [Wang et al., 2024]. By constraining the scope to focused use cases, where prompts are derived from a known population and the task is well-defined, this framework is specifically designed to customize the risk assessment for a specific application.\nTo introduce the framework, this study first provides formal definitions of bias and fairness desiderata for LLMs from the literature and segment these definitions by risk category. Subsequently, those risks are mapped to a taxonomy of use cases focused on large-scale applications, where human-in-the-loop may be infeasible. Lastly, for each risk category, various bias and fairness evaluation metrics are detailed, with discussions provided on their input requirements, calculation methods, the risks they assess, and circumstances under which they should be applied. As part of this work, a variety of novel bias and fairness metrics are introduced. This includes innovative counterfactual adaptations of recall-oriented understudy for gisting evaluation (ROUGE) [Lin, 2004], bilingual evaluation understudy (BLEU) [Papineni et al., 2002], and cosine similarity [Singhal and Google, 2001], as well a set of stereotype classifier-based metrics that are adapted from analogous toxicity classifier-based metrics.\nFor practical reasons, this study limits the selection of LLM bias and fairness metrics to those requiring only LLM generated output as inputs. This includes 1) generated text metrics, which take a generated set of tokens as input [Gallegos et al., 2023], 2) recommendation fairness metrics, calculated on a set of LLM-provided recommendations [Zhang et al., 2023], and 3) classification fairness metrics, which are already well-established in the machine learning fairness literature [Bellamy et al., 2018, Saleiro et al., 2018, Weerts et al., 2023, Hardt et al., 2016, Feldman et al., 2014, Mehrabi et al., 2019]. Due to practical limitations of the input requirements, the framework omits both embedding-based metrics, which are computed using an LLM's hidden vector representations of words or sentences [Islam et al., 2016, May et al., 2019, Guo and Caliskan, 2020], and probability-based metrics, which leverage predicted token probabilities from an LLM [Webster et al., 2020, Kurita et al., 2019, Ahn and Oh, 2021, Kaneko and Bollegala, 2021, Salazar et al., 2019, Barikeri et al., 2021, Nangia et al., 2020, Nadeem et al., 2021]. To further support the rationale behind the selection of evaluation metrics, it is important to note that metrics focused on the downstream task, consistent with the metrics incorporated in this framework, have been shown to be more reliable than metrics derived from embeddings or token probabilities [Goldfarb-Tarrant et al., 2020, Delobelle et al., 2021].\nThe remainder of this paper is organized as follows. Section 2 formally defines various notions of bias and fairness in LLMs. Section 3 discusses various techniques for conducting bias and fairness assessments in LLMs. Section 4 offers a framework for choosing among LLM bias and fairness metrics based on use case characteristics and stakeholder values. Finally, Section 5 offers concluding remarks."}, {"title": "Bias and Fairness Risks for LLM Use Cases", "content": "This section formally defines several pre-requisite terms and concepts upon which subsequent sections rely, several of which are adapted from those provided by Gallegos et al. [2023]. The discussions in this paper explore concepts of"}, {"title": "Preliminary Definitions", "content": "This section provides preliminary definitions to be used throughout the subsequent sections.\nLarge Language Model (LLM). An LLM $M : X \\rightarrow Y$ is a pre-trained, transformer-based model that maps a text sequence $X \\in X$ to an output $Y \\in Y$, where $X$ denotes the set of all possible text inputs (i.e. prompts) and the form of $\\hat{Y}$ is specific to the LLM and the use case [Gallegos et al., 2023]. Let $\\theta$ parameterize M, such that $\\hat{Y} = M(X;\\theta)$.\nPopulation of Prompts. A population of prompts, denoted $P_X$, is a collection of LLM inputs. To characterize well-defined use cases, subsequent sections refer to a 'known population of prompts', indicating that practitioners possess information about the prompt domain and are able draw representative samples from $P_X$. For instance, a population of prompts might consist of clinical notes, where each individual prompt includes a collection of notes, accompanied by specific instructions for the LLM to generate a summary [Chuang et al., 2024].\nLarge Language Model Use Case. An LLM use case is characterized by an LLM $M(X; \\theta)$ and a population of prompts $P_X$. In the interest of concise notation, LLM use cases will be hereafter denoted as $(M, P_X)$. An LLM use case is evaluated on a finite set of responses generated by $M(X; \\theta)$ from a sample of $N$ prompts $X_1, ..., X_N$, drawn from the population $P_X$.\nProtected Attribute Groups. A protected attribute group $G \\in G$ represents a subset of people characterized by a shared identity trait, where $G$ is a partition [Gallegos et al., 2023].\nProtected Attribute Group Lexicon. A protected attribute group lexicon $A \\in A$ is a collection of words that correspond to protected attribute group $G \\in G$.\nCounterfactual Input Pair. A counterfactual input pair is a pair of prompts, $X'$ and $X''$, which are identical in every way except the former mentions protected attribute group $G'$ and the latter mentions protected attribute group $G''$ [Gallegos et al., 2023]. For an LLM use case $(M, P_X)$, an evaluation set of counterfactual input pairs is denoted $(X'_1, X''_1), ..., (X'_N, X''_N)$. To create each pair, a prompt is drawn from the subset of prompts containing words from the protected attribute lexicon $A$, i.e. $P_{X|A} = \\{ X : X \\in P_X, X \\cap A \\neq \\emptyset \\}$, and counterfactual variations are obtained via counterfactual substitution.\nFairness Through Unawareness (FTU). Given a protected attribute lexicon $A$, an LLM use case $(M, P_X)$ satisfies FTU if for each $X \\in P_X$, $X \\cap A = (\\emptyset)$. In simpler terms, FTU implies none of the prompts for an LLM use case include any mention of a protected attribute word [Gallegos et al., 2023]."}, {"title": "LLM Bias and Fairness Risks", "content": "This section presents formal definitions of various notions of bias and fairness applicable to LLMs. A taxonomy for these definitions is proposed, organized by risk category. Specifically, the risk categories included in this taxonomy are toxicity, stereotyping, counterfactual fairness, and allocational harms."}, {"title": "Toxicity", "content": "Drawing from the definitions of toxicity and derogatory language outlined in the survey conducted by Gallegos et al. [2023], let the definition of toxic text encompass any offensive language that 1) launches attacks, issues threats, or incites hate or violence against a social group, or 2) includes the usage of pejorative slurs, insults, or any other forms of expression that specifically target and belittle a social group. To formalize this, a corresponding fairness desideratum is introduced below.\nNon-Toxicity. Let $T$ denote the set of all toxic phrases. An LLM use case $(M,P_X)$ exhibits non-toxicity if $M(X; \\theta) \\cap T = \\emptyset$ for each $X \\in P_X."}, {"title": "Stereotyping", "content": "Stereotyping is an important type of social bias that should be considered in the context of LLMs [Bommasani et al., 2023, Bordia and Bowman, 2019, Zekun et al., 2023]. This study follows the work by Gallegos et al. [2023], in which stereotypes are defined as negative generalizations about a protected attribute group, often reflected by differences in frequency with which various groups are linked to stereotyped terms [Bommasani et al., 2023]. To formalize this notion, two fairness desiderata are considered, which are also proposed by Gallegos et al. [2023]: equal group associations (EGA) and equal neutral associations (ENA).\nEqual Group Associations [Gallegos et al., 2023]. For two protected attribute groups $G', G''$, and a set of neutral words $W$, an LLM use case $(M, P_X)$ satisfies equal group associations if, for each $w \\in W$, $P(w \\in \\hat{Y} | \\hat{Y} \\cap A' \\neq \\emptyset) = P(w \\in \\hat{Y} | \\hat{Y} \\cap A'' \\neq \\emptyset)$. Put simply, equal group associations requires that each neutral word in $W$ is equally likely to be contained in the output of M regardless of which protected attribute group is mentioned.\nEqual Neutral Associations [Gallegos et al., 2023]. For two protected attribute groups $G', G''$, with respective asso-ciated lexicons $A', A''$, and a set of neutral words $W$, an LLM use case $(M, P_X)$ satisfies Equal Neutral Associations if, $P(A' \\cap \\hat{Y} \\neq \\emptyset)| \\hat{Y} \\cap W \\neq \\emptyset) = P(A'' \\cap \\hat{Y} \\neq \\emptyset)| \\hat{Y} \\cap W \\neq \\emptyset)$. In other words, Equal Neutral Associations requires that co-occurrence of protected attribute words with a set of neutral words W is equally probable for both protected attribute groups."}, {"title": "Counterfactual Fairness", "content": "In many contexts, it is undesirable for an LLM to generate substantially different output as a result of different protected attribute words contained in the input prompts, all else equal [Huang et al., 2020, Nozza et al., 2021, Wang et al., 2024]. Following previous work [Huang et al., 2020, Garg et al., 2019], this concept is hereafter referred to as (lack of) counterfactual fairness. Depending on context and stakeholder values, the practitioner may wish to assess an LLM for differences in overall content or sentiment resulting from inclusion of different protected attribute words in a prompt. Below, this section provides a formal definition of the corresponding fairness desiderata known as counterfactual invariance, adapted from Gallegos et al. [2023].\nCounterfactual Invariance. For two protected attribute groups $G', G''$, an LLM use case $(M, P_X)$ satisfies counter-factual invariance if, for a specified invariance metric $v(\\cdot,\\cdot)$, expected value of the invariance metric is less than some tolerance level $\\epsilon$:\n$\\mathbb{E}[v(M(X'; \\theta), M(X'';\\theta))] \\leq \\epsilon,$\nwhere $(X', X'')$ is a counterfactual input pair corresponding to $G', G''$ [Gallegos et al., 2023]."}, {"title": "Allocational Harms", "content": "Allocational harms, which Gallegos et al. [2023] define as an unequal distribution of resources or opportunities among different protected attribute groups, have been widely studied in the machine learning fairness literature [Saleiro et al., 2018, Bellamy et al., 2018, Weerts et al., 2023, Kamishima et al., 2012, Zhang et al., 2018, Hardt et al., 2016, Feldman et al., 2014, Pleiss et al., 2017, Kamiran et al., 2012, Agarwal et al., 2018, Kamiran and Calders, 2011, Chouldechova, 2016]. The corresponding fairness desideratum, is known as group fairness, defined formally below.\nGroup Fairness. Given two protected attribute groups $G', G''$, and a tolerance level $\\epsilon$, an LLM use case $(M, P_X)$ satisfies group fairness if\n$|B(M(X;\\theta)|G') - B(M(X;\\theta)|G'')| \\leq \\epsilon,$\nwhere $B$ is a statistical performance metric (e.g. false negative rate) applied to M, conditioned on membership in a protected attribute group [Gallegos et al., 2023]. Here, conditioning on $G$ implies calculating $B$ on the subset of input prompts that either contain a direct mention of group $G$ or, in the case of person-level prompt granularity, correspond to individuals belonging to group G. Note that the choice of $B$ will depend on context and stakeholder values."}, {"title": "Mapping Bias and Fairness Risks to LLM Use Cases", "content": "Section 2.1 establishes the characterization of a well-defined LLM use case based on a model and a known population of prompts. To set up the decision framework, this section segments use cases by task, according to the following three categories: 1) text generation and summarization, 2) classification, and 3) recommendation. Descriptions and examples are provided in Table 1.\nThe bias and fairness evaluation framework proposed in this work is intended for large-scale applications, in which the volume of generated responses makes exhaustive human review impractical. It is important to note that, for scenarios in which the practitioner manually evaluates each generated output, the evaluations proposed here may be unnecessary if concerns related to bias and fairness can be effectively addressed by the individual who is reviewing the outputs.\nText Generation and Summarization. First, consider use cases where an LLM generates text outputs that are not constrained to a predefined set of classes (e.g., positive vs. negative) or list elements (e.g., products to recommend). For the sake of brevity, this group of use cases will be hereafter referred to as \u201ctext generation and summarization,\u201d acknowledging that this category can encompass additional use cases including, but not limited to, machine translation, retrieval augmented generation (RAG), and question-answering. An example of a text generation use case could be utilizing an LLM to compose personalized messages for customer outreach. Similarly, a summarization use case may involve employing an LLM to extract pertinent information or provide summaries from clinical notes. These use cases carry the potential risk of generating toxic text in their outputs. Moreover, if these use cases fail to meet the criteria of FTU, meaning that the prompts include references to protected attributes, they also pose the risk of perpetuating stereotypes or exhibiting counterfactual unfairness.\nClassification. LLMs have been widely used for text classification [Sun et al., 2023, Widmann and Wich, 2022, Bonikowski et al., 2022, Howard and Ruder, 2018, Sun et al., 2019, Chai et al., 2020, Chen et al., 2020, Lin et al., 2021]. In the context of bias and fairness, it is important to distinguish whether the text inputs can be mapped to a protected attribute, either by containing direct mentions of a protected attribute group, or in the case of person-level prompt granularity, corresponding to individuals belonging to certain protected attribute groups. For instance, one example of a person-level classification use case could involve utilizing an LLM to classify customer feedback as positive or negative in order to assign appropriate follow-ups. Similar to traditional person-level classification challenges in machine learning, these use cases present the risk of allocational harms. On the other hand, classification use cases that do not involve person-level data and satisfy FTU are not subject to these bias and fairness risks.\nRecommendation. Recommendation is another potential application of LLMs [Bao et al., 2023, Gao et al., 2023], such as using an LLM to recommend products to customers. Zhang et al. [2023] show that LLMs used as recommendation engines can discriminate when exposed to protected attribute information. Given this concern, it follows that LLM recommendation use cases pose the risk of counterfactual unfairness if they do not satisfy FTU."}, {"title": "Bias and Fairness Evaluation Metrics", "content": "This section introduces a range of evaluation metrics segmented by the use case task and its applicable bias and fairness risks. The proposed framework encompasses three distinct use case categories: 1) text generation and summarization, 2) classification, and 3) recommendation. For each category, bias and fairness evaluation metrics are included that address the applicable risks.\nAccounting for prompt-specific risk is important to accurately reflect the risk of a specific use case, as Wang et al. [2024] find in their evaluations that toxicity probability is 26 to 101 times higher when dealing with toxic prompts compared to non-toxic prompts. Moreover, Goldfarb-Tarrant et al. [2020], Delobelle et al. [2021] have demonstrated that evaluation metrics which take into account the LLM's task provide a more accurate reflection of the associated risk compared to metrics based on embeddings or token probabilities. Accordingly, the objective of this work is to assess the risks associated with a particular use case, taking into consideration not only the LLM used but also the task at hand and the prompt population. Following the characterization of a use case based on a model and a known population of prompts, as established in Section 2.1, each metric definition presented below is contextualized within an evaluation sample of size N drawn from a known population of prompts $P_X."}, {"title": "Bias Metrics for Text Generation and Summarization Use Cases", "content": "Gallegos et al. [2023] propose a detailed taxonomy of evaluation metrics for bias evaluations in LLMs. They partition these metrics into three main categories: embedding-based metrics, probability-based metrics, and generated-text metrics. While embedding- and probability-based metrics require access to an LLM's upstream architecture, generated-text metrics instead treat an LLM like a black box and can be easily calculated from LLM output alone. Hence, given their ease of application, only generated-text metrics are included in the framework proposed here.\nWhile Gallegos et al. [2023] further segment generated-text metrics into three subcategories (namely distribution metrics, classifier metrics, and lexicon metrics), this work segments these metrics according to the risk taxonomy outlined in 2.2. In particular, generated-text bias and fairness metrics are segmented into the following categories: toxicity metrics, stereotype metrics, and counterfactual fairness metrics. Toxicity metrics leverage a pre-trained toxicity classifier, such as Perspective API, to assign a toxicity score to an LLM's output [Chowdhery et al., 2022, Lees et al., 2022, Wang et al., 2024, Bommasani et al., 2023, Gehman et al., 2020]. Stereotype metrics assess the relative co-occurrence of stereotype words with protected attribute words [Bordia and Bowman, 2019, Bommasani et al., 2023] or leverage a pre-trained stereotype classifier to assign a stereotype score to an LLM's output [Zekun et al., 2023]. Lastly, counterfactual fairness metrics assess fairness through a causal lens by exploiting variation in mentions of protected attributes in otherwise-identical prompts [Huang et al., 2020]. Select metrics are detailed in each category below."}, {"title": "Toxicity Metrics", "content": "Toxicity metrics leverage a pre-trained toxicity classifier to assess general toxicity that is not specific to a particular protected attribute. Given a toxicity classifier $T : Y \\rightarrow [0, 1]$, this section defines toxicity metrics in the context of an LLM use case $(M, P_X)$. Specifically, these metrics are evaluated on a set of $m$ responses per prompt, $\\{Y_{ij}\\}_{j=1}^m$, generated by M, from a sample of prompts $X_1, ..., X_N$ drawn from $P_X$. In practice, variation in responses for the same prompt can be achieved by choosing a non-zero temperature for the LLM or by utilizing various random seeds [Wang et al., 2024]. Below, formal definitions of three toxicity metrics are provided: Expected Maximum Toxicity, Toxicity Probability, and Toxic Fraction. All three metrics range in values from 0 to 1, with smaller values indicating a greater degree of fairness.\nExpected Maximum Toxicity (EMT) [Gehman et al., 2020]. EMT estimates the maximum predicted toxicity probability among the top $m$ generations:\n$EMT = \\frac{1}{N} \\sum_{i=1}^N \\max_{1 \\leq j \\leq m} T(Y_{ij}).$\nNote that the standard choice of $m$ for this metric is $m = 25$ [Gehman et al., 2020, Wang et al., 2024].\nToxicity Probability (TP) [Gehman et al., 2020]. TP is calculated as the empirical probability of having at least one toxic prediction ($T(\\hat{Y}) \\geq 0.5$) among the top $m$ generations:\n$TP = \\frac{1}{N} \\sum_{i=1}^N I(\\max_{1 \\leq j \\leq m} T(Y_{ij}) \\geq 0.5).$\nAs with EMT, the standard choice of $m$ for this metric is $m = 25$ [Gehman et al., 2020, Wang et al., 2024].\nToxicity Fraction (TF) [Bommasani et al., 2023]. TF is defined as the fraction of generations that are classified as toxic:\n$TF = \\frac{1}{N} \\sum_{i=1}^N \\frac{1}{m} \\sum_{j=1}^m I(T(Y_{ij}) \\geq 0.5),$\nThis metric effectively estimates the likelihood that responses generated by M on prompts from $P_X$ contain toxic text [Bommasani et al., 2023]. Note that while the standard choice of $m$ for this metric is $m = 1$ [Bommasani et al., 2023], a larger value of $m$ may be preferred in practice if sampling a large N is infeasible."}, {"title": "Stereotype Metrics", "content": "Stereotype metrics aim to identify harmful stereotypes specific to protected attributes that might be present in an LLM's output. Because these metrics rely on mentions of protected attribute groups, these metrics may be unnecessary if FTU is satisfied for an LLM use case. Among stereotype metrics, this work distinguishes between metrics based on co-occurrence of protected attribute words and stereotypical words, and metrics that leverage a stereotype classifier."}, {"title": "Co-occurrence-Based Metrics", "content": "This section outlines a set of metrics that assess stereotype risk based on relative co-occurrence of protected attribute words with neutral words of interest. In particular, formal definitions of two co-occurrence-based stereotype metrics are provided: Co-Occurrence Bias Score and Stereotypical Associations.\nCo-Occurence Bias Score (COBS) [Bordia and Bowman, 2019]. Given two protected attribute groups $G', G''$ with associated sets of protected attribute words $A', A''$, a set of stereotypical words $W$, and an LLM use case $(M, P_X)$, the full calculation of COBS is as follows:\n$P(w|A) = \\frac{\\sum_{i=1}^N cooccur(w, A'|Y_i)/\\sum_{i=1}^N cooccur(w, A''|\\tilde{Y}_i)}{\\sum_{i=1}^N \\sum_{\\alpha \\in A'} C(\\alpha, Y_i)/\\sum_{i=1}^N \\sum_{\\alpha \\in A''} C(\\alpha, \\tilde{Y}_i)}$\n$\\begin{aligned}\nCOBS = \\frac{1}{|W|} \\sum_{w \\in W} log \\frac{P(w|A')}{P(w|A'')}\n\\end{aligned}$\nAbove, $C(x, Y_i)$ denotes the count of $x$ in $Y_i$ and $\\tilde{Y}_i$ represents the LLM output $Y_i$, with words from $A' \\cup A''$ and stop words excluded. The co-occurrence function $cooccur(w, A|Y)$ computes a weighted count of words from $A$ that are found within a context window centered around $w$, each time $w$ appears in $Y$. In words, COBS computes the relative likelihood that an LLM M generates output having co-occurrence of $w \\in W$ with $A'$ versus $A''$. This metric has a range of possible values of $[-\\infty, \\infty]$, with values closer to 0 signifying a greater degree of fairness.\nStereotypical Associations (SA) [Bommasani et al., 2023]. Consider a set of protected attribute groups $G$, an associated set of protected attribute lexicons $A$, and an associated set of stereotypical words $W$. Additionally, let $C(x, Y)$ denote the number of times that the word $x$ appears in the output $Y$, $I(\\cdot)$ denote the indicator function, $P_{ref}$ denote a reference distribution, and $TVD$ denote total variation difference. For a given LLM $M(X; \\theta)$ and a sample of prompts $X_1, ..., X_N$ drawn from $P_X$, the full computation of SA is as follows:\n$\\gamma(w|A') = \\sum_{\\alpha \\in A'} \\sum_{i=1}^N C(\\alpha, Y_i) I(C(w, Y_i) > 0)$\n$\\pi(w|A') = \\frac{\\gamma(w|A')}{\\sum_{A \\in A} \\gamma(w|A')}$\n$\\mathbb{P}(w) = \\{\\pi(w|A'): A' \\in A\\}$\n$SA = \\frac{1}{|W|} \\sum_{w \\in W} TVD(\\mathbb{P}(w), \\mathbb{P}_{ref}).$\nIn words, SA measures the relative co-occurrence of a set of stereotypically associated words across protected attribute groups. SA ranges in value from 0 to 1, where smaller values indicate greater fairness."}, {"title": "Metrics Leveraging a Stereotype Classifier", "content": "It has been shown that stereotype classifiers can be an effective tool for assessing stereotype risk in LLM use cases [Zekun et al., 2023]. This section introduces three new metrics by extending the toxicity metrics outlined in Section 3.1.1, leveraging a pre-trained stereotype classifier $S_t : Y \\rightarrow [0,1]$ rather than a toxicity classifier. Namely, these metrics include: Expected Maximum Stereotype, Stereotype Probability, and Stereotype Fraction. All three metrics range in values from 0 to 1, with smaller values indicating a greater degree of fairness.\nExpected Maximum Stereotype (EMS). EMS, analogous to EMT, estimates the maximum predicted stereotype probability among the top $m$ generations:\n$EMS = \\frac{1}{N} \\sum_{i=1}^N \\max_{1 \\leq j \\leq m} S_t(Y_{ij}).$\nFollowing the convention of EMT, practitioners may wish to use $m = 25$ for this metric."}, {"title": "Counterfactual Fairness Metrics", "content": "Counterfactual metrics aim to assess differences in LLM output when different protected attributes are mentioned in input prompts, all else equal. Given two protected attribute groups $G', G''$, these metrics are defined in the context of an LLM use case $(M, P_X)$. In particular, these metrics are evaluated on a sample of counterfactual response pairs $(\\hat{Y}'_1, \\hat{Y}''_1), ..., (\\hat{Y}'_N, \\hat{Y}''_N)$ generated by M, from a sample of counterfactual input pairs $(X_1, X'_1), ..., (X_N, X''_N)$ drawn from $P_{X|A}$. Note that, in scenarios where a large N is infeasible, practitioners may opt to generate multiple response pairs per counterfactual input pair.\nThese metrics, which are categorized into counterfactual similarity metrics and counterfactual sentiment metrics, respectively quantify the differences in text similarity and sentiment by leveraging the variations in LLM output observed across counterfactual input pairs. Due to their reliance on mentions of protected attributes in input prompts, if FTU is satisfied for an LLM use case, these metrics need not be used."}, {"title": "Counterfactual Similarity", "content": "Counterfactual similarity metrics measure the similarity in outputs generated from counterfactual input pairs according to a specified invariance metric $v$", "metrics": "Counterfactual ROUGE-L", "follows": "n$r = \\frac{LCS(\\hat{Y"}, ", \\hat{Y}", ")}{len(\\hat{Y}", "n$r' = \\frac{LCS(\\hat{Y}', \\hat{Y}'')}{len(\\hat{Y}'')}$\n$CROUGE-L = \\frac{1}{N} \\sum_{i=1}^N \\frac{2rr'}{r+r'},$\nwhere $LCS(\\cdot, \\cdot)$ denotes the longest common subsequence of tokens between two LLM outputs, and $len(\\hat{Y})$ denotes the number of tokens in an LLM output. The CROUGE-L metric effectively uses ROUGE-L to assess similarity as the longest common subsequence (LCS) relative to generated text length.\nGiven its reliance on matching token sequences, practitioners should mask protected attribute words in coun-terfactual output pairs before computing CROUGE-L. For instance, suppose, for the counterfactual input pair (X', X'') = ('What did he do next', \u2018What did she do next\u2019), an LLM generates the output pair (Y',\u0176'') = ('then he drove his car to work', 'then she drove her car to work'). In this context, these two responses are effectively identical. Masking the tokens {\u2018he\u2019, \u2018she\u2019, \u2018his\u2019, \u2018her\u2019} accomplishes this computationally.\nCounterfactual BLEU (CBLEU). This work introduces CBLEU, defined as the average BLEU score [Papineni et al., 2002] over counterfactually generated output pairs. The full calculation of CBLEU is as follows:\n$precision(\\hat{Y}', \\hat{Y}'') = \\frac{\\sum_{snt \\in \\hat{Y}'} \\sum_{b-gram \\in snt} min(C'(b-gram, \\hat{Y}'|\\hat{Y}''), C(b-gram, \\hat{Y}''))}{\\sum_{snt \\in \\hat{Y}'} \\sum_{b-gram \\in snt} C(b-gram, \\hat{```json\n{\n  \"title\": \"AN ACTIONABLE FRAMEWORK FOR ASSESSING BIAS AND FAIRNESS IN LARGE LANGUAGE MODEL USE CASES\",\n  \"authors\": [\n    \"Dylan Bouchard\"\n  ],\n  \"abstract\":", "Large language models (LLMs) can exhibit bias in a variety of ways. Such biases can create or exacerbate unfair outcomes for certain groups within a protected attribute, including, but not limited to sex, race, sexual orientation, or age. This paper aims to provide a technical guide for practitioners to assess bias and fairness risks in LLM use cases. The main contribution of this work is a decision framework that allows practitioners to determine which metrics to use for a specific LLM use case. To achieve this, this study categorizes LLM bias and fairness risks, maps those risks to a taxonomy of LLM use cases, and then formally defines various metrics to assess each type of risk. As part of this work, several new bias and fairness metrics are introduced, including innovative counterfactual metrics as well as metrics based on stereotype classifiers. Instead of focusing solely on the model itself, the sensitivity of both prompt-risk and model-risk are taken into account by defining evaluations at the level of an LLM use case, characterized by a model and a population of prompts. Furthermore, because all of the evaluation metrics are calculated solely using the LLM output, the proposed framework is highly practical and easily actionable for practitioners.", "sections\": [\n    {\n      \"title\": \"Introduction", "content\": \"The versatility of current Large Language Models (LLMs) in handling various tasks [Minaee et al., 2024, Liu et al., 2023, Ray, 2023] presents challenges when it comes to evaluating bias and fairness at the model level. Existing approaches primarily focus on assessing risk using benchmark data sets containing predefined prompts [Gehman et al., 2020, Dhamala et al., 2021, Nozza et al., 2021, Smith et al., 2022, Parrish et al., 2021, Li et al., 2020], masked tokens [Zhao et al., 2018, Rudinger et al., 2018, Nadeem et al., 2021, Levy et al., 2021], or unmasked sentences [Nangia et al., 2020, Barikeri et al., 2021, Jiao et al., 2023, Felkner et al., 2023], assuming that these adequately capture specific bias or fairness risks [Gallegos et al., 2023]. However, these assessments are likely to overestimate the risk for use cases where the population of prompts is low risk [Wang et al., 2024]. Moreover, to the best of the author's knowledge, the current literature does not provide a framework for effectively aligning LLM use cases with suitable metrics for evaluating bias and fairness.\nThis work aims to address these limitations by developing an actionable LLM bias and fairness evaluation framework defined at the use case level. Drawing inspiration from the classification fairness framework proposed by Saleiro et al. [2018], the framework proposed in this work enables practitioners to map an LLM use case to an appropriate set of bias and fairness evaluation metrics by considering relevant characteristics of the use case and stakeholder values. This evaluation approach is unique in that it incorporates actual prompts from the practitioner's use case, taking into account the prompt-specific risks that have been demonstrated to significantly increase the likelihood of biased and unfair outcomes [Wang et al., 2024]. By constraining the scope to focused use cases, where prompts are derived from a known population and the task is well-defined, this framework is specifically designed to customize the risk assessment for a specific application.\nTo introduce the framework, this study first provides formal definitions of bias and fairness desiderata for LLMs from the literature and segment these definitions by risk category. Subsequently, those risks are mapped to a taxonomy of use cases focused on large-scale applications, where human-in-the-loop may be infeasible. Lastly, for each risk category, various bias and fairness evaluation metrics are detailed, with discussions provided on their input requirements, calculation methods, the risks they assess, and circumstances under which they should be applied. As part of this work, a variety of novel bias and fairness metrics are introduced. This includes innovative counterfactual adaptations of recall-oriented understudy for gisting evaluation (ROUGE) [Lin, 2004], bilingual evaluation understudy (BLEU) [Papineni et al., 2002], and cosine similarity [Singhal and Google, 2001], as well a set of stereotype classifier-based metrics that are adapted from analogous toxicity classifier-based metrics.\nFor practical reasons, this study limits the selection of LLM bias and fairness metrics to those requiring only LLM generated output as inputs. This includes 1) generated text metrics, which take a generated set of tokens as input [Gallegos et al., 2023], 2) recommendation fairness metrics, calculated on a set of LLM-provided recommendations [Zhang et al., 2023], and 3) classification fairness metrics, which are already well-established in the machine learning fairness literature [Bellamy et al., 2018, Saleiro et al., 2018, Weerts et al., 2023, Hardt et al., 2016, Feldman et al., 2014, Mehrabi et al., 2019]. Due to practical limitations of the input requirements, the framework omits both embedding-based metrics, which are computed using an LLM's hidden vector representations of words or sentences [Islam et al., 2016, May et al., 2019, Guo and Caliskan, 2020], and probability-based metrics, which leverage predicted token probabilities from an LLM [Webster et al., 2020, Kurita et al., 2019, Ahn and Oh, 2021, Kaneko and Bollegala, 2021, Salazar et al., 2019, Barikeri et al., 2021, Nangia et al., 2020, Nadeem et al., 2021]. To further support the rationale behind the selection of evaluation metrics, it is important to note that metrics focused on the downstream task, consistent with the metrics incorporated in this framework, have been shown to be more reliable than metrics derived from embeddings or token probabilities [Goldfarb-Tarrant et al., 2020, Delobelle et al., 2021].\nThe remainder of this paper is organized as follows. Section 2 formally defines various notions of bias and fairness in LLMs. Section 3 discusses various techniques for conducting bias and fairness assessments in LLMs. Section 4 offers a framework for choosing among LLM bias and fairness metrics based on use case characteristics and stakeholder values. Finally, Section 5 offers concluding remarks."], "content": "This section formally defines several pre-requisite terms and concepts upon which subsequent sections rely, several of which are adapted from those provided by Gallegos et al. [2023]. The discussions in this paper explore concepts of"}, {"title": "Preliminary Definitions", "content": "This section provides preliminary definitions to be used throughout the subsequent sections.\nLarge Language Model (LLM). An LLM $M : X \\rightarrow Y$ is a pre-trained, transformer-based model that maps a text sequence $X \\in X$ to an output $Y \\in Y$, where $X$ denotes the set of all possible text inputs (i.e. prompts) and the form of $\\hat{Y}$ is specific to the LLM and the use case [Gallegos et al., 2023]. Let $\\theta$ parameterize M, such that $\\hat{Y} = M(X;\\theta)$.\nPopulation of Prompts. A population of prompts, denoted $P_X$, is a collection of LLM inputs. To characterize well-defined use cases, subsequent sections refer to a 'known population of prompts', indicating that practitioners possess information about the prompt domain and are able draw representative samples from $P_X$. For instance, a population of prompts might consist of clinical notes, where each individual prompt includes a collection of notes, accompanied by specific instructions for the LLM to generate a summary [Chuang et al., 2024].\nLarge Language Model Use Case. An LLM use case is characterized by an LLM $M(X; \\theta)$ and a population of prompts $P_X$. In the interest of concise notation, LLM use cases will be hereafter denoted as $(M, P_X)$. An LLM use case is evaluated on a finite set of responses generated by $M(X; \\theta)$ from a sample of $N$ prompts $X_1, ..., X_N$, drawn from the population $P_X$.\nProtected Attribute Groups. A protected attribute group $G \\in G$ represents a subset of people characterized by a shared identity trait, where $G$ is a partition [Gallegos et al., 2023].\nProtected Attribute Group Lexicon. A protected attribute group lexicon $A \\in A$ is a collection of words that correspond to protected attribute group $G \\in G$.\nCounterfactual Input Pair. A counterfactual input pair is a pair of prompts, $X'$ and $X''$, which are identical in every way except the former mentions protected attribute group $G'$ and the latter mentions protected attribute group $G''$ [Gallegos et al., 2023]. For an LLM use case $(M, P_X)$, an evaluation set of counterfactual input pairs is denoted $(X'_1, X''_1), ..., (X'_N, X''_N)$. To create each pair, a prompt is drawn from the subset of prompts containing words from the protected attribute lexicon $A$, i.e. $P_{X|A} = \\{ X : X \\in P_X, X \\cap A \\neq \\emptyset \\}$, and counterfactual variations are obtained via counterfactual substitution.\nFairness Through Unawareness (FTU). Given a protected attribute lexicon $A$, an LLM use case $(M, P_X)$ satisfies FTU if for each $X \\in P_X$, $X \\cap A = (\\emptyset)$. In simpler terms, FTU implies none of the prompts for an LLM use case include any mention of a protected attribute word [Gallegos et al., 2023]."}, {"title": "LLM Bias and Fairness Risks", "content": "This section presents formal definitions of various notions of bias and fairness applicable to LLMs. A taxonomy for these definitions is proposed, organized by risk category. Specifically, the risk categories included in this taxonomy are toxicity, stereotyping, counterfactual fairness, and allocational harms."}, {"title": "Toxicity", "content": "Drawing from the definitions of toxicity and derogatory language outlined in the survey conducted by Gallegos et al. [2023], let the definition of toxic text encompass any offensive language that 1) launches attacks, issues threats, or incites hate or violence against a social group, or 2) includes the usage of pejorative slurs, insults, or any other forms of expression that specifically target and belittle a social group. To formalize this, a corresponding fairness desideratum is introduced below.\nNon-Toxicity. Let $T$ denote the set of all toxic phrases. An LLM use case $(M,P_X)$ exhibits non-toxicity if $M(X; \\theta) \\cap T = \\emptyset$ for each $X \\in P_X."}, {"title": "Stereotyping", "content": "Stereotyping is an important type of social bias that should be considered in the context of LLMs [Bommasani et al., 2023, Bordia and Bowman, 2019, Zekun et al., 2023]. This study follows the work by Gallegos et al. [2023], in which stereotypes are defined as negative generalizations about a protected attribute group, often reflected by differences in frequency with which various groups are linked to stereotyped terms [Bommasani et al., 2023]. To formalize this notion, two fairness desiderata are considered, which are also proposed by Gallegos et al. [2023]: equal group associations (EGA) and equal neutral associations (ENA).\nEqual Group Associations [Gallegos et al., 2023]. For two protected attribute groups $G', G''$, and a set of neutral words $W$, an LLM use case $(M, P_X)$ satisfies equal group associations if, for each $w \\in W$, $P(w \\in \\hat{Y} | \\hat{Y} \\cap A' \\neq \\emptyset) = P(w \\in \\hat{Y} | \\hat{Y} \\cap A'' \\neq \\emptyset)$. Put simply, equal group associations requires that each neutral word in $W$ is equally likely to be contained in the output of M regardless of which protected attribute group is mentioned.\nEqual Neutral Associations [Gallegos et al., 2023]. For two protected attribute groups $G', G''$, with respective asso-ciated lexicons $A', A''$, and a set of neutral words $W$, an LLM use case $(M, P_X)$ satisfies Equal Neutral Associations if, $P(A' \\cap \\hat{Y} \\neq \\emptyset)| \\hat{Y} \\cap W \\neq \\emptyset) = P(A'' \\cap \\hat{Y} \\neq \\emptyset)| \\hat{Y} \\cap W \\neq \\emptyset)$. In other words, Equal Neutral Associations requires that co-occurrence of protected attribute words with a set of neutral words W is equally probable for both protected attribute groups."}, {"title": "Counterfactual Fairness", "content": "In many contexts, it is undesirable for an LLM to generate substantially different output as a result of different protected attribute words contained in the input prompts, all else equal [Huang et al., 2020, Nozza et al., 2021, Wang et al., 2024]. Following previous work [Huang et al., 2020, Garg et al., 2019], this concept is hereafter referred to as (lack of) counterfactual fairness. Depending on context and stakeholder values, the practitioner may wish to assess an LLM for differences in overall content or sentiment resulting from inclusion of different protected attribute words in a prompt. Below, this section provides a formal definition of the corresponding fairness desiderata known as counterfactual invariance, adapted from Gallegos et al. [2023].\nCounterfactual Invariance. For two protected attribute groups $G', G''$, an LLM use case $(M, P_X)$ satisfies counter-factual invariance if, for a specified invariance metric $v(\\cdot,\\cdot)$, expected value of the invariance metric is less than some tolerance level $\\epsilon$:\n$\\mathbb{E}[v(M(X'; \\theta), M(X'';\\theta))] \\leq \\epsilon,$\nwhere $(X', X'')$ is a counterfactual input pair corresponding to $G', G''$ [Gallegos et al., 2023]."}, {"title": "Allocational Harms", "content": "Allocational harms, which Gallegos et al. [2023] define as an unequal distribution of resources or opportunities among different protected attribute groups, have been widely studied in the machine learning fairness literature [Saleiro et al., 2018, Bellamy et al., 2018, Weerts et al., 2023, Kamishima et al., 2012, Zhang et al., 2018, Hardt et al., 2016, Feldman et al., 2014, Pleiss et al., 2017, Kamiran et al., 2012, Agarwal et al., 2018, Kamiran and Calders, 2011, Chouldechova, 2016]. The corresponding fairness desideratum, is known as group fairness, defined formally below.\nGroup Fairness. Given two protected attribute groups $G', G''$, and a tolerance level $\\epsilon$, an LLM use case $(M, P_X)$ satisfies group fairness if\n$|B(M(X;\\theta)|G') - B(M(X;\\theta)|G'')| \\leq \\epsilon,$\nwhere $B$ is a statistical performance metric (e.g. false negative rate) applied to M, conditioned on membership in a protected attribute group [Gallegos et al., 2023]. Here, conditioning on $G$ implies calculating $B$ on the subset of input prompts that either contain a direct mention of group $G$ or, in the case of person-level prompt granularity, correspond to individuals belonging to group G. Note that the choice of $B$ will depend on context and stakeholder values."}, {"title": "Mapping Bias and Fairness Risks to LLM Use Cases", "content": "Section 2.1 establishes the characterization of a well-defined LLM use case based on a model and a known population of prompts. To set up the decision framework, this section segments use cases by task, according to the following three categories: 1) text generation and summarization, 2) classification, and 3) recommendation. Descriptions and examples are provided in Table 1.\nThe bias and fairness evaluation framework proposed in this work is intended for large-scale applications, in which the volume of generated responses makes exhaustive human review impractical. It is important to note that, for scenarios in which the practitioner manually evaluates each generated output, the evaluations proposed here may be unnecessary if concerns related to bias and fairness can be effectively addressed by the individual who is reviewing the outputs.\nText Generation and Summarization. First, consider use cases where an LLM generates text outputs that are not constrained to a predefined set of classes (e.g., positive vs. negative) or list elements (e.g., products to recommend). For the sake of brevity, this group of use cases will be hereafter referred to as \u201ctext generation and summarization,\u201d acknowledging that this category can encompass additional use cases including, but not limited to, machine translation, retrieval augmented generation (RAG), and question-answering. An example of a text generation use case could be utilizing an LLM to compose personalized messages for customer outreach. Similarly, a summarization use case may involve employing an LLM to extract pertinent information or provide summaries from clinical notes. These use cases carry the potential risk of generating toxic text in their outputs. Moreover, if these use cases fail to meet the criteria of FTU, meaning that the prompts include references to protected attributes, they also pose the risk of perpetuating stereotypes or exhibiting counterfactual unfairness.\nClassification. LLMs have been widely used for text classification [Sun et al., 2023, Widmann and Wich, 2022, Bonikowski et al., 2022, Howard and Ruder, 2018, Sun et al., 2019, Chai et al., 2020, Chen et al., 2020, Lin et al., 2021]. In the context of bias and fairness, it is important to distinguish whether the text inputs can be mapped to a protected attribute, either by containing direct mentions of a protected attribute group, or in the case of person-level prompt granularity, corresponding to individuals belonging to certain protected attribute groups. For instance, one example of a person-level classification use case could involve utilizing an LLM to classify customer feedback as positive or negative in order to assign appropriate follow-ups. Similar to traditional person-level classification challenges in machine learning, these use cases present the risk of allocational harms. On the other hand, classification use cases that do not involve person-level data and satisfy FTU are not subject to these bias and fairness risks.\nRecommendation. Recommendation is another potential application of LLMs [Bao et al., 2023, Gao et al., 2023], such as using an LLM to recommend products to customers. Zhang et al. [2023] show that LLMs used as recommendation engines can discriminate when exposed to protected attribute information. Given this concern, it follows that LLM recommendation use cases pose the risk of counterfactual unfairness if they do not satisfy FTU."}, {"title": "Bias and Fairness Evaluation Metrics", "content": "This section introduces a range of evaluation metrics segmented by the use case task and its applicable bias and fairness risks. The proposed framework encompasses three distinct use case categories: 1) text generation and summarization, 2) classification, and 3) recommendation. For each category, bias and fairness evaluation metrics are included that address the applicable risks.\nAccounting for prompt-specific risk is important to accurately reflect the risk of a specific use case, as Wang et al. [2024] find in their evaluations that toxicity probability is 26 to 101 times higher when dealing with toxic prompts compared to non-toxic prompts. Moreover, Goldfarb-Tarrant et al. [2020], Delobelle et al. [2021] have demonstrated that evaluation metrics which take into account the LLM's task provide a more accurate reflection of the associated risk compared to metrics based on embeddings or token probabilities. Accordingly, the objective of this work is to assess the risks associated with a particular use case, taking into consideration not only the LLM used but also the task at hand and the prompt population. Following the characterization of a use case based on a model and a known population of prompts, as established in Section 2.1, each metric definition presented below is contextualized within an evaluation sample of size N drawn from a known population of prompts $P_X."}, {"title": "Bias Metrics for Text Generation and Summarization Use Cases", "content": "Gallegos et al. [2023] propose a detailed taxonomy of evaluation metrics for bias evaluations in LLMs. They partition these metrics into three main categories: embedding-based metrics, probability-based metrics, and generated-text metrics. While embedding- and probability-based metrics require access to an LLM's upstream architecture, generated-text metrics instead treat an LLM like a black box and can be easily calculated from LLM output alone. Hence, given their ease of application, only generated-text metrics are included in the framework proposed here.\nWhile Gallegos et al. [2023] further segment generated-text metrics into three subcategories (namely distribution metrics, classifier metrics, and lexicon metrics), this work segments these metrics according to the risk taxonomy outlined in 2.2. In particular, generated-text bias and fairness metrics are segmented into the following categories: toxicity metrics, stereotype metrics, and counterfactual fairness metrics. Toxicity metrics leverage a pre-trained toxicity classifier, such as Perspective API, to assign a toxicity score to an LLM's output [Chowdhery et al., 2022, Lees et al., 2022, Wang et al., 2024, Bommasani et al., 2023, Gehman et al., 2020]. Stereotype metrics assess the relative co-occurrence of stereotype words with protected attribute words [Bordia and Bowman, 2019, Bommasani et al., 2023] or leverage a pre-trained stereotype classifier to assign a stereotype score to an LLM's output [Zekun et al., 2023]. Lastly, counterfactual fairness metrics assess fairness through a causal lens by exploiting variation in mentions of protected attributes in otherwise-identical prompts [Huang et al., 2020]. Select metrics are detailed in each category below."}, {"title": "Toxicity Metrics", "content": "Toxicity metrics leverage a pre-trained toxicity classifier to assess general toxicity that is not specific to a particular protected attribute. Given a toxicity classifier $T : Y \\rightarrow [0, 1]$, this section defines toxicity metrics in the context of an LLM use case $(M, P_X)$. Specifically, these metrics are evaluated on a set of $m$ responses per prompt, $\\{Y_{ij}\\}_{j=1}^m$, generated by M, from a sample of prompts $X_1, ..., X_N$ drawn from $P_X$. In practice, variation in responses for the same prompt can be achieved by choosing a non-zero temperature for the LLM or by utilizing various random seeds [Wang et al., 2024]. Below, formal definitions of three toxicity metrics are provided: Expected Maximum Toxicity, Toxicity Probability, and Toxic Fraction. All three metrics range in values from 0 to 1, with smaller values indicating a greater degree of fairness.\nExpected Maximum Toxicity (EMT) [Gehman et al., 2020]. EMT estimates the maximum predicted toxicity probability among the top $m$ generations:\n$EMT = \\frac{1}{N} \\sum_{i=1}^N \\max_{1 \\leq j \\leq m} T(Y_{ij}).$\nNote that the standard choice of $m$ for this metric is $m = 25$ [Gehman et al., 2020, Wang et al., 2024].\nToxicity Probability (TP) [Gehman et al., 2020]. TP is calculated as the empirical probability of having at least one toxic prediction ($T(\\hat{Y}) \\geq 0.5$) among the top $m$ generations:\n$TP = \\frac{1}{N} \\sum_{i=1}^N I(\\max_{1 \\leq j \\leq m} T(Y_{ij}) \\geq 0.5).$\nAs with EMT, the standard choice of $m$ for this metric is $m = 25$ [Gehman et al., 2020, Wang et al., 2024].\nToxicity Fraction (TF) [Bommasani et al., 2023]. TF is defined as the fraction of generations that are classified as toxic:\n$TF = \\frac{1}{N} \\sum_{i=1}^N \\frac{1}{m} \\sum_{j=1}^m I(T(Y_{ij}) \\geq 0.5),$\nThis metric effectively estimates the likelihood that responses generated by M on prompts from $P_X$ contain toxic text [Bommasani et al., 2023]. Note that while the standard choice of $m$ for this metric is $m = 1$ [Bommasani et al., 2023], a larger value of $m$ may be preferred in practice if sampling a large N is infeasible."}, {"title": "Stereotype Metrics", "content": "Stereotype metrics aim to identify harmful stereotypes specific to protected attributes that might be present in an LLM's output. Because these metrics rely on mentions of protected attribute groups, these metrics may be unnecessary if FTU is satisfied for an LLM use case. Among stereotype metrics, this work distinguishes between metrics based on co-occurrence of protected attribute words and stereotypical words, and metrics that leverage a stereotype classifier."}, {"title": "Co-occurrence-Based Metrics", "content": "This section outlines a set of metrics that assess stereotype risk based on relative co-occurrence of protected attribute words with neutral words of interest. In particular, formal definitions of two co-occurrence-based stereotype metrics are provided: Co-Occurrence Bias Score and Stereotypical Associations.\nCo-Occurence Bias Score (COBS) [Bordia and Bowman, 2019]. Given two protected attribute groups $G', G''$ with associated sets of protected attribute words $A', A''$, a set of stereotypical words $W$, and an LLM use case $(M, P_X)$, the full calculation of COBS is as follows:\n$P(w|A) = \\frac{\\sum_{i=1}^N cooccur(w, A'|Y_i)/\\sum_{i=1}^N cooccur(w, A''|\\tilde{Y}_i)}{\\sum_{i=1}^N \\sum_{\\alpha \\in A'} C(\\alpha, Y_i)/\\sum_{i=1}^N \\sum_{\\alpha \\in A''} C(\\alpha, \\tilde{Y}_i)}$\n$\\begin{aligned}\nCOBS = \\frac{1}{|W|} \\sum_{w \\in W} log \\frac{P(w|A')}{P(w|A'')}\n\\end{aligned}$\nAbove, $C(x, Y_i)$ denotes the count of $x$ in $Y_i$ and $\\tilde{Y}_i$ represents the LLM output $Y_i$, with words from $A' \\cup A''$ and stop words excluded. The co-occurrence function $cooccur(w, A|Y)$ computes a weighted count of words from $A$ that are found within a context window centered around $w$, each time $w$ appears in $Y$. In words, COBS computes the relative likelihood that an LLM M generates output having co-occurrence of $w \\in W$ with $A'$ versus $A''$. This metric has a range of possible values of $[-\\infty, \\infty]$, with values closer to 0 signifying a greater degree of fairness.\nStereotypical Associations (SA) [Bommasani et al., 2023]. Consider a set of protected attribute groups $G$, an associated set of protected attribute lexicons $A$, and an associated set of stereotypical words $W$. Additionally, let $C(x, Y)$ denote the number of times that the word $x$ appears in the output $Y$, $I(\\cdot)$ denote the indicator function, $P_{ref}$ denote a reference distribution, and $TVD$ denote total variation difference. For a given LLM $M(X; \\theta)$ and a sample of prompts $X_1, ..., X_N$ drawn from $P_X$, the full computation of SA is as follows:\n$\\gamma(w|A') = \\sum_{\\alpha \\in A'} \\sum_{i=1}^N C(\\alpha, Y_i) I(C(w, Y_i) > 0)$\n$\\pi(w|A') = \\frac{\\gamma(w|A')}{\\sum_{A \\in A} \\gamma(w|A')}$\n$\\mathbb{P}(w) = \\{\\pi(w|A'): A' \\in A\\}$\n$SA = \\frac{1}{|W|} \\sum_{w \\in W} TVD(\\mathbb{P}(w), \\mathbb{P}_{ref}).$\nIn words, SA measures the relative co-occurrence of a set of stereotypically associated words across protected attribute groups. SA ranges in value from 0 to 1, where smaller values indicate greater fairness."}, {"title": "Metrics Leveraging a Stereotype Classifier", "content": "It has been shown that stereotype classifiers can be an effective tool for assessing stereotype risk in LLM use cases [Zekun et al., 2023]. This section introduces three new metrics by extending the toxicity metrics outlined in Section 3.1.1, leveraging a pre-trained stereotype classifier $S_t : Y \\rightarrow [0,1]$ rather than a toxicity classifier. Namely, these metrics include: Expected Maximum Stereotype, Stereotype Probability, and Stereotype Fraction. All three metrics range in values from 0 to 1, with smaller values indicating a greater degree of fairness.\nExpected Maximum Stereotype (EMS). EMS, analogous to EMT, estimates the maximum predicted stereotype probability among the top $m$ generations:\n$EMS = \\frac{1}{N} \\sum_{i=1}^N \\max_{1 \\leq j \\leq m} S_t(Y_{ij}).$\nFollowing the convention of EMT, practitioners may wish to use $m = 25$ for this metric."}, {"title": "Counterfactual Fairness Metrics", "content": "Counterfactual metrics aim to assess differences in LLM output when different protected attributes are mentioned in input prompts, all else equal. Given two protected attribute groups $G', G''$, these metrics are defined in the context of an LLM use case $(M, P_X)$. In particular, these metrics are evaluated on a sample of counterfactual response pairs $(\\hat{Y}'_1, \\hat{Y}''_1), ..., (\\hat{Y}'_N, \\hat{Y}''_N)$ generated by M, from a sample of counterfactual input pairs $(X_1, X'_1), ..., (X_N, X''_N)$ drawn from $P_{X|A}$. Note that, in scenarios where a large N is infeasible, practitioners may opt to generate multiple response pairs per counterfactual input pair.\nThese metrics, which are categorized into counterfactual similarity metrics and counterfactual sentiment metrics, respectively quantify the differences in text similarity and sentiment by leveraging the variations in LLM output observed across counterfactual input pairs. Due to their reliance on mentions of protected attributes in input prompts, if FTU is satisfied for an LLM use case, these metrics need not be used."}, {"title": "Counterfactual Similarity", "content": "Counterfactual similarity metrics measure the similarity in outputs generated from counterfactual input pairs according to a specified invariance metric $v$", "metrics": "Counterfactual ROUGE-L", "follows": "n$r = \\frac{LCS(\\hat{Y"}, {"What did he do next": "What did she do next\u2019)", "then he drove his car to work": "then she drove her car to work", "follows": "n$precision(\\hat{Y}', \\hat{Y}'') = \\frac{\\sum_{snt \\in \\hat{Y}'} \\sum_{b-gram \\in snt} min(C'(b-gram, \\hat{Y}'|\\hat{Y}''), C(b-gram, \\hat{Y}''))}{\\sum_{snt \\in \\hat{Y}'} \\sum_{b-gram \\in snt} C(b-gram, \\hat{"}]