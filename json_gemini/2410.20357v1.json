{"title": "Dynamics as Prompts: In-Context Learning for Sim-to-Real System Identifications", "authors": ["Xilun Zhang", "Shiqi Liu", "Peide Huang", "William Jongwon Han", "Yiqi Lyu", "Mengdi Xu", "Ding Zhao"], "abstract": "Sim-to-real transfer remains a significant challenge in robotics due to the discrepancies between simulated and real-world dynamics. Traditional methods like Domain Randomization often fail to capture fine-grained dynamics, limiting their effectiveness for precise control tasks. In this work, we propose a novel approach that dynamically adjusts simulation environment parameters online using in-context learning. By leveraging past interaction histories as context, our method adapts the simulation environment dynamics to real-world dynamics without requiring gradient updates, resulting in faster and more accurate alignment between simulated and real-world performance. We validate our approach across two tasks: object scooping and table air hockey. In the sim-to-sim evaluations, our method significantly outperforms the baselines on environment parameter estimation by 80% and 42% in the object scooping and table air hockey setups, respectively. Furthermore, our method achieves at least 70% success rate in sim-to-real transfer on object scooping across three different objects. By incorporating historical interaction data, our approach delivers efficient and smooth system identification, advancing the deployment of robots in dynamic real-world scenarios.", "sections": [{"title": "I. INTRODUCTION", "content": "Learning-based methods like deep Reinforcement Learning (RL) allow robots to tackle complex tasks in areas such as object manipulation [1], [2] and locomotion for quadrupedal robots [3], [4] and humanoids [5], [6]. However, RL's high sample complexity and risks of unsafe exploration [7]\u2013[9] make it necessary to train policies in simulations and then deploy in the real world. A key challenge is the sim-to-real gap, caused by discrepancies between simulated and real-world dynamics [10]-[12], which can lead to catastrophic failures during deployment.\nTraditional sim-to-real approaches aim to develop robust policies by randomizing environment parameters during training, known as Domain Randomization (DR) [1], [13]. While effective in some cases [3], [13], DR captures only average dynamics, limiting precision in fine-grained control tasks. In contrast, System Identification (SysID) methods aim to align the simulation and real-world performance through actively adjusting the simulation environment parameters, which often requiring iterative SysID model updates to test new parameters [14], [15]. For instance, in a kitchen environment, when a robot tries to scoop grilled celery from a pan (Figure 1), traditional offline SysID methods would involve learning a new SysID model that predict the center of mass of the celery at each iteration, making the process time-consuming and inefficient. Humans, on the other hand, can quickly adapt online. A more intuitive solution is to develop a model with online SysID, allowing for more efficient parameter estimation across different environment dynamics. In-context learning has gained traction as a method for adjusting model behavior without gradient updates, widely used in Natual Language Processing (NLP) [16] and recently applied in robotics to improve generalization [17]\u2013[19]. For example, Xu et al. [19] enhanced the Decision Transformer (DT) [20] by using new task demonstrations as prompts for online adaptation. Most current in-context learning approaches focus on adapting policies when rewards or expert demonstrations changes, assuming fixed environment dynamics [18], [21]. While different dynamics could be framed as diverse tasks in a multi-task RL setting, more than it's a counter-intuitive setting, it also becomes impractical with a high-dimensional continuous environment parameter space, requiring many tasks to capture the full range of behaviors. In this paper, we explore a novel question: \u201cCan we adapt simulation environment parameters using the in-context learning paradigm?\u201d Our goal is to eliminate the optimization loop in SysID, in order to accelerate the parameter estimation process by incorporating the in-context learning ability of transformer models.\nWe introduce in-Context AdaPTation module for sim-to-REal system identification, or CAPTURE, to bridge the sim-to-real gap. CAPTURE aims to dynamically adjust the environment parameters online to align simulated and real-world trajectories using next-token prediction based on past interaction data, which includes simulated trajectories, actions, environment parameters, and real-world trajectories as shown in Figure 1. CAPTURE frames the SysID problem as an in-context learning formulation, treating interaction histories as \"context.\" Unlike existing techniques [4], [22], [23] that rely on short state-action history, CAPTURE aim to learn the complex SysID search process itself through rich and multi-episodic interaction history data. Beyond learning single-step expert parameter matching behaviors, longer interaction histories enables the learned SysID causal transformer to capture a better dynamics representation of the environments. By incorporating in-context learning, CAPTURE provides a smoother and more accurate prediction of subsequent environmental parameters and dynamic behaviors.\nIn summary, this study makes the following contributions:\n1) We propose a novel method that can identify real-world environment parameters without any network parameter updates using in-context learning."}, {"title": "II. RELATED WORK", "content": "Sim-to-real transfer is a pivotal area of robotics research, focusing on the application of simulation-trained models to real-world tasks. DR involves injecting variability into the parameters of the simulation environment regarding dynamical or visual attributes [1], [13], but struggle with over-conservative or average task behaviours. In the following subsections, prior works on SysID for domain adaptation and in-context learning will be discussed in more detail."}, {"title": "A. Sim-to-Real SysID for Domain Adaptation", "content": "There are two primary approaches on SysID for sim-to-real transfer: offline and online. Offline SysID typically requires iterative refinement of the identification module through repeated training cycles [14], [15], [24]\u2013[27]. In contrast, online SysID focuses on the determination of environment parameters or latent variables without the need for model updates. This approach has proven effective in highly dynamic systems, employing strategies such as RMA [4], which leverages short-term historical state-action pairs to infer environment dynamics [22], [28]\u2013[30]. [31] describes exploring the object dynamics through curiosity-driven exploration first and then deploying on the task environment. [23] propose a meta-learning framework, prioritizing task-specific adaptation over simple trajectory alignment. In addition to aligning environment parameters, [32] introduced a human-in-the-loop correction method to mitigate the sim-to-real gap. More recently, [33] proposed reconstructing real-world environmental variations in simulation to enhance the generalizability on real-world policy deployment. Most relevant to our work, IIDA [29] uses long-term historical state action pairs to infer latent real-world dynamic models. In contrast, our method focuses on distilling the sim-to-real parameter update process to create more accurate simulation environments, effectively closing the sim-to-real gap."}, {"title": "B. In-context Learning in Robotics", "content": "In-context learning has garnered significant attention in NLP [16], [34] and computer vision [35] due to its remarkable ability to infer tasks from context. This ability to infer tasks through contextual information, such as expert demonstrations, allows for adaptation to new tasks without updating the model's weights [36], which has been shown to be beneficial in robotics settings [19], [37]\u2013[41]. The potential of in-context learning for generalizing to unseen tasks has been further explored in recent studies. Laskin et al. [17] employed transformer models to distill the RL learning history, showing RL algorithms can be distilled into transformer models and successfully in-context adapt to new goal settings [18]. Previous work on in-context adaptation has either focused on RL algorithm distillation or policy generalization abilities, where CAPTURE focuses on learning environment parameters through interaction histories."}, {"title": "III. METHODOLOGY", "content": "In this work, we take a different approach from traditional in-context policy adaptation. Instead of directly adapting the task policy, we focus on using the history of environment parameter updates to estimate the next-iteration environment parameters such that the simulation and real-world performance can be aligned. We assume that the sim-to-real performance gap will narrow as the discrepancy between the simulation and real-world environment parameters decreases."}, {"title": "A. Problem Formulation", "content": "In this section, we define the task of predicting accurate simulation environment parameters to align simulated dynamics with real-world environments. We outline how to model the interaction between dynamics behaviours and environment parameters as a sequence, forming the training data for SysID causal transformer models. We begin by introducing the simulation parameters, followed by the task policy and data generation notations, and conclude with the SysID causal transformer notations for domain adaptation.\nEnvironment Parameter Space. We define the task-related environment parameter space $ \\epsilon \\in E $ that parameterized quantities such as the center of mass and sliding frictions. We also assume that the environment parameter space $E$ is finite and bounded, encompassing properties of different objects. We modify the environment parameters with Robosuite [42], which provides API for modifying the environment parameters through Python code.\nSysID Causal Transformer and Interaction Histories. During the SysID causal transformer and data collection setting, we use previous SysID iterations as context, including simulated state trajectories $ \\tau^{sim} = \\{ s_0^{sim}, s_1^{sim}, ..., s_T^{sim} \\} $, real state trajectories $ \\tau^{real} = \\{ s_0^{real}, s_1^{real}, ..., s_T^{real} \\} $, rollout action $ a \\sim \\pi(a | s_0, \\epsilon) $, and the past environment parameters $ \\epsilon $. A robust SysID process explores complex parameter behaviours. We aim to capture this behavior using a causal transformer that infers parameters from past interactions. Following [17], we treat these sequential interactions as history, where current environment parameters depend on previous SysID iterations. Formally, we define the history as:\n$ h_i = (\\epsilon_i, a_i, \\tau_i^{sim}, \\tau_i^{real}, ..., \\epsilon_{i-k}, a_{i-1}, \\tau_{i-1}^{sim}, \\tau_{i-1}^{real}) $                                                                     (1)\nwhere $ h_i $ is the history containing the past k iterations at i-th iteration. Our goal is to learn a causal transformer such that it can replicate the SysID process given history. We define the SysID causal transformer, $ P_{\\theta} $, with the objective of modeling the distribution of simulation parameters conditioned on the history. This encourages the simulated trajectories $ \\tau^{sim} $ eventually behave close to real-world trajectories, $ \\tau^{real} $, bridge the sim-to-real gap.\nGiven an ideal search strategy that successfully adapts to the target environment parameters, our goal is to learn the underlying search capabilities from this process by predicting the next iteration in the history. The optimization objective can be formalized as:\n$ \\theta^* = arg \\min_{\\theta} [L(P_{\\theta}(h_i), \\epsilon_i^{sim})] $                                                                    (2)\nwhere $ P_{\\theta}(h_i) $ represents the predicted next environment parameters from the model, $ L(.) $ is the Mean-Square-Error (MSE) loss function that measures the discrepancy between the predicted and the ground-truth next-iteration environment parameters."}, {"title": "B. Environment-Conditioned RL Training", "content": "The environment-conditioned RL task policy $ \\pi(a | s_0, \\epsilon) $ is trained to adapt to varying environment parameters $ \\epsilon \\in E $. For each episode, $ \\epsilon $ is sampled uniformly from the parameter space $ E $. Within the episode, the agent selects an action a from $ \\pi(a | s_0, \\epsilon) $, considering the initial state $ s_0 $ and environment parameter $ \\epsilon $. This action is executed, producing a state trajectories $ \\{s_1, s_2, ..., s_T \\} $ and a reward r. Each episode $ \\{ a, r, s_0, \\epsilon \\} $, is stored in the replay buffer. After certain episodes, the policy is updated using Soft Actor-Critic (SAC) [43], refining actions for smoother domain adaptation with predicted parameters."}, {"title": "C. Source-to-Target SysID Iteration Generation", "content": "In the data generation process, we develop source-to-target adaptation transitions that mimic sim-to-real adaptation. Each iteration includes four elements: the current simulation parameter $ \\epsilon_i $, the policy action $ a_i $, the simulated trajectories $ \\tau^{source} $, and the collected target environment trajectories $ \\tau^{Target} $ under the same action $ a_i $. The trajectories and actions are obtained through simulation rollouts using an environment-conditioned task policy.\nIn simulation, both source and target values are known, allowing for direct single-step mapping from source to target. However, this approach often performs poorly in real-world deployment when the target's dynamics representation (state trajectories) lacks sufficient detail. Rather than learning a single-step mapping, we focus on learning a search algorithm that finds the target environment parameter with dynamic representations. The duration of the parameter iteration history K indicates the number of iterations that we pre-defined to generate a complete transition sequence from $ \\epsilon^{source} $ to $ \\epsilon^{target} $. We pick a transition number K = 7 during data generation.\nIn the sim-to-real SysID setting, a search algorithm must balance exploration and precision, as it lacks the ground-truth target value and relies only on performance labels (higher or lower). Linear interpolation is suboptimal here because it limits exploration during adaptation. To overcome this, we propose emulating a randomized binary search process [44], which optimally navigates a constrained space by dynamically adjusting the upper and lower search bounds at each iteration. To further promote exploration, we use a beta distribution when selecting the environment parameters for the next iteration. An ablation study is discussed in Section IV-B on how different search algorithms impact parameter estimation. The transition iteration generation process is illustrated in Figure 3, and the formal pseudocode is described in Algorithm 1."}, {"title": "D. SysID Causal Transformer", "content": "Given the collected SysID parameter transition histories, D, our goal is to distill the binary search process through parameter transition sequences with length K, where each iteration represents an adaptation iteration. The model predicts the next environment parameter $ \\epsilon_{i+1} $ at iteration i using a next-token prediction framework with a shifted input setup [45]. We sample a multi-episode window of size k from D, where k is a subsequence of the full K iterations. The SysID causal transformer processes this history to predict the next parameter. Each iteration block contains 2+ 2T tokens: one action, one parameter, and T state trajectory tokens for both simulated and real rollouts.\nDuring rollout, the model attends to preceding tokens to predict $ \\epsilon_{i+1} $ using relative timestep embedding [46] to focus on subsequence order. Starting with initial tokens $ \\{ \\epsilon_0, a_0, \\tau^{sim}, \\tau^{real} \\} $, we update actions with an environment-conditioned policy in the new simulation $ \\epsilon_{i+1} $ and initial state, obtaining updated trajectories $ \\tau^{sim} $ and $ \\tau^{real} $. The process is detailed in Algorithm 2."}, {"title": "IV. EXPERIMENTS", "content": "We conducted two sets of experiments to evaluate the performance of CAPTURE: object scooping and table air hockey. In both tasks, we demonstrated that CAPTURE significantly outperforms the baselines in both sim-to-sim and sim-to-real transfer scenarios. The experiment setups will be explained in Section IV-A, followed by descriptions of our baseline and ablation methods in Section IV-B. The sim-to-sim evaluation results compared with baselines and ablations results are detailed in Section IV-C, and the sim-to-real experiment results compared with baselines are described in Section IV-D."}, {"title": "A. Experimental Setups", "content": "We evaluate our algorithm using two tasks: object scooping and table air hockey. For object scooping, inspired by [31], [47], the goal is to identify the object's center of mass in kitchen scenarios, which often involve complex items like celery, carrots, and eggplants with varying centers of mass. We aim to determine the balance point for successful scooping through online interactions using CAPTURE.\nIn table air hockey, we test the scalability of CAPTURE with a higher-dimensional parameter and action space [15], [48]. This task requires precise control and adaptability to match simulated and real-world dynamics. Tunable environment parameters are listed in Table I, with setups shown in Figure 4.\nObject Scooping. In this task, our objective is to identify the optimal scooping points during food transfer from one toasting pan to another using a spatula. In this setting, CAPTURE needs to identify the center of mass noted as $ X_{com} $, and then scoop at the corresponding placement such that the object can be balanced on the spatula. The range of $ X_{com} $ is defined based on the relative position of the objects, where -1.0 means the center of mass located at the most left of the object, and vice versa. To handle pose estimation uncertainties, a classifier labels the object as tilted left (-1), right (1), or balanced (0) and uses them as state trajectories.\nTable Air-Hockey. The setup involves a robot-controlled mallet hitting a puck on an air-hockey table. The table is divided into left and right sections with different friction levels, causing varied puck behavior. We expect CAPTURE to learn surface friction and damping differences from both sides via incorporating context information. The five parameters considered are left-surface friction $ \\mu_{left} $, right-surface friction $ \\mu_{right} $, left-wall damping $ \\zeta_{left} $, right-wall damping $ \\zeta_{right} $, and puck damping $ \\zeta_{puck} $. Lower absolute damping values make objects more responsive, and trajectory evaluation is based on the sum of point-wise L2 distances."}, {"title": "B. Baselines and Ablations", "content": "To discover how different module of CAPTURE affects the performances, the baselines aim to demonstrate the effec-"}, {"title": "C. Sim-to-Sim SysID Evaluation", "content": "In the sim-to-sim transfer, we evaluate whether CAPTURE can align trajectories by adjusting the environment parameters in-context without updating the model's parameters. We simulated 100 pairs of random environment parameters to mimic unknown real dynamics and test the performance across three seeds. For each pair, one simulation environment is designated as the \"real\" (target) environment, where only the dynamics performance is provided to the model, not the parameters. To improve parameter estimation independent of actions, we roll out the model with an environment-conditioned policy for online evaluation, as described in Section III-B. In the results, baseline methods are shown with solid lines, while dashed lines indicate different ablation settings for data collection.\nObject Scooping Sim-to-Sim Evaluation. In the sim-to-sim transfer, we evaluated the normalized context differences, which are one-dimensional in this setting, as shown in Figure 5. Since we use an angle classifier for smoother real-world deployment, reporting trajectory differences would be meaningless, as the trajectory here is represented by a label. Instead, we measure the task's success rate, defined as lift the object with label (0). Figure 5 shows that CAPTURE achieves a success rate 50% higher than other SysID methods and 70% higher than the DR approach. This is expected, as the baselines lack historical interaction data, making identification only dependent on current scooping points. In contrast, CAPTURE uses a rich previous interaction history, allowing it to gradually narrow down the center of mass search space.\nTable Air Hockey Sim-to-Sim Evaluation. In Figure 6, CAPTURE offers better parameter estimation with smoother and more accurate adaptation curves. In scenarios where environment parameters require rollout histories, baselines struggle due to their inability to account for historical interactions. For instance, while the ED method might successfully detect the left wall after hitting it, it tends to forget earlier right wall interactions. This short-term memory leads to faster adaptation in simple environments but falls short in more complex ones. In dynamic settings, where SysID needs to identify parameters on both sides for sustained task performance, maintaining a history of parameter updates becomes critical, as it informs subsequent iterations.\nIn Table II, we show that with lower context differences between the source and target, the point-wise L2 trajectory distance also becomes smaller accordingly. CAPTURE are able to improve trajectory differences by about 40% compared to identification baselines, and 50% compared to DR."}, {"title": "D. Sim-to-Real SysID Evaluation", "content": "We evaluate the task performance during sim-to-real SysID in real-world setups of object scooping and table air hockey. Our method has shown significant performance improvement on trajectory alignment and success rate compared to baseline methods. We evaluated all of our baselines in the sim-to-real transfer.\nObject Scooping Sim-to-Real Evaluation. In this experiment, we verify that CAPTURE can accurately identify the center of mass across various objects during scooping. To verify the effectiveness of our algorithm, we selected three different objects (i.e.,celery, carrot, and eggplant) with asymmetrical properties to ensure the difficulty of identifying the center of mass. We evaluated each object ten times starting at the absolute center point. Similarly to the sim-to-sim transfer setting, we use task success rate to reflect the task performance instead of trajectory matching. To obtain the real-world object 3D pose, we utilize a point cloud to localize the object and provide the tilting direction labels.\nInspecting Table III, we find that DR excels when scooping objects with centralized centers of mass, such as the eggplant, achieving a success rate of 90% or higher from just 1 iteration. However, for objects with more complex mass distributions (i.e.,celery and carrot), DR's performance drops significantly. CAPTURE is able to adapt to different objects and achieve at least 70% at 7th iterations. After successfully lifting the object, one-step adaptation methods randomly sample other parameter values due to the absence of history and lack of target-to-target parameter transition during training, while CAPTURE consistently lifts the object in subsequent iterations. This performance demonstrates CAPTURE 's ability to generalize to unseen scenarios (target-to-target adaptation) by leveraging context history. Its ability to maintain high success rates, especially with objects that have complex mass properties, underscores its effectiveness in real-world scooping tasks.\nAir Hockey Sim-to-Real Evaluation. We set up the real-world table air hockey as shown in Figure 4. To create varying friction across the two surfaces, we installed separate fans under each side of the table, with adjustable fan voltages controlling the sliding friction. We evaluated the sim-to-real transfer performance over 15 trials using 3 different seeds, with each trial having randomized fan voltages on both sides. The results from the sim-to-real air hockey experiment, presented in Table IV, show the performance of different methods in trajectory matching over multiple adaptation iterations. For one-iteration adaptation, ED performs best with a trajectory difference of 0.40, as it tries to adapts to the target parameter within one iteration. However, as iterations increase, CAPTURE steadily improves, outperforming the baselines. By the 7th and 9th iterations, CAPTURE achieves the lowest trajectory differences of 0.29 and 0.27, respectively. In the final iterations, CAPTURE delivers about 20% better performance than the top baseline methods."}, {"title": "V. CONCLUSION", "content": "This paper introduces a novel in-context learning approach to bridge the sim-to-real gap in robotic tasks by adjusting environment parameters online. By leveraging interaction histories as context, we enable dynamics adaptation to real-world environments without requiring model updates. Evaluated in scooping and table air-hockey tasks, our method outperforms traditional approaches such as domain randomization and TuneNet, reducing the sim-to-real gap and improving both sim-to-sim and sim-to-real performance. The approach leverages historical multi-episode data to infer system parameters and provide a better real-world dynamics prediction. While our method demonstrates strong performance, it still requires to train a new SysID model for new task environments. Nonetheless, the framework offers a more efficient and accurate solution for real-world deployment of simulation-based robotic systems."}]}