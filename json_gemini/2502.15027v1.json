{"title": "INTERFEEDBACK: Unveiling INTERACTIVE INTELLIGENCE OF LARGE MULTIMODAL MODELS VIA HUMAN FEEDBACK", "authors": ["Henry Hengyuan Zhao", "Wenqi Pei", "Yifei Tao", "Haiyang Mei", "Mike Zheng Shou"], "abstract": "Existing benchmarks do not test Large Multimodal Models (LMMs) on their interactive intelligence with human users which is vital for developing general-purpose AI assistants. We design InterFeedback, an interactive framework, which can be applied to any LMM and dataset to assess this ability autonomously. On top of this, we introduce InterFeedback-Bench that evaluates interactive intelligence using two representative datasets, MMMU-Pro and MathVerse, to test 10 different open-source LMMs. Additionally, we present InterFeedback-Human, a newly collected dataset of 120 cases designed for manually testing interactive performance in leading models such as OpenAI-01 and Claude-3.5-Sonnet. Our evaluation results show that state-of-the-art LMM (e.g., OpenAI-01) can correct their results through human feedback less than 50%. Our findings point to the need for methods that can enhance LMMs' capabilities to interpret and benefit from feedback.", "sections": [{"title": "1 INTRODUCTION", "content": "In this paper, we are curious about the question \u201cHow do Large Multimodal Models perform with human feedback?\u201d It is central to developing general-purpose AI assistants with Large Multimodal Models (LMMs). While these models are increasingly used to tackle multimodal tasks, their ability to interact with humans remains largely unknown. We argue that an LMM functioning as the general assistant should possess two capabilities: 1) exceptional problem-solving skills and 2) the ability to improve itself through feedback (e.g., human feedback, execution results). In this work, we focus on the latter capability, which has been rarely examined in existing benchmarks.\nHumans are remarkably adaptive, continuously refining their skills by learning from feedback-a process fundamental to acquiring knowledge and solving problems. For example, when confronted with a challenging question, we often seek assistance from a teacher or search online, gathering useful feedback that enables us to iteratively improve our solutions. Similarly, advanced LMM models should also be capable of learning from feedback, thereby enhancing their problem-solving abilities\nOn the other hand, a surge of large multimodal models (LMMs) has developed, designed to handle various tasks, including general vision-language understanding expert-level multimodal understanding and scientific reasoning . However, these LMMs are primarily tested in a static way, overlooking their great potential in human-AI interaction (HAI) such as interactive coding computer usage and clinical reasoning. Consequently, a standard benchmark to test these LMMs for HAI problem-solving remains underexplored.\nThe key challenge in evaluating the interactive intelligence of LMMs is the automatic model tests. In practice, for the same query, different LMMs often produce varied responses, necessitating that humans offer tailored feedback for each conversation round. To address this issue, we propose InterFeedback a straightforward problem-solving framework that enables any LMM to tackle mul-"}, {"title": "2 RELATED WORK", "content": "2.1 LARGE MULTIMODAL MODELS\nThe LLaVA-series works demonstrate that training with supervised fine-tuning (SFT) multimodal data and expand the vision lens would produce compatible multimodal reasoning ability. By adopting a large-scale image-text corpus for instruction tuning, Qwen2-VL , CogVLM , InternVL2\nhave achieved exceptional performance on various multimodal abilities. Moreover, Molmo\nproposes to train an LMM from scratch with only the human-annotated data. Unlike\nthese large models, MiniCPM-V and Phi-3.5-Vision\npropose\nto train lightweight yet SOTA LMMs. Despite these LMMs have demonstrated their understanding\nand reasoning ability on various difficulty-level multimodal benchmarks such as MMMU-Pro and MathVista it is still unknown how well the interactive intelligence\nin an Human-AI Interaction scenario. In this paper, we conduct the evaluation of these LMMs to\nexplore this basic yet vital capability (i.e., improving themselves from human feedback).\n2.2 MULTIMODAL BENCHMARKS\nTraditional vision-language benchmarks focus on visual question answering image captioning as well as other\nbenchmarks for specialized scenarios such as scene text understanding commonsense reasoning outside knowledge The recent development of LMM posts a strong need for modernized multi-modal benchmarks\nsuch as MMBench MMMU-pro and MathVerse which involve comprehensively evaluating cur-rent LMMs on various multimodal abilities. However, these benchmarks primarily focus on static\ntesting processes, overlooking the interactive testing process that is vital in human-AI interaction\nscenarios.\n2.3 HUMAN-AI INTERACTION\nInvestigating how humans and AI systems communicate and collaborate is critical for shaping applications such as virtual assistants personalized recommendations autonomous vehicles and healthcare diagnostics Recent LLMs-driven techniques such as memory and iterative mechanisms offer expert-level collaboration. While LMMs excel in multimodal tasks, their potential for HAI problem-solving remains underexplored. By offering a unified framework and meticulously curated data, our InterFeedback-Bench enables evaluation of LMMs on these capabilities and lays a foundation for advancing multimodal HAI problem-solving."}, {"title": "3 INTERFEEDBACK-BENCH", "content": "In this section, we begin by introducing the interactive benchmarking component of our InterFeedback-Bench in Section 3.1. Here, we propose an interactive human-AI framework, Inter-Feedback, designed as the evaluation tool for assessing LMMs' performance with feedback. Next, in Section 3.2, we detail the human benchmarking aspect of our benchmark, including the data sources and testing standards.\n3.1 INTERACTIVE BENCHMARKING\n3.1.1 FORMULATION\nThe InterFeedback-Bench formalizes the interactive problem-solving process with feedback in a partially observable Markov decision process (POMDP) (S, O, A, T, R) with state space S, observation O, action space A, transition function \\(T: S \\times A \\rightarrow S\\), and reward function \\(R: S \\times A \\rightarrow R\\)."}, {"title": "3.1.2 DATA SOURCES", "content": "To ensure the quality and difficulty of multimodal tasks, inspired by previous benchmarks demonstrated on pre-existing datasets , we choose to test LMMs on two challenging datasets: MathVerse and MMMU-Pro . MathVerse is a visual math benchmark that includes various mathematic problems, and 3,940 samples (testmini Set) are used in our work. MMMU-Pro is a comprehensive multimodal benchmark and we use 1,730 expert-level questions (single image mode). Both datasets are challenging even for the model GPT-40 which achieves only 64.7% accuracy on MMMU-Pro(Standard 4 Opt)."}, {"title": "3.1.3 DATA CONSTRUCTION PROCESS", "content": "We choose to use leading LMMs, such as GPT-40, for stimulating the humans to give feedback mimicking human-AI interactions. The primary challenge, however, is ensuring that the feedback generated by these models is reliable as even the SOTA LMM like GPT-40 and Claude-3.5-Sonnet perform not all correctly on all test samples. Therefore, we construct the test data by selecting the intersection set that feedback provider \\(M_p\\) solves correctly while \\(M_r\\) does not, Specifically, the pipeline includes three parts: 1) feedback receiver LMM locally-running; 2) feedback provider LMM API-calling; and 3) intersection set selection. Such a data construction process leads to each tested LMM having a different test data set.\nSpecifically, given a test dataset D, we begin by having the feedback receiver model \\(M_r\\) process every instance in D to produce a negative set \\(U_n\\) consisting of tasks it fails to solve correctly. Next, the feedback provider model \\(M_p\\) processes the same dataset to generate a positive set \\(U_p\\) comprising tasks it solves correctly. We then define \\(U_{test}\\) as the intersection of \\(U_n\\) and \\(U_p\\), i.e.,\n\\[U_{test} = U_n \\cap U_p,\\]\nwhich means that \\(U_{test}\\) contains tasks that \\(M_p\\) solves correctly but \\(M_r\\) does not. This approach ensures that the feedback generated by \\(M_p\\) is both relevant and reliable."}, {"title": "3.1.4 INTERFEEDBACK FRAMEWORK", "content": "To enable an interactive problem-solving process, we propose a new straightforward framework InterFeedback. It includes two roles: feedback receiver \\(M_r\\) and feedback provider \\(M_p\\)"}, {"title": "3.2 HUMAN BENCHMARKING", "content": "In the previous section, we employed leading LMMs as feedback providers. Naturally, how well do these models perform when tasked with receiving feedback? We begin to assess the SOTA LMMS with a human-in-the-loop process. The feedback provider \\(M_p\\) is a trained user who fully understands all the questions in the newly curated dataset InterFeedback-Human. The feedback receiver \\(M_r\\) is the closed commercial LMM such as OpenAI-01, GPT-40, Gemini-2.0, and Claude-3.5-Sonnet. This evaluation aims to assess how effectively these leading models can serve as assistants in a human-AI interaction system.\n3.2.1 DATA SOURCES\nWe gather challenging data examples across diverse domains: visual logic, mathematics, and coding. These were selected to probe the cognitive depth of the models, especially when confronted with complex, multi-step reasoning problems. The visual logic data we manually collected from publicly available resources. The emphasis on visual logic tasks reflects the growing demand for models to handle image-based reasoning challenges, such as pattern recognition (e.g., determining the next shape in a sequence) and character-based logic (e.g., interpreting transformations between symbols). We also collect the multimodal mathematic data from the existing dataset MathVerse and the multimodal expert-level data from MMMU-Pro"}, {"title": "3.2.2 DATA STATISTICS", "content": "In summary, InterFeedback-Human encompasses a total of 120 tasks distributed across the five task types: 80 visual logic tasks, 10 mathematical logic tasks (sampled from NuminaMath ), 10 coding tasks (sampled from CodeComprehension ), 10 MMMU-Pro tasks, and 10 MathVerse tasks."}, {"title": "3.2.3 HIERACHICAL FEEDBACK", "content": "We design a hierarchical feedback generation scheme to gradually increase the information intensity. Specifically, we ask the human to give the following three-level feedback:\n\u2022 Level 1: Provide a basic and simple description that leads to the correct answer.\n\u2022 Level 2: Provide an expanded explanation that leads to the correct answer.\n\u2022 Level 3: The correct answer is GT Answer. Provide a comprehensive and detailed explanation that leads to the correct answer.\nSince most of our questions have four options, giving more than three rounds of feedback might let the model guess the answer by elimination rather than by reasoning. For example, if the correct answer is A and the model already gave B, C, and D, a third round of feedback is unnecessary. Therefore, we directly provide the GT Answer in Level 3 feedback prompts to test the models' ability to explain their thinking process."}, {"title": "3.2.4 EVALUATION INTEGRATION", "content": "To ensure fairness and consistency in our evaluation, we engaged only one experienced user. Since human-in-the-loop feedback is inherently subjective, involving multiple participants could introduce variability due to differences in background and expertise. This approach helps maintain the reliability of the relative performance comparisons across candidate LMMs."}, {"title": "4 EXPERIMENTS", "content": "4.1 EXPERIMENT SETUP\nEvaluation Models. We evaluate the performance of foundation models served as the feedback receiver \\(M_r\\) across 10 representative LMMs\nThe feedback provider \\(M_p\\) includes the three best available models from three model families: OpenAI (gpt-40-2024-08-06), Gemini , and Claude \nEvaluation Metrics. In addition to the Accuracy metric, we leverage the Correction Rate, defined as the percentage of corrected answers of all erroneous samples. Let N denote the total number of samples, \\(N_e\\) the number of erroneous samples, and \\(N_c\\) the number of samples that have been corrected. The Accuracy and Correction Rate metrics can be formulated as follows:\n\\[Accuracy = (1-\\frac{N_e}{N}) \\times 100\\%, \\quad \\text{Correction Rate} = (\\frac{N_c}{N_e}) \\times 100\\%.\\]\nImplementation Details. We set the temperature to 0 for all tested models and API models. The image resolution of the Qwen2-VL model we restrict to 512 \u00d7 512 to avoid the memory exceeded error. All evaluations were conducted on two NVIDIA RTX A6000 GPUs. To ensure the reliability of results, we obtain the intersection set for both the feedback receiver and provider models that are able to output the correct answer format. Based on our preliminary experiments, we limited"}, {"title": "4.2 EXPERIMENTAL ANALYSIS ON INTERACTIVE BENCHMARKING", "content": "To thoroughly investigate the ability of LMMs to integrate feedback and improve their problem- solving performance, we present evaluation results for various models on two datasets\u2014MathVerse in Table 1 and MMMU-Pro in Table 2, respectively. Below, we provide a detailed discussion of key findings.\nInteractive process could improve the performance of most LMMs. As demonstrated in both tables, integrating our proposed framework InterFeedback enables most models to benefit from feedback provided by SOTA LMMs, such as GPT-40 and Claude-3.5-Sonnet. Notably, even the weaker model Fuyu-8B sees 24.1% of its erroneous samples corrected through GPT-40's feedback.\nCurrent LMMs struggle to enhance performance through feedback. As shown in the tables, most LMMs are unable to correct all erroneous samples, even when provided with feedback from state-of-the-art closed-source models such as Claude-3.5-Sonnet and GPT-40. For example, consider the two leading open-source models, Qwen2-VL-7B and Molmo. Qwen2-VL-7B achieves a 66.8% correction rate on the MathVerse dataset with GPT-40's feedback, but only a 50.4% correction rate on the MMMU-Pro dataset. Similarly, Molmo-7B attains correction rates of 55.1% and 51.7% on the MathVerse and MMMU-Pro datasets, respectively. Overall, the correction rates for the rest models remain below 50%. This suggests that even with constructive feedback from advanced LMMs, current models struggle to enhance performance through feedback generally.\nAccuracy result may not truly reflect the model's capability. As shown in Table 1, although InternVL2-8B achieves a higher accuracy (38.1%), its correction rate is only 49.6%. In contrast, Qwen2-VL-7B, with a lower accuracy of 22.5%, attains the highest correction rate of 66.8% when using GPT-40's feedback. Similarly, Molmo-7B surpasses InternVL2-8B in correction rate despite"}, {"title": "4.3 EXPERIMENTAL ANALYSIS ON HUMAN BENCHMARKING", "content": "In this section, we will introduce the human evaluation results of several well-known closed-source families: OpenAI (GPT-40, OpenAI-01), Claude (Claude-3.5-Sonnet-20241022), and Gemini (Gemini-2.0-Flash-Exp)."}, {"title": "5 CONCLUSION", "content": "In this work, we introduced InterFeedback-Bench, the first solution to concern the critical importance of evaluating the interactive intelligence of current LMMs. We build an interactive framework InterFeedback which can be applied to any LMM and dataset to bootstrap the testing in an interactive way. We conduct the comprehensive evaluations on 10 open-source LMMs by demonstrating with two representative datasets MathVerse and MMMU-Pro. Additionally, we present InterFeedback-Human, a new benchmark for manually testing the leading models such as OpenAI-01 and Claude-3.5 with 120 curated samples. Our evaluation results show that even the SOTA LMM (like OpenAI-01) can only correct their results through human feedback with less than 50%. Several findings point to the essential need for methods that improve the LMM's ability to receive feedback to improve themselves."}]}