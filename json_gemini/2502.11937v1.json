{"title": "FitLight: Federated Imitation Learning for Plug-and-Play Autonomous Traffic Signal Control", "authors": ["Yutong Ye", "Yingbo Zhou", "Xiao Du", "Zhusen Liu", "Hao Zhou", "Xiang Lian", "Mingsong Chen"], "abstract": "Although Reinforcement Learning (RL)-based Traffic Signal Control (TSC) methods have been extensively studied, their practical applications still raise some serious issues such as high learning cost and poor generalizability. This is because the \"trial-and-error\" training style makes RL agents extremely dependent on the specific traffic environment, which also requires a long convergence time. To address these issues, we propose a novel Federated Imitation Learning (FIL)-based framework for multi-intersection TSC, named FitLight, which allows RL agents to plug-and-play for any traffic environment without additional pre-training cost. Unlike existing imitation learning approaches that rely on pre-training RL agents with demonstrations, FitLight allows real-time imitation learning and seamless transition to reinforcement learning. Due to our proposed knowledge-sharing mechanism and novel hybrid pressure-based agent design, RL agents can quickly find a best control policy with only a few episodes. Moreover, for resource-constrained TSC scenarios, FitLight supports model pruning and heterogeneous model aggregation, such that RL agents can work on a micro-controller with merely 16KB RAM and 32KB ROM. Extensive experiments demonstrate that, compared to state-of-the-art methods, FitLight not only provides a superior starting point but also converges to a better final solution on both real-world and synthetic datasets, even under extreme resource limitations.", "sections": [{"title": "I. INTRODUCTION", "content": "With the rapid development of cities and the rapid growth of population, more and more cities are suffering from heavy traffic congestion, resulting in various serious problems, such as economic losses, increasing commuting costs, and environmental pollution. The Traffic Signal Control (TSC) has attracted widespread attention as a promising solution to traffic congestion [1]\u2013[3]. In most real-world applications, the control policies are rule-based (e.g., FixedTime [4], GreenWave [5], SCOOT [6], and SCATS [7]) that follow some pre-defined rules of traffic plan. To better deal with dynamic traffic scenarios, some adaptive methods have been proposed (e.g., Max-Pressure [8], MaxQueue [9], and SOTL [10]), which control traffic in a heuristic manner. Due to the prosperity of Artificial Intelligence (AI) and Internet of Things (IoT) technologies, using Reinforcement Learning (RL), to control traffic signals has been a promising way.\nAlthough RL-based methods can achieve better control performance, their usage is greatly restricted by the issues of high learning cost and poor generalizability. This is because of the \"trial-and-error\" learning style of RL agents, which requires the RL agent to make a large number of attempts in a specific traffic environment to gradually learn the control strategy. Worse still, as the size of the road network and the number of RL agents increase, the size of the policy space will also increase exponentially, which will make it extremely difficult to find the optimal control strategy in multi-intersection scenarios and require a lot of training costs. Therefore, how to effectively improve both the training efficiency and generalization ability is becoming a major challenge in the application of RL-based TSC methods.\nTo tackle this problem, we propose a novel Federated Imitation Learning (FIL)-based framework named FitLight for efficient and effective multi-intersection TSC, which enables RL agents plug-and-play for different traffic environments without additional pre-training cost. In other words, after obtaining a very high-quality solution in the first episode, FitLight can quickly converge to a better final control strategy. As shown in Figure 1, FitLight is built on a cloud-edge framework consisting of one cloud server and multiple edge nodes (i.e., RL agent and its corresponding intersection). Unlike existing methods that either train RL agents directly within the traffic environment or employ imitation learning over pre-collected data for pre-training, FitLight seamlessly integrates imitation learning into the reinforcement learning process. This integration allows the RL agent to achieve a high-quality initial solution in the first episode due to the supervision of imitation learning. Subsequently, due to our novel hybrid pressure-based agent design, the RL agent seamlessly transitions into the reinforcement learning phase, ultimately converging to an even better control strategy. Moreover, FitLight's support for model pruning and heterogeneous model aggregation ensures that RL agents can be deployed in resource-constrained TSC scenarios. In summary, this paper makes the following four major contributions:\n\u2022 We propose a novel Federated Imitation Learning (FIL)-based framework that can plug and play for any traffic environment without additional pre-training costs.\n\u2022 We introduce an imitation learning mechanism combined with a hybrid pressure-based agent design, enabling real-time imitation learning and smooth transitions to reinforcement learning,"}, {"title": "II. RELATED WORK", "content": "To improve the performance of TSC, various methods based on RL have been proposed. For example, PressLight [11], CoLight [12], MPLight [13], MetaLight [14], and RTLight [15] performed TSC optimization based on the concept of pressure from the Max Pressure (MP) control theory [8] to design the state and reward. Unlike these methods, IPDALight [16] proposed a new concept named intensity, which investigates both the speed of vehicles and the influence of neighboring intersections. To reflect the fairness of individual vehicles, FairLight [17] and FELight [18] considered the relationship between waiting time and driving time, and the extra waiting time of vehicles, deceptively. To exploit the cooperation among RL agents in the road network, FedLight [19] and RTLight [15] adopt federated reinforcement learning to share knowledge. HiLight [20] cooperatively controls traffic signals to directly optimize average travel time by using hierarchical reinforcement learning. UniLight [21] uses a universal communication form between intersections to implement cooperation. However, the RL agents of these methods are trained from the randomly initialized models, resulting in a long training time before obtaining the final control strategy.\nTo improve learning efficiency, imitation learning [22]\u2013[25] that makes the RL agent learn from the expert demonstration is a promising way. Currently, imitation learning can be divided into two categories: behavioral cloning [26], [27] and adversarial imitation learning [28]\u2013[30], both of which have been applied in TSC. Specifically, DemoLight [31] is a behavioral cloning-based method that reduces the imitation learning task to a common classification task [32], [33] by minimizing the action difference between the agent strategy and the expert strategy. However, since this method is trained in the single-intersection environment and relies on the pre-collected expert trajectory from the same environment, it cannot be applied to multi-intersection scenarios and is very specific to the training environment. On the other hand, as an adversarial imitation learning-based method, InitLight [34] uses a generative adversarial framework to learn expert's behaviors, where the discriminator iteratively differentiates between pre-collected expert and agent trajectories (generated through real-time agent-environment interactions). Although InitLight can use trajectories from different environments to train RL agents, it still needs a pre-training process to obtain an initial model.\nTo the best of our knowledge, FitLight is the first federated imitation learning framework for TSC to enable RL agents to plug-and-play for any traffic environment without additional pre-training cost, where the RL agent can achieve a high-quality initial solution in the first episode and then converge to an even better final result."}, {"title": "III. OUR FITLIGHT APPROACH", "content": "To make RL agents plug-and-play in different traffic scenarios without pre-training, we design a novel FitLight approach, based on a cloud-edge architecture, where the cloud server is used for knowledge sharing among intersections, and each intersection equips an RL agent and an expert strategy. Figure 2 details the FitLight components and workflow. In our approach, the federated imitation learning framework consists of a cloud server and several edge nodes. The cloud server first dispatches models pruned from a base model to edge nodes, and then shares knowledge among different intersections by aggregating heterogeneous models during the RL training. For each edge node, we deploy an RL agent to monitor traffic dynamics using connected sensors, e.g., cameras, make the traffic signal control decision, and update network parameters. Once capturing the current traffic state s, the RL agent will choose one best action a to control traffic lights. Meanwhile, the expert strategy also gives a decision $a_e$, which will be stored as the label of the current state for imitation learning. This expert guidance enables the RL agent to quickly identify a high-quality solution. We will give the details of our approach in the following subsections."}, {"title": "A. Intersection Modeling", "content": "The right part of Figure 2 shows an intersection example with three components, i.e., arrival and departure lanes, directed roads, and control phase setting:\n\u2022 Arrival and Departure Lanes: The intersection consists of a set of arrival lanes $L_a = {l_1, l_2,..., l_{|L_a|}}$ and a set of departure lanes $L_d = {l_1, l_2,..., l_{|L_d|}}$ , where vehicles can enter and exit the intersection, respectively.\n\u2022 Directed Roads: Based on direction marks at the end of each arrival lane, we define a directed road as $(l_a, l_d)$, $l_a \u2208 L_a, l_d \u2208 L_d$, where $l_d$ is the departure lane indicated by the direction mark on the ground of $l_a$. For example, $(l_7, l_7)$ and $(l_8, l_g)$ are two directed roads.\n\u2022 Control Phases: According to common sense, the vehicles turning right are not restricted by traffic. Therefore, we design a set of eight feasible control phases $P = {p_1, p_2,..., p_8}$ , which are obtained by combining 2 of 8 directed roads and indicate the rights-of-way signaled to vehicles by traffic lights. For example, the intersection on the right side of Figure 2 shows a scenario with control phase $p_4$ enabled, where the vehicles on lane $l_7$ can turn left to enter lane $l_7$ and the vehicles on lane $l_8$ can go straight to enter lane $l_g$. Note that, the number of control phases is fixed, regardless of the number of lanes."}, {"title": "B. Hybrid Pressure", "content": "Unlike most existing works that utilize pressure from MP control theory to model traffic dynamics, in this paper, we introduce a novel concept of Hybrid Pressure (HP) for the design of RL agents. Specifically, HP considers more dynamics from the individual vehicle level to intersection level rather than only the number of vehicles. Therefore, it can be used for more accurate modeling of RL elements.\nDefinition 1: (Hybrid Pressure of a Vehicle). The hybrid pressure of a vehicle $veh$ on a lane of an intersection $hp_{veh}$ is defined as:\n$hp_{veh} = log(1 + \\frac{l_{max} - d}{l_{max}} + \\frac{v_{max} - v}{v_{max}} + w_{tveh}),$ (1)\nwhere $l_{max}$ is the lane length, d indicates the distance between the vehicle and the intersection, $v_{max}$ is the maximum speed of the lane, v is the current speed of the vehicle, $w_{tveh}$ and $d_{tveh}$ are the overall waiting time and driving time of the vehicle along its route so far from the time when it entering the traffic network, respectively. Note that we normalize the distance and speed by $l_{max}$ and $v_{max}$ to constrain their ranges. Moreover, we use log(.) to smooth the absolute value of $hp_{veh}$ and plus 1 to make sure $hp_{veh}$ is always greater than 0.\nWe use the HP of a vehicle to indicate the traffic priority when the vehicle arrives at an intersection. According to Definition 1, we can find that the vehicles with a shorter distance to the intersection, a slower speed, and a longer cumulative waiting time will have higher $h_{veh}$ value, i.e., have greater priority for the right of way. When waiting at some intersection, the waiting time of a vehicle increases cumulatively, which results in an increase in the corresponding lane HP value. Along with the increasing lane HP values, the vehicles on some feeder roads can move eventually. Due to the elegant combination of individual vehicles' features, HP can more accurately reflect the traffic dynamics.\nDefinition 2: (Hybrid Pressure of a Directed Road). The hybrid pressure of a directed road $(l_a, l_d)$ is defined as the difference in hybrid pressure between the arrival and departure lanes, where a lane's hybrid pressure is the sum of all vehicles' HP on that lane.\n$hp_{(l_a, l_d)} = \\sum_{veh \u2208 l_a}hp_{veh} - \\sum_{veh \u2208 l_d}hp_{veh}$ (2)\nTo denote the hybrid pressure of all the vehicles on a lane, Definition 2 defines the HP of a directed road $(l_a, l_d)$. Since the hybrid pressure of a lane is in a summation form, it implicitly reflects the number of vehicles on on both $l_a$ and $l_d$. Therefore, $hp_{(l_a, l_d)}$ not only reflects the status of individual vehicles but also captures the imbalance in traffic conditions between the upstream and downstream lanes. In our approach, the TSC controller tends to allow the directed road with higher HP values to move first.\nDefinition 3: (Hybrid Pressure of an Intersection). The hybrid pressure of an intersection $I$ equals the difference between the arrival and departure lanes' HP values, i.e.,\n$hp_I = \\sum_{l \u2208 L_a} hp_l - \\sum_{l \u2208 L_d} hp_l,$ (3)\nDefinition 3 presents how to calculate the FI for an intersection, which can be used to evaluate the overall traffic pressure faced by the intersection. From this definition, we can find that the hybrid pressure of the intersection can approximately reflect the imbalance of upstream and downstream traffic status at the intersection. Therefore, similar to the MP control theory, if the HP of the intersection can be controlled at a low level, the throughput of vehicles crossing this intersection will be maximized."}, {"title": "C. Edge Node Design", "content": "In our approach, the traffic lights of each edge node are controlled by a Proximal Policy Optimization (PPO) [35] agent. Unlike most existing works that directly train the agent by reinforcement learning, we deploy an expert algorithm to guide the agent for efficient convergence by imitation learning, which can make the RL agent find a high-quality solution in the first episode.\nExpert Algorithm. Based on the concept of hybrid pressure, we design a simple but effective control heuristic named MaxHP, which greedily selects the control phase with the maximum HP values. Similar to the classic MP-based heuristic control method MaxPressure [8], by allowing vehicles in the lane with the largest HP value to pass, MaxHP can reduce the HP value of the intersection."}, {"title": "Agent Design", "content": "In this paper, we design the key elements of the PPO agent by using the proposed HP concept, i.e.,\n\u2022 State: State is the information of the intersection captured by the agent as its own observation for phase selection. Take the standard intersection in Figure 2 as an example, the state includes the HP of all directed lanes (i.e., $hp_{11,14}, hp_{12,12},\u2026\u2026\u2026, hp_{112,112}$ and the current control phase (i.e., $p_4$).\n\u2022 Action: Based on the observed current traffic state, the PPO agent needs to choose one best control phase to maximize the throughput of the intersection. For the intersection example in Figure 2, the PPO agent has 8 permissible control phases (i.e., $p_1,..., p_8$).\n\u2022 Reward: Once an action is completed, the environment will return a reward to the agent. The reward mechanism plays an important role in the RL learning process. It is required that a higher reward needs to imply a better action choice. As mentioned in Definition 3, to encourage the agent to maximize the throughput of the intersection by minimizing the hybrid pressure of the intersection $hp_I$, in this paper, we define the reward as $r = -hp_I$.\nA PPO agent consists of two trainable networks, i.e., Actor $\u03b8_A$ and Critic $\u03b8_C$, where $\u03b8$ is the model parameter. The Actor model is responsible for learning the policy, and determining which action to take given the current state. The Critic model, on the other hand, serves as a value estimator, assessing whether the action selected by the Actor will lead to an improved state in the traffic environment. Therefore, the feedback from the Critic model can also be used to optimize the Actor model. In this paper, since we also utilize imitation learning to guide the agent training, as shown in the right part of Figure 2, there are two loss functions from reinforcement learning $L_{RL}$ and imitation learning $L_I$. To calculate these loss values for optimizations, a mini-batch of trajectory samples is collected from the agent trajectory memory, where each sample is a quintuple $(s, a, a_e, r, s')$.\nFirst, the reinforcement learning loss $L_{RL}$ includes the losses of both the Critic model $L_C$ and the Actor model $L_A$. Note that, the reinforcement learning of PPO requires that trajectory samples in a mini-batch be continuous.\nCritic Model. We optimize the Critic model by:\n$L_C = E[|\u03b8_C(s_t)_{target} - \u03b8_C(s_t)|],$ (4)\nwhere $E$ is an operator to calculate the empirical average over a mini-batch of samples, and $\u03b8_C(s_t)_{target}$ can be calculated as $\u03b8_C(s_t)_{target} = r_{t+1} + \u03b3 \\cdot \u03b8_C(s_{t+1})$ by using the Temporal-Difference (TD) algorithm [36] to estimate the target value.\nActor Model. As a policy gradient-based RL algorithm, the objective of the Actor model is formulated as follows:\n$L_A = E[min(R_t, clip(R_t, 1 \u2013 \u03c3, 1 + \u03c3))A_t],$ (5)\nwhere $R_t = \\frac{A(a_t|s_t)}{A_{old}(a_t|s_t)}$ is the importance sampling that obtains the expectation of samples under the new Actor model A we need to update, $A_t$ is an estimated value of the advantage function at time step t, and \u03c3 is the clipping parameter that restricts the upper/lower bounds in the clip() function to stabilize the updating process. Note that, the samples are gathered from an old Actor model $A_{old}$. The advantage function $A_t$ is computed with the Generalized Advantage Estimator (GAE) [37] as follows:\n$A_t = \u03b4_t + (\u03b3\u03bb)\u03b4_{t+1} + (\u03b3\u03bb)^2\u03b4_{t+2}\n+ \u00b7\u00b7\u00b7 + (\u03b3\u03bb)^{|B|-t+1}\u03b4_{|B|-1},$ (6)\nwhere \u03b3\u2208 [0, 1] is the discount factor of future rewards, \u03bb \u2208 [0, 1] is the GAE parameter, |B| is the batch size of the sampled mini-batch, and $\u03b4_t = r_t + \u03b3\u03b8_C (s_{t+1}) \u2013 \u03b8_C (s_t)$.\nMoreover, the Actor model also has a loss from imitation learning, which is used to guide the agent's behavior.\nImitation Learning. In our approach, we use MaxHP as the expert algorithm to label the state s by selecting the corresponding action $a_e$. This labeling process enables us to apply supervised learning to minimize the discrepancy between the Actor's actions and the expert's actions. Since the control phases are discrete actions, we employ the cross-entropy loss function [38], [39] to handle this multi-class classification task:\n$L_I = -\\sum_{i=1}^{|P|}a_{ei} log(a_i),$ (7)\nwhere P is the number of classes (control phases), $a_{ei}$ is the indicator variable of the label (i.e., the action chosen by the expert algorithm) that is encoded by a one-hot vector, $a_i$ is the prediction probability of the i-th action given by the Actor model.\nFinally, considering the balance of exploitation and exploration for the RL agent training, we use a balance factor \u03b1 to adjust the weighting of different losses:\n$L = \u03b1(L_C + L_A) + (1 \u2212 \u03b1)L_I,$ (8)\nwhere \u03b1 increases with the number of training episodes, facilitating a gradual transition from imitation learning to reinforcement learning."}, {"title": "D. Cloud Server Design", "content": "In our approach, we use a cloud server to coordinate the training process among RL agents at different intersections. Specifically, for real traffic environments with extremely limited resources, the cloud server first sends pruned initial models that meet the requirements to each intersection. During the training process, the cloud server facilitates the knowledge sharing, by aggregating gradient information from these heterogeneous models, enabling effective collaboration among agents at different intersections."}, {"title": "a) Model Pruning", "content": "To meet resource requirements, we employ structured pruning at initialization. This technique leverages the concept that a randomly initialized dense network contains a subnetwork (referred to as a \"winning ticket\") capable of achieving performance comparable to the original dense network [40], [41]. Specifically, for a dense network with parameters \u03b8, network pruning results in a new model $\u03b8_M$, where $M = {0,1}^{||\u03b8||}$ is a binary mask used for the pruning, and \u2299 denotes the Hadamard product (element-wise multiplication). In our approach, we generate multiple Actor and Critic subnetworks for each intersection, applying a fixed set of pruning ratios to different network layers. As illustrated in the left part of Figure 2, we create three pruned submodels from the base model for different intersections. In these submodels, lighter colors indicate pruned neurons, while darker colors represent retained neurons."}, {"title": "b) Knowledge Sharing", "content": "As a specialized form of supervised learning, imitation learning also requires a substantial number of samples to train RL agents effectively. To enhance learning efficiency and maximize the use of trajectory samples, we introduce a knowledge-sharing mechanism that aggregates gradients from heterogeneous submodels. Specifically, since the submodels for each intersection are derived from the same base model, we aggregate their gradients using a weighted average operation as follows:\n$\u2207L = \\frac{\\sum_{i=1}^{N} L_i\u2299M_i}{\\sum_{i=1}^{N} |M_i|},$ (9)\nwhere N is the number of intersections, $L_i$ and $M_i$ represent the gradient of the loss function and the binary mask from the i-th intersection's submodel, respectively.\nAs shown in the left part of Figure 2, each colored neural network represents different agents' subnetworks, where the generated subnetworks share some subsets of parameters across multiple agents. In the aggregated model, each neuron is colored with the colors of agents who share the corresponding neuron."}, {"title": "E. FitLight Implementation", "content": "Algorithm 1 details the training process of a FitLight agent. In lines 1-2, the agent is initialized with the pruned structure. Lines 5-10 show the interaction between the agent and the intersection, where the exact algorithm gives the label $a_e$ for the current state s. When the agent stores enough trajectory samples, lines 12-16 update the parameters of the PPO agent by using the local loss. Lines 18-19 show our knowledge-sharing mechanism, where the cloud server collects the gradient from all intersections and then dispatches the aggregated gradient to them for parameter update."}, {"title": "IV. PERFORMANCE EVALUATION", "content": "To evaluate the effectiveness of our approach, we conduct experiments on a Ubuntu server equipped with an Intel Core i9-12900K CPU, 128GB memory, and NVIDIA RTX 3090 GPU. We implement our FitLight approach on an open-source traffic simulator, Cityflow [42], by using Python. During the simulation of traffic scenarios, similar to prior work [11], [19], [34], we set the phase duration to 10 seconds. In FitLight, the base model of the PPO agent contains two neural networks, i.e., the Actor model with three layers (containing 13, 32, and 8 neurons) and the Critic model with three layers (containing 13, 32, and 1 neurons). We use the Adam optimizer for parameter updating and set the learning rate \u03b7 of Actor and Critic models to 0.0005 and 0.001, respectively. By default, we set the discount factor \u03b3 to 0.99, the GAE parameter \u03bb is set to 0.95, the batch size N of the data sampled for training to 5, the clipping parameter \u03f5 to 0.2, and the balance factor \u03b1 to 0.001\u00d7# of episode. We design comprehensive experiments to answer the following four research questions:\nRQ1 (Effectiveness): Can FitLight explore a better TSC strategy starting from imitation learning?\nRQ2 (Efficiency and Generalizability): Can FitLight be plug-and-played in any traffic environment?\nRQ3 (Benefits): Why can FitLight improve the learning performance and generalizability of RL models?\nRQ4 (Applicability): Can FitLight be deployed in real-world embedded scenarios with extremely limited resources?"}, {"title": "a) Baselines", "content": "Since we set the duration of the control phase to a constant value of 10s, for a fair comparison, we chose eight representative baseline methods with constant duration, including three Non-RL methods and five RL-based methods as follows: i) FixedTime [4]; ii) MaxPressure [8]; iii) MaxHP, our proposed expert algorithm; iv) PressLight [11]; v) A2C [19]; vi) FedLight [19]; vii) PPO [35], and; viii) InitLight [34]. On the other hand, to further evaluate the performance of our approach, we also conduct two state-of-the-art dynamic duration-based methods for comparison: i) FairLight [17], and; ii) IPDALight [16]. Moreover, to validate the effectiveness of our proposed approach, we compare with four ablation methods as follows: i) FairLight(c) [17], a constant duration version of FairLight; ii) IPDALight(c) [16], a constant duration version of IPDALight; iii) FitLight(p), a pressure-based method that replaces the hybrid pressure in the sate representation and reward design of FitLight with pressure, and; iv) FitLight(mp), the light version of the FitLight model pruning. For FitLight(mp), to meet the extremely resource-constrained requirements of [43] (i.e., a micro-controller with merely 16 KB RAM and 32 KB ROM), we set the pruning rate of each layer of the model to 0.2, 0.4, and 0.6, respectively. Under this setting, the memory cost of each PPO agent is only 14.83 KB."}, {"title": "b) Datasets", "content": "We consider nine public multi-intersection datasets provided by [44]. For all datasets, each intersection of all the road networks has four incoming roads and four outgoing roads, where each road has three lanes, i.e., turning left, going straight, and turning right. The details of the datasets are as follows:\n\u2022 Synthetic datasets: Four synthetic datasets, Syn1-4, contain 1 \u00d7 3, 2 \u00d7 2, 3 x 3, and 4 \u00d7 4 intersections, respectively. The vehicle arrival rates are modeled using a Gaussian distribution, with an average rate of 500 vehicles per hour for each entry lane.\n\u2022 Real-world datasets: Five real-world datasets (i.e., Hangzhou1, 2, and Jinan1-3) were collected using cameras deployed in the Gudang sub-district of Hangzhou and the Dongfeng sub-district of Jinan. Each dataset from Hangzhou contains 16 intersections arranged in a 4 \u00d7 4 grid, while each dataset from Jinan includes 12 intersections arranged in a 3 \u00d7 4 grid."}, {"title": "A. Results of the Control Performance (RQ1)", "content": "To answer RQ1, we compared FitLight against the fourteen baseline methods, in terms of average travel time, where we trained all RL-based methods using 200 episodes. Table I shows experimental results for different control methods. For each dataset, the TSC methods with the best or second-best performance are highlighted in bold. From this table, MaxHP can achieve a shorter average travel time than MaxPressure and competitive results compared with some RL-based methods, especially for larger datasets. These results show the effectiveness of our proposed hybrid pressure and illustrate the reason why we chose MaxHP as the expert algorithm. In this table, FitLight can outperform all constant duration methods. This is because, due to our proposed knowledge sharing mechanism, FitLight enables RL agents to jointly explore the optimal control strategy. On the other hand, due to the full use of duration, dynamic duration methods are significantly better than constant methods. However, our FitLight methods can still achieve a similar level with these two dynamic duration methods, where the biggest gap is only 3.51%. Compared with the original uncompressed model, the perfomance decrease of FitLight(mp) is negligible, showing the practicality of our model pruning approach."}, {"title": "B. Results of the Convergence (RQ2)", "content": "To evaluate the efficiency and the generalization ability of FitLight, Figure 3 evaluates the convergence performance of the RL-based methods on the nine datasets. Compared to all baseline methods, our FitLight method achieves almost the lowest average travel time at the beginning of RL training for all the datasets with much fewer fluctuations. This is because our proposed knowledge sharing mechanism enables RL agents to perform effective imitation learning with only very few trajectory samples. In addition, although InitLight uses a pre-trained model that can also perform well at the beginning and converge fast, our FitLight can always achieve the lowest average travel time for all datasets eventually. This result shows that due to a better initial solution obtained by imitation learning, the subsequent reinforcement learning process of FitLight enables RL agents to explore a better final result, which confirms the seamless transition between imitation learning and reinforcement learning of our method. Note that, for all the evaluated datasets, FitLight can converge within 2 episodes, significantly faster than most baseline methods. The above facts evidently reveal the efficiency and the generalizability of our FitLight approach.\nTo further illustrate the advantages of FitLight, Table II provides the detailed convergence information for different RL-based methods, focusing on jumpstart performance (i.e., average travel time during the first episode) and the episode at which the convergence begins, where the best and the second-best results are highlighted in bold. Here, the criterion for the convergence is based on the method described in [16]. From this table, we can find that FitLight achieves the best jumpstart performance on all datasets and the fastest convergence on most datasets. Due to the pre-trained initial model, InitLight can converge faster on some datasets (e.g., Hangzhoul and Jinan1-3). However, the gap is only 1 episode, and FitLight can achieve better control performance without any pre-training. Note that due to the model pruning, although the jumpstart performance of the pruned FitLight(mp) model has slightly decreased, it can still converge to a final result that is similar to the unpruned version. These results demonstrate the plug-and-play capability of our FitLight approach for arbitrary traffic scenarios."}, {"title": "C. Quality of the Reward Function (RQ3)", "content": "In this paper, we use the newly proposed concept of hybrid pressure to represent the state and design the reward of the RL agent, which makes the agent take more traffic dynamics of individual vehicles into account. To justify our hybrid pressure design and understand why FitLight can plug and play for any datasets, as shown in Figure 4, we compare the average travel time and the average reward of each episode on different datasets. From this figure, we observe that the average travel time is closely correlated with absolute values of the average reward (i.e., the average HP of intersections). These results support the effectiveness of our HP-based agent design in reducing the average travel time of vehicles. Moreover, the smooth change of rewards during training also confirms that our FitLight method supports a smooth transition from imitation learning to reinforcement learning. Thus, our FitLight can improve both the control performance and the generalization ability of RL agents."}, {"title": "D. Results of the Deployment Cost (RQ4)", "content": "To analyze the deployment cost of FitLight, we built a cloud-edge simulation platform consisting of the server mentioned above and 16 Raspberry Pi 4B boards (with ARM Cortex-A72 CPU and 2G RAM), where we deploy a FitLight agent on each Raspberry Pi board to simulate real-world intersection scenario. For memory cost, as mentioned above, due to the model pruning, the model size of each agent is only 14.83 KB. which can be deployed on most resource-constrained embedded systems. For computation cost, the agent only needs 0.05 ms to make a control decision of phase selection. Within an autonomous TSC system, the agent also needs to update its model parameters after collecting some trajectory data. In our approach, an agent takes 5 samples from its trajectory memory at a time for model update, which costs around 51.67 ms. Compared with the 10-second signal phase duration, this inference and training costs can meet the real-time requirements of embedded devices in most real-world scenarios. On the other hand, since FitLight includes a federated learning-based knowledge-sharing mechanism, we also evaluate the communication cost of our approach. In the experiment, we set the phase duration of traffic lights to 10 seconds, which means that every 10 seconds, the agent of an intersection needs to interact with its traffic environment and store a trajectory sample. Once the agent collects a batch of 5 new samples, it will use these samples to calculate and share the gradient. In other words, every 50 seconds, each edge device needs to send and receive 14.83 KB gradients, respectively. This communication cost is tolerable for most IoT devices, since in each hour, one device only has a communication overhead of 2.09 MB."}, {"title": "V. CONCLUSION", "content": "Due to trial-and-error attempts during the training process, existing RL-based TSC methods suffer from the problem of high learning costs and poor generalizability. To address this problem, in this paper, we propose a novel FIL-based approach named FitLight, which enables RL agents to be plug-and-play for any traffic scenarios. Based on the proposed federated imitation learning frameworks and hybrid pressure-based agent design, our FitLight agent can smoothly transition from imitation learning to reinforcement learning. Therefore, the RL agent can quickly find a high-quality initial solution and then find a better final control strategy. Experimental results on various well-known benchmarks show that, compared with the state-of-the-art RL-based TSC methods, FitLight can not only converge faster to competitive results, but also exhibit stronger robustness in different traffic scenarios. Especially, our approach can achieve near-optimal performance in the first episode."}]}