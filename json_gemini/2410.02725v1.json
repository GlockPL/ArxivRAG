{"title": "Adaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better, Even Mid-Generation", "authors": ["Rohin Manvi", "Anikait Singh", "Stefano Ermon"], "abstract": "Inference-time computation is a powerful paradigm to enhance the performance of large language models (LLMs), with Best-of-N sampling being a widely used technique. However, this method is computationally expensive, requiring both (1) an external reward model and (2) the generation of multiple samples. In this work, we introduce a new generative self-evaluation scheme designed to adaptively reduce the number of generated samples while maintaining or even improving performance. We use a generative reward model formulation, allowing the LLM to predict mid-generation the probability that restarting the generation will yield a better response. These predictions are obtained without an external reward model and can be used to decide whether or not to generate more samples, prune unpromising samples early on, or to pick the best sample. This capability is very inexpensive as it involves generating a single predefined token. Trained using a dataset constructed with real unfiltered LMSYS user prompts, Llama 3.1 8B's win rate against GPT-4 on AlpacaEval increases from 21% to 34% with 16 samples and math performance on GSM8K improves from 84% to 91%. By sampling only when the LLM determines that it is beneficial to do so and adaptively adjusting temperature annealing, we demonstrate that 74% of the improvement from using 16 samples can be achieved with only 1.2 samples on average. We further demonstrate that 50\u201375% of samples can be pruned early in generation with minimal degradation in performance. Overall, our methods enable more efficient and scalable compute utilization during inference for LLMs.", "sections": [{"title": "1. Introduction", "content": "As large language models (LLMs) continue to advance, delivering high-quality responses across diverse applications becomes increasingly important. One promising direction to enhance response quality is the strategic use of inference-time computation, particularly through methods like Best-of-N sampling Snell et al. (2024); Charniak and Johnson (2005); Cobbe et al. (2021), which selects the best response from multiple candidates. However, this method incurs substantial inference cost from querying an external reward model and producing a large, fixed number of samples.\nIn this work, we introduce a new reward modeling paradigm, which we denote as capability-aware self-evaluations. This paradigm allows for adaptive allocation of inference-time compute, aiming to reduce the computational overhead while maintaining or improving LLM performance across various domains. We demonstrate that LLMs can directly model the probability that restarting generation yields in a better response, enabling informed decisions about whether to continue generating a response, initiate new ones, as well as rank responses. These predictions are obtained by simply appending a predefined self-evaluation prompt to the partially or fully generated response and generating a single predefined token whose likelihood is used as the prediction. This is in contrast to preference-based reward models which can primarily only be used to rank responses.\nOur self-evaluation method is highly cost-effective, requiring no external reward models and incurring only the minimal cost of generating a single token. In contrast, an external reward model inherently requires more memory and storage. Additionally, it is unable to reuse the KV cache obtained when generating the response and would have to process the input and response from scratch.\nTo demonstrate adaptive inference-time compute allocation, we introduce two techniques: (1) adaptive sampling and (2) early pruning of unpromising samples. Adaptive sampling involves resampling a response for a given prompt until it is predicted that further samples will not yield additional improvements, thus conserving computation for complex tasks that will benefit from it. Furthermore, early pruning discards samples midway through generation if they are likely to result in suboptimal completions. These are not possible with standard reward models."}, {"title": "2. Preliminaries and Notation", "content": "An autoregressive language model generates a sequence $y = (y_1, y_2, \\dots, y_T)$ given an input context $x$ by predicting tokens sequentially. Assuming the model is parameterized by $\\theta$, the conditional probability distribution of generating a sequence $y$ given context $x$ is\n$P_\\theta(y|x) = \\prod_{t=1}^{T} P_\\theta(y_t|x, y_{<t}),$ (2.1)\nwith the convention $y_{<t} = (y_1, y_2, \\cdots, y_{t-1})$. For ease of notation, we define $p_\\theta(y_t|x) := p_\\theta(y_t|y_{<t}, x)$. For a vocabulary size $M$, the probability of predicting the t-th token $y_t$ is determined using a softmax with temperature $\\gamma$ on logit scores $z$ of all the tokens:\n$P_\\theta(y_t|x) = \\frac{\\exp(z_t/\\gamma)}{\\sum_{i=1}^{M} \\exp(z_i/\\gamma)},$ (2.2)\nwhere $z_t = \\text{logit}_\\theta(y_t|x, y_{<t})$. Higher values of $\\gamma$ introduce more randomness; as the temperature $\\gamma$ approaches zero, the distribution becomes concentrated on the token with the highest logit.\nNext-token prediction is a typical approach used for pre-training and fine-tuning of LLMs. In particular, supervised fine-tuning (SFT) minimizes the cross-entropy loss between the model's predicted next token and the target token in a given sequence. Given a dataset $D = \\{(x, y)\\}$ of input context $x$ and target response $y$, the SFT loss is given by:\n$L_{SFT}(\\theta, D) = -E_{(x,y)~D} [\\sum_{t=1}^{y} \\log P_\\theta (y_t|x, y_{<t})].$ (2.3)\nOn-policy pairwise preference dataset is a preference dataset that consists of responses generated by a single model:\n$D_{\\text{preference}} = \\{(x, y_1, y_2, l)\\}$, (2.4)"}, {"title": "3. Adaptive Inference-Time Compute via Capability-Aware and Mid-Generation Self-Evaluations", "content": "A simple, yet effective method for allocation of test-time compute is Best-of-N, allowing the model to improve upon its responses over greedy sampling. However, the generation of a large, fixed number of samples is computationally expensive and the efficacy of this approach relies heavily on the quality of an external reward model, which can additionally incur substantial computational cost. In the following section, we address the cost and robustness of the reward model by establishing a general framework for self-evaluation via token prediction. We show how to train capability-aware and mid-generation self-evaluators to reduce the cost of generating a large, fixed number of samples."}, {"title": "3.1. Capability-Aware and Mid-Generation Self-Evaluations", "content": "In the classic reward modeling paradigm, a pre-trained LM is used as an initialization for reward modeling. However, this approach also relies on a newly added reward head to output the reward, which diverges from the token prediction task for which the LLM was trained, which can hurt performance for reward modeling. Additionally, generally preference-based reward models learn only to rank arbitrary responses. This may be limiting for comparing on-policy samples (such as those from Best-of-N) during inference to determine how good the response is with respect to the model's own capabilities. Furthermore, reward models are generally only able to evaluate full responses. Responses that are clearly unpromising very early on in generation still have to be generated until the end for evaluation, wasting computational resources. Can we use an alternate paradigm to model rewards to address these limitations?"}, {"title": "Reward Modeling with Token Prediction.", "content": "To obtain a self-evaluation of a response from an LLM, we simply append a predefined self-evaluation prompt $I$ to the generated response $y$ and obtain a score in the form of the likelihood of a predefined token $t_{\\text{good}}$ corresponding to the likelihood of the response being good. This approach allows us to acquire rewards without any external reward model, making it highly cost-effective as we can reuse the KV cache obtained during the generation of the response. Also, this leverages the existing zero-shot capability of the models to judge its own responses as a prior, which has been shown to be effective in works such as Madaan et al. (2023).\nFormally, given a preference dataset $D_{\\text{pref}} = \\{ (x, y_{\\text{good}}, y_{\\text{bad}}) \\}$, which contains input context $x$ and response pairs $y$, we can train a self-evaluation model by maximizing the likelihood of the good token $\\log p_\\theta (t_{\\text{good}} | (x, y_{\\text{good}}))$ for good responses $y_{\\text{good}}$, and the likelihood of the bad token $\\log p_\\theta (t_{\\text{bad}} | (x, y_{\\text{bad}}))$ for bad responses $y_{\\text{bad}}$. To achieve this, we minimize the SFT loss in Eq. (2.3) on a modified variant of the preference dataset $D_{\\text{self-evaluation}}$\n$D_{\\text{self-evaluation}} = \\{((x, y_{\\text{good}}, I), t_{\\text{good}})\\} \\cup \\{((x, y_{\\text{bad}}, I), t_{\\text{bad}})\\}$. (3.1)\nDuring inference, we normalize the likelihood of $t_{\\text{good}}$ as the score to rank responses:\n$\\text{Score} = \\frac{p_\\theta (t_{\\text{good}} | x, y, I)}{\\sum_{t \\in \\{t_{\\text{good}}, t_{\\text{bad}}\\}} p_\\theta(t | x, y, I)}$. (3.2)\nGiven that the language model has a fixed vocab size $|V|$, probability mass can be spuriously assigned to tokens $t \\notin \\{t_{\\text{good}}, t_{\\text{bad}}\\}$, resulting in this normalization to be desirable.\nNote: it is beneficial to establish within the self-evaluation prompt that the LLM has to perform a classification task and that the target tokens are natural responses to effectively leverage the zero-shot capability of the model. We find that with a poorly designed self-evaluation prompt $I$, the model will overgeneralize during training and respond with $t_{\\text{good}}$ or $t_{\\text{bad}}$ for all queries, regardless of relevance. We need to keep the underlying policy or model unchanged when learning to self-evaluate. To do so, we need the model's prior distribution over tokens to be similar to what we expect after training where $p_\\theta(t_{\\text{good}} \\cup t_{\\text{bad}} | x, y, I) \\approx 1$. Fortunately, this can easily be done by explicitly stating that the LLM should respond with only $t_{\\text{good}}$ or $t_{\\text{bad}}$ in the self-evaluation prompt $I$.\nThe probability that the model cannot generate a more preferred response can be easily derived from on-policy pairwise preferences and ties. It is the probability that the current sample $y$ results in"}, {"title": "$\\mathbb{a}$ Win or $\\mathbb{a}$ Tie against another sample $y'$", "content": "More formally:\n$P_{y'~p_\\theta(y|x)} (r(x, y) - r(x, y') \\geq -\\epsilon) = P(\\text{Win} \\cup \\text{Tie} | x, y)$. (3.3)\nAccounting for ties in reward modeling is especially important for on-policy pairwise data where responses coming from the same model are very likely to be similar. Ties commonly occur when generating responses from the same model (e.g. 40% of the time) and are even more common with simple tasks. Since ties indicate that the model cannot do better, they are crucial when learning to do capability-aware self-evaluations. If ties are not specified, it is also harder to model the reward as the model has to distinguish between extremely similar responses. Furthermore, since $(100-40)/2 = 30\\%$ of samples result in a Loss, we see that the model can do significantly better on a query only roughly 30% of the time.\nNotice that $P(\\text{Win} \\cup \\text{Tie} | x, y)$ monotonically increases with the sample's underlying reward $r(x, y)$. This means that it can also be used to determine if one sample has a higher reward than another. Since inference-time compute strategies only care about being able to rank responses, it turns out that modeling $P(\\text{Win} \\cup \\text{Tie} | x, y)$ is more useful than $r(x, y)$ since it can also be used inform decisions on whether or not to allocate more compute.\nThe probability that restarting generation will not yield a more preferred response can be used to evaluate the quality of a partial response $Y_{y_{1:t}}$. In the context of adaptive inference-time compute, if we find that this probability is low, we can stop spending additional inference resources on generating the remainder of the partial response, pruning this poor partial response. Again, in the context of on-policy pairwise preferences or ties, this probability is simply $P(\\text{Win} \\cup \\text{Tie})$ conditioned on a partial response $Y_{1:t}$:\n$P_{\\substack{y'~p_\\theta(y|x) \\\\ Y_{t+1:T}~p_\\theta(Y_{t+1:T}|x,y_{1:t})}}(r(x, y_{1:T}) - r(x, y') \\geq -\\epsilon) = P(\\text{Win} \\cup \\text{Tie} | x, y_{1:t})$. (3.4)\nModeling these probabilities with an on-policy pairwise preference dataset with ties. To train LLMs to make capability-aware self-evaluations, we construct a dataset derived from an on-policy pairwise preference dataset with ties, where good responses are those that resulted in a Win or Tie $(y_{\\text{win}}, y_{\\text{tie}})$, and bad responses are those that resulted in a Loss $(y_{\\text{loss}})$. To train LLMs to make capability-aware self-evaluations mid-generation, we simply include the same examples but with responses randomly truncated $(y_{\\text{win, trunc}}, y_{\\text{tie, trunc}}, y_{\\text{loss, trunc}})$.\nWe use the following self-evaluation prompt and target tokens:\n$I = \\text{'Would you do better if you started over? (\u201cYes.\" or \"No.", "}$\n$t_{\\text{good}} = \\text{": "o'}, t_{\\text{bad}} = \\text{'Yes'}$\nOur final dataset $D_{\\text{capability-aware}}$, used to minimize the SFT loss (Eq. (2.3)), is constructed as:\n$D_{\\text{capability-aware}} = \\{((x, y_{\\text{win}}, I), t_{\\text{good}})\\} \\cup \\{((x, y_{\\text{tie}}, I), t_{\\text{good}})\\} \\cup \\{((x, y_{\\text{loss}}, I), t_{\\text{bad}})\\} \\cup \\{((x, y_{\\text{win, trunc}}, I), t_{\\text{good}})\\} \\cup \\{((x, y_{\\text{tie,trunc}}, I), t_{\\text{good}})\\} \\cup \\{((x, y_{\\text{loss,trunc}}, I), t_{\\text{bad}})\\}$. (3.5)\nDuring inference, we compute the normalized likelihood of the $t_{\\text{good}}$ (\u2018No') token to score full or partial responses:\n$p_\\theta(\\text{Win} \\cup \\text{Tie} | x, y_{1:t}) = \\frac{p_\\theta(t_{\\text{good}} | x, y_{1:t}, I)}{\\sum_{\\tau \\in \\{t_{\\text{good}}, t_{\\text{bad}}\\}} p_\\theta(\\tau | x, y_{1:t}, I)}$ (3.6)"}, {"title": "Takeaways for Capability-Aware and Mid-Generation Self-Evaluations", "content": "Though traditional reward models can rank samples from a model effectively, they cannot quantify how much better a model can do if it resamples. We introduce capability-aware self-evaluation which allows a model to model the probability that it cannot generate a more preferred response. This consists of three components: (1) reward modeling using token prediction to better leverage the pretrained model's existing knowledge and capabilities (2) relying on on-policy pairwise preferences and ties to model the probability that the model cannot generate a more preferred response, (3) using truncated responses to model the probability that restarting generation will not yield a more preferred response to consider the efficacy of partial completions. We will now explore how these techniques can be used to make Best-of-N more efficient and scalable."}, {"title": "3.2. Making Best-of-N Efficient and Scalable", "content": "Best-of-N is very computationally expensive, as it requires the generation of a large, fixed number of samples. This leads to wasted computation on queries that do not require more compute, and underutilization of compute on queries that could benefit from it. To remedy this and make Best-of-N more practical, we introduce two new primitives that leverage the capability-aware and mid-generation self-evaluations that we introduced in the last section. The first is adaptive sampling, where additional samples are allocated to a query only if they are predicted to be beneficial. The second is early pruning, where unpromising samples are stopped from being generated further to save inference computation."}, {"title": "3.2.1. Adaptive Sampling and Annealing", "content": "We introduce adaptive sampling as a technique to allocate inference-time compute only when it is beneficial to do so. We further introduce exponentially increasing parallel sampling to mitigate the main disadvantage of adaptive sampling, latency. Finally, we introduce a temperature annealing strategy to balance exploration and exploitation while adaptively sampling and boost efficiency.\nResampling until meeting a threshold. We adaptively sample by resampling only when the model predicts it can produce a better response. We do so by computing the likelihood that the model cannot generate a better response $p_\\theta(\\text{Win} \\cup \\text{Tie} | x, y)$ for every sample. If this probability exceeds a predefined threshold $\\tau$ for any sample, we stop resampling and select the sample with the highest $p_\\theta(\\text{Win} \\cup \\text{Tie} | x, y)$ as the final response. Otherwise, we resample, repeating this process until the threshold is met or a maximum number of samples $N_{\\text{max}}$ is reached. This approach concentrates computational resources on prompts where improvement is likely, avoiding unnecessary sampling when further gains are improbable.\nIncreasing number of parallel samples exponentially. To reduce latency, we sample in exponentially increasing batch sizes. Specifically, for the k-th sampling iteration, the batch size $N_k$ is defined as:\n$N_1 = 1, N_k = 2^{k-2} \\text{ for } k > 1$. (3.7)\nThis ensures that the cumulative number of samples by the k-th iteration is $2^{k-1}$. This exponential increase minimizes latency, reducing the number of iterations needed to meet $N_{\\text{max}}$, while allowing for enough self-evaluations to determine if larger batches of samples are necessary.\nTemperature annealing schedule based on number of samples generated so far. To balance exploitation and exploration, we vary the temperature $\\gamma$. The temperature for the k-th iteration is given by:\n$\\gamma_k = 1 - 2^{-(k-1)}$. (3.8)"}, {"title": "Algorithm 1 Adaptive Sampling and Annealing", "content": "Require: Input prompt $x$, maximum samples $N_{\\text{max}}$, threshold $\\tau$\n1: Initialize $k \\leftarrow 1, N_{\\text{cum}} \\leftarrow 0, S \\leftarrow \\emptyset$\n2: while $N_{\\text{cum}} < N_{\\text{max}}$ do\n3: $N_k \\leftarrow 1$ if $k=1$ else $2^{k-2}$\n4: $N_{\\text{cum}} \\leftarrow N_{\\text{cum}} + N_k$\n5: $\\gamma_k \\leftarrow 1 - 2^{-(k-1)}$\n6: Sample $N_k$ responses $\\{y_i\\}_{i=1}^{N_k}$ using temperature $\\gamma_k$\n7: for each $y_i$ in $\\{y_i\\}_{i=1}^{N_k}$ do\n8: Compute $p_i \\leftarrow p_\\theta(\\text{Win} \\cup \\text{Tie} | x, y_i)$; add $(y_i, p_i)$ to S\n9: end for\n10: if $\\text{Any } p_i > \\tau$ then break\n11: end if\n12: $k \\leftarrow k + 1$\n13: end while\n14: Select $y^*$ from S with highest $p_i$\n15: Return $y^*$\nInitially, the temperature starts low (e.g., $\\gamma_1 = 0$) to prioritize high-probability responses. As $k$ increases, $\\gamma_k$ quickly approaches 1, encouraging more diverse and creative sampling. This annealing schedule allows the model to first focus on exploiting the most likely responses, then explore alternative options as more samples are generated."}, {"title": "3.2.2. Early Pruning of Unpromising Samples", "content": "One downside to adaptive sampling requires the generation of samples in series, which introduces latency. However, can we still leverage parallel sampling to avoid increasing the latency of response generation but still enable adaptive test-time compute allocation? One approach to reducing computational costs in parallel generation is to early prune unpromising samples based on mid-generation self-evaluations.\nWhen to prune. Pruning too early risks discarding samples that could improve, while pruning too late offers minimal savings. Thus, we balance this trade-off when selecting the fixed number of initial tokens (e.g., 64 or 128) before making any pruning decisions.\nWhich samples to prune. After generating an initial number of tokens, we compute the intermediate self-evaluations for each partially generated sample. After ranking samples by the resulting scores, we stop the generation of the bottom x% (e.g., 50% or 75%) to conserve computation. This ensures that only the most promising partial samples continue to be generated."}, {"title": "Takeaways for Scaling Best-of-N", "content": "We discuss two strategies to efficiently scale Best-of-N leveraging Capability-Aware and Mid-Generation Self-Evaluation: (1) Adaptive Sampling and Annealing and (2) Early Pruning of Unpromising Samples. For adaptive sampling, we allocate inference-time compute dynamically by exponentially increasing the number of samples for Best-of-N. This allows us to increase the number of samples for more difficult queries, while saving samples on easier or intractable queries. Early Pruning takes an alternative approach, where a larger sample budget is initially used to sample a fixed number of tokens (e.g 128 or 256), at which point (mid-generation), a self-evaluation is done, where the least promising samples are pruned to conserve computation."}, {"title": "4. Experiments", "content": "We evaluate our methods by first examining the sample efficiency of self-evaluation. We then evaluate its impact on optimizing inference-time compute with adaptive sampling and early pruning of unpromising samples."}, {"title": "4.1. Training Data and Evaluations", "content": "Construction of On-Policy Pairwise Preference Dataset with Ties. Training reward models typically requires human-labeled preferences, which are costly to obtain, especially for on-policy data generated by the model being trained. To mitigate this, we utilize an existing reward model, ArmoRM (Wang et al., 2024) trained on approximately 1,000,000 preferences. This reward model serves as our underlying reward $r(x, y)$.\nWe conduct our experiments using the Llama 3.1 8B Instruct model, which is fine-tuned on a preference dataset of 50,000 real user prompts from LMSYS (Chiang et al., 2024) and pairs of on-policy responses (sampled from the same Llama 3.1 8B Instruct model). These responses are scored using ArmoRM, the underlying reward. Preferences or ties between responses are determined using a threshold $\\epsilon$ of 0.01 on the reward difference, resulting in an on-policy pairwise preference dataset with approximately 40% ties.\nEvaluation Protocol We evaluate the performance of our self-evaluation model and adaptive test-time compute approaches in two domains: (1) the AlpacaEval (Dubois et al., 2024) benchmark and (2) the GSM8K dataset (Cobbe et al., 2021). AlpacaEval 2.0 is an automatic benchmark that compares model responses to those generated by GPT-4 across approximately 800 representative prompts. The final metric is the win rate against GPT-4, adjusted to increase correlation with human preferences (Spearman correlation of 0.98). While highly correlated with human judgments, this metric is relative\u2014it measures success based on outperforming another model rather than achieving absolute human satisfaction. GSM8K is a collection of 8.5K grade school math word problems involving multi-step reasoning and basic arithmetic operations. We use GSM8K as it is a popular benchmark for reasoning as well as absolute measure of performance that is not relative to another LLM. This is particularly useful in evaluating adaptive sampling where it allocates compute resources to queries that benefit from it."}, {"title": "4.2. Performance and Sample Efficiency", "content": "Baselines. To evaluate performance with Best-of-N, we use two baselines. The Zero-Shot (LLM-as-a-Judge) baseline uses Llama 3.1 8B Instruct without additional training. We prompt and get scores for zero-shot predictions in the exact same manner that we do for capability-aware self-evaluations. The Bradley-Terry Reward Model also uses Llama 3.1 8B Instruct as the base model and is trained using the same on-policy pairwise preference dataset with ties.\nAs shown in Table 1, the zero-shot approach performs modestly on both benchmarks, confirming that LLMs have a non-trivial ability to self-evaluate. The Bradley-Terry reward model unsurprisingly does much better. Our token prediction method outperforms both. On AlpacaEval, it achieves a 33.8% win rate against GPT-4 using Best-of-16 sampling, slightly surpassing the Bradley-Terry model's 33.2% and nearing the underlying reward model's 36.3% (trained on 1 million preferences). On GSM8K, token prediction attains 91.0% accuracy, significantly outperforming the Bradley-Terry model's 87.7%. This suggests that our method is more sample-efficient and generalizes better, especially on queries that might be uncommon among real user prompts like those in GSM8K.\nThese results demonstrate that token prediction effectively leverages the pre-trained model's priors for self-evaluation, approaching the performance of the larger Llama 3.1 70B, which achieves 38.1% on AlpacaEval and 95.1% on GSM8K. With additional data and hyperparameter tuning, we anticipate that we can easily further close the gap with the underlying reward.\nWe also examined other probabilities to model with token prediction, specifically $p_\\theta(\\text{Win})$ or $p_\\theta(\\text{Win} | \\text{Tie})$. Our findings, as shown in Table 3, indicate that including or removing ties does not significantly impact performance, but modeling $p_\\theta(\\text{Win})$ is significantly more difficult as it forces the model to distinguish between samples that result in wins or ties, which are both cases in which the model performs relatively well for its capability.\nNotably, using Best-of-N sampling with our methods allows the Llama 3.1 8B model to approach the performance of Llama 3.1 70B, a model roughly 10\u00d7 larger that gets 38.1% and 95.1% respectively. This highlights the efficacy of Best-of-N sampling in enhancing model performance. In the following section, we address the expensive nature of this method with adaptive compute strategies."}, {"title": "4.3. Efficiency and Scaling of Adaptive Sampling and Annealing", "content": "We evaluate the efficiency gains and performance trade-offs of our adaptive sampling and annealing strategy on the GSM8K dataset. Our objective is to retain the performance benefits of Best-of-16 sampling while significantly reducing the average number of samples used and minimizing latency. We want to evaluate the methods' ability to allocate samples when necessary. For our experiments, the capability-aware self-evaluations are always used to select the best sample. This is so that the final selection method is not a confounding factor.\nAs a baseline, we first assess the performance Best-of-N. The best sample is selected using self-evaluation with token prediction. Figure 1 shows that increasing the number of samples from 1 to 16 incrementally improves the GSM8K Pass@1 accuracy from 84.2% to 91.0%. This represents the maximum achievable performance with our method, which we define as 100% of the maximum improvement. However, generating a fixed large number of samples per query is expensive.\nTo mitigate this, we introduce adaptive sampling controlled by a threshold $\\tau$ and a maximum number of samples $N_{\\text{max}}$. Initially, we test adaptive sampling using the underlying reward model to decide whether to generate additional samples. As shown in Figure 1, this approach does not significantly outperform random selection. While the underlying reward model effectively ranks responses, it does not consider the model's capability to improve upon its own outputs, substantially limiting its effectiveness in informing resampling decisions.\nIn contrast, our adaptive sampling method utilizing capability-aware self-evaluations via token prediction yields substantial efficiency gains. Table 4 demonstrates that by adjusting the threshold $\\tau$ for the win-or-tie probability $p_\\theta(\\text{Win} \\cup \\text{Tie} | x, y)$, we can control the average number of samples. For instance, setting $\\tau = 0.98$ results in 97.1% of the maximum performance while using only an average of 4.1 samples, compared to the 16 samples required for maximum performance.\nMoreover, incorporating the annealing schedule further enhances performance, particularly at lower thresholds. As illustrated in Table 2, with annealing, we achieve 73.5% of the maximum improvement using an average of just 1.2 samples when $\\tau = 0.92$. This indicates that our annealing strategy effectively balances exploitation and exploration during sampling.\nOur adaptive sampling approach offers significant compute savings compared to Best-of-N. By adjusting $\\tau$ and $N_{\\text{max}}$, we can fine-tune the trade-off between performance and efficiency. The latency introduced by adaptive sampling is minimal due to the exponentially increasing batch sizes."}, {"title": "4.4. Efficiency Gains of Pruning Unpromising Samples", "content": "We evaluate the efficiency gains from early pruning of unpromising samples on the AlpacaEval benchmark. We begin with 16 samples and prune from them. Our primary metrics are the percent of maximum improvement achieved and the number of tokens generated per prompt. The latter is a more accurate measure of computational cost than the number of full samples generated, as partial responses also consume computational resources.\nOur experiments control two variables: the number of tokens generated before making pruning decisions (64 or 128 tokens), and the percentage of samples pruned (e.g., pruning 75%). Again, the capability-aware self-evaluations are used the final sample selection method.\nImpact of Evaluation Timing on Performance. As shown in Figure 1, pruning at 64 tokens improves performance over random pruning at 0 tokens. For instance, pruning 50% of samples at 64 tokens achieves a win rate of 33.4%, closely approaching the maximum of 33.8% without pruning. The timing of when we perform self-evaluations significantly affects performance. Pruning at 128 tokens yields even better results. Pruning 75% of samples at 128 tokens attains a win rate of 33.2%, nearly matching the maximum performance without pruning. This improvement is expected, as longer partial responses provide more context for accurate self-evaluation.\nOverall, there are substantial savings if one prunes at the right time and the right number of samples.\nIn terms of computational cost measured by number of tokens, pruning 75% of 16 samples at 128 tokens reduces the average number of tokens generated per prompt from 7,259 to 3,220 as shown in Figure 3, resulting in significant efficiency gains."}, {"title": "5. Related Work", "content": "Reward Modeling in RLHF. Traditionally, reward models (RMs) within Reinforcement Learning with Human Feedback (RLHF) have been trained using discriminative techniques, often drawing on ranking models like the Bradley-Terry (BT) framework (Bradley and Terry, 1952). These RMs are generally treated as binary classifiers, distinguishing between preferred and dispreferred completions for a given prompt (Stiennon et al., 2020; Ouyang et al., 2022). The preference signal is predicted by generating a continuous score, typically derived from a linear head appended to an autoregressive language model. An alternative line of work departs from this discriminative paradigm by directly modeling preferences through joint probability estimations, as seen in methods that predict the likelihood of one response being preferred over another (An et al., 2023).\nRecent advancements in RLHF have introduced implicit reward models that circumvent the necessity for a distinct reward classifier by learning the distribution of preferred and dispreferred completions via objectives like Direct Preference Optimization (DPO) (Rafailov et al., 2023) and Implicit Preference Optimization (IPO) (Gheshlaghi Azar et al., 2023). These methods embed preference learning directly into the policy optimization process, unlike explicit preference modeling.\nAnother approach to reward modeling is the application of prompting techniques, such as the \u201cLLM as a Judge\" framework (Zheng et al., 2023), where large language models (LLMs) are prompted to act as evaluators without additional finetuning. Despite the potential of these methods, even advanced models like GPT-40 underperform when compared to dedicated RMs in more complex evaluations, such as those found in the RewardBench benchmark (Lambert et al., 2024).\nTechniques for Inference-time Compute. During inference, the integration of a reward model with a proposal distribution (LLM) can be employed to refine the output responses to a given prompt. One notable paradigm in this context is Self-Consistency (Wang et al., 2023), which is designed for factual queries with extractable answers. In this approach, the language model selects the response it generates with the highest frequency across multiple samples. Optimizations such as Early Stopping Self-Consistency have been proposed, which terminate the sampling process early if a subset of responses shows strong consistency. However, these approaches face limitations due to their dependence on identifying the most \u201cconsistent\" response from a large pool of discrete answers, restricting their applicability to tasks like multiple-choice or mathematical problem-solving.\nIn addition, search algorithms such as Best-of-N and Beam Search have been explored in works such as"}]}