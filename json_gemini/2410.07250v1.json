{"title": "Reconstruction of Particle Flow Energy Distribution Using Deep Learning Algorithms", "authors": ["Han Zhang", "Shengxiang Lin", "Xingyi Zhang", "Yu Wang", "Yangguang Zhang"], "abstract": "In high-energy particle physics, extracting information from complex detector signals is crucial for energy reconstruction. Recent advancements involve using deep learning to process calorimeter images from various sub-detectors in experiments like the Large Hadron Collider (LHC) for energy map reconstruction. This paper compares classical algorithms-MLP, CNN, U-Net, and RNN-with variants that include self-attention and 3D convolution modules to evaluate their effectiveness in reconstructing the initial energy distribution. Additionally, a test dataset of jet events is utilized to analyze and compare models' performance in handling anomalous high-energy events. The analysis highlights the effectiveness of deep learning techniques for energy image reconstruction and explores their potential in this area.", "sections": [{"title": "1 Introduction", "content": "The Large Hadron Collider (LHC) experiments yield detailed measurements of charged particle trajectories and energy deposition within calorimeters, which are instrumental in reconstructing and identifying particle energies [1]. Jets, originating from the fragmentation of quarks and gluons in high-energy collisions, play a pivotal role in analyzing LHC data and probing new phenomena [2].\n\nThe Particle Flow (PFlow) algorithm, initially developed in the ALEPH experiment [3] and subsequently refined within the CMS experiment at the LHC [4\u20136], has significantly advanced the reconstruction of hadronic jets and \u03c4 leptons [7\u20139]. Although the ATLAS experiment has achieved precise jet energy measurements [10], the pursuit of more efficient methodologies remains a critical endeavor.\n\nGiven the escalating complexity of experimental data, deep learning algorithms present a promising avenue to enhance the precision and efficiency of energy reconstruction. Their capacity to learn from complex patterns and interactions within the data positions them as a valuable tool in addressing the challenges of modern particle physics experiments.\n\nDeep learning algorithms for image recognition are extensively utilized in the analysis of calorimeter showers and jets, employing techniques such as Convolutional Neural Networks (CNNs) [11\u201313] and Graph Neural Networks (GNNs) [13, 14]. These algorithms play a crucial role in the reconstruction of energy deposition in calorimeters and jet tagging, particularly in complex environments characterized by high particle density and noise [15\u201317]. GNNs are currently the leading models in particle physics due to their ability to represent particles and interactions as graph nodes and edges, which suits the complex geometric layouts of energy deposition graphs and offers a more robust solution compared to traditional methods [18-20]. Although GNNs are prevalent, our research also explores other techniques, such as CNNs and Recurrent Neural Networks (RNNs) for effective image reconstruction, highlighting the potential of these methods for processing image data.\n\nWith the advancement of deep learning, Transformers, introduced by Vaswani et al. [21], emerge as the dominant architecture in Natural Language Processing (NLP) owing to their capability to capture long-range dependencies and execute parallel processing through the self-attention mechanism. Although initially designed for textual data, the flexibility and powerful representation of Transformers enable their application to other types of data. The Vision Transformer (ViT) extends this concept to image recognition, as demonstrated by Dosovitskiy et al. [22], offering a novel perspective in handling complex data structures. Further advancements in deep learning have shown that the self-attention mechanism in Transformers [21] outperforms traditional models in various tasks, and its introduction into high-energy physics presents a promising and innovative approach [20, 23, 27], which we explore further in this paper.\n\nThis paper does not attempt to fully reconstruct the true particle energy distributions. Instead, it preliminarily explores whether deep learning algorithms are less demanding regarding the complexity and realism of the training datasets, and their ability to adapt to special cases, such as jets, that are not present in the training datasets, by evaluating their performance on more realistic and complex scenarios after being trained on simple datasets."}, {"title": "2 Simulation experiments and datasets", "content": "2.1 Simulation framework\n\nGiven the difficulty of performing complete Monte Carlo simulations, this study employs a simplified stochastic algorithm that adheres to basic physical laws to construct energy distribution maps. In this algorithm, necessary simplifications of the physical background ensure that the generated dataset effectively simulates the particle collision processes observed in actual experiments.\n\nThis simulation framework models particle interactions within diverse detector systems and generates synthetic datasets for analysis. It assigns energy, charge, and type to particles based on probabilistic distributions and simulates their interactions with electromagnetic and hadronic calorimeters, as well as a tracking system, incorporating the effects of magnetic fields on charged particles. The resulting data is represented as 56\u00d756 grayscale images, which are saved and visualized for detailed examination. This approach facilitates the study of detector responses and enables the creation of realistic synthetic datasets for research purposes.\n\n2.2 Image data overview\n\nEmcal: This image represents data from the electromagnetic calorimeter, designed to accurately measure the energy of electrons and photons. The calorimeter absorbs these particles, generating electromagnetic showers and capturing the total energy deposited within the detector.\n\nHcal: This image displays information from the hadronic calorimeter, responsible for estimating the energy of hadrons like protons and neutrons. It detects the hadronic showers produced during particle interactions and records the total deposited energy, offering an approximation of the hadron's original energy.\n\nTracker_n and Tracker_p: These images are produced by the tracking system, which traces the trajectories of charged particles within a magnetic field. By collecting data at multiple points, the system reconstructs the particles' paths, allowing for the identification of particle type, momentum, and charge. The notation \"n\" denotes negatively charged particles, while \"p\" indicates positively charged particles.\n\nTruth: The output image acts as the ground truth reference, representing the ideal target used to assess the model's prediction accuracy.\n\n2.3 Dataset descriptions and usage\n\nThe Gauss_S1.00_NL0.30_B0.00 dataset consists of 10,000 sets of Emcal, Hcal, Tracker_n, Tracker_p, and Truth images generated under conditions that ignore particle deflection. This dataset is primarily used to validate the model's ability to learn fundamental principles of energy reconstruction. In contrast, the Gauss_S1.00_NL0.30_B0.50 dataset includes 10,000 sets of corresponding images with particle deflection considered, and it is used to evaluate the model's capacity to learn from particle deflection phenomena. The specific parameter configurations for these datasets are detailed in Table 1, with representative examples illustrated in Figure 1.\n\nThe Gauss_S1.00_NL0.30_B0.50_jet dataset consists of 1,000 sets of Emcal, Hcal, Tracker_n, Tracker_p, Jet, and Truth images, generated with consideration of particle deflection and specific scenarios such as jet streams. The Jet images were produced using Pythia 8. Note that this dataset is designed exclusively for testing purposes and is not suitable for model training."}, {"title": "3 Deep neural network models", "content": "The neural network model is designed to regress the fractional energy deposition in each pixel cell using advanced deep learning techniques, with the goal of generating accurate neutron energy deposition images. For a rigorous comparison of model performance, all models are trained on a consistent training set. To enhance training efficiency and effectiveness, a progressive volume increase strategy is implemented, gradually expanding the training set size to accelerate model convergence.\n\nA\n\nThe architecture of the models is depicted in Appendix\n\n3.1 Multilayer Perceptron (MLP)\n\nTo investigate the regression of fractional energy deposition in each pixel, we employ MLP as the foundational model. The MLP, notable for its straightforward architecture and robust regression performance, is configured with 4 input nodes corresponding to the Emcal, Hcal, Tracker_n, and Tracker_p datasets. The network comprises 2 hidden layers, each with 1024 neurons, utilizing the ReLU activation function to capture nonlinear relationships. The predicted fractional energy deposition is output through a single node.\n\n3.2 CNNs\n\n3.2.1 CNN\n\nThe CNN model processes 4-channel input images, consisting of Emcal, Hcal, Tracker_n, and Tracker_p channels. It begins with convolutional operations using 3\u00d73, 5\u00d75, and 7\u00d77 kernels, each producing 2 output channels, with padding to preserve spatial dimensions. The generated feature maps are merged along the channel axis, resulting in a combined 6-channel feature map.\n\nThe combined feature map is subsequently processed through an encoder module consisting of 2 convolutional layers, which include convolution, batch normalization, and ReLU activation, expanding the feature map to 64 channels. The decoder module, also comprising 2 convolutional layers, reduces the channel count to 1, and a Sigmoid activation function is applied to produce the final single-channel output image.\n\nAdditionally, a Squeeze-and-Excitation Block (SEBlock) is integrated into the network to enhance performance and efficiency.\n\n3.2.2 CNN with self-attention modules\n\nThis model builds upon the previous CNN framework by integrating a self-attention module to replace the SEBlock, enhancing representation learning and the model's ability to handle shifts caused by charged particles. The model is divided into three main parts: the encoder, the attention module, and the decoder.\n\nIn the attention module, the multi-channel feature map from the encoder is processed using 1\u00d71 convolutional kernels to generate the query (Q), key (K), and value (V) matrices. Attention scores are computed and utilized to enhance the features by multiplying them with the original image. The enhanced features are then fed into the decoder to generate the final output."}, {"title": "3.2.3 CNN with 3D convolution modules", "content": "The optimization presented integrates 3D convolutional layers into a CNN framework to address multi-channel image reconstruction tasks. By applying convolutional kernels of sizes 3\u00d73\u00d73, 3\u00d75\u00d75, and 3\u00d77\u00d77, the model extracts spatial features across multiple scales, enhancing its ability to capture and represent complex patterns within the multi-channel input images.\n\nThis approach leverages the depth dimension to improve feature extraction, which is crucial for reconstructing detailed and accurate representations from volumetric data. After the initial 3D convolutions, the model transitions to 2D convolutions, focusing on high-level spatial features and enabling efficient reconstruction. This methodology effectively addresses the challenges associated with multi-channel image reconstruction by providing a more nuanced understanding of the input data."}, {"title": "3.3 U-Nets", "content": "3.3.1 U-Net\n\nThe U-Net architecture features a U-shape formed by its encoder-decoder structure [25]. This model processes four 56\u00d756 input images: Emcal, Hcal, Tracker_n, and Trac-ker_p. The encoder employs 3\u00d73 convolutional layers followed by batch normalization to increase channel depth, with subsequent 2\u00d72 downsampling to reduce spatial dimensions and capture hierarchical features. In the decoder, nearest-neighbor upsampling is used along with 1\u00d71 convolutions to adjust the number of output channels. Skip connections are utilized to concatenate upsampled features with corresponding encoder features. LeakyReLU activation functions are applied to enhance non-linearity and mitigate gradient vanishing, and a sigmoid function is employed to produce the final single-channel output.\n\nThis design facilitates the effective integration of both local and global features, thereby enhancing the accuracy of energy deposition map reconstruction.\n\n3.3.2 U-Net with self-attention modules\n\nThe improved version of the U-Net has been enhanced in two main aspects: the introduction of a self-attention module [21] and an increase in network depth.\n\nFirstly, a self-attention module is introduced following the convolutional layers. This module captures the relationships between various positions in the feature map, boosting the model's capacity to represent complex features while enhancing its robustness and adaptability.\n\nSecondly, the encoder has been extended from a single layer to three layers, each employing 3\u00d73 convolutional kernels for feature extraction and 2\u00d72 kernels for downsampling. This expansion allows for more nuanced feature extraction at multiple levels of abstraction. In the decoder, nearest-neighbor interpolation is used for upsampling, while 1\u00d71 convolutional kernels adjust the number of output channels. The upsampled feature maps are subsequently merged with the encoder's feature maps using skip connections, preserving the model's original structure.\n\nOverall, these enhancements provide the improved U-Net with better performance in handling displacements, leading to more accurate and robust feature representation."}, {"title": "3.4 RNN", "content": "The Sequence-to-Sequence model [26] is employed to analyze sequential data derived from the positional relationships in image pixel values, with the objective of learning displacement patterns.\n\nThe input data is structured as 56 'word vectors' of size 224, formed by concatenating pixel values from each channel's image along the rows. The output data consists of 56 'word vectors' of size 56.\n\nThe model utilizes an encoder-decoder architecture based on Gated Recurrent Units (GRU). Specifically, the architecture includes a two-layer GRU encoder that processes the input sequence to extract contextual information, encoding it into hidden states. These hidden states serve as the initial state for the two-layer GRU decoder, which generates the output sequence corresponding to the input sequence. The decoder's output is subsequently projected into the target space via a fully connected layer to generate the final predictions."}, {"title": "4 Results on the datasets without jets", "content": "The test set is employed to assess the models' performance, where the energy value corresponding to each pixel in the predicted image is denoted as $E_{pre}$, and the energy value corresponding to each pixel in the ground truth image is denoted as $E_{tru}$. After normalization, the two are referred to as $E_{pre,nor}$ and $E_{tru,nor}$, respectively. The normalized energy difference $\u0394E_{nor}$ is calculated using the following formula:\n\n$\u0394E_{nor} = E_{pre,nor} - E_{tru,nor}$                                                (1)\n\nWhen plotting the histogram of the energy difference distribution, we removed the points with a true value of zero to make the graph clearer. A Gaussian function is used to fit the energy difference distribution. Figure 4 illustrates the performance of the MLP model on the Gauss_S1.00_NL0.30_B0.00 dataset. Figure 3 presents the fitted distributions of the energy difference for various models on the Gauss_S1.00_NL-0.30_B0.50 dataset, highlighting the performance of each model."}, {"title": "5 Results on the dataset with jets", "content": "Models that demonstrated strong performance on datasets with displacement but without jets were selected to test their reconstruction ability on datasets containing jets. To assess their effectiveness on the jet dataset, two metrics were established.\n\nThe first metric evaluates the accuracy of the jet's positional information, determined using the anti-k, clustering algorithm [28]. Figure 4 shows the jet center obtained by this method. The evaluation standard is the jet position deviation $D_{jet}$, defined as the pixel-based distance between the predicted jet center on the reconstructed image and the corresponding jet center on the ground truth image. Figure 5 shows the distributions of jet position deviation for each model, with the data fitted to a Gaussian function.\n\nThe second metric assesses the accuracy of the jet's energy information. This evaluation begins by identifying the jet range using the anti-kt clustering algorithm [28], followed by calculating the total energy within this range. The evaluation standard is the jet energy deviation $\u0394E_{jet,nor}$ defined by the following formula:\n\n$\u0394E_{jet,nor} = \u2211_{i\u2208 jet} E_{pre,nor_i} \u2013 \u2211_{ie jet} E_{tru,nor_i}$                                                (2)\n\nThe distributions of jet energy deviation for each model, fitted to a Gaussian function, are illustrated in Figure 6.\n\nFor jet position accuracy, both the base CNN and U-Net exhibited moderate performance, revealing certain limitations in addressing the intricate features associated with jets. In contrast, the RNN demonstrated markedly higher sensitivity to jet positions compared to the CNN and U-Net, which is attributed to its proficiency in capturing temporal dependencies within the data. The inclusion of 3D convolutions within the CNN framework resulted in a marked improvement in jet position accuracy, demonstrating their efficacy in capturing spatial features and reducing positional errors. Among the models evaluated, the U-Net architecture augmented with self-attention mechanisms exhibited superior performance in jet position accuracy, underscoring the utility of self-attention in concentrating on salient regions within the data. From a variance perspective, however, 3D convolutions achieved the most favorable outcomes. While the precision ceiling of these models does not reach the levels attained by the U-Net, they maintain a notable absence of large errors, thereby reflecting enhanced robustness.\n\nRegarding jet energy prediction accuracy, the U-Net model emerged as the most accurate overall, with mean deviations close to zero and peak values second only to those of the CNN with 3D convolutions. This superior accuracy is likely attributable to U-Net's deep architecture and the presence of skip connections. While the CNN with 3D convolutions exhibited concentrated prediction errors, its overall energy estimates tended to be lower, potentially due to interference from localized information. The RNN, despite its high sensitivity to jet energy, demonstrated less effectiveness in predicting static energy distributions compared to the CNN and U-Net. The base CNN and U-Net models were less effective in predicting jet energy, suggesting a need for further refinement or enhancement to better capture and model complex energy distributions.\n\nIn summary, the U-Net with self-attention and the CNN with 3D convolutions emerged as the most robust models, excelling in jet position and energy prediction, respectively. The RNN, while demonstrating high sensitivity, did not achieve the same level of overall prediction accuracy. The base CNN and U-Net models were comparatively less effective,"}, {"title": "6 Conclusion and outlook", "content": "This study presents significant progress in applying deep learning techniques to particle flow reconstruction in high-energy physics, particularly in enhancing jet reconstruction in the LHC experiment. Our models range from basic to advanced approaches, designed to capture the spatial relationships and particle displacement shifts within the data. By integrating self-attention mechanisms and 3D convolution methods, the models' sensitivity to complex features, such as particle shifts, has been significantly improved, leading to more accurate particle energy reconstructions.\n\nFuture work will focus on optimizing the current models and developing deep learning approaches tailored to particle physics to better capture intricate physical phenomena. The key directions include:\n\nModel optimization: Improving the CNN architecture to minimize the impact of local information. The integration of 3D convolution has demonstrated strong robustness, highlighting the potential of this approach.\n\nGNNs: While machine vision methods often produce sparse data with many non-informative regions, representing particles as point clouds and using GNNs for reconstruction offers a more efficient approach by preserving spatial information and increasing data density. A key challenge lies in how to connect nodes in the point cloud, as manually defining connections can introduce bias. To address this, applying a Dynamic Edge Convolution network [29] to automatically learn the connections offers a more adaptive and unbiased solution, improving the accuracy of particle energy reconstruction.\n\nTransformer-based models: Although CNNs outperform traditional sequence models in learning particle displacements, transformers demonstrate higher sensitivity to anomalous events, such as jets. Applying transformer variants, such as U-shaped transformers [30], holds potential for further exploration.\n\nLooking ahead, we aim to apply these models to fully Monte Carlo simulated datasets, covering a broader range of particle types and higher-resolution images in more complex scenarios. This will allow for rigorous testing and further enhancement of the models' generalization capabilities."}]}