{"title": "Diffusion for Out-of-Distribution Detection on Road Scenes and Beyond", "authors": ["Silvio Galesso", "Philipp Schr\u00f6ppel", "Hssan Driss", "Thomas Brox"], "abstract": "In recent years, research on out-of-distribution (OoD) detection for semantic segmentation has mainly focused on road scenes \u2013 a domain with a constrained amount of semantic diversity. In this work, we challenge this constraint and extend the domain of this task to general natural images. To this end, we introduce 1. the ADE-OoD benchmark, which is based on the ADE20k dataset and includes images from diverse domains with a high semantic diversity, and 2. a novel approach that uses Diffusion score matching for OoD detection (DOoD) and is robust to the increased semantic diversity.\nADE-OOD features indoor and outdoor images, defines 150 semantic categories as in-distribution, and contains a variety of OoD objects. For DOOD, we train a diffusion model with an MLP architecture on semantic in-distribution embeddings and build on the score matching interpretation to compute pixel-wise OoD scores at inference time. On common road scene OoD benchmarks, DOOD performs on par or better than the state of the art, without using outliers for training or making assumptions about the data domain. On ADE-OOD, DOOD outperforms previous approaches, but leaves much room for future improvements.", "sections": [{"title": "1 Introduction", "content": "Deep neural networks achieve remarkable results when applied to their training domain, but their robustness and reliability on out-of-distribution (OoD) data is often inadequate. Furthermore, they typically do not know that they do not know, i.e. the complexity and overconfidence of neural networks makes it difficult to reliably detect whether inputs are out-of-distribution. In practice, this makes it hard to decide whether a model prediction can be trusted or not. For deployment in the real world, and especially in safety critical applications, this is a major problem; consequently OoD detection is an active research area.\nThis work falls in the category of out-of-distribution detection for semantic segmentation. In this task, a semantic segmentation training dataset is given,"}, {"title": "2 Related Work", "content": "In the following, we provide an overview of benchmarks and approaches for OoD detection in semantic segmentation data, and of diffusion model based approaches for other OoD detection tasks."}, {"title": "2.1 Out-of-Distribution Detection for Semantic Segmentation", "content": "Detecting semantic out-of-distribution objects in segmentation data has risen in popularity in recent years, especially in the context of autonomous driving.\nMethods. Generative models have been used for out-of-distribution detection in many settings. In semantic segmentation, some approaches use GANS for image re-synthesis, and use reconstruction quality as a proxy for anomaly scores [3, 28, 52]. Due to the implicit nature of GANs, these methods require comparison networks trained with proxy outliers, whereas our method can exploit the score estimate from the diffusion model.\nA similar direction, but with non-parametric retrieval instead of parametric generation, is taken by cDNP [13], which shows that strong OoD detection results can be obtained with nearest-neighbor distances in certain deep segmentation embedding spaces. We build on their findings but improve upon it by using diffusion models as parametric density estimators instead of non-parametric kNNs.\nMany recent methods for dense OoD detection use the Mask2Former [8] segmentation model. Its separate mask/class prediction system is equipped with a rejection category, which was found to yield high quality anomaly scores [1, 20, 33, 37], especially when exposed to outliers during training [21]. These works belong to a widespread practice of exploiting the prediction confidence of semantic segmentation models as a proxy score for anomaly [5, 7, 12, 14, 18, 19, 24, 27, 34, 45].\nSome approaches specifically target the driving domain: notably, JSRNet [47] achieves strong results on driving benchmarks by modelling the road surface. In contrast to ours, such methods are not applicable to other domains.\nBenchmarks. Commonly used benchmarks are RoadAnomaly [28], SegmentMeIfYouCan [6], and Fishyscapes Lost&Found [4, 36]. All of them focus on road scenes, which are limited in terms of scene structure and semantic categories (see overview in Sec. 3). Our proposed ADE-OoD benchmark uses the ontology of ADE20k, a large scale scene segmentation dataset, to overcome this constraint."}, {"title": "2.2 Out-of-Distribution Detection with Diffusion Models", "content": "Multiple recent works have used diffusion models for OoD detection [15\u201317, 30\u201332, 40, 48, 49, 51, 54]. The approaches differ in terms of downstream task and use of the diffusion model to derive OoD scores.\nDownstream Tasks. One set of works [15\u201317, 30] applies diffusion models for OoD classification, where the task is to classify whole images at inference time as out-of-distribution. The approach DFDD [49] applies diffusion to OoD object detection, where the task is to detect OoD objects in form of bounding boxes.\nAnother set of works [15, 32, 48, 51, 54] applies diffusion models for OoD segmentation, where the task is to predict a pixel-wise segmentation of OoD regions. All of these works are either in a medical or an industrial inspection domain.\nIn this work, we present the novel approach for OoD segmentation on semantic segmentation data. In this task, a training dataset of images with semantic segmentation annotations is given. At inference time, the task is to segment OoD regions in images, where OoD is defined as not belonging to the set of training classes. Our setting differs from industrial and medical anomaly detection for two main reasons: the larger size of the input images (unsuitable for direct application of diffusion models), and the higher visual diversity and scene complexity which makes it more challenging to model the in-distribution data.\nOoD Score Computation. Most related works compute OoD scores based on reconstruction errors [15, 17, 30, 32, 48, 51, 54]. This is done by corrupting the input image, reconstructing it with the diffusion model, and using the error between the input and the reconstruction as OoD score. Goodier et al. [16] estimate the likelihood of given input data and use it as OoD score. Most related to our work, are methods [31, 40] that avoid expensive input reconstructions by leveraging the theory of denoising score matching [23, 42, 46] and deriving OoD scores from estimated scores. Shin et al. [40] do this by perturbing the input data with Gaussian noise, estimating the score for the perturbed data point, and using its L2 distance to the perturbation as OoD score. Mahmood et al. [31] estimate the score for given data points and use the norm of the estimated score. In contrast to this, our approach computes OoD scores based on the directional difference of the estimated score and the perturbation."}, {"title": "3 ADE-OoD: OoD Detection Beyond Road Scenes", "content": "In this section we introduce ADE-OoD, a novel benchmark for OoD detection on semantic segmentation data. The key characteristic of the proposed benchmark is higher diversity. It stands out by a much larger set of in-distribution categories than the typically used 19 Cityscapes classes. Furthermore, it contains images from more diverse indoors and outdoors scenes.\nBenchmark Overview. The benchmark uses the training set of the ADE20k [56] dataset, with its 150 categories, to define the in-distribution domain; for this reason, the benchmark is named ADE-OoD. It consists of 111 real-world images, each containing in-distribution and out-of-distribution segments. For each image, a binary OoD mask is provided.\nBenchmark Construction. We obtained the images for the benchmark from the validation set of ADE20k and from OpenImages [26]. The images were selected according to the following criteria: (1) presence of clear and unambiguous in- and out-of-distribution entities only, (2) presence of diverse in- and out-of-distribution categories, as well as varying indoor and outdoor settings (3) adequate scene complexity to match the original ADE20k training data, (4) collecting an adequate number of samples for statistically significant results. For annotation, we have set up a semi-automatic tool, based on the popular promptable segmentation model SAM [25]."}, {"title": "4 Diffusion Models Background", "content": "In this section, we provide background information on score matching and diffusion models. Both are relevant for our approach DOOD, as we build on the score matching interpretation of diffusion models to compute OoD scores."}, {"title": "4.1 Denoising Score Matching", "content": "Let q(x) be an unknown data distribution and {x \u2208 \u211d\u1d30}\u1d3a\u2081 a dataset of samples from that distribution. Let p(x, \u0398) be a probability density model that should fit the data distribution. Score matching [23] is an approach to learn the parameters \u0398 such that the score \u2207\u2093 log p(x, \u0398) of the model distribution matches the score \u2207\u2093 log q(x) of the data distribution. In the following, we denote the score of the model distribution as s\u2091(x) := \u2207\u2093 log p(x, \u0398).\nAs the score of the data distribution is usually not known, techniques exist to match the score based on the given finite set of data samples [23, 46]. Denoising score matching [46] is one such technique. It perturbs data points x according to a noise distribution q(x | x) and matches the score of the resulting perturbed distribution q(x) = \u222b q(x | x)q(x)dx. It has been shown that matching the score of the perturbed distribution, amounts to matching the score \u2207\u2093 log q(x | x) [46]. Typically, Gaussian noise is used as perturbation: q(x | x) = \ud835\udca9(x | x, \u03c3\u00b2I) [42, 43, 46]. Intuitively, this amounts to matching the score of a smoothed data distribution where the amount of smoothing depends on the chosen \u03c3. With \u2207\u2093 log q(x | x) = \u2212(x \u2013 x)/\u03c3\u00b2, this leads to the following objective:\n$\\\u0398 = \\arg \\min_{\\Theta} \\mathbb{E}_{q(x)}\\left[\\frac{1}{2} ||s_{\\Theta}(x) - \\frac{(x - x)}{\\sigma^2}||^2\\right].$\nNote that with a reparameterization x = x + \u03c3\u03b5 with \u03f5 ~ \ud835\udca9(0,I), one can reformulate this as matching the score s\u2091(x) to \u2212\u03f5/\u03c3.\nIn the following, we provide background on Gaussian diffusion models, which can be interpreted as denoising score matching over Gaussian perturbations of different smoothing levels [22]."}, {"title": "4.2 Diffusion Models and their Relation to Score Matching", "content": "Diffusion models [41] are a class of generative models that learn a data distribution q(x\u2080) by defining a forward diffusion process in form of a Markov chain with steps q(x\u209c|x\u209c\u208b\u2081) that gradually transform the data distribution into a simple known distribution. A model p\u2091(x\u209c\u208b\u2081|x\u209c) with parameters \u0398 is then trained to approximate the steps q(x\u209c\u208b\u2081|x\u209c) in the Markov chain of the reverse process.\nIn case of Gaussian diffusion models, the forward diffusion process gradually replaces the data with Gaussian noise following a noise schedule \u03b2\u2081,.., \u03b2\u209c:\nq(x\u209c|x\u209c\u208b\u2081) := \ud835\udca9(x\u209c; \u221a1 \u2013 \u03b2\u209cx\u209c\u208b\u2081, \u03b2\u209cI).       "}, {"title": "5 DOOD: Diffusion Score Matching for OoD Detection", "content": "In the following, we describe DOOD, our proposed method for the detection and localization of out-of-distribution entities in semantic segmentation scenarios. As illustrated in Fig. 2, the method consists of two steps: (1) feature extraction and (2) OoD score estimation. In the first step, we use a semantic segmentation model trained on in-distribution data to extract feature maps from the input images (Sec. 5.1). For estimating the OoD scores, we train a diffusion model on the features extracted from the training dataset (Sec. 5.2). At inference time, we use this diffusion model to estimate the OoD scores (Sec. 5.3)."}, {"title": "5.1 Feature Extraction", "content": "To obtain feature representations suitable for OoD detection on segmentation data, we apply a feature extractor that was pretrained on in-distribution data of the given segmentation task. More specifically, we build on the findings from [13], which show that the self-attention features of transformer encoders induce meaningful distances between in-distribution and out-of-distribution features. In particular, we employ the key features of the ViT [10] and MiT [53] used as encoders in semantic segmentation models. These are trained with a standard supervised loss on the respective predictions from the Segmenter [44] or SETR [55] decoders.\nAfter the feature extraction, our approach operates on dense feature maps of dimensions F \u2208 \u211d\u1d34\u02e3\u1d42\u02e3\u1d9c, i.e. each feature vector has dimensionality C and corresponds to one of the H\u00b7W square patches of the original image."}, {"title": "5.2 Diffusion Model Architecture and Training", "content": "In the second stage of the method, we train a diffusion model on the features extracted from the training dataset of the in-distribution semantic segmentation data. In the following, we describe the design of the diffusion model in terms of the denoiser architecture and the training procedure.\nArchitecture. Typical diffusion models for image generation use a convolutional 2D U-Net [38] architecture for the denoiser network [22, 35]. This architecture works well for image generation, because it embeds spatial context information in the sampling process. For our application, however, information from neighboring features is harmful. This is because our approach relies on the diffusion model failing to estimate the score for OoD features; if the model can exploit the strongly correlated neighboring features it can accomplish this task regardless of whether the input is anomalous or not (as validated in Fig. 4).\nAs a remedy, we propose to use a MLP-based denoiser network that has an input dimensionality of C and operates on each local feature individually. Our diffusion model hence models the distribution q(x\u2080) of individual features x\u2080 \u2208 \u211d\u1d9c. This is illustrated in Figure 2. The architecture of our MLP denoiser network has the same structure of residual and skip connections as the U-Net architecture in [35]. However, it replaces the convolutional layers with linear ones, maintains a constant hidden dimensionality, uses no attention layers, and uses a simplified time embedding. Further details are provided in the Appendix.\nTraining. Training of the denoiser is done with a DDPM formulation, i.e. with the same noise schedule, model parametrization, and loss. We first extract feature maps F from all images in the training dataset and disassemble them into individual features x\u2080. Those features represent samples of the inlier feature distribution q(x\u2080). We compute statistics across all features and use it for normalization to a range from -1 to 1 before feeding data to the diffusion model. We use the same statistics for normalization at inference time.\nDuring the training, the features are perturbed with noise \u03f5 ~ \ud835\udca9(0, I) to x\u209c(x\u2080, \u03f5) as stated in Eq. 4, and the denoiser is trained to estimate the perturbation noise \u03f5\u2080(x\u209c, t) using the MSE loss from Eq. 5. As described in the Background section, this can be interpreted as the denoiser learning to estimate the gradient of the smoothed data distribution \u03c3\u209c\u2207\u2093\u209c log p(x\u209c) and the score s\u2091(x\u209c) can be computed from the model output via s\u2091(x\u209c) = \u2212\u03f5\u2080(x\u209c,\u209c)/\u03c3\u209c."}, {"title": "5.3 OoD Score Computation", "content": "OoD Score Computation for a Single Diffusion Timestep. At inference time, given an input image, the goal is to estimate pixel-wise OoD scores. First, we extract the feature map F from the image and normalize it as described above. We disassemble the feature map to individual features x\u2080 and compute an OoD score for each feature individually.\nFor computing the OoD score of a single feature x\u2080, we perturb the feature by applying the forward diffusion process up to a timestep t: x\u209c(x\u2080, \u03f5) = \u221a\u03b1\u0305\u209cx\u2080 + \u221a1 \u2013 \u03b1\u0305\u209c\u03f5 with \u03f5 ~ \ud835\udca9(0, I) (same as Eq. 3). We feed this perturbed feature x\u209c(x\u2080, \u03f5) to our denoiser to estimate the score s\u2091(x\u209c) = \u2212\u03f5\u2080(x\u209c,t)/\u03c3\u209c with \u03c3\u209c = \u221a1 \u2013 \u03b1\u0305\u209c. Intuitively, this score points towards higher probability in the smoothed data distribution of the respective timestep. Based on this, we compute the OoD score \ud835\udc52L,t as directional error (i. e. cosine similarity) between the estimated score s\u2091(x\u209c) and the perturbation \u03f5:\n$e_{L,t} = \\frac{s_\\Theta(x_t) \\cdot \\epsilon}{||s_\\Theta(x_t)|| ||\\epsilon||} = -\\frac{\\epsilon_\\Theta(x_t, t) \\cdot \\epsilon}{||\\epsilon_\\Theta(x_t, t)|| ||\\epsilon||} = -\\frac{\\frac{\\epsilon_\\Theta(x_t, t)}{\\sigma_t} \\cdot \\epsilon}{||\\frac{\\epsilon_\\Theta(x_t, t)}{\\sigma_t}|| ||\\epsilon||}$.\nFinally, we reassemble the OoD scores \ud835\udc52L,t of each individual feature to an OoD score map EL,t. In practice, the OoD scores for all features are computed in parallel as a single batch.\nOoD Score Aggregation Across Timesteps. We compute OoD score maps for multiple diffusion timesteps, i. e. we compute EL,t for each t \u2208 T = {t\u2081, ..., t\u0274}. As the perturbations \u221a1 \u2013 \u03b1\u0305\u209c\u03f5 in each timesteps have different expected magnitudes \u221a1 \u2013 \u03b1\u0305\u209c, we weight directional errors in different timesteps accordingly. The final OoD score map EL is therefore computed as a weighted sum as follows:\n$E_L = \\sum_{t \\in T} \\sqrt{1-\\overline{\\alpha}}_t E_{L,t}.$\nIn practice, again, the OoD score maps EL,t of the different timesteps are computed in parallel as a single batch.\nRegarding reconstruction-based approaches, the advantage of this OoD score computation is that all steps can be parallelized, which enables much faster runtimes. The difference to previous score-based methods is that here the OoD scores are computed from directional errors and aggregated across multiple timesteps. In Sec. 6.4 we show experimentally that this improves performance consistently.\nCompound OoD Scores. Following [13], we average the OoD scores Et obtained from the diffusion model with the uncertainty scores of the segmentation model (in the form of the LogSumExp of the predicted logits). The average is computed after standardization according to means and standard deviations obtained on the training set."}, {"title": "6 Experiments", "content": "In this section, we provide experimental results for the presented method and for the proposed benchmark. In Sec. 6.1, we compare our method to the state-of-the-art in OoD detection for semantic segmentation on road scene benchmarks, which are the current standard in the field. In Sec. 6.2, we provide evaluations on the proposed ADE-OoD benchmark. Following that, we validate the principal design choices of our approach, namely the use of an MLP architecture for the denoiser network (Sec. 6.3), and the proposed OoD score computation (Sec. 6.4).\nExperimental Setup. To train the segmentation models for feature extraction, we follow [13] or use off-the-shelf pre-trained parameters whenever available. For the MLP architecture of our diffusion model, we use a hidden dimension equal to the input feature channel size. We use a learning rate of 5e-5 and a batch size of 4096. We train for 70k iterations, which takes ca. 5 hours on an RTX 3090 GPU. We aggregate OoD scores across the last 25 timesteps, i.e. T = {1, ..., 25}. Please note that we use the same experimental settings across all benchmarks.\nWe report the standard evaluation metrics Average Precision (AP) and FPR at 95%TPR (FPR). As the results vary strongly across datasets, we additionally report the average AP and FPR rank across benchmarks for each method."}, {"title": "6.1 Comparing DOoD to the State-of-the-Art on Road Scenes", "content": "In this section we evaluate DOoD on the common driving-oriented benchmarks RoadAnomaly [28], Fishyscapes [4, 36] validation, and SegmentMeIfYouCan [6] (SMIYC). In all cases, the Cityscapes dataset is used for training and the Cityscapes categories are considered as in-distribution.\nCompared Methods. We compare with a diverse set of state-of-the-art methods. Notably, many are similarly based on Mask2Former architecture (Rba, M2A, EAM, Maskomaly). Our evaluation includes approaches that use external datasets as proxy outliers for negative training (Outlier Exposure/OE) and approaches which are designed specifically for driving (DS) scenarios. These methods build on data or assumptions that make them less general (as detailed in Sec. 6.2) and therefore not directly comparable. For example the DS method M2A suppresses OoD scores for all \"stuff\" classes except \"road\", and JSRNet explicitly models the road surface. We therefore included them in the evaluation for completeness, but excluded them from the ranking.\nThe compared methods build on diverse segmentation models. Following [13], we apply our approach on the representation spaces of two different segmentation model architectures: Segmenter-B and SETR-L. These are popular high-performance models with pre-trained Cityscapes weights available off-the-shelf. Since SMIYC is submission-based, we report results for one model per dataset."}, {"title": "6.2 ADE-OoD Evaluation", "content": "In this section we provide evaluations on the proposed ADE-OoD benchmark.\nEvaluated Methods. We report results for a diverse set of methods, including the hybrid generative-discriminative GMMSeg, the Mask2Former based RbA and M2A, and cDNP. For M2A category partitioning we follow ADE20k's stuff/things split. We use off-the-shelf ADE20k weights for all methods, matching the architectures used in the respective paper for driving data. We use SETR-ViT-L for our approach, as it is compatible to our previous experiments and available for download. Note that it is difficult to evaluate many methods that use outlier exposure or are specific to driving scenes, as they require additional outlier data or their assumptions do not apply."}, {"title": "6.3 Analysis of the Diffusion Model Architecture", "content": "As described in Sec. 5.2, we use an MLP architecture for the diffusion model.\nThe OoD scores from the U-Net architecture can be correct on boundaries of OoD objects, but too low within the object. We attribute this to the receptive field of the U-Net, which allows it to extrapolate information from neighbor features and as such successfully denoise OoD inputs, which is not desirable. The MLP architecture operates on each feature individually and does not suffer from this problem. Indeed, the proposed MLP architecture outperforms the common U-Net architecture qualitatively and quantitatively by a large margin."}, {"title": "6.4 Analysis of the OoD Score Computation", "content": "As described in Sec. 2.2, we propose to compute OoD scores from the directional error between a perturbation and the estimated score. Moreover, we propose to aggregate OoD scores across multiple timesteps. A natural question that arises is which timesteps should be used for the aggregation. Related works either face similar questions [16, 17, 54], or work with a fixed noise magnitude [40].\nIn the following, we analyze these two aspects, which shows that the proposed OOD score EL outperforms others and is more robust to the choice of timesteps. Setup. We compare three different OoD scores. For all scores, the input feature x\u2080 is first perturbed with noise \u03f5 to x\u209c via the forward diffusion process up to a timestep t. The OoD scores are computed from the model output \u03f5\u2080(x\u209c,\u209c) as:\nMSErecon: applies reverse diffusion process to obtain a reconstructed x\u0302\u2080, uses MSE(x\u2080, x\u0302\u2080) as OoD score [51]. Requires t sequential forward passes.\nMSEscore: uses MSE(\u03f5, \u03f5\u2080(x\u209c, t)) as OoD score [40]. Requires 1 forward pass.\nE_{L,t}: uses $\\frac{-\\epsilon\\theta(x_t,t) \\cdot \\epsilon}{||\\epsilon\\theta (x_t,t)|| ||\\epsilon||}$ as OoD score (Ours). Requires 1 forward pass.\nTo analyze the effects of different diffusion timesteps, we report quantitative results APt=1 for using only the first timestep and APbest for the best timestep."}, {"title": "6.5 Computational Costs", "content": "We provide a comparison of computational costs on the RoadAnomaly benchmark for cDNP, RbA, and our approach (all on a RTX 3090 GPU):\nOne can observe that RbA, whose runtime basically consists of a Mask2Former forward pass, is the cheapest. Our approach, while having a higher inference time than cDNP, it has a smaller GPU memory footprint. An advantage of our approach is that it offers the possibility to trade of runtime and performance via changing the number of diffusion timesteps that are aggregated in the OoD score computation. For example, aggregating five timesteps T = {1, ...,5} enables a comparable runtime as previous methods but with better performance."}, {"title": "7 Conclusion", "content": "In this work we considered the problem of out-of-distribution detection in semantic segmentation data. We introduced, to the best of our knowledge, the first diffusion model based approach for this task. We explored design possibilities in terms of model architecture and OoD scoring functions, the best of which resulted in an approach which is competitive on the major driving-oriented OoD detection benchmarks, and achieves state of the art results on RoadAnomaly and SMIYC-Anomaly without outlier exposure and domain-specific priors.\nFurthermore, we proposed a novel benchmark based on the ADE20k dataset, with the goal of assessing the quality of OoD detection approaches in settings with higher diversity, i.e. beyond 19 in-distribution categories and driving scenarios. We provide evaluations of state-of-the-art approaches on the benchmark and show that our proposed method copes well with the increased diversity."}, {"title": "A Additional Information on the ADE-OoD Benchmark", "content": "In this section, we provide additional information on the ADE-OoD benchmark. We provide details on the construction process and show samples from the benchmark alongside predictions from segmentation models. We analyze the predictions regarding the challenges of the benchmark."}, {"title": "A.1 Construction Details", "content": "Image Selection. As described in Sec. 5 of the main paper, the images for the benchmark were selected from the ADE20k [56] validation set and from OpenImages [26] according to the following criteria:\n1. Presence of clear and unambiguous in- and out-of-distribution entities only.\n2. Presence of diverse in- and out-of-distribution categories, as well as varying indoor and outdoor settings.\n3. Adequate scene complexity to match the original ADE20k training data.\n4. Adequate number of samples for statistically significant results.\nMore specifically, we selected images by (1.) automatically selecting ADE20k images with large unlabeled regions, or manually selecting images from Open-Images (via the OpenImages web visualizer) that contain semantic classes that are not labeled in ADE20k, (2.) manually discarding images that do not fit to the described criteria (e.g. images that contain ambiguous entities), (3.) validating the selected images by applying SOTA ADE20k segmentation models and manually inspecting their outputs.\nIn total we selected 84 images from OpenImages, and 27 from ADE20k. The complete benchmark data is provided on https://ade-ood.github.io and the source of each image is indicated by its filename.\nAnnotation Process. For the 27 images selected from ADE20k, we used the available ground truth segmentation annotations, using the \"unlabelled\" class mask as OoD (after checking for problematic cases as described above).\nThe annotation process for the 84 images selected from OpenImages was carried out in a semi-automatic fashion using the Segment Anything Model (SAM) [25], a promptable object segmentation model trained on very large data collections, which we prompted using hand-selected points. For optimal mask coherence and quality, objects and object parts were segmented individually before merging all resulting masks. After obtaining binary masks for each image, we refined them using dilation and erosion operations. This allowed to remove small noise elements (points and holes) in the masks.\nBenchmark Size and Statistical Significance. To ensure that results on the benchmark are meaningful and reliable and that the number of samples in the benchmark is adequate, we tested its statistical robustness and compared it with that of other benchmarks. To this end, we computed results on 10 random subsets of the test data, each containing 90% of the original samples."}, {"title": "A.2 Examples and Analysis of Segmentation Model Predictions", "content": "Examples. A collection of samples (images and ground truth OoD masks) from the ADE-OOD benchmark is shown.\nAnalysis of Segmentation Model Predictions. Additionally, indicates the most prominent predicted classes for the OoD regions obtained from SETR and Mask2Former models.\nThe predictions give us useful information about how the trained networks handle unknown objects, and therefore about the challenges of the current models on the proposed benchmark. We can make the following observations:\nSeveral times the OoD objects are classified as a spatially neighboring and frequent class, though this is not visually related to the object (e.g. wall, person, building). This is a form of extrapolation from the context that both models do, often with the same categories.\nThe remaining predicted classes are visually similar to the OoD objects (e.g. tent for the parachute in the second row, minibike for the segways in the fourth row), or would likely feature in the surrounding environment (towel in the restroom in the first row, fence in the outdoor scene in the first row). These \"good guess\" predictions differ between the two models, confirming that the chosen objects are interesting for OoD detection."}, {"title": "B Diffusion Model Architecture", "content": "The architecture of the denoiser in our diffusion model, illustrated in Fig. 7, is based on the commonly used U-Net architecture, but replaces the convolutional layers with linear layers. The network is composed of consecutive sets of residual blocks, structured into input blocks and output blocks. Each block receives features and timestep embedding as input. Skip connections forward input block features directly to the corresponding output block, like in a convolutional U-Net. In our experiments we use 6 input blocks and 6 output blocks.\nResidual Block Architecture. As shown in Fig. 8, each residual block first processes incoming features with a sequence of a normalization layer (Group-Norm [50]), a non-linearity (SiLU [11]), and a linear layer. Then the timestep embedding is added and the result is processed by another sequence of a normalization layer, a non-linearity and a linear layer. The original input is summed to the output to form a residual connection. We use a constant hidden dimension throughout all linear layers.\nLightweight Timestep Embedding. As common in diffusion models, the current diffusion timestep is supplied to the network in form of a timestep embedding. In order to reduce the computational costs, we compute the timestep embedding as follows:\nInstead of using transformer-style periodic sinusoidal time embeddings, as done in DDPM and ViT works [10, 22], we directly use the scalar time embedding t.\nIn each residual block, instead of further processing the timestep embedding as done for example in DDPM, we directly add the scalar time embedding to the incoming features (as shown in Fig. 8).\nWith these timestep embedding simplifications, we observe on average a 17% inference time speedup and no decrease in OoD detection performance."}, {"title": "C Qualitative Examples and Analysis", "content": "In Fig. 9 we show qualitative examples showcasing the differences between the predictions of our approach DOoD, Mask2Anomaly (M2A) [37], and RbA [33]. The examples highlight the respective advantages and disadvantages of the approaches, all of which are high-performing, despite being based on very different paradigms.\nThe predictions from M2A and RbA are both based on Mask2Former, which decouples the prediction of masks and categories, as opposed to the typical per-pixel classification paradigm. For this reason, the scores are very uniform over the pixel sets corresponding to the same mask, and have smooth edges. In contrast to this, the predictions from our method are strictly local (as they are computed on each patch individually) and have no enforced spatial correlation.\nEven though the predictions of M2A and RbA are more \u201ctidy\u201d, the approaches have limitations. In the examples shown, it can be seen that where all approaches produce false positives, the two Mask2Former-based approaches have - mistakenly - high OoD scores. This can happen also for large sets of pixels, if they are"}, {"title": "D Ablation: Compounding OoD Scores from the Diffusion Model and the Segmentation Model", "content": "As described in the main paper (Sec. 4.3), we compound OoD scores obtained from the diffusion model with the segmentation model uncertainty (using the LogSumExp scoring function). In this section we compare the results from both individually, and the compound ones."}, {"title": "E Results on Dynamic Environmental Conditions", "content": "We tested our approach DOoD on WD-Pascal [2], a benchmark containing images of road scenes in adverse environmental conditions. Quantitative results, along with an example image from the benchmark and our method's prediction for it, are shown below:"}, {"title": "F Results on Remote Sensing Data", "content": "The focus of our work is out-of-distribution detection for semantic segmentation on general natural images. For completeness, in the following, we demonstrate the applicability of our method DOoD to even more general data domains via the example of remote sensing data. We constructed a benchmark by training on WHDLD [39] and testing on DLRSD [39], using classes only found in the latter (airplane, car, court, sand, ship, tank) as OoD. A qualitative example and early results on this benchmark are shown below:"}]}