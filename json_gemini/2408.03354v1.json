{"title": "The Use of Large Language Models (LLM) for Cyber Threat Intelligence (CTI) in Cybercrime Forums", "authors": ["Vanessa Clairoux-Tr\u00e9panier", "Isa-May Beauchamp", "Estelle Ruellan", "Masarah Paquet-Clouston", "Serge-Olivier Paquette", "Eric Clay"], "abstract": "Large language models (LLMs) can be used to analyze cyber threat intelligence (CTI) data from cybercrime forums, which contain extensive information and key discussions about emerging cyber threats. However, to date, the level of accuracy and efficiency of LLMs for such critical tasks has yet to be thoroughly evaluated. Hence, this study assesses the accuracy of an LLM system built on the OpenAI GPT-3.5-turbo model [7] to extract CTI information. To do so, a random sample of 500 daily conversations from three cybercrime forums-XSS, Exploit.in, and RAMP was extracted, and the LLM system was instructed to summarize the conversations and code 10 key CTI variables, such as whether a large organization and/or a critical infrastructure is being targeted. Then, two coders reviewed each conversation and evaluated whether the information extracted by the LLM was accurate. The LLM system performed strikingly well, with an average accuracy score of 98%. Various ways to enhance the model were uncovered, such as the need to help the LLM distinguish between stories and past events, as well as being careful with verb tenses in prompts. Nevertheless, the results of this study highlight the efficiency and relevance of using LLMs for cyber threat intelligence.", "sections": [{"title": "1 Introduction", "content": "With the rise of large language models (LLMs), which are text-generating technologies trained on vast amounts of data [3, 4, 12], the scope and aim of artificial intelligence (AI) have changed. Now, AI can be used for a myriad of applications, such as writing stories on the fly or summarizing complex scientific content [11]. One key application where AI can be useful is cyber threat intelligence (CTI) [3, 4, 12]. Indeed, there exists a vast array of cybercrime conversations in forums on both the clear and dark web [1, 5]. These conversations often leak key information that can be used by companies and governments to detect and sometimes even prevent cyber attacks [1, 5]. The question is whether and how LLMs can accurately be used for cyber threat intelligence on such forums. Indeed, can we trust such technology for CTI? Can it replace first-level threat analysts, that is, analysts who read and extract relevant information from cybercrime forums?\nThis study assesses the extent to which an LLM system is accurate when extracting and summa- rizing information from cybercrime forums. Using a random sample of 500 daily conversations from three cybercrime forums-XSS, Exploit[.]in, and RAMP\u2014we instructed an LLM to summarize the con- versations and extract specific CTI information from them. CTI information included whether a sale was conducted, whether a large organization or a critical infrastructure was discussed, whether initial"}, {"title": "2 Methods and Data", "content": "The following sections describe the LLM system developed for this study, the CTI variables it coded, and the manual process used to verify the results."}, {"title": "2.1 LLM System for CTI", "content": "For this study, the LLM system, powered by the gpt-3.5-turbo-16k-0613 model [6, 7], is designed to extract and summarize relevant information from cybercrime forums. The system operates through a multi-step process that involves selecting high-quality sources, summarizing conversations, and coding key variables. This section outlines the detailed methodology employed in generating unit summaries, including pseudo-code and an example of a modular prompt."}, {"title": "2.1.1 Data Collection and Preprocessing", "content": "The first step involves selecting and collecting data from high-quality cybercrime forums. These forums are continuously monitored, and new messages are extracted daily. The system processes both new and existing discussion threads, ensuring comprehensive coverage of relevant conversations."}, {"title": "2.1.2 Contextual Information Extraction", "content": "For each extracted message, the system determines the context based on whether the discussion thread is new or has been previously processed. If the thread is new, the title serves as the context. For existing threads, a summary of the prior conversation is used to provide context."}, {"title": "2.1.3 Prompt Design and Variable Extraction", "content": "The core of the system's functionality lies in the use of carefully designed prompts that guide the LLM to extract specific information. Prompts are formulated to capture the intent and requirements for data extraction, such as identifying transactions, potential targets, or vulnerabilities mentioned in the conversation. The prompts are designed from the perspective of a cyber threat intelligence analyst, focusing on the key elements that are critical for threat analysis and reporting. The system's output is structured as unit summaries, which include a text summary of the conversation and the extracted variables in a standardized format. This format facilitates easy analysis and integration into further processing pipelines. We explicitly instruct the LLM to output the information in the specified format."}, {"title": "2.1.4 Example Modular Prompt with Analyst Persona", "content": "\"As a cyber threat intelligence analyst, your task is to review the\nconversation and identify key indicators. Please extract the following\ninformation: 1. An actor is selling something? 2. The conversation involves\nthe sale of initial access to a corporate or organization network? 3.\nTargeted or abused products or technology names?\". Output your answer in\nthe following format, as an example."}, {"title": "2.2 Key CTI Variables and Prompts", "content": "From daily cybercrime conversations, ten variables were extracted using specific prompts with the LLM. These variables were chosen because they provide key information for CTI, such as which technology or industry is targeted. Having readily accessible and easily identifiable information for CTI ensures that analysts can narrow down their focus to conversations relevant for their organization. Anyone wishing to reproduce the analysis could develop their own key variables."}, {"title": "2.3 Coding Process", "content": "To assess the accuracy of the LLM system, a random sample of 500 daily conversations from three cybercrime forums-XSS, Exploit.in, and RAMPs were extracted using the Flare interface. Flare is an information technology (IT) security company that maintains a cyber threat intelligence platform by monitoring various online spaces\u00b9.\nThen, two analysts went over each of the daily conversations from cybercrime forums and assessed the accuracy of the summary coded by the LLM system. They also individually coded the ten variables according to their interpretation, then compared them to the coding performed by the LLM. Specifi- cally, they used a binary coding scheme: 1 indicated agreement with the LLM coding and 0 indicated disagreement.\nOnce the individual coding was completed, the two analysts performed an inter-coder agreement to obtain a common decision for each unit summary. Such inter-coder agreement allowed them to pinpoint discrepancies, but also find areas of improvement to fine-tune the LLM system. In the end, the inter- coder agreement was high, with an average of 98.2%, a minimum of 96.2% and a maximum of 99.8%. A merged database was created resulting from the agreement. This merged database was used to determine the accuracy of the LLM system in coding each of the ten CTI variables from the 500 daily conversations on cybercrime forums. The results are presented below."}, {"title": "3 Assessing the accuracy of the LLM System for CTI", "content": "The LLM system performed strikingly well in coding variables that represented valuable CTI information from cybercrime forums. Indeed, on average, the LLM was accurate in 97.96% of cases, with a minimum accuracy of 95% for the variable is_targeting_critical_infrastructure and a maximum of 100% for the variable industries. The percentage accuracy for each variable is presented below.\nFor the summary variable, the coders ensured that the summary provided by the LLM system was accurate, and it was the case for 98.8% (N=500) of the conversation flows analyzed. However, it must be acknowledged that sometimes, the coders did notice that some key CTI information was missing from the summary. This is because \"summarizing\" a conversation means cutting text elements, which sometimes may be interpreted as important by some and not by others. The variable is_sale was also well coded by the LLM system, with an accuracy of 97.2%. Such variables captured when a user was conducting or announcing a sale. Most cases in which the LLM was wrong related to individuals expressing interest in sales or discussing sales, while not conducting or announcing any. The LLM system also performed really well when coding the variable is_initial_access, flagging messages involving the sale of initial"}, {"title": "4 Coders' Insights to Fine-Tune the LLM System", "content": "Given the results of the analysis, there is no doubt that LLM systems can be useful in extracting key \u0421\u0422\u0406 information from cybercrime forums. Indeed, the summaries generated from daily conversations focused on relevant information, even when a large number of messages were posted. Such relevant information was sometimes even missed by the analysts. The LLM also demonstrated an exceptional ability to code key CTI variables, as evidenced by the results obtained: it had an accuracy of 98% on average. Such high results were unexpected and showcase interesting future research avenues.\nReviewing the same conversations allowed the coders to discuss and identify areas for improvement in coding the CTI variables. These areas are useful for any researcher or analyst wishing to use LLMs to code variables based on text input. They are presented below."}, {"title": "4.1 Difficulties in Detecting Stories and Past Events", "content": "Across variables, the small number of observations that were miscoded often involved stories or past events mentioned by a user. Indeed, the LLM sometimes encountered difficulties when processing past events reported by users. For example, the LLM system coded the variable is_sale as TRUE for a conversation flow that was summarized as follows:\n\"A police officer was caught selling fake certificates through a darknet market. The price per certificate was $9,000, and during a bulk sale of $80,000 worth of certificates, the officer attempted to deliver them personally, leading to his arrest.\" (382)\nHowever, in this message, the user was reporting a past event. Consequently, the LLM incorrectly categorized this example as a case of a user selling certificates rather than understanding it as a narrative of a past event involving a third party. Similarly, the LLM coded the variable is_sale as TRUE for a conversation that was summarized as follows:\n\"[An actor] reported that Russian national Evgeny Doroshenko has been charged in the USA for working as an initial access broker. Doroshenko is suspected of hacking at least one company in New Jersey and has been providing similar services since 2019. He set the starting price for access to the compromised company at $3,000, with an auction increment of $500 or an instant sale price of $6,000. His preferred attack method is brute-forcing Remote Desktop Protocol services. Doroshenko's personal information, including phone numbers and home address in Astrakhan, was found linked to his Telegram account. Following the news of the charges, he attempted to contact moderators on the Exploit forum.\" (275)\nAgain, in this conversation, a past event was reported, and therefore, the LLM should not have coded it as an active sale. When the LLM incorrectly coded such messages as TRUE, it over-represents the number of sales in the dataset. Hence, it is important to enhance the LLM's ability to distinguish historical narratives from current facts to ensure a more accurate and contextually appropriate analysis of the information provided by users."}, {"title": "4.2 Verb Tenses in Prompts", "content": "The coders also noticed that, given that the prompts were written in the present tense and the unit summaries were written in the past tense, the LLM sometimes had difficulties in coding accurately. For example, the variable is_sale was not coded as TRUE for the conversation flow related to this summary:\n\"Two actors participated in a conversation. [the first actor] posted a message that was likely truncated and is not informative on its own. Actor [2] indicated that a previous discussion or sale has been closed and the item in question has been sold. No further details are provided.\"\n(60)\nMost likely, the verb tense in the prompt, \"an actor is selling something,\" was the reason why the LLM did not code this conversation as involving a sale. However, upon reflection, it could be considered a sale because the item in question has been sold. In the end, if the goal is to code all sales, past or present, it might be necessary to revisit the verb tenses in the prompts given to the LLM."}, {"title": "4.3 The Importance of Data Chunking", "content": "During the coding process, the coders went through the summary variables and then the whole conver- sation on the cybercrime forum to validate that no information was missing and subsequently assess the accuracy of the CTI variables. When reading the complete conversation, the coders noticed that the way the data was chunked influenced the results. For example, given this summary:\n\"No actionable intelligence was extracted from the new message by [an actor] dated [xx- xx-xx] as it contained no specific information related to the concepts of interest for threat intelligence.\" (165)\nAlmost all variables were coded as FALSE. However, when reading the complete conversation, one could quickly see that the discussion revolved around selling a database containing various Telegram chats and channels. The main user made various updates on the database's availability and scope, including"}, {"title": "4.4 About Coding General or Vague Concepts", "content": "When coding CTI information, some variables relate to specific well-defined concepts, such as the variable industries. Indeed, the prompt for industries was well defined and included clear descriptions of industries, such as finance, technology, or critical infrastructure. On the other hand, other variables re- ferred to general or vague concepts. For example, the coders noticed that some organizations that could be considered \"large\", and therefore coded as TRUE for the variable is_targeting_large_organization, were not coded as such by the LLM system. Since no definition was provided for what constitutes a \"large organization,\" this variable was subject to the LLM's interpretation. This means that only organizations recognized as such by the LLM were identified, or as the coders noticed only without-a-doubt large organizations were flagged (e.g., Apple, Netflix, Microsoft). Large organizations that were less known or required additional searches were not identified.\nThis is not ideal, but also not completely problematic. By leaving the concept to be interpreted by the LLM, at least well-known organizations were identified. Hence, there is a certain interest in leaving some concepts subject to the LLM's interpretation, especially given the high accuracy rate for all variables. Still, further research on the topic is needed."}, {"title": "4.5 Links between Variables with Similar Concepts", "content": "During the analyses, the coders noticed some disparities between interrelated variables such as industries, is_targeting_critical_infrastructure, and is_targeting_large_organization. For ex- ample, consider this summary:\n\"A user announced the sale of an SQL Injection vulnerability in the Peru Military Document Management System. This vulnerability allows access to documents, users, passwords, and more, with the server holding over 50 databases under the domain 'mil.pe.\" (259)\nIn this example, the LLM coded FALSE for is_targeting_critical_infrastructure and TRUE for is_targeting_large_organization. In the industries variable, the LLM indicated ['Critical Infrastructure', 'Military']. This discrepancy raises questions about the definitions of these three variables, as discussed above, and how the LLM understood them. It also showcases that some variables may be used to code others. For example, a technology mentioned in the summary was not always associated with an organization, despite the obvious connection between the two. In this excerpt:\n\"A user expressed interest in purchasing generated iCloud accounts, offering a rate of $0.5 per account\" (116)\nThe LLM indicated \"iCloud\" in the targeted_technologies variable but did not indicate \"Ap- ple\" in the targeted_organization variable. Linking the two variables could enhance the information coded."}, {"title": "4.6 The Title as Key", "content": "As a reminder, the LLM system codes variables based on the conversation flows and is given some context. Such context is either the title or, if part of the conversation was already coded by the LLM, the summary of the previous conversation. The coders noticed that often, the title provided key information that, if given to the LLM, would have avoided some coding errors. For example, here is an LLM summary:\n\"An actor expressed interest in purchasing data, with a priority on information from Israel and less emphasis on data from Iran.\" (312)"}, {"title": "4.7 The LLM System is Imperfect, just like Humans", "content": "One last element that needs to be considered is that there were instances where the coders simply did not understand the logic behind the LLM's coding. For example, given this summary:\n\"A user advertised a product called 'Red Node HVNC', which is a hidden VNC (Virtual Network Computing) tool that allows remote control over a target machine via a web browser. [The actor] claims the tool is fully undetected (FUD) by antiviruses, including Windows Defender, and does not require a crypter for distribution. A demonstration video and a feature comparison chart were provided. Another user criticized the product, suggesting it is overpriced and questioning the lack of moderation for such products. [The principal actor] defended their product, stating it is privately coded, FUD, fast, and the first HVNC to work over a web browser, unlike others cloned from 'tinynuke.\u201d (160)\nThe LLM coded the variable is_initial_access as FALSE even though the actor sells a tool that allows remote control over a target machine via a web browser. Hence, in some cases, the LLM missed the information. The LLM system was therefore not flawless, just like humans. Although rare, it made coding errors that could not be explained."}, {"title": "5 Conclusion", "content": "Cyber threat intelligence has traditionally been plagued by a lack of actionability and challenges around making effective use of analyst time. Our research indicates that there is significant potential for lever- aging LLMs to conduct an initial review of original source data to categorize events and prioritize those that may be most relevant to CTI teams. This presents a substantial opportunity to use analyst time more effectively by narrowing searches and automatically alerting on relevant findings.\nWhen language models go wrong, they tend to do so in predictable ways. Providing the LLM with the context needed to categorize the event was important, and it did sometimes miscategorize events, particularly when the prompt left room for interpretation. Another potential error was introduced when conversations spanned multiple days, so the summarization may have missed crucial context, thus resulting in an incorrect label. Our analysis also showed that the model struggled with differentiating between stories and original events occurring.\nDespite these issues, OpenAI's GPT-3.5-turbo model [7] was able to produce exceptional results across our study of 500 events. This indicates language models are currently a scalable and cost-effective solution for identifying relevant CTI data and could have exceptional potential when paired with state- of-the-art (SOTA) models for reporting data and fine-tuning results. Leveraging SOTA models, such as Claude 3.5 [2] Sonnet or GPT-40 [10, 9, 8], has the potential to further improve results or allow for categorization of even more complex data. This represents a significant opportunity for further study."}]}