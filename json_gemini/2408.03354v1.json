{"title": "The Use of Large Language Models (LLM) for Cyber Threat\nIntelligence (CTI) in Cybercrime Forums", "authors": ["Vanessa Clairoux-Tr\u00e9panier", "Isa-May Beauchamp", "Estelle Ruellan", "Masarah Paquet-Clouston", "Serge-Olivier Paquette", "Eric Clay"], "abstract": "Large language models (LLMs) can be used to analyze cyber threat intelligence (CTI)\ndata from cybercrime forums, which contain extensive information and key discussions\nabout emerging cyber threats. However, to date, the level of accuracy and efficiency\nof LLMs for such critical tasks has yet to be thoroughly evaluated. Hence, this study\nassesses the accuracy of an LLM system built on the OpenAI GPT-3.5-turbo model [7]\nto extract CTI information. To do so, a random sample of 500 daily conversations from\nthree cybercrime forums-XSS, Exploit.in, and RAMP was extracted, and the LLM\nsystem was instructed to summarize the conversations and code 10 key CTI variables,\nsuch as whether a large organization and/or a critical infrastructure is being targeted.\nThen, two coders reviewed each conversation and evaluated whether the information\nextracted by the LLM was accurate. The LLM system performed strikingly well, with\nan average accuracy score of 98%. Various ways to enhance the model were uncovered,\nsuch as the need to help the LLM distinguish between stories and past events, as well\nas being careful with verb tenses in prompts. Nevertheless, the results of this study\nhighlight the efficiency and relevance of using LLMs for cyber threat intelligence.", "sections": [{"title": "Introduction", "content": "With the rise of large language models (LLMs), which are text-generating technologies trained on vast\namounts of data [3, 4, 12], the scope and aim of artificial intelligence (AI) have changed. Now, AI can\nbe used for a myriad of applications, such as writing stories on the fly or summarizing complex scientific\ncontent [11]. One key application where AI can be useful is cyber threat intelligence (CTI) [3, 4, 12].\nIndeed, there exists a vast array of cybercrime conversations in forums on both the clear and dark web [1,\n5]. These conversations often leak key information that can be used by companies and governments to\ndetect and sometimes even prevent cyber attacks [1, 5]. The question is whether and how LLMs can\naccurately be used for cyber threat intelligence on such forums. Indeed, can we trust such technology for\nCTI? Can it replace first-level threat analysts, that is, analysts who read and extract relevant information\nfrom cybercrime forums?\nThis study assesses the extent to which an LLM system is accurate when extracting and summa-\nrizing information from cybercrime forums. Using a random sample of 500 daily conversations from\nthree cybercrime forums-XSS, Exploit[.]in, and RAMP\u2014we instructed an LLM to summarize the con-\nversations and extract specific CTI information from them. CTI information included whether a sale\nwas conducted, whether a large organization or a critical infrastructure was discussed, whether initial"}, {"title": "Methods and Data", "content": "The following sections describe the LLM system developed for this study, the CTI variables it coded,\nand the manual process used to verify the results."}, {"title": "LLM System for CTI", "content": "For this study, the LLM system, powered by the gpt-3.5-turbo-16k-0613 model [6, 7], is designed to\nextract and summarize relevant information from cybercrime forums. The system operates through a\nmulti-step process that involves selecting high-quality sources, summarizing conversations, and coding\nkey variables. This section outlines the detailed methodology employed in generating unit summaries,\nincluding pseudo-code and an example of a modular prompt."}, {"title": "Data Collection and Preprocessing", "content": "The first step involves selecting and collecting data from high-quality cybercrime forums. These forums\nare continuously monitored, and new messages are extracted daily. The system processes both new and\nexisting discussion threads, ensuring comprehensive coverage of relevant conversations."}, {"title": "Contextual Information Extraction", "content": "For each extracted message, the system determines the context based on whether the discussion thread\nis new or has been previously processed. If the thread is new, the title serves as the context. For existing\nthreads, a summary of the prior conversation is used to provide context."}, {"title": "Prompt Design and Variable Extraction", "content": "The core of the system's functionality lies in the use of carefully designed prompts that guide the LLM\nto extract specific information. Prompts are formulated to capture the intent and requirements for\ndata extraction, such as identifying transactions, potential targets, or vulnerabilities mentioned in the\nconversation. The prompts are designed from the perspective of a cyber threat intelligence analyst,\nfocusing on the key elements that are critical for threat analysis and reporting. The system's output\nis structured as unit summaries, which include a text summary of the conversation and the extracted\nvariables in a standardized format. This format facilitates easy analysis and integration into further\nprocessing pipelines. We explicitly instruct the LLM to output the information in the specified format."}, {"title": "Example Modular Prompt with Analyst Persona", "content": null}, {"title": "Key CTI Variables and Prompts", "content": "From daily cybercrime conversations, ten variables were extracted using specific prompts with the LLM.\nThese variables were chosen because they provide key information for CTI, such as which technology or\nindustry is targeted. Having readily accessible and easily identifiable information for CTI ensures that\nanalysts can narrow down their focus to conversations relevant for their organization. Anyone wishing\nto reproduce the analysis could develop their own key variables. "}, {"title": "Coding Process", "content": "To assess the accuracy of the LLM system, a random sample of 500 daily conversations from three\ncybercrime forums-XSS, Exploit.in, and RAMPs were extracted using the Flare interface. Flare is\nan information technology (IT) security company that maintains a cyber threat intelligence platform by\nmonitoring various online spaces\u00b9.\nThen, two analysts went over each of the daily conversations from cybercrime forums and assessed\nthe accuracy of the summary coded by the LLM system. They also individually coded the ten variables\naccording to their interpretation, then compared them to the coding performed by the LLM. Specifi-\ncally, they used a binary coding scheme: 1 indicated agreement with the LLM coding and 0 indicated\ndisagreement.\nOnce the individual coding was completed, the two analysts performed an inter-coder agreement to\nobtain a common decision for each unit summary. Such inter-coder agreement allowed them to pinpoint\ndiscrepancies, but also find areas of improvement to fine-tune the LLM system. In the end, the inter-\ncoder agreement was high, with an average of 98.2%, a minimum of 96.2% and a maximum of 99.8%. A\nmerged database was created resulting from the agreement. This merged database was used to determine\nthe accuracy of the LLM system in coding each of the ten CTI variables from the 500 daily conversations\non cybercrime forums. The results are presented below."}, {"title": "Assessing the accuracy of the LLM System for CTI", "content": "The LLM system performed strikingly well in coding variables that represented valuable CTI information\nfrom cybercrime forums. Indeed, on average, the LLM was accurate in 97.96% of cases, with a minimum\naccuracy of 95% for the variable is_targeting_critical_infrastructure and a maximum of\n100% for the variable industries. \nFor the summary variable, the coders ensured that the summary provided by the LLM system was\naccurate, and it was the case for 98.8% (N=500) of the conversation flows analyzed. However, it must be\nacknowledged that sometimes, the coders did notice that some key CTI information was missing from the\nsummary. This is because \"summarizing\" a conversation means cutting text elements, which sometimes\nmay be interpreted as important by some and not by others. The variable is_sale was also well coded\nby the LLM system, with an accuracy of 97.2%. Such variables captured when a user was conducting\nor announcing a sale. Most cases in which the LLM was wrong related to individuals expressing interest\nin sales or discussing sales, while not conducting or announcing any. The LLM system also performed\nreally well when coding the variable is_initial_access, flagging messages involving the sale of initial"}, {"title": "Coders' Insights to Fine-Tune the LLM System", "content": "Given the results of the analysis, there is no doubt that LLM systems can be useful in extracting key \u0421\u0422\u0406\ninformation from cybercrime forums. Indeed, the summaries generated from daily conversations focused\non relevant information, even when a large number of messages were posted. Such relevant information\nwas sometimes even missed by the analysts. The LLM also demonstrated an exceptional ability to code\nkey CTI variables, as evidenced by the results obtained: it had an accuracy of 98% on average. Such\nhigh results were unexpected and showcase interesting future research avenues.\nReviewing the same conversations allowed the coders to discuss and identify areas for improvement\nin coding the CTI variables. These areas are useful for any researcher or analyst wishing to use LLMs\nto code variables based on text input. They are presented below."}, {"title": "Difficulties in Detecting Stories and Past Events", "content": "Across variables, the small number of observations that were miscoded often involved stories or past\nevents mentioned by a user. Indeed, the LLM sometimes encountered difficulties when processing past\nevents reported by users. For example, the LLM system coded the variable is_sale as TRUE for a\nconversation flow that was summarized as follows:\n\"A police officer was caught selling fake certificates through a darknet market. The price\nper certificate was $9,000, and during a bulk sale of $80,000 worth of certificates, the officer\nattempted to deliver them personally, leading to his arrest.\" (382)\nHowever, in this message, the user was reporting a past event. Consequently, the LLM incorrectly\ncategorized this example as a case of a user selling certificates rather than understanding it as a narrative\nof a past event involving a third party. Similarly, the LLM coded the variable is_sale as TRUE for a\nconversation that was summarized as follows:\n\"[An actor] reported that Russian national Evgeny Doroshenko has been charged in the\nUSA for working as an initial access broker. Doroshenko is suspected of hacking at least\none company in New Jersey and has been providing similar services since 2019. He set the\nstarting price for access to the compromised company at $3,000, with an auction increment of\n$500 or an instant sale price of $6,000. His preferred attack method is brute-forcing Remote\nDesktop Protocol services. Doroshenko's personal information, including phone numbers and\nhome address in Astrakhan, was found linked to his Telegram account. Following the news\nof the charges, he attempted to contact moderators on the Exploit forum.\" (275)\nAgain, in this conversation, a past event was reported, and therefore, the LLM should not have\ncoded it as an active sale. When the LLM incorrectly coded such messages as TRUE, it over-represents\nthe number of sales in the dataset. Hence, it is important to enhance the LLM's ability to distinguish\nhistorical narratives from current facts to ensure a more accurate and contextually appropriate analysis\nof the information provided by users."}, {"title": "Verb Tenses in Prompts", "content": "The coders also noticed that, given that the prompts were written in the present tense and the unit\nsummaries were written in the past tense, the LLM sometimes had difficulties in coding accurately. For\nexample, the variable is_sale was not coded as TRUE for the conversation flow related to this summary:\n\"Two actors participated in a conversation. [the first actor] posted a message that was likely\ntruncated and is not informative on its own. Actor [2] indicated that a previous discussion or\nsale has been closed and the item in question has been sold. No further details are provided.\"\n(60)\nMost likely, the verb tense in the prompt, \"an actor is selling something,\" was the reason why the\nLLM did not code this conversation as involving a sale. However, upon reflection, it could be considered\na sale because the item in question has been sold. In the end, if the goal is to code all sales, past or\npresent, it might be necessary to revisit the verb tenses in the prompts given to the LLM."}, {"title": "The Importance of Data Chunking", "content": "During the coding process, the coders went through the summary variables and then the whole conver-\nsation on the cybercrime forum to validate that no information was missing and subsequently assess the\naccuracy of the CTI variables. When reading the complete conversation, the coders noticed that the way\nthe data was chunked influenced the results. For example, given this summary:\n\"No actionable intelligence was extracted from the new message by [an actor] dated [xx-\nxx-xx] as it contained no specific information related to the concepts of interest for threat\nintelligence.\" (165)\nAlmost all variables were coded as FALSE. However, when reading the complete conversation, one\ncould quickly see that the discussion revolved around selling a database containing various Telegram chats\nand channels. The main user made various updates on the database's availability and scope, including"}, {"title": "About Coding General or Vague Concepts", "content": "When coding CTI information, some variables relate to specific well-defined concepts, such as the variable\nindustries. Indeed, the prompt for industries was well defined and included clear descriptions of\nindustries, such as finance, technology, or critical infrastructure. On the other hand, other variables re-\nferred to general or vague concepts. For example, the coders noticed that some organizations that could be\nconsidered \"large\", and therefore coded as TRUE for the variable is_targeting_large_organization,\nwere not coded as such by the LLM system. Since no definition was provided for what constitutes a \"large\norganization,\" this variable was subject to the LLM's interpretation. This means that only organizations\nrecognized as such by the LLM were identified, or as the coders noticed only without-a-doubt large\norganizations were flagged (e.g., Apple, Netflix, Microsoft). Large organizations that were less known or\nrequired additional searches were not identified.\nThis is not ideal, but also not completely problematic. By leaving the concept to be interpreted by the\nLLM, at least well-known organizations were identified. Hence, there is a certain interest in leaving some\nconcepts subject to the LLM's interpretation, especially given the high accuracy rate for all variables.\nStill, further research on the topic is needed."}, {"title": "Links between Variables with Similar Concepts", "content": "During the analyses, the coders noticed some disparities between interrelated variables such as industries, \nis_targeting_critical_infrastructure, and is_targeting_large_organization. For ex-\nample, consider this summary:\n\"A user announced the sale of an SQL Injection vulnerability in the Peru Military Document\nManagement System. This vulnerability allows access to documents, users, passwords, and\nmore, with the server holding over 50 databases under the domain 'mil.pe.\" (259)\nIn this example, the LLM coded FALSE for is_targeting_critical_infrastructure and\nTRUE for is_targeting_large_organization. In the industries variable, the LLM indicated\n['Critical Infrastructure', 'Military']. This discrepancy raises questions about the definitions of these\nthree variables, as discussed above, and how the LLM understood them. It also showcases that some\nvariables may be used to code others. For example, a technology mentioned in the summary was not\nalways associated with an organization, despite the obvious connection between the two. In this excerpt:\n\"A user expressed interest in purchasing generated iCloud accounts, offering a rate of $0.5\nper account\" (116)\nThe LLM indicated \"iCloud\" in the targeted_technologies variable but did not indicate \"Ap-\nple\" in the targeted_organization variable. Linking the two variables could enhance the information\ncoded."}, {"title": "The Title as Key", "content": "As a reminder, the LLM system codes variables based on the conversation flows and is given some context.\nSuch context is either the title or, if part of the conversation was already coded by the LLM, the summary\nof the previous conversation. The coders noticed that often, the title provided key information that, if\ngiven to the LLM, would have avoided some coding errors. For example, here is an LLM summary:\n\"An actor expressed interest in purchasing data, with a priority on information from Israel\nand less emphasis on data from Iran.\" (312)"}, {"title": "The LLM System is Imperfect, just like Humans", "content": "One last element that needs to be considered is that there were instances where the coders simply did\nnot understand the logic behind the LLM's coding. For example, given this summary:\n\"A user advertised a product called 'Red Node HVNC', which is a hidden VNC (Virtual\nNetwork Computing) tool that allows remote control over a target machine via a web browser.\n[The actor] claims the tool is fully undetected (FUD) by antiviruses, including Windows\nDefender, and does not require a crypter for distribution. A demonstration video and a\nfeature comparison chart were provided. Another user criticized the product, suggesting it is\noverpriced and questioning the lack of moderation for such products. [The principal actor]\ndefended their product, stating it is privately coded, FUD, fast, and the first HVNC to work\nover a web browser, unlike others cloned from 'tinynuke.\u201d (160)\nThe LLM coded the variable is_initial_access as FALSE even though the actor sells a tool that\nallows remote control over a target machine via a web browser. Hence, in some cases, the LLM missed\nthe information. The LLM system was therefore not flawless, just like humans. Although rare, it made\ncoding errors that could not be explained."}, {"title": "Conclusion", "content": "Cyber threat intelligence has traditionally been plagued by a lack of actionability and challenges around\nmaking effective use of analyst time. Our research indicates that there is significant potential for lever-\naging LLMs to conduct an initial review of original source data to categorize events and prioritize those\nthat may be most relevant to CTI teams. This presents a substantial opportunity to use analyst time\nmore effectively by narrowing searches and automatically alerting on relevant findings.\nWhen language models go wrong, they tend to do so in predictable ways. Providing the LLM with\nthe context needed to categorize the event was important, and it did sometimes miscategorize events,\nparticularly when the prompt left room for interpretation. Another potential error was introduced\nwhen conversations spanned multiple days, so the summarization may have missed crucial context, thus\nresulting in an incorrect label. Our analysis also showed that the model struggled with differentiating\nbetween stories and original events occurring.\nDespite these issues, OpenAI's GPT-3.5-turbo model [7] was able to produce exceptional results\nacross our study of 500 events. This indicates language models are currently a scalable and cost-effective\nsolution for identifying relevant CTI data and could have exceptional potential when paired with state-\nof-the-art (SOTA) models for reporting data and fine-tuning results. Leveraging SOTA models, such\nas Claude 3.5 [2] Sonnet or GPT-40 [10, 9, 8], has the potential to further improve results or allow for\ncategorization of even more complex data. This represents a significant opportunity for further study."}]}