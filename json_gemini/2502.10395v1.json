{"title": "An Integrated Platform for Studying Learning with Intelligent Tutoring Systems: CTAT+TutorShop", "authors": ["Vincent Aleven", "Conrad Borchers", "Yun Huang", "Tomohiro Nagashima", "Bruce McLaren", "Paulo Carvalho", "Octav Popescu", "Jonathan Sewall", "Kenneth Koedinger"], "abstract": "Intelligent tutoring systems (ITSs) are effective in helping students learn; further research could make them even more effective. Particularly desirable is research into how students learn with these systems, how these systems best support student learning, and what learning sciences principles are key in ITSs. CTAT+Tutorshop provides a full stack integrated platform that facilitates a complete research lifecycle with ITSs, which includes using ITS data to discover learner challenges, to identify opportunities for system improvements, and to conduct experimental studies. The platform includes authoring tools to support and accelerate development of ITS, which provide automatic data logging in a format compatible with DataShop, an independent site that supports the analysis of ed tech log data to study student learnings. Among the many technology platforms that exist to support learning sciences research, CTAT+Tutorshop may be the only one that offers researchers the possibility to author elements of ITSs, or whole ITSs, as part of designing studies. This platform has been used to develop and conduct an estimated 147 research studies which have run in a wide variety of laboratory and real-world educational settings, including K-12 and higher education, and have addressed a wide range of research questions. This paper presents five case studies of research conducted on the CTAT+Tutorshop platform, and summarizes what has been accomplished and what is possible for future researchers. We reflect on the distinctive elements of this platform that have made it so effective in facilitating a wide range of ITS research.", "sections": [{"title": "1 Introduction", "content": "Intelligent tutoring systems (ITSs) [42] are a form of ed tech proven to be highly effective in helping students learn [10, 31, 38]. Although there is a large research literature on learning with ITSs, further research is highly desirable. The better we understand how students learn with this technology, what features make it effective, and how to carry out data-driven improvement cycles, the better we will be able to enhance the engineering of these systems and make them even more effective. Further, developing a deeper understanding of student learning in highly-scaffolded environments (as ITSs are) is interesting in its own right from a learning sciences perspective. What makes ITSs particularly attractive vehicles for learning sciences research is that they can be used in a full research lifecycle. That lifecycle includes mining log data from students' interactions to discover learner challenges (e.g., through methods from Educational Data Mining) and identify opportunities for system improvements, creating alternative designs aimed at bringing about improvements, and testing them in A/B experiments. ITSs can serve to address to a wide range of research questions, not only questions about how best to implement particular ITS functionality, but also about what learning sciences principles are key in the context of the high scaffolding provided by ITSs\nThis paper reports on the use of CTAT+Tutorshop as a research platform for investigating learning sciences questions around learning with ITSs, a role it has served for over 20 years. The Cognitive Tutor Authoring Tools (CTAT) were originally developed to facilitate the development of ITSs for both programmers and non-programmers [4]. These tools are largely domain-independent and can be (and have been) applied for start-to-finish tutor development in many task domains and at many levels of instruction, including K-12, colleges, and universities. Tutors built with CTAT have the main features that are characteristic of ITSs: they adaptively guide students in complex learn-by-doing activities with both within-problem guidance (or an \"inner loop,\" or \"step loop\") [63] and individualized mastery learning [14]. Tutorshop was first conceived as a Learning Management System (LMS) to support the use of CTAT-built tutors in real educational settings such as classrooms and university courses. The latest version of the CTAT authoring tools, which runs on the web, is embedded in Tutorshop, bringing the systems even closer together. This embedding for example facilitates collaborative authoring and publishing tutors on Tutorshop, where they are available for testing and real-world use. Tutors developed on the platform produce rich log data, in DataShop format [28], an independent site that offers many analytics and tools for analyzing student learning. Many major tutoring systems and educational games have been built and used within this platform including: Mathtutor [3], the Genetics Tutor [15], the Stoich Tutor [39], ORCCA [27], Chem Tutor [54], the Signals Tutor [57], Lynnette [34, 60], Gwynnette [44], several incarnations of the Fractions Tutor [16, 49, 51], and Decimal Point [46].\nMany of these ITSs were used as part of regular instruction in schools and university courses, but they have also been used in a very large number of research studies (estimated to be 147, see below). Many of these studies tested effects of new tutoring features (e.g., new implementations of learning principles within tutoring systems) or tested the effectiveness of tutoring systems against other forms of instructions, in a range of study designs. Over the lifetime of the CTAT+Tutorshop platform, we have helped researchers become familiar with the platform (through courses, summer workshop, and consulting) and run studies on the platform (through consulting, sometimes by taking a larger role in helping them implement their study). Although past papers have described both CTAT and Tutorshop [4], they have not focused on their use as a research platform. In the current paper, we address that gap.\nThe combination of CTAT+Tutorshop (together with DataShop) supports research in the first place by facilitating authoring, deploying, and using ITSs in real educational settings. In addition, the tools have some dedicated functionality to facilitate research studies, described in more detail below, including, first and foremost DataShop logging [28]. There are many advantages to having authoring tools integrated in a research platform, even if at first blush, authoring ed tech might seem antithetical to the idea of platform-enabled research - isn't the number one reason for using an existing platform that the researcher does not need to create their own? Arguably, the CTAT+Tutorshop authoring tools facilitate a wide range of research. First, as mentioned earlier, a range of ITSs have been built with these tools that are now available for use in research studies so researchers do not need to build their own. Interestingly, some ITSs created with CTAT that were primarily created for use in classrooms were also used as a platform for research [6, 15, 27]; the two motivations often go hand in hand. Furthermore, authoring tools facilitate the creation of experimental conditions for use in experimental studies. These conditions can be variations of existing ITSs, authored with the authoring tools. Researchers can also use the authoring tools to create a new ITS to serve as a new platform for research [62].\nIt is not clear that it matters whether or how one learning-research-enabling platform is different from another, since there is a very large number of research questions to be answered in that sense, the more high-quality platforms, the merrier. If we nonetheless had to point out the distinguishing characteristics of CTAT+Tutorshop, we would say it is unique in that it focuses on ITSs and integrates (programmer and non-programmer) authoring tools. Close comparisons are Assistments (+ eTrials), OATtutor, and MATHia (+"}, {"title": "2 Background: Intelligent Tutoring Systems with CTAT+Tutorshop", "content": "Generally, ITSs support deliberate practice of cognitive skills with feedback, just-in-time instruction (aka hints) and individualized mastery learning [14, 29]. Typically, they support practice of problems that require multiple reasoning steps to solve and that can be solved in multiple ways that is, have multiple solution paths students may follow. We call the guidance along multi-step solution paths \"within-problem guidance,\" which is meant to be equivalent to VanLehn's [63] notion of an \"inner loop\" or \"step loop.\" Within-problem guidance enables students to succeed on complex problems, with which they would likely struggle otherwise. As well, with such guidance, students learn better ([64, discussed in [38]). As an additional form of adaptivity, ITSs support personalized problem selection every learner gets the right (individualized) amount of practice to master the knowledge components (KCs) targeted in the instruction. The adaptive guidance within problems and adaptive problem selection aimed at mastery learning set ITSs apart from other ed tech.\nThe Cognitive Tutor Authoring Tools (CTAT; [4]) support the start-to-finish development of two types of ITSs, example-tracing tutors, which can be created without programming (e.g., by researchers without a technology background), and model-tracing tutors, which are more suitable for problems with large solution spaces, but require Al programming to create a rule-based cognitive model. Many research studies show that students learn very effectively with CTAT-authored tutors (studies listed in Table 1).\nTutorshop is an LMS dedicated to using CTAT-built tutors in real educational settings (schools, colleges) as well as for personal use. It lets teachers create class lists, assign work to students, and view performance reports for their classes. It allows students to see their assigned work, launch the tutoring system, and view reports about their performance and progress. Tutorshop also allows researchers to create or modify tutors and/or problem sets. It also allows researchers to perform a variety of useful administrative functions related to experiments, such as assigning different activities to students in different conditions, or tracking progress. We note that CTAT-authored tutors can be embedded easily in a range of widely-used LMSs (e.g., OpenEdX, OLI, Canvas, Moodle), where they could be used for research.\nA researcher running a study on CTAT+Tutorshop may opt to create a new tutoring system for use in their study or decide to use one of the existing tutoring systems available within Tutorshop. Either way, they will likely create variations of the tutoring system to serve as conditions in the experiment. Once an author has identified the targeted problem types and problems for which to build a tutor and has selected which of the two tutor paradigms to use, key design decisions pertain to:\n\u2022\tThe layout of the tutor interface: How the problem is divided into subgoals and steps, when subgoals or steps are made visible, whether/how the interface prompts the student for the steps.\n\u2022\tThe knowledge component (KC) model: How the overall competency to be learned in a given tutor unit is decomposed into KCs, which the tutor uses to track a student's knowledge growth.\n\u2022\tAdaptive step-level guidance: The author needs to provide step-level hint messages as well as feedback messages presented upon successful completion of steps or specific errors."}, {"title": "3 How CTAT+Tutorshop Can Be Used for Research", "content": "The CTAT+Tutorshop platform supports research into how students learn with ITSs, how these systems best support learning, and what learning sciences principles are key in ITSs. The platform supports a research and engineering cycle in which data from an ITS is used to discover regularities and challenges in learning with ITSs, to identify opportunities for system improvement, and to test alternative interventions in experimental studies. Although not all studies go through the full cycle, the cycle is available when the research questions warrant going through it. The paragraphs below elaborate on each cycle stage.\nIdentify regularities and challenges Several studies conducted on the platform focused on Identifying challenges that students experience when learning with a given ITS (or across multiple ITSs) as well as on discovering patterns in students learning with one or more ITSs. These studies involve data mining, using log data captured in DataShop. A common type of analysis is KC model analysis, where the researcher tries to identify a KC model that best captures the distinct skills evident in students' longitudinal performance on problem-solving steps (e.g. [24, 36]).\nIdentify opportunities for improvement, redesign One study used the results from an analysis of log data to inform a data-driven redesign of the given ITS. This study represents a form of design loop adaptivity, in the terminology of the Adaptivity Grid, a conceptual framework for understanding how learning technologies can adapt to both differences and similarities among learners [2]. Design-loop adaptivity means making a system more adaptive to challenges experienced by all learners in a given task domain, based on data from the system. Ideally, the effect of the redesign is evaluated in a close-the-loop study, by comparing it against the original system."}, {"title": "4 Research Done with CTAT+Tutorshop", "content": "We do not have an exact count of the number of research studies that have been run on the CTAT+Tutorshop platform, nor of the exact number of researchers that have been active on the platform. The reason that obtaining such a count is difficult is that the platform has been used for many different purposes, including tutor development and use of tutors in regular instruction. Also, the platform does not have an explicit representation of a study, and we do not require users to formally describe their purpose. Instead, we give an impression of the amount of research done and the number of active researchers, by counting the datasets in DataShop that come from CTAT tutors and the Tutorshop accounts that created content."}, {"title": "5 Case Studies", "content": "We present five cases of research studies carried out with CTAT+Tutorshop; between them they illustrate a range of research questions and research designs. They also illustrate the many ways that CTAT+Tutorshop are helpful in conducting research.\nCase study 1: Use of CTAT+Ttuorshop to test learning sciences principles in the context of an ITS\nA series of studies with a chemistry tutor, the Stoich Tutor [39-41], illustrates how CTAT+Tutorshop can be used without extensions to conduct research studies with ITSs. The Stoich Tutor, built with CTAT, provides learning support for high school students learning the sub-area of chemistry called stoichiometry. Key motivations were to provide better learning support in introductory chemistry courses but also to serve as a platform for learning sciences research. This tutor was used to explore several e-Learning principles [13], including the use of worked and erroneous examples for learning [41] and the impact of polite versus direct feedback for learning [39-40]. The conditions in these different experiments were implemented using standard features of the CTAT authoring tools. To create a polite language version, the researchers edited all hints, error messages, and success messages. Worked examples were created by having the tutor fill out some of the steps of standard problems, through a CTAT feature called tutor-performed actions, by which the tutor (rather than the student) updates the interface. What was left for the students to do was explain the terms in the solution equation using simple menus (an interaction already in the tutor). Finally, erroneous examples were built as their own kind of tutor problems, where a problem and erroneous solution were presented and the student was asked to explain and correct the error. The Stoich Tutor, like many tutors described in this paper, was deployed in TutorShop with DataShop as the data repository. The tutor variants in these studies illustrate why having authoring tools within a platform for learning sciences research is useful. The Stoich Tutor was used by later researchers [5] to address other research questions, illustrating another advantages of authoring tools in a platform for learning sciences research: tutors accumulate, so researchers have a choice.\nCase study 2: CTAT-built ITS v. Non-ITS control condition\nA study by [7] illustrates that CTAT+TutorShop can facilitate classroom experiments that compare learning with an ITS against a non-ITS control condition, in this case, problem-solving practice on paper without technology support. The case study also illustrates that methods often used to analyze log data from ed tech can sometimes be fruitfully applied to data from non-tech learning environments. The motivation for the study was to investigate the learning benefits of immediate feedback and as-needed instruction through hints (as is typical of an ITS) against a naturalistic homework control condition. The study employed a matched, within-subject design. It leveraged an existing CTAT-authored ITS for middle-school mathematics, Mathtutor (built on Tutorshop); specifically, the study focused on three units that deal with the topic of graph interpretation. Working with some of the teachers who participated in the study, the researchers created analogous paper versions of the tutor problems to serve as the control condition. The experimental conditions were set up in Mathtutor by having students work on two sets of Mathtutor units in a crossover fashion. The control condition (paper) was administered separately (i.e., outside of Mathtutor). Students were randomly assigned to conditions at the class level. The sequence of content units was fixed and was the same in each condition. Students started with either paper- or tutor-based practice and switched to the other condition for the second set of content. The Mathtutor/Tutorhop LMS facilities were used to assign classes their relevant content units each day. CTAT's capabilities for authoring tests were used to create pre/post test problems. A regular spreadsheet was used to randomly assign classes to conditions. An interesting challenge was converting the paper data from the control condition to DataShop format so the learning processes could be compared between the two conditions. Two human coders entered students' problem-solving steps on paper into a slightly modified version of the given tutor units so as to generate log data. This methodology allows for novel modeling of learning rates during problem solving on paper as a key way of studying the relative affordances of ITSs versus paper.\nCase study 3: Secondary analysis of log data from CTAT tutor\nOur next case study illustrates how CTAT and DataShop data can be used for secondary data analyses, specifically, to investigate the variability in students' learning rate. The researchers, Koedinger and collaborators [30] set out to identify fast learners in educational settings, for which they used 27 DataShop datasets from K-12 and college courses using Al Tutors, Online Courses, and Ed Games. Over half of these datasets were from CTAT tutors. A key strength of these datasets, and indeed of many of the datasets in DataShop, is the availability of cognitive models (KC models). Koedinger et al. used them to model student learning, applying a variation of the Additive Factors Model [11], a logistic growth model widely used in the field of Educational Data Mining that predicts the increase in accuracy resulting from each additional opportunity to practice a KC,. These analyses provided evidence to support the hypothesis that the path to expertise requires extensive practice: Students needed about 7 additional opportunities per KC in addition to lectures to master each KC. However, these analyses also provided evidence that, contrary to previous theoretical proposals, learners acquire competence at largely similar rates. The authors observed high variability in how easy or hard it is to learn each KC and noted that learners varied substantially in prior knowledge, but not learning rate. The results of this secondary data analysis investigation, using the detailed data and knowledge components available in DataShop made possible by CTAT and online courses, suggests that achievement gaps may largely be the result of opportunity gaps (different prior knowledge) but not learning rate; in other words, provided with enough opportunities in a high-support learning environment (such as an ITS) every learner can learn anything. However, the results also pose challenges to learning theory to explain the regularity in learning rate: Why are students so similar in learning rate? One proposal, still untested, is that different students might be pulling from different backgrounds, domain-general knowledge to support their learning leading to similar learning rates despite varied initial knowledge.\nCase study 4: Educational game v. simple tutor\nCTAT, Tutorshop, and DataShop have also been used to support the development of and experimentation with game-based learning environments. Since 2014, McLaren and colleagues have experimented with a learning game, Decimal Point, focused on middle school children learning decimals and decimal operations. Over the past decade, McLaren's lab has run a series of classroom experiments involving over 1,500 students with the game. The studies have included a comparison of the game to a standard tutoring system, a study of whether providing agency can lead to more learning and enjoyment, and whether students benefit from hints and feedback in the game. The most fundamental findings from this line of research are that Decimal Point has led both to more learning and enjoyment than the tutor and the game has led to a gender effect in which female students learn more from the game than male students. The game was developed using CTAT and involved writing a small amount of custom code to create mini-game templates of combined CTAT widgets to implement the five general problem types the game targets: addition of decimals, number lines, sorting, completing sequences, and putting decimals into less-than and greater-than buckets. The code written for the mini-games involved game mechanics and animations that were used to create a game-like context. The game is deployed on TutorShop and data is collected and analyzed in the DataShop. TutorShop has enabled ease of deployment at schools, while the DataShop has supported a fine-grained analysis of student learning beyond test scores [46] and a knowledge component analysis that has resulted in a deeper understanding of students' decimal skills.\nCase study 5: Test of design-loop adaptivity approach\nOur final case study illustrates how CTAT+Tutorshop can support and test \"design-loop adaptivity\" (Aleven et al., 2017) through the full research cycle described above. CTAT+Tutorshop have in fact facilitated several new learning engineering workflows that realize design-loop adaptivity, combined with classroom experiments that evaluate their effectiveness [24, 32]. Here, we report the design-loop adaptivity example of [24]. Data from a CTAT-authored ITS, which had been collected in DataShop, was analyzed, the tutor units were then updated based on insights of this analysis, and finally a close-the-loop study was conducted to test, in a between-subjects classroom experiment, whether the data-driven redesign enhanced student learning. The key idea is the use of log data to identify difficult skills and then to create focused tasks that target these difficult skills effectively.\nThe approach tested by Huang et al. [24] involves a three-stage workflow that combines existing and new learning analytics and instructional design methods. It focused on three units in Mathtutor [3] in which students learn to write or explain algebraic expressions that model real-world situations. The redesigned ITS differed from the original ITS in three aspects: a more fine-grained KC model, new focused tasks targeting difficult skills, and a new problem selection algorithm. The data-driven redesign was carried out using past data of Mathtutor collected from DataShop. CTAT+Tutorshop facilitated the process of authoring new focused tasks in three main ways. First, CTAT's authoring facilities were used to create new problem types. Second, CTAT was used to add customized feedback messages for selected errors identified in the log data. Third, CTAT libraries were used to parse and evaluate algebraic expressions. To create and integrate a new task selection algorithm, the researchers made use of TutorShop's API for custom outer loops, described above. The new algorithm leveraged the standard student model, which provides probabilities that the student has mastered each KC targeted in the tutor [14]. Finally, Tutorshop was used to configure the order in which students needed to go through the materials (e.g., students were required to finish the pre-test before moving on to practice in the first session).\nThe experiment was conducted with high school Algebra 1 classes for one month, during two 40-minute normal class periods per week. Students were randomly assigned to two conditions within each class period. This randomization was realized through importing a pre-generated spreadsheet to Tutorshop which specifies the mapping between student IDs and Assignments, where different conditions correspond to different Assignments. Tutorshop's dashboard was used to monitor students' progress in real time(with"}, {"title": "6 Discussion and Conclusion", "content": "ITSs support a learning scenario that is common and important in many task domains: deliberate practice, with complex practice problems, to develop cognitive skills. A distinguishing characteristic of ITSs is that they provide step-level guidance and individualized mastery learning; few ed tech products offer this combination. Given the strong evidence that ITSs help students learn very effectively [10, 38], further research involving ITSs would be highly valuable; it would strengthen the scientific understanding of learning with these systems and help develop a strong learning engineering practice for ITS. The CTAT+Tutorshop platform, together with DataShop support an iterative cycle of design and research with ITSs, enabling researchers to develop ITSS and conduct studies as well as to get insights into learner data. What is distinctive about CTAT+Tutorshop among research-enabling ed tech platforms is that it supports ITS research and provides both ITS authoring tools and several existing ITSs built with these tools; this way researchers can create experimental variations of existing tutors or build new tutoring systems from scratch.\nBased on the number of datasets in DataShop that come from CTAT tutors, the number of studies run on the platform may be as high as 147.\nGenerally, CTAT+Tutorshop have supported a large variety of studies addressing important, interesting research questions. These studies have moved forward the state-of-the-art in the learning sciences regarding learning in highly-scaffolded environments, as well as the learning engineering of ITSs. As indicated by Table 1 and illustrated by five case studies, the CTAT+Tutorshop platform makes it possible to address research questions covering a full research cycle (identify challenges using data from the ITS, improve the design of the ITS, test whether the improved ITS indeed enhances student learning). The studies used a wide range of research designs, including studies with multiple tutor versions, sometimes with factorial designs, experiments comparing tutor v. non-tutor conditions (with the non-tutor conditions running outside of the platform), and studies of design loop adaptivity. In some studies, the experimental manipulations occurred in the tutor's step loop [40], in others they changed the task loop [51], and in yet others, they were situated in both loops [24]. In some studies, the experimental tutor versions were authored straight with the tools [40, 51], in others, they involved custom programming [20, 33, 43, 44, 46, 66]. In all studies, Tutorshop's LMS capabilities were helpful in administering the experimental conditions.\nSeveral additional features would help improve the platform. First, it would help to record, both on Tutorshop and in the DataShop log stream, meta-data of studies (e.g., contact information for the investigators, research questions, description of conditions, link to the pre-registration, link to the tutors in Tutorshop and to the code base, if there is a separate code base outside of Tutorshop e.g., when custom programming was needed), so researchers could more easily get an up-to-date impression of what kinds of questions have and have not been investigated on the platform. Third, it might help as well to offer multi-armed bandit testing within the platform, in order to take advantage of advantageous conditions already while a study is still running, possibly within both the step loop and the task loop. To run this kind of study, one might create versions of the same ITS with and without a step loop (i.e., with the usual step-level guidance vs. giving feedback only at the end of the problem), or vary specific step loop features separately Alternatively, one could separately design multi-step problems v. single-step problems for the same learning objectives"}]}