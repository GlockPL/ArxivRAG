{"title": "Two Heads Are Better Than One:\nCollaborative LLM Embodied Agents for Human-Robot Interaction", "authors": ["Mitchell Rosser", "Marc G. Carmichael"], "abstract": "With the recent development of natural lan-\nguage generation models - termed as large lan-\nguage models (LLMs) a potential use case\nhas opened up to improve the way that humans\ninteract with robot assistants. These LLMs\nshould be able to leverage their large breadth\nof understanding to interpret natural language\ncommands into effective, task-appropriate and\nsafe robot task executions. However, in reality,\nthese models suffer from hallucinations, which\nmay cause safety issues or deviations from the\ntask. In other domains, these issues have been\nimproved through the use of collaborative AI\nsystems where multiple LLM agents can work\ntogether to collectively plan, code and self-check\noutputs. In this research, multiple collaborative\nAI systems were tested against a single indepen-\ndent AI agent to determine whether the success\nin other domains would translate to improved\nhuman-robot interaction performance. The re-\nsults show that there is no defined trend be-\ntween the number of agents and the success of\nthe model. However, it is clear that some col-\nlaborative AI agent architectures can exhibit a\ngreatly improved capacity to produce error-free\ncode and to solve abstract problems.", "sections": [{"title": "1 Introduction", "content": "In the past two years, the emergence of incredibly sophis-\nticated generative AI models has led to paradigm shifts\nacross many fields of research. These generative AI mod-\nels are based on the transformer architecture and can\nprocess natural language instructions, parsing them into\nmeaningful, contextually appropriate outputs [Firoozi\net al., 2023]. The scale of these models termed as\nLarge Language Models (LLMs) - has led to the inciden-\ntal development of emergent properties, whereby they\ncan adeptly respond to situations they have never been\nexplicitly trained on [Sorin and Klang, 2023]. Whilst\nthere is a potentially enormous opportunity for improved\nhuman-robot interaction using these models, it is still\nunclear how to ensure safe and effective task execution.\nIn this work we evaluate the potential for multiple col-\nlaborative AI agents to achieve improved human-robot\ninteractions compared to a single AI agent. Through em-\npirical comparison of three AI agent architectures, their\nperformance is measured and compared in a task involv-\ning a quadrupedal robot and human user."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Robot Task Planning Using Large\nLanguage Models", "content": "Existing literature has explored several methodologies\nfor leveraging singular LLM agents for robot task plan-\nning. [Liang et al., 2023] demonstrates a system, referred\nto as Code-As-Policies, that can convert human language\nprompts into executable Python code. From experimen-\ntation with this system, they conclude that LLM-based\nrobot planners can be used to generate executable robot\ncode, enabling the robot to reason within their environ-\nment, generalise beyond their training information (ex-\nhibit zero-shot tendencies), and calculate values where\nnecessary to control robot functioning. Simultaneously,\nthey note that there are limitations to this methodology.\nNotably, there is no way to know if a response will be\ncorrect prior to execution and, no testing was done on\nasking the system to perform unfeasible tasks.\nA lack of supervision for these LLM agents is discussed\nagain in [Ahn et al., 2022]. Here, the researchers identify\na crucial flaw in using LLMs for robotics tasks: the mod-\nels do not fully understand the robot or the domain in\nwhich they operate. The paper's SayCan system utilises\nreinforcement learning based skills and associated 'affor-\ndance functions' to fix this issue. The 'affordance func-\ntion' determines the likelihood of success for an action\nin any given state and is a remnant of the learned tem-\nporal differences from the reinforcement learning process\nof each skill.\nIn addition to these flaws in single-agent systems,\nLLMs are known to hallucinate. This is the term de-\nscribing the phenomenon whereby an LLM \"responds\nto a question with text that seems like a plausible an-\nswer, but is factually inaccurate or irrelevant\" [Verspoor,\n2024].\nWhen looking beyond the domain of robotics, poten-\ntial solutions emerge. One of these solutions is the use of\nmultiple collaborative agents [Wu et al., 2023]. Whilst\nit has been shown that this method improves perfor-\nmance in other areas of research, it is yet to be seen if\nthe same improvements can be achieved in the field of\nhuman-robot interaction."}, {"title": "2.2 Multi-Agent Cooperation", "content": "Multi-agent cooperation is an idea that has been found\nto improve the effectiveness of LLMs in completing tasks.\nIn [Ni and Buehler, 2024], it can be observed that mul-\ntiple LLM agents communicating to solve a task can\nlead to an improved ability to \"write, execute and self-\ncorrect code\" within the domain of solving mechanical\nproblems. The researchers found that these collabora-\ntive LLM teams perform better than singular AI agents.\nThe architecture of this communication is Microsoft's\nAutoGen framework. [Wu et al., 2023], shows how a\nmulti-agent system built on Microsoft AutoGen outper-\nforms single model systems on OptiGuide, a supply chain\noperation optimisation benchmark. Further, they noted\na simplification in the workflow with this experiment. It\nwas demonstrated that the multi-agent implementation\nrequired far less code, was three times as fast in achiev-\ning a realistic output and reduced the required number\nof human interactions. In another experiment focused\non chess, [Wu et al., 2023] noted that the multi-agent\nsystem improves the rule-following and grounding of the\nLLMs. By having an agent dedicated to checking the\nvalidity of moves, the game saw far fewer illegal moves\nthan when the individual player agents were told to en-\nsure that their own moves were legal."}, {"title": "2.3 Increasing performance in\nHuman-Robot Interaction", "content": "Considering these improvements in other domains, we\nset out to investigate whether a collaborative AI system\nwould outperform independent LLMs tasked with han-\ndling human-robot interaction.\nThis research was conducted via experimentation\nwhich saw three systems compared on a series of tri-\nals designed to ascertain the systems' problem-solving\nabilities. At the same time, the safety, sociability, time-\nliness, and token efficiency of all systems were tracked,\nenabling a comparison on these metrics as well."}, {"title": "3 Methodology", "content": "The question at the core of this investigation was:\nwhen tasked with control of a robot for natural language\ntask completion, does collaboration between multiple A\u0399\nAgents within one robot lead to improved performance\nover the use of singular embodied agents?\nTesting was conducted in a manner inspired by the\nmethodology shown in [Ni and Buehler, 2024]. We took\na structured approach, testing three different agent com-\nbinations (as detailed in Section 3.1) on a series of seven\ndifferent trials, repeating this three times, each time with\nan independent observer [Figure 3]. The unique indepen-\ndent human observer was present to provide blind feed-\nback on the system's performance. This feedback was\ncollected as described in Section 3.2. Figure 1 shows the\ntesting platform, a Boston Dynamics Spot robot, and the\ntesting environment. The environment and perception\nsystem were simplified using fiducials to reduce the like-\nlihood of perception errors. These fiducials represented\nobjects (chair, person, donut, apple, refrigerator, oven,\nmicrowave) that would be useful for the AI system when\nattempting to satisfy the task it was prompted with.\nThe robot was controlled by the AI system through\nthe use of a custom library [Rosser, 2024]. The AI sys-\ntem would produce python code to access this library,\ncombining it with the necessary logic and calculations\nto achieve the task goal [Figure 4]. The library then in-\nteracted with spot using the Spot ROS driver [Staniaszek\net al., 2024]"}, {"title": "3.1 AI System Architectures", "content": "Three different AI systems were tested in this research,\neach containing a different number of LLM agents [Fig-\nure 5]. All agents in this research used OpenAI's\nChatGPT-4 as their underlying model with no addi-\ntional fine-tuning or training. The different agent roles\nwere instilled through the use of detailed system prompts\n[Rosser, 2024]. The agents were then combined together\nusing Microsoft Autogen's group chat feature. To inspire\na greater consideration of safety the agents were all in-\nformed in their system prompts that \"you are a member\nof a team of Als controlling a guide dog robot to assist\na visually impaired user safely navigate the world\".\nConfig A was an independent agent, consisting of only\none LLM agent instructed to interpret a natural lan-\nguage instruction or statement and produce executable\ncode from it.\nConfig B was comprised of the same coding agent in\naddition to a reviewing agent and a chat manager agent.\nThe reviewer's responsibility was to provide feedback to\nthe coder on how to improve its code on the grounds of\ncorrect coding, effective task completion and safe execu-\ntion. Once happy with the code, the reviewer could pass\nthe code on for execution. The chat manager's respon-\nsibility was to promote the next speaker throughout the\nconversation depending on the previous message. For\nthis conversation, it could either pass on the code for\nexecution if the reviewer approved it, or return back to\nthe coder with feedback.\nConfig C had a similar architecture to config B except\nfor the inclusion of a planning agent before the coder.\nIn this configuration, the planner would interpret the\nprompt and develop a set of natural language instruc-\ntions that the coder should use as a scaffold to build its\ncode from. The coder was then instructed to use this\nplan to write its code before handing off to the reviewer\nfor a review cycle as in config B."}, {"title": "3.2 Recorded Feedback", "content": "Both qualitative and quantitative data was recorded for\neach trial to understand the AI systems' relative per-\nformances and their usage characteristics. The recorded\ndata is summarised below in Table 2."}, {"title": "3.3 Conducting Testing", "content": "Test participants were from the general public, recruited\nby incidental interaction and screened based on their fa-\nmiliarity with AI and Robotics. The first three partici-\npants who self-reported as having a low familiarity with\nusing AI and robotics were selected for this testing. This\nwas done to try to remove bias stemming from any tech-\nnical expectations of the AI system and robot.\nIn addition, before each observer arrived for testing, all\ncode was pre-generated for all of the configurations and\ntrials. This was done in attempt to remove the impact\nof computation time on the observer's scoring. If code\nerrored, the participant was instructed not to record any\ninformation. The code would then be regenerated so that\na score was able to be recorded from the participant."}, {"title": "3.4 Case Study", "content": "In addition to the live experimentation, further insights\nwere gathered through a case study. Twelve examples of\ncode from two prompts (3 and 5) with two attempts for\neach configuration were provided for review to a tech-\nnically proficient person with a sound understanding of\nPython code. Prompts 3 and 5 were selected as these\nrepresented the most vague, and, as such, the most dif-\nficult, prompts in the experiment. Each code example\nwas anonymised without indication of the configuration\nthat produced it. The reviewer was instructed to provide\na qualitative analysis of the produced code, identifying\nthe differences (if any) between the three configurations."}, {"title": "4 Results", "content": "Following the experimentation, there were 63 samples\ncollected: three attempts for each configuration, for each\nof seven prompts. The results clearly show that configu-\nration B is the least error prone, having the fewest initial\ntask execution attempts result in a runtime error. Con-\nfiguration B exhibited a 9 and 14 percentage point drop\nwhen compared to configurations A and C respectively\n[Figure 6]."}, {"title": "4.1 Case Study Results", "content": "Within the results of the case study, it can be noted\nthat configuration B stays on task well. As stated by\nthe technically proficient reviewer, configuration B \"does\nwell adhering to the context situation\" and is able to de-\nfer \"to the user to take over in more complicated situa-\ntions it isn't fit to handle\". This is accomplished through\na \"flexible user input\" and \"handling issues\nwell\".\nConversely, it was noted that configuration C tends to\n\"pretend to do activities it is not capable of doing\" indi-\ncating to the reviewer that the system \"seems to have a\nvery loose grasp on the context of the environment and\nthe system's own capabilities\". Configuration A pro-\nduces simple solutions but is often seen to be \"pretend-\ning to do things or placing parts in the code for an en-\ngineer to finish\". It also is noted to not implement error\nhandling well."}, {"title": "5 Discussion", "content": "This research set out to investigate whether 'when tasked\nwith control of a robot for natural language task com-\npletion, does collaboration between multiple AI Agents\nwithin one robot lead to improved performance over\nthe use of singular embodied agents?'. Whilst an over-\nall performance increase has not been observed for the\ncollaborative systems, there are noticeable performance\ndifferences in some situations. Configuration B is by\nfar the least error prone model and exceeds expecta-\ntions with greater success on abstract problem-solving\nprompts. Outside of this set, configuration A generally\nhas similar behaviour to configuration B but with a sig-\nnificant increase in errors. Configuration C is noted as\nan overall poor performer with the worst error rate and\nno exceptional performances across any prompt.\nThe superior error rate of configuration B compared\nto configuration A was expected, however, the high er-\nror rate of configuration C was surprising. It appears as\nif for configuration C the review mechanism was not as\neffective as in configuration B. This trend is also present\nin the observer-rated performance scores of the systems,\nhowever, to a less pronounced degree. While all config-\nurations generally had satisfactory performances, only\nconfigurations A and B had exceptional performances\nthat clearly outperformed the others. It is also noted\nthat the case study identified configuration C as hav-\ning \"a very loose grasp on the context of the environ-"}]}