{"title": "MAJL: A Model-Agnostic Joint Learning Framework for Music Source Separation and Pitch Estimation", "authors": ["Haojie Wei", "Jun Yuan", "Rui Zhang", "Quanyu Dai", "Yueguo Chen"], "abstract": "Music source separation and pitch estimation are two vital tasks in music information retrieval. Typically, the input of pitch estimation is obtained from the output of music source separation. Therefore, existing methods have tried to perform these two tasks simultaneously, so as to leverage the mutually beneficial relationship between both tasks. However, these methods still face two critical challenges that limit the improvement of both tasks: the lack of labeled data and joint learning optimization. To address these challenges, we propose a Model-Agnostic Joint Learning (MAJL) framework for both tasks. MAJL is a generic framework and can use variant models for each task. It includes a two-stage training method and a dynamic weighting method named Dynamic Weights on Hard Samples (DWHS), which addresses the lack of labeled data and joint learning optimization, respectively. Experimental results on public music datasets show that MAJL outperforms state-of-the-art methods on both tasks, with significant improvements of 0.92 in Signal-to-Distortion Ratio (SDR) for music source separation and 2.71% in Raw Pitch Accuracy (RPA) for pitch estimation. Furthermore, comprehensive studies not only validate the effectiveness of each component of MAJL, but also indicate the great generality of MAJL in adapting to different model architectures.", "sections": [{"title": "1 Introduction", "content": "The digital music industry has been growing rapidly in the last few years due to the mass publication of music through smartphone apps, enabling hundreds of millions of people to access a song via large music platforms. This has created huge music streaming companies such as Spotify (worth $37B) and QQ Music (worth $10B). The digital music industry in the US has a market of close to $10B in 2024 and has been growing at more than 10% in each of the last five years [44] and that in China has a market of $5B and has been growing at 30~50% in each of the last five years [31].\nMusic Information Retrieval (MIR) is a pivotal research domain that supports the functionality of music platforms. Within MIR, music source separation (MSS) and pitch estimation (PE) emerge as critical tasks with far-reaching implications. MSS facilitates several downstream tasks [7, 8], such as lyrics extraction [19] and music transcription [2], accentuating its pivotal role. PE has great significance for various applications [9, 10], including content-based music recommendation, query/search by singing [69] and multimodal generation [46, 62]. Notably, real-world musical compositions are often mixture music, typically as .wav or .mp3 files, which do not inherently provide the pitch information of music data. To extract target sources along with their corresponding pitches, simultaneous execution of both MSS and PE tasks becomes imperative.\nThe MSS task entails generating isolated stems for vocals, bass, or drums from raw audio or spectrograms of mixture music, as shown in the top row of Figure 1(a). The PE task involves extracting fundamental frequencies (f0) from clean music audio or spectrograms,"}, {"title": "2 Related Work", "content": "Music Source Separation (MSS) is a crucial task in MIR, aiming to isolate individual sources from mixture music. Many deep learning methods have been proposed for MSS, generally categorized into common models and side-information informed models.\nCommon models operate solely on hidden features extracted from the time or frequency domains. For example, U-Net [34], Spleeter [26], CWS-PResUNet [45], ResUNetDecouple+ [41] and BandSplitRNN [47] predict target sources using frequency domain features. In contrast, Demucs [14], Wave-U-Net [65] and its follow-ups [48, 54] leverage time domain features. Other methods such as KUIELAB-MDX-Net [38], Hybrid Demucs [13] and HT Demucs [60] fuse both domain features to enhance the performance of MSS. The side information informed models use additional information, such as lyrics, pitches, or spatial information to improve MSS. For example, JOINT3 [61] employs phoneme-level lyrics alignment, while Soundprism [16, 18] and SPAIN-NET [55] leverage pitches and spatial information, respectively. These methods exclusively address the MSS task, which do not support simultaneous PE task.\nPitch Estimation (PE) is a fundamental task in MIR, aimed at extracting the fundamental frequency (f0). PE can be broadly classified into PE from clean music and PE from mixture music.\nFor clean music, existing methods encompass heuristic-based and data-driven methods. Heuristic-based methods like ACF [17], YIN [12], SWIPE [5], and pYIN [49] leverage candidate-generating functions to predict pitches. Conversely, data-driven methods, including CREPE [37], DeepF0 [64], and HARMOF0 [70], rely on supervised training of models for PE. While these methods achieve accurate pitch results from clean music, their performance is constrained when applied to mixture music due to the presence of other existing sources. For mixture music, existing methods comprise pipeline and end-to-end methods. Pipeline methods involve utilizing MSS models (e.g., Spleeter [26] and U-Net [34]) to extract target sources from the mixture music, and then using PE models to predict corresponding pitches. However, a mismatch between the data distributions at training and testing times often limits the performance of PE from mixture music. End-to-end methods (e.g., CNN-Raw [15], JDC [42], MTANet [20], MCSSME [73]) are designed to directly predict pitches from mixture music. Nevertheless, these methods encounter performance limitations as a result of the presence of other sources in mixture music."}, {"title": "3 Problem Formulation", "content": "Music Source Separation (MSS). The task of MSS is to extract a target source from mixture music signals. The mixture music signals can be represented as either the raw audio waveform $x$ or its corresponding spectrogram $X \\in \\mathbb{R}^{T \\times F}$, where T is the number of audio frames and F is the number of frequency bins. It should be noted that the spectrogram is computed using the short-time Fourier transform (STFT) as a feature representation of the original music signal. The output of MSS is the target source s, and the spectrogram of target source is represented as $S \\in \\mathbb{R}^{T \\times F}$. Thus, the MSS task can be formulated as $F_{mss}: x (X) \\rightarrow s$.\nPitch Estimation (PE). The task of PE aims to estimate the pitch sequence of clean music from a raw audio waveform or its spectrogram representation. Following previous studies (e.g., 360 in CREPE [37] and 352 in HARMOF0 [70]), each pitch is typically represented as a N-dimensional one-hot vector $y$. As a result, the output of PE is a sequence of pitch vectors $Y \\in \\mathbb{R}^{T \\times N}$, where N is the number of pitch values. Furthermore, the input of PE is typically obtained from the output of MSS. Thus, the PE task can be formulated as $F_{pe}: s(S) \\rightarrow Y$.\nJoint Cascade Framework (JCF). The JCF is designed to leverage the cascade relationship between MSS and PE, enabling joint learning of both tasks. It comprises a Music Source Separation Module (MSS Module) and a Pitch Estimation Module (PE Module). Firstly, the features of mixture music are input into the MSS Module to obtain predicted sources. Then, the PE Module extracts corresponding pitches from the predicted sources.\nIn line with previous studies [34, 37, 41, 67, 70], the training of JCF involves using the Mean Absolute Error (MAE) loss for MSS and the Binary Cross Entropy (BCE) loss for PE, respectively. Therefore, the loss function for MSS is defined as:\n$L_{mss}(s, \\hat{s}) = \\sum_{i=0}^{L} | s_i - \\hat{s_i} |$ (1)\nwhere s is the target sources, $\\hat{s}$ is the predicted sources and L is the length of mixture music. And the loss function for PE is defined as:\n$L_{pe} (y, \\hat{y}) = - \\sum_{i=0}^{N} (y_i log \\hat{y_i} + (1 - y_i) log(1 - \\hat{y_i}))$ (2)\nwhere N is the number of pitch values, y is the ground truth of pitch results and $\\hat{y}$ is the predicted pitch value. Thus, the total loss for naive joint learning of MSS and PE is:\n$L_{total} = L_{mss} + L_{pe}$ (3)\nThe JCF is unable to solve the two challenges mentioned in Section 1. Therefore, we propose our Model-Agnostic Joint Learning (MAJL) framework for both tasks, building upon and extending the JCF."}, {"title": "4 Method", "content": "As shown in Figure 2, the Model-Agnostic Joint Learning (MAJL) framework contains two important components: two-stage training method and Dynamic Weights on Hard Samples (DWHS). Besides, the MSS Module and PE Module within MAJL can be easily replaced with existing MSS and PE models."}, {"title": "4.1 Two-Stage Training Method", "content": "To address the limited availability of fully-labeled datasets and leverage large single-labeled datasets, we design a two-stage training method within our framework. This method comprises an initialization stage (Stage I) and a semi-supervised training stage (Stage II), as shown in Figure 2.\nInitialization Stage (Stage I): During Stage I, our framework is trained using fully-labeled music data (e.g., MIR-1K or MedleyDB). Then the trained framework is employed to generate pseudo labels for target sources or corresponding pitches. Additionally, we compute confidence values for each pitch result and each frame of target sources. For predicted pitches, a pitch value is considered present when $max(\\hat{y}) \\ge 0.5$; otherwise, a pitch value is considered absent. Then the confidence value ($conf_i$) is defined as:\n$conf_i = \\begin{cases} max(\\hat{y}) & max(\\hat{y}) \\ge 0.5 \\\\ 1 - max(\\hat{y}) & max(\\hat{y}) < 0.5 \\end{cases}$ (4)\nIt should be noted that this confidence value can also be applied to predicted sources due to the inherent cascade relationship between music source separation and pitch estimation.\nTo maintain a consistent format for confidence values across different dataset types (fully-labeled and single-labeled datasets), we set the confidence values of true labels to 1. Therefore, for fully-labeled datasets (e.g., MIR-1K and MedleyDB), the confidence values of MSS ($confi_{mss}$) and PE ($confi_{pe}$) are defined as:\n$confi_{mss} = 1 \\ \\ \\ confi_{pe} = 1$ (5)\nThen, for MSS datasets (e.g., MUSDB18), which belongs to single-labeled datasets, the confidence values of MSS ($confi_{mss}$) and PE ($confi_{pe}$) are defined as:\n$confi_{mss} = 1 \\ \\ \\ confi_{pe} = conf_i$ (6)\nWhile for PE datasets (e.g., MIR_ST500), which also belongs to single-labeled datasets, the confidence values of MSS ($confi_{mss}$) and PE ($confi_{pe}$) are defined as:\n$confi_{mss} = conf_i \\ \\ \\ confi_{pe} = 1$ (7)\nSemi-supervised Training Stage (Stage II): After the initialization stage, we obtain pseudo labels and confidence values from single-labeled datasets. We then combine fully-labeled music data, single-labeled music data, and pseudo-labels to create a synthetic dataset. To filter pseudo-labels from single-labeled music data, we set a threshold (th). The detailed filtering process involves applying weights for MSS and PE in the loss computation. Subsequently, we retrain our framework from scratch using the synthetic dataset. Thus, the loss function of stage II is written as:\n$L_{total} = confi_{mss} \\times L_{mss} + confi_{pe} \\times L_{pe}$ (8)\nHere, $confi_{mss}$ equals 1 if $confi_{mss} \\ge th$, and 0 otherwise. $confi_{pe}$ is calculated using the same way as $confi_{mss}$."}, {"title": "4.2 Dynamic Weights on Hard Samples (DWHS)", "content": "4.2.1 Analysis of Different Cases in DWHS. The naive joint learning method can not ensure simultaneous improvements in both tasks due to the problems of error propagation and misalignment between distinct objectives. To address these problems concurrently, we should identify hard samples and assign appropriate weights to each sample in both tasks. This can be achieved by comparing the predicted pitches from target sources with those from predicted sources (c.f. Figure 3). By this comparison, we can determine which module is delivering poor predictions and identify samples that are hard for either MSS or PE. A comprehensive analysis of different cases arising from this comparison is summarized in Table 1. Detailed explanations of each case in Table 1 are provided as follows.\nFor Case 1, both the predicted pitches from predicted sources and those from target sources are correct. This result indicates that there is no issue with the MSS Module, the PE Module or the quality of data. For Case 2, the predicted pitches from predicted sources are correct, while those from target sources are incorrect. This result indicates the presence of noisy pitch labels in the data. To mitigate the impact of noisy labels, the weights of such samples for PE should be within the range of 0 to 1. For Case 3, the predicted pitches from predicted sources are incorrect, while those from target sources are correct. This result indicates that the predicted sources are quite different from the original target sources, making the data hard for the MSS. To emphasize learning on hard samples, the weights"}, {"title": "4.2.2 Module of DWHS", "content": "By leveraging the above analysis, we can assign different weights to each sample based on identified cases, thereby aligning the focus of two tasks during joint learning. The most direct method involves setting different weights for different cases as outlined in Table 1. However, this approach incurs high training costs due to the difficulty of manually determining proper weights for each case. Therefore, we introduce the DWHS, which automatically extracts the appropriate weights for different cases.\nThe DWHS utilizes a simple network called the Dynamic Weight Module (DWM) to determine dynamic weights for the MSS and PE tasks under different cases. As illustrated in Figure 4, the inputs for the DWM consist of predicted_source2Pitch, Pitches, and target_source2Pitch from Figure 3, maintaining the same format as $Y \\in \\mathbb{R}^{T \\times N}$. These inputs are concatenated and passed through two CNN layers with ReLU activation, configured as depicted in Figure 4 with 3 \u00d7 3 kernels. Following the CNN layers, there is a flatten layer, a fully connected layer with ReLU activation, and finally, a fully connected layer with sigmoid activation that generates dynamic weights for both tasks. This process enables dynamic assignment of weights without the need for manual specification of specific weights. Specifically, the weights ($@_{mss}$ and $w_{pe}$) corresponding to different cases outlined in Table 1 are automatically extracted by the Dynamic Weight Module (DWM) of the DWHS."}, {"title": "4.2.3 Loss of DWHS", "content": "To ensure the weights extracted by the DWM for different cases align with the specified weights shown in Table 1, we design the loss function for the DWM of DWHS to address four specific cases. For Case 1, the Mean Absolute Error (MAE) loss is employed to ensure the predicted weights are around 1. Then the loss function for DWM of DWHS in Case 1 is defined as:\n$L_{dwhs_1} = |@_{mss} - 1| + |@_{pe} - 1|$ (9)\nFor Case 2, the MAE loss is utilized to ensure the predicted weights of MSS are around 1. For the predicted weights of PE, the Bayesian Personalized Ranking (BPR) loss [59] is employed to ensure lower weights for noisy music data. Then the loss function for DWM of DWHS in Case 2 is defined as:\n$L_{dwhs_2} = |@_{mss} \u2013 1| \u2013 ln \\sigma(1 \u2013 w_{pe})$ (10)\nFor Case 3, the BPR loss is used to ensure hard samples for MSS receive higher weights. Simultaneously, the MAE loss is used to ensure the predicted weights of PE are around 1. Then the loss function for DWM of DWHS in Case 3 is defined as:\n$L_{dwhs_3} = \u2212 ln \\sigma(@_{mss} \u2013 1) + |@_{pe} - 1|$ (11)\nFor Case 4, the MAE loss is employed to ensure the predicted weights of MSS are around 1. Additionally, the BPR loss is used to ensure hard samples for PE receive higher weights. Then the loss function for DWM of DWHS in Case 4 is defined as:\n$L_{dwhs_4} = |@_{mss} \u2013 1| \u2013 ln \\sigma(@_{pe} \u2013 1)$ (12)\nThus, the final loss of DWHS is written as:\n$L_{dwhs} = L_{dwhs_1} + L_{dwhs_2} + L_{dwhs_3} + L_{dwhs_4}$ (13)\nIt is important to note that if there is no music data belonging to a specific case, the loss function for that case is automatically set to 0. For example, if there are no music data belonging to Case 1, then the $L_{dwhs_1}$ becomes 0.\nThus, with the DWHS, the loss function of stage I is written as:\n$L_{total} = @_{mss} \\times L_{mss} + @_{pe} \\times L_{pe} + L_{dwhs}$ (14)\nAnd the loss function of stage II is written as:\n$L_{total} = confi_{mss} \\times @_{mss} \\times L_{mss} + confi_{pe} \\times @_{pe} \\times L_{pe} + L_{dwhs}$ (15)\nwhere $confi_{mss}$ and $confi_{pe}$ are the same as those in Eq. 8."}, {"title": "5 Experimental Setup", "content": "Datasets. We evaluate our framework using four public datasets: MIR-1K [27], MedleyDB [4], MIR_ST500 [66], and MUSDB18 [57]. MIR-1K and MedleyDB provide both mixture and clean vocal tracks, along with pitch labels for vocal parts, making them fully-labeled datasets. In contrast, MIR_ST500 and MUSDB18 provide PE and MSS labels, respectively, classifying them as single-labeled datasets. It should be noted that all pitch labels are transformed into frequency bins represented in Hz format the same as MIR-1K."}, {"title": "6 Experimental Results", "content": "6.1 Overall Performance\nIn this experiment, we conduct a comprehensive comparison of MAJL with several baselines, encompassing end-to-end methods, pipeline methods, and joint learning methods. We evaluate the performance of MAJL on both fully-labeled datasets and single-labeled datasets. These experimental results not only demonstrate the effectiveness of MAJL in joint learning of both tasks, but also highlight its superiority in either the MSS task or the PE task."}, {"title": "6.1.1 Results on Fully-labeled Dataset", "content": "To show the superiority of our framework, we perform a comparison with several baselines. For the MSS and PE modules, we choose ResUNetDecouple+"}, {"title": "4.2.1 Implementation Details", "content": "The raw audio is sampled at 16kHz and then transformed into a spectrogram using the short-time Fourier transform (STFT) with a Hann window size of 2048 and a hop length of 320 (20ms). We use the librosa [50] and torch-librosa [40] to perform the audio processing. During training of the model-agnostic joint learning (MAJL) framework, we use a batch size of 16 and the Adam optimizer [39]. The learning rate is initialized to 0.001 and is reduced by 0.98 of the previous learning rate every 10 epochs. In our framework, there are mainly three hyper-parameters, $w_{noise}, upper\\_bound$ and threshold (th). The hyper-parameters $upper\\_bound$ and $w_{noise}$ only used for the naive DWHS in additional methods (Section A.3), where $upper\\_bound$ ranges from 1 to 10 and $w_{noise}$ ranges from 0 to 1. While the hyper-parameter threshold (th) is used to filter pseudo labels, ranging from 0.5 to 1.\nEach training audio is divided into segments of 2.56 seconds. For the MIR-1K [27] dataset, we randomly split the dataset into training (80%) and testing (20%) sets. We treat MIR_ST500 [66] and MUSDB18 [57] datasets as single-labeled datasets since they lack the target sources and pitch labels, respectively. The splitting way for MIR_ST500 and MUSDB18 datasets is introduced in [66] and [57], respectively. During experiments, we only consider the target source of vocals due to the lack of fully-labeled data from other sources such as bass and drums."}, {"title": "4.2.4 Comparison Systems", "content": "We compare MAJL with several existing methods, including end-to-end, pipeline, and joint learning methods. For end-to-end methods, we chose CNN-Raw [15] and JDC [42] and MTANet [20], as they achieved the best performance in the pitch estimation task among all end-to-end methods. For pipeline methods, we consider U-Net [34] and ResUNetDecouple+"}, {"title": "A.1 Related Work Details", "content": "A.1.1 Music Source Separation. Music source separation (MSS) is a crucial task in music information retrieval (MIR), involving the decomposition of music into its constitutive components, such as isolating vocals, bass, and drums [58]. When focusing specifically on clean vocals and accompaniment without further separating the accompaniment into individual instruments, it becomes a singing voice separation task [52], a universal and specialized form of MSS. Various deep learning methods address the MSS task, broadly categorized into three types: the traditional methods, the deep learning-based methods, and the side-information informed methods.\nTraditional Methods are based on digital signal processing, include Independent Components Analysis (ICA) [63], Principal Component Analysis (PCA) [71], and Non-negative Matrix Factorization (NMF) [1]. For instance, ICA is initially employed to solve the blind source separation task [32], akin to the MSS task. The PCA-based method [29] utilizes robust principal component analysis to represent accompaniment through a separated low-rank matrix and singing voices through a separated sparse matrix. And the NMF-based methods [1] model each source with a dictionary, capturing source signals within the non-negative span of this dictionary. Although these traditional methods are interpretable and accomplish the MSS task to some extent, they often lack the ability to distinguish between different instruments.\nDeep Learning-Based Methods fall into three categories: methods in frequency domain, methods in time domain, and hybrid methods in both domains. For example, U-Net [34], Spleeter [26], CWSPResUNet [45], ResUNetDecouple+ [41] and BandSplitRNN [47] belong to methods in frequency domain, which process mixture music in frequency domain (e.g. spectrogram) to predict masks or spectrograms of target sources. Alternatively, Demucs [14], WaveU-Net [65], and its follow-ups [48, 54] are the methods in time domain. They directly process raw audio using one-dimensional convolutional networks to predict target sources in time domain. Additionally, other methods such as KUIELAB-MDX-Net [38], Hybrid Demucs [13], and HT Demucs [60] are hybrid methods in both domains. They combine features from both domains for improved performance of MSS. However, the above methods focus only on the features extracted from raw audio, ignoring the important role of auxiliary information such as melody, rhythm, lyrics, and so on in the MSS task.\nSide-Information Informed Methods use additional information, such as lyrics, music scores (pitches), or spatial details, to improve the MSS performance. For instance, JOINT3 [61] utilizes phoneme-level lyrics alignment to improve the performance of MSS. SPAINNET [55] and Soundprism [16] use music scores and spatial information to enhance the performance of MSS, respectively. However, these methods treat side-information as auxiliary features and lack the capability to perform highly related tasks in MIR simultaneously."}, {"title": "A.1.2 Pitch Estimation", "content": "Pitch estimation (PE) is a fundamental task in MIR, playing a crucial role in various downstream applications. Following previous studies [23, 37, 70], we default to using"}, {"title": "A.1.3 Joint Learning For MSS and PE", "content": "With the development of joint learning, several research tasks in MIR have shown the potential for mutual improvement through multi-tasks joint learning. Among these tasks, MSS and PE are closely related, leading to the exploration of methods that take advantage of their mutually beneficial relationship. Several existing methods have attempted to exploit the mutually beneficial relationship between MSS and PE.\nFor example, JDC [42] is a joint learning approach that addresses voice detection and pitch estimation. However, it mainly employs voice detection as an auxiliary task for accompaniment processing. Other methods, such as [30] and [6], utilize transcription as an auxiliary task, incorporating joint transcription and source separation"}, {"title": "A.3 Additional Methods", "content": "The most direct method involves setting different weights for different cases as outlined in Table 6. We refer to this method as naive DWHS, which utilizes two hyper-parameters ($w_{noise}$ and upper_bound) to assign weights to hard samples and noisy samples, as specified in Table 6.\nStarting with default weights set at 1, for Case 1, where there are no issues with the MSS Module, PE Module, or the music data, the default weights remain unchanged. In Case 2, involving music data with noisy pitch labels, we decrease the weight of such samples by adjusting the weight ($w_{noise}$) within the range of 0 to 1. This adjustment aims to mitigate the impact of noisy labels. In Case 3, where the music data is hard for the MSS task, we increase the weight of such samples in the MSS task by setting the weight to 1/\u0177t. The \u0177t is defined as $max(y) \u00d7 max(\u0177)+(1-max(y))\u00d7(1-max(\u0177))$, where y is the ground truth of pitch results and \u0177 is the predicted value. For Case 4, where the music data is hard for the PE task, we increase the weight of such samples in the PE task by setting the weight to 1/yt. Besides, we constrain 1/\u0177t to fall within the range of 1 to upper_bound to avoid invalid weights that are too large. The weights ($@_{mss}$ and wpe) for different cases are shown in Table 6.\nWith the naive DWHS, the loss function of stage I is written as:\n$L_{total} = @_{mss} \u00d7 L_{mss} + @_{pe} \u00d7 L_{pe}$ (19)\nAnd the loss function of stage II is written as:\n$L_{total} = confi_{mss} \u00d7 @_{mss} \u00d7 L_{mss} + confi_{pe} \u00d7 @_{pe} \u00d7 L_{pe}$ (20)\nwhere $confi_{mss}$ and $confi_{pe}$ is the confidence value of labels for music source separation and pitch estimation, respectively."}, {"title": "A.4 Additional Experimental Results", "content": "A.4.1 More Overall Performance. Our framework is easily adaptable for use with some new music source separation models. To further substantiate this, we have added results for BandSplitRNN [47] and CREPE [37] in Table 7, showing that our method is effective for both tasks. These results also show that our framework is model-agnostic."}, {"title": "4.4 Significance Test", "content": "In this section, we validate the significance of the improvements achieved by our framework through statistical significance tests. Thus, we perform multiple experimental runs (6 times) from training to testing and calculate p-values for the SDR and the RPA. The summarized results of these experiments are provided in Table 9.\nIn this experiment, the pipeline method, ResUNetDecouple+ with CREPE is used as the baseline, since it has displayed the best performance among all baselines. While our framework use both singlelabeled datasets (MUSDB18 and MIR_ST500) in the Stage II, and the DWHS is used in this experiment. It is important to note that due to the stochastic nature of gradient-based optimization techniques like the Adam optimizer [39] employed in this paper, there may be slight variations in results even under identical experimental conditions. This variability arises from random factors during training within our framework: the random initialization of model parameters and the random ordering of training samples in each epoch. For each experiment in Table 9, these random initialization use the default initialization method in PyTorch [53]. The final results reported in this paper are based on the best performance achieved from these repeated experiments. Based on the results presented in Table 9, we calculate the p-values for both the SDR and the RPA. The obtained p-value for SDR is 1.80e-4, and for RPA it is 1.78e-6. These results indicate that our proposed framework achieves a significant improvement when compared to the best-performing baseline."}, {"title": "A.5 Future Work", "content": "Our experiments have primarily focused on vocals, given that vocals are a common target source for both MSS and PE tasks, as highlighted in previous studies [33, 51]. Furthermore, the availability of music data containing other target sources and corresponding pitches was limited. However, it is important to note that our framework is applicable to a variety of musical instruments, including drums, bass and so on.\nExpanding our framework to encompass other instruments requires the acquisition or creation of annotated data for both MSS and PE tasks. Overcoming this challenge may involve adapting transfer learning techniques from related domains or exploring further unsupervised training methods. In conclusion, the future of our research holds the potential to adapt and expand our ModelAgnostic Joint Learning (MAJL) framework to encompass a broader range of musical instruments. These directions align with the ongoing evolution of music production techniques and computational audio analysis, making our framework important in advancing the field of music information retrieval."}]}