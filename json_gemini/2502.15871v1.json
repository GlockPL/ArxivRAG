{"title": "A Comprehensive Survey on the Trustworthiness of Large Language Models in Healthcare", "authors": ["Manar Aljohani", "Jun Hou", "Sindhura Kommu", "Xuan Wang"], "abstract": "The application of large language models (LLMs) in healthcare has the potential to revolutionize clinical decision-making, medical research, and patient care. As LLMs are increasingly integrated into healthcare systems, several critical challenges must be addressed to ensure their reliable and ethical deployment. These challenges include truthfulness, where models generate misleading information; privacy, with risks of unintentional data retention; robustness, requiring defenses against adversarial attacks; fairness, addressing biases in clinical outcomes; explainability, ensuring transparent decision-making; and safety, mitigating risks of misinformation and medical errors. Recently, researchers have begun developing benchmarks and evaluation frameworks to systematically assess the trustworthiness of LLMs. However, the trustworthiness of LLMs in healthcare remains underexplored, lacking a systematic review that provides a comprehensive understanding and future insights into this area. This survey bridges this gap by providing a comprehensive overview of the recent research of existing methodologies and solutions aimed at mitigating the above risks in healthcare. By focusing on key trustworthiness dimensions including truthfulness, privacy and safety, robustness, fairness and bias, and explainability, we present a thorough analysis of how these issues impact the reliability and ethical use of LLMs in healthcare. This paper highlights ongoing efforts and offers insights into future research directions to ensure the safe and trustworthy deployment of LLMs in healthcare.", "sections": [{"title": "1 Introduction", "content": "The application of LLMs in healthcare is advancing rapidly, with the potential to transform clinical decision-making, medical research, and patient care. However, incorporating them into healthcare systems poses several key challenges that need to be addressed to ensure their reliable and ethical use. As highlighted in Bi et al. (2024), a major concern is the trustworthiness of AI-enhanced biomedical insights. This encompasses improving model explainability and interpretability, enhancing robustness against adversarial attacks, mitigating biases across diverse populations, and ensuring strong data privacy protections. Key concerns include truthfulness, privacy, safety, robustness, fairness, and explainability, each of which plays a vital role in the reliability and trustworthiness of AI-driven healthcare solutions.\nTruthfulness, defined as \"the accurate representation of information, facts, and results by an AI system\" (Huang et al., 2024), is critical in healthcare, as inaccuracies can lead to misdiagnoses or inappropriate treatment recommendations. Ensuring that generated information is both accurate and aligned with verified medical knowledge is essential. Additionally, privacy concerns arise from the risk of exposing sensitive patient data during model training and usage, potentially leading to breaches or violations of regulations such as HIPAA (Health Insurance Portability and Accountability Act) and GDPR (General Data Protection Regulation). Ensuring patient confidentiality while leveraging LLMs for diagnostics and treatment recommendations is a critical challenge. Safety, defined as \u201censuring that LLMs do not answer questions that can harm patients or healthcare providers in healthcare settings\u201d (Han et al., 2024b), further underscores the necessity of implementing stringent safeguards to mitigate harm. Robustness refers to an LLM's ability to consistently generate accurate, reliable, and unbiased outputs across diverse clinical scenarios while minimizing errors, hallucinations, and biases. It also encompasses the model's resilience against adversarial attacks, ensuring that external manipulations do not compromise its integrity. A truly robust LLM in healthcare must demonstrate stability, reliability, and fairness,"}, {"title": "2 Datasets, Models, and Tasks", "content": "We first conducted an extensive search for papers on the trustworthiness of LLMs in healthcare. Our search utilized a range of keyword combinations, including terms such as 'large language models,' 'foundation model,' 'medical,' 'clinical,' 'explainability,' 'truthfulness,'\u2018trustworthiness,\u2019\u2018safety,' 'fairness,' 'robustness,' and 'privacy.' We explored several reputable venues, including Arxiv, PubMed, ACL, EMNLP, NAACL, ICML, NeurIPS, ICLR, KDD, Nature, Science, AAAI, and IJCAI, with a focus on recent publications post-2021. After reviewing the search results, we identified a total of 30,595 papers. Following the removal of duplicates, we narrowed the focus to 69 papers that specifically addressed the truthfulness, privacy, safety, robustness, fairness, bias, and explainability of LLMs in the healthcare domain.\nWe then summarized all the datasets, models, and tasks relevant to research on trust in LLMs for healthcare, providing a comprehensive overview of their applications and contributions to this domain. The datasets used in studies of trust in LLMs for healthcare are categorized by the dimensions of trustworthiness they address in Appendix A, where we highlight key details such as data type, content, task, and dimensions of trustworthiness. The content of each dataset specifies its composition, while the task refers to the primary purpose for which the dataset is utilized. The data type varies across datasets and includes web-scraped data, curated domain-specific datasets, public text corpora, synthetic data, real-world data, and private datasets, providing a comprehensive overview of their relevance to healthcare applications. The models assessed in studies on trust in LLMs for the healthcare domain are outlined, along with their trustworthiness dimensions, in Appendix B, where we summarized key details such as the model name, release year, task, and the institution responsible for its development. The tasks covered various primary focuses of LLMs in healthcare. Based on insights from the survey by Liu et al. (2024), these tasks are outlined as follows:\nMedical Information Extraction (Med-IE)\nMed-IE extracts structured medical data from unstructured sources such as EHRs, clinical notes, and research articles. Key tasks include entity recognition (identifying diseases, symptoms, and treatments), relationship extraction (understanding entity connections), event extraction (detecting clinical events and attributes), information summarization (condensing medical records), and adverse drug event detection (identifying medication-related risks).\nMedical Question Answering (Med-QA) Med-QA systems interpret and respond to complex medical queries from patients, clinicians, and researchers. Their core functions include query understanding (interpreting user questions), information retrieval (finding relevant data in medical databases), and inference and reasoning (drawing conclusions, inferring relationships, and predicting outcomes based on retrieved data).\nMedical Natural Language Inference (Med-NLI) Med-NLI analyzes the logical relationships between medical texts. Key tasks include textual entailment (determining if one statement logically follows another), contradiction detection (identifying conflicting statements), neutral relationship identification (recognizing unrelated state-"}, {"title": "3 Trustworthiness of LLMs in Healthcare", "content": "We examine the challenges related to the trustworthiness of LLMs in healthcare, outlining key strategies for identifying and mitigating these concerns. From our literature review screening, we identified truthfulness, privacy and safety, robustness, fairness and bias, and explainability as key trustworthiness dimensions of LLMs as highlighted in TrustLLM (Huang et al., 2024), particularly in healthcare. Figure 1 provides a summary of the recent research on trust in LLMs for healthcare across key dimensions of trustworthiness."}, {"title": "3.1 Truthfulness", "content": "Ensuring the truthfulness of LLMs in healthcare is vital, as inaccurate information can significantly impact patient care and clinical outcomes. Given their influence on diagnoses and treatment decisions, it is essential to develop effective methods to detect and mitigate hallucinations and factual inaccuracies.\nHallucinations in medical LLMs arise from reliance on unverified sources, biases in training data, and limitations in contextual understanding and sequential reasoning (Ahmad et al., 2023). Addressing these issues requires robust evaluation frameworks, self-correction mechanisms, and uncertainty quantification techniques.\nThe Med-HALT benchmark (Pal et al., 2023) is designed to evaluate hallucinations in medical LLMs by using reasoning-based tests like 'False Confidence' and 'None of the Above,' as well as memory-based tests to assess how well the model recalls medical knowledge. On the other hand, the interactive self-reflection methodology (Ji et al., 2023) aims to reduce hallucinations in medical question-answering tasks by introducing an iterative feedback loop where the model refines its responses through self-evaluation and knowledge adjustment.\nIn the context of multimodal models, Med-HVL (Yan et al., 2024b) introduces two key metrics-Object Hallucination and Domain Knowledge Hallucination\u2014to quantify hallucinations in Large Vision-Language Models (LVLMs). This framework also uses the CHAIR (Caption Hallucination Assessment with Image Relevance) metric to assess object hallucinations in image captioning. Med-HallMark (Chen et al., 2024) takes it a step further by providing a multi-task evaluation framework with a hierarchical categorization of hallucinations, introducing the MediHall Score for assessing hallucination severity and the MediHallDetector, a multitask-trained LVLM for hallucination detection.\nResearchers have also investigated semantic entropy, a probabilistic measure of uncertainty, to detect hallucinations in LLMs. For example, Farquhar et al. (2024) leverage semantic entropy to identify confabulations\u2014hallucinations where the model generates arbitrary or incorrect outputs. While effective, this approach is computationally expensive, limiting its scalability. To overcome this, Han et al. (2024a) introduce Semantic Entropy Probes (SEPs), which approximate semantic entropy directly from hidden states. By eliminating the need for multiple output samples, SEPs significantly reduce computational overhead. Both methods have been successfully applied to biomedical datasets, such as BioASQ.\nAlthough these techniques offer valuable contributions, hallucination mitigation methods often lack adaptability, being either task-specific or requiring expensive retraining. To address this gap, MEDAL (Li et al., 2024) introduces a model-agnostic post-processing framework that integrates with any medical summarization model. MEDAL uses a self-examining correction model to improve factual accuracy without adding extra computational costs, providing a practical solution to the issue of medical hallucinations.\nCollectively, these studies underscore the multifaceted challenge of ensuring truthfulness in medical LLMs. By leveraging benchmarking frameworks, self-correction mechanisms, and entropy-based uncertainty measures, researchers can develop complementary strategies for detecting, mitigating, and quantifying hallucinations. A key focus of these efforts is the development of quantifiable scoring methods, enabling systematic assessment and comparison across different models. These evaluation techniques not only help identify the most reliable and effective LLMs for healthcare applications but also provide actionable insights for further improvements.\nFactual accuracy is fundamental to building trust in LLMs, especially in healthcare, where reliable and verifiable information is critical. However, current LLMs lack effective mechanisms to trace claims back to their original sources, underscoring the urgent need for improved validation techniques to ensure safe and trustworthy medical applications. To address these challenges, several studies have introduced innovative approaches to enhance the transparency, accuracy, and reliability of healthcare LLMs.\nTan et al. (2024) propose an approach that integrates multiple perspectives from scientific literature to evaluate conflicting arguments, thereby improving LLM reasoning. Similarly, Akhtar et al. (2022) introduce PubHealthTab, a table-based dataset designed for validating public health claims against noisy evidence, while Sarrouti et al. (2021) present HEALTHVER, a dataset tailored for evidence-based fact-checking of health-related claims. These structured benchmarks provide a foundation for assessing and refining the reliability of LLM-generated medical information.\nBeyond dataset-driven validation, self-correction mechanisms have been explored to improve LLM truthfulness. Gou et al. (2024) introduce CRITIC, a framework inspired by human fact-checking practices, enabling LLMs to validate and refine their responses through iterative feedback and evaluation. Expanding on automated fact-checking, Cohen et al. (2023) propose a cross-examination framework, where an examiner LLM identifies inconsistencies through multi-turn interactions with the original model. Unlike fully automated verification pipelines, CRITIC incorporates human-like evaluation strategies, enhancing the trustworthiness of fact-checking in medical contexts.\nOverall, these studies advance the factual accuracy and transparency of LLMs in healthcare by introducing structured benchmarks, iterative validation processes, and automated fact-checking strategies. By incorporating these approaches, researchers can enhance the reliability of medical LLMs, ensuring they deliver more accurate, evidence-based insights to support clinical decision-making."}, {"title": "3.2 Privacy", "content": "LLM-based healthcare applications pose significant privacy risks due to their ability to memorize and reproduce sensitive patient data (Das et al., 2024). Unauthorized data exposure can lead to confidentiality breaches, ethical concerns, and compliance violations (Pan et al., 2020). Addressing these risks requires privacy safeguards at different stages of model development.\nA major challenge is unintended data retention and leakage, where LLMs memorize fine-tuning data, increasing re-identification risks. Studies show that domain-specific LLMs, such as Medalpaca, can retain sensitive data, making privacy breaches more likely (Yang et al., 2024a). Additionally, adversarial attacks like prompt injection and inference attacks can further exploit these vulnerabilities, as demonstrated by the SecureSQL benchmark (Song et al., 2024).\nTo mitigate these risks, pre-training privacy safeguards focus on de-identification. Altalla' et al. (2025) assess GPT-3.5 and GPT-4 in clinical note de-identification and synthetic data generation. Similarly, Liu et al. (2023b) propose a GPT-4-enabled framework for masking private information while maintaining text structure. However, de-identification remains imperfect, as attackers may infer sensitive details from anonymized text.\nDuring fine-tuning, techniques such as federated learning (Zhao et al., 2024) and differential privacy (Singh et al., 2024), as highlighted by Liu et al. (2024), play a crucial role in safeguarding patient data. Federated learning enables decentralized training without sharing raw data, but it demands high computational resources. Differential privacy adds noise to protect sensitive information but can reduce model accuracy.\nAdversarial defenses remain limited. The SecureSQL benchmark (Song et al., 2024) highlights LLM vulnerabilities to structured query attacks. While chain-of-thought (COT) prompting offers partial mitigation, it does not eliminate the risk of data exposure.\nResearchers address privacy concerns in healthcare LLMs through two primary approaches: de-identification, which alters real data to prevent re-identification, and synthetic data generation, which creates artificial data to eliminate reliance on sensitive patient information. While these strategies enhance privacy protection and maintain model effectiveness, challenges remain in long-term memorization control and adversarial robustness, requiring further research to strengthen data security and prevent unintended information retention."}, {"title": "3.3 Safety", "content": "Ensuring the safety of LLMs in healthcare is critical, as these models must not generate harmful responses. A key safety concern, noted by Han et al. (2024d), is that modifying just 1.1% of a model's weights can embed persistent biomedical inaccuracies while maintaining overall performance. This highlights the need for rigorous validation mechanisms and safety assessments to prevent misleading medical information before clinical use.\nTo systematically assess safety, MedSafetyBench (Han et al., 2024b) was introduced as the first benchmark designed to evaluate LLM safety in medical contexts. It includes 1,800 harmful medical queries alongside safety-optimized responses generated using advanced LLMs and adversarial techniques. Results indicate that publicly available medical LLMs fail to meet safety standards, but fine-tuning with MedSafetyBench significantly improves safety without compromising performance.\nA major challenge is that adversarial actors can manipulate LLMs to generate unsafe outputs, while excessive safety alignment may induce hallucinations. To address this, Das and Srihari (2024) propose UNIWIZ, a two-step framework that unifies safety alignment and factual knowledge retrieval. Their safety-priming approach synthesizes safety-focused training data, while a retrieval mechanism ensures that model outputs remain factually accurate. Models fine-tuned on UNIWIZ outperform larger state-of-the-art instruction-tuned models across multiple safety and accuracy metrics.\nAnother key contribution to safety alignment is from Han et al. (2024c), who provide the first comprehensive safety evaluation for medical LLMs (MedLLMs). They define key concepts of medical safety and alignment and introduce Med-Harm, a dataset designed to evaluate both general and medical-specific risks. This dataset assesses how well LLMs handle harmful medical questions, ensuring they adhere to safety and ethical standards in medical AI.\nFurther advancing safety assessments, Kanithi et al. (2024) introduce MEDIC, a multi-dimensional trustworthiness evaluation framework that systematically assesses medical LLMs across critical dimensions of clinical competence including medical reasoning and clinical safety.\nThese studies collectively offer a multi-faceted approach to LLM safety in healthcare. MedSafetyBench (Han et al., 2024b) provides a standardized benchmark for safety evaluation and fine-tuning, while UNIWIZ (Das and Srihari, 2024) introduces a structured framework that prevents hallucinations while reinforcing safety. Han et al. (2024c) focus on comprehensive safety alignment, establishing Med-Harm to evaluate domain-specific risks. Lastly, MEDIC (Kanithi et al., 2024) offers a holistic evaluation franmework for improving practical application of LLMs in clinical settings. Together, these efforts contribute to a more rigorous and systematic framework for assessing and improving LLM safety in medical applications."}, {"title": "3.4 Robustness", "content": "Enhancing the robustness of LLMs is crucial for their reliability in healthcare applications. A key approach involves developing adversarial test samples tailored to the medical domain, such as synthetic anomaly cases (Yuan et al., 2023) and boundary stress testing (Wang et al., 2024), to assess model resilience. However, creating clinically meaningful adversarial samples presents unique challenges, as Alberts et al. (2023) highlight the need to align adversarial testing methods with the complexities of real-world medical data, where medical dependencies must be accounted for.\nIn addition to adversarial testing, uncertainty quantification is another important avenue to improve robustness. LLM-TTA (O'Brien et al., 2024) explores test-time adaptation techniques to enhance model performance on rare or unfamiliar cases, which are common in medical diagnostics. Unlike adversarial robustness, which focuses on resistance to manipulated inputs, uncertainty quantification aims to identify when models are likely to be incorrect, providing a complementary safety mechanism.\nAnother critical question is whether benchmark performance truly reflects medical robustness. MedFuzz (Ness et al., 2024) challenges assumptions in MedQA by modifying questions to test if models rely on rigid, dataset-specific patterns rather than genuine clinical reasoning. This research exposes vulnerabilities in LLMs, revealing that subtle changes in input can significantly impact performance, raising concerns about their reliability in dynamic medical environments.\nInstruction robustness is also a growing concern. Ceballos-Arroyo et al. (2024) examine how variations in medical instructions affect performance across different LLMs, finding that specialized medical models may be more fragile than general-purpose models when instructions are reworded. This counterintuitive result suggests that excessive domain adaptation may decrease flexibility and reduce robustness.\nAdversarial vulnerabilities also pose direct security risks. Yang et al. (2024b) investigate two adversarial attack strategies across medical tasks using real patient data, demonstrating that fine-tuned models are especially vulnerable to poisoning attacks that subtly alter learned weights. While adversarial data does not always degrade general performance, it can introduce dangerous biases into specific medical predictions, making early attack detection a priority.\nTo protect against adversarial manipulations, Tang et al. (2024) introduce Secure Your Model, a framework that strengthens LLM robustness with cryptographic prompt authentication. This mechanism ensures that only verified and secure prompts are processed, mitigating vulnerabilities associated with prompt injections and adversarial attacks, and reducing the risk of model exploitation in medical contexts.\nWhile all these studies address LLM robustness, they differ in their primary focus. MedFuzz (Ness et al., 2024) and Ceballos-Arroyo et al. (2024) expose vulnerabilities in existing benchmarks and task instructions, questioning whether current evaluation methods truly measure robustness or merely reflect dataset biases. In contrast, Yang et al. (2024b) and Alberts et al. (2023) highlight adversarial threats, demonstrating how medical LLMs can be subtly manipulated, raising concerns about their security in real-world applications. Meanwhile, LLM-TTA (O'Brien et al., 2024) takes a different approach, focusing on uncertainty quantification rather than adversarial resistance to enhance reliability in handling unfamiliar cases. Secure Your Model (Tang et al., 2024) provides an additional security layer by introducing proactive adversarial defenses through prompt protection mechanisms, ensuring resilience against manipulation risks.\nThese studies highlight that robustness is a multifaceted challenge, requiring advancements in evaluation methods and defensive mechanisms. Ensuring LLMs can handle adversarial scenarios, integrate domain knowledge, and adapt to language variations is crucial for their safe deployment in healthcare. Strengthening robustness through testing and resilience-building enhances the trustworthiness of medical LLMs, making them more reliable in complex clinical settings."}, {"title": "3.5 Fairness and Bias", "content": "Ensuring fairness in LLMs is crucial in healthcare, where biased models can result in unequal treatment outcomes. Research has highlighted biases in clinical data and practice related to race, gender, and disability. For example, Omiye et al. (2023) examine the potential for harmful or inaccurate race-based content in LLMs, while Zack et al. (2024) discuss how language models encode societal biases that can affect healthcare outcomes. These studies underscore the need for fairness in LLM development to ensure equitable healthcare delivery.\nEfforts to address bias focus on both detection and mitigation. Swaminathan et al. (2024) offers an automated method for detecting race-based medicine stereotypes, while BiasMedQA (Schmidgall et al., 2024) benchmarks cognitive biases in medical tasks across multiple models, revealing varying bias resilience. Mitigation strategies, such as bias education, and one-shot and few-shot bias demonstrations, are proposed to reduce but not fully eliminate bias. Pfohl et al. (2024) introduce frameworks for assessing health equity-related harms in LLMs, including EquityMedQA, a dataset for equity-focused testing. Additionally, Wei et al. (2024) distinguishe between intrinsic fairness, rooted in model training, and behavioral fairness, which relates to model operation in real-world applications, advocating for both to ensure equitable outcomes.\nBias is also explored in closed-source models. Zack et al. (2024) evaluate racial and gender biases in clinical scenarios, finding that models often amplify societal biases. Similarly, Adam et al. (2022) show that biased AI recommendations can affect emergency decisions, while Yang et al. (2024c) identify healthcare disparities in model predictions based on patient demographics. For open-source LLMs, techniques such as reinforcement learning with clinician feedback (Zack et al., 2024) and data augmentation during pre-training (Parray et al., 2023) enhance training data quality to reduce bias.\nIn contrast, for closed-source models, where internal representations are inaccessible, strategies like instruction fine-tuning (Singhal et al., 2023) and prompt engineering (Schmidgall et al., 2024) are employed to improve fairness in outputs.\nFurther, Zhang et al. (2020) investigate how LLM embeddings can encode biases, particularly in clinical tasks, and applies adversarial debiasing to mitigate disparities. Lin and Ng (2023) identify cognitive biases in BERT, while Ke et al. (2024) use a multi-agent framework to explore how LLMs can mitigate biases in clinical decision-making. Additionally, Zhu et al. (2023) introduce CI4MRC, a method that addresses name-related bias in Machine Reading Comprehension (MRC) tasks by applying a causal interventional paradigm.\nIn addressing bias, solutions vary depending on whether the bias is at the individual level (e.g., name-related information) or dataset level (e.g., biased racial distributions). Open-sourced LLMs benefit from direct accessibility, enabling robust interventions to mitigate bias. In contrast, closed-sourced models, due to their inaccessibility, rely on methods such as instruction fine-tuning and external post-processing tools to refine model outputs and reduce bias."}, {"title": "3.6 Explanability", "content": "The lack of explainability in LLMs poses a significant barrier to building trust with clinical practitioners, thereby restricting their adoption in real-world healthcare systems. To address this challenge, efforts in clinical practice and research have prioritized improving the transparency of LLMs by developing more effective explanation mechanisms and incorporating human oversight. For instance, Shariatmadari et al. (2024) enhance biomedical applications by integrating knowledge graphs with language models and visualizing attention probabilities to provide clear and interpretable explanations for model predictions. Elsborg and Salvatore (2023) employ local explanation models to generate intuitive, case-specific insights, further advancing the explainability of LLMs and fostering trust in their medical applications.\nAnother critical area of research focuses on medical imaging explainability. Ghosh et al. (2023) propose a method that iteratively decomposes a black-box (BB) model into interpretable expert models and a residual network, where expert models specialize in specific data subsets and explain their reasoning using First-Order Logic (FOL).\nEfforts to improve reasoning and multimodal integration in LLMs have also gained momentum. MedViLaM (Xu et al., 2024) and MedThink (Gai et al., 2024) focus on improving medical understanding and reasoning by integrating both textual and visual data for complex medical tasks such as question answering and medical image classification, aiming to provide a more holistic view for decision-making. In contrast, MedExQA (Kim et al., 2024) prioritizes explainability in medical QA systems by providing multiple explanations for its responses. Causal Graphs Meet Thoughts (Luo et al., 2025) and Retrieval and Reasoning on KGs (Ji et al., 2024) explore how to enhance complex reasoning in LLMs through knowledge graphs (KGs), helping these models retrieve relevant information from structured sources to answer questions more effectively and with better justification.\nFor more specialized applications, TOSRR (Liu et al., 2025) introduce tree-organized self-reflective retrieval, which aims to improve performance in the niche area of Traditional Chinese Medicine (TCM), combining LLMs with self-reflection techniques for more accurate and contextually relevant responses. Studies like DDCoT (Zheng et al., 2023) and Layered Chain-of-Thought Prompting (San-"}, {"title": "4 Future Directions", "content": "We have reviewed key trust challenges in LLMs and existing solutions. This section highlights current limitations and proposes future directions.\nWhile efforts to enhance LLM truthfulness in healthcare have advanced, gaps remain, including limited adaptability in hallucination mitigation and weak source attribution. Future research should prioritize model-agnostic post-processing, improved self-correction, enhanced uncertainty quantification, and real-time fact-checking via structured knowledge integration.\nExisting privacy safeguards, such as de-identification and federated learning, remain imperfect. Strengthening de-identification methods, reinforcing federated learning defenses, refining differential privacy, and exploring homomorphic encryption and real-time audits are crucial next steps.\nSafety remains a pressing concern due to adversarial attacks and excessive safety alignment-induced hallucinations. Future work should focus on robust validation mechanisms, refined safety alignment strategies, and comprehensive evaluation frameworks.\nImproving robustness remains critical, with adversarial testing and uncertainty quantification methods needing to better handle medical data complexities. Future research should focus on clinically relevant adversarial tests, enhancing uncertainty techniques, and improving instruction robustness.\nDespite progress in fairness, key gaps persist, including the need for comprehensive bias assessments and real-world testing of mitigation strategies. Standardizing fairness metrics, conducting real-world evaluations, and assessing long-term impacts on healthcare equity are critical for progress.\nAdvancements in explainability have yet to bridge significant gaps. Future research should focus on integrating explainability into clinical workflows, developing interactive explanations, and improving multimodal integration to enhance transparency and trust.\nRecently, multi-agent frameworks like TriageAgent (Lu et al., 2024) have been introduced to streamline complex clinical tasks through agent collaboration. By harnessing their capabilities, we can embed trustworthiness mitigation and evaluation within a multi-agent system, enabling proactive monitoring and intervention across key areas such as truthfulness, privacy, robustness, fairness, and explainability in healthcare LLMs.\nAddressing these gaps will ensure that LLMs can be effectively integrated into healthcare systems, improving their reliability, privacy, safety, fairness, and transparency."}, {"title": "5 Conclusion", "content": "The integration of LLMs into healthcare holds great promise, but realizing their full potential requires addressing the critical challenges outlined in this survey. A key concern is truthfulness, as inaccuracies in medical LLMs pose serious risks to patient safety, making their detection and mitigation an ongoing research priority. Equally vital are privacy, safety, robustness, fairness, and explainability, ensuring responsible deployment in real-world clinical settings.\nWhile existing solutions show progress, much work remains to enhance the reliability, transparency, and ethical implications of LLMs in healthcare. Future research must refine these areas, balancing performance with trustworthiness while preventing LLM-based systems systems from worsening healthcare disparities. Comprehensive benchmarks, cross-disciplinary collaboration, and model accountability frameworks will be essential. Additionally, regulatory oversight and ethical guidelines must ensure LLM applications align with medical standards and patient rights.\nUltimately, achieving safe and equitable AI-driven healthcare will require ongoing efforts to improve both technical capabilities and societal frameworks."}, {"title": "Limitations", "content": "This survey provides a comprehensive overview of the challenges associated with LLMs in healthcare, but it primarily focuses on existing methodologies, leaving out emerging technologies that could address these issues in new ways. It also lacks practical insights into the real-world implementation of these solutions, such as deployment challenges, cost considerations, and system integration, which would make the findings more applicable to healthcare settings.\nWhile the paper addresses privacy and safety, it does not fully explore broader ethical issues like informed consent, patient autonomy, and human oversight. Additionally, the survey focuses on current research without delving into the long-term societal and health impacts of LLM deployment, such as changes in doctor-patient relationships, patient trust, and healthcare workflows."}]}