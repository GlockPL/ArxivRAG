{"title": "Manual Verbalizer Enrichment for Few-Shot Text Classification", "authors": ["Quang Anh Nguyen", "Nadi Tomeh", "Mustapha Lebbah", "Thierry Charnois", "Hanene Azzag", "Santiago Cordoba Mu\u00f1oz"], "abstract": "With the continuous development of pre-trained language models, prompt-based training becomes a well-adopted paradigm that drastically improves the exploitation of models for many natural language processing tasks. Prompting also shows great performance compared to traditional fine-tuning when adapted to zero-shot or few-shot scenarios where the number of annotated data is limited. In this framework, the role of verbalizers is essential, as an interpretation from masked word distributions into output predictions. In this work, we propose MaVEN, an approach for verbalizer construction by enrichment of class labels using neighborhood relation in the embedding space of words for the text classification task. In addition, we elaborate a benchmarking procedure to evaluate typical baselines of verbalizers for document classification in few-shot learning contexts. Our model achieves state-of-the-art results while using significantly fewer resources. We show that our approach is particularly effective in cases with extremely limited supervision data.", "sections": [{"title": "Introduction", "content": "Fine-tuning PLM (Devlin et al., 2019a; Zhuang et al., 2021; Brown et al., 2020) resulted in large improvements in various NLP tasks. Classic approaches replace the PLM's output layer with a task-specific head and fine-tune the entire model (Devlin et al., 2019a; Liu et al., 2019; Raffel et al., 2020). However, additional classification layers import a great amount of randomly initialized parameters that need a sufficient amount of labeled data to be trained. Classical fine-tuning, therefore becomes inapplicable for few-shot or zero-shot scenarios (Yin et al., 2019; Zhang et al., 2023).\nPrompting has become a novel paradigm where downstream tasks are transformed to suit the pre-training objective. Prompt-based fine-tuning allows to exploit PLMs' knowledge while reducing the gap between pre-training and fine-tuning (Petroni et al., 2019; Chen et al., 2022). In this framework, templates and verbalizers (Schick and Sch\u00fctze, 2021a; Gao et al., 2021) are crucial elements to map between task-specific inputs and labels, to textual data for the LM. For example, given a piece of text:\nx = \"Dollar rises against euro...\"\nThe task is to predict if this text belongs to which class among politics, sports, science, or economics. A template T first transforms the given text into a cloze question. For instance, one may choose for this task:\nT(x) = \"66 news: Dollar rises against euro...\"\nThe task of predicting labels without conceptual meaning is transformed into identifying whether the most probable choice for the masked position is \u201cpolitics\u201d, \u201csports\u201d, \u201cscience\u201d or \u201ceconomics\". This task, namely masked language modeling aligns coherently with the pre-training of a variety of masked LMs, notebly BERT (Devlin et al., 2019b), RoBERTa (Zhuang et al., 2021).\nA masked LM takes the wrapped text, marks the missing position with its MASK token, and produces probabilities for the masked token over the vocabulary. Ideally in this case, one would expect the probability of the word \u201ceconomics\" to be higher than that of \"sports\". This straightforward approach maps each class to a single word, its textual name. In general, a verbalizer refers to a mapping from the label space to the vocabulary space, where each label is mapped to multiple vocabulary tokens.\nIn many cases, verbalizers are defined manually using human knowledge of the downstream task, to"}, {"title": "Related Works", "content": "Prompt-based fine-tuning In this framework, the input is wrapped with a task-specific template to reformulate the classification task as language modeling as described in section 1. The verbalizer then transforms the distribution of the MASK token into label prediction (see section 3 for formal definitions). The choice of textual templates and verbalizer, have a significant influence on the classification performance (Gao et al., 2021).\nPET and iPET (Schick and Sch\u00fctze, 2021a,b) use task-specific manual templates and verbalizers that work efficiently. However, their construction requires both domain expertise of downstream tasks and understanding of biases in the MASK distribution produced by the PLMs. Otherwise, the search process for an optimal template and verbalizers may be computationally exhaustive with a large number of classes. Meanwhile, (Lester et al., 2021; Liu et al., 2022; Li and Liang, 2021) propose to freeze the PLM and instead optimize prompt tokens. Despite being human-independent and storage-saving, continuous prompts have only been studied in data-abundant scenarios, and produce tokens that are hard to interpret. Here, we study textual templates instead and focus on the search for label words for the verbalizer.\nEnrichment of manual verbalizer Previous works also propose methods to improve the semantics of label words for a given manual verbalizer. KPT (Hu et al., 2022) incorporates external knowledge into the verbalizers, along with multiple steps of refinement and calibration to obtain words with wide coverage of given classes. Still, such knowledge bases may not always be available. Therefore, we are motivated to derive a method to improve the manual verbalizer independently from additional resources. On the other hand, NPPrompt (Zhao et al., 2023) searches for cognates of initial manual words using the embedding layer of the same PLM. This approach attains greater coherence in later PLM fine-tuning."}, {"title": "Methodology", "content": "Let M be a language model with vocabulary V. Following (Schick and Sch\u00fctze, 2021a,b), we define the template - verbalizer pair. Let (x, y) be an example of the classification problem, where x represents one or many sentences and y is its label in the label set Y. A template T maps x into a masked sequence T(x) of tokens in V\u222a {MASK}. A verbalizer v : Y \u2192 P(V) maps each label to a set of words characterizing the class (called label words). The probability of the label conditioned on the input is then modeled by the logits of its label words conditioned on the masked sequence:\np(y|x) \\propto exp\\left(\\frac{1}{|v(y)|} \\sum_{w \\in v(y)} M(w|T(x)) \\right)\\tag{1}\nWhere M(w|T(x)) denotes the logit of MASK being predicted as w by the LM conditional on the masked sequence T(x)."}, {"title": "Baselines", "content": "Manual Label words can be predefined manually from users' knowledge of classes (Gao et al., 2021; Schick and Sch\u00fctze, 2021a). To minimize the necessity of domain expertise, here the manual verbalizers derive directly from the class names.\nSoft WARP (Hambardzumyan et al., 2021) proposes to represent each label y by a prototype vector vy instead of concrete words, initialized with static embeddings of the manual label words and optimized alongside the PLM.\nAuto Among automatic methods, PETAL (Schick et al., 2020) allows identifying words suitable to represent classes from training data. Consider the classification problem as many one-vs-rest binary problems to find label words for each class separately. For each label, PETAL takes the top kauto words that maximize the likelihood ratio of positive examples and minimize that of negative examples.\nIn addition to applying verbalizers to small masked LMs, we also evaluate the performance large language models (LLMs) as follows.\nInstruction tuned LLM (Instruct) Instruction tuning is an effective technique to enhance the capabilities and controllability of LLMs (Zhang et al., 2024; Wei et al., 2022). It involves further training of the generative LLMs using textual (instruction, output) pairs. Numerous instruction-tuned LLMs, including InstructGPT (Ouyang et al., 2022), Flan-T5 (Chung et al., 2022), TO (Sanh et al., 2022), BLOOMZ (Muennighoff et al., 2023), etc. achieve remarkable zero-shot performance. They mainly differ in their backbone model and their instruction dataset construction.\nWe use Mistral-7B-Instruct-v0.2, an instruction-tuned version of Mistral-7B-v0.2 (Jiang et al., 2023), for its reasonable size. Mistral is publicly available and achieves state-of-the-art performance compared to similar-sized LLMs. The prompt is adapted from P3 (Bach et al., 2022) for zero-shot inference. For few-shot inference, (Dong et al., 2023), in-context learning (ICL) is combined with the instruction, where labeled examples are included in the prompt as a demonstration. Due to machine memory limitations, we only apply ICL for N 32."}, {"title": "Manual Verbalizer Enrichment by Nearest Neighbors' Embeddings", "content": "In this paper, we propose Manual Verbalizer Enrichment by Nearest Neighbors' Embeddings (MaVEN), an extended formulation of NPPrompts (Zhao et al., 2023), adapted for prompt-based fine-tuning. Noting that the probability score that the LM assigns to a specific topic is dispersed over multiple label words, we hypothesize that the manual verbalizer captures only a part of this mass and thus is sensitive to the choice of label words. Our motivation therefore is to automatically extend the verbalizer to capture more semantic information by including semantically related words.\nIn most practical scenarios, a natural manual verbalizer can often be obtained using the names of classes, as class names naturally encode the semantic meaning of texts belonging to the class. We assume that for our classification problem, let v be the initial manual verbalizer. In our case, v(y) includes words extracted directly from the name of the class y. Let E be a word embedding function, the word embedding layer of the LM for example. For each core word wo \u2208 v(y), we define the neighborhood of wo as:\nN_k(w_o) = \\{w_o\\} \\cup top-k [s(w_o, w)] \\tag{2}\nw\nWhere s is the cosine similarity in this embedding space E.\nWe enlarge the verbalizer v(y) as the union of neighborhoods of all core words:\nv(y) = \\bigcup_{w_o \\in v(y)} N_k(w_o) \\tag{3}\nThe hyperparameter k represents the size of the neighborhood in the embedding space around the initial core words. In our experiments, without specifying differently, we take k = 15."}, {"title": "Experiments", "content": "Five datasets (section 4.2) are considered context for three baselines (section 3) and MaVEN in few-shot prompt-based fine-tuning. For each dataset, from the original training set, we sample a labeled set D of cardinality N. For each run, split D into two halves: Dtrain is used for fine-tuning with the template - verbalizer pair and Dvalid for validation (Zheng et al., 2022). The best checkpoint is retained from the score obtained on the validation set. Details of hyperparameters is in appendix A.\nThe underlying pre-trained language model (PLM) is ROBERTa-large (Liu et al., 2019) as in (Schick et al., 2020) for datasets in English, or CamemBERT-large (Martin et al., 2020) for datasets in French. We report the average and standard deviation of accuracy from 3 repetitions with different samplings of D, to evaluate the result variation with different training data instances.\nOur implementation is based on the toolkit Open-Prompt (Ding et al., 2022) and the Transformers package (Wolf et al., 2020). Experiments are executed on two types of GPUs: NVIDIA Tesla V100 and NVIDIA Quadro RTX 5000."}, {"title": "Datasets and templates", "content": "Our experiments are done on three public English datasets and two datasets in French (table 1). For each dataset, four textual templates are created. The manual verbalizers for each dataset can be found in appendix B.\nAG AG's News (Zhang et al., 2015). Given a headline x, a news needs to be classified into one of 4 categories. For this dataset:\nTo(x) = MASK news: x\nT\u2081(x) = x This topic is about MASK.\nT2(x) = [Category: MASK] x\nT3(x) = [Topic: MASK] x\nDBpedia The DBpedia ontology classification dataset (Zhang et al., 2015) is constructed by picking 14 non-overlapping classes from DBpedia 2014. Given a title x\u2081 and its description x2, the task is to predict the category of the object in the title.\nTo(x) = x1x2 In this sentence, x1 is MASK.\nT1(x) = X1X2 X1 is MASK.\nT2(x) = x1X2 The category of x1 is MASK.\nT3(x) = x1x2 The type of x1 is MASK.\nYahoo Yahoo! Answers (Zhang et al., 2015) is a text classification dataset of questions from Yahoo!. Given a question (title and content) and its answer, one of ten possible categories has to be assigned. For a concatenation x of the question title, question content and the answer, we define:\nTo(x) = MASK question: x.\nT\u2081(x) = x This topic is about MASK.\nT2(x) = [Topic: MASK] x.\nT3(x) = [Category: MASK] x.\nMLSUM Fr originated from MultiLingual SUMmarization (Scialom et al., 2020), a large-scale dataset from online newspapers. From this base, the French split is preprocessed and annotated for the task of topic classification by grouping the topic tag into one of ten categories\u00b9.\nFrN This real-world private dataset in French is provided by our collaborator in a private company, consisting of press articles. The dataset contains over 5 million articles with silver multi-label annotated among 28 sectors by the data aggregator Factiva2. Our collaborators have manually annotated 1,364 articles, of which 1,048 articles belonging to the 10 most frequent sectors are used for experiments in this paper.\nFor these last two, let x is the concatenation of the title, the summary, and the body text, and:\nTo(x) = Nouvelle MASK: x\nT\u2081(x) = Actualit\u00e9 MASK: x\nT2(x) = MASK: x\nT3(x) = [Cat\u00e9gorie: MASK] x"}, {"title": "Main Results", "content": "Table 2 shows the result over five datasets and three baselines, for different quantity of data N.\nFor zero-shot learning, we observe that MaVEN achieves similar performance to the manual verbalizers, with the exception of FrN. We hypothesize that in this case, the neighborhoods of the class names do not model sufficiently the vocabulary of their classes without finetuning.\nFor extremely low-data settings, such as N\u2208 {32, 64}, we observe a clear superiority of MaVEN. Compared to the manual verbalizer, MaVEN achieves an improvement of 2.3 on DBpedia, 10.0 on FrN, and 2.4 on MLSUM Fr for N = 32. In other cases for N \u2208 {32,64}, MaVEN ranks as either the best or the second best among all verbalizers. For larger values of N, the gap between MaVEN manual verbalizer declines. Given more and more training data, the LM learns to attribute the probability mass only to the core word, and thus, neighbor words become less useful.\nIn summary, MaVEN consistently achieves the highest average score across five datasets all few-shot learning contexts. It shows an improvement of 2.9 in average over the manual verbalizer for N = 32. For the zero-shot case, it slightly under-performs the manual verbalizer.\nAdditionally, we remark that for N \u2265 64, the automatic verbalizer perform similarly, the manual verbalizer for all datasets (with N > 32 for AG and N\u2265 128 for others). The main reason for this evolution is that the automatic algorithm mines for label words from likelihood on training data. With very few labeled data, the evaluation of this likelihood is less accurate. Notably, on AG and MLSUM Fr, the automatic verbalizer exceeds the manual verbalizer and MaVEN, which suggests that initial words given by the manual verbalizer of these datasets are biased and less accurate than words extracted from the data.\nCompared to Instruct and ICL applied on Mistral, notice that combining RoBERTa or Camembert with verbalizers (MaVEN included) achieves a similar, sometimes higher, level of accuracy, despite having about 20 times fewer parameters (355M vs 7B). This finding encourages further research into optimizing smaller LMs to"}, {"title": "Impact of the Neighborhood Size k", "content": "Motivated by remarks in (Nguyen et al., 2024) that using more label words produces stronger verbalizers, in this section, we inspect the impact of the parameter k for our MaVEN.\nIn practice, a fixed value between 10 and 15 guarantees a decent level of performance. We also observe that the dependence on k is minor compared to the variation due to textual templates, discussed in section 4.5."}, {"title": "Effectiveness of Ensemble Models", "content": "In figure 1, we assess outcomes by utilizing individual templates and by three different methods of ensemble. Generally, ensemble models yield more accurate predictions compared to using the most efficient template alone. Ensemble approaches not only improve prediction accuracy but also enhance stability and reduce the reliance on prompt selection, which typically relies on large validation sets (Perez et al., 2021), especially when individual templates show significant performance variations. Additionally, ensemble models are generally less sensitive to changes in the neighborhood size k, as discussed in section 4.4.\nAmong the three methods, voting tends to be less effective than probability and logit averaging. However, this difference is minimal when compared to the overall improvement achieved by assembling individual templates."}, {"title": "Effect of the Embedding Space E", "content": "In this section, we evaluate the influence of the embedding space E to MaVEN. The embedding space intervenes in two manners: the choice of the neighborhood Nk (wo) and the initialization of weights q via s(wo, w) (section 3). The vanilla MaVEN utilizes the same embedding layer as the token embedding layer of the LM (ROBERTa-large to be precise) as suggested by (Zhao et al., 2023), assuming the same embeddings as the fine-tuned LM yields more coherence."}, {"title": "Sensitivity to the Initial Seed Label Words", "content": "As described in section 3, MaVEN relies on the manual label words used for initialization. The seed wo determines the neighborhood Nk(wo), which in turn influences the selection of additional label words and their initial weights.\nWe propose a procedure to (i) find a reasonable initialization when manual seed words are not available and (ii) quantify the sensitivity of MaVEN's performance to varying initialization. First, we use the automatic verbalizer algorithm PETAL (section 3.1, Schick et al., 2020) to extract kauto label words for each class."}, {"title": "Conclusion", "content": "In this paper, we propose MaVEN, a novel method to extend the manual verbalizer that is effective for few-shot learning via prompt-based fine-tuning of PLMs. By leveraging the neighborhood relationship in the embedding space of PLMs, MaVEN was able to identify words related to the topic title to construct verbalizers without the need for data or external knowledge. Experiments show that MaVEN outperforms other constructions of verbalizer for extremely few-shot contexts."}, {"title": "Discussion and Limitations", "content": "As an extension of the manual verbalizer, MaVEN requires initial core words that contain the semantics meaning of the class. Therefore, theoretically, MaVEN is not applicable if class names are not meaningful descriptions of the classes. In reality, however, class titles often fully capture class concepts, and we rarely encounter cases where class titles are unavailable. The practicality of our proposed method remains. Otherwise, a substitute is proposed in section 4.7. In traditional fine-tuning where data amount is not limited, data instances represent classes. Meanwhile, in few-shot or zero-shot learning cases, class titles are the alternative representation of classes instead of data instances as in traditional fine-tuning.\nThe formulation and construction of verbalizers studied in this work focus on masked LMs, exploited only in encoder mode. Meanwhile, recent released PLMS (GPT Brown et al., 2020, LLaMA Touvron et al., 2023, Falcon Almazrouei et al., 2023, etc.) are auto-regressive models that are more powerful on a variety of benchmarks. This opens the potential to adapt verbalizer constructions for generative models in decode mode, to exploit the rich knowledge incorporated in these large LMs.\nOur work includes datasets and verbalizers in English and French only. It is not guaranteed that the conclusions generalize well. Other works in other languages or more research on verbalizers with multi-lingual models can be explored."}, {"title": "Hyperparameters", "content": "For simplicity, most choices of hyperparameters are based on existing works and practical considerations."}]}