{"title": "Safe Beyond the Horizon: Efficient Sampling-based MPC with Neural Control Barrier Functions", "authors": ["Ji Yin", "Oswin So", "Eric Yang Yu", "Chuchu Fan", "Panagiotis Tsiotras"], "abstract": "Abstract-A common problem when using model predictive control (MPC) in practice is the satisfaction of safety specifications beyond the prediction horizon. While theoretical works have shown that safety can be guaranteed by enforcing a suitable terminal set constraint or a sufficiently long prediction horizon, these techniques are difficult to apply and thus are rarely used by practitioners, especially in the case of general nonlinear dynamics. To solve this problem, we impose a tradeoff between exact recursive feasibility, computational tractability, and applicability to \"black-box\" dynamics by learning an approximate discrete-time control barrier function and incorporating it into a variational inference MPC (VIMPC), a sampling-based MPC paradigm. To handle the resulting state constraints, we further propose a new sampling strategy that greatly reduces the variance of the estimated optimal control, improving the sample efficiency, and enabling real-time planning on a CPU. The resulting Neural Shield-VIMPC (NS-VIMPC) controller yields substantial safety improvements compared to existing sampling-based MPC controllers, even under badly designed cost functions. We validate our approach in both simulation and real-world hardware experiments.", "sections": [{"title": "I. INTRODUCTION", "content": "Model Predictive Control (MPC) is a versatile control ap- proach widely used in robotics applications such as autonomous driving [68], bio-inspired locomotion [8, 29], or manipulation of deformable objects [51], to name just a few. These methods address safety by incorporating state and control constraints into the finite-horizon optimization problem, ensuring that the system remains safe over the prediction horizon [8, 41, 51, 66]. However, safety of the system beyond the prediction horizon is often overlooked by practitioners, potentially leading to the violations of safety constraints at future timesteps.\nThis is a well-known problem in the field of MPC. The question of whether a sequence of safe control actions can always be found under an MPC controller has been studied extensively in the literature under the name of recursive feasibility [34, 44, 10, 26, 42]. A simple method of achieving recursive feasibility is by enforcing a control-invariant terminal set constraint at the end of the prediction horizon [34, 10]. However, it is difficult to find such a control-invariant set for general nonlinear systems. Methods such as bounding the system dynamics [10, 9, 20] often result in over-conservative sets, while methods that numerically solve the Hamilton-Jacobi"}, {"title": "II. RELATED WORK", "content": "Sampling-based MPC. Sampling-based MPC has become a popular alternative to traditional MPC methods that use gradient-based solvers, in part due to the advent of parallel computing and the recent advances in GPU hardware. By not using gradients, sampling-based MPC can be applied to any problem without having specific requirements on the problem structure. MPPI [68] is a popular sampling-based MPC approach that formulates a variational inference problem, then solves it in the case of the Gaussian distribution, having strong connections to stochastic optimal control [69] and maximum entropy control [60]. Separately, the Cross-Entropy Method (CEM) [58] has become popular in the reinforcement learning community [66], in part due to its simplicity. Both approaches were recently shown to be part of the VIMPC family of MPC algorithms [49]. Consequently, some works have looked at expanding the VIMPC family to include different choices of divergences [66] and sampling distributions beyond Gaussians [37, 49].\nSafety in Sampling-based MPC. Research efforts such as [77] and [3] assess the risk associated with uncertain areas in the state space during the exploration phase, and enhance MPPI's safety by incorporating a risk penalty into its cost function. While these methods empirically enhance safety, they lack formal assurances. Another class of MPPI alternatives, for instance, [69, 54], leverage an auxiliary tracking controller to follow the MPPI output trajectories, improving robustness against unforeseeable disruptions, but these improvements are limited when the simulation-to-reality gap is significant. More recently, the use of Control Barrier Functions (CBF) [63, 76] has offered formal safety guarantees for MPPI controllers."}, {"title": "III. PROBLEM FORMULATION", "content": "We consider the discrete-time, nonlinear dynamics\n$$x_{k+1} = f(x_k, u_k),$$\nwith state $$x \\in X \\subseteq \\mathbb{R}^{n_x}$$ and control $$u \\in U \\subseteq \\mathbb{R}^{n_u}$$. Let $$U^K$$ denote the set of control trajectories of length $$K$$. Following the MPC setup, we assume that a cost function $$J : U^K \\to \\mathbb{R}$$ encoding the desired behavior of the system is given. Moreover, we consider state constraints defined by an avoid set $$A \\subset X$$ described as the superlevel set of some specification function $$h$$, i.e.,\n$$A := \\{x \\in X \\mid h(x) > 0\\}.$$\nThe goal is then to find a sequence of controls $$u = \\{u_0, u_1, ..., u_{K-1}\\} \\in U^K$$ that minimizes the cost function $$J$$ while satisfying the system dynamics (1) and safety constraints $$x_k \\notin A$$ for all $$k \\geq 0$$.\nA. Variational Inference MPC For Sampling-based Optimization\nTo solve the above optimization problem, we deviate from traditional MPC solvers and use a variational inference MPC formulation to solve the problem via sampling-based optimization. To this end, we make use of the control-as- inference framework [38] to model the problem. Specifically, let $$o$$ be a binary variable that indicates \"optimality\" such that, for all controls $$u \\in U^K$$,\n$$p(o = 1 \\mid u) \\propto \\exp(-J(u)),$$\nWe assume a prior $$p_0(u)$$ on the control trajectory. The \u201coptimal\u201d distribution can then be obtained via the posterior distribution\n$$p(u \\mid o = 1) = \\frac{p(o = 1 \\mid u)p_0(u)}{p(o = 1)} = Z^{-1} \\exp(-J(u))p_0(u).$$\nwhere $$Z := \\int \\exp(-J(u))p_0(u) du$$ denotes the unknown normalization constant. Since sampling from $$p(u \\mid o = 1)$$ is intractable, we use variational inference to approximate the posterior $$p(u \\mid o = 1)$$ with a tractable distribution $$q_v(u)$$ parametrized by some vector $$v$$ by minimizing the forward KL divergence, i.e.,\n$$\\min_v D_{KL}\\left(p(u \\mid o = 1) \\|\\| q_v(u)\\right).$$\nIn the special case of $$q_v$$ being a Gaussian distribution with mean $$v$$ and a fixed control input covariance $$\\\\Sigma$$, intrinsic to the robotic system of interest [68], we can solve (5) in closed-form to obtain the optimal $$v^*$$ as (see Section B1 for details)\n$$v^* = \\mathbb{E}_{p(u \\mid o=1)}[u].$$"}, {"title": "C. Discrete-time Control Barrier Functions (DCBF)", "content": "A discrete-time control barrier function (DCBF) [76] asso- ciated to the avoid set $$A$$ is a function $$B : X \\to \\mathbb{R}$$ such that\u00b9\n$$B(x) > 0, \\quad \\forall x \\in A,$$\n$$B(x) < 0 \\implies \\inf_{u \\in U} B(f(x, u)) - B(x) \\leq -\\alpha(B(x)),$$\nwhere $$\\\\alpha$$ is an extended class-$$\\\\mathcal{K}$$ function [74]. As in [76], we restrict our attention to the class of linear extended class-$$\\\\mathcal{K}$$ functions, i.e.,\n$$\\alpha(B(x)) = \\alpha \\cdot B(x), \\quad \\alpha \\in (0, 1).$$The following theorem from [76] proves that a controller satisfying the condition (14b) renders the sublevel set $$S = \\{x \\mid B(x) < 0\\}$$ forward-invariant.\nTheorem 1 ([76, Property 3.1]). Any control policy $$\\\\pi : X \\to U$$ satisfying the condition\n$$B(f(x, \\\\pi(x))) - B(x) \\leq -\\alpha(B(x)),$$\nrenders the sublevel set $$S = \\{x \\mid B(x) < 0\\}$$ forward- invariant.\nHence, one way to guarantee recursive feasibility, and thus safety beyond the prediction horizon, is to enforce the condition (16) at every time step of the optimization problem. Another point to note is that $$S$$ is a control-invariant set, and thus the constraint $$B(x_k) \\leq 0$$ can be imposed as a terminal state constraint to guarantee recursive feasibility for MPC as is done classically [34, 10]. Thus, one can try to enforce these constraints by incorporating them into the cost function as in (11) [76], i.e., modify the cost according to one of the following options,\n$$J_{new}(u) = J(u) + C \\sum_{k=0}^K \\mathbb{1}\\{B(f(x, u))-B(x)>-\\alpha(B(x))\\},$$\n$$J_{new}(u) = J(u) + C \\sum_{k=0}^K \\left[B(f(x)) - B(x) + \\alpha(B(x))\\right]_{+},$$\nNote that we use the opposite sign convention as compared to [76]."}, {"title": "IV. NEURAL SHIELD VIMPC", "content": "In this section, we propose Neural Shield VIMPC (NS- VIMPC), a sampling-based MPC paradigm that efficiently samples trajectories using a DCBF modeled using a neural network. We illustrate the proposed NS-VIMPC algorithm in Fig. 2.\nA. Approximating DCBF Using Neural Policy Value Functions\nLet $$\\\\pi$$ : X \u2192 U. Define the policy value function Vh,\u03c0 as,\n$$V^{h,\\\\pi}(x_0) := \\max_{k \\geq 0} h(x_k).$$\nWe then have the following theorem.\nTheorem 2. Vh,\u03c0 satisfies (14a) and (14b) and is a DCBF.\nProof: From the definition of Vh,\u03c0 (19), we have that\n$$V^{h,\\\\pi}(x_k) \\geq h(x_k),$$\n$$V^{h,\\\\pi}(x_k) \\geq V^{h,\\\\pi}(f(x_k, \\\\pi(x_k))).$$\nUsing the definition of the avoid set A (2) and (20), it follows that Vh,\u03c0(x) > 0 for all x \u2208 A, satisfying the first condition (14a) of a DCBF. When Vh,\u03c0(xk) < 0, we have \u2212\u03b1(Vh,\u03c0(xk)) > 0, and (21) implies that,\n$$V^{h,\\\\pi}(f(x_k, \\\\pi(x_k))) - V^{h,\\\\pi}(x_k) \\leq 0 \\leq -\\alpha(V^{h,\\\\pi}(x_k)).$$\nSince $$\\\\pi(x_k) \\in U$$, this implies the second condition (14b). Thus, the policy value function Vh,\u03c0 is a DCBF.\nAlthough we have constructed a DCBF from (19), the challenge is that the policy value function Vh,\u03c0 cannot be easily evaluated at arbitrary states since the maximization in (19) is taken over an infinite horizon. To fix this, we train a neural network approximation Vr\u03b8 of Vh,\u03c0, extending the approach of [61] to the discrete-time case. To begin, we first rewrite (19) in a dynamic programming form,\n$$V^{h,\\\\pi}(x_0) = \\max\\left\\{\\max_{0 \\leq k < T} h(x_k), \\hat{V}^{h,\\\\pi}(x_T)\\right\\}.$$\nWe can then train a neural network Vr\u03b8 to approximate the value function Vh,\u03c0 by minimizing the loss\n$$\\mathcal{L}(\\theta) = \\mathbb{E}_{x_0} \\left\\|V_\\theta^{h,\\\\pi}(x_0) - \\max\\left\\{\\max_{0 \\leq k < T} h(x_k), \\hat{V}^{h,\\\\pi}(x_T)\\right\\} \\right\\|^2,$$\nover all states x0. One problem, however, is that the minimizer of (24) is not unique. For example, if h(x) \u2264 h for all x, then Vr\u03b8 = h is a minimizer of (24) but does not necessarily satisfy (19). To fix this, we follow the approach of [21, 59] and, inspired by reinforcement learning [62], introduce a discount factor \u03b3 \u2208 (0, 1) to define the discounted value function\n$$\\hat{V}_{\\theta}^{h,\\\\pi,\\\\gamma}(x_k) = \\max\\left\\{h(x), (1-\\gamma)h(x) + \\gamma \\hat{V}_{\\theta}^{h,\\\\pi,\\\\gamma}(x_{k+1})\\right\\},$$\nand the corresponding loss L,\n$$\\mathcal{L}(\\theta) = \\mathbb{E}_{x_k} \\left\\|V_\\theta^{h,\\\\pi,\\\\gamma}(x_k) - \\hat{V}_{\\theta}^{h,\\\\pi,\\\\gamma}(x_k)\\right\\|^2,$$\n$$\\hat{V}_{\\theta}^{h,\\\\pi,\\\\gamma}(x_k) = \\max\\left\\{h(x), (1-\\gamma)h(x) + \\gamma \\hat{V}_{\\theta}^{h,\\\\pi,\\\\gamma}(x_{k+1})\\right\\}.$$\nHence, one way to guarantee recursive feasibility, and thus"}, {"title": "V. SIMULATIONS", "content": "We first performed simulation experiments to better under- stand the performance of the proposed Neural Shield VIMPC (NS-VIMPC) controller. Although many sampling-based MPC controllers fall under the VIMPC family with different choices of the prior po and r, we choose to instantiate the MPPI algorithm (see Section B1 for details), and call the resulting controller Neural Shield-MPPI (NS-MPPI).\nBaseline methods. We compared NS-MPPI against the follow- ing sampling-based MPC methods.\nBaseline MPPI (MPPI) [68], which forward simulates a set of randomly sampled trajectories for optimal control.\nShield MPPI (S-MPPI) [76], which extends MPPI by taking h in (2) to be a DCBF and by adding the DCBF constraint violation into the cost as in (18).\nCEM [6], which samples trajectories similar to the baseline MPPI but with the weight w = 1 for only the k-lowest cost"}, {"title": "VII. LIMITATIONS", "content": "One limitation of our approach is that the DPNCBF is only an approximation of a DCBF. As such, the typical safety guarantees of DCBFs may not hold if the function is not a true DCBF. Moreover, as noted in Remark 4, the use of variational inference means that v* may not satisfy the state constraints despite the optimal distribution p(u | o = 1) having zero density at states that violate the state constraints. While theoretically we were only able to show that v* stays safe under the assumption that Usafe is convex, empirical results in Fig. 9 show that NS-MPPI can perform well despite the fact that Usafe is typically not convex."}, {"title": "VIII. CONCLUSION", "content": "In this study, we have adapted the policy neural control barrier function (PNCBF) to a discrete-time setting and"}]}