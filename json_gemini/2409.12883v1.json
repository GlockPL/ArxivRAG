{"title": "Improving Prototypical Parts Abstraction for Case-Based Reasoning Explanations Designed for the Kidney Stone Type Recognition", "authors": ["Daniel Flores-Araiza", "Francisco Lopez-Tiroa", "Cl\u00e9ment Larose", "Salvador Hinojosa", "Andres Mendez-Vazqueza", "Miguel Gonzalez-Mendoza", "Gilberto Ochoa-Ruiz", "Christian Daul"], "abstract": "The in-vivo identification of the kidney stone types during an ureteroscopy would be a major medical advance in urology, as it could reduce the time of the tedious renal calculi extraction process, while diminishing infection risks. Furthermore, such an automated procedure would make possible to prescribe anti-recurrence treatments immediately. Nowadays, only few experienced urologists are able to recognize the kidney stone types in the images of the videos displayed on a screen during the endoscopy. This visual recognition by urologists is also highly operator dependent. Thus, several deep learning (DL) models have recently been proposed to automatically recognize the kidney stone types using ureteroscopic images. However, these DL models are of black box nature and do not establish the relationship of the visual features they used to take the decision with the color, texture and morphological features visually analysed in biological laboratories to determine the type of extracted kidney stone fragments using the reference morphoconstitutional analysis (MCA) procedure. This contribution proposes a case-based reasoning DLmodel which uses prototypical parts (PPs) and generates local and global descriptors. The PPs encode for each class (i.e., kidney stone type) visual feature information (hue, saturation, intensity and textures) similar to that used by biologists during MCA. The PPs are optimally generated due a new loss function used during the model training. Moreover, the local and global descriptors of PPs allow to explain the decisions (\"what\" information, \"where in the images\") in an understandable way for biologists and urologists. The proposed DL model has been tested on a database including images of the six most widespread kidney stone types in industrialized countries. The overall average classification accuracy was 90.37\u00b10.6%. When comparing this results with that of the eight other DL models of the kidney stone state-of-the-art, it can be seen that the valuable gain in explanability was not reached at the expense of accuracy which was even slightly increased with respect to that (88.2 \u00b1 2.1%) of the best method of the literature. These promising and interpretable results also encourage urologists to put their trust in AI-based solutions.", "sections": [{"title": "1. Introduction", "content": "1.1. Medical context\nUrolithiasis (i.e., renal calculus formation) is a world-wide issue (Quhal and Seitz (2021)) entailing large expendi-tures on health systems (Roberson et al. (2020)). As reported in (Kasidas et al. (2004)), urolithiasis affects at least 10% of the population in industrialized countries and the risk of recurrence reaches up to 40% in North America.\nKidney stones are aggregations of crystals that form in the urine. When their diameter becomes large (a few mil-limeters), kidney stones can remain blocked in the urinary tract (e.g., in a kidney calyx or a ureter) and cause severe pain. Kidney stones are classified into seven main types and twenty three sub-types (each type includes a given number of sub-types) according to their crystalline structure and biochemical composition. The formation causes depend on numerous risk factors such as patient genetics, age, weight, and sex, as well as the environment (warm or cold climate), lifestyle, comorbidity, or iatrogenic infections. A detailed description of the kidney stone types and sub-types, as well as the etiology (i.e., the causes of urolithiasis) can be found in (Cloutier et al. (2015)).\nUreteroscopes (flexible endoscopes with a CCD matrix and optics on their distal tip) are used to display kidney stones on a screen. An optical fiber passing through the operative channel of the endoscope allows urologists to irradiate kidney stones using laser light pulses. The stones are then fragmented with an appropriately adjusted laser energy and pulse frequency. The fragments are extracted and analyzed in a biology laboratory to determine their type and sub-type using a reference procedure referred to as morpho-constitutional analysis (MCA) Daudon et al. (2016).\nMCA is a two-step procedure (see on the top right side on Fig. 1). First, the aspect of the kidney stone fragment surfaces and sections are visually observed with a micros-cope. In this step, biologists describe the morphology of the crystal agglomeration using standardized key-terms relating to the colors, textures, and structure topology visible on the fragment surfaces and sections. This morphology analysis allows to recognize some types (i.e., crystal types as for instance whewellite, weddellite, and uric acid correspon-ding to types I, II and III, respectively) and some sub-types (as struvite, brushite or cystine denoted by IV.c, IV.d and V.a, respectively). In the second step, the fragments are powdered, and the spectra obtained by a Fourier Transform Infrared Spectroscopy (FTIR) gives the biochemical compo-sition of the kidney stones. This constitutional information is required to identify the remaining types and sub-types that cannot be distinguished using solely the morphological analysis. MCA is a reliable solution for recognizing kidney stone types and their sub-types (Corrales et al. (2021)).\nHowever, even if the MCA is currently the reference solution for identifying kidney stones, this method also has its drawbacks. On the one hand, the MCA has to be performed ex-vivo and therefore requires the extraction of the kidney stone fragments from the urinary tract, which is a long and tedious task. This extraction process usually takes at least half an hour and involves the risk of infection. On the other hand, the biology laboratories in the majority of hospitals are not only in charge of the identification of kidney stones, but they are also responsible for the analysis of other tissues in the frame of various pathologies. For this reason, the kidney stone identification results are often only available after some weeks (T\u00fcrk et al. (2016)), whereas for some renal calculus types (e.g., with a very short recidivism time of some days) an immediate diagnosis and treatment is strongly recommended.\nTherefore, automated methods for in-vivo identifica-tion (i.e., performed inside the urinary tract) using the images acquired with an endoscope and displayed during the ureteroscopy would be an important step towards a significant improvement, both in terms of the endoscopic"}, {"title": "1.2. Automated kidney stone identification", "content": "Given its diagnostic importance, several authors have dealt with automated kidney stone identification. Some initial attempts were first made using shallow-based (i.e., classical) machine learning methods (Serrat et al. (2017); Mart\u00ednez et al. (2020)), which led to promising results. Nonetheless, deeplearning-based (DL-based) approaches have rapidly become the favored approach to classify renal calculi (see the right bottom part of Fig. 1).\nThe first DL solution in the literature (Black et al. (2020)) was based on a ResNet-101 architecture designed to recognize five kidney stone sub-types. The network weights were pre-trained with ImageNet and fine-tuned with kidney stone images acquired in ex-vivo under controlled acquisition conditions (i.e., in an enclosed environment with a diffuse and homogeneous scene illumination, and with well-chosen camera positions). Since only a limited number of images were available in this work, the authors augmented the data by manually extracting patches from the images, the size of the patches being chosen to capture enough texture and color information to allow for class separation. Encouraging results were reached by this pre-cursor work since the five sub-types of kidney stones were classified with an acceptable overall performance (the recall, specificity, and precision values were 94.40%, 96.40%, and 82.11%, respectively). Although this work highlighted the potential of DL approaches to identify kidney stones, in-vivo data (images acquired in the urinary tract and use of an endoscope instead of a conventional CCD camera) are by far more challenging than ex-vivo data captured from controlled viewpoints and without strong illumination changes and specular reflections.\nIn (Estrade et al. (2022)), the authors considered three sub-types with different biochemical compositions, namely sub-types Ia (calcium oxalate monohydrate), IIb (calcium oxalate dihydrate) and IIIb (uric acid), the aim of this contribution being to identify kidney stones which can belong to one of five classes (three classes of pure kidney stones of sub-types Ia, IIb and IIIb and two classes of mixed stone compositions of sub-types Ia+IIb and Ia+IIIb). The images were gathered in two datasets, that of the kidney stone fragment surface images and that of the fragment section images. Data augmentation was also performed by applying geometrical transformations (i.e., a combination of translations, scaling and rotations) on the images. Two ResNet-152-V2 architectures were trained to classify the kidney stones either only with surface data or only with section data. While for the five classes taken individually, the specificity is constantly high (at least 90%), and the recall values are very different (from 50% up to 98%), the overall percentage (percentage over the five classes) of correct renal calculus identification is rather satisfactory, both for surface (83%) and for section (81%) data.\nHowever, the separate use of surface and section data is an obstacle for improving the efficiency of kidney stone recognition. Indeed, a contribution (Lopez-Tiro et al. (2024)), which compares the efficiency of the main kidney stone identification approaches based on in-vivo images, has shown that, when a model is trained by simultaneously using surface and section data, the performances can be improved both by well-tuned shallow-based machine learning and DL approaches. One of the most significant improvements with data fusion was reported in (Lopez-Tiro et al. (2023b)). The latter reports an accuracy increase of 11% over five classes when attention and multi-view feature fusion strategies are used instead of single views."}, {"title": "1.3. Scientific motivation and paper structure", "content": "A decision requires a justification in all medical applica-tions. In urology, an anti-recurrence treatment of lithiasis is supported by the MCA report, which explains the decision made by humans. Some attempts have been made for shallow-based machine learning to understand the meaning-ful decision features (e.g., discriminant features in appropri-ate color spaces (Mart\u00ednez et al. (2020)) or efficient texture representations (Serrat et al. (2017))) for identifying kidney stones. Shallow-based machine learning has the advantage that the classification exploits physically interpretable fea-tures. However, this interpretability comes at the cost of a lower accuracy.\nOn the other hand, the limitations of DL architectures lie on their \"black box\" nature (Petsiuk et al. (2018)) since the training of millions (or even billions) of parameters allows for high performance in terms of accuracy at the expense of erroneous decisions that cannot be associated to the incorrect set of values used by the neural network."}, {"title": "2. Recent XAI advances in image processing", "content": "Attempts to explain the decisions of DL models can be divided into post-hoc and self-explainable methods (Xie et al. (2020)). In the former category, the behavior of a model is systematically observed after its training, for instance, by analyzing the model responses concerning input modifications (see Fig. 2.(b)). This first category also includes approaches that generate saliency maps using the inner states and weights of the model (Petsiuk et al. (2018); Lundberg and Lee (2017a)). Other methods provide coun-terfactual examples highlighting minimal input alterations needed to change the model's output. The \"added value\" of this latter approach is that it does not only indicate what feature tends to change the class label but also how much the feature value must vary to modify the output (Jeanneret et al. (2023)). However, although they are easy to implement, post-hoc explanations can be biased and unreliable (Adebayo et al. (2018)).\nIn contrast, \"self-explanatory\" models are designed to make their decision-making transparent (Brendel and Bethge (2019); Alvarez-Melis and Jaakkola (2018)). These methods provide insights into the internal behavior of mod-els through concepts easily and utilized by domain experts, such as concept activation vectors (Kim et al. (2017); Chen et al. (2020)) or model attention and activation spaces for explanations with adversarial auto-encoders (Guyomard et al. (2022)). Recently, an increasing number of self-explainable approaches have been built on ProtoPNet (Chen et al. (2019)). This network configures the activation space by learning a hidden layer of prototypical parts (PP) given the activation patterns learned by the convolutional layers of the model. When faced with challenging recognition tasks, human experts often try to define decision rules by searching to localize in sub-image regions specific prototypical aspects characterizing the classes.\nHowever, PP-based methods can still suffer from ambi-guity between the learned parts since it can be challenging to define what constitutes a \"part\" for some classes. Fur-thermore, what a DL model considers as a PP might differ from human perception. For instance, it has been shown in (Flores-Araiza et al. (2023)) that ProtoPNet can identify many classes using visually analogous features, making it difficult for clinicians to build trust in the network's classifications."}, {"title": "2.1. Self-explainable methods", "content": "ProtoPNet resulted from pioneer work in the field of PP-networks. The concept of interpretable prototypes allowed to improve the understanding of the decisions taken by image classification models. These prototypes, learned from the model's latent space, are refined during model training to closely reflect the training data. Knowing such prototypes enables a direct and understandable explanation of the decisions taken by deep neural networks (DNNs) while maintaining their performance. ProtoPNet has inspired the design of numerous self-explainable models. For instance, TesNet (Wang et al. (2021)) constructs the latent space on a Grassman manifold, without considering the number of PPs required for each class. Conversely, ProtoPool (Rymarczyk et al. (2022)) and ProtoTree (Nauta et al. (2021a)) were both conceived to reduce the number of prototypes needed for inference: ProtoPool employs a differentiable assignment strategy to semantically merge similar prototypes, whereas ProtoTree organizes prototypes into a binary decision tree to combine global interpretability with local explanation capabilities. The extension of part-prototype networks into areas such as deep reinforcement learning (PW-Net in (Kenny et al. (2023)), and model debugging (ProtoPDebug in (Bontempelli et al. (2023)), highlights the adaptability of the method and the broad interest on this approach."}, {"title": "2.2. Limitations of current PP-based XAI methods", "content": "Case-based reasoning architectures like ProtoPNet tend to produce very similar PPs, leading to a collapse with just a few training images, especially in datasets with a limited number of samples (Flores-Araiza et al. (2023)). This behavior, similar to the mode collapse observed in GANs (Bau et al. (2019)), is particularly an issue in med-ical diagnosis, which requires fine-grained differentiation between classes. Further, this issue may impair the model's ability to recognize subtle distinctions, risking overfitting and poor generalization. A high similarity among PPs is another issue since it reduces the diversity of informative features, making the interpretability less meaningful for the specialist (urologists or biologist in the context of our work) utilizing them. Moreover, a semantic gap exists between similarities in the latent and input spaces, particularly un-der strong photometric perturbations (as occurring in en-doscopy), where PPs do not align with human prioritization of visual features (Hoffmann et al. (2021)). Additionally, most cases of non-human aligned PPs have been found in erroneous classification cases (Nauta et al. (2021b)).\nThis work explores appropriate modifications of the loss functions in a ProtoPNet implementation to counteract the issues of prototype homogeneity and semantic ambiguity. For instance, (Nauta et al. (2021b)) makes an analysis of the prototypes under various realistic photometric perturba-tions. These perturbations, naturally occurring according to the image domain task, serve to clarify the meaning of PPs by quantifying the influence of visual characteristics relat-ing to textures or hue and saturation values in the HSI color space (Daul et al. (2000)). Our approach adopts the term \"descriptor\" for these additional prototype characterizations used to assess the significance of visual features in relation to the identified prototypes. To sum up, this contribution uses descriptors to quantify the relevance of specific visual attributes to the learned prototypes of the model. This quan-tification allows to refine the interpretability of AI models by classifying images based on prototypical components. In particular, knowing the contribution of the descriptors to the class attribution enables highlighting the trade-off between interpretability and performance in complex medical appli-cations in which human-aligned interpretability is required."}, {"title": "2.3. Contributions and overview of this work", "content": "This work aims to reduce the limitations of DL mod-els and XAI-methods by providing comprehensive visual explanations in the context of kidney stone type identifi-cation. Traditional XAI methods, often relying on visual explanations based only on heatmaps, tend to oversimplify the complex process of classifying kidney stones into spe-cific types. This paper introduces a novel approach that improves the interpretability and effectiveness of DL mod-els used as computer-aided diagnostic tools. As illustrated by the overview in Fig. 3, the proposed DL architecture extracts semantic features from an input image using a Convolutional Neural Network (CNN). These features are then compared to those extracted from learned PPs. The resemblance of the learned PPs and the image parts to be recognized is quantified by semantic feature similar-ity scores, the class labels being obtained by a weighted combination of similarity scores. This method ensures that the explanations are faithful to the model's inner behavior by using the same PPs for both the model output and its explanations. The proposed model limits the number of PPs to facilitate user understanding while achieving competitive performance against its non-interpretable counterpart. The generated explanations are based on descriptors similar to that standardly used during a MCA, i.e., the explanation tries to mimic the rules followed by biologists when they vi-sually identify kidney stone types with the microscope. This way to proceed is a first step towards clinical applicability and urologist acceptability (Flores-Araiza et al. (2023)).\nHowever, recognizing kidney stone types in images requires high-level expertise. Indeed, biologists undergo training in centers dedicated to the recognition of kidney stones and must then gain experience over several months, or even years, to carry out the MCA described in Section 1.1. Only a few urologists are able to identify the kidney stones on a screen during a ureteroscopy. The explanations, even based on PPs covering small areas of the kidney stone images, rely on features that are difficult to analyze for non-experts. This contribution addresses this issue by quantifying and understanding the sensitivity of PPs to various perturbations (kidney stone aspect changes due to the endoscope's viewpoint, changing illumination, etc.).\nThis approach produces easily understandable predic-tions for specialists, making the decisions of an automated kidney stone classification clear. It reproduces the morpho-logical analysis part of MCA described in Section 1.1, build-ing trust in the AI system, and allows for the adjustment of the model's output when specialists express this need.\nThe proposed DL architecture employs a case-based rea-soning approach based on ProtoPNet, which is augmented by a Deep Metric Learning (DML) focused loss function to refine the embedding space of extracted features. This re-finement enhances the discrimination power of PPs, thereby improving the overall accuracy and interoperability of the model. A DML-focused loss function aids in optimizing the distances between embeddings by better guiding the definition of the decision space, thus enhancing the models'"}, {"title": "3. Proposed DL architecture", "content": "This section starts with an overview of the proposed ProtoPNet-based solution's modus operandi. Then, it de-scribes the model's training and highlights its limitations. Finally, it shows how these limitations can be overcome using an appropriate loss function to avoid the PP collapse of a ProtoPNet-based architecture."}, {"title": "3.1. Components of the inference stage", "content": "The goal of the proposed DL architecture is to produce a classification based on a set of clear and comprehensible descriptors providing an explanation that is interpretable by biologists performing MCA and urologists making ureteroscopies. Explanations are based on learned PPs to reach this goal. After the training, the DL model is used for inference. This sub-section details the inference stage sketched in Fig. 3.\nPrototypical Image Encoding: The first stage of the inference pipeline deals with the encoding of the images into a set of feature activations. This is achieved through the use of a pre-trained CNN-backbone extracting semantic features from input image $x_n$. With an appropriate training, this first feature extraction step should lead to diversity and representativity in terms of extracted features. Three CNN-backbones $w_{base}$ were tested (namely VGG16, ResNet50, and DenseNet201) to evaluate their impact on the perfor-mance of the proposed approach. Two layers of 1 \u00d7 1 convolutions $W_{add}$ follow the extraction backbone $w_{base}$ to adjust the depth of the feature activation maps to a 128-channel depth (see Fig. 3). A CNN-backbone, together with the two layers of 1 \u00d7 1 convolutions, form feature extractor f. The latter is applied to input image $x$, so that $Z = f(x)$ generates the convolutional output Z of W \u00d7 H latent feature tensors in space $Z \\in \\mathbb{R}^{W \\times H \\times D}$. The coordinates in the three-dimensional discrete latent feature space Z are $w \\in [1, W=7]$, $h \\in [1, H=7]$ and $d \\in [1, D=128]$, where w and h define column and line numbers in a regular square grid of adjacent convolutional patches extracted from image x (see Fig. 3). Thus, discrete space Z encodes $L = W \u00d7 H$ latent feature tensors $z_{wh}$ of dimension 1 \u00d7 1 \u00d7 128 and associated each with an image patch located on column w and line h of the patch grid.\nThe learned prototypical-part (PPs) tensors $p_{m.k}$ are also of 1 x 1 x 128 dimensions to enable their comparison with the latent feature tensors $z_{wh}$. As sketched in Fig. 3, in the proposed model, the PPs form a single layer referred to as the \"prototype layer\". This layer is based on $P = M \u00d7 K$ PPs since a constant number of M prototypes are learned for each of the K classes. The prototypes $P_{m.k}$ are indexed by m and k, with $m\\in [1, M]$ and $k \\in [1, K]$. The learned PPs are expected to be representative of the prototypical activation patterns of the class k to which they belong. Thus, squared L2 distances $d_{z_{w.h}>p_{m.k}}$ are determined in the latent space (in $\\mathbb{R}^{D}$) for all combinations of the $p_{m.k}$ and $z_{wh}$ tensors. These squared distances are obtained with Eq. (1)\n$d_{z_{w.h} \\geq p_{m.k}} = ||z_{w,h} - p_{m,k}||^{2}$          (1)\nand are used in Eq. (2) to convert them into scores $s_{w,h,m,k}$ quantifying the similarity of the PPs tensors $p_{m.k}$ and the $z_{h.w}$ patch tensors.\n$s_{h,w,m,k} = ln(\\frac{d_{z_{hw}P_{m,k}}}{d_{Z_{h.w}P_{m,k} + \\epsilon}} + 1)$       (2)\nIn Eq. (2), e is a small value to avoid division by zero.\nAs noticeable in the \"map\" column of the \"similarity\" block in Fig. 3, map $S_{m.k}$ is a matrix of similarity scores $s_{w.h.m.k}$ which encodes the similarity between a given $p_{m.k}$ tensor and all the latent feature tensors $z_{wh}$ extracted from the W x H = 49 patches of input image $x_n$ (i.e., $S_{m.k}$ are matrices with dimension 7 \u00d7 7). Also, as maps $S_{m.k}$ preserve the spatial arrangement of input image $x_n$, they can be upscaled (i.e., using bilinear interpolation) to produce heat maps $H_{m.k}$, shown in Fig. 4. A global max-pooling operation is applied to each similarity map $S_{m,k}$ to obtain the largest similarity score $S_{m.k}$ between a PP $p_{m.k}$ and all the latent feature patches $z_{wh}$ extracted from input image $x_n$. This highest similarity score $S_{m.k}$ quantifies the best resemblance of a prototypical-part to a particular area (patch) in $x_n$. Finally, the $P = M X K$ highest similarity scores $S_{m.k}$ of all PPs-tensors $p_{m.k}$ are processed with a fully-connected (FC) layer and a softmax function that provides K class label probabilities $a_k$ as output. Algorithm 1 gives an overview of the described PPs-based classification."}, {"title": "3.2. Training procedure", "content": "As detailed in Algorithm 2 and sketched in Fig. 3, the training procedure begins with an initialization followed by three sequentially chained loops (corresponding to three training phases), which are iterated $N_{tc}$ times (parameter $N_{tc}$ fixes the training cycle number).\nIn Phase 1 of Algorithm 2, the convolutional layers of CNN-backbone $f$ are updated for $N_1 = 10$ epochs. During this phase, the weights of the feature extractor are updated, with learning rate \u03b7, to capture the semantic features of the input images. These semantic features are encoded in $H \u00d7 W$ latent feature tensors $Z_{w.h}$.\nIn Phase 2 of Algorithm This phase ensures that PPs $p_{m.k}$ have a direct visual equivalence from a patch area in a training image of their respective class and enables their use for case-based reasoning explanations. In Phase 3 of Algorithm 2, the weights of the FC-layer (i.e., the last layer of the model) are updated in $N_1 = 20$ epochs. This phase aims to adjust the final classification layer so that the similarity scores accurately lead to the correct class labels. The first and third training phases use a same loss function L.\nConvergence was ensured by iterating the sequence of three phases thrice ($N_{tc} = 3$). The selected model is the one that led to the smallest network error among all errors measured with the loss L selected for training during all third phase epochs in any training iterations. All model con-figurations were run five times for training, and their average performance and standard deviation are reported in Table 3 of Section 5.1, which presents the impact on the results of the training of the most important hyperparameters.\nProjection of PPs: In the second training phase (phase 2 in Fig. 3), latent features tensors $z_{wh} \\in \\mathbb{R}^{D} (D = 128)$ are extracted from each image $x_n$ of a training set: $z_{w,h}^{n} = f(x_n)_{w,h}$. Then, the prototypical-parts $p_{m.k}$ take the values of their most similar convolutional patch tensor $z_{h.w}$ from the training images. To do so, distances $d_{z_{w.h}op_{m.k}}$ (see Eq. (1)) are determined between all patches (h, w) of the training images $x_n$ of class k and all $M$ PPs $p_{m.k}$ of same class k. Each prototype $p_{m.k}$ of class k is associated with the latent feature tensor $z_{h}$ of the image patch leading to the smallest distance $d_{z_{w.h}.p_{m.k}}$ to allow for a faithful representa-tion of the learned prototypes. Besides the information used by the DL model to generate final class labels, the learned prototypes are exploited to generate visual explanations. The following update is performed when searching the projection prototype $p_{m.k}$ of class k, which minimizes its distance with $z_{w}^{h.w}$\n$p_{m,k} \\leftarrow arg\\underset{f(x) \\in f_{train}}{min} ||p_{m,k} - z_{wh}||^{2}$      (4)\nIn Eq. (4), the set $f_{train}$ gathers all tensors $z_{wh} = f(x)_{w,h}$ corresponding to patches extracted from images $x_n$, all of which belong to class k.\nGlobal descriptors: The importance of a visual pertur-bation $i \\in \\{S, H, T, B\\}$ for a given PP $p_{m.k}$ is referred to as \"global descriptor\" and is determined using a set $X_{train}$ of training images. The local scores $\\Phi_{i,m,k,n}^{local}$ in the $X_{train}$ training images $x_n$ (with $x \\in X_{train} and n \\in [1, |X_{train}|])$ are weighted by the similarity scores $s_{m,k}$ (see Eq. (3)) from them they are derived to obtain the global descriptor values:\n$D_{i,m,k}^{global} =  \\frac{\\sum_{n=1}^{n=|X_{train}|} \\Phi_{i,m,k,n}^{local} s_{m.k}}{\\sum_{n=1}^{n=|X_{train}|}s_{m,k}}$       (5)\nIn contrast, if PP $p_{m.k}$ is clearly present in image $x_n$, the model will assign the prototype with a high similarity score"}, {"title": "3.4. Strategy for avoiding PPs collapse", "content": "Exploring novel loss functions to learn PPs is critical for enhancing the original ProtoPNet implementation by avoid-ing PPs collapse. The loss used by ProtoPNet (see Eqs. (7) to (9)) aims to distinguish PPs belonging to different classes and to gather PPs of same classes. However, this loss is not designed to prevent PPs of the same class from forming too-compact point clusters of tensors in the latent space to classify the convolutional tensor patches $z_{h.w}$ obtained from input images $x_n$ with CNN f. According to the distribution of the features extracted from the input images, PPs may even be learned around a single point in the latent tensor space. Such situations increase the risk of learning PPs that are too similar.\nIntegrating Deep Metric Learning (DML) into the loss function is a strategy which can improve intra-class simi-larity (clusterization) and inter-class diversity (separation). Adequate clusterization and separation are beneficial as they help the model to recognize and reinforce the discriminating features of each class. It makes the model more capable of identifying what makes each class unique, enhancing its ability to accurately categorize new input data. Clustering is particularly important in generating case-based reasoning explanations because the latter reflects the characteristics of a class cluster that lead to the classification of the input in that cluster. Moreover, maintaining some separation within a particular cluster based on the main visual characteristics allows for explanations using a diverse and informative set of cases. An appropriate balance between clusterization and separation is a key factor for designing DL models, leading to more accurate classification and precise explanations."}, {"title": "3.4.1. Existing losses and their limitation", "content": "In this subsection we discuss the existing losses for PP-based models and their shortcomings\nCross-Entropy (CE) Loss: This loss allows for the model to assign probabilities to each class prediction. Such an approach is advantageous in various applications (e.g.", "1": "that observation x belongs to class k.\n$L_{CE"}, "frac{1}{|X_{train}|} \\sum_{n=1}^{X_{train}} \\sum_{k=1}^{K}y_{n,k} log(\\hat{y}_{n,k})$          (6)\nProtoPNet Loss: The ProtoPNet model is designed to learn a latent space that allows for effective clustering of the most significant patches of an input image. These patches are grouped around prototypes that are semantically similar and belong to the true classes of the images. Consequently, the centers of the prototype groups (representing each a class) are distinctly separated from each other (i.e., the distance between center pairs corresponds to large L2-norm values). This class separation is obtained with loss function $L_{ProtoPNet}$ given in Eq. (7) and whose components are explained below.\n$L_{ProtoPNet} = L_{CE} + L_{Cls} + L_{Sep} + L_1$         (7)\nWhile the CE-loss $L_{CE}$ penalizes misclassified samples, the minimization of the \"cluster cost\" $L_{Cls}$ in Eq. (8) encourages each of the $|X_{train}|$ images $x_n$ from training set $X_{train}$ to have at least one latent patch $z_{h.w}$ close to at least one prototype $p_{m.k}$ of its own class k. $P_k$ refers to the group of M prototypes belonging to the class of image x.\n$L_{Cls} =  \\frac{1}{|X_{train}|} \\sum_{n=1}^{X_{train}} min \\underset{z_{w,h}, p_{m,k}\\in P_k}{||p_{m,k} - z_{w,h}||_1^2}$       (8)\nThe minimization of separation cost $L_{Sep}$ given in Eq. (9) favors a latent patch $z_{h.w}$ of a training image $x_n$ to stay far away from the prototypes $p_{m.k}$ which do not belong to its own class k, which is mathematically formulated by $p_{m.k} \\notin P_k$\n$L_{Sep} =  \\frac{1}{|X_{train}|} \\sum_{n=1}^{X_{train}} min \\underset{z_{w,h}, p_{m,k}\\notin P_k}{||p_{m,k} - z_{w,h}||_1^2}$      (9)\nIn Eq. (7), $L_1$ is a regularization term that prevents the FC-layer of the model from learning excessively large weights. $L_1$ stands for the L1-norm of the weight parameters of the FC layer. Using this norm encourages sparsity of the weights of the model, of feature extractor f consisting of $w_{base}$ and $W_{add}$ (see beginning of Section 3.1 and Algorithm 2), and of the last FC-layer wh.\nThe four components in Eq. (7) shape the latent space into a semantically meaningful cluster structure, facilitating"]}