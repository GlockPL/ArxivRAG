{"title": "Peri-LN: Revisiting Layer Normalization in the Transformer Architecture", "authors": ["Jeonghoon Kim", "Byeongchan Lee", "Cheonbok Park", "Yeontaek Oh", "Beomjun Kim", "Taehwan Yoo", "Seongjin Shin", "Dongyoon Han", "Jinwoo Shin", "Kang Min Yoo"], "abstract": "Designing Transformer architectures with the optimal layer normalization (LN) strategy that ensures large-scale training stability and expedite convergence has remained elusive, even in this era of large language models (LLMs). To this end, we present a comprehensive analytical foundation for understanding how different LN strategies influence training dynamics in large-scale Transformer training. Until recently, Pre-LN and Post-LN have long dominated standard practices despite their limitations in large-scale training. However, several open-source large-scale models have recently begun silently adopting a third strategy without much explanation. This strategy places layer normalization (LN) peripherally around sublayers, a design we term Peri-LN. While Peri-LN has demonstrated promising empirical performance, its precise mechanisms and benefits remain almost unexplored. Our in-depth analysis shows that Peri-LN strikes an ideal balance in variance growth-unlike Pre-LN and Post-LN, which are prone to vanishing gradients and \u201cmassive activations.\" To validate our theoretical insight, we conduct large-scale experiments on Transformers up to 3.2B parameters, showing that Peri-LN consistently achieves more balanced variance growth, steadier gradient flow, and convergence stability. Our results suggest that Peri-LN warrants broader consideration for large-scale Transformer architectures, providing renewed insights into the optimal placement and application of LN.", "sections": [{"title": "1. Introduction", "content": "Building on a rapidly expanding lineage of Transformer-based large language models, open-source models have shown remarkable impact (Hoffmann et al., 2022; Dubey et al., 2024; Guo et al., 2025). As the demand for larger and more powerful models grows, various training stabilization techniques have been introduced (Yang et al., 2022; Zhai et al., 2023; Loshchilov et al., 2024). Among these, the choice of where and how to apply layer normalization (LN: LayerNorm or RMSNorm; Ba et al., 2016; Zhang & Sennrich, 2019) critically influences model convergence (Xiong et al., 2020; Kedia et al., 2024; Wortsman et al., 2024; Chung et al., 2024). However, their immense computational requirements have restricted deeper exploration of the underlying Transformer structure. Are we truly employing the optimal LN placement? In practice, fully revealing the results of massive resource investments can be challenging (Rivi\u00e8re et al., 2024). Despite its importance, there is still no consensus on a single best LN placement strategy.\nTwo prominent LN placements have been widely explored. Post-LN (Vaswani et al., 2017) normalizes the hidden state after adding the sub-layer output to the residual stream (that is, Norm(x + Module(x)) where x is input hidden state. Norm is LN). This helps constrain the variance of hidden states but may inadvertently weaken gradient signals, particularly in deeper models (Kedia et al., 2024). Pre-LN (Dubey et al., 2024), by contrast, normalizes before passing the hidden state to the sub-layer (that is, x + Module(Norm(x))). While this can enhance gradient propagation, it also admits so-called \"massive activations,\" where hidden states grow exponentially across layers (Sun et al., 2024; Wortsman et al., 2024; Zhai et al., 2023).\nPrevious studies on deep convolutional neural networks (CNNs) have analyzed the impact of batch normalization on variance changes during the initialization stage of ResNet architectures, demonstrating its relationship to model performance (De & Smith, 2020). They noted that, in models without normalization, hidden activation growth at initialization can be exponential, leading to poor performance and stability. In contrast, in pre-normalized CNNs, the variance of hidden activations was shown to increase linearly as model depth grows. In the same vein, Kedia et al. (2024) reported that, for Transformer architectures as well, the variance in the forward propagation of Transformer-based language models at initialization increases linearly with depth. However, in the context of Transformer architectures,"}, {"title": "2. Background and Motivation", "content": "The analysis of activation variance at model initialization has long been central to understanding normalization layers and enhancing stability in convolutional neural networks (CNNs) (De & Smith, 2020; He et al., 2016; Brock et al., 2021a). De & Smith (2020) showed that batch normalization in residual blocks can bias networks toward the identity function, thereby stabilizing gradients and improving overall training dynamics.\nSimilar investigations have emerged for Transformer architectures, examining how variance propagates and how gradients behave in both post-layer normalization (Post-LN) (Vaswani et al., 2017) and pre-layer normalization (Pre-LN) (Dubey et al., 2024) configurations (Xiong et al., 2020; Kedia et al., 2024; Wortsman et al., 2024; Li et al., 2024b).\nEarly work comparing Post- and Pre-LN primarily focused on gradient scales and loss behavior. Xiong et al. (2020) observed that Pre-LN architectures tend to exhibit more stable gradients, but can still encounter issues such as gradient spikes and divergence, especially in deeper models or large-scale pre-training scenarios (Zhai et al., 2023; Wortsman et al., 2024; Fishman et al., 2024; Chung et al., 2024).\nAmong these challenges, the phenomenon of \u201cmassive activations\" has attracted particular attention (Dettmers et al., 2022; Yu et al., 2024; Fishman et al., 2024). Sun et al. (2024) identified that in Pre-LN architectures, large spikes in activation magnitude can persist across layers due to residual connections. These massive activations act as fixed biases, potentially narrowing the model's focus to certain tokens and may influence generalization. However, the underlying mechanisms behind these large values and their exact impact on the training process-remain not yet well understood.\nAnalytical work has provided theoretical frameworks to ex-"}, {"title": "3. Comparative Analysis", "content": "In this section, we discuss how different placements of layer normalization (LN 2) in Transformer architecture affect both training stability and the statistics of hidden states (activations 3)."}, {"title": "3.1. Post- & Pre-Normalization in Transformers", "content": "Post-LN. The Post-Layer Normalization (Post-LN) (Vaswani et al., 2017) scheme, normalization is applied after summing the module's output and residual input:\n$y_{l} = Norm(x_{l} + Module(x_{l})),$  (1)\nwhere $x_{l}$ is the input hidden state of l-th layer, $y_{l}$ is the output hidden state of l-th layer, and Module denotes Attention or Multi-Layer Perceptron (MLP) module in the Transformer sub-layer. Norm denotes normalization layers such as RMSNorm or LayerNorm. It is known that by stabilizing the activation variance at a constant scale, Post-LN prevents activations from growing. However, several evidence (Xiong et al., 2020; Kedia et al., 2024) suggest that Post-LN can degrade gradient flow in deeper networks, leading to vanishing gradients and slower convergence.\nPre-LN. The Pre-Layer Normalization (Pre-LN) (Dubey et al., 2024) scheme, normalization is applied to the mod-"}, {"title": "3.2. Variance Behavior from Initialization to Training", "content": "As discussed by Xiong et al. (2020) and Kedia et al. (2024), Transformer models at initialization exhibit near-constant hidden-state variance under Post-LN and linearly increasing variance under Pre-LN. Most of the previous studies have concentrated on this early-stage behavior. However, Recent studies have also reported large output magnitudes in both the pre-trained attention and MLP modules (Dehghani et al., 2023; Wortsman et al., 2024; Fishman et al., 2024). To bridge the gap from initialization to the fully trained stage, we extend our empirical observations in Figure 1 beyond initial conditions by tracking how these variance trends evolve at intermediate points in training.\nWe find that Post-LN maintains a roughly constant variance, which helps avert exploding activations. Yet as models grow deeper and training proceeds, consistently normalizing $x_{l}+Module(x_{l})$ can weaken gradient flow, occasionally causing partial vanishing gradients and slower convergence."}, {"title": "3.3. Placing Module Output Normalization", "content": "Peri-LN. The Peri-Layer Normalization (Peri-LN) applies LN twice within each layer\u2014before and after the module\u2014and further normalizes the input and final output embeddings. Formally, for the hidden state $x_{l}$ at layer l:\n1. Initial Embedding Normalization:\n$y_{0} = Norm(x_{0}),$\n2. Input- & Output-Normalization per Layer:\n$y_{l} = x_{l} + Norm (Module(Norm(x_{l}))),$\n3. Final Embedding Normalization:\n$y_{L} = Norm(x_{L}),$\nwhere x denotes the output of the embedding layer, the hidden input state. $y_{0}$ represents the normalized input hidden state. $x_{L}$ denotes the hidden state output by the final layer L of the Transformer sub-layer. This design unifies pre- and output-normalization to regulate variance from both ends. For clarity, the locations of normalization layers in the Post-, Pre-, and Peri-LN architectures are illustrated in Figure 2.\nControlling Variance & Preserving Gradients. By normalizing both the input and output of each sub-layer, Peri-LN constrains the residual spikes common in Pre-LN, while retaining a stronger gradient pathway than Post-LN. Concretely, if Norm(Module(Norm($x_{l}$))) has near-constant variance $B_{0}$, then\n$Var(x_{l+1}) \\approx Var(x_{l}) + B_{0},$\nleading to linear or sub-exponential hidden state growth rather than exponential blow-up. We empirically verify this effect in Section 4.4.2.\nOpen-Sourced Peri-LN Models: Gemma2 & OLMo2. Both Gemma2 and OLMo2, which apply output layer normalization, employ the same peri-normalization strategy within each Transformer layer. However, neither model rigorously examines how this placement constrains variance or mitigates large residual activations. Our work extends Gemma2 and OLMo2 by offering both theoretical and empirical perspectives within the Peri-LN scheme. Further discussion of the OLMo2 is provided in Appendix G."}, {"title": "3.4. Stability Analysis in Normalization Strategies", "content": "We analyze training stability in terms of the magnitude of activation. To this end, we examine the gradient norm with respect to the weight of the final layer in the presence of massive activation. For the formal statements and detailed proofs, refer to Appendix D.\nProposition 3.1 (Informal). Let $L(\u00b7)$ be the loss function, and let $W(2)$ denote the weight of the last layer of MLP(\u00b7). Let $\\gamma$ be the scaling parameter in Norm(\u00b7), and let D be the dimension. Then, the gradient norm for each normalization strategy behaves as follows.\n(1) Pre-LN (exploding gradient). Consider the following sequence of operations:\nx = Norm(x), a = MLP(x), o = x + a, (3)\nthen\n$\\frac{\\partial L(o)}{\\partial W_{i,j}^{(2)}} \\propto ||h||,$ (4)\nwhere h := ReLU ($W(1)$ + $b(1)$). In this case, when a massive activation ||h|| occurs, an exploding gradient $||\\frac{\\partial L}{\\partial W^{(2)}} ||$ can arise, leading to training instability."}, {"title": "4. Experiments", "content": "In this section, we provide a comprehensive empirical comparison of Post-, Pre-, and Peri-Layer Normalization (LN) across large-scale Transformer pre-training and subsequent evaluations on the language domain."}, {"title": "4.1. Experimental Setting", "content": "Excluding the embedding parameters, model sizes were set to 400M, 1.5B, and 3.2B parameters, respectively. Each model was trained on 30 billion tokens. For a fair comparison, we pre-trained each model with five different training seeds. We conducted a systematic exploration of learning rates, ranging from 1 \u00d7 10\u22124 to 5 \u00d7 10-3. The global batch size is set to 256. We use Adam optimizer with cosine learn-"}, {"title": "4.2. Pre-Training Large Language Models", "content": "Figure 3(a) illustrates the pre-training loss across learning rates for models ranging in size from 400M to 3.2B parameters. Notably, the Peri-LN architecture consistently achieves superior loss curves over this entire model size. Since Pre-LN shows best performance at learning rate 2 \u00d7 10-3 across all model size, we set this to the default learning rate for Pre-LN and Peri-LN. Unlike Pre-LN, Post-LN's appropriate learning rate lies in a lower range, so we have provided a separate summary in Appendix C.1. In Figures 3(b) and 3(c), we compare the pre-training loss and the gradient norm curve at each LN strategy's best-performing learning rate of 400M size models. The same trend was observed across different model sizes. In particular, when sweeping over training seeds, Pre-LN and Post-LN exhibited many spikes in the gradient norm curve, whereas Peri-LN showed relatively few spikes, supporting Proposition 3.1. This reduction is consistent across both large and small learning rates.\nWe provide additional results in Appendix F, including experiments using LayerNorm (instead of RMSNorm), analyses of varying sequence lengths, the effect of different training-token counts, and an ablation study on embedding LN. We also investigate pre-training using the OLMo2-style"}, {"title": "4.3. Benchmark Evaluations & Instruction Tuning", "content": "To evaluate how well the pre-trained model's training loss aligns with its benchmark performance, we conducted five separate benchmark evaluations. Furthermore, to examine how supervised fine-tuning (SFT) improves scores and instruction-following capabilities under different layer normalization strategies in Transformer architectures, we performed additional training using the LIMA dataset (Ouyang et al., 2022; Zhou et al., 2023). Diverged checkpoints were excluded when calculating the evaluation score (mostly occurs in Pre-LN). Additional training hyperparameters for SFT are given in Appendix B.2. As shown in Table 2, Peri-LN consistently demonstrates superior performance across all model sizes. Additionally, we note that, beyond the improved scores, the standard deviation of the benchmark results across different training seeds is reduced by more than half with Peri-LN. From this, we observe that Peri-LN helps maintain consistency not only in gradient stability and final loss but also in benchmark performance. For the evaluation loss, we used 10K random samples from the C4 dataset (Raffel et al., 2020). Detailed configurations and individual scores are provided in Appendix H."}, {"title": "4.4. Systematic Analysis", "content": "Despite emerging empirical evidence that Peri-LN can stabilize training and bolster performance, many open questions remain: Which design factors are crucial for reaping its benefits at scale (\u00a74.4.1)? How does it regulate hidden-state variance and gradient flow more effectively (\u00a74.4.2, \u00a74.4.3)? And why might certain variants outperform standard LN placements in large Transformer architectures (\u00a74.4.4)? Addressing these points, we present a systematic analysis of Peri-LN's mechanics at following sub-sections."}, {"title": "4.4.1. LEARNABLE PARAMETER Y OF RMSNORM", "content": "To investigate the impact of module output normalization on training stability, as proposed in the Proposition 3.1, we fixed the learnable parameter $\u03b3$ of RMSNorm to 1, isolating the effect of normalization. As shown in Figure 4, we observe that simply normalizing the output of each module reduces gradient spikes and provides improvements in the loss comparing to the pre-normalization only transformers. Nonetheless, we also confirmed that allowing $\u03b3$ to be learnable yields slightly better performance. We confirm that this trend remains consistent in both the 400M and 1.5B models. In this experiment, we omitted Peri-LN's embedding layer normalization in order to isolate and evaluate the precise role and benefits of output layer normalization."}, {"title": "4.4.2. GROWTH OF HIDDEN STATE", "content": "To examine in greater depth how Peri-LN affects the forward propagation, we analyzed the absolute magnitude and variance of the hidden states using 1,000 samples from the Wikitext dataset (Merity et al., 2016). Figure 5 illustrates how normalization strategies influence the forward-path hidden states over the course of training.\nTwo particular aspects of hidden-state behavior were noted: First, with respect to model depth, Post-LN maintains stable hidden-state magnitude and variance owing to the presence of a normalization layer in the main path (\"Highway\"). By contrast, because Pre-LN does not normalize the outputs of each Attention and MLP module, the magnitude and variance of the hidden states exhibit exponential growth following the addition operation in the residual path. For the Peri-LN architecture, which leverages Output-LN, we observed that hidden states remain comparatively well-managed. Next, regarding changes over training iterations, Post-LN continues to normalize each block's output, which prevents any substantial increase or decrease in the trend as training progresses. Meanwhile, Pre-LN exhibits a relatively linear variance distribution at initialization, but escalates exponentially to extremely large values over successive iterations. In contrast, Peri-LN fluctuates more moderately, owing to the role of Output-LN in controlling hidden-state magnitude and variance throughout training."}, {"title": "4.4.3. LAYER-WISE GRADIENT NORM & VARIANCE", "content": "Ensuring a uniform gradient flow in large-scale model training is crucial for balanced learning across the entire network"}, {"title": "4.4.4. HIDDEN STATE REPRESENTATION", "content": "We utilize angular distance (Li et al., 2024b) as a metric to assess redundancy between hidden states at initialization and after training. This approach allows us to quantify how similar or distinct the representations were across layers. As shown in Figure 7(a), Post-LN exhibits smaller angular distances due to the LN being located on the main path, whereas Pre-LN and Peri-LN begin with very similar states. As shown in Figure 7(c), at the end of training, Pre-LN tends to produce more redundant hidden state representations compared to the others. This phenomenon may stem from Pre-LN's repeated residual additions, which amplify certain representations over others.\nTo investigate this further, we focus on module output normalization, which is the main distinguishing factor between Pre-LN and Peri-LN. As shown in Figure 7(b), the learnable scale starts around 1 in the early stages of training and gradually changes with increasing depth. Because Peri-LN preserves the identity path, it appears to adjust the scale of the module output accordingly. This suggests that the exponential growth of the main path's magnitude in Pre-LN diminishes the relative contribution of individual modules, resulting in more redundant hidden representations. To support this, Figure 7(d) shows that fixing the learnable scale of Peri-LN's module output LN at 1 causes the main path contribution to decrease in deeper layers. This finding confirms the role of module output normalization in controlling hidden state redundancy."}, {"title": "5. Conclusion", "content": "We explore the placement of layer normalization within the Transformer architecture to better understand its role during training. By systematically comparing Post-LN, Pre-LN, and Peri-LN, we highlight their distinct impacts on stability, gradient propagation, and optimization dynamics. Our findings suggest that placing LN on module outputs in addition to the Pre-LN can help manage large activations while preserving beneficial gradient flow, thereby offering a promising balance for stable optimization. By unifying these approaches under the term Peri-LN, we seek to consolidate existing variants and encourage deeper investigation into this underexplored alternative."}, {"title": "A. Related Work", "content": "Activation Dynamics in Large Language Models. Studies on the distribution and magnitude of activations in deep neural networks have revealed that certain outlier features can significantly affect model behavior and efficiency. Dettmers et al. (2022) examined Transformer architectures, highlighting how specific feature dimensions may exhibit unusually large values (outliers) that disrupt quantization and overall system performance. Extending this line of work, Sun et al. (2024) identified the occurrence of \u201cmassive activations\u201d\u2014extremely large activation values that persist across multiple layers. Unlike standard outliers, these massive activations remain relatively invariant to different inputs, effectively functioning as implicit bias terms in large language models (LLMs). Notably, such extreme values can skew the self-attention mechanism, causing the model to attend disproportionately to certain tokens. These observations demonstrate that even with standard normalization layers in place, hidden biases may linger in internal representations, underscoring the importance of deeper analyses of activation behavior in LLMs.\nVariance Control and Normalization in Convolutional Networks. The interplay between activation variance and training stability has also been extensively explored in convolutional neural networks (CNNs). De & Smith (2020) showed that Batch Normalization (BN) stabilizes the training of residual networks by effectively downscaling activation variance in the residual branches, thereby improving gradient behavior. However, BN imposes certain constraints, such as dependence on batch size and additional computational overhead for estimating batch statistics. Consequently, several normalization-free or alternative normalization approaches have been investigated. For instance, Brock et al. (2021b) introduced \u201cNormalizer-Free ResNets,\" which manage activation variance through learnable scaling parameters. This approach achieved competitive performance without relying on BN, highlighting the critical role of effective variance control in fostering stable optimization and strong generalization in CNNs.\nLayer Normalization in Transformers. Training stability in Transformer architectures is closely tied to the choice and placement of layer normalization (LN). Xiong et al. (2020) reported that Transformers employing a Post-LN structure often suffer from gradient instabilities at initialization, requiring a careful learning-rate warm-up phase to mitigate these issues. In contrast, Pre-LN helps maintain more stable gradients during the early stages of training. However, Kedia et al. (2024) showed that while Post-LN can lead to vanishing or exploding gradients in deep Transformers, Pre-LN may induce hyperbolic gradient growth. These findings illustrate the nuanced trade-offs of normalization placement and draw parallels to earlier CNN studies, where careful management of activation variance proved essential for stable deep learning.\nGradient Propagation and Depth Scaling Ensuring consistent gradient propagation across many layers is pivotal for stable training in very deep models. Yang & Hu (2021) (Tensor Programs IV) introduced the concept of Maximal Update Parametrization (\u00b5P) in the infinite-width regime to preserve feature learning, preventing gradients from collapsing into kernel-like dynamics. Building on this, Yang et al. (2024) (Tensor Programs VI) proposed Depth-\u00b5P, which scales residual branches and learning rates according to network depth. Their theoretical analysis indicates that improper depth-dependent scaling leads to vanishing or exploding gradients, ultimately diminishing the diversity of learned representations. These insights highlight the necessity for principled scaling strategies and careful initialization to maintain robust gradient flow in deep architectures.\nSummary. Taken together, these studies underscore the importance of managing activation variance and hidden biases to achieve stable training and expressive internal representations in modern deep networks. In Transformer-based models, normalization choice and placement-such as Post-LN, Pre-LN, or other variants\u2014play a significant role in controlling gradient dynamics and overall performance. While Post-LN and Pre-LN have received significant attention, we focus on a comparative analysis that includes Peri-LN, an alternative normalization placement that has thus far been underexplored but holds potential for enhancing training stability and model performance."}, {"title": "D. Proof of Theoretical Insight", "content": "To support the claim that Peri-LN enhances the stability of training in such cases, we analyze the gradient norm in the final layer. For simplicity, we use RMSNorm and ReLU here.\nProposition D.1. Consider the following sequence of operations:\n$\\tilde{x} = RMSNorm(x),$\na = ReLU($xW^{(1)}+b^{(1)})W^{(2)}+b^{(2)},$\no = x + a.\nThen,\n$\\frac{\\partial L(o)}{\\partial W_{i,j}^{(2)}} = h_{i}(\\tilde{p_{j}} - y_{j}),$ (9)\nwhere h := ReLU ($xW^{(1)}+b^{(1)}$), $\\tilde{p}$ := softmax(o), and y is the label (one-hot vector).\nProof. By the chain rule,\n$\\frac{\\partial L(o)}{\\partial W_{i,j}^{(2)}} = \\frac{\\partial L(o)}{\\partial o} \\times \\frac{\\partial o}{\\partial a} \\times \\frac{\\partial a}{\\partial W_{i,j}^{(2)}}$ (10)\n(a) It is known that\n$\\frac{\\partial L(o)}{\\partial o_{k}} = \\tilde{p_{k}} - y_{k}.$ (11)\nSo,\n$\\frac{\\partial L(o)}{\\partial o} = [\\tilde{p_{1}} - y_{1}  \\tilde{p_{2}} - y_{2}   \\tilde{p_{D}} - y_{D}] .$ (12)\n(b) Since o x + a,\n$\\frac{\\partial o}{\\partial a} = I.$ (13)\n(c) Recall that\na := ReLU($xW^{(1)}+b^{(1)})W^{(2)}+b^{(2)}.$\n(14)\nFor convenience, let\nh := ReLU($xW^{(1)}+b^{(1)}).$\n(15)\nThen, we have\n$\\frac{\\partial a_{k}}{\\partial W_{i,j}^{(2)}} = \\frac{\\partial}{\\partial W_{i,j}^{(2)}}(\\sum_{p=1}^{H}h_{p}W_{p,k}^{(2)} +b^{(2)}) = h_{i} \\delta_{k,j}$ (16)\nIn vector representation,\n$\\frac{\\partial a}{\\partial W_{i,j}^{(2)}} = [0  h_{i}  0],$  (17)\nwhere the only nonzero entry is in the j-th component.\nThus, by putting these all together,\n$\\frac{\\partial L(o)}{\\partial W_{i,j}^{(2)}} = h_{i}(\\tilde{p_{j}} - y_{j}).$\n(18)"}, {"title": "Proposition D.2.", "content": "Consider the following sequence of operations:\n$\\tilde{x} = RMSNorm(x),$\na = ReLU($xW^{(1)}+b^{(1)})W^{(2)}+b^{(2)},$\n$\\tilde{a} = RMSNorm(a),$\no = x + $\\tilde{a}.\nThen,\n$\\frac{\\partial L(o)}{\\partial W_{i,j}^{(2)}} \\leq \\frac{4\\gamma\\sqrt{D}||h||}{||a||},$ (19)\nwhere $\\gamma$ is the scaling parameter used in RMSNorm(\u00b7), D is the dimensionality, and h := ReLU ($xW^{(1)}+b^{(1)}).\nProof. By the chain rule,\n$\\frac{\\partial L(o)}{\\partial W_{i,j}^{(2)}} = \\frac{\\partial L(o)}{\\partial o} \\times \\frac{\\partial o}{\\partial \\tilde{a}} \\times \\frac{\\partial \\tilde{a}}{\\partial a} \\times \\frac{\\partial a}{\\partial W_{i,j}^{(2)}}$ (20)\n(a) We have\n$||\\frac{\\partial L(o)}{\\partial o}|| = ||p-y|| < ||p|| + ||y|| = 2.$ (21)\n(b) We also have\n$||\\frac{\\partial o}{\\partial \\tilde{a}}|| = ||I|| = 1.$ (22)\n(c) Recall that\n$\\tilde{a} := RMSNorm(a) = \\gamma \\frac{a}{\\sqrt{\\sum_{k=1}^{D} (a_{k})^2 + \\epsilon}}.$ (23)\nThen, $\\frac{\\partial \\tilde{a}}{\\partial a}$ is the Jacobian matrix J of RMSNorm(\u00b7). For brevity, let\n$\\alpha := \\frac{1}{D} \\sum_{k=1}^{D} (a_{k})^2 .$\n(24)\nThen,\n$J_{p,q}= \\frac{\\partial \\tilde{a_{p}}}{\\partial a_{q}} = \\gamma \\cdot \\frac{\\partial}{\\partial a_{q}} \\frac{a_{p}}{\\sqrt{\\alpha+\\epsilon}}$\n$\\frac{1}{\\sqrt{\\alpha + \\epsilon}} + a_{p} \\cdot \\gamma \\cdot \\frac{\\partial}{\\partial a_{q}} \\frac{1}{\\sqrt{\\alpha + \\epsilon}}$\n(25)\n$\\gamma \\cdot \\frac{1}{\\sqrt{\\alpha + \\epsilon}} \\delta_{p,q} + a_{p} \\cdot \\gamma \\cdot \\frac{1}{2\\alpha} (\\frac{1}{\\sqrt{\\alpha + \\epsilon}})$ (26)\n$(a + )^{3/2}$  $\\delta_{p,q} - \\frac{\\gamma\\alpha_{p}\\alpha_{q}}{(a + )^{3/2}}.$\nIn matrix representation,\n$\\alpha\\neq0\n$J= \\frac{\\gamma}{\\sqrt{\\alpha+\\epsilon}}[I- \\frac{\\gamma}{\\sqrt{\\alpha+\\epsilon}} \\frac{a}{\\frac{\\gamma}{\\sqrt{\\alpha+\\epsilon}}} \\frac{\\gamma}{\\sqrt{\\alpha+\\epsilon}}]$\n(27)\n(28)\nThen, we have\n$||J|| = ||\\frac{\\gamma}{\\sqrt{\\alpha+\\epsilon}}[I- \\frac{\\gamma}{\\sqrt{\\alpha+\\epsilon}} \\frac{a}{\\frac{\\gamma}{\\sqrt{\\alpha+\\epsilon}}} \\frac{\\gamma}{\\sqrt{\\alpha+\\epsilon}}]||$\n(29)"}, {"title": "Proposition D.3.", "content": "Consider the following sequence of operations:\na = ReLU($xW^{(1)}+b^{(1)})W^{(2)}+b^{(2)},$\no = x + a,\n$\\tilde{o} = RMSNorm(o).$\nThen,\n$\\frac{\\partial L(o)}{\\partial W_{i,j}^{(2)}} < \\frac{4\\gamma\\sqrt{D}||h||}{||x+a||},$  (35)\nwhere $\\gamma$ is the scaling parameter used in RMSNorm(\u00b7), D is the dimensionality, and h := ReLU ($xW^{(1)}+b^{(1)}).\nProof. The proof is analogous to the proof of the previous proposition."}, {"title": "E. Additional Results on Growth of Hidden State", "content": "In this section, we examine the 400M- and 3.2B-parameter models, which were omitted in Section 4.4.2 due to space constraints. As illustrated in Figures 10 and 11, these models exhibit the same overall trend."}, {"title": "F. Additional Experimental Results on Ablation Study", "content": "F.1. Amount of Training Tokens\nIn order to investigate whether the learning behavior of each LN strategy varies with the number of training tokens, we conducted an additional round of learning-rate exploration for both the Pre-LN and Peri-LN architectures. As shown in Figure 12, even as the number of training tokens increases, there is no observable shift in the optimal learning-rate range. Based on these findings, we conclude that our overall results remain consistent, even when the training token count is further increased. Furthermore, although a learning rate of 5 \u00d7 10-\u00b3 leads to divergence in the smaller-scale experiments with 8B or 16B training tokens, it does not do so in the 30B-token setting. We attribute this discrepancy to the 10% warmup rate, suggesting that the warmup phase may be insufficient for the smaller-scale experiments.\nF.2. Sequence Length\nIn language models, the number of iterations per token is influenced by the sequence length, which in turn, along with the batch size, affects training statistics. We conducted an experiment to determine whether the performance trend changes when the sequence length is reduced from 8192, as set in the main text, to 2048. As shown in Figure 13, Peri-LN still surpasses Pre-LN in the learning rate exploration.\nF.3. Warm-up\nWarmup is widely recognized to influence training stability. To investigate whether a 10% warmup rate might unfairly disadvantage Pre-LN, we conducted an additional learning-rate exploration using a 30% warmup rate. As illustrated in Figure 14, the overall trend remained unchanged, and Peri-LN continued to exhibit better performance than Pre-LN in terms of loss. Furthermore, we observed that increasing the warmup rate from 10% to 30% did not reduce the frequency of gradient norm spikes in Pre-LN.\nF.4. RMSNorm & LayerNorm\nAs illustrated in Figure 15, we conducted experiments in which RMSNorm and LayerNorm were interchanged. Consistent with the findings reported in (OLMo et al., 2024), we did not observe any notable performance differences in our RMSNorm and LayerNorm replacement experiments."}, {"title": "F.5. Embedding Layer Normalization of Peri-Layer Normalization Transformers", "content": "Motivated by Takase et al. (2023), we empirically explore the addition of embedding layer normalization to improve training stability and overall model performance in Transformer architectures. As illustrated in Figures 16, 17, and 18, incorporating Embedding LN in the Peri-LN architecture yields a slight improvement in pre-training loss. Furthermore, our empirical observations suggest that this effect becomes more pronounced in smaller models."}, {"title": "G. Output-Layer Normalization with QK-Norm Architecture", "content": "Query and Key layer-normalization (QK-Norm) has been widely studied in modern Transformer architectures (Wortsman et al., 2024; Zhai et al., 2023; OLMo et al., 2024). In particular, OLMo et al. (2024) reported that QK-Norm combined with module output layer-normalization (output-LN, B in Figure 19 referred to as \u201creordered norm\" in the OLMo2 paper) improves both training loss and stability. As shown in Figure 19, QK-Norm is applied after the Query and Key projections, similar to output-LN. From another perspective, QK-Norm is also applied immediately before the attention"}]}