{"title": "Bridging Conversational and Collaborative Signals for Conversational Recommendation", "authors": ["Ahmad Bin Rabiah", "Nafis Sadeq", "Julian McAuley"], "abstract": "Conversational recommendation systems (CRS) leverage contextual information from conversations to generate recommendations but often struggle due to a lack of collaborative filtering (CF) signals, which capture user-item interaction patterns essential for accurate recommendations. We introduce Reddit-ML32M, a dataset that links reddit conversations with interactions on MovieLens 32M, to enrich item representations by leveraging collaborative knowledge and addressing interaction sparsity in conversational datasets. We propose an LLM-based framework that uses Reddit-ML32M to align LLM-generated recommendations with CF embeddings, refining rankings for better performance. We evaluate our framework against three sets of baselines: CF-based recommenders using only interactions from CRS tasks, traditional CRS models, and LLM-based methods relying on conversational context without item representations. Our approach achieves consistent improvements, including a 12.32% increase in Hit Rate and a 9.9% improvement in NDCG, outperforming the best-performing baseline that relies on conversational context but lacks collaborative item representations.", "sections": [{"title": "1 Introduction", "content": "Conversational recommender systems (CRS) aim to elicit user preferences and generate recommendations through interactive dialogues [9, 16]. Unlike traditional recommenders, CRS leverages conversational context to adapt to user preferences. However, existing CRS approaches face two main challenges: (1) They do not leverage collaborative filtering (CF) signals with conversational context, leading to suboptimal recommendations [6, 25], and (2) CRS datasets are typically sparse, making it difficult to develop robust item representations for accurate recommendations. To this end, a typical CRS consists of two components: (1) a generator to produce natural language responses and (2) a recommender to rank items aligned with user preferences [4, 9, 16].\nFrom a CRS model perspective, CF-based approaches excel at modeling user-item relationships but fail to leverage conversational context [12, 20]. Similarly, knowledge-driven methods improve semantic understanding using external knowledge graphs but lack the collaborative signals needed to model item relationships [25]. Large language models (LLMs) show promise in CRS due to their natural language understanding capabilities [9], but they rely at most on textual metadata, which constrains their ability to capture item relationships [10, 23]. Recent methods integrate CF embeddings with LLMs to improve recommendation tasks. For instance, A-LLMRec incorporates CF embeddings into prompts, while COLLM maps them onto token embeddings [13, 24]. ILM aligns CF embeddings with textual embeddings but lacks conversational context [23]. While these methods incorporate CF with LLMs to improve recommendations, they are not designed to utilize conversational context. Our framework bridges this gap by integrating CF signals and conversational data into a unified CRS approach.\nFrom CRS dataset perspective, current CRS datasets suffer from low interactions per item. For example, ReDIAL dataset [16] contains 31,988 interactions, with 78.98% of items having fewer than 10 interactions [10]. Similarly, Reddit-Movie [9] has 51,148 for 31,396 items as shown in Table 1. To our knowledge, no existing dataset bridges conversational context with CF information. To address these limitations, we propose a dataset that integrates CF signals with conversations and mitigates sparsity in CRS datasets.\nIn this work, we make two key contributions: First, we construct a new Reddit-ML32M dataset by linking the Reddit-Movie dataset [9] with the MovieLens 32M dataset [7]. This dataset enriches the interaction space, increasing the total number of interactions from 51,148 to over 30 million and improving dataset density from 0.013% to 0.68%, as shown in Table 1. The dataset facilitates the development of richer item representations for CRS tasks by exploiting both conversational data with CF information at the item level. Second, we propose a framework that integrates conversational context from LLMs with CF-based item representations as depicted in Figure 1. GPT-3.5t is used in a zero-shot setting as a backbone to generate initial recommendations from conversational context, which are then refined by ranking them against item embeddings generated by a pre-trained SASRec model [12]. This"}, {"title": "2 Method", "content": "2.1 Problem Formulation\nLet $U := \\{u_1,..., u_m\\}$ and $I := \\{o_1, ..., o_N \\}$ represent sets of M users and N items, respectively. We define $W$ as the vocabulary of words forming an utterance $s := (w_i)_{i=1}^n$, with each $w_i \\in W$.\nConversation Modality. We model a multi-turn conversation for recommendation tasks as $C := (u_t, s_t, I_t)_{t=1}^T$ with T turns, where $I_t \\subseteq I$ represents the items mentioned in turn t. For each turn, $s_t$ is the utterance, $u_t$ is the user in the conversation, and $I_t$ may be empty if no items are mentioned. A user seeking recommendations is a seeker, while a recommender provides recommendations.\nInteraction Modality. Observed user-item interactions are defined as $R := \\{(u,v) | u \\in U, v \\in I, and \\mathbb{1}(u, v) = 1\\}$, where $\\mathbb{1}(u, v) = 1$ if user u interacted with item v. For conversations, $R$ is:\n$R = \\{(u, v) | u \\in U, \\exists t$ such that $v \\in I_t$ and $\\mathbb{1} (u, v, t) = 1\\}$ \\qquad(1)\nwhere $\\mathbb{1}(u, v, t)$ indicates if seeker u mentions item v at turn t. However, CRS datasets often lack sufficient user-item interactions for learning robust collaborative signals, as shown in Table 1 1.\nLinking with Historical Interaction Dataset. To address the user-item interactions limitations of conversational datasets, we create a linked historical interaction dataset $D_{CF} = (X, V, Z)$ to provide collaborative signals for learning item representations. The dataset consists of three components: (1) X is the set of users in $D_{CF}$, and (2) V is the set of items in $D_{CF}$, with $V \\subseteq I$ to ensure alignment with the items in the conversational dataset, and (3) Z is the set of user-item interaction sequences. Each user $x \\in X$ has"}, {"title": "Sequential Recommendation for Item Representation Learning.", "content": "The goal of sequential recommendation is to predict the next item in a user interaction sequence based on their historical interactions. Given the interaction sequences Z, for a user $x \\in X$, let $Z_{x}^{1:k} = (i_1^x,i_2^x,...,i_k^x)$ be the first k items of the sequence. The item embeddings are represented by the matrix $E \\in \\mathbb{R}^{|V|\\times d}$ as:\n$E_{1:k}^x = (E_{i_1^x}, E_{i_2^x},...,E_{i_k^x}) \\in \\mathbb{R}^{k \\times d}$ \\qquad(2)\nThis sequence embedding matrix is input into a sequential recommender to predict a next item $i_{k+1}^x$. The training objective is maximizing likelihood of predicting the next item:\n$\\max_\\Theta \\sum_{x \\in X} \\sum_{k=1}^{|Z_x|-1} \\log p(i_{k+1}^x | Z_{x}^{1:k}; \\Theta)$ \\qquad(3)\nwhere $p(i_{k+1}^x | Z_{x}^{1:k}; \\Theta)$ represents the probability of the (k + 1)-th interaction, conditioned on the previous k items, and $\\Theta$ is the set of model parameters. By optimizing Equation 3, the model generates item embeddings E that encode collaborative signals. Since $V \\subseteq I$, embeddings can be directly applied for the conversational dataset to enable effective item ranking and recommendations. Note that, while our framework focuses on sequential models for generating item embeddings, it is adaptable to non-sequential settings by replacing the backbone model with non-sequential alternatives such as FISM [11]. This flexibility allows the framework to support a range of recommendation paradigms.\nObjective. The recommender component, following representative works [4, 9, 16, 25], optimizes item selection during each conversation turn. During the k-th turn, the component exploits the preceding context $(u_t, s_t, I_t)_{t=1}^{k-1}$ to generate a ranked list $\\hat{I}_k$, which aligns with ground-truth items in $I_k$."}, {"title": "2.2 Proposed Framework", "content": "We propose a framework to improve CRS as illustrated in Figure 1. Unlike previous models focusing on conversational contexts [4, 5, 9, 16, 22, 25], we introduce dataset links between CRS and CF modalities to integrate user interaction information from both textual content and item representations generated via implicit feedback. Additionally, we incorporate LLMs in a zero-shot"}, {"title": "Prompting.", "content": "For each conversation, the context $(u_t, s_t, I_t)_{t=1}^{k-1}$ is used to prompt the LLM, with the k-th turn being the recommender's reply. $s_t$ contains raw text for processing by the LLM. A prompt specifies that the input is intended for conversational recommendation. Pre-trained LLMs are employed in a zero-shot setting to generate item predictions [9, 19]. prompt structures the input as:\n$\\text{prompt} = F([\\text{TASK}], [\\text{FORMAT}], [\\text{CONTEXT}])$ \\qquad(4)\nwhere prompt is the constructed prompt, [TASK] is a task description template, [FORMAT] is format requirements, and [CONTEXT] is the conversational context. Figure 1 illustrates the transformation.\nLLMs. We employ LLMs in a zero-shot setting to generate recommendations from prompt [9]. For reproducibility, we configure deterministic inference by setting the temperature parameter $\\tau = 0$, ensuring the softmax selects the highest-probability word at each step [1]. The response = $(w_1, w_2,..., w_n)$ generated as:\n$w_i = \\arg \\max_j o_{i,j}$ for $i = 1, 2, ..., n$\nwhere $o_j$ is the logit for candidate word j at step i. This approach leverages the pre-trained knowledge of LLMs without fine-tuning."}, {"title": "Post-LLM Item Retrieval.", "content": "Rather than evaluating model weights or output logits from LLMs directly, we implement an exact matching post-processing step that maps a list of recommendations in natural language into a set of in-dataset items $I_{LLM}$."}, {"title": "Embedding-Based Item Ranking.", "content": "To refine the LLM-generated recommendations $I_{LLM}$, we compute a pairwise similarity matrix between the embeddings of $I_{LLM}$ and all items in the dataset V. Using the pretrained embedding matrix $E \\in \\mathbb{R}^{|V| \\times d}$ from SASRec, where d is the embedding dimensionality, we first retrieve the embeddings of the LLM-recommended items, $E_{LLM} \\in \\mathbb{R}^{m \\times d}$. The pairwise similarity matrix $S \\in \\mathbb{R}^{m \\times |V|}$ is computed as:\n$S_{i,j} = \\frac{e_i e_j}{||e_i|| ||e_j||}$  \\quad \\forall i \\in I_{LLM}, j \\in V, \\qquad(5)\nwhere $e_i$ and $e_j$ are the embeddings of items i and j, respectively. The matrix contains similarity scores between each LLM recommended item and all items in V. We use max pooling across the"}, {"title": "rows of S to aggregate scores for each item in V:", "content": "$s(j) = \\max_{i \\in I_{LLM}} S_{i,j}, \\forall j \\in V$. \\qquad(6)\nThis results in a score vector $s \\in \\mathbb{R}^{|V|}$, where s(j) represents the maximum similarity of item $j \\in V$ with any item in $I_{LLM}$. Items in V are ranked based on their scores as $\\hat{I}_k = \\text{argsort}_{j \\in V} (-s(j))$."}, {"title": "3 Experiments", "content": "Experimental Setup. Our framework integrates embeddings generated by SASRec [12] for collaborative filtering with conversational recommendations generated by GPT-3.5-t to create a unified CRS that leverages the strengths of both approaches. To assess performance, we evaluate our framework against three groups of representative baseline models. First, traditional item-based recommendation models include popularity (PopRec), FISM [11], item2vec [3], and SASRec [12], trained on interactions from the Reddit Movie dataset, as shown in Table 1. These models serve as representative baselines for assessing recommendation performance. Second, traditional CRS models include ReDIAL\u00b2 [16] and UniCRS [22]. UniCRS leverages pre-trained language models with prompt tuning to generate recommendations. We do not other works [17, 18], as UniCRS is representative with similar results. Third, zero-shot LLM-based CRS models, including LLaMA2-7B2 [21] and GPT-3.5-t, which generate conversational recommendations without fine-tuning [9]. GPT-3.5-t stands out as a state-of-the-art model for CRS tasks, as noted in [10] under the zero-shot setting defined in [9]. Our experiments confirm that GPT-3.5-t achieves the best results across key metrics, validating its position as a leading baseline for CRS.\nEvaluation Protocol. Following prior studies [4, 9, 10, 16, 22, 25], we generate predictions for each conversation and select the top k recommendations. We evaluate against ground truth items $\\hat{I}_k$. We report Hit Rate (H@k) and NDCG@k (N@k) for K = {1, 5, 10}."}, {"title": "3.1 Results and Analysis", "content": "We assess performance of our framework against eight baselines as shown in Table 2. Our method consistently outperforms all baselines, demonstrating the effectiveness of integrating CF signals into conversational recommendations. Specifically, our framework achieves a 12.32% improvement in H@5 over the state-of-the-art"}, {"title": "4 Related Work", "content": "Exploiting collaborative signals can be done using matrix factorization [14] or a Graph Convolutional Network (GCN) [8]. SASRec [12] uses a self-attention network to exploit sequential patterns within collaborative signals. Using conversational context commonly involves exploiting the semantics of conversational text and recommending items in order to maximize alignment between conversation and item attributes. ReDial [16] predicts user sentiment in the conversational context and feeds it to an autoencoder-based recommendation. EAR [15] extracts item attributes and attribute-specific user preferences from the conversational context and feeds them to an attribute-aware Bayesian Personalized Ranking (BPR) model for recommendation. KBRD [4] extracts item and non-item entities from the conversation and feeds them through a Relational Graph Convolutional Network (RGCN) to generate a knowledge-enhanced user preference representation. KGSF [25] exploits a word-oriented knowledge graph via a GCN in addition to item-oriented knowledge to recommend items based on keywords in the conversation. UniCRS [22] jointly optimizes entity embeddings from an RGCN and contextual word embeddings from pretrained language models. Recent works have explored the direct use of conversational context by prompting LLMs [6, 9]. ILM [23] uses item-text contrastive learning for learning text-aligned item representations from collaborative signals. [24] proposes tuning a low-rank adapter associated with LLM and a collaborative information encoding module so that collaborative information can be exploited by LLMs. TALLRec [2] proposes instruction tuning of LLMs to improve their ability to exploit collaborative signals."}, {"title": "5 Conclusion and Future Directions", "content": "We present the Reddit-ML32M dataset and a framework that integrates CF signals with conversational context to improve CRS. Our approach achieves consistent performance improvements, with a 12.32% increase in recall and a 9.91% improvement in NDCG compared to existing methods. The dataset serves as a resource for research in CRS, while our methodology shows promising future directions for constructing diverse datasets across different domains. This opens opportunities for broader CRS applications and tackling interaction sparsity in future work."}]}