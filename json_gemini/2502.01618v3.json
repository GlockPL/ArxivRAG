{"title": "A Probabilistic Inference Approach to Inference-Time Scaling of LLMs using Particle-Based Monte Carlo Methods", "authors": ["Isha Puri", "Shivchander Sudalairaj", "Guangxuan Xu", "Kai Xu", "Akash Srivastava"], "abstract": "Large language models (LLMs) have achieved significant performance gains via scaling up model sizes and/or data. However, recent evidence suggests diminishing returns from such approaches, motivating scaling the computation spent at inference time. Existing inference-time scaling methods, usually with reward models, cast the task as a search problem, which tends to be vulnerable to reward hacking as a consequence of approximation errors in reward models. In this paper, we instead cast inference-time scaling as a probabilistic inference task and leverage sampling-based techniques to explore the typical set of the state distribution of a state-space model with an approximate likelihood, rather than optimize for its mode directly. We propose a novel inference-time scaling approach by adapting particle-based Monte Carlo methods to this task. Our empirical evaluation demonstrates that our methods have a 4-16x better scaling rate over our deterministic search counterparts on various challenging mathematical reasoning tasks. Using our approach, we show that Qwen2.5-Math-1.5B-Instruct can surpass GPT-40 accuracy in only 4 rollouts, while Qwen2.5-Math-7B-Instruct scales to ol level accuracy in only 32 rollouts. Our work not only presents an effective method to inference-time scaling, but also connects the rich literature in probabilistic inference with inference-time scaling of LLMs to develop more robust algorithms in future work.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) have demonstrated remarkable improvements in performance through scaling up model sizes and/or data. While frontier models have relied heavily on larger datasets and an ever-increasing number of learnable parameters (Kaplan et al., 2020; Snell et al., 2024), smaller LLMs have successfully leveraged domain-specific data to match the performance of larger, general-purpose models (Sudalairaj et al., 2024; Pareja et al., 2024). However, recent reports indicate plateaus in performance gains through such scaling methods. Consequently, inference-time (aka compute-time / test-time) scaling has emerged as a promising alternative to improve model performance (Beeching et al., 2024). Proprietary models like OpenAI's o1 (OpenAI et al., 2024) and o3 have demonstrated the benefits of allocating more computation resources at inference time, particularly for complex reasoning and math tasks. These inference-time scaling techniques not only enhance model capability but also allow smaller models to achieve performance levels comparable to their larger counterparts, making advanced AI more accessible for low-resource devices.\nRecent work (Lightman et al., 2023a) has framed inference-time scaling as a search problem guided by a process reward model (PRM). This perspective has led to the successful application of classic algorithms such as best-of-n (BoN; Brown et al., 2024), beam search (Zhou et al., 2024; Snell et al., 2024), and Monte Carlo tree search (MCTS; Guan et al., 2025), which refine model outputs by systematically exploring a broader search space. This process is sometimes referred to as \"thinking/reasoning\".\nHowever, we argue that a search-based formulation becomes problematic when the reward model is imperfect an inherent issue since these models are only approximations of an unknown true classification or preference function. Empirically, this often leads to reward hacking, where the final output is optimized to score well according to the reward model but fails to be useful and/or correct (Snell et al., 2024).\nIn this paper, we propose a shift in perspective by framing inference-time scaling as a probabilistic inference task. Unlike search-based methods that seek the mode of the reward model's distribution, we leverage sampling-based techniques to explore the typical set, which is more likely to overlap with the ground truth. This approach reduces reliance on potentially flawed reward models, as probabilistic inference naturally balances exploitation and exploration by trusting the reward model only up-to a certain probability (Andrieu et al., 2010). More specifically, unlike existing search-based methods in inference-time scaling, our probabilistic approach to scaling strikes a unique balance between exploration and exploitation. If the search process discovers a partial solution with a high process reward score, the next step will resample that solution more heavily but will typically not have it completely dominate the next step of particles, allowing for more diverse options to still continue their exploration.\nThe idea of using more computation to refine results is a fundamental feature of many classic probabilistic inference methods. For instance, Markov chain Monte Carlo (MCMC) methods improve inference asymptotically with more iterations, while particle-based Monte Carlo methods enhance accuracy as the number of particles increases.\nBuilding on this principle, we introduce a novel approach to inference-time scaling by adapting particle-based Monte Carlo algorithms from probabilistic inference. Our method explicitly accounts for imperfections in reward models by maintaining a diverse set of candidates within the solution space. By iteratively updating their weights based on observed evidence (approximate reward), our approach ensures robust scaling even when the reward model is imperfect.\nOur key contributions are as follows.\n1. We formulate inference-time scaling as probabilistic inference over a state space model (SSM) jointly defined by a language model (transition kernel) and a process reward model (emission model), which enables direct application of probabilistic inference methods.\n2. We propose inference-time scaling algorithms based on the particle filtering (PF) algorithm, which is robust to imperfection in reward modeling. We study its scaling performance and the effective temperature in LLM generation and how to optimally allocate computation"}, {"title": "3. We study ways to use PRMs and propose a more robust and performant way to obtain rewards for partial answers which we refer to as model-based aggregation.", "content": ""}, {"title": "4. We demonstrate that the proposed methods have 4-16x faster scaling speed than previous methods based on a search formulation on the MATH500 and AIME 2024 datasets, with small language models in the Llama and Qwen families. We show that PF can scale Qwen2.5-Math-1.5B-Instruct to surpasses GPT-40 accuracy with only a budget of 4 and scale Qwen2.5-Math-7B-Instruct to ol accuracy with a budget of 32.", "content": ""}, {"title": "2. Related Work", "content": "Process reward models (PRMs) aim to provide more granular feedback by evaluating intermediate steps rather than only final outputs. They are trained via process supervision, a training approach where models receive feedback on each intermediate step of their reasoning process rather than only on the final outcome. Lightman et al. (2023a) propose a step-by-step verification approach to PRMs, improving the reliability of reinforcement learning. DeepSeek PRM (Wang et al., 2024) uses Mistral to annotate training data for PRMs. Zhang et al. (2025b) introduces Qwen-PRM, which combines both Monte Carlo estimation and model/human annotation approach to prepare training data for a PRM. PRIME (Cui et al., 2025) proposes to train an outcome reward model (ORM) using an implicit reward objective. The paper shows that implicit reward objective directly learns a Q-function that provides rewards for each token, which can be leveraged to create process-level reward signal. This process eliminates the need for any process labels, and reaches competitive performance on PRM benchmarks.\nInference-time scaling has been a key training-free strategy for enhancing LLM performance. Brown et al. (2024) explores a best-of-N (BON) decoding strategy, demonstrating improvements in output quality through selective refinement. (Snell et al., 2024) provides insights into how scaling compute resources can yield better inference efficiency from a compute optimality perspective. While not implementing full Monte Carlo tree search (MCTS), Zhou et al. (2024) explores a tree-search-like approach within language models. Additionally, Guan et al. (2025) introduces rSTAR, a method that combines MCTS for data generation and training to improve mathematical reasoning. Beeching et al. (2024) discusses beam search and dynamic variable-time search (DVTS) as inference-time scaling techniques to improve open-source LLMs. DVTS works by running multiple independent subtrees in parallel so to avoid all leaves stuck in local minima.\nParticle-based Monte Carlo methods are powerful tools for probabilistic inference. Sequential Monte Carlo (Moral, 1997) or particle filtering (Swendsen & Wang, 1986) has been the classical way to approximate complex posterior distributions over state-space models. Particle Gibbs (PG) sampling (Andrieu et al., 2010) extends these approaches by integrating MCMC techniques for improved inference. (Lew et al., 2023) and (Loula et al., 2025) introduce a probabilistic programming language that applies SMC methods to steer/constrain LLM generation. (Zhao et al., 2024) and (Feng et al., 2024) introduce Twisted SMC methods for inference in language models."}, {"title": "3. Background", "content": "State space models are a class of probabilistic models used to describe sequential systems that evolve stepwise, typically over time (S\u00e4rkk\u00e4, 2013). They consist of a sequence of hidden states {xt}T\nt=1 and corresponding observations {ot}T\nt=1, where xt \u2208 X represents the latent state at step t, and ot \u2208 Y is the observation. The evolution of states is governed by a transition model p(xt | X<t\u22121), and the observations are governed by the emission model p(Otxt). The joint distribution of states and observations is given by:\np(x1:T, 01:T) = P(x1)\u041fT\nt=2P(Xt | X<t\u22121)\u041fT\nt=1P(Ot | Xt),\nwhere p(x1) is the prior distribution over the initial state.\nProbabilistic inference in SSMs involves estimating the posterior distribution of the hidden states given the observations, p(x1:T|01:T) (S\u00e4rkk\u00e4, 2013). This task is generally intractable due to the high dimensionality of the state space and the dependencies in the model. Common approaches approximate the posterior through sampling-based methods or variational approaches (MacKay, 2003).\nParticle filtering (PF) is a sequential Monte Carlo method to approximate the posterior distribution in SSMs (Swendsen & Wang, 1986; Moral, 1997). PF represents the posterior using a set of N weighted particles {(xi)t, w(i)t}N\ni=1, where x(i)t denotes the ith particle at time t, and w(i)t is its associated weight. The algorithm iteratively propagates particles using the transition model and updates weights based on the emission model: w(i)t \u221d w(i)t\u22121P(Ot | x(i)t)."}, {"title": "4. Method", "content": "We begin by formulating inference-time scaling for LLMs as probabilistic inference over a state-space model (SSM), where the transition kernel is defined by the LLM and the emission probabilities are given by the PRM (Section 4.1). Next, in Section 4.2, we introduce how particle filtering (PF) can be applied to this inference task. We then extend our approach to incorporate multiple iterations and parallel chains, providing more ways to allocate computation budgets."}, {"title": "4.1. Inference-time scaling LLMs with PRMS as probabilistic inference over SSMs", "content": "For a LLM M (or p\u043c), our approach to inference-time scaling attempts to estimate the latent states of the following joint distribution over tokens (or chunks, e.g. steps in math problems) 21:T and observations 01:T representing the acceptance of the tokens, given prompt c\nPM (\u04251:T, 01:\u0422 | c) \u221d \u041fT\nt=1PM(Xt | C, X<t\u22121) \u041fT\nt=1[P(Ot | C, Xt)' (1)\nwhere\n\u2022 The transition kernel pM (xt | C, X<t\u22121) is defined by M;\n\u2022 The emission model or likelihood p(ot | C,Xt) = B(ot; r(c, xt)) is a Bernoulli whose parameter is defined by a reward function r of each xt for prompt c.\nFigure 1 shows the plate diagram of this SSM we define.\nIn inference-time scaling, we would like to find the sequence of latent states such that all steps are accepted (ot = 1 for all t), i.e. estimating PM (X1:T | C, 01:T = 1). This interpretation makes PF directly applicable.\nFurther, as the optimal or the ground-truth reward function r is often unknown in practice, we approximate r via a model r\u0302 that is suitable for the task. Following previous works, we use pre-trained PRMs r\u0302 for such approximation when solving reasoning tasks in the domain of mathematics (for example), which gives us an approximate likelihood (ot | c, xt) = B(ot;r\u0302(c, xt)). Thus, our task is to estimate the latent states of the following joint given ot = 1 for all t\nPM (\u04251:\u0422, 01:\u0422 | c) \u221d \u041fT\nt=1[PM(Xt | C, X<t\u22121) \u041fT\nt=1[P(Ot | C, Xt). (2)\nSampling v.s. search An alternative to our sampling-based approach would be to find a point estimation of the distribution via optimization, which essentially reduces to variants of existing search-based inference-time scaling methods like MCTS, beam search, etc. However, we argue that such search-based methods are not robust in the case of PRM-based noisy approximations to the reward function. On the other hand, sampling using (2) can produce a closer estimation of (1) than optimization. This can be understood by comparing the typical set and the mode of a distribution: the mode of (2) is more sensitive to approximation errors in r\u0302 than the typical set. This aligns with the classic insight that while sampling-based methods remain invariant to reparameterization in the likelihood, maximum-a-posteriori (MAP) inference\u2014which underlies search-based methods\u2014does not (Murphy, 2012)."}, {"title": "In essence, sampling-based approaches are more robust to approximation errors in the likelihood, making them a better fit for this task an advantage we will demonstrate empirically in the following sections.", "content": ""}, {"title": "4.2. Particle filtering for inference-time scaling", "content": "We now consider inference-time scaling with an LLM PM and a PRM r\u0302 via sampling from the posterior of (2) by conditioning on accepting all steps. The direct application of the classic particle filtering algorithm to this inference-time scaling setup requires defining the following components.\n\u2022 State initialization and transition PM(xt | C, X<t-1) is done by prompting the LLM with the prompt c to generate responses for a single step. These steps are determined automatically through stop-token delimiters. The LLM temperature is a hyperparameter to tune optimally for different tasks (see ablation in Section 5.4);\n\u2022 Weight update w(i)t \u221d w(i)t\u22121r\u0302(x(i)t) uses the PRM to compute the reward per step, as detailed next.\nPRMS for likelihood estimation and weight update How to aggregate the step-level rewards remains a choice when one uses PRMs. There are three common ways to assign rewards to a partial answer using PRMs: prod, which takes the product of rewards across all steps; min, which selects the minimum reward over all steps; and last, which uses the reward from the final step. Zhang et al. (2025b) studies the optimal way for reward aggregation and points out that the \"best choice\" depends on if the PRM training data is prepared using MC rollout and/or human/model annotation. While prod aligns directly with the weight update rule described earlier, min and last do not allow for online weight updates. Therefore, for these methods, we compute the weight based on the entire partial trajectory instead.\nBeyond these three approaches, we also explored a model-based reward aggregation method that performed surprisingly well. This method feeds the PRM with partial answers but only considers the final reward token, effectively prompting the model to provide an aggregated reward for the partial answer. Interestingly, we tested the Qwen PRM both for its original purpose as a true process reward model and repurposed as an outcome reward model. When used as a true PRM, it receives the question and a list of steps generated by the policy model, calculates scores for each step and selects the last score-a practice introduced and evaluated in Beeching et al. (2024). As an ORM, the PRM takes in a question and a concatenated string of generated steps, producing a score that we convert into a weight for the resampling process."}, {"title": "4.2.1. MULTIPLE ITERATIONS AND PARALLEL CHAINS", "content": "The PF approach to inference-time scaling can be used to define a MCMC kernel that enables two new types of scaling: multiple iterations of complete answers inspired by PG and parallel simulations inspired by parallel tempering.\nParticle Gibbs is a type of MCMC algorithm that uses PF as a transition kernel (Andrieu et al., 2010). Specifically, at each iteration, PG samples a new set of particles using PF with a reference particle from the previous iteration. This integration combines the efficiency of PF with the theoretical guarantees of MCMC, making PG suitable for high-dimensional or challenging posterior distributions. The adaption of PG to inference-time scaling is essentially a multi-iteration extension of the PF algorithm presented, which works as follows: For each iteration, we run a modified PF step with an additional sampling step to sample 1 reference particle according to (3). For any PF step that is not the initial step, the PF is executed with a reference particle: This reference particle is never replaced during the resampling step, but its partial trajectory can still be forked during resampling. Note that typically, a reasonably large number of particles is needed to show the benefits of multiple iterations, which we also confirm in our results in Section 5.4.\nParallel tempering In parallel tempering (aka replica exchange MCMC sampling), multiple MCMC chains run in parallel at different temperatures and swap the states to allow better exploration. The key idea is that the chain running in high temperature can explore better, e.g. traversing between different modes of the target, and the swap makes it possible to let the low temperature chain exploit the new region found by the other chain."}, {"title": "5. Evaluation", "content": "We thoroughly evaluate our proposed methods in this section. We detail our experimental setup in Section 5.1 and start with highlighted results on comparison with other closed-source models and competitive inference-time scaling methods with open-source models (Section 5.2). We then study how the main algorithm, particle filtering, scales with more computation and compare it with its competitors (Section 5.3). We further perform an extensive ablation study on key algorithmic choices like reward models, reward aggregation and LLM temperatures (Section 5.4). We finally study different possible allocations of the computation budget through iterative and parallel extensions (Section 5.5)."}, {"title": "5.1. Setup", "content": "Models We consider two types of open-source small language models (SLMs) as our policy models for generating solutions. The first is general-purpose models, of which we used Llama-3.2-1B-Instruct and Llama-3.1-8B-Instruct (Grattafiori et al., 2024). The second is math-specialized models, where we used Qwen2.5-Math-1.5B-Instruct and Qwen2.5-Math-7B-Instruct (Yang et al., 2024). These small models are well-suited for inference-time scaling, enabling efficient exploration of multiple trajectories.\nProcess Reward Models To guide our policy models, we utilized Qwen2.5-Math-PRM-7B (Zhang et al., 2025a), a 7B process reward model. We selected this model because it demonstrated superior performance compared to other PRMs we tested, including Math-Shepherd-mistral-7b-prm (Wang et al., 2024), Llama3.1-8B-PRM-Deepseek-Data (Xiong et al., 2024), and EurusPRM-Stage2 (Yuan et al., 2024). This result as an ablation study is provided in Section 5.4, where we also study the different ways to aggregate step-level rewards from PRMs discussed in Section 4.2.\nBaselines\n\u2022 Pass@1: single greedy generation from the model, serving as the \"bottom-line\u201d performance.\n\u2022 BON/WBON (Brown et al., 2024): (weighted) best-of-N is the most straightforward inference-time scaling method using reward models.\n\u2022 DVTS (Beeching et al., 2024): a parallel extension of beam search that improves the exploration hence overall scaling performance.1\nDatasets To evaluate our methods and baselines, we consider widely-used datasets spanning multiple domains and difficulty levels and challenging benchmarks, ensuring a robust assessment of the methods' performance across basic and advanced problem-solving and reasoning tasks.\n\u2022 MATH500 (Lightman et al., 2023b): A dataset containing 500 high-difficulty competition-level problems from various mathematical domains.\n\u2022 \u0391\u0399\u039c\u0395 2024 (\u0391\u0399-\u039c\u039f, 2023): A collection of 30 problems from the American Invitational Mathematics Examination (AIME I and II) 2024.\nParsing and scoring To evaluate model-generated responses, we enforce a structured answer format using a system prompt (see Appendix A.2). This prompt ensures that the final answer is enclosed within a \\boxed{} expression, facilitating automated extraction. We provide a detailed version of our scoring process in Appendix A.3."}, {"title": "5.2. Main results", "content": "We first present our main results, comparing our approach against a set of strong baselines in Table 1. Inference-time scaling results are based on a budget of 64 samples, with Qwen2.5-Math-PRM-7B serving as the reward model. Specifically, it is used as an ORM in WBoN and as a PRM otherwise.\n\u2022 Among all inference-time scaling methods, PF consistently achieves the best performance, outperforming other scaling methods by a significant margin.\n\u2022 PF with Llama-3.1-8B-Instruct outperforms its much larger counterpart, Llama-3.1-70B-Instruct, on"}, {"title": "5.3. Scaling with inference-time compute", "content": "We now zoom in on how PF scales with inference-time compute. Figure 2 shows the change of performance (in terms of accuracy) with an increasing computation budget (N = 1,2,4,8, 16, 32, 64, 128) for all SLMs we consider. As we can see, PF scales 4-16x faster than the next best competitor DVTS, e.g. DVTS requires a budget of 32 to reach the same performance of PF with a budget of 8 with LLama-3.2-1B-Instruct and requires a budget of 128 to"}, {"title": "5.4. Ablation study", "content": "Performance of different PRMS To investigate the impact of the choice of PRM on our method, in Figure 4 we present the results of an ablation study on a subset of 100 questions from the MATH500 dataset, where we compare the accuracy of our method across various reward functions as the number of particles increases. Qwen2.5-Math-PRM-7B consistently outperforms other models, making it the natural choice for our main results. Interestingly, while EurusPRM-Stage2 performs relatively poorly with smaller budgets, it gradually improves and eventually matches Qwen2.5-Math-PRM-7B at higher budgets.\nReward aggregation within PRMs As mentioned in Section 4.2 and reported by many previous works (Zhang et al., 2025b), there exist multiple ways to use PRMS to calculate reward scores which can have large impact on final performance. Figure 5 studies 3 existing ways to use a set of PRM scores\u2014using the last reward, the minimum reward, and the product of all the rewards. We also study \"Model Aggregation\", through which we use the PRM as an ORM with partial answers. As we can see, using Model Aggregation-in essence, feeding into a PRM the entire partial answer alongside the question - scales the best with an increasing budget."}, {"title": "5.5. Budget allocation over iterations and parallelism", "content": "The multi-iteration and parallel-chain extensions introduced in Section 4.2.1 provides two more axes to spend computation in addition to the number of particles. We now explore how different ways to allocate budgets changes the performance. Specifically, we study for a fixed budget N\u00d7T\u00d7M, how the combination of N, T, M can yield the best performance, where N is the number of particles, T is the number of iterations, and M is the number of parallelism.\nAllocating budget between N and T Figure 7 shows results of Llama-3.2 1B model when configured with various test-time compute budget allocations. Although the plot shows that various Particle Gibbs configurations do not have a marked benefit over an equivalently budgeted particle filtering run, a PG experiment with 16 particles and 4 iterations powered by a Qwen 2.5 7B Math Instruct policy model achieved a 87.2% accuracy on MATH500, beating o1 performance. Configurations with larger N values typically do better than equivalently budgeted runs with less particles."}, {"title": "5.6. Conclusion", "content": "In this paper, we introduce a set of inference-time scaling algorithms with PRMs that leverage particle-based Monte Carlo methods. Our evaluation demonstrates that these algorithms consistently outperform search-based approaches by a significant margin.\nHowever, inference-time scaling comes with computational challenges. Hosting and running a reward model often introduces high latency, making the process more resource-intensive. Additionally, for smaller models, extensive prompt engineering is often required to ensure outputs adhere to the desired format. Finally, hyperparameters such as temperature are problem-dependent and may require extensive tuning across different domains.\nWe hope that the formal connection of inference scaling to probabilistic modeling that we established in this work will lead to systematic solutions for the current limitations of these methods and pave the way for bringing advanced probabilistic inference algorithms into LLM inference-time scaling in future work."}, {"title": "Algorithm 1 Particle Filtering for Inference-Time Scaling", "content": "Input: the number of particles N, a reward model r\u0302, a LLM PM and the prompt c\nInitialize N particles {x(i)1 ~ pm(\u00b7 | c)}Ni=1\nt\u2190 1\nwhile not all particles stop do\nUpdate rewards w = [f(x(i)t),...,f(x(N)t)]\nCompute softmax distribution \u03b8 = softmax(w)\nSample indices {j(i)t}Ni=1 ~ Pt(j = i)\n(i)t = \u03b8\u03b5\nUpdate the set of particles as {x(i)t\u2192 xjt}Ni=1\n(i)t+1 (i)t 1 Ni=1\nTransition {x1 ~PM(C, xjt)}i=1\nt-t+1\nend while\nReturn: the set of particles in the end"}, {"title": "4.2.1. MULTIPLE ITERATIONS AND PARALLEL CHAINS", "content": "The PF approach to inference-time scaling can be used to define a MCMC kernel that enables two new types of scaling: multiple iterations of complete answers inspired by PG and parallel simulations inspired by parallel tempering.\nParticle Gibbs is a type of MCMC algorithm that uses PF as a transition kernel (Andrieu et al., 2010). Specifically, at each iteration, PG samples a new set of particles using PF with a reference particle from the previous iteration. This integration combines the efficiency of PF with the theoretical guarantees of MCMC, making PG suitable for high-dimensional or challenging posterior distributions. The adaption of PG to inference-time scaling is essentially a multi-iteration extension of the PF algorithm presented, which works as follows: For each iteration, we run a modified PF step with an additional sampling step to sample 1 reference particle according to (3). For any PF step that is not the initial step, the PF is executed with a reference particle: This reference particle is never replaced during the resampling step, but its partial trajectory can still be forked during resampling. Note that typically, a reasonably large number of particles is needed to show the benefits of multiple iterations, which we also confirm in our results in Section 5.4.\nParallel tempering In parallel tempering (aka replica exchange MCMC sampling), multiple MCMC chains run in parallel at different temperatures and swap the states to allow better exploration. The key idea is that the chain running in high temperature can explore better, e.g. traversing between different modes of the target, and the swap makes it possible to let the low temperature chain exploit the new region found by the other chain."}, {"title": "Algorithm 2 Particle Gibbs for Inference-Time Scaling", "content": "Input: same as Algorithm 1 with the number of Gibbs iterations T\nRun Algorithm 1 to get a set of particles {x(i)1}Ni=1\nfor j = 1,..., T do\nCompute rewards w = [f(x(i)1)t,...,f(x(N)1)t]\nCompute softmax distribution \u03b8 = softmax(w)\nSample reference particle x(i)ref := x(i)j where j ~ P(j = i) = \u03b8i\nInitialize N \u2013 1 particles {x(i)t ~ pm(. | c)}Ni=1\nt\u2190 1\nwhile not all particles stop do\nUpdate w = [f(x(i)ref)t,...,f(x(N)t)t], f(x(i)ref)t\nCompute softmax distribution \u03b8 = softmax(w)\nSample indices {j(i)t}Ni=1 ~ Pt(j = i) = \u03b8i\nUpdate the set of particles as as {x(i)t\u2192 xjt}Ni=1\n(i)t+1 (i)t 1 Ni=1\nTransition {x1 ~ PM(. |\u0441, xjt)}i=1\nt-t+1\nend while\nend for\nReturn: the set of particles in the end"}, {"title": "A.1. Algorithm details", "content": "For a set of parallel chains with temperatures T\u2081 > T2 > ..., at each iteration, we swap the states of every pair of neighboring chains k, k + 1 with the following probability\nA = min (1,\u03c0k(x(k+1))\u03c0k+1(x(k)))."}, {"title": "Algorithm 3 Particle Gibbs with Parallel Tempering for Inference-Time Scaling", "content": "Input: same as Algorithm 2 with the number of parallel chains M and a list of temperature T1,..., TM\nfor j = 1,..., T do\nfor k = 1, ..., M do\nif j = 1 then\nRun Algorithm 1 to get a set of particles {x(i)1}Ni=1 for chain k\nelse\nInitialize N 1 particles {x(i)t ~PM(\u00b7 | c)}Ni=1\nt\u2190 1\nwhile not all particles stop do\nUpdate w = [f(x(i)j)t,...,f(x(N)j)t]\nCompute softmax distribution \u03b8 = softmax(w/Tk)\nSample indices {j(i)t}Ni=1 ~ Pt(j = i) ) = \u03b8i\nUpdate the set of particles as {x(i)t}Ni=1\nTransition {x(i)j+1 ~ PM (.|\u0441, x(i)j+1)}Ni=1\nt-t+1\nend while\nend if\nCompute rewards w = [f(x(i)1)t,...,f(x(N)1)t]\nCompute softmax distribution \u03b8 = softmax(w/Tk)\nSample reference particle x(i)ref := x(i)j where j ~ P(j = i) = \u03b8i\nend for\nfor k = 1,..., M 1 do\nExchange the reference particle between chain k and k + 1 with probability according to (4)\nend for\nend for\nReturn: M set of particles in the end"}]}