{"title": "Analyzing the Evolution of Graphs and Texts", "authors": ["Xingzhi Guo"], "abstract": "With the recent advance of representation learning algorithms on graphs (e.g., DeepWalk/GraphSage) and natural languages (e.g., Word2Vec/BERT), the state-of-the art models can even achieve human-level performance over many downstream tasks, particularly for the task of node and sentence classification. However, most algorithms focus on large-scale models for static graphs and text corpus without considering the inherent dynamic characteristics or discovering the reasons behind the changes. This dissertation aims to efficiently model the dynamics in graphs (such as social networks and citation graphs) and understand the changes in texts (specifically news titles and personal biographies). To achieve this goal, we utilize the renowned Personalized PageRank algorithm to create effective dynamic network embeddings for evolving graphs. Our proposed approaches significantly improve the running time and accuracy for both detecting network abnormal intruders and discovering entity meaning shifts over large-scale dynamic graphs. For text changes, we analyze the post-publication changes in news titles to understand the intents behind the edits and discuss the potential impact of titles changes from information integrity perspective. Moreover, we investigate self-presented occupational identities in Twitter users' biographies over five years, investigating job prestige and demographics effects in how people disclose jobs, quantifying over-represented jobs and their transitions over time.", "sections": [{"title": "1 Introduction", "content": "Modern machine learning techniques rely heavily on dense numerical vectors. Predictive and clustering algorithms, including Support Vector Machines and deep neural networks, operate on low-dimensional vectors that typically range from 60 to 512 dimensions. While image classification models naturally use dense matrices of pixels to represent images, discrete symbols, such as English words and graph vertices, present a more challenging problem. Unlike images that can be directly digitized by camera sensors, languages and graphs are constructed from discrete symbols, which makes it difficult to directly capture using low-dimensional vectors. In recent years, there has been significant progress in the field of natural language processing (NLP) and graph representation learning, which involves learning meaningful and low-dimensional dense vector representations of words and vertices. These vector representations are useful for a variety of NLP tasks as well as graph tasks, such as sentiment analysis, text summarization, node classification and link prediction. We briefly introduce the two seminal works in word and node embeddings as follows."}, {"title": "1.1 Word and Graph Embeddings", "content": null}, {"title": "1.1.1 Word Embeddings: Represent words by vectors", "content": "The advent of Word2Vec[116] has brought about significant advancements in artificial intelligence, specifically in the area of language embeddings or word embeddings. Word embeddings involve assigning a discrete symbolic unit (such"}, {"title": "1.1.2 Graph Embeddings: Represent vertices by vectors", "content": "Network or graph embeddings are a concept that is similar to word embeddings. It involves representing discrete vertices using low-dimensional vectors. The seminal network embedding algorithm, DeepWalk [126], proposed a wise problem formulation that converted a graph into node sequences path by random walk, thereby leading the graph learning community into the deep learning era."}, {"title": "1.1.3 Our Motivations", "content": "The aforementioned graph algorithms only focus on static graphs, while real-world graphs are highly dynamic, with structural changes occurring over time. For instance, Wikipedia volunteers tirelessly update Wiki articles, as well as hyperlinks, to reflect the latest events that happen every day. However, most network"}, {"title": "1.2 Subset Node Embeddings over Dynamic Graph", "content": "Dynamic graph representation learning is a task to learn node embeddings over dynamic networks, and has many important applications, including knowledge graphs, citation networks to social networks. Graphs of this type are usually large-scale but only a small subset of vertices are related in downstream tasks. Current methods are too expensive to this setting as the complexity is at best linear-dependent on both the number of nodes and edges. In Chapter 3, we propose a new method, namely Dynamic Personalized PageRank Embedding (DYNAMICPPE) for learning a target subset of node representations over large-scale dynamic networks. Based on recent advances in local node embedding and a novel computation of dynamic personalized PageRank vector (PPV), DYNAMICPPE has two key ingredients: 1) the per-PPV complexity is O(md/e) where m,d, and e are the number of edges received, average degree, global precision error respectively. Thus, the per-edge event update of a single node is only dependent on d in average; and 2) by using these high quality PPVs"}, {"title": "1.3 Subset Node Anomaly Detection over Dynamic Graph", "content": "Tracking a targeted subset of nodes in an evolving graph is important for many real-world applications. Existing methods typically focus on identifying anomalous edges or finding anomaly graph snapshots in a stream way. However, edge-oriented methods cannot quantify how individual nodes change over time while others need to maintain representations of the whole graph all time, thus computationally inefficient. Chapter 4 proposes DYNANOM, an efficient framework to quantify the changes and localize per-node anomalies over large dynamic weighted-graphs. Thanks to recent advances in dynamic representation learning based on Personalized PageRank, DYNANOM is 1) efficient: the time complexity is linear to the number of edge events and independent on node size of the input graph; 2) effective: DYNANOM can successfully track topological changes reflecting real-world anomaly; 3) flexible: different type of anomaly score functions can be defined for various applications. Experiments demonstrate these properties on both benchmark graph datasets and a new large real-world dynamic graph. Specifically, an instantiation method based on DYNANOM achieves the accuracy of 0.5425 compared with 0.2790, the best baseline, on the task of node-level anomaly localization while running 2.3 times faster than the baseline. We present a real-world case study and further demonstrate the usability of DYNANOM for anomaly discovery over large-scale graphs."}, {"title": "1.4 Efficient Contextual Node Representation Learning over Dynamic Graphs", "content": "Real-world graphs grow rapidly with edge and vertex insertions over time, motivating the problem of efficiently maintaining robust node representation over evolving graphs. Recent efficient GNNs are designed to decouple recursive message passing from the learning process, and favor Personalized PageRank (PPR) as the underlying feature propagation mechanism. However, most PPR-based GNNs are designed for static graphs, and efficient PPR maintenance remains as an open problem. Further, there is surprisingly little theoretical justification for the choice of PPR, despite its impressive empirical performance. In Chapter 5, we are inspired by the recent PPR formulation as an explicit l1-regularized optimization problem and propose a unified dynamic graph learning framework based on sparse node-wise attention. We also present a set of desired properties to justify the choice of PPR in STOA GNNs, and serves as the guideline for future node attention designs. Meanwhile, we take advantage of the PPR-equivalent optimization formulation and employ the proximal gradient method (ISTA) to improve the efficiency of PPR-based GNNs by 2-5 times. Finally, we instantiate a simple-yet-effective model (GOPPE) with robust positional encodings by maximizing PPR previously used as attention. The model performs comparably to or better than the STOA baselines and greatly outperforms when the initial node attributes are noisy during graph evolution, demonstrating the effectiveness and robustness of GOPPE."}, {"title": "1.5 Post-publication Modification in News Headlines", "content": "Digital media (including websites and online social networks) facilitate the broadcasting of news via flexible and personalized channels. Unlike conventional newspapers which become \u201cread-only\" upon publication, online news sources are free to arbitrarily modify news headlines after their initial release. The motivation, frequency, and effect of post-publication headline changes are largely unknown, with no offline equivalent from where researchers can draw parallels. In Chapter 6, we collect and analyze over 41K pairs of altered news headlines by tracking ~411K articles from major US news agencies over a six month period (March to September 2021), identifying that 7.5% articles have at least one"}, {"title": "1.6 Information and Evolution of Personal Biographies", "content": "Occupational identity concerns the self-image of an individual's affinities and socioeconomic class, and directs how a person should behave in certain ways. Understanding the establishment of occupational identity is important to study work-related behaviors. However, large-scale quantitative studies of occupational identity are difficult to perform due to its indirect observable nature. But profile biographies on social media contain concise yet rich descriptions about self-identity. Analysis of these self-descriptions provides powerful insights concerning how people see themselves and how they change over time. In this paper, we present and analyze a longitudinal corpus recording the self-authored public biographies of 51.18 million Twitter users as they evolve over a six-year period from 2015-2021. In particular, we investigate the demographics and social approval (e.g., job prestige and salary) effects in how people self-disclose occupational identities, quantifying over-represented occupations as well as the occupational transitions w.r.t. job prestige over time. We show that males are more likely to define themselves by their occupations than females do, and that self-reported jobs and job transitions are biased toward more prestigious occupations. We also present an intriguing case study about how self-reported jobs changed amid COVID-19 and the subsequent \"Great Resignation\" trend with the latest full year data in 2022. These results demonstrate that social media biographies are a rich source of data for quantitative social science studies, allowing unobtrusive observation of the intersections and transitions obtained in online self-presentation."}, {"title": "1.7 List of Publications and Submissions", "content": "Published 1. Giorgian Borca-Tasciuc, Xingzhi Guo, Stanley Bak, and Steven Skiena. \"Provable Fairness for Neural Network Models using Formal Verification\", European Workshop on Algorithmic Fairness (EWAF 2023), Extended abstract [24]. 2. Zhuoyi Lin, Lei Feng, Xingzhi Guo, Rui Yin, Chee Keong Kwoh, and Chi Xu. COMET: Convolutional Dimension Interaction for Deep Matrix Factorization. In, ACM Transactions on Intelligent Systems and Technology (TIST), 2023 [108] 3. Xingzhi Guo, Baojian Zhou, and Steven Skiena. Subset Node Anomaly Tracking over Large Dynamic Graphs. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery & Data Mining (KDD '22)., New York, NY, USA.[73] 4. Xingzhi Guo, Brian Kondracki, Nick Nikiforakis, and Steven Skiena. 2022. Verba Volant, Scripta Volant: Understanding Post-publication Title Changes in News Outlets. In Proceedings of the ACM Web Conference 2022 (WWW '22). ACM, New York, NY, USA. [71] 5. Syed Fahad Sultan, Xingzhi Guo, and Steven Skiena. 2022. Low-dimensional genotype embeddings for predictive models. In Proceedings of the 13th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics (BCB '22), ACM, New York, NY, USA, Article 52. [154] 6. Xingzhi Guo, Baojian Zhou, and Steven Skiena. 2021. Subset Node Representation Learning over Large Dynamic Graphs. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining (KDD '21). ACM, New York, NY, USA. [29] 7. K. Gillespie, I. C. Konstantakopoulos, X. Guo, V. T. Vasudevan and A. Sethy, \"Improving Device Directedness Classification of Utterances With Semantic Lexical Features,\" ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020. [56] In Submission 1. Xingzhi Guo, Baojian Zhou, and Steven Skiena. Efficient Contextual Node Representation Learning over Dynamic Graphs. 2. Zhen Chen, Xingzhi Guo, Baojian Zhou, Deqing Yang, and Steven Skiena, Accelerating Personalized PageRank Vector Computation. [29] 3. Xingzhi Guo, Dakota Handzlik, Jason J. Jones, and Steven Skiena. Biographies and Their Evolution: Occupational Identities over Twitter.[74] 4. Xingzhi Guo and Steven Skiena. Hierarchies over Vector Space: Orienting Word and Graph Embeddings. [67] 5. Xingzhi Guo, Baojian Zhou, Haochen Chen, Sergiy Verstyuk, and Steven Skiena. Why Do Embedding Spaces Look as They do?"}, {"title": "2 Personalized PageRank", "content": "Personalized PageRank (PPR) [123] is a renowned algorithm that forms the foundation of the proposed algorithms in this thesis. In this section, we present the standard notations and preliminaries to provide an overview of the recent advancements and the current state-of-the-art PPR research. PageRank is an algorithm used to measure the centrality of nodes in a graph. This algorithm was developed by Google to measure the importance of web pages and to improve their search engine results. The PageRank of a node is calculated by measuring the probability of landing on that node during a random walk on the graph. Figure 2.1 illustrates an example of PageRank in the left subfigure where node 1 is the most important nodes of high PageRank value. Personalized PageRank is an extension of the PageRank algorithm that is used to measure the importance of nodes in a graph relative to a particular starting node. In other words, it measures the centrality of nodes in an \"ego net\" around a starting node. An ego net is a sub-graph that consists of a focal node (the starting node) and its neighbors. To calculate PPR, one starts with a random walk on the graph, beginning at the focal node (or starting node). However, to prevent the walk from getting away from the local area, the algorithm also includes a probability of randomly jumping back to the starting node. This ensures that the algorithm explores the entire graph while still focusing on the ego net around the starting node. Likewise, the PPR value of a node is calculated by measuring the probability of landing on that node during this modified random walk. In Figure 2.1 (right), we illustrate the PPR values given the starting node is node 7. Compare to PageRank, although the graph structure is the same, The PPR values are significantly different from PageRank as it favors the nodes around the starting node 7."}, {"title": "2.1 Notations", "content": "We use bold capitalized letter (e.g. A and X) to represent matrics and bold lower letters for vectors (e.g., a and x). Let G(V,E,W,X) be a directed weighted-graph where V = {1,2,...,n} is the node set, E\u2286 V\u00d7V is the set of edges, and W is corresponding edge weights of E. If presented, X \u2208 Rdx|V| is the nodes' d-dim raw attributes or features. For each node v, Neiout(v) stands for out-neighbors of v. For all v\u2208V, the generalized out-degree vector of V is d where v-th entry d(v) = \u2211u\u2208Nei(v) W(v,u) and W(v,u) could be the weight of edge (v,u) in graph G."}, {"title": "2.2 Personalized PageRank Formulation", "content": "As a measure of the relative importance among nodes, Personalized PageRank (PPR), a generalized version of original PageRank [123], plays an important role in many graph mining tasks. In this thesis, most proposed algorithms are built on PPR. Specifically, the Personalized PageRank vector (PPV) of a node s in G is defined as the following: Definition 1 (Personalized PageRank Vector (PPV)). Given a graph G(V,E,W) with |V| = n and |E| = m. Define the lazy random walk transition matrix P=(1-\u03b2)D-1A+\u03b2In, \u03b2\u2208 [0,1) where D is the generalized degree matrix and A is the adjacency matrix of G. \nGiven teleport probability \u03b1\u2208 [0,1) and the source node s, the Personalized PageRank vector of s is defined as:\n$\\pi_{\\alpha,s} = (1 - \\alpha)P^T\\pi_{\\alpha,s} + \\alpha 1_s,$(2.1)\nwhere the teleport probability \u03b1 is a small constant (e.g. \u03b1=.15), and 1s is an indicator vector of node s, that is, s-th entry is 1, 0 otherwise. We simply denote PPV of s as \u03c0s. Clearly, \u03c0, can be calculated in a closed form, i.e., $\\pi_s = \\alpha (I_n - (1 - \\alpha)P^T)^{-1}1_s$ but with time complexity O(n\u00b3). The most commonly used method is Power Iteration [123], which approximates \u03c0s iteratively: $\\pi_s^{(t)} = (1 - \\alpha)P^T\\pi_s^{(t-1)} + \\alpha 1_s$. After t = [log1-\u03b1\u03f5] iterations, one can achieve ||\u03c0s \u2013 \u03c0||1 \u2264 \u03f5. Hence, the overall time complexity of power iteration is O(mlog1-\u03b1\u03f5) with O(m) memory requirement. However, the per-iteration of the power iteration needs to access the whole graph which is time-consuming. Even worse, it is unclear how one can efficiently use power-iteration to obtain an updated \u03c0, from Gt to Gt+1. Other types of PPR can be found in [172, 176, 182] and references therein."}, {"title": "2.3 Approximate PPR using Forward Push Algorithm", "content": "forward push algorithm [9], a.k.a. the bookmark-coloring algorithm [15], approximates \u03c0s(v) locally via an approximate ps(v). The key idea is to maintain solution vector ps and a residual vector rs (at the initial rs=1s,ps=0).\n\nAlgorithm 1 FORWARDLOCALPUSH [194]\n1: Input: ps,rs,G,\u03f5,\u03b2=0\n2: while \u2203u,rs(u) > \u03f5d(u) do\n3:  PUSH(u)\n4: while \u2203u,rs(u) <\u2212\u03f5d(u) do\n5:  PUSH(u)\n6: return (ps,rs)\n7: procedure PUSH(u)\n8:  ps(u)+=\u03b1rs(u)\n9:  for v\u2208Nei(u) do\n10:  rs(v)+=(1-\u03b1)rs(u)(1-\u03b2)/d(u)\n11:  rs(u)=(1-\u03b1)rs(u)\u03b2\n\nAlgorithm 1 is a generalization from Andersen et al. [9] and there are several variants of forward push [9, 15, 111], which are dependent on how \u03b2 is chosen (we assume \u03b2=0). The essential idea of forward push is that, at each PUSH step, the frontier node u transforms her a residual probability rs(u) into estimation probability ps(u) and then pushes the rest residual to its neighbors. The algorithm repeats this push operation until all residuals are small enough 2. Methods based on local push operations have the following invariant property.\nLemma 2 (Invariant property [83]). FORWARDLOCALPUSH has the following invariant property\n$\\pi_s(u) = p_s(u) + \\sum_{v \\in V} r_s(v) \\pi_v(u), \\forall u \\in V.$(2.2)"}, {"title": "2.4 Approximate PPR as l\u2081Regularized Optimization", "content": "Interestingly, recent research [51, 52] discovered that Algorithm 1 is equivalent to the Coordinate Descent algorithm, which solves an explicit optimization problem. Furthermore, with the specific termination condition, a l\u2081-regularized quadratic objective function has been proposed as the alternative PPR formulation (Lemma 4).\n\nLemma 4 (Variational Formulation of Personalized PageRank). Solving PPR in Equ.2.1 is equivalent to solving the following l\u2081-regularized quadratic objective function where the PPR vector \u03c0:=D1/2x* :\n$x^* = \\text{argmin}_{x} f(x) := \\frac{1}{2} x^T W x + x^T b + \\epsilon ||D^{1/2} x||_1, \\text{ where }\\$(2.3)\nW=D-1/2(D\u2212(1\u2212\u03b1)A)D-1/2, b=-\u03b1D-1/2es\n\nWe present the proof details in Appendix 5.8.1.\nLemma 4 provides a unique perspective to understand PPR and tackle it with intensively studied optimization methods for the l\u2081-regularization problem, specifically proximal gradient methods such as ISTA and FISTA.\nISTA Solver for PPR: Equ.2.3 has the composition of a smooth function g(x) and a non-smooth function h(x), Note that\n$\\nabla^2 g(x) = W = \\alpha I_n + (1-\\alpha) (I_n - D^{-1/2} A D^{-1/2})$,Normalized Laplacian of $A_{\\text{max}} \\le 2$"}, {"title": "2.5 Incrementally Maintain PPR", "content": "For a dynamic graph, the graph snapshot at time t is denoted as Gt (Vt,Et,Wt, X(t)). The notation ( subscript for time t) are used for other graph attributes. For instance, if there is no ambiguity, the degree of a node u can be denoted as d(t) or du). To maintain the PPVs (PPR Vectors) for a dynamic graph, the naive approach is to recalculate the PPVs from scratch. However, this approach comes at the cost of a significant computational budget. Interestingly, Zhang et al. [194] proposed a novel updating strategy for calculating dynamic PPVs. Given a new edge eu,v with dt+1(u)=dt(u)+1 at time t+1, the PPR adjustments are : $\\pi^{t+1}(u) = \\pi^t (u) * \\frac{d^t(u) + 1}{d^t(u)}, rt+1(u) =rt(u)- \\frac{1 - \\alpha \\pi^t (u)}{\\alpha d^t(u)}, rt+1(v) = r^t (v)+ \\frac{1 - \\alpha}{\\alpha d^t(u)} $\n\nHowever, the original update of per-edge event strategy proposed in [194] is not suitable for batch update between graph snapshots. Guo et al. [66] propose to use a batch strategy, which is more practical in real-world scenario where there is a sequence of edge events between two consecutive snapshots. This motivates us to develop a new algorithms for dynamic PPVs and then use these PPVs to obtain high quality dynamic embedding vectors. In Chapter 3, 4, 5 we present our efficient algorithms that build upon the key preliminaries discussed above."}, {"title": "3 Subset Node Embeddings over Dynamic Graph", "content": null}, {"title": "3.1 Introduction", "content": "Graph node representation learning aims to represent nodes from graph structure data into lower dimensional vectors and has received much attention in recent years [65, 78, 79, 102, 126, 140, 159]. Effective methods have been successfully applied to many real-world applications where graphs are large-scale and static [188]. However, networks such as social networks [14], knowledge graphs [90, 174, 175, 175], and citation networks [36] are usually time-evolving where edges and nodes are inserted or deleted over time. Computing representations of all vertices over time is prohibitively expensive because only a small subset of nodes may be interesting in a particular application. Therefore, it is important and technical challenging to efficiently learn dynamic embeddings for these large-scale dynamic networks under this typical use case. Specifically, we study the following dynamic embedding problem: We are given a subset S={V1,V2,...,Vk} and an initial graph Gt with t=0. Between time t and t+1, there are edge events of insertions and/or deletions. The task is to design an algorithm to learn embeddings for k nodes with time complexity independent on the number of nodes n per time t where k\u226an. This problem setting is both technically challenging and practically important. For example, in the English Wikipedia graph, one needs focus only on embedding movements of articles related to political leaders, a tiny portion of the whole Wikipedia. Current dynamic embedding methods [45, 120, 200, 205, 207] are not applicable to this large-scale problem setting due to the lack of efficiency. More specifically, current methods have the dependence issue where one must learn all embedding vectors. This dependence issue leads to per-embedding update is linear-dependent on n, which is inefficient when graphs are large-scale. This obstacle motivates us to develop a new method. In this paper, we propose a dynamic personalized PageRank embedding (DYNAMICPPE) method for learning a subset of node representations over large-sale dynamic networks. DYNAMICPPE is based on an effective approach to compute dynamic PPVs [194]. There are two challenges of using Zhang et al. [194] directly: 1) the quality of dynamic PPVs depends critically on precision parameter \u03f5, which unfortunately is unknown under the dynamic setting; and 2) The update of per-edge event strategy is not suitable for batch update between graph snapshots. To resolve these two difficulties, first, we adaptively update \u03f5 so that the estimation error is independent of n,m, thus obtaining high-quality PPVs. Yet previous work"}, {"title": "3.2 Related Work", "content": "There are two main categories of works for learning embeddings from the dynamic graph structure data. The first type is focusing on capturing the evolution of"}, {"title": "3.3 Preliminaries", "content": "Notations for dynamic graphs We use [n] to denote a ground set [n] := {0,1,...,n-1}. The graph snapshot at time t is denoted as Gt (Vt,Et). More specifically, the embedding vector for node v at time t denoted as w\u2208 Rd and d is the embedding dimension. The i-th entry of w is w(i) \u2208R. The embedding of node v for all T snapshots is written as W=[w,w,...,w]. We use nt and mt as the number of nodes and edges in Gt which we simply use n and m if time t is clear in the context. Given the graph snapshot Gt and a specific node v, the personalized PageRank vector (PPV) is an n-dimensional vector \u03c0\u2208R\u201d and the corresponding i-th entry is \u03c0(\u03b9). We use p\u2208R\" to stand for a calculated PPV obtained from a specific algorithm. Similarly, the corresponding i-th entry is pt(i). The teleport probability of the PageRank is denoted as \u03b1. The estimation error of an embedding vector is the difference between true embedding w and the estimated embedding w is measure by || || 1 := \u2211=1|\u03c9 (\u2170) -\u0175(i)|.\""}, {"title": "3.3.1 Dynamic graph model and its embedding", "content": "Given any initial graph (could be an empty graph), the corresponding dynamic graph model describes how the graph structure evolves over time. We first define the dynamic graph model, which is based on Kazemi and Goel [96].\nDefinition 5 (Simple dynamic graph model [96]). A simple dynamic graph model is defined as an ordered of snapshots G\u00ba,G1, G2,...,GT where G\u00ba is the initial graph. The difference of graph Gt at time t=1,2,...,T is \u2206Gt (\u2206Vt,\u2206Et):=Gt\\Gt-1 with \u25b3Vt:=Vt\\Vt-1 and \u2206Et :=Et\\Et\u22121. Equivalently, AGt corresponds to a sequence of edge events as the following\n$\\AG_t = \\{e_1, e_2,..., e_m \\},$ (3.1)\nwhere each edge event e has two types: insertion or deletion, i.e, e=(u,v,event) where event \u2208 {Insert,Delete} 3. The above model captures the evolution of a real-world graph naturally where the structure evolution can be treated as a sequence of edge events occurred in this graph. To simplify our analysis, we assume that the graph is undirected. Based on this, we define the subset dynamic representation problem as the following."}, {"title": "3.4 Proposed method", "content": "To obtain dynamic embedding vectors, the general idea is to obtain dynamic PPVs and then project these PPVs into embedding space by using two kernel functions [128, 177]. In this section, we present our proposed method DYNAMICPPE where it contains two main components: 1) an adaptive precision strategy to control the estimation error of PPVs. We then prove that the time complexity of this dynamic strategy is still independent on n. With this quality guarantee, learned PPVs will be used as proximity vectors and be \"projected\" into lower dimensional space based on ideas of Verse [161] and InstantEmbedding [128]. We first show how can we get high quality PPVs and then present how use PPVs to obtain dynamic embeddings. Finally, we give the complexity analysis."}, {"title": "3.4.1 Dynamic graph embedding for single batch", "content": "For each batch update \u2206Gt, the key idea is to dynamically maintain PPVs where the algorithm updates the estimate from pt-1 to pt and its residuals from rt-1 to rt. Our method is inspired from Guo et al. [66] where they proposed to update a personalized contribution vector by using the local reverse push 4. The proposed dynamic single node embedding, DYNAMICSNE is illustrated in Algorithm 3. It takes an update batch AGt (a sequence of edge events), a target node s with a precision et, estimation vector of s and residual vector as inputs. It then obtains an updated embedding vector of s by the following three steps: 1) It first updates the estimate vector p and r from Line 2 to Line 9; 2) It then calls the forward local push method to obtain the updated estimations, p; 3) We then use the hash kernel projection"}, {"title": "3.4.2 DYNAMICPPE", "content": "Our finally algorithm DYNAMICPPE is presented in Algorithm 4. At every beginning, estimators p are set to be zero vectors and residual vectors rare set to be unit vectors with mass all on one entry (Line 4). The algorithm then call the procedure DYNAMICSNE with an empty batch as input to get initial PPVs for all target nodes 7 (Line 5). From Line 6 to Line 9, at each snapshot t, it gets an update batch AGt at Line 7 and then calls DYNAMICSNE to obtain the updated embeddings for every node v.\n\nAlgorithm 4 DYNAMICPPE(G0,S,\u03f5,\u03b1)\n1: Input: initial graph G\u00ba, target set S, global precision \u03f5, teleport probability \u03b1\n2: t=0\n3: for s\u2208S:={U1,U2,...,Uk} do\n4:  p=0,r=1s\n5:  DYNAMICSNE(G\u00ba,\u00d8,s,p,r,1/mo,\u03b1)\n6: for t\u2208 {1,2,...,T} do\n7:  read a sequence of edge events \u2206Gt:=Gt\\Gt-1\n8:  for s\u2208S:={V1,V2,...,Uk} do\n9:  w = DYNAMICSNE(Gt,\u2206Gt,s,pt-1,rt-1,\u03f5/mt,\u03b1)\n10: return W=[w,w,...,w],\u2200sES.\n\nBased on our analysis, DYNAMICPPE is an dynamic version of InstantEmbedding. Therefore, DYNAMICPPE has two key properties observed in [128]: locality and global consistency. The embedding quality is guaranteed from the fact that InstantEmbedding implicitly factorizes the proximity matrix based on PPVs [161]."}, {"title": "3.4.3 Complexity analysis", "content": "Time complexity The overall time complexity of DYNAMICPPE is the k times of the run time of DYNAMICSNE. We summarize the time complexity of DYNAMICPPE as in the following theorem\n\nTheorem 10. The time complexity of DYNAMICPPE for learning a subset of k nodes is $O(\\frac{km_t}{\u03b1^2}+ \\frac{kd_t}{(e\u03b1^2)}+ \\frac{m_t}{(e\u03b1)}+ k T\\text{min}\\{n, \\frac{m}{k}\\})$\n\nSpace complexity The overall space complexity has two parts: 1) O(m) to store the graph structure information; and 2) the storage of keeping nonzeros of p and r. From the fact that local push operation [9], the number of nonzeros in p is at most. Thus, the total storage for saving these vectors are O(kmin{n, m}). Therefore, the total space complexity is O(m+kmin(n,m)). Implementation Since learning the dynamic node embedding for any node v is independent with each other, DYNAMICPPE is are easy to parallel. Our current implementation can take advantage of multi-cores and compute the embeddings of S in parallel."}, {"title": "3.5 Experiments", "content": "To demonstrate the effectiveness and efficiency of DYNAMICPPE, in this section, we first conduct experiments on several small and large scale real-world dynamic graphs on the task of node classification, followed by a case study about changes of Chinese cities in Wikipedia graph during the COVID-19 pandemic."}, {"title": "3.5.1 Datasets", "content": "We compile the following three real-world dynamic graphs, more details can be found in Appendix 3.7.3. Enwiki20 English Wikipedia Network We collect the internal Wikipedia Links (WikiLinks) of English Wikipedia from the beginning of Wikipedia, January 11th, 2001, to December 31, 2020 8. The internal links are extracted using a regular expression proposed in [39]. During the entire period, we collection 6,151,779 valid"}, {"title": "3.5.2 Node Classification Task", "content": "Experimental settings We evaluate embedding quality on binary classification for Academic graph (as same as in [205]), while using multi-class classification for other tasks. We use balanced logistic regression with l2 penalty in on-vs-rest setting, and report the Macro-F1 and Macro-AUC (ROC) from 5 repeated trials with 10% training ratio on the labeled nodes in each snapshot, excluding dangling nodes. Between each snapshot, we insert new edges and keep the previous edges. We conduct the experiments on the aforementioned small and large scale graph. In small scale graphs, we calculate the embeddings of all nodes and compare our proposed method (DynPPE.) against other state-of-the-art models from"}, {"title": "3.5.3 Change Detection", "content": "Thanks to the contributors timely maintaining Wikipedia, we believe that the evolution of the Wikipedia graph reflects how the real world is going on. We assume that when the structure of a node greatly changes, there must be underlying interesting events or anomalies. Since the structural changes can be well-captured by graph embedding, we use our proposed method to investigate whether anomalies happened to the Chinese cities during the COVID-19 pandemic (from Jan. 2020 to Dec. 2020). Changes of major Chinese Cities We target nine Chinese major cities (Shanghai"}]}