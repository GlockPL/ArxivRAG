{"title": "A Comparison of LLM Fine-tuning Methods and Evaluation Metrics with Travel Chatbot Use Case", "authors": ["Sonia Meyer", "Shreya Singh", "Bertha Tam", "Christopher Ton", "Angel Ren"], "abstract": "This research compares large language model (LLM) fine-tuning methods, including Quantized Low Rank Adapter (QLoRA), Retrieval Augmented fine-tuning (RAFT), and Reinforcement Learning from Human Feedback (RLHF), and additionally compared LLM evaluation methods including End to End (E2E) benchmark method of \"Golden Answers\", traditional natural language processing (NLP) metrics, RAG Assessment (Ragas), OpenAI GPT-4 evaluation metrics, and human evaluation, using the travel chatbot use case. The travel dataset was sourced from the the Reddit API by requesting posts from travel-related subreddits to get travel-related conversation prompts and personalized travel experiences, and augmented for each fine-tuning method. We used two pretrained LLMs utilized for fine-tuning research: LLaMa 2 7B, and Mistral 7B. QLORA and RAFT are applied to the two pretrained models. The inferences from these models are extensively evaluated against the aforementioned metrics. The best model according to human evaluation and some GPT-4 metrics was Mistral RAFT, so this underwent a Reinforcement Learning from Human Feedback (RLHF) training pipeline, and ultimately was evaluated as the best model. Our main findings are that: 1) quantitative and Ragas metrics do not align with human evaluation, 2) Open AI GPT-4 evaluation most aligns with human evaluation, 3) it is essential to keep humans in the loop for evaluation because, 4) traditional NLP metrics insufficient, 5) Mistral generally outperformed LLaMa, 6) RAFT outperforms QLoRA, but still needs postprocessing, 7) RLHF improves model performance significantly. Next steps include improving data quality, increasing data quantity, exploring RAG methods, and focusing data collection on a specific city, which would improve data quality by narrowing the focus, while creating a useful product.", "sections": [{"title": "I. INTRODUCTION", "content": "The tourism and travel industry took a nosedive during the COVID-19 pandemic, which was declared by the World Health Organization (WHO) on March, 11, 2020, due to the severe lockdown measures restricting travel. [1] Now, post-COVID, travelers are coming back with a vengeance and so was the \"revenge travel\" phenomena named to reflect the negative aspects of tourists exceeding the carrying capacity of destinations and business and governments desperately scrambling to recover income and grow the economy once again at the cost of tourism quality and sustainability. [2] The travel and tourism industry is projected to grow by 5.8% per year for the next decade, which outpaces the overall economy at 2.7%. [3]\nThe industry has faced challenges with widespread labor shortages due in short to poor working conditions, which along with the rise of large language model (LLM) applications presents a unique opportunity to incorporate technology into the travel and tourism industry. [3] Already, travelers overwhelmingly prefer to utilize technology for the full stack of travel from planning to booking to implementation. 74% of travelers use the internet for travel planning, 45% specifically use smartphones, 36% are willing to pay more for an interactive and smooth booking experience, and 80% prefer to use self-service to find information. [4] Technology is institutionalized with the global distribution system (GDS), an international reservation system for travel agents and suppliers, which is essential to online travel booking and customer service applications. [4]\nLarge language models (LLMs) were made possible with the 2017 \"Attention is All You Need\" [5] transformer architecture without recurrent networks or convolutions, and work by predicting masked words or upcoming words. [6] With LLMs, more data results in better predictions, most models have at least one billion parameters. [6] They have been used for many purposes recently including carrying out task-based instructions, multilingual processing, medical and clinical applications, and programming. [6] According to the literature, LLMs are typically trained on a huge corpus of text data with hundreds of billions of hyperparameters. [7] Just in this year, 2023, we have seen several incredible applications utilize powerful LLMs, including in the booming travel industry. With the projected growth of the travel industry post-COVID and recent technological advances, there are potent opportunities for groundbreaking innovation with tangible effects on tourism income. Current artificial intelligence (AI) applications in travel focus on booking, however, there is a gap when it comes to artificial intelligence (AI) assisted travel planning and recommendations (as of May 2024). While not specifically tuned for marketing, ChatGPT remains a competitive chatbot for this application with its high quality conversational ability, however, to get customized travel recommendations, it requires very specific and detailed prompts, and it has a knowledge cut off of September 2021.\nUsing the travel use case, this research compared two finetuning methods: 1) Quantized Low Rank Adapters (QLoRA) and 2) Retrieval-Augmented fine-tuning (RAFT). Two pretrained 7B models, LLaMa 2 and Mistral, are fine-tuned with these two methods, resulting in four models, then their inferences are evaluated against an extensive set of metrics using GPT as a baseline for comparison. The best model is finetuned with the third method, 3) Reinforcement Learning from Human Feedback (RLHF), resulting in 5 total models (see Figure 1). The evaluation metrics include: End to End (E2E) benchmark method of \"Golden Answers\", traditional natural language processing (NLP) metrics, RAG Assessment (Ragas), OpenAI GPT-4 evaluation metrics, and human evaluation.\nA highly effective fine-tuning technique called QLoRA was introduced in 2023. [8] Given the significant computational resources required for fine-tuning Large Language Models (LLM), QLoRA implements several innovative approaches to save memory without compromising performances. It applies gradient backpropagation through a frozen 4-bit quantized pretrained language model into Low Rank Adapters (LoRA). Additionally, it employs double quantization to lower the average memory demand. Furthermore, it incorporates paged optimizers to handle memory spikes. The second method implemented was a novel method called Retrieval Augmented Fine Tuning (RAFT), a training procedure for domain-specific Retrieval Augmented Generation (RAG), which can adapt pretrained LLMs like LLaMa 2 and Mistral for RAG in specific domains, such as ours in travel. [9] RAG is a text generation method for outsourcing relevant information, from a knowledge base, or a large corpus of relevant, factual and quality information, to supply an LLM with contextual clues for producing factual responses. Then, an RLHF training pipeline will be done for domain-specific LLM curation, aligned with human preferences, with reward model training. [10]\nbetter perform on RAG tasks. The key concept is data augmentation to generate \"question, answer, document\" triplets before fine-tuning. This is done by generating realistic questions paired with elaborate chain of thought answering scheme and purposefully including relevant and irrelevant context documents. Through a chain of thought with the distractor documents, the model learns to extract the correct information from the entire chunk of context through reasoning, ignoring the distractors. RAFT operates by training the model to disregard any retrieved documents that do not contribute to answering a given question, thereby eliminating distractions. The optimal ratio of oracle to distractor documents during training varies across datasets, but including some distractors improves generalization. Finally, during RAG, RAFT retrieves the top-k documents from the database. With RAFT, Zhang et. al addresses the limitations of fine-tuning with RAG, and evaluate their new methods on datasets including: PubMed, HotpotQA, and code documentation. RAFT consistently outperforms standard fine-tuning or RAG alone. [9]\nWith the following literature survey on the existing research, we tried to gain an understanding on the current state of one of the most recent and fastest growing technologies (see Figure 2). We attempted to get the basic understanding of the LLMs, different types of LLM chatbots, technologies, approach used, learnt different types of approaches works better in certain types of research works, and the performance of these may vary depending on the tasks. Some of the research studies shows, some chatbots perform well without the fine-tuning as well. Below is the summary of different research papers the team has gone through, some of the papers are summarized in the tabular form as well (see Table II), showcasing the brief on the goal, approach used and its conclusion. These papers helped the team to have a better understanding on the topic."}, {"title": "II. RELATED WORK", "content": "Through a comprehensive review of literature and existing research papers, we build an understanding for state-or-theart techniques and approaches aimed to achieve competitive performances. New innovations are expressed in different variations for enhancing large language models. Table II summarizes the selection of models, objective addressed, unique approach, performance results and ultimate findings.\nA survey of existing solutions show that initiatives have been developed to overcome shortcoming identified with public general-purpose and pretrained LLM. Some of the feature performance issues garnered from literature review are potential risks of hallucination, underdeveloped retrieval approaches, and inefficiencies in the use of computational resources.\nZhang et. al introduced Retrieval Augmented fine-tuning (RAFT) as a novel training strategy for fine-tuning LLMs to reference, ChatGPT uses between 1.5 and 3 billion parameters depending on the model, while these imitation models use only 0.3-150 million parameters. [15] Such imitation models include Alpaca, Vicuna, Koala, and GPT-4ALL. [15] Despite comparable evaluation on crowd rating and canonical NLP benchmarks and even better performance on simple instructions, the imitation models simply excel at imitating ChatGPT deceiving crowd raters, but actually there is a big gap when it comes to accuracy and breaking down complex tasks. [15]\nZhou et al. (2023) introduces LIMA, a 65B parameter language model fine-tuned on 1,000 curated prompt-response pairs. [16] The model was highly generalizable, and specifically mentions having trip planning ability. Ablation experiment showed diminishing returns with scaling quantity of data, just 30 dialogue examples dramatically improved multiturn conversation ability, which was measured by the rate of excellent responses going from 45% to 75%.\nLin et al. (2023) propose LLM-EVAL, a unified automatics evaluation method for conversations using LLMs, which is much more simplified and efficient in comparison to current methods using human annotation, ground truth responses, and multiple LLM prompts. LLM-EVAL is a single prompt based evaluation method that uses a unified schema to evaluate conversational quality. Lin claimed that traditional evaluation metrics like BLEU and ROGUE are insufficient for natural conversations. LLM-EVAL outperformed other supervised, unsupervised, and LLM-based evaluation metrics. Supervised techniques include dialogue quality (fluency, relevance, knowledge) and topic transition known as GRADE. Unsupervised techniques include DEB, which is BERT fine-tuned with relevant and adversarial irrelevant responses, and FED, which uses embedding features and probabilities to determine dialogue quality. LLM based methods include GPTScore to assign quality ratings, using InstructGPT, and G-EVAL which uses chain of thought and a form filling paradigm. They use several chat datasets like DSTC10 Hidden Set, including JSALT, NVM, ESL, Topical-DSTC10, and Persona-DSTC10. They took Spearman correlation coefficient to compare the difference between human ratings and automatic metrics across appropriateness, content, grammar, and relevance, and additionally tested and compared the use of LLM-EVAL on different LLMs including Anthropic Claude and OpenAI ChatGPT. Claude and ChatGPT are optimized for chat, and performed better with LLM-EVAL. [17]\nZhao et al. (2023) surfaces the observation that as multimodal capabilities of generative algorithms become world apparent, their explicit knowledge is subjected to hallucination. World knowledge exists in the forms of images, visuals, and audio formats. Each format would require its own domain-specific retrieval-augmented generation method. Specifically for tasks that query for knowledge, there exists 2 main open-ended challenges. Conventionally, one of the challenges involves storing training data within the parameters of a pretrained model. Limitations to this approach include the size constraint of the parameters and the need to update the parameters with new knowledge. The second challenge is pairing a generative model with a retrieval step to achieve better responses from the input data or external references. A remedy for reducing hallucinations is a foundation of standardized knowledge index. However it is structured, this foundation can be transformed into a dense representation that is optimal for efficient storage and search.The recommendation is to integrate retrieval-based practices into the pre-training process so that the generative model can better generalize against new knowledge and adapt accordingly. [7]\nBanerjee et al. (2023) found that LLM-powered chatbots are effective platforms to provide services to customers, but there is an important need to assess its performance in efficient and measurable ways. In order to benchmark a chatbot's performance, accuracy and usefulness will serve as the two main characteristics for evaluation. The first being how accurate the chatbot is able to complete a set of tasks and the second being how useful it is able to fulfill a user's needs. The proposed metrics is the End to End (E2E) benchmark method using cosine similarity given a set of predefined answers called the \"Golden Answers.\" The E2E metric compares the chatbot's output results to an expert human answer, or the \"Golden Answer.\" They used the Universal Sentence Encoder (USE) and Sentence Transformer (ST) models to create vectorized embeddings and obtain the underlying context and meaning of the texts [18]. Banerjee et al. and Lin et al. argues that traditional metrics like Recall-Oriented Understudy for Gisting Evaluation (ROUGE), which uses n-grams overlaps, are insufficient to capture the deep complexity and semantic meaning of a conversational chatbot. [18] [17] E2E is user centric, considers semantic meaning, and improved with advanced prompt engineering alongside human evaluation metrics, unlike ROUGE. [18]\nMaroengsit et al. (2019) discusses the difficulties to evaluate and compare different chatbot systems based on their effectiveness, efficiency, ability to satisfy users, and achieve their specific goal or desired prompt. Chatbots have been around since the 1960's and there has been many technological changes to advance chatbot development. However, there has not been an updated holistic study on evaluation methods of chatbots and there's a huge potential for chatbot developers to apply these evaluation techniques to their own work. There are two types of chatbots, rule-based and AI-based. Depending on the chatbot type there will be different evaluation methods used to asses those architectures. The main steps of architecture processes in chatbot systems are Natural Language Processing (NLP), Natural Language Understanding (NLU), and Natural Language Generation (NLG). While NLP focuses on text preprocessing via pattern matching, parsing, Term FrequencyInverse Document Frequency (TF-IDF), or Word2Vec, NLU is processing by obtaining semantic understanding of texts. This is accomplished by having the model understand the conversation between the user and the model itself using the following techniques commonly found in the literature: intent classification, dialogue planning, name entity recognition, vector recognition using cosine similarity, lexicon, or long short-term memory. NLG is response generation, where the point is to gain information from a user's response by asking them a question and seeing how they reply. Essentially, NLG determines how the system, or agent, responds to the user base from the information gathered in each conversation. There are three main evaluation methods: content evaluation, user satisfaction, and functional evaluation. Content evaluation uses automatic evaluation metrics such as precision and recall using BLEU or ROUGE. This is beneficial because even though having human judges for evaluation is accurate, it takes a long time and is expensive. Thus, it is more efficient and faster evaluation time with lower costs. User satisfaction is based on a Likert five-point scale, which uses human users' satisfaction surveys to evaluate complex chatbot systems where there is not just one correct answer. However, the issue is the presence of bias. Future chatbot developers can use these results as a guideline when deciding the most well-suited evaluation methods based on their uses cases. [19]\nSvikhnushina et al. (2023) proposed a Dialog system Evaluation framework using Prompting (DEP) to specifically evaluate social chatbots with prompt engineering, where the prompts guide the process of how a chatbot generates a response. The issue is that prompt-based learning hasn't been studied enough when evaluating dialog systems, so they focused on a comprehensive evaluation of dialog chatbot systems using prompting. There are three types of prompting, zero-shot, one-shot, and few-shot prompting. Zero-shot prompting of LLMs do not need previously labeled data to do new tasks, while few-shot prompting requires the LLMs to have some labeled prompts to conduct new tasks. The three evaluation scores they used were \u201cBad,\u201d \u201cOkay,\u201d and \u201cGood.\u201d The DEP method essentially collects chat logs between the LLM and chatbot system and then prompts the LLM to generate scores based on the chat logs. However, there are several problems such as being unable to \u201ccapture subjective perceptions\u201d. [20] when the user chats with the system, and likely is not able to generalize to real-world settings. Therefore, there is a need and importance to optimize social chatbots by streamlining the evaluation process in a more efficient manner. Their solution to streamline the evaluation process was to removing human involvement during each step, saving time and cost. Instead of using LLMs that use prompting, which tend to create misinformation, they gave instructions and prompted LLMs to conduct a specific behavior using the proposed DEP framework, which they found yielded better results. This indicates how you can leverage LLMs to create realistic conversations. Thus, they found that the most well-performing prompts contain few-shot learning and instructions, which show its ability to generalize on a corpora of data. [20]\nIrvine et al. (2023) focuses on utilizing human feedback and intuitive metrics to create engaging social chatbots to improve user retention and engagement based on human feedback. In order to aid chatbots in understanding the intentions of a user's query, feedback was collected automatically generated pseudolabels during every interaction with the user. These serve as user engagement proxies to train a reward model, so when the chatbot generates a poor sample response during inference time, or the time it takes to complete the forward propagation, then it will be given a low score. These are basically the worst responses that will be rejected because they do not make sense, are not appropriate responses, etc. Four types of metrics were used, such as mean conversation length (MCL) to measure the chatbot's engagement level with a user, retry rate, user star rating, and retention rate. The retry rate is how often user requests to regenerate a response and the user star rating is essentially user satisfaction feedback based on a rating scale. On the other hand, retention rate is the most challenging and expensive evaluation rate. The problem is that it is manually extensive, costly, and time consuming to have any human involvement such as expert annotators rank responses. Although human experts can improve the responses that LLMs generate, these disadvantages may likely limit the chatbot development and evaluation process instead. Therefore, there's a lack of engaging chatbots that retains users to come back and talk, so instead of purely fine-tuning LLMs on conversational data, one may incorporate human feedback during development to collect more data for the human reference or knowledge base, which has shown improved retention scores of over 30%. [21]\nThus, the solution is to use human feedback to create engaging chatbots and improved engagement levels. This assumes that the longer a user chats with the system, the more likely they are to return and chat in the future, indicating higher retention. It was found that chatbots have longer average user interactions and user retention when grouped training reward models with the human feedback methodology. [21]\nWong et al. (2023) demonstrate how LLMs, particularly ChatGPT, have the ability to drastically change the tourism industry, such as improving customer experience in 3 travel stages: before the trip, en-route, and post-trip encounters. Unlike previous AI, it is able to improve trip planning efficiency, create more personalized recommendations by having it act as a tour guide or local, and having a readily available and streamlined personal guide in your pocket during the trip in cases of emergency. ChatGPT provides efficient travel solutions by looking for travel information, filtering out irrelevant travel information, and then makes a comparison of different options in a neat fashion that is human-readable. The problem is when travelers research where to go using the internet, through travel agencies, or from word of mouth, there is a huge abundance of information. This makes it difficult, overwhelming, and time-consuming to search for valuable information and make decisions on where and what to go or do. There are a lot of communication problems when traveling such as a lack of cultural understanding, language barriers, etcetera. The largest limitation of utilizing LLMs such as ChatGPT for travel, is that it is still not entirely accurate since sources can be misleading. ChatGPT is unable to fully explain their answers in a rational way, bringing up transparency and accountability issues. The responses can be biased bringing up ethical issues and spreading disinformation. This is because there could be prejudiced information in the training data itself and there is limited domain knowledge as it is not able to be trained on certain semantics and ways of human expression, which could cause the LLM to misinterpret the user query. Also, the data is limited to the year 2021 so any information after that date is not in the training set at all. However, despite these limitations, one can utilize the benefits of using LLMs such as ChatGPT across the three stages for a tourist, before, during, and after traveling. These advantages and disadvantages of LLMs in the travel and tourism industry are important takeaways to take into consideration while developing our travel domain-specific chatbot. [22]"}, {"title": "III. TRAVEL DATASET", "content": "Table III provides a high level overview and Figure 3 provides a visualization of the data structure needed for each model. The travel dataset was collected entirely from Reddit, and specifically pulled from travel domain subreddits. There are a plethora of travel subreddits available, with r/travel being the largest and most active with 8.9 million subscribers. It is one of the most popular communities in the top 1% of subreddits as of December 2023. [30] Calls made to the Reddit API to curate a corpus of Question-and-Answer (Q&A) formatted data, collected conversational data from 201 subreddits. [31] This consists of 27 travel-related subreddits, 30 country subreddits, and 144 city subreddits which include r/solotravel, r/travelhacks, r/roadtrip, and so on so the data can capture the context of conversations. The data collected for this project was for the purpose of fine-tuning the chatbot, providing domain specific knowledge, and to provide current updated travel information to address LLM knowledge cutoffs. Due to the Large Language Model (LLM) capabilities of generating recommendations due to its pretraining [32], the Reddit post would serve as the question and the compiled comments as the answer. Tools used for data collection and processing include: Python, PRAW, Pandas, NLTK, Regex, BERTopic, and Ollama.\nChatGPT was also used to generate the top 30 countries for tourism, and tourist cities were collected from Wikipedia. A targeted list of travel subreddits was collected from a blog post, however, subreddits with a heavy focus on sharing images were omitted. [30] In total, there were 201 subreddits: 27 travel related subreddits, 30 country subreddits, and 144 city subreddits. There are a total of 16,300 entries sourced from Reddit, which was further preprocessed and reduced the entries from 16,300 to 10,500 rows.\nReddit data is appropriate for question and answer type data as each post has multiple comments or answers. Reddit is categorized by subreddits, which allow for convenient filtering for travel related questions and answers by targeting travel specific domains. Python Reddit API Wrapper (PRAW) was used to make API requests. [33] Two methods of collecting posts were used: 1) hot, and 2) top. Top means best of all time and takes a time frame as a parameter. Top 1,000 posts of the year in travel related subreddits, top 50 posts of the year in country subreddits, and top 20 posts of the year in city subreddits. Hot is what is currently trending, so these posts are collected daily with smaller requests to reduce requesting the same post multiple times if it is trending for multiple days. Hot 100 posts of travel related subreddits, and hot 20 in country and city subreddits. The top of the year posts were collected at once for the prior year, whereas the hot daily posts were collected daily for 4 days. Despite making so many requests, only 4,000+ posts were collected, perhaps due to not having enough posts within subreddits to meet the request 1,000 or some subreddits did not exist. Figure 4 is a snapshot of the hottest post on the travel subreddit titled \"Passport Questions & Issues Megathread (2023),\" along with two sample comments from multiple Reddit users, along with details such as upvotes and the date it was posted. [34]\nFigure 5 is a sample of the raw data collected from Reddit. It shows the hottest post on the travel subreddit in json format, the title being \"Passport Questions & Issues Megathread (2023).\" The kind attribute represents \"one of\", in this case, it is one of a widget called listing, which means a table listing format was done on this specific post to display information in a clear manner. Within this json file, all the posts are contained within the \"data\" key, where the \"after\" key within the \"data\" key contains a value \u201ct3_17r1pqu,\u201d which is essentially the unique ID of the Reddit post. The \u201cchildren\u201d key contains all the information about the hot posts in the travel subreddit. Then, the \u201cselftext\u201d key essentially contains a long list of texts as part of the body field of the post. [35] Comparing Figure 5 to 4 the information in the \u2018selftext' key matches the body of the Reddit post.\nTo estimate an LLM's generalizability against topics of any domain, it is important to evaluate its performance across a set of diverse and representative questions. Given the objective of building a prototype that specializes in providing travel recommendations, the training data should be comprehensive of the real world and have a high cardinality of topics relating to travel. To assess the current landscape of Reddit data under the travel subreddit, a sample of the top 1000 posts can be collected as an experiment. An unsupervised approach, BERTopic modeling, clustered documents of the same topics and identified various categorizations. Given the stochastic nature across different clustering models, an ensemble of these results may produce more consistent outcomes. By grouping like-documents together, it would be a step towards reducing the amount of noise in each comment under the respective threads by summarizing the main points with transfer learning of open source LLMs. Downstream for this workflow are strategic partitioning to target potential model degradation areas. If a model is habitually underperforming when responding to questions of a particular domain, additional data collection and processing for this domain are actionable steps. Eventually, an holistic approach for evaluating the general knowledge capabilities of LLMs can be conducted by sampling each topic category for a representative sample.\nWe apply topic modeling on both the responses and questions to define the typical landscape for dialogues taking place on Reddit. For instance, in the top1000_travel_subreddit dataset, there are already predefined categories for question, itinerary, images, and advice. However, these categories are too general and do not yield enough information for specific subtopics within. With topic modeling applied on the responses field, we can drill down into the parent categories and attempt to identify different segments or sentiments of conversations (see Figure 6)."}, {"title": "B. Data Process", "content": "Posts were collected from the top travel subreddits as well as city and country subreddits of popular tourist destinations. However, the subreddits are not equal in their engagement and activity. As shown in Figure 7 subreddits like 'awardtravel' and 'cruise' have vastly more posts than 'canada'. It is important to know the distribution of content in our Reddit data. Figure 8 shows a closer look at the top 25 subreddits in terms of posts above the upvote ratio threshold that have been collected. Some of the top 25 subreddits include: 'awardtravel', 'shoestrings', 'flight', 'solotravel', 'onebag', 'cruise', 'travel', 'travelhacks', etc. and includes locations: 'germany', 'brazil', 'pattaya', and 'puntacana'.\nSince data is requested from the Reddit API daily, the multiple DataFrames need to be concatenated and deduplicated on 'Post ID' in the case that a single post is trending for multiple days, taking the most recent post to get the most up to date and best comments (see Figure 10). Each PRAW request contained 100 threads, and each thread had a different total number of comments, ranging from about 90 and up to 7400+ comments (see Figure 11). The degree of interaction from users is a variable in the dataset. Given the limited context window of LLMs, the entire corpus of information among the subreddits must be partitioned into smaller, and more manageable chunks. To address this issue and ensure a high quality opinionated-based but credible contexts, there are two cutoff parameters: 1) upvote ratio greater than 0.8, which is the number of upvotes over total votes (see Figure 12), and 2) first or top 20 comments, which Reddit already has sorted by their internal confidence metrics. [33] The comments contained a multitude of special characters, which were removed using NLTK and Regex library packages."}, {"title": "C. Question & Answer (Q&A) Format", "content": "QLORA requires Q&A format, but there exists a many to one relationship between a user's Reddit post and comments from other users in a thread of comments. To reduce the amount of context under each post, there are considerations for slicing and retaining the top N of comments or summarization using transfer learning with existing language models. Using open-source LLMs to summarize concatenated comments, the LLM has the ability to filter out noise including delimiters, emojis, and other irrelevant tokens from raw comments. The summaries were derived using LLaMa 2 provided by Ollama, and by designing a first-person narrative prompt template to craft LLM's response. With summarizing as a dimension reduction step, the most important information from a set of comments is retained, while removing noise and irrelevant punctuation, and curating a generated recommendationoriented response that can serve fine-tuning tasks (see Figure 13). Contexts extracted for knowledge bases must be chunked using optimal chunking strategies such as splitting by delimiters or specific lengths to capture semantic groupings before they are embedded for storage.\nThe dot score was calculated on the Reddit dataset to find the context relevancy based on the question posted and the falcon summarized cohesive comments, the more the dot score is the more relevant question-answer would be, makes the data more cleaner and also one of the assumption that cleaner data is more important than the quantity garbage data to train the model and see the better performance (see Figure 14)."}, {"title": "D. RAFT Data Augmentation", "content": "Retrieval Augment fine-tuning (RAFT) is a novel approach, published in March 2024, to target domain specific RAG solutions, addressing caveats in QLoRA and RAG, such as the naive nature of LLMs that are not able to distinguish between context and noise since its training recipe is curated based on the domain-targeted information provided. [9] Thus, it is highly adaptable and ideal in specific domains, such as ours in travel. Due to the disparity of RAG's dependency on retrieval quality and having a clean, up-to-date knowledge base without noise, RAFT addresses this issue that RAG has in its implementation, where the LLM is fine-tuned to a specific domain. Then, a dataset with questions and answers are made in training. In inference time with zero-shot prompting, there is just the question and generated output. In the RAG phase, there is the question, retrieved documents (to be fed into the LLM), and a generated answer. RAFT will take the entire dataset to train and is instructed to use reasoning via Chain of Thought (CoT) as it is given a question, context, and verified answers. So the model's behavior is trained to memorize knowledge as the removal of oracle documents are done during some instances in training. Each sample in the training data has the question, answer (with context), oracle documents (that are verified and relevant to the question), and distractor documents (to reinforce model behavior to memorize correct answers). The training dataset that is highly customized to one's needs are then used to generate responses. [9]\nThe authors provide the following figure (15) to demonstrate the overall flow of data through the RAFT process. The training dataset is prepared by providing the oracle document (Attention is all you need), which contains the (golden) answer and three sampled negative \"distractor\" documents (Adam, GloVe, Resnet). It is important to note that the ideal combination of golden and distractor documents is dependent on the dataset, but they found 1:4 golden:distractor to be an ideal ratio. The idea is to have the model ignore these three distractor documents since they do not contain the information relevant to the question, so it is training the behavior of the model to learn to memorize domain-specific knowledge rather than having the model attempt to derive from the context given. Then, through a chain of thought with the distractor documents, the model learns to extract the correct information from the entire chunk of context through reasoning, ignoring the distractors. With this CoT reasoning, it shows it greatly improves performance. At the end of training it reaches the final correct answer (Attention is all you need) and contains the golden answer along with the query.\nNote: Figure is from Gorilla Lab in UC Berkeley, led by Tianjun Zhang and Shishir G. Patil, where they explained the RAFT technique that enables the model to learn the structure of the documents and prepare potential candidates before doing retrieval. Each sample contains the query, answer, mix of relevant contextual documents and noisy documents, then uses chain-of-thought to reason and let the model learn the contextual data directly. Thus, it can identify what is noisy and ground truth to generate an accurate finalized answer [9].\nDataset preparation for retrieval fine-tuning task involves"}, {"title": "E. RLHF Data Selection", "content": "RLHF uses a reward model train the LLM on human feedback to classify responses as being good or bad based on a thumbs up or thumbs down icon of binary range of 0 or 1, or smiley faces that range from 0.0, 0.25, 0.50, 0.75, and 1.0. [10] The reward model used is direct preference optimization (DPO) trainer, which expects a very specific format for the dataset, as the model is be trained to directly optimize the preference of which sentence is the most relevant, given two options good or bad, it expects the inputs as triples of prompt, chosen, rejected (human annotated chosen or rejected responses, where the \u201cprompt\u201d contains the context inputs, \"chosen\" contains the corresponding chosen responses and \"rejected\" contains the corresponding negative (rejected) responses. These inputs also need to be already formatted with the template of the model. [29]\n<|im_start|>user\\nINSTRUCTION\\n<|im_end|>\\n <|im_start|>assistant\\n...\nRLHF models require human annotated data with good and bad responses. A reinterpretation of of human annotation was applied by selecting the top 10 and bottom 10 percent data, with highest and the worst comments to differentiate among the good and bad response/ accepted or rejected data, for"}, {"title": "IV. FINE-TUNING METHODS", "content": "When choosing the model, LLaMa 2 is one of the models available in different variants, and is having the accuracy of about 68% while the GPT 3.5 has the accuracy of 70%, which is the slight difference, but considering the fact, the size of the model is also comparatively small, as the models has been trained with the huge difference number of parameters. It has performed fairly well in the multiple benchmarks, reference LLaMa 2-Chat 70B passed the helpfulness evaluation on par with GPT-3.5 precisely, with a 36% win rate and 31.5% tie rate. [36", "37": "Table IV gives a quick look at the comparison of LLM models.\nThe following"}]}