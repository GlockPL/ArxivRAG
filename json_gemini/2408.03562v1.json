{"title": "A Comparison of LLM Fine-tuning Methods and Evaluation Metrics with Travel Chatbot Use Case", "authors": ["Sonia Meyer", "Shreya Singh", "Bertha Tam", "Christopher Ton", "Angel Ren"], "abstract": "This research compares large language model (LLM) fine-tuning methods, including Quantized Low Rank Adapter (QLoRA), Retrieval Augmented fine-tuning (RAFT), and Reinforcement Learning from Human Feedback (RLHF), and additionally compared LLM evaluation methods including End to End (E2E) benchmark method of \"Golden Answers\", traditional natural language processing (NLP) metrics, RAG Assessment (Ragas), OpenAI GPT-4 evaluation metrics, and human evaluation, using the travel chatbot use case. The travel dataset was sourced from the the Reddit API by requesting posts from travel-related subreddits to get travel-related conversation prompts and personalized travel experiences, and augmented for each fine-tuning method. We used two pretrained LLMs utilized for fine-tuning research: LLaMa 2 7B, and Mistral 7B. QLoRA and RAFT are applied to the two pretrained models. The inferences from these models are extensively evaluated against the aforementioned metrics. The best model according to human evaluation and some GPT-4 metrics was Mistral RAFT, so this underwent a Reinforcement Learning from Human Feedback (RLHF) training pipeline, and ultimately was evaluated as the best model. Our main findings are that: 1) quantitative and Ragas metrics do not align with human evaluation, 2) Open AI GPT-4 evaluation most aligns with human evaluation, 3) it is essential to keep humans in the loop for evaluation because, 4) traditional NLP metrics insufficient, 5) Mistral generally outperformed LLaMa, 6) RAFT outperforms QLoRA, but still needs postprocessing, 7) RLHF improves model performance significantly. Next steps include improving data quality, increasing data quantity, exploring RAG methods, and focusing data collection on a specific city, which would improve data quality by narrowing the focus, while creating a useful product.", "sections": [{"title": "I. INTRODUCTION", "content": "The tourism and travel industry took a nosedive during the COVID-19 pandemic, which was declared by the World Health Organization (WHO) on March, 11, 2020, due to the severe lockdown measures restricting travel. [1] Now, post-COVID, travelers are coming back with a vengeance and so was the \"revenge travel\" phenomena named to reflect the negative aspects of tourists exceeding the carrying capacity of destinations and business and governments desperately scrambling to recover income and grow the economy once again at the cost of tourism quality and sustainability. [2] The travel and tourism industry is projected to grow by 5.8% per year for the next decade, which outpaces the overall economy at 2.7%. [3]\nThe industry has faced challenges with widespread labor shortages due in short to poor working conditions, which along with the rise of large language model (LLM) applications presents a unique opportunity to incorporate technology into the travel and tourism industry. [3] Already, travelers overwhelmingly prefer to utilize technology for the full stack of travel from planning to booking to implementation. 74% of travelers use the internet for travel planning, 45% specifically use smartphones, 36% are willing to pay more for an interactive and smooth booking experience, and 80% prefer to use self-service to find information. [4] Technology is institutionalized with the global distribution system (GDS), an international reservation system for travel agents and suppliers, which is essential to online travel booking and customer service applications. [4]\nLarge language models (LLMs) were made possible with the 2017 \"Attention is All You Need\" [5] transformer architecture without recurrent networks or convolutions, and work by predicting masked words or upcoming words. [6] With LLMs, more data results in better predictions, most models have at least one billion parameters. [6] They have been used for many purposes recently including carrying out task-based instructions, multilingual processing, medical and clinical applications, and programming. [6] According to the literature, LLMs are typically trained on a huge corpus of text data with hundreds of billions of hyperparameters. [7] Just in this year, 2023, we have seen several incredible applications utilize powerful LLMs, including in the booming travel industry. With the projected growth of the travel industry post-COVID and recent technological advances, there are potent opportunities for groundbreaking innovation with tangible effects on tourism income. Current artificial intelligence (AI) applications in travel focus on booking, however, there is a gap when it comes to artificial intelligence (AI) assisted travel planning and recommendations (as of May 2024). While not specifically tuned for marketing, ChatGPT remains a competitive chatbot for this application with its high quality conversational ability, however, to get customized travel recommendations, it requires very specific and detailed prompts, and it has a knowledge cut off of September 2021.\nUsing the travel use case, this research compared two fine-tuning methods: 1) Quantized Low Rank Adapters (QLORA) and 2) Retrieval-Augmented fine-tuning (RAFT). Two pretrained 7B models, LLaMa 2 and Mistral, are fine-tuned with these two methods, resulting in four models, then their inferences are evaluated against an extensive set of metrics using GPT as a baseline for comparison. The best model is fine-tuned with the third method, 3) Reinforcement Learning from"}, {"title": "II. RELATED WORK", "content": "Through a comprehensive review of literature and existing research papers, we build an understanding for state-or-the-art techniques and approaches aimed to achieve competitive performances. New innovations are expressed in different variations for enhancing large language models. Some of the feature performance issues garnered from literature review are potential risks of hallucination, underdeveloped retrieval approaches, and inefficiencies in the use of computational resources.\nZhang et. al introduced Retrieval Augmented fine-tuning (RAFT) as a novel training strategy for fine-tuning LLMs to better perform on RAG tasks. The key concept is data augmentation to generate \"question, answer, document\" triplets before fine-tuning. This is done by generating realistic questions paired with elaborate chain of thought answering scheme and purposefully including relevant and irrelevant context documents. Through a chain of thought with the distractor documents, the model learns to extract the correct information from the entire chunk of context through reasoning, ignoring the distractors. RAFT operates by training the model to disregard any retrieved documents that do not contribute to answering a given question, thereby eliminating distractions. The optimal ratio of oracle to distractor documents during training varies across datasets, but including some distractors improves generalization. Finally, during RAG, RAFT retrieves the top-k documents from the database. With RAFT, Zhang et. al addresses the limitations of fine-tuning with RAG, and evaluate their new methods on datasets including: PubMed, HotpotQA, and code documentation. RAFT consistently outperforms standard fine-tuning or RAG alone. [9]\nWith the following literature survey on the existing research, we tried to gain an understanding on the current state of one of the most recent and fastest growing technologies. We attempted to get the basic understanding of the LLMs, different types of LLM chatbots, technologies, approach used, learnt different types of approaches works better in certain types of research works, and the performance of these may vary depending on the tasks. Some of the research studies shows, some chatbots perform well without the fine-tuning as well. Below is the summary of different research papers the team has gone through, some of the papers are summarized in the tabular form as well, showcasing the brief on the goal, approach used and its conclusion. These papers helped the team to have a better understanding on the topic.\nGudibande et al. (2023) warns against the temptation of open source language models trained on imitation data to mimic ChatGPT's proprietary abilities. [15] These models use ChatGPT conversations with real users collected through ShareGPT, Human-ChatGPT Comparison Corpus, r/ChatGPT subreddit, and the Discord ChatGPT bot to train. [15] For"}, {"title": "III. TRAVEL DATASET", "content": "The travel dataset was collected entirely from Reddit, and specifically pulled from travel domain subreddits. There are a plethora of travel subreddits available, with r/travel being the largest and most active with 8.9 million subscribers. It is one of the most popular communities in the top 1% of subreddits as of December 2023. [30] Calls made to the Reddit API to curate a corpus of Question-and-Answer (Q&A) formatted data, collected conversational data from 201 subreddits. [31] This consists of 27 travel-related subreddits, 30 country subreddits, and 144 city subreddits which include r/solotravel, r/travelhacks, r/roadtrip, and so on so the data can capture the context of conversations. The data collected for this project was for the purpose of fine-tuning the chatbot, providing domain specific knowledge, and to provide current updated travel information to address LLM knowledge cutoffs. Due to the Large Language Model (LLM) capabilities of generating recommendations due to its pretraining [32], the Reddit post would serve as the question and the compiled comments as the answer. Tools used for data collection and processing include: Python, PRAW, Pandas, NLTK, Regex, BERTopic, and Ollama.\nChatGPT was also used to generate the top 30 countries for tourism, and tourist cities were collected from Wikipedia. A targeted list of travel subreddits was collected from a blog post, however, subreddits with a heavy focus on sharing images were omitted. [30] In total, there were 201 subreddits: 27 travel related subreddits, 30 country subreddits, and 144 city subreddits. There are a total of 16,300 entries sourced from Reddit, which was further preprocessed and reduced the entries from 16,300 to 10,500 rows.\nReddit data is appropriate for question and answer type data as each post has multiple comments or answers. Reddit is categorized by subreddits, which allow for convenient filtering for travel related questions and answers by targeting travel specific domains. Python Reddit API Wrapper (PRAW) was used to make API requests. [33] Two methods of collecting posts were used: 1) hot, and 2) top. Top means best of all time and takes a time frame as a parameter. Top 1,000 posts of the year in travel related subreddits, top 50 posts of the year in country subreddits, and top 20 posts of the year in city subreddits. Hot is what is currently trending, so these posts are collected daily with smaller requests to reduce requesting the same post multiple times if it is trending for multiple days. Hot 100 posts of travel related subreddits, and hot 20 in country and city subreddits. The top of the year posts were collected at once for the prior year, whereas the hot daily posts were collected daily for 4 days. Despite making so many requests, only 4,000+ posts were collected, perhaps due to not having enough posts within subreddits to meet the request 1,000 or some subreddits did not exist. \nTo estimate an LLM's generalizability against topics of any domain, it is important to evaluate its performance across a set of diverse and representative questions. Given the objective of building a prototype that specializes in providing travel recommendations, the training data should be comprehensive of the real world and have a high cardinality of topics relating to travel. To assess the current landscape of Reddit data under the travel subreddit, a sample of the top 1000 posts can be collected as an experiment. An unsupervised approach, BERTopic modeling, clustered documents of the same topics and identified various categorizations. Given the stochastic nature across different clustering models, an ensemble of these results may produce more consistent outcomes. By grouping like-documents together, it would be a step towards reducing the amount of noise in each comment under the respective threads by summarizing the main points with transfer learning of open source LLMs. Downstream for this workflow are strategic partitioning to target potential model degradation areas. If a model is habitually underperforming when responding to questions of a particular domain, additional data collection and processing for this domain are actionable steps. Eventually, an holistic approach for evaluating the general knowledge capabilities of LLMs can be conducted by sampling each topic category for a representative sample.\nWe apply topic modeling on both the responses and questions to define the typical landscape for dialogues taking place on Reddit. For instance, in the top1000_travel_subreddit dataset, there are already predefined categories for question, itinerary, images, and advice. However, these categories are too general and do not yield enough information for specific subtopics within. With topic modeling applied on the responses field, we can drill down into the parent categories and attempt to identify different segments or sentiments of conversations.\nPosts were collected from the top travel subreddits as well as city and country subreddits of popular tourist destinations. However, the subreddits are not equal in their engagement and activity. As shown in Figure 7 subreddits like 'awardtravel' and 'cruise' have vastly more posts than 'canada'. It is important to know the distribution of content in our Reddit data. Figure 8 shows a closer look at the top 25 subreddits in terms of posts above the upvote ratio threshold that have been collected. Some of the top 25 subreddits include: 'awardtravel', 'shoestrings', 'flight', 'solotravel', 'onebag', 'cruise', 'travel', 'travelhacks', etc. and includes locations: 'germany', 'brazil', 'pattaya', and 'puntacana'.\nSince data is requested from the Reddit API daily, the multiple DataFrames need to be concatenated and deduplicated on 'Post ID' in the case that a single post is trending for multiple days, taking the most recent post to get the most up to date and best comments. Each PRAW request contained 100 threads, and each thread had a different total number of comments, ranging from about 90 and up to 7400+ comments. The degree of interaction from users is a variable in the dataset. Given the limited context window of LLMs, the entire corpus of information among the subreddits must be partitioned into smaller, and more manageable chunks. To address this issue and ensure a high quality opinionated-based but credible contexts, there are two cutoff parameters: 1) upvote ratio greater than 0.8, which is the number of upvotes over total votes , and 2) first or top 20 comments, which Reddit already has sorted by their internal confidence metrics. [33] The comments contained a multitude of special characters, which were removed using NLTK and Regex library packages."}, {"title": "D. RAFT Data Augmentation", "content": "Retrieval Augment fine-tuning (RAFT) is a novel approach, published in March 2024, to target domain specific RAG solutions, addressing caveats in QLoRA and RAG, such as the naive nature of LLMs that are not able to distinguish between context and noise since its training recipe is curated based on the domain-targeted information provided. [9] Thus, it is highly adaptable and ideal in specific domains, such as ours in travel. Due to the disparity of RAG's dependency on retrieval quality and having a clean, up-to-date knowledge base without noise, RAFT addresses this issue that RAG has in its implementation, where the LLM is fine-tuned to a specific domain. Then, a dataset with questions and answers are made in training. In inference time with zero-shot prompting, there is just the question and generated output. In the RAG phase, there is the question, retrieved documents (to be fed into the LLM), and a generated answer. RAFT will take the entire dataset to train and is instructed to use reasoning via Chain of Thought (CoT) as it is given a question, context, and verified answers. So the model's behavior is trained to memorize knowledge as the removal of oracle documents are done during some instances in training. Each sample in the training data has the question, answer (with context), oracle documents (that are verified and relevant to the question), and distractor documents (to reinforce model behavior to memorize correct answers). The training dataset that is highly customized to one's needs are then used to generate responses. [9]\nThe authors provide the following  to demonstrate the overall flow of data through the RAFT process. The training dataset is prepared by providing the oracle document (Attention is all you need), which contains the (golden) answer and three sampled negative \"distractor\" documents (Adam, GloVe, Resnet). It is important to note that the ideal combination of golden and distractor documents is dependent on the dataset, but they found 1:4 golden:distractor to be an ideal ratio. The idea is to have the model ignore these three distractor documents since they do not contain the information relevant to the question, so it is training the behavior of the model to learn to memorize domain-specific knowledge rather than having the model attempt to derive from the context given. Then, through a chain of thought with the distractor documents, the model learns to extract the correct information from the entire chunk of context through reasoning, ignoring the distractors. With this CoT reasoning, it shows it greatly improves performance. At the end of training it reaches the final correct answer (Attention is all you need) and contains the golden answer along with the query.\nNote: Figure is from Gorilla Lab in UC Berkeley, led by Tianjun Zhang and Shishir G. Patil, where they explained the RAFT technique that enables the model to learn the structure of the documents and prepare potential candidates before doing retrieval. Each sample contains the query, answer, mix of relevant contextual documents and noisy documents, then uses chain-of-thought to reason and let the model learn the contextual data directly. Thus, it can identify what is noisy and ground truth to generate an accurate finalized answer [9].\nDataset preparation for retrieval fine-tuning task involves"}, {"title": "C. Question & Answer (Q&A) Format", "content": "QLORA requires Q&A format, but there exists a many to one relationship between a user's Reddit post and comments from other users in a thread of comments. To reduce the amount of context under each post, there are considerations for slicing and retaining the top N of comments or summarization using transfer learning with existing language models. Using open-source LLMs to summarize concatenated comments, the LLM has the ability to filter out noise including delimiters, emojis, and other irrelevant tokens from raw comments. The summaries were derived using LLaMa 2 provided by Ollama, and by designing a first-person narrative prompt template to craft LLM's response. With summarizing as a dimension reduction step, the most important information from a set of comments is retained, while removing noise and irrelevant punctuation, and curating a generated recommendation-oriented response that can serve fine-tuning tasks. Contexts extracted for knowledge bases must be chunked using optimal chunking strategies such as splitting by delimiters or specific lengths to capture semantic groupings before they are embedded for storage.\nThe dot score was calculated on the Reddit dataset to find the context relevancy based on the question posted and the falcon summarized cohesive comments, the more the dot score is the more relevant question-answer would be, makes the data more cleaner and also one of the assumption that cleaner data is more important than the quantity garbage data to train the model and see the better performance."}, {"title": "E. RLHF Data Selection", "content": "RLHF uses a reward model train the LLM on human feedback to classify responses as being good or bad based on a thumbs up or thumbs down icon of binary range of 0 or 1, or smiley faces that range from 0.0, 0.25, 0.50, 0.75, and 1.0. [10] The reward model used is direct preference optimization (DPO) trainer, which expects a very specific format for the dataset, as the model is be trained to directly optimize the preference of which sentence is the most relevant, given two options good or bad, it expects the inputs as triples of prompt, chosen, rejected (human annotated chosen or rejected responses, where the \u201cprompt\u201d contains the context inputs, \"chosen\" contains the corresponding chosen responses and \"rejected\" contains the corresponding negative (rejected) responses. These inputs also need to be already formatted with the template of the model. [29]\nRLHF models require human annotated data with good and bad responses. A reinterpretation of of human annotation was applied by selecting the top 10 and bottom 10 percent data, with highest and the worst comments to differentiate among the good and bad response/ accepted or rejected data, for"}, {"title": "IV. FINE-TUNING METHODS", "content": "When choosing the model, LLaMa 2 is one of the models available in different variants, and is having the accuracy of about 68% while the GPT 3.5 has the accuracy of 70%, which is the slight difference, but considering the fact, the size of the model is also comparatively small, as the models has been trained with the huge difference number of parameters. It has performed fairly well in the multiple benchmarks, reference LLaMa 2-Chat 70B passed the helpfulness evaluation on par with GPT-3.5 precisely, with a 36% win rate and 31.5% tie rate. [36] Hence, given its open source availability, size, less complex, we chose LLaMa 2 7B. Mistral 7B was selected for the same factors, and additionally due to the novelty of being a newly launched model. It had been trained on fewer parameters yet it has performed at par compared to LLaMal and LLaMa 2 13 B in most of the benchmarks like Multi-task Language Understanding (MMLU), TrivialQA, etc. [37]\nThe following techniques, Quantized Low Rank Adapter (QLORA), Retrieval-Augmented Fine-tuning (RAFT) were passed through LLaMa 2 and Mistral pretrained language models to assess their performance under each proposed approach, resulting to 4 candidate models.\nQuantized Low Rank Adapter (QLoRA) Comprehen-sively tuning a LLM for a particular domain is impractical for individuals due to financial and resource constraints. Therefore, the alternative fine-tuning methodology, QLoRA is employed. Instead of training every single parameter from scratch, LoRA (Low Rank Adaptation), strategically updates a smaller subset of the model's parameters. And Q focuses on the quantized LLMs that are most important for the model. So together, QLoRA, is an efficient fine-tuning method tailored for quantized LLMs that is more resource-efficient to maintain high accuracy and response quality while reducing the computational and financial cost associated with traditional LLM training. The final input from the training dataset is formatted as a question-and-answer (Q&A) pair structure.\nThe Figure 18 shows the architecture and data flow for the fine-tuning method, QLoRA. [38] Singh (2023) found that if there is a huge corpus of task-specified datasets that have been labeled, then fine-tuning is preferable over the retrieval method, RAG, especially for domain-specific tasks. For example, specialized topics in the travel domain are"}, {"title": "V. MODEL TRAINING & PERFORMANCE", "content": "Model validation is used to evaluate the model's performance on data it has not been trained on. Specifically, the validation set that was set apart from the training and testing set is used to assess that performance during the training process and tune the hyperparameters of the models. The validation process helps avoid overfitting by ensuring that the model is generalizing well when exposed to new data. The dataset initially had 16,000 rows, but Figure 22 shows this raw dataset was further filtered using the dot-score on question and summaries to get the more relevant information, resulting in about 10,000 rows. Figure 21 shows the dataset split used in Mistral QLoRA with data filtered on greater than 0.65 dot-score. The dataset was split into a 80:20 ratio, where 20% was further split into 50% to produce the validation and testing split.\nLLaMa 2 QLORA: For this model, various experiments were carried out with the dot score threshold, which affects the quality alignment with Question and the cohesive answers"}, {"title": "VI. LLM EVALUATION METHODS", "content": "LLMs have given a new dimension to artificial intelligence and evaluating LLMs. Since we have so many developments in this field every single day, it is important to have good metrics with which to evaluate these models. It is critical to evaluate the LLMs, as these are important to uncover its performance, its challenges, strengths and limitations, and help the developer to work in the certain areas to get better results and performance as per the requirement of the client. Model variants are evaluated with a standardized evaluation process that is defined by consistent prompt templates. A set of specific templates are designed to help capture any risk of hallucinations, noise or inaccurate responses from the two open-source LLM options and processed implementations. In the event that retrieved context is limited and parameters have not been updated to address unknown and niche domain topics, the prompt templates can help ensure that the model is able to admit naivety where appropriate. Evaluation of an LLM is different from machine learning (ML) model evaluation due to the complexity of the task involved, unlike ML models, which have more structured prediction tasks. The evaluation of LLMs is still an ongoing research topic. For the project, as mentioned earlier in the chapter, implemented the following different approaches: QLORA and RAFT on LLaMa 2 and Mistral, and RLHF on the best model. In addition to evaluating our models, we will also evaluate against pretrained LLaMa 2, Mistral, and ChatGPT, considered to be a state-of-the-art (SOTA), to establish a baseline.\nA. Quantitative Evaluation Metrics\nComputational Evaluation: Recall-Oriented Understudy for Gisting Evaluation (ROUGE) is inexpensive, but compares well with the human generated text. An additional setback of this metric is it matches the semantics, but does not handle syntactic meaning. This metric would help to evaluate the model by checking how well it is. It has a set of metrics to measure the overlap and similarity between the generated text and a reference text, the included metrics are:\nROUGE-1: Measures the overlap of unigrams, or single words.\nROUGE-2: Measures the overlap of bigrams, or pairs of words.\nROUGE-L: Measures the longest common subsequence (LCS), rewarding longer shared sequences between the generated and reference texts. [43]\nROUGE-S: skip-gram concurrence metric, allows to search for consecutive words from the reference text that appear in the model output but are separated by one-or-more other words. [44]\nBertScore is another evaluation metric which comprises of F1, Precision, and Recall. It first generates the bert embeddings of the generated and referenced text and compare these embeddings to assess the semantic similarity. [45]\nWhile ROGUE and cosine similarity are not able to adequately capture the complexity of conversational recommendation systems [18] [17] and thus recommender chatbots, however, we will include them because they are very common standard metrics. Additional metrics will include response relevance, and we will compare the summary of user input to final output using the BERT embedding model to compare semantic similarity.\nBLEU (Bilingual Evaluation Understudy) Score: It measures the similarity between the machine response text and the reference response using n-grams, which are contiguous sequences of n words. It calculates the precision of n-grams in the machine-generated responses by comparing them to the reference responses. The precision is then modified by a brevity penalty to account for translations that are shorter than the reference translations.\nDot Score, Cosine Similarity, and Embedding DistanceDot product calculates the vector similarity measure that accounts for both magnitude and direction. Generated from LangChain Evaluators,embedding distance calculates how dissimilar the vectors are; it is the angle of the vector, so the lower the number, the better, and accounts for direction only. For embedding distance, LangChain's embedding distance evaluator or cosine distance was as the metric. The formulas for the aforementioned metrics are shown below:\nDot Product = $A \u00b7 B = \\sum_{i=1}^{n} A_i B_i$\nCosine Similarity = $\\frac{A \u00b7 B}{||A|| ||B||}$\nEmbedding Distance = 1 \u2013 Cosine Similarity\nBeyond the content generated, we also evaluated the computation speed and response generation time, which will greatly affect the user experience.\nThe models have been evaluated using quantitative metrics: ROUGE variants (ROUGE-1, ROUGE-2, ROUGE), BertScore, dot score, cosine similarity, and embedding distance. All these metrics give the score between 0 to 1, 0 being the worst to 1 being the best response, except for embedding distance, which is the opposite. These metrics compare the ground truth with the model's responses and calculate the respective scores ranging between 0 to 1. ROUGE captures the overlaps n-grams between the ground truth and the responses captured, this metric was used keeping the standard and as the starting point of evaluating the model's responses, it does not prove to be the good enough metric to evaluate the performance though as it just checks the overlaps. Another metric of cosine similarity has been used to evaluate the lexical, frequency based similarity between the ground truth and the model's responses, but still does not capture the semantics. Finally, BertScore has been used and is the most reliable metric amongst the quantitative metrics for text comparison. [46] BertScore addresses two common issues that n-gram-based metrics often encounter. First, n-gram models tend to incorrectly match paraphrases because semantically accurate expressions may differ from the surface form of the reference text, which can lead to incorrect performance estimation. BertScore, on the other hand, performs similarity calculations using contextualized token embeddings shown to be effective for entailment detection. Second, n-gram models are not able to capture long-range dependencies and penalize semantically significant reordering. [46]\nB. Qualitative Evaluation Metrics\nHuman Evaluation: Human evaluation is a critical and important subjective method of evaluation. There is a lot of flexibility in the study design including survey collection to verify the performance of the chatbot, which will help in determining how structured, relevant, coherent, and continuous the response of the chatbot is. This is possibly one of the most reliable but also time-consuming and expensive methods to evaluate the performance of the model. Commonly, the survey is a simple thumbs up or down where the user can rate the chatbot's response as positive or negative. Every reaction/ thumbs up/ thumbs down has the score ranges from 0 to 1, 0 = worst, 1 = best. We additionally offered users a second option, enabling them to select from five smiling faces with varying scores: 0, 0.25, 0.5, 0.75, and 1. This provides users with a"}, {"title": "VII. RESULTS", "content": "We have the following 7 candidate models on which we have generated inferences for evaluation:\nMistral: pretrained Mistral, Mistral QLoRA, Mistral RAFT\nLLaMa 2: pretrained LLaMa 2, LLaMa 2 QLoRA, LLaMa 2 RAFT\nBaseline: GPT-4\nRLHF on best model: Mistral RAFT RLHF\nWe have 4 processed models, and a final 5th model, where RLHF was applied to the best model picked from the 4 processed models.\nAppendix shows the overall results of metric evaluations for all models. There are 27 total metrics: 17 quantitative and 15 qualitative. Quantitative metrics are around golden question and answer dot score, cosine similarity, and embedding distances, as well as traditional natural language processing metrics like ROUGE and BLEU, and finally inference generation times. Qualitative metrics include Ragas, and a select subset of OpenAI's GPT-4 evaluations: coherence, conciseness, helpfulness, and relevance. Each metric has the variance, and was zero for nearly all quantitative metrics. This validates our hypothesis that traditional NLP metrics are insufficient for the complexity of LLMs. For example, although this was a qualitative metric, context recall's best was 0.219 and worst was 0.113 for a difference of 0.105, out of a 0 to 1 scale. Comparing models on metrics with no variance is not useful. Due to the number of evaluation metrics and the lack of variance in so many of them, this chapter will focus on analyzing the nonzero variance metrics.\nindicates the best model, and next best model if the best is a baseline model) and worst performing models highlighted in green and red respectively. Mistral RAFT was selected as the best model, then further fine-tuned with RLHF, so then Mistral RAFT RLHF is the final best model. Mistral RAFT was the best model by human evaluation of the fine-tuned models, but did not outperform GPT-4 as a baseline. However, Mistral RAFT RLHF did then outperform GPT-4. RLHF results will be discussed in more detail later on. Mistral RAFT was often next best of the fine-tuned models, including for the following metrics: human evaluation, conciseness, helpfulness, and golden answer dot score, and was selected as the worst for faithfulness and context precision. And yet, human evaluation still determined this model to be the best fine-tuned model, which calls into question the validity of the Ragas and quantitative metrics and highlights the importance of keeping humans in the loop when it comes to evaluation.\nHuman evaluation was carried out by the researchers, ranking inference on a scale of 1 (worst) to 5 (best), normalized to 0 to 1. Figure 31 shows Mistral RAFT RLHF was the best, Mistral RAFT next best, and Mistral QLoRA the worst. QLoRA models were very repetitive and required post processing, RAFT models produced the best answers, but also produced multiple answer options and sometimes instructions to a travel agent, which also needed to be removed with post processing.\nshows the Ragas metric against 3 baseline models on the bottom and 5 processed models on the top. For faithfulness, pretrained LLaMa was the best, GPT-4 next best, and Mistral RAFT the worst. It is not surprising that baseline models are the best, but Mistral RAFT was the highest rating by human evaluation. Outside baseline models, Mistral RAFT RLHF was the best. Answer relevancy had pretrained Mistral as the best, LLaMa QLoRA and Mistral QLoRA tied for second, and GPT-4 as worst. Context precision has pretrained LLaMa as best with pretrained Mistral and LLaMa QLoRA as second, and Mistral RAFT as worst, again at odds with human evaluation. Answer correctness has LLaMa QLoRA as best, Mistral RAFT RLHF as next best, and LLaMa RAFT as worst. The final Ragas metric, coherence, has pretrained LLaMa and Mistral RAFT RLHF tied for best and Mistral QLoRA for worst. In general, context recall is low across models, answer relevancy very high across models, correctness and precision are of similar values for each respective model, and visually see the largest variance with faithfulness. Across the Ragas metrics, Mistral RAFT performed the worst, but is the best according to human evaluation, indicating that Ragas metrics"}, {"title": "VIII. CONCLUSION", "content": "The travel industry currently lacks chatbots that can deliver real-time insights for all questions and concerns related to itinerary planning", "models": 1}]}