{"title": "NSA: Neuro-symbolic ARC Challenge", "authors": ["Pawe\u0142 Batorski", "Jannik Brinkmann", "Paul Swoboda"], "abstract": "The Abstraction and Reasoning Corpus (ARC) evaluates general reasoning capabilities that are difficult for both machine learning models and combinatorial search methods. We propose a neuro-symbolic approach that combines a transformer for proposal generation with combinatorial search using a domain-specific language. The transformer narrows the search space by proposing promising search directions, which allows the combinatorial search to find the actual solution in short time. We pre-train the trainsformer with synthetically generated data. During test-time we generate additional task-specific training tasks and fine-tune our model. Our results surpass comparable state of the art on the ARC evaluation set by 27% and compare favourably on the ARC train set. We make our code and dataset publicly available at https://github.com/Batorskq/NSA.", "sections": [{"title": "1. Introduction", "content": "The Abstraction and Reasoning Corpus (ARC) challenge [7] is a difficult few-shot benchmark for testing visual reasoning capabilities of machine learning models. The capabilities of recent general-purpose LLM systems are, as of now, not good enough to solve ARC at human performance in a reasonably limited amount of time [19, 20, 28]. Arguably their pre-training seems to have not imbued them with enough of the necessary concepts required to solve ARC tasks reliably and without an excessive number of tries. It is unclear whether LLMs lack the correct level of abstraction and the specific type of high-level visual reasoning used for solving visual riddles. On the other hand, specialized combinatorial search methods using domain specific languages (DSL) [2, 11, 17, 27] struggle with the great variety and generality of the ARC tasks, with no compact enough description of the search space being available to effectively solve the problem via traditional methods. At the same time, tasks in the ARC benchmark are not hard for humans, with most tasks seemingly requiring only some visual pattern matching and a limited amount of reasoning. In user studies, human participants solved around 80% of the tasks on average [15].\nThis indicates that (i) currently pure pre-trained ML systems are not yet capable of solving ARC with reasonably limited compute and may lack some fundamental capabilities to do so and (ii) specialized combinatorial approaches relying on DSLs cannot cope with the search space complexity. However, both approaches can be used to capture some aspects of humans solving ARC tasks well: DSLs can be used for reasoning on the right abstraction level in terms of composing elementary transformations. ML can be used to pick promising transformation candidates, mimicking the intuitive way a human will restrict his reasoning instead of enumerating all possible transformations. We propose a neuro-symbolic approach to solving ARC that combines these strengths of ML and DSL-based combinatorial methods. In our approach the DSLs give us valuable inductive bias that allows us to reason on the right abstraction level, while the ML part guides the search process by proposal generation, allowing the combinatorial search to actually find solutions in a limited amount of time."}, {"title": "2. Related Work", "content": "The ARC challenge [7] was proposed to benchmark general reasoning capabilities that seem easy for humans but hard for current ML methods and hand-crafted search algorithms. It has lead to a large number of attempts using various paradigms for its solution. To spur progress, leaderboards have been made available that measure performance of submitted systems. For inclusion in the primary leaderboard, the model may not call any outside or proprietary LLM and a 30-minute computation time limit for each task must be obeyed. For models that do not adhere to these requirements, a secondary leaderboard can be used.\nWe categorize related work on solving the ARC challenge into learning based and DSL-based approaches and also discuss simpler alternatives to full ARC."}, {"title": "2.1. ML", "content": "The work [28] analyzes one-shot prompting of large language models (LLMs) for ARC Challenge problems. The authors examine the performance of LLaMA 2 [24] and GPT-4 [1] with various grid representations to address ARC tasks. By applying few-shot and in-context few-shot prompting, they solve only 13 tasks from a chosen subset of ARC tasks that can be easily addressed by the DSL approach in [27]. However, incorporating the abstract graph reasoning proposed in [27] improves their results, enabling them to solve 23 out of 50 tasks. While this graph abstraction significantly enhances the LLMs' reasoning capabilities on ARC tasks, the overall performance remains far below the results achieved with simple abstraction-based reasoning alone. The work [20] explores various methods of representing ARC images in LLMs.\nThe work [16] uses a differential neural computer to solve ARC problems with a grid size of up to 10 \u00d7 10. The approach [4] uses DreamCoder [9] for solving a small subset of ARC tasks. An imitation learning algorithm based on the decision transformer [6] and a custom clustering method is used in [23] for solving small ARC tasks. Due to scaling or representational capacity issues the above works are all applied to small subsets of ARC and do not aim to solve the full ARC dataset.\nThe works [19] and [26] use pre-trained LLMs for directly outputting the transformation of the test input image when conditioned on each task's input output pairs. Greenblatt's pure LLM method [12] used GPT-4o [22] and few-shot prompting to sample python programs for describing ARC transformations. For each task up to 8000 programs are sampled. While achieving impressive results, this puts the approach outside the evaluation protocol of using 30 minutes per task and not using a closed source LLM. The vision transformer [8]-based work [18] uses the spatial image structure and trains on procedurraly generated tasks [14]. However, an evaluation on the ARC train or eval set is missing.\nThe closest work to ours is [5], which uses a large language model fine-tuned on code generation to write programs using the DSL [13]. Similar to us, the transformer is fine-tuned using synthetically generated training data using hindsight relabeling. In contrast to us, the DSL [13] does not come with a combinatorial search, hence relying for the full search process on the ML model. Also, since the used transformer is several orders of magnitude larger than ours, test time adaptation is not possible to do, thus relying more on the capabilities of their pre-trained model.\nThe concurrent work [3] proposes, similarly to us, to use test-time training for ARC. They use an 8B-parameter LLM, but due to the great computational demands this work does not allow for 30-minute per task inference."}, {"title": "2.2. DSL & Combinatorial", "content": "The ARC challenge has inspired several dedicated DSLs for desribing its transformations, sitting at different levels of abstraction and representational power.\nThe graph based approach [27] decomposes a transformation into a series of filters and transformation primitives, that together work on a more abstract graph representation. It comes together with a combinatorial search algorithm that finds the right overall transformation. In its basic form it is somewhat limited in its representational power by a small set of elementary transformation primitives, since otherwise the combinatorial search would have difficulties traversing a larger search space.\nAinosoon et al [2] propose an imperative style DSL for ARC and tasks are solved through bread-first depth search on a search tree.\nThe work [17] introduces a novel DSL inspired by [27] and frames the ARC Challenge as a generalized planning problem. By incorporating graph abstraction [27], their solution prunes actions extensively to streamline the search process. Their approach closely resembles ours. However, while we expand the DSL to support a broader search space, their extension remains minimal, likely due to challenges in finding correct solutions as the search space grows. Additionally, they evaluate their solution on only a small subset of ARC tasks.\nThe work [11] provides a DSL written as functional programs in OCaml and searches heuristically for solutions using minimum description length as quality criterion. It reaches state of the art results for pure DSL-based methods.\nMichael Hodel's DSL [13] decomposes a transformation into a series of 165 elementary operations. Hand-written programs are given that can solve all of the 400 ARC train tasks. However, to describe the most complex transformations found in the ARC train set, sequences of up to 60 elementary transformations are necessary. This can limit the practical effectivity of the DSL since such long programs might be currently too complex for ML models to generate."}, {"title": "2.3. Simplified ARC Benchmarks", "content": "Due to the complexity of the original ARC benchmark proposed by [7], several authors recommend first focusing on simpler tasks, as some of the original ARC tasks are challenging even for humans. The work in [28] introduces the 1D-ARC benchmark, which simplifies ARC tasks by reducing them to one dimension, making object association easier to analyze by limiting relationships to right-left interactions. Meanwhile, [21] presents ConceptARC, designed to simplify ARC tasks so that nearly all are easily solvable by humans, maintaining two-dimensional images but with an average human success rate of 95%. Although our paper does not include evaluations on these simplified ARC benchmarks, it underscores the difficulty of the original ARC tasks."}, {"title": "3. Method", "content": "Our method consists of three parts.\nDSL: The domain-specific language (DSL) for encoding image transformations.\nProposal Generation: We train a transformer model to predict which specific transformations in the DSL need to be used to get the input/output relationships on the few-shot examples of the current task,\nCombinatorial Search: for finding the exact subset and order of transformations and their parameters.\nOur methodological contribution lies in (i) extending ARGA's DSL by additional transformation primitives, enlarging its representational capacity. (ii) Introducing a transformer-guided search via proposing transformation primitives. (iii) Generating synthetic training examples by hindsight relabeling for pre-training and test-time fine-tuning the transformer."}, {"title": "3.1. DSL & Symbolic Engine", "content": "We use the \"Abstract Reasoning with Graph Abstractions\u201d (ARGA) DSL from [27]. The ARGA DSL consists of three main objects: (i) abstract representations, (ii) filters and (iii) transformation primitives.\nAbstract representations describe the input images in terms of a more abstract graphical structure. ARGA associates connected components of the input pixel grids w.r.t. different connectivity criteria as graph nodes. For example, a node might be a same-color 4-connected subset of the pixel grid. Edges are added according to various spatial proximity criteria. Note that there can be multiple abstract representations of the same image. In Figure 3 we show two different graph abstractions of the same image.\nFilters select certain nodes in the abstraction graph. Possible filter operations are to select by size, color, neighborhood relations etc.\nTransformation primitives modify input nodes selected by filters. Exemplary transformation primitives are update color of a node, rotate, extend by appending additional pixels in some directions etc.\nThus, a full transformation consists of choosing the used abstraction, a sequence of filters and filter parameters and a sequence of transformation primitives and their parameters. An example of a full transformation is included in Figure 3 ARGA provides 4 base filters out of the box. More complex filters can be constructed by composing basic ones. Additionally, 12 transformation primitives are given.\nWe add 15 more generic and widely usable transformation primitives and call the resulting DSL extended ARGA, or abbreviate as ARGAe. The list of additional transformation primitives is given in Table 1.\nRemark. Importantly, increasing ARGA's representational capacity by adding more transformation primitives might make it worse in practice. Since more transformation primitives mean a larger search space to traverse, some solutions found previously by ARGA's combinatorial search on the smaller base set of transformation primitives might not be found on the enlarged set. This arguably is one of the reasons why [27] did not include a larger set of transformation primitives.\nCombinatorial Search ARGA implements a greedy best-first search for traversing a search tree. Each search tree node corresponds to a set of graphs that were obtained by a partial transformation of the input images. Search tree nodes are expanded in the order of ascending distance to the output images. Transformations are pruned by choosing constraints which they are not allowed to violate. Hashing is used to detect different transformations that result in the same update on the considered images. A taboo list is used to suppress exploring transformations that have not led to better results."}, {"title": "3.2. Transformer-Guided Search", "content": "To guide the search process, we train a transformer to generate transformation primitive proposals. Given several input/output images of a single ARC task, it predicts (i) how many transformation primitives are used and (ii) which transformation primitives are used in each step."}, {"title": "4. Experiments", "content": "In this section we evaluate the effectiveness of our neuro-symbolic approach on both the train and eval dataset of ARC [7]. We find that our approach can significantly improve upon comparable sota methods on the ARC evaluation set and that both the neural and symbolic parts of our approach are necessary for good performance."}, {"title": "Data Generation", "content": "We pre-train our model on 31125 artificially generated training tasks. Input images were taken from both the train and eval set, similar to the data generation process from CodeIt [5]. We stress that no ground truth transformations were seen during training. Roughly 40% of the generated tasks have one transformation primitive, another 40% have two and 20% need three. For tasks using one transformation primitive we balanced the training set so that each transformation primitive was generated similarly often."}, {"title": "Architecture & Training Details", "content": "Our transformer encoder for the proposal generation has 8 layers, 512 feature dimensions and 2048 feedforward dimensions and 8 attention heads per layer. This gives 25.3 million parameters overall. We pre-train for 27 epochs with a learning rate of 5e-5 and batch size 32 using the ADAM optimizer. For TTA we finetune for 15 epochs with the same parameters. Our custom tokenizer has a vocabulary size of 45. During our training we also perform learning rate decay. After every 10 epochs we multiply the learning rate by 0.1."}, {"title": "Our Methods", "content": "We provide three methods corresponding to our individual contributions.\nARGAE: The pure ARGA [27] DSL approach with extended transformation set. We use the provided combinatorial search algorithm on all possible filters and transformations.\nNSA w/o TTA: Using transformer-guided search with the pre-trained transformer, we predict up to three consecutive transformation primitives from ARGAE. We have three classification tokens for transformation primitive proposal. If the second one predicts no_trans, meaning only one transformation primitive will be needed, we take the top-5 transformation primitives proposed from the first classification token and feed it to the search algorithm. If the third classification token predicts no_trans, meaning two transformation primitives are needed to describe the overall transformation, we feed the top-4 transformation primitives of the first two classification tokens to the combinatorial search. In the remaining case we feed the top-3 transformation primitive predictions of each classification token to the combinatorial search.\nNSA: Using transformer-guided grid search with test-time adaptation, for each task, we fine-tune on 2500 training samples generated on the given input for 15 epochs. Other parameters are equal to NSA w/o TTA."}, {"title": "Literature Baselines", "content": "We compare against recent SOTA methods that follow a comparable evaluation protocol, that is compute is limited to 30 minutes and evaluation is done on at least some subset of either the ARC train or eval set. This gives us the original ARGA method [27] and the other DSL methods [2, 2, 10]. Among the pure ML-based methods we compare to [19]. Additionally, we compare against the DSL/LLM method CodeIt [5]. Hence, we do not compare against methods [3, 12, 18] that do not follow this evaluation protocol or report results on a different task set.\nSome comparable methods from the literature have evaluated on subsets of both train and test sets instead of the full ones for various reasons. For example, CodeIt [5] uses the train set to choose hyperparameters. ARGA [27] only evaluates on promising tasks for which the authors think it has the representational capacity etc. We also believe that some approaches forego testing on the full train/evaluation set due to the large time investment this would incur."}, {"title": "Evaluation Protocol", "content": "For each task, we allow 30 minutes of computation time. We report results on each set for comparison. For TTA, this includes the time to generate new data and finetune the model. Most other approaches allow for trying three transformations and counting only the potentially correct one. In contrast to this, but similar to [19], we only make one attempt for each task."}, {"title": "Results & Discussion", "content": "In Table 2 we compare our results with existing baselines from the literature. We outperform baselines that follow the ARC evaluation protocol by 27% on the evaluation set and land on the third place on the ARC train set. Our best-performing method NSA solves 78 tasks on the train set, with 104 being solved by [2]. We solve 75 tasks on the eval set, with 59 being solved by the next-best approach CodeIt [5]. Interestingly, we see that also our method NSA w/o TTA outperforms comparable methods on the ARC eval set with 63 tasks solved. When using TTA in NSA performance is pushed even further.\nIt is noteworthy, that our approach has similar performance on both train and test set, while most other methods do significantly better on the training set. Possibly this is due to the nature of the ARGAE DSL, which seems to include transformations that help solve tasks in both evaluate and train.\nWe can also see the trade-off between more expressive DSL vs. more required compute time for the combinatorial search in ARGAE play out differently for train and eval. ARGAE performance is drastically reduced on the train set as compared to plain ARGA. It seems that the increased representational capacity has not helped, but rather made the combinatorial search more difficult, thus leading to a lower number of found transformations. On the eval set, the situation is reversed, with increased representational capacity not hindering the search process as much. NSA overcomes this trade-off by limiting the search space while retaining ARGAe's representational capacity.\nWhile TTA gives a significant boost, there is a trade-off between computation spent on the combinatorial search vs. more fine-tuning. In our experince it pays most to invest compute into fine-tuning. We have observed that performance gains do not plateau after one or two fine-tuning iterations but can continue up to 15 epochs. Investing more time in data generation and fine-tuning enhances prediction accuracy and increases the likelihood that the transformer identifies the correct transformation primitive. This approach allows our search engine to prioritize finding the correct transformation among the most probable predictions. We find it especially beneficial to prioritize fine-tuning over allocating more time to task-solving.\nCorrespondingly, the data-generation and fine-tuning time for each task amounts to around 22 minutes. The average time spent on data generation, fine tuning and task solving for NSA is included in Figure 5."}, {"title": "NSA Components Ablations", "content": "In Table 2 we further examine the necessity of our two key components:\n\u2022 Narrowing the search space by proposal generation through transformer guided search and\n\u2022 Test-time adaptation (TTA) during inference.\nWe see that transformer guided search without TTA, i.e. NSA w/o TTA improves upon ARGAE 8-fold on the train set and almost 3-fold on the eval set. Additionally using TTA in our method NSA further improves by 62% on the train set and by 20% on the exal set as compared to no TTA with our method NSA w/o TTA."}, {"title": "TTA Ablations", "content": "We analyze the importance of how long to train and how many data to train on during TTA fine-tuning. For both ablations we take 50 random tasks overall, such that for 25 of those we can predict the necessary transformation primitives without TTA, while for the rest we cannot do so.\nFirst, we analyze the influence of the number of TTA epochs on the overall results by evaluating models with 0, 1, 5, 10, and 15 TTA epochs in Figure 6. For this experiment, we provide two metrics:\n\u2022 Prediction Inclusion - This metric indicates whether the true transformation primitives lie within the set of proposed ones. It is essential to assess whether the transformer, after TTA, can correctly predict the transformation primitives at all.\n\u2022 Prediction Rank - This metric calculates whether the true transformation primitive has been predicted as most probable (rank 1), second most probable (rank 2) and so on. Prediction rank is crucial because the search engine prioritizes the most probable transformation primitive first; if the true transformation ranks low, the search may become harder.\nAs shown in Figure 6, the average prediction rank does not change much across all TTA epochs. However, the number of tasks solved fluctuates with different counts of TTA epochs. Notably, some more difficult tasks yield correct predictions only when using 15 TTA epochs, while easier ones often require just 1 TTA epoch for a correct prediction. Prioritizing the total number of solved tasks, we choose 15 fine-tuning epochs for our experiments. This figure also highlights the importance of TTA, as even a single epoch significantly boosts the number of tasks with correctly predicted transformation primitives from 50% to 88% compared to using no TTA on the ablation tasks.\nSecond, in Figure 7 we examine the effect of varying the number of TTA training tasks on Prediction Rank and Predicted Inclusion. We tested sample sizes of 100, 500, 1500, and 2500, along with a baseline comparison of no TTA (denoted as 0 samples in the figure). The results show that after 500 epochs Predicted Inclusion stabilizes at 88%. Using only 100 already significantly outperforms no TTA, but does not saturate yet. As the number of TTA samples increases, Prediction Rank improves steadily, suggesting that a larger sample count enables the transformer to make more accurate predictions."}, {"title": "5. Conclusion", "content": "We have presented a neuro-symbolic approach for ARC challenge. We show that combining abstraction capabilities of hand-designed DSLs with a learned proposal generation outperforms comparable purely hand-designed search as well as pure ML methods. One immediate approach is to scale up both the DSL by increasing its representational capacity even more and equally increase the transformer network for the proposal generation, e.g. by using pre-trained LLMs and generating larger amounts of training data. However, this approach would soon require more compute than can be provided in 30 minutes. Another idea is to use several iterations for guessing the correct transformation, iteratively improving the current transformation candidate by inspecting where errors occurred, analoguously to LLM approaches [29]."}]}