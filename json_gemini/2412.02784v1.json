{"title": "FathomGPT: A Natural Language Interface for Interactively Exploring Ocean Science Data", "authors": ["Nabin Khanal", "Chun Meng Yu", "Jui-Cheng Chiu", "Anav Chaudhary", "Ziyue Zhang", "Kakani Katija", "Angus G. Forbes"], "abstract": "We introduce FathomGPT, an open source system for the interactive investigation of ocean science data via a natural language interface. FathomGPT was developed in close collaboration with marine scientists to enable researchers to explore and analyze the FathomNet image database. FathomGPT provides a custom information retrieval pipeline that leverages OpenAI's large language models to enable: the creation of complex queries to retrieve images, taxonomic information, and scientific measurements; mapping common names and morphological features to scientific names; generating interactive charts on demand; and searching by image or specified patterns within an image. In designing FathomGPT, particular emphasis was placed on enhancing the user's experience by facilitating free-form exploration and optimizing response times. We present an architectural overview and implementation details of FathomGPT, along with a series of ablation studies that demonstrate the effectiveness of our approach to name resolution, fine tuning, and prompt modification. We also present usage scenarios of interactive data exploration sessions and document feedback from ocean scientists and machine learning experts.", "sections": [{"title": "1 INTRODUCTION", "content": "The ocean is experiencing unprecedented rapid change, and visually monitoring marine biota at the spatiotemporal scales needed for responsible stewardship is a formidable task. The volume and rate of this required data collection rapidly outpaces our abilities to process and analyze them. However, recent advances in machine learning have enabled fast, sophisticated analysis of visual data. FathomNet [24] is an open-source image database that standardizes and aggregates expertly curated labeled data, including imagery of marine animals, underwater equipment, debris, among other concepts, and encourages contributions from distributed data sources. Additionally, FathomNet data can be used to train and deploy models on other institutional videos to reduce annotation effort and enable automated tracking of underwater concepts when integrated with robotic vehicles, accelerating the processing of visual data to achieve a healthy and sustainable global ocean.\nFathomNet currently hosts 109,871 images containing 296,795 individual species, and each month over 2,000 additional images are ingested. FathomNet is publicly available via both an interactive website and a REST API, and more advanced users can interact with the FathomNet database directly using SQL queries. However, despite the increasing richness of this dataset and the increasing need for researchers to make use of machine learning tools to annotate and analyze images, it remains difficult to support a range of tasks useful for the intended audience, which includes ocean scientists, policy makers, and ocean enthusiasts. In a 2023 survey, Crosby et al. [11] delineate core issues that prevent researchers and other users from incorporating scientific databases into their scientific workflows, which include: challenges with data sharing, a lack of expertise in machine learning technologies, and a lack of accessible tools and interfaces.\nWe introduce FathomGPT to address these issues, to promote database accessibility, and to support a wide range of data explorations involving retrieving, visualizing, and analyzing ocean images, species annotations, taxonomic information, and scientific measurements. FathomGPT introduces a natural language interface that leverages large language models (LLMs) to enable different exploration pathways and to facilitate a range of scientific analysis tasks. We worked closely with our marine science collaborators to identify the core tasks that FathomNet is intended to support, and we participated in a series of workshops to elicit additional usage scenarios for interacting with the FathomNet database (Section 4). Based on this feedback, we designed FathomGPT to enable users to freely write natural language prompts in order to make it easy to: ask general knowledge questions about marine life and ocean ecosystems; look up the scientific name of species via their colloquial name or by providing descriptions of their morphology, their color, their predator/prey relationships, or their habitat; craft complex database queries that enable users to make comparisons between species, to understand changes over time, and to reason about relationships between scientific measurements and species observations; review \"data cards\" consisting of annotated images, taxonomic information, and scientific measurements; search for species observations by uploading an image or by interactively selecting/segmenting a specific pattern within the uploaded image; and generate custom interactive charts.\nDuring each FathomGPT session, our system retains a memory of previous user prompts, so that a user can, for example, refine or broaden a query, modify a visualization, or make additional inquiries regarding particular concepts. As with other LLM interfaces, the interaction style sometimes resembles a conversation with an intelligent agent. Our contributions include various technical innovations (described in Section 3) that facilitate robust interactions:\n\u2022 We present an efficient architecture (see Figure 2) optimized to reduce both overall latency and minimize the tokens used so that our system can retrieve information from a relational database within a few seconds in most cases (see Table 1)."}, {"title": "2 RELATED WORK", "content": "Many recent projects explore the use of large language models (LLMs) to support innovative interfaces. For example, Promptify [6] utilizes a suggestion engine powered by LLMs to help users quickly explore and craft diverse prompts for text-to-image generative models, allowing users to create appealing images on their first attempt while requiring significantly less cognitive load. Graphologue [22] provides a means to identify \"long-winded responses,\" which can be one of the issues faced by users working with LLMs, and proposes a method of crafting interactive graphs that break responses into smaller node-link graphical chunks that are much easier to digest. Chen et al. [8] present an interaction method between a user and an LLM in which the model crafts and retains a personalized memory of the user, enabling better context awareness and producing more meaningful responses. GenAssist [20] is a system that makes text-to-image generation accessible for blind and low-vision creators. The system uses LLMs to generate visual questions and uses specialized vision-language models to extract answers and summarize results, assisting creators in verifying whether or not generated image candidates accurately followed their initial prompt.\nThe FathomGPT system provides a natural language interface that enables users to conduct free-form explorations of ocean science data and also makes use of vision-language models to search for specified patterns and images within the database."}, {"title": "2.2 Scientific information retrieval using LLMs", "content": "The rapid growth in large language models (LLMs) featuring task-agnostic architectures and pre-training has led to their use in a wide range of applications [4, 7, 50]. LLMs such as OpenAI's GPT models have shown remarkable performance across different domains of natural language processing tasks including translation, summarization, question answering, and generation [1, 4, 7, 14]. Our system leverages OpenAI's GPT models to connect to external tools, to generate complex SQL queries, and to generate custom code that retrieves and visualizes information from the open-source FathomNet ocean image database [24].\nRecent work in geospatial data analysis demonstrates the effectiveness of using LLMs to translate natural language prompts into SQL queries, facilitating access to spatial databases [23]. Another recent project uses an LLM model to target medical decision-making, showing increases in speed and accuracy on a range of retrieval tasks [25]. These applications underscore the transformative impact of LLMs in information retrieval.\nOne popular approach to generating and presenting data using LLMs is retrieval-augmented generation, or RAG [18, 27]. RAGs incorporate data from external sources- such as structured databases, unstructured text from the web, scientific articles, PDF documents, and other domain-specific data- in order to generate information that enhances the accuracy and credibility of the generated responses [27]. The FathomGPT pipeline incorporates prompt-to-SQL generation to quickly retrieve results from the FathomNet database."}, {"title": "2.3 Generating SQL queries", "content": "Generating SQL queries from natural language prompts is often framed as a sequence-to-sequence problem, leveraging LSTMs or transformer-based architectures for supervised learning [21, 46, 50, 57]. More recently, LLMs have shown that they are effective in converting text to SQL [42]. The use of LLMs in combination with particular prompting strategies currently lead text-to-SQL benchmarks [28, 29, 56]. Recent research investigates how best to design prompt engineering strategies and how best to represent questions and database schemas in order to produce good results when interacting with LLMs [17].\nEffective prompting can lead to drastic performance improvements [43]. Providing the model with demonstrations is known as in-context learning [7]. In this approach, exemplars are provided so that an input prompt returns an ideal response from the model. Based on the number of examples provided, the prompting strategy can be classified as zero-shot, one-shot, or few-shot. Another approach to prompting involves providing Chain-of-Thought demonstrations [53] that ask the model to generate a series of intermediate reasoning steps, which can improve performance on a range of reasoning tasks. Yet another approach called ReAct interweaves a series of verbal reasoning and task-related actions, enhancing the LLM's ability to develop and adjust reasoning strategies by incorporating new information [54].\nFathomNet uses a few-shot in-context learning approach to generate accurate results and reduce the overall response time. This provides a balanced approach that emphasizes engaging user interactions and optimizes the way in which prompts are modified to require fewer tokens while still producing meaningful results."}, {"title": "2.4 Generating visualizations", "content": "A number of recent projects have shown promising results in using LLMs to generate data visualizations [51]. For example, LIDA generates grammar-agnostic visualizations using 4 modules consisting of a summarizer, a goal explorer, a visgenerator, and an infographer [15]. Similarly, ChartGPT decomposes the visualization generation process into a step-by-step reasoning pipeline [49]. FathomGPT generates custom code using the Plotly Open Source Graphing Library [40] to quickly create custom interactive visualizations requested by the user."}, {"title": "2.5 Name resolution with knowledge graphs", "content": "A structured knowledge base, or knowledge graph (KG), contains semantic triples consisting of a subject, relation, and object that map an entity to its characteristics or to other entities. There are many knowledge bases available- including Wikidata and Freebase [55]- that can be used for question-answering tasks, such as retrieving data using natural language prompts from a wide variety of databases [38, 45].\nName resolution and concept resolution map common names and descriptions to a list of scientific names. For ocean science research, name resolution is an important task since species are known by many different names across the world, yet in scientific databases and in the research literature references to species are standardized using scientific names. Additionally, it can be useful to resolve species characteristics to scientific names, even when a common name of a creature is not known.\nThe KG required for resolution needs to contain mappings between the scientific name and the characteristics of a species, such as their coloring, their habitat, or list of their predators. The available structured knowledge bases containing scientific data, such as Wikidata and WoRMS [9], have a limited amount of data compared to the plaintext data available on Wikipedia. For example, they do not provide the color of the creature as a separate field. Our work focuses on leveraging LLMs to generate KGs from Wikipedia pages using prompt engineering to extract species characteristics from the plain-text data.\nThe user prompt also needs to be structured, and we use LLMs to convert the prompt into a semantic triple, similar to prior work that explores aligning natural language prompts with graph databases [30]. Unlike previous work that trained Memory Networks for question answering tasks [5], FathomGPT does not require any additional training. We instead use a graph alignment method similar to the entity linking solution used for the Relation Extraction model by the LAMA probe [39], which performs string matching between subject-relation-object triples."}, {"title": "2.6 Pattern analysis", "content": "Marine biodiversity often presents distinctive patterns and morphological features [2], which can be used to identify and classify different species. Analyses of these patterns can shed light on genetic relationships within and evolutionary relationships between species [58]. Differences in patterns may indicate ecological preferences and adaptive capabilities, and species within the same habitat may have unique visual characteristics that are affected by the environment [52]. Additionally, exterior features within species can vary significantly when living in different habitats [13]. By examining patterns within different populations living in different places, we can elucidate the adaptive variation of these changes and their implications for species resilience and conservation.\nPattern analysis methods support species and specimen identification, particularly for species with distinctive patterns. Stewart et al. [44] highlight the advantages of AI-based approaches in identifying individual animals as opposed to relying on human annotation. Furthermore, multiple studies have been conducted for land-based species with distinctive patterns. Parham et al. [37] utilize AI-based approaches to annotating species in an image, targeting zebras, giraffes, and sea turtles, and Crall et al. [10] present a method of identifying individual animals of a species through both sequential and nearest-neighbor approaches using patterns as the discriminating factor. However, natural language representations of these patterns are relatively unexplored. Pattern representations exist in high dimensional feature spaces that discourage direct interaction with the pattern itself. Currently, to the best of our knowledge, LLMs are not able to effectively describe the patterns found in marine life through prompt conditioning and fine-tuning.\nFathomGPT introduces an interface for interactively guiding the user to highlight patterns on an image, and then to use those patterns to retrieve images of species with similar markings. Since human-annotator pipelines for marine life often depend on features such as shape, color, pose, and patterns, a natural language-based tool for interacting with pattern representations can further improve both database lookup and image annotation tasks, as well as improve current image captioning pipelines."}, {"title": "3 THE FATHOMGPT PLATFORM", "content": "FathomGPT was designed in close collaboration with our ocean science partners, and during the development process we also solicited feedback from museum installation specialists, machine learning experts, and user experience designers. While our primary audience is ocean scientists who are familiar with the FathomNet database, we focused extensively on the design of the user experience to make sure that interactions with FathomGPT were engaging and useful to a wide range of users. In particular, we were highly cognizant of the importance of optimizing response times. FathomGPT features a chatbot-like experience that can be used by users to retrieve and visualize information from the database, and a core design goal was to make sure users could interact with the database in a fast, reliable, and seamless manner. The system is designed to generate a response to a user's query quickly, achieving a maximum response time of 30 seconds even for very complex requests (and most often under 5 seconds). Table 1 shows the response time the system takes for prompts of increasing complexity. After every prompt and between every intermediate processing step, Our interface also provides the user with information about what stage of the process the system is at currently."}, {"title": "3.1 The prompt evaluator", "content": "At the core of the FathomGPT pipeline (Figure 2), we use an LLM that is able to determine how best to create meaningful responses to different types of user prompts, making use of the function calling features available in gpt-3.5-turbo-0125 [33]. We provide the LLM with five FathomGPT prompt processing functions- related to name resolution, query generation, taxonomic data, visualization code, and general information- along with their description, the required input, and expected output type.\nDuring conversations involving successive prompts, this \"prompt evaluator\" model is the only part of the system that has access to the entire context from the user's previous conversation. The model is instructed to reformat the user prompt, modifying it to include relevant aspects of the previous context, and only then pass this modified prompt to the appropriate functions. For example:\nPrompt 1: \"Show me images of Merluccius productus.\u201d\nPrompt 2: \"What is the average temperature where that species is found according to the database?\"\nHere, despite the user's referential use of \"that\" in the second prompt, the prompt evaluator intelligently transforms it before passing it to other functions:\nModified Prompt: \"What is the average temperature where Merluccius productus is found according to the database?\"\n(See Table 2 for additional prompt modification examples.)\nThe LLM's remarkable ability to process text effectively will simplify all other processing steps following this step, as we now no longer need to pass the previous conversation to the LLM in downstream function calls. We use gpt-3.5-turbo-0125 instead of gpt-4-turbo to reduce the time it takes to execute an OpenAI API call [35, 36]. However, the gpt-3.5-turbo-0125 model has a context length of 16k, which limits the length of the conversation that FathomGPT can have. Therefore, to preserve the tokens that can be passed for a conversation, this model is inputted with only the elements from the previous messages that might be needed to understand the previous context (see Section 5.3).\nOnce the prompt evaluator determines which next function is appropriate for the prompt, that function is run. If another step is needed for processing the input prompt, control is sent back to the prompt evaluator, along with the output from the most recent function. Otherwise, it is formatted and sent to the frontend.\nTo prevent the prompt evaluator from falling into a loop when a function does not return the expected output, there is a limit on how many functions can be called by the evaluator for any single request. Within the function, we include error feedback functionality that pass any errors to the GPT-4 model to attempt to resolve them (see Section 5.1.3)."}, {"title": "3.2 Name resolution", "content": "Compared to other methods for name resolution, such as querying the World Register of Marine Species (WoRMS) or using GPT-4 directly, our method expands the coverage of the common name to scientific name mapping from WoRMS by including names from Wikipedia, and at the same time focuses the results to only include concepts that are currently available in the ever-growing FathomNet database. Our method also supports name resolution using descriptions (e.g., \"tentacles\", \"orange creatures\"), a feature that is not available in WoRMS and which will cause GPT-4 to return results that are not available as FathomNet concepts. Even when using the recently released GPT-40 and explicitly limiting it to only output FathomNet concepts, we found that this newer model would frequently generate hallucinations (see Section 5.4).\nOur name resolution process first attempts to map the common name to the scientific name using a pre-processed JSON dictionary. The data is pulled from Wikipedia and the WoRMS database. If the prompt contains a common name, we normalize the string and return the corresponding scientific names.\nIf the prompt is more complex and requires processing features (rather than common names), then we utilize knowledge graph name resolution. This is our main method for name resolution involving creature descriptions. We create knowledge graphs from Wikipedia for each available creature, which we refer to as the \"Species KGs\" (see Figure 3). We extract a subject-relation-object triple from the description of the species in the user prompt, which we refer to as the \"Prompt KG\", and we perform a graph alignment of the Prompt KG and the Species KGs to obtain the list of scientific names for creatures that match the prompt.\nEach Species KG maps a scientific name to a list of its characteristics (morphology, color, predator/prey relations, environment). We generate the Species KG by providing GPT-3.5 with the text from Wikipedia's entry for the creature and specific instructions to extract various characteristics for each concept, including \"aliases\", in \"body parts\", \"colors\", \"predators\", \"diet\", \"environment\", and \"descriptors\". This returns the corresponding characteristics extracted from the Wikipedia text as a JSON object (e.g. {\u201ccolors"}, {"title": "3.3 SQL query generation", "content": "The FathomNet database is a relational database with records of marine species found in global oceans. The data is stored in tables that contain images of marine life and also information about where individual creatures are located within the images. Each image is further described in other tables containing additional data, such as where and when the image was captured, and the temperature, oxygen levels, and water pressure at the time and location of the image capture. Thus, we use a text-to-SQL approach to search for information in the database.\nOpenAI provides APIs to fine-tune their existing model to better align its responses to our requirements [34]. Fine-tuning improves model performance by training the model on data specific to our domain and formatted in the same way that we want the model to respond, enabling our model enables to more effectively integrate with the FathomNet database than it otherwise would be able to. For example, during the development process we found that the general knowledge available to the LLM would mask the more specific domain knowledge available in the FathomNet database. For example, FathomNet contains precise longitudes and latitudes for marine regions, while asking the LLM about location information generally resolves to a single decimal. Other examples involve the non-fine-tuned model getting confused about the database schema and mismatching ids for data types that are unrelated, misinterpreting how the data should be ordered, and returning unexpected results that do not align with user prompts. (See our ablation study in Section 5.2 for further details regarding the impact of fine-tuning.)\nThe SQL generation function is invoked by the prompt evaluator, which provides it with a prompt that has been augmented to include the previous context and the appropriate prompt type. We use different fine-tuned models for generating SQL queries based on the identified prompt type, which include similarity search prompts, visualization prompts, and prompts that output images, text, or tables. During development, we found that using three different models prevented the LLM from generating unnecessarily complex SQL queries for even simple prompts.\nBenchmarks on Text-to-SQL generations show that prompt engineering methods can improve query generation [17]. In-context learning is an effective approach that helps the model to understand the input prompt and its expected response. We provide the database structure to the LLM along with the foreign key and primary key information for the tables. We also include two demonstration examples in the prompt (based on the type of prompt the SQL query is created for). In cases where the SQL generation model produces errors, we provide the GPT-4 model with the problematic SQL and the error message and instruct it to produce new SQL that resolves the error [41].\nOnce the SQL is generated, the FathomNet database will be queried and the results will be processed in a way that is appropriate to the prompt's expected output. The query itself is also available to the user for inspection."}, {"title": "3.3.1 Textual outputs.", "content": "Some prompts will generate text outputs. For example, the user prompt \"Give me the name of the species found most frequently in the region Monterey Bay at a depth level lower than 5000m\" will successfully generate the following SQL:\nSELECT TOP 1\nb.concept AS species,\nCOUNT(*) AS frequency\nFROM\ndbo.bounding_boxes AS b\nJOIN dbo. images AS i ON b.image_id = i.id\nJOIN dbo.marine_regions AS mr ON i.latitude\nBETWEEN mr.min_latitude AND mr.max_latitude\nAND i.longitude BETWEEN mr.min_longitude\nAND mr.max_longitude\nWHERE\nmr.name = 'Monterey Bay'\nAND i.depth_meters < 5000\nGROUP BY\nb.concept\nORDER BY\nfrequency DESC;\nHere, the FathomNet database will return a response containing the columns \"species\" and \"frequency\" with a single row containing \"Strongylocentrotus fragilis\" as the species and \u201c11399\u201d as the frequency. Our fine-tuned model will incorporate these results into the generated response to produce the sentence: \"The most frequently found species in the region Monterey Bay at depth level lower than 5000m is Strongylocentrotus fragilis.\"\nTabular output can be generated for some prompts, such as \"Generate me a list of top 20 species from the database that are found in pressure between 300 and 600 dbar\". The generated query returns a table of data, which is transformed by our fine-tuned model into a JSON object that is then rendered by our web interface. We use the FathomNet API to generate taxonomic information and display it to the user. In a pre-processing step, we collect the taxonomic trees for each species available in FathomNet and create a dictionary that maps each taxon to its ancestor and children nodes. A user can write a prompt that asks for taxonomic information about a species, such as \"Show me the taxonomic tree of dinner plate jellyfish\". The result is presented as a hierarchy of text formatted to highlight taxonomic relationships."}, {"title": "3.3.2 Image outputs.", "content": "Most frequently, users craft prompts that request images of species. FathomGPT generates SQL queries that return image URLs from FathomNet, and then displays these images in a table in our web interface. In most cases, FathomNet images are associated with a set of scientific measurements collected at the time and location the image was captured, and because each image represents a particular species, we can also also retrieve the species taxonomy. When a user clicks on an image, we display a \"data card\" that highlights the bounding box of the species within the image and that provides the additional information alongside the image (see the example in Figure 1, top).\nUsers can also upload their own image and perform a similarity search to find images in the database that match most closely. For example, a user could enter the prompt \"Find images similar to my input image at a depth greater than 2000 meters\" along with an input image. We extract feature vectors using a Vision Transformer model trained on ImageNet [12, 16] for each bounding box surrounding each species in each FathomNet image. (We found that using Vision Transformer for our similarity searches gave the best Top-1 Accuracy and Top-10 Recall Scores compared to the EfficientNet V2 Large, Densenet, and EfficientNet B7 models [19, 47, 48]). Once the prompt evaluator determines that the user is requesting a similarity search, we extract the feature vectors from the input image via the Vision Transformer model, which then enables us to compute and rank the cosine similarity scores for the input image feature vector and the pre-computed feature vectors for the cropped images stored in the FathomNet database. Our SQL generation model can then utilize the cosine similarity scores to generate a SQL query that returns the relevant similar images."}, {"title": "3.3.3 Visualization outputs.", "content": "Users can write prompts to generate custom interactive visualizations. FathomGPT uses the Plotly graphing library to produce these visualizations, which can include bar charts, line charts, heat maps, area charts, box plots, and scatter plots, among others (see Figure 1, bottom).\nFirst, we instruct GPT-3.5-Turbo to generate an appropriate SQL query to retrieve the data necessary to populate the visualization and also to identify the data types that the visualization will ingest. We then simultaneously use the generated SQL to query FathomNet and at the same time instruct GPT-4 to generate Plotly code based on the identified data types and the user prompt. Once the SQL query returns data from FathomNet, it is passed into the generated Plotly code, which creates a Plotly object that is then displayed in our web interface. If the data types do not match, i.e., the query outputs do not align with the Plotly inputs, the visualization code will produce an error. We are able to catch the error and then regenerate the Plotly code using the query results themselves (see Section 5.1.3).\nDuring development, we noticed that GPT is able to produce effective complex code more readily than it can produce complex SQL. For example, if a user asks FathomGPT to generate a bar chart representing data that is split into multiple categories, this categorization can either be performed directly in the SQL or within the Plotly code. However, we found that the chances of the model generating an error were higher when generating SQL to categorize the data. At the same time, we want to ensure that the query does not return too much data, so we instruct the model to filter out data that is not required for the visualization, reducing both the amount of tokens and amount of data processing required by the Plotly code. That is, we leverage the abilities of the model to optimize the visualization generation process.\nUsers can also modify the Plotly code that is created simply by entering a follow-up prompt that asks FathomGPT to update the visualization. For example, a user might ask FathomGPT to \"Show a bar chart of population of species in Monterey Bay whose population is greater than 2000\". If the resulting visualization does not include a label for each bar, the user can simply ask FathomGPT to produce a new version of the visualization with a prompt such as \"Add the population number on top of each bar\"."}, {"title": "3.4 Pattern extraction and search", "content": "Users can extract patterns from an uploaded image and then use them to search for FathomNet images containing similar patterns. We utilize a segmentation step before pattern extraction to ensure patterns that are taken from the uploaded image and are not influenced by other objects in the image. We use the Segment Anything model [26], which enables us to rapidly generate accurate masks for objects without the need for additional fine-tuning. Segment Anything requires an initial point to begin the segmentation process, which is given in the form of a mouse click input to indicate which pattern needs to be extracted. The user is then presented with 3 masks of decreasing rigidity on the mask constraint (so that the different masks each cover a larger part of the pattern), as we found large variations in results depending on the input image. The masked image selected by the user is then utilized for further pattern extraction. The complete pattern analysis pipeline is showcased in Figure 6.\nWe utilize color differentiation to obtain complete patterns from the body of a specimen. Considering the physical phenomena of reflection, refraction, and scattering of light underwater, the same color might have different brightness levels. Hence, we use the HSV value to distinguish colors to obtain better results than using the RGB value. After the user selects a target HSV value, we mask the pixels with the same type of colors and generate a sub-image containing a pattern with the same color as the target color. Our pattern extraction interface enables a user to click on the target pixel in the image to extract the pattern corresponding to the color of this pixel. Moreover, users can adjust the color range value to determine the inclusivity of adjacent colors.\nOnce a pattern is extracted, it can then be used to search for similar patterns in the FathomNet database. In order to support better image similarity search on images using natural language, we search for not only the whole image but also parts of the image. We use EfficientNet V2 [48] to generate feature vectors, striking a balance between speed and semantic richness. Search results are subsequently ranked according to the L2 norm and then presented to the user."}, {"title": "4 USAGE SCENARIOS", "content": "In this section, we present example usage scenarios describing how different users can engage with FathomGPT in order to retrieve and analyze data stored in the FathomNet database. All of the functionality demonstrated below (and more) is currently available via links available at our project website (https://github.com/CreativeCodingLab/FathomGPT), and we present additional examples in the Supplementary Video documentation accompanying this paper."}, {"title": "4.1 Scenario 1: Comparing the species Praya dubia and Bathyraja abyssicola", "content": "FathomGPT can be used by ocean scientists to search for, visualize, and analyze data from the FathomNet database. In this scenario, the scientist wants to compare information about two different deep sea creatures, Praya dubia and Bathyraja abyssicola. They ask FathomGPT for their common names with the prompt \"What is the common name of Praya dubia?\" and learn that the two creatures are the giant siphonophore and the deepsea skate, respectively. Here we depict the free-form interrogation users can carry out during an exploratory session using FathomGPT.\nThe scientist first asks FathomGPT for some background information about the deep sea creatures available in FathomNet (Figure 7). The scientist wants to understand the relationships between the temperature and pressure levels of these deep sea creatures, so she asks FathomGPT to generate a scatter plot comparing this information across both measurements. FathomGPT converts the natural language prompt to an SQL query, runs the query to obtain data from the FathomNet database, and then displays an interactive visualization (Figure 8).\nShe immediately notices that the two species live in separate temperature and pressure levels. Specifically, she observes that one specimen of Bathyraja abyssicola is found at a temperature that was much higher than expected. She then investigates by asking FathomGPT about this outlier. FathomGPT then shows the scientist additional details, such as oxygen and salinity levels (Figure 9).\nSince she is particularly interested in Praya dubia and Bathyraja abyssicola, she compares the regions and depths where the two creatures are found. She learns from FathomGPT that both species are found in the Monterey Bay region, but at different depths (Figure 10). At this point, the scientist has learned something about both of these species and can continue using FathomGPT to ask additional questions as desired."}, {"title": "4.2 Scenario 2: Locating regions with high populations of Aurelia aurita", "content": "FathomGPT can also be used by ocean scientists to locate regions with a high concentration of a species and to inspect outliers. They can use the location information collected in previous ocean survey missions to make a determination about where to go to collect additional image samples.\nIn this scenario, the ocean scientist wants to find the exact locations where Aurelia aurita (moon jellyfish) were previously observed. He asks FathomGPT to generate a heatmap of these observations (Figure 11). The scientist learns that many moon jellies have been observed in the area centered around (37\u00b0N, 122\u00b0W). He also noticed one sample that was found far away from the others. He plots the samples on a map color-coded by the image observer and notices that the outlier was collected during a different mission than the other samples, explaining why it was not located around the same area and providing a starting point for further investigation (Figure 12). FathomGPT creates interactive charts, so the scientist can simply hover over a point to quickly find information about the observation."}, {"title": "4.3 Scenario 3: Identifying an unfamiliar species via a photo", "content": "In addition to supporting scientific research tasks, FathomGPT can be used by citizen scientists and ocean enthusiasts as a tool to gather information about marine life. In this usage scenario, we provide an example of a user interested in identifying and learning more about a jellyfish that they were stung by on a beach in Monterey Bay.\nFirst, the user uploads a photo\u00b9 of that jellyfish to FathomGPT to find the names of similar-looking creatures. The user sees that Benthocodon and Poralia rufescens look similar to the jellyfish that stung them (Figure 13). The user further narrows down their question by determining which species they are most likely to find at the beach. The user prompts FathomGPT with questions such as \"Are benthocodon found near the shore?\" and learn that they are much more likely to encounter Poralia rufescens, since Benthocodon"}]}