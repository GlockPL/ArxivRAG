{"title": "An Attempt to Unraveling Token Prediction Refinement and Identifying Essential Layers of Large Language Models", "authors": ["Jaturong Kongmanee"], "abstract": "This research aims to unravel how large language models (LLMs) iteratively refine token predictions (or, in a general sense, vector predictions). We utilized a logit lens technique to analyze the model's token predictions derived from intermediate representations. Specifically, we focused on how LLMs access and use information from input contexts, and how positioning of relevant information affects the model's token prediction refinement process. Our findings for multi-document question answering task, by varying input context lengths (the number of documents), using GPT-2, revealed that the number of layers between the first layer that the model predicted next tokens correctly and the later layers that the model finalized its correct predictions, as a function of the position of relevant information (i.e., placing the relevant one at the beginning, middle, or end of the input context), has a nearly inverted U shape. We found that the gap between these two layers, on average, diminishes when relevant information is positioned at the beginning or end of the input context, suggesting that the model requires more refinements when processing longer contexts with relevant information situated in the middle, and highlighting which layers are essential for determining the correct output. Our analysis provides insights about how token predictions are distributed across different conditions, and establishes important connections to existing hypotheses and previous findings in AI safety research and development.", "sections": [{"title": "Introduction", "content": "Recent advances in technologies have seen an increasing number of applications of capable AI systems in various domains. One example is that of large language models (LLMs) that have exhibited improved capabilities in content and code generation and language translation. As LLMs rapidly advance in sophistication and generality, understanding them is essential to ensure their alignment with human values and prevent catastrophic outcomes [Hendrycks and Mazeika, 2022, Hendrycks et al., 2023, Ji et al., 2024]. Research that aims to improve an understanding of LLM goes beyond simple performance metrics, to unravel the internal workings of LLMs. This work utilized an observational approach that analyzes the inner workings of neural networks: Logit Lens (as discussed in detail in Section 2). We focused on how LLMs access and use information from input contexts, and how positioning of relevant information affects the model's token prediction refinement process, to gain more insights that potentially aid the development of methods that can ensure trustworthiness in LLMs.\nMechanistic interpretability (MI) is a growing approach that aims to precisely define computations within neural networks. It seeks to fully specify a neural network's computations, aiming for a granular understanding of model behavior, akin to reverse-engineering the model's processes into"}, {"title": "Logit Lens: A View into Next Token Information", "content": "Logit lens technique, as first introduced by nostalgebraist [2020], provides a way for examining intermediate representations of model's predictions at different stages of the model's output generation process, thereby providing insights into how the model's internal representations evolve as model layers iteratively refine predictions.\nSpecifically, given a pre-trained large language model (LLM), we use logit lens to decode a probability distribution over tokens from each intermediate layer. These token distributions represent model predictions after $l \\in \\{1, .., L\\}$ layers of input processing. Given a sequence of tokens $t_1, \u2026, t_n \\in V$, and $h_n^{(i)} \\in \\mathbb{R}^d$ denoting the hidden state of token $t_i$ at layer $l$, the logits of the predictive distribution $p(t_{n+1}/t_1,.., t_n)$ are given by\n\\[[logit_1, ..., logit_{|V|}] = W_u \\cdot LayerNorm_l (h_n^{(i)}),\\]\nwhere $W_u \\in \\mathbb{R}^{|V| \\times d}$ denotes an embedding matrix, and LayerNorm is the pre-embedding layer normalization. The logit lens applied the same unembedding operation to the earlier hidden states $h_n^{(i)}$.\n\\[[logit_1, ..., logit_{|V|}] = W_u \\cdot LayerNorm_l (h_n^{(i)}),\\]\nwhere an intermediate predictive distribution over tokens at layer l, $p_l (t_{n+1}|t_1, .., t_n)$, can be obtained. In this experiment, we utilized logit lens to hidden states of GPT-2 processing data of internet. For example, given an instance \"Hinton is a prominent figure in the field of artificial intelligence and deep learning.\", we observed how distributions over next token predictions gradually converge to the final distribution. The observed, general trend is that initial predictions of tokens"}, {"title": "Relation of Logit Lens to Input Context Lengths and the Position of Relevant Information within Input Context", "content": "Input contexts for LLMs can contain thousands of tokens, especially when processing lengthy documents or integrating external information. For these tasks, LLMs must efficiently handle long sequences. Study done by [Ivgi et al., 2023, Liu et al., 2024] explored how LLMs use input contexts to perform downstream tasks.\n[Liu et al., 2024] designed an experiment to assess how LLMs access and use information from input contexts. They manipulated two factors: (1) the length of the input context and (2) the position of relevant information within it, to evaluate their impact on model performance. They hypothesized that if LLMs can reliably use information from long contexts, performance should remain stable regardless of the position of relevant information.\nMulti-document question answering was selected for the experiments, where models must reason over multiple documents to extract relevant information and answer a question. This task reflects the retrieval-augmented generation process used in various applications. In more details about the set up, they controlled two key factors: (i) input context length by varying the number of documents (simulating different retrieval volumes), and (ii) the position of relevant information by altering the document order, placing the relevant one at the beginning, middle, or end of the context. Their findings showed that the position of relevant information in the input context significantly impacts model performance, revealing that current language models struggled to consistently access and use information in long contexts. Notably, they observed a U-shaped performance curve: models performed best when relevant information is at the beginning or the end of the input context, but their performance declines sharply when the information is located in the middle. In this experiment, we followed procedures and data sets used in [Liu et al., 2024], to examine how token predictions are distributed as a function of input context length and position of relevant information.\nIn which layer(s) does the model's correct prediction happen and finalize? As illustrated in Figure 2, the top-1 token that matches the correct answer typically emerges in the middle or later layers. When handling long input contexts with relevant information placed in the middle, the correct top-1 tokens are identified similar to earlier, but it takes many additional layers before the model finalizes the prediction. The logit lens reveals how predictions are iteratively refined as they pass through each successive layer. Early layers may produce outputs that seem plausible but are far from accurate. The model begins with rough guesses, which are gradually improved as it incorporates more context and relevant information. This study indicates that when identifying bottlenecks or inefficiencies in the model, it's crucial to consider the position of relevant information. This positioning impacts to some extent the refinement process of token prediction, highlighting which layers on average are critical for determining the final output."}, {"title": "Relation of Logit Lens to Probing", "content": "Probing refers to training a classifier to predict properties from internal representation to identify such properties, which is presumably encoded in learned representations [Alain, 2016, Ettinger et al., 2016, Hupkes et al., 2018] (see Figure 3). High probing accuracy in a layer indicates that the correct answer can be extracted from its hidden states (see Section 4.1 in [Levinstein and Herrmann, 2023]). However, this standard is often too easy to meet, particularly in straightforward classification tasks with high-dimensional hidden states [Hewitt and Liang, 2019]. In contrast, a high logit lens performance signifies that the layer effectively encodes correct answers along a direction in the residual stream, providing significantly more insight as shown in interventional experiments [Li et al., 2022] that internal representations can be used to control the output of the network. Conversely, low logit lens performance does not necessarily mean that the correct answers cannot be decoded from that layer."}, {"title": "Relevance to AI Safety", "content": "Understanding inner working mechanisms of LLMs is essential for ensuring their safe development as we move toward more powerful models. Mechanistic interpretability approach has the potential to significantly advance LLM/AI safety research by providing a richer, stronger foundation for model"}, {"title": "Discussion and Limitations", "content": "The effectiveness of the logit lens can vary significantly depending on the task and model used. For instance, tasks that require deep, abstract reasoning, layers far from the output may offer little relevant information when analyzed solely through token prediction.\nExplaining how each layer of a model contributes to the final prediction by reducing complex representations to token predictions introduces biases. Many layers may serve other functions (e.g., refining internal features or re-weighting contextual information) which are not directly captured by logits. For instance, middle layers may not just be involved in predicting the next token, so using the logit lens alone may overlook their other roles in the model.\nThe logit lens relies on predicting which tokens have high probabilities at different layers. However, token distributions can be highly skewed, with some tokens dominating; even though they may not reflect the true nature of the underlying hidden state. This can obscure the true purpose or significance of intermediate representations.\nUseful perspective on how language models construct their predictions can be obtained using logit lens. Its capabilities, however, are constrained when it comes to unveiling the model with more complex internal operations. Logit lens do not fully reveal the layered, nuanced, and abstract processes that underlie a language model's true capabilities. Thus it is essential to utilize logit lens in combination with other techniques that focus on different aspects of model behavior (e.g., neuron activity and attention patterns), to uncover the internal workings, and to obtain a deeper and more comprehensive understanding of how language models function."}, {"title": "Conclusion", "content": "Investigating the intermediate representations by examining how a model iteratively refines token predictions, offers insights into how a model's internal representations evolve. In this study, we utilized the logit lens to meticulously examine next token prediction refinements across different conditions, to some degree, to uncover critical nuances in how predictions are progressively refined. A deeper grasp of the prediction refinement process will enhance the development in AI safety research and development. That is, by establishing connections between the previous findings and existing hypotheses, we aim to advance the understanding of LLM behavior. These insights are vital for the development of AI technologies that are not only highly effective but also secure and aligned with human values."}]}