{"title": "Empowering Meta-Analysis: Leveraging Large Language Models for Scientific Synthesis", "authors": ["Jawad Ibn Ahad", "Rafeed Mohammad Sultan", "Abraham Kaikobad", "Fuad Rahman", "Mohammad Ruhul Amin", "Nabeel Mohammed", "Shafin Rahman"], "abstract": "This study investigates the automation of meta-analysis in scientific documents using large language models (LLMs). Meta-analysis is a robust statistical method that synthesizes the findings of multiple studies (support articles) to provide a comprehensive understanding. We know that a meta-article provides a structured analysis of several articles. However, conducting meta-analysis by hand is labor-intensive, time-consuming, and susceptible to human error, highlighting the need for automated pipelines to streamline the process. Our research introduces a novel approach that fine-tunes the LLM on extensive scientific datasets to address challenges in big data handling and structured data extraction. We automate and optimize the meta-analysis process by integrating Retrieval Augmented Generation (RAG). Tailored through prompt engineering and a new loss metric, Inverse Cosine Distance (ICD), designed for fine-tuning on large contextual datasets, LLMs efficiently generate structured meta-analysis content. Human evaluation then assesses relevance and provides information on model performance in key metrics. This research demonstrates that fine-tuned models outperform non-fine-tuned models, with fine-tuned LLMs generating 87.6% relevant meta-analysis abstracts. The relevance of the context, based on human evaluation, shows a reduction in irrelevancy from 4.56% to 1.9%. These experiments were conducted in a low-resource environment, highlighting the study's contribution to enhancing the efficiency and reliability of meta-analysis automation.", "sections": [{"title": "I. INTRODUCTION", "content": "Meta-analysis is a powerful statistical approach that combines the findings of multiple studies to provide a comprehensive understanding of the same research topic [1]. A meta-analysis paper, or meta article, offers a structured analysis of numerous individual support articles. Individual studies often face limitations, such as small sample sizes or narrow focus, making it hard to draw definitive conclusions [2]. Meta-analysis aggregates data from different studies, providing robust estimates that inform research decisions, guide treatments, and influence healthcare policies [3]-[6]. In applied scientific fields, meta-analyses play a crucial role: consolidating the results of clinical trials [7], [8], evaluating public health strategies, material performance, and farming practices [9]-[11], and assessing the impacts, behavior, policies, and teaching methods of climate in fields such as environmental science, psychology, economics, and education [12]-[15]. Meta-analysis involves the analysis of extensive datasets, as it incorporates numerous studies, which presents a significant challenge to big data. The process is often labor-intensive, requiring manual extraction and analysis of data from multiple research articles, which is both time-consuming and susceptible to human error. This underscores the critical need for an automated pipeline to streamline and improve the efficiency of meta-analysis generation. The advancements in large language models (LLMs) offer promising potential, suggesting that these models could be utilized to manage the vast data requirements of scientific meta-analysis.\nTraditional meta-analysis faces challenges with big data scalability. (a) Manual Meta-analysis: Involves manual data extraction and analysis, limiting scalability in rapidly expanding domains [20]. While Al advances automate tasks like information retrieval and summarization, focusing primarily on shorter summaries, LLMs show strong potential in summarization but still face limitations. (b) LLMs as summarizers: Although LLMs like Llama2-13b have demonstrated proficiency in generating summaries and performing question-answering tasks [16], [17], [21], their utility in synthesizing extensive research findings for meta-analysis remains constrained. Current approaches in this domain predominantly focus on generating condensed summaries for shorter, narrative-based content rather than synthesizing large-scale scientific data. To address this gap, researchers have explored RAG techniques. (c) Retrieval Augmented Generation: By integrating retrieval-based mechanisms, RAG enables LLMs to access and summarize large datasets through document retrieval [18], [19], [22]. However, this method falls short when applied to meta-analysis, which demands specialized data extraction techniques and a deeper synthesis of scientific contexts [23]. LLMs' current limitations highlight a need for targeted fine-tuning and tailored approaches to handle complex, structured large-scale scientific data.\nTo bridge this gap, our research introduces a novel approach that leverages LLMs with RAG to automate and streamline the meta-analysis process. We have built a comprehensive dataset with various meta-analysis scenarios in various scientific fields, which contains the content of the meta-articles along with the content of the support papers. This dataset facilitates both training and evaluation to stimulate further research. Its purpose is to fine-tune LLMs, enabling them to understand and replicate data extraction patterns for meta-analysis. We introduce a novel loss function, Inverse Cosine Distance (ICD), specifically designed for training LLMs in large-context scenarios to handle large-data challenges. This function enhances the performance of LLMs in generating meta-analysis with high relevance and accuracy. By fine-tuning LLMs for large-context tasks and employing specific prompt engineering techniques, shown in Fig. 1, we aim to overcome the limitations of existing methods of handling big contextual data. By integrating RAG with our fine-tuning strategy, LLMs generate precise, instruction-based meta-analysis content, ensuring quality and efficiency. Our approach reduces the labor-intensive aspects of meta-analysis, enabling LLMs to handle large contexts and generate structured abstracts effectively. This work holds significant potential for improving research synthesis across various domains.\nOur contribution comprises (1) preparing a comprehensive dataset to fine-tune LLMs for meta-analysis generation, (2) fine-tuning LLMs with the novel ICD loss function, enhancing their ability to handle large-context scientific data and extract relevant information for meta-analysis, and (3) leveraging these fine-tuned LLMs by integrating RAG to generate precise, instruction-based meta-analysis from large-scale scientific data."}, {"title": "II. RELATED WORKS", "content": "Meta-Analysis Strategy: In recent years, the landscape of meta-analysis has witnessed significant advancements, particularly with the development of comprehensive databases, facilitating systematic reviews. Csizmadia et al. [24] contributed to this domain by introducing a global database of innovation and quality management, meticulously compiling the PRISMA methodology, covering records from 1975 to 2021. Furthermore, Yudhanto et al. [25] addressed the labor-intensive nature of data collection for meta-analysis, presenting a method using bibliometric studies from the Science Direct Database. Their approach, which involved data collection from published searches using desired keywords over the last decade, significantly streamlined the process, contributing to the efficiency of meta-analysis procedures. To further enhance this progress, our paper introduces a novel approach that harnesses the capabilities of LLMs and RAG. This approach aims to streamline the meta-analysis process, empower LLMs to handle large contexts efficiently, and conduct a structured meta-analysis of the provided research papers. Our contribution lies in developing a Comprehensive Meta-Analysis Dataset, which serves as a valuable resource for training and evaluating the efficiency of our proposed approach.\nLarge Context Summarization: Recent advancements in large-context summarization have led to the development of techniques to generate concise and informative summaries from extensive documents. In particular, Subbiah et al. [16] introduce a fragmentation strategy, converting full stories into manageable fragments and associating them with prompts to facilitate effective summarization. Keswani et al. [21] focus on summarization and question-answering tasks using the Llama-2 (13B) model, employing clustering techniques based on cosine similarity to improve efficiency despite computational constraints. Furthermore, observations on different LLM summarization performances have been made using zero-shot prompt techniques [26], [27]. These advancements underscore the significance of leveraging LLMs for large-context summarization tasks. Leveraging these developments, our proposed method successfully utilizes large contexts by breaking them into smaller, manageable chunks. This strategy facilitates easier summarization by smaller LLMs, thereby enhancing the overall efficiency of the process.\nSummarization Quality Assessment by Evaluation: Evaluation metrics are crucial for assessing the effectiveness of automated summarization systems. Traditional metrics often have limitations in accurately capturing the quality of generated summaries [28]. Innovative frameworks like HumanELY [29] have been proposed, incorporating key evaluation metrics including relevance, coverage, coherence, harm, and comparison. Additionally, novel scoring systems leveraging LLMs have been introduced, shedding light on how different identities influence performance [30]. A taxonomy of LLM-based NLG evaluation methods has also been presented, delineating their advantages and drawbacks [31]. Despite these efforts, achieving comprehensive evaluation frameworks for NLG systems remains challenging. Inspired by the pioneering work of Chaudhary et al. [32] on generating both relevant and irrelevant queries, we adopt a similar methodology to evaluate the efficacy of our generated meta-analysis. Our evaluation metrics encompass not only relevance but also nuances, categorizing outputs into Relevant, Somewhat-Relevant, and Irrelevant. Incorporating this thorough evaluation framework allows us to deliver a detailed evaluation of the performance of our automated meta-analysis synthesis, thereby enriching the depth of analysis and insights in our research. Our approach introduces a novel evaluation concept based on hard voting, contributing to meta-analysis automation. Leveraging a comprehensive meta-analysis dataset, innovative training methods, and fine-tuning strategies tailored for instruction-based meta-analysis abstracts, our approach stands out for its effectiveness and reliability in advancing research synthesis efficiency across domains."}, {"title": "III. METHODOLOGY", "content": "Several innovative efforts have been made to guarantee that LLMs can manage lengthy contexts. However, to incorporate big textual data challenges, LLMs require numerous amounts of resources. This study presents a novel approach for generating meta-analysis using LLMs, particularly with long context lengths. This section formally outlines our method for using LLM to produce meta-analysis content.\nProblem Formulation: Consider there are $m_i$ number of meta-articles, where $j \\in [1, n]$. For each meta-article, there is a set of support articles, $S_i == {v_1, v_2, ..., v_{|S_i|}}}$. $v_i$ represents the abstract of $i^{th}$ support article related to $j^{th}$ meta-article. We aim to build a model $M$ to generate a meta-article's abstract, $y$ using all abstracts inside the set $S_i$. This study focuses on generating a relevant $y$ through a low context length LLM (e.g., Llama-2 7B, Mistral 7B, and Gemma 7B).\nHere we have investigated two important aspects of this problem. (a) Handling large context length: Typically, meta-analyses are performed through manual analysis and data extraction from supporting articles. Recently, LLMs have demonstrated their ability to summarize extensive textual data. While substantial research has focused on summarization [18], [19], [22], the application of LLMs for meta-analysis remains unexplored. Meta-analysis often involves structured data derived from supporting articles, yet most LLMs operate within constrained context-length environments during fine-tuning. Our objective is to address this limitation by efficiently managing large contextual data and segmenting it into smaller chunks to facilitate effective fine-tuning of LLMs. (b) Enhance information retrieval: Prior research has demonstrated that fine-tuning LLMs enhances their data extraction capabilities [23]. However, when generating large contextual, analytical data, LLMs require access to external knowledge sources. RAG has shown promising results in addressing this challenge. In our approach to enabling context-length-restricted LLMs to generate meta-analysis and to further expand the scope of knowledge from supporting articles, we aim to integrate RAG with our fine-tuned meta-analysis generator LLMs.\nGenerative language models are capable of producing reviews or summaries of given contexts. They still have issues producing analytical context based on substantial context inputs. Our first step in addressing this big-data challenge is to create a dataset to generate a meta-analysis. Large scientific context datasets like MAD have not been used before to fine-tune context-length-restricted LLMs.\nThe dataset, MAD that we constructed consists of two columns: one containing meta articles' abstracts and the other containing the abstracts of the support articles. For example, consider the meta-article titled \"Intervention methods for improving reduced heart rate variability in patients with major depressive disorder: A systematic review and meta-analysis\" [20]. We used the abstract of this paper as our target meta-analysis abstract. From Table 1 of this paper, we identified that it conducted a meta-analysis of over twenty studies. We manually extracted the abstracts of these support articles by following the references listed in the table. These twenty abstracts were placed in the second column ($S_2$) alongside the meta-article's abstract ($y$). Essentially, the goal is for the LLM to generate a meta-analysis abstract from these support articles' abstracts.\nUsing this approach, we gathered 625 meta-articles from ScienceDirect, along with the abstracts of all the support articles included in that meta-analysis. In total, dataset MAD includes 6344 support articles' abstracts and 625 meta-articles' abstracts. The dataset statistics, along with the demographic information of the human evaluators who assessed model performance, are shown in Table I, and the distribution of support articles in meta-articles is shown in Fig. 3.\nGiven the limitation in context length for many language models, processing long or complex documents as a whole can become inefficient and may lead to suboptimal results. To address this, chunking the support articles into smaller, meaningful segments allows for more effective input to the language model. By chunking, we ensure that all support article abstracts in the set $S_i$ are considered while maintaining manageable input sizes for low-context models.\nTo manage the input size and improve model performance, we divide the support articles set $S = {v_1, v_2, ..., v_{|S_i|}}$ into multiple smaller overlapping chunks. Overlapping will be done with some portions of abstracts. This will allow the coherence and continuity between chunks, reducing the chances of information loss. Chunking of $S_i$ into k possibly overlapping chunks is defined as:\n$C(S_i) = {C_1, C_2, ..., C_k}$\nwhere:\n* $C_i \\subseteq S_i$ for each $i \\in [1, k]$,\n* $\\bigcup_{i=1}^{k} C_i = S$ (the union of all chunks covers the entire set, though the chunks may overlap),\n* $C_i \\cap C_l \\neq \\emptyset$ for $i \\neq l$ (contents will overlap).\nFor example, suppose the set of support article abstracts $S_i = {v_1, v_2, v_3, v_4, v_5}$ is divided into three overlapping chunks. The chunking is as follows: $C(S_i) = {C_1, C_2, C_3}$ where: $C_1 = {v_1, v_2}, C_2 = {(portion of)v_2, v_3, v_4}, C_3 = {(portion of)v_2, v_3}$. In this example, $v_2$ and $v_3$ overlap in $C_2$ and $C_3$.\nAfter creating the dataset MAD, two popular LLMs are considered for the experiment: Llama-2 (7B) [33] and Mistral-v0.1 (7B) [34]. They are fine-tuned on the constructed dataset MAD and evaluated using the test set. To further improve their performance in generating relevant outcomes, we applied the RAG approach [35] to the fine-tuned versions of each model. Fig 2 depicts the methodology for fine-tuning LLMS and generating meta-analysis.\nModel Architecture Overview: We utilized two prominent LLMs in this study. (a) Llama-2 (7B), a transformer-based LLM developed by Meta, includes 32 attention heads, a 32,000-token vocabulary, and a context length of 4,096. It uses the Swish-Gated Linear Unit (SwiGLU) activation function [36]. (b) Mistral-v0.1 (7B) features similar architecture with 32 attention heads and a 32,000-token vocabulary but offers a larger context length of 8,192. It employs the Sigmoid Linear Unit (SiLU) activation function [37] and incorporates grouped-query attention (GQA) with sliding window attention (SWA) for efficient handling of variable sequences [34]. Mistral-v0.1 (7B) outperforms both Llama-2 (7B) and Llama-2 (13B) in benchmarks, making it our model of choice.\nFine-tuning LLMs: Original models like Llama-2 and Mistral-v0.1 benefit from extensive pre-training on massive datasets, allowing them to grasp complex linguistic structures. However, this generic training might not be ideal for specialized tasks like generating meta-analysis abstracts from lengthy source materials. Fine-tuning bridges this gap by adapting these pre-trained models to new datasets and large data tasks. Following the selection of models, Llama-2 (7B) and Mistral-v0.1 (7B) were fine-tuned on the processed MAD dataset, utilizing chunked samples $C_i^j$ paired with their respective meta-article's abstracts $y^j$. This supervised fine-tuning process paired each chunked sample $C_i^j$ with its meta-analysis abstract $(C_i^j, y^j)$, where $C_i^j \\subseteq S_i$ and $y^j$ serves as label meta-article's abstract. The models, M, were trained to recognize patterns for generating meta-analysis content from large contexts, accommodating the multiple chunks associated with each $y^j$. Instruction-based fine-tuning was employed with a focus on prompt engineering. Various prompt configurations were tested to optimize the models' ability to generate accurate and coherent meta-analysis abstracts from long contexts. This approach ensured that the models effectively learned the specific patterns required for high-quality content generation.\nInverse Cosine Distance (ICD): To support fine-tuning, a specialized training mechanism is used with the ICD loss metric. The ICD function measures the dissimilarity between the model-generated output $\\hat{y}$ and the ground truth $y^j$ vectors, incorporating a small positive constant $\\epsilon$ in the denominator to ensure numerical stability and improve the fine-tuning process. The formulation for ICD is given below:\n$ICD = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{cosine\\_simi(y, \\hat{y}) + \\epsilon}$\nDuring fine-tuning, models M processed chunked samples $C_i^j$ to produce predicted abstracts $\\hat{y}$. The ICD loss, calculated using formula 1, measured the dissimilarity between $\\hat{y}$ and the ground truth abstracts $y^j$, guiding parameter updates via backpropagation. The process, constrained by resources, was carried out for 2 epochs over 5 iterations, refining the model and improving abstract accuracy.\nCombining Fine-tuned model with RAG: Fine-tuning LLMs is highly effective for specific tasks; however, models with limited context, such as Llama-2 (7B) and Mistral-v0.1 (7B), face challenges when dealing with chunked data samples. For instance, when generating $\\hat{y}$ from a particular chunk $C_i^j \\subseteq S_i$, these models may lack information from other chunks $C_i$ of the same dataset, MAD. RAG addresses this issue by retrieving relevant information from other chunks for the $j^{th}$ data sample, thereby reducing the need for extensive fine-tuning and minimizing irrelevant content. This approach involves storing each chunked test sample $C_i^j$ in a vector database. Relevant chunks are then retrieved using semantic search based on queries and the stored chunks. The retrieved content is subsequently fed into the LLMs, which process these additional contexts to generate a more accurate meta-analysis abstract.(Additional Details in the Supplementary)"}, {"title": "IV. EXPERIMENT", "content": "Setup Dataset: We used the dataset MAD for fine-tuning the LLMs. The dataset is split into a train, test, and validation set. The training set includes 400 meta-analysis scenarios. Given that support papers' abstracts, $S_i$, often exceed the given context limit, chunk-based preprocessing is applied to chunk support papers' abstracts, $S_i$. Chunking wasn't applied to meta-articles' abstract $y^j$ as the context length was manageable without chunking. Table I illustrates how chunking reduces context length. The same chunking approach is applied to the validation set (175 samples) and test set (50 samples). After chunking, the training set expands to 3659 samples. After fine-tuning the models, we then tested our fine-tuned model on the benchmark Open-i dataset [38], writer_summaries [27], and the large scientific document dataset CL-SciSumm [39] to compare performance, shown in Table II.\nImplementation Details1: The dataset MAD requires careful management due to the context size limitations of LLMs, which have a maximum context length of 4096 tokens. The \"Recursive TextSplitter\" from LangChain2 is used to chunk the support papers' abstracts, $S_i$, into overlapping segments of 200 tokens, capped at 2000 tokens. This approach converts meta-article abstracts into target values $y^j$ and the chunks $C_i^j$ into features for supervised fine-tuning. Due to the seven billion parameters of Llama-2 (7B) and Mistral-v0.1 (7B), the Quantized Low-Rank Adapters (QLoRA) configuration [44] is employed for model loading. A custom trainer class, extending the transformer trainer and incorporating the ICD loss function, is used to measure dissimilarity between generated and target meta-analysis content, guiding iterative model weight updates. Inputs are tokenized using Transformers' AutoTokenizer, and LangChain facilitates retrieval augmentation. All experiments were conducted on NVIDIA Tesla T4 (2x) GPUs using the PyTorch framework. (Further details are provided in the supplementary paper.)\nPrompt Selection: The selection of prompts significantly influences model performance by guiding task handling. After detailed experimentation with multiple prompts, we came up with the most impactful prompt that leverages LLMs to generate meta-analysis accurately. Table IV shows the impact of prompts on relevancy. A comparison between the two prompts is shown there. Prompt 1 demonstrated superior effectiveness over Prompt 2 in generating meta-analysis abstracts, achieving a high relevancy rate with every LLM.\nEvaluation metrics: For evaluating the generated texts from LLMs, Bilingual Evaluation Understudy (BLEU) [45], that quantifies the resemblance between generated and reference texts and Recall-Oriented Understudy for Gisting Evaluation (ROUGE) [46], that assesses how much information from reference summaries is captured. are used. For the abstract generated by fine-tuned models, the cosine similarity [47] metric is used to quantify the similarity between two vectors after combining fine-tuning with RAG.\nHuman evaluation: After generating responses with LLMs, we conduct a human evaluation process to ensure alignment with human judgment. Human judges categorize the generated text as relevant, somewhat-relevant, or irrelevant, following the criteria from [32]. Relevant responses closely resemble the ground truth, showing high similarity and inclusion of crucial information. Somewhat-relevant responses have acceptable similarity, containing valuable information within an acceptable margin. Irrelevant responses lack important information or include unrelated content. This classification framework ensures a rigorous assessment of generated meta-analysis abstracts against expected standards. Three independent evaluators assessed each model's response, with majority voting used to determine the final decision. Each evaluator worked independently, without access to others' assessments. In total, 13 evaluators were involved in evaluating all the model responses. To reduce bias, the evaluations were conducted by university students rather than the authors. Demographic details of the evaluators are provided in Table I. For further details on the evaluation process, refer to the Supplementary Material."}, {"title": "B. Results and Analysis", "content": "We present a detailed overview of our experimental evaluations, focusing on how context-length restricted LLMs perform in generating meta-analysis with lengthy inputs. Notably, previous research has not utilized large context datasets for meta-analysis, making our study unique. For comparison, we also used a short context dataset to evaluate the models' performance, as shown in Table II. Considering the architecture of our models, the benchmark performance is reliable.\nAfter fine-tuning the LLMs, human evaluation of the generated outputs is essential. We applied our proposed human evaluation metrics-Relevant, Somewhat-Relevant, and Irrelevant to assess the results of the meta-analysis generation task. As shown in Table III, our approach of fine-tuning LLMs with large context dataset, MAD outperforms other methods, producing more relevant meta-analysis content and reducing unnecessary context generation.\nThe non-fine-tuned Llama-2 (7B) model performs better than the non-fine-tuned Mistral-v0.1 (7B) model in generating relevant and somewhat relevant meta-analysis abstracts. After fine-tuning, the rate of irrelevant content generation significantly decreases, resulting in a highly effective meta-analysis abstract generation. Table III also highlights the alignment between machine-generated and human-generated texts, which is referred by SWGT. The integration of RAG has shown promising outcomes in terms of generating relevant meta-analyses. Table V provides two instances of our method's creation of meta-analysis abstracts, demonstrating their encouraging resemblance to the abstracts of meta-articles. This validates the dependability of our method.\nOur observation includes (1) fine-tuning with a large context scientific dataset, MAD, letting LLMs learn the patterns for generating meta-analysis content with higher relevancy. This proves the reliability of our approach to handling big data management challenges. (2) BLEU and ROUGE scores are utilized to compare relevant and irrelevant human-evaluated contexts, where a generated text is considered irrelevant if it contains less than 10% context translation using large meta-papers' input (represented by BLEU). (3) Fine-tuned models exhibit improved performance over base models, indicating more significant agreement between the generated abstract in the RAG approach and the real meta-analysis abstract. It highlights how well the fine-tuning approach works to help models find the patterns required to generate high-quality meta-analysis abstracts."}, {"title": "C. Ablation Study", "content": "We perform ablation studies focusing on three crucial areas: prompt variant analysis, temperature variation, and the impact of our proposed loss metric on fine-tuned models. These studies provide deeper insights into the performance factors for meta-analysis generation.\nPrompt Variant Analysis: Prompt selection is fundamental in steering the meta-analysis generation process. In Table IV, we compare the effectiveness of two distinct prompts. We evaluated the relevancy and quality of meta-analysis abstracts produced by Llama-2 (7B) and Mistral-v0.1 (7B) across both prompts. Our results show that Prompt 1 consistently outperforms Prompt 2 in terms of relevancy, generating more accurate and precise meta-analysis abstracts. Specifically, Prompt 1 achieved higher relevancy scores across all versions of Llama-2 and Mistral, with fewer instances of irrelevant content. Given these results, Prompt 1 was used in all subsequent experiments.\nVarying Temperature: The temperature parameter controls the randomness of predictions, influencing the balance between exploration and exploitation during the generation process. We explored the impact of different temperatures (0.1, 0.5, and 0.7) on summary quality. As shown in Fig 4(a), a temperature setting of 0.7 provided the best results across various evaluation metrics, including BLEU, ROUGE-1, ROUGE-2, and ROUGE-L. The higher temperature yielded more diverse outputs without sacrificing relevancy or quality, making it the optimal setting for our meta-analysis generation tasks.\nImpact of Our Loss Metric: We implemented a specialized loss function, the ICD, designed to enhance the performance of meta-analysis summarization tasks. Fig 4(b) compares the performance of models fine-tuned with ICD against models using a standard loss function across both Llama-2 FT and Mistral-v0.1 FT versions. ICD emphasizes the directional similarity between the generated outputs and ground truth vectors by utilizing cosine similarity, capturing nuanced semantic details. This metric outperformed the standard loss function, improving the alignment between the generated summaries and their reference summaries. The ICD's ability to capture subtle semantic nuances beyond simple word matching proved crucial in fine-tuning the models for more accurate and coherent meta-analysis generation."}, {"title": "D. Discussion", "content": "This study represents insights into generating meta-analysis leveraging LLMs using a large-context scientific dataset, MAD. The result section provides evidence of our fine-tuned models' performance, showing the successive relevancy rate for generating meta-analysis. It was observed that the fine-tuned models for Llama-2 (7B) and Mistral-v0.1 (7B) outperformed their non-fine-tuned versions by generating significantly relevant meta-analyses. As expected, integrating RAG with fine-tuned models allows them to generate highly aligned meta-analyses.\nLimitations: One key limitation of this study is the maximum context length of the LLMs, which required chunking the input data. To mitigate potential information loss, overlapping context techniques and RAG were employed. However, due to hardware constraints, the model's evaluation was performed on only 50% of the test sets, which proved resource-intensive. Additionally, training the models in a highly quantized configuration limited the fine-tuning potential, impacting the ability to fully optimize the model's parameters for better performance."}, {"title": "V. CONCLUSION", "content": "This study demonstrates the effectiveness of automating meta-analysis generation using fine-tuned LLMs on extensive scientific datasets. Our approach significantly improved the relevance of generated meta-analysis abstracts, achieving 87.6% relevance and reducing irrelevance from 4.56% to 1.9%, demonstrating its potential and highlighting further promising research opportunities in automating scientific synthesis. We introduced novel methods to address the challenges posed by limited context length and resource constraints, including using ICD as a tailored loss metric for training. Integrating RAG further optimized the process by ensuring efficient synthesis of research findings without sacrificing context. Human evaluation confirmed the improvements in model performance, particularly in maintaining the relevancy and accuracy of structured meta-analysis content.\nFuture works: While this study achieved notable improvements in meta-analysis generation, future research should focus on expanding the dataset in various fields that need meta-analysis and refining the model's ability to generate even more accurate and reliable outputs, particularly in resource-constrained environments. Further optimizations to LLM fine-tuning and scaling could lead to broader applicability in automating complex scientific analysis."}, {"title": "ETHICS STATEMENT", "content": "This study was conducted with a strong commitment to ethical integrity, particularly in the generation and evaluation of meta-analysis abstracts in the scientific field using LLMs. We engaged 13 human evaluators from diverse backgrounds, ensuring their participation was voluntary and informed. We carefully collected only essential information to assess their qualifications for the task, and any data that could potentially identify participants were securely deleted after the evaluation was completed. We took significant measures to protect the well-being of all participants, ensuring that the evaluation process posed no physical or psychological risk. Recognizing that even subtle biases or inaccuracies in scientific research can have serious consequences, we implemented rigorous protocols to ensure that all generated content adhered to the highest ethical standards. Our approach was designed to avoid any language or conclusions that could perpetuate harm or inequity based on race, gender, or other social determinants of health. By adhering to these principles, we have ensured that our research upholds the highest ethical standards, fostering a safe and respectful environment for both human participants and the broader community."}]}