{"title": "Ontology-grounded Automatic Knowledge Graph Construction by LLM under Wikidata schema", "authors": ["Xiaohan Feng", "Xixin Wu", "Helen Meng"], "abstract": "We propose an ontology-grounded approach to Knowledge Graph (KG) construction using Large Language Models (LLMs) on a knowledge base. An ontology is authored by generating Competency Questions (CQ) on knowledge base to discover knowledge scope, extracting relations from CQs, and attempt to replace equivalent relations by their counterpart in Wikidata. To ensure consistency and interpretability in the resulting KG, we ground generation of KG with the authored ontology based on extracted relations. Evaluation on benchmark datasets demonstrates competitive performance in knowledge graph construction task. Our work presents a promising direction for scalable KG construction pipeline with minimal human intervention, that yields high quality and human-interpretable KGs, which are interoperable with Wikidata semantics for potential knowledge base expansion.", "sections": [{"title": "1. Introduction", "content": "Knowledge Graphs (KGs) are structured representations of information that capture entities and their relationships in a graph format. By organizing knowledge in a machine-readable way, KGs enable a wide range of intelligent applications, such as semantic search, question answering, recommendation systems, and decision support [1]. The ability to construct high-quality, comprehensive KGs is thus critical for harnessing the power of these technologies across various domains.\nTraditionally, the process of constructing KGs has relied heavily on manual effort by domain experts to define the relevant entities and relationships, populate the graph with valid facts, and ensure logical consistency [2]. However, this manual curation approach is time-consuming, expensive, and difficult to scale to large, evolving domains. There is a strong need for (semi-)automatic methods that can aid the KG construction process by extracting structured knowledge from unstructured data sources such as text.\nRecent years have seen growing interest in leveraging Large Language Models (LLMs) for various knowledge capture and reasoning tasks [3]. Pre-trained on vast amounts of text data, LLMs can generate fluent natural language and have been shown to memorize and recall factual knowledge [4], [5]. However, directly applying LLMs to KG construction still faces several challenges. First, LLMs may generate inconsistent or redundant facts due to the lack of an explicit, unified schema [6]. Second, the generated KGs may be incomplete or biased towards the knowledge present in the LLM's training data, which may not fully cover the target domain, especially for proprietary documents not included in pre-training set. Finally, it can be challenging to integrate LLM-generated KGs with existing knowledge bases due to misalignment with standard ontologies.\nIn this work, we propose a novel approach that harnesses the reasoning power of LLMs and the structured schema of Wikidata to construct high-quality KGs for proprietary knowledge domains. Our approach begins by discovering the scope of knowledge through the generation of Competency Questions (CQ) and answers from unstructured documents. We then summarize the relations and properties from these QA pairs into an ontology, matching candidate properties against those defined in Wikidata and extending the schema as needed. Finally, we use the resulting ontology to ground the transformation of CQ-answer pairs into a structured KG. By incorporating the Wikidata schema into our pipeline and grounding generation of KG on the same ontology, we aim to reduce redundancy, leverage the implicit knowledge captured during LLM pretraining while improving interpretability, and ensure interoperability with public knowledge bases. The generated KGs could be parsed with RDF parsers and used in downstream applications, or audited for correctness.\nThe main contributions of this work are as follows:\n1. We propose a novel ontology-grounded approach to LLM-based KG construction that leverages ontology based on Wikidata schema to guide the extraction and integration of knowledge from unstructured text.\n2. We introduce a pipeline that combines competency question generation, ontology alignment, and KG grounding to systematically construct high-quality KGs that are consistent, complete, and interoperable with existing knowledge bases.\n3. We demonstrate the effectiveness of our approach through experiments on benchmark datasets, showing improvements in KG quality compared to traditional methods alongside with interpretability and utility of generated KGs."}, {"title": "2. Literature Review", "content": "Knowledge graph construction has been an active area of research in recent years, with a wide range of approaches proposed for extracting structured knowledge from unstructured data sources [2]. Early methods relied heavily on rule-based systems and hand-crafted features to identify entities and relations in text [7]. With the advent of deep learning, neural network-based approaches have become increasingly popular, enabling more flexible and scalable KG construction [8]."}, {"title": "3. Method: Ontology-grounded KG Construction", "content": "Our proposed approach for ontology-grounded KG construction using LLMs consists of four main stages: 1) Competency Question Generation, 2) Relation Extraction and Ontology Matching, 3) Ontology Formatting, and 4) KG Construction. Figure 1 provides an overview of the pipeline."}, {"title": "3.1. Competency Question (CQ)-Answer Generation", "content": "The first step in our pipeline is to generate a set of competency questions (CQs) and answers that capture the key information needs of the target domain. We employ an LLM to generate CQs based on the input documents. The LLM is provided with a set of instructions and examples to guide the generation process, encouraging the creation of well-formed, relevant questions that can be answered using the given documents. This step helps to scope the KG construction task within the knowledge domain, and ensure that the resulting KG aligns with the intended use cases. This also allows further ontology expansion by incorporating user-submitted domain-defining questions when interacting with the knowledge base, which serves as a user friendly interface of refining ontology by submitting new CQs and use our proposed pipeline to attach the incremental knowledge scope to existing ontology."}, {"title": "3.2. Relation Extraction and Ontology Matching", "content": "During our preliminary experiments of prompting LLMs to directly generate ontology on documents, we noted that the LLM spontaneously recalled Wikidata knowledge in response, consistent with previous works [15]. In our preliminary experiments, this behaviour also transfers to small 7B/14B models.\nFollowing this direction, in the second step we extract relations from CQs and match them against Wikidata properties to better elicit model memories on Wikidata when constructing and using ontology. We first prompt LLMs to extract properties from CQ and write brief description on usage of extracted properties, including their domain and range, following editing guidelines of Wikidata. To match these properties against existing entries in Wikidata ontology, we pre-populate a candidate property list with all Wikidata properties after filtering out properties related to external database/knowledge base IDs. These extracted properties are then matched against the candidate list by a vector similarity search between description of properties. The representation for property is sentence embedding constructed from description of properties, and the top 1 closest candidate is retrieved for each extracted property. This matching result between each pair of extracted property and matched top 1 candidate is then vetted by LLM to see if they are really semantically similar as a final deduplication step. If a match is validated, the candidate property is added to the final property list; otherwise, the newly minted property is kept in the final list if we allow expansion from the candidate property list derived from Wikidata, and discarded when the final property list is required to be a subset of candidate property list. The first scenario is suitable for cases when no prior schema is known for the domain and some new properties outside of common ontology are expected, whereas the latter is for a known target list of possible properties."}, {"title": "3.3. Ontology Formatting", "content": "In the third stage, we use LLM to generate an OWL ontology based on the matched and newly created properties. We copy the description, domain and range field from all properties under Wikidata semantics. For new properties, LLM is prompted to infer and summarize classes for the domain and range of the relations to output a complete OWL ontology, following the format of copied Wikidata properties. This step ensures that the resulting KG is grounded in a formal, machine-readable ontology that captures relationships between entities, and close to the semantics of Wikidata for interoperability."}, {"title": "3.4. KG Construction", "content": "In the final stage, we use the LLM to construct a KG based on the CQs and related answers grounded by the generated ontology in the previous stage. For each (CQ, answer) pair, LLM extracts relevant entities and maps them to the ontology using the defined properties. The output is a set of RDF triples that constitutes the final KG."}, {"title": "4. Experiments and Discussion", "content": "We evaluate our ontology-grounded approach to KG construction (KGC) on three datasets for KGC datasets: Wiki-NRE [16], SciERC [17], and WebNLG [18]. As Wiki-NRE and WebNLG are partially based on Wikidata and DBpedia (derived from Wikipedia contents), and in our proposed pipeline, Wikidata schema is utilized, we include SciERC for a more robust evaluation, since SciERC contains relation types that are not equivalent by nature to properties in Wikidata.\nWe used a subset Wiki-NRE's test dataset containing 1,000 samples with 45 relation types following the split in [19], due to cost constraints. SciERC's test set contain 974 samples under a schema with 7 relation types. For WebNLG, we used test set in Semantic Parsing (SP) task, with 1,165 samples and 159 relation types. For evaluation, we adopt partial F1 on KG triplets based on standards in [18]. All experiments are conducted for one-pass\nWe note in the previous reports that annotation in KGC reports may be incomplete in terms of both possible relation types and KG triplets [19], [20].\nAs our pipeline is designed to autonomously uncover knowledge structure with no prior assumption on knowledge schema, we report our result in two ways, corresponding to the two configurations of final de-duplication step in Section 3.2:\n1. Target schema constrained: In this setting, we match all relation types in test sets to its closest equivalent in Wikidata and constrict ontology to the relation universe in test set.\n2. No schema constraint: In this setting, we do not filter matched ontology, even if they are not in schema of test dataset. This setting is close to real-life applications when processing documents with unknown schema.\nFor property conjunction, evaluate for, compare, feature of in SciERC, we select the closest properties proposed by LLM based on our subjective opinion.\nTo highlight our system's competency, rather than directly prompting triplets, we parse output KG with RDF parser and extract all valid RDF triples for KG related to each document in test set, and present triplets to evaluation script for assessment. This ensures that our evaluation is on the generated KG ready to be consumed in downstream application.\nWe test our pipeline on both Mistral-7B-instruct [21] and GPT-40\u00b9. Due to cost constraints, we have only tested GPT-40 on target schema constrained setting. For embedding property usage comment, we select bge-small-en [22]. We use GenIE [23], PL-Marker [24], and ReGen [25] as fine-tuned baseline for Wiki-NRE, SciERC, and WebNLG dataset, respectively (collectively named Non-LLM Baseline). For LLM-based systems, we use results reported in [19] for Wiki-NRE and WebNLG on the same Mistral model, and GPT-4 results in [3] for SciERC. (collectively named LLM Baseline). We note that it is highly unlikely that Mistral-7B poses an advantage over an earlier version of GPT-4, when interpreting result of SciERC."}, {"title": "4.2. Result", "content": ""}, {"title": "4.3. Discussion", "content": ""}, {"title": "4.3.1. Performance discrepancy on different grounding ontology", "content": "It is worth noting that the relatively lower performance on no schema constraint setting across all datasets is due to the fact that the LLM discovers a richer ontology than the predefined target schema. While this expanded schema may capture additional relevant information, it can hinder extraction performance when evaluated solely against the limited target schema. This showcases the trade-off between schema completeness and strict adherence to a predefined ontology, and our pipeline performs best on a large set of documents with a limited scope of knowledge, requiring a concise schema.\nFurthermore, the flipside of performance deficit in an absence of schema constraints, i.e. additional ontology entries outside of dataset-defined properties, cannot be evaluated against the dataset directly, as the ontology is not entirely covered by test set annotations. Hence, the virtue of no schema constraint setting is to demonstrate that our pipeline can indeed provide a coverage of the properties in test set, though somewhat limited compared to baselines, when also capturing ontology outside test set schema, which is potentially more useful when discovering ontology on a novel document set with no expert knowledge in its schema conposition. This ability may be validated by manual evaluation on the full set of captured ontology in a future work down the line.\nNevertheless, the marginal performance deficit leaves room for improvement. Recent reports explored that long input context may pose challenge to LLMs even if such long context length is technically supported [26]. We conjecture that aside from trimming grounding ontology, which hinders the knowledge coverage of our pipeline, few-shot fine-tuning on the new ontology or general pretraining in KG construction task may be helpful. We leave these as possible future directions."}, {"title": "4.3.2. Utility of generated KG", "content": "It should be emphasized that, while the selected evaluation tasks evaluate the correctness of extracted triplets, the extracted knowledge graph can do more than that. With ongoing discussion related to grounding LLM knowledge on trusted knowledge sources to reduce hallucination [6], explicitly generating KG provides a path to audit knowledge elicited when interacting with LLM, and with evidence demonstrating that LLM has the potential to reason on graph and generate an explicit path to retrieve required knowledge [27], our pipeline may serve as a foundation for an interpretable QA system, where an LLM autonomously extracts ontology and deduces correct retrieval query based on the ontology when handling a set of unstructured document. The interpretability arise from the fact that KG and query could be understood and verified by users. Moreover, our usage of Wikidata schema offers potential interoperability with the whole Wikidata knowledge base, which safely expands the knowledge scope of QA system. We propose to continue research on this significant direction."}, {"title": "4.3.3. Computational resources", "content": "We note the growing concern of sustainability in LLM applications due to intensive requirement on computational resources. This pipeline consumes three separate LLM calls per document, plus one call per extracted relation. It is not straight forward to compare the carbon footprint of our approach compared to Non-LLM baselines, as our work at this stage does not require model fine-tuning, whereas all of the Non-LLM baselines employed various tuning techniques when producing the result. On the other hand, our smallest model adopted, Mistral-7B, is more than 10x larger in terms of parameter size compared to T5 models used in Non-LLM baselines. Larger models naturally require more powerful GPU clusters in terms of both GPU quantity and capability, but our zero-shot approach may provide an advantage in terms of resource cost compared to Non-LLM baselines when processing a small number of documents with no training requirement.\nWhen comparing with LLM baselines, we note that the approach by [14], [3] consumes 1 and 2 LLM calls per document, respectively. However, we note that these baselines treat knowledge triplet as evaluation target, while we generate a formatted ontology at the end, which is more useful. Nevertheless, we recognize the performance burden and propose to explore techniques in fine-tuning and guided decoding to achieve better performance with smaller model and better reproducibility."}, {"title": "5. Conclusion", "content": "We have demonstrated the effectiveness of our ontology-grounded approach to KG construction using LLMs. By leveraging the structured knowledge in Wikidata, pretrained on LLM, and grounding KG construction with generated ontology, our pipeline is able to construct high-quality KGs across various domains while maintaining competitive performance with state-of-the-art baselines. Generated KGs that are conformant with Wikidata schema leaves possibly wide open, of building an interpretable QA system that has robust access to both common knowledge and proprietary knowledge base."}, {"title": "A. Sample generated KG", "content": "This KG was generated under no schema constraint setting for this document: Mohammad Firouzi (Born 1958 Tehran ) is a prolific Iranian musician, whose primary instrument is the barbat.\n<Prefixes and definition of dependencies omitted >\nwd: Mohammad_Firouzi a wd: human ;\nrdfs:label \"Mohammad Firouzi\"@en ;\nwdt: occupation wd: Musician;\nwdt: CountryOfCitizenship wd: Iran;\nwdt: Place Of Birth wd: Tehran;\nwdt: Date Of Birth \"1958\" ^^ xsd: date\nNote that in official annotation, only triplets related to place of birth and nationality exist, hence the evaluation will be penalized with low precision."}, {"title": "B. Preprocessing of Wikidata schema", "content": "To save space in LLM input context and mitigate performance drop on selected target schema when ontology is large, we only include commonly used properties by restricting data type on item, quantity, string, monolingual text, point in time.\u00b2 To align with common pretraining objectives of LLM, we substitute entity identifiers (e.g. P19) with its literal label (rdfs:label in PascalCase (e.g. PlaceOfBirth)."}, {"title": "C. Prompts", "content": "All prompts are reused across all datasets."}, {"title": "C.1. CQ generation", "content": "We prompt LLM to generate up to 3 CQs per document for efficiency considering nature of test datasets, but note that this may be adjusted.\nWrite competency questions based on the abstract level concepts\nin the document. Write questions that can be answered using\nthe document only.\nWrite up to 3 questions per document.\nBelow are the examples and follow the same format when\n####\ngenerating competency questions:\nDocument: Douglas Noel Adams (11 March 1952\n11 May 2001) was\nan English author, humourist, and screenwriter, best known\nfor The Hitchhiker's Guide to the Galaxy (HHGTTG).\nOriginally a 1978 BBC radio comedy, The Hitchhiker's Guide\nto the Galaxy developed into a \"trilogy\" of five books that\nsold more than 15 million copies in his lifetime. It was\nfurther developed into a television series, several stage\nplays, comics, a video game, and a 2005 feature film. Adams'\nS contribution to UK radio is commemorated in The Radio\nAcademy's Hall of Fame.\n####\nQuestions:\nCQ1. What is the date of birth of Douglas Noel Adams?\nCQ2. What is the date of death of Douglas Noel Adams?\nCQ3. What is the occupation of Douglas Noel Adams?\nCQ4. What is the country of citizenship of Douglas Noel Adams?\nCQ5. What is the most notable work of Douglas Noel Adams?\nCQ6. What is the original medium of The Hitchhiker's Guide to\nthe Galaxy?\nCQ7. In what year was The Hitchhiker's Guide to the Galaxy\noriginally broadcast?\nCQ8. How many books are in The Hitchhiker's Guide to the Galaxy\n\"trilogy\"?\nCQ9. What other media adaptations were created based on The\n####\nHitchhiker's Guide to the Galaxy?"}, {"title": "C.2. CQ answering", "content": "Use the provided document to answer user query. If you don't\nknow the answer, just say that you don't know, don't try to\nmake up an answer.\nPassage: {doc}\nQuery: {query}"}, {"title": "C.3. Relation extraction", "content": "You are an assistant in building a knowledge graph. Analyze the\nfollowing competency questions and identify all\nrelationships and concepts concepts mentioned in the\nquestion.\nExtract relation first, then describe the usage of each\nrelation based on your understanding given the context of\ncompetency questions.\nAfterwards, extract all relation-related concepts.\nYou should only extract properties between entities and\nliterals, not entities themselves, or classes of entities.\nTherefore, not all CQs contain valid properties.\nIf you don't know the answer, just say that you don't know, don\n't try to make up an answer.\nMerge all relations into one list and all concepts into one\nlist.\nDo not reply using a complete sentence, and only give the\nanswer in the following format.\nBelow are the examples and follow the same format to extract\nthe relations:\n####\nDocument: Douglas Noel Adams (11 March 1952\n11 May 2001) was\nan English author, humourist, and screenwriter, best known\nfor The Hitchhiker's Guide to the Galaxy (HHGTTG).\nOriginally a 1978 BBC radio comedy, The Hitchhiker's Guide\nto the Galaxy developed into a \"trilogy\" of five books that"}, {"title": "C.4. Ontology matching", "content": "Decide if the two properties are semantically similar in an\nontology.\nYou should say yes if you decide that these propties are\nsimilar, or if they are inverse properties.\nAnswer in \"yes\" or \"no\" only.\nProperty 1: {pl}\nProperty 2: {p2}"}, {"title": "C.5. Ontology formatting", "content": "For properties under Wikidata schema, we retrieve schema:description, rdfs:domain, rdfs:range\nfor each property and include it in resulting ontology. Otherwise LLM is prompted to author\nontology as so:\nUse the relations (properties) and their usage comments to\nbuild\nan ontology in RDF format.\nIf you don't know the answer, just say that you don't know, don\n't try to make up an answer.\nDon't provide anything other than an ontology in RDF format.\nInfer and summarize classes for domain and range of the\nrelations across the concepts provided, and add these\nclasses to relations only if required for clousre of\nrelations.\nFor each relation, add relevant ontology entry for it.\nAdd rdfs:comment based on the usage comments.\nUse wdt: namespace for all relations discovered. Use entities\nunder these prefixes if necessary:\n@prefix rdf: <http://www.w3.org/1999/02/22 \u2013 rdf-syntax-ns#>\n@prefix xsd: <http://www.w3.org/2001/XMLSchema#>\n@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n@prefix owl: <http://www.w3.org/2002/07/owl#>\n@prefix wikibase: <http://wikiba.se/ontology#>\n@prefix schema: <http://schema.org/> .\n@prefix wd: <http://www.wikidata.org/entity/> .\n@prefix wdt: <http://www.wikidata.org/prop/direct/>\nUse turtle syntax."}, {"title": "C.6. KG generation", "content": "Your task is to construct a knowledge graph based on the\nprovided ontology.\nFocus on understanding relationships from the question answer\npair and document,\nand extract related entities, then mapping them to the ontology\nusing the properties defined in the ontology.\nDo not include new properties other than those in ontology.\nOnly use those properties in the ontology.\nOutput in turtle format following the ontology provided.\nYou should only include knowledge in question answer pairs and\nthe document.\nDo not make up answers.\nUse this ontology based on Wikidata as the starting point:\n{ont}"}]}