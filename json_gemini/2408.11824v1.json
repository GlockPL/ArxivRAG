{"title": "AppAgent v2: Advanced Agent for Flexible Mobile Interactions", "authors": ["Yanda Li", "Chi Zhang", "Wanqi Yang", "Bin Fu", "Pei Cheng", "Xin Chen", "Ling Chen", "Yunchao Wei"], "abstract": "With the advancement of Multimodal Large Language Models (MLLM), LLM-driven visual agents are increasingly impacting software interfaces, particularly those with graphical user interfaces. This work introduces a novel LLM-based multimodal agent framework for mobile devices. This framework, capable of navigating mobile devices, emulates human-like interactions. Our agent constructs a flexible action space that enhances adaptability across various applications including parser, text and vision descriptions. The agent operates through two main phases: exploration and deployment. During the exploration phase, functionalities of user interface elements are documented either through agent-driven or manual explorations into a customized structured knowledge base. In the deployment phase, RAG technology enables efficient retrieval and update from this knowledge base, thereby empowering the agent to perform tasks effectively and accurately. This includes performing complex, multi-step operations across various applications, thereby demonstrating the framework's adaptability and precision in handling customized task workflows. Our experimental results across various benchmarks demonstrate the framework's superior performance, confirming its effectiveness in real-world scenarios. Our code will be open source soon.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) like ChatGPT (OpenAI, 2023) and GPT-4 (OpenAI, 2023) have greatly advanced natural language processing, enabling their integration into intelligent agents that revolutionize autonomous decision-making. These agents (Schick et al., 2024; Qin et al., 2023b), initially tailored for text-based interactions, exhibit advanced human-like features, including adaptive memories that enhance their environmental engagements and processing capabilities across diverse NLP tasks.\nHowever, real-world applications often require beyond textual processing, necessitating the integration of visual data and other modalities. This requirement exposes shortcomings in traditional text-only agents and highlights the urgent need for advanced multimodal systems. These systems (Gao et al., 2023; Sur\u00eds et al., 2023; Wu et al., 2023a) are critical in complex environments like mobile and operating system platforms where they need to perform multi-step reasoning, extract and integrate information, and respond adaptively to user inputs. Innovative solutions such as the AppAgent (Yang et al., 2023b) and MobileAgent (Wang et al., 2024) have shown promise by enabling more natural interactions with smartphone applications through human-like interactions.\nDespite these advancements, accurately recognizing graphical user interfaces (GUIs) remains a key challenge, impacting the decision-making accuracy of multimodal agents. Previous methods (Liu et al., 2024; Wang et al., 2024) relying on visual features often face inaccuracies due to limitations in recognition models. Additionally, the dynamic nature of mobile environments, which frequently introduce new features, poses further challenges. Even sophisticated models like GPT-4, while proficient with well-known apps, struggle with lesser-known apps due to unfamiliar visual elements. The rapid updates in app interfaces and functionalities further hinder these models' effectiveness across diverse applications.\nTo address this challenge, AppAgent (Yang et al., 2023b) adopts a human-like approach by automated exploration and watching demos. This strategy allows the agent to store UI element descriptions in a document rather than relying on rigid memorization, thus enhancing decision-making by leveraging contextual understanding. However, Ap-"}, {"title": "Method", "content": "In this section, we provide a detailed description of our multimodal agent framework as Figure 1, which is structured into two primary phases: exploration and deployment. At each round, the agent analyzes the current GUI with task requirements, generating observations, thoughts, actions, and summaries. The summary, serving as memory, is carried over to the next execution prompt, ensuring continuity throughout the task execution process."}, {"title": "Agent Framework", "content": "Our multimodal agent framework is implemented on the Android 15 environment using the Android Studio emulator. The agent interacts with the mobile phone by invoking commands through the AndroidController. This interaction process is based on analyzing the current GUI interface's structured data parsing information, combined with OCR and detection models to extract detailed information from screenshots. The data extracted includes Android ID, numerical labels marked on the screenshots, features of the elements, texts, and the coordinates of the UI elements. This setup allows the agent to perform efficiently within a dynamic mobile environment, integrating advanced recognition capabilities with intelligent decision-making"}, {"title": "Agent Interactions", "content": "During both the exploration and execution phases, the agent interacts with the mobile phone, translating human commands or outputs from LLMs into instructions that the Android system can recognize and execute. We detail these commands as follows:\nTapButton: Initiates tap action on user interface element. This can be specified either by entering the element's number identifier in the screenshot or by describing its visual features.\nText: Simulates typing by entering a string of text into the designated area.\nLongPress: Applies a prolonged press on a specified element area.\nSwipe: Executes a swipe action in a specified direction on an element. This can be used for scrolling pages vertically or horizontally.\nBack: Simulates the device's back button to return to the previous UI state.\nHome: Commands the agent to return to the main screen. This is crucial for agent to re-execute the tasks and cross-apps tasks.\nWait: Pauses the operation to allow the system to catch up, refresh the screen snapshot.\nStop: Signals the completion of tasks and ends the current operation.\nOnce these commands are transformed into corresponding instructions, they are executed by the Android system through the AndroidController. This setup ensures precise command execution, allowing the agent to perform tasks efficiently within the Android environment. More details about action space are displayed in Appendix."}, {"title": "Exploration Phase", "content": "The exploration phase is aimed at analyzing the GUI in relation to the current task. It involves identifying and documenting the functions of UI elements through two alternative methods: agent-driven and manual exploration. All prompts used are displayed in Appendix."}, {"title": "Agent-Driven exploration", "content": "This method starts with the agent analyzing the current UI interface to identify elements requiring interaction and to determine the specific actions needed. Once these elements and actions are pinpointed, the agent executes the planned actions. Following the execution of action, the agent takes screenshots before and after the interaction to compare and analyze the changes. This comparison allows the agent to record the operational functions of the UI elements and assess the effectiveness of each action taken.\nAfterwards, the agent enters reflection phase. If the agent determines that the executed action is completely irrelevant to the task, it performs a return operation. The irrelevant action is recorded in a useless_list and is fed back into the LLM. If the results of the actions align with the intended user"}, {"title": "Manual Exploration", "content": "This method is introduced to overcome the limitations encountered during agent-driven exploration, such as the LLM's erroneous judgments due to its incomplete understanding of certain apps and UI elements. Manual exploration allow GPT-4 to observe manual operations, compare screenshots before-and-after operations similar to agent-driven, gaining a clearer understanding of new UI elements and task workflows. The exploration is enhanced with advanced OCR and detection models, providing comprehensive UI analysis based on human interactions. Humans guide the sequence of actions and conclude the process, thereby streamlining the operational workflow and accelerating the learning process. Importantly, just like in automatic exploration, the information regarding UI elements and their functionalities observed during manual exploration is meticulously documented. This manual exploration ensures that the agent can overcome shortcomings of the automated processes by incorporating sophisticated understanding and adjustments that only human insight can provide."}, {"title": "Development Phase", "content": "During the deployment phase, the agent can utilizes the knowledge acquired to perform user tasks effectively. Initially, the agent fetches the current GUI information and traverses the elements using Self-query retriever for document retrieval. The self-query retriever converts document content into embeddings, stored in a vector store, from which it retrieves the most pertinent document based on resource IDs or OCR-derived information.\nThe agent then integrates this document into the prompt for agent, analyzing the current GUI screenshot, document content, and specific task requirements to make informed decisions and execute actions based on the positional information of UI elements. Alternatively, the agent can also operate without loading the document, directly handling the majority of common tasks effectively. After each action, the agent updates its prompts with historical information and action outcomes, thereby enhancing its memory and improving decision-making for subsequent steps.\nThe process continues until the agent determines that the task has been completed, at which point it exits the current process and reports task completion. This structured approach ensures that actions are executed precisely and efficiently, leveraging the detailed knowledge base created during the exploration phase to optimize performance and user satisfaction."}, {"title": "Document Generation", "content": "This document serves as a specialized knowledge base, meticulously designed to store comprehensive information about UI elements collected during the exploration phase. The database includes various data for each UI element such as Android ID, visible labels, text content, visual features (e.g., color and shape), screen coordinates, and functionalities as interpreted by GPT-4.\nTo enhance accessibility and utility, we have developed a novel structured storage format suitable for managing diverse element types. This format not only facilitates organized data retrieval but also supports dynamic updates based on real-time interactions during the deployment phase. As the agent operates across various applications, it actively updates the document in response to new UI elements and adapts its strategies accordingly.\nThis dynamic updating mechanism ensures that the agent remains adaptable and efficient, capable of adjusting its actions based on user requirements and contextual changes. The continual enhancement of the document significantly improves the agent's understanding and manipulation of application interfaces, leading to more accurate and contextually appropriate interactions. Meanwhile, markedly enhances the user experience and operational efficiency of the agent."}, {"title": "Advanced Features", "content": "This subsection highlights the key functionalities that enhance our multimodal agent framework, focusing on visual feature decision-making, safety checks, and cross-app task management. These features collectively improve the agent's safety, versatility, and efficiency, ensuring robust performance in complex and dynamic environments."}, {"title": "Visual Features Decision-Making", "content": "When the agent confronts scenarios where the desired interactive element is not numerically tagged, and other numerically tagged elements are ineffective for task completion, it automatically transitions to an alternative visual feature UI layout. This process leverages advanced OCR technology (Liao et al., 2020) and detection models (Liu et al., 2023b) to accurately recognize and annotate text and icons within the interface. By numerically annotating these elements using established methodologies, the agent is equipped to make informed decisions based on the newly adapted UI screenshot. This capability is crucial for handling icons in previously unknown scenarios, ensuring that the agent can navigate and interact with various UI elements effectively, regardless of prior exposure. This dynamic decision-making process significantly enhances the agent's ability to adapt to new environments and execute tasks with higher precision and reliability."}, {"title": "Safety Check", "content": "In modern LLMs and agent systems, safety is crucial, particularly in automated processes that can lead to privacy breaches. To tackle this issue, we implemented a safety check during the deployment phase. The agent reviews the current UI screenshot, and if the next steps involve sensitive actions like account passwords, payment or other privacy-related concerns, it will switch to manual mode so the user can handle these operations personally. For privacy, the agent will not retain any information from this process. Once the user completes the sensitive task and inputs \"finish,\" the agent will automatically continue with the deployment phase and carry on with the task until it's completed. The safety check offers several key advantages. It ensures that sensitive tasks remain secure by involving human judgment and minimizes the risk of data leakage. Furthermore, it increases user trust in the system, providing assurance that private information is handled carefully, while still enabling the agent to effectively complete its assigned tasks."}, {"title": "Cross-Apps Task", "content": "In addition to its core functionalities, our framework is capable of handling complex tasks that span multiple applications. This ability allows the agent to perform tasks that require interactions across different interfaces. When engaging in such cross-app tasks, the agent evaluates its progress based"}, {"title": "Experiments", "content": "In this section, we will conduct a comprehensive evaluation with our agent framework. The experiments were conducted on the Android platform to maintain consistency and simplify validation. We utilized the Android Studio emulator for the experiments, which included comprehensive testing on the public benchmarks and qualitative results. This dual approach allowed us to benchmark our agent against standardized criteria while also gaining deeper insights into its real-world performance on mobile applications and environments."}, {"title": "Quantitative Results", "content": "In this section, we present a comprehensive evaluation of our agent using three distinct benchmarks: DroidTask (Wen et al., 2024), AppAgent (Yang et al., 2023b), and Mobile-Eval (Wang et al., 2024). We begin with DroidTask to test complex task performance, comparing against AppAgent for different exploration methods, and conclude with Mobile-Eval to assess comprehensive capabilities. Results in the ensuing sections demonstrate the superiority of our approach in varied application scenarios."}, {"title": "DroidTask", "content": "In this study, we employ the DroidTask dataset (Wen et al., 2024), an Android Task Automation benchmark suite designed to evaluate the capabilities of mobile task automation systems. DroidTask consists of 158 high-level tasks derived from 13 popular applications. We conducted our experiments using the DroidTask dataset. Due to variations in the app versions and device models used during evaluation, the specific workflows for implementing functionalities in the apps may differ. Consequently, we employ the \"Completion Rate\" as our evaluation metric, similar to (Wen et al., 2024). The Completion Rate is defined as the probability of accurately completing all the actions in a"}, {"title": "AppAgent", "content": "AppAgent (Yang et al., 2023b) has introduced a benchmark that spans ten commonly used applications with diverse functionalities, including Twitter, Telegram, Temu, among others. We compare our agent against AppAgent on this benchmark to assess our agent's adaptability across various functions and interfaces. The primary evaluation metric is the success rate, which reflects the proportion of tasks that the agent successfully completes within an application. The results are detailed in Table 1. The results of our agent-driven exploration are comparable to those obtained from AppAgent with watching demos. After integrating the documents generated through manual exploration, our agent's performance improved significantly, underscoring the effectiveness of our exploration phase."}, {"title": "Mobile-Eval", "content": "We evaluated our agent on the Mobile-Eval benchmark. Mobile-Eval is a comprehensive benchmark introduced for mobile agents, containing 10 commonly used mobile apps to test agent performance across different tasks. Mobile-Eval assesses the following metrics:\nSuccess (Su): Marks an instruction as successful if the agent completes it entirely.\nProcess Score (PS): Evaluates step accuracy by calculating the ratio of correct steps to total steps.\nRelative Efficiency (RE): Compares the steps taken by the agent to human performance to measure efficiency.\nCompletion Rate (CR): Measures the proportion of steps the agent completes compared to a human's total steps.\nWe compared our agent's performance against the original Mobile-Agent benchmark scores and human performance, as shown in Table 2. Without integrating the documentation and solely relying on the deployment phase, we achieved the results outlined below The upper table shows the results for Mobile-Agent, and the lower table presents results for our agent. Our agent excelled in completing each task, achieving a 100% success rate across all instructions in the 10 task categories. The average PS score across three instruction sets exceeded 90%, indicating that our agent efficiently and accurately completed tasks with minimal errors. This demonstrates its ability to closely emulate human behavior and execute specified tasks effectively on various general apps."}, {"title": "User study", "content": "To demonstrate our qualitative results, we conducted a user study, as shown in Figure 3. The task involved a series of complex operations, including"}, {"title": "Analysis of UI Interface Parsing", "content": "In our agent, we employ two primary methods for parsing UI interfaces: structured data and visual features. Structured data provides precise and rich information, including details about widget interactivity\u2014such as clickability and scrollability. In this experiment, we utilized XML data parsed from Android systems to enhance our understanding and manipulation of these interactive elements. This method is well-suited for most generic apps and, in conjunction with our agent, can complete the majority of tasks efficiently.\nNevertheless, there are challenges associated with mobile platforms that feature custom-developed apps and icons. Specifically, structured data cannot be parsed for custom icons built on Android, which necessitates the use of visual features for extracting widget information. This approach allows for more accurate recognition of text and icons. However, visual features alone cannot"}, {"title": "Related works", "content": ""}, {"title": "LLM-based agents", "content": "Agents have rapidly evolved with the advancement of large language models. Models such as MetaGPT (Hong et al., 2023), HuggingGPT (Shen et al., 2024), and AssistGPT (Gao et al., 2023), Seeclick (Cheng et al., 2024) have demonstrated exceptional performance in agent applications, garnering widespread adoption across various domains. Some agents employ large language models such as ChatGPT (OpenAI, 2023) or GPT-4 (OpenAI, 2023) for task decision-making, achieving notable developments in general domains including music (Huang et al., 2024; Yu et al., 2023), gaming (Wu et al., 2023b;, FAIR), and autonomous"}, {"title": "Agent for mobile devices", "content": "There are already several agents developed for mobile devices that utilize large language models effectively. DroidBot-GPT (Wen et al., 2023a) automates Android app interactions by interpreting app GUI states and actions into natural language prompts, thus facilitating action selection. AppAgent (Yang et al., 2023b) identifies and enumerates UI components based on XML, subsequently making decisions and executing actions with the aid of GPT-4V. MobileAgent (Wang et al., 2024) incorporates visual features, integrating OCR technology and icon detection to enhance UI recognition capabilities. AutoDroid (Wen et al., 2023a) seamlessly"}, {"title": "Conclusion", "content": "This paper introduces a multimodal agent framework that significantly enhances the interaction capabilities of smartphone applications. Our experiments across various applications demonstrate the framework's ability to improve GUI recognition and task execution, confirming its effectiveness in adapting to diverse application environments.\nWe integrate parsers with visual features to construct a more flexible action space and develop a newly structured knowledge base for diverse element storage. Through two phases, exploration and deployment, we enable the agent to effectively manage the dynamic nature of mobile interfaces. These capabilities not only align with but also extend the current research on intelligent agents, especially in the contexts of multimodality and mobility.\nWhile building upon existing technologies, our approach contributes incremental advancements in the precision and adaptability of agents operating"}, {"title": "Limitations", "content": "Throughout the comprehensive testing process, we identified several limitations of our agent: Our method relies on the agent's ability to recognize numerical tags on the UI to determine specific UI elements. This approach can lead to confusion when the UI element itself contains numbers. Such errors can be mitigated through preliminary manual exploration and documentation to clarify the context.\nWhen attempting to interact with hidden UI elements, such as accelerating a video by clicking on the screen, the agent lacks the necessary prior knowledge and cannot detect the acceleration button within the current UI. This limitation hampers its ability to perform specific operations. Future work will focus on enhancing UI recognition and incorporating prior knowledge to address these issues effectively."}, {"title": "Ethics Statement", "content": "Our research introduces a novel multimodal agent framework designed to interact seamlessly with smartphone applications, enhancing both user experience and decision-making capabilities. In developing and deploying this technology, we are committed to addressing several key ethical considerations:\nPrivacy and Data Protection: We ensure strict adherence to global privacy standards, implementing robust data security measures to protect user information.\nReliability and Safety: We implement safety checks to ensure the reliability of our agent, particularly in dynamic environments.\nSocietal Impact: We consider the broader impacts of our technology, including potential effects on employment and environmental sustainability.\nContinuous Monitoring: We commit to continuously monitoring and refining our technology to address emerging challenges and integrate user feedback."}, {"title": "Prompt Structure Description", "content": "In this section, we describe the main prompts used by our agent, highlighting their structure and purpose across different operational phases. The parts enclosed in bold black angle brackets are parameters that can be replaced during the coding phase, while the red text indicates areas to be filled in by the user, and the blue text represents annotations."}, {"title": "Explanation of DroidTask Results", "content": "In figure 2, we present the performance of our agent and AutoDroid on the DroidTask benchmark. The differential in testing environments, AutoDroid's real device testing on specific Android phone compared to our emulator-based approach, alongside discrepancies in application versions between the two setups, precluded direct execution of some tasks. For a small subset of tasks that could not be completed, we identified alternative testing methods. For instance, whereas our application lacks a date-sorting option for document names, we considered sorting by the initial letter of the document names as an alternative. This adjustment maintains the same procedural flow and steps, albeit with a slightly different selection at the end. Additionally, there are tasks that our application does not support and for which no alternative exists; these cases were treated as error examples. Therefore, under identical conditions, the performance of our agent would be higher than currently observed."}, {"title": "Details of Action Space", "content": "In this section, we provide a detailed description of the usage and parameters for each action space:\nTapButton(element: int/str): Initiates a tap action on a user interface element. For example, TapButton(5) taps the UI element labeled as '5'. TapButton('hat') taps the UI element with text 'hat'.\nText(text: str): Simulates typing by entering a string of text into a designated input area. For instance, Text(\"Hello, world!\") inputs the string \"Hello, world!\" into the text field.\nLongPress(element: int): Applies a prolonged press on a specified element. For example, LongPress (3) applies a long press to the element labeled '3'."}, {"title": "Case Study", "content": "As illustrated in Figures 8, 9 and 10, we present several case studies showcasing the qualitative results obtained across diverse applications, tasks, and specialized functionalities. Figure 8 highlights a scenario where our agent triggers a safety check during sensitive operations. Figures 9 and 10 display the qualitative results of our agent handling multi-step tasks. These examples demonstrate the robustness of our agent, emphasizing its capability to effectively manage a variety of complex scenarios."}]}