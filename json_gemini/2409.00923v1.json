{"title": "Development of Occupancy Prediction Algorithm\nfor Underground Parking Lots", "authors": ["Shijie Wang"], "abstract": "Abstract-Bird's Eye View (BEV) refers to a top-down viewing\nperspective, offering an overhead view of the surroundings.\nOccupancy Prediction is a crucial task within the BEV per-\nception framework, aiming to predict the occupancy status of\nthe vehicle's surrounding environment. This includes forecasting\nthe future positions and trajectories of dynamic objects such\nas pedestrians and other vehicles. Despite the growing trend of\nvision-based occupancy grid prediction under BEV perception,\ncurrent research primarily focuses on outdoor highway driving\nscenarios, lacking studies on underground parking lot environ-\nments. Underground parking lots present unique challenges due\nto significant variations in lighting conditions, demanding robust\nperception systems capable of adapting to diverse illumination\nscenarios. Achieving superior perception performance in such\nenvironments remains a critical challenge for the autonomous\ndriving industry.\nThe core objective of this study is to address the perception\nchallenges faced by autonomous driving in adverse environments\nlike basements. Initially, this paper commences with data col-\nlection in an underground garage. A simulated underground\ngarage model is established within the CARLA simulation\nenvironment, and SemanticKITTI format occupancy ground\ntruth data is collected in this simulated setting. Subsequently,\nthe study integrates a Transformer-based Occupancy Network\nmodel to complete the occupancy grid prediction task within\nthis scenario. A comprehensive BEV perception framework is\ndesigned to enhance the accuracy of neural network models in\ndimly lit, challenging autonomous driving environments. Finally,\nexperiments validate the accuracy of the proposed solution's\nperception performance in basement scenarios. The proposed\nsolution is tested on our self-constructed underground garage\ndataset, SUSTech-COE-ParkingLot, yielding satisfactory results.", "sections": [{"title": "I. INTRODUCTION", "content": "The field of autonomous driving is undoubtedly one of the\nhottest topics in the current tech industry. Autonomous driving\nsystems are complex operational systems that encompass all\naspects required by AI, including perception, cognitive rea-\nsoning, decision-making, and control execution. The technical\ndifficulty is extremely high, and because they involve the\nphysical world, the margin for error is extremely low, aiming\nfor even 100% safety. Therefore, the requirements for technical\ncapability boundaries are very stringent. Among the many\ntasks in autonomous driving, the perception and recognition\ntask is the starting point for an intelligent agent to recognize\nthe physical world, making it particularly important.\nWith the introduction of BEV (Bird's Eye View) into au-\ntonomous driving perception tasks, both academia and industry\nhave been developed. BEV has seen such rapid development\ndue to its distinct advantages: increasingly complex sensor\ncalibration, data integration from multiple sensors, and the\nrepresentation and processing of multi-source data need to be\nimplemented within a unified BEV view. BEV has advantages\nsuch as no occlusion and scale issues among objects, and\nobjects and lane elements in the BEV view can be conveniently\napplied in subsequent modules, such as planning and control\nmodules [1].\nThe main goal of BEV perception is to learn robust and\ngeneralized feature representations from camera and LiDAR\ninputs. A key issue is how to implement multi-source data\nfusion in the early or mid-stages of the solution. However,\naligning and integrating multi-source data has always been a\ncritical point with room for innovative development. Compared\nto the LiDAR branch, which easily obtains 3D point cloud\nattributes, it is particularly challenging for the camera branch\nto obtain 3D spatial information using monocular or multi-\nview settings. From this perspective, there is significant room\nfor development in obtaining spatial information from visual\nsolutions.\nCurrently, vision-based BEV solutions still lag significantly\nbehind BEV LiDAR solutions. The performance gap between\nvision algorithms and LiDAR algorithms on the nuScenes\ndataset exceeds 20%, and on the Waymo dataset, this gap\nreaches 30%. From an academic perspective, the key to vision\nalgorithms achieving performance on par with LiDAR is better\nunderstanding the view transformation from 2D external input\nto 3D geometric input. From an industrial perspective, equip-\nping an autonomous vehicle with a set of LiDAR devices is far\nmore expensive than vision software. This demand drives the\ndevelopment of vision-based autonomous driving algorithms.\nAmong various perception tasks based on vision and LiDAR,\nvision perception has the potential to compete with LiDAR\nperception. Moreover, current research and development in-\ndicate that BEV feature representation is also suitable for\nmulti-camera input. These factors collectively promote the\ndevelopment of BEV visual perception.\nIn the field of vision-based BEV perception, lighting issues\nare particularly challenging. The lighting quality in different\nscenarios affects the images captured by cameras differently,\nsignificantly impacting the effectiveness of BEV perception.\nBasement scenarios are important for autonomous driving\ntasks, characterized by poor lighting conditions [2]. Despite\nthe ongoing development of BEV perception technology,\ncurrent open datasets mainly feature outdoor scenarios, and\nexisting solutions still struggle to adapt well to basement envi-\nronments. Experiments in this study found that state-of-the-art\n(SOTA) algorithm models from major public datasets perform\npoorly in basement scenarios. Therefore, this paper aims to\ndesign a solution to optimize BEV perception performance in\nbasement scenarios."}, {"title": "II. RELATED WORK", "content": "Vision-based solutions have garnered widespread attention\nin the field of autonomous driving due to their low cost, ease\nof deployment, and wide applicability. Additionally, cameras\ncan provide rich visual attributes of the scene to aid in overall\nscene understanding for vehicles. Recently, there has been a\nsurge of work on 3D object detection or segmentation from\nRGB images. Inspired by DETR's [5] success in 2D detection,\nDETR3D [6] links learnable 3D object queries with 2D images\nthrough camera projection matrices, achieving end-to-end 3D\nbounding box prediction without the need for non-maximum\nsuppression (NMS). M\u00b2BEV [7] has also investigated the fea-\nsibility of simultaneously running multi-task perception based\non BEV features. BEVFormer [8] proposes a spatiotemporal\nfusion Transformer structure that aggregates BEV features\nfrom both current and previous frames using deformable\nattention [9]. Compared to object detection tasks, occupancy\ngrid prediction tasks can represent the occupancy status of\neach small voxel unit instead of assigning fixed-size bounding\nboxes to objects. This helps autonomous driving perception\ntasks better identify objects with irregular states. Compared to\n2D BEV schemes, 3D voxelized scene representations contain\nmore information, which is more accurate in complex driving\nenvironments (such as rugged roads). sense voxel semantics\ncan provide a more comprehensive 3D scene representation,\nand efficiently and accurately achieving this goal through\nvision-based approaches is challenging."}, {"title": "B. Semantic Scene Completion", "content": "3D semantic scene completion presents a significant chal-\nlenge for autonomous driving vehicles due to the limited range\nof sensors. SSCNet [10] first defined the task of semantic scene\ncompletion, which involves jointly inferring the completion\nof the scene based on geometric and semantic information\ngiven incomplete visual observations. In recent years, with\nthe release of the SemanticKITTI dataset [11], the task of\nsemantic scene completion in large-scale outdoor scenes has\ngained attention.\nCompleting semantic scene information based on sparse\nobservations is ideal for autonomous driving vehicles. Vehicles\nwith this capability can obtain dense 3D voxelized semantic\nrepresentations of scenes, which can aid in reconstructing\n3D static maps and perceiving dynamic objects. However,\nlarge-scale 3D semantic scene completion in driving scenarios\nis still in its early stages of exploration and development.\nExisting work typically relies on point cloud data as input\n[12]. In contrast, the recent MonoScene [13] study explored\nsemantic scene completion based on monocular images. This\nwork proposed feature projection from 2D to 3D and utilized\ncontinuous 2D and 3D Unet to achieve 3D semantic scene\ncompletion. However, the feature projection from 2D to 3D\nis susceptible to false features introduced by unoccupied\npositions, and multi-layered 3D convolution can reduce system\nefficiency."}, {"title": "\u0421. \u041e\u0441\u0441\u0438\u0440\u0430\u043f\u0441\u0443 Prediction", "content": "3D occupancy prediction, as a hot topic in perception tasks,\nhas been widely discussed recently. It involves predicting\nenvironmental occupancy and semantic information at the\nfinest granularity, exhibiting good scalability and adaptability\nto downstream tasks. In recent works, solutions attempt to\npredict semantic occupancy solely from image inputs, while\nOpenOccupancy [14] employs a multisensor fusion approach\nto tackle this task. TPVFormer [15] combines inputs from\nsurrounding multi-camera views and elevates features to a\nthree-view space using a Transformer-based [16] approach.\nDue to its training with sparse LiDAR point clouds for super-\nvision, its predictions are also sparse. Similarly, SurroundOcc\n[17] adopts a Transformer-based method to generate 3D voxel\nfeatures at multiple scales and combines these features through\ndeconvolution upsampling. Additionally, this work proposes a\nmethod to extract dense semantic occupancy information from\nsparse LiDAR data for supervision, enabling dense prediction"}, {"title": "B. Main Work", "content": "Construction of Simulation Environment: Utilizing the\nUnreal Engine-based simulation platform CARLA, we sim-\nulate a realistic scenario of the SUSTech-COE underground\nparking lot, constructed using 3D modeling software. By\nmeticulously studying the layout and features of the under-\nground garage based on engineering CAD drawings, we aim\nto provide a highly realistic simulation environment to ensure\nthe quality of subsequent data collection and driving scene\nexperiments.\nData Collection and Scripting: We develop corresponding\nscripts within the constructed simulation environment using\nthe Python API interface provided by the CARLA simulator to\nfacilitate data collection. Specifically, we configure vehicle and\nsensor parameters within the CARLA simulation world, con-\ntrol vehicle and sensor behaviors through scripts, and collect a\nlarge amount of camera image information, sparse point cloud\ndata, and semantic information. This data collection process\nprovides the foundation for capturing dynamic changes in\ndriving scenes within the virtual environment.\nMulti-frame Fusion for Point Cloud Densification: Using\nthe collected data, we apply a multi-frame fusion algorithm to\ndensify sparse point cloud data and its corresponding semantic\ninformation [3], [4]. By integrating multiple frames of data\ninto a single frame's point cloud, we significantly improve the\ndensity and accuracy of the point cloud. This step aims to\ngenerate high-quality point cloud data for training subsequent\nalgorithm models.\nOccupancy Grid Prediction with Vision-based Semantic\nScene Completion Model: Finally, we employ a Transformer-\nbased vision semantic scene completion model to predict\noccupancy grids. This model, leveraging the powerful feature\nextraction capabilities of Transformers and featuring a novel\ntwo-stage design, achieves high-performance perception while\nreducing the computational resources required for training.\nThrough these steps, we have established a complete work-\nflow from simulation environment construction, data collec-\ntion, and processing to final occupancy grid prediction, validat-\ning the effectiveness and practicality of the proposed method."}, {"title": "B. Data Collection", "content": "The collected data is saved in the SemanticKITTI format,\nand images captured by stereo cameras for each frame are\nalso stored. This collected data then undergoes ground truth\ngeneration, where point cloud information is voxelized and\nsemantic scene completion is performed using appropriate\nalgorithms."}, {"title": "1) Scenario Settings", "content": "In the CARLA environment, we\nconfigure the predefined driving scenarios to replicate real-\nworld scene information accurately."}, {"title": "Map Settings", "content": "In CARLA, we create a map for the\nSUSTech COE ParkingLot (SUSTech College of Engineering\nunderground parking lot) to replicate the real-world environ-\nment. Since underground parking lots are generally unaffected\nby weather conditions and have dim lighting, we set the world\nweather to cloudy and overcast."}, {"title": "Frame Rate Settings", "content": "We set the simulator's frame rate\nto 1000 frames per second and configured the communication\nmode between the server and client to synchronous mode. In\nsynchronous mode, the flow of time in the CARLA world is\ncontrolled by external scripts, meaning that the client running\nthe Python code takes control and instructs the server when\nto update the next frame."}, {"title": "2) Ego Vehicle Settings", "content": "We configure the basic information\nfor the vehicle and make different positional adjustments to\nmeet data collection requirements."}, {"title": "Spawn Point Settings", "content": "In the CARLA world, we create an\nautonomous vehicle (Ego Vehicle) using the Tesla Model 3\nseries and select different spawn points based on the data col-\nlection requirements. The driving scenario in an underground\nparking lot is more complex compared to open roads, with\nmore lanes and obstacles such as walls and pillars. To collect\ncomplete information about the entire parking lot, the vehicle\nneeds to navigate through every lane, covering all corners and\nareas obscured by walls and pillars to ensure the quality of\ndata collection. Therefore, it is essential to carefully choose\nthe spawn points of the ego vehicle within the underground\nparking lot. For this purpose, we manually select 22 different\nspawn points to cover all road segments within the parking\nlot."}, {"title": "Anti-collision Settings", "content": "During the vehicle spawning pro-\ncess, CARLA suspends the vehicle in the air to prevent\ncollisions with objects along its z-axis, such as the ground,\nwhich could lead to issues with the physics engine. While\nwaiting for the vehicle to land, data collection cannot begin.\nTherefore, it is necessary to calculate the free fall time of"}, {"title": "3) Sensor Settings", "content": "On the autonomous vehicle, we attach\nrelevant sensors such as cameras, LiDAR, and set their corre-\nsponding parameters [23]. We reference the sensor parameter\nsettings from the KITTI official specifications and make nu-\nmerical adjustments based on the situation in CARLA. In the\nreal world, these parameters need to be determined through\ncamera calibration. We can directly set their corresponding\nvalues in an ideal simulated environment [24]."}, {"title": "Camera Settings", "content": "We employ PointGray Flea2 color and\ngrayscale cameras, with parameters set as shown in Table I."}, {"title": "Lidar Settings", "content": "We use the Velodyne-HDL-64E LiDAR\nwith parameters set as shown in Table II."}, {"title": "Sensor Parameter Settings", "content": "The external parameter set-\ntings of all sensors relative to the ego vehicle coordinate\nsystem are shown in Table III."}, {"title": "4) Data Acquisition and Storage", "content": "Once the vehicle and\nsensor parameters are properly set, the core part of data\ncollection can begin:\nConstruct the corresponding file directory structure based\non the SemanticKITTI dataset structure, with the addition of\na calib.txt file, representing the parameter information of the\nvehicle-mounted sensors and the affine transformation rela-\ntionship, on the basis of the file structure of SemanticKITTI.\nSave the data obtained from the built-in LiDAR sensors\nin CARLA in the .bin file format under the velodyne folder.\nThe required point cloud data is saved as the following\nvectors. Since the semantic annotations for each entity in\nthe world in CARLA are not the same as SemanticKITTI,\nsemantic remapping is also needed. The indices corresponding\nto different entity categories in CARLA are replaced with\nthose in KITTI, and saved in the label file format under the\nlabels folder.\nIn addition, for each sensor's parameters, it is necessary to\nperform affine transformations to different coordinate systems\nand calculate intrinsic, extrinsic, and projection matrices. Fi-\nnally, these parameters are output to the camera calibration\nparameter file 'calib' and the pose parameter file 'poses' to\nprepare the data for subsequent ground truth generation tasks."}, {"title": "1) In the 'calib' file", "content": "the first four lines contain the camera\ncalibration data, each consisting of a rectified 3x4 pro-\njection matrix. Specifically:\n\u2022 Lines 1 and 3 represent the matrices corresponding to\nthe left-side RGB and grayscale cameras, respectively.\n\u2022 Lines 2 and 4 represent the matrices for the correspond-\ning right-side RGB and grayscale cameras.\n\u2022 These matrices can map a point from the camera\ncoordinate system to the image coordinate system of\nthe corresponding camera.\n\u2022 The last line consists of the transformation matrix that\nrotates the coordinate system from the left-side RGB\ncamera to the top-mounted LiDAR coordinate system.\nThis matrix maps a point from the LiDAR coordinate\nsystem to the left-side RGB camera coordinate system."}, {"title": "2) The 'poses' file", "content": "records the pose information of the\ncamera throughout the data collection process. Each line\nis a 3x4 transformation matrix representing the pose of\nthe current frame relative to the starting frame in the\nleft camera coordinate system. This matrix maps a point\nfrom the current frame to the starting frame's coordinate\nsystem."}, {"title": "3) Implement Details", "content": "In the first stage, VoxFormer utilizes\nMobileStereoNet for direct depth estimation, enabling low-\ncost generation of pseudo-LiDAR point clouds. The occupancy\nprediction network used for depth correction is adapted from\nLMSCNet [31], a lightweight 2D convolutional neural network\n[32], [33]. During training, the input is a voxelized pseudo\npoint cloud of size 256 \u00d7 256 \u00d7 32, and the output is an\noccupancy grid of size 128 \u00d7 128 \u00d7 16."}, {"title": "C. Data Preprocessing", "content": "In this phase, the preliminary data collected in the previ-\nous steps needs to undergo a series of preprocessing steps\nto provide the input data required for the occupancy grid\nprediction model inference and the ground truth needed for\ntraining. Since the ground truth generation for the first phase\ndepends on the ground truth from the second phase, this paper\nwill first describe the method for generating the ground truth\nfor the second phase."}, {"title": "1) Second-stage Truth Generation", "content": "Based on the trajectory\ninformation depicted in the poses file, the sparse point clouds\nin the point cloud data are completed through multi-frame\nfusion and voxelized. These are then mapped onto a voxel\ngrid map, which is a fixed-size cubic structure composed of\nmultiple voxels, and saved. The generation of this ground truth\ncan supervise the training of the occupancy grid prediction\nnetwork model. The specific process is illustrated in Figure 3."}, {"title": "A. Environment Configuration", "content": "Since the inference and training of the occupancy grid\nprediction model require GPU computational support, we used\nSSH to connect to a remote server cluster for model inference\nand training. The specific hardware and software environment\nare as follows:\n\u2022 Programming Languages: Python 3.8, C++\n\u2022 Deep Learning Framework: PyTorch 1.9.1\n\u2022 Operating System: Ubuntu 22.04\n\u2022 Graphics Cards: 2 NVIDIA L40S GPUs\n\u2022 Development Environment: PyCharm Professional,\nDocker\n\u2022 Python Packages:\ntorchvision 0.10.1\nCUDA Toolkit 11.1\ntorchaudio 0.9.1\nmmcv 1.4.0\nmmdet 2.14.0\nmmsegmentation 0.14.1\ntimm\n\u2022 Compilation Environment: gcc 6.2, NVCC 11.1\nWe chose Miniconda for Python package management.\nMiniconda is a lightweight distribution of Anaconda, serving\nas a package and environment manager for data science and\nmachine learning [27], [28], [29]. Unlike Anaconda, which\nincludes numerous pre-installed packages, Miniconda only\ncontains the essential components, making installation and\nupdates faster and more efficient."}, {"title": "V. RESULTS", "content": "In this paper, we use a self-collected underground parking\ngarage dataset as the training dataset. The dataset consists of\n22 regions, with regions 0-10 used as the training set and\nregions 11-21 used as the test set. The Loss curve for the first\nstage of training is shown in Figure 11."}, {"title": "VI. DISCUSSION", "content": "Due to limitations in time and personal ability, there are\nstill areas in this paper that need improvement. For instance,\nthe number of training epochs could be increased, given the\nconstraints on computational power and scene modeling. Addi-\ntionally, the diversity of the data collected needs enhancement.\nThe excellent performance demonstrated may be due to the\nhigh similarity between the training and testing data.\nTherefore, further work is required to explore the diversity\nof the dataset to enhance the model's generalization ability.\nThis will ensure that the model can be widely applied in\nvarious underground parking lot scenarios and perform with\nthe same perception capabilities as demonstrated in this ex-\nperiment."}, {"title": "VII. CONCLUSIONS", "content": "In this paper, we designed a data collection scheme based on\nthe CARLA simulator environment, including semantic map-\nping, point cloud voxelization, and multi-frame fusion voxel\ncompletion algorithms. The completion algorithm enhances\ndata reliability, providing stronger robustness.\nThe VoxFormer model, based on a powerful deformable\nattention mechanism and combining its depth prediction\nmodel with a downsampling occupancy grid prediction model,\nachieves efficient and accurate occupancy grid prediction.\nAfter 24 epochs of training, VoxFormer consistently achieved\ngood prediction results in the underground parking lot sce-\nnario.\nFrom the above results, it can be concluded that fine-tuning\nthe model with the underground parking lot data collected in\nthis paper allows it to adapt well to the lighting conditions\nand road conditions of the underground parking environment,\nexhibiting good perception performance."}]}