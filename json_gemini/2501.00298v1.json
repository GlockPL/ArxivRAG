{"title": "Enhancing Deployment-Time Predictive Model Robustness for Code Analysis and Optimization", "authors": ["Huanting Wang", "Patrick Lenihan", "Zheng Wang"], "abstract": "Supervised machine learning techniques have shown promising results in code analysis and optimization problems. However, a learning-based solution can be brittle because minor changes in hardware or application workloads \u2013 such as facing a new CPU architecture or code pattern - may jeopardize decision accuracy, ultimately undermining model robustness. We introduce PROM, an open-source library to enhance the robustness and performance of predictive models against such changes during deployment. PROM achieves this by using statistical assessments to identify test samples prone to mispredictions and using feedback on these samples to improve a deployed model. We showcase PROM by applying it to 13 representative machine learning models across 5 code analysis and optimization tasks. Our extensive evaluation demonstrates that PROM can successfully identify an average of 96% (up to 100%) of mispredictions. By relabeling up to 5% of the PROM-identified samples through incremental learning, PROM can help a deployed model achieve a performance comparable to that attained during its model training phase.", "sections": [{"title": "1 INTRODUCTION", "content": "Supervised machine-learning (ML) is a powerful tool for code analysis and optimization tasks like bug detection [28, 39, 73, 75] and runtime- or compiler-based code optimization [18, 19, 52, 53, 68, 74, 76-78, 82, 84]. ML works by training a predictive model from training samples and then applying the trained model to previously unseen programs within operational environments that often have diverse application workloads and hardware [32, 79].\nML solutions, while powerful, can be fragile. Small changes in hardware or application workloads can reduce decision accuracy and model robustness [59]. This often arises from \"data drift\" [51, 70], where the training and test data distributions no longer align. Data drift can occur when the assumption that past training data reflects future test data is violated. In code optimization, this can result from changes in workload patterns, runtime libraries, or hardware micro-architectures. It is a particular challenge for ML-based performance optimizations, where obtaining sufficient performance training data is difficult [20, 48].\nExisting efforts to enhance ML robustness for code optimization have predominantly focused on improving the learning efficiency or model generalization during the design time. These approaches include synthesizing benchmarks to increase the training data size [15, 20, 69], finding better program representations [18, 73, 75, 82], and combining multiple models to increase the model's generalization ability [43, 65, 75]. While important, these design-time methods are unlikely to account for all potential changes during deployment [42]. Although there has been limited exploration into the validation of model assumptions [25] for runtime scheduling, existing solutions assume a specific ML architecture and lack generalizability.\nWe introduce PROM, an open-source toolkit designed to address data drift during deployment, specifically targeting code optimization and analysis tasks. PROM is not intended to replace design-time solutions but to offer a complementary approach to improve ML robustness during deployment. Its primary objective is to ensure the reliability of an already deployed system in the face of changes and support continuous improvements in the end-user environment. To this end, PROM offers a Python interface for training and deploying"}, {"title": "2 MOTIVATION", "content": "As a motivating example, consider training and using an ML model to detect source code bugs. In this pilot study, we consider VULDE [39], which uses a long short-term memory (LSTM) network for bug detection. Following the original setup, we train and test the model on labeled samples from the common vulnerabilities and exposures (CVE) dataset. We use the open-source release of VULDE and ensure results are comparable to those in the source publication of VULDE.\nFigure 1(a) shows what happens if we train VULDE using CVE data collected between 2012 and 2014 and then apply it to real-life code samples developed between 2015 and 2023. This mimics a scenario where the code and bug patterns may evolve after a trained model is deployed. The trained model achieves an F1 score of more than 0.8 (ranging from 0 to 1, with higher being better) when the test and training data are collected from the same time window. However, the F1 score drops to less than 0.3 when tested on samples collected from future time windows. This shows that data drift can severely impact model performance, which was also reported in prior studies [49, 75].\nTo illustrate code pattern changes, Figures 1(b) and 1(c) show two cases of \u201cdouble-free\" vulnerabilities, one from 2012 and one from 2023. Earlier cases were simpler, where the same memory was freed twice (e.g., name is freed on lines 6 and 8). A model trained on these samples is unlikely to detect the later, more complex case in Figure 1(c), where a \"double-free\u201d occurs due to concurrent threads calling the same buffer-free routine. Because it is difficult to collect a training dataset which covers all possible code patterns seen in deployment time, data drift can happen in the real-life deployment of ML models for code-related tasks."}, {"title": "3 BACKGROUND", "content": ""}, {"title": "3.1 The Need of Credibility Evaluation", "content": "To detect unreliable prediction outcomes, a straightforward approach is to analyze probability distribution given by an ML model. For instance, a high prediction probability for a specific label might suggest high confidence in the prediction. However, this alone does not fully capture the prediction's reliability, as ML models can produce skewed probabilities for rarely seen samples. Consider multi-class classification for example. An ML model predicts the likelihood, $r^i$, that a given input belongs to each class ($c_1$ to $c_n$). However, if the input's pattern significantly differs from training samples, the model might assign a low probability to some classes (e.g., $r^1 \\approx 0.0$ for class $c_1$). This can disproportionately inflate the probabilities of other classes ($r^2$ to $r^n$) as the sum"}, {"title": "3.2 Statistical Assessment", "content": "PROM uses statistical assessments to evaluate prediction credibility and confidence. Unlike typical probabilistic evaluations in ML models, which assess the likelihood of a test sample belonging to a certain class or value in isolation, statistical assessments draw from historical data distributions. They answer questions like: \u201cHow likely is the test sample to belong to a class compared to all other possible classes\u201d? By framing the sample within the broader context of historical decisions and probabilistic distributions, statistical assessments quantify the uncertainty of a prediction."}, {"title": "3.3 Conformal Prediction", "content": "PROM is built on conformal prediction (CP) [5, 10], which, given a model g and a significance level, defines a prediction region that contains the true value with a certain probability. CP constructs this region based on training data distribution, accounting for noise and variability. CP was designed to improve prediction coverage by calculating a prediction range. PROM utilizes CP, for a different purpose: evaluating the reliability of a model prediction.\nP-value. PROM uses the p-value [66], given by CP, to assess the prediction credibility in its predictions and the confidence of the credibility. We use it to quantify evidence that contradicts a null hypothesis. For instance, in determining the confidence of a classifier's prediction, the null hypothesis would assert that there is no substantial difference between the predicted class and any other class, as observed from the model's probability distribution. Likewise, in assessing the credibility of a prediction, the null hypothesis assumes that the prediction does not fall within a specific prediction region computed by CP. In PROM (see also Sec. 5.1.2), a high p-value suggests that the likelihood of the observed data under the null hypothesis is small, providing strong evidence against it. Conversely, a low p-value indicates a high likelihood of the observed data under the null hypothesis, thus offering weaker evidence against it. In other words, a high p-value suggests that the prediction is reliable."}, {"title": "4 OVERVIEW of PROM", "content": "Figure 2 illustrates how PROM can enhance learning-based methods during deployment. Users of PROM are ML model or application developers. PROM requires no change to a user model's structure and working mechanism. In this paper, we refer to the user model as the \u201cunderlying model\".\nUser involvement. For a given input, the underlying model works as it would without PROM during inference. Users of"}, {"title": "4.1 Implementation", "content": "We implemented PROM as a Python package, offering an API for use during both the ML model design and deployment phases. PROM provides interfaces to assess framework setup (Sec. 5.2), automatically searches for hyperparameter settings on the training and calibration datasets, and provides examples to showcase its utilities. These include all the case studies and ML models used in this work (Sec. 6) and simpler examples for beginners. PROM supports classification and regression methods built upon classical ML methods (e.g., support vector machines) and more recent deep neural networks. Figures 3 and 4 provide an overview of PROM's role during the model design and deployment phases, described as follows.\n4.1.1 Model design phase. As depicted in Figure 4, using PROM requires overwriting a handful of methods in PROM'S ModelDefinition class and exposing the underlying model's internal outputs.\nTraining data partitioning. PROM is based on split CP [5, 10], which divides the training data into a \u201ctraining dataset\u201d and a \"calibration dataset\". PROM uses the calibration dataset to detect drifting test samples during deployment. By default, it randomly sets aside 10% of the training data (up to 1,000 samples) for calibration, a method shown to be effective in prior work [5, 72]. PROM also offers a way to assess the suitability of the calibration dataset (Sec. 5.2), or users can provide their own holdout calibration dataset.\nChanges to user models. For classification tasks, the user model should implement a prediction function (line 15) that returns both a prediction and a probability vector. Most ML classifiers already associate probabilities with each class, which can be easily accessed. Popular frameworks like scikit-learn, PyTorch, and TensorFlow directly provide probability distributions. For example, scikit-learn's predict_proba"}, {"title": "4.1.2 Model deployment phase.", "content": "During deployment, the user model functions as usual, taking a test sample and making a prediction. The difference with PROM is that it also suggests whether to accept or reject the prediction. The user can use this outcome to identify mispredictions and provide ground truth, and PROM will use these relabeled drifting samples to update the model."}, {"title": "5 METHODOLOGY", "content": "As shown in Figure 5, during deployment, PROM uses multiple (default: 4) nonconformity functions to independently compute the prediction's credibility and confidence scores. These scores are compared to a pre-defined significance level, 1 \u2212 \u03b5 (Sec. 4.1.1), to decide whether to accept the prediction. If both scores fall below the threshold, the test sample is flagged as drifting. The results are then aggregated using majority voting, where each nonconformity function (expert) decides whether the prediction should be accepted, forming an ensemble \"expert committee\u201d."}, {"title": "5.1 Nonconformity Measures", "content": "PROM computes the p-value of a prediction using nonconformity functions, which are then used to derive credibility and confidence scores. Previous work on using CP to detect drifting samples [11, 37, 83] focuses primarily on classification tasks and does not extend to regression. PROM is the first framework to support both classification and regression for data drift detection. Additionally, prior methods only consider the predicted label, ignoring the probability distribution across labels, whereas PROM accounts for probabilities across all labels in classification tasks (Sec. 3.1).\n5.1.1 Nonconformity functions. PROM integrates multiple ready-to-use nonconformity functions, and the choice of nonconformity functions can be customized by passing a list to the relevant PROM interface. By default, PROM uses 4 nonconformity functions: LAC [58], TopK [6], APS [57] and RAPS [6]. Other nonconformity functions can be easily incorporated into PROM by implementing an abstract class.\nFor regression tasks, our nonconformity functions compute the nonconformity score using the residual error between the prediction and the ground truth. Since we do not have the ground truth during deployment, we approximate it using the k-nearest neighbour algorithm [17, 36]. This approximation is based on the null hypothesis that the test sample is similar to those encountered during design time. Specifically, PROM finds the k-nearest neighbors (we set k to be 3 in this work), denoted as $N_k(n + 1)$, of $s_{n+1}$. The distance is measured by computing the Euclidean distance [21] between the test sample $s_{n+1}$ and calibration samples on the feature space. We then approximate the true value of $s_{n+1}$ by averaging the distance of k-nearest neighbors, $\\hat{y_{s_{n+1}}} = \\frac{\\sum_{i \\in N_k(n+1)} y_{s_i}}{k}$. The estimated value is then passed to a regression-based nonconformity function to compute the nonconformity score of the test sample. Essentially, we approximate the ground truth by assuming the samples seen at the design time are sufficient to generate an accurate prediction. If this assumption is violated due to drifting test samples, it will likely result in a large residual error (and a greater nonconformity score)."}, {"title": "5.1.2 Computing p-value.", "content": "Prom uses a p-value to assess whether a test sample s fits within the prediction region defined by the calibration dataset, which reflects the training data distribution. To compute the p-value, a subset of calibration samples is selected, their nonconformity scores are adjusted, and these scores are used to derive the p-value for the model's prediction.\nCalibration nonconformity scores. As shown in Figure 6, PROM dynamically selects a subset of calibration samples to adjust the nonconformity score. Specifically, it computes the Euclidean distance between each calibration sample and the test input based on their feature vectors, sorting the samples by distance. By default, the closest 50% of calibration samples are selected. If the dataset contains fewer than 200 samples, all of them are selected; a threshold that can be configured via the PROM API. This nearest subset of calibration samples is chosen to estimate the nonconformity of the test data relative to the training data. These distances are also used as weights to adjust nonconformity scores.\nFor a calibration dataset with n samples, $(a_1, a_2,..., a_n)$, PROM computes nonconformity scores and feature vectors $(v_1, v_2, ..., v_n)$ offline. For a new test sample $s_{n+1}$, it extracts the feature vector $v_{n+1}$, calculates the distances to calibration samples, and selects K samples. The weight $w_i$ for each selected sample i is given by:\n$w_i = exp \\left(-\\frac{||v_i - v_{n+1}||^2}{\\tau}\\right), i \\in \\{1, ..., k\\}$\nwhere $||v_i - v_{n+1}||^2$ is the 12-norm, and \u03c4 is a temperature hyperparameter (default 500). This weight is used to adjust the nonconformity score: $a_{i^{'}} = w_i \\times a_i$.\nP-value for classification. After selecting the calibration samples and adjusting their nonconformity scores, PROM calculates the p-value for each test sample. First, it determines the nonconformity score $a_{y^p}, a_{n+1}$ for the predicted outcome $y^p$. Then, it evaluates the similarity of the test sample to the chosen calibration samples to compute the p-value, $p_{s_i}$, as:\n$p_{s_i} = \\frac{COUNT \\{i \\in \\{1, ..., n\\} : y_i = y^P and a_{y^P}^i \\geq a_{n+1}\\}}{COUNT \\{i \\in \\{1, ..., n\\} : y_i = y^P\\}}$\nThis counts the proportion of calibration samples with the predicted label $y^p$ whose nonconformity scores are \u2265 than the test sample's score. A low p-value (near 1/n) suggests high nonconformity, meaning the test sample is significantly"}, {"title": "5.2 Initialization Assessment", "content": "PROM provides a Python function to evaluate whether the framework is properly initialized at design time after obtaining the calibration dataset (Sec. 4.1.1) and the trained underlying model. This is achieved by computing the coverage rate by performing cross-validation on the holdout calibration dataset. Specifically, PROM automatically splits the calibration dataset R times (R = 3 by default) into two internal datasets for calibration (80%) and validation (20%). It then applies the trained model to the internal validation set and calculates the coverage as:\n$\\frac{1}{Rn_{val}} \\sum_{j=1}^R \\sum_{i \\in val} \\{y_i^{(val)} \\in C (x_i^{(val)})\\} \\approx 1 - \\epsilon$\nwhere $n_{val}$ is the size of the validation set, $y_i^{(val)}$ is the ground truth of the ith validation example, and $C(x_i^{(val)})$ is the prediction region of the ith validation example computed by PROM using the calibration data. The coverage ratio should be approximately the pre-defined significant level, 1 \u2013 \u03b5, with minor fluctuations in deviation [5]. A large deviation indicates an ineffective initialization, which usually stems from a poorly trained or designed underlying model. In this case, PROM will alert the users when the deviation is more than 0.1, enabling them to enhance the underlying model or adjust the significance level during the design time.\nA parameter selection function with a grid search algorithm is provided to help users set the optimal parameters automatically, such as the significant level and cluster size (Sec. 5.1.2). After evaluating the candidate parameters on the validation dataset, PROM will save the selected parameters and use them to predict the confidence in the underlying model at deployment time."}, {"title": "5.3 Credibilty and Confidence Evaluation", "content": "Credibility score. For each nonconformity function, we use the p-value (Sec. 5.1) computed for the predicted class as the credibility score. The higher the p-value is, the more likely the test sample is similar to the training-time samples, hence a higher credibility score.\nConfidence score. PROM estimates the confidence score by evaluating the statistical significance of the prediction using a Gaussian function, $f(x) = e^{-\\frac{(x-1)^2}{2xc^2}}$, where c (default 3) is a constant, and x is the prediction set size for the test sample. The prediction set includes labels likely associated with the test sample, where the nonconformity score exceeds the significance level, 1 \u2013 \u03b5. An empty set suggests the test sample is not linked to any known class, while multiple labels indicate uncertainty, resulting in a low confidence score. As with the credibility score, the prediction set is built from the p-value (Sec. 5.1). Regression tasks apply the same approach, using the labels introduced by clustering (Sec. 5.1.2). According to our prediction with rejections strategy, a sample is flagged as drifting if both scores fall below the significance level."}, {"title": "5.4 Improve Deployment Time Performance", "content": "PROM can enhance the performance of deployed ML systems through incremental learning [4, 30]. For example, suppose a predicted compiler option is likely to be sub-optimal. In that case, the compiler system can use auto-tuning to sample a larger set of configurations to find the optimal one. The idea is to apply other (potentially more expensive) measures to drifting samples. The ground truths can then be added back to the training dataset of the underlying model in a feedback loop for offline retraining. Since model retraining occurs only during instances of data drift, it reduces the overhead associated with the collection of training data.\nAs we will show later, updating a trained model with up to 5% of identified drifting samples significantly enhances robustness post-deployment. The goal is not to reduce training time but to provide a framework for assessing robustness. Without such a system, frequent retraining or risking performance degradation is required. In code optimization tasks, the main expense is labeling data, not training, and by focusing only on mispredicted samples (e.g. for relabeling), our approach reduces labeling overhead and shortens retraining time. By filtering out mispredictions, PROM detects ageing models and supports implementing corrective methods. This, in turn, will improve user experience and trust in ML systems."}, {"title": "6 EXPERIMENTAL SETUP", "content": "Evaluation methodology. As shown in Table 1, we apply PROM to detect drifting samples across 5 case studies, covering 13 representative ML models for classification and regression. We faithfully reproduced all methods following the methodologies in their source publications and used available open-source code. We adhered to the original training methods to ensure comparable design-time results.\nIntroduce data drift. We introduce changes by separating the training and testing data. We try to mimic practical scenarios by testing the trained model on a benchmark suite not used in the training data or code samples newer than the model training data and the PROM calibration dataset. Note that our primary goal is to detect whether PROM can successfully detect drifting samples, not to improve the design of the underlying model.\nPrior practices. Prior work often assumes an ideal scenario by splitting training and test samples at the benchmark or method level, where both sets may share similar characteristics [8]. In contrast, our evaluation introduces data drift to reflect real-world scenarios where workload characteristics change during deployment. As a result, baseline ML models perform worse on test samples than reported in their original publications [13, 31]."}, {"title": "6.1 Case Study 1: Thread Coarsening", "content": "This problem develops a model to determine the optimal OpenCL GPU thread coarsening factor for performance optimization. Following [41], an ML model predicts a coarsening factor (ranging from 1 to 32) for a test OpenCL kernel, where 1 indicates no coarsening.\nUnderlying models. We consider three ML models designed for this problem: a Multilayer Perceptron (MLP) used in [41], a long-short-term memory (LSTM) used in DEEPTUNE [19], and a Gradient boosting classifier (GBC) used in IR2VEC [71]. Like these works, we train and test the models using the labeled dataset from [41], comprising 17 OpenCL kernels from three benchmark suites on four GPU platforms.\nMethodology. As in [19, 82], we train the baseline model using leave-one-out cross-validation, which involves training the baseline model on 16 OpenCL kernels and testing on another one. We then repeat this process until all benchmark suites have been tested once. To introduce data drift, we train the ML models on OpenCL benchmarks from two suites and then test the trained model on another benchmark suite."}, {"title": "6.2 Case Study 2: Loop Vectorization", "content": "This task constructs a predictive model to determine the optimal Vectorization Factor (VF) and Interleaving Factor (IF) for individual vectorizable loops in C programs [34, 47]. Following [34], we explore 35 combinations of VF (1, 2, 4, 8, 16, 32, 64) and IF (1, 2, 4, 8, 16). We use LLVM version 17.0 as our compiler, configuring VF and IF individually for each loop using Clang vectorization directives.\nUnderlying models. We replicate three ML approaches: K.STOCK ET AL. [62] (using SVM), DEEPTUNE [19], and Magni et al. [41], which use neural networks. We use the 6,000 synthetic loops from [34], created by changing the names of the parameters from 18 original benchmarks in the LLVM vectorization test suite. We used the labeled data from [82], collected on a multi-core system with a 3.6 GHz AMD Ryzen9 5900X CPU and 64GB of RAM.\nMethodology. Following [82], we initially allocate 80% (4,800) of loop programs to train the model, reserving the remaining 20% (1,200) for testing its performance. To introduce data drift, we use programs generated from 14 benchmarks for training and evaluate the model on the programs from the remaining 4 benchmarks."}, {"title": "6.3 Case Study 3: Heterogeneous Mapping", "content": "This task develops a binary classifier to determine if the CPU or the GPU gives faster performance for an OpenCL kernel.\nUnderlying models. We replicated three deep neural networks (DNNs) proposed for this task: DEEPTUNE [19], PROGRAML [18], and IR2VEC [71]. We use the DEEPTUNE dataset, comprising 680 labeled instances collected by profiling 256 OpenCL kernels from 7 benchmark suites.\nMethodology. Following [19], we train and evaluate the baseline model using 10-fold cross-validation. This involves training a model on programs from all but one of the subsets and then testing it on the programs from the remaining subset. To introduce data drift, we train the models using 6 benchmark suites and then test the trained models on the remaining suites. We repeat this process until all benchmark suites have been tested at least once."}, {"title": "6.4 Case Study 4: Vulnerability Detection", "content": "This task develops an ML classifier to predict if a given C function contains a potential code vulnerability. Following [75], we consider the top-8 types of bugs from the 2023 CWE [26].\nUnderlying models. We replicated four representative ML models designed for bug detection: CODEXGLUE [40], LINEVUL [28], both based on Transformer networks, and VULDE [39] which based on a Bi-LSTM network. We evaluate this task with a dataset comprising 4,000 vulnerable C program samples labeled with one of the eight vulnerability types, each with around 500 samples. The vulnerable code samples cover 2013 and 2023 and are collected from the National Vulnerability Database (NVD), CVE, and open datasets from GitHub.\nMethodology. As with prior approaches, we initially train the model on 80% of the randomly selected samples and evaluate its performance on the remaining 20% samples. Then, we introduce data drift by training the model on data collected between 2013 and 2020 and testing the trained model on samples collected between 2021 and 2023."}, {"title": "6.5 Case Study 5: DNN Code Generation", "content": "This task builds a regression-based cost model to drive the schedule search process in TVM [16] for DNN code generation on multi-core CPUs. The cost model estimates the potential gain of a schedule (e.g., instruction orders and data placement) to guide the search.\nUnderlying model. We apply PROM to TLP [84], a cost model-based tensor program tuning method integrated into the TVM compiler v0.8 [16]. We use 2, 308 \u00d7 4 samples collected from 4 Transformer-based BERT models of different sizes in the TenSet dataset [85].\nMethodology. For the baseline, we trained and tested the cost model on the BERT-base dataset, where the model is trained on 80% (400K) randomly selected samples and then tested on the remaining 20% (100K)samples. To introduce data drift, we tested the trained model on the other three variants of the BERT model. We ran the TVM search engine for around 8 hours (4,000 iterations) for each DNN on a 12-core 2.7GHz AMD EPYC 9B14 CPU server."}, {"title": "6.6 Performance Metrics", "content": "Performance to the oracle. For code optimization tasks (case studies 1 to 3), we compute the ratio of the predicted performance to the best performance obtained by exhaustively trying all options. A ratio of 1.0 means the prediction leads to the best performance given by an \u201coracle\u201d method.\nMisprediction threholds. For code optimization, we consider a prediction to be a misprediction if runtime performance is 20% or more below the Oracle performance (case studies 1-3) or if predicted performance deviates by 20% or more from profiling results (case study 5). For bug detection (case study 4), a misprediction happens when the model misclassifies a test input.\nCoverage deviation. This \u201csmaller-is-better\u201d metric measures the difference between the confidence level and PROM'S true coverage rate on the model. A zero deviation means the coverage rate matches the predefined significance level.\nMetrics for data drift detection. We consider the following \u201chigher-is-better\u201d metrics for detecting drifting samples:"}, {"title": "7 EXPERIMENTAL RESULTS", "content": "Highlights. Table 2 summarizes the main results of our evaluation. All the tested models were impacted by changes in the application workloads (Sec. 7.1), where the performance relative to the Oracle predictor drops significantly from training time to deployment time. PROM can detect 96.2% of the drifting samples on average (Sec. 7.2). When combined with incremental learning, PROM enhances the performance of deployed models, improving prediction performance by up to 6.6x (Sec. 7.3). PROM also outperforms existing CP-based methods and related work (Sec. 7.5).\n7.1 Impact of Drifting Data\nThis experiment assesses the impact of data drift by applying a trained model to programs from an unseen benchmark suite or CVE dataset (Sec. 6). For code optimization tasks (case studies 1-3 and 5), we report the performance ratio relative to the oracle (Sec. 6.6). In case study 4, we report the accuracy of bug prediction.\nFigure 7 shows the classifiers' performance (case studies 1-4) during design and deployment, while Table 3 presents the regression model's performance (case study 5). The violin diagrams in Figure 7 show the distribution of test sample performance, with the violin's width representing the number of samples in each range. The line inside shows the median, and the top and bottom lines indicate the extremes. Outliers are marked as circles. Ideally, a model's violin would be wide at the top, reflecting good performance for most samples.\nDesign time performance. For case studies 1-4, we assess design-time performance by holding out 10% of the training samples as a validation set and applying the trained model to it. In case study 5, the training data covers all DNN models, but the model is tested on samples from unseen schedules. This process is repeated 10 times, and the average performance is used as the design-time result. This ideal setting assumes that validation and training samples come from the same project or benchmark, with similar workload patterns, yielding comparable results to those reported in the original model publications.\nDeployment time performance. An ML model's robustness can suffer during deployment. From Figure 7, this can be observed from the bimodal distribution of the violin shape or a lower prediction accuracy at the deployment stage. From the violin diagrams, we observe a wider violin shape towards the bottom of the y-axis and a lower median value compared to the design-time result. From Table 3, this also can be seen from a lower deployment-time prediction accuracy than the design-time performance. The impact of drifting samples is clearly shown in vulnerability detection of Figure 7(d), where the prediction accuracy drops by an average of 62.5%. For DNN code generation (Table 3), the accuracy of performance estimation can also drop from 84.5% to as low as 22.4%. The results highlight the impact of data drift."}, {"title": "7.2 Detecting Drifting Samples", "content": "Figure 8 reports PROM's performance in predicting drifting samples across case studies and underlying models. For all tasks, PROM achieves an average precision of 0.86 with an average accuracy of 0.87. This means it rarely filters out correct predictions. For the binary classification task of heterogeneous mapping (Figure 8(c)), PROM achieves an average F1 score of 0.74. In this case, PROM sometimes rejects correct predictions. This is because the probability distribution of binary classification is often less informative for CP than in multiclass cases [64]. For the regression task of case study 5, PROM can detect most of the drifting samples with a recall of 0.95 and an average precision of 1. Furthermore, the underlying model's quality also limits the performance of PROM. When the information given by the underlying model becomes noisy, PROM can be less effective. Averaged across case studies, in detecting mispredictions, PROM achieves a recall of 0.96, a false-positive rate of 0.14 and a false-negative rate of 0.04, suggesting that PROM is highly accurate in detecting drifting samples."}, {"title": "7.3 Incremental Learning", "content": "In this experiment, we use PROM to identify drifting samples and update the underlying models by retraining with a small set of PROM-identified samples. PROM preserves the performance of the methods close to their original levels, as shown by improved accuracy (Figure 9(d)), the performance-to-oracle ratio (Table 3), and violin plots (Figures 9(a) to 9(c)), where test sample distributions shift towards higher performance with a better median than native deployment without PROM. Overall, PROM requires labeling at most 5% (sometimes just one) of drifting samples to update the model. Without it, one would need to label random test samples, leading to higher maintenance costs and unnecessary user confirmations for samples the model can predict correctly."}, {"title": "7.4 Individual Case Studies", "content": "We now examine case studies showing how PROM improves the underlying model with incremental learning.\nCase study 1: thread coarsening. In this experiment", "2": "loop vectorization. This experiment introduced changes by testing the underlying model on loops extracted from unseen benchmarks. Figure 7(b) and 9(b) show that drifting data led to a performance reduction for all methods", "3": "heterogeneous mapping. This experiment tests the underlying models on OpenCl kernels from unseen benchmarks. Figures 7(c) and 9(c) show that all ML models deliver low performance on unseen benchmarks. PROM also successfully detects 100% of the drifting samples (recall) on average with an accuracy rate of 58%. Further", "4": "vulnerability detection. In this experiment, we tested a trained model on a vulnerability"}]}