{"title": "ENHANCING NIGHTTIME VEHICLE DETECTION WITH\nDAY-TO-NIGHT STYLE TRANSFER AND LABELING-FREE\nAUGMENTATION", "authors": ["Yunxiang Yang", "Hao Zhen", "Yongcan Huang", "Jidong J. Yang"], "abstract": "Existing deep learning-based object detection models perform well under daytime conditions but face\nsignificant challenges at night, primarily because they are predominantly trained on daytime images.\nAdditionally, training with nighttime images presents another challenge: even human annotators\nstruggle to accurately label objects in low-light conditions. This issue is particularly pronounced in\ntransportation applications, such as detecting vehicles and other objects of interest on rural roads at\nnight, where street lighting is often absent, and headlights may introduce undesirable glare. This study\naddresses these challenges by introducing a novel framework for labeling-free data augmentation,\nleveraging CARLA-generated synthetic data for day-to-night image style transfer. Specifically, the\nframework incorporates the Efficient Attention Generative Adversarial Network for realistic day-\nto-night style transfer and uses CARLA-generated synthetic nighttime images to help the model\nlearn vehicle headlight effects. To evaluate the efficacy of the proposed framework, we fine-tuned\nthe YOLO11 model with an augmented dataset specifically curated for rural nighttime environments,\nachieving significant improvements in nighttime vehicle detection. This novel approach is simple\nyet effective, offering a scalable solution to enhance AI-based detection systems in low-visibility\nenvironments and extend the applicability of object detection models to broader real-world contexts.", "sections": [{"title": "1 Introduction", "content": "Accurate and reliable vehicle detection is essential for a wide range of transportation applications, such as traffic\nmonitoring and incident management. However, large performance gaps exist for vehicle detection between daytime\nversus nighttime conditions. This especially true for rural environments, where streetlighting is often absent. The\ndisproportionate nighttime fatalities in rural areas compared to urban areas have long been recognized [1]. Nighttime\nvehicle detection presents unique challenges, including limited visibility, unpredictable lighting conditions, and\nlower resolution from standard roadside cameras compared with daytime scenarios. In rural settings, where lighting\ninfrastructure is often sparse or non-existent, these challenges are even more pronounced, coupled with the headlight\nglare that further aggravates the issue [2]. Advances in object detection technology must address these challenges to\noffer reliable performance in such undesirable conditions.\nComputer vision techniques leverage appearance information like color, shape, or typical vehicle patterns to detect\nvehicles from different views achieve good performance [3, 4], but most of these works address the problem during the\ndaytime. At night, above appearance features become invalid, and headlights/taillights are almost the only obvious\nfeatures. Efforts for nighttime vehicle detection have made significant strides in recent years, particularly in applications\nthat utilize ego cameras and roadside cameras.\nEgo cameras on vehicles offer a \"driver\" perspective and are primarily utilized in autonomous driving applications.\nNighttime vehicle detection using ego cameras has been studied due to the relatively higher quality of data and less\nchallenging nature of the tasks. Their primary focus is on accurately identifying nearby vehicles, a task essential for\nreal-time decision-making and navigation for the safety of the ego vehicle. For ego camera based nighttime vehicle\ndetection task, current researches utilize Generative Adversarial Networks (GANs) and image-to-image translation\ntechniques have applied [5\u20137] for enhancing object detection in challenging scenarios such as nighttime and adverse\nweather conditions. Common approaches focus on translating images from a source domain (e.g., daytime) to a target\ndomain (e.g., nighttime) while preserving critical object features. Models like AugGAN [5] and CycleGAN [6\u20138] are\npopular approaches that leverage structure-aware mechanisms to maintain semantic and geometric consistency during\nstyle transfer. Techniques such as semantic segmentation and geometric attention maps [6] further ensure that essential\nobject details are retained, enabling robust object detection performance in the target domain. These models generate\nhigh-quality synthetic datasets that mimic target domain characteristics, which are then utilized to train and fine-tune\nobject detection models, resulting in improved accuracy and robustness.\nIn addition to domain translation, cross-domain learning techniques are integrated to bridge the performance gap between\nsource and target domains. For instance, Convolutional Block Attention Mechanisms (CBAM) [9] enhance detection\naccuracy by focusing on salient image regions, while feature enhancement modules, fuse daytime and nighttime data\nto mitigate ambient light interference. Furthermore, advanced loss functions and data augmentation strategies refine\nmodel training, addressing challenges like reduced visibility and occlusion. Collectively, these methodologies highlight\nthe efficacy of GAN-based frameworks, feature enhancement, and domain adaptation in improving vehicle detection in\nlow-visibility environments.\nWhile the aforementioned methods primarily address challenges in autonomous driving scenarios using data captured\nby ego cameras. However, their applications are limited to the localized environment surrounding autonomous vehicles.\nNetwork-level traffic flow and incident monitoring is typically achieved by roadside cameras operated by state or local\ntransportation agencies. These cameras are commonly mounted on roadside utility poles and capture a broader view\nfor effective traffic monitoring and incident management. Such objectives are critical for enhancing the operational\nefficiency and safety of entire road networks. These roadside cameras play a critical role in enhancing situational\nawareness by detecting incidents and disseminating alerts (e.g. speed warnings or hazard notifications) to drivers,\nthereby improving road safety. However, these camera are frequently characterized by low-resolution images and\npoor video quality. This challenge becomes even more pronounced under adverse conditions, such as nighttime or\ninclement weather, significantly complicating vehicle detection in these environments. Both roadside and ego cameras\nface common challenges in nighttime vehicle detection. These include difficulties in distinguishing vehicles from other\nobjects under low illumination, glare from headlights, and insufficient detail captured by standard imaging sensors.\nSuch limitations are particularly acute in rural settings, where minimal or inconsistent lighting exacerbates detection\ndifficulties. Addressing these challenges is critical to improving the effectiveness of both camera types in their respective\noperational contexts. In this paper, we primarily focus on the nighttime vehicle detection from roadside cameras in rural\nsettings.\nSeveral studies have been conducted to address the challenges of nighttime vehicle detection. Fu et al. [10] proposed a\nframework to improve nighttime object detection accuracy using a StyleMix-based method that generates day-night\nimage pairs for training and a Kernel Prediction Network (KPN) to enhance nighttime-to-daytime image translation.\nWhile this framework aims to adapt models trained on daytime images for nighttime detection, the data used in their\nstudy was captured from a top-down perspective, and the resulting augmented nighttime images fail to accurately\nrepresent real roadside conditions. Specifically, the images lack critical challenges such as low illumination, poor\ncontrast, and the presence of glare from headlights and reflections, which are common in real-world scenarios. Similarly,\nGuo et al. [11] employed CycleGAN to generate nighttime traffic images from daytime data, integrating these images\nwith a Dense Traffic Detection Network (DTDNet) to enhance detection accuracy and address the scarcity of nighttime\nannotations. Nevertheless, their dataset is limited in perspective, as it was collected using phone cameras from specific\nangles. Consequently, the approach does not adequately account for real-world challenges such as low illumination and\nheadlight glare, reducing its effectiveness in more complex and realistic nighttime environments.\nThe suboptimal performance of detection models in nighttime rural scenarios stems from several interconnected\nchallenges. Nighttime environments are characterized by low illumination and poor contrast, which hinder models'\nability to distinguish vehicles from the background and accurately delineate vehicle boundaries. Additionally, intense\nheadlight glare and reflections often confuse models, as these bright spots can obscure objects of interest or be\nmisinterpreted as vehicles. With these issues, nighttime images frequently suffer from noise, motion blur, and low\nresolution, resulting from reduced sensor performance in low-light conditions. This degradation in data quality further\nimpacts model accuracy and reliability. Another significant limitation is the scarcity of large, diverse, and annotated\nnighttime datasets. Most existing datasets are predominantly consist of daytime images, leading to an imbalance that\nprevents models from effectively generalizing to nighttime conditions. Furthermore, domain adaptation remains a"}, {"title": "2 Framework", "content": "Our proposed framework, illustrated in Figure 1, introduces a novel labeling-free data augmentation method that enables\nrealistic day-to-night image style transfer using synthetic data generated by CARLA [15]. The framework comprises\ntwo main components:\n1. Synthetic nighttime data generation under rural settings: This component leverages the CARLA simulator\nto generate synthetic nighttime images that incorporates realistic headlight effects and varying illumination\nconditions, as observed from roadside cameras in rural environments. The CARLA simulator is integral to this\nprocess, as it can faithfully model vehicle headlight effects at night, effectively addressing the limitations of\nexisting AI models that often fail to capture headlight effects during day-to-night style transfers.\n2. Day-to-night style transfer process: To address data scarcity of nighttime road scene images in rural environ-\nments, a CycleGAN model is trained to perform day-to-night style transfer. Daytime images are collected and\nprocessed using the state-of-the-art YOLO11 model [16] to perform vehicle detection and classification. The\nresulting annotations are directly mapped to the style-transferred nighttime images, enabling the creation of an\naugmented nighttime dataset without additional labeling effort. To enhance dataset diversity and realism, the\nfinal augmented dataset combines human-labeled real nighttime low-light images (44%) with style-transferred\nimages (56%). This dataset is subsequently used to fine-tune the YOLO11 model, which is evaluated against\nits raw counterpart on a real-world nighttime test dataset.\nBy combining realistic synthetic data generation with effective style transfer techniques and automated annotation\nmapping, our framework addresses critical challenges in rural nighttime vehicle detection, offering a novel and practical\nsolution to improve model performance in real-world scenarios."}, {"title": "3 Method", "content": "This section introduces our proposed method, which addresses the challenges of nighttime vehicle detection in\nrural environments through three key steps: (1) Synthetic Nighttime Data Generation: The process of generating\nrealistic nighttime images is described, where the CARLA simulator is utilized to incorporate critical features such as\nheadlight effects and varying illumination conditions. (2) Day-to-Night Image Style Transfer: The model architecture\nemployed for performing day-to-night image translation is presented, enabling the creation of nighttime images that\nclosely resemble real-world scenarios. (3) Labeling-Free Data Augmentation for Nighttime Images: The approach for\nachieving labeling-free augmentation is described, where annotations from daytime images are directly mapped onto\nstyle-transferred nighttime images, facilitating the development of a robust augmented dataset."}, {"title": "3.1 Synthetic nighttime data generation", "content": "As discussed in the previous section, the primary challenges in improving nighttime vehicle detection arise from the low\nquality of roadside camera images and the difficulty of collecting sufficiently large and diverse datasets. To address these\nissues, synthetic nighttime images are generated using CARLA [15], a widely used open-source platform primarily\ndesigned for autonomous driving research. CARLA offers extensive control over various environmental and operational\nparameters, such as weather conditions, lighting, vehicle types, headlight settings (e.g., low-beam, high-beam), as well\nas camera positions and viewing angles. These customizable options enable the creation of a comprehensive and diverse\ndataset that accurately reflect real-world rural transportation settings. In particular, for rural highway safety research,\nthe simulator allows for the strategic placement of cameras in critical locations, such as curves and ramps, where lower\nspeed limits are often imposed [17].\nTo closely mimic realistic rural environments, synthetic images were collected under the following scenarios: (1)\ndeparting and approaching vehicles relative to cameras, (2) side-view and top-view perspectives, and (3) scenes with\nmultiple vehicles and single vehicles. Several representative examples are presented in Figure 2. It is important to\nnote that, in this study, all synthetic images were generated under clear weather conditions with no environmental\nmodifications."}, {"title": "3.2 Day-to-night image style transfer", "content": "The Efficient Attention GAN (EAGAN) [18] builds upon the CycleGAN framework by integrating efficient attention\nblocks into the generator networks while enabling attention sharing between corresponding encoder and decoder\nblocks. This mechanism allows the re-utilization of the long-range dependencies computed from the source-domain\nimages during the reconstruction of their target-domain counterparts. This design makes EAGAN a robust choice for\nhigh-quality image-to-image (I2I) translation tasks, particularly in scenarios where maintaining consistency between\ndomains is critical.\nIn this study, the EAGAN architecture is adopted to perform day-to-night style transfer in rural environments. The\nmodel is trained using datasets from two specific domains: real-world daytime images and CARLA virtual nighttime\nimages.\nThe I2I translation task generally considers transforming image x from domain X (daytime) to image y in domain Y\n(nighttime), represented as mappings: $G: x \\rightarrow y, F : y \\rightarrow x$, where G and F are generator networks. The objective is\nto ensure that the distributions G(X) and F(Y) are indistinguishable from X and Y, respectively, while preserving\nsemantic information and cycle consistency.\nTo train the EAGAN for the data augmentation purpose from daytime images and CARLA nighttime images, the model\ninput consists of {Real X, Real Y}, where Real X, and Real Y are from the domains X and Y. The detailed information\nflow is shown in Figure 3. Following the standard training process for GANs, the discriminators and generators are\ntrained simultaneously by optimizing a min-max adversarial objectives. Instead of the traditional adversarial loss, the\nleast-square adversarial loss proposed by [19] was used due to improved stability, which also encourages the generator\nto produce realistic images indistinguishable by the discriminator:\nmin $L_{GAN}(G) = E_{z \\sim P_{data(x)}} [(D(G(x)) - 1)^2]$                                                         (1)\nWhere $P_{data(x)}$ denotes the true data distribution of domain X, comprising real daytime images. D(G(x)) represents\nthe output of the discriminator, which assigns a score between 0 and 1. Ideally, D(G(x)) should approach 1 if the\ngenerated nighttime image appears realistic. The term $(D(G(x)) \u2013 1)^2$ imposes a penalty on the generator if the\ndiscriminator fails to classify G(x) as realistic, i.e., when the score deviates from 1.\nmin $L_{GAN} (D) = E_{y\\sim P_{data(y)}} [(D(y) - 1)^2] - + E_{x\\sim P_{data(x)}} [D(G(x))^2]$                        (2)\nWhere $P_{data(y)}$ denotes the true data distribution of domain Y, i.e., CARLA nighttime images. The term\n$E_{y\\sim P_{data(y)}} [(D(y) - 1)^2]$ ensures that the discriminator assigns a score close to 1 to authentic CARLA nighttime\nimages from domain Y. Conversely, $E_{x\\sim P_{data(x)}} [D(G(x))^2]$ penalizes the discriminator if it assigns a high score to\nthe nighttime images G(x) generated from real daytime images x. This dual mechanism maintains the discriminator's\nreliability in distinguishing between real and generated samples."}, {"title": "3.3 Labeling-free data augmentation for nighttime images", "content": "The You Only Look Once (YOLO) family of models has revolutionized object detection, offering real-time detection\nand high accuracy. YOLO11, the latest version [16], builds upon this legacy with attention mechanisms, deeper\nfeature extraction layers, and an anchor-free detection approach. It is specifically designed to address challenges\nsuch as detecting small, occluded, or fast-moving vehicles. By integrating strengths of CNNs and self-attention\nmechanism, YOLO11 improves both detection accuracy and computational efficiency, making it well-suited for real-\nworld applications. In our study, YOLO11 is applied as an \"annotator\" to automatically annotate daytime images. The\nobtained labels from daytime images are directly applied to the style-transferred nighttime images as the objects of\ninterest (i.e., vehicles) will remain in same locations. This allows us to leverage the accurate vehicle detection capability\nof the YOLO11 model to automatically obtain labels for its style-transferred nighttime counterparts."}, {"title": "4 Experiments", "content": "Our data was gathered from multiple public traffic cameras in California, which includes both daytime and nighttime\nimages, serving distinct purposes for training and testing. Specifically, the data is organized into three categories: (1)\ntraining datasets for the EAGAN model, (2) fine-tuning datasets for the YOLO11 model, and (3) an evaluation dataset\nfor comparing the performance of the original YOLO11 model and the fine-tuned version.\nA detailed summary of each dataset is provided in Table 1."}, {"title": "4.2 Image style transfer", "content": "For the image style transfer with EAGAN, we targeted two domains: domain X, which consists of daytime images in\nreal-world settings, and domain Y, comprising CARLA-generated nighttime images. The EAGAN was trained for 200\nepochs with scheduled learning rate (Equation 6) that was initialized at 0.0002 and started to decrease linearly after the\n100th epoch. This learning rate decay strategy ensure smooth model convergence, leading to better generalization and\nperformance [22].\n$lr(t) =\n    \\begin{cases}\n        lr_0 & \\text{if } t \\leq n_{epochs} \\\\\n        lr_0.( 1 - \\frac{t-n_{epochs}}{N_{epochs\\_decay}} ) & \\text{if } n_{epochs} < t \\leq N_{epochs} + N_{epochs\\_decay} \\\\\n        0 & \\text{if } t > n_{epochs} + N_{epochs\\_decay}\n    \\end{cases}$\n                                                                                                                            (6)\nWhere $lr_0$ = initial learning rate t = current epoch $n_{epochs}$ = the number of epochs that the learning rate decay starts\n$N_{epochs\\_decay}$ = the number of epochs that the learning rate decay ends.\nTable 2 shows the detailed parameter settings for EAGAN training.\nEach epoch takes approximately 150 seconds, with the entire training process completing in about 8 hours on a\nsingle NVIDIA A6000 GPU. Figure 4 showcases test examples from our trained EAGAN model. The results confirm\nsuccessful day-to-night translation, including effective addition of headlight features. Notably, the model accurately\nplaces headlights in the correct locations of the vehicles, demonstrating its ability to reliably locating the vehicles and\nidentifying their positions. Interestingly, some shadow-related effects were observed:\n1. Vehicle Shadows under Sunlight: For shadows cast by vehicles in sunny conditions (e.g., rows 1 and 2 in\ncolumns 5 and 6 of Figure 4), the model tends to interpret the shadow in front of the car's bumper as part of the\nvehicle. This results in a slight angular misalignment between the illuminated headlights and the front of the\ncar in the transferred image. However, this minor deviation does not impair the model's ability to recognize\nvehicles at night."}, {"title": "4.3 Nighttime vehicle detection and classification", "content": "For this experiment, the YOLO11-Small model was employed. Figure 5 showcase sample predictions generated by the\noriginal YOLO11-Small model, which serve as labels for their style-transferred nighttime images.\nAlthough CARLA can generate realistic nighttime road scene images, there are still subtle differences in appearance\ncompared to real-world nighttime road scenes. To address this domain adaption gap, we incorporated a selection of\nmanually annotated real-world nighttime images into the training dataset. This approach allows the model to learn\nrelevant features from both CARLA-generated and real-world nighttime images, enhancing its overall performance and\nrobustness.\nTo fine-tune the model, a learning rate scheduling strategy was implemented for different components of the model.\nInitailly, the backbone was fine-tuned with a learning rate of 0.0001 for 50 epochs. Subsequently, the backbone network\nwas frozen, and a learning rate of 0.00005 was applied exclusively to the neck network for another 50 epochs. Finally,\nboth the backbone and neck networks were frozen, and a learning rate of 0.00001 was applied to the head network\nfor an additional 50 epochs. This block-wise adaptation strategy, distinct from the approach used in EAGAN training,\nfacilitates enhanced convergence and improved generalization.\nFor evaluation, a selection of representative real-world nighttime images captured by roadside traffic cameras in rural\nareas was analyzed, as shown in Figures 6\u20138. These nighttime images present various challenges, including low\nambient light, poor image quality, and issues caused by headlight glare. The original YOLO11 model frequently\nstruggled to distinguish vehicles from the background, even under relatively favorable lighting conditions, and often\nproduced low confidence scores when vehicles were detected. In contrast, the fine-tuned YOLO11 model, trained on the\naugmented dataset, achieved a 100% detection success rate, with significantly higher confidence scores, demonstrating\nthe effectiveness of the proposed framework.\nTable 3 presents detailed classification results across various metrics for the original and fine-tuned YOLO11 models:\nTable 3 reveals significant improvements across classes, indicating that the fine-tuned model effectively captures most\nvehicles in the nighttime scenes, addressing the key limitation of current state-of-the-art object detection models. The\nConsistent gains in mAP metrics further highlight the fine-tuned model's robustness in detecting and localizing vehicles\nunder challenging nighttime conditions. Class-specific refinements enhance detection for both smaller vehicles (Sedan)\nand larger ones (SVP-BV). Notably, the Fine-tuned model shows a slightly lower bounding box precision for the\nSVP-BV class, largely due to the diverse mix of vehicle types in this newly defined class."}, {"title": "5 Conclusion", "content": "In this work, we proposed a novel framework for enhancing nighttime vehicle detection, featuring a labeling-free\nmethod to create an augmented dataset to fine-tuning object detection models with improve performance for nighttime\nconditions. We employed the EAGAN as the image translator to generate corresponding nighttime images from their\ndaytime counterparts. Additionally, we adopted different learning rate scheduling strategies during EAGAN training\nand YOLO11 fine-tuning to ensure smooth convergence and enhanced generalization. A performance comparison\nbetween the original YOLO11 model and the fine-tuned version demonstrated that the YOLO11 model fine-tuned with\nthe augmented dataset significantly outperformed the original YOLO11 model for nighttime vehicle detection. Its\nability to detect and localize the vehicles with high confidence highlights the effectiveness of fine-tuning with properly\naugmented data, making it a more reliable solution for real-world applications.\nNevertheless, we acknowledge several limitations that should be addressed in future research: (1) While CARLA\ncurrently offers a wide variety of vehicle types, it still lacks coverage of all vehicle types on the road, particularly the\ntractor-trailers and RVs, which limits the diversity of synthetic data. Additionally, the headlights in CARLA need\nfurther refinement to better replicate glare effects observed in real-world settings. (2) Although the EAGAN model\nincorporates an attention-sharing mechanism in the generators of CycleGAN, future research could explore alternative\nmechanisms to more effectively address the observed shadow effects. (3) For proof of the concept, the training and\ntesting datasets utilized in this study are relatively small. Future work should consider significantly expanding the\ndatasets using our proposed data augmentation approach, which is expected to further enhance model performance and\nrobustness."}]}