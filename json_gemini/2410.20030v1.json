{"title": "SCube: Instant Large-Scale Scene Reconstruction using VoxSplats", "authors": ["Xuanchi Ren", "Yifan Lu", "Hanxue Liang", "Zhangjie Wu", "Huan Ling", "Mike Chen", "Sanja Fidler", "Francis Williams", "Jiahui Huang"], "abstract": "We present SCube, a novel method for reconstructing large-scale 3D scenes (geometry, appearance, and semantics) from a sparse set of posed images. Our method encodes reconstructed scenes using a novel representation VoxSplat, which is a set of 3D Gaussians supported on a high-resolution sparse-voxel scaffold. To reconstruct a VoxSplat from images, we employ a hierarchical voxel latent diffusion model conditioned on the input images followed by a feedforward appearance prediction model. The diffusion model generates high-resolution grids progressively in a coarse-to-fine manner, and the appearance network predicts a set of Gaussians within each voxel. From as few as 3 non-overlapping input images, SCube can generate millions of Gaussians with a $1024^3$ voxel grid spanning hundreds of meters in 20 seconds. Past works tackling scene reconstruction from images either rely on per-scene optimization and fail to reconstruct the scene away from input views (thus requiring dense view coverage as input) or leverage geometric priors based on low-resolution models, which produce blurry results. In contrast, SCube leverages high-resolution sparse networks and produces sharp outputs from few views. We show the superiority of SCube compared to prior art using the Waymo self-driving dataset on 3D reconstruction and demonstrate its applications, such as LiDAR simulation and text-to-scene generation.", "sections": [{"title": "1 Introduction", "content": "Recovering 3D geometry and appearance from images is a fundamental problem in computer vision and graphics which has been studied for decades. This task lies at the core of many practical applications spanning robotics, autonomous driving, and augmented reality; just to name a few. Early algorithms tackling this problem use stereo matching and structure from motion (SfM) to recover 3D signals from image data (e.g. [43]). More recently, a line of work starting from Neural Radiance Fields [31] (NeRFs) has augmented traditional SfM pipelines by fitting a volumetric field to a set of images, which can be rendered from novel views. NeRFs augment traditional reconstruction pipelines by encoding dense geometry, and view-dependent lighting effects. While radiance-field methods present a drastic step forward in our ability to recover 3D information from images, they require a time-consuming per-scene optimization scheme. Furthermore, since each scene is recovered in isolation, radiance fields do not make use of data priors, and cannot extrapolate reconstructions away from the input views. Thus, radiance-field methods require dense view coverage in order to produce high-quality 3D reconstructions."}, {"title": "2 Related Work", "content": "3D Scene Representation. Scenes in the wild are often large in scale and contain complicated internal structures which cause representations such as tri-planes [12], dense voxel grids [35], or meshes [19, 45] to fail due to capacity or memory limitations. Optimization-based reconstruction methods [15, 31] use high-resolution hash grids [1, 32], but these are non-trivial to infer using a neural network [29]. In contrast, sparse voxel grids are effective for learning scene-reconstruction [39, 73] thanks to efficient sparse neural operators [8, 53]. Recently, Gaussian splatting [22] has enabled real-time neural rendering and has been applied to overfitting large scenes [64, 74]. [30, 38] use a hybrid of the above two representations, but the voxel grid or octree is only used to regularize the Gaussian positions without any data priors learned. This is in contrast to our VoxSplat that allows reconstruction in a direct inference pass thanks to the efficiency of sparse grids and the high representation power of Gaussian splats. We support operating only on sparse-view images, significantly lifting the input requirements by learning from large datasets.\nSparse-view 3D Reconstruction. Sparse-view images often contain insufficient correspondences required by traditional reconstruction methods [43]. One line of work uses learned image-space priors such as depth [9], normal maps [68], and appearance from GANs [42] or diffusion models [61] to augment an optimization process such as NeRF. To speed up inference, another line of work uses a feed-forward model to predict renderable features [4, 6, 28, 56, 71]. Alternatively, some papers perform learning directly in 3D space, which yields better consistency and less distortion [5, 13, 17, 66]. Our setting is similar to [13] where input images come from the same rig, but ours is more challenging since we do not use temporally-sequenced inputs with high overlap. We remark that semantic scene completion works [21, 25, 50, 60] also reconstruct voxels but at much lower resolutions and without appearance.\nGenerative Models for 3D. 3D reconstruction can also be formulated as a conditional generative problem (i.e.modeling the distribution of scenes given partial observations). Text and single-image to-3D generation has been explored for objects [17, 27, 37, 46, 54, 55, 59, 67]. Extending this task to large-scenes is comparatively unexplored, and object-based methods often fail due to scaling limitations or assumptions on the data. [47, 70] recursively apply an image generative model to inpaint missing regions in 3D, but produces blurry reconstructions at a limited scale. XCube [39] is among the first to directly learn high-resolution 3D scene priors. Here, we extend this model with multiview image conditioning and enable it to predict appearance on top of geometry."}, {"title": "3 Method", "content": "Our method reconstructs a high-resolution 3D scene from N sparse images $I = \\{I_i\\}_{i=1}^N$ in two stages: (1) We reconstruct the scene geometry represented as a sparse voxel grid G with semantic features (\u00a7 3.1). (2) We predict the appearance A of the scene that allows for high-quality novel view synthesis (\u00a7 3.2) using VoxSplats and a sky panorama. We can express our pipeline as taking samples from the distribution $p(G, A|I) = p(A|G,I)p(G|I)$. In order to improve the final view quality of the output, we apply an optional post-processing step discussed in \u00a7 3.3."}, {"title": "3.1 Voxel Grid Reconstruction", "content": "Background: 3D Generation with XCube. XCube [39] is a 3D generative model that produces high-quality samples for both objects and large outdoor scenes. XCube uses a hierarchical latent diffusion model to generate sparse voxel hierarchies, i.e., a hierarchy of sparse voxel grids where each fine voxel is contained within a coarse voxel. XCube learns a distribution over latent X encoded by a sparse structure Variational Autoencoder (VAE). Both the VAE and the diffusion model are instantiated with sparse convolutional neural networks [14], and can generate geometry at up to $1024^3$ resolution. We use XCube as the backbone for our geometry reconstruction module. We remark that while the original paper only focused on unconditional or text-conditioned generation, we introduce a novel image-based conditioning C.\nImage Conditioned Geometry Generation. To condition XCube on posed input images, we lift DINO-v2 [33] features computed on the input images to 3D space as follows. First, we use the pre-trained DINOv2 model to extract robust visual features for input images, and process the DINO feature using several trainable 2D conv layers to reduce the feature channel to C + D. We then split the channel C + D into two parts for each pixel j and input image index i: one part is a regular C-dimensional feature F and the other will be a D-dimensional Softmax-normalized vector $0 \\in \\mathbb{R}^D$. Here 0 can be viewed as a distribution over the depth of the corresponding pixel, and we follow a strategy similar to LSS [36] to unproject the images into a dense 3D voxel grid \u03a9 where v denotes the index of a voxel and d\u2208 [1, D] indexes the depth buckets:\n$F_v = F, C= \\Sigma F_{jd} \\in \\mathbb{R}^C$. (1)\nNote that we quantize the depth into D bins dividing the range from a predefined near to far. Unlike image-conditioning techniques used in object-level or indoor-level datasets where the camera frusta have significant overlap, our large-scale outdoor setting only takes sparse low-overlapping views captured from an ego-centric camera. Hence previous methods [27, 49, 51] that broadcast the same features to all the voxels along the rays corresponding to the pixel are not suitable here to precisely locate the geometries such as vehicles. The use of the weight @ allows us to handle occlusions effectively and produce a more accurate conditioning signal. After building C, we directly concatenate it with the latent X and feed it into the XCube diffusion network as conditioning.\nTraining and Inference. Our training pipeline is similar to [39], where we first train a VAE to learn a latent space over sparse voxel hierarchies. We add semantic logit prediction as in [39] to the grid and empirically find that it helps the model to learn better geometry. Then we train the diffusion model conditioned on C using the following loss:\n$L = L_{Diffusion} + \\lambda L_{Depth}, L_{Depth} = E_{x,i,j}Focal(0, [0]_{gt}),$ (2)\nwhere $L_{Diffusion}$ is the loss for diffusion model training (see Appendix A for details). Focal(\u00b7) is the multi-class focal loss [26]. This additional depth loss is an explicit supervision to properly weigh the image features and encourage correct placement into the corresponding voxels. Due to the generative nature of XCube, we could learn the data prior to generate complete geometry even if some of the ground-truth 3D data is incomplete."}, {"title": "3.2 Appearance Reconstruction", "content": "The VoxSplat Representation. In the second stage, we fix the voxel grid G generated from the geometry stage and predict a set of Gaussian splats in each voxel to model the scene appearance. Gaussian splatting [22] is a powerful 3D representation technique that models a scene's appearance volumetrically as sum of Gaussians:\n$G(x) = RGB \\cdot a \\cdot \\varepsilon^{-(x-\\mu)^T\\Sigma^{-1}(x-\\mu)},$ (3)\nwhere a \u2208 [0, 1] is the opacity, \u03bc\u2208 $\\mathbb{R}^3$ is the center of each Gaussian, and \u2211 = RSSTRT \u2208 $\\mathbb{R}^{3\u00d73}$ is its covariance. The covariance matrix is factorized into a rotation matrix R parameterized by a quaternion q and a scale diagonal matrix S = diag(s). Each Gaussian additionally stores a color value RGB. Note that the original paper uses a set of SH coefficients for view-dependent colors, but we only use the 0th-order SH in our model (i.e., without view-dependency) which we found to be sufficient for sparse-view reconstruction.\nWhile the original Gaussian Splatting paper and its follow-ups [22, 65, 69] propose many heuristics to optimize the positions of Gaussians for a given scene, we instead choose to predict M Gaussians per-voxel using a feed-forward model. We limit the positions of the Gaussians within a neighborhood of their supporting voxels, thus preserving the geometric structure of the supporting grid. By grounding the splats on a voxel scaffold, our reconstructions achieve better geometric quality without resorting to heuristics. Fittingly, we dub our voxel-supported Gaussian splats VoxSplats.\nThe output of our network is \\{[$\\mu_\\upsilon$, $\\tilde{a}_\\upsilon$, $\\delta_\\upsilon$, $\\tilde{q}_\\upsilon$, RGB] \u2208 \\mathbb{R}^{14}\\}_M for each voxel v. To compute the per-Gaussian parameters used for rendering we apply the following activations:\n$\\mu_\\upsilon = r \\cdot tanh \\tilde{\\mu} + Center, a_\\upsilon = sigmoid(\\tilde{a}_\\upsilon), s_\\upsilon = exp \\delta_\\upsilon, R = quat2rot(q_\\upsilon)$, (4)\nwhere Center is the centroid of the voxel v, and r is a hyperparameter that controls the range of a Gaussian within its supporting voxel. Here, we set r to three times the voxel size. We can efficiently render the Gaussians predicted by our model using rasterization [22] or raytracing [11].\nSky Panorama for Background. To capture appearance away from the predicted geometry, our model builds a sky feature panorama L \u2208 [$\\mathbb{R}^{H_p\u00d7W_p\u00d7C_p}$] from all input images, which can be considered as an expanded unit sphere with an inverse equirectangular projection. For each pixel in the panorama L, we get its cartesian coordinate P = (x, y, z) on the unit sphere and project P to the image plane to retrieve the image feature; since only the view direction decides the sky color, we zero the translation part of the camera pose in the projection step. We also apply a sky mask to ensure the panorama only focuses on the sky region.\nTo render a novel viewpoint with its extrinsics and intrinsics, we recover the background appearance by sampling the sky panorama and decoding it into RGB values. For each camera ray from the novel view, we calculate its pixel coordinate on the 2D sky panorama L with equirectangular projection and get the sky features via trilinear interpolation, resulting in a 2D feature map for the novel view. We finally decode the 2D feature map with a CNN network to get the background image $I_{bg}$, which will be alpha-composited with the foreground image from Gaussian rasterization:\n$I_{pred} (u, v) = I_{Gs} (u, v) + (1 - T(u, v)) \\cdot I_{bg} (u, v)$ (5)\nwhere $I_{Gs} (u, v)$ is the rendered image of Gaussians, (u, v) indicates the pixel coordinate, and T(u, v) is the accumulated transmittance map of the Gaussians (see [22] for details).\nArchitecture Details. We predict the (M \u00d7 14)-dimensional vector \\{[$\\mu_\\upsilon$, $\\tilde{a}_\\upsilon$, $\\delta_\\upsilon$, $q_\\upsilon$, RGB]\\}_M for each voxel via a 3D sparse convolutional U-Net which takes as input the sparse voxel grid \u03a9 outputted by the geometry stage, where each voxel contains a feature sampled from the input images as follows: We process each input image $I_i$ using a CNN to get the image feature, and then cast a ray from each image pixel into \u03a9, accumulating the feature in the first voxel intersected by that ray. Voxels that are not intersected by any rays receive a zero feature vector.\nFor the sky panorama model, we use the same image feature as above. In the training stage, we set smaller $H_p$ and $W_p$ for faster training and lower memory usage; in the inference stage, we increase $H_p$ and $W_p$ to get a sharper and more detailed sky appearance.\nGiven a set of training images \\{$I_i$\\} and sky masks \\{$M^i$\\}, distinct from the inputs, we supervise the appearance model using the loss:\n$L = \\lambda_1L_1(I_{pred}, I_{gt}) + \\lambda_2L_1(T^i, M^i) + \\lambda ASSIMLSSIM (I_{pred}, I_{gt}) + \\lambda_{LPIPS}LLPIPS (I_{pred}, I_{gt}),$ (6)\nwhere the training views $I_{gt}$ are sampled from nearby 10 views of the input images; the predicted views $I_{pred}$ and transmittance masks $T^i$ are rendered using Eq (5); and $L_{LPIPS}/L_{SSIM}$ are perceptual and structural metrics defined in [72] and [58]."}, {"title": "3.3 Postprocessing and Applications", "content": "Optional GAN Postprocessing. The novel views directly rendered from our appearance model sometimes suffer from voxelization artifacts or noise. We resolve this with an optional lightweight conditional Generative Adversarial Network (GAN) that takes the rendered images as input and outputs a refined version. The discriminator of this GAN takes 256 \u00d7 256 image patches sampled from the input sparse view images, as well as the generated images conditioned on the rendered images. Drawing inspiration from [40, 42, 44], we fit the GAN independently for each scene at inference time, which takes ~20min to train. Due to the excessive time cost, we apply this step optionally only when higher-quality images are needed (which we call SCube+).\nApplication: Consistent LiDAR Simulation. LiDAR simulation [75] aims at reproducing the point cloud output given novel locations of the sensor and is an important application for training and verification of autonomous driving systems. The generated LiDAR point clouds should accurately reflect the underlying 3D geometry and a sequence of LiDAR scans should be temporally consistent. Our method enables converting sparse-view images directly into LiDAR point clouds, i.e., a sensor-to-sensor conversion scheme.\nApplication: Text-to-Scene Generation. Our method can be easily extended to generate 3D scenes from text prompts. Similar to MVDream [46], we train a multi-view diffusion model with the architecture of VideoLDM [2] that generates images from text prompts. The original spatial self-attention layer is inflated along the view dimension to achieve content consistency [24, 63]."}, {"title": "4 Experiments", "content": "In this section, we validate the effectiveness of SCube. First, we present our new data curation pipeline that produces ground-truth voxel grids (\u00a7 4.1). Next, we demonstrate SCube's capabilities in scene reconstruction (\u00a7 4.2), and further highlight its usefulness in assisting the state-of-the-art Gaussian splatting pipeline (\u00a7 4.3). Finally, we showcase other applications of our method (\u00a7 4.4) and perform ablation studies to justify our design choices (\u00a7 4.5)."}, {"title": "4.1 Dataset Processing", "content": "Accurate 3D data is essential for our method to learn useful geometry and appearance priors. Fortunately, many autonomous driving datasets [3, 52] are equipped with 3D LiDAR data, and one can simply accumulate the point clouds to obtain the 3D scene geometry [20, 39]. However, the LiDAR points usually do not cover high-up regions such as tall buildings and contain dynamic (non-rigid) objects that are non-trivial to accumulate.\nWe hence build a data processing pipeline based on Waymo Open Dataset [52] as shown in Fig. 3, consisting of three steps: Step 1, we accumulate the LiDAR points in the world space, removing the points within the bounding boxes of dynamic objects such as cars and pedestrians. We additionally obtain the semantics of each accumulated LiDAR point, where non-annotated points are assigned the semantics of their nearest annotated neighbors. Step 2, we use the multi-view stereo (MVS) algorithm available in COLMAP [43] to reconstruct the dense 3D point cloud from the images, and the semantic labels of the points are obtained by Segformer [62]. Step 3, we add point samples for the dynamic objects according to their bounding boxes at a given target frame."}, {"title": "4.2 Large-scale Scene Reconstruction", "content": "Evaluation and Baselines. To assess our method's power for 3D scene reconstruction, we follow the common protocol to evaluate the task of novel view synthesis [4, 41, 66]. Given input multi-view images (details about choosing views are in Appendix A) at frame T, we render novel views at future timestamps T + 5 and T + 10, and compare them to the corresponding ground-truth frames by calculating Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Measure (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS) [72]. We exclude the regions of moving objects for T + 5 and T + 10 evaluation, and only use three front views when computing the metrics.\nWe use PixelNeRF [66], PixelSplat [4], DUSt3R [56], MVSplat [6], and MVSGaussian [28] as our baselines for comparison. [4, 6, 28, 66] take images and their corresponding camera parameters as input and reconstruct NeRFs or 3D Gaussian representations. DUSt3R [56] directly estimates per-pixel point clouds from the images. We append additional heads to the its decoder which predicts other 3D Gaussian attributes along with the mean positions and finetune it with a rendering loss. For all other baselines, we take the official code and re-train them on our dataset. We tried to add the state-of-the-art depth estimator Metric3Dv2 [18] for depth supervision but empirically found that the performance degraded.\nResults and Analysis. We show our results quantitatively in Tab. 1 and visually in Fig. 4. Our method outperforms all the baselines for both the current frame (reconstruction) and future frames (prediction) by a considerable margin on all metrics. PixelNeRF is limited by the representation power of the network, and simply fails to capture high-frequency details in the scene. PixelSplat highly relies on overlapping regions in the input views and cannot adapt well to our sparse view setting. It fails to model the true 3D geometry as shown in the top-down view, and simply collapses the images into constant depths. The multi-view-stereo-based methods [6, 28] cannot enable extreme novel view synthesis such as the bird-eye view, and could not recover highly-occluded regions. Thanks to the effective pre-training of DUSt3R, it is able to learn plausible displacements in the image domain, but the method still suffers from missing regions, misalignments, or inaccurate depth boundaries. In contrast, our method can reconstruct complete scene geometry even for far-away regions. It is both accurate and consistent while producing high-quality novel view renderings.\nTo better demonstrate the power of learning priors in 3D, we build another baseline using the state-of-the-art metric depth estimator Metric3Dv2 [18] to unproject the images into point clouds using 2D learned priors. As shown in Fig. 5, our method can reconstruct more complete, uniform, and accurate scenes, justifying the power of representing and learning geometry directly in the true 3D space."}, {"title": "4.3 Assisting Gaussian Splatting Initialization", "content": "Our method creates scene-level 3D Gaussians with accurate geometry and appearance, which can be used to initialize large-scale 3D Gaussian splatting [22] training. This is particularly useful in outdoor driving scenarios where structure-from-motion (SfM) may fail due to the sparsity of viewpoints.\nTo demonstrate this, we consider and compare three initialization methods: Random initialization is where points are uniformly sampled within the range of (-20m, 20m)\u00b3 around each camera. Metric3Dv2 initialization is where we use the unprojected cloud from Metric3Dv2 [18]'s monocular depth and align its scale to metric-scale LiDAR. SCube (ours) initialization directly adopts the positions and colors of the Gaussians from our pipeline. For input to these methods, we choose the views from the first frame T and control the number of initial points to 200k. We then incorporate R subsequent frames into the full training, with every 3 frames skipped to be used in the test set. The number of training iterations is fixed to 30k and the initial positional learning rate is set to 1.6e-5. We select 15 static scenes for experiments and report their average metrics.\nThe results consistently demonstrate SCube's effectiveness as an initialization strategy that provides accurate 3D grounding and alleviates overfitting on the training views."}, {"title": "4.4 Other Applications", "content": "We demonstrate the applications of our method as described in \u00a7 3.3. Fig. 6 shows the consistent LiDAR simulation results, where the simulated sequences could effectively cover a long range away from the input camera positions, while resolving intricate geometric structures such as buildings, trees, or poles. Fig. 7 exemplifies the text-to-scene generation capability enabled by our method. The 3D geometry and appearance respect the input text prompt and the corresponding images."}, {"title": "4.5 Ablation Study", "content": "Image-Conditioning Strategy. We replace the image conditioning strategy described in Eq (1) in the voxel grid reconstruction stage with a vanilla scheme that broadcasts the same feature to all the voxels along the pixel's ray. The final IoU of the fine-level voxel grid drops from 34.31% to 30.33%, and the mIoU that considers the accuracy of the voxel's semantic prediction drops from 20.00% to 16.61%. This shows the effectiveness of our conditioning strategy being able to disambiguate voxels at different depths.\nTwo-stage Reconstruction. We disentangle the voxel grid and appearance reconstruction stages to make the best use of different types of models. Using a single-stage model that simultaneously predicts the sparse voxels and the appearance from images, we can only achieve a PSNR/LPIPS of 17.88/0.57, in comparison to 19.34/0.48 when using the two-stage model.\nAppearance Reconstruction. We validate the effect of voxel grid resolution and the number of Gaussians per voxel M in the appearance reconstruction stage. We find that higher-resolution voxel grids are crucial for capturing detailed geometry, and using a larger number of Gaussians only slightly increases the performance. Thus, we set M = 4 as a moderate value for the final results. Compared in Fig. 8, the GAN-based postprocessing, despite the time cost, is beneficial for producing high-quality images by sharpening the renderings."}, {"title": "5 Discussion", "content": "Conclusion. In this work, we have introduced SCube, a feed-forward method for large 3D scene reconstruction from images. Given sparse view non-overlapping images, our method is able to predict a high-resolution 3D scene representation consisting of voxel-supported Gaussian splats (VoxSplat) and a light-weight sky panorama in a single forward pass within tens of seconds. We have demonstrated the effectiveness of our method on the Waymo Open Dataset, and have shown that our method outperforms the state-of-the-art methods in terms of reconstruction quality.\nLimitations. Our method does suffer from some limitations. First, the current method is not able to handle complicated scenarios such as dynamic scenes under extreme lighting or weather conditions. Second, the quality of appearance in occluded regions still carries uncertainty. Third, the method itself still requires ground-truth 3D training data which is not always available for generic outdoor scenes. In future work, we plan to address these limitations by incorporating more advanced neural rendering techniques and by exploring more effective ways to generate training data."}, {"title": "\u2013 Appendix \u2013", "content": "Additional Data Processing Details. For each data sample, we crop the point cloud obtained from \u00a7 4.1 into a local chunk of 102.4m \u00d7 102.4m. The point cloud is then voxelized into the fine-level and coarse-level grids used in \u00a7 3.1 with $1024^3$ and $256^3$ resolutions respectively (with voxel sizes of 0.1m and 0.4m). Our dataset contains 20243 chunks for training and 5380 chunks for evaluation, out of the 798 training and 202 validation sequences.\nInput and Evaluation Details. Waymo dataset provides 5 views for each camera frame, namely front, front-left, front-right, side-left and side-right. However, not all of the baseline methods we compared with in \u00a7 4.2 can handle the unconventional camera intrinsic in the side-left and side-right views. We hence only use the first three views (with a resolution of 1920 \u00d7 1280) in \u00a7 4.2 for both the input and the evaluation metrics. However, in \u00a7 4.3 we opt to use all 5 views for the input to both our method and the baseline due to compatibility and maximized performance.\nFor the baselines, the original PixelSplat [4] method does not have depth supervision. To make the comparison fair, we attempt to add a depth supervision loss to it.\nTraining Details. The diffusion loss in Eq (2) is defined similar to [16, 39] with a v-parametrization as:\n$L_{Diffusion} = E_{t,X,\\varepsilon~N(0,1)} [||v(\\sqrt{a_t}X + \\sqrt{1 - a_t}\\varepsilon) - (\\sqrt{a_t}X - \\sqrt{1 - a_t}X)||_2^2]$, (7)\nwhere v() is the diffusion network, t is the randomly sampled diffusion timestamp, and $a_t$ is the scheduling factor for the diffusion process, whose details are referred to in [16].\nWe train all of our models using the Adam [23] optimizer with $\u03b2_1$ = 0.9 and $\u03b2_1$ = 0.999. We use PyTorch Lightning [10] for building our distributed training framework. For the voxel grid reconstruction stage, we train both coarse-level and fine-level voxel latent diffusion models with 64\u00d7 NVIDIA Tesla A100s for 2 days. For the appearance reconstruction model, we train it using 8\u00d7 NVIDIA Tesla A100s for 2 days. Empirically, we use \u03bb = 1.0 for $L_{Depth}$ in Eq (2). Additionally, we use $\u03bb_1$ = 0.9, $\u03bb_2$ = 1.0, $\u03bb_{SSIM}$ = 0.1 and $\u03bb_{LPIPS}$ = 0.6 in Eq (6). For image condition, we set the feature channel C = 32, the number of depth bins D = 64, $z_{near}$ = 0.1 and $z_{far}$ = 90.0. We linearly increase the interval of depth bins."}, {"title": "B Network Architecture", "content": "Voxel Grid Reconstruction. We follow [39] to implement the Sparse Structure VAE and the Diffusion UNet. Appearance Reconstruction. We process the original input images with three 2D convolutional layers."}, {"title": "CSCube+ without Per-scene Training", "content": "In \u00a7 3.3 we introduce a GAN postprocessing module to refine the rendered images, which is finetuned on each scene. To further improve the efficiency of our method, we hereby present a postprocessing module that is jointly trained on the full dataset, without the need of per-scene finetuning."}, {"title": "D.1 Geometry Quality", "content": "We note that the uncertainty of the scene geometry given our input images is large, and the problem that the model tackles is indeed non-trivial and sometimes even ill-posed.\nWe compute an additional metric called 'voxel Chamfer distance' that measures the L2-Chamfer distance between the predicted voxels and ground-truth voxels."}, {"title": "D.2 Visual Ablation Study", "content": "In addition to the quantitative ablation study in Tab. 3, we present a qualitative demonstration in Fig. 10. For the single-stage model, we test the upper bound of it by feeding the ground-truth 10243 voxel grids because otherwise the fully-dense high-resolution condition will lead to out-of-memory."}, {"title": "D.3 Additional Results on Text-2-Scene Generation", "content": "We provide additional text-2-scene generation results in Fig. 11 and Fig. 12."}, {"title": "D.4 Additional Results on Large-scale Scene Reconstruction", "content": "We provide additional results on large-scale scene reconstruction from real-world captures in Fig. 13."}, {"title": "D.5 Additional Results on LIDAR Simulation", "content": "We provide additional LiDAR Simulation results in Fig. 14. We also show the result on a long sequence input in Fig. 15."}]}