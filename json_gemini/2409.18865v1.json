{"title": "Positional Encoder Graph Quantile Neural Networks for Geographic Data", "authors": ["William E. R. de Amorim", "Scott A. Sisson", "T. Rodrigues", "David J. Nott", "Guilherme S. Rodrigues"], "abstract": "Positional Encoder Graph Neural Networks (PE-GNNs) are a leading approach for modeling\ncontinuous spatial data. However, they often fail to produce calibrated predictive distributions, lim-\niting their effectiveness for uncertainty quantification. We introduce the Positional Encoder Graph\nQuantile Neural Network (PE-GQNN), a novel method that integrates PE-GNNs, Quantile Neu-\nral Networks, and recalibration techniques in a fully nonparametric framework, requiring minimal\nassumptions about the predictive distributions. We propose a new network architecture that, when\ncombined with a quantile-based loss function, yields accurate and reliable probabilistic models with-\nout increasing computational complexity. Our approach provides a flexible, robust framework for\nconditional density estimation, applicable beyond spatial data contexts. We further introduce a struc-\ntured method for incorporating a KNN predictor into the model while avoiding data leakage through\nthe GNN layer operation. Experiments on benchmark datasets demonstrate that PE-GQNN sig-\nnificantly outperforms existing state-of-the-art methods in both predictive accuracy and uncertainty\nquantification.", "sections": [{"title": "Introduction", "content": "Large spatial datasets are collected in a wide range of applications in economics (Anselin, 2022), meteo-\nrology (Bi et al., 2023), urban transportation (Lv et al., 2014; Derrow-Pinion et al., 2021; Kashyap et al.,\n2022), social networks (Xu et al., 2020), e-commerce (Sreenivasa and Nirmala, 2019) and other fields.\nGaussian Processes (GPs) (Rasmussen and Williams, 2006; Cressie and Wikle, 2011) are a fundamental\ntool for modelling spatial data on continuous domains. They are flexible and interpretable models for\nunknown functions, both in spatial and more general regression settings. However, with time complexity\n$O(n^3)$ and storage complexity $O(n^2)$, naive GP methods quickly become intractable for large datasets.\nThis has led to a large range of approximate inference methods, such as those based on sparse approxima-\ntions to covariance or precision matrices (Reinhard Furrer and Nychka, 2006; Lindgren et al., 2011), low\nrank approximations (Cressie et al., 2022) or nearest neighbour approximations (Vecchia, 1998; Datta\net al., 2016; Katzfuss and Guinness, 2021).\nGiven the difficulty of GP computations, it's of interest to explore scalable methods for large spatial\ndatasets using neural networks (NNs) and to enhance their ability to quantify uncertainty. A state-of-\nthe-art method for making spatial predictions using Graph Neural Networks (GNNs) is the Positional\nEncoder Graph Neural Network (PE-GNN) of Klemmer et al. (2023). Our contribution is to make three\nkey modifications to the PE-GNN architecture to enhance its ability to make accurate spatial predictions\nand to quantify uncertainty. These modifications will be explained further below.\nNNs are popular in data modeling and prediction tasks like computer vision and natural language\nprocessing (NLP). However, traditional NNs struggle to handle spatial dynamics or graph-based data ef-\nfectively. GNNs (Kipf and Welling, 2017; Veli\u010dkovi\u0107 et al., 2018; Hamilton et al., 2017) offer a powerful\nand scalable method for applying NNs to graph-structured data. The idea is to share information through\nthe edges of a graph, allowing nodes to exchange information during learning. GNNs are versatile and\ncan uncover nonlinear relationships among inputs, hidden layers, and each node's neighborhood infor-\nmation. The success of GNNs in spatial applications largely depends on the spatial graph construction,\nincluding choice of distance metric and the number of neighboring nodes, and traditional GNNs often\nstruggle to model complex spatial relationships. To address this, Klemmer et al. (2023) introduced the\nPE-GNN, which enhances predictive performance in spatial interpolation and regression. However, PE-\nGNN is not designed to provide a full probabilistic description of the target's distribution, and assuming\na Gaussian distribution for predictions can lead to poorly calibrated intervals, such as 80% intervals that\nfail to contain the true outcome 80% of the time. Recently, Bao et al. (2024) proposed a new frame-\nwork called Spatial Multi-Attention Conditional Neural Processes (SMACNPs) for spatial small sample\nprediction tasks. SMACNPs use GPs parameterized by NNs to predict the target variable distribution,"}, {"title": "Background", "content": "Positional Encoder Inspired by the Transformer architecture (Vaswani et al., 2017) for geographic\ndata (Mai et al., 2020), PE-GNN (Klemmer et al., 2023) employs a PE with two components: a sinu-\nsoidal transformation and a fully-connected NN. The first is a deterministic transformation formed by the\nconcatenation of sinusoidal functions, including variations in frequency and scale. The spatial dimen-\nsions (typically represented as latitude and longitude) are handled separately. The second component is\na fully-connected NN, denoted NN(\u0398PE), taking the output produced by the sinusoidal transformation\nas input and processing it through a fully-connected NN. Let CB = [C1,...,CnB]T be the matrix con-\ntaining the spatial coordinates of a batch of datapoints, typically of dimension n\u00df \u00d7 2, where each ci\ncorresponds to the pair (latitude, longitude). This transformation results in the desired vector space rep-\nresentation, thereby generating the coordinate embedding matrix Cemb = PE(CB, Omin, Omax, \u04e8\u0420\u0415) =\nNN(ST(CB,0min, Omax), \u0398\u03a1\u0395).\nGraph Neural Network GNNs are powerful and scalable solutions for representation learning\nand inference with graph-structured data. They leverage the topological structure of correlations be-\ntween nearby graph nodes and represent each node in a latent space embedding suitable for the specific\ndownstream task (Wu et al., 2022). Popular GNN architectures use this graph structure to update the\nembeddings of each node, considering both the features of each node and its neighbors, in an iterative\nprocess (Wu et al., 2022). The first step comprises aggregating features from each node's neighbours. Af-\nter aggregation, we combine each node's prior representation with the output of the first step. The initial\nembedding of each node is its feature vector, so HB) = XB. Then, for each GNN layer k \u2208 {1, . . ., K},\n-(0)\nan iteration of the two step process described above is executed.\nThe most popular GNN architectures follow this backbone, but differ in the way they aggregate\nneighbours messages and update the embeddings. Graph Convolutional Networks (GCNs) (Kipf and\nWelling, 2017) are inspired by the convolution operation from Convolutional Neural Networks (CNNs).\nFor weighted graphs, GCN layer k has the following update equation\nHk\nH) = f(k) (D1/2 [AB + IB] D1/2 H-1) W(k)),\nfor k \u2208 {1, . . ., K}.\n(1)\nHere, f(k) is an activation function (e.g., ReLU) and W(k) is a matrix of learnable parameters, while"}, {"title": "Method", "content": "In this work, we propose a novel approach to spatial data prediction tasks: the Positional Encoder Graph\nQuantile Neural Network (PE-GQNN). Algorithm 1 shows the step-by-step procedure to train a PE-\nGQNN model, and Figure 2 illustrates its complete pipeline. Here, each rectangle labeled \"GNN\" and\n\"LINEAR\" represents a set of one or more neural network layers, with the type of each layer defined\nby the title inside the rectangle. At each layer, a nonlinear transformation (e.g. ReLU) may be applied.\nEach datapoint p\u2081 comprises three components pi = {Yi, Xi, Ci}. The component y\u2081 is the target variable,\nand as the focus here is regression, then y\u2081 is a continuous scalar. Additionally, \u00e6\u00bf is the feature vector\nand c\u2081 contains the geographical coordinates associated with observation i.\nAfter initializing the model and hyperparameters, the first step of PE-GQNN is to randomly sample\na batch B of datapoints, P1, ..., PnB. The batch can be fully represented by the target YB(nB\u00d71), features\nXB (nB xp), and coordinates matrices CB(nB\u00d72), respectively. The next step uses the matrix of geographi-\ncal coordinates CB = [C1, ..., Cnp] to obtain spatial embeddings for each datapoint (Algorithm 1, Step\n5). This process receives CB as input, and after passing through deterministic sinusoidal transformations\nand a fully-connected NN, outputs the spatial embedding matrix of the batch Comb (nexu), containing the\nspatial context of each pair of coordinates. CB is also used to compute the distance between each pair of\ndatapoints (Algorithm 1, Step 6). From these distances and a predefined number of nearest neighbors, a\ngraph can be constructed, with each datapoint as a node and edge weights computed from the distances,\nleading to the batch adjacency matrix AB.\nAt Step 13 of Algorithm 1, the first distinction between PE-GQNN and PE-GNN arises: instead\nof using the concatenation of the feature matrix and the spatial embedding as the input for the GNN\noperator, we apply the GNN operator only to the feature matrix XB. One or more fully-connected layers"}, {"title": "Experiments", "content": "PE-GQNN was implemented using PyTorch (Paszke et al., 2019) and PyTorch Geometric (Fey and\nLenssen, 2019). The source code is available at: https://github.com/WilliamRappel98/\nPE-GQNN. We conducted comprehensive simulations to explore the prediction performance and other\nproperties of the proposed model, comparing it with state-of-the-art methods. Computation was per-\nformed on an Intel i7-7500U processor with 16 GB of RAM, running Windows 10."}, {"title": "Conclusion", "content": "In this work, we have proposed the Positional Encoder Graph Quantile Neural Network (PE-GQNN)\nas an innovative framework to enhance predictive modeling for geographic data. Through a series of\nrigorous experiments on real-world datasets, we have demonstrated the significant advantages of PE-\nGQNN over competitive methods. The empirical results underscored the capability of PE-GQNN to\nachieve lower MSE, MAE, and MPE across all datasets and GNN backbones compared to traditional\nGNN and PE-GNN. Notably, PE-GQNN demonstrated substantial improvements in predictive accuracy\nand uncertainty quantification, as evidenced by its consistent performance in quantile calibration metrics\nsuch as MPE and MADECP. The PE-GQNN framework's ability to provide a full description of the\npredictive conditional distribution, including quantile predictions and prediction intervals, provides a\nnotable improvement in geospatial machine learning. PE-GQNN provides a solid foundation for future\nadvancements in the field of geospatial machine learning."}]}