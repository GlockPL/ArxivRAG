{"title": "Mechanisms for Data Sharing in Collaborative Causal Inference (Extended Version)", "authors": ["Bj\u00f6rn Filter", "Ralf M\u00f6ller", "\u00d6zg\u00fcr L\u00fctf\u00fc \u00d6z\u00e7ep"], "abstract": "Collaborative causal inference (CCI) is a federated learning method for pooling data from multiple, often self-interested, parties, to achieve a common learning goal over causal structures, e.g. estimation and optimization of treatment variables in a medical setting. Since obtaining data can be costly for the participants and sharing unique data poses the risk of losing competitive advantages, motivating the participation of all parties through equitable rewards and incentives is necessary. This paper devises an evaluation scheme to measure the value of each party's data contribution to the common learning task, tailored to causal inference's statistical demands, by comparing completed partially directed acyclic graphs (CPDAGs) inferred from observational data contributed by the participants. The Data Valuation Scheme thus obtained can then be used to introduce mechanisms that incentivize the agents to contribute data. It can be leveraged to reward agents fairly, according to the quality of their data, or to maximize all agents' data contributions.", "sections": [{"title": "1 Introduction", "content": "Causal inference estimates the causal effect of treatment variables within a particular population, a method widely adopted across various fields. In healthcare, for example, it assesses the effectiveness of pharmaceutical interventions [1] and explores gene effects on phenotypes [2]. It also finds application in policy decision-making [3], recommender systems [4], education [5], advertisement [6], agriculture [7], and numerous other domains.\nVarious methodologies for causal inference have emerged to analyze interventional data, obtained from controlled experimental trials, or observational data. However, their effectiveness can be hindered by data quality issues, including data sparsity and non-representativeness. For example, patient preferences for hospitals can limit each institution's records, causing biases and data sparsity."}, {"title": "2 Preliminaries", "content": "We collect the basic notions and methods for causal structure learning. Directed acyclic graphs (DAGs) can model conditional independencies. For a DAG D = (V, E) let the vertices V = V1, ..., Vp correspond to a p-variate random vector X \u2208 R, while the edges E denote relationships between the variables they connect. A probability function P over X is called Markov relative to a DAG D if it factorizes as \\(P(x_1, ..., x_p) = \\prod_{i=1}^p P(x_i | Pa_{X_i}(D))\\), where \\(Pa_{X_i}(D)\\) are the parents of node Xi.\nIn causal inference, a DAG D is not only used to represent associational but also causal relationships. Here, intuitively, the parents \\(Pa_V_i(D)\\) of a node Vi \u2208 V can be interpreted as the direct causes of Vi, and the children as the direct"}, {"title": "3 Collaborative Causal Inference", "content": "We assume, that n self-interested agents N := {1, ..., n} want to solve a common causal inference problem by estimating a CPDAG and using observational data of p variables, V = {V1, ..., Vp} to estimate causal effects between variable-pairs (Vi, V\u2081) \u2208 V \u00d7 V. Furthermore, agents are non-malicious and they may acquire data from different and potentially biased populations, but these data collectively form the common target population of interest. Each agent i \u2208 N can produce data points Di for a fixed cost ci > 0 per data point. Both single agents as well as coalitions of agents use their available data to infer a CPDAG C = (V, Ec) and then infer the causal effects using the IDA algorithm. We will make the rather strict assumption, that all agents are equally interested in all effects between all possible variable pairs (Vi, V\u2081) \u2208 V \u00d7 V.\nLet N denote the grand coalition containing all agents and let C \u2286 N denote some coalition of agents. Dc denotes the data provided by coalition C. Let E(DC) = \\(C_C, \\hat{\\Theta}_C\\) be the estimator computed from Dc as described in Section 2. It consists of an estimated CPDAG Cc and the estimated distributions of causal effects \\(\\hat{\\Theta}_C = {\\hat{\\theta}_{ij} | (V_i, V_j) \\in V \\times V}\\). To simplify notation, the subscript C is replaced by index i when C = {i}. Thus, Dc = UiecDi.\nIn the following, it will be necessary for agents and coalitions, to know the quality of their estimators, to calculate their improvement rate, and to allocate fair incentives to agents. Therefore let v : Ec \u00d7 EB \u2192 R be the valuation function"}, {"title": "4 Data Valuation Scheme", "content": "We assume coalitions are interested in both the inferred CPDAG C, as well as the estimated distributions of causal effects is for all pairs of variables (Vi, V\u2081) \u2208 V \u00d7 V. In practice, the ground truth CPDAG and causal effects are unknown quantities and must be inferred from the observed data. The best available estimate for these are the CPDAG and estimated distributions computed using all data available to the grand coalition N, due to the asymptotic consistency of the estimators [14]. Thus, we will usually treat the grand coalition estimates of the CPDAG CN as the surrogate for the CPDAG of the true underlying DAG, similarly, for all Vi, Vj \u2208 V \u00d7 V, we use \\(\\hat{\\theta}_{ij}^{N}\\) (the estimated distributions using the grand coalitions' data) as the surrogate for the ground truth population ACE \u0472ij.\nDifferent CPDAGs often yield vastly different effects. Suppose we have two CPDAGS, C\u2081 and C2, over the same set of variables V, and we are interested in the possible causal effects of V\u00bf \u2208 V on V; \u2208 V \\ {V}. To infer these, one has to determine the sets Pav; (C\u2081) and Pav; (C2) and has to use these to calculate the possible causal effects (see Section 2).\nTherefore, to evaluate the data of a coalition CCN, we propose to first take into account the difference between the CPDAG estimated by that coalition, Cc = (V, Ec) and the grand coalition estimates of the CPDAG CN = (V, EN). To do this, we introduce a measure extending the measure of structural intervention distance (SID) [18]. We will call this the distribution-SID (dSID). It is the number of node-pairs Vi, Vi, for which the distribution of causal effects of Vi on Vi given C\u2081 is different from the distribution given C2. For a discussion of the SID and other possible distance measures from the literature see Section 7 on related work. We show how the dSID can be computed in appendix A.\nThis leads to the first part of our data valuation measure. Let & be an estimator, consisting of a CPDAG C\u025b = (V,E) and the estimated distributions for all"}, {"title": "5 Modeling an Individual Agent", "content": "To understand how agents will behave in a multi-agent setting, we first consider the case of a single agent i \u2208 N. In this case, the agent only has access to the data he produces himself and the estimator arising from this data. We will assume that each agent i has a marginal fixed cost ci > 0 for producing a data point. Thus his cost for producing data Di is costi(Di) = ci|Di|.\nUsing data Di, agent i will be able to produce some estimator Ei. However, there is no benchmark to assess the quality of E\u00bf without access to more data. Thus, agent i can only evaluate the quality of his estimator based on estimators he has produced before, using a subset of his current data. Let T = {1, 2, ...} be a series of timestep. For each t\u2208T, let Di be the data produced so far by i at timestep t with D-1 C D for all t > 1. Let \u0190 (D) = E be the estimator obtained by agent i at time t (using data Di).\nFor the rest of this paper, we will assume that an agent will compare his current estimator to the estimators he produced before, using the current estimator as the benchmark (the current estimator presents his best knowledge of the real distribution underlying the data). The agent will act according to how much his estimator improved from older versions. If the improvement rate is judged as too small by the agent, he will decide that further improvements are not worth the cost and thus stop producing further data. We will assume, that for as long as an agent produces data, he will produce the same amount of data at every timestep. Let A\u2081 denote this amount of data, thus for all t > 1, |D\\D\u22121| = \u2206i. So while producing data, at every timestep i occurs additional cost ci\u2206i.\nv(E, E) assesses the discrepancy between estimators Et and Et, with 0 indicating identical CPDAGs and causal effect distributions, and -1 indicating completely different CPDAGs. Conversely, -v(E, E) measures the improve- ment in estimator quality from Et to Et, with 0 denoting no improvement and 1 completely different estimators.\nWhile the PC algorithm's asymptotic consistency guarantees eventual con- vergence to the true underlying distribution with sufficient data [17], noise in early-stage data may not consistently improve estimator quality over time. Thus, v(\u03b5, \u03b5) > v(\u03b5-1, E) is not guaranteed for every t,t' \u2208 T with t > t' > 1. For this reason, it makes sense for an agent to not only take into account the improvement from the last estimator to the current one, v(E-1, E) but also the average improvement rate from older estimators towards the current one: \\(\\frac{1}{t-1}\\sum_{t'=1}^{t-1}\\frac{v(E_{t'},E_t)}{t-t'}\\). With this, we can define the improvement rate as follows:\n\\(imi(t) := \\frac{1}{t-1}\\sum_{t'\\in{1,...,t-1}}\\frac{v(E_{t'},E_t)}{t-t'}\\),"}, {"title": "6 Modeling Multiple Agents", "content": "We will now study how agents behave in a collaborative setting as described in Section 3, where multiple agents produce data and the resulting model can be shared. In this framework, a server mediates the interaction between agents. First, the server publishes the mechanism to the agents, then, at each timestep t, each agent i contributes some data Di to the server and finally, the server in turn rewards the agents with an estimator Et of a certain quality. The server also provides agents with the opportunity to check the quality of any estimator they possess, that is, at time t the server computes v(E, E) for any estimator Et which i received at some time t' < t. This way, i can always check the improvement rate of models he received so far, compared to the best available estimator at the time.\nFor time steps T = {1, 2, ...} let D\u207a = {D\u2081, ..., Dh} be the data provided by agents 1 to n at time t. A mechanism is formalized as a function M(D) : D\u2081 \u00d7 \u00d7 Dn \u2192 E1 \u00d7 ... \u00d7 En, which maps the agents' contributions to estimators they receive from the mechanism. At t, so far each agent i generated and transmitted data Di to the server. For this, he is rewarded with an estimator Et of quality v(E, E). From now on, E will denote the estimator i receives at t from the server, while \u0190t will denote the estimator obtained only from i's data D. The first requirement for such a mechanism is, for it to be feasible: A mechanism which returns an estimator Et to agent i is said to be feasible if for any i \u2208 N and any D, it satisfies v(E, E\u2081) \u2264 v(Ey, Ev). Feasibility is necessary to make"}, {"title": "6.1 Standard Collaborative Setting", "content": "We first examine the most basic setting for collaborative causal inference. Here, each agent immediately has access to the model of the grand coalition, obtained from all data available to the mechanism at time t, Ev. Thus at all t \u2208 T and for all i \u2208 N: [M(D\u207a)]\u2081 = Ev. This mechanism is feasible and also satisfies individual rationality since v(E, \u0190) \u2265 v(\u0190t, E\u2081). We will assume that agents will behave as they would if they were to act on their own. That is, at each time step an agent i \u2208 N will produce and contribute some data to the mechanism and then evaluate, how much the estimator of the grand coalition has improved through their data contribution. If this improvement makes up for the cost i incurred, he will continue to produce data, until the improvement does not outweigh their cost anymore. When first entering the mechanism, i will contribute initial data D, since he does not know yet, how much this data will improve the grand coalition's estimator. After that, at each timestep t > 1, i will compute the improvement rate imi(t) according to Equation 7, but now comparing estimators of the grand coalitions at times t' <t to the current estimator EN.\nAgain, i's utility is calculated as u\u2081(t) = \\sum_{t'\\in{1,...,t}}(imi(t) \u2013 cii), and agent i will continue to produce data as long as this is not decreasing and stop producing once it decreases. Thus the behavior of an agent i in the standard collaborative setting will be as described in definition 2. However, we will denote the optimal time for agent i to stop producing further data in the collaborative setting by topt, as opposed to t-opt in the single agent setting."}, {"title": "6.2 Data Maximizing Mechanism", "content": "We will now present a mechanism that maximizes the data provided by all agents i \u2208 N. A mechanism M is data-maximizing given costs c = {C1, ..., Cn} if it maximizes the data collected at equilibrium. Since at each timestep i produces a fixed amount of data \u2206i, the total amount of data provided by agent i is topt Di"}, {"title": "6.3 Achieving Fairness", "content": "While the mechanism described above maximizes the amount of data extracted from each agent, it does not satisfy fairness. In parallel to work done by Qiao and colleagues [12], we can use our data valuation function v(E, B), to design a reward scheme ri for every agent i \u2208 N at each time t \u2208 T, which fulfills the following fairness conditions defined in the work of Qiao and colleagues [12]. At each t \u2208 T, let v := mini\u2208nv(E, E\u2081) for the empty coalition C = 0. Then:"}, {"title": "7 Related Work", "content": "The data valuation scheme presented in this paper is inspired by the scheme presented by Qiao and colleagues [12], in particular w.r.t. the use of negative reverse KL divergence. However, they only performed one regression to estimate one effect, they did not estimate a CPDAG and did not estimate the causal effects between all pairs of variables.\nFor this reason, a measure for distances between CPDAGs was needed. One possible measure here would be the structural hamming distance (SHD). However, as described by Peters and B\u00fchlmann [18], the SHD does not necessarily describe well, how different two graphs are regarding the causal effects they decode. They therefore introduce the structural intervention distance (SID), which, given two DAGs Hand G over the same vertices V, describes how many intervention distributions between variable pairs are falsely estimated by H wrt G. The SID can be applied to CPDAGs as well, here however it returns a range of the SIDs between all possible DAG extensions of the CPDAGs, whereas here a definite measure was needed.\nA data maximizing mechanism for federated learning was first presented by Karimireddy and colleagues [11]. They assumed, however, that each data point provided by any agent had the same value and that the quality of a model could be directly computed. In our setting of collaborative causal inference, these assumptions do not hold anymore, therefore the data valuation scheme was introduced, both to measure the value of an agent's data as well as to measure the quality of a given estimator."}, {"title": "8 Conclusion and Future Work", "content": "This paper presents novel mechanisms for CCI aimed at motivating the engagement of self-interested agents through the provision of superior causal effect estimators compared to those agents that could develop independently. Our initial mechanism incentivizes all participating agents to maximize data contribution, thereby facilitating the construction of a comprehensive model based on extensive data and expected to yield high accuracy. Building upon this, our second mechanism assigns better estimators to agents supplying higher-quality data, ensuring a fair reward allocation given individual contributions.\nA limitation of our mechanism lies in the assumption of known data production costs ci for agents, prompting a future research direction towards maximizing data despite unknown ci's. Furthermore, the presumption of honesty and non-malicious behavior among all agents may not hold in practical scenarios, with some parties potentially exploiting the transparent framework for personal gain or harm to others. Consequently, a more robust mechanism resilient to such behavior would be preferable."}, {"title": "A Distribution Structural Intervention Distance", "content": "To see how the dSID can be computed, we will first need some further preliminaries, consisting of some more graph-theoretical definitions and the concept of covariate adjustment."}, {"title": "A.1 Further Preliminaries", "content": "A partially directed acyclic graph (PDAG) is a partially directed graph without directed cycles. A PDAG G is a maximally oriented PDAG (MPDAG) if and only if the edge orientations in G are complete under the Meek orientation rules [19].\nTo identify the total causal effect of a variable V\u00bf \u2208 V on a variable V; \u2208 V, one has to find an expression for P(y | do(x)). Using only the preintervention probabilities of the observed variables V, this can be done through covariate adjustment. Here, a so-called adjustment set is used, to estimate the causal effect [13]. A graph-theoretical criterion can be used to determine, whether a set ZC V \\ {V, V;} is an adjustment set wrt two nodes Vi, Vj \u2208 V. This is the so-called adjustment criterion:"}, {"title": "A.2 An Algorithm for Computing the dSID", "content": "Recall, that the dSID is defined as follows: Let C\u2081 and C2 be two CPDAGS over the same set of variables V.\ndSID: G\u00d7G\n\u2192N\n(C1, C2)\n|{(Vi, V\u2081) \u2208 V \u00d7 V, Vi \u2260 V; | the distribution of causal\neffects of Vi on V; is falsely identified in C\u2081 with respect to C2}|\nwith the distribution of causal effects of Vi on V; given CPDAG C defined as:\n\\(P_C(v_j | do(V_i = v_i)) = \\sum_{D \\in CE(C)}\\frac{1}{|CE(C)|}P_D(v_j | do(V_i = v_i))\\) (13)\nTo compute the dSID, for all (V;, V;) \u2208 V \u00d7 V, we first need to identify all possible effects of Vi on V; in all DAG-extensions of both C\u2081 and C2. We then need to check, whether all effects found given C\u2081 can be found given C2 as well, and vice versa. The dSID Algorithm is depicted in figure 1. It will be described in detail in the following.\nAs mentioned before, to identify the possible effects of Vi on Vi given a CPDAG C, we can compute Pav; (C) and use these as adjustment sets to compute the effects. However, suppose D,D' \u2208 CE(C) with Pav;(D) \u2260 Pav;(D'). It is still possible, that Pav;(D') can be an adjustment set wrt (Vi, V\u2081) in D (this is the case, when Pav (D') fulfills the adjustment criterion wrt (Vi, Vj) in D).\nNow, the effect of Vi on V; would be identical given both D and D'. We will therefore start by identifying the subclasses of DAGs, such that the effect of Vi on V; in each subclass is identical. For this, the IDGraphs Algorithm [21] can be used (line 3 in figure 1), which runs in O(2m(C))poly(|V|), where m(C) is the number of undirected edges incident to Vi on a proper possibly causal path from Vi to V. While m(C) could be potentially as big as p, in practice it tends to"}]}