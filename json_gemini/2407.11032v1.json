{"title": "Mechanisms for Data Sharing in Collaborative Causal Inference (Extended Version)", "authors": ["Bj\u00f6rn Filter", "Ralf M\u00f6ller", "\u00d6zg\u00fcr L\u00fctf\u00fc \u00d6z\u00e7ep"], "abstract": "Collaborative causal inference (CCI) is a federated learning method for pooling data from multiple, often self-interested, parties, to achieve a common learning goal over causal structures, e.g. estimation and optimization of treatment variables in a medical setting. Since obtaining data can be costly for the participants and sharing unique data poses the risk of losing competitive advantages, motivating the participation of all parties through equitable rewards and incentives is necessary. This paper devises an evaluation scheme to measure the value of each party's data contribution to the common learning task, tailored to causal inference's statistical demands, by comparing completed partially directed acyclic graphs (CPDAGs) inferred from observational data contributed by the participants. The Data Valuation Scheme thus obtained can then be used to introduce mechanisms that incentivize the agents to contribute data. It can be leveraged to reward agents fairly, according to the quality of their data, or to maximize all agents' data contributions.", "sections": [{"title": "1 Introduction", "content": "Causal inference estimates the causal effect of treatment variables within a particular population, a method widely adopted across various fields. In healthcare, for example, it assesses the effectiveness of pharmaceutical interventions [1] and explores gene effects on phenotypes [2]. It also finds application in policy decision-making [3], recommender systems [4], education [5], advertisement [6], agriculture [7], and numerous other domains.\nVarious methodologies for causal inference have emerged to analyze interventional data, obtained from controlled experimental trials, or observational data. However, their effectiveness can be hindered by data quality issues, including data sparsity and non-representativeness. For example, patient preferences for hospitals can limit each institution's records, causing biases and data sparsity."}, {"title": "2 Preliminaries", "content": "We collect the basic notions and methods for causal structure learning. Directed acyclic graphs (DAGs) can model conditional independencies. For a DAG D = (V, E) let the vertices V = V1, ..., Vp correspond to a p-variate random vector X \u2208 R, while the edges E denote relationships between the variables they connect. A probability function P over X is called Markov relative to a DAG D if it factorizes as P(x1, ..., xp) = \\prod_{i=1}^{p} P(x_i | Pa_{X_i}(D)), where Pa_{X_i}(D) are the parents of node Xi.\nIn causal inference, a DAG D is not only used to represent associational but also causal relationships. Here, intuitively, the parents Pa_{V_i}(D) of a node Vi \u2208 V can be interpreted as the direct causes of Vi, and the children as the direct"}, {"title": "3 Collaborative Causal Inference", "content": "We assume, that n self-interested agents N := {1, ..., n} want to solve a common causal inference problem by estimating a CPDAG and using observational data of p variables, V = {V1, ..., Vp} to estimate causal effects between variable-pairs (Vi, Vj) \u2208 V \u00d7 V. Furthermore, agents are non-malicious and they may acquire data from different and potentially biased populations, but these data collectively form the common target population of interest. Each agent i \u2208 N can produce data points Di for a fixed cost ci > 0 per data point. Both single agents as well as coalitions of agents use their available data to infer a CPDAG C = (V, Ec) and then infer the causal effects using the IDA algorithm. We will make the rather strict assumption, that all agents are equally interested in all effects between all possible variable pairs (Vi, Vj) \u2208 V \u00d7 V.\nLet N denote the grand coalition containing all agents and let C \u2286 N denote some coalition of agents. Dc denotes the data provided by coalition C. Let E(DC) = (Cc, \\Theta_c) be the estimator computed from Dc as described in Section 2. It consists of an estimated CPDAG Cc and the estimated distributions of causal effects \\Theta_C = {\\theta_{ij}^C | (V_i, V_j) \\in V \\times V}. To simplify notation, the subscript C is replaced by index i when C = {i}. Thus, Dc = \\bigcup_{i\\in C}D_i.\nIn the following, it will be necessary for agents and coalitions, to know the quality of their estimators, to calculate their improvement rate, and to allocate fair incentives to agents. Therefore let v : E_c \\times E_B \\rightarrow \\mathbb{R} be the valuation function"}, {"title": "4 Data Valuation Scheme", "content": "We assume coalitions are interested in both the inferred CPDAG C, as well as the estimated distributions of causal effects \\Theta_{ij}^\\varepsilon for all pairs of variables (Vi, Vj) \u2208 V \u00d7 V. In practice, the ground truth CPDAG and causal effects are unknown quantities and must be inferred from the observed data. The best available estimate for these are the CPDAG and estimated distributions com-puted using all data available to the grand coalition N, due to the asymptotic consistency of the estimators [14]. Thus, we will usually treat the grand coali-tion estimates of the CPDAG CN as the surrogate for the CPDAG of the true underlying DAG, similarly, for all Vi, Vj \u2208 V \u00d7 V, we use \\Theta_{ij}^N (the estimated distributions using the grand coalitions' data) as the surrogate for the ground truth population ACE \\Theta_{ij}.\nDifferent CPDAGs often yield vastly different effects. Suppose we have two CPDAGs, C1 and C2, over the same set of variables V, and we are interested in the possible causal effects of Vi \u2208 V on Vj \u2208 V \\ {Vi}. To infer these, one has to determine the sets Pavj (C1) and Pavj (C2) and has to use these to calculate the possible causal effects (see Section 2).\nTherefore, to evaluate the data of a coalition C \u2286 N, we propose to first take into account the difference between the CPDAG estimated by that coalition, Cc = (V, Ec) and the grand coalition estimates of the CPDAG CN = (V, EN). To do this, we introduce a measure extending the measure of structural intervention distance (SID) [18]. We will call this the distribution-SID (dSID). It is the number of node-pairs Vi, Vj, for which the distribution of causal effects of Vi on Vj given C1 is different from the distribution given C2. For a discussion of the SID and other possible distance measures from the literature see Section 7 on related work. We show how the dSID can be computed in appendix A.\nDefinition 1 (Distribution Structural Intervention Distance (dSID)). Let G be the space of CPDAGs over p variables. We then define\ndSID: G\u00d7G \u2192N\n(C1, C2) \u2192 {(Vi, Vj) \u2208 V \u00d7 V, Vi \u2260 Vj | the distribution of causal effects of Vi on Vj is falsely identified in C1 with respect to C2}|\nThis leads to the first part of our data valuation measure. Let \\varepsilon be an estimator, consisting of a CPDAG C\\varepsilon = (V,E) and the estimated distributions \\Theta_\\varepsilon for all"}, {"title": "5 Modeling an Individual Agent", "content": "To understand how agents will behave in a multi-agent setting, we first consider the case of a single agent i \u2208 N. In this case, the agent only has access to the data he produces himself and the estimator arising from this data. We will assume that each agent i has a marginal fixed cost ci > 0 for producing a data point. Thus his cost for producing data Di is costi(Di) = ci|Di|.\nUsing data Di, agent i will be able to produce some estimator Ei. However, there is no benchmark to assess the quality of Ei without access to more data. Thus, agent i can only evaluate the quality of his estimator based on estimators he has produced before, using a subset of his current data. Let T = {1, 2, ...} be a series of timestep. For each t\u2208T, let Dt_i be the data produced so far by i at timestep t with D^{t-1}_i \u2286 D^t_i for all t > 1. Let E(D^t_i) = E_i^t be the estimator obtained by agent i at time t (using data D^t_i).\nFor the rest of this paper, we will assume that an agent will compare his current estimator to the estimators he produced before, using the current estimator as the benchmark (the current estimator presents his best knowledge of the real distribution underlying the data). The agent will act according to how much his estimator improved from older versions. If the improvement rate is judged as too small by the agent, he will decide that further improvements are not worth the cost and thus stop producing further data. We will assume, that for as long as an agent produces data, he will produce the same amount of data at every timestep. Let \\Delta_i denote this amount of data, thus for all t > 1, |D^t_i \\setminus D^{t-1}_i| = \\Delta_i. So while producing data, at every timestep i occurs additional cost c_i\\Delta_i.\nv(E_i^t, E_i^{t'}) assesses the discrepancy between estimators E_i^t and E_i^{t'}, with 0 indicating identical CPDAGs and causal effect distributions, and -1 indicating completely different CPDAGs. Conversely, -v(E_i^{t'}, E_i^{t}) measures the improve-ment in estimator quality from E_i^{t'} to E_i^{t}, with 0 denoting no improvement and 1 completely different estimators.\nWhile the PC algorithm's asymptotic consistency guarantees eventual con-vergence to the true underlying distribution with sufficient data [17], noise in early-stage data may not consistently improve estimator quality over time, Thus, v(E_i^t, E_i^{t'}) > v(E_i^{t-1}, E_i^t) is not guaranteed for every t,t' \u2208 T with t > t' > 1. For this reason, it makes sense for an agent to not only take into account the improvement from the last estimator to the current one, v(E_i^{t-1}, E_i^t) but also the average improvement rate from older estimators towards the current one: -\\frac{\\sum_{t'=1}^{t-1} v(E_i^{t'},E_i^{t})}{t-1}. With this, we can define the improvement rate as follows:\nim_i(t) := \\frac{1}{t-1} \\sum_{t'\\in{1,...,t-1}} \\frac{v(E_i^{t'}, E_i^t) + v(E_i^{t-1}, E_i^t)}{t-t'} \\qquad (7)"}, {"title": "6 Modeling Multiple Agents", "content": "We will now study how agents behave in a collaborative setting as described in Section 3, where multiple agents produce data and the resulting model can be shared. In this framework, a server mediates the interaction between agents. First, the server publishes the mechanism to the agents, then, at each timestep t, each agent i contributes some data D^t_i to the server and finally, the server in turn rewards the agents with an estimator E_i^t of a certain quality. The server also provides agents with the opportunity to check the quality of any estimator they possess, that is, at time t the server computes v(E_i^t, E_i^{t'}) for any estimator E_i^{t'} which i received at some time t' < t. This way, i can always check the improvement rate of models he received so far, compared to the best available estimator at the time.\nFor time steps T = {1, 2, ...} let D\u207a = {D^t_1, ..., D^t_n} be the data provided by agents 1 to n at time t. A mechanism is formalized as a function M(D) : D^t_1 \\times ... \\times D^t_n \\rightarrow E^t_1 \\times ... \\times E^t_n, which maps the agents' contributions to estimators they receive from the mechanism. At t, so far each agent i generated and transmitted data D^t_i to the server. For this, he is rewarded with an estimator E_i^t of quality v(E_i^t, E^t_N). From now on, E_i^t will denote the estimator i receives at t from the server, while E^t_i will denote the estimator obtained only from i's data D^t_i. The first requirement for such a mechanism is, for it to be feasible: A mechanism which returns an estimator E_i^t to agent i is said to be feasible if for any i \u2208 N and any D, it satisfies v(E_i^t, E_i^t) \u2264 v(E_N^t, E_i^t). Feasibility is necessary to make"}, {"title": "6.1 Standard Collaborative Setting", "content": "We first examine the most basic setting for collaborative causal inference. Here, each agent immediately has access to the model of the grand coalition, obtained from all data available to the mechanism at time t, E^t_N. Thus at all t \u2208 T and for all i \u2208 N: [M(D\u207a)]i = E^t_N. This mechanism is feasible and also satisfies individual rationality since v(E^t_N, E^t_i) \u2265 v(E_i^t, E_i^t). We will assume that agents will behave as they would if they were to act on their own. That is, at each time step an agent i \u2208 N will produce and contribute some data to the mechanism and then evaluate, how much the estimator of the grand coalition has improved through their data contribution. If this improvement makes up for the cost i incurred, he will continue to produce data, until the improvement does not outweigh their cost anymore. When first entering the mechanism, i will contribute initial data D^t_i, since he does not know yet, how much this data will improve the grand coalition's estimator. After that, at each timestep t > 1, i will compute the improvement rate imi(t) according to Equation 7, but now comparing estimators of the grand coalitions at times t' <t to the current estimator E^t_N.\nAgain, i's utility is calculated as u_i(t) = \\sum_{t'\\in{1,...,t}} (imi(t) \u2013 c_i\\Delta_i), and agent i will continue to produce data as long as this is not decreasing and stop producing once it decreases. Thus the behavior of an agent i in the standard collaborative setting will be as described in definition 2. However, we will denote the optimal time for agent i to stop producing further data in the collaborative setting by t_{i,Col}^{opt}, as opposed to t-opt in the single agent setting."}, {"title": "6.2 Data Maximizing Mechanism", "content": "We will now present a mechanism that maximizes the data provided by all agents i \u2208 N. A mechanism M is data-maximizing given costs c = {c1, ..., cn} if it maximizes the data collected at equilibrium. Since at each timestep i produces a fixed amount of data \\Delta_i, the total amount of data provided by agent i is t_{i}^{opt} \\Delta_i"}, {"title": "6.3 Achieving Fairness", "content": "While the mechanism described above maximizes the amount of data extracted from each agent, it does not satisfy fairness. In parallel to work done by Qiao and colleagues [12], we can use our data valuation function v(E, B), to design a reward scheme r_i^t for every agent i \u2208 N at each time t \u2208 T, which fulfills the following fairness conditions defined in the work of Qiao and colleagues [12]. At each t \u2208 T, let v := \\min_{i \\in N} v(E^t_i, E^t_N) for the empty coalition C = 0. Then:"}, {"title": "7 Related Work", "content": "The data valuation scheme presented in this paper is inspired by the scheme presented by Qiao and colleagues [12], in particular w.r.t. the use of negative reverse KL divergence. However, they only performed one regression to estimate one effect, they did not estimate a CPDAG and did not estimate the causal effects between all pairs of variables.\nFor this reason, a measure for distances between CPDAGs was needed. One possible measure here would be the structural hamming distance (SHD). However, as described by Peters and B\u00fchlmann [18], the SHD does not necessarily describe well, how different two graphs are regarding the causal effects they decode. They therefore introduce the structural intervention distance (SID), which, given two DAGs H and G over the same vertices V, describes how many intervention distributions between variable pairs are falsely estimated by H wrt G. The SID can be applied to CPDAGs as well, here however it returns a range of the SIDs between all possible DAG extensions of the CPDAGs, whereas here a definite measure was needed.\nA data maximizing mechanism for federated learning was first presented by Karimireddy and colleagues [11]. They assumed, however, that each data point provided by any agent had the same value and that the quality of a model could be directly computed. In our setting of collaborative causal inference, these assumptions do not hold anymore, therefore the data valuation scheme was introduced, both to measure the value of an agent's data as well as to measure the quality of a given estimator."}, {"title": "8 Conclusion and Future Work", "content": "This paper presents novel mechanisms for CCI aimed at motivating the engagement of self-interested agents through the provision of superior causal effect es-timators compared to those agents that could develop independently. Our initial mechanism incentivizes all participating agents to maximize data contribution, thereby facilitating the construction of a comprehensive model based on exten-sive data and expected to yield high accuracy. Building upon this, our second mechanism assigns better estimators to agents supplying higher-quality data, ensuring a fair reward allocation given individual contributions.\nA limitation of our mechanism lies in the assumption of known data pro-duction costs ci for agents, prompting a future research direction towards maxi-mizing data despite unknown ci's. Furthermore, the presumption of honesty and non-malicious behavior among all agents may not hold in practical scenarios, with some parties potentially exploiting the transparent framework for personal gain or harm to others. Consequently, a more robust mechanism resilient to such behavior would be preferable."}, {"title": "A Distribution Structural Intervention Distance", "content": "To see how the dSID can be computed, we will first need some further prelimi-naries, consisting of some more graph-theoretical definitions and the concept of covariate adjustment."}, {"title": "A.1 Further Preliminaries", "content": "A partially directed acyclic graph (PDAG) is a partially directed graph without directed cycles. A PDAG G is a maximally oriented PDAG (MPDAG) if and only if the edge orientations in G are complete under the Meek orientation rules [19].\nTo identify the total causal effect of a variable Vi \u2208 V on a variable Vj \u2208 V, one has to find an expression for P(y | do(x)). Using only the preintervention probabilities of the observed variables V, this can be done through covariate adjustment. Here, a so-called adjustment set is used, to estimate the causal effect [13]. A graph-theoretical criterion can be used to determine, whether a set Z \u2286 V \\ {Vi, Vj} is an adjustment set wrt two nodes Vi, Vj \u2208 V. This is the so-called adjustment criterion:"}, {"title": "A.2 An Algorithm for Computing the dSID", "content": "Recall, that the dSID is defined as follows: Let C1 and C2 be two CPDAGS over the same set of variables V.\ndSID: G\u00d7G\n\u2192N\n(C1, C2)\n|{(Vi, Vj) \u2208 V \u00d7 V, Vi \u2260 Vj | the distribution of causal\neffects of Vi on Vj is falsely identified in C1 with respect to C2}|\nwith the distribution of causal effects of Vi on Vj given CPDAG C defined as:\nP_C(v_j | do(V_i = v_i)) = \\sum_{D \\in CE(C)} \\frac{1}{|CE(C)|} P_D(v_j | do(V_i = v_i)) \\qquad (13)"}, {"title": "B Proof of Theorem 1", "content": "Theorem 1 (Data maximization with known costs). The mechanism M defined by equation 10 is data-maximizing for e\u2192 0+. A rational agent i will contribute A_i^{t_{opt}} data points where A_i^{t_{opt}} > A_i^{t-opt}, yielding a total of \\sum_{j \\in N}A_i^{t_{opt}} data points.\nProof. We will define the best response of agent i to a mechanism M and data contributions of all other agents over time D_{-i} = {D^1_{-i}, D^2_{-i}...} as the time, at which i will stop to produce data:\nB^M_i(D_{-i}) := t^{opt}_i, \\qquad (21)\nsuch that for all t < t_i^{opt} imi(t) > ci\u2206i and imi(t^{opt}) < ci\u2206i.\nFirst, we will see that given fixed data contributions D^t_{-i} from other users for all t\u2208 T, for each agent i, i's best response to our mechanism M consists of more data than to any other feasible and IR mechanism M. Will will then continue to show, that this implies that the equilibrium contribution of the agent is also data maximizing.\nLemma 1. For given data contributions over time Di and any feasible and IR mechanism M, define best responses of agent i B^M_i(D_{-i}) and B^{\\overline{M}}_i(D_{-i}) for our mechanism M as defined in equation 10 and the other mechanism M. Then, for any i \u2208 N and any Dt_i,\nB^M_i(D_{-i}) \u2265 B^{\\overline{M}}_i(D_{-i}) \\qquad (22)\nFor now, we will assume the above lemma and continue with our proof. Since the best responses are defined as the time, at which i stops contributing more data, we will write B^M_i(D_{-i}) = t^M_i and B^{\\overline{M}}_i(D_{-i}) = t^{\\overline{M}}_i. By definition 3, the total data collected by mechanism M is calculated as"}]}