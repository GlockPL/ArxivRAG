{"title": "Previous Knowledge Utilization In Online Anytime Belief Space\nPlanning", "authors": ["Michael Novitsky", "Moran Barenboim", "Vadim Indelman"], "abstract": "Online planning under uncertainty remains a\ncritical challenge in robotics and autonomous systems. While\ntree search techniques are commonly employed to construct\npartial future trajectories within computational constraints,\nmost existing methods discard information from previous plan-\nning sessions considering continuous spaces. This study presents\na novel, computationally efficient approach that leverages\nhistorical planning data in current decision-making processes.\nWe provide theoretical foundations for our information reuse\nstrategy and introduce an algorithm based on Monte Carlo Tree\nSearch (MCTS) that implements this approach. Experimental\nresults demonstrate that our method significantly reduces com-\nputation time while maintaining high performance levels. Our\nfindings suggest that integrating historical planning information\ncan substantially improve the efficiency of online decision-\nmaking in uncertain environments, paving the way for more\nresponsive and adaptive autonomous systems.", "sections": [{"title": "I. INTRODUCTION", "content": "Autonomous agents often operate under uncertainty due\nto sensor noise and incomplete information, maintaining a\nbelief (probability distribution) over possible states instead\nof direct access to the true environment state. Partially\nObservable Markov Decision Processes (POMDPS) provide\na framework for such settings, but solving them optimally is\ncomputationally intractable (PSPACE-complete) [1], mainly\ndue to the curse of history, curse of dimensionality, and\ncontinuous state, action and observation spaces common in\nreal-world applications.\nRecent advancements have introduced online algorithms\n[2] [3] [4] that find approximate solutions to POMDPs. These\nalgorithms operate within limited budget constraints, such\nas restricted time, and employ a sampling-based approach\nto construct partial trees and search for the optimal action\nthat maximizes the expected cumulative reward. By sampling\na subset of the belief space, these algorithms effectively\naddress both the curse of history and the curse of dimen-\nsionality, which are key obstacles in solving POMDPs.\nIn POMDPS, the reward function of a belief node is\ntypically formulated as the expected reward over states.\nHowever, this formulation may be insufficient for certain\nproblems, such as information gathering and active sensing.\nIn such cases, the problem is commonly addressed as Belief\nSpace Planning (BSP) or p-POMDP [5], where the reward is\ndefined over the belief itself. Information-theoretic measures,\nsuch as information gain and differential entropy, are com-\nmonly used to quantify uncertainty in the decision-making\nprocess [6]. However, exact calculation of information-\ntheoretic rewards becomes intractable for general distribu-\ntions, as it requires integrating over all possible states.\nTo address this challenge, approximation methods such as\nkernel density estimation (KDE) and particle filter estima-\ntion [7] have been proposed in the literature. Nonetheless,\nthese methods still incur significant computational expenses,\nwith computation complexity scaling quadratically with the\nnumber of samples. As reward calculation is performed for\neach node in the tree, it becomes the primary source of\ncomputational complexity in online planning algorithms.\nThe main objective of this paper is to improve planning\nefficiency within a non-parametric setting, continuous state,\naction and observation spaces, and general reward func-\ntions. To address these challenges, we contribute a novel\napproach that leverages the Multiple Importance Sampling\nframework [8] to tackle the problem of reusing information\nfrom previous planning sessions. Our approach introduces\na new algorithm specifically designed to utilize knowledge\ngathered during prior planning sessions. We demonstrate\nhow our method can be integrated with Monte Carlo Tree\nSearch (MCTS) to create a novel online algorithm called\nIncremental Reuse Particle Filter Tree (IR-PFT). We evaluate\nour algorithm in an online planning setting, demonstrating\nreduced planning time without performance loss."}, {"title": "II. RELATED WORK", "content": "Solving POMDPs is challenging, but recent advances,\nsuch as the POMCP algorithm [2], have made significant\nprogress. POMCP extends the UCT algorithm [9] to handle\npartial observability. During each simulation, a state particle\nis sampled from the current belief, propagated through\nthe search tree, and information like visitation count and\naccumulated reward is recorded. Action selection follows\na Multi-Armed Bandit approach. POMCP assumes discrete\nstate, action and observation spaces, and a state-based reward\nfunction. The number of samples at each belief node depends\non the number of simulations it has participated in, with\nless visited nodes having fewer samples. POMCPOW [10]\nextends POMCP to continuous action and observation spaces\nby using progressive widening and representing beliefs as\nweighted particle sets. It assumes access to an observation\nlikelihood model, where each simulated state is added to\nthe weighted belief, and a new state is sampled based on\nits weight. In PFT-DPW [10] the authors adopt particle\nfilter formulation for belief update and each belief is rep-\nresented with a constant number of samples. [11] introduce\nP-POMCP which propagates a set particles in each simu-\nlation using particle filter and adds it to existing particles\nin visited nodes. Frequently visited nodes achieve better\nrepresentation, with convergence proven asymptotically for\ncontinuous, bounded p. [6] introduce the IPFT algorithm,\nwhich extends PFT [10]. They use a reward defined as a\nlinear combination of differential entropy and expected state\nreward. In each simulation, a particle set is sampled from\nthe root belief and propagated through the tree. Entropy\nestimates are averaged across particle sets at each belief\nnode to estimate differential entropy. [4] propose LABECOP\nfor continuous observation spaces. At each belief node b,\na state particle s is sampled, an action a is chosen using\nmodified UCB, and an observation o is sampled. Previous\nstates from b, a are reweighted by o to improve value function\nestimate Q(b,a). SITH-BSP [12], [13] and AI-FSSS [14]\nmake use of simplification of reward function calculation\nand observation space sampling accordingly, while preserv-\ning action consistency. [15] quantify the effect of applying\nsimplification and extend p-POMDP to PP-POMDP, while\nproviding stochastic bounds on the return. DESPOT [3] and\nsubsequent works [16], [17], [18] propose algorithms that\nuse determinized random sampling to build the search tree\nincrementally, with recent work addressing large observation\nspaces [17]. The use of a - vectors in [16], [17], [18] restricts\ntheir application to POMDPs with state-dependent reward\nfunctions. Previous methods start each planning session from\nscratch, while iX-BSP [19], [20] proposes reuse but assumes\nan open loop setting and doesn't address non-parametric\nbeliefs. In this work, we address continuous state, action, and\nobservation spaces with general belief-dependent rewards, a\nnon-parametric framework, and a closed-loop setting."}, {"title": "III. NOTATIONS", "content": "A. POMDP\nPOMDP is a 7-tuple (S, A, O, PT, PO, r, bo), where S, A\nand O correspond to state, action and observation spaces.\nPT(Sk+1 Sk, ak) is the state transition density function,\nPO(Ok+1 Sk+1) is the observation density function, r(b, a, b')\nrepresents the reward function based on the current be-\nlief b, the action a, and the subsequent belief b', while\nbo denotes the current belief over states. We denote by\nHk = (bo, a0, 01, \u2026, Ok) = {bo, 01:k, a1:k-1} the history up\nto time k, which consists of a series of actions made and\nobservations received. Since the exact state of the world is\nnot known and we only receive observations, a probability\ndistribution (belief) over states is maintained bk = P(Sk|Hk).\nIt is assumed that the belief is sufficient statistics for the\ndecision making and a Bayesian update is used to update\nthe belief recursively:\nbk+1 =\n\u03b7\u03a1\u039f(Ok+1 Sk+1) \u222b PT(Sk+1|Sk,ak)bkdsk. (1)\nSk\nwhere \u03b7 is a normalization term. A policy \u03c0\u2208 \u03a0is a\nmapping from belief space to action space \u03c0: b \u2192 a. We\ndefine the value function V\u2122 for any policy\nand horizon\nd as\nV", "2)\nbk+1": "k+d\nwhere \u3160 \u3160k:k+d\u22121 represents a sequence of policies for\nhorizon d and Gk = \u03a3k+d-1 (bi, \u03c0 (bi), bi+1) is the return.\nSimilarly, we define the action value function Q\u2122 as\nQ", "V": "bk+1)]. (3)\nB. Non-Parametric Setting\nIn our work we assume a non-parametric setting, where\nwe use collections of state particles to estimate complex\nbelief distributions. We leverage the particle filter method\n[21] to update our approximations of posterior distributions\nas we receive new observations from the environment. The\ntheoretical belief be is approximated using m particles\n{s}1, assuming resampling at each particle filter step,\nwhich ensures uniform weights of 1\nm\nbk\nm\nm\n\u221118(88). (4)\ni=1\nGiven resampled belief bk, action ak, and propagated belief\nb+1, calculating P(b+1|bk, ak) involves determining all the\nmatchings between the states in bk and those in b bk+1 which is\n#P-complete [22]. We assume, similar to [23], that the beliefs\nare not permutation invariant, meaning particle beliefs with\ndifferent particle orders are not considered identical. This\nassumption simplifies the derivation of the propagated belief\nlikelihood. Consequently, we can express bk as {sh, 1\nand bk+1\n1m\nas {+}1\nP(6k+1/6\u043a, \u0430\u043a) = \n1\nm\n\u2211\n\u03b4(+1 - Sk). (5)\nmi=1\nIn the rest of the paper we assume a non-parametric setting\nand for the ease of notation we remove the hat sign from\nall beliefs.\nC. Importance Sampling\nImportance sampling estimates properties of a target dis-\ntribution p(x) by sampling from a proposal distribution\nq(x), assigning weights to adjust each sample's contribution\naccording to p(x)\nIS\n[f(x)] = \n1\n\u2211\u03c9\u00b2 \u00b7 f(x\u00b2), \u03c9\u00b2 =\nN\np(xi)\nx\u00b2 ~ q. (6)\nq(xi)\ni=1\nThe distribution q must satisfy q(x\u00b2) = 0 \u21d2 p(x\u00b2) = 0. With\nM proposal distributions {qm} m=1, Multiple Importance\nSampling formulation [8] can be used:\nMIS [f(x)] = \n1\n\u03a3\u0399\u03a3wm (x,m) P(x,m)\nNm\nm=1\ni=1\nqm (xi,m)\nf(x,m). (7)"}, {"title": "Here", "content": "Here, nm denotes the number of samples that originate from\ndistribution qm, xi,m denotes the ith sample that originates\nfrom distribution qm and the weights wm must satisfy\nqm(xi,m) =0 \u21d2 wm(x)f(x)p(x) = 0.\nM\nf(x,m) \u22600 = \u2211wm (xi,m) = 1. (8)\nm=1\nWe assume that the weights wm are determined using the\nbalance heuristic which bounds the variance of the estimator\n[8] and in this case the MIS estimator is\nEMIS [f(x)] = \u03a3\u03a3\np(xi,m)\nM nm\nm=1 i=1 j=1 Njqj(xi,m)\n-f(x,m). (9)\nThe PFT-DPW algorithm [10] is based on the UCT\nalgorithm [9] and expands its application to a continuous\nstate, action and observation setting. It utilizes Monte-Carlo\nsimulations to progressively construct a policy tree for the\nbelief MDP [10]. At every belief node bk and action ak\nit sets up visitation counts N(bk, ak) and N(bk), where\nN(bk) = \u03a3\u03b1\u03bc \u039d(bk, ak) and action-value function is cal-\nculated incrementally\nQ(bk, ak) \u2211 i=1 (10)\nN\nby averaging accumulated reward upon initiating from node\nbk and taking action ak within the tree. Notably, Q(bk, ak)\n(10) is not equal to Q\u2122 (bk, ak) (3) as the policy varies\nacross different simulations within the tree, causing the\ndistribution of the trajectories to be non-stationary, hence the\nabsence of the superscript. The particle filter generates a\npropagate belief b+1 and posterior belief bk+1 from bk and\nak, sampling observation Ok+1 and computing reward r\nbk+1,bk+1, Ok,r \u2190 GPF(m)(bk,ak). (11)\nTo handle continuous spaces, Double Progressive Widening\nlimits a node's children to kN, where N is the node visit\ncount, and k and a are hyperparameters [10]."}, {"title": "IV. APPROACH", "content": "Our contributions are threefold: (1) an efficient incre-\nmental update method for the Multiple Importance Sam-\npling (MIS) estimator, enabling action-value estimation from\nprior and newly arriving data; (2) the application of MIS\nfor experience-based value estimation using expert-provided\ndata without planning; and (3) an MCTS-inspired online\nalgorithm that speeds up computations by reusing data from\nprevious planning sessions.\nA. Incremental Multiple Importance Sampling Update\nIn our setting, samples arrive incrementally in batches.\nA straightforward computation of (9) would necessitate a\ncomplexity of O(M\u00b2\u00b7navg), where M denotes the number of\ndifferent distributions and navg denotes the average sample\ncount across all distributions. We develop an efficient way to\nupdate the estimator (9) incrementally in the theorem below.\nTheorem 1: Consider an MIS estimator (9) with M differ-\nent distributions and nm samples for each distribution qm \u2208\n{91,..., qm }. Given a batch of L I.I.D samples from distribu-\ntion qm', where qm' could be one of the existing distributions\nor a new, previously unseen distribution, EMIS [f(x)] (9)\ncan be efficiently updated with a computational complexity\nof O(M-navg + M\u30fbL) and memory complexity O(M\u00b7Navg).\nB. Experience-Based Value Function Estimation\nWe assume that we have access to a dataset\nD {T, GD (12)\nof trajectories executed by an agent that followed a policy\n\u03c0. Each trajectory is defined as the sequence\n\u00b2 (baki)\u2192(bka+1, Oki+1, bk+10k+1) \u2192\n\u2192(bk+doki+d, bki+d), (13)\nwhere ki represents the starting time index and is used to\ndifferentiate between different steps in trajectory \u03c4\u00b2 and d\nis the horizon length. We assume that the agent applied a\nparticle filter with resampling at each step of the trajectory.\nThe return Gi associated with trajectory \u03c4\u00b2 is defined as the\naccumulated reward,\nd-1\nG\u2211(bk+jaki+j, bk+j+1). (14)\nj=0\n\u03c0\nIn this section, we evaluate V (bk) for the current belief bk\nusing only the dataset D (12), without planning. Such estima-\ntion is important in data-expensive domains like autonomous\nvehicles [24] and robotic manipulation tasks [25]. In the\nnext section, we will expand our methodology to include\nplanning.\nReusing trajectories where the initial belief is set to bk\npresents no challenge - we can aggregate all trajectories that\nbegin with belief bk and action ak and assuming we have N\nsuch trajectories, we define a sample-based estimator\nN\nQ* (bk, ak) Gi (15)\ni=1\nHowever, in continuous state, action and observation spaces,\nthe probability of sampling the same belief twice is zero.\nConsequently, each trajectory in the dataset D (12) will have\nan initial belief that is different from bk.\nTo be able to reuse trajectories from (12), we discard the\ninitial belief and action of the trajectory, instead linking the\ncurrent belief and action to the remainder of the trajectory.\nFormally, given a trajectory \u03c4\u00b2 \u2208 D, \u03c4\u03af = (ba) \u2192\nTsuffix where\nTsuffix=(bk+1,0k+1,+1,+1) ...\n\u2192(bki+d, Oki+d, bki+d). (16)"}, {"title": "and the current belief", "content": "and the current belief bk and action ak, we construct a new\ntrajectory \u03c4\u03af,\nTi (bk, ak) \u2192 Tsuffix\n(17)\nTo estimate Q (bk, ak) using the information within tra-\\jectory \u03c4\u00b2, two adjustments are required. Firstly, we need\nto modify the initial term in the return Gr to be equal to\nr(bk, ak, bki+1), recognizing that bk \u2260 bk and ak \u2260 aki\nConsequently, we define the return of trajectory \u03c4\nG\u011ci-r(bak, biki+1) + r(bk, ak, b\u00b2+1). (18)\nSecondly, we need to adjust the weight of G\u011ci due\nto the disparity between P(Tsuffix bk, ak, \u03c0) and\nP(Tsuffix bk, ak, \u03c0), which is acheived through importance\nsampling. The distribution P(,\u03b1, \u03c0) of partial\ntrajectory Tsuffix is determined by the initial belief be and\naction a. Given N\u012bs partial trajectories sampled from the\nsame distribution P(\u00b7|b, \u03b1, \u03c0), we define an Importance\nSampling estimator\nwhere wi.\nNIS\nQTS(bk, ak) = \u2211G (19)\ni=1\n1\n\u2211\nP(Tsuffix|bk,ak,\u03c0)\nki\nP(Touffix ba;,\u03c0)\nAs a result of our approach to constructing reusable\ntrajectories as described in (17), we can efficiently calculate\nthe weights w\u00b2 utilizing the theorem presented below.\nTheorem 2: Given belief node bk, action ak and trajectory\n\u03c4\u00b2 = (\u03b1)\nTouffix where T Tsuffix is defined in (16),\nthe following equality holds:\nP(Tsuffix bk, ak, \u03c0) P(bk+1/bk, \u0430\u043a)\nP(Tsuffixbki, ak, \u03c0) (+16)\nkm',\n(20)\nWe denote by M the number of unique distributions\nof partial trajectories {P(|bm, \u03b1, \u03c0)}=1, where each\ndistribution is defined by the initial belief bm and action\nam. Additionally, we denote the sample count from each\ndistribution as nm. Consequently, we can reformulate the\ndataset D (12) as follows:\nD \n\u2211G,nmk\nm=1,l=1 (21)\nUsing this formulation, we define a multiple importance\nsampling estimator assuming the balance heuristic (9),\nQMIS(bk, ak)\u03a3\u03a3\u039c\nP(Tsuffix bk, ak, \u03c0)\u011em\nkm=11=1 P(suffix be (22)\n1,m\nkj\nwhere Tsuffix represents the lth partial trajectory that was\nsampled from the distribution P(Ibm, am, \u03c0) and Glm is\nthe adjusted accumulated reward (18).\nUsing Theorem 2, we can re-write the MIS estimator (22)\nQMIS(bk, ak)\u03a3\u03a3\u039c\nm=11=1\u03a3\nP(blmbk, ak)\nkm+1\npj+1Gm\n(23)\nSince each element in the second sum of (23) corresponds to\na propagated belief, which might appear more than once, we\ncan rewrite the sum in a more compact form. Specifically,\nwe group the terms based on unique propagated beliefs and\naccount for their multiplicity:\nMC(bkm,akm)| NM(6mm)\nQMIS(bk, ak) \u03a3\u03a3 (6) \u03a3\nWl,mym k (24)\nm=1\nl=1\ns=1\nThe weights W(bm) are defined by:\nW(b-l,m) =\nP(blmbk, ak)\nkm+1\nM\n1,m\nP(+1,3) (25)\nkm+1\ny=1\nC(bkm, akm) denotes the set of reused propagated belief\nchildren associated with bkm and akm. The term N(blm\u2081)\nrepresents the visitation count of bem, indicating the\nnumber of trajectories that pass through the propagated\nbelief bl,m balm. Note that brum represents\nunique propagated beliefs, which differs from (23), where\nit denotes the propagated belief associated with a single\ntrajectory."}, {"title": "We now define the estimator", "content": "We now define the estimator\nQMIS(bk, ak) \uc2a5\nMC(bkm,akm)|\nkm=1\nl=1\nN(6mm)\nWk(bm) \u03a3Gmm,l,y\ns=1\n(26)\nwhere the weights W(bm) are defined in (25). Gm,l,y is\nthe extended return defined as\nGm,l,y = \u011em,l,y + \u03a3 r(bi, rollout (bi), bi+1). (27)\ni=k+dprev\ndprev is the horizon of propagated belief bulm, Gm,l,y\nshares the same values in the summation as the return\nk\n\u011em,ly (18), but it also includes additional terms from the\nextended trajectory due to the horizion extension using the\nrollout policy Trollout. Therefore, only the rewards for these\nadditional terms need to be computed when extending the\nhorizon, while all shared terms (\u011em,1,y) can be reused.\nSince the tree policy varies between simulations, the\nupdate represented by (26) operates in a heuristic manner,\nwith its convergence yet to be established. We intend to\nexplore this aspect in a future work.\nAfter extending the horizon of a reused propagated belief\nnode b and reusing its action-value function, the counter\nN(bk,ak) is incremented by the visitation count N(b\u00af)\nusing the relation N(bk, ak) b \u2208C(bk,ak) N(bk).\nThis approach accelerates our algorithm for a given num-\nber of simulations, offering a speedup over PFT-DPW [10].\nTo summarize, here is the high-level logical flow of our\nalgorithm: At each iteration, we either reuse a propagated\nbelief node by extending its horizon by Ad, as illustrated in\nand compute the extended return for the subtree\nrooted at the reused node b\u00af, or create a new node. Subse-\nquently, the Multiple Importance Sampling (MIS) estimator\n(26) is employed to evaluate the action-value function. To\navoid the computational expense of a naive calculation, we\nleverage Theorem 1 to perform efficient incremental updates\nof the estimator in (26)."}, {"title": "D. Algorithm Description", "content": "D. Algorithm Description\nThe complete algorithm is outlined across multiple meth-\nods - Algs. 1, 2, 3 and 4. Alg. 1 illustrates a general planning\nloop wherein the agent iteratively plans and executes actions\nuntil the problem is solved. After each planning session,\nreuse candidates are updated based on the preceding planning\ntree. The main algorithm is detailed in Alg. 2 with key\nmodifications compared to the PFT-DPW algorithm [10]\nhighlighted in red. The ActionProgWiden method (line 5)\nis implemented following the same approach as described\nin [10]. ShouldReuse method (line 7) evaluates three con-\nditions: current node b is the root, the balance between\nreused and new nodes, and the availability of reuse can-\ndidates. The second criterion is important because, while\nacquiring estimates from prior partial trajectories is runtime-\nefficient, generating new trajectory samples from the correct\ndistribution is essential. Currently, our algorithm only applies\nreuse to the root node, as it promises the most significant\ncomputational savings. Since the root node typically has the\nshallowest depth in the tree, we can optimize by conserving\nnumerous reward computations for most of its descendants.\nWhile extending reuse to nodes at other depth levels is\nfeasible, it falls outside the scope of this work.\nThe GetReuseCandidate method (line 8) selects a reuse\ncandidate propagated belief b' from the dataset D based on\na distance function fD (line 2). An example of fD is ||E[b--\nbMLE]|| where bMLE represents the maximum likelihood\npropagated belief, given belief b and action a which can\nbe calculated using (5) with O(m) complexity where m is\nthe number of samples. Since fD is applied to the entire\ndataset, it needs to be computationally efficient. Additionally,\nreusing nodes with high visitation counts will further reduce\nthe overall runtime of the algorithm.\nThe FillHorizonPropagated method (line 9), addresses\ndiscrepancies in horizon lengths when reusing nodes from the\nprevious planning sessions. Algorithm 4 performs recursive\ntraversal of the subtree defined by propagated belief b' and\nextends its depth by d using the rollout policy.\nAt lines 10 and 11, we increment counters, where N(b'\u00af)\nrepresents the count of trajectories passing through reuse can-\ndidate propagated belief node b'. At line 12, we increment\nthe PLAN procedure counter by N(b'\u00af).\nAt lines 13 and 29 we utilize (26) to update Q(b,a),\nleveraging efficiency through the application of Theorem (1).\nAt line 14 we store the propagated belief b'\u00af.\nLines 17-19 are executed when we choose not to reuse\nand instead initialize a new propagated belief from scratch.\nA new belief is generated using the particle filter method\n[21], after which the propagated belief and posterior belief\nare saved, and a rollout is performed.\nAt lines 23 and 24 we sample uniformly both propagated\nand posterios beliefs.\nUpdateReuseCandidates method in Algorithm 3 inserts\nnew reuse candidates that have a visitation count larger than\na threshold nmin, as we aim to reuse nodes with higher\nvisitation counts, which leads to a greater speedup."}, {"title": "V. RESULTS", "content": "V. RESULTS\nWe assess the performance of the IR-PFT algorithm by\ncomparing it to the PFT-DPW algorithm [10]. Our evaluation\nfocuses on two main aspects: runtime and accumulated\nreward, with statistics measured for each. In all experiments,\nthe solvers were limited to 1000 iterations for each planning\nphase. All experiments were conducted using the standard 2D\nLight Dark benchmark, wherein the agent's objective is to\nreach the goal while simultaneously minimizing localization\nuncertainty by utilizing beacons distributed across the map;\n. The reward function is\ndefined as a weighted sum of the average distance to goal and\ndifferential entropy estimator which is calculated using [7].\nEach algorithm was evaluated using different quantities of\nparticles-specifically, 5, 10, 15, and 20, while maintaining\na constant horizon length of d\n10. In the following\nresults reuse was done according to ShouldReuse method\n(line 7) as detailed in the previous section. We compared\nthe runtime of IR-PFT with and without reuse vs PFT-\nDPW. The results are depicted as a function\nof number of particles. Additionally, we included a speedup\nchart, which provides more insightful information, in\nThe runtime of IR-PFT consistently outperformed that\nof PFT. We observed a saturation in the speedup at a factor\nof approximately 1.5, which is attributed to the savings in\nreward computation, the most computationally intensive part\nof algorithm. We compare the accumulated rewards of IR-\nPFT with and without reuse (Figure 4d). The results show\nnegligible differences, indicating that our method improves\nruntime without compromising planning performance."}, {"title": "VI. CONCLUSIONS AND FUTURE WORK", "content": "VI. CONCLUSIONS AND FUTURE WORK\nIn this paper, we have proposed a general framework\nwhich allows to reuse prior information during the current\nplanning session. We derived theoretical justification for\nreuse via Multiple Importance Sampling and introduced a\nnew MCTS-like algorithm, IR-PFT which reuses information\nfrom previous planning session and allows to speed up\ncalculations in current planning session. In order to evaluate\nIR-PFT algorithm, we conducted an empirical performance\nstudy. Specifically, we compared the performance of our\napproach with and without the reuse of prior information. We\nmeasured various performance metrics, including computa-\ntion time and the accumulated reward. Our results clearly\nindicate a speed-up in the planning process when prior\ninformation is leveraged. Importantly, despite the accelerated\ncomputations, our approach maintains the same level of per-\nformance as the traditional planning approach without reuse.\nIncorporating prior information significantly boosts planning\nefficiency, delivering time savings while maintaining high-\nquality results. These findings underscore the effectiveness\nand potential of the proposed approach."}, {"title": "APPENDIX", "content": "APPENDIX\nA. Proof of Theorem 1\nConsider an MIS estimator (9) with M different dis-\ntributions and nm samples for each distribution qm \u2208\n{91,..., qM}. Given a batch of L I.I.D. samples from dis-\ntribution qm' which may be an existing or new distribution,\nEMIS [f(x)] \u03a3\u03a3\np(xi,m)\nM nm\nm=1 i=1 j=1 Njqj(xi,m)\n-f(xi,m),\n(9) can be efficiently updated with a computational com-\nplexity of O(M \u00b7 Navg + M \u00b7 L) and memory complexity\nO(M. Navg).\nFor every distribution qm we have the term\np(xi,m)\nNm1jqj (im) f (xi,m).\nM\nj=1Njqj\n\u2211Nm1jqj (xi,m) \u2190 \u2211]1nj.qj (xi,m)+L\u00b7qm' (xi,m) -\nIn case m\u2260 m':\nM\nO(1) complexity. We have nm samples and M distributions\nso the complexity of this update is O(Mnm).\nM\nIn case m = m':\nFor existing samples j1nj.qj (xi,m) \u2190 1 Nj\nqj(xi,m) + L. qm'(xi,m) - O(1) complexity. We have nm\nsamples existing samples so in total O(nm) complexity.\nFor each\nM1njqj (xi,m) f (xi,m) -\n\u2211Nm1jqj\nnew sample we need to calculate\nj=1Njqj\nO(M) complexity. We have\nL new samples so in total O(LM) complexity. The total\ncomplexity of the update is O(M \u00b7 Navg + M \u00b7 L).\nB. Proof of Theorem 2\nP(Tsuffix bk, ak, \u03c0) _ P(b+1,..., b+L/bk, \u03b1\u03ba, \u03c0)\nP(Tsuffix bk, aki \u03c0)\nApplying chain rule yields,\nP(b+1,...,b+b, \u03b1\u03ba, \u03c0)\n= (28)\nOki+L\nP(bk+1/bk, ak) P(+1,+L+1,\u03c0)\nP(bk+1/b) P(+1,+L+1, \u03c0)\nki+1ki (29)\n=\nP(bk\u00b2+1/bk, \u0430\u043a)\nP(bk+1/b)"}]}