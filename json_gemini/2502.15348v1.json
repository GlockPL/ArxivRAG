{"title": "Constructing a Norm for Children's Scientific Drawing: Distribution Features Based on Semantic Similarity of Large Language Models", "authors": ["Yi ZHANG", "Fan WEI", "Jingyi LI", "Yan WANG", "Yanyan YU", "Jianli CHEN", "Zipo CAI", "Xinyu LIU", "Wei WANG", "Peng WANG", "Zhong WANG"], "abstract": "The use of children's drawings to examining their conceptual understanding has been proven to be an effective method, but there are two major problems with previous research: 1. The content of the drawings heavily relies on the task, and the ecological validity of the conclusions is low; 2. The interpretation of drawings relies too much on the subjective feelings of the researchers. To address this issue, this study uses the Large Language Model (LLM) to identify 1420 children's scientific drawings (covering 9 scientific themes/concepts), and uses the word2vec algorithm to calculate their semantic similarity. The study explores whether there are consistent drawing representations for children on the same theme, and attempts to establish a norm for children's scientific drawings, providing a baseline reference for follow-up children's drawing research. The results show that the representation of most drawings has consistency, manifested as most semantic similarity>0.8. At the same time, it was found that the consistency of the representation is independent of the accuracy (of LLM's recognition), indicating the existence of consistency bias. In the subsequent exploration of influencing factors, we used Kendall rank correlation coefficient to investigate the effects of \"sample size\", \"abstract degree\", and \"focus points\" on drawings, and used word frequency statistics to explore whether children represented abstract themes/concepts by reproducing what was taught in class. It was found that accuracy (of LLM's recognition) is the most sensitive indicator, and data such as sample size and semantic similarity are related to it; The consistency between classroom experiments and teaching purpose is also an important factor, many students focus more on the experiments themselves rather than what they explain. In addition, most children tend to use examples they have seen in class to represent more abstract themes/concepts, indicating that they may need concrete examples to understand abstract things.", "sections": [{"title": "1. Introduction", "content": "Children's drawing is a commonly used way of examining children's cognitive and emotional status [1]. The reason for this is not only because drawing is a pleasurable activity for children [2], but also because its effectiveness in examining children's psychological activities is considered reliable and has been extensively studied in many fields. For example, from the perspective of individual learning, drawing is used to examine children's cognition of a certain scientific concept. Brechet et al. had over 250 French children draw their brains to understand how the \"black box\" of the brain works [3]. In terms of emotions and interpersonal relationships, campus bullying is a topic of great concern to psychologists, so drawing has also been used in related research. For example, Marengo et al. analyzed more than 600 drawings of Italian children to understand this issue [4]. However, there are still some fundamental issues that have not been overcome in the current psychological analysis of children's drawings, which mainly focus on methodological perspectives. This is mainly reflected in two aspects: firstly, the issue of ecological validity. As drawing is a non-verbal representation, it must be digitized through effective encoding in order to be analyzed using statistical methods. Therefore, encoding method is currently the mainstream way to study children's drawing. For example, Tolsberg et al. used coding to understand the longitudinal development of math word problems among Estonian elementary school students [5]. The coding rule for that study is as follows: those without drawings are coded as 9, those depicted as pictorial are coded as 1, and those depicted as schematic are coded as 2. The main difference between 1 and 2 is whether the relationship between the things is revealed. Therefore, how to develop coding rules is a crucial issue. This issue is particularly prominent in learning science, as emotions and interpersonal relationships has a large number of scales or tools (such as bullying problems [6]), and some drawing studies on this issue can use these general tools [7]. However, many researchers in learning science area had to adopt self-developed coding rules. In a psychology journal, there were a total of 12 original studies on the topic of children's drawing research, of which only 4 studies did not involve self-developed coding at all (only 33.3%) [8]. The high proportion of self-developed coding naturally raises a question: how can these highly task dependent coding rules be transferred and generalized to other drawing studies? In other words, different studies have different tasks, so how should other drawing studies refer to the results of these codes? Obviously, existing drawing research lacks a unified norm, which allows students to draw a certain field or object relatively freely under the condition of no tasks or fewer tasks, in order to fully expose their drawing preferences for that object in the naturally state. If this norm exists, different drawing tasks can be compared with it, then there is a common reference frame for drawing research on the same theme. The second issue is more obvious: the understanding of drawing is one of the most prominent areas of individual heterogeneity, and different people's perspectives on the same drawing may be completely different [9]. Therefore, for studies do not utilize machines, consistency testing is an essential step - the same drawing must be identified by at least two researchers, and the results of these two identifications must pass consistency testing (such as Cohen Kappa coefficient [3] [5]). On the contrary, if machine learning models are used to identify drawings, this process be likely to omitted. Because obviously, the same model is based on the same algorithm, its generation results should be more consistent rather than researchers. Based on the above two issues, this study aims to construct a norm of drawings for children's understanding of scientific themes, and to use the Large Language Model (LLM) to understand and identify children's drawings, attempting to provide a reference frame for future research on children's conceptual understanding of drawing. This study focuses on three questions: 1. Do children's drawings have a consistent representation for the same drawing theme (such as whether the concept of solar eclipse is mostly expressed through light path diagrams)? 2. Is there a relationship between the consistent representations (if it exists) and the accuracy of drawing recognition of LLM? 3. What other factors are related to the consistent representations (if it exists)?"}, {"title": "2. Method", "content": "2.1 Participants\nSelect a total of 1473 students from two primary schools in Beijing. Among them, there are 74 students in grade 4, and the rest are in grades 5 and 6. The male to female ratio is 1.01:1. The textbooks involved two versions [10] [11].\nWe excluded drawings that were unclear, incomplete, or clearly did not belong to the prescribed content, as well as drawings with inconsistent recognition results by LLM multiple times (see section 2.2.3 of this article), and finally included a total of 1420 drawings in the sample.\nAll children's personal information is strictly confidential, and the drawings were sealed after being photographed to ensure that personal information of students were not leaked. And informed consent from the participants' guardian was obtained. This research was approved by the Research Ethical Review Committee of Beijing Doers Educational Consulting Co., Ltd on March 13, 2024 (ECS-008).\n2.2 Proceeding\n2.2.1 Collection of drawings:\nFirstly, all children draw in class, that is, they draw whatever they just learned (following the order of textbooks), without deliberately setting contents or tasks. Secondly, to ensure the consistency and interpretability of the data, all participating teachers in this study are required to strictly follow the textbook and strictly prohibit free design teaching scaffolds and guidance to prevent individual differences among teachers from interfering with the results. Thirdly, the prompt was: \"We have just learned the knowledge of please use drawing to represent it\u201d. Other than that, no hints will be given to prevent any suggestion or misleading to students. Fourth, ensure that all students can complete the drawing.\n2.2.2 Drawings Preprocessing:\nFirstly, number and take photos of all the drawings. Secondly, preprocessing the photos includes two parts: \u2460removing students' personal information such as names and student IDs; \u2461 removing the theme/concept names (some students have the habit of writing the names of the themes/concepts on their drawings) to prevent prompting LLM. The solution is to use the \"rubber stamp\" function of Photoshop software (v.2018) to cover these names with surrounding images.\n2.2.3 LLM identification of drawings:\nFirstly, input the photos into LLM for identification. The prompt is \"What is the specific content of this drawing? How did you see it from the picture?\", and copy all the results and reasons generated by LLM into an Excel file for analysis.\nIn addition, to ensure the stability of LLM recognition, all drawings are independently recognized twice by LLM. Drawings with significantly different recognition results (such as the first recognition as a solar eclipse and the second recognition as a lunar phase) will undergo a third independent identification, and those with two occurrences of the three results will be considered as the result. If three different results occur, the drawing will be removed. The so-called 'independence' refers to deleting the current page after one recognition is completed and re recognizing it. Since the LLM used does not capture conversations with front-end users and place them in its pre training set, the above operation method can ensure the independence of the two recognitions.\nThis study used ChatGLM-4.0 to identify drawings. The reason for adopting this LLM is that its image recognition models mainly use convolutional neural networks (CNN) and recurrent neural networks (RNN) (derived from the self-introduction of this LLM), and these two models have been proven to be efficient and reliable in image recognition (because the two models are so famous and there are too numerous related studies, so this article will not list the literatures here). And there have been some studies published using this LLM for image recognition [12] [13], further proving its reliability in image recognition. And its language generation method adopts a model based on the transformer algorithm, which has high efficiency and reliability in language generation. More importantly, this LLM is open to individual users for free, making it more suitable for numerous primary school teachers to identify children's drawings, as mentioned above.\nTo verify that LLM's image recognition reasonings can indeed explain children's drawings representations, we counted the number of objects appearing in the drawings in all LLM image recognition reasoning (as long as one object appears in the drawings in these sentences, it is considered). The results showed that 77.4% (weighted average) of the reasoning statements for image recognition contained at least one object appearing in the drawings. This indicates that LLM is indeed describing the specific content of the drawings, rather than fabricating reasons. At the same time, in order to prevent LLM from generating image recognition reasons by reading text on the drawings, we randomly selected 16 drawings without any prompts (2 drawings for each theme/concept, and all drawings of the \"solar system\" concept have prompts, so this concept cannot be selected). The results showed that LLM described at least one object in all the 16 drawings.\n2.3 Semantic Similarity Analysis\n2.3.1 Algorithm\nThis study used the semi supervised Word2vec algorithm to generate word vectors, and then used the cosine similarity algorithm to calculate semantic similarity values.\nThe Word2vec is a deep learning algorithm for word embedding proposed by the Tomas Mikolov team at Google. Traditional spatial distance-based algorithms (such as Euclidean distance or Manhattan distance) in handling sentences with the same semantics but different representations usually have poor performance [14]. In contrast, the word2vec algorithm creatively proposes a negative sampling method, which calculates the conditional probability of the sampled sample coming from the corpus through context word frequency statistics, and then determines its polarity [15]. This method can better identify similar semantic sentences with different representations.\nCosine similarity is a widely used algorithm for calculating semantic similarity. Its main principle is to model the text as a term vector and calculate the cosine value between the term vectors to determine the degree of similarity between two texts [16]. The cosine similarity algorithm has significant advantages in semantic similarity calculation, with evidence showing that it has higher resolution than Jaccobi similarity [17].\nBy incorporating supervision into unsupervised algorithms, it has been widely applied in NLP semantic analysis and comparative research on cross class text learning between machines and humans, demonstrating its advantages [18].\n2.3.2 Proceeding of Semantic Similarity Analysis:\nFirstly, we collated all the statements generated by LLM for image recognition, match the statements with the image numbers one by one, and outputted the results as an Excel file. Then, used the JIEBA tokenizer to segment the statements in this Excel file to words (removed all punctuation marks). Then we trained a semi supervised word2vec algorithm (dimension 100, context window size 5, negative sampling 5) to embed word vectors into the segmentation results, and then used the word vector embedding results to calculate the average word vector of each sentence. Finally, the cosine similarity algorithm was used to calculate the semantic similarity between all pairs of sentences, thereby forming a similarity matrix.\nIn addition, although the theoretical range of semantic similarity calculated by the cosine similarity algorithm should be [-1,1], all semantic similarity results in this article are greater than 0. Therefore, for the sake of simplicity in calculation, all calculations regarding the concentration of semantic similarity in the following text were defaulted the value range of [0,1] instead of [-1,1].\n2.3.3 The Semi Supervised Algorithms\nDue to word2vec being an unsupervised algorithm, supervised learning is clearly superior in most cases in NLP processing. Therefore, we introduce keywords to transform the above algorithm into a semi supervised learning method, thereby increasing the accuracy of word vector embedding.\nThe specific method was to first use the JIEBA tokenizer to segment the text to be analyzed and removed all punctuation marks. Secondly, used the TF-IDF algorithm to search for keywords in the text and assigned weights to these keywords. The number of keywords of each text was set at 10, because after repeated experiments, when the number exceeds 10 (such as 15), there will be a large number of high-frequency function words (such as \"this\", \"yes\", etc.) in the keywords. Thirdly, implemented semi supervision by adding Python statements. Finally, embed the generated word vectors into an Excel file to calculate the average word vector for each sentence in the original text.\n2.4 Distribution Analysis and Visualization\nUsed a heat map to represent the similarity values of the semantic similarity matrix for each theme/concept, and used a histogram to represent the specific distribution of similarity in [0,1].\nSpecifically, we divided all data in each similarity matrix into eight equal parts in order of size (similar to quartiles), and separately calculated the difference between the seventh point (87.5%, from the minimum) and the second point (12.5%), which represents the size of the region where 75% of the semantic similarity values in this matrix are concentrated within [0,1] (hereinafter referred to as the \"75% Region\"). The smaller the value, the higher the numerical concentration, which is used to examine whether the theme/concept has a consistent representation.\n2.5 Exploration of Related Factors\nFinally, we used Kendall rank correlation coefficient (Kendall - t) to calculate the effects of three factors: Sample Size, Abstract Degree of Themes/Concepts, and Reasoning Degree of Themes/Concepts on the Accuracy, Semantic Similarity, and 75% Region of children's drawing representations.\nThere are two reasons for using this indicator (Kendall - \u03c4): firstly, this article is only an observational study, not a control study, so exploring correlation is more suitable than establish the causal relationship [19]. Secondly, the related factor exploration in this article aims to understand the impact of a certain factor on 9 concepts, which means we only have a sample size of 9. The commonly used Pearson correlation coefficient is an indicator that strictly depends on sample size, but due to the limitations of researchers, we do not have the ability to explore more concepts of children's drawings. The Kendall \u03c4 is more suitable for dealing with low sample size correlation problems, so we chose this indicator to explore the relevant factors.\nAt the same time, we also examined the reproduction rate of children's experiments/phenomena taught in class (mainly for abstract themes/concepts) through the method of word frequency. Because we suspect that students may have difficulty grasping the connotations of abstract concepts, they may use experiments/phenomena taught in class to represent these concepts in drawings."}, {"title": "3. Results", "content": "3.1 Overall\nThe total number of samples is 1420, covering 9 scientific themes or concepts: Circuits, Solar eclipses, Boiling, Solar system, Increasing the carrying capacity of a boat, Life history of a plant, Buoyancy, Electromagnetics, Physical and chemical changes. The average recognition accuracy of LLM is 46.7%, and the median semantic similarity is 0.9885. In addition, the mean of the \"75% Region\" is 0.0035 (i.e. occupying 0.35% of [0,1]). From the overall situation, it can be seen that LLM has a low recognition accuracy for image drawing content, but the semantic similarity concentration is high. This seems imply that there was a consistency preference in the representation of most of the drawings, but there is a possibility of \"collective bias\" in this preference (i.e. those children's drawing representations were inaccurate, but the way these erroneous representations were consistent).\n3.2 Results of every theme/concept\n3.2.1 Circuit\nThe main learning content (experiment) of this concept is to light up a bulb to let students know that the circuit must be closed in order to work (i.e. form a closed loop).\nThe average accuracy of LLM image recognition is 84.3%, which is the highest among all nine themes/concepts, indicating that most students can grasp the correct representation method of this concept.\nThe median value of semantic similarity is 0.9998, and 75% Region is concentrated within 0.02% of [0,1], indicating that the vast majority of students have a high degree of consistency in the representation of the form of circuits. Considering the high accuracy, it indicates that the majority of students' drawings are correct and consistent.\n3.2.2 Solar eclipse\nThe main content of this concept is to understand the phenomenon and causes of solar eclipses, but it does not require representation using light paths.\nThe average accuracy of LLM image recognition is 44%, which is basically at the average level of nine themes/concepts.\nThe median value of semantic similarity is 0.9999, and 75% Region is concentrated within the 0.001% range of [0,1], indicating that the vast majority of students have a high degree of consistency in the representation of this concept. However, it is worth noting that the accuracy of LLM image recognition is not high, indicating the possibility of \u201ccollective bias\u201d in students' representation. We found a possible reason for the high consistency rate but low accuracy rate when rechecking children's drawings: some students did not annotate any explanation (only drew three circles representing the sun, earth, and moon, without any signs), and the drawings did not look like them, which led LLM to mistake them for bicycle chains or other unrelated objects.\n3.2.3 Boiling\nThe main content (experiment) of this concept is to boil water through alcohol lamp. The textbook requires students to know the phenomenon of water boiling and the temperature changes during this process (i.e., it keeps rising until about 100 \u00b0C and no longer changes).\nThe average accuracy of LLM's image recognition is 0, ranking at the bottom of the nine experiments, indicating that all students' representations misled LLM.\nThe median value of semantic similarity is 0.967, and 75% Region is concentrated within the 1.49% range of [0, 1]. Although the 75% Region belongs to the second to last of the 9 concepts, the concentration is still very high, indicating that the vast majority of students have a high degree of consistency in the representation of boiling forms. However, the image recognition results of LLM are completely unrelated to this concept, indicating that almost all student representations have problems. This result is very surprising, as can be seen from the representative drawing where the problem lies: the vast majority of students' drawings focus on the experimental process and equipment, with almost no description of the changes in water during this process. This experiment does indeed require the use of these experimental equipment, but these devices are not what this class is going to learn, which means that students' focus is entirely on the experimental activity itself - rather than what the experiment is going to explain (changes in water), which is truly unexpecting.\nIt should be noted that we modified a keyword while tuning the model training related to this concept. Due to the significant difference between the results provided by the TF-IDF and our feeling, we conducted a retrospective analysis of all processes and found that an important word, \"chemistry,\" was not recognized as a keyword by TF-IDF. However, this word appeared very frequently in the reasons given by LLM (about 49% of reason sentences mentioned this word). Since the TF-IDF algorithm is a statistical method based on word frequency, it cannot truly understand the meaning of the text, and the word \"chemistry\" appears only once in each sentence, far less frequently than \"this\", so it is not surprising that it is excluded by algorithm. Based on this, we selected the function word with the lowest weight from the 10 keywords provided by TF-IDF, replaced it with \"chemistry\", modified the corresponding word weight, and attempted to retrain. The results were very close to our intuitive feeling.\n3.2.4 Solar system\nThe main content of this concept is to understand the solar system, especially the Sun and the eight major planets and their relative positions, but it does not require students learn the motion state and trajectory.\nThe average accuracy of LLM image recognition is 0.65, which is higher than the median value, indicating that most students can correctly represent this concept.\nThe median value of semantic similarity is 0.9999, and 75% Region is concentrated within the 0.001% range of [0, 1]. Similar to the concept 'Circuits', high accuracy and consistency in representation indicate that children have a correct and consistent understanding of this concept.\n3.2.5 Increasing the carrying capacity of a boat\nThe main content of this theme in textbook is understanding the relationship between buoyancy and the volume of displacement by changing the volume and load capacity of the boat. But the presentation of this lesson in the textbook is quite unique: first, boats with different bottom areas were made using aluminum foil of the same area, and then small iron hoops were added to different boats to increase their weight and test their load-bearing capacity. Ultimately, guide students to identify factors associated with buoyancy. That is to say, students cannot directly adjusted variables, but need to first created a 'boat'.\nThe average accuracy of LLM's image recognition is 0.033, second only to \"boiling\" and ranking second to last among the nine concepts, indicating that the vast majority of students' representations have misled LLM.\nThe median value of semantic similarity is 0.987, and 75% Region is concentrated within the 1.29% range of [0, 1]. Overall, the result of this concept is very similar to \"Boiling\": with low accuracy but high semantic concentration, indicating that children's drawing misled LLM in the same representation way.\nHowever, the reason for this result differs greatly from the concept of \"Boiling\", which is due to students' excessive focus on the experimental process rather than the experimental purpose, and the problems with this theme are more caused by the textbook. As mentioned above, the original goal of this lesson was to understand the relationship between buoyancy and the volume of displacement. However, in terms of presentation, the textbook first requires students to use aluminum foil to make a boat, and then complete the task of adjusting the weight. That is to say, there is a 'plot transition' in this lesson, and from the representative drawing below, it can be seen that the vast majority of children were stuck at this transition point - they were most impressed with how to make a boat, rather than adjusting variables. Therefore, the most frequently depicted object in the drawing was a square aluminum foil paper with creases left by the boat making around it. This directly led to errors in the overall direction of LLM: from the perspective of word frequency statistics, 59.6% of LLM's image recognition reasons showed that it was a square box like object that had nothing to do with the boat.\n3.2.6 Life history of a plant\nThe main content of this theme in the textbook is the life history of a plant, which refers to the entire process of the plant from seeds to flowering and fruiting. In the previous classes, students have already gone through tasks such as cultivating and caring for plants, and recorded the growth and changes of the plants. Therefore, the main purpose of this lesson is to establish a comprehensive impression of the entire process of the plant development and changes through previous records.\nThe average accuracy of LLM image recognition is 0.712, ranking second among the six concepts, indicating that most students can correctly represent this theme.\nThe median value of semantic similarity is 0.9999, and 75% Region is concentrated within the 0.001% range of [0, 1]. Explain that children have a correct and consistent understanding of this concept.\n3.2.7 Buoyancy\nThis theme is very similar to the goal of \"increasing the carrying capacity of a boat\", which is to control the sinking or floating of the boat by changing its weight and shape. These two themes come from two different versions of textbooks, so there is some overlap in content. But unlike 'increasing the carrying capacity of a boat', this lesson did not ask students to make a boat, but instead posed the core question at the beginning: how to keep a small boat floating on the water sink? Therefore, when learning this theme, the entire class was focused on exploring how to adjust variables and control the buoyancy of the boat, not to make a boat. So, as will be seen in the following text, there is a significant difference between the results than \"Increasing the carrying capacity of the boat\".\nThe average accuracy of LLM's image recognition is 0.5, ranking six among 9 concepts, indicating that a considerable number of students' drawings misled LLM, but it is significantly better than \"Increasing the carrying capacity of the boat\".\nThe median value of semantic similarity is 0.944, with 75% Region concentrated within [0, 1] of 3.38%, ranking last among all 9 concepts. After rechecking all the drawings, we found the main reason for the divergence of semantic similarity: the textbook of this lesson did not strictly define what is a \"boat\", so as shown in the representative drawing below, some students used foam plastic and leather balls. We also found such as plastic balls, wood blocks, iron boxes, well-made submarine models and warship models were used in some drawings. It is precisely because the textbook does not impose too many restrictions that students exhibited rich imagination, which also leads to the diversity of LLM's descriptions of the reasons for image recognition.\n3.2.8 Electromagnetics\nThis concept in textbook is mainly reproduced the experiment of electromagnetic phenomenon discovered by Danish scientist Hans C. Oersted in 1820. And enhance electromagnetic phenomena through wire winding to make coils, to explore related factors that affect the strength of this phenomenon.\nThe average accuracy of LLM image recognition is 0.514, ranking fourth among 9 concepts.\nThe median value of semantic similarity is 0.9995, and 75% Region is concentrated within the 0.11% range of [0, 1], indicating that students have strong consistency in their representation of this concept.\n3.2.9 Physical and chemical changes\nThe main content of this theme is to explore various changes in the kitchen, and to understand the difference between \"physical changes\" and \"chemical changes\" based on whether new substances are produced. This theme mainly corresponds to the \"Next Generation Science Education Standards\" (NGSS): \"When two or more different substances are mixed, a new substance with different properties may be formed\" (5-PS1-4) [20].\nThe average accuracy of LLM image recognition is 0.513, ranking fifth among the 9 concepts, that is, in the middle.\nThe median value of semantic similarity is greater than 0.9999, and 75% Region is concentrated within almost 0% range of [0, 1], indicating that students have a huge strong consistency in their representation of this theme. It should be noted that, as shown in the representative drawing below, children exhibit two completely different tendencies towards drawing this theme: one is to express it through graphics, and the other is to express it through mind maps. However, from the perspective of semantic similarity, the difference between these two forms of expression does not affect the understanding of semantics by the word2vec algorithm, because even when using graphics to represent this theme, a considerable number of students have added prompts in the drawings. The detailed information will be elaborated in the \"Discussion\" section of this article.\n3.3 Accuracy of Human Recognition\nDue to the fact that the above results were generated by LLM's recognition, in order to ensure the reliability of the results, we recruited 45 adults to recognize 27 drawings (3 randomly selected drawings for each theme), and analyzed their correctness and reasons given. The reason why we only selected 27 drawings is because once the number of drawings on the same theme exceeds three, the participants will guess the theme through previous drawings, this is something LLM cannot do.\nIn terms of accuracy, 85.19% of the themes of drawings that humans and machines made roughly consistent judgments, indicating that the machine's results in terms of image recognition accuracy are reliable. However, in terms of semantic similarity, the average median value of the nine concepts of human similarity is only 0.372, which is far from the results of LLM. Due to the lack of further evidence, we are unable to explain this result. However, there is a logical possibility that human language generation ability is much stronger than that of machines, and therefore, the diversity of reasons for image recognition is much more complex than LLM, resulting in a significant difference in semantic similarity between machines and humans.\n3.4 Analysis of Related Factors\nAs mentioned above, this article uses Kendall rank correlation coefficient (Kendall-t) to analyze the factors that affect drawing's representations.\n3.4.1 Relationship with sample sizes\nIs there a relationship between the above results and the total sample size? We separately calculated the correlation coefficients between \"sample size\" and three factors (\"accuracy\", \"median value of semantic similarity\", and \"75% Region\"), and the results showed that the correlation coefficients of the three factors were 0.611, 0.609, and -0.493, respectively. It can be seen that sample size has a strong correlation with accuracy and median value of semantic similarity, and has a moderate correlation with semantic similarity concentration, and this moderate correlation is not significant (p=0.07).\n3.4.2 Relationship with the abstract degree of themes/concepts\nBased on J. Piaget's theory of developmental stages in children, the abstract thinking ability of elementary school students has not yet been fully developed [21]. Therefore, we speculate whether the above results are related to the degree of abstraction of concepts? That is to say, the higher the degree of abstraction, the lower the accuracy and the lower the semantic similarity? However, how to define the degree of abstraction of concepts? Children and adults have different understandings of abstraction. Some adults believe that concrete concepts (such as mathematical laws of force or motion) are abstract and difficult to grasp in the eyes of children because they cannot be directly seen [22].\nFor this purpose, we coded the abstract degree of all 9 themes/concepts. The coding rule is: a phenomenon or something that can be seen or directly felt by sensory organs without any intervention was coded 1, otherwise it was coded 2. Based on this rule, the following encoding is obtained:\nWe invited 37 science teachers (excluding researchers) to evaluate the above coding rule, of which 31 fully agreed (83.8%); 4 out of the remaining 6 people do not agree with \u201cLife history of a plant\u201d, mainly because the textbook uses Impatiens as the main exemplar to show this theme, and there are fewer other plants involved; 3 people do not agree with \"Buoyancy\u201d, mainly because the lesson also includes the content of feeling buoyancy by pressing a small bottle, so it is not only the regulation of variables; One person simultaneously questioned both of the above themes. So overall, the above coding rule have been recognized by most science teachers.\nAs mentioned above, we still use Kendall-tau to analyze the correlation between abstract degree and \"accuracy\", \"median value of semantic similarity\", \"75% Region\". The results showed that the correlation coefficients of the three factors were -0.314, -0.301, and 0.246, respectively, indicating that the abstract degree of the concept is not significantly related to the above three factors.\n3.4.3 Reproducibility the contents in the classroom\nAnother factor we are concerned about is whether students will express abstract themes/concepts by reproducing contents that have appeared in class through drawings? Due to the elusive of these themes/concepts for children, textbooks often present them through representative examples, such as express the \u201cLife history of a plant\" through Impatiens. So, will students represent these difficult things by reproducing the examples that have appeared in the classroom? We used the examples that appeared in the textbook as the basis and calculated the proportion of these example words in the LLM's recognition reasons through word frequency statistics. And calculated the total proportion using weighted average (weighted by sample size).\n3.4.4 Relationship with the focus points\nThis is what we discovered after carefully examining the drawings: we found that the concept of \"boiling\" originally required students to understand the process of water to steam change, but most of the students drew the experimental activities (i.e. the experimental apparatus). This experiment is an indispensable process for heating water, but not the purpose of the lesson. However, it was used by students as a representation of the phenomenon of water boiling. In other words, there is a significant deviation between the students' focus and the purpose in the textbook.\nTherefore, we developed a coding rule based on the role of experiments in lessons, and used it as an independent variable to conduct correlation analysis on the above three factors. The coding rule are as follows: those themes without experiments or experimental results were equivalent to the teaching purposes will be coded 1; The experimental results were not the teaching purposes, but only the indispensable process of teaching purposes will be coded 2 (simple drawing and hands activities are not considered experiments). The coding result is as follows:\nDue to the fact that the above coding rule involve a deep understanding of textbook content, we invited two senior science teachers (both of whom have senior professional titles) to review the above rules and received their unanimous agreement.\nThe correlation coefficients are -0.624, -0.488, and 0.465 indicating that the consistency between experimental results and teaching purposes can explain the accuracy very well. Specifically, it can effectively solve the problem of \"collective bias\" in students' drawings: students are more concerned with the experiment itself, but if the experiment deviates from the teaching purposes, students are likely to choose to remember the experiment rather than the real purposes. Meanwhile, there is a moderate correlation with the other two items (median value of semantic similarity value and 75% Region).\""}, {"title": "4. Discussion", "content": "This article outlines the representations of 9 scientific concepts in children's eyes through 1420 children's scientific drawings", "sample size\", \"abstract degree\", and \"focus points\" on drawings, and used word frequency statistics to explore whether children represented abstract themes/concepts by reproducing what was taught in class. It was found that accuracy (of LLM's recognition) is the most sensitive indicator, and data such as sample size and semantic similarity are related to it; The consistency between classroom experiments and teaching purpose is also an important factor, many students focus more on the experiments themselves rather than what they explain. In addition, most children tend to use examples they have seen in class to represent more abstract themes/concepts, indicating that they may need concrete examples to understand abstract things.\n4.1 The contradiction between accuracy and consistency of representation\nHowever, looking back at the entire result, we found a rather strange thing": "some themes/concepts (solar eclipse, buoyancy, electromagnetics, physical and chemical changes) have an accuracy rate close to"}]}