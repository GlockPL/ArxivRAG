{"title": "Compressing Model with Few Class-Imbalance Samples: An\nOut-of-Distribution Expedition", "authors": ["Tian-Shuang Wu", "Shen-Huan Lyu", "Ning Chen", "Zhihao Qu", "Baoliu Ye"], "abstract": "In recent years, as a compromise between privacy and performance, few-sample model\ncompression has been widely adopted to deal with limited data resulting from privacy\nand security concerns. However, when the number of available samples is extremely lim-\nited, class imbalance becomes a common and tricky problem. Achieving an equal number of\nsamples across all classes is often costly and impractical in real-world applications, and pre-\nvious studies on few-sample model compression have mostly ignored this significant issue.\nOur experiments comprehensively demonstrate that class imbalance negatively affects the\noverall performance of few-sample model compression methods. To address this problem,\nwe propose a novel and adaptive framework named OOD-Enhanced Few-Sample Model\nCompression (OE-FSMC). This framework integrates easily accessible out-of-distribution\n(OOD) data into both the compression and fine-tuning processes, effectively rebalancing\nthe training distribution. We also incorporate a joint distillation loss and a regularization\nterm to reduce the risk of the model overfitting to the OOD data. Extensive experiments\non multiple benchmark datasets show that our framework can be seamlessly incorporated\ninto existing few-sample model compression methods, effectively mitigating the accuracy\ndegradation caused by class imbalance.", "sections": [{"title": "1 Introduction", "content": "As deep learning technology has advanced, models have become larger and more complex,\nresulting in challenges related to computational resources and storage in real-world appli-\ncations. For instance, CNNs with millions of parameters cannot be deployed directly on\nedge devices like watches or cameras. Model compression has emerged as a key focus for\nachieving efficient inference with limited storage space. To compress the model, network\npruning motheds (Li et al., 2017; He et al., 2017) try to remove less significant weights or\nneurons, while knowledge distillation methods (Hinton, 2015) let the compact model learn\nfrom soft labels of the pre-trained model, and quantization methods (Nagel et al., 2019;\nRomero et al., 2015) try to reduce the precision of model weights and activations.\nThese compression methods have been successful in reducing the size of the model.\nHowever, they most commonly rely on a large number of samples to ensure the stability\nand accuracy of the model's performance. However, in real-world applications, particularly\nin sensitive fields such as medicine and finance, these methods are often confronted by\nprivacy and security concerns. In addressing this problem, few-sample model compression\nhas emerged as a prominent strategy for balancing privacy and performance and has received\nconsiderable attention in recent years.\nConsider a small hospital that is planning to implement an efficient intelligent diagnos-\ntic system. Due to its limited sample size, the hospital cannot train a high-quality model\nindependently. Moreover, it is prohibited to obtain complete case data from a larger hos-\npital due to privacy policies. In such contexts, the deployment of few-sample compression\nmethods has emerged as a promising solution. As shown in Figure 1, the large hospital can\nprovide a pre-trained model based on its sufficient datasets, and then the small hospital\ncan use its limited samples to compress and fine-tune this model, transforming it into a\nlightweight version. This kind of method facilitates efficient deployment of the model on\nintelligent diagnostic equipment.\nWhile previous few-sample model compression strategies have shown promising results in\naddressing privacy concerns and optimizing model performance with limited data, they have\nnot considered the problem of class imbalance in small sample settings. Existing research\non few-sample model compression usually adopts the N-way K-shot experimental setup,\nwhere the training set contains N classes, with K samples assigned to each class. However,\nthis setting overlooks the common issue of class imbalance in few-sample scenarios, where"}, {"title": "2 Related Work", "content": "Few-sample model compression aims to derive compact models from pre-trained overpa-\nrameterized networks using few samples. The main challenge is that the student model\ntends to overfit due to the scarcity of training data, leading to high inference errors. These\nerrors progressively escalate through layer-wise propagation and accumulation (Dong et al.,\n2017), severely compromising output reliability.\nTo mitigate this problem, existing approaches employ layer-wise optimization to enforce\nintermediate-layer consistency between the compressed and original models. For instance,"}, {"title": "2.2 Class Imbalance Problem", "content": "In classification tasks, class imbalance occurs when the sample sizes of different classes\nvary significantly (Ochal et al., 2023), causing the model to focus on majority classes and\nneglect critical minority-class information. Current solutions for addressing class imbalance\nproblems can be classified into three main categories: data-level methods, algorithm-level\nmethods, and hybrid approaches that combine both.\nData-level methods achieve class balance by either removing majority-class samples (Liu\net al., 2008; Lin et al., 2017; Mohammed et al., 2020) or generating additional minority-class\nsamples (Chawla et al., 2002; Sharma et al., 2022; Abdi and Hashemi, 2015). These methods\nare simple and effective, making them the most commonly employed strategies. However,\nthey also have limitations. For instance, removing majority-class samples may result in the\nloss of valuable information, leading to underfitting, while generating additional minority-\nclass samples may cause overfitting (Zhou et al., 2020), thereby impacting the model's\ngeneralization performance.\nAlgorithm-level methods (Zhou and Liu, 2005; Fern\u00e1ndez et al., 2018; He et al., 2024)\nattempt to mitigate the preference for majority classes by modifying existing machine learn-\ning algorithms. However, these methods often require extensive domain knowledge and\nexperimentation, which may not be ideal for few-shot scenarios. Hybrid methods typically\ncombine data-level or algorithm-level strategies with ensemble learning (Galar et al., 2011;\nChawla et al., 2003), taking advantage of the strengths of both strategies. Nevertheless,\nsuch hybrid methods inherit the limitations of the data-level or algorithm-level approaches\nand usually involve substantial computational overhead, making them difficult to implement\non lightweight devices.\nAlthough the class imbalance problem has been extensively studied across various tasks,\nresearch on the generalization performance of these methods in the context of few-sample\nmodel compression remains limited."}, {"title": "3 Preliminaries", "content": "Few-sample Model Compression aims to get a compact model from the pre-trained re-\ndundant model with few samples for multi-class classification, where the input space is rep-\nresented by $X \\in R^d$, and the label space $y$ consists of ${1, ..., K}$. Let $D_{full} = \\{(x_i, Y_i)\\}_{i=1}^K \\in$\n$X \\times Y$ denote the full dataset containing N samples, and $D_{few} = \\{(x_i, Y_i)\\}_{i=1}^M \\in X \\times Y$ rep-\nresent the training dataset, consisting of M samples, where M \u00ab N. Let $m_j$ denote the\nnumber of samples of class $j$ in $D_{few}$, such that $M = \\sum_{j=1}^K m_j$. $M_t$ is a redundant model\npre-trained on the full dataset $D_{full}$. Current mainstream few-sample model compression\nmethods typically consist of two stages: compression and fine-tuning. The following analysis\nwill be conducted separately for these two stages. In the compression stage, most methods\ncombine pruning and knowledge distillation as the primary compression strategies, although\nthere are cases where only one of these techniques is used. For instance, the PRACTISE\nmethod only employs pruning. To provide a more thorough introduction to the algorithmic\nprocess of few-sample model compression, the discussion in this section will be based on\nthe methods including both pruning and knowledge distillation.\nTaking a representative few-sample model compression method, cross distillation (Bai\net al., 2020), as an example, in the compression stage, they use pruning methods to remove\nredundant channels or residual blocks from the $M_t$, which effectively reduces the model's\nparameter count. Then they distill the knowledge from the $M_t$ to the pruned model and\nmake every layer have the same outputs as the corresponding layer in the $M_t$, ensuring\nthat the performance of the pruned model on the classification task can approximate that\nof the teacher model. In the final fine-tuning stage, they retrain the model on the training\nset to recover accuracy and further enhance performance, yielding a lightweight model.\nThroughout the entire process, they can only use a few-sample dataset $D_{few}$."}, {"title": "4 Method", "content": "Few-shot learning is particularly susceptible to class imbalance, as it is often impractical\nto expect an equal amount of data for each class when data is limited. The detrimental\neffects of class imbalance are well-documented, yet its impact within the context of model\ncompression remains insufficiently explored. In the following, I will analyze, from a theo-\nretical perspective, the adverse effects of class imbalance on model compression. Existing\nfew-sample model compression techniques typically integrate network pruning, knowledge\ndistillation, and fine-tuning. Therefore, I will individually assess the influence of class im-\nbalance on each of these three techniques.\nIn network pruning, we compress the model by removing redundant filters or channels.\nLet the activation output of the l-th layer be denoted as $A^{(l)} \\in R^{C_l\\times H_l\\times W_l}$, where $C_l$\nrepresents the number of channels (filters), and $H_l$ and $W_l$ denote the height and width of\nthe feature map, respectively. The objective of pruning is to rank the filters based on some\nimportance score $s_k^{(l)}$ and remove the channels with the lowest scores. The importance score"}, {"title": "4.2 Feasibility of Using OOD Data", "content": "Due to the lack of research on the class imbalance problem in few-sample model compression,\nwe considered Class Imbalance Few Shot Learning (CIFSL). During this investigation, we\ndiscovered that certain resampling methods can generalize well to few-shot scenarios (Ochal\net al., 2023). Consequently, we examined popular resampling methods from recent years,\nsuch as ROS (Random Over-Sampling) (Japkowicz and Stephen, 2002), which duplicates\nsamples from the minority class. However, these methods usually result in overfitting to\nminority classes. Other works have introduced synthetic samples to augment the minority\nclass without repetition (Chawla et al., 2002) and some recent approaches have leveraged\nunlabeled data from the distribution to compensate for the lack of training samples, while\nthese methods are highly costly in the context of few-sample model compression. Therefore,\nWei et al. (2022) use OOD data to rebalance class priors and they rigorously proved that\nincorporating OOD data into the training set can be harmless through Theorem 1:\nTheorem 1 When labels are uniformly sampled from the label space within the distribution,\naugmenting the training set $D_{mix} = D_{train} \\cup D_{out}$ does not affect the prediction of the\nBayesian classifier:\n$\\arg \\max_{Y\\in Y} P_{mix} (x|y)P_{mix}(y) = \\arg \\max_{Y\\in Y} P_s(x|y)P_s(y)$,\nwhere $P_{mix}(X,Y)$ represents the data distribution of $D_{mix}$.\nTheorem 1 indicates that when labels are uniformly sampled from the label space of the\nin-distribution data, augmenting the training set with OOD instances does not alter the\nBayesian classifier's prediction. Based on this, we can infer that the feature space of the"}, {"title": "4.3 Our Method: A Framework Using OOD Data", "content": "In this section, we propose a novel framework called OOD-Enhanced Few-Sample Model\nCompression (OE-FSMC) for addressing the class imbalance in the few-sample model com-\npression, leveraging the Open Sampling technique. As shown in Figure 2, our approach\nconsists of three key components: (1) OOD Set Handling, (2) Compression Stage,\nand (3) Fine-tuning Stage.\nOOD Set Handling: Previous studies commonly utilized the entire TinyImage dataset,\nwhich contains approximately 300K randomly sampled images, as the OOD dataset. How-\never, under the constraints of model compression, limited storage capacity makes it im-\npractical to store such a large dataset. To address this, we pre-sample 500 images from\nthe OOD dataset using random sampling. During this process, we ensure the purity of the\nOOD dataset by removing any overlapping samples with the full training dataset $D_{full}$. The\nexperimental section provides a detailed analysis of the impact of n (the size of the OOD\ndataset) and demonstrates that reducing the OOD dataset size has a negligible effect on\naccuracy.\nCompression stage: When compressing the pre-trained teacher model, we randomly\nsample m (discussed further in the experimental section) instances from the OOD dataset\nto construct an auxiliary dataset. Labels for these OOD samples are assigned based on\nEquation 4, ensuring balanced attention across all classes during the compression process.\nDuring network pruning, the sparse representation of minority-class features in the pa-\nrameter space makes their critical channels vulnerable to excessive removal. This can lead"}, {"title": "5 Experiment", "content": "To address the following research questions, we conducted extensive experiments on three\npublicly available datasets:\n\u2022 RQ1: Is class imbalance a significant issue for few-sample model compression meth-\nods?\n\u2022 RQ2: Can our framework effectively mitigate the issue of class imbalance in few-\nsample model compression, and is it compatible with current state-of-the-art few-\nsample model compression methods?\n\u2022 RQ3: How effective is each component of our framework?"}, {"title": "5.1 Experiment Setting", "content": "Data Description: To validate that our framework effectively alleviates the class imbal-\nance issue in few-sample model compression, we select image classification datasets CIFAR-\n10/100 and ILSVRC-2012. In the few-shot setting, we sample 10/30/50 data points from\nthe original datasets. For each class, the number of samples is set according to the imbal-\nance standard in long-tailed CIFAR-10/100 (Krizhevsky and Hinton, 2009). The samples\nare placed under the most common imbalance type\u2014a long-tail distribution, with the im-\nbalance ratio set to 100.\nCompared Method: We demonstrate that our framework effectively alleviates the class\nimbalance issue in few-sample model compression by integrating it with the following meth-\nods:"}, {"title": "6 Conclusion", "content": "The class imbalance problem in the few-sample model compression is a critical problem\nthat has been overlooked by previous methods. In this paper, we propose a novel ex-\nploratory framework called OOD-Enhanced Few-Sample Model Compression (OE-FSMC),\naimed at mitigating the negative impact of class imbalance on few-sample model compres-\nsion. Specifically, we leverage easily accessible OOD data to balance both the compression\nand fine-tuning processes. In addition, we implement a joint distillation loss and a regular-\nization term to prevent the model from overfitting to the OOD data. Extensive experiments\nconducted across a wide range of few-sample model compression strategies and benchmark\ndatasets demonstrate the effectiveness and generality of our framework. In future work,\nwe will explore further applications, including its generalization to compression strategies\nthat are not commonly utilized in current few-sample model compression methods, such as\nmodel quantization."}]}