{"title": "A Survey on Adversarial Robustness of LiDAR-based Machine Learning Perception in Autonomous Vehicles", "authors": ["Junae Kim", "Amardeep Kaur"], "abstract": "In autonomous driving, the combination of AI and vehicular technology offers great potential. However, this amalgamation comes with vulnerabilities to adversarial attacks. This survey focuses on the intersection of Adversarial Machine Learning (AML) and autonomous systems, with a specific focus on LiDAR-based systems. We comprehensively explore the threat landscape, encompassing cyber-attacks on sensors and adversarial perturbations. Additionally, we investigate defensive strategies employed in countering these threats. This paper endeavors to present a concise overview of the challenges and advances in securing autonomous driving systems against adversarial threats, emphasizing the need for robust defenses to ensure safety and security.", "sections": [{"title": "I. INTRODUCTION", "content": "In the field of autonomous driving systems, the combination of Artificial Intelligence (AI) and vehicular technology has opened new possibilities. These advanced systems enable unmanned vehicles, including drones and other autonomous platforms, to not only perceive their surroundings but also make real-time decisions while navigating complex traffic scenarios. With the potential to revolutionize various sectors, autonomous systems promise heightened safety, streamlined logistics, and the capacity for multitude of beneficial operations.\nHowever, beneath the promise of seamless automation lies a significant challenge: the vulnerability of these systems to adversarial attacks, which encompass a wide range of techniques, including those that share characteristics with Adversarial Machine Learning (AML) attacks. AML attacks, designed to manipulate machine learning models, typically involve the introduction of carefully crafted perturbations or alterations to input data. However, it is important to note that perturbing inputs represent just one facet of AML attacks [1]. The diverse AML landscape includes other techniques like backdoor data poisoning [2], where an adversary can inject a small number of poisoned samples with a backdoor trigger into the training data. When activated during deployment, these triggers manipulate the machine learning (ML) model. These alterations may be imperceptible to humans but can signif-icantly disrupt the operation of the ML system, potentially causing malfunction.\nThe shared characteristics between adversarial threats and AML attacks include the ability to add noise or manipulate minor parts of inputs in ways unrecognizable by humans, with the ultimate target being the ML system, leading to its malfunction. By exploiting vulnerabilities in the system, adversarial attacks aim to create inputs that appear legitimate but are subtly altered to trick the system. This survey paper sets out to explore the intersection of adversarial attacks from an AML perspective and ML-based autonomous driving, addressing a significant subject in this domain.\nThe autonomous driving systems are categorized into six levels by the Society of Automotive Engineers [3], ranging from Level 0 indicating no automation to Level 5 representing complete automation, as illustrated in Figure 1. While Level 5 automation is yet to be achieved, ongoing testing efforts are underway [4]. Legal constraints in numerous countries also limit the testing and deployment of Autonomous Vehicles (AV).\nAs we progress beyond Level 2 automation, the reliance on sensors like LiDAR (Light Detection and Ranging) radar, and cameras, coupled with machine learning algorithms be-comes crucial [5]. Specifically, LiDAR's capability for pre-cise distance measurements and high-resolution 3D mapping capabilities plays a vital role in improving environmental perception and situational awareness [6]\u2013[10]. Over the last decade, LiDAR has become the most popular AV sensor as it enhances various aspects of autonomous systems, in-cluding obstacle detection, mapping, localization, and object recognition. Moreover, LiDAR-based autonomous systems are expected to have a significant impact on the Defence industry, promising efficiency and security in challenging terrains. Figure 2 illustrates the pipeline of AV systems with sensors and the ML system, and potential points of attack.\nHowever, like any technology, ML-based LiDAR systems are not immune to vulnerabilities. Various attacks are pos-sible, and our survey paper first concentrates on adversarial attacks, specifically traditional AML attacks and certain cyber-attacks on sensors that can cause malfunctions in ML-based perception modules. Attackers can employ AML techniques or cyber-attack methods to deceive or manipulate the perception systems of autonomous devices. These attacks can disrupt sensor inputs, potentially resulting in incorrect decisions and compromising safety and security. Subsequently, we proceed to investigate the theoretical aspects of defensive techniques against these adversarial attacks and analyze their limitations.\nThis survey paper addresses the gap in the existing literature by providing a comprehensive overview of ML approaches, adversarial attacks, and defenses specifically applicable to LiDAR-based systems in the context of autonomous driv-ing.\nWhile some existing survey papers explored ML in autonomous driving [11]\u2013[14], they have primarily concen-trated on ML functionality from a performance perspective, neglecting the importance of ensuring the robustness of the ML system against AML attacks or cyber-attacks on sensors targeting ML systems. In addition, unlike previous surveys that often focus on limited aspects of AML attacks, such as image spaces or specific attack methods [15] and/or defenses [12] without in-depth analysis of their limitations, our survey takes a broader approach, encompassing the diverse spectrum of adversary threats and defenses in LiDAR-based perception systems. Moreover, our survey places a significant emphasis on identifying and analyzing the limitations of existing AML defense strategies in this domain. To achieve this, we begin with a literature review, examining 3D LiDAR sensory data and Machine Learning (ML) approaches in autonomous driv-ing. By analyzing the current state of research on adversarial attacks, we aim to identify research gaps, emerging trends, and advancements in securing autonomous driving systems against adversarial threats.\nOur survey aims to make significant contributions to the field:\n\u2022 A Survey of 3D LiDAR-Based Machine Learning Mod-els: Our survey paper investigates 3D LiDAR-based ma-chine learning models in the context of autonomous driv-ing, with a specific emphasis on widely recognized and highly cited models. By summarizing these prominent models in existing research, we provide readers with a clear overview of the state-of-the-art in this crucial area.\n\u2022 Analyzing Adversarial Attacks and Defensive Strategies in 3D Autonomous Driving Systems: A central objective of our survey paper is to shed light on the evolving threat landscape facing autonomous driving systems. AML at-tacks and cyber-attacks present a significant challenge, potentially compromising the safety and security of these systems. Consequently, we conduct a comprehensive analysis of existing AML attacks and cyber-attacks tai-lored to the realm of 3D autonomous driving, as well as explore and analyze defensive strategies to mitigate these threats.\nOur survey covers a range of attacks, including cyber-attacks on sensors such as sensor spoofing, as well as physical attacks, and adversarial perturbations. We not only explain the methodologies behind these attacks but also explore potential defensive strategies and countermeasures. It is worth noting that existing defensive strategies often fall short of providing effective countermeasures. This paper highlights significant gaps in current research on autonomous system resilience against adversarial threats."}, {"title": "II. AUTONOMOUS VEHICLE (AV) SYSTEM", "content": "The processing pipeline of an AV system, depicted in Figure 2, comprises several stages that work together to enable the vehicle to operate autonomously. This pipeline includes sensors, preprocessing such as data pre-processing and sensor fusion, and ML modules like perception, decision-making, path planning, and control systems.\nSensors, such as cameras, LiDAR, GPS devices, etc., collect raw data from the environment. These sensors capture and relay information about the AV's surroundings in which the AV operates, including road infrastructure, traffic dynam-ics, environmental conditions, etc. for the AV's process. In this survey, we focus on AV systems using LiDAR sensors since LiDAR sensors offer a significant advantage over other sensors, such as radars and cameras, due to their higher resolution and precision. Additionally, LiDAR proves to be versatile, performing reliably under both daytime and night-time conditions [16], [17].\nSensor data including LiDAR point cloud data requires pre-processing before being supplied to the ML algorithms. Data preprocessing plays a crucial role in preparing and refining the raw sensor data for further analysis. It involves data cleaning, filtering, and feature extraction, ensuring that the data input to the subsequent ML stages is of high quality and relevance. Sensor fusion, another vital part of pre-processing, combines data from multiple sensors to enhance perception accuracy. This fusion process is not necessary if the system has a single sensor.\nThe ML component of the pipeline interprets and extracts meaningful insights from the pre-processed and fused sensor data. This step involves a range of tasks, including perception, decision-making, and route planning. The ML algorithms are central to providing an autonomous driving capability.\nThe perception module is responsible for sensing and com-prehending the vehicle's environment. It processes data from an array of sensors, including LiDAR, cameras, GPS, and others, enabling the recognition of objects, identification of road features, and a holistic understanding of the environment. ML algorithms, specifically designed for object detection, semantic segmentation, object tracking, and scene comprehen-sion, are integrated within this module. They work together to interpret sensor data, extract meaningful information, and create a representation of the environment that the AV can use for navigation.\nOnce the perception system has gathered and processed sensor data, the next key step is decision-making. This module is dedicated to making important decisions on how to interact with the environment, based on the perceived environment. Here, the ML algorithm determines the vehicle's actions, including lane changes, yielding to other road users, and managing emergency scenarios. It heavily relies on the insights gained from the perception and ML components.\nPath planning forms another integral part of the pipeline. It leverages map data and desired trajectory information to chart the optimal route for the AV. This component guides how the AV navigates from its current location to its destination, ensuring safe and efficient travel.\nFinally, the control system executes the actions defined by the decision-making and path-planning stages. It manages the physical aspects of the AV, including steering control and velocity management, to enact the planned route accurately.\nThroughout this intricate pipeline, there are various poten-tial points of weakness, primarily at the interfaces between components and in communication with external entities. The ML perception typically handles the sensor data directly and processes it to understand the vehicle's surroundings. On the other hand, ML decision-making or planning does not typically deal directly with sensor data but relies on the processed infor-mation from the perception module to make decisions about how the vehicle should act. Thus, from an adversarial attack perspective, the ML perception module is exposed to potential attacks, as it directly interfaces with sensor inputs. Adversarial attacks on the perception module can manipulate the way the vehicle perceives its environment, potentially causing it to make incorrect or dangerous decisions. The ML decision-making module or planning module relies on the accuracy of the perception data, so it can indirectly be affected by attacks on the perception module. We describe ML-targeted attacks in Section III-B. In addition, AVs often communicate with other vehicles and infrastructure through communication systems. These communication channels can be vulnerable to attacks such as typical cyber-attacks or cyber-attacks on sensors described in Section III-A. Adversaries can send false data, manipulate data, or disrupt legitimate communication between vehicles, potentially causing accidents or traffic congestion. Therefore, robust security measures and vigilant oversight over the AV system are essential for protecting the AV against potential adversarial threats, ensuring the reliability and safety of autonomous driving technology.\nIn this survey, our primary focus is the robustness of LiDAR-based autonomous driving systems. The LiDAR sys-tem generates data that reveals the presence of obstacles in the environment and the vehicle's relative position to these obstacles. This data offers insights into the contours of roads, nearby infrastructure, and vegetation. In the following subsec-tion, we investigate the LiDAR system, the characteristics of LiDAR data, and the machine learning approaches applied to analyze this data. We then provide a literature survey regarding the ML perception modules that are related to the robustness of AV systems."}, {"title": "A. LiDAR System", "content": "In a typical LiDAR system, a laser emits infrared light pulses at various angles onto target objects. The LiDAR sensor subsequently detects and processes the reflected echoes, creat-ing a 3D point cloud. Each point in the cloud is represented by a 3-dimensional vector (x, y, z), providing spatial coordinates within the LiDAR coordinate system. Additional features like color or intensity can be integrated if needed. This point cloud is a fundamental element that can be further processed and analyzed for a wide array of applications, including environment and object perception, localization, and object recognition.\nLiDAR, however, has limitations. It requires a direct line of sight of objects and may face challenges in environments with obstacles or occlusions [9], [18]. A LiDAR can only see objects that reflect its signal. If the signal does not return, whether due to absorption, transparent materials, or reaching its range limits, LiDAR interprets it as an absence of objects. Reflective surfaces may introduce data inaccuracies or false readings in LiDAR measurements. Furthermore, adverse weather conditions such as heavy rain, fog, or snow can disrupt LiDAR data collection, diminishing its reliability under unfavorable weather conditions."}, {"title": "B. LiDAR Point Cloud", "content": "Three-dimensional LiDAR provides high-resolution point clouds with accurate representations of the length, width, and height of objects, offering a comprehensive view of the environment. However, 3D LiDAR generates a substantial amount of data per scan due to its detailed output. The extraction and interpretation of geometric details from 3D range data are notably more intricate in comparison to 2D data. Moreover, 3D laser data introduces an additional complication as the lower layers of the scanner frequently capture ground or floor surfaces, further adding to the complexity of data analysis.\nReliable perception of the surrounding environment is typi-cally achieved through two subtasks: simultaneous localization and mapping (SLAM) and detection and tracking of mov-ing objects (DATMO) [22]. SLAM aims to create a map comprising static elements of the environment, providing the vehicle with knowledge of its surroundings. On the other hand, DATMO utilizes this map to detect and track dynamic objects in real-time.\nPoint clouds are characterized by several properties that make them challenging to work with:\n\u2022 Massive: Point clouds can be massive, especially when representing complex scenes or large environments. Each point in the point cloud requires storage for its 3D coordinates and potentially additional features like color or intensity.\n\u2022 Noise: Point clouds often contain noise, which refers to unwanted or random variations in the measured point coordinates or features. Noise can be caused by numer-ous factors, such as sensor inaccuracies, environmental conditions, or reflections.\n\u2022 Incomplete: In real-world scenarios, it is challenging to obtain complete and perfectly sampled point clouds. Incomplete point clouds arise when some parts of the 3D scene are occluded, not captured by the sensor, or simply missing due to limitations in data acquisition.\n\u2022 Irregular: Point clouds are considered irregular because there is no predefined structure or grid-like organization to the points. Unlike images, which have a fixed grid of pixels, points in a point cloud can be distributed arbitrarily in 3D space.\n\u2022 Unstructured: Point clouds lack inherent connectivity or spatial relationships between points. Each point is independent, with no knowledge of its neighbors or their arrangement. They are collected from laser reflections off surfaces, without a predefined pattern like images.\n\u2022 Unordered: The order of points in a point cloud is arbi-trary, and there is no predefined sequence or arrangement. Different scans or data acquisition processes can result in different orderings of the points in the point cloud.\n\u2022 Sparse: Although LiDAR provides accurate distance mea-surements, the point clouds are sparse and have non-uniform densities across the scene. This sparsity is a result of factors such as the sensor's range, resolution settings, and the geometry of the environment being scanned. It means that there are fewer points available to accurately capture the nuances of the environment, mak-ing it harder for ML algorithms to accurately understand and navigate their surroundings.\nThese properties make point cloud processing a challenge in the field of computer vision and 3D data analysis. Spe-cialized algorithms and deep learning architectures, such as PointNet [23], have been developed to handle these properties and extract meaningful information."}, {"title": "C. Machine Learning for LiDAR Data", "content": "The primary focus of this paper is on the ML perception module, which is considered a central point of vulnerability due to its pivotal role in comprehending the environment. While decision-making is also a significant concern, it typ-ically relies on the information processed by the perception module to guide the vehicle's actions. Furthermore, commu-nication vulnerabilities, while important, may have different implications, as they can impact the ML perception modules. Therefore, we prioritize our focus on ML perception, as it is directly impacted by adversarial threats.\nAs introduced earlier, 3D point cloud data holds a significant role in autonomous driving, serving as a vital component within the perception systems of self-driving vehicles. Process-ing LiDAR data can be challenging due to the characteristics of point clouds outlined in Section II-B. Traditional deep learning techniques, such as Convolutional Neural Networks (CNNs), have primarily been designed to work with data organized on structured grids, like 2D images (composed of pixels) or 3D volumes (represented as voxels). In such grid-based structures, adjacent data points have well-defined relationships, enabling efficient application of convolutional operations. In contrast, point clouds possess inherent structural irregularity. They consist of individual points scattered throughout 3D space, lacking any predefined grid or regular arrangement. Notably, LiDAR-generated point clouds are less susceptible to adversarial attacks compared to images [24]\u2013[26], making it more challenging to launch attacks on LiDAR-based AVs. Nevertheless, it is important to recognize that, while challeng-ing, such attacks are not impossible and there has been a recent increase in their prevalence and sophistication as discussed in Section III.\nThere have been several survey papers on deep learning approaches using 3D LiDAR data [11]\u2013[14], [27]. In this section, we highlight the most popular approaches commonly targeted for AML attacks. We explore a range of popular deep learning models that differ in their approaches, especially regarding feature extraction, which is a principal factor in processing 3D point clouds.\nPointNet [23] is a pioneering deep learning model that addresses the challenge of processing irregular and unstruc-tured 3D point cloud data. PointNet's main goal is to extract meaningful features and patterns from these point sets without relying on any predefined order or connectivity. It achieves this by utilizing continuous symmetric functions, approximated through shared Multi-Layer Perceptrons (MLPs), to process the 3D coordinates and additional features, such as colors, of each point independently. As a result, PointNet exhibits permutation invariance, enabling it to handle point clouds regardless of the order in which points are presented. Addition-ally, PointNet generates embeddings for each point within a point cloud, and these embeddings undergo transformation and aggregation to ensure invariance to geometric transformations, such as rotations or translations. This property ensures that the classification results remain unaffected by rotations of the input point clouds.\nPointNet++ [28] is an extension of PointNet that introduces a hierarchical feature learning approach. It organizes the points into nested sets and processes them hierarchically, enabling the model to capture both local patterns and global context in the point cloud data. PointNet++ is specifically designed for point set segmentation tasks. It achieves this by subsampling the point cloud into overlapping regions (query points) and processing each region using a 'Set Abstraction Layer' to extract local features. These features are then interpolated and propagated back to the original points to capture global contextual information.\nPointNet and PointNet++ serve as foundational architectures for processing raw 3D point cloud data directly. PointNet-based models are commonly employed in tasks such as object detection, segmentation, and scene understanding. Frustum PointNet [29] extends the PointNet architecture to fuse LiDAR and camera data for increased perception reliability. The Frustum PointNet approach begins by creating 3D bounding boxes called frustums around objects initially detected in 2D images. These frustums define regions of interest within the 3D point cloud data. Subsequently, PointNet is applied to process the point cloud data within these frustums, enabling tasks such as object detection and localization in 3D space. Similar to Frustum PointNet, Frustum ConvNet [30] uses PointNet operations at lower layers of its network architecture. However, Frustum ConvNet employs convolutional layers as part of its architecture, for capturing spatial relationships and aggregat-ing local features, whereas Frustum PointNet relies on fully connected layers. At these lower layers, Frustum ConvNet may share similarities with Frustum PointNet in terms of processing individual points within frustums using shared MLPs. Frustum ConvNet extends beyond this by incorporating convolutional layers, which enable it to capture spatial relationships and context more effectively.\nVoxel grids are used to convert point clouds into a 3D grid format, making it easier to apply 3D convolutions. VoxelNet [20] is a deep learning model that leverages 3D voxel grids to represent 3D data. It introduces a 3D detection network tailored for precise object detection in sparse LiDAR point clouds. The approach involves partitioning the point cloud into uniformly spaced 3D voxels, a random selection of a fixed number of points from each voxel, and utilizing a voxel feature encoding layer to construct a comprehensive volumetric representation. VoxelNet unifies the tasks of feature extraction and bounding box prediction within a single, end-to-end trainable deep network, eliminating the necessity for manual feature engineering. In 3D object detection workflows, PointNet can be utilized as a feature extractor to process individual points within the voxels generated by VoxelNet. This facilitates more detailed feature extraction at the point level within each voxel. It is important to note that PointNet is not an intrinsic component of the VoxelNet architecture; its integration depends on the specific implementation. Fur-thermore, VoxelNet organizes point cloud data into a 3D voxel grid, effectively establishing a Bird's-Eye View (BEV) representation, which is particularly advantageous for object detection tasks.\nSECOND [21] introduced sparsity-aware convolutional lay-ers, which make it efficient in handling sparse LiDAR data without the need for voxelization. LiDAR sensors often pro-duce sparse point cloud data, meaning that some regions may have no data points at all, and others may have varying point densities. SECOND employs sparsity-aware convolu-tional layers, which are specialized for processing sparse data efficiently. Instead of applying convolution operations to all points uniformly, they adaptively select and process only the relevant points, effectively ignoring empty regions with no points. In addition, when processing a point in a sparse region, the sparsity-aware layers consider their local neighborhood of points. This neighborhood may vary in size and shape depending on the point distribution. The layers extract features from this dynamic neighborhood.\nPointPillars [19] initially transforms the original point cloud data into a top-down, 2D BEV representation of the 3D environment. This 2D perspective is further analyzed and feature extraction is performed using 2D convolutional layers. The BEV representation is then subdivided into a grid com-posed of \"pillar-like structures\". These pillar-like structures are essentially the grid cells used to organize the 3D point cloud data in the BEV representation. Within each of these pillars, PointPillars utilizes 2D convolutional layers to process the points and extract features. PointPillars operates in real-time for object detection, allowing it to predict object candidates belonging to multiple classes. It provides estimations of their 3D-oriented bounding boxes and associated class confidence values.\nPIXOR [31] is another 3D object detection architecture that also has its feature extraction approach tailored to the task. PIXOR operates on 2D camera images, specifically focusing on LiDAR-camera fusion for 3D object detection from BEV. The model utilizes a 3D occupancy grid representation with accumulated reflectance and employs a classification heat map and regression features for object localization. The BEV representation is preferred due to its accuracy and avoidance of object overlap, enhancing computational efficiency. The top-down perspective in BEV eliminates depth ambiguity ensuring clear separation of objects at different distances. PIXOR is one of the fastest LiDAR object detection models and is further improved in PIXOR++ [32].\nPointRCNN [33] is an approach in 3D object detection compared to previously mentioned methods like PointNet, VoxelNet, PointPillars, and PIXOR, in terms of its feature extraction and overall approach. Unlike traditional CNNs designed for structured grids, PointRCNN operates directly on unstructured point cloud data, allowing it to capture intricate spatial information for precise object detection in 3D scenes. PointRCNN introduces a two-stage detection framework that first generates region proposals using a 'Region Proposal Network' and then refines these proposals using 3D CNNs. The proposals are often represented in a BEV for object de-tection. Both PointRCNN and PointNet are capable of directly handling unstructured 3D point cloud data, but PointRCNN is specifically designed for accurate 3D object detection and localization tasks, while PointNet is for a broader range of tasks beyond object detection, including segmentation, classi-fication, and scene understanding due to the strengths of its permutation and transformation invariance.\nMotionNet [34] presents a distinctive approach to motion prediction using 3D point cloud data. It achieves this by converting 3D point clouds into BEV maps, streamlining subsequent computations. The model encompasses various features, including the ability to generalize to unseen objects, integration of temporal information, the capture of multi-scale spatio-temporal features, etc. MotionNet's architecture relies on standard 2D and pseudo-1D convolutions, rendering it well-suited for real-time operations in autonomous driving scenarios. However, its performance may exhibit variability based on the specific application.\nSome studies focus on the fusion of multiple sensors with LiDAR. Ku et al. [35] introduce an approach for 3D object detection that can jointly generate 3D object proposals and perform object detection by aggregating information from multiple views or camera angles. This multi-view approach is designed to enhance the accuracy and robustness of 3D object detection, especially in situations where objects might be partially hidden or obstructed when viewed from a single perspective. EPNet [36] is also a system that combines in-formation from 3D data sources with image data to enhance object detection capabilities. It integrates image semantics to provide additional context and information, which in turn improves the accuracy of object detection.\nIn addition to the above-mentioned approaches, various other deep learning approaches, including those proposed by Huang et al. [37] and Priya and Pankaj [38], have been developed to process 3D LiDAR point cloud data. Given the inherent vulnerabilities in deep learning, it is necessary to prioritize efforts aimed at identifying relevant adversarial attacks and defenses. In the following section, we will explore adversarial attacks."}, {"title": "III. ATTACKS", "content": "In the realm of computer science, an 'adversary' refers to those attempting unauthorized access or corruption of a net-work. Originating in 2004 for anti-spam filter robustness [39], adversarial machine learning has evolved to challenge the security of ML models. Autonomous vehicles, heavily reliant on a diverse network of sensors, including LiDAR, radar, cameras, and GPS, leverage advanced ML algorithms for processing multi-modal inputs. These algorithms play a pivotal role in environment perception and vital operational decisions. While sensors are traditionally deemed trusted components in AV control systems, any compromise to their integrity, resulting in falsified readings, introduces significant risks to both vehicle safety and security. These risks stem from a two-fold vulnerability: both sensors and ML algorithms are susceptible to adversarial attacks.\n\u2022 Sensors: The sensors themselves are susceptible to a range of conventional cyber-attacks, including authenti-cation breaches, Denial-of-Service (DoS), jamming at-tempts, or even direct physical attacks [40]\u2013[43]. Along-side these established threats, malicious actors can exploit these vulnerabilities by targeting the AV communica-tion channel to execute various deception attacks, such as Sybil, spoofing, or replay attacks as elaborated in Section III-A. These deceptive tactics can result in the manipulation or corruption of sensor data, leading to erroneous perceptions and decisions by the autonomous vehicle.\n\u2022 ML Algorithms: Beyond sensor vulnerabilities, AVs face a distinct susceptibility to adversarial targeting in the realm of ML models. Attackers can exploit weaknesses in these algorithms, deceiving vehicles into making un-safe decisions. Such manipulations may include subtle alterations to the training data, injection of backdoor triggers, or the introduction of carefully crafted adver-sarial perturbations into the sensory inputs, challenging the ML model's recognition capabilities. Notably, these perturbations do not alter the semantic meaning of the scene but wield a transformative influence on the ML model's output. Section III-B provides a comprehensive exploration of adversarial attacks on ML algorithms."}, {"title": "A. Attacks on Sensors", "content": "LiDAR sensor systems are susceptible to attacks that can compromise their integrity, availability, and accuracy [44]\u2013[46]. These attacks intend to mislead autonomous systems by altering data from LiDAR sensors, which can result in incorrect perceptions and unsafe decisions by autonomous vehicles.\nAttackers can accomplish this by emitting deceptive signals to manipulate distance measurements or introducing false ob-jects to the LiDAR perception system. We focus on the cyber-attacks aiming to disrupt or manipulate the LiDAR readings which eventually lead to disrupting the ML decision making. Since sensors are typically considered trusted components in an AV's control system, falsified readings could lead to unforeseen consequences if the sensors are compromised.\nSensor attacks typically occur within the vehicle's commu-nication channel and have the potential to manipulate LiDAR sensor data in several ways, such as creating fake objects (Sybil attacks), injecting malicious points in LiDAR data (Spoofing attacks), or even replaying outdated point clouds (Replay attacks) to deceive the AV system.\n1) Spoofing Attacks: Spoofing, a form of masquerading attack, severely impacts the trustworthiness of LiDAR systems. In spoofing attacks, a spoofing device emits laser pulses toward the victim LiDAR, which disrupts the timing of the laser reception events and consequently, alters the calculated 3D positions of objects. This manipulation results in the creation of 'spoofed points' within the LiDAR's point cloud. These spoofed points can lead to object misdetection by the down-stream object detector. For instance, the attack can relocate points originally associated with an object [47] or even render the object undetectable [48]. Alternatively, spoofed points clustered together can falsely trigger the detection of a non-existent object [49]. Spoofing attacks can be synchronized or asynchronized. The synchronized attacks require precise knowledge of the victim LiDAR's scanning pattern beforehand to synchronize the malicious laser firing timing [50], [51]. Asynchronized attacks do not need such knowledge and thus are more deployable [47], [52].\nPeti"}, {"title": "B. Attacks on ML", "content": "This section explores studies on attacks targeting ML models behind LiDAR-based perception and decision-making processes", "68": "in 2013", "69": "poisoning attacks to corrupt model training [70", "71": ".", "50": [72], "73": [75], "24": [26], "75": "."}, {"24": [76], "78": ".", "21": "."}, {"24": ".", "77": ".", "79": ".", "78": [80], "81": "proposed a theoretical framework for analyzing DNNs' robustness in semantic space and introduced a method to detect robust regions within networks. Through extensive experiments"}, {"24": [25], "76": "adversarial attacks encompass a range of techniques often involving subtle alterations. Since LiDAR data may already contain some degree of noise due to sensor limitations", "25": ".", "types": "Evasion attacks", "Attacks": "Evasion attacks target trained ML models during inference time [2"}, {"24": "pioneered an optimization algorithm based on the Carlini and Wagner framework [82", "83": "extended upon the Fast Gradient Sign Method (FGSM) [72", "attacks": "distributional attacks that involve imperceptible perturbations to the distribution of points, and shape attacks that involve deforming the shape represented by a point cloud. Their method constrained the perturbation magnitude onto the sur-face of an epsilon ball in different dimensions. Zhou et al. [84", "85": "introduced a malicious point-dropping technique by learning a saliency map. A saliency map assigns scores to points in a point cloud data, reflecting their con-tribution to the model recognition loss. High scores highlight significant segments in the 3D scene. Yang et al. [86", "87": "employing a pointwise gradient method to update only the attached points without altering the original point cloud. Additionally, Wicker and Kwiatkowska [88"}]}