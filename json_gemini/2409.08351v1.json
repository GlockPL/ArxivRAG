{"title": "Bayesian Inverse Graphics\nfor Few-Shot Concept Learning", "authors": ["Octavio Arriaga", "Jichen Guo", "Rebecca Adam", "Sebastian Houben", "Frank Kirchner"], "abstract": "Humans excel at building generalizations of new concepts\nfrom just one single example. Contrary to this, current computer vision\nmodels typically require large amount of training samples to achieve\na comparable accuracy. In this work we present a Bayesian model of\nperception that learns using only minimal data, a prototypical proba-\nbilistic program of an object. Specifically, we propose a generative in-\nverse graphics model of primitive shapes, to infer posterior distributions\nover physically consistent parameters from one or several images. We\nshow how this representation can be used for downstream tasks such\nas few-shot classification and pose estimation. Our model outperforms\nexisting few-shot neural-only classification algorithms and demonstrates\ngeneralization across varying lighting conditions, backgrounds, and out-\nof-distribution shapes. By design, our model is uncertainty-aware and\nuses our new differentiable renderer for optimizing global scene parame-\nters through gradient descent, sampling posterior distributions over ob-\nject parameters with Markov Chain Monte Carlo (MCMC), and using a\nneural based likelihood function.", "sections": [{"title": "Introduction", "content": "Children have the remarkable ability to learn new concepts from only a small set\nof examples [13,54,27]. Replicating this human capacity has been a long stand-\ning challenge within the few-shot learning research community, and has been\nconsidered a milestone for building machines capable of having the same flex-\nibility and learning capacity of humans [49,30]. Current deep learning models\nhold state of the art results in many few-shot learning tasks, owning great part\nof their success to the unprecedented availability of large datasets and compu-\ntational resources [32,19]. This has resulted in large language models (LLMs)\nand vision transformers (ViT) [8] showing realistic generative capabilities [41],\nas well as zero-shot task generalization. These new deep learning paradigm con-\ntains architectures with billions of parameters, which are optimized over billions\nof data samples. For instance, a generic vision model like SAM [25] contains\nmore than half a billion parameters, which were optimized using more than one\nbillion segmentation masks.\nDespite the striking results of these models, this high sample complexity\nstill remains exceptionally large when compared to the learning ability of hu-\nmans [30]. Few-shot learning methods aim to reduce this sample and model\ncomplexity by extending learning algorithms with meta-learning, composition\nand intuitive physics [28]. However, DL models rarely use composable structures\nthat are physically consistent. Rather, they are often justified by meta design\nchoices, such as optimization ease through residual connections [16], or preven-\ntion of feature information loss through densely connected layers [20] or multiple\nfeaturemap resolutions [52,42]. Moreover, these design choices are often validated\nonly through predictive accuracy which disregards any form of uncertainty quan-\ntification, eventually leading into uncalibrated predictions [50]. Although these\nissues are discussed within some of the few-shot learning literature [28,29], few-\nshot learning datasets continue to favour large parametric models by having\nlarge training datasets; thus, defying the purpose of learning from only few data\npoints.\nIn order to address the open challenges within the current paradigm of com-\nputing large point estimates using billions of samples, we propose exploring\nan opposite question: How can we build the smallest uncertainty aware vision\nmodel that can generalize from only few training images? We approach this\nquestion using an inverse graphics framework based on probabilistic cognitive\nmodels [12,43,11,15]. Specifically, we apply the Bayesian workflow [10] to build"}, {"title": "Related Work", "content": "Few-shot learning models have been classified into metric learning, meta-learning,\nmemory-augmented networks and generative models [53]. One of the most rele-\nvant models under the metric learning classification is the prototypical network\nmodel [46]. This model is trained using meta learning episodes that build new\nclassification problems at every optimization step. The model learns to embed\nimages into a latent vector space that is reused to classify new samples based on\ntheir distance to the projected mean of the support classes. Other meta-learning\nneural algorithms perform a double optimization loop that updates the model\nweights within episodes and training samples [9,35]. The method most similar"}, {"title": "Minimal-data Benchmarks", "content": "As indicated in Table 1, current few-shot datasets contain a large amount\nof training samples; thus, undermining the ability of current few-shot models\nto learn from only few data points. Under this consideration, we adapted the\nubiquitously employed CLEVR dataset [23] for few-shot learning and few-shot\npose estimation. Specifically, we present the following 4 benchmarks FS-CLVR,\nFS-CLVR-room, FS-CLVR-dark and YCB-OOD. These benchmarks assess in a con-\ntrollable environment the generalization of few-shot learning models when using\nonly minimal data. All samples include their respective 6D poses making it a\nsuitable benchmark for few-shot pose estimation models. The FS-CLVR-room,\nFS-CLVR-dark and YCB-00D validate respectively the model generalization to\nnew backgrounds, darker lighting conditions, and to out-of-distribution (OOD)\ncomplex shapes. These FS-CLVR datasets were rendered using the same shape,\nmaterials and colors employed in CLEVR, as well as a vertical field of view\n(VFOV) of 42.5\u00b0 present in most commercial depth cameras. Each of these\nbenchmarks contain 300 images separated into 30 training classes and 20 test\nclasses, each class having 6 shot images. The YCB-00D has no training samples\nand it contains 10 test classes with 6 shots each. This dataset is meant to vali-\ndate the OOD generalization of few-shot models to unseen classes, and consists\nof the following 10 classes from the YCB dataset [6]: power-drill, tomato-soup,\nairplane-A, foam brick, softball, apple, cracker box, mustard bottle, tuna fish can\nand mug. Furthermore, while these datasets mostly consist of primitive shapes,\nthey still remain challenging to generic perception algorithms and deep learning\nmodels. Specifically, they pose the following open problems: finite and infinite\nsymmetries, textureless objects, reflective materials, changing lighting conditions\nand ultimately few number of training samples. Finally, as shown in Figure 2a\nthese few shot tasks can remain challenging for humans."}, {"title": "Merging Bayesian Inference, Graphics and DL", "content": "Our inverse graphics model uses elements from three different fields: Bayesian\ninference, computer graphics, and deep learning. This allows us to reduce the\nweaknesses of each model by using their complementary strengths. For exam-\nple, most DL architectures ignore the best known physical descriptions of the\nproblems they aim to solve [30]. Specifically, most computer vision tasks take\nas input a set of color images from different viewpoints and lighting conditions,\nand aim to extract information about the physical world. However, most DL\nvision models don't use any physical simulation between light and matter [47].\nIn contrast to DL, our model does incorporate this explicit physical knowledge\nby using a computer graphics pipeline that also encodes known physical limits\nas prior distributions. Furthermore, many physical phenomena can be described\nusing different models that compromise between accurate predictions and com-\nputational complexity. Fortunately, the computer graphics community has been\ndeveloping the right abstractions, algorithms, and hardware, to efficiently sim-\nulate and optimize these models in a physically consistent manner [39,56,21].\nOur model uses a common physical approximation known as ray-tracing [38,5],\nin which an image is rendered by simulating the intersection of light rays with\nthe properties of a given scene, such as the geometry of the objects, or the\nlocation of different light sources. Finally, our renderer does not consider all nu-\nances that real images could have. Those can include complex material models\n(metallic-roughness), soft shadows, and smooth cornered shapes. To address this\ngap between simulation and reality we propose a neural likelihood function that\nmeasures image similarity in features space."}, {"title": "Scene Optimization", "content": "To maximize realism between our generative model and the training dataset,\nwe optimize the scene parametrization \u03c8 = {\u03c8\u03c2, \u03c80}, including global \u03c8g and\nobject parameters 40. The global parameters \u03c8g = {xk,lk,CF, PF, kf, k+, }\ninclude the kth light position, light intensities, floor color, floor pattern, floor\nambient, floor diffuse variable. The object parameters 4o = {cj,k;,k,k;, aj}\ninclude the jth training objects' color, ambient, diffuse, specular, and shininess\nproperty. Specifically $x_k,l_k,C_F,C_F,c_j \\in R^3, P_f \\in R^{H\\times W\\times3}$ and all remaining\nvariables are scalars. To match real and rendered images, for all Ne channels\nand all Np pixels we minimize the L2 loss over \u03c8 using gradient descent through\nthe differentiable renderer R(\u03c8):\n\u03c8* = argmin L2(ID, R(\u03c8)).\nMoreover, the reflected colors from a realistic material depend on the lighting\nconditions. This implies that color reflections contain information about light\nlocation and intensity. Thus, we optimized the scene parameters by alternating\ngradient updates between \u03c8g and \u03c8o with N = 7 epochs, k = 5, and a pattern\nimage of shape 200 \u00d7 200 \u00d7 3. We used ADAM with a learning rate of 0.01.\nFigure 4 shows some results of our optimization problem at different epochs. In\ntotal we optimized 120K pysically interpretable parameters. All material point\nestimates optimized on the FS-CLVR training set are displayed in Figure 4b."}, {"title": "Neural Likelihood", "content": "While maximizing realism does close the simulation-reality gap, there are still\nelements for which our differentiable renderer is unable to match perfectly with\nthe true images. These elements are related to soft shadows, material reflections\nand different shape topologies. Thus, we propose a neural metric that measures\nimage discrepancy in feature space. We apply this neural metric as a likelihood\nfunction when fitting a probabilistic graphical model using MCMC. Using a neu-\nral network within MCMC implies performing a forward pass for every sample;\nmoreover, not all featuremaps are relevant for all tasks. Thus, we opted to use\nonly those feature maps Fm, where each channel m remains invariant between\nour rendered images Ir and the true images ID\nFm(IR) \u2248 Fm (ID).\nThis implies that we use only those neural features that map similar images\nto similar feature values. To construct this invariant transformation we compute\nthe mean square error (MSE) across all feature maps channels M of VGG16 [45].\nWe choose this network due to both its fast inference time, and its widespread\napplication as a feature extraction model [55]. Moreover, our methodology is\ngeneric and can be applied to any deep learning model. Specifically, we used\nthe training pairs of true images and the previously optimized image scenes"}, {"title": "Probabilistic Inverse Graphics Models", "content": "We provide our inverse graphical model with physically consistent priors for \u03a9\nby considering the following elements:\n\u2022 The object translation prior p(x) is a truncated Gaussian distribution (TrN)\nwith its limits determined such that the translation of an object has the\nhighest probability mass in the middle of the image plane and 0 mass outside.\n\u2022 The angle prior p(\u03b8) across the object's the z-axis was selected using a von\nMises distribution which assigns equal probability to all angles.\n\u2022 The object scales' prior p(s) are modelled as a Log-normal (Log) distribu-\ntion excluding negative scales, or extremely small or large objects.\n\u2022 The prior distribution p(\u03ba) for the shape classes {sphere, cube, cylinder} is a\nGumbel-Softmax distribution (Gmb) [22], which locates most of the proba-\nbility mass as one-hot vectors instead of uniformly distributed classes, mod-\nelling that objects should not be simultaneously multiple classes at once.\n\u2022 We used the optimized parameters from our scene optimization pipeline to\nbuild priors for our material variables VM. Specifically, we fitted a Gaussian\nMixture Model (GMM) to each variable using expectation maximization\n(EM). We used 2 mixture components for each variable, and a diagonal\ncovariance matrix.\nThe parameters and distributions of all our priors are shown in the supple-\nmentary Section 11. All GMM prior models are shown in the supplementary\nSection 12."}, {"title": "Building Priors", "content": "7.2 Probabilistic Generative Model\nHaving defined our prior distributions, we proceed to build a probabilistic gener-\native model of images. Figure 5a shows our model in plate notation and Figure 5b\nshows its prior predictive samples. These prior predictive samples reflect what\nthe model expects to see before making any observation. Moreover, our model\noutputs a probability for each sample. This all being referred as the prior pre-\ndictive distribution (PPD). We now proceed to explain in detail the probability\ndensity functions (PDFs) and the deterministic functions applied to our model.\nWe define the (PDF) over possible materials M by considering each material\nproperty in o as independent:\np(\u039c\u03c8M) = \\prod_{\u0442\u0435\u043c} \\sum_{Lk=0}^{K} \u03c0_k \\mathcal{N}(\u03c8_\u039c|\u03bc_k, \u03a3_k).\nAs shown in Figure 5a the PDF over possible affine transformations A includes\na TrN, von Mises, and a Log distribution, and considers the translation, angle\nand scale as independent events:\np(Ax, 0, s) = \\prod_{teT} [TrN(x|\u03bc\u03c4, \u03c3\u03c4)]\\prod_{ses} \\frac{exp(-\\frac{(ln(s) - \u03bc_s)^2}{2\u03c3_s^2})}{\\sqrt{\u03c3_s\\sqrt{2\u03c0}}}"}, {"title": "Posterior Sampling", "content": "where T = {x,y}, S = {$x,Sy, Sz} and Io is the Bessel function of order zero.\nFinally, we compute the probability density function over objects p(O|M, S, A)\nusing the previously defined PDFs for materials 5 and transforms 6, as well as\np(\u03ba\u03b9\u03c3, t) corresponding to the Gumbel-SoftMax probability with a class proba-\nbility and a temperature t\np(\u03ba\u03b9\u03c3, t)p(\u039c\u03c8\u043c)p(A|x, 0, s).\nFurthermore, the object variable O is passed through our deterministic differ-\nentiable rendering function R outputting image samples, and similarly to [14]\nwe build an affine transform that enforces objects to not collide with the floor.\nHaving defined a forward generative model of images, we now perform Bayesian\ninference in order to obtain the posterior distributions over the variables \u03a9 given\nan observation I, here denoting a test image.\np(\u03a9|\u0399) ~ p(\u0399\u03a9)p(\u03a9).\nWe define a color likelihood function Le using a truncated normal distribution\nover each pixel argument (u, v, c) of an image I with shape [H, W, 3]:\n\u03be(u, v, c) = Ir (u, v, c|N) \u2013 Ip(u, v, c)\nL(u, v|\u03a9)c = \\prod_{u=0}^{W} \\prod_{v=0}^{H} \\prod_{c=0}^{3} TrN (\u03be(u, v, c), \u03c3\u03c4).\nAll parameters of our PGM are displayed in the supplementary Section 11. Given\nour target probability function, and an observation I, we can now sample from\na conditioned posterior distribution using MCMC."}, {"title": "Posterior Sampling", "content": "For each of the test images we sampled from the target distribution using the\nRosenbluth-Metropolis Hastings (RMH) MCMC method. We used 20 chains"}, {"title": "Probabilistic Generative Model", "content": "7.3 Posterior Sampling"}, {"content": "In this section we show the data driven priors fitted for the FS-CLVR dataset for\nthe variables color c, ambient ka, diffuse kd, specular ks, and shininess a."}, {"title": "Bijection results", "content": "Reparametrizing probability distributions can increase the number of effective\nsamples without increasing computional costs. We transformed the prior prob-\nability distributions to fit Normal distributions using the appropiate bijection\nfunctions. Specifically for x, 0, s, k we apply the following bijections parametrized\nby w and \u03c6:\nx = wxx + x\n\u03b8 = sigmoid(\u03b8)\ns = sigmoid(wss + \u03c6, -\u03c0, \u03c0)\n\u03ba = softmax(\u03c9\u03ba\u03ba)\nMoreover, the GMM priors were reparametrized by minimizing the negative\nlog-likelihood with respect to an affine bijector and a Normal distribution:\nm = \u03c9km + \u03c6k\nVisualizations of reparametrizations for all GMM, as well as translation, theta\nand scale distributions are shown below:"}, {"title": "Posterior results", "content": "This section contains more posterior results. Specifically, we show the median\npoint estimates of all our predictions in Figure 12 for the FS-CLVR dataset, in\nFigure 13 for FS-CLVR-room, Figure 14 for FS-CLVR-dark and Figure 15 for the\nYCB-00D dataset."}, {"title": "Prototypical program results", "content": "In the image below we show our probabilistic prototypical program (P3) in\ngraphical form. This model is derived from our probabilistic generative model 5a\nby removing the variables translation x and rotation @ of our inverse graphics\nmodel."}, {"title": "MAML and ProtoNets results", "content": "We now present the additional experimental results of the CNN models used\nin MAML and ProtoNets. This CNN model corresponds to the default archi-\ntecture proposed in both papers. Specifically it consists of 4 blocks, each having\nin sequence the following layers: Conv2D with 64 3 \u00d7 3 filters, BatchNorm, ReLU\nand MaxPool2D. Initially we did some preliminary experiments for determining\nthe image resolutions. We set the training iteration steps to 20K and the image\nsizes of the FS-CLVR dataset were downsampled to 28 \u00d7 28, 56 \u00d7 56, 84 \u00d7 84,\n112 \u00d7 112, 168 \u00d7 168. The corresponding test accuracies for these resolutions\nwere respectively 67.9%, 80.5%, 71%, 30.4%, 22.4% under the MAML training\nframework. From here it could be seen that the lower resolutions could achieve\nbetter performance. In all our experiments we trained the models for 60K iter-\nations using the optimizer (ADAM) and learning rates (1e-3) suggested by the\noriginal authors."}, {"title": "Evaluation of pose estimation", "content": "In this section, we present our evaluation results of pose estimation with respect\nto the ADI metric (Average Distance of Indistinguishable Model Point) [18] for\nour different version of models on our different test datasets in Table 4. This\nADI metric is commonly used for symmetric objects and measures the average\ndeviation of the transformed model points to the closest model point. The values\nin the Table 4a and 4b represents the mean value of the ADI error for our total\n6 shots of each test class in our FS-CLVR, FS-CLVR-room, FS-CLVR-dark, and\nYCB-00D test dataset."}, {"title": "Conclusion and Future Work", "content": "Our Bayesian Workflow"}, {"content": "This paper presents a novel Bayesian inverse graphics framework that encodes\nimages as prototypical probabilistic programs. This approach addresses some of\nthe limitations of existing neural algorithms such as high sample and model com-\nplexity, and lack of uncertainty quantification. Moreover, we use these probabilis-\ntic programs to build a few-shot learning classification algorithm. This algorithm\nintegrates our newly introduced differentiable renderer with probabilistic pro-\ngramming languages, gradient descent optimization libraries, and deep learning\nframeworks. Our method achieves higher classification accuracy than standard\nfew-shot neural methods, while using considerably less parameters. Furthermore,\nwe demonstrate generalization to different lighting conditions, backgrounds, and\nunseen complex objects. Additionally, we proposed a neural likelihood function\nwhich combines deep learning models with Bayesian inference. In future work\nwe plan to extend our generative models to merge primitive shapes to fit more\ncomplex objects."}, {"title": "Physics based priors and likelihood parameters", "content": "The parameters and visualizations of our prior distributions and likelihood\nfunctions are displayed in Figure 9 and Table 3 shows all our prior and likelihood\nparameters."}]}