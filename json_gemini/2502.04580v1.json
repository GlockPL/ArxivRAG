{"title": "Technical Debt in In-Context Learning: Diminishing Efficiency in Long Context", "authors": ["Taejong Joo", "Diego Klabjan"], "abstract": "Transformers have demonstrated remarkable in-context learning (ICL) capabilities, adapting to new tasks by simply conditioning on demonstrations without parameter updates. Compelling empirical and theoretical evidence suggests that ICL, as a general-purpose learner, could outperform task-specific models. However, it remains unclear to what extent the transformers optimally learn in-context compared to principled learning algorithms. To bridge this gap, we introduce a new framework for quantifying optimality of ICL as a learning algorithm in stylized settings. Our findings reveal a striking dichotomy: while ICL initially matches the efficiency of a Bayes optimal estimator, its efficiency significantly deteriorates in long context. Through an information-theoretic analysis, we show that the diminishing efficiency is inherent to ICL. These results clarify the trade-offs in adopting ICL as a universal problem solver, motivating a new generation of on-the-fly adaptive methods without the diminishing efficiency.", "sections": [{"title": "1. Introduction", "content": "Transformers, particularly large language models (LLMs), are able to perform in-context learning (ICL) (Brown et al., 2020); they can adapt to new tasks simply by conditioning on demonstrations in their input prompt (Xie et al., 2022). Not only conveniently operated without any explicit parameter updates, but ICL even with just a few demonstrations (a.k.a. few-shot ICL) surprisingly outperforms task-specific state-of-the-art models in diverse tasks, from question answering to common sense reasoning (Chowdhery et al., 2023; Touvron et al., 2023; Brown et al., 2020).\n\nThis raises a fundamental question about how we shape artificial intelligence systems: Could ICL serve as a universal learner, obviating the need for task-specific models? To answer this, we must first address a more precise question:\n\nHow optimal is ICL as a learning algorithm, compared to principled learning algorithms?\n\nAt first glance, the impressive performance of few-shot ICL and more recently many-shot ICL (Bertsch et al., 2024; Agarwal et al., 2024) might seem to be an affirmative answer. However, this conclusion would be premature. Even when ICL outperforms state-of-the-art task-specific models or matches (super) human-level performances, it may still not be an optimal learning algorithm. This is evidenced by the results that carefully fine-tuned LLMs often outperform ICL when provided with the same amount of demonstrations (Min et al., 2021; Zhao et al., 2024).\n\nThe question in principle can be accurately answered by comparing ICL with principled learning algorithms across LLMs with different data and model scales (Wei et al., 2023; Ravent\u00f3s et al., 2023) on diverse types of tasks (Srivastava et al., 2022; Wei et al., 2022). However, the computational demands for training modern LLMs pose significant challenges for evaluating optimality of ICL as a learning algorithm. The goal of this work is to answer the question without such prohibitive computational demands.\n\nPrevious attempts. To answer the question, theoretical studies have analyzed asymptotic behavior of ICL using rich tools from statistics and learning theory, such as regret and generalization bounds (Jeon et al., 2024; Zhang et al., 2023; Bai et al., 2023; Li et al., 2023b). However, these asymptotic results fall short of fully characterizing real-world LLM behavior. For instance, the regret upper bound for LLMs become nearly vacuous in few-shot regimes (Langford & Caruana, 2001; Dziugaite & Roy, 2017), which cannot explain the striking few-show ICL performances. Moreover, because other principled learning algorithms have the similar asymptotic behavior, it remains unclear whether ICL is a better learning algorithm than such learning algorithms.\n\nPhysics-style or synthetic benchmarking approaches have provided valuable insights that transformers might optimally learn in-context, isolating core aspects of LLM training in controlled environments (Allen-Zhu & Li, 2023; Garg et al., 2022; Ahn et al., 2023). These approaches by nature can enable an efficient, comprehensive comparison between ICL and principled learning algorithms with arbitrarily high levels of statistical significances, while providing insights that often generalize to real-world LLMs despite inherent simplifications (cf. \u00a7A.1). Concretely, by examining ICL performances across different demonstration sizes in a stylized benchmark, Garg et al. (2022) and follow-up works (Aky\u00fcrek et al., 2022; Von Oswald et al., 2023) show that ICL seemingly learns new tasks with an efficiency comparable to provably optimal algorithms. However, these works have not yet provided an explicit relationship between relevant quantities (e.g., sample complexity and the optimality gap). Thus, the question of to what extent transformers can learn optimally in-context remains unanswered.\n\nNew benchmarking framework. We revisit the performance profiles (Dolan & Mor\u00e9, 2002)-classic benchmarking framework for optimization software\u2014for benchmarking optimality of ICL as a learning algorithm in the stylized ICL setting (Garg et al., 2022). Our framework can quantify how many more demonstrations are required for ICL to achieve a certain performance compared to principled learning algorithms. Thus, our analysis can accurately describe optimality of ICL with a more intuitive measure, making fundamental progress in physics-style approaches for ICL.\n\nUnveiling diminishing efficiency in long context. As a result, we uncover a new insight on optimality of ICL in \u00a73:\n\nWhile ICL initially matches the efficiency of the Bayes optimal estimator, its efficiency significantly deteriorates in long context.\n\nMore precisely, for low performance requirements, ICL achieves near optimal sample complexity comparable to the Bayes optimal estimator, aligning with its strong few-shot performance observed in practice. However, ICL's sample complexity sharply deteriorates beyond a certain threshold, often requiring 1.5 times more demonstrations to achieve high performance requirements than the Bayes optimal estimator. Further, we provide evidence that ICL may lack fundamental statistical properties (e.g., consistency and asymptotic efficiency) unlike the principled learning algorithms, which allow learning algorithms to benefit from large sample sizes. Crucially, this novel insight would be difficult to uncover through many-shot ICL experiments on real-world LLMs due to intractability (Agarwal et al., 2024) or analysis tools in the stylized setting (Garg et al., 2022), as ICL errors generally decrease with more demonstrations.\n\nIntrinsic suboptimality of ICL. We prove that ICL without diminishing efficiency has stringent necessary conditions (e.g., negligible excess risk) using information-theoretic tools. Crucially, the result is independent to particular instantiation of models and environments, suggesting the diminishing efficiency is intrinsic to the ICL mechanism itself.\n\nThis discovery unveils a hidden technical debt in the ICL mechanism: the price we pay for its training-free adaptability is a fundamental inefficiency in sample complexity that compounds as we push toward higher performance targets with the current ICL mechanism as is.\n\nImpact and outlook. Taken together, our findings suggest a more nuanced view of ICL than the prevailing excitement for replacing task-specific fine-tuned models with ICL as a universal problem solver. While ICL's ability to adapt without training remains attractive, our work reveals the fundamental technical debt that must be considered in AI system designs. Crucially, this debt appears intrinsic to the ICL mechanism and thus unlikely to be serviced by simply scaling data and model sizes. We hope these insights clarify the trade-offs in adopting ICL as a universal problem solver and motivate a new generation of \u201con-the-fly\" adaptive methods without the diminishing efficiency."}, {"title": "2. Setup", "content": "In \u00a72.1, we describe the meta ICL environment for evaluating ICL as a learning algorithm, followed by designs of a transformer for solving the meta ICL task (\u00a72.2). We then devise principled predictors (\u00a72.3) and compare them with transformers using performance measures defined in \u00a72.4."}, {"title": "2.1. Meta ICL Environment", "content": "In the meta ICL (Garg et al., 2022), each prompt characterizes an instance of a learning problem. Specifically, a prompt $H_T$ consists of demonstrations with a test input, i.e., $H_T \\equiv (X_1, Y_1, \\cdots, X_T, Y_T, X_{T+1})$, and each output is generated by some function $f^*$, i.e., $Y_t = f^*(X_t)$ for $t \\in [T+1] \\triangleq \\{1, 2, \\cdots, T+1\\}$. Here, the goal of a transformer is formalized as accurately predicting $Y_{T+1}$ with $H_T$, which requires to (implicitly) infer the underlying function $f^*$ from the demonstrations. We denote the set of demonstrations as $D_T \\triangleq (X_1, Y_1, \\cdots, X_T, Y_T)$.\n\nFor the data generating distribution of a prompt $H_T$, we follow the approach of sampling target functions $f^*$ from a hierarchical distribution (Panwar et al., 2024) to capture a more interesting aspect of a learning algorithm-model selection. Under the hierarchical $f^*$, the prompt $H_T$ is realized by the following sampling process, which is denoted as $H_T \\sim P(\\cdot; \\mathcal{E})$ with parameters $\\mathcal{E} \\triangleq ([M], \\sigma_w, \\sigma^2)$.\n\n1) Sample the implicit dimension $m \\sim \\mathcal{U}([M])$ from a uniform distribution over set $[M]$ and construct the (unobservable) feature space $\\Phi_m(X)$:\n\n$\\Phi_m(x) = [1, cos(\\frac{1 \\pi x}{T}), sin(\\frac{1 \\pi x}{T}), \\cdots, cos(\\frac{m \\pi x}{T}), sin(\\frac{m \\pi x}{T})]$ where $T > 0$ controls the frequency of the trigonometric functions.\n\n2) Sample weight $W_m \\sim \\mathcal{N}(0, I_{2m+1})$, where $I_{2m+1}$ is the identity matrix with rank $2m + 1$. The weight $W_m$ defines the target function $f^*$:\n\n$f^*(x) = \\frac{w_m \\phi_m(x)}{\\sqrt{2m + 1}}$,"}, {"title": "2.2. Transformers", "content": "For a transformer TF, we adopt the setup from Garg et al. (2022) and follow-up works (Panwar et al., 2024; Von Oswald et al., 2023; Aky\u00fcrek et al., 2022; Ravent\u00f3s et al., 2023) that use the GPT-2 (Radford et al., 2019) architecture (cf. details in Appendix A.2). For optimizing $\\theta$ in the pretraining stage, we use the following minimization objective\n\n$\\mathcal{L}(\\theta) \\triangleq \\mathbb{E}_{H_{Train}} \\sum_{t=0}^{T_{train}-1} [l(TF_{\\theta}(H_t), Y_{t+1})]$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (1)\n\nwhere $H_{Train}$ is generated by the prompt distribution described in \u00a72.1. We use the squared loss function for $l$, following previous works in the regression setting. Also, we set $T_{train} = 50$ for all scenarios, which is roughly $2.(2M+1)$ as in the previous works (Garg et al., 2022; Panwar et al., 2024). We train $TF_{\\theta}$ separately for each scenario."}, {"title": "2.3. Principled Baselines", "content": "To benchmark ICL, we derive principled baselines that learn from demonstrations $D_t$ and produce a prediction function $f_b(\\cdot; D_t)$, where $b$ is the identifier of a particular baseline. We denote $f_b(x) \\triangleq f_b(x; D_t)$ and $f_{ICL}(X_{t+1}) \\triangleq TF_{\\theta}(H_t)$ when ever there is no ambiguity.\n\nThe optimal baseline is Bayesian model averaging (BMA), which makes prediction by aggregating models from different hypothesis classes:\n\n$f_{BMA}(x) = \\sum_{m \\in [M]} P(F_m | D_t) w_m(D_t) \\phi_m(x)$,                (2)\n\nwhere $P(F_m | D_t)$ is the posterior probability of model class $F_m$ and $w_m$ is the ridge regression estimator for $F_m$, defined as $w_m(D_t) = (\\Phi_{m,t}^T \\Phi_{m,t} + \\sigma^2 I_{2m+1})^{-1} \\Phi_{m,t}^T Y_t$ with $\\Phi_{m,t} \\in [\\mathbb{R}]^{t \\times (2m+1)}$ whose $k$-th row is $\\Phi_m(X_k)$ and $Y_t = (Y_1, \\cdots, Y_t) \\in [\\mathbb{R}]^t$. It is a standard result that\n\n$f_{BMA} \\in arg\\underset{f \\in \\mathcal{F}}{min} \\mathbb{E}_{Y_{t+1}} [l(f(X_{t+1}; D_t), Y_{t+1}) | H_t]$   (3)\n\nholds almost everywhere for all $t \\in \\mathbb{N}$, where $\\mathcal{F}$ is the set of all functions from $H_t$ to $\\mathbb{R}$ (Ahuja & Lopez-Paz, 2023; Bishop, 2007).\n\nIn addition, we consider a family of principled baselines that embodies different model selection strategies while having the same model fitting capacity as the optimal predictor. Such baselines make predictions by\n\n$f(x; D_t) = w_{m_t}^T(D_t) \\phi_{m_t}(x)$,                                                                           (4)\n\nwhere $m_t = arg\\underset{m \\in [M]}{max} \\{Scores(m)\\}$ with $Scores(\\cdot)$ being some model selection criterion of $b$."}, {"title": "2.4. Measures for Benchmarking Optimality of ICL", "content": "Inspired by seminal work (Dolan & Mor\u00e9, 2002) that benchmarks (deterministic) optimization software, we first define the base metric measuring the optimality of a learning algorithm in $s \\in \\mathcal{S}$. Then, we present the performance measures summarizing the base metric across $\\mathcal{S}$. In the following, we let $\\mathcal{B}$ contain all baseline learning algorithms and ICL. We set the test prompt length as $T = 2 T_{train} = 100$, which is within the length generalization regime (Zhou et al., 2024).\n\nBase metric. Our base metric is the performance ratio, which normalizes the sample complexity of a learning algorithm by that of the best algorithm among all baselines.\n\nDefinition 2.1. For $b \\in \\mathcal{B}$ at $s \\in \\mathcal{S}$, the performance ratio of a requirement $r$ against $\\mathcal{B} \\subset \\mathcal{B}$ is defined as $R_b(r; \\mathcal{B}) \\triangleq \\frac{N_b(r)}{min_{b' \\in \\mathcal{B}} \\{N_{b'}(r)\\}}$, where $N_b(r) \\triangleq min \\{t | \\mathbb{E}[l(f_t(X_{t+1}), Y_{t+1})] \\leq r\\}$ is the sample complexity of achieving the performance $r$.\n\nThe performance ratio quantifies the relative efficiency of a learning algorithm, addressing data-dependent nature of the sample complexity. Specifically, $R_b(r; \\mathcal{B})$ indicates that the learning algorithm $b$ requires $R_b(r; \\mathcal{B})$ times more demonstrations to achieve performance $r$ at scenario $s$ compared to the best learner among $\\mathcal{B}$. When $BMA \\in \\mathcal{B}$, algorithms with $R_b(r; \\mathcal{B}) = 1$ have optimal efficiency at $s$ due to (3).\n\nPerformance measures. Based on the performance ratio across different scenarios, our goal is to report a \"single\" score that summarizes how optimal ICL is across $\\mathcal{S}$. However, naively summarizing the performance ratio for a requirement $r$ is inappropriate because the difficulty of achieving $r$ varies across learning problems, making comparisons inconsistent. Therefore, we define the reference performance quantile $\\mathcal{B}_{ref}(s)$ as the Q-th quantile of reference performances at $s$ for $Q \\in (0, 1)$. Here, we measure the performance quantile in a reverse order, for making higher performance quantile analogous to higher performance. The reference performances at $s$ is defined as a set of performances achieved by reference models $\\mathcal{B}_{ref} \\subseteq \\mathcal{B}$; that is, $\\{\\mathbb{E}[l(f(X_{t+1}), Y_{t+1})]| b \\in \\mathcal{B}_{ref}, t \\in [T]\\}$.\n\nWith this idea, the performance ratios across $\\mathcal{S}$ is summarized by the mean performance ratio and the performance profile, which are defined as follows.\n\nDefinition 2.2. For the performance quantile $\\mathcal{B}_{ref}$, the mean performance ratio of $b \\in \\mathcal{B}$ against $\\mathcal{B} \\subset \\mathcal{B}$ is defined as $\\mathcal{MPR}(b; \\mathcal{B}_{ref}, \\mathcal{B}) \\triangleq \\mathbb{E}_{s \\in \\mathcal{S}} R_b(\\mathcal{B}_{ref}(s); \\mathcal{B})$.\n\nDefinition 2.3. For the performance quantile $\\mathcal{B}_{ref}$, the performance profile of $b \\in \\mathcal{B}$ against $\\mathcal{B} \\subset \\mathcal{B}$ at a ratio $\\tau > 1$ is defined as $P_b(\\tau; \\mathcal{B}_{ref}, \\mathcal{B}) = \\frac{\\{s \\in \\mathcal{S} : R_b(\\mathcal{B}_{ref} (s); \\mathcal{B}) \\leq \\tau\\}}{\\|S\\|}$.\n\nThe two measures capture complementary aspects of optimality of ICL. Specifically, the mean performance quantile quantifies the average inefficiency of a model $b$ in attaining a certain performance, which is assumed to be achievable by $b$. In contrast, the performance profile measures the frequency with which a model $b$ can achieve the performance quantile given a tolerance for inefficiency. These intuitive measures provide novel insights into optimality of ICL that are not apparent in previous error rates-based comparisons and asymptotic analyses."}, {"title": "2.5. On Usage of Stylized Setting", "content": "The stylized setting discussed in this section provides a rigorous benchmark for studying optimality of ICL with, in theory, arbitrarily high levels of statistical significance, including performance comparisons against BMA as a fundamental limit. While it simplifies certain real-world elements (e.g., autoregressive loss), empirical evidence suggests that insights from this setting generalize remarkably well to real-world tasks (Ahn et al., 2024; Li et al., 2023c). Though the setting necessarily simplifies real-world LLMs, its ability to deepen our understanding of ICL demonstrates its value. Refer to Appendix A.1 for more details."}, {"title": "3. Benchmarking ICL Efficiency", "content": "We measure to what extents transformers efficiently learn a new task through ICL compared to the optimal learning algorithm (\u00a73.1) and principled baselines (\u00a73.2)."}, {"title": "3.1. Can Transformer Optimally Learn In Context?", "content": "We first examine the efficiency of ICL compared to the Bayes optimal predictor, which learns new concepts with optimal efficiency (cf. (3)). For comprehensive evaluation, we design the test scenarios with various levels of SNRs: $\\mathcal{S} = \\{([M], \\sigma_w^2, \\sigma^2) | M = 10, \\sigma_w \\in \\{0.003, 0.03, 0.3\\}, \\sigma^2 \\in \\{0.1, 1, 10\\}\\}$ (cf. \u00a72.1). Also, to minimize the impacts of stochasticity of the sampling process of $H_t$, we evaluate performances for each scenario 512 times. Then, we analyze the mean performance ratio of ICL against BMA for all quantiles of performances achieved by ICL; that is, we measure $\\mathcal{MPR}(ICL; \\mathcal{B}_{ref}, \\mathcal{B}_1)$ with $\\mathcal{B}_{ref} \\triangleq \\{ICL\\}$, $Q \\in \\{0.01, 0.1, \\cdots, 0.9, 0.99\\}$, and $\\mathcal{B}_1 \\triangleq \\{ICL, BMA\\}$. In this way, we measure the efficiency of ICL in achieving each performance level under various difficulties in extracting information from prompts. In the following, we regard prompts with more than 40 demonstrations as the many-shot regime where the average performance quantile is approximately 0.5 (cf. Figure A3 in Appendix).\n\nFigure 1 reveals a striking dichotomy in optimality of ICL.\n\nNear optimal few-show efficiency. For low performance quantiles ($Q \\leq 0.3$), ICL demonstrates its remarkable near optimal efficiency. Specifically, the mean performance ratio is at most 1.1, which means that it requires only 10% more demonstrations on average than the optimal learning algorithm to achieve the performance lower than $\\mathcal{B}^{0.3}_{ref}(s)$ for $s \\in \\mathcal{S}$. Considering the average sample complexity for the performance quantile of 0.3 is 19, this explains ICL's impressive few-shot performance observed in practice (e.g., demonstration sizes of 5 and 15 in Brown et al. (2020)).\n\nSuboptimal many-shot efficiency. Starting from $Q = 0.3$ or more apparently from $Q = 0.7$ onward, the performance ratio grows almost monotonically with $Q$, increasing from around 1.1 at $Q = 0.3$ to around 1.2 at $Q = 0.7$ and to around 1.45 at $Q = 0.99$. That is, ICL becomes increasingly suboptimal compared to the optimal learning algorithm when pursuing high performance requirements. Considering higher performance quantiles requires larger demonstration sizes, this indicates that efficiency of ICL decreases with the demonstration size.\n\nImportantly, these findings do not contradict established benefits of many-shot ICL (Bertsch et al., 2024; Agarwal et al., 2024); as we analyze later in Figure 3, ICL still achieves monotonic improvement in MSE with more demonstrations. Rather, our novel evaluation framework reveals that this improvement comes at an increasingly inefficient sample complexity, indicating significant diminishing returns in extracting information from demonstrations."}, {"title": "3.2. Benchmarking ICL Against Principled Baselines", "content": "We have shown that ICL is significantly inefficient compared to BMA in high performance regimes. While BMA is learnable by minimizing (1), it might seem unrealistic for ICL to compete with BMA that performs the expensive model averaging operation. Thus, we compare ICL with more practical baselines with a computational constraint that select a single model using principled criteria (cf. (4)): Akaike Information Criterion (AIC) (Akaike, 1974) as a minimax-rate optimal model selection mechanism, Bayesian Information Criterion (BIC) (Schwarz, 1978) as a consistent model selection mechanism, and Bayesian Model Comparison (BMC) as an efficient BMA alternative selecting maximum a posteriori model class. These baselines represent the spectrum of principled model selection methods, which often asymptotically converge to either AIC or BIC (Ding et al., 2018).\n\nTo quantitatively assess relative efficiency, we measure performance profiles $p_b(\\tau; \\mathcal{B}_2rel \\mathcal{B}_{ref}, \\mathcal{B}_2)$ with $\\mathcal{B}_{ref} = \\{ICL, AIC\\}$ and $\\mathcal{B}_2 = \\{ICL, AIC, BIC, BMC\\}$. This allows us to measure the probability that each method achieves a reference performance level within given sample complexity budgets, which evaluates both efficiency and effectiveness (i.e., maximum achievable performances) of learning algorithms.\n\nSuperiority of ICL in few-shot regimes. Perhaps not surprisingly (given the results from comparison with BMA), ICL dominates the baselines with restricted capacity under low performance requirements. Specifically, it achieves the perfect performance profile at $\\tau = 1$ for $Q \\leq 0.3$. This means that it optimally attains the performance requirement in all scenarios when $Q < 0.3$. Given that each baseline has its own strength in certain scenarios, this guarantee is quite strong and not observed in other baselines. Further, for $Q = 0.4$, ICL reaches a perfect performance profile within $\\tau < 1.2$. This means that ICL attains the required performance of $Q = 0.4$ in all scenarios by using at most 20% more demonstrations on average compared to the best method in each scenario. Conversely, all baselines selecting a single model struggle in the low-performance regime due to high uncertainty under a small number of demonstrations preventing them from selecting the proper model class (Hoeting et al., 1999; Wasserman, 2000).\n\nInferiority of ICL in many-shot regimes. Figure 2 illustrates diminishing efficiency of ICL in long context regimes. Specifically, as the performance requirement increases, the initial performance profile at $\\tau = 1$ is reduced, indicating the reduced probability that ICL learns the most efficiently among $\\mathcal{B}_2$. Beside, the computational budget $\\tau$ required to reach perfect performance profile increases as the performance requirement increases. Eventually for $Q \\geq 0.8$, even at $\\tau = 3$, ICL achieves the performance profile around 0.8, which means that ICL cannot reach the performance requirements for 20% of cases by using even 3 times more demonstrations than other models.\n\nCrucially, this increasingly suboptimal behavior is opposite to the behaviors of principled baselines. In Figure 2, as opposed to ICL, the principled learning algorithms significantly reduce the time to reach the (near) perfect performance profiles as $Q$ increases. Eventually, despite their significant deficiencies in few-shot regimes, all such baselines become more effective (achieving higher performance profiles at $\\tau = 3$) and more efficient (sharply improving the performance profiles with respect to $\\tau$) than ICL in many-shot regimes. Therefore, some characteristics enabling learning algorithms to leverage large number of demonstrations might be missing in the ICL mechanism.\n\nTo gain further insights, we qualitatively analyze MSEs across different numbers of demonstrations for each scenario. As a trivial baseline, we also consider an ensemble that aggregates the ridge estimators $\\{w_m\\}_{m \\in [M]}$ using equal weights. Figure 3 shows that while all methods show decreasing MSEs with more demonstrations, ICL exhibits persistent discrepancies from the principled learning algorithms in many-shot regimes. Further, in Figure 4, we analyze the squared prediction difference between each model and the Bayes optimal predictor for each scenario. Critically, it reveals that while consistent estimators (BMC, BIC) seem to converge in $L^2$ (albeit at different rates), ICL's $L^2$ distance to $f_{BMA}$ plateaus after receiving few demonstrations. This behavior mirrors the trivial ensemble, which does not update its hypothesis about the model class with demonstrations. This suggests another fundamental limitation: ICL may lack asymptotic efficiency and consistency (cf. Ding et al. (2018) for formal definitions). These findings challenge the prevailing optimism about ICL's scalability."}, {"title": "3.3. On Sources of the Diminishing Efficiency", "content": "We observe a significant suboptimality of ICL under high performance requirements, which typically requires longer context sizes than the pretraining prompt (cf. Figure A3 in Appendix). Given universally observed deficiencies of machine learning models in the out-of-distribution regimes (Hendrycks & Dietterich, 2019; Koh et al., 2021), it is tempting to attribute the diminishing efficiency to the deficiencies in out-of-distribution regimes.\n\nWe take a closer look at this in Figure 3, which corresponds to the achievable error due to the bias-variance decomposition. Recalling that $T_{Train} = 50$ was used for pretraining, Figure 3 and Figure A6 in Appendix show no apparent differences in the achievable error between in-distribution and out-of-distribution regimes, except in low SNR scenarios $(\\sigma_w, \\sigma^2_{\\epsilon}) = (0.1, 0.03)$ and $(\\sigma_w, \\sigma^2_{\\epsilon}) = (1, 0.3)$. This finding aligns with the length generalization literature, which suggests that transformers often generalize to contexts up to 2.5 times longer than those seen during pretraining (Zhou et al., 2024). Further, given that the average performance quantile at $T_{train}$ is 0.6, Figure 1 reveals that fundamental inefficiency already emerges in the in-distribution regime.\n\nTherefore, the diminishing efficiency observed in \u00a73.1 and \u00a73.2 cannot be fully attributed to the transformers' out-of-distribution generalization capability. Rather, as we analyze next in \u00a74, it is intrinsic to the ICL mechanism itself."}, {"title": "4. Analyzing Suboptimality of ICL", "content": "Using information-theoretic tools, we explain why ICL's efficiency as a learning algorithm diminishes in long context."}, {"title": "4.1. ICL Error Decomposition", "content": "Adopting a Bayesian viewpoint (Jeon et al., 2024), we denote the oracle distribution with $e$ drawn from an environment $\\mathcal{E}$ by $P_\\theta(\\cdot) \\triangleq P(Y_{t+1} \\in \\cdot|H_t, e) = P(Y_{t+1} \\in \\cdot|X_{t+1}, e)$ (e.g., $\\mathcal{E}$ characterizes the sampling process in \u00a72.1 with $e = (m, W_m)$). Similarly, we let $TF_\\theta$ models the conditional distribution of outputs, i.e., $TF_\\theta(H_t) \\equiv P_{\\theta}(Y_{t+1} \\in \\cdot| H_t) \\triangleq P_{\\theta}(\\cdot)$. All subsequent discussions in this section assumes no distribution shift; that is, $\\mathcal{E}$ is the environment under which $TF_\\theta$ was pretrained. We assume that $Y_{t+1}$ is either discrete or continuous.\n\nWith this notation, the ICL performance with $t$ demonstrations from $\\mathcal{E}$ is defined as $\\mathbb{E} [-\\log P(Y_{t+1})] = \\mathbb{E} [-\\log P_{\\theta}(Y_{t+1})] + \\mathbb{E} [D_{KL}(P_t || P_{\\theta})]$ (Jeon et al., 2024). Here, the first term is the (irreducible) aleatoric uncertainty and constant with respect to $t$ in our setting. The second term can be further decomposed as\n\n$\\mathbb{E} [D_{KL}(P_t || P_{\\theta})] = \\mathbb{E} [\\int \\log \\frac{dP_{\\theta}(y)}{dP_t(y)} P_t(dy)] = \\mathbb{E} [\\int \\log \\frac{P(Y_{t+1})}{P(Y_{t+1})} P_t(dy)] = \\mathbb{E}_{\\mathcal{E}} [D_{KL}(P_t || P)] + \\mathbb{E} [\\log \\frac{P(\\theta(Y_{t+1}))}{P(Y_{t+1})} P_t(dy)]$           (5)\n\n$\\triangleq \\mathbb{E}_{e} [D_{KL}(P_t || P)] + \\mathbb{E}_{es} [\\log \\frac{P_{es}(Y_{t+1})}{P(Y_{t+1})} P_t(dy)]$                                                                      (5)\n\nwhere the second equality comes from the law of total expectation and $P(Y_{t+1}) = P(Y_{t+1} \\in \\cdot|H_t, \\mathcal{E})$ is the posterior over $Y_{t+1}$ given $H_t$.\n\nIn (5), the Bayes risk $\\epsilon_{Bayes}^t$ measures how well the Bayes-optimal predictor performs under uncertainty on $e$. It is nonnegative and decreases monotonically with more demonstrations; that is, $\\epsilon_{Bayes}^{t'} < \\epsilon_{Bayes}^t$ for all $t \\in \\mathbb{N}$ (Jeon et al., 2022). Demonstration size $t$ required to bring this risk below a threshold $q$ is captured by $N_{BMA}(q) \\triangleq min_{t \\in \\mathbb{N}} \\{\\epsilon_{Bayes}^t \\leq q\\}$. Here, $q$ represents the absolute value of the performance requirement (e.g., MSE), whereas $Q$ in \u00a73 denotes the performance quantile.\n\nThe excess risk $\\epsilon_{es}^t$ measures the performance of the transformer relative to the Bayes optimal predictor. Due to the non-negativity of excess risk and independence between $TF_\\theta$ and $\\epsilon_{Bayes}^t$, this term determines when ICL emerges and how well it can perform. For instance, if $TF_\\theta$ achieves an excess risk curve such that $\\epsilon_{es}^t - \\epsilon_{es}^{t'} \\leq \\epsilon_{Bayes}^{t'} - \\epsilon_{Bayes}^t$, non-trivial ICL performance emerges, improving upon the zero-shot performance with demonstrations. Further, if $\\epsilon_{es}^t \\rightarrow 0$ as $t \\rightarrow \\infty$, then ICL is Bayes-risk consistent and asymptotically matches BMA. In \u00a74.2, we dissect $\\epsilon_{es}^t$ based on our empirical results (\u00a73)."}, {"title": "4.2. On Excess Risk", "content": "Interpreting the transformer's prediction in the meta-ICL setup as the Gaussian distribution (e.g., by adding a small random Gaussian noise to the prediction), the squared prediction difference in Figure 4 is directly proportional to the excess risk, up to a constant scale and shift. The same applies to each baseline's squared prediction difference, interpreted as its own excess risks.\n\nIn this regard, Figure 4 illustrates that the transformer's excess risk remains roughly bounded within a modest interval in a certain length generalization regime (e.g., $t \\leq 2T_{train}$), suggesting that it would perform ICL non-trivially due to the monotonicity of $\\epsilon_{"}]}