{"title": "How DNNs break the Curse of Dimensionality: Compositionality and Symmetry Learning", "authors": ["Arthur Jacot", "Seok Hoan Choi", "Yuxiao Wen"], "abstract": "We show that deep neural networks (DNNs) can efficiently learn any composition\nof functions with bounded F\u2081-norm, which allows DNNs to break the curse of\ndimensionality in ways that shallow networks cannot. More specifically, we\nderive a generalization bound that combines a covering number argument for\ncompositionality, and the F\u2081-norm (or the related Barron norm) for large width\nadaptivity. We show that the global minimizer of the regularized loss of DNNs can\nfit for example the composition of two functions f* = hog from a small number\nof observations, assuming g is smooth/regular and reduces the dimensionality (e.g.\ng could be the modulo map of the symmetries of f*), so that h can be learned in\nspite of its low regularity. The measures of regularity we consider is the Sobolev\nnorm with different levels of differentiability, which is well adapted to the F\u2081 norm.\nWe compute scaling laws empirically and observe phase transitions depending on\nwhether g or h is harder to learn, as predicted by our theory.", "sections": [{"title": "1 Introduction", "content": "One of the fundamental features of DNNs is their ability to generalize even when the number of\nneurons (and of parameters) is so large that the network could fit almost any function [48]. Actually\nDNNs have been observed to generalize best when the number of neurons is infinite [8, 21, 20].\nThe now quite generally accepted explanation to this phenomenon is that DNNs have an implicit\nbias coming from the training dynamic where properties of the training algorithm lead to networks\nthat generalize well. This implicit bias is quite well understood in shallow networks [11, 37], in\nlinear networks [24, 30], or in the NTK regime [28], but it remains ill-understood in the general deep\nnonlinear case.\nIn both shallow networks and linear networks, one observes a bias towards small parameter norm\n(either implicit [12] or explicit in the presence of weight decay [44]). Thanks to tools such as the\nF\u2081-norm [5], or the related Barron norm [46], or more generally the representation cost [14], it is\npossible to describe the family of functions that can be represented by shallow networks or linear\nnetworks with a finite parameter norm. This was then leveraged to prove uniform generalization\nbounds (based on Rademacher complexity) over these sets [5], which depend only on the parameter\nnorm, but not on the number of neurons or parameters.\nSimilar bounds have been proposed for DNNs [7, 6, 40, 33, 25, 41], relying on different types of\nnorms on the parameters of the network. But it seems pretty clear that we have not yet identified\nthe 'right' complexity measure for deep networks, as there remains many issues: these bounds are\ntypically orders of magnitude too large [29, 23], and they tend to explode as the depth L grows [41]."}, {"title": null, "content": "Two families of bounds are particularly relevant to our analysis: bounds based on covering numbers\nwhich rely on the fact that one can obtain a covering of the composition of two function classes from\ncovering of the individual classes [7, 25], and path-norm bounds which extend the techniques behind\nthe F\u2081-norm bound from shallow networks to the deep case [32, 6, 23].\nAnother issue is the lack of approximation results to accompany these generalization bounds: many\ndifferent complexity measures R(0) on the parameters @ of DNNs have been proposed along with\nguarantees that the generalization gap will be small as long as R(0) is bounded, but there are often\nlittle to no result describing families of functions that can be approximated with a bounded R(0)\nnorm. The situation is much clearer in shallow networks, where we know that certain Sobolev spaces\ncan be approximated with bounded F\u2081-norm [5].\nWe will focus on approximating composition of Sobolev functions, and obtaining close to optimal\nrates. This is quite similar to the family of tasks considered [40], though the complexity measure we\nconsider is quite different, and does not require sparsity of the parameters."}, {"title": "1.1 Contribution", "content": "We consider Accordion Networks (AccNets), which are the composition of multiple shallow networks\nfL:1 = fL\nf1, we prove a uniform generalization bound $\\mathcal{L}(f_{L:1}) - \\mathcal{L}_N(f_{L:1}) \\leq R(\\theta) \\sqrt{\\log N} /\\sqrt{N}$ for a complexity measure\n$R(\\theta) = \\sum_{l=1}^L \\frac{||f_l||_{F_1}}{Lip(f_l)} \\sqrt{d_l + d_{l-1}}$\nthat depends on the F1-norms $||f_l||_{F_1}$ and Lipschitz constanst $Lip(f_l)$ of the subnetworks, and the\nintermediate dimensions $d_0, ..., d_l$. This use of the F\u2081-norms makes this bound independent of the\nwidths $w_1,..., w_l$ of the subnetworks, though it does depend on the depth L (it typically grows\nlinearly in L which is still better than the exponential growth often observed).\nAny traditional DNN can be mapped to an AccNet (and vice versa), by spliting the middle weight\nmatrices $W_l$ with SVD $U\\Sigma V^T$ into two matrices $U\\sqrt{\\Sigma}$ and $\\sqrt{\\Sigma}V^T$ to obtain an AccNet with\ndimensions $d_l = Rank W_l$, so that the bound can be applied to traditional DNNs with bounded rank.\nWe then show an approximation result: any composition of Sobolev functions $f^* = f_L \\circ \\dots \\circ f_1$\ncan be approximated with a network with either a bounded complexity R(0) or a slowly growing one.\nThus under certain assumptions one can show that DNNs can learn general compositions of Sobolev\nfunctions. This ability can be interpreted as DNNs being able to learn symmetries, allowing them to\navoid the curse of dimensionality in settings where kernel methods or even shallow networks suffer\nheavily from it.\nEmpirically, we observe a good match between the scaling laws of learning and our theory, as well as\nqualitative features such as transitions between regimes depending on whether it is harder to learn the\nsymmetries of a task, or to learn the task given its symmetries."}, {"title": "2 Accordion Neural Networks and ResNets", "content": "Our analysis is most natural for a slight variation on the traditional fully-connected neural networks\n(FCNNs), which we call Accordion Networks, which we define here. Nevertheless, all of our results\ncan easily be adapted to FCNNs.\nAccordion Networks (AccNets) are simply the composition of L shallow networks, that is $f_{L:1} =$\n$f_L\\circ \\dots \\circ f_1$ where $f_l(z) = W_l\\sigma(V_lz + b_l)$ for the nonlinearity $\\sigma : \\mathbb{R} \\to \\mathbb{R}$, the $d_l \\times w_l$ matrix\n$W_l$, $w_l \\times d_{l-1}$ matrix $V_l$, and $w_l$-dim. vector $b_l$, and for the widths $w_1, ..., w_L$ and dimensions\n$d_0, ..., d_L$. We will focus on the ReLU $\\sigma(x) = max{0, x}$ for the nonlinearity. The parameters $\\theta$ are\nmade up of the concatenation of all $(W_l, V_l, b_l)$. More generally, we denote $f_{l_2:l_1} = f_{l_2} \\circ \\dots \\circ f_{l_1}$\nfor any $1 \\leq l_1 \\leq l_2 < L$.\nWe will typically be interested in settings where the widths $w_l$ is large (or even infinitely large), while\nthe dimensions $d_l$ remain finite or much smaller in comparison, hence the name accordion.\nIf we add residual connections, i.e. $f_{L:1}^{res} = (f_L + id) \\circ \\dots \\circ (f_1 + id)$ for the same shallow nets\n$f_1,..., f_L$ we recover the typical ResNets."}, {"title": null, "content": "Remark. The only difference between AccNets and FCNNs is that each weight matrix $M_l$ of the\nFCNN is replaced by a product of two matrices $M_l = V_lW_{l-1}$ in the middle of the network (such\na structure has already been proposed [34, 35]). Given an AccNet one can recover an equivalent\nFCNN by choosing $M_l = V_lW_{l-1}$, $M_0 = V_0$ and $M_{L+1} = W_L$. In the other direction there\ncould be multiple ways to split $M_l$ into the product of two matrices, but we will focus on taking\n$V_l = U\\sqrt{\\Sigma}$ and $W_{l-1} = \\sqrt{\\Sigma}V^T$ for the SVD decomposition $M_l = U\\Sigma V^T$, along with the choice\n$d_l = Rank M_l$. One can thus think of AccNets as rank-constrained FCNNs."}, {"title": "2.1 Learning Setup", "content": "We consider a traditional learning setup, where we want to find a function $f : \\Omega \\subset \\mathbb{R}^{d_{in}} \\to \\mathbb{R}^{d_{out}}$\nthat minimizes the population loss $\\mathcal{L}(f) = \\mathbb{E}_{x\\sim\\pi}[l(x, f(x))]$ for an input distribution $\\pi$and a\n$\\rho$-Lipschitz and $\\rho$-bounded loss function $l(x, y) \\in [0, B]$. Given a training set $x_1, ..., x_N$ of size N\nwe approximate the population loss by the empirical loss $\\mathcal{L}_N(f) = \\sum_{i = 1}^N l(x_i, f (x_i))$ that can be\nminimized.\nTo ensure that the empirical loss remains representative of the population loss, we will prove high\nprobability bounds on the generalization gap $\\mathcal{L}_N(f) - \\mathcal{L}(f)$ uniformly over certain functions families\n$f\\in\\mathcal{F}$.\nFor regression tasks, we assume the existence of a true function $f^*$ and try to minimize the distance\n$l(x, y) = || f^*(x) - y||^p$ for $p \\geq 1$. If we assume that $f^*(x)$ and y are uniformly bounded then one\ncan easily show that $l(x, y)$ is bounded and Lipschitz. We are particularly interested in the cases\n$p\\in {1, 2}$, with p = 2 representing the classical MSE, and p = 1 representing a $L_1$ distance. The\np = 2 case is amenable to 'fast rates' which take advantage of the fact that the loss increases very\nslowly around the optimal solution $f^*$, We do not prove such fast rates (even though it might be\npossible) so we focus on the p = 1 case.\nFor classification tasks on k classes, we assume the existence of a 'true class' function $f^* : \\Omega \\to$\n${1, ..., k}$ and want to learn a function $f : \\Omega \\to \\mathbb{R}^k$ such that the largest entry of $f(x)$ is the $f^*(k)$-th\nentry. One can consider the hinge cost $l(x, y) = max{0, 1 - (y_{f^*(k)} - max_{i \\neq f^*(x)} y_i)}$, which is\nzero whenever the margin $y_{f^*(k)} - max_{i \\neq f^*(x)} y_i$ is larger than 1 and otherwise equals 1 minus the\nmargin. The hinge loss is Lipschitz and bounded if we assume bounded outputs $y = f(x)$. The\ncross-entropy loss also fits our setup."}, {"title": "3 Generalization Bound for DNNS", "content": "The reason we focus on accordion networks is that there exists generalization bounds for shallow\nnetworks [5, 46], that are (to our knowledge) widely considered to be tight, which is in contrast to the\ndeep case, where many bounds exist but no clear optimal bound has been identified. Our strategy\nis to extend the results for shallow nets to the composition of multiple shallow nets, i.e. AccNets.\nRoughly speaking, we will show that the complexity of an AccNet $f_\\theta$ is bounded by the sum of the\ncomplexities of the shallow nets $f_1,..., f_L$ it is made of.\nWe will therefore first review (and slightly adapt) the existing generalization bounds for shallow\nnetworks in terms of their so-called F\u2081-norm [5], and then prove a generalization bound for deep\nAccNets."}, {"title": "3.1 Shallow Networks", "content": "The complexity of a shallow net $f(x) = W\\sigma(Vx + b)$, with weights $W\\in \\mathbb{R}^{w \\times d_{out}}$ and\n$V\\in \\mathbb{R}^{d_{in} \\times w}$, can be bounded in terms of the quantity $C = \\sum_{i=1}^w ||W_i.|| \\sqrt{||V_i.||^2 + b_i^2}$.\nFirst note that the rescaled function $\\frac{1}{C}f$ can be written as a convex combination $\\frac{1}{C}f(x) =$\n$\\sum_{i=1}^w  \\frac{||W_i.|| \\sqrt{||V_i.||^2 + b_i^2}}{C}\\frac{W_i.}{||W_i.||} \\sigma(V_i.x + b_i)$ for $W_i. = \\frac{W_i.}{||W_i.||}$, $V_i. = \\frac{V_i.}{\\sqrt{||V_i.||^2 + b_i^2}}$ and $b_i = \\frac{b_i}{\\sqrt{||V_i.||^2 + b_i^2}}$\nsince the coefficients $\\frac{||W_i.|| \\sqrt{||V_i.||^2 + b_i^2}}{C}$ are positive and sum up to 1. Thus f belongs to C times the\nconvex hull\n$\\mathcal{B}_{F_1} = Conv \\{x \\to w\\sigma(vx + b) : ||w||^2 = ||v||^2 + b^2 = 1\\}$."}, {"title": null, "content": "We call this the F\u2081-ball since it can be thought of as the unit ball w.r.t. the F\u2081-norm $|| f ||_{F_1}$ which we\ndefine as the smallest positive scalar s such that\u00b9 $f \\in s\\mathcal{B}_{F_1}$. For more details in the single output\ncase, see [5].\nThe generalization gap over any F\u2081-ball can be uniformly bounded with high probability:\nTheorem 1. For any input distribution \u03c0 supported on the L2 ball B(0, b) with radius b, we have\nwith probability 1 \u2013 \u03b4, over the training samples $x_1,..., x_N$, that for all $f \\in \\mathcal{B}_{F_1} (0, R) = R \\mathcal{B}_{F_1}$\n$\\mathcal{L}(f) - \\mathcal{L}_N(f) \\leq \\rho b R \\sqrt{\\frac{d_{in} + d_{out}}{N}} \\sqrt{\\log N} + c_{OV} \\sqrt{\\frac{2 \\log 2/\\delta}{N}}$\nThis theorem is a slight variation of the one found in [5]: we simply generalize it to multiple outputs,\nand also prove it using a covering number argument instead of a direct computation of the Rademacher\ncomplexity, which will be key to obtaining a generalization bound for the deep case. But due to this\nchange of strategy we pay a log N cost here and in our later results. We know that the log N term\ncan be removed in Theorem 1 by switching to a Rademacher argument, but we do not know whether\nit can be removed in deep nets.\nNotice how this bound does not depend on the width w, because the F\u2081-norm (and the F\u2081-ball)\nthemselves do not depend on the width. This matches with empirical evidence that shows that\nincreasing the width does not hurt generalization [8, 21, 20].\nTo use Theorem 1 effectively we need to be able to guarantee that the learned function will have a\nsmall enough F\u2081-norm. The F\u2081-norm is hard to compute exactly, but it is bounded by the parameter\nnorm: if $f(x) = W\\sigma(Vx + b)$, then $||f||_{F_1} \\leq \\frac{1}{2} (||W|| + ||V|| + ||b||_2)$, and this bound is tight\nif the width w is large enough and the parameters are chosen optimally. Adding weight decay/L2-\nregularization to the cost then leads to bias towards learning with small F\u2081 norm."}, {"title": "3.2 Deep Networks", "content": "Since an AccNet is simply the composition of multiple shallow nets, the functions represented by an\nAccNet is included in the set of composition of F\u2081 balls. More precisely, if $||W_l||^2 + ||V_l||^2 + ||b_l||^2 <$\n$2R_l$ then $f_{L:1}$ belongs to the set $\\{g_L \\circ \\dots \\circ g_1 : g_l \\in \\mathcal{B}_{F_1} (0, R_l)\\}$ for some $R_l$, which is width\nagnostic.\nAs already noticed in [7], the covering number number is well-behaved under composition, this\nallows us to bound the complexity of AccNets in terms of the individual shallow nets it is made up of:\nTheorem 2. Consider an accordion net of depth L and widths $d_1, ..., d_o$, with corresponding set of\nfunctions $\\mathcal{F} = \\{f_{L:1} : ||f_l||_{F_1} < R_l, Lip(f_l) \\leq \\rho_l\\}$. With probability 1 \u2013 \u03b4 over the sampling of the\ntraining set X from the distribution \u03c0 supported in B(0, b), we have for all $f \\in \\mathcal{F}$\n$\\mathcal{L}(f) - \\mathcal{L}_N(f) \\leq C\\rho b \\rho_{L:1}\\sum_{l=1}^L \\frac{R_l \\sqrt{d_l + d_{l-1}}}{\\rho_l} \\frac{\\sqrt{\\log N}}{\\sqrt{N}}(1 + o(1)) + c_O\\sqrt{\\frac{2\\log 2/\\delta}{N}}$\nTheorem 2 can be extended to ResNets $(f_L + id) \\circ \\dots \\circ (f_1 + id)$ by simply replacing the Lipschitz\nconstant $Lip(f_l)$ by $Lip(f_l + id)$.\nThe Lipschitz constants $Lip(f_l)$ are difficult to compute exactly, so it is easiest to simply bound it\nby the product of the operator norms $Lip(f_l) \\leq ||W_l||_{op} ||V_l||_{op}$, but this bound can often be quite\nloose. The fact that our bound depends on the Lipschitz constants rather than the operator norms\n$||W_l||_{op}, ||V_l||_{op}$ is thus a significant advantage.\nThis bound can be applied to a FCNNs with weight matrices $M_1, ..., M_{L+1}$, by replacing the middle\n$M_l$ with SVD decomposition $U\\Sigma V^T$ in the middle by two matrices $W_{l-1} = \\sqrt{\\Sigma}V^T$ and $V_l = U\\sqrt{\\Sigma}$,\nso that the dimensions can be chosen as the rank $d_l = Rank M_{l+1}$. The Frobenius norm of the new\nmatrices equal the nuclear norm of the original one $||W_{l-1}||_F = ||V_l||^2_F = ||M_l||_*$. Some bounds"}, {"title": "4 Breaking the Curse of Dimensionality with Compositionality", "content": "In this section we study a large family of functions spaces, obtained by taking compositions of\nSobolev balls. We focus on this family of tasks because they are well adapted to the complexity\nmeasure we have identified, and because kernel methods and even shallow networks do suffer from\nthe curse of dimensionality on such tasks, whereas deep networks avoid it (e.g. Figure 1).\nMore precisely, we will show that these sets of functions can be approximated by a AccNets with\nbounded (or in some cases slowly growing) complexity measure\n$R(\\theta) = \\prod_{l=1}^L Lip(f_l) \\sum_{l=1}^L \\frac{||f_l||_{F_1}}{Lip(f_l)} \\sqrt{d_l + d_{l-1}}.$\nThis will then allow us show that AccNets can (assuming global convergence) avoid the curse of\ndimensionality, even in settings that should suffer from the curse of dimensionality, when the input\ndimension is large and the function is not very smooth (only a few times differentiable)."}, {"title": "4.1 Composition of Sobolev Balls", "content": "The family of Sobolev norms capture some notion of regularity of a function, as it measures the size\nof its derivatives. The Sobolev norm of a function $f : \\mathbb{R}^{d_{in}} \\to \\mathbb{R}$ is defined in terms of its derivatives\n$\\partial^\\alpha f$ for some $d_{in}$-multi-index $\\alpha$, namely the $W^{\\nu,p}(\\pi)$-Sobolev norm with integer $\\nu$ and $p > 1$ is\ndefined as\n$||f||_{W^{\\nu,p}(\\pi)} = \\sum_{|\\alpha| \\leq \\nu} ||\\partial^\\alpha f||_{L^p(\\pi)} .$\nNote that the derivative $\\partial^\\alpha f$ only needs to be defined in the 'weak' sense, which means that even\nnon-differentiable functions such as the ReLU functions can actually have finite Sobolev norm.\nThe Sobolev balls $B_{W^{\\nu,p}(\\pi)}(0, R) = \\{f : ||f||_{W^{\\nu,p}(\\pi)} \\leq R\\}$ are a family of function spaces with a\nrange of regularity (the larger $\\nu$, the more regular). This regularity makes these spaces of functions\nlearnable purely from the fact that they enforce the function f to vary slowly as the input changes.\nIndeed we can prove the following generalization bound:\nProposition 3. Given a distribution \u03c0 with support the L2 ball with radius b, we have that with\nprobability 1 \u2013 \u03b4 for all functions $f \\in \\mathcal{F} = \\{f : ||f||_{W^{\\nu,2}} \\leq R, ||f||_{\\infty} \\leq R\\}$\n$\\mathcal{L}(f) - \\mathcal{L}_N(f) \\leq 2\\rho C_1 R E_{\\nu/d_{in}}(N) + C_O\\sqrt{\\frac{2 \\log 2/\\delta}{N}}$\nwhere $E_r(N) = N^{-\\frac{1}{2}}$ if $r > \\frac{1}{2}$, $E_r(N) = N^{-\\frac{1}{2}} \\log N$ if $r = \\frac{1}{2}$, and $E_r(N) = N^{-r}$ if $r < \\frac{1}{2}$.\nBut this result also illustrates the curse of dimensionality: the differentiability $\\nu$ needs to scale with\nthe input dimension $d_{in}$ to obtain a reasonable rate. If instead $\\nu$ is constant and $d_{in}$ grows, then the\nnumber of datapoints N needed to guarantee a generalization gap of at most $\\epsilon$ scales exponentially in\n$d_{in}$, i.e. $N \\sim e^{\\epsilon^{-d_{in}}}$. One way to interpret this issue is that regularity becomes less and less useful the\nlarger the dimension: knowing that similar inputs have similar outputs is useless in high dimension\nwhere the closest training point $x_i$ to a test point x is typically very far away."}, {"title": "4.1.1 Breaking the Curse of Dimensionality with Compositionality", "content": "To break the curse of dimensionality, we need to assume some additional structure on the data or task\nwhich introduces an 'intrinsic dimension' that can be much lower than the input dimension $d_{in}$:"}, {"title": null, "content": "Manifold hypothesis: If the input distribution lies on a $d_{surf}$-dimensional manifold, the error rates\ntypically depends on $d_{surf}$ instead of $d_{in}$ [39, 10].\nKnown Symmetries: If $f^* (g\\cdot x) = f^*(x)$ for a group action. w.r.t. a group G, then $f^*$ can be\nwritten as the composition of a modulo map $g^* : \\mathbb{R}^{d_{in}} \\to \\mathbb{R}^{d_{in}/G}$ which maps pairs of inputs which\nare equivalent up to symmetries to the same value (pairs x, y s.t. $y = g\\cdot x$ for some $g \\in G), and then\na second function $h^* : \\mathbb{R}^{d_{in}/G} \\to \\mathbb{R}^{d_{out}}$, then the complexity of the task will depend on the dimension\nof the modulo space $\\mathbb{R}^{d_{in}/G}$ which can be much lower. If the symmetry is known, then one can for\nexample fix $g^*$ and only learn $h^*$ (though other techniques exist, such as designing kernels or features\nthat respect the same symmetries) [31].\nSymmetry Learning: However if the symmetry is not known then both $g^*$ and $h^*$ have to be learned,\nand this is where we require feature learning and/or compositionality. Shallow networks are able\nto learn translation symmetries, since they can learn so-called low-index functions which satisfy\n$f^*(x) = f^*(Px)$ for some projection P (with a statistical complexity that depends on the dimension\nof the space one projects into, not the full dimension [5, 2]). Low-index functions correspond exactly\nto the set of functions that are invariant under translation along the kernel ker P. To learn general\nsymmetries, one needs to learn both $h^*$ and the modulo map $g^*$ simultaneously, hence the importance\nof feature learning.\nFor $g^*$ to be learnable efficiently, it needs to be regular enough to not suffer from the curse of\ndimensionality, but many traditional symmetries actually have smooth modulo maps, for example\nthe modulo map $g^*(x) = ||x||^2$ for rotation invariance. This can be understood as a special case of\ncomposition of Sobolev functions, whose generalization gap can be bounded:\nTheorem 4. Consider the function set $\\mathcal{F} = \\mathcal{F}_L \\circ \\dots \\circ \\mathcal{F}_1$ where $\\mathcal{F}_l =$\n$\\{f_l: \\mathbb{R}^{d_{l-1}} \\to \\mathbb{R}^{d_l} s.t. ||f_l||_{W^{v_l,2}} < R_l, ||f_l||_{\\infty} \\leq b_l, Lip(f_l) \\leq \\rho_l\\}$, and let $r_{min} = min_l \\frac{v_l}{d_{l-1}}$ for\n$r_l = \\frac{v_l}{d_{l-1}}$, then with probability 1 \u2013 \u03b4 we have for all f \u2208 F\n$\\mathcal{L}(f) - \\mathcal{L}_N(f) \\leq \\rho C_o \\sum_{l=1}^L (C_{\\ell} \\rho_{L:l+1}R_l)^{r_{min}+1} E_{r_{min}} (N) + C_O \\sqrt{\\frac{2\\log 2/\\delta}{N}}$\nwhere $C_{\\ell}$ depends only on $d_{l-1}, d_l, v_{\\ell}, b_{l-1}$.\nWe see that only the smallest ratio $r_{min}$ matters when it comes to the rate of learning. And actually\nthe above result could be slightly improved to show that the sum over all layers could be replaced by\na sum over only the layers where the ratio $r_l$ leads to the worst rate $E_{r_l} (N) = E_{r_{min}} (N)$ (and the\nother layers contribute an asymptotically subdominant amount).\nComing back to the symmetry learning example, we see that the hardness of learning a function of\nthe type $f^* = h \\circ g$ with inner dimension $d_{mid}$ and regularities $\\nu_g$ and $\\nu_h$, the error rate will be (up\nto log terms) $N^{-min\\{\\frac{v_g}{d_{in}}, \\frac{v_h}{d_{mid}}\\}}$ . This suggests the existence of three regimes depending on which"}, {"title": null, "content": "term attains the minimum: a regime where both g and h are easy to learn and we have $N^{-\\frac{1}{2}}$ learning,\na regime g is hard, and a regime where h is hard. The last two regimes differentiate between tasks\nwhere learning the symmetry is hard and those where learning the function knowing its symmetries is\nhard.\nIn contrast, without taking advantage of the compositional structure, we expect $f^*$ to be only\n$min\\{v_g, v_h\\}$ times differentiable, so trying to learn it as a single Sobolev function would lead to an\nerror rate of $N^{-min\\{\\frac{1}{2}, \\frac{min\\{v_g,v_h\\}}{d_{in}}\\}} \\ll N^{-\\frac{min\\{v_g,v_h\\}}{d_{in}}}$ which is no better than the compositional\nrate, and is strictly worse whenever $v_h < v_g$ and $\\frac{v_h}{d_{mid}} < \\frac{v_g}{d_{in}}$ (we can always assume $d_{mid} \\leq d_{in}$ since\none could always choose g = id).\nFurthermore, since multiple compositions are possible, one can imagine a hierarchy of symmetries\nthat slowly reduce the dimensionality with less and less regular modulo maps. For example one could\nimagine a composition $f_1 \\circ \\dots \\circ f_L$ with dimensions $d_l = d_0 2^{-l}$ and regularities $\\nu_l = d_0 2^{-l}$ so that\nthe ratios remain constant $r_l = \\frac{v_l}{d_{l-1}} = \\frac{d_0}{2}$ leading to an almost parametric rate of $N^{-\\frac{1}{2}} \\log N$\neven though the function may only be $d_0 2^{-L}$ times differentiable. Without compositionality, the rate\nwould only be $N^{-2^{-L}}$.\nRemark. In the case of a single Sobolev function, one can show that the rate $E_{\\nu/d_{in}}(N)$ is in some\nsense optimal, by giving an information theoretic lower bound with matching rate. A naive argument\nsuggests that the rate of $E_{min\\{r_1,...,r_L\\}} (N)$ should similarly be optimal: assume that the minimum\n$r_l$ is attained at a layer l, then one can consider the subset of functions such that the image\n$f_{l-1:1}(B(0, r))$ contains a ball $B(z, r') \\subset \\mathbb{R}^{d_{l-1}}$ and that the function $f_{L:l+1}$ is $\\beta$-non-contracting\n$||f_{L:l+1}(x) - f_{L:l+1}(y)|| \\geq \\beta ||x \u2212 y||$, then learning $f_{L:1}$ should be as hard as learning $f_l$ over the\nball B(z, r') (more rigorously this could be argued from the fact that any $\\epsilon$-covering of $f_{L:1}$ can be\nmapped to an $\\epsilon/\\beta$-covering of $f_l$), thus forcing a rate of at least $E_{r_l} (N) = E_{min\\{r_1,...,r_1\\}}(N)$.\nAn analysis of minimax rates in a similar setting has been done in [22].\nRemark. The type of model that seems most appropriate to learning composition of Sobolev functions\nare arguably Deep Gaussian Processes (DGP). While it should be relatively straightforward to\nuse DGP with a Lipschitzness constraint to recover optimal rates when the dimensions $d_l$ and\ndifferentiabilities $\\nu_l$ of the true functions are known in advance, it might not possible in general. We\nwill show that AccNet can recover almost optimal rates thanks to the adaptivity of the F\u2081 norm."}, {"title": "4.2 Breaking the Curse of Dimensionality with AccNets", "content": "Now that we know that composition of Sobolev functions can be easily learnable, even in settings\nwhere the curse of dimensionality should make it hard to learn them, we need to find a model that can\nachieve those rates. Though many models are possible 2, we focus on DNNs, in particular AccNets.\nAssuming convergence to a global minimum of the loss of sufficiently wide AccNets with two types\nof regularization, one can guarantee close to optimal rates:\nTheorem 5. Given a true function $f_{L^*:1} = f_{L^*} \\circ \\dots \\circ f_1$ going through the dimensions $d_0,..., d_{L^*}$,\nalong with a continuous input distribution $\\pi_0$ supported in B(0, b0), such that the distributions $\\pi_l$\nof $f_l(x)$ (for $x \\sim\\pi_0$) are continuous too and supported inside B(0, b_l) \\subset \\mathbb{R}^{d_l}$. Further assume\nthat there are differentiabilities $\\nu_l$ and radii $R_l$ such that $||f_l||_{W^{\\nu_l,2} (B(0,b_l))} \\leq R_l$, and $\\rho_l$ such that\n$Lip(f_l) \\leq \\rho_l$. For an infinite width AccNet with $L > L^*$ and dimensions $d_l \\geq d_1,..., d_{L^*-1}$, we\nhave for the ratios $r_l = \\frac{v_l}{d_{l-1}}+3$:\n(1) At a global min $f_{L:1}$ of $\\mathcal{L}_N(f_{L:1}) + \\lambda R(\\theta)$, we have $\\mathcal{L}(f_{L:1}) = \\tilde{O}(N^{-min\\{\\frac{1}{2},r_1,...,r_{L^*}\\}})$.\n(2) At a global min $f_{L:1}$ of $\\mathcal{L}_N(f_{L:1}) + \\lambda \\tilde{R}(\\theta)$, we have $\\mathcal{L}(f_{L:1}) = \\tilde{O}(N^{-\\frac{1}{2}+\\sum_{l=1}^{L^*} max\\{0,r_l-\\frac{1}{2}\\}}$.\nThere are a number of limitations to this result. First we assume that one is able to recover the global\nminimizer of the regularized loss, which should be hard in general\u00b3 (we already know from [5] that\nthis is NP-hard for shallow networks and a simple F\u2081-regularization). Note that it is sufficient to\nrecover a network $f_{L:1}$ whose regularized loss is within a constant of the global minimum, which\nmight be easier to guarantee, but should still be hard in general. The typical method of training with\nGD on the regularized loss is a greedy approach, which might fail in general but could recover almost\noptimal parameters under the right conditions (some results suggest that training relies on first order\ncorrelations to guide the network in the right direction [2, 1, 36]).\nWe propose two regularizations because they offer a tradeoff:\nFirst regularization: The first regularization term leads to almost optimal rates, up to the change\nfrom $r_l = \\frac{v_l}{d_{l-1}}$ to $r_l = \\frac{v_l}{\\frac{d_l}{d_{l-1}}+3}$ which is negligible for large dimensions $d_l$ and differentiabilities $v_l$. The\nfirst problem is that it requires an infinite width at the moment, because we were not able to prove\nthat a function with bounded F\u2081-norm and Lipschitz constant can be approximated by a sufficiently\nwide shallow networks with the same (or close) F\u2081-norm and Lipschitz constant (we know from [5]\nthat it is possible without preserving the Lipschitzness). We are quite hopeful that this condition\nmight be removed in future work.\nThe second and more significant problem is that the Lipschitz constants $Lip(f_l)$ are difficult to\noptimize over. For finite width networks it is in theory possible to take the max over all linear regions,\nbut the complexity might be unreasonable. It might be more reasonable to leverage an implicit bias\ninstead, such as a large learning rate, because a large Lipschitz constant implies that the nework is\nsensible to small changes in its parameters, so GD with a large learning rate should only converge to\nminima with a small Lipschitz constant (such a bias is described in [26]). It might also be possible to\nreplace the Lipschitz constant in our generalization bounds, possibly along the lines of [45].\nSecond regularization: The second regularization term has the advantage that it can be optimized\nwith GD. Furthermore it does not require an infinite width, only a sufficiently large one. The issue is\nthat it leads to rates that could be far from optimal depending on the ratios $r_l$: it recovers the same\nrate as the first regularization term if no more than one ratio $r_l$ is smaller than $\\frac{1}{2}$, but if many of these\nratios are above $\\frac{1}{2}$, it can be arbitrarily smaller.\nIn Figure 2, we compare the empirical rates (by doing a linear fit on a log-log plot of test error as a\nfunction of N) and the predicted optimal rates $min\\{\\frac{v_g}{d_{in}}, \\frac{v_h}{d_{mid}}\\}$ and observe a pretty good match.\nRemark. As can be seen in the proof of Theorem5, when the depth L is strictly larger than the true\ndepth L*, one needs to add identity layers, leading to a so-called Bottleneck structure, which was\nproven emerge as a result of weight decay in [27, 26, 47]. These identity layers add a term that scales\nlinearly in the additional depth $\\frac{(L-L^*)d_{min}}{\\sqrt{N}}$ to the first regularization, and an exponential prefactor\n$\\frac{(2^{d_{min}})^{L-L^*}}{\\sqrt{N}}$ to the second."}, {"title": "5 Conclusion", "content": "We have given a generalization bound for Accordion Networks and as an extension Fully-Connected\nnetworks. It depends on F\u2081-norms and Lipschitz constants of its shallow subnetworks. This allows us\nto prove under certain assumptions that AccNets can learn general compositions of Sobolev functions\nefficiently, making them able to break the curse of dimensionality in certain settings, such as in the\npresence of unknown symmetries."}]}