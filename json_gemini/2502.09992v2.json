{"title": "Large Language Diffusion Models", "authors": ["Shen Nie", "Fengqi Zhu", "Zebin You", "Xiaolu Zhang", "Jingyang Ou", "Jun Hu", "Jun Zhou", "Yankai Lin", "Ji-Rong Wen", "Chongxuan Li"], "abstract": "Autoregressive models (ARMs) are widely regarded as the cornerstone of large language models (LLMs). We challenge this notion by introducing LLaDA, a diffusion model trained from scratch under the pre-training and supervised fine-tuning (SFT) paradigm. LLaDA models distributions through a forward data masking process and a reverse process, parameterized by a vanilla Transformer to predict masked tokens. By optimizing a likelihood bound, it provides a principled generative approach for probabilistic inference. Across extensive benchmarks, LLaDA demonstrates strong scalability, outperforming our self-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong LLMs like LLaMA3 8B in in-context learning and, after SFT, exhibits impressive instruction-following abilities in case studies such as multi-turn dialogue. Moreover, LLaDA addresses the reversal curse, surpassing GPT-40 in a reversal poem completion task. Our findings establish diffusion models as a viable and promising alternative to ARMs, challenging the assumption that key LLM capabilities discussed above are inherently tied to ARMs.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) (Zhao et al., 2023) fall entirely within the framework of generative modeling. Specifically, LLMs aim to capture the true but unknown language distribution Pdata(\u00b7) by optimizing a model distribution p\u03b8(\u00b7) through maximum likelihood estimation, or equivalently KL divergence minimization between the two distributions:\n\nmax Epdata (x) log po (x) \u21d4 min KL(Pdata(x)||po(x)). (1)\n\u03b8\n\u03b8\n\nGenerative modeling principles\n\nThe predominant approach relies on the autoregressive modeling (ARM)-commonly referred to as the next-token prediction paradigm-to define the model distribution:\n\nPo(x) = Po(x\u00b9) \u03a0\u03a1\u04e9(x\u00b2 | x\u00b9,...,xi\u22121), (2)\ni=2\n\nAutoregressive formulation\n\nwhere x is a sequence of length L, and x\u00b2 is the i-th token.\n\nThis paradigm has proven remarkably effective (Radford,\n2018; Radford et al., 2019; Brown, 2020; OpenAI, 2022)\nand has become the foundation of current LLMs. Despite\nits widespread adoption, a fundamental question remains\nunanswered: Is the autoregressive paradigm the only viable\npath to achieving the intelligence exhibited by LLMs?"}, {"title": "2. Approach", "content": "In this section, we introduce the probabilistic formulation\u00b2, along with the pre-training, supervised fine-tuning, and inference procedures for LLaDA, as illustrated in Fig. 2.\n2.1. Probabilistic Formulation\nUnlike ARMs in Eq. (2), LLaDA defines a model distribution p\u03b8(x0) through a forward process and a reverse process (Austin et al., 2021a; Ou et al., 2024). The forward process gradually masks tokens independently in 20 until the sequence is fully masked at t = 1. For t \u2208 (0, 1), the sequence xt is partially masked, with each being masked with probability t or remaining unmasked with probability 1 - t. The reverse process recovers the data distribution by iteratively predicting masked tokens as t moves from 1 to 0.\nThe core of LLaDA is a mask predictor, a parametric model\nPo (xt) that takes xt as input and predicts all masked tokens\n(denoted M) simultaneously. It is trained using a cross-\nentropy loss computed only on the masked tokens:\n\nL(\u03b8) = -Et,xo,xt1[x = M] log pe (xoxe), (3)\ni=1\n\nwhere xo is sampled from the training data, t is sampled uniformly from [0, 1], and xt is sampled from the forward process. The indicator function 1[\u00b7] ensures that the loss is computed only for masked tokens.\n\nOnce trained, we can simulate a reverse process (see Sec. 2.4\nfor details) parameterized by the mask predictor and define the model distribution po(x0) as the marginal distribution"}, {"title": "2.2. Pre-training", "content": "LLaDA employs a Transformer (Vaswani, 2017) as the mask predictor, whose architecture is similar to existing LLMs. However, LLaDA does not use a causal mask, as its formulation allows it to see the entire input for predictions.\nWe trained two variants of LLaDA with different sizes: 1\nbillion (B) and 8B. We summarize the model architecture\nof LLaDA 8B and LLaMA3 8B (Dubey et al., 2024) here\nand details are provided in Appendix B.2. We have ensured\nconsistency in most hyperparameters while making several\nnecessary modifications. We use vanilla multi-head atten-\ntion instead of grouped query attention (Ainslie et al., 2023)\nfor simplicity, as LLaDA is incompatible with KV caching,\nresulting in a different number of key and value heads. Con-\nsequently, the attention layer has more parameters, and we\nreduce the FFN dimension to maintain a comparable model\nsize. Additionally, the vocabulary size differs slightly due\nto a tokenizer (Brown, 2020) adapted on our data.\n\nThe LLaDA model is pre-trained on a dataset comprising\n2.3 trillion (T) tokens, adhering to a data protocol that aligns"}, {"title": "2.3. Supervised Fine-Tuning", "content": "We enhance the capability of LLaDA to follow instructions by supervised fine-tuning (SFT) with paired data (po, ro), where po is the prompt and ro denotes the response. This is the simplest and most basic post-training method for LLMs. Technically, this requires to model the conditional distribution pe (ro|po) instead of po(x0) in pre-training.\nThe implementation is similar to pre-training. As shown in\nFig. 2 (b), we leave the prompt unchanged and mask the\ntokens in the response independently, as done for 20. Then,\nwe feed both the prompt and the masked response rt to the\npre-trained mask predictor to compute the loss for SFT:\n\n-Et,po,ro,rt\u03a31[re = M] log po(roopo, ,rt) e) (5)\nl=1\n\nwhere L' denotes a dynamic length specified later, and all\nother notations remain the same as before.\n\nNote that this approach is fully compatible with pre-training.\nEssentially, the concatenation of po and ro can be treated\nas clean pre-training data 20, while the concatenation of\npo and rt serves as the masked version xt. The process is\nidentical to pre-training, with the only difference being that\nall masked tokens happen to appear in the ro portion.\n\nThe LLaDA 8B model undergoes SFT on a dataset com-\nprising 4.5 million pairs. Consistent with the pre-training\nprocess, both data preparation and training follow the SFT\nprotocols utilized in existing LLMs (Chu et al., 2024; Yang\net al., 2024), without introducing any additional techniques\nto optimize LLaDA's performance. The dataset spans mul-\ntiple domains, including code, mathematics, instruction-\nfollowing, and structured data understanding. We append\nEOS tokens to the end of short pairs in each mini-batch\nto ensure equal lengths across all data. We treat |EOS| as a\nnormal token during training and remove it during sampling,\nenabling LLaDA to control the response length automati-\ncally. Please refer to Appendix B.1 for more details.\nWe train for 3 epochs on the SFT data using a similar sched-\nule to the pre-training phase. The learning rate is linearly\nincreased from 0 to 2.5 \u00d7 10-5 over the first 50 iterations\nand then kept constant. During the final 10% of iterations,\nit is linearly reduced to 2.5 \u00d7 10-6. Additionally, we set\nthe weight decay to 0.1, the global batch size to 256, and\nthe local batch size to 2 per GPU. The SFT experiment was\nexecuted once, without any hyperparameter tuning."}, {"title": "2.4. Inference", "content": "As a generative model, LLaDA is capable of both sampling new text and evaluating the likelihood of candidate text.\nWe begin with the sampling. As illustrated in Fig. 2 (c), given a prompt po, we discretize the reverse process to sample from the model distribution pe (ro|po), starting from a fully masked response. The total number of sampling steps is a hyperparameter, which naturally provides LLaDA with a trade-off between efficiency and sample quality, as analyzed in Sec. 3.3. We employ uniformly distributed timesteps by default. In addition, the generation length is also treated as a hyperparameter, specifying the length of the fully masked sentence at the beginning of the sampling process. As de-tailed in Appendix B.4, since both pre-training and SFT are conducted using datasets with variable lengths, the final results are insensitive to this length hyperparameter.\nAt an intermediate step from time t \u2208 (0, 1] to s \u2208 [0, t), we feed both po and rt into the mask predictor and predict all masked tokens simultaneously. Subsequently, we remaskof the predicted tokens in expectation to obtain rs, ensuring that the transition of the reverse process aligns with the forward process for accurate sampling (Austin et al., 2021a).\nIn principle, the remasking strategy should be purely random. However, inspired by the annealing tricks of sampling in LLMs (Holtzman et al., 2019; Brown, 2020), we explore two deterministic yet effective remasking strategies. Specifically, similarly to Chang et al. (2022), we remask theof predicted tokens with the lowest confidence based on the predictions, called low-confidence remasking. Additionally, for LLaDA after SFT, we can divide the sequence into several blocks and generate them from left to right, called semi-autoregressive remasking. Within each block, we apply the reverse process to perform sampling. We provide more details and ablation studies in Appendix. B.3.\nFor conditional likelihood evaluation, we can naturally utilize the upper bound in Eq. (5). However, we find that the following equivalent form (Ou et al., 2024) exhibits lower variance and is more stable for evaluation:\n\n-Elo,ro,ri\u03a31[r = M] log po(roopo, ,rt) (6)\ni=1\n\nwhere l is uniformly sampled from {1, 2, . . ., L}, and ri is obtained by uniformly sampling l tokens from ro without replacement for masking. In addition, we employ the unsupervised classifier-free guidance (Nie et al., 2024). We refer the readers to more details in Appendix A.2.\nWe present the training, sampling, and likelihood evaluation algorithms, along with theoretical details, in Appendix A."}, {"title": "3. Experiments", "content": "We evaluate the scalability, instruction-following, and in-context learning capabilities of LLaDA on standard benchmarks, followed by analyses and case studies on more controlled datasets to provide a comprehensive assessment.\n3.1. Scalability of LLaDA on Language Tasks\nWe first investigate the scalability of LLaDA on downstream tasks in comparison with the ARM baselines we constructed."}, {"title": "3.2. Benchmark Results", "content": "To comprehensively evaluate the in-context learning and instruction-following capabilities of LLaDA 8B, we conducted detailed comparisons with existing LLMS (Touvron\net al., 2023; Dubey et al., 2024; Chu et al., 2024; Yang et al.,\n2024; Bi et al., 2024; Jiang et al., 2023) of similar scale.\nThe selection of tasks and evaluation protocols was aligned\nwith existing studies, encompassing 15 popular benchmarks\nin general tasks, mathematics, code, and Chinese. Further\ndetails are provided in Appendix B.5. For a more direct\ncomparison, we re-evaluated representative LLMs (Touvron\net al., 2023; Dubey et al., 2024) in our implementation.\nAs shown in Tab. 1, after pretraining on 2.3T tokens,\nLLaDA 8B demonstrates remarkable performance, surpass-\ning LLaMA2 7B on nearly all tasks, and is overall competi-\ntive with LLaMA3 8B. LLaDA shows advantages in math\nand Chinese tasks. We conjecture that the strengths stem\nfrom the same factors as its relatively weaker performance\nin some tasks-differences in data quality and distribution,\nlargely due to the closed-source situation of LLM datasets.\nNotably, we have carefully ruled out the possibility of data\nleakage by taking GSM8K as an example. First, as shown\nin Fig. 3, LLaDA outperformed ARM baselines regarding\nGSM8K. Moreover, the conclusion remains on a fully un-\nseen GSM8K-like task (Ye et al., 2024) in Appendix B.7.\nFurther, Tab. 2 compares the performance of LLaDA 8B In-\nstruct with existing LLMs. We observed that SFT improved\nLLaDA's performance on most downstream tasks. A few\nmetrics, such as MMLU, showed declines, and we conjec-\nture may be due to the suboptimal quality of the SFT data.\nOverall, since we did not perform alignment with reinforce-\nment learning (RL), our results are slightly behind LLaMA3\n8B Instruct, though the gaps in many metrics remain small.\nNotably, even with only SFT, LLaDA demonstrates impres-"}, {"title": "3.3. Reversal Reasoning and Analyses", "content": "To quantify the reversal reasoning ability of mod-els (Berglund et al., 2023), we follow the protocol estab-lished in Allen-Zhu & Li (2023). Specifically, we constructa dataset of 496 famous Chinese poem sentence pairs. Givena sentence from a poem, models are tasked with generatingthe subsequent line (forward) or the preceding line (rever-sal) without additional fine-tuning. Examples can be foundin Appendix B.8. This setting provides a straightforwardand more realistic evaluation compared to previous stud-ies (Nie et al., 2024; Kitouni et al., 2024).\nAs shown in Tab. 3, LLaDA effectively addresses the rever-sal curse (Berglund et al., 2023), demonstrating consistentzero-shot performance across both forward and reversaltasks. In contrast, both Qwen 2.5 and GPT-40 exhibit asignificant gap between the two. The results on forwardgeneration confirm that both ARMs are strong, benefitingfrom significantly larger datasets and greater computationalresources than LLaDA. However, LLaDA outperforms both by a large margin in the reversal task.\nWe emphasize that we did not design anything special forreversal tasks. Intuitively, LLaDA treats tokens uniformlywithout inductive bias, leading to balanced performance.See more details in Appendix A.2.\nWe also analyze the effect of remasking strategies and sampling steps, as detailed in Appendix B.3 and Appendix B.6."}, {"title": "3.4. Case Studies", "content": "We present samples generated by LLaDA 8B Instruct in\nTab. 4, showcasing its instruction-following capabilities.\nFirst, the table illustrates LLaDA's ability to generate co-\nherent, fluent, and extended text in a non-autoregressive\nmanner. Second, it highlights the model's multi-turn dia-\nlogue capability, effectively retaining conversation history"}, {"title": "5. Conclusion and Discussion", "content": "We introduce LLaDA, a principled and previously unexplored approach to large language modeling based on diffusion models. LLaDA demonstrates strong capabilities in scalability, in-context learning, and instruction-following, achieving performance comparable to strong LLMs. In addition, LLaDA offers unique advantages such as bidirectional modeling and enhanced robustness, effectively addressing several inherent limitations of existing LLMs. Our findings not only establish diffusion models as a viable and promising alternative but also challenge the prevailing assumption that these essential capabilities are inherently tied to ARMs.\nWhile promising, the full potential of diffusion models re-"}]}