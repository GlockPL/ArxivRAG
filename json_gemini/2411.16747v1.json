{"title": "FollowGen: A Scaled Noise Conditional Diffusion Model for Car-Following Trajectory Prediction", "authors": ["Junwei You", "Rui Gan", "Weizhe Tang", "Zilin Huang", "Jiaxi Liu", "Zhuoyu Jiang", "Haotian Shi", "Keshu Wu", "Keke Long", "Sicheng Fu", "Sikai Chen", "Bin Ran"], "abstract": "Vehicle trajectory prediction is crucial for advancing autonomous driving and advanced driver assistance systems (ADAS). Although deep learning-based approaches-especially those utilizing transformer-based and generative models-have markedly improved prediction accuracy by capturing complex, non-linear patterns in vehicle dynamics and traffic interactions, they frequently overlook detailed car-following behaviors and the inter-vehicle interactions critical for real-world driving applications, particularly in fully autonomous or mixed traffic scenarios. To address the issue, this study introduces a scaled noise conditional diffusion model for car-following trajectory prediction, which integrates detailed inter-vehicular interactions and car-following dynamics into a generative framework, improving both the accuracy and plausibility of predicted trajectories. The model utilizes a novel pipeline to capture historical vehicle dynamics by scaling noise with encoded historical features within the diffusion process. Particularly, it employs a cross-attention-based transformer architecture to model intricate inter-vehicle dependencies, effectively guiding the denoising process and enhancing prediction accuracy. Experimental results on diverse real-world driving scenarios demonstrate the state-of-the-art performance and robustness of the proposed method.", "sections": [{"title": "1. Introduction", "content": "Vehicle trajectory prediction is critical in advancing autonomous driving and advanced driver assistance systems (ADAS). Predicting future vehicle positions is essential for collision avoidance, route planning, and adaptive cruise control. In recent years, deep learning-based approaches have propelled the field forward by providing adaptive solutions that can learn complex, nonlinear patterns and temporal dependencies from large datasets [1-3, 7, 8, 11, 16, 27, 28, 30, 31, 33, 36, 40, 45, 49, 50]. Among them, denoising diffusion model-based generative models have attracted great attention for their ability to capture diverse motion patterns under uncertainty. Recent advancements in these models provide a flexible framework for generating multimodal trajectory distributions and refining noise into realistic motion sequences [5, 9, 21, 47]. By incorporating spatial constraints, social interactions, and geometric properties, diffusion-based models have the potential to produce vehicle trajectories that closely align with real-world dynamics, which makes them highly effective for autonomous driving applications where accurate predictions are critical.\nTo further improve prediction accuracy, recent studies integrate interaction information between agents within deep learning models, focusing on general interactions with surrounding vehicles to capture the complex dynamics between a target vehicle and neighboring agents in dynamic traffic scenarios. For instance, the study [19] proposes a grid-based method, which utilizes occupancy grid maps centered on the target vehicle and surrounding agents to approximate trajectory distributions. Extending grid-based techniques, the hierarchical CapsNet framework [34] uses geographic grid maps to encode both spatial and temporal dependencies among vehicles, preserving the spatial relationships essential for predictions in Vehicle-to-Everything (V2X) networks. Additionally, graph-based models combine GCNs with temporal encoders or attention mechanisms to capture multi-agent interactions, jointly predicting vehicle trajectories with interpretability [38, 39, 51, 52].\nIn addition to general interactions, car-following behavior has traditionally been a central focus in traffic modeling due to its direct impact on safe following distances and the stability of traffic flow. Recently, integrating car-following dynamics into deep learning-based trajectory prediction models has gained momentum, enabling more precise modeling of inter-vehicle dependencies. For instance, TransFollower [56] introduces a transformer-based architecture that leverages historical driving data and future speed profiles of the leading vehicle to capture the dependencies in car-following behavior. Another study [41] proposes an integrated prediction framework that combines car-following and lane-changing behaviors in a unified framework with BiLSTM and TCN layers, along with an attention-based switch mechanism, to capture transitions between these behaviors, thus enabling precise trajectory predictions in dynamic traffic environments.\nInspired by prior research on car-following behavior, it is clear that the impact of detailed car-following dynamics on trajectory prediction merits further investigation. Unlike broader multi-agent interactions, car-following behavior focuses on fine-grained adjustments, such as acceleration, deceleration, and precise following distance relative to the leading vehicle, which is essential, particularly in dense traffic scenarios. These micro-interactions are also important in fully autonomous vehicles (AVs) or mixed environments, where precise responses to the leading vehicle are essential for maintaining safe and efficient traffic flow [20]. In particular, in mixed-traffic environments where AVs and human-driven vehicles (HVs) coexist, AVs must account for the variability and unpredictability of the leading HV behavior. However, trajectory prediction based specifically on car-following behaviors remains largely unexplored in both mixed and AV-only contexts, leaving a gap in current deep-learning models that do not fully capture this critical information. At the same time, diffusion model-based generative frameworks offer a promising but unexplored approach for capturing the dynamics of car-following behavior in trajectory prediction.\nHence, this study aims to develop a diffusion model-based generative framework for predicting vehicle trajectory integrating the detailed car-following interactions between two adjacent vehicles in varied traffic environments, as illustrated in Fig. 1. The main contributions of this paper are as follows:\n\u2022 We introduce FollowGen, a novel generative framework for forecasting vehicle trajectory in car-following scenarios incorporating detailed inter-vehicle interactions.\n\u2022 We develop a temporal feature encoding pipeline consisting of GRU, vehicle location-based attention, and Fourier embedding, effectively extracting the temporal features from the historical vehicle trajectory.\n\u2022 We propose a noise scaling strategy that conditions the isotropic Gaussian noise on encoded historical movement features of the vehicle. Scaled noise substitutes isotropic noise in the diffusion process.\n\u2022 We model the car-following inter-vehicle dynamics via a cross-attention-based transformer architecture. The extracted interaction embedding is induced in the denoising network to guide the trajectory generation process.\n\u2022 We validate the robustness and generality of FollowGen on multiple real-world scenarios, including HV following HV, HV following AV, and AV following HV, through comparative and ablative studies."}, {"title": "2. Related Work", "content": "Generative Model-Based Trajectory Prediction. Generative models effectively enhance vehicle trajectory prediction by capturing the inherent uncertainty and variability in driving behavior that deterministic models often overlook. These models can generate a distribution of possible future trajectories, producing a more comprehensive and realistic prediction framework. The major types of generative models include GANs, flow-based methods, VAEs, and diffusion models. In general, GAN-based models utilize a generator to produce plausible trajectories and a discriminator to evaluate their realism, which refines predictions through adversarial training [6, 10, 46, 54]. Unlike GANs, flow-based approaches transform a simple distribution into a complex one by learning invertible mappings and thus enable the generation of diverse trajectories [4, 29]. VAEs and their variants typically encode trajectories into a latent space and decode them back, allowing the generation of diverse trajectories by sampling from the latent space [12, 15, 22, 23, 32].\nDiffusion Models in Trajectory Prediction. Diffusion models have recently gained prominence in trajectory prediction due to their robust ability to handle uncertainty and generate diverse, realistic trajectories. Starting from a simple, usually Gaussian distribution, diffusion models gradually transform this distribution into the complex distribution of real-world trajectories by learning the underlying data structure. Moreover, diffusion models have shown superiority in handling complex traffic scenarios where interactions between multiple agents and environment must be carefully considered. The flexibility of the diffusion framework to incorporate spatial and temporal dependencies through advanced architectures, such as transformers and GNNs, further enhance their predictive performance. In summary, diffusion models' capability to learn and generalize from large datasets with robustness to uncertainty makes them a prevailing method to predict trajectories.\nPlenty of studies in literature have verified this. A diffusion-based model for environment-aware trajectory prediction is introduced in [48], where its robustness and ability to handle complex traffic scenarios by leveraging conditional diffusion processes to model trajectory uncertainty is highlighted. A multi-modal vehicle trajectory prediction framework presented in [25] uses a conditional diffusion model to address trajectory sparsity and irregularity in world coordinate systems. Combined with CNNs, a hierarchical vector transformer diffusion model developed in [43] captures trajectory uncertainty and further improves prediction performance. Another trajectory prediction framework called motion indeterminacy diffusion (MID) introduced in [18], is designed to handle the indeterminacy of human behavior and provide accurate stochastic trajectory predictions. A recent study [44] has also explored using a diffusion model for pedestrian trajectory prediction in semi-open autonomous driving environments, focusing on reducing computational overhead and improving the accuracy of multi-agent joint trajectory predictions. The Conditional Equivariant Diffusion Model (EquiDiff) [5] combines the diffusion model with SO(2) equivariant transformer to utilize the geometric properties of location coordinates. It also applies RNNs and Graph Attention Networks (GAT) to extract social interactions from historical trajectories. Although the majority of diffusion-based trajectory prediction models have incorporated social interactions into their structures for improved performance, the integration of detailed car-following behaviors and complex vehicular interactions remains unexplored. Therefore, the proposed FollowGen is dedicated to integrating the advantages of diffusion models with finer details of car-following dynamics to capture detailed vehicle adjustments and enhance trajectory prediction performance."}, {"title": "3. Methodology", "content": null}, {"title": "3.1. Problem Formulation", "content": "To formalize the trajectory prediction problem in the context of car following, we first group vehicles as platoons. For simplicity, if a platoon contains two vehicles where one vehicle is driving followed by another, the vehicle in the front is defined as the leading vehicle while the other one is the following vehicle. FollowGen aims to capture the intricate dynamics and the probabilistic nature of inter-vehicular dependencies within a platoon. Let $x^{his}_{fol} \\in \\mathbb{R}^{T_{his} \\times D}$ and $v^{his}_{fol} \\in \\mathbb{R}^{T_{his} \\times 1}$ denote the historical positions and speeds of the i-th vehicle in a two-vehicle platoon, where $T_{his}$ is the number of historical time steps, D is the spatial dimensionality (e.g., D = 2 for 2D positions), and i \u2208 {lea, fol} denotes the leading and following vehicles respectively. Let $\\Delta x^{his} \\in \\mathbb{R}^{T_{his} \\times D}$ and $\\Delta v^{his} \\in \\mathbb{R}^{T_{his} \\times 1}$ represent the historical spacing and speed difference between the two vehicles. The goal is to predict the future trajectory $x^{fut}_{fol} \\in \\mathbb{R}^{T_{fut} \\times D}$ of the following vehicle given the historical information stated above, as shown in the equation below:\n$x^{fut}_{fol} = f(x^{his}_{fol}, v^{his}_{fol}, x^{his}_{lea}, v^{his}_{lea}, \\Delta x^{his}, \\Delta v^{his}) \\quad(1)$\nwhere f() represents the proposed FollowGen model. The overall framework of FollowGen is shown in Fig. 2. The proposed model has four main modules: Historical Feature Encoding (Section 3.2.1), which uses a structured pipeline to encode the following vehicle's historical features, Noise Scaling and Addition (Section 3.2.2), which scales Gaussian noise with historical features; Car-Following Interaction Modeling (Section 3.3.1), which captures detailed car-following dependencies; and Condition Guided Denoising (Section 3.3.2) which removes noise using a denoising network guided by car-following interactions for efficient trajectory prediction."}, {"title": "3.2. Forward Process", "content": null}, {"title": "3.2.1 Historical Feature Encoding", "content": "We design a pipeline to encode effective features from historical trajectories. Initially, the historical trajectory $x^{his}_{fol} \\in \\mathbb{R}^{T_{his} \\times D}$ of the following vehicle is concatenated with its historical speed $v^{his}_{fol} \\in \\mathbb{R}^{T_{his} \\times 1}$, forming an input $Z^{input} \\in \\mathbb{R}^{T_{his} \\times (D+1)}$, which is then fed into stacked GRU layers [13] for data fusion and temporal feature extraction. The output of the GRU layers $z^{gru} \\in \\mathbb{R}^{T_{his} \\times H}$ is then passed through a location-based attention layer.\nGiven the initial attention weights $w_0 \\in \\mathbb{R}^{T_{his} \\times 1}$, the operation of the location-based attention is formulated as follows:\n$w_1 = softmax(W \\cdot (z^{GRU} \\oplus w_0) + b) \\quad(2)$\n$Z^{loc} = w_1 \\odot z^{GRU} \\quad(3)$\nwhere $W \\in \\mathbb{R}^{H \\times 1}$ is the weight matrix for linear projection, $b \\in \\mathbb{R}^{1}$ is a bias vector, $w_1 \\in \\mathbb{R}^{T_{his} \\times 1}$ represents the updated attention weights, $Z^{loc} \\in \\mathbb{R}^{T_{his} \\times H}$ is the output of the location-based attention layer, and $\\odot$ refers to Hadamard product. Subsequently, after another linear projection, the GRU output $z^{GRU}$ is transformed into a weighted sequence denoted as $z^{GRU} \\in \\mathbb{R}^{T_{his} \\times H'}$.\nWhile GRU is adept at capturing long-term temporal dependencies of a sequence, Fast Fourier transform (FFT) [14, 53] is applied subsequently to reveal the periodic patterns. Taking the weighted GRU output $z^{GRU}$ as input, FFT is formulated as the equation below:\n$Z_{FFT}[i] = \\sum_{n=0}^{N-1} z^{GRU}[n] \\cdot e^{-j \\cdot 2\\pi \\cdot i} \\quad(4)$\nwhere N is the length of the input sequence, $z^{GRU}[n]$ is the value of the input time-domain sequence at the n-th sample, n \u2208 [0, N \u2212 1], $e^{-j \\cdot 2\\pi \\cdot i}$ is the complex exponential function that represents the basis functions of FFT, j is the imaginary unit, and $Z_{FFT}[i]$ is the value of the transformed frequency-domain sequence at the i-th frequency bin, i \u2208 [0, N \u2013 1]. A linear layer is connected at the end, yielding the encoded historical feature embeddings $z^{his}_{fol}$."}, {"title": "3.2.2 Noising Scaling and Addition", "content": "Generally, the noise scaling strategy reshapes the isotropic Gaussian noise used in the diffusion process by extracting historical trajectory features. By conditioning the noise on historical features, we ensure that the forward diffusion process incorporates necessary conditions or restrictions reflective of the system's true dynamics. This results in a more informed and directed process of transitioning from data to noise, ensuring that the generated future trajectories are not only a product of random noise but are informed by the system's past. Statistically, this means that instead of sampling from the standard normal distribution $\\mathcal{N}(0, I)$ as what traditional diffusion models would do, the noise is now sampled from the distribution $\\mathcal{N}(0, \\Sigma_{cov})$, where the covariance matrix $\\Sigma_{cov} \\in \\mathbb{R}^{H\" \\times H\"}$ is a diagonal matrix, and is represented from the encoded historical embedding $z^{his}_{fol} \\in \\mathbb{R}^{T_{his} H\"}$. Specifically, to find $\\Sigma_{cov}$, we first take the mean along the time dimension of $z^{his}_{fol}$ which yields a vector denoted as $\\mu \\in \\mathbb{R}^{H\"}$, and then apply the Softplus activation function [55] upon $\\mu$. The resulting vector, denoted as $\\sigma^2 \\in \\mathbb{R}^{H\"}$, is used as the scaling factor to reshape the standard normal distribution to maintain the variance of $\\Sigma_{cov}$. This process is formulated as follows:\n$\\sigma^2 = \\log(1 + e^{\\mu}) \\quad(5)$\n$\\Sigma_{cov} = diag(\\sigma^2) \\quad(6)$\nIn practice, given that $\\epsilon_0 \\sim \\mathcal{N}(0, I)$ is an independent standard normal variable randomly sampled from a standard normal distribution, the scaled noise denoted as $\\epsilon$ can also be expressed directly as follows:\n$\\epsilon = \\Sigma_{cov}^{\\frac{1}{2}} \\odot \\epsilon_0 = diag(\\sigma) \\odot \\epsilon_0, \\quad \\epsilon \\sim \\mathcal{N}(0, \\Sigma_{cov}) \\quad(7)$\nOn this basis, the forward incremental noise addition process will take the future trajectory $x^{fut}_{fol}$ as input and gradually add the scaled noise to the input for K time steps, which is formulated as follows:\n$x^{fut}_{fol,k} = \\sqrt{\\alpha_k} x^{fut}_{fol,k-1} + \\sqrt{\\beta_k} \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\Sigma_{cov}) \\quad(8)$\n$q(x^{fut}_{fol,k}|x^{fut}_{fol,k-1}) = \\mathcal{N}(x^{fut}_{fol,k}; \\sqrt{\\alpha_k} x^{fut}_{fol,k-1}, \\beta_k \\Sigma_{cov}) \\quad(9)$\nwhere $\\beta_k$ is the time step-specific factor to control the intensity of the noise added at each step, $\\epsilon$ represents the noise vector sampled from a Gaussian distribution with covariance matrix $\\Sigma_{cov}$ as stated above, $x^{fut}_{fol,k}$ is the data distribution at time step k after undergoing k times of noise addition, $x^{fut}_{fol,k-1}$ is the data vector at the previous time step k - 1, and $\\alpha_k = 1 - \\beta_{\\kappa}$.\nDefine $\\bar{\\alpha_k} = \\prod_{i=1}^k \\alpha_i$, and the diffusion process at any step k from the original data $x^{fut}_{fol}$ can be expressed in a closed form:\n$x^{fut}_{fol,k} = \\sqrt{\\bar{\\alpha_k}} x^{fut}_{fol} + \\sqrt{(1 - \\bar{\\alpha_k})}\\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\Sigma_{cov}) \\quad(10)$\n$q(x^{fut}_{fol,k}|x^{fut}_{fol}) = \\mathcal{N}(x^{fut}_{fol,k}; \\sqrt{\\bar{\\alpha_k}} x^{fut}_{fol}, (1 - \\bar{\\alpha_k}) \\Sigma_{cov}) \\quad(11)$\nUltimately, when K \u2192 \u221e, $x^{fut}_{fol, K}$ will approximate following the prior noise distribution used in the diffusion process, $x^{fut}_{fol, K} \\sim \\mathcal{N}(0, \\Sigma_{cov})$. The distribution of the entire sequence from $x^{fut}_{fol}$ to $x^{fut}_{fol, k}$ conditioned on the original data $x^{fut}_{fol}$ is shown as follows:\n$q(x^{fut}_{fol, 0:K}|x^{fut}_{fol}) = \\prod_{k=1}^K q(x^{fut}_{fol,k}|x^{fut}_{fol,k-1}) \\quad(12)$"}, {"title": "3.3. Reverse Process", "content": null}, {"title": "3.3.1 Car-Following Vehicular Interaction Modeling", "content": "We propose a cross-attention-based transformer architecture [17] to model the intricate dependencies and dynamic interactions between the specified car-following variables. As mentioned in Section 3.1, these variables, including historical positions and speed profiles of leading vehicles, as well as the spacing and speed difference, are processed through dedicated GRU layers to capture temporal patterns. The results are then linearized and concatenated to pass through a pooling layer, which forms the key and value vectors, denoted as K and V, respectively, where K \u2208 $\\mathbb{R}^{d_k}$, V\u2208 $\\mathbb{R}^{d_v}$. We can also define the following vehicle's encoded trajectory $z^{his}_{fol}$ as a query vector Q, Q \u2208 $\\mathbb{R}^{d_q}$. Q is used within the cross-attention transformer block to selectively weigh the leading vehicles' features, synthesizing a contextualized output that incorporates the interactive behavior of the vehicles in a platoon. The formulation of Q, K and V is shown as follows:\n$Q=z^{his}_{fol} \\quad(13)$\n$K, V = Pooling(Concat(Linear(GRU($x^{his}_{lea}$)),\nLinear(GRU($v^{his}_{lea}$)),\nLinear(GRU($\\Delta x^{his}$)))) \\quad(14)$\nExpressing the multi-head cross-attention operation as:\n$z_{MCA} = Concat(head_1, ..., head_i, ..., head_h)W^{out} \\quad(15)$\nwhere $W^{out}$ is the output weight matrix that linearly transforms the concatenated vector from all the heads into the desired output dimension, each attention head, $head_i$, is computed as the equation below:\n$head_i = softmax(\\frac{QW^{que}(KW^{key})^T}{\\sqrt{d_k}}) (W^{val}) \\quad(16)$\nwhere $W^{que}$, $W^{key}$, and $W^{val}$ are the parameter matrices specific to each head for the queries, keys, and values.\nFinally, the output of the cross-attention transformer block enriched with vehicle relational information is embedded into a denoising network to direct and enhance the prediction accuracy."}, {"title": "3.3.2 Condition Guided Denoising", "content": "Taking the scaled noise $ \\epsilon \\sim \\mathcal{N}(0, \\Sigma_{cov})$ as input, the denoising network reconstructs the corresponding clean and accurate future trajectory of the following vehicle. In doing so, it reverses the diffusion process by sequentially predicting and removing the noise distribution introduced at each time step, thereby progressively restoring the corresponding trajectory to its original uncorrupted distribution. Given the estimated data distribution $x^{fut}_{fol,k}$ at any time step k and the contextual information c from the cross-attention transformer block, estimation of the data distribution at time k-1 is shown as the following equation:\n$p_{\\theta}(x^{fut}_{fol,k-1}|x^{fut}_{fol,k}, c) = \\mathcal{N}(x^{fut}_{fol,k-1}; \\mu_{\\theta}(x^{fut}_{fol,k}, k, c), \\Sigma_{\\theta}(k)) \\quad(17)$\nwhere $\\mu_{\\theta}(x^{fut}_{fol,k}, k, c) \\in \\mathbb{R}^{T_{fut} \\times D}$ is the predicted mean for recovering $x^{fut}_{fol,k-1}$, informed by the context encoding c, and $\\Sigma_{\\theta}(k) \\in \\mathbb{R}^{T_{fut} \\times D \\times D}$ is the learned covariance matrix at time step k. The joint probability over the sequence conditioned on c, is given by:\n$p_{\\theta}(x^{fut}_{fol,0:K}|c) = p(x^{fut}_{fol, K}) \\prod_{k=1}^K p_{\\theta}(x^{fut}_{fol,k-1}|x^{fut}_{fol,k}, c) \\quad(18)$\n$p(x^{fut}_{fol,k}) = \\mathcal{N}(x^{fut}_{fol,k}; 0, \\Sigma_{cov}) \\quad(19)$"}, {"title": "3.4. Training Objective", "content": "The training objective of FollowGen contains three parts. The first is to maximize the variational lower bound (ELBO), which can be simplified to measure the accuracy of noise prediction, as shown below:\n$\\mathcal{L}_{simp}(\\theta) = \\mathbb{E}_{x^{fut}, \\epsilon \\in \\mathbb{R}^{T_{fut} \\times D}, k} [||\\epsilon - \\epsilon_{\\theta}(x^{fut}_{fol,k}, k, c)||^2] \\quad(20)$\nIn the second part, to discourage unrealistic predictions where the following vehicle overtakes the leading vehicle, we introduce a spacing penalty. Denote the projected longitudinal spacing between the leading vehicle's future trajectory and the following vehicle's predicted trajectory as:\n$\\Delta x^{fut} = (x^{fut}_{lea} - x^{fut}_{fol}) \\cdot e_d \\quad(21)$\nwhere $e_d$ is the unit vector in the direction of travel. The spacing penalty $\\mathcal{L}_{spacing}$ is defined as:\n$\\mathcal{L}_{spacing} = \\mathbb{E} \\begin{cases} 0, & \\text{if } \\Delta x^{fut} > 0, \\\\ (-\\Delta x^{fut})^2, & \\text{if } -\\delta < \\Delta x^{fut} < 0, \\\\ \\delta(-\\Delta x^{fut} - \\delta), & \\text{if } \\Delta x^{fut} < -\\delta, \\end{cases} \\quad(22)$\nwhere $\\delta$ is a threshold parameter controlling the transition between the quadratic and linear penalty regions. This formulation penalizes predictions where the following vehicle is ahead of the leading vehicle and thus ensures realistic spacing patterns.\nThe third part applies a collision penalty to prevent the following vehicle from getting too close to the leading vehicle by enforcing a minimum rational safe distance:\n$\\mathcal{L}_{collision} = \\mathbb{E} [exp(-\\frac{\\Delta x^{fut}}{dist})] \\quad(23)$\nwhere dist is a predefined threshold for safe spacing.\nThe total loss function, $\\mathcal{L}_{total}$, combines these three components with $\\lambda_1$ and $\\lambda_2$ as weighting factors for the spacing and collision penalties:\n$\\mathcal{L}_{total} = \\mathcal{L}_{simp} + \\lambda_1 \\mathcal{L}_{spacing} + \\lambda_2 \\mathcal{L}_{collision} \\quad(24)$\nIn the denoising network, the noise at each time step is predicted through an adapted U-Net [35] that incorporates the encoded car-following interactions."}, {"title": "4. Experiments", "content": null}, {"title": "4.1. Experimental Setup", "content": "Dataset. We evaluate the proposed FollowGen on the Large Car-Following Dataset Based on the Lyft Level-5 Dataset [24]. This dataset includes over 29,000 HV-following-AV (H-A), 9,000 AV-following-HV (A-H), and 42,000 HV-following-HV (H-H) instances, with a total driving distance of more than 150,000 kilometers, providing diverse car-following scenarios essential for our study.\nMetrics. We evaluate our model using Root Mean Squared Error (RMSE), Average Displacement Error (ADE), Final Displacement Error (FDE), and Missing Rate (MR) to test the trajectory prediction performance across various time horizons. RMSE captures the overall prediction accuracy by averaging errors across all timestamps, while FDE measures the Euclidean distance between predicted and actual endpoints, which offers an assessment of final position accuracy. ADE averages the displacement errors across each timestamp, reflecting cumulative accuracy over the trajectory. Finally, MR calculates the percentage of predictions exceeding a specified FDE threshold of 2 meters, which provides insight into prediction reliability across different horizons. In this study, each car-following scenario is evaluated separately.\nImplementation Details. The proposed FollowGen is trained with Intel Core I7 CPUs and a single NVIDIA RTX 4090 GPU. For each scenario of H-A, A-H, and H-H, we employ a diffusion process with steps K = 200 using a linear beta schedule. The augmented loss function $\\mathcal{L}_{total}$ is applied with weights $\\lambda_1 = \\lambda_2 = 0.001$ based on hyperparameters. $\\delta$ is set to be 2, and dist is defined as 2 meters. AdamW is applied as the optimizer with a learning rate of 0.001 and an epsilon value of 0.01. With the batch size of 64, the training process converges substantially within 20 epochs for both HA and HH cases, while it takes 50 epochs to converge for the AH scenario. Gradient clipping with a maximum norm of 1.0 was applied for stability."}, {"title": "4.2. Comparison with State-of-the-art", "content": "We compare the proposed FollowGen model with state-of-the-art methods, including BAT [26], TUTR [42], and CRA-T-Pred [37], across three car-following scenarios: H-H, A-H, and H-A, evaluated over prediction horizons of T = 3s, 4s, and 5s, as presented in Table 1. FollowGen demonstrates superior prediction capabilities, which is particularly evident in metrics that emphasize final position accuracy and reliability, such as FDE and MR. For instance, in the H-H scenario, at T = 5s, FollowGen achieves an FDE of 3.35, which is approximately 44% lower than the 5.18 of BAT and 11% lower than the 3.62 of CRA-T-Pred. Similarly, the MR is 42% lower than the 0.91 of BAT and 25% lower than the 0.66 of CRA-T-Pred, showcasing FollowGen's robustness in predicting human-human interactions. In the A-H scenario, FollowGen's FDE at T = 5s is 3.35, compared to 8.07 for BAT, translating to a 59% improvement. MR is also significantly reduced by 32% compared to TUTR. The H-A scenario sees similar benefits: FollowGen achieves an FDE of 2.58 at T = 5s, which is 24% percent lower than the 3.19 of CRA-T-Pred. The MR is also 20% lower than the value achieved by BAT, which reflects FollowGen's strength in mixed human-AV interactions. These findings suggest that FollowGen's diffusion-based generative approach adeptly captures the complexities and stochastic nature of interactions in diverse scenarios."}, {"title": "4.3. Diffusion Process Evaluation", "content": "We examine the effect of the diffusion process parameters on FollowGen's performance to further understand its advantages. Specifically, we evaluate the impact of varying diffusion steps K and beta schedules.\nIn this study, we tested four values of diffusion steps: K = 50, 100, 200, and 500. The results are compared in Fig. 2. A small number of steps, such as K = 50, result in insufficient refinement of the generated trajectories, and leading to less accurate predictions. Conversely, excessive steps, like K = 500, increase computational cost without significant improvement in performance and may cause over-smoothing. We found that K = 200 achieves a balanced level of refinement, efficiently capturing essential driving behaviors. The choice of beta schedule also influences performance, with the linear schedule consistently outperforming the sigmoid and quadratic options, as shown in Table 2. This suggests that the linear schedule enables smoother noise variance control during diffusion, which enhances the prediction stability, especially in mixed human-AV interaction scenarios where uncertainty is high."}, {"title": "4.4. Ablation Study", "content": "An ablation study is conducted to evaluate the contribution of each component in the proposed FollowGen model. We construct three variants of the FollowGen model by systematically removing or altering key components and assessing their impact on prediction performance. The logic of constructing these variants is described as follows:\nw/o Noise Scaling. This variant avoids using the scaling factor encoded from historical information; instead, it still uses the isotropic Gaussian noise during diffusion.\nw/o Location-Based Attention and FFT. This variant eliminates the GRU layers, the location-based attention, and the FFT layer used for historical information encoding. A linear layer is used as a substitute.\nw/o Cross-Attention Transformer. This variant similarly replaces the cross-attention transformer block with a linear layer for car-following dependency modeling.\nThe results of the ablation study are shown in Table 3. For all scenarios, the full model configuration consistently achieves the best performance across all metrics, which verifies the importance of each component in FollowGen. Notably, the absence of noise scaling or cross-attention transformer, key elements in capturing historical and car-following dependencies, leads to a marked degradation in predictive accuracy. In the A-H scenario, for instance, removing the cross-attention transformer increases the FDE from 3.35 to 15.21, highlighting its critical role in modeling dependencies. Similarly, in the H-A scenario, excluding noise scaling causes the FDE to rise from 2.58 to 11.80, demonstrating the importance of noise scaling in adapting to historical information for robust predictions. The results suggest that each component contributes uniquely to enhancing the model's capability to capture the complex dynamics of vehicle interactions, and their synergy is crucial for accurate and reliable predictions."}, {"title": "5. Conclusion", "content": "This study develops FollowGen, a novel approach for vehicle car-following trajectory prediction through a scaled noise conditional diffusion model. Experimental results demonstrate that the model better predicts vehicle trajectories across diverse real-world scenarios by encoding historical features and integrating detailed inter-vehicular car-following dynamics within the generative framework. Future work could extend FollowGen to more complex interactions like lane-changing, integrate multimodal sensor data for adaptability, and optimize diffusion efficiency for potential real-time applications."}, {"title": "6. Key Module Architectures", "content": "The section presents the architectures of the cross-attention transformer block and the denoising network, the critical modules of FollowGen that are not detailed earlier."}, {"title": "6.1. Cross-Attention Transformer Block", "content": "The cross-attention transformer block [17], shown in Fig. 4, models inter-vehicle dependencies using Q, K, and V derived from historical and car-following features. The block employs scaled dot-product attention, where Q interacts with K through a normalized softmax function, producing attention weights. These weights are used to combine V into a context-rich representation. Multi-head attention ensures diverse interaction patterns are captured, and the outputs are concatenated and transformed through a linear layer. Residual connections and layer normalization stabilize training, resulting in the refined output ZCAT that encapsulates fine-grained vehicle interactions."}, {"title": "6.2. Denoising Network", "content": "Fig. 5 shows the structure of the denoising network, which is responsible for reversing the diffusion process and reconstructing the trajectory. The network takes the noised future trajectory $x^{fut}_{fol,k} \\in \\mathbb{R}^{T_{fut} \\times D}$ and the interaction-enhanced output ZCAT as inputs. During the downsampling stage, ZCAT is concatenated with the feature maps after the first convolutional block to introduce vehicle interaction conditions. The network applies transposed convolutional layers during upsampling to progressively refine the trajectory. The final output is the predicted noise $\\hat{\\epsilon} \\in \\mathbb{R}^{T_{fut} \\times D}$, which is iteratively removed to recover the denoised trajectory."}, {"title": "7. Implementation Details", "content": "In addition to the implementation details provided in the main text, we present a more specific breakdown of the hyperparameters and configurations used in FollowGen, as listed in Table 4."}, {"title": "8. Additional Results", "content": "While Fig. 3 in the main text visualizes"}]}