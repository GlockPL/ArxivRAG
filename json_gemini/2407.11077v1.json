{"title": "Deep Reinforcement Learning with Symmetric Data Augmentation Applied for Aircraft Lateral Attitude Tracking Control", "authors": ["Yifei Li", "Erik-Jan van Kampen"], "abstract": "Symmetry is an essential property in some dynamical systems that can be exploited for state transition prediction and control policy optimization. This paper develops two symmetry-integrated Reinforcement Learning (RL) algorithms based on standard Deep Deterministic Policy Gradient (DDPG), which leverage environment symmetry to augment explored transition samples of a Markov Decision Process(MDP). The firstly developed algorithm is named as Deep Deterministic Policy Gradient with Symmetric Data Augmentation (DDPG- SDA), which enriches dataset of standard DDPG algorithm by symmetric data augmentation method under symmetry assumption of a dynamical system. To further improve sample utilization efficiency, the second developed RL algorithm incorporates one extra critic network, which is independently trained with augmented dataset. A two-step approximate policy iteration method is proposed to integrate training for two critic networks and one actor network. The resulting RL algorithm is named as Deep Deterministic Policy Gradient with Symmetric Critic Augmentation (DDPG-SCA). Simulation results demonstrate enhanced sample efficiency and tracking performance of developed two RL algorithms in aircraft lateral tracking control task.", "sections": [{"title": "I. INTRODUCTION", "content": "Symmetry property commonly exists in the motions of various mechanical systems, such as aircrafts [1], cars[2] and robotic arms[3]. The common cognition for symmetry property is that the trajectories are symmetric with respect to a reference plane. To be more specific, the knowledge of the trajectory in one symmetric side is predicable according to the knowledge of the trajectory in the other side. A simple case of cart-pole motion with symmetry is seen in Ref [4].\nMathematical models for a dynamical system are mainly categorized as Ordinary Differential Equation(ODE) and Markov Decision Process(MDP). ODE describes state transition by first-derivative equations of state variables, while MDP describes state transition by multi-step transitional state samples. Furthermore, MDP evaluates the state transition samples with a reward function, and state trajectories with a state/state-action value function. Symmetry in ODE-based dynamical system is studies in various methods. Ref[5] uses Lie group operator to calculate the transformation function, which assists to find the symmetric solutions of original equations. Ref[6] uses Koopman theory to derive symmetry maps of a nonlinear dynamical system. In Ref[7], theoretical analysis is provided on the interplay of symmetries and stability in nonlinear dynamical systems. To consider symmetry in MDP model, Ref[8] derives symmetry from equivalence relation between states. Ref[4] derives symmetry from homomorphism, which is a more rigorously defined property of a MDP. In Ref[9], [10], group symmetry property is used to construct equivariant layers in network structure.\nThe state-of-the-art works on integrating model symmetry into Reinforcement Learning(RL) algorithms use three main approaches:(1) sample augmentation with symmetry[16], [17], [18], [19], [20], [21], [22], [23], [24] (2) loss function extension with symmetry[6], [11], [12], [13], [14], [15] and (3) network architecture modification with symmetry[12], [20], [25], [26]. Sample augmentation with symmetry is based on the assumption that symmetry holds in the whole environment to be explored by a RL agent. For a dynamical system, the state trajectory executed by an exploration policy always has a symmetric counterpart. Therefore, samples collected from explored trajectory can be used to calculate samples from symmetric trajectory without actually executing the exploration policy. For a RL agent, the number of state transition samples is doubled in this way so that sample efficiency is improved. Especially when the exploration is costly for some online-operated mechanical systems, such as aircraft. In [16], the explored state-action pairs are mirrored in Maximum aposteriori Policy Optimization(MPO) algorithm for the agent to learn faster. In [17], [18], the expert-guided detection method is used to verify symmetries by assuming the system dynamics is invariant. The detected symmetries are then used to augment explored dataset in simple experiments(Grid, Cart-pole, Acrobot), verifying a data- efficient learning of the environment transition functions. In [19], the Lie point symmetry group is used to augment samples in neural Partial Differential Equation (PDE) solver. In [20], [21], the generalization bounds for data augmentation and equivariant networks using symmetry are derived, which describes their effects on learning in a theoretical framework. In [22], [23], symmetry is combined with experience replay technique for data augmentation. In [24], the equivariance set of a MDP model is learned by using an equivariance loss in dynamical model training. The equivariance set is then used for data augmentation in offline RL. In [6], a pre-trained forward model is used for Koopman latent representation of equivariant dynamical systems, leading to a symmetry transformation for data augmentation in offline Q-learning. Loss extension with symmetry method considers a symmetry loss term in loss function of critic network or actor network in RL framework. The purpose of training becomes the agent learning a policy which provides symmetric dynamical motions. In [11], [12], the standard loss function in Proximal Policy Optimization(PPO) algorithm is extended with a symmetry loss term. The control policy trained with this extended loss function is capable of achieving behavior imitation learning in robotic locomotion task. More researches on RL with extended symmetry loss can be seen in [13], [14], [15]. The third method to integrate symmetry in RL framework is modifying network architecture. In [12], a re-constructed policy network with symmetry is implemented through combining common actions and symmetric actions. In [20], the ResNet and U-Net convolutional neural network(CNN) architectures are used to incorporate various symmetries such as rotation, uniform motion, and scaling, so that the generalization capability of deep learning models is improved. In [25], the imperfect symmetry of physical system is studied by using weight relaxation scheme in a CNN.\nIn recent years, RL algorithms are widely used for model- independent control of various flight vehicles, such as multi- rotors [26], [27], [28], [29], fixed-wing aircraft[30], [31], [32] and vertical take-off and landing (VTOL) aircraft[33]. The basic idea is training a networked flight controller with experienced flight data. The reward function is designed as tracking error of altitude and attitude angles for outer- loop and inner-loop tracking tasks, respectively. However, collecting flight data through interacting with an online- operated aircraft is costly which limits training performance of RL algorithms. To address this issue, the symmetry property inherent in fixed-wing aircraft lateral motion can be exploited for flight data augmentation and further improving RL-based flight controller performance.\nIn this paper, we consider symmetry of one-step Markov transition samples. By assuming symmetry of state-action pair in first step, a symmetry condition for next-step states is derived. Then symmetric data augmentation method is proposed to calculate symmetric state transition samples. To exploite symmetric samples in RL framework, we propose DDPG-SDA algorithm that mixes explored and augmented samples in one replay buffer, leading to a doubled size of dataset compared to standard DDPG. To improve DDPG- SDA sample utilization efficiency, we propose DDPG-SCA that uses two-step policy iteration method for separate training of two critic networks and one actor network.\nThe contributions of this paper are summarized as\n\u2022 Symmetric data augmentation method is proposed to calculate symmetric samples from explored samples, which is further integrated into RL framework to im- prove sample efficiency.\n\u2022 A two-step approximate policy iteration method is proposed to improve sample utilization efficiency, leading to DDPG-SCA algorithm.\n\u2022 To reduce training complexity, we propose 'tanh-ReLU' activation functions for two-hidden-layer actor network in DDPG-SCA algorithm.\nThe remainder of this paper is structured as follows."}, {"title": "II. FOUNDATIONS", "content": "This section presents basic formulations of a discrete- time optimal control problem for control-affine nonlinear system. Firstly, the state value function and state-action value function which evaluate states and state-action pairs are defined. Secondly, exact policy iteration is introduced to numerically solve the optimal control problem by minimizing the state-action value function. Finally, approximate policy iteration is introduced to implement exact policy iteration with neural network approximators.\nDiscrete-time Optimal Control Problem\nConsider a control-affined nonlinear dynamic system in discrete-time domain as\n$x_{t+1} = F(x_t)x_t + G(x_t)u_t, t \\in N$  (1)\nwhere $x_t \\in R^n$ is state variable, $u_t \\in R^m$ is input variable. $F(x_t) \\in R^{n \\times n},G(x_t) \\in R^{n \\times m}$ are nonlinear functions associated with state $x_t$. The subfix $t$ denotes the index of time step. $N$ represents the set of non-negative integers.\nDefine a performance index starting from an initial state $x_o$ as\n$J(x_o, u_o) = \\sum_{t=0}^{\\infty} \\gamma^t r(x_t, U_t)$ (2)\nwhere $r(x_t, u_t)$ is a reward function defined at timestep t. $\\gamma \\in [0,1]$ is discount factor. The control series ${u_t}(t=0,1,2,...)$ is constructed as a control policy for $x_o$, in the form of $u_t = h(x_t)$.\nFrom RL perspective, the state value function starting from any state $x_o \\in S$ is denoted with $V^h(x_0)$ and defined by\n$V^h(x) = \\sum_{t=0}^{\\infty} \\gamma^t r(x_t, h(x_t))$ (3)\nwhere $x^h_t$ denotes the state vector $x_t$ that follows control policy $h(x_t)$. $S \\in R^{n \\times 1}$ denotes state space of system (1).\nThe state-action value function starting from $x_o \\in S$ is denoted with $Q^h (x_o, a_o)$ and defined as\n$Q^h (x_o, a_o) = r(x_o, a_o) + \\sum_{t=1}^{\\infty} \\gamma^t r(x_t, h(x_t))$ (4)"}, {"title": "B. Exact Policy Iteration", "content": "where $Q^h (x_0, a_0)$ represents the sum of rewards from initial state vector $x_0$ to the end of system motion, by taking action $a_0$ at $x_0$ and following a policy $u_t = h(x_t)(t = 1, 2,\u2026)$.\nEq.(3) can be rewritten in a recursive form as\n$V^h(x_o) = r(x_o, u_o) + \\gamma V^h (x_1)$ (5)\nwhich is the Bellman equation for function $V^h(x_o)$.\nEq.(4) can be rewritten in a recursive form as\n$Q^h(x_o, a_o) = r(x_o, a_o) + \\gamma V^h (x_1)$ (6)\nwhich is the Bellman equation for function $Q^h(x_0, a_0)$.\nFor a deterministic control policy, the following equation holds:\n$V^h (x_o) = Q^h (x_o, h(x_o))$ (7)\nSubstituting (7) into (6):\n$Q^h (x_o, a_o) = r(x_o, a_o) + \\gamma Q^h(x_1, h(x_1))$ (8)\nThe optimal control policy that minimizes state value function function $V^h(x_0)$ is denoted with $u^*_t = h^*(x_t)(t = 0,1,2,...)$ and defined by\n$h^*(x_t) \\in argmin [V^h (x_o)]$ (9)\nSubstituting Eq.(9) into Eq.(5):\n$V^{h^*} (x_o) = min [r(x_o, u_o) + \\gamma V^h (x_1)]$ (10)\nThe optimal control policy that minimizes $V^h(x_{t+1})$ is denoted with $u^*(x_{t+1}) = h^*(x_{t+1})(t = 0,1,2\u2026\u2026)$ and defined by:\n$h^*(x_{t+1}) \\in argmin [V^h(x_{t+1})]$ (11)\nThe optimal state-action value function associated with state $x_0$ and action $a_0$ is defined by\n$[Q^h (x_o, a_o)]^* : = [Q^* (x_o, a_o)]$\n$ = argmin [r(x_o, a_o) + \\gamma V^h (x_1)]$\n$ = r(x_o, a_o) + \\gamma V^{h^*} (x_1)$ (12)\nwhich is obtained by taking action $a_0$ at $x_0$ and following optimal control policy $u^*_{t+1} = h^*(x_{t+1})$\nThe optimal action $a_0^*$ at state $x_0$ is defined to be the action that minimizes $Q^h (x_o, a_o)$:\n$a^* \\in argmin{Q^h(x_o, a_o)}$\n$ = argmin{r(x_o, a_o) + \\gamma V^h (x_1)}$\n$ = argmin{r(x_o, a_o) + \\gamma Q^h (x_1, h(x_1))}$ (13)\nSubstitute Eq.(13) into Eq.(6):\n$Q^h (x_o, a_o) = min [r(x_o, a_o) + \\gamma Q^h (x_1, h(x_1)]$ (14)"}, {"title": "B. Exact Policy Iteration", "content": "In order to solve $a^*$ in Eq.(14), an iterative method named exact policy iteration is introduced, which recurrently executes policy evaluation and policy improvement steps as\nPolicy Evaluation\n$(Q^h)^{i+1}(x_t, a_t) = r(x_t, a_t) + \\gamma (Q^h) [x_{t+1}, h^i(x_{t+1})]$ (15)\nPolicy Improvement\n$h^{i+1}(x_t) = arg min [(Q^h)^{i+1}(x_t, h(x_t))]$ (16)\nwhere index $i$ denotes $i$th policy iteration. The initial function $(Q^h)^0$ uses result of $(i - 1)$ th iteration, i.e. $(Q^h)^i(.) =(Q^h)^{i-1}(\u00b7)$. The initial function $h^i(\u00b7)$ uses result of $(i - 1)$ th iteration, i.e. $h^i(\u00b7) = h^{i-1}(.)$"}, {"title": "C. Approximate Policy Iteration", "content": "From a practical perspective, the structures of functions $Q^h (x_t, a_t)$, $h(x_t)$ are usually unknown so that Eqs.(15)(16) are difficult to be solved in each iteration. To address this issues, nonlinear approximators such as neural network are used to approximate functions $Q^h(x_t, a_t)$, $h(x_t)$. The policy iteration with approximators is named as approximate policy iteration.\nSpecifically, two neural networks are used in approximate policy iteration: (1) critic network $Q^h_\\psi (x_t, a_t)$ with parameter set $\\psi$ for state-action value function $Q^h (x_t, a_t)$; (2) actor network $\\mu_v(x_t)$ with parameter set $v$ for policy $h(x_t)$.\nThen Eqs.(15) (16) are reconstructed as\nApproximate Policy Evaluation\n$(Q_\\psi)^{i+1}(x_t, a_t) = r(x_t, a_t) + \\gamma (Q_\\psi)^i [x_{t+1}, (\\mu_v)^i(x_{t+1})]$ (17)\nApproximate Policy Improvement\n$(\\mu_v)^{i+1} (x_t) = arg min [(Q_\\psi)^{i+1}(x_t, \\mu_v(x_t))]$ (18)\nIn order to execute step (17) and step (18) recurrently by iteration index $i$, gradient descent method is used to update parameter sets $\\psi, v$ progressively."}, {"title": "III. SYMMETRIC DYNAMICAL MODEL", "content": "This section discusses the symmetry property of dynamical system in Eq.(1). By assumption of Markov property, the symmetry of one-step state transition pairs in system (1) is firstly defined. Then the symmetry is extendedly defined for overall system. Moreover, symmetric reward function and symmetric state-action (Q) value function are defined.\nSymmetric Dynamical System\nThe Markov property of system in Eq.(1) is assumed to hold and described as: the state transition only depends on current state, instead of historical state. Then a state transition sample is denoted with $(x_t, a_t, x_{t+1})$, i.e. $x_t$ transfers to $x_{t+1}$ by taking action $a_t$.\nDefinition 1(Symmetric one-step state transitions) For two"}, {"title": "A. Symmetric Data Augmentation", "content": "state transition samples denoted with $(x_t, a_t, x_{t+1})$ and $(x'_t, a'_t, x'_{t+1})$ in one time step $t$, and a reference point $x = x^*$, they are symmetric to $x^*$ when\n$\\frac{X_t + X'_t}{2} = x^* $ (19)\n$a_t = -a'_t$ (20)\n$\\frac{X_{t+1} + X'_{t+1}}{2} = x^*$ (21)\nEqs.(19), (20) are usually assumed to hold, but Eq.(21) holds conditionally. In order to discuss the condition for Eq.(21), the following theorem is provided.\nTheorem 1. (Symmetry of $X_{t+1}$)\nFor a discrete-time system model (1), two state transition samples $(x_t, a_t, x_{t+1})$, $(x'_t, a'_t, x'_{t+1})$ and a reference point $x = x^*$ are selected. By assuming Eqs.(19), (20) hold, $x^*$ is a symmetric point for $x_{t+1}, x'_{t+1}$ when following conditions hold:\n(1) $x^* = 0, G(x_t) = G(x'_t), F(x_t) = F(x'_t)$\n(2) $x^* \\neq 0, G(x_t) = G(x'_t), F(x_t) = F(x'_t) = 1$\nThe poof is seen in Appendix A.\nDefinition 2. (Symmetric dynamical system)\nThe discrete-time dynamical system in Eq.(1) is a symmetric dynamical system with respect to a reference point $x = x^*$, when the following equations hold:\n$\\frac{X_t + X'_t}{2} = x^*,  \\forall x_t, x'_{t+1} \\in S $\n$a_t = -a'_t, \\forall a_t, a'_t \\in A $ (22)\n$\\frac{X_{t+1} + X'_{t+1}}{2} = x^*, \\forall x_t, x'_{t+1} \\in S $\nwhere $S \\in R^{n \\times 1}, A \\in R^{m \\times 1}$ are state space and action space of dynamical system (1). Figure 1 illustrates symmetry relation of state-action set $S \\times A$ in a dynamical system.\nDefinition 3. (Symmetric reward function)\nFor reward function $r(x_t, u_t)$ defined in Eq.(2), it is symmetric with respect to state-action pairs $(x_t, a_t)$ and $(x'_t, a'_t)$ in state-action space $S \\times A$ when\n$r(x_t, a_t) = r(x'_t, a'_t)$ (23)\nDefinition 4. (Symmetric state-action value function)\nFor state-action value function $Q^h (x_t, a_t)$ defined in Eq.(4), it is symmetric with respect to state-action pairs $(x_k, a_t)$ and $(x'_t, a'_t)$ in state-action space $S \\times A$, when\n$Q^h (x_t, a_t) = Q^h (x'_t, a'_t)$ (24)"}, {"title": "IV. DDPG WITH SYMMETRIC DATA AUGMENATION", "content": "This section presents a symmetry-integrated RL algo- rithm, namely DDPG-SDA. The symmetric data augmenta- tion (SDA) algorithm is firstly developed based on symmetry assumptions defined in Section III. Then symmetric data augmentation is combined with standard DDPG algorithm, leading to DDPG-SDA."}, {"title": "A. Symmetric Data Augmentation", "content": "The symmetric data augmentation method is implemented by Eqs.(19)(20)(21)(23), and summarized as\n$S'_t = A s_t + Bx^*$ (25)\nwhere $s_t= [X_t, a_t, X_{t+1},r_t]^T$ is explored sample, $s'_t = [X'_t, a'_t, X'_{t+1},r'_t]^T$ is augmented sample. The matrices A and B are given as\n$A = \\begin{bmatrix}\n-1 & 0 & 0 & 0 \\\\\n0 & -1 & 0 & 0 \\\\\n0 & 0 & -1 & 0 \\\\\n0 & 0 & 0 & -1\n\\end{bmatrix}$, $B = \\begin{bmatrix}\n2 \\\\\n0 \\\\\n2 \\\\\n0\n\\end{bmatrix}$ (26)"}, {"title": "B. DDPG with Symmetric Data Augmentation", "content": "The standard Deep Deterministic Policy Gradient (DDPG) is an off-policy RL algorithm for progressive optimization of a critic network to approximate Q value function, and a de- terministic actor network to approximate policy function[34].\nFor a DDPG agent, explored samples St are stored in a replay buffer denoted by $D$, i.e. $s_t \\in D$. Based on this, we propose to store augmented samples $s'_t$ by Eq.(25) in same replay buffer, i.e.\n$S_t \\in D, S'_t \\in D$\nThen an enriched dataset is available for training critic and actor networks. When one training step is executed, a batch of mixed state transitions with $s_t$ and $s'_t$ are sampled to calculate losses for critic/actor networks. The losses are fur- ther used to update critic/actor network weights by gradient descent algorithm.\nDespite of unchanged RL algorithm framework, we name the integrated learning algorithm that enriches dataset with symmetric data augmentation for standard DDPG as DDPG- SDA, i.e. deep deterministic policy gradient with symmetric data augmentation (see pseudo code in Algorithm 1)."}, {"title": "V. DDPG WITH SYMMETRIC CRITIC AUGMENATION", "content": "This section presents the second symmetry-integrated RL algorithm, namely DDPG-SCA. Firstly, performance analysis is provided for previous DDPG-SDA. Two modifications for RL algorithm framework are proposed to exploit augmented samples, i.e. piecewise Q value function approximation and two-step approximate policy iteration. Finally, the innovative RL algorithm with modified framework (DDPG-SCA) is developed and compared with previous two RL algorithms."}, {"title": "A. Drawback of DDPG-SDA", "content": "DDPG-SDA algorithm makes use of augmented samples by mixing them with explored samples in training procedure. However, this mixed sample batch slows down DDPG-SDA agent learning in explored dataset $(S\\times A)_{exp}$ because of less explored samples in one sample batch. Increasing sample batchsize is one solution to accelerate learning but a large batchsize may conversely degrade learning performance."}, {"title": "B. Q Value Function Approximation with Two Critics", "content": "In order to efficiently exploit enlarged dataset for training critic/actor networks, we propose to modify RL framework so that it enables separate storage and utilization of explored and augmented samples.\nFor this purpose, two individual replay buffers denoted by $D_1, D_2$ are used for one RL agent, i.e.\n$S_t \\in D_1, S'_t \\in D_2$\nIn one training step, two critic networks are trained according to samples from buffers $D_1$ and $D_2$, respectively. Therefore, the actual Q value function is approximated by two critic networks\n$Q_\\psi^h (x_t, a_t) \\approx \\begin{cases}\nQ_\\psi^1 (x_t, a_t), & \\text{if } (x_t, a_t) \\in D_1 \\\\\nQ_\\psi^2 (x_t, a_t), & \\text{if } (x_t, a_t) \\in D_2\n\\end{cases}$ (27)"}, {"title": "C. Two-step Approximate Policy Iteration", "content": "The alternate training for two critic and one actor networks is achieved by two-step approximate policy iteration method, i.e.\nStep 1 (Approximate policy iteration with samples in $D_1$)\n$(Q_1)^{i+1}(x_t, a_t) = r(x_t, a_t) + \\gamma (Q_1)^i [x_{t+1}, (\\mu)^i(x_{t+1})]$ (28)\n$(\\mu)^{i+1}(x_t) = arg min [(Q_1)^{i+1}(x_t, \\mu(x_t))]$ (29)\nStep 2 (Approximate policy iteration with samples in $D_2$)\n$(Q_2)^{i+1}(x_t, a_t) = r(x_t, a_t) + \\gamma (Q_2)^i [x_{t+1}, (\\mu)^{i+1}(x_{t+1})]$ (30)\n$(\\mu)^{i+2}(x_t) = arg min [(Q_2)^{i+1}(x_t, \\mu(x_t))]$ (31)\nIn step 1, critic and actor networks $Q_\\psi^1 (x_t, a_t)$, $\\mu_v(x_t)$ are updated by approximate policy iteration in Section II.C. with samples from $D_1$. In step 2, critic and actor networks $Q_\\psi^2 (x_t, a_t)$, $\\mu_v(x_t)$ are updated by same iteration method with samples from $D_2$. As a result, critic networks $Q_\\psi^1 (x_t, a_t)$, $Q_\\psi^2 (x_t, a_t)$ are separately trained with explored and augmented datasets, actor network $\\mu_v(x_t)$ is trained with overall dataset. Specific update procedure by gradient descent algorithm is seen in Appendix B."}, {"title": "D. DDPG with Symmetric Critic Augmentation", "content": "On the basis of standard DDPG algorithm with enlarged dataset by symmetric data augmentation, the proposed mod- ifications for RL framework in Section V.B&C. result into an innovative RL algorithm named as DDPG-SCA, i.e. deep deterministic policy gradient with symmetric critic augmen- tation. By using two critic networks, enlarged number of samples from explored dataset and augmented dataset are utilized in one training step so that training on overall dataset is accelerated. The pseudo code of DDPG-SCA is provided in algorithm 2."}, {"title": "VI. AIRCRAFT MODEL", "content": "This section introduces aircraft dynamical model in lateral plane. Firstly, a simplified linear model in continuous-time domain is presented for angular and angular rate variables. Then Euler method is used for model discretization. Lastly, Theorem 1 in Section III.A. is used to analyze symmetry property of state variables in discretized model."}, {"title": "B. Discretization", "content": "The lateral dynamical model in Eqs.(32) is discretized by \"one-order Euler method [38] with time step $\\Delta t$ as\n$\\phi_{t+1} = \\phi_t + p_t\\Delta t$\n$p_{t+1} = p_t + (L'_p p_t + L'_r r_t + L'_\\beta \\beta_t + L'_{\\delta_a} \\delta_{at} + L'_{\\delta_r} \\delta_{rt})\\Delta t$\n$\\beta_{t+1} = \\beta_t + [Y'_p p_t + Y'_r + (Y^*_r - 1)r_t + Y' _\\beta \\beta_t + Y'_{\\delta_a} \\delta_{at} + Y'_{\\delta_r} \\delta_{rt}]\\Delta t$\n$r_{t+1} = r_t + [N'_p p_t + N'_r r_t + N'_\\beta \\beta_t + N'_{\\delta_a} \\delta_{at} + N'_{\\delta_r} \\delta_{rt}]\\Delta t$ (33)\nEqs.(33) is rewritten in state-space form as\n$X_{t+1} = F(x_t)x_t + G(x_t)u_t$ (34)\nwhere $x_t = [\\phi_t, p_t, \\beta_t, r_t]$, $u_t = [\\delta_{at}, \\delta_{rt}]$.\nThe coefficient matrices $F(x_t), G(x_t)$ are given as\n$F(x_t) = \\begin{bmatrix}\n1 & \\Delta t & 0 & 0 \\\\\n0 & 1 + L'_p\\Delta t & L'_\\beta \\Delta t & L'_r \\Delta t \\\\\n0 & Y'_p \\Delta t & 1 + Y'_\\beta\\Delta t & (Y^*_r - 1)\\Delta t \\\\\n0 & N'_p\\Delta t & N'_\\beta \\Delta t & 1 + N'_r\\Delta t\n\\end{bmatrix}$ $G(x_t) = \\begin{bmatrix}\n0 & 0 \\\\\nL'_{\\delta_a}\\Delta t & L'_{\\delta_r}\\Delta t \\\\\nY'_{\\delta_a}\\Delta t & Y'_{\\delta_r}\\Delta t \\\\\nN'_{\\delta_a} \\delta_{at}\\Delta t & N'_{\\delta_r} \\delta_{rt}\\Delta t\n\\end{bmatrix}$"}, {"title": "C. Symmetry Analysis", "content": "This subsection analyzes symmetry in aircraft lateral dynamics in Eq.(34) via Theorem 1. Theorem 1 categorizes symmetry planes into two cases: $x^* = 0$ or $x^* \\neq 0$, where second case requires enhanced constraint on system matrix $F(x_t)$. Coupling effects exist in Eq.(34) among increments of states from time step t to t+1 and are included in $F(x_t)$.\nApplying assumptions in Theorem 1 on system (34), one has\n$\\frac{X_t + X'_t}{2} = x^*$\n$a_t = -a'_t$ (35)\nwhere $x_t = [\\phi_t, p_t, \\beta_t, r_t]$, $x'_t = [\\phi'_t, p'_t, \\beta'_t, r'_t]$ are state pairs symmetric to reference plane $x^* = [\\phi^*, p^*, \\beta^*, r^*]$, $a_t = [\\delta_{at}, \\delta_{rt}]$, $a' = [\\delta'_{at}, \\delta'_{rt}]$ are action pairs symmetric to 0.\nBecause $F(x_k) = F(x'_k) \\neq 0, G(x_k) = G(x')$ hold for system (34), one can conclude by case (1) in Theorem 1 that\n$\\frac{X_{t+1} + X'_{t+1}}{2} = 0$ (36)"}, {"title": "VII. SIMULATION RESULTS", "content": "This section presents simulation results of developed RL algorithms by setting aircraft lateral model as environment. Training behaviors of are firstly compared to evaluate RL agents's learning abilities. The tracking performances for untrained reference signal of RL-based lateral controllers are presented."}, {"title": "IX. APPENDIX", "content": "Proof of Theorem 1\nTheorem 1. (Symmetry of $X_{t+1"}, "nFor a discrete-time system model (1), two state transition samples $(x_t, a_t, x_{t+1})$, $(x'_t, a'_t, x'_{t+1})$ and a reference point $x = x^*$ are selected. By assuming Eqs.(19), (20) hold, $x^*$ is a symmetric point for $x_{t+1}, x'_{t+1}$ when following conditions hold:\n(1) $x^* = 0, G(x_t) = G(x'_t), F(x_t) = F(x'_t)$\n(2) $x^* \\neq 0, G(x_t) = G(x'_t), F(x_t) = F(x'_t) = 1$.\nProof. Assume states $x_t, x'_t$ are symmetric to the reference $x = x^*$, i.e.\n$\\frac{(X_t + X'_t)}{2} = x^*$ (38)\nand at, are symmetric to 0, i.e.\n$a_t = -a'_t$ (39)\nThe next-step state $x_{t+1}$ by taking actions at at xt is derived by system model (1) as\n$x_{t+1} = F(x_t)x_t + G(x_t)a_t$ (40)\nThe next-step state $x'_{t+1}$ by taking actions at at x't is derived by system model (1) as"]}