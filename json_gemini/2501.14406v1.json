{"title": "Adaptive Rank Allocation for Federated Parameter-Efficient Fine-Tuning of Language Models", "authors": ["Fei Wu", "Jia Hu", "Geyong Min", "Shiqiang Wang"], "abstract": "Pre-trained Language Models (PLMs) have demonstrated their superiority and versatility in modern Natural Language Processing (NLP), effectively adapting to various downstream tasks through further fine-tuning. Federated Parameter-Efficient Fine-Tuning (FedPEFT) has emerged as a promising solution to address privacy and efficiency challenges in distributed training for PLMs on mobile devices. However, our measurements reveal two key limitations of FedPEFT: heterogeneous data leads to significant performance degradation, and a fixed parameter configuration results in communication inefficiency. To overcome these limitations, we propose FedARA, a novel Federated Adaptive Rank Allocation for parameter-efficient fine-tuning of language models. Specifically, FedARA employs truncated singular value decomposition (SVD) adaptation to enhance flexibility and expressiveness, significantly mitigating the adverse effects of data heterogeneity. Subsequently, it utilizes dynamic rank allocation to progressively identify critical ranks, effectively improving communication efficiency. Lastly, it leverages rank-based module pruning to remove inactive modules, steadily reducing local training time and peak memory usage in each round. Extensive experiments show that FedARA consistently outperforms weak baselines by an average of 8.49% and strong baselines by 6.95% across various datasets under data heterogeneity while significantly improving communication efficiency by 2.40x. Moreover, experiments on AGX Orin, Orin Nano and Raspberry Pi 5 devices demonstrate substantial decreases in total training time and energy consumption by up to 48.90% and 46.95%, respectively.", "sections": [{"title": "I. INTRODUCTION", "content": "Pre-trained Language Models (PLMs) [1]\u2013[3] have demonstrated extraordinary performance and foundational capabilities in modern Natural Language Processing (NLP). Fine-tuning further enhances their performance on downstream tasks in mobile computing, such as sequence classification, question answering, and causal language modeling. With advances in hardware performance and engineering optimizations [4], executing fine-tuning and inference on mobile devices is becoming increasingly attainable [5].\nPre-training and fine-tuning are evolving into two distinct yet mutually reinforcing stages in NLP. 1) Pre-training: it involves self-supervised training on large-scale corpora from sources like books, news articles, and Wikipedia [6], aiming to learn general representations of contextual language and create a generalizable pre-trained model. 2) Fine-tuning: it extends the pre-trained model to specific downstream tasks through supervised training, optimizing its performance to meet domain requirements. Overall, these two stages greatly enhance the model's generality and flexibility across a wide range of NLP tasks.\nAlthough PLMs have demonstrated superior performance [7], they still require fine-tuning with local data to achieve optimal results in specific domains. This data is typically generated by users, and directly collecting such sensitive and private data poses a significant risk of privacy leakage. To address this issue, federated learning (FL) is considered an effective approach [8], as it protects data privacy by keeping raw data on local devices, ensuring compliance with regulations such as GDPR [9] and the Data Protection Act 2018 [10]. The FL process involves a cloud server orchestrating the aggregation of local parameter updates from individual clients, repeating this cycle until training convergence.\nHowever, the massive scale of PLMs, with parameters often reaching millions or even billions [11], results in significant local computation and communication costs during full fine-tuning in FL, making deployment on mobile devices nearly impossible. Leveraging the success of parameter-efficient fine-tuning (PEFT) methods in centralized training, where only a few low-rank parameters are trained using matrix decomposition while the pre-trained model remains frozen [12]\u2013[15], the number of trainable parameters is typically reduced to less than 1% of the original model. Furthermore, FL addresses PEFT's limitations by fully leveraging distributed data resources and enhancing learning quality in a privacy-preserving manner. Consequently, Federated PEFT (FedPEFT) demonstrates its significant advancements and applications across various domains [16]\u2013[23]. Building on this progress, FedPEFT is increasingly recognized as a key method for distributed fine-tuning on mobile devices [24], [25]. The complete FedPEFT training process is shown in Figure 1.\nNevertheless, FedPEFT faces two major challenges when applied to mobile devices. 1) FedPEFT exacerbates the impact of non-independent and identically distributed (non-IID) data on model performance. Due to the diverse preferences of mobile device users, data on these devices is typically non-IID. While performance degradation caused by non-IID data is a well-known challenge in FL, our preliminary measurements show that FedPEFT exacerbates this issue significantly. Specifically, on the 20News [26] dataset with the DistilBERT [3] model, different FedPEFT methods under non-IID data result in an average accuracy drop of 14.62%"}, {"title": "II. BACKGROUND AND MOTIVATIONS", "content": "A. Federated Parameter-Efficient Fine-Tuning (FedPEFT)\nFedPEFT orchestrates PEFT strategies within the FL framework to efficiently train local models, drastically reducing communication overhead by only transmitting a small number of PEFT modules. These modules can be Adapters [12], [13], Prompt Tuning [14], or LoRA [15]. Among them, LoRA-based methods are especially popular because they introduce no latency during inference. FedPEFT not only preserves privacy but also effectively aggregates diverse knowledge across clients, facilitating the collaborative development of a high-quality global model. By harnessing the advantages of both PEFT and FL, FedPEFT has become an essential technique for deploying fine-tuning on mobile devices [20]\u2013[22], [24], [25].\nAs illustrated in Figure 1, FedPEFT training process unfolds as follows: Step 1: A cloud server selects a subset of clients to broadcast the initial parameters. Step 2: On these clients, the weights of the pre-trained model are frozen, and trainable PEFT modules are inserted for local training. Step 3: Only these PEFT modules are updated to the cloud server from each client. Step 4: The cloud server aggregates these updates from different clients. This cycle (Steps 1-4) is repeated for multiple rounds in FL until the model converges.\nB. Preliminary Measurements\nWe conducted a series of preliminary measurements to explain the motivations behind the design of FedARA.\nObservation-1: FedPEFT outperforms FedFFT on the advanced embedded devices. As shown in AdaFL [24], FedPEFT achieves comparable accuracy to federated full fine-tuning (FedFFT) while significantly reducing local training time by 1.6x, communication cost by 200\u00d7, and memory usage by 1.5\u00d7 on previous-generation embedded devices like NVIDIA Jetson TX2, Nano, and Raspberry Pi 4B. However, its impact on overall training time on advanced devices remains underexplored. FedPEFT comprises two main methods: FedAdapter [24] and FedLoRA [16]\u2013[22]. LoRA-based methods have gradually replaced adapter-based methods, such as AdaFL [24], to avoid additional inference latency. To fill this gap, we evaluated FedLoRA on the 20News dataset using the advanced embedded devices. Following FedPEFT'S typical settings [24], we set the the communication bandwidth to 1MB/s. As depicted in Figure 2a, the results indicate that FedFFT's overall training time is 42x, 40\u00d7, and 18\u00d7 longer than FedLoRA on AGX Orin, Orin Nano, and RPi 5, respectively.\nObservation-2: The performance of FedPEFT significantly decreases under non-IID data. Figure 2b illustrates the accuracy of different FedPEFT methods using the DistilBERT model for both IID (white bars) and non-IID data (black bars). The non-IID division follows the FedAvg setting [8], with a severe class imbalance in data distribution. The parameter sizes for Adapter-h (A-h) and Adapter-p (A-p) are set to match those of the LORA method. Our measurements indicate a significant performance drop in all FedPEFT methods under non-IID conditions compared to IID conditions, with average accuracy reductions of 14.62% and 12.81% on the 20News and News Category datasets, respectively. Existing research lacks effective strategies to mitigate the adverse impact of non-IID data on FedPEFT performance.\nObservation-3: The contribution of low-rank modules to accuracy varies by their inserted positions. In FedPEFT, low-rank modules can be placed at various layers and components, such as Query (Q), Key (K), Value (V), Out (O), F1, and F2. Here, O denotes the linear layer in the multi-head attention block, while Fl and F2 represent the two linear layers of the feed-forward network. As shown in Figure 2c, we allocated a fixed rank budget to specific components or layers to evaluate their impact on accuracy. The left sub-figure shows the allocation of the entire rank budget to a single layer, while the right sub-figure illustrates its distribution across a single component in all layers. The findings reveal that deeper layers near the output often have a more significant effect than shallower ones, with components F1 and F2 typically exerting a greater influence than component O. Moreover, the impact of these inserted positions varies across datasets, suggesting that there is no one-size-fits-all configuration. Fixed-rank budget"}, {"title": "III. OVERVIEW OF FEDARA", "content": "Due to LoRA's limitations in handling complex data heterogeneity and the communication inefficiency of the fixed rank configuration, we propose FedARA, an adaptive rank allocation framework for FedPEFT of language models. Our approach introduces a truncated SVD structure to better address data heterogeneity in federated settings. Additionally, we incorporate a dynamic rank allocation algorithm to enhance communication efficiency and employ rank-based module pruning to reduce computational time and peak memory usage per round in clients.\nThe system architecture of the FedARA framework, shown in Figure 3, includes a server and multiple clients. Clients utilize the truncated SVD method to train local models and generate local masks, while the server aggregates the global model and arbitrates global masks. The ranks of the global model are dynamically allocated through the arbitrated global masks. Communication between the server and clients involves transmitting masks and progressively pruned parameters. Local clients detect rank changes in each round to remove inactive SVD modules safely.\nAlgorithm 1 outlines the process of federated adaptive rank allocation. It starts with parameter initialization and client selection (line 5-7). The server prunes parameters using global rank masks (lines 8-9) and broadcasts them to the clients (line 10). Clients perform parallel local training (lines 11-12), updating parameters based on local datasets (lines 24). Local rank masks are generated from updated parameters (lines 25-"}, {"title": "IV. DESIGN DETAILS OF FEDARA", "content": "In this section, we provide a detailed description of the design of FedARA with the three closely correlated methods, including truncated SVD adaptation, dynamic rank allocation, and rank-based module pruning:\nA. Truncated SVD Adaptation\nIn previous work, LoRA [15] decomposes the trainable weight matrix into pre-trained weights and incremental weights as:\n$W = W_{pre} + \\Delta W = W_{pre} + BA$  (1)\nwhere $W_{pre}, \\Delta W \\in \\mathbb{R}^{d_{out}\\times d_{in}}$, $B \\in \\mathbb{R}^{d_{out}\\times r}$, and $A \\in \\mathbb{R}^{r\\times d_{in}}$ with $r < \\{d_{out},d_{in}\\}$. Pre-trained weights $W_{pre}$ are frozen and only the low-rank matrices B and A are fine-tuned. LoRA improves parameter efficiency compared to full fine-tuning when $r(d_{out} + d_{in}) < d_{out}d_{in}$. The matrix A is initialized with random Gaussian values, and B is set to zero, ensuring $\\Delta W = 0$ at the beginning.\nHowever, our pre-experiments and existing studies [19], [20] reveal a significant performance gap of FedLoRA between IID and non-IID settings. We hypothesize that the inherent structural constraints of LoRA make it struggle to capture and represent global similar features from heterogeneous client data. Therefore, we consider enhancing the structure of LoRA to improve its expressiveness.\nDORA [30] provide valuable insights, which decompose pre-trained weights into their magnitude and directional components to enhance expressiveness, but their directional normalization and additional magnitude parameters introduce significant computational and memory overhead. This makes them unsuitable for resource-constrained mobile computing in FL.\nInspired by truncated Singular Value Decomposition (SVD), which decomposes a matrix into the eigenvalues and eigenvectors of its low-rank subspace, we propose truncated SVD adaptation, formulated as\n$W = W_{pre} + \\Delta W = W_{pre} + BEA,$ (2)\nwhere $E \\in \\mathbb{R}^{r\\times r}$ is a diagonal matrix, and $B\\in \\mathbb{R}^{d_{out}\\times r}$, $A\\in \\mathbb{R}^{r\\times d_{in}}$. Matrices A and B are initialized with random Gaussian values, while E is set to zero, ensuring $\\Delta W = 0$ at the start of training.\nAs illustrated in Figure 4, an additional diagonal matrix E is introduced between the low-rank subspace at a negligible cost ($r < d$). This E matrix enhances expressiveness and flexibility in addressing data heterogeneity by enabling independent magnitude updates. Moreover the inherent structure of SVD facilitates the effective separation of crucial information from noise across different clients in non-IID settings. Unlike the original truncated SVD, we do not enforce strict orthogonality constraints on B and A, as such constraints limit adaptability and introduce additional overhead in complex and resource-constrained FL scenarios.\nInterestingly, we found that when E is set to the identity matrix, truncated SVD degenerates into the standard LoRA. This unifies truncated SVD and LoRA adaptation under the same umbrella, highlighting the greater generality of truncated SVD adaptation.\nNext, we analyze LoRA and truncated SVD adaptation from the perspective of the generalization bound [28]. Let $W = \\{W_i\\}_{i=1}^L$ denote the $L$ parameter matrices of a pre-trained model. Define a subset $\\mathcal{I} \\subseteq \\{1, 2, . . . , L\\}$ as the indices of parameter matrices to be fine-tuned. Assume that $\\ell_{W,b}(\\Delta W, Z)$ is $\\sigma$-sub-Gaussian under $(\\Delta W, Z) \\sim P_{\\Delta W \\vert W,b} \\times \\mu$. Then, for LoRA (adapting A and B):\n$\\vert gen(\\mu, ABA) \\vert \\leq \\sqrt{\\frac{2q\\sigma^2 ln 2}{n}} \\sqrt{\\sum_{i \\in \\mathcal{I}} (rd^{(i)}_{in}+rd^{(i)}_{out})},$(3)\nand for truncated SVD adaptation (adapting B, E, and A):\n$\\vert gen(\\mu, ABEA) \\vert \\leq \\sqrt{\\frac{2q\\sigma^2 ln 2}{n}} \\sqrt{\\sum_{i \\in \\mathcal{I}} (rd^{(i)}_{in}+rd^{(i)}_{out} + r^2)},$(4)\nhere, $n$ denotes the training set size, and $q$ reflects the number of bits used to encode each dimension in tunable parameters. Due to the low-rank dimension $r < \\text{min}(d_{in}, d_{out})$, truncated SVD achieves almost the same generalization bound to LoRA. However, it enhances expressiveness through the additional matrix E, providing greater potential to capture and represent similar features across different clients in FL.\nTo further measure the impact of structures between LoRA and truncated SVD adaptation in FL, we define the discrepancy between the global model $\\Theta_{\\text{global}}$ and the local models $\\Theta_{\\text{local}}$ across selected clients during FL rounds as:\n$\\text{Disc} = \\frac{1}{S} \\sum_{i=1}^S\\Vert \\Theta_{\\text{global}} - \\Theta_{\\text{local}}^i \\Vert_2^2,$(5)\nwhere S is the number of selected clients per round, and $\\Vert . \\Vert_2$ denotes the Euclidean norm."}, {"title": "B. Dynamic Rank Allocation", "content": "The fixed rank configuration assigns uniform ranks to modules across different layers and components, resulting in communication inefficiency due to its inability to adapt to the varying contributions of layers and components to model performance. This inefficiency manifests in two aspects: fixed rank allocation yields suboptimal performance under similar communication costs and requires higher costs to achieve comparable performance.\nInspired by AdaLoRA [27], which introduces a framework for centralized rank assignment, we propose a dynamic rank allocation method in FL that efficiently utilizes limited parameters to prioritize the most critical weights, thereby enhancing communication efficiency. This method progressively identifies the most significant weights while pruning less important parameters across clients, involving three key steps: Generation of Local Rank Masks, Arbitration of Global Rank Masks, and Communication based on Masks.\n1) Generation of Local Rank Masks: During federated fine-tuning, the generation of local rank masks is closely related to rank budgets, which is the total sum of ranks across all layers and components. This budget is progressively reduced, eventually approaching the pre-set average target rank. The relationship between the budget $b(t)$ and the FL round $t$ is given by:\n$b^{(t)} = \\begin{cases}\nb^{(0)}, & 0 < t < t_w, \\\\\nb^{(T)} + (b^{(0)} - b^{(T)}) \\cdot (1 - \\frac{t-t_w}{T - t_w -t_f})^3, & t_w \\leq t < T - t_f, \\\\\nb^{(T)}, & \\text{otherwise}.\\end{cases}$ (6)\nwhere $b^{(t)} = b^{(0)} - b^{(R)}$, $t_w$ represents the warm-up rounds, $t_f$ indicates the final stabilized rounds, T denotes the total rounds.\nLocal clients identify relatively important rank candidates based on the reduced rank budget. The importance of each rank is evaluated using triplets, as shown in Figure 6, which consist of the diagonal value in E, the left vector in B, and its corresponding right vector transpose in A. This can be represented as:\n$\\mathcal{I}_{n,i} = \\mathcal{I}(E_{n,i}) + \\frac{1}{d_1} \\sum_{j=1}^{d_1} \\mathcal{I}(B_{n,j,i}) + \\frac{1}{d_2} \\sum_{j=1}^{d_2} \\mathcal{I} (A_{n,i,j}),$ (7)\nwhere $\\mathcal{I}(E_{n,i})$, $\\mathcal{I}(B_{n,j,i})$, and $\\mathcal{I} (A_{n,i,j})$ represent the importance scores of the corresponding parts. To avoid the overhead of sensitivity and gradient computation in AdaLoRA [27], the importance score $\\mathcal{I}(w_{i,j})$ in our design is directly represented in a simple yet effective manner as $\\mathcal{I}(W_{i,j}) = \\vert W_{i,j}\\vert$. As illustrated in Figure 6, each SVD module initially contains r triplets, which can be inserted into six different components.\nThe local rank mask of each client is generated based on the sorting of all triplets. Specifically, if the importance of a rank falls within the top-b(t), it is marked as True in the mask; otherwise, it is marked as False. Each client uploads these masks to the server along with the parameters.\n2) Arbitration of Global Rank Masks: After collecting all candidates of the local rank masks from all selected clients, the server determines the global rank masks by arbitrating the fraction of clients reporting True at each position against a preset threshold Th. This arbitration process is given by:\n$M^{(i)}_{global} = \\begin{cases} \\text{True}, & \\text{if } \\frac{1}{|S|}\\sum_{k \\in S} M_k^{(i)} > T_h \\\\ \\text{False}, & \\text{otherwise}.\\end{cases}$(8)"}, {"title": "C. Rank-based Module Pruning", "content": "Although the dynamic parameter allocation method based on truncated SVD modules reduces communication overhead in FL by limiting the upload and download of less critical ranks, it does not decrease the number of trainable parameters during local training. This limitation arises because, in frameworks like PyTorch [31] and Transformers [32], trainable parameters are managed at the matrix level. To address this, we introduce a rank-based module pruning mechanism that adjusts the number of trainable parameters.\nSpecifically, each SVD module on the local client has a rank parameter that changes over time according to local training and the ranking results of triplets. Dynamic rank allocation ensures that the ranks of SVD modules either remain constant or gradually decrease as FL training progresses. If certain ranks within an SVD module prove more critical than others, they are maintained; otherwise, less critical ranks are gradually pruned. Eventually, some ranks may be reduced to a pre-set threshold (set to zero in this work), indicating their low importance and enabling their safe removal.\nThus, FedARA involves continuously monitoring the rank changes of each inserted SVD module in every training round. If the rank of a module falls below the pre-set threshold, all three matrices of the SVD module are changed to non-trainable. This process has no impact on the model's performance, as these inactive ranks are excluded from the communication and aggregation steps. This method accelerates the training process and minimizes memory usage by avoiding unnecessary training of specific SVD modules in local clients, making it particularly advantageous for deployment on mobile devices."}, {"title": "V. EXPERIMENTAL SETUPS", "content": "FedARA extends AdaLoRA [27], the SOTA dynamic parameter configuration method in centralized training, to an FL framework. We utilize PyTorch [31] and Hugging Face's Transformers library [32], which support a wide range of pre-trained NLP models. FedARA uses a random client selection mechanism, which can be replaced with more sophisticated strategies [33], [34]. Communication is synchronized between clients and the cloud server using the weighted average method [8], ensuring compatibility with various optimizers and aggregation techniques [35]\u2013[38]. Additionally, FedARA supports advanced model quantization methods to further reduce communication and computation overhead [39], [40].\nMetrics The primary evaluation metric is the final accuracy after a fixed number of FL rounds, ensuring consistent performance comparison. Secondary metrics include communication overhead, adaptive rank allocation results, total training time, memory usage, and energy consumption on embedded devices. These metrics provide a comprehensive assessment of the system's efficiency and effectiveness.\nHardware Following prior FL research [24], our experiments are conducted through emulation on a laptop with RTX 4070 GPU. The local training time is measured on three embedded devices with hardware specifications similar to common mobile devices, AGX Orin, Orin Nano, and Raspberry Pi 5. These training results were fed into our emulation framework to determine total training time in a homogeneous FL setting. In line with the existing works [24], we maintain a typical communication bandwidth of 1MB/s between server and clients. This setting accounts for a wide range of environments, from IoT to household WiFi and cellular networks.\nDatasets and Division: FedARA is evaluated on four different NLP datasets, as detailed in Table I. (1) 20News [26]: content from 20 different newsgroups; (2) Semeval [41]: various NLP and semantic evaluation tasks; (3) AG News [42]: different types of news articles; (4) News Category [43]: a range of news categories. In the News Category dataset, we focus on the top 15 most frequent labels, covering 70% of the data. The datasets are split into training, validation, and testing sets in an 8:1:1 ratio. For IID settings, a Dirichlet distribution with $\\alpha$ set to 1000. Non-IID settings follow FedAvg's approach [8], partitioning each dataset by label, with each client receiving data for 1-2 labels, creating a pathological non-IID data distribution.\nModel Our study employs two prominent NLP models: BERT [2] with 12 transformer blocks, and its streamlined variant DistilBERT [3] with 6 blocks, which retains near-original performance while offering a smaller model in size through knowledge distillation. Both models leverage the transformer architecture, which remains foundational for even the latest NLP models [11], [44]. Our research specifically targets at"}, {"title": "VI. EVALUATION", "content": "A. Performance under non-IID data\nTable II illustrates the accuracy of various baselines across different datasets under non-IID, with FedARA's initial rank (r) set to 12 and 6, respectively. The best performance is highlighted in bold, while the second-best is underline. Each result is averaged over five different random seeds. The accuracy is presented in the format A(B), where A represents the accuracy under the non-IID setting, and B denotes the decrease in accuracy relative to the IID setting, both expressed as percentages. A higher A indicates better performance under the non-IID data, while a lower B reflects less impact of non-IID on the method. We also list the total communication overhead measured in GigaBytes (GB).\nFedARA effectively mitigates the performance degradation caused by non-IID data. FedARA achieves outstanding performance across four various datasets and two different models. Specifically, FedARA improves DistilBERT's average accuracy on four datasets by 8.49% over FedLoRA and 11.58% over FedAdapter-h, while also outperforming FedLoRA-B-dr and FedLoRA-B by 6.95% and 7.75%, respectively. On the News Category dataset, FedARA enhances BERT's performance by 9.78% compared to FedLoRA. Moreover, in comparison to FedAdapter-h, the largest accuracy improvement of 37.59% is observed on the Semeval dataset, which is characterized by many labels and a small sample size, while the smallest improvement of 1.47% occurs on the AG News dataset, which has fewer labels and a large sample size. More importantly, FedARA alleviates the effect of Non-IID data on the 20News and News Category datasets with the BERT model from nearly 20% to 10% or less compared to FedLoRA.\nThe outstanding performance of FedARA is attributed to the truncated SVD modules. These SVD modules enhance flexibility by adding the E matrix to decouple the magnitude and directional updates, enabling better adaptation to complex heterogeneous data. Compared to other baselines, FedARA presents a superior capability in similar feature capturing and representation across diverse clients."}, {"title": "B. Dynamic Rank Allocation", "content": "FedARA effectively improve communication efficiency. Compared to other baselines, FedARA effectively reduces the communication overhead while maintaining impressive performance. Specifically, as shown in Figure 7, while starting with a similar communication cost (r=12) to baseline methods on the BERT model and AG News dataset, FedARA achieves a substantial reduction during the process of federated fine-tuning (stabilizing at 131.42 MB), 71.17% lower than its initial cost during the warm-up round. Consequently, as shown in Table II, when the initial rank of FedARA is twice that of FedLoRA, the total communication overhead of FedLoRA and FedLoRA-B-dr exceeds that of FedARA by nearly 20% across different models. When the initial rank of FedARA is the same as FedLoRA, the total communication overhead of FedLoRA and FedLoRA-B-dr is approximately 2.40\u00d7 that of FedARA. The overall communication overhead of FedLoRA-B is nearly 20% higher than that of FedARA. Importantly, FedARA achieves the lowest total communication overhead while maintaining significantly better average performance compared to other baselines.\nThe improvement in communication efficiency originates from dynamic rank allocation, which continuously identifies and transmits relatively important parameters during each round of communication while pruning less important ones. Figure 8 displays final rank allocation results for FedARA using the DistilBERT model and the 20News dataset. Initially set with a rank of 12 at each position, it targets a final average rank of 3, as this empirically provides the best balance between performance and communication overhead. Darker blocks indicate higher ranks, mainly in deeper layers and key components like query, key, fl, and f2, emphasizing their importance in retaining critical model information, consistent with our preliminary experiments (Figure 2c)."}, {"title": "B. Training Time Performance", "content": "We evaluate the training time performance of FedARA and other baselines on various embedded devices. On Raspberry Pi 5 (RPi 5), local training times per batch (batch size = 4) are 1.00 second for the DistilBERT model and 2.01 seconds for the BERT model. AGX Orin and Orin Nano demonstrate significantly faster training times under the DistilBERT model, being 6.67\u00d7 and 5.56\u00d7 faster than RPi 5, respectively. For the BERT model, these differences increases to 8.74\u00d7 and 6.70x.\nFigure 9 compares total training times for the DistilBERT model across these devices. On RPi 5, FedARA reduces the average total training time to 68 minutes, which is 27.65% and 24.38% faster than FedLoRA and FedLoRA-B-dr, respectively. On Orin Nano and AGX Orin, FedARA achieves training times of 25 and 23 minutes, respectively, marking reductions of 46.57% and 48.90% compared to FedLoRA, and 8.99% and 12.20% compared to FedLoRA-B. The results suggest that the weak local computing capability of embedded devices may undermine the advantages brought by reduced communication overhead. These findings align with preliminary observations (Figure 2d) that high-end devices are mainly bottlenecked by communication, while computation constrains lower-end devices.\nFedARA achieves impressive training time performance through dynamic rank allocation and rank-based module pruning. The method of dynamic rank allocation method greatly reduces communication time in FL by reducing the volume of transmissions, while rank-based module pruning method further reduces local computation time and resource consumption on client devices by removing inactive SVD modules safely."}, {"title": "C. Ablation Experiments", "content": "To validate the effectiveness of our proposed methods, we conduct ablation studies comparing FedLoRA, FedARA, and FedSVD, where FedSVD replaces LoRA modules with SVD ones but lacks the adaptive rank strategy. All methods are tested with a uniform initial rank of 4.\nAblation study of truncated SVD adaptation. We assess the effectiveness of SVD modules on performance. As depicted in Figure 10, FedSVD shows that SVD modules improved accuracy by an average of 7.71% across two different models compared to FedLoRA. This indicates that the SVD structure plays a crucial role in mitigating performance degradation under non-IID settings."}, {"title": "VII. RELATED WORK", "content": "PEFT. PEFT methods are originally derived from centralized training, aimed at enhancing fine-tuning efficiency and reducing resource usage. The most representative techniques include Adapters [12], [13], Prompt tuning [14], and LoRA [15]. In our proposed FedARA model, we integrate PEFT by replacing LORA modules with more expressive truncated SVD modules during local training.\nFedPEFT. The strength of PEFT effectively compensate for the significant communication overhead associated with FL based on PLMs. Meanwhile, FedPEFT has the potential to address unique challenges in FL, such as client heterogeneity and multitasking [16], [17]. Moreover, established studies have advanced the system implementation of FedPEFT [24], [25]. Our proposed FedARA extends FedPEFT by focusing on efficient deployment on mobile devices.\nNon-IID. To alleviate non-IID issues in FL, traditional strategies like client selection, optimization, and aggregation have proven its effectiveness [33], [35]\u2013[38]. Innovative methods like SLORA and FeDeRA used sparse tuning and SVD for training initial value optimization, respectively [19], [20]. Additionally, the latest studies are exploring asymmetrical training techniques to mitigate the negative effects. [23], [28], [29], by harnessing the distinct functions of the two matrices"}, {"title": "VIII. CONCLUSION", "content": "FedARA is a novel framework for Federated Parameter Efficient Fine-tuning (FedPEFT) of language models. To enhance performance under non-IID data and improve the efficiency of parameter allocation. FedARA integrates adaptive parameter allocation based on truncated SVD modules and optimizes system efficiency from both communication and computational aspects. Through extensive experiments, FedARA has demonstrated superior performance and greater efficiency compared to existing methods, making it inherently suitable for supporting mobile computing."}]}