{"title": "Complete Chess Games Enable LLM Become A Chess Master", "authors": ["Yinqi Zhang", "Xintian Han", "Haolong Li", "Kedi Chen", "Shaohui Lin"], "abstract": "Large language models (LLM) have shown remarkable abilities in text generation, question answering, language translation, reasoning and many other tasks. It continues to advance rapidly and is becoming increasingly influential in various fields, from technology and business to education and entertainment. Despite LLM's success in multiple areas, its ability to play abstract games, such as chess, is underexplored. Chess-playing requires the language models to output legal and reasonable moves from textual inputs. Here, we propose the Large language model ChessLLM to play full chess games. We transform the game into a textual format with the best move represented in the Forsyth-Edwards Notation. We show that by simply supervised fine-tuning, our model has achieved a professional-level Elo rating of 1788 in matches against the standard Elo-rated Stockfish when permitted to sample 10 times. We further show that data quality is important. Long-round data supervision enjoys a 350 Elo rating improvement over short-round data.", "sections": [{"title": "1 Introduction", "content": "Recently, Large Language Models (LLMs) based on transformer architectures (Vaswani et al., 2017) have demonstrated capabilities well beyond language modeling. A key milestone was the advent of ChatGPT (Ouyang et al., 2022). Extensive research has focused on developing efficient LLM base models (Du et al., 2021; Biderman et al., 2023; Black et al., 2022; Computer, 2023; Touvron et al., 2023a), including supervised models (Taori et al., 2023a; Chiang et al., 2023; Anand et al., 2023; K\u00f6pf et al., 2023) and models using Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017; Ouyang et al., 2022; Rando and Tram\u00e8r, 2023; Bai et al., 2023). Recent research (Wei et al., 2022; Li et al., 2024) shows that as models scale, their capabilities increase. This raises questions about LLMs' intelligence and learning structures. Chess, an ancient game, has dialogue-like characteristics in its notational structures such as Forsyth-Edwards Notation (FEN), Standard Algebraic Notation (SAN), and Universal Chess Interface (UCI). Machine learning in chess has evolved to include reinforcement learning and neural networks based on supervised learning from human gameplay. Developments include AI-based engines like Leela Chess Zero (LC0)\u00b9 and Stock-fish NNUE2, which refine their algorithms through new learning. Deep learning has shown the potential of AI in strategic games. The ChessGPT (Feng et al., 2023) model demonstrated the ability to choose optimal moves by learning from human language and chess data. However, models like ChessGPT cannot generate the best move based on the current game state and complete an entire match. Our focus is on match completeness and quality of gameplay.\nOur contributions can be listed as follows:\n\u2022 Dataset. We collected a large dataset of chess games with over 20B tokens from open-source platforms. Data quality matters; long round data supervision outperforms short-round data by 350 Elo points.\n\u2022 Model. Our ChessLLM is designed to play entire chess games through dialogues. After fine-tuning, it achieved an Elo rating of 1788, winning 61% of games against Stockfish at skill level 0, 56% at skill level 1, and 30% at skill level 2.\n\u2022 Eval Method. We propose evaluation methods based on full games against Stockfish, including move validity, Elo rating, and win rate. We are the only ones using a large language model for chess that can complete full games."}, {"title": "2 Related work", "content": ""}, {"title": "2.1 Large Language Model", "content": "The emergence of Large language models (LLMs) GPT-4 (Achiam et al., 2023), stands as a noteworthy testament to the significant advancements in natural language understanding and generation. Unlike commercial models, open-source models, such as Alpaca (Taori et al., 2023b), Vicuna (Zheng et al., 2023), and Llama2 (Touvron et al., 2023b), have recently become more accessible. Due to their proficiency in text reasoning, LLMs are increasingly being utilized in everyday applications(Chen et al., 2024). Comprehensive benchmarks, such as MMLU (Hendrycks et al., 2021) and HELM (Lee et al., 2023), have been devised for thorough assessments of the LLMs' overall capabilities. Our work takes this evaluation process one step further, particularly highlighting and investigating the capacity of LLMs' ability to play abstract games."}, {"title": "2.2 Supervised Fine-tuning", "content": "Supervised Fine-tuning has emerged as a revolutionary technique within the field of machine learning and has been the subject of a multitude of studies. Owing to the continuous advancements in the domain of transfer learning, pre-trained models, fine-tuned in a supervised manner, have demonstrated superior performance in numerous tasks. Notably, in the context of natural language processing (NLP), the work by Howard and Ruder became a pioneering model of this technique. Their method (Howard and Ruder, 2018) leverages the power of transfer learning for comprehensive language modeling tasks, thus effectively surpassing previous benchmarks. Manipulating the same concept, BERT (Devlin et al., 2019), an innovative model fine-tuned in a supervised manner for a wide array of NLP tasks. BERT demonstrated remarkable success within various NLP tasks, setting new performance standards.\nIn this work, we trained ChessLLM with supervised fine-tuning."}, {"title": "2.3 Chess", "content": "The quest to develop artificial intelligence capable of playing chess can be traced back to the inception of computer science (Turing, 1953; Campbell et al., 2002). The application of machine learning, particularly deep learning, in the domain of chess has been explored extensively in recent years (Silver et al., 2018; McGrath et al., 2022). One of the pivotal works in this field is the study by DeepChess (David et al., 2016), which presented an end-to-end learning method for chess based solely on deep neural networks, demonstrating the powerful capabilities of machine learning in comprehending and mastering strategic games without a priori knowledge.\nIn this work, we applied LLMs to chess and evaluated them with Elo rating."}, {"title": "3 A Large Scale Dataset of Chess", "content": "We introduce a large-scale dataset by collecting chess games online and generating the best moves based on Stockfish's evaluations. Previous research relied on Portable Game Notation (PGN) for strategy learning, interpreting moves as actions in a Markov Decision Process. ChessGPT sees additional value in PGN data, such as Elo ratings indicating player strength and annotated moves providing computer-generated evaluations. These annotations aid in value function learning, thus ChessGPT retains all this information for easier strategy learning. We argue that the core of chess is making the best decision for a given Forsyth-Edwards Notation (FEN) position. Human players focus on the current position rather than past moves. While ChessGPT uses historical moves, formats like PGN can be inefficient for large language models (LLMs) due to their expanding token length. The FEN format remains constant, making it more suitable for LLMs. Therefore, we constructed our dataset as FEN-Best move pairs."}, {"title": "Best Move Construction", "content": "Our Best Move dataset was created through a search method using Stockfish. It consists of two parts: the short round dataset from Chessdb\u00b3 and the long round dataset from self-play endgames based on Stockfish evaluations. Stockfish evaluates positions using heuristic functions and an alpha-beta game tree search. We searched for valid moves from current positions, with search depths of 12-50 for short rounds and 50-200 for long rounds, limiting each search to two seconds. The highest win-rate moves were selected as the best moves."}, {"title": "4 Model", "content": "The Generative Pre-trained Transformer (GPT-3) is an autoregressive language model that generates human-like text through deep learning. It trains on casual language modeling, predicting the next word based on previous words. We trained a GPT-like model using open-llama-3B (Geng and Liu, 2023) and the chess resources from Section 3. Unlike policy behavior data in robotics or gaming, chess state and move data can be expressed textually. This allows chess to be rendered as a text-based game, enabling imitation learning for policy through casual language modeling of the game dataset (Figure 2). This innovative approach of applying language modeling to chess signifies a novel shift in policy learning, leveraging the game's unique aspects to develop superior gameplay tactics."}, {"title": "5 Evaluation Methods", "content": "Chess requires a dynamic evaluation method beyond a fixed set typical of NLP tasks. We propose supplementing the evaluation set with actual games to better assess the model's capabilities."}, {"title": "5.1 Actual Games", "content": "Playing against Stockfish, a top chess engine, offers a strategic challenge. Stockfish uses advanced algorithms to determine optimal moves. Players can choose time controls (blitz, quick, or traditional) to set the gameplay tempo. The engine analyzes moves and positions to find the best move using its evaluative function. In our experiments, we analyzed metrics such as pass@1 and win rate. We believe using Stockfish against our model more authentically simulates real-world human-model interactions and offers greater robustness than a static evaluation set."}, {"title": "Pass@1 in Actual Games.", "content": "We evaluated our model's performance across different data scales, focusing on its ability to generate legal moves successfully."}, {"title": "Win Rating.", "content": "The win rating refers to victories, draws, and losses out of 100 rounds when the model competes against Stockfish or other engines."}, {"title": "Elo Rating.", "content": "We ran a series of matches between our model and Stockfish, recording strategies and moves. The Elo rating is calculated using the formula\n$Elo_{N} = Elo_{O} + (R_{A} \u2013 R_{E})K,$\n$R_{E} = \\frac{1}{1+10^{\\frac{Elo_{S}-Elo_{M}}{400}}},$\nwhere $Elo_{N}$ is the updated Elo rating after the game. $Elo_{O}$ is the previous Elo rating before the game. $K$ is the weight of the tournament. In professional chess, $K$ is often set to 10 for high-ranked players and 20 for low-ranked players. $R_{A}$ is the actual result of the game (1 for win, 0.5 for draw, 0 for loss). $R_{E}$ is the expected result of the game. $Elo_{S}$ is the old Elo rating of Stockfish. $Elo_{M}$ is the old Elo rating of the model. Moreover, we refer to the method introduced by Stockfish to convert between its skill level and Elo rating. The specific calculation method is shown as follows.\n$SK = 37.247e^{3} \u2013 40.852e^{2} + 22.294e \u2013 0.311,$\n$e = \\frac{Elo - 1320}{1870},$\nwhere $SK$ represents skill level $SK = 0, 1, 2, ..., 20$, and $Elo$ represents Stockfish's Elo rating."}, {"title": "5.2 Evaluation Set", "content": "While games against Stockfish provide a robust performance assessment, their length introduces substantial evaluation costs. Thus, we also use an evaluation set to measure the model's prowess. Data distribution in the evaluation set focuses on games spanning 10-20 rounds (30%) and 20-40 rounds (50%), emphasizing the model's middle-game capabilities. This approach manages the inherent uncertainty in chess match lengths, ensuring the model does not exhibit forgetting phenomena after exposure to long rounds."}, {"title": "Distribution of Training set and Evaluation set", "content": "Our training data was generated with $depth = 1$ and $timelimited = 0.1$, while the data used in the game process was generated with $timelimited = 10$ and without depth limited. The eval set is produced by $depth = 1$ and $timelimited = 0.1$, the same as the train set. These two datasets are from different domains, so our method is effective not only on in-domain data."}, {"title": "Legal Move Accuracy.", "content": "We used Stockfish to generate legal move responses for 10,000 unique board positions not in the training set, evaluating our model's proposed moves for legality to ensure proper convergence."}, {"title": "Best Move Accuracy.", "content": "Stockfish generated best move responses, allowing us to compare its outcomes with our model to calculate the accuracy rate for best move predictions."}, {"title": "6 Experiment Analysis", "content": ""}, {"title": "6.1 Evaluation Set", "content": "We evaluated in-distribution data to analyze our model's performance on the evaluation set under varying computing power. From Fig. 3, we observed that on in-distribution data, model performance improves with an increase in training tokens, but at a diminishing rate. This relationship is crucial for understanding model scalability and resource allocation during training. Note that \"same distribution\" refers to the FEN board state distribution and its corresponding best move."}, {"title": "Legal Move and Best Move Accuracy.", "content": "Fig. 3 Left shows that with only 0.5B tokens, our model achieves a legal move accuracy of 99.11% on in-distribution boards, indicating its impressive preliminary chess playing ability. As data volume increases, performance improves, demonstrating the model's scalability and potential for further enhancement. The high accuracy with just 0.5B tokens underscores the model's efficiency and effectiveness. Fig. 3 Right shows the Best Move accuracy under the same distribution. With 2.75B tokens, the model achieved a Best Move accuracy of 40.11%. Although the logic is similar, the generation steps differ, highlighting our model's ability to accurately predict the best moves in most cases, proving its practical utility."}, {"title": "6.2 Actual Games", "content": ""}, {"title": "Pass@1 in Actual Games.", "content": "The temperature and $top_{p}$ parameters were both set at 1.0, and $top_{k}$ was set at 50 we generated once to calculate Pass@1. Matches against Stockfish, using only one sampling iteration per match, evaluated the legality of our model's moves. Figure 1 shows our model's results. Despite fluctuations from incorporating more endgame strategies, the model consistently achieves over 90% move legality. The legality remains stable against opponents of varying strengths."}, {"title": "Elo rating.", "content": "Table 1 shows our model's performance in 100 rounds each against Stockfish at skill levels 0, 1, 2etc., computing Elo ratings. With temperature and $top_{p}$ parameters were both set at 0.7, and $top_{k}$ was set at 50. we used up to 10 sampling iterations, performing the move upon obtaining a legal one. Our model achieves an Elo score of about 1788, positioning it at the top of amateur chess performance."}, {"title": "6.3 Eval Set Accuracy and Actual Games", "content": "Figure 4 shows that within the evaluation set, an increase in Best Move accuracy correlates with Elo rating gains. A significant Elo rating jump occurs when the model's Legal Move accuracy reaches 99.8%. This increase is due to the reduction in errors after the model learns to generate legal moves, reinforcing that continuous error correction and learning the correct moves significantly improve Elo ratings."}, {"title": "6.4 Compare with Other LMs", "content": ""}, {"title": "General Policy.", "content": "General Policy is proposed by ChessGPT (Feng et al., 2023). Table 2 showcases the results, delineating the effectiveness of various models in identifying the most fitting move for the black chess piece."}, {"title": "Win Rating.", "content": "We conduct matches between ChessLLM and other Language Models (LMs) such as LLAMA (Touvron et al., 2023a), Red-Pajama (Computer, 2023), ChessGPT-Base (Feng et al., 2023), and ChessGPT-Chat (Feng et al., 2023), calculating their respective win rating. As other models cannot guarantee the legality of the moves they generate, we bring in Stockfish to aid in this process. Should the model fail to produce a valid move even after 50 sampling efforts, a mechanism is employed wherein there's a 50% chance of favoring either the best move identified by Stockfish or a randomly picked move from the list of all possible legal moves. Similarly, as ChessGPT is unable to generate the best move for the next step, we generate all legal moves through Stockfish and utilize their proposed general policy for selection, picking the most optimal move as recognized by the model."}, {"title": "6.5 Impact of Token Quantity and Quality", "content": "We have investigated the impact of data quantity and quality on the generation of legal moves. Figure 1 Left presents the $Pass@1$ indicators for two groups of data. It can be observed that the model performance significantly improves with the addition of more high-quality data, supplementing the data beyond the original distribution. Figure 1 Right presents an augmentation in the number of tokens, it is observed that the model's Elo rating experiences an enhancement. Concurrently, the enrichment of the model with data not within the distribution can expedite the elevation of the model's Elo rating."}, {"title": "7 Conclusion", "content": "In this paper, we convert chess to a text game and introduce a large-scale Fen-Best Move pair dataset. With the dataset, we propose the Large language model ChessLLM that can play a complete chess game. Considering the limitation of the evaluation set in out-of-distribution data, we propose the need to evaluate model capabilities in actual games. ChessLLM finally achieves an Elo rating of 1788 through the SFT method. In subsequent work, we will discuss how to improve ChessLLM by improving the data quality."}, {"title": "8 Limitations", "content": "In this study, we explored the problem of LLM playing chess games and found that with high-quality synthetic data of complete games, LLM can have the extrapolation and combat capabilities of chess games. In the future, we will continue to explore this capability by improving the data quality, RLHF, and self-play + MCTS so that LLM can become better at chess games. Our ultimate goal is to enable LLM to excel in various games through high-quality game data."}, {"title": "9 Ethics Statement", "content": "In this research, we adhere to strict ethical guidelines and principles. The study has been designed and implemented with respect for the rights, privacy, and well-being of all individuals involved. Our findings and conclusions are reported accurately and objectively, avoiding any misrepresentation or manipulation of data. The entire process and outcomes are free from intellectual property and ethical legal disputes."}]}