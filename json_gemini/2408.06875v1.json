{"title": "Advancing Interactive Explainable AI via Belief Change Theory", "authors": ["Antonio Rago", "Maria Vanina Martinez"], "abstract": "As AI models become ever more complex and intertwined in humans' daily lives, greater levels of interactivity of explain- able AI (XAI) methods are needed. In this paper, we pro- pose the use of belief change theory as a formal foundation for operators that model the incorporation of new informa- tion, i.e. user feedback in interactive XAI, to logical repre- sentations of data-driven classifiers. We argue that this type of formalisation provides a framework and a methodology to develop interactive explanations in a principled manner, pro- viding warranted behaviour and favouring transparency and accountability of such interactions. Concretely, we first de- fine a novel, logic-based formalism to represent explanatory information shared between humans and machines. We then consider real world scenarios for interactive XAI, with differ- ent prioritisations of new and existing knowledge, where our formalism may be instantiated. Finally, we analyse a core set of belief change postulates, discussing their suitability for our real world settings and pointing to particular challenges that may require the relaxation or reinterpretation of some of the theoretical assumptions underlying existing operators.", "sections": [{"title": "1 Introduction", "content": "To achieve the safe, regulated and trustworthy deployment of AI while maximising its potential, a number of applica- tions benefit from interactive explanations, where a human provides feedback to the AI model (see (Wu et al., 2022) for a recent overview). Interactivity has also been recognised as a core tenet of ensuring that AI is contestable (Hirsch et al., 2017; Lyons, Velloso, and Miller, 2021), as recommended by design principles such as those of the ACM\u00b9 and en- forced by legal regulations such as the GDPR\u00b2. Meanwhile, the field of explainable AI (XAI), with its overarching ob- jective of fostering trust in AI models, predominantly fo- cuses on static explanations which do not support such in- teractivity (see (Ali et al., 2023) for an overview). Some XAI methods provide interactivity via user feedback, e.g. in human-in-the-loop reinforcement learning (Retzlaff et al., 2024), recommender systems (Rago et al., 2021) and text classification (Arous et al., 2021), where explainability has been said to be beneficial, but this research area remains rel- atively unexplored. Further, formal frameworks for interac- tivity in XAI are lacking, despite their crucial role in trust- worthiness (Marques-Silva and Ignatiev, 2022), giving scant prospect for regulations on interactive XAI to be defined and systematically enforced.\nIn this paper, we propose the use of belief change the- ory (Alchourr\u00f3n, G\u00e4rdenfors, and Makinson, 1985) within the modelling of interactive explanations for data-driven classifiers. We assume as given a set of explana- tions about a classifier, in the form of rules, e.g. as in (Guidotti et al., 2018; Ribeiro, Singh, and Guestrin, 2018; Shih, Choi, and Darwiche, 2018; Grover et al., 2019; Ignatiev, Narodytska, and Marques-Silva, 2019), and we envisage the possibility of users providing feedback thereon, also in the form of rules. We see revision operators as being particularly well-suited to modelling the process of feedback incorporation, as evidenced in the related setting of editing multi-label classifiers (Coste-Marquis and Marquis, 2021).\nWe argue that such formalisations lay the groundwork for the design and development of interactive explanations that promote transparency, interpretability and accountability in human-machine interactions. As an example of the impor- tance of this topic, the recently endorsed AI Act\u00b3 regulatory framework for the European Union, guarantees the right of consumers to launch complaints and receive meaningful ex- planations. Such legal requirements make it evident that novel methodologies and tools are needed to provide for- mal guarantees about not only AI models' behaviour but also about all related human-machine interactions.\nAfter covering the related literature (\u00a72), we make the fol- lowing contributions:\n\u2022 We define a novel, logic-based formalism to represent how information is shared between humans and ma- chines, specifically classification models, in XAI (\u00a73).\n\u2022 We consider a set of real world scenarios of interactive XAI where our formalism may be instantiated with dif- ferent prioritisations of new and existing knowledge (\u00a74).\n\u2022 We instantiate a core set of belief revision postulates in our formalism, discussing their strengths and weaknesses (\u00a75), before looking ahead to what is required for belief revision to make advancements in interactive XAI (\u00a76)."}, {"title": "2 Related Work", "content": "Within the area of belief revision the work of Falappa, Kern-Isberner, and Simari (2002) proposes a non-prioritised revision operator based on the use of expla- nations by deduction. The epistemic input is accompanied by an explanation supporting it and beliefs are dynami- cally qualified as defeasible or undefeasible and revised accordingly. Recently, Coste-Marquis and Marquis (2021) proposed a belief change operator, called a rectification operator, that aims to modify, according to some available background knowledge, a Boolean circuit that exhibits the same input-output behaviour as a multi-label classifier. The operation ensures that the rectified circuit complies with the background knowledge through different notions of compliance. Though this proposal also aims to model mod- ifications to logical representations of classifiers through belief change operators, there exist significant differences. First, we assume partial and approximate knowledge of the classifier's behaviour and therefore a potentially incomplete and not coherent logical representation of it; this has a direct impact on the analysis of suitable postulates. Second, the classifier's representation and the feedback provided by users are specified by means of rules rather than proposi- tional logical sentences. We believe this encoding provides greater interpretability from a user's point of view. Finally, instead of prioritising the input or feedback, we study alternatives according to different scenarios of interactive explanations, allowing for the possibility for the logical representation to gradually differ from the original classifier specification as feedback is incorporated. Finally, the work from Schwind, Inoue, and Marquis (2023) proposes a series of operators that determine how a Boolean classifier should be edited whenever it does not label a data point in the correct way. The paper studies the incorporation of positive, negative and combined (positive and negative) instances. Besides only focusing on Boolean classifiers, the differences mentioned above for the multi-label approach also hold in this case."}, {"title": "3 Formalising Classifiers and Explanations", "content": "In this section, we formalise classifiers' outputs and expla- nations based on propositional logic and formal rules, ex- tending the language from (Amgoud, 2023).4\nWe assume a single-label classification problem where F = {f1,..., fm} is the set of m > 1 features, where each fi \u2208 F has a discrete domain D(fi), and C = {C1,...,Cn} is the set of n > 1 possible classes or classification labels. We let V = D(f\u2081) \u00d7 . . . \u00d7 D(fm) be the (combinatorial) set of all possible data points, i.e. assignments of values to all features. Straightforwardly, we then let a dataset be a set of data points D\u2286 V. Then, a classifier M : D \u2192 C is a total mapping such that for any x \u2208 D, we say that M predicts\nIn the supplementary material we provide example illustra- tions of our approach, as well as a proof of Theorem 1.\n5 Note that M is a total mapping wrt D, i.e. the data points for which the classes predicted by the classifier are known. Here, D may represent any dataset, e.g. that used for training.\nclass c\u2208 C iff M(x) = c. For a given x, x\u00b2 is the value v \u2208 D(fi) assigned to feature fi.\nSyntax. To model a classifier, we assume a propositional language based on two finite alphabets F = f1,..., fm and C = C1,..., Cn, representing elements in F and C, resp. For each symbol f in F, we assume a discrete set of constants D(f) corresponding to the domain (D(f)) of feature f \u2208 F. A feature atom is of the form (f,v), where f \u2208 F and v \u2208 D(f); a feature literal is either a feature atom a or \u00aca. On the other hand, a classification atom is of the form c, with c \u2208 C. Intuitively, a feature atom represents the fact that value v is assigned to feature f, while a classification atom represents a set of classes (in particular, an atom represents a singleton, as we will see later).\nA feature (classification, resp.) formula is any logical for- mula built from feature (classification, resp.) literals using classical connectives \u00ac, ^, v. We use F (C, resp.) to denote the set of all feature (classification, resp.) formulas.\nWe distinguish the following set of feature formulas, in- tuitively to link each of them to a specific data point in V.\nDefinition 1. A (data) instance x is conjunction of feature atoms such that each feature f \u2208 F appears exactly once. We will call V the set of all possible data instances.\nIntuitively, feature formulas represent sets of data points in V, while a data instance represents a specific data point in V. On the other hand, classification formulas represent sets of classifications labels. The concept of a rule, defined below, allows us to map feature formulas into classification formulas, which ultimately seek to represent a mapping be- tween data points and a set of potential classification labels.\nDefinition 2. A ruler is of the form \u03c6 \u21d2 \u03c8, where \u03c6 \u2208 F and \u03c8 \u2208 C. We call \u03c6 the body of r, denoted body(r), and \u03c8 the head of r, denoted head(r). If \u03c6 is a data instance (i.e. \u03c6 \u2208 V) and c is a positive literal, we call r an instance rule.\nIntuitively, we use a ruler to establish the set of classes, defined by the classification formula in the head of the rule, that is assigned to the set of data instances characterised by the feature formula body(r). Note that when a rule estab- lishes that a certain set of data points are assigned to a non- singleton set of classes, we interpret that any of those classes in the set could be assigned, but only one of them.\nSemantics. Function If : F \u2192 2V maps feature formulas to sets of data points in V. Formally, for feature formulas \u03c6, \u03c8:\n\u2022 if \u03c6 = (fi, v), then If ((fi, v)) = {x \u2208 V | x\u00b2 = v}\n\u2022 If(\u03c6 ^ \u03c8) = If(\u03c6) \u2229 If(\u03c8)\n\u2022 If(\u03c6 v \u03c8) = If(\u03c6) UIf(\u03c8)\n\u2022 If(\u00ac\u03c6) = {x \u2208 V | x \u2209 If (\u03c6)}\n\u2022 If(\u22a5) = \u00d8, If(T) = V\nThe semantics for classification formulas is defined with Ic : C \u2192 2C, which maps classification formulas to sets of classes in C. Formally, for classification formulas \u03c6, \u03c8:\n\u2022 if \u03c6 = c, where c is a classification atom, then Ic(c) = {c}\n\u2022 Ic(\u03c6 ^ \u03c8) = Ic(\u03c6) \u2229 Ic(\u03c8)\n\u2022 Ic(\u03c6 v \u03c8) = Ic(\u03c6) UIc(\u03c8)\n\u2022 Ic(\u00ac\u03c6) = {c \u2208 C \\ c \u2209 Ic(\u03c6)}\n\u2022 Ic(\u22a5) = \u00d8, Ic(T) = C\nTo interpret rules in this setting, we define function I such that, given a rule r, I(r) = (If(body(r)),Ic(head(r)))."}, {"title": "Here, I maps the set of data points represented by the for- mula body(r) into a set of classes determined by head(r).", "content": "Using the language defined above, we can logically model classifiers, and explanations therefor (defined later), by means of rules since they express mappings of sets of data points into sets of classification labels. In addition to ex- tending the representation language from (Amgoud, 2023), note that the spirit of the aforementioned paper is different to ours. In that work, the authors formally define functions that generate different types of explanations and study their properties in relation to existence and correctness. In this work, we assume explanations from a classifier have already been provided in the form of rules and we model the inter- actions with the model's users through operations that could update such rules as a result of the user's feedback.\nBased on the semantics, we can now define notions that help us establish relationships among rules. The first is en- forcement: intuitively, a set of rules enforces another set of rules whenever every possible assignment of data points to a class that the enforced set of rules represents is also an assignment that is represented by the enforcing set.\nDefinition 3. Given sets of rules Ri and Rj, Ri enforces Rj, denoted Ri \u2287 Rj6, iff \u2200ri \u2208 Rj, \u2200x \u2208 If(body(r;)), \u2203ri \u2208 Ri, such that x \u2208 If (body(ri)) and Ic(head(ri)) \u2286 Ic(head(rj)).\nThe second notion we define is consistency, requiring that sets of rules do not assign incompatible labels to data points.\nDefinition 4. Given a set of rules R, R is consistent iff \u2200x \u2208 \u22c3r\u2208R If (body(r)), \u2200ri, rj \u2208 R such that x \u2208 If (body(ri)) and x \u2208 If(body(rj)) then Ic(head(ri))\u2229Ic(head(rj)) \u2260 \u00d8. Otherwise, R is inconsistent.\nThe notion of coherence defined below aims to capture the relationship between rules and models. Intuitively, given a threshold \u03c4\u0454 [0, 1], a set of rules is T-coherent with a model iff the proportion of instances captured by the body of every rule, such that the model's classification of the instance is included in the head of the rule, is at least 7. This generalises the notion of compatibility in (Amgoud, 2023) allowing a percentage of the classifications described by the set of rules to differ from the classifications provided by the classifier.\nDefinition 5. Given a classifier M and a threshold \u03c4\u2208 [0,1], we say that ruler is T-coherent with M iff If (body(r)) \u2229 D = \u00d8 or:\n|\n{x \u2208 If (body(r)) \u2229 D|M(x) \u2208 Ic(head(r))}\n|\n|If (body(r)) \u2229 D|\n\u2265T\nWe say that a set of rules R is T-coherent with Miff \u2200r \u2208 R, r is T-coherent. Whenever \u315c = 1, we drop the \u0442 prefix and say that a (set of) rule(s) is coherent with M.\nLemma 1. Given a classifier M, a rule r is coherent with Miff \u2200x \u2208 If (body(r)) \u2229 D, M(x) \u2208 Ic(head(r)).\nNext, we formalise whether a set of rules completely (and exclusively) represents the set of known data points D.\nFor ease of presentation, when R\u2081 or Rj are singletons we omit bracket notation, writing ri\u2287 rj, instead of {ri} = {r;}.\nDefinition 6. Given a classifier M, a set of rules R is com- plete for M iff R is a set of instance rules such that |R| = |D| and \u2200x \u2208 D, \u2203r \u2208 R where I(r) = ({x}, {M(x)}).\nWe now represent the knowledge we have about a classi- fier by means of rules as follows.\nDefinition 7. Given a classifier M, an explanation knowl- edge base for M is a set KM = Ka U Ke, where Ka is a set of instance rules, called the data, and Ke is a set of general rules, called the explanations.\nOur intention is for Ka to represent data points for which the classification is known, these may come either from training or evaluation phases or from previous use of the classifier. Ka logically represents the classifier, such that they encode exactly the same classifications. In addition to this, there exist different methods in the literature to elicit behavioural patterns from classifiers, often expressed as rules functioning as explanations, we use Ke to represent that kind of knowledge. Although consistency is generally expected, a priori we impose no restrictions of coherence of Ke with M, as they represent tentative knowledge ob- tained from potentially imprecise methods. These explana- tion rules may have been extracted by existing formal meth- ods for explaining (discrete) classifiers from the literature, such as (Guidotti et al., 2018; Ribeiro, Singh, and Guestrin, 2018; Shih, Choi, and Darwiche, 2018; Grover et al., 2019; Ignatiev, Narodytska, and Marques-Silva, 2019). Moreover, T-coherence corresponds to the notion of precision in (Ribeiro, Singh, and Guestrin, 2018), and could be used to allow for tolerance in the correctness of explanations. How- ever, for this paper we will assume that T = 1.\nTheorem 1. Given a classifier M and an explanation knowledge base KM = Kau Ke, where Ka is complete for and coherent with M, a set of rules R is coherent with M iff Ru Ka is consistent.\nThis result shows that preserving consistency with Ka preserves coherence with M (see the consistency postulate in \u00a75) whenever Ka is complete and coherent with M. Note, however, that we do not make this assumption in general, since, as discussed later, we aim for a framework that is tol- erant to inconsistency and in which the logical representa- tion of M may differ from M due to feedback incorpora- tion."}, {"title": "4 Interactive Explanations", "content": "We now demonstrate how interactive explanations may be modelled with our formalism, and consider how such expla- nations may be deployed in real world settings. We consider interactive explanations which give users the ability to pro- vide feedback to classifiers in a number of ways, in the form of rules (Definition 2), which we call here feedback. When a rule is provided as feedback (we limit to single rules), the goal is to analyse how/if this knowledge can be incorpo- rated, possibly modifying both the explanation knowledge base and the feedback itself. This type of feedback mir- rors rule-based explanations from XAI (as we discuss in \u00a73) that intuitively represent knowledge in any domain and eas- ily translate to and from natural language."}, {"title": "We define the following basic desiderata for this process:", "content": "\u2022 Constrained Inconsistency: specific scenarios may re- quire some tolerance to inconsistency, e.g. requiring only Ka to be kept consistent after an interaction.\n\u2022 Bounded model incoherence: while we expect Ka to be coherent with M, the weaker notion of T-coherence of the explanations Ke with M could be accepted, for a given T.\n\u2022 Minimal information loss: the information contained in KMU {r} should be modified or removed minimally, and only when it jeopardises the above desiderata.\nBelief revision incorporates new information following two main principles: consistency (preservation) and mini- mal change. Our desiderata for interactive explanations co-incide with these aims in minimising the amount of informa- tion loss. However, we relax the notion of consistency, and allow the classifier and its logical representation to drift in a restricted manner through the notion of T-coherence.\nWe now give a (non-exhaustive) set of real world ap- plication settings where interactive explanations may be deployed. We base our settings on those proposed in (Retzlaff et al., 2024) for human-in-the-loop systems.\nIn the first setting, named S1, we envisage a classifica- tion model which is in development, e.g. being debugged by a developer as in (Lertvittayakumjorn, Specia, and Toni, 2020). Here, the user provides feedback to update, and cor- rect, the model. In this case, the model's trust in the feed- back can be regarded as credulous, since the model should be updated to align with the feedback, i.e. any feedback r takes priority over the existing knowledge (informally rep- resented with r > KM). For example, if a user provides r which contradicts an instance rule representing an exist- ing data point in Ka, e.g. due to the default settings of the model or changing preferences of the user, we would like to incorporate r and update the conflicting instance rule to align with the new conditions specified by the user.\nIn the second setting, S2, we introduce a model which is being refined in an evaluation stage by group of users, e.g. as in domain expert information fusion (Holzinger et al., 2021), before the model is deployed at scale. In this case, a single model is being updated by feedback from multiple users, and so the model's trust in the feedback must be bal- anced with that in the existing knowledge. Here, a single user's feedback should not necessarily take precedence over existing knowledge (informally, r \u2243 KM), and so both the new and the existing knowledge may be modified in order for consistency and coherence with the model to be main- tained with minimal information loss. For example, if a user provides r which contradicts Ke, it may be desirable to pre- serve Kd but modify Ke or r by weakening or rejection to incorporate as much of the new knowledge as possible.\nIn the final setting, S3, we consider a model which has already undergone commercial deployment at scale, but al- lows for feedback from the sizeable group of users for com- pleting gaps in the knowledge, e.g. as in autonomous vehi- cles (Wu et al., 2023). Here, the model will be updated by users' feedback, but due to the size of this group and the fact that the model has already been deployed commercially, e.g. potentially raising legal issues, the trust in the feedback is sceptical, and it thus prioritises existing over new knowl- edge (informally, r < KM). The new knowledge can thus be modified in order to ensure its consistency with the existing knowledge. For example, if the user provides some r which does not violate the consistency of Ke or the coherence of Ka, then it may be incorporated to KM as is to minimise information loss. Meanwhile, if it contradicts KM, then we may preserve \u041a\u043c while only part of r may be incorporated.\nIt is important to note that these modifications are not per- formed over the model itself but its logical representation, creating in each interaction a new knowledge base that may differ substantially from the original knowledge base (and the model). A distance between different versions of the knowledge base could be measured through T-coherence or more conventional distance measures, and be used as a way of checking the effect of feedback, e.g. as an indicator for when the retraining of the model is required.\nHaving presented out motivational scenarios, in the next section we analyse the suitability of belief revision operators to model interactions with explanation knowledge bases."}, {"title": "5 Revision of Explanation KBs", "content": "One of the main contributions of the foundational models of belief change is the development of a style of research and development methodology based on providing axiomatic characterisations of the operators' behaviour in terms of postulates. The postulates focus on conditioning and con- straining the inputs and the results of the operators, rather than providing insights into how the results are achieved. Representation theorems are used both to provide seman- tic characterisations for belief change operators, as well as linking these characterisations to computational implemen- tations, providing provable guarantees on the behaviour of such algorithms. In the following we analyse a core set of postulates for belief base revision (Hansson, 1993), translate them in our logical setting and discuss their suitability with respect to the different scenarios of interactive explanations.\nIn this work, we adopt the approach to belief revision known as base revision, where existing knowledge is rep- resented as a finite set of formulas (Hansson, 1993), which we call an explanation knowledge base KM, as described in \u00a73. The new information consists of a single ruler that is obtained from the interaction with the user(s) of the model. In the following analysis we use KM * r to describe the application (and the results) of operator * over the existing knowledge base KM and input (feedback) r.\nSuccess states that the epistemic input is always accepted, i.e. new knowledge is prioritised. This can be formalised in our framework by means of our notion of enforcement of the feedback rule, i.e. \u041am*r = {r}. In setting S1, the success postulate can be used to enforce the feed- back taking priority over the existing KM (in the presence of inconsistency). Prioritised revision operators are suit- able for this setting, while this is not the case for (possi- bly S2 and) S3, where the existing knowledge should be prioritised. A first approach to define non-prioritised be- haviour could be modelled by a simple relative success pos- tulate (Ferm\u00e9, Mikalef, and Taboada, 2003), which states"}, {"title": "that either the input is fully (explicitly) accepted or rejected, i.e. either r \u2208 KM * r or KM * r = Km, resp. More fine- grained alternatives would allow for the specification of con- ditions under which the input could be fully or partially ac- cepted. For instance, weak success (Resina et al., 2020) may state that if KMU{r} is consistent then Km*r = {r}. Mean- while, proxy success and weak proxy success (Resina et al., 2020) state that the revision should incorporate part of the input, e.g. to ensure all users' feedback plays a part in S2. Formally, proxy success could be defined requiring that \u2203r' such that {r} \u2287 {r'}, Km*r = {r'} and KM * r = KM*r'. In weak proxy success, r' is not conditioned by r: \u2203r' such that Km * r = {r'} and KM * r = Km * r'. These weak- ened postulates seem appropriate for S2 and S3, where gaps in KM could be filled more often with these weaker con- straints, but less so for S1, where success may be preferred given the trust in the user here. However, any version of success that allows for the incorporation of only part of a rule could induce bias in the dataset. A potentially prob- lematic example could be when only a stricter version of a feedback rule, covering only a subset of a feature (e.g. an ethnic minority in a population), rather than its entirety, is incorporated to KM.\nInclusion states that the only addition to the existing knowledge can be the feedback itself, instantiated in our set- ting as KM*r \u2286 Kmu{r}. This raises issues in our settings, since it may be desirable that Kd, Ke or both are modified, for instance making rules more specific. In S1, it is desirable that we incorporate r as is, but we may wish for Ke to be adapted to this new information. Also, in S2 and S3, we may want to incorporate only part of r, since it may be unrealis- tic to incorporate r in its entirety given the higher priority of KM. An alternative is weak inclusion (Resina et al., 2020), which states that if r \u2208 Km * r, then KM * r \u2264 Kmu{r}. This relaxation alleviates the second aforementioned issue, and we would thus posit that this is desirable in S3, where existing and new information is restricted from modification, e.g. from a legal standpoint if users have already seen it. However, in S2 we would expect that Ke being adapted to r would be more suitable. We thus propose three alternate for- mulations of inclusion based on our notion of enforcement, prioritising the suitable data in each setting. For S1, we sug- gest that KM * r \u2286 A \u222a {r}, where KM \u2287 A, allowing the existing explanations to adapt to the new information. For S2, we suggest that Km * r \u2286 A, where KM \u222a {r} \u2287 A, allowing for the modification of both existing and new in- formation. For S3, we suggest that KM * r \u2264 KMUA, where {r} \u2287 A, ensuring that only the feedback is modified.\nConsistency conventionally requires that a knowledge base becomes consistent after the revision, even if it is not so beforehand. Formally, KM * r is required to be consistent, which, by Theorem 1, may cover the first two of our desiderata whenever Ka is coherent with M. Thus, the notion of consistency seems to be desirable\nA singleton set containing r is consistent by Definition 4, so our version of the postulate does not condition on the consistency of the input. Allowing for sets of feedback rules, as in multiple revision (Fuhrmann and Hansson, 1994), is future work.\nacross our settings, whenever neither consistency nor co- herence is relaxed. In particular, consistency preserva- tion (Alchourr\u00f3n, G\u00e4rdenfors, and Makinson, 1985), which requires that a consistent KB be consistent after operating (adding the condition that KM is consistent to the consis- tency postulate above) seems suitable for all settings, since it requires feedback not introduce such inconsistencies, rather than requiring it fix any which already exist. Note, however, that it may be the case that we are interested in only Ka remaining/becoming consistent after the revision, given the tentative and approximate nature of Ke. An alternative to be considered is to ensure that the revision does not increase the amount of inconsistency (given a measure for it (Thimm, 2016; Grant and Martinez, 2018)) in KM or Ka.\nRelevance concerns minimal change of existing knowl- edge, stating that if r' \u2208 Km and r' \u2209 KM * r, then there is a set of rules R such that Km * r \u2286 R \u2286 K\u043c\u222a {r}, R is consistent and R\u222a {r'} is inconsistent. Relevance for- malises our third desideratum in terms of only removing in- formation from the data or explanations if it were inconsis- tent with the feedback being provided by the user, render- ing it suitable across our settings. This postulate has impor- tant implications for data protection, ensuring that the non- conflicting knowledge is preserved and therefore is desirable in all three settings. However, as defined above, this postu- late forces KM * r to be a subset of Km\u222a {r}; in light of our previous discussion, if we want to have the possibility of not only deleting but modifying both the existing knowledge and feedback, we could consider a weaker notion closer to the postulate known as core-retainment: in our setting this could be formalised as if r' \u2208 Km and r' \u2209 KM * r, then there is a set of rules R such that R \u2286 Kmu {r}, R is consistent but R\u222a {r'} is inconsistent.\nUniformity, formulated in our setting, states that if \u2200R \u2286 \u041a\u043c, Ru {r} is inconsistent if and only if Ru {r'} is in- consistent, then Km \u2229 (Km + r) = \u041a\u043c \u2229 (KM * r'). The intuition here is that if r and r' are inconsistent with \u041a\u043c in the exact same way, revising by either retains the same knowledge from KM. Once again, uniformity seems to be appropriate across the settings, guaranteeing the regularity of the effects of feedback, which could be useful for ensur- ing that regulatory guidelines are met."}, {"title": "6 Discussion and Future Work", "content": "summarises the results of our analysis. Some of the existing postulates are suitable for all of these settings in their original form, i.e. relevance and uniformity, while the others require alternate versions from the literature. How- ever, across all studied postulates, we believe that there is scope for novel, tailored versions which may be more suitable in the individual settings, as we have indicated. Indeed, even in the cases where there are suitable postu- lates, others may be preferable, e.g. as we suggested for success. We believe that this highlights many fruitful av- enues for future work. Among these, a next step is to characterise the behaviour of each setting with a specific set of postulates and provide the corresponding construc- tions. Regarding constructions, it seems possible to im- plement S1 with minimal modifications to traditional be-"}, {"title": "lines of research to future work.", "content": "lief revision base operators such as partial meet and ker- nel (Hansson, 1993). The other two of our envisaged set- tings lend themselves to non-prioritised revisions that could be implemented through operators such as credibility lim- ited (Ferm\u00e9, Mikalef, and Taboada, 2003) and screened re- vision (Makinson, 1997), in which a portion of the knowl- edge Kp Ka U Ke is protected from the revision. For ex- ample, it may be the case that unless data points from the dataset Ka are explicitly mentioned in the feedback, then we protect Ka from changes, i.e. Kp = Kd \\ {r}, and modify only explanations. In S2, Ke may be seen as be- ing modifiable while Ka is protected (no matter what ris being provided), i.e. Kp = Kd, for example if the dataset has been curated to be unbiased. Another case could be when a subset of Ka U Ke needs to be protected from the revision, for example rules representing data points or ex- planations which have already been delivered to users, semi-revision (Hansson, 1997) could be useful here as it allows r to be discarded. Our analysis also suggests that for S2 and S3 it may be desirable to only retain part of the infor- mation contained in r. The closest operator in the literature that behaves in this way is selective revision (Resina et al., 2020). All these operators are implemented based on classi- cal AGM operators, either checking conditions or modifying the input before applying an AGM revision operator or re- curring to other operators such us consolidation (restoring consistency) over KM\u222a {r}. For setting S2 and S3 we may need to combine their implementations.\nIn light of the discussion about consistency, we need to define alternative postulates that better satisfy our proposed desiderata, that includes tolerating some degree of incon- sistency and T-coherence of Ke with the model for T \u2260 1.\nFinally, our analysis assumes independence of interactions and that feedback consists of a single rule. Operators such as those based on iterative revision (Darwiche and Pearl, 1994) and improvement (Konieczny and P\u00e9rez, 2008) are worth studying for continuous feedback, e.g. coming from different users or over time, as well as multiple revi- sion (Fuhrmann and Hansson, 1994) in order to allow arbi- trary sets of rules as feedback. We leave exploration of these"}]}