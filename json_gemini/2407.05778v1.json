{"title": "When is the consistent prediction likely to be a correct prediction?", "authors": ["Alex Nguyen", "Dheeraj Mekala", "Chengyu Dong", "Jingbo Shang"], "abstract": "Self-consistency (Wang et al., 2023) suggests that the most consistent answer obtained through large language models (LLMs) is more likely to be correct. In this paper, we challenge this argument and propose a nuanced correction. Our observations indicate that consistent answers derived through more computation i.e. longer reasoning texts, rather than simply the most consistent answer across all outputs, are more likely to be correct. This is predominantly because we demonstrate that LLMs can autonomously produce chain-of-thought (CoT) style reasoning with no custom prompts merely while generating longer responses, which lead to consistent predictions that are more accurate. In the zero-shot setting, by sampling Mixtral-8x7B model multiple times and considering longer responses, we achieve 86% of its self-consistency performance obtained through zero-shot CoT prompting on the GSM8K and MultiArith datasets. Finally, we demonstrate that the probability of LLMs generating a longer response is quite low, highlighting the need for decoding strategies conditioned on output length.", "sections": [{"title": "1 Introduction", "content": "Self-consistency (Wang et al., 2023) demonstrates that sampling multiple answers for a question and considering the most consistent answer, leads to improvement in performance. In other words, they suggest that the consistent answer is more likely to be the correct answer. In this paper, we challenge this argument and propose a refinement. We observe that not all consistent answers from LLMS are highly likely to be correct. Instead, the consistent answers obtained via longer reasoning texts, involving more output tokens and computational effort, are more likely to be accurate.\nThis is majorly because of a notable phenomenon we observe: LLMs can produce chain-of-thought (CoT) style reasoning texts while generating longer responses. CoT reasoning (Wei et al., 2022) entails guiding large language models (LLMs) (Radford et al., 2019; Brown et al., 2020; Chung et al., 2022; Touvron et al., 2023; OpenAI, 2023) through step-by-step breakdowns of examples (Mekala et al., 2022), significantly enhancing their performance on reasoning benchmarks. Traditionally, eliciting CoTs from LLMs without any demonstrations required the inclusion of specific prefixes in prompts (Kojima et al., 2022). However, we observe that LLMs can generate CoTs independently, without any prefix prompts while generating longer responses.\nOur primary findings reveal that the consistent answers obtained through longer reasoning texts are more likely to be correct than any consistent answers. For each question, by simply sampling multiple answers from the LLM and considering responses exceeding a certain length threshold, and choosing the most consistent answer, we observe a significant improvement in performance. Among these longer responses, we observe the spontaneous appearance of CoTs without any specific prompts. Leveraging this, we achieve 86% of the zero-shot CoT self-consistency performance on two mathematical reasoning benchmarks. Additionally, we investigate why CoTs appear infrequently and find that the model often blurts out the answer in the initial tokens, a tendency more pronounced in discriminative tasks than in generative ones. Therefore, we advocate for decoding strategies that account output length before generating the response.\nThe remainder of the paper is structured as follows: initially, we describe the experimental setup encompassing the language models, and the datasets employed (section 2). Subsequently, we"}, {"title": "2 Experiment Setup", "content": "We employ two open-sourced pre-trained models, Mixtral-8x7B (Jiang et al., 2024) and Llama-2 70B (Touvron et al., 2023) for our experiments. We follow the prompting pipeline as in (Kojima et al., 2022), which includes a reasoning extraction step that generates the reasoning text, followed by an answer extraction step that extracts the answer from the reasoning. This study focuses on the reasoning extraction step. To encourage diversity in the reasoning extraction, we sample with a temperature of 1.2 using top-k sampling and set k = 40. On the other hand, answer extraction should include as little variation as possible so we sample 50 tokens with greedy decoding. More details in Appendix A.1.\nOur main baselines are zero-shot CoT (Kojima et al., 2022) (denoted by ZEROSHOT-COT) where we add the prefix Let's think step by step while generating the reasoning text. For generating longer responses, unlike (Kojima et al., 2022), we do not add any prefixes during the reasoning extraction. We prompt the LLM with the question alone and consider the response only if the number of tokens generated is more than 60. We denote this with ZEROSHOT-LENGTH. We also compare with no such length threshold and denote it as ZEROSHOT."}, {"title": "3 Consistent Predictions via Longer Reasoning Texts are more likely to be correct", "content": "In this section, we study the effect of reasoning text length on performance. We consider GSM8K (Cobbe et al., 2021), MultiArith (Roy and Roth, 2016) datasets and Mixtral-8x7B, Llama-2 70B language models. We examine token lengths ranging from 0 to 100 and divide them equally into ten buckets. For each question, we resample reasoning texts with no custom prompts until we obtain ten texts per bucket.\nFirstly, for each bucket, we obtain answers using the reasoning texts corresponding to that bucket and obtain the most consistent answer. Subsequently, we plot the average frequency of the most consistent answer per bucket obtained using both models for the GSM8K dataset in Figure 1 and the MultiArith dataset in Figure 4. Furthermore, we plot the mean accuracy of the most consistent answer per bucket for both models on the GSM8K dataset in Figure 2 and the MultiArith dataset in Figure 5. We observe that the average frequency"}, {"title": "4 Self-Consistency with a Minimum Consistency Threshold", "content": "In the previous section, we noted that the consistent predictions obtained through longer reasoning texts are more likely to be correct. In this section, we evaluate ZEROSHOT-LENGTH comprehensively on both generative and discriminative tasks. For generative tasks, we use two mathematical reasoning datasets: GSM8K and MultiArith, which require models to generate solutions. For discriminative tasks, we use AQUA-RAT (Ling et al., 2017), a"}, {"title": "4.1 Self-Consistency Performance vs Minimum consistency Threshold Analysis", "content": "We vary the minimum consistency threshold and plot the self-consistency performance of the Mixtral-8x7B model on the GSM8k, MultiArith, AQUA-Rat datasets in Figure 7 and SST2 dataset in Figure 12 in Appendix. We observe that the performance improves with an increase in the threshold. Additionally, the performance of ZEROSHOT-LENGTH consistently surpasses the ZEROSHOT on most datasets, highlighting the advantages of verbose reasoning texts."}, {"title": "5 Blurting vs Reasoning Analysis", "content": "In this section, we analyze the types of reasoning texts generated by ZEROSHOT-LENGTH and their likelihood. This analysis is performed on all the reasoning texts generated by ZEROSHOT-LENGTH until the minimum consistency threshold of 12 is achieved for each question across all datasets. We notice three kinds of reasoning texts: (1) CoT-style text; (2) blurt text, where the model directly outputs the answer in the first few tokens and then explains it; (3) noisy text, which has meaningless text. We focus on CoT-style and blurt texts in this analysis. We consider the model to be blurting the answer if the final answer appears within the first ten tokens of the reasoning text. Additionally, we utilize the three-shot prompting-based classifier introduced in section 3 to identify CoTs in the generated reasoning texts.\nWe present the likelihood of blurting and exhibiting CoT reasoning by the Mixtral-8x7B and Llama-2 70B models for all datasets in Table 2. We observe that models blurt the answer more frequently. For E.g. the likelihood of blurting is more than that of CoT in MultiArith, SST2, and AQUA-RAT datasets.\nMoreover, we examine the conditional probabilities p(Correct | Blurt) and p(Correct | CoT), which quantify the odds of an answer being correct given that the reasoning text is blurted or follows a CoT-style, respectively. Although the model exhibits a higher tendency for blurting, the probability of the answer being correct is significantly higher when the reasoning follows a CoT-style compared to when the answer is blurted, consistently across all tasks.\nFinally, we focus on the most consistent answer that is correct and compute the probabilities of it being derived from a CoT or the model blurting the answer. By comparing p(CoT | Correct Consistent) and p(Blurt | Correct Consistent), we observe a substantial difference between the odds of the correct consistent answer originating from a CoT versus the model blurting the answer for generative datasets. For the discriminative datasets, since the model exhibits a significant tendency to blurt the answer, i.e. p(Blurt) >> p(CoT), and the performance when blurted is not much different from performance with CoT i.e. p(Correct | Blurt) \u2248 p(Correct | CoT), the correct consistent answer is more likely to be the result of the model blurting the answer directly."}, {"title": "6 Likelihood Analysis", "content": "We measure the likelihood of models generating longer and shorter responses to a question with no custom prompts. To quantify this, we consider the reasoning texts generated by ZEROSHOT until the minimum consistency threshold of 12 is attained for each question in the dataset. We divide the token lengths ranging from 0 to 100 into ten buckets and plot the average probability of a reasoning text whose length falls within each bucket. The prob-"}, {"title": "7 Related Work", "content": "CoT Reasoning Eliciting CoTs from LLMs typically necessitate prompt engineering (Kojima et al., 2022; Wei et al., 2022; Mekala et al., 2024) or intensive fine-tuning (Rajani et al., 2019). (Wang and Zhou, 2024) unveils CoT responses without prompting by exploring multiple decoding paths. In contrast, we show that if sampled enough number of times, models generate CoTs within their lengthier responses with no prompting.\nSelf-Consistency Self-consistency (Wang et al., 2023) improves reasoning performance by sampling multiple responses and considering the most frequent one. Our work demonstrates that this phenomenon can be better leveraged by considering only the longer responses that required additional computation from LLMs. (Pfau et al., 2024) trains LLMs to use filler tokens to perform such computation and observe improvements in performance."}, {"title": "8 Conclusion", "content": "In this paper, we demonstrate that the consistent answers derived from verbose reasoning texts exhibit a higher likelihood of being correct. Leveraging this, our experiments illustrate that the performance of zero-shot prompting can be significantly enhanced on reasoning tasks. We show that this improvement is predominantly due to the spontaneous emergence of CoTs within the lengthier reasoning texts. Finally, we reveal the intrinsic propensity of models to produce extended responses is relatively low, thereby underscoring the necessity for decoding strategies tailored to generate longer outputs."}, {"title": "9 Limitations", "content": "The limitation is highlighted in section 6, where we demonstrate that we have to sample numerous times for the models to generate longer responses."}, {"title": "10 Ethical Considerations", "content": "This paper introduces a minor correction to the self-consistency method. We do not expect any significant ethical concerns."}, {"title": "A Appendix", "content": "A.1 More details on Experimental Settings\nWe follow the same approach as in (Wang and Zhou, 2024) to remove illformed responses. If any responses are empty or have a length the same as the maximum decoded step, we filter them, as the response could be unfinished or repeating. Rarely, the model may also repeat the input question, so we remove responses ending in a question mark.\nAlong with the filters mentioned in (Wang and Zhou, 2024), we introduce two of our own design. We noticed that if the extracted reasoning does not contain a solution, the model will ignore the reasoning and attempt to solve the input question during the answer extraction step. However, we want our analysis to only reflect reasoning done during the reasoning step, so we filter any responses where the extracted answer is not a sub-string of the reasoning. Finally, we ignore any responses that do not produce a valid prediction after answer extraction (integer for GSM8k and MultiArith, (A) through (E) for AQUA-RAT).\nA.2 Self-consistency Accuracy Comparison\nWe vary the number of times an answer is sampled per question during reasoning extraction step and plot the self-consistency accuracy (Wang et al., 2023) for GSM8k and MultiArith in Figure 10, 11 respectively. Our results indicate that the self-consistency performance of the ZEROSHOT-LENGTH setting surpasses that of the ZEROSHOT setting, suggesting that longer reasoning texts contribute to more consistent and correct predictions. This can be attributed to the high presence of CoTs in the longer reasoning texts. Moreover, we observe the performance gap between ZEROSHOT and ZEROSHOT-COT being reduced significantly by ZEROSHOT-LENGTH.\nA.3 Self-Consistency Accuracy vs Minimum Consistency Threshold for SST2 Dataset\nWe vary the minimum consistency threshold and plot the self-consistency accuracy for SST2 dataset in Figure 12.\nA.4 CoT-style Detection Prompt\nThe prompt for CoT-style detection using few-shot prompting Llama-3-Chat-70B is:"}]}