{"title": "Meta 3D TextureGen: Fast and Consistent Texture Generation for 3D Objects", "authors": ["Raphael Bensadoun", "Yanir Kleiman", "Idan Azuri", "Omri Harosh", "Andrea Vedaldi", "Natalia Neverova", "Oran Gafni"], "abstract": "The recent availability and adaptability of text-to-image models has sparked a new era in many related domains that benefit from the learned text priors as well as high-quality and fast generation capabilities, one of which is texture generation for 3D objects. Although recent texture generation methods achieve impressive results by using text-to-image networks, the combination of global consistency, quality, and speed, which is crucial for advancing texture generation to real-world applications, remains elusive.\n\nTo that end, we introduce Meta 3D TextureGen: a new feedforward method comprised of two sequential networks aimed at generating high-quality and globally consistent textures for arbitrary geometries of any complexity degree in less than 20 seconds. Our method achieves state-of-the-art results in quality and speed by conditioning a text-to-image model on 3D semantics in 2D space and fusing them into a complete and high-resolution UV texture map, as demonstrated by extensive qualitative and quantitative evaluations. In addition, we introduce a texture enhancement network that is capable of up-scaling any texture by an arbitrary ratio, producing 4k pixel resolution textures.", "sections": [{"title": "1. Introduction", "content": "3D generative models have advanced considerably, in part thanks to the impressive progress in text-to-image [13, 16, 43, 44, 46, 47] and text-to-video [17, 22, 52] generation. These advances concern three related fronts: (i) generation of 3D shapes, including the development of new and powerful shape representations [1, 10, 37, 51, 62], (ii) generation of textures [6, 8, 34, 45]; and (iii) combined generation of shape and texture, often called 'text-to-3D' [25, 26, 40, 49, 57]. As new shape representations usually include appearance information too, areas (i) and (iii) are converging. However, texture generation remains important, as it allows to control appearance independently of shape, and is applicable to any 3D asset, whether produced by an artist or generated automatically.\n\n\"Moonlight is sculpture; sunlight is painting\". After the subtleties of geometry, textures and colors add a remarkable layer of expressiveness, as implied in this famous quote by Nathaniel Hawthorne [19]. Creating textures is a key mode of expression for 3D artists and crucial to the impact of 3D content in applications such as gaming, animation, and virtual/mixed reality. However, creating high-quality and diverse textures, whether realistic or stylized, is difficult and time-consuming, particularly for complex 3D shapes, and requires specific professional skills.\n\nContrary to image and video generation, where billions of images and videos are available for training, 3D generation is hampered by the lack of large-scale 3D datasets. For this reason, 3D generation networks, including texture generation, are often derived from pre-trained image or video generation networks. This allows texture generators to inherit some of the qualities of their peers, including realism, faithfulness and open-ended nature, while only utilizing a comparatively small amount of 3D training data. However, there are still significant quality and speed gaps between texture and 2D image and video generation:\n\n(i) Global consistency and text faithfulness. The gap between the image-text relationship when generating a single image compared to generating a sequence of images or views, translates to a lack of global consistency and text faithfulness in the generated texture. This is further intensified by the strong bias of text-to-image models towards frontal views, as well as their lack of 3D understanding. These inconsistencies range from small texture misalignments (often referred to as \u201cseams\u201d), to a lack of symmetry or an overall incoherent look, to catastrophic failures such as the \"Janus effect\" [59], where multiple instances of a given anatomical feature (e.g. a face or an eye) appear in multiple places across the object.\n\n(ii) Semantic alignment with the target 3D shape. The text-to-image model is required to generate texture that fits the given 3D object, and must thus be conditioned on its shape. However, fusing fine 3D shape information into 2D space in a coherent manner, such that fine 3D information is preserved yet translated efficiently to 2D space is difficult to achieve. Previous attempts generated texture by either conditioning in UV space on vertex or normal maps [66], or in image space, on depth maps [67]. However, they struggle with precise alignment and fine-detail preservation, resulting in lower texture quality for highly detailed 3D objects, which is a considerable limitation.\n\n(iii) Inference speed. While previous methods rely on iterative generation for improving global consistency and gaining complete shape coverage, they require multiple generation steps, ranging from several to thousands of forward passes, such as via Score Distillation Sampling (SDS) [40]. This results in a long inference time of minutes, which is compute intensive and renders these methods unsuitable for many practical use cases, such as user-generated content applications, or allowing designers to perform quick iterations as part of their creative process.\n\nWe introduce Meta 3D TextureGen, a new texture generation method that successfully addresses these gaps, while attaining state-of-the-art results. Our method is fast, as it only requires a single forward pass over two diffusion processes. The method achieves excellent view and shape consistency, as well as text fidelity, by conditioning the first fine-tuned text-to-image model on 2D renders of 3D features, and generating all texture views jointly, accounting for their statistical dependencies and effectively eliminating global consistency issues such as the Janus problem.\n\nThe second image-to-image network operates in UV space, it creates a high-quality output by completing missing information, removing residual artifacts, and enhancing the effective resolution, bringing our generated textures to being close to application-ready. Moreover, we introduce an additional network that enhances the texture quality and increases resolution by an arbitrary ratio, effectively achieving a 4k pixel resolution for the generated textures.\n\nTo the best of our knowledge, this is the first approach to achieve high quality and diverse texturing of arbitrary meshes using merely two diffusion-based processes, without resorting to costly interleaved rendering or optimization-based stages. Moreover, this is the first work to explicit condition networks on geometry in 2D, such as position and normal renders in order to encourage local and global consistency, finally alleviating the Janus effect.\n\nSamples of our generated textures are provided on a diverse set of shapes and prompts throughout the paper, as well as on static and animated shapes in the video."}, {"title": "2. Related work", "content": "A number of architectures have been proposed for text-to-image synthesis, including earlier efforts using Generative"}, {"title": "2.1. Image generation", "content": null}, {"title": "2.2. Multi-view generation", "content": "The field of multi-view generation, which involves the generation of multiple perspectives of a single object or scene from noise or a few reference images, has demonstrated its utility in the generation of 3D shapes. Zero-1-to-3 [28] and Consistent-1-to-3 [63] generate novel views through viewpoint-conditioned diffusion model. Zero123++ [48], MVDream [49] and Instant 3D [25] opt for a grid-like generation of six and four views respectively. ConsistNet [61] use a different diffusion process for each view and introduce a 3D pooling mechanism to share information between views. Additional layers and architectures to enhance multi-view consistency are proposed by SyncDreamer [29], Consistent123 [58], DMV3D [60] and MVDiffusion++ [54] which denoise multiple views of the 3D object simultaneously. The obtained multi-view images in these works are then utilized as guidance to reconstruct the texture and geometry of a 3D object.\n\nIn contrast to our task of texture generation, these models are designed for the generation of 3D objects, where the geometry is not predetermined and is concurrently produced with the texture. This application inherently provides the flexibility to modify the geometry to achieve more consistent multi-view images, for both texture and geometry."}, {"title": "2.3. Texture generation", "content": "Texture generation aims to create high-quality and realistic or stylized textures for 3D objects based on textual descriptions. Early works, such as CLIP-Mesh [36] and Text2Mesh [35] proposed to optimize a texture via differentiable rendering, using CLIP [41] guidance to match the text prompt. Other optimization-based methods such as Fantasia3D [9], Latent-Paint [34] and Paint-It [64], combine differentiable rendering with SDS [40] to utilize gradients from diffusion models. Texturify [50] and Mesh2Tex [5] opt for a GAN-based approach incorporating a latent texture code and a mapping network similarly to StyleGAN [23].\n\nThe rapid emergence of large-scale text-to-image models, particularly diffusion models, has led to several advancements in texture generation. Several methods, such as TexDreamer [31] and Geometry Aware Texturing [11] aim to generate a UV map in a straight-forward manner, applying the diffusion process directly in UV space. While these methods tend to be fast, they are limited to human texture generation and clothing items respectively, and cannot generalize to arbitrary objects. Point-UV Diffusion [66] proposes a point-cloud diffusion approach to generate a colored point-cloud, which colors are subsequently projected onto the UV map for further refinement, yet requires to train a separate model for each object category, and does not generalize to arbitrary objects.\n\nA significant area of work, which includes TEXTure [45], Text2Tex [8], Intex [53] and Paint3D [67], consists of iterative inpainting using pre-trained depth-to-image diffusion models in a zero-shot manner. This involves generating a single view at a time and iteratively rotating the mesh until a sufficient area is covered, using interleaved renderings as guidance for further inpainting steps. While these approaches are training-free, their inference"}, {"title": "3. Preliminaries and data processing", "content": "Our method takes a representation of the 3D shape features in the form of rendered images and baked texture maps in UV space, which are used in the first and second stage respectively. Here we detail the different channels that we render for each shape."}, {"title": "3.1. Shape renders", "content": "We render the following channels for each shape. Each channel is rendered from four views which are stitched to a single image.\n\nCombined pass. As ground truth data used for training, which is not extracted at inference time, we render the shape with all material properties. This render, often referred to as \"beauty pass\", preserves lighting effects and material properties that are applied to the object. These are crucial to preserve to correctly represent different types of materials such as wood, plastic, metal, etc., which react differently to light and thus cannot be represented faithfully using only their diffuse color. We use Blender [12] to render the combined pass with even lighting from all directions.\n\nPosition and normal passes. These are used as conditioning for training and inference. Each pixel in the position pass represents the XYZ position of the corresponding point on the shape, and each pixel in the normal pass represents the normal direction of the shape at the corresponding point. Both are normalized to the range [0, 1] and rendered without lighting, hence written as-is to the output image."}, {"title": "3.2. UV maps", "content": "We bake each channel into a texture in UV space. This process involves producing a UV layout for each shape and baking the texture to an image.\n\nUV layout. Our in-house dataset contains objects from various sources which may have various UV layouts, from layouts meticulously created by an artist, to scanned objects with a procedurally generated layout, and objects with partial or corrupt UV layout. A single object may contain many texture files, in which case the UV layout of each part may overlap the layout of parts that are mapped to a different texture. For our method, we require a UV layout that maps the shape onto a single square texture with no overlapping UV islands, so we automatically rearrange the UV islands of the shape such that there is no overlap between them. For objects that do not have a suitable UV map, we generate a new UV map using Blender's Smart Project feature, and filter out objects for which this process fails to produce a desirable UV layout.\n\nBaked channels. We use Blender to bake the combined, position, and normal passes mentioned above to the UV space. Baking a texture is a similar process to rendering an object, but the rendered pixel are written to the corresponding location on the UV map rather than being painted in the render view. The combined pass is used as the target image for training, while the position and normal passes are used as conditioning for the network.\n\nBackprojected textures. To simulate the input textures that are produced by the first stage, we take the color renders of the shape and project them onto the texture in UV space, using the same process as described in Sec. 4.2.1. The network goal is to reconstruct the full texture map from these partial views."}, {"title": "4. Method", "content": "Given a 3D object and description of a desired texture, Meta 3D TextureGen produces as output a corresponding texture in UV space. As shown in Fig. 2, Meta 3D TextureGen employs a two-stage approach. The first stage operates in image space, conditioned on a text description and renders of the 3D shape features, and produces renders of the textured shape from multiple views. The second stage operates in UV space, taking a weighted incidence-based backprojection of the first stage output as condition, as well as the 3D shape features used for the first stage, but in UV space. The end result of the second stage is a complete UV texture map which is consistent between different views and matches the text prompt. An optional extension of the second stage is a texture enhancement network that extends the MultiDiffusion [3] approach from 1D to 2D image-patch overlaps, increasing the texture map resolution by \u00d74.\n\nAs demonstrated in our experiments (Sec. 5), by conditioning the fine-tuned text-to-image model on renders of 3D shape features while generating all views in tandem, the first stage is able to generate diverse yet globally consistent renders of textured 3D shapes, while the second stage focuses on generating the missing areas that are occluded in image space and improving the overall quality of the generated texture map.\n\nNext, we provide a detailed overview of each stage. We focus here on the novel or unusual aspects of our method and refer the reader to the supplement for details."}, {"title": "4.1. Stage I: Generation in image space", "content": "The goal of the first stage is to generate globally consistent images of a given 3D object based on a textual description of the desired output. To this end, we use a diffusion-based neural network fine-tuned from a pre-trained image generator. In order to produce consistent views that match the given 3D object, the network takes as input a grid of position and normal renders from multiple angles, in addition to the text conditioning. Specifically, for each channel we produce a grid of 4 matching viewpoints and combine them to a single image. The four viewpoints are fixed at training and inference time, and provide a 360\u00b0 view of the object at 90\u00b0 intervals, with a fixed elevation angle of 20\u00b0."}, {"title": "4.1.1 Geometry-aware 2D conditioning.", "content": "Multiple methods [8, 66, 67] use depth maps as a way to represent 3D assets in 2D images leveraging depth-conditioned pre-trained diffusion models in a zero-shot manner. In contrast, we advocate for the use of position and normal renders.\n\nAs seen in Fig. 3, the additional information in these representations provides the following benefits for using them as conditioning: (i) position values are global and not view-dependent, providing point correspondence between"}, {"title": "4.1.2 Multi-view image generation from text", "content": "The first stage consists of a U-Net based latent diffusion model, fine-tuned from a model with a similar architecture to Emu [13] denoted by f. Its goal is to generate a grid of four consistent views of an arbitrary mesh S, in image space, guided by a text prompt t*, denoted by I. For this purpose, the diffusion model is conditioned on two grids of matching position and normal renders, denoted as Pgrid(S) and Ngrid(S') respectively.\n\nThe generated multi-view image grid I can then be formulated as follows:\n\n$I(S,t*) = f(z,t*, Pgrid(S), Ngrid(S))$,\n\nwhere z is 2D noise map where each pixel is sampled i.i.d. from a standard Gaussian distribution. Note that in this equation t* denotes the textual prompt; in practice, this network is also conditioned on the diffusion step, sometime called 'time'. We do not show it here explicitly for succinctness and clarity."}, {"title": "4.2. Stage II: Generation in UV space", "content": "The goal of the second stage is to generate the final texture in UV space. Given the viewpoints from the first stage output, the network aims at inpainting missing areas due to self occlusions and improving the overall quality of the generated texture, in UV space. The inputs for the second stage are the partial texture map, obtained by backprojecting and blending the views generated by the first stage, in addition to the position and normal UV maps."}, {"title": "4.2.1 Backprojection and incidence-based weighted blending", "content": "Backprojection is a technique where a 2D image or projection is mapped onto the UV texture map of a 3D model. This involves identifying the corresponding face on the 3D model for each non-background pixel in the image and assigning the color value at the corresponding coordinate in the texture map.\n\nAlthough the first stage results in highly consistent views of the generated texture due to the conditioning on 3D semantics, we have observed, similarly to previous works [8, 30, 66], that textures generated over areas that are not facing the camera (low incidence angles) are less reliable. This can lead to artifacts when na\u00efvely averaging different texture views together, particularly in areas with high frequency details such as fine patterns or writings. To overcome this"}, {"title": null, "content": "issue, similarly to SyncMVD [30], we blend the backprojections into a single UV map using a weighted average by the incidence angles. Specifically, we utilize the cosine similarity between the viewing direction and per-pixel normal vectors in image space to determine per-pixel weight contributions to the blended texture. Formally, the incidence of a pixel p in a rendering Is of a 3D shape S, for each view i (which we denote by $(I's,p)) is defined as $(I's,p) = cos(0(p),\u0e17\u0e35(I,p)) where 0\ud45c,\uade0 is the angle between 2 and \u1ef9, (p) is the viewing direction from camera i to pixel p, and \u00f1(I's, p) is the normal vector at pixel p of the rendered shape S from camera i.\n\nFinally, denoting the backprojection operation as BP, we define each pixel p of the blended partial texture \u010cuv (S,t*) as follows:\n\n$Cuv(S,t*) = \\frac{\\sum_{j=0}^{n} BP(I(S, t^*)_j) \\odot BP(\\phi(I(S,t^*)j,p)^\\alpha)}{\\sum_{j=0}^{n} BP(\\phi(I(S,t^*)j,p)^\\alpha) + \\epsilon}$ (2)\n\nwhere I(S,t*); is the j'th view of I(S,t*) and I(S,t) is the pixel p of I(S,t*)j. \u20ac is a small constant to avoid zero division. We use n = 4, as the number of generated views and \u03b1 = 6 for all of our experiments."}, {"title": "4.2.2 UV-space inpainting network", "content": "The first stage followed by the weighted backprojection operator results in a texture map that is sparse in a varying degree, depending on the input shape. The degree of sparsity is determined by two factors: (i) occlusions caused by insufficient coverage of the selected views in respect to the shape structure, resulting in missing areas, and (ii) pixel-level \"holes\" resulting from the absence of one-to-one correspondence between each occupied pixel in the generated rendering and the UV map. To obtain the full texture, we opt for an inpainting approach.\n\nSimilarly to Stage I, the inpainting is modeled by a U-Net based latent diffusion model fine-tuned from the same pre-trained network which we denote by g. g is conditioned on the blended partial map Cuv (S, t*), the inpainting mask denoting the missing areas and pixels to inpaint Muv(S), along with Puv (S) and Nuv (S), to obtain the final texture map Texture (S, t*) as follows:\n\n$Texture(S,t*) = g(z, \u010cuv(S,t*), \u039c\u03c5\u03bd (S), \u03a1\u03c5\u03bd (S), \u039d\u03c5\u03bd (S))$, (3)\n\nwhere z is a 2D noise map where each pixel is sampled i.i.d. from a standard Gaussian distribution."}, {"title": "4.2.3 Texture enhancement network", "content": "Our two-stage texture generation approach yields a text-aligned, high-quality and consistent UV texture map at a resolution of 1024 \u00d7 1024 pixels. While this resolution is satisfying for some applications, other applications may require a higher resolution of 4k (4096 \u00d7 4096) pixels. To that end, we introduce an additional, yet optional component to the second stage for up-scaling the generated texture map resolution and quality. This is the texture enhancement network, which is flexible in terms of the output resolution and ratio, as it operates in a patched-based fashion.\n\nThe reason for employing a patch-based approach [38, 55] is due to the memory limitations of current GPUs that do not support the generation of 4k resolution images. As patch-based prediction results in inconsistencies between different patches, manifesting both locally (seams) and globally as pattern/color mismatches, we extend the MultiDiffusion [3] approach from 1D image-patch overlaps to 2D (panoramas to square-shaped images) to mitigate these issues, aggregating the different latent patches and applying a weighted Gaussian average at each diffusion time step. In addition, we employ a tiled-VAE approach for the encoder-decoder to enable the encoding and decoding of high-resolution textures."}, {"title": "5. Experiments", "content": "We evaluate our method in comparison to state-of-the-art previous work, namely TEXTure [45], Text2tex [8], SyncMVD [30], Paint3D [67], and the commercial product Meshy 3.0 [33]. Our method achieves state-of-the-art results according to user studies and numerical metric comparisons. Samples supporting the qualitative advantage are provided in Figs. 4 and 5, while quantitative comparisons are provided in Tab. 1. Additionally, we provide a qualitative ablation study in Fig. 8 to better assess the effects of different contributions. Diverse sets of generated samples are provided in Fig. 1, Fig. 7, and Fig. 10, as well as in the appendix, including animated samples in the video."}, {"title": "5.1. Data", "content": null}, {"title": "5.1.1 Training data", "content": "Our dataset consists of 260k textured 3D objects sourced from an in-house collection. Text captions are extracted for each object similarly to Cap3D [32]."}, {"title": "5.1.2 Evaluation data", "content": "To evaluate our methods and the baselines quantitatively and qualitatively, we use a set of 54 objects with CC license that do not have a 'No-AI' tag from the Sketchfab website. In addition, we use 2 objects from the Stanford 3D Scanning Repository. For each 3D object, we provide 4 creative text prompts for generations, which we use for our user study. Additionally, we provide a single text prompt describing the original texture of each object, which is necessary for evaluation using metrics such as FID (Frechet Inception Distance) [20] and KID (Kernel Inception Distance) [4]. The complete list of objects and prompts is provided in the supplementary. Additionally, in all qualitative and quantitative comparisons, we do not employ the texture enhancement network in order to allow for fair comparison in terms of resolution, as both our method and the baselines generate texture maps at a resolution of 1024 \u00d7 1024."}, {"title": "5.2. Quantitative comparisons", "content": "In order to quantitatively evaluate our method, we employ the FID and KID metrics. These metrics aim at evaluating the quality of the generated textures. Furthermore, we conduct a user study to evaluate how well the generated texture represent the objects in terms of visual quality and text alignment, as well as the presence of artifacts."}, {"title": "5.2.1 User study", "content": "For the user study, we rendered 360\u00b0 rotation videos of the generated textured meshes from our evaluation set. In each question we present two videos side-by-side, one generated by our model and another generated by one of the baselines, along with the text prompt used to generate the textures. The order of meshes, prompts and baselines are randomized, as well as the left-right ordering of the baseline and our method in order to eliminate bias. Similarly to [8], participants were asked to choose which object best represents the given prompt. This question captures both text alignment and overall visual quality, as textures of low quality do not represent the desired object well. In addition, we ask which object displays fewer visual artifacts to capture cases in which an object is generally of better quality, e.g., more detailed or realistic, but includes some errors or inconsistencies. The decision for each texture is determined by max-voting. An example question screenshot is provided in the supplementary. 33 users participated in the study, with 754 responses. A breakdown comparing our method with the baselines (see Sec. 2.3) can be seen in Tab. 1. Overall, our method was preferred over all baselines, both in terms of overall quality and when considering artifacts."}, {"title": "5.2.2 Metrics", "content": "For the FID and KID calculations, we render the ground-truth textured meshes and the generated textured meshes from 32 evenly spaced viewpoints under identical conditions, the standard image FID and KID scores are then calculated between these two sets of rendered images. For runtime, we compare inference time for each method, where we define inference time as the time it takes to generate a complete texture map for a given text prompt and predefined mesh. Even though we report faster runtime for the baselines compared with the numbers reported in the original papers, we emphasize that we could not run them in the same exact setup as ours (single H100 vs. A100 GPU), which should translate to some reduction in runtime. However, given our method's advantage of not running multiple generation iterations, combined with the significant runtime difference, we expect that our method would be the fastest when running on the same GPU."}, {"title": "5.3. Qualitative comparisons", "content": "We provide several qualitative comparisons with previous work, focusing on different aspects of visual quality: text fidelity, global consistency, local consistency, and texture\nRuntime for Meshy is estimated using the Meshy 3.0 API."}, {"title": "5.4. Ablation study", "content": "In order to assess the importance of different contributions to our method, we provide an ablation study in Fig. 8. We compare five cases: (a) excluding the first stage (no image space), (b) excluding the second stage (no UV space), (c) excluding the weighted-incidence blending (simple averaging), (d) our method without texture enhancement (SR), and (e) our method with texture enhancement.\n\nOmitting stage I (no image space). In this scenario we fine-tuned a diffusion model that operates in UV space exclusively, similarly to the second stage. However, we omit the partial texture and inpainting mask conditioning and"}, {"title": "6. Limitations", "content": "The generation of PBR material maps, such as tangent normals, metallic and roughness are not covered by this method, and are left as future work. Although Meta 3D TextureGen is currently the fastest method for texture generation, it is not real-time nor fast enough to cover all possible applications. However, the introduction of recent methods of speeding-up text-to-image models, such as ImagineFlash [24], could directly translate into real-time texture generation, given that the bottlenecks are the text-to-image forward passes. While training on 3D datasets is crucial for achieving global consistency, the reliance on 3D datasets is somewhat limiting for training large models compared with the size of image and video datasets."}, {"title": "7. Ethical considerations", "content": "The application of generative methods in general extends to a wide range of use cases, many of which are not covered in this work. Before implementing these methods in real-world scenarios, it is crucial to thoroughly examine the data, model, its potential uses, as well as considerations of safety, risk, bias, and societal impact. In the specific case of texture generation, the limitations of the existing shape provide some risk mitigation, as users would be bound to a pre-defined structure."}, {"title": "8. Conclusions", "content": "We introduce Meta 3D TextureGen, a new method for texturing 3D objects from text descriptions. While there has been impressive progress in this domain, our method brings texture generation to be significantly closer to an applicable tool for 3D artists and general users to create diverse textures for assets in gaming, animation and VR/MR. This is done by providing global consistency (e.g. eliminating the Janus problem), strong control (adherence to text prompts), speed, and high-resolution (4k) to the generation process."}, {"title": "A. Additional implementation details", "content": null}, {"title": "A.1. Training details", "content": "All of the models presented in the manuscript have a similar architecture, and are fine-tuned from the same base text-to-image generation model that operates at a resolution of 1024 \u00d7 1024. Their multiple conditionings are encoded via the original image encoder matching to our base model and are concatenated altogether via channel-wise concatenation. To adapt the architecture to these new inputs, we simply add the relevant number of additional channels as zero-weighted input channels for the first convolution layer. The text-to-multiview network (Stage I) was fine-tuned to minimize the L2 loss, and both the UV space inpainting (Stage II) and texture enhancement networks to minimize the L1 loss. We use v-prediction formulation where the noise schedule was rescaled to enforce zero terminal SNR [27]. We empirically found that the latter is beneficial when training diffusion models on renderings and UV maps, which possess large background areas, such as rendering background and un-mapped pixels for UV maps.\n\nWe fine-tune all of our models with a learning rate of le-5 and a batch size of 256 on 32 H100 gpus. Stage I and Stage II models were fine-tuned for 15k steps each and the texture enhancement model was trained for 28k steps. For stage I and stage II we employ DDPM solver and use 60 diffusion steps for inference. For the multi-diffusion texture enhancement we employ DDIM solver with 50 diffusion steps."}, {"title": "A.2. Texture enhancement model training pipeline", "content": "Our training pipeline for the diffusion model enhances image quality by addressing artifacts and upscaling the texture by an arbitrary ratio. The design of our upsampler draws inspiration from the widely utilized open-source Real-ESRGAN framework [56]. Despite its effectiveness, Real-ESRGAN's degradations often produce artifacts such as over-smoothed textures, excessively sharpened edges, and patterns with high contrast, leading to noticeable ringing effects. We have noticed that our method does not exhibit these issues. Besides changing the architecture to a diffusion model and training on high quality texture maps, we modified the data degradation pipeline to empirically better match our needs, omitting the Unsharp Masking operation as well as the additive Gaussian noise. Our patch-based approach, followed by Multi-Diffusion blending, allows us to upsample an image by an arbitrary ratio, without introducing seams or noticeable artifacts between the patches. Moreover, we employ a tiled-VAE approach in order to overcome memory issues arising from encoding and decoding large images and latent maps. These choices resulted in a robust upsampler, tailor-made for upsampling texture maps to a very high resolution."}, {"title": "B. Experiments details", "content": null}, {"title": "B.1. Evaluation dataset", "content": "All meshes on our evaluation dataset were taken from Sketchfab, under CC Attribution license and respecting any NoAI requests by the artists. We present a list of all meshes, with credit to the artists, as well as the prompts we used, in Tabs. 4 to 9. Prompts not marked in bold were used during the user study, while those marked in bold were used for FID and KID calculation."}, {"title": "B.2. Applications", "content": "The vast majority of texture generation evaluation is performed on a single asset detached from any environment (i.e. with a white background). While this is important for capturing fine details and artifacts, it lacks the broader context of a method's ability to produce multiple assets that can blend in an environment, whether realistic or stylized, in a manner that is desirable for real-world applications. In addition to the single asset evaluations, we demonstrate the usability and applicability of our method in diverse real-world scenarios, utilizing it for building both realistic and stylized environments in virtual reality in Fig. 10 and the supplementary video."}, {"title": "B.3. User study", "content": "We conducted a user study, presenting pair-wise comparisons between our method and five different baselines - TEXTure, Text2Tex, SyncMVD, Paint3D and Meshy on textured meshes. To eliminate biases, left-right ordering, as well as mesh, prompt and baseline orderings have all been randomized. A screenshot of the survey is shown in Fig. 14. Table 3 includes a breakdown of participants to different backgrounds, according to their familiarity and proficiency with 3D objects. Of the 33 participants we had in our study, 10 were 3D artists, 18 had some proficiency with 3D objects and 5 had no prior background."}, {"title": "C. Visualization details", "content": "In addition to meshes used during evaluation, we made use of additional meshes and skyboxes for visualization purposes. These meshes are also under CC Attribution license, and can be found in Tab. 2."}]}