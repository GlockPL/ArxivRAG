{"title": "The Quest for the Right Mediator: A History, Survey, and Theoretical Grounding of Causal Interpretability", "authors": ["Aaron Mueller", "Jannik Brinkmann", "Millicent Li", "Samuel Marks", "Koyena Pal", "Nikhil Prakash", "Can Rager", "Aruna Sankaranarayanan", "Arnab Sen Sharma", "Jiuding Sun", "Eric Todd", "David Bau", "Yonatan Belinkov"], "abstract": "Interpretability provides a toolset for understanding how and why neural networks behave\nin certain ways. However, there is little unity in the field: most studies employ ad-hoc eval-\nuations and do not share theoretical foundations, making it difficult to measure progress\nand compare the pros and cons of different techniques. Furthermore, while mechanistic\nunderstanding is frequently discussed, the basic causal units underlying these mechanisms\nare often not explicitly defined. In this paper, we propose a perspective on interpretability\nresearch grounded in causal mediation analysis. Specifically, we describe the history and\ncurrent state of interpretability taxonomized according to the types of causal units (medi-\nators) employed, as well as methods used to search over mediators. We discuss the pros\nand cons of each mediator, providing insights as to when particular kinds of mediators and\nsearch methods are most appropriate depending on the goals of a given study. We argue that\nthis framing yields a more cohesive narrative of the field, as well as actionable insights for\nfuture work. Specifically, we recommend a focus on discovering new mediators with better\ntrade-offs between human-interpretability and compute-efficiency, and which can uncover\nmore sophisticated abstractions from neural networks than the primarily linear mediators\nemployed in current work. We also argue for more standardized evaluations that enable\nprincipled comparisons across mediator types, such that we can better understand when\nparticular causal units are better suited to particular use cases.", "sections": [{"title": "1 Introduction", "content": "To understand how neural networks (NNs) generalize, we must understand the causes of their behavior. These\ncauses include inputs, but also the intermediate computations of the network. How can we understand what\nthese computations represent, such that we can arrive at a deeper algorithmic understanding of how and\nwhy models behave the way they do? For example, if a model decides to refuse a user's request, was the\nrefusal mediated by an underlying concept of toxicity, or by the presence of superficial correlates of toxicity\n(such as the mention of particular demographic groups)? The former would be significantly more likely to\nrobustly and safely generalize. These questions motivate the field of causal interpretability, where we aim to\nextract causal graphs explaining how intermediate NN computations mediate model outputs.\nThis survey takes an opinionated stance on interpretability research: we ground the state of the field through\nthe lens of causal mediation analysis (\u00a72). We start by presenting a history of causal interpretability for\nneural networks (\u00a73), from backpropagation (Rumelhart et al., 1986) to the beginning of the current causal\nand mechanistic interpretability wave.\nWe then survey common mediators (units of causal analysis) used in causal interpretability studies (\u00a74),\ndiscussing the pros and cons of each mediator type. Should one analyze individual neurons? Full MLP\noutput vectors? Model subgraphs? More broadly: what is the right unit of abstraction for analyzing"}, {"title": "2 Preliminaries", "content": "The counterfactual theory of causality. Lewis (1973) poses that a causal dependence holds iff the\nfollowing condition holds:\n\u201cAn event E causally depends on C [iff] (i) if C had occurred, then E would have occurred,\nand (ii) if C had not occurred, then E would not have occurred.\u201d\nLewis (1986) extends this definition of causal dependence to be whether there is a causal chain linking\nC to E; a causal chain is a connected series of causes and effects that proceeds from an initial event to a\nfinal one, with potentially many intermediate events between them. This idea was later extended from a\nbinary notion of whether the effect happens to a more nuanced notion of causes having influence on how or\nwhen events occur (Lewis, 2000). Other work defines notions of cause and effect as continuous measurable\nquantities (Pearl, 2000); this includes direct and indirect effects (Robins & Greenland, 1992; Pearl, 2001),\nwhich are common metrics in causal interpretability studies.\nCausal abstractions in neural network interpretability. Causal interpretability is based on the ab-\nstraction of causal graphs. These graphs consist of nodes, which can be inputs, causes, actions, compu-\ntations, transformations, among other events. They also consist of directed edges, which represent causal\nrelationships between nodes; the source of the edge is the cause, and the target of the edge is the effect. If\nwe are given an input cause x and output effect y, there may exist many causal nodes between them; these\nintermediate nodes are called mediators.\nIn the causality literature, a mechanism is defined as a causal chain from cause C to effect E. The mecha-\nnistic interpretability literature, while closely related to causal interpretability, does not enforce this causally-\ngrounded definition of mechanism: mechanistic interpretability is often defined as reverse-engineering neural\nnetworks to better understand how and why they behave in certain ways (cf. Miller et al., 2024; Nanda et al.,\n2023). The overlap between mechanistic and causal interpretability is significant, but not total: for example,\nsparse autoencoders (Bricken et al., 2023; Cunningham et al., 2024) are correlational, but many methods,\nsuch as circuit discovery (Elhage et al., 2021; Conmy et al., 2023) and alignment search (Geiger et al., 2021;\n2024), employ methods to causally implicate model components (or other abstractions discovered in poten-\ntially correlational ways) in model behavior. We believe that the causality-based definition of mechanism\nis a useful one that makes precise the main challenge of mechanistic interpretability to reverse-engineer\nan algorithmic understanding of neural network behaviors, where \"algorithm\" is essentially equivalent to a\ncomplete causal graph explaining how a model will generalize.\nThe abstraction of causal graphs extends naturally to neural networks: we can treat the computation graph\nof a neural network as the full causal graph which explains how inputs x are transformed into a probability\ndistribution over outputs y. In this case, all model components\u00b9 can be viewed as causal nodes that mediate\nthe transformation of x to y. This is discussed in detail in \u00a74.\nCounterfactual interventions. In interpretability, \u201ccausal method\" generally refers to a method that\nemploys counterfactual interventions (Lewis, 1973) to some part of the model or its inputs. Much early"}, {"title": "3 A History of Causal Interpretability", "content": "Interpretability at the beginning of deep learning. In 1986, Rumelhart et al. published an algorithm\nfor backpropagation and an analysis of this algorithm. This enabled and massively popularized research in\ntraining multi-layer perceptrons now often called feedforward layers. This paper arguably represents the\nfirst mechanistic interpretability study: the authors evaluated their method by inspecting each activation\nand weight in the neural network, and observing whether the learned algorithm corresponded to the human\nintuition of how the task should be performed. In other words, they reverse-engineered the algorithm of the\nnetwork by labeling the rules encoded by each neuron and weight!\nThroughout the 1990s and early 2000s, the idea of extracting rules from the parameters and activations\nof NNs remained popular. At first, this was a manual process: networks were either small enough to be\nmanually interpreted (Rumelhart et al., 1986; McClelland & Rumelhart, 1985) or interpreted with the aid of\ncarefully crafted datasets (Elman, 1989; 1990; 1991); alternatively, researchers could prune them (Mozer &\nSmolensky, 1988; Karnin, 1990) to a sufficiently small size to be manually interpretable. Later, researchers\nproposed techniques for automatically extracting rules (Hayashi, 1990) or decision trees from NNs (Craven\n& Shavlik, 1994; 1995; Krishnan et al., 1999; Boz, 2002) often after the network had been pruned. At this\npoint, interest in causal methods based on interventions had not yet been established, as networks were often\nsmall and/or simple enough to directly understand without significant abstraction. Nonetheless, as the size\nof neural networks scaled up, the number of rules encoded in a network increased; thus, rule/decision tree"}, {"title": "Large-scale pre-trained models.", "content": "The 2010s were a time of rapid change in machine learning. In 2012,\nthe first large-scale pre-trained neural network, AlexNet (Krizhevsky et al., 2012), was released. Not long\nafter, pre-trained word embeddings became common in natural language processing (Mikolov et al., 2013a;b;\nPennington et al., 2014), and further pre-trained deep networks followed (He et al., 2016). These were based\non ideas from deep learning. This represented a significant paradigm shift: formerly, each study would build\nad-hoc models which were not shared across studies, but which were generally more transparent.3 After\n2012, there was a transition toward using a shared collection of significantly larger and more capable-but\nalso more opaque models. This raised new questions on what was encoded in the representations of these\nshared scientific artifacts. The rapid scaling of these models rendered old rule extraction methods either in-\ntractable or made its results difficult to interpret; thus, interpretability methods in the early 2010s tended to\nprominently feature scalable and relatively fast correlational methods, including visualizations (Zeiler & Fer-\ngus, 2014) and saliency maps (Simonyan et al., 2014). This trend continued into 2014-2015, when recurrent\nneural network-based (Elman, 1990) language models (Mikolov et al., 2010) began to overtake statistical\nmodels in performance (Bahdanau et al., 2015); for example, visualizing RNN and LSTM (Hochreiter &\nSchmidhuber, 1997) hidden states was proposed as a way to better understand their incremental processing\n(Karpathy et al., 2016; Strobelt et al., 2017).\nAt the same time, interpretability methods started to embrace auxiliary (correlational) models\u2014for example,\nLIME (Ribeiro et al., 2016a;b) and Anchors (Ribeiro et al., 2018). These models aimed to learn local\ndecision boundaries, or some human-interpretable simplified representation of a model's behavior. Other\nworks interpreted predictions via feature importance measures like SHAP (Lundberg & Lee, 2017). Influence\nfunctions (Koh & Liang, 2017) traced the model's behavior back to specific instances from the training data.\nAnother line of work also seeked to directly manipulate intermediate concepts to control model behavior at\ntest time (Koh et al., 2020). The primary difference between these visualization-/correlation-/input-based\nmethods and current methods is that these methods prioritize discovering high-level patterns about responses\nto particular kinds of inputs, such that we can generate hypotheses as to the types of input concepts these\nmodels are sensitive to. In contrast, current work prioritizes highly localized and causal explanations of\nhow and in which regions of the computation graph models translate particular inputs into general output\nbehaviors."}, {"title": "The rise of causal interpretability.", "content": "2017-2019 featured perhaps the largest architectural shift (among many) in machine learning methods at\nthis time: Transformers (Vaswani et al., 2017) were released and quickly became popular due to scalability\nand high performance. This led directly to the first successful large-scale pretrained language models, such as\n(Ro)BERT(a) (Devlin et al., 2019; Liu et al., 2019b) and GPT-2 (Radford et al., 2019). These significantly\noutperformed prior models, though it was unclear why and at this scale, analyzing neural networks at\nthe neuron level using past techniques had long become intractable. This combination of high performance\nand little mechanistic understanding created demand for interpretability techniques that allowed us to see\nhow language models had learned to perform so well. Hence, correlational probing methods rose to meet\nthis demand: here, classifiers are trained on intermediate activations to extract some target phenomenon.\nProbing classifiers were used to investigate the latent morphosyntactic structures encoded in static word\nembeddings (K\u00f6hn, 2015; Gupta et al., 2015) or intermediate hidden representations in pre-trained language\nmodels-for example, in neural machine translation systems (Shi et al., 2016; Belinkov et al., 2017; Conneau\net al., 2018) and pre-trained language models (Hewitt & Manning, 2019; Hewitt et al., 2021; Lakretz et al.,\n2019; 2021). However, probing classifiers lack consistent baselines, and the claims made in these studies\n2017-2018 featured the first hints of our current wave of causal\ninterpretability, with research that directly investigated intervening on activations and manipulating neurons.\nFor example, Giulianelli et al. (2018) trained a probing classifier, but then used gradients from the probe to\nmodify the activations of the network. Other studies analyzed the functional role of individual neurons in\nstatic word embeddings (Li et al., 2017) or latent representations of generative adversarial networks (GANs;\nGoodfellow et al., 2014) by forcing certain neurons on or off (Bau et al., 2019b). The idea of manipulating\nneurons to steer behaviors was then applied to downstream task settings, such as machine translation (Bau\net al., 2019a). The field was more widely popularized in 2020, when Vig et al. (2020) proposed a method for\nassigning causal importance scores to neurons and attention heads. It was an application of the counterfactual\ntheory of causality (Lewis, 1973; 1986), as well as Pearl's operationalization and measurements of individual\ncausal nodes' effect sizes (Pearl, 2001; 2000). This encouraged a new line of work that aimed to faithfully\nlocalize model behaviors to specific components, such as neurons or attention heads- -an idea that would\nbecome foundational to current causal and mechanistic interpretability research.\nAt the same time, however, researchers began to realize the significant performance improvements that could\nbe gained by massively increasing the number of parameters and training corpus sizes of neural networks\n(Brown et al., 2020; Kaplan et al., 2020). Massively increasing model sizes resulted in more interesting\nsubjects of study, but also rendered causal interpretability significantly more difficult just as its popularity\nbegan. Thus, a primary challenge of causal interpretability has been to balance the often-contradictory goals\nof (i) obtaining a causally efficacious understanding of how and why models behave in a given manner, while\nalso (ii) designing methods that are efficient enough to scale to ever-larger models.\nPresently, there exist many subfields of interpretability that propose and apply causal methods to understand\nwhich model components contribute to an observed model behavior (e.g., Elhage et al., 2021; Geiger et al.,\n2021; Conmy et al., 2023). Recently, there have also been efforts to discover more human-interpretable\nmediators by moving toward latent-space structures aside from (collections of) neurons (Cunningham et al.,\n2024; Bricken et al., 2023; Wu et al., 2023). These methods and their applications are the focus of the survey\nthat follows."}, {"title": "4 Selecting a mediator type", "content": "In this section, we discuss different types of causal mediators in neural networks, and the pros and cons of\neach. Figure 2 visualizes a computation graph, and units thereof that are often used as mediators in causal\ninterpretability studies. In causal interpretability, we often do not want to treat the full computation graph\nas the final causal graph, as it is large and difficult to directly interpret. Thus, we typically want to build\nhigher-level causal abstractions that capture only the most important mediators, and/or where each causal\nnode is human-interpretable. It is also possible to group components together into a single causal node,\nmeaning there are many possible mediators of a given type.\nOne possible mediator type is a full layer l-typically the output of a layer (\u00a74.1). This generally refers\nto a vector al composed of activations af, where we will refer to each ai as a neuron.4 One can also\nuse the output vector of an intermediate submodule within the layer (e.g., an MLP), rather than the\noutput of the whole layer. For example, in Transformers (Vaswani et al., 2017),5 a layer typically consists\nof two submodules: a multi-layer perceptron (MLP) and an attention block, which can be arranged either\nsequentially or in parallel. The output of these submodules is also a vector of activations, so we will refer to\ntheir individual dimensions as neurons as well.6\nOne can also use single neurons or sets of neurons as a mediator (\u00a74.2). If we use a set of neurons (possibly of\nsize 1) {ai, aj,...} from a vector a, this is referred to as a basis-aligned subspace of a. A one-dimensional\nbasis-aligned subspace is equivalent to a neuron; we will use basis-aligned subspace primarily to refer to\nneuron groups of size > 1. Basis alignment is a key concept: values that align with neuron directions will\nbe discoverable without any external modules appended to the original computation graph. For example, it\nis straightforward to exhaustively search over and intervene on single neurons; it is less tractable, but still\ntheoretically possible, to enumerate all 2n possible combinations of neurons without using any additional\nparameters.\nHowever, causally relevant features are not guaranteed to be aligned with neurons in activation space; indeed,\nthere are many cases where human-interpretable features correspond to subspaces that are not spanned by a\n(set of) neuron(s) (Elhage et al., 2022b; Bricken et al., 2023). Thus, in recent studies, it is common to study\nnon-basis-aligned spaces (\u00a74.3). Each channel of a non-basis-aligned subspace can be defined as weighted\nlinear combination of neuron activations. For example, to obtain a non-basis-aligned direction, we could\nlearn coefficients a and \u1e9e to weight activations a\u017c and aj (optionally with a bias term b):\nd = a. a\u00a1 + \u03b2\u03b1j + ... + b\n(2)\nNote that these new constants a and \u1e9e are not part of the original computation graph. This means that\ndiscovering directions often requires external components that weight components from the computation\ngraph in some way\u2014e.g., classifiers or autoencoders (\u00a75.2).\nThe primary trade-off between these mediator types is their granularity and number. This section proceeds\nin order of increasing granularity and quantity."}, {"title": "4.1 Full layers and submodules.", "content": "Full layers and submodules are relatively coarse-grained mediators. Thus, they are a common starting point if\none does not know where in a model a particular type of computation is happening. Early probing classifiers\nstudied the information encoded in full layers (Shi et al., 2016; Hupkes et al., 2018; Belinkov et al., 2017;\nConneau et al., 2018; Hewitt & Manning, 2019; Giulianelli et al., 2018), and recent studies that leverage\nclassifiers as part of causal techniques still frequently do the same (e.g., Elazar et al., 2021; Marks & Tegmark,"}, {"title": "4.2 Basis-aligned subspaces", "content": "Neurons. Compared to full layers and submodules, neurons represent more fine-grained components within\nneural networks that could feasibly represent individual features (though we discuss below that this is not\noften the case due to polysemanticity). Individual neurons can be considered the smallest meaningful unit\nwithin a neural network; an activation from a neuron is simply a scalar corresponding to a single dimension\n(1-dimensional subspace) of a hidden representation vector. Each neuron can differ from another based on\nits functional role in the network; for instance, Bau et al. (2020) locate neurons in a GAN responsible for\ngenerating specific types of objects in images, such as trees or windows, and verify this causally by ablating\nor artificially activating those neurons.\nNeurons are a natural choice for mediator, as they are both fine-grained and easy to exhaustively iterate over\n(see \u00a75.1). However, a major disadvantage of using neuron-based interpretability methods is polysemanticity.\nIndividual neurons are often polysemantic\u2014i.e. they respond to multiple seemingly unrelated inputs (Arora\net al., 2018). For example, if the same neuron were sensitive to capitalized words, animal names, one-digit\nnumbers, among other phenomena, it would be difficult to disentangle each of these individual patterns\nsuch that we can assign a coherent textual label to the neuron. Elhage et al. (2022b) investigate this\nphenomenon and suggest that neural networks represent features through linear superposition, where they\nrepresent features along non-basis-aligned linear subspaces, resulting in interpretable units being smeared\nacross multiple neurons. In other words, in an activation vector of size n, a model can encode m\u226b n\nconcepts as linear directions (Park et al., 2023), such that only a sparse subset of concepts are active given\na particular input.\nBasis-aligned multi-dimensional subspaces. The computations of individual neurons are not entirely\nindependent: it may often be the case that sets of neurons compose to encode some concept. For example,\nin language models, localized subsets of neurons can be implicated in encoding gender bias (Vig et al., 2020),\nand implementing fundamental latent linguistic phenomena (Finlayson et al., 2021; Mueller et al., 2022; Bau\net al., 2019a; Lakretz et al., 2019). Thus, some initial causal interpretability work employed heuristic-based\nsearches over sets of neuron responsible for some behavior (e.g., Bau et al., 2019b; Vig et al., 2020; Cao et al.,\n2021). This is a generalization of individual neurons as mediators, where multiple dimensions in activation\nspace are intervened upon simultaneously."}, {"title": "4.3 Non-basis-aligned spaces", "content": "Non-basis-aligned multi-dimensional subspaces. Due to their polysemanticity, neurons, attention\nheads, and sets thereof do not necessarily correspond to cleanly interpretable features or concepts. For\nexample, it is common that individual neurons activate on many seemingly unrelated inputs (Elhage et al.,\n2022b), and this issue cannot be cleanly resolved by adding more dimensions. This is because the features\nmay actually be encoded in directions or subspaces that are not aligned to neuron bases (Mikolov et al.,\n2013a; Arora et al., 2016).\nTo overcome this disadvantage, one can generalize causal mediators to include arbitrary non-neuron-basis-\naligned activation subspaces. This allows us to capture more sophisticated causal abstractions encoded in\nlatent space, such as causal nodes corresponding to greater-than relationships (Wu et al., 2023), or equality\nrelationships (Geiger et al., 2024). A common way of locating these is through learned rotation operations\n(Geiger et al., 2021; 2024), which preserve linearities and therefore are still in the activation subspace.\nThe primary advantage of considering an arbitrary subspace as a mediator is its expressivity: subspaces often\ncapture distributed abstractions that are not fully captured by a single neuron. However, they are generally"}, {"title": "4.4 Non-linear Mediators", "content": "Non-basis-aligned directions/subspaces are the most general linear mediator type. However, recent work\nhas demonstrated that some features in language models can be represented non-linearly and/or using mul-\ntiple dimensions. For example, there exist circular features representing days of the week or months of the\nyear (Engels et al., 2024). Similarily, past work has found that many concepts can be more easily extracted\nusing non-linear probes (Liu et al., 2019a), and that non-linear concept erasure techniques tend to outperform\nstrictly linear techniques (Iskander et al., 2023; Ravfogel et al., 2022). However, in causal and mechanistic\ninterpretability, most work has thus far tended toward using linear representations as units of causal anal-\nsis. Thus, there is significant potential in future work for systematically locating non-linearly-represented\nfeatures-e.g., using group sparse autoencoders (Theodosis & Ba, 2023), which could isolate multiple direc-\ntions simultaneously, and/or probing and clustering techniques to identify multi-dimensional features (Engels\net al., 2024). Non-linear features have not been extensively studied, despite their expressivity; we therefore\nadvocate investigating these mediators in \u00a77."}, {"title": "5 Searching for task-relevant mediators", "content": "Once one has selected a task and a type of mediator, how does one identify task-relevant mediators of that\ntype? The answer depends largely on the type of mediator chosen. If NNs have only a finite set of mediators\nof the chosen type-as is the case for native model components such as neurons, layers, and submodules-one\ncould perform an exhaustive search over all possible mediators, choosing which to keep according to some\nmetric; \u00a75.1 discusses this approach. However, other mediator types, including non-basis-aligned directions\nand subspaces, carry a continuous space of possible mediators, rendering an exhaustive search impossible. A"}, {"title": "5.1 Exhaustive search over mediators", "content": "Suppose we are given a neural network with a finite set of candidate mediators {z}\\1, such as the set\nof all neurons. One way to identify task-relevant mediators from this set is to assign each mediator zi a\ntask-relevancy score S(zi) and then select the mediators with the top scores. This generally entails iterating\nover each candidate mediator zi, setting its activation to some counterfactual value (either from a different\ninput where the answer is flipped, or a value that destroys the information within the neuron, such as its\nmean value), and then measuring how much this intervention changes the output. For example, Vig et al.\n(2020) and Finlayson et al. (2021) perform counterfactual interventions to the activation of each neuron\nindividually and then quantify how much each neuron changes the probability of correct completions. The\ntask relevancy score S(zi) is typically the indirect effect (IE; Pearl, 2001; Robins & Greenland, 1992), as\ndefined in Eq. 1. This metric is based on the notion of counterfactual dependence, where we measure the\ndifference in some output metric m before and after intervening on a given component zi.\nExhaustive searches have many advantages: their results are comprehensive, causally efficacious, and rela-\ntively conceptually precise if our mediators are fine-grained units, like neurons. They are also open-ended,\nmeaning that we are not required to have a pre-existing causal hypothesis as to how a model performs the\ntask: we may simply ablate, artificially activate, or otherwise influence a component's activations, and then\nobserve how it changes the output behavior or probability of some continuation. Due to these advantages,\nthis method is the most common when we have a finite set of mediators-for example, in neuron-based\nanalyses (Vig et al., 2020; Geiger et al., 2021; Finlayson et al., 2021) or attention-head-based analyses (Vig\net al., 2020; Conmy et al., 2023; Syed et al., 2023).\nHowever, exhaustive searches also have two significant disadvantages. The most obvious is that, in its exact\nform, an exhaustive search requires O(N) forward passes, which does not scale efficiently as models scale; this\nis both because the number of components increases, but also because the computational cost of inference\nscales with model size. This may be why exhaustive searches have not often been extended to sets of neurons\nor heads, as this results in a combinatorial explosion in the size of the search space. Searches over sets of\ncomponents can be approximated using greedy or top-k approaches, as in Vig et al. (2020), but this does not\nprovide a comprehensive solution to the problem of assigning causal credit to groups of components. That\nsaid, there exist fast linear approximations to activation patching that are technically not causal and not\nalways accurate, but that only require O(1) forward and backward passes\u2014most prominently, attribution\npatching (Kram\u00e1r et al., 2024; Syed et al., 2023) and improved versions thereof inspired by integrated\ngradients (Sundararajan et al., 2017; Marks et al., 2024; Hanna et al., 2024).\nThe second and more difficult disadvantage to overcome is that using exhaustive search constrains us to finite\nsets of mediators. Thus, this approach will not work well as-is if the search space is continuous (infinitely\nlarge). This is a key motivation behind the methods in the following subsection."}, {"title": "5.2 Optimizing over large spaces of mediators", "content": "For some types of mediators, the collection of candidate mediators is continuous or far too large to exhaus-\ntively search over; this precludes using methods described in \u00a75.1. To search over large but enumerable sets,\nsome researchers employ modified versions of exhaustive search, including greedy search methods (Vig et al.,\n2020) or manual searches (Wang et al., 2023). For continuous spaces, however, interpretability researchers\ngenerally use optimization. We taxonomize these optimization problems based on whether they require the"}, {"title": "5.2.1 Supervised mediator searches", "content": "By supervised mediator searches, we mean parametric approaches which require labeled task data and/or\nhuman-generated hypothesized causal graphs. For example, these methods might require the researcher to\npropose candidate intermediate concepts which they expect the model to use in performing some task, or\na candidate mechanism by which the model might complete the task. Others might simply require labeled\ndata for training classifiers.\nSupervised probing. In supervised probing approaches, the researcher proposes task-relevant concepts,\nand searches for mediators whose values are correlated with those concepts. Generally, one probes the hidden\nrepresentation vector at the end of a particular layer, and the probe searches over all possible subspaces and\ndirections therein for signals that are predictive of the labels. There exist many papers that employ probing\nclassifiers (Belinkov & Glass, 2019), though many of them do not validate the causal efficacy of the probe's\nfindings (Belinkov, 2021). A drawback of this is that NN units are often correlated with a concept without\ncausally mediating the concept. Thus this approach can return many false positives-i.e. it may return\nproposed mediators which do not actually causally mediate the concept in question (Hewitt & Liang, 2019;\nElazar et al., 2021; Amini et al., 2023; Belinkov, 2021).\nThus, much recent work complements supervised probing approaches with additional checks of causality-for\nexample, by applying causal mediation analysis to the directions identified by supervised probing (Marks\n& Tegmark, 2023; Nanda et al., 2023); by backpropagating from the classifier to modify the behavior of\nthe model (Giulianelli et al., 2018) or generate counterfactual representations (Tucker et al., 2021); or by\ndirectly comparing the probe's predictions to a causally grounded probe (Amini et al., 2023). Another\nline of work uses the directions discovered by probes to guard or erase information about a particular\nconcept from the model's representations. For example, a direction in a model's activation space that\nis most predictive of the target concept can be nullified via orthogonal projections, such that the model\ncan no longer use that information (Ravfogel et al., 2020); this process can then be repeated until linear\nguarding is achieved. Concept erasure and guarding can then be used to measure the causal importance"}, {"title": "5.2.2 Unsupervised mediator searches", "content": "Supervised search methods (see \u00a75.2.1) require specific hypotheses about the internal representations of neural\nnetworks. However, neural networks implement various behaviors, many of which may be counterintuitive to\nhumans and therefore more likely to be missed. For example, while Li et al. (2023) hypothesized a constant\nboard state representation in Othello, Nanda et al. (2023) later found that the model actually switches the\nboard state representation with every turn, taking the view of \"my pieces vs. opponent's pieces\" rather than\n\"black pieces vs. white pieces\". Therefore, it can be desirable to use techniques for searching for mediators\nwithout specifying a hypothesis ahead of time.\nHence, some studies employ unsupervised methods. Because these techniques are unsupervised, they return\na large but finite-collection of mediators. Unsupervised methods are largely correlative, meaning that the\ndiscovered mediators may not necessarily capture causally relevant or faithful aspects of the NN's computa-\ntion. However, the discovered mediators can then be implicated in NN computation post-hoc by employing\nadditional techniques, such as those from \u00a75.1, to select task-relevant mediators from this collection.\nFeature disentanglement using sparse autoencoders. Exhaustive search for meaningful non-basis-\naligned directions is impossible due to the infinite search space. The feature disentanglement literature\ntackles this problem by performing an unsupervised search for directions in neuron activations which both\n(1) capture the information encoded in the internal representations and (2) are disentangled from other\nmeaningful directions. Bengio et al. (2013) characterize disentangled representations as factors for variations\nin the training dataset. To identify these factors of variations, Sharkey et al. (2023) used sparse autoencoders\\"}]}