{"title": "The Quest for the Right Mediator: A History, Survey, and Theoretical Grounding of Causal Interpretability", "authors": ["Aaron Mueller", "Jannik Brinkmann", "Millicent Li", "Samuel Marks", "Koyena Pal", "Nikhil Prakash", "Can Rager", "Aruna Sankaranarayanan", "Arnab Sen Sharma", "Jiuding Sun", "Eric Todd", "David Bau", "Yonatan Belinkov"], "abstract": "Interpretability provides a toolset for understanding how and why neural networks behave in certain ways. However, there is little unity in the field: most studies employ ad-hoc evaluations and do not share theoretical foundations, making it difficult to measure progress and compare the pros and cons of different techniques. Furthermore, while mechanistic understanding is frequently discussed, the basic causal units underlying these mechanisms are often not explicitly defined. In this paper, we propose a perspective on interpretability research grounded in causal mediation analysis. Specifically, we describe the history and current state of interpretability taxonomized according to the types of causal units (mediators) employed, as well as methods used to search over mediators. We discuss the pros and cons of each mediator, providing insights as to when particular kinds of mediators and search methods are most appropriate depending on the goals of a given study. We argue that this framing yields a more cohesive narrative of the field, as well as actionable insights for future work. Specifically, we recommend a focus on discovering new mediators with better trade-offs between human-interpretability and compute-efficiency, and which can uncover more sophisticated abstractions from neural networks than the primarily linear mediators employed in current work. We also argue for more standardized evaluations that enable principled comparisons across mediator types, such that we can better understand when particular causal units are better suited to particular use cases.", "sections": [{"title": "1 Introduction", "content": "To understand how neural networks (NNs) generalize, we must understand the causes of their behavior. These causes include inputs, but also the intermediate computations of the network. How can we understand what these computations represent, such that we can arrive at a deeper algorithmic understanding of how and why models behave the way they do? For example, if a model decides to refuse a user's request, was the refusal mediated by an underlying concept of toxicity, or by the presence of superficial correlates of toxicity (such as the mention of particular demographic groups)? The former would be significantly more likely to robustly and safely generalize. These questions motivate the field of causal interpretability, where we aim to extract causal graphs explaining how intermediate NN computations mediate model outputs.\nThis survey takes an opinionated stance on interpretability research: we ground the state of the field through the lens of causal mediation analysis (\u00a72). We start by presenting a history of causal interpretability for neural networks (\u00a73), from backpropagation (Rumelhart et al., 1986) to the beginning of the current causal and mechanistic interpretability wave.\nWe then survey common mediators (units of causal analysis) used in causal interpretability studies (\u00a74), discussing the pros and cons of each mediator type. Should one analyze individual neurons? Full MLP output vectors? Model subgraphs? More broadly: what is the right unit of abstraction for analyzing"}, {"title": "2 Preliminaries", "content": "The counterfactual theory of causality. Lewis (1973) poses that a causal dependence holds iff the following condition holds:\n\u201cAn event E causally depends on C [iff] (i) if C had occurred, then E would have occurred,\nand (ii) if C had not occurred, then E would not have occurred.\u201d\nLewis (1986) extends this definition of causal dependence to be whether there is a causal chain linking C to E; a causal chain is a connected series of causes and effects that proceeds from an initial event to a final one, with potentially many intermediate events between them. This idea was later extended from a binary notion of whether the effect happens to a more nuanced notion of causes having influence on how or when events occur (Lewis, 2000). Other work defines notions of cause and effect as continuous measurable quantities (Pearl, 2000); this includes direct and indirect effects (Robins & Greenland, 1992; Pearl, 2001), which are common metrics in causal interpretability studies.\nCausal abstractions in neural network interpretability. Causal interpretability is based on the abstraction of causal graphs. These graphs consist of nodes, which can be inputs, causes, actions, computations, transformations, among other events. They also consist of directed edges, which represent causal relationships between nodes; the source of the edge is the cause, and the target of the edge is the effect. If we are given an input cause x and output effect y, there may exist many causal nodes between them; these intermediate nodes are called mediators.\nIn the causality literature, a mechanism is defined as a causal chain from cause C to effect E. The mechanistic interpretability literature, while closely related to causal interpretability, does not enforce this causally-grounded definition of mechanism: mechanistic interpretability is often defined as reverse-engineering neural networks to better understand how and why they behave in certain ways (cf. Miller et al., 2024; Nanda et al., 2023). The overlap between mechanistic and causal interpretability is significant, but not total: for example, sparse autoencoders (Bricken et al., 2023; Cunningham et al., 2024) are correlational, but many methods, such as circuit discovery (Elhage et al., 2021; Conmy et al., 2023) and alignment search (Geiger et al., 2021; 2024), employ methods to causally implicate model components (or other abstractions discovered in potentially correlational ways) in model behavior. We believe that the causality-based definition of mechanism is a useful one that makes precise the main challenge of mechanistic interpretability to reverse-engineer an algorithmic understanding of neural network behaviors, where \"algorithm\" is essentially equivalent to a complete causal graph explaining how a model will generalize.\nThe abstraction of causal graphs extends naturally to neural networks: we can treat the computation graph of a neural network as the full causal graph which explains how inputs x are transformed into a probability distribution over outputs y. In this case, all model components\u00b9 can be viewed as causal nodes that mediate the transformation of x to y. This is discussed in detail in \u00a74.\nCounterfactual interventions. In interpretability, \u201ccausal method\" generally refers to a method that employs counterfactual interventions (Lewis, 1973) to some part of the model or its inputs. Much early"}, {"title": "3 A History of Causal Interpretability", "content": "Interpretability at the beginning of deep learning. In 1986, Rumelhart et al. published an algorithm for backpropagation and an analysis of this algorithm. This enabled and massively popularized research in training multi-layer perceptrons now often called feedforward layers. This paper arguably represents the first mechanistic interpretability study: the authors evaluated their method by inspecting each activation and weight in the neural network, and observing whether the learned algorithm corresponded to the human intuition of how the task should be performed. In other words, they reverse-engineered the algorithm of the network by labeling the rules encoded by each neuron and weight!\nThroughout the 1990s and early 2000s, the idea of extracting rules from the parameters and activations of NNs remained popular. At first, this was a manual process: networks were either small enough to be manually interpreted (Rumelhart et al., 1986; McClelland & Rumelhart, 1985) or interpreted with the aid of carefully crafted datasets (Elman, 1989; 1990; 1991); alternatively, researchers could prune them (Mozer & Smolensky, 1988; Karnin, 1990) to a sufficiently small size to be manually interpretable. Later, researchers proposed techniques for automatically extracting rules (Hayashi, 1990) or decision trees from NNs (Craven & Shavlik, 1994; 1995; Krishnan et al., 1999; Boz, 2002) often after the network had been pruned. At this point, interest in causal methods based on interventions had not yet been established, as networks were often small and/or simple enough to directly understand without significant abstraction. Nonetheless, as the size of neural networks scaled up, the number of rules encoded in a network increased; thus, rule/decision tree\nLarge-scale pre-trained models. The 2010s were a time of rapid change in machine learning. In 2012, the first large-scale pre-trained neural network, AlexNet (Krizhevsky et al., 2012), was released. Not long after, pre-trained word embeddings became common in natural language processing (Mikolov et al., 2013a;b; Pennington et al., 2014), and further pre-trained deep networks followed (He et al., 2016). These were based on ideas from deep learning. This represented a significant paradigm shift: formerly, each study would build ad-hoc models which were not shared across studies, but which were generally more transparent.3 After 2012, there was a transition toward using a shared collection of significantly larger and more capable-but also more opaque models. This raised new questions on what was encoded in the representations of these shared scientific artifacts. The rapid scaling of these models rendered old rule extraction methods either intractable or made its results difficult to interpret; thus, interpretability methods in the early 2010s tended to prominently feature scalable and relatively fast correlational methods, including visualizations (Zeiler & Fer-gus, 2014) and saliency maps (Simonyan et al., 2014). This trend continued into 2014-2015, when recurrent neural network-based (Elman, 1990) language models (Mikolov et al., 2010) began to overtake statistical models in performance (Bahdanau et al., 2015); for example, visualizing RNN and LSTM (Hochreiter & Schmidhuber, 1997) hidden states was proposed as a way to better understand their incremental processing (Karpathy et al., 2016; Strobelt et al., 2017).\nAt the same time, interpretability methods started to embrace auxiliary (correlational) models\u2014for example, LIME (Ribeiro et al., 2016a;b) and Anchors (Ribeiro et al., 2018). These models aimed to learn local decision boundaries, or some human-interpretable simplified representation of a model's behavior. Other works interpreted predictions via feature importance measures like SHAP (Lundberg & Lee, 2017). Influence functions (Koh & Liang, 2017) traced the model's behavior back to specific instances from the training data. Another line of work also seeked to directly manipulate intermediate concepts to control model behavior at test time (Koh et al., 2020). The primary difference between these visualization-/correlation-/input-based methods and current methods is that these methods prioritize discovering high-level patterns about responses to particular kinds of inputs, such that we can generate hypotheses as to the types of input concepts these models are sensitive to. In contrast, current work prioritizes highly localized and causal explanations of how and in which regions of the computation graph models translate particular inputs into general output behaviors.\n2017-2019 featured perhaps the largest architectural shift (among many) in machine learning methods at this time: Transformers (Vaswani et al., 2017) were released and quickly became popular due to scalability and high performance. This led directly to the first successful large-scale pretrained language models, such as (Ro)BERT(a) (Devlin et al., 2019; Liu et al., 2019b) and GPT-2 (Radford et al., 2019). These significantly outperformed prior models, though it was unclear why and at this scale, analyzing neural networks at the neuron level using past techniques had long become intractable. This combination of high performance and little mechanistic understanding created demand for interpretability techniques that allowed us to see how language models had learned to perform so well. Hence, correlational probing methods rose to meet this demand: here, classifiers are trained on intermediate activations to extract some target phenomenon. Probing classifiers were used to investigate the latent morphosyntactic structures encoded in static word embeddings (K\u00f6hn, 2015; Gupta et al., 2015) or intermediate hidden representations in pre-trained language models-for example, in neural machine translation systems (Shi et al., 2016; Belinkov et al., 2017; Conneau et al., 2018) and pre-trained language models (Hewitt & Manning, 2019; Hewitt et al., 2021; Lakretz et al., 2019; 2021). However, probing classifiers lack consistent baselines, and the claims made in these studies\nThe rise of causal interpretability. 2017-2018 featured the first hints of our current wave of causal interpretability, with research that directly investigated intervening on activations and manipulating neurons. For example, Giulianelli et al. (2018) trained a probing classifier, but then used gradients from the probe to modify the activations of the network. Other studies analyzed the functional role of individual neurons in static word embeddings (Li et al., 2017) or latent representations of generative adversarial networks (GANs; Goodfellow et al., 2014) by forcing certain neurons on or off (Bau et al., 2019b). The idea of manipulating neurons to steer behaviors was then applied to downstream task settings, such as machine translation (Bau et al., 2019a). The field was more widely popularized in 2020, when Vig et al. (2020) proposed a method for assigning causal importance scores to neurons and attention heads. It was an application of the counterfactual theory of causality (Lewis, 1973; 1986), as well as Pearl's operationalization and measurements of individual causal nodes' effect sizes (Pearl, 2001; 2000). This encouraged a new line of work that aimed to faithfully localize model behaviors to specific components, such as neurons or attention heads- -an idea that would become foundational to current causal and mechanistic interpretability research.\nAt the same time, however, researchers began to realize the significant performance improvements that could be gained by massively increasing the number of parameters and training corpus sizes of neural networks (Brown et al., 2020; Kaplan et al., 2020). Massively increasing model sizes resulted in more interesting subjects of study, but also rendered causal interpretability significantly more difficult just as its popularity began. Thus, a primary challenge of causal interpretability has been to balance the often-contradictory goals of (i) obtaining a causally efficacious understanding of how and why models behave in a given manner, while also (ii) designing methods that are efficient enough to scale to ever-larger models.\nPresently, there exist many subfields of interpretability that propose and apply causal methods to understand which model components contribute to an observed model behavior (e.g., Elhage et al., 2021; Geiger et al., 2021; Conmy et al., 2023). Recently, there have also been efforts to discover more human-interpretable mediators by moving toward latent-space structures aside from (collections of) neurons (Cunningham et al., 2024; Bricken et al., 2023; Wu et al., 2023). These methods and their applications are the focus of the survey that follows."}, {"title": "4 Selecting a mediator type", "content": "In this section, we discuss different types of causal mediators in neural networks, and the pros and cons of each. Figure 2 visualizes a computation graph, and units thereof that are often used as mediators in causal interpretability studies. In causal interpretability, we often do not want to treat the full computation graph as the final causal graph, as it is large and difficult to directly interpret. Thus, we typically want to build higher-level causal abstractions that capture only the most important mediators, and/or where each causal node is human-interpretable. It is also possible to group components together into a single causal node, meaning there are many possible mediators of a given type.\nOne possible mediator type is a full layer l-typically the output of a layer (\u00a74.1). This generally refers to a vector al composed of activations af, where we will refer to each ai as a neuron.4 One can also use the output vector of an intermediate submodule within the layer (e.g., an MLP), rather than the output of the whole layer. For example, in Transformers (Vaswani et al., 2017),5 a layer typically consists of two submodules: a multi-layer perceptron (MLP) and an attention block, which can be arranged either sequentially or in parallel. The output of these submodules is also a vector of activations, so we will refer to their individual dimensions as neurons as well.6\nOne can also use single neurons or sets of neurons as a mediator (\u00a74.2). If we use a set of neurons (possibly of size 1) {ai, aj,...} from a vector a, this is referred to as a basis-aligned subspace of a. A one-dimensional basis-aligned subspace is equivalent to a neuron; we will use basis-aligned subspace primarily to refer to neuron groups of size > 1. Basis alignment is a key concept: values that align with neuron directions will be discoverable without any external modules appended to the original computation graph. For example, it is straightforward to exhaustively search over and intervene on single neurons; it is less tractable, but still theoretically possible, to enumerate all 2n possible combinations of neurons without using any additional parameters.\nHowever, causally relevant features are not guaranteed to be aligned with neurons in activation space; indeed, there are many cases where human-interpretable features correspond to subspaces that are not spanned by a (set of) neuron(s) (Elhage et al., 2022b; Bricken et al., 2023). Thus, in recent studies, it is common to study non-basis-aligned spaces (\u00a74.3). Each channel of a non-basis-aligned subspace can be defined as weighted linear combination of neuron activations. For example, to obtain a non-basis-aligned direction, we could learn coefficients a and \u1e9e to weight activations a\u017c and aj (optionally with a bias term b):\nd = \\alpha \\cdot a\\_{i} + \\beta a\\_{j} + ... + b\nNote that these new constants a and \u1e9e are not part of the original computation graph. This means that discovering directions often requires external components that weight components from the computation graph in some way\u2014e.g., classifiers or autoencoders (\u00a75.2).\nThe primary trade-off between these mediator types is their granularity and number. This section proceeds in order of increasing granularity and quantity."}, {"title": "4.1 Full layers and submodules.", "content": "Full layers and submodules are relatively coarse-grained mediators. Thus, they are a common starting point if one does not know where in a model a particular type of computation is happening. Early probing classifiers studied the information encoded in full layers (Shi et al., 2016; Hupkes et al., 2018; Belinkov et al., 2017; Conneau et al., 2018; Hewitt & Manning, 2019; Giulianelli et al., 2018), and recent studies that leverage classifiers as part of causal techniques still frequently do the same (e.g., Elazar et al., 2021; Marks & Tegmark,\nIn some cases, coarse-grained mediators like these can inform methods for understanding factual recall in language models (Geva et al., 2023), or updating these factual associations (Meng et al., 2022; 2023). However, full layers encode many features and have many causal roles in a network, which makes it difficult to interpret how, exactly, relevant information is encoded in a layer (Conmy et al., 2023). Additionally, intervening on full layers or submodules often causes side effects outside the scope of the intervention (McGrath et al., 2023).\nThe primary advantage of using full layers as mediators is their small quantity and broad scope of information. This means that even slow or resource-intensive methods will generally be easy to apply to all layers. In some cases, this is enough granularity. However, an obvious disadvantage is that this mediator is generally opaque: even if we know that information is encoded in a layer somehow, it is unclear precisely how this information is encoded, composed, or used. Thus, layers and submodules have little explanatory power, and are better used as coarser starting points for later finer-grained investigations (e.g., Brinkmann et al., 2024; Geva et al., 2023) or for downstream applications such as model editing (Meng et al., 2022; 2023; Sharma et al., 2024; Gandikota et al., 2023; 2024)."}, {"title": "4.2 Basis-aligned subspaces", "content": "Neurons. Compared to full layers and submodules, neurons represent more fine-grained components within neural networks that could feasibly represent individual features (though we discuss below that this is not often the case due to polysemanticity). Individual neurons can be considered the smallest meaningful unit within a neural network; an activation from a neuron is simply a scalar corresponding to a single dimension (1-dimensional subspace) of a hidden representation vector. Each neuron can differ from another based on its functional role in the network; for instance, Bau et al. (2020) locate neurons in a GAN responsible for generating specific types of objects in images, such as trees or windows, and verify this causally by ablating or artificially activating those neurons.\nNeurons are a natural choice for mediator, as they are both fine-grained and easy to exhaustively iterate over (see \u00a75.1). However, a major disadvantage of using neuron-based interpretability methods is polysemanticity. Individual neurons are often polysemantic\u2014i.e. they respond to multiple seemingly unrelated inputs (Arora et al., 2018). For example, if the same neuron were sensitive to capitalized words, animal names, one-digit numbers, among other phenomena, it would be difficult to disentangle each of these individual patterns such that we can assign a coherent textual label to the neuron. Elhage et al. (2022b) investigate this phenomenon and suggest that neural networks represent features through linear superposition, where they represent features along non-basis-aligned linear subspaces, resulting in interpretable units being smeared across multiple neurons. In other words, in an activation vector of size n, a model can encode m\u226b n concepts as linear directions (Park et al., 2023), such that only a sparse subset of concepts are active given a particular input.\nBasis-aligned multi-dimensional subspaces. The computations of individual neurons are not entirely independent: it may often be the case that sets of neurons compose to encode some concept. For example, in language models, localized subsets of neurons can be implicated in encoding gender bias (Vig et al., 2020), and implementing fundamental latent linguistic phenomena (Finlayson et al., 2021; Mueller et al., 2022; Bau et al., 2019a; Lakretz et al., 2019). Thus, some initial causal interpretability work employed heuristic-based searches over sets of neuron responsible for some behavior (e.g., Bau et al., 2019b; Vig et al., 2020; Cao et al., 2021). This is a generalization of individual neurons as mediators, where multiple dimensions in activation space are intervened upon simultaneously."}, {"title": "4.3 Non-basis-aligned spaces", "content": "Non-basis-aligned multi-dimensional subspaces. Due to their polysemanticity, neurons, attention heads, and sets thereof do not necessarily correspond to cleanly interpretable features or concepts. For example, it is common that individual neurons activate on many seemingly unrelated inputs (Elhage et al., 2022b), and this issue cannot be cleanly resolved by adding more dimensions. This is because the features may actually be encoded in directions or subspaces that are not aligned to neuron bases (Mikolov et al., 2013a; Arora et al., 2016).\nTo overcome this disadvantage, one can generalize causal mediators to include arbitrary non-neuron-basis-aligned activation subspaces. This allows us to capture more sophisticated causal abstractions encoded in latent space, such as causal nodes corresponding to greater-than relationships (Wu et al., 2023), or equality relationships (Geiger et al., 2024). A common way of locating these is through learned rotation operations (Geiger et al., 2021; 2024), which preserve linearities and therefore are still in the activation subspace.\nThe primary advantage of considering an arbitrary subspace as a mediator is its expressivity: subspaces often capture distributed abstractions that are not fully captured by a single neuron. However, they are generally"}, {"title": "4.4 Non-linear Mediators", "content": "Non-basis-aligned directions/subspaces are the most general linear mediator type. However, recent work has demonstrated that some features in language models can be represented non-linearly and/or using mul-tiple dimensions. For example, there exist circular features representing days of the week or months of the year (Engels et al., 2024). Similarily, past work has found that many concepts can be more easily extracted using non-linear probes (Liu et al., 2019a), and that non-linear concept erasure techniques tend to outperform strictly linear techniques (Iskander et al., 2023; Ravfogel et al., 2022). However, in causal and mechanistic interpretability, most work has thus far tended toward using linear representations as units of causal anal-ysis. Thus, there is significant potential in future work for systematically locating non-linearly-represented features-e.g., using group sparse autoencoders (Theodosis & Ba, 2023), which could isolate multiple direc-tions simultaneously, and/or probing and clustering techniques to identify multi-dimensional features (Engels et al., 2024). Non-linear features have not been extensively studied, despite their expressivity; we therefore advocate investigating these mediators in \u00a77."}, {"title": "5 Searching for task-relevant mediators", "content": "Once one has selected a task and a type of mediator, how does one identify task-relevant mediators of that type? The answer depends largely on the type of mediator chosen. If NNs have only a finite set of mediators of the chosen type-as is the case for native model components such as neurons, layers, and submodules-one could perform an exhaustive search over all possible mediators, choosing which to keep according to some metric; \u00a75.1 discusses this approach. However, other mediator types, including non-basis-aligned directions and subspaces, carry a continuous space of possible mediators, rendering an exhaustive search impossible. A"}, {"title": "5.1 Exhaustive search over mediators", "content": "Suppose we are given a neural network with a finite set of candidate mediators {z}N1, such as the set of all neurons. One way to identify task-relevant mediators from this set is to assign each mediator zi a task-relevancy score S(zi) and then select the mediators with the top scores. This generally entails iterating over each candidate mediator zi, setting its activation to some counterfactual value (either from a different input where the answer is flipped, or a value that destroys the information within the neuron, such as its mean value), and then measuring how much this intervention changes the output. For example, Vig et al. (2020) and Finlayson et al. (2021) perform counterfactual interventions to the activation of each neuron individually and then quantify how much each neuron changes the probability of correct completions. The task relevancy score S(zi) is typically the indirect effect (IE; Pearl, 2001; Robins & Greenland, 1992), as defined in Eq. 1.12 This metric is based on the notion of counterfactual dependence, where we measure the difference in some output metric m before and after intervening on a given component zi.\nExhaustive searches have many advantages: their results are comprehensive, causally efficacious, and rela-tively conceptually precise if our mediators are fine-grained units, like neurons. They are also open-ended, meaning that we are not required to have a pre-existing causal hypothesis as to how a model performs the task: we may simply ablate, artificially activate, or otherwise influence a component's activations, and then observe how it changes the output behavior or probability of some continuation. Due to these advantages, this method is the most common when we have a finite set of mediators-for example, in neuron-based analyses (Vig et al., 2020; Geiger et al., 2021; Finlayson et al., 2021) or attention-head-based analyses (Vig et al., 2020; Conmy et al., 2023; Syed et al., 2023).\nHowever, exhaustive searches also have two significant disadvantages. The most obvious is that, in its exact form, an exhaustive search requires O(N) forward passes, which does not scale efficiently as models scale; this is both because the number of components increases, but also because the computational cost of inference scales with model size. This may be why exhaustive searches have not often been extended to sets of neurons or heads, as this results in a combinatorial explosion in the size of the search space. Searches over sets of components can be approximated using greedy or top-k approaches, as in Vig et al. (2020), but this does not provide a comprehensive solution to the problem of assigning causal credit to groups of components. That said, there exist fast linear approximations to activation patching that are technically not causal and not always accurate, but that only require O(1) forward and backward passes\u2014most prominently, attribution patching (Kram\u00e1r et al., 2024; Syed et al., 2023) and improved versions thereof inspired by integrated gradients (Sundararajan et al., 2017; Marks et al., 2024; Hanna et al., 2024).13\nThe second and more difficult disadvantage to overcome is that using exhaustive search constrains us to finite sets of mediators. Thus, this approach will not work well as-is if the search space is continuous (infinitely large). This is a key motivation behind the methods in the following subsection."}, {"title": "5.2 Optimizing over large spaces of mediators", "content": "For some types of mediators, the collection of candidate mediators is continuous or far too large to exhaus-tively search over; this precludes using methods described in \u00a75.1. To search over large but enumerable sets, some researchers employ modified versions of exhaustive search, including greedy search methods (Vig et al., 2020) or manual searches (Wang et al., 2023). For continuous spaces, however, interpretability researchers generally use optimization. We taxonomize these optimization problems based on whether they require the"}, {"title": "5.2.1 Supervised mediator searches", "content": "By supervised mediator searches, we mean parametric approaches which require labeled task data and/or human-generated hypothesized causal graphs. For example, these methods might require the researcher to propose candidate intermediate concepts which they expect the model to use in performing some task, or a candidate mechanism by which the model might complete the task. Others might simply require labeled data for training classifiers.\nSupervised probing. In supervised probing approaches, the researcher proposes task-relevant concepts, and searches for mediators whose values are correlated with those concepts. Generally, one probes the hidden representation vector at the end of a particular layer, and the probe searches over all possible subspaces and directions therein for signals that are predictive of the labels. There exist many papers that employ probing classifiers (Belinkov & Glass, 2019), though many of them do not validate the causal efficacy of the probe's findings (Belinkov, 2021). A drawback of this is that NN units are often correlated with a concept without causally mediating the concept. Thus this approach can return many false positives-i.e. it may return proposed mediators which do not actually causally mediate the concept in question (Hewitt & Liang, 2019; Elazar et al., 2021; Amini et al., 2023; Belinkov, 2021).\nThus, much recent work complements supervised probing approaches with additional checks of causality-for example, by applying causal mediation analysis to the directions identified by supervised probing (Marks & Tegmark, 2023; Nanda et al., 2023); by backpropagating from the classifier to modify the behavior of the model (Giulianelli et al., 2018) or generate counterfactual representations (Tucker et al., 2021); or by directly comparing the probe's predictions to a causally grounded probe (Amini et al., 2023). Another line of work uses the directions discovered by probes to guard or erase information about a particular concept from the model's representations. For example, a direction in a model's activation space that is most predictive of the target concept can be nullified via orthogonal projections, such that the model can no longer use that information (Ravfogel et al., 2020); this process can then be repeated until linear guarding is achieved. Concept erasure and guarding can then be used to measure the causal importance"}, {"title": "5.2.2 Unsupervised mediator searches", "content": "Supervised search methods (see \u00a75.2.1) require specific hypotheses about the internal representations of neural networks. However, neural networks implement various behaviors, many of which may be counterintuitive to humans and therefore more likely to be missed. For example, while Li et al. (2023) hypothesized a constant board state representation in Othello, Nanda et al. (2023) later found that the model actually switches the board state representation with every turn, taking the view of \"my pieces vs. opponent's pieces\" rather than \"black pieces vs. white pieces\". Therefore, it can be desirable to use techniques for searching for mediators without specifying a hypothesis ahead of time.\nHence, some studies employ unsupervised methods. Because these techniques are unsupervised, they return a large but finite-collection of mediators. Unsupervised methods are largely correlative, meaning that the discovered mediators may not necessarily capture causally relevant or faithful aspects of the NN's computa-tion. However, the discovered mediators can then be implicated in NN computation post-hoc by employing additional techniques, such as those from \u00a75.1, to select task-relevant mediators from this collection.\nFeature disentanglement using sparse autoencoders. Exhaustive search for meaningful non-basis-aligned directions is impossible due to the infinite search space. The feature disentanglement literature tackles this problem by performing an unsupervised search for directions in neuron activations which both (1) capture the information encoded in the internal representations and (2) are disentangled from other meaningful directions. Bengio et al. (2013) characterize disentangled representations as factors for variations in the training dataset. To identify these factors of variations, Sharkey et al. (2023) used sparse autoencoders (SAEs) to perform dictionary learning on a one-layer transformer, identifying a large (overcomplete) basis of features. SAEs are trained to reconstruct the input activations while only activating a sparse subset of dictionary features. Cunningham et al. (2024) then applied SAEs to language models and demonstrated that the observed dictionary features are highly interpretable and can be used to localize and edit model behavior. Since then, numerous researchers have explored this area (Templeton et al., 2024; Rajamanoharan et al.,"}, {"title": "6 Related Work", "content": "Causally-grounded interpretability surveys do not always focus on model internals, and surveys that focus on model internals do not necessarily require causal grounding. We give a brief overview of both types here.\nMechanistic/model-internal interpretability surveys. Some surveys catalogue studies that aim to understand the latent representations of neural networks (Belinkov & Glass, 2019; Danilevsky et al., 2020; Belinkov"}]}