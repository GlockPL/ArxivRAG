{"title": "EMPIRICAL EVALUATION OF PUBLIC HATESPEECH DATASETS", "authors": ["Sardar Jaf", "Basel Barakat"], "abstract": "Despite the extensive communication benefits offered by social media platforms, numerous challenges\nmust be addressed to ensure user safety. One of the most significant risks faced by users on\nthese platforms is targeted hate speech. Social media platforms are widely utilised for generating\ndatasets employed in training and evaluating machine learning algorithms for hate speech detection.\nHowever, existing public datasets exhibit numerous limitations, hindering the effective training\nof these algorithms and leading to inaccurate hate speech classification. This study provides a\ncomprehensive empirical evaluation of several public datasets commonly used in automated hate\nspeech classification. Through rigorous analysis, we present compelling evidence highlighting the\nlimitations of current hate speech datasets. Additionally, we conduct a range of statistical analyses\nto elucidate the strengths and weaknesses inherent in these datasets. This work aims to advance the\ndevelopment of more accurate and reliable machine learning models for hate speech detection by\naddressing the dataset limitations identified.", "sections": [{"title": "Introduction", "content": "Social media is one of the most widely used online medium for sharing and communication methods for people in\nmodern society, where people can easily share information, news, updates and their opinions on current trends. One of\nthe potential risks associated with easily sharing and publishing information by online users that is accessible worldwide\nis the integrity of the information. Particularly, the dissemination of hatespeech. Thus, one of the pressing needs for\nmany online platforms and users is to ensure published information on online platforms are free from hatespeech.\nTo address this need, numerous efforts have been made for the provision of datasets to be used in the design of AI\nsystems to efficiently and accurately detect, classify and remove hatespeech content. However, despite the availability\nof many datasets, which are crucial component for the development of AI powered hatespeech detection/classification\nsystems, the quality of those datasets are questionable. Generally, poor quality datasets would lead to poor AI systems.\nTherefore, in this paper, we focus on evaluating public hatespeech datasets that are based on social media platforms.\nOur aim is to empirically evaluate the quality of many public hatespeech datasets in order to assess their suitability for\nAI hatespeech classifiers.\nThe contributions of this study are as follows:\n\u2022 We offer the first empirical evaluation attempt of a large number of datasets.\n\u2022 We empirically demonstrate that the quality of dataset content has a greater positive impact on AI hatespeech\nclassification than factors such as content volume, context diversity, and data modalities.\n\u2022 We novel approach utilizes hatespeech dataset features to identify correlation between each feature and\nmachine learning classification performance. This approach has the potential to be generalised to datasets in\nother domain."}, {"title": "Literature Review", "content": "One of the primary sources for collecting big data for text analyses is social media platforms. These platforms have\nbeen used by researchers from different disciplines as a data collection source [1]. For academic research projects,\nsocial media data has been explored widely for research and practical applications of hatespeech detection, analyses\nand classification. As a result, many datasets have been compiled from various social media platforms for hatespeech\nprocessing. In this section, we will highlight core aspects of some of the published datasets and machine learning\napplications for the task of hatespeech classification.\nHatespeech datasets are largely produced by extracting content (e.g., text, images, memes, videos, emojis etc.) from\nsocial media platforms, online forums, blogs and various other online communities.\nTo develop machine learning approaches to process hatespeech (which involves data analyses, classification, visualisation\netc.), access to labelled corpora is essential. Since there is no commonly accepted benchmark corpus for processing\nhatespeech, authors usually collect from online platforms and annotate them using different annotation approaches. This\npractice resulted in considerable variation in the size of the published datasets, topics, domains, languages, hatespeech\ncategories, platforms, content types, etc. Some datasets are very large (containing over hundred thousand entries [2] [3]\n4]) whereas others are small (contain a few thousands entries [5] [6] [7]) or few hundreds entries [8]. The main reasons\nfor such data size variation are: (i) as in any text annotation, annotating hatespeech is an extremely time-consuming\nprocess, (ii) there are, usually, much fewer hateful than non-hateful (neutral) comments present in randomly sampled\ndata from social media platforms. Therefore, accomplishing this task necessitates the collection of extensive datasets\nthat can be annotated to identify a substantial number of hatespeech instances. The negative impact of this imbalanced\ndistribution of content types is that it would generally be difficult to build a balanced dataset, where there are equal\nsamples of hateful and neutral content.\nSome authors attempted to increase the sample size of hatespeech content whereas keeping the size of data instances\nto be annotated at a reasonable level, [9]\u00b9 proposed an approach to pre-select the text instances to be annotated by\nquerying an online platform (Twitter) for topics that are likely to contain a higher degree of hatespeech (e.g. \u201cIslam\nterror\"). The strength of this approach is it increases the proportion of hatespeech samples in the resulting dataset, and\nthus resulting in the possibility of achieving a balanced dataset. However, the limitation of this approach is it focuses\nthe resulting dataset on specific topics and certain subtypes of hatespeech (e.g. hatespeech targeting Muslims)[10].\nSince there is no commonly accepted benchmark corpus for hatespeech classification, authors usually collect and\nlabel their own data [10]. For this reason, most of the available datasets are based on content from one or few data\nsources. Some of the major sources of datasets are: Yahoo[11] [12] [4], Twitter[5] [9], Reddit [13], Qian et al.[14],\nYouTube[2][6], Facebook[6], Stormfront[15] or dynamically generated text[16]. The result of collecting data from\ndifferent online platforms for creating hatespeech dataset is that the dataset are likely to have different characteristics,\nand subtypes of hatespeech[10]. This is largely because of the nature and purpose of the online platforms, and thus they\nmay have special characteristics. For instance, a platform especially created for adolescents, one should expect quite\ndifferent types of hatespeech than on a a platform that is used by a cross-section of the general public since the resulting\ndifferent demographics will have an impact on the topics discussed and the language used [10].\nThe above issues related to hatespeech datasets have lead to the creation and availability of several datasets for the task\nof automated hatespeech classification. Recent approaches to hatespeech classification usually involves training machine learning\nalgorithm(s) on sample data.\""}, {"title": "Methodology", "content": "Hatespeech is gaining increasing attention from industry, government organisations and academia. The proliferation\nof information published on social media platforms provides the means to create datasets for processing, analysing,\ndetecting and classifying hatespeech content. Variations in the available datasets (e.g, in size, topic, domain, language,\nhatespeech categories, platform, content type, etc.) could be beneficial. However, it can make it challenging for\nresearchers to determine which dataset is suitable for training machine learning algorithms for hatespeech classification.\nFor example, a dataset based only on Twitter content may, or may not, be suitable for producing a generalisable machine\nlearning model that perform well on non-twitter data, such as YouTube comments.\nThe aim of this study is to evaluate multiple publicly available dataset to assess their suitability for training and testing\ndeep learning algorithm for hatespeech classification. We have selected ten datasets from hatespeechdata.com website,\nwhich is a widely used platform for hosting hatespeech dataset. Our goal in evaluating multiple dataset is to examine\ntwo application aspects of each dataset: (i) to examine the suitability of a dataset in testing the performance of a\ndeep learning based system for hatespeech classification, and (ii) to examine the suitability of a dataset in producing\ngeneralisable deep learning model by training a deep learning algorithm on a dataset but testing it on other dataset\nwith different domain, text, genre, and size. The selected dataset names, platforms where the data are collected from,\nand the publication years of the dataset are presented. We chose the evaluated datasets based on several\ncharacteristics: different platforms, dataset size, content type, length of individual text entry, and publication time.\nExamining dataset with content extracted from various platforms helps us to evaluate the generalisability of deep\nlearning algorithms. Similarly, the different years were chosen to ensure that the evaluation would be generalisable, as\nsome hateful terms might be more popular in specific years. We selected datasets with different sizes because machine\nlearning algorithm performance is usually dependent on dataset size.\nThe labelled content in each selected dataset varies from one dataset to another. The proposed dataset by [13], has\nvarious categories of abuse (e.g., targeted Identity, affiliation, and person), and counter speech. The published dataset\nby [5] has three types of content (classes) i.e., Hatespeech, Offensive language and Neither. The datasets, from [14]\nand [16], have only two classes: Hate/Not Hate and Offensive/Vulgar. Kennedy et al.'s dataset [2] content is focused on\nparticular categories (such as religions, three hate classes for races, and a hate score to indicate the hate level). The\ndataset from [3] contains text and images. They are labelled as \u201cHate\u201d or \u201cNot-Hate\u201d.\u201cHate\u201d content is further divided\ninto five classes of different types of hate. Waseem and Havoy's dataset [9], has three labels: \u201cSexism\u201d, \u201cRacism\", and\n\"None\".\nThe content of the available dataset has been labelled with different types of hatespeech. Therefore, there are inconsistent\nlabels between the dataset. In order to design and evaluate a supervised deep learning hatespeech classifier trained on\""}, {"title": "Data collection", "content": "Hatespeech is gaining increasing attention from industry, government organisations and academia. The proliferation\nof information published on social media platforms provides the means to create datasets for processing, analysing,\ndetecting and classifying hatespeech content. Variations in the available datasets (e.g, in size, topic, domain, language,\nhatespeech categories, platform, content type, etc.) could be beneficial. However, it can make it challenging for\nresearchers to determine which dataset is suitable for training machine learning algorithms for hatespeech classification.\nFor example, a dataset based only on Twitter content may, or may not, be suitable for producing a generalisable machine\nlearning model that perform well on non-twitter data, such as YouTube comments.\nThe aim of this study is to evaluate multiple publicly available dataset to assess their suitability for training and testing\ndeep learning algorithm for hatespeech classification. We have selected ten datasets from hatespeechdata.com website,\nwhich is a widely used platform for hosting hatespeech dataset. Our goal in evaluating multiple dataset is to examine\ntwo application aspects of each dataset: (i) to examine the suitability of a dataset in testing the performance of a\ndeep learning based system for hatespeech classification, and (ii) to examine the suitability of a dataset in producing\ngeneralisable deep learning model by training a deep learning algorithm on a dataset but testing it on other dataset\nwith different domain, text, genre, and size. The selected dataset names, platforms where the data are collected from,\nand the publication years of the dataset are presented in Table 1. We chose the evaluated datasets based on several\ncharacteristics: different platforms, dataset size, content type, length of individual text entry, and publication time.\nExamining dataset with content extracted from various platforms helps us to evaluate the generalisability of deep\nlearning algorithms. Similarly, the different years were chosen to ensure that the evaluation would be generalisable, as\nsome hateful terms might be more popular in specific years. We selected datasets with different sizes because machine\nlearning algorithm performance is usually dependent on dataset size.\nThe labelled content in each selected dataset varies from one dataset to another. The proposed dataset by [13], has\nvarious categories of abuse (e.g., targeted Identity, affiliation, and person), and counter speech. The published dataset\nby [5] has three types of content (classes) i.e., Hatespeech, Offensive language and Neither. The datasets, from [14]\nand [16], have only two classes: Hate/Not Hate and Offensive/Vulgar. Kennedy et al.'s dataset [2] content is focused on\nparticular categories (such as religions, three hate classes for races, and a hate score to indicate the hate level). The\ndataset from [3] contains text and images. They are labelled as \u201cHate\u201d or \u201cNot-Hate\u201d.\u201cHate\u201d content is further divided\ninto five classes of different types of hate. Waseem and Havoy's dataset [9], has three labels: \u201cSexism\u201d, \u201cRacism", "and": "one"}, {"title": "Label binarization", "content": "The available datasets have multiple types (classes) of hatespeech content. Each dataset contains different types\nof hatespeech, which results in inconsistent hatespeech labels between the datasets. One of the main reasons that\npublished hatespeech datasets have different labels for hatespeech content is because there is no uniform consensus in\nthe research community on the different classes of hatespeech, which is a challenge that requires further effort from\nthe research community to address. Since there is no consensus in the research community on the different types of\nhatespeech, authors of the published datasets have labelled their data with different classes of hate. Some datasets\ncontain fine-grained categories of hate, where the victims are targeted based on their race; religion; sexuality; ethnicity;\ngender etc. [2, 13]. Other datasets contain broad categories such as \u201chateful\u201d, \u201cabusive\u201d or \u201cneutral\u201d [3, 16, 9].\npresents a summary of some of the categories of hatespeech in each dataset. Some datasets contain very few and general\nhatespeech content (such as \u201chate\u201d or \u201coffensive\", as in the dataset published by [14] and [5]) whereas other datasets\nhave many and fine-grained hatespeech types such as the dataset published by [2] which includes various subtypes of\nhatespeech based on gender or religion.\nPrior to exploring and analysing the content of the selected datasets for this study, we have binarised the labels. The\nobjective is to ensure all the datasets contain consistent labels (\u201cHate\u201d or \u201cNot-Hate\u201d). The label binarization process is\nperformed as follows:\n\u2022 Merge fine-grained labels of hatespeech to broad labels. If the label of content indicates any type of hatespeech,\nthen we convert it to a broad label \"Hate\". If the label indicates the content is \"neutral\" or \"not hateful\", then\nwe convert to \"Not-hate\".\n\u2022 Drop ambiguous labels by discarding any content in a dataset where the content has an ambiguous label, such\nas \"abusive\", because such content may not be considered hateful.\n\u2022 We convert content that is labelled as \"neutral\", \"not-hate\", or \"not-abusive\" to the \"Not-hate\" label.\nThe label binarization process provides us with a dataset containing consistent labels of \u201cHate\u201d or \u201cNot-Hate\u201d, which we\ncan use to evaluate their suitability for training and testing a deep learning algorithm for binary hatespeech classification.\nWe evaluate a baseline deep learning system on each dataset to assess its performance in two tasks: (i) performing\nbinary classification of hatespeech (i.e., classifying text as either \u201cHate\u201d or \u201cNot-hate\u201d), and (ii) performing transfer\nlearning classification to test the generalisation of the system where we train the system on one dataset and test it on\nmultiple other dataset.\nOne of the major issues with all the public datasets that negatively impact the performance of machine learning\nalgorithms is the imbalance in the features. The sample size for different categories of text is often uneven, with\""}, {"title": "Dataset balancing", "content": "The available datasets are imbalanced, as they contain unequal sample size for different text categories. As we discussed\nin Section 3.2, we have binarised the labelled content in each dataset so that they contain only \"Hate\" and \"Not-Hate\".\nAs it can be seen , the differences in the sample size for different labelled content is large in all the datasets.\nSuch differences in sample sizes negatively affect the training of machine learning algorithms (including deep learning\nalgorithms), as the algorithms become biased towards the majority sample. To balance the sample size of \"Hate\" and\n\"Not-Hate\" content in each dataset, we apply under-sampling methods, reducing the size of the majority sample to match\nthe size of the minority one. The columns show the balanced sample size of \"Hate\u201d/\u201cNot-Hate\" content for\neach dataset."}, {"title": "Statistical Analysis", "content": "To gain a better understanding of the nature and prevalence of hatespeech, this study utilised a quantitative approach to\nanalyse the frequency of hate terms in both hateful and non-hateful speech. Our analysis began with collecting, 1523\nfrequently used hate terms from Hatebase\u00b2 , a publicly available database of hatespeech terms. We then conducted an"}, {"title": "Hatespeech Classification", "content": "Text preprocessing: cleaning and normalization. Since the content of the published dataset is collected from different\nonline platforms (e.g., Twitter, Facebook, YouTube comments, etc.), they have different features such as structure, topic,\nuser writing style, etc. We stored each dataset in comma separated value files (CSV) with two columns (text and label).\nThe text column contains the text content and the label column contains one of two values, \"hate\" or \"not-hate\". We\nhave performed the following transformations on the text before training and evaluating our model:\n\u2022 Lower casing. We convert all the text to lower case English characters.\n\u2022 Removing non-English text. We remove content that is not part of the English alphabet.\n\u2022 Normalising emojis by replacing with token \u201c\u201d.\n\u2022 Normalising tag. We transform all hashtags to the token \u201c<HASHTAG>\" and all usernames to the token\n\"@USER\".\n\u2022 Removing duplication. We remove sequentially duplicated items such as words, spaces, characters etc.\n\u2022 Removing punctuation. We remove all the English and non-English punctuation.\n\u2022 Removing stop words. We remove all stop words such as 'a', 'the', 'of' etc.\n\u2022 Normalizing URL. We transform all hyperlinks and website address to the token \u201c\u201d.\n\u2022 Normalising HTML elements. We convert all named and numeric character references from HTMLs such as\n\"&gt;\" and \"&#amps;\" in the text to their corresponding Unicode characters \u201c<\u201d and \u201c&\u201d, respectively.\n\u2022 Removing new line in text. We remove all new line in each text to create a single line text.\nDeep learning model implementation. We have implemented a baseline deep learning text classification system. We\nhave trained and tested the system on ten publicly available datasets.\nOur model is based on a widely utilised deep learning algorithms for text classification. We have also utilised a\ngeneral-purpose language model that has contributed to many natural language processing tasks.\nAfter preprocessing and normalizing the text in each dataset 3.5) we have tokenised the input data (textual information)\nusing Bidirectional Encoder Representations from Transformers model (BERT) [21], which is an essential step for\ntraining any deep learning algorithms. The BERT model is a multilingual language model trained on a very large text\ndata. BERT model is a multi-layer bidirectional Transformer, which is a deep learning model used in several Natural\nLanguage Processing tasks [22].\nNext, the vector representation of the data is processed by a dropout layer with a dropout rate of 0.3. This step involves\nrandomly excluding 30% of the training data during the training phase of the model in order to prevent the algorithm\nfrom memorizing the data pattern from the dataset, as is usually referred to as overfitting.\nThe output from the dropout later is used as input to a single deep learning dense layer of neural network. We\noptimised the model learning capacity using Adam optimizer with learning rate of 2e - 5. During the training phase,"}, {"title": "Dataset evaluation and results", "content": "We empirically evaluated ten different datasets using a baseline classifier. For each dataset evaluation, we divided the\ndataset into two parts: a training set and a test set, ensuring that the content of the two sets was distinct. We allocated\n80% of each dataset to the training set for training the baseline classifier and 20% to the test set for evaluating the\nclassifier's performance. within the training set, we further partitioned the data by allocating 90% for training the deep\nlearning architecture and reserving 10% for validating the training accuracy during the training stage.\nWe conducted two types of evaluations on each dataset:\n\u2022 Mono-dataset evaluation. In the model evaluation process, we trained and tested our baseline classifier,\nindividually on each dataset, to classify their content as either \u201cHate\u201d or \u201cNot-Hate\". The aim of this experiment\nis to assess the suitability of each dataset for binary classification of hatespeech content.\n\u2022 Generalised learning evaluation. In this evaluation, we used the baseline classifier that we used for the mono-dataset evaluation, but we tested it differently. In this evaluation, we have conducted multiple experiments on\nthe baseline classifier. In each experiment, we trained the model on one dataset and tested it on nine other\ndatasets. For this experiment, our approach is: given dataset d is a member of a set of dataset Di,\u2026\u2026,n, we"}, {"title": "Dataset evaluation", "content": "We empirically evaluated ten different datasets using a baseline classifier. For each dataset evaluation, we divided the\ndataset into two parts: a training set and a test set, ensuring that the content of the two sets was distinct. We allocated\n80% of each dataset to the training set for training the baseline classifier and 20% to the test set for evaluating the\nclassifier's performance. within the training set, we further partitioned the data by allocating 90% for training the deep\nlearning architecture and reserving 10% for validating the training accuracy during the training stage.\nWe conducted two types of evaluations on each dataset:\n\u2022 Mono-dataset evaluation. In the model evaluation process, we trained and tested our baseline classifier,\nindividually on each dataset, to classify their content as either \u201cHate\u201d or \u201cNot-Hate\". The aim of this experiment\nis to assess the suitability of each dataset for binary classification of hatespeech content.\n\u2022 Generalised learning evaluation. In this evaluation, we used the baseline classifier that we used for the mono-dataset evaluation, but we tested it differently. In this evaluation, we have conducted multiple experiments on\nthe baseline classifier. In each experiment, we trained the model on one dataset and tested it on nine other\ndatasets. For this experiment, our approach is: given dataset d is a member of a set of dataset Di,\u2026\u2026,n, we"}, {"title": "Results", "content": "We have conducted multiple empirical evaluations of our baseline classifier, which we described in section 3.5, on ten\npublicly available datasets. In this section, we report the empirical evaluation outcomes of the effectiveness of each\ndataset for training and testing a baseline deep learning classifier to examine the suitability of different public dataset\nfor hatespeech classification.\nFor each dataset evaluation, we conducted two experiments: (i) we evaluated the suitability of each dataset for training\nand testing a baseline classifier for the binary classification of hatespeech content. The training and testing samples are\nfrom the same dataset. Thus, we refer to this experiment as \u201cmono-dataset experiment\", and we refer to the baseline\nclassifier in this experiment as \u201cmono-dataset classifier\u201d. (ii) The second experiment involved testing the suitability of\neach dataset to produce a generalised baseline classifier, which has the same architecture as the mono-dataset classifier\nbut trained and tested in a more generalisable approach. We trained the baseline classifier on a dataset and tested it\non nine other datasets, excluding the dataset that we used for training the classifier. We refer to this experiment as\n\"generalised learning experiment\", and we refer to the baseline classifier as \u201cgeneralised classifier\".\nFor each experiment-due to space limitation- we report the system performance using weighted F1-score, which is\nbased on the harmonic mean of recall and precision.\""}, {"title": "Mono-Dataset Experiment.", "content": "presents the performance of the mono-dataset classifier. Out of the ten selected dataset,the model performs best\nwhen trained on the dataset published by Davidson et al. [5] achieving a weighted F1-score of 0.930. The second-best\nperformance is based on the Suryawanshi et al. [8] dataset with a weighted F1-score of 0.902, which is slightly behind"}, {"title": "generalised Learning Experiment.", "content": "In this experiment, we evaluated our baseline classifier by training it on one dataset and testing it on nine other datasets,\nexcluding the dataset used for training. We repeated the experiment for all the datasets. The results from this experiment\nprovide a clear indication of the suitability of each dataset for producing a classifier that can be generalised to unseen\ndata. We refer to the baseline classifier in this experiment as \u201cgeneralised classifier\"\u00b3 be referred to as \u201ctransfer\nlearning\"."}, {"title": "Discussion", "content": "The classifier performed well when trained and tested on a single dataset at a time, which we referred to in Section 4 as\nmono-dataset experiment. It produced a weighted F1-score between 0.81 and 0.93 for six out of ten datasets that we\nused for training and testing the classifier. In order to evaluate each datasets, we applied a generalised learning approach"}, {"title": "p-Test", "content": "We have calculated two statistical measures (P - test and T - test) using the content of each dataset based on the\nlabels \"Hate\" or \"Not-Hate\u201d. Details of our approach to computing these statistics are presented in section 3.4. Since\nmachine learning algorithm performance is based on its learning from identifiable patterns in a given dataset, the p\nvalue offers good indication on the available patterns in the ten datasets we have chosen for this study. A p value of 0\nindicates that the patterns in the dataset occurred by chance, which may reflect poor dataset annotation. A p value of 1\nindicates there is no difference in the patterns in the dataset. Our data analyses results are shown.\nThe result of our analyses highlighted that the dataset from [3] has consistently performed poorly in both of the\nexperiments: mono-dataset experiment and generalised learning experiment. From table 10 it can be noted that p the\nvalue for this dataset is very close to 1 (0.9947), which means there is no recognisable pattern between \u201cHate\" and\n\"Not-Hate\" content in this dataset. The lack of distinguishable patterns in the dataset highlights the main reason for\nthe baseline classifier failing to learn sufficiently from this dataset, hence performing poorly, producing a weighted\nF1-score of 0.363 in the generalised learning experiment.\nIn contrast, as can be seen from table 9, the classifier produced a weighted F1-score of more than 0.5 for all those\ndatasets with p value 0.0001 and <0.1011, indicating that the model learned sufficient patterns to produce a weighted\nF1-score of over 0.531."}, {"title": "Confusion matrix", "content": "In supervised text classification, machine learning algorithms learn from a set of labelled data. Any given labelled\ndataset contain annotation errors due to many reasons (e.g., annotation procedure, annotator competency, data quality\nchecking, ambiguities in natural language, etc.). Thus, machine learning algorithms are expected to make mistakes\nsince they learn from annotated data. We use confusion matrices to highlight the classification errors the classifier\nmade in each experiment (mono-dataset and generalised learning experiments). Due to space limitations, we provide\ncomprehensive details on the classification errors of the best and worst performing classifiers compared to other models."}, {"title": "mono-dataset classifier error analyses", "content": "Training the classifier on the dataset published by Davidson et al. [5] produced the lowest classification error rate. The\nclassifier correctly classified 92% \"Not-Hate' content and 94% \"Hate\" content. However, the classifier miss-classifies\n8% of \"Not-Hate\" content as \"Hate\" and 6 of it \"Hate\" content as \"Not-Hate\". The classifier's misclassification total\nerror rate between the classes (\u201cHate\u201d and \u201cNot-Hate\u201d) is (14%), as shown , which is lower compared to when\nthe classifier is trained on other dataset."}, {"title": "generalised model's error analyses", "content": "In this section, we examine the confusion matrix graphs to analyse the errors made by each classifier when evaluated on\ntransfer learning performance.\nIn the generalised learning experiment, we trained our classifier on one dataset and tested it on the remaining nine\ndatasets. This experiment produced ten classifiers, each of which tested on nine datasets, excluding the one was used\nfor training. For each experiment, we obtained one confusion matrix, resulting in nine confusion matrices per classifier.\nTherefore, this substantial experiment produced a total of ninety confusion matrices. Due to space limitations, we will\nfocus our discussion only on a subset of confusion matrices. Specifically, we will examine the classification errors of\nthe classifiers that have the highest or the lowest mean score of weighted F1-score when tested on the nine different\ndatasets. As shown in Table 9, the highest weighted F1-score (0.656) produced by the classifier trained on the Qian\net al. [14]'s dataset, while the lowest weighted F1-score (0.636) was produced by the classifier trained on Gomez et.\nal. [3]'s dataset.\nThe confusion matrices presented in ascending order based on the weighted F1-score, which is\nshown in Table 8.\nWe grouped our analyses of the confusion matrices based on the following criteria: i) the classifier produced the highest\nmean weighted F1-score compared to the other nine models, ii) the classifier produced the lowest mean weighted\nF1-score."}, {"title": "Model performance Analysis", "content": "To develop an efficient hatespeech classifier, it is crucial to train the model on a high-quality dataset that provides\nsufficient information for accurate classification and real-world implementation. To gain a deeper understanding of\nthe role of features in the effectiveness of the classifier, we conducted a correlation test between the dataset features\nand the classification F1 score. This analysis helps to identify the most informative features and optimize the dataset\nfor classifier performance. presents the results of our correlation analysis for the mono dataset test, showing the\nPearson correlation between each feature and the F1 score of the classifier. This information can be used to identify the\nmost informative features of the dataset.\nBased on our analysis, we observed that the Median word count for the not-hate part of the datasets and the P-value are\nhighly correlated, indicating that these features have a significant impact on the performance of the hatespeech classifier.\nOn the other hand, the T-value is the least correlated feature, suggesting that it may not have a significant impact on the\nclassifier's effectiveness.\nThis finding highlights the importance of statistical characteristics of the dataset used in the hatespeech classifier, as\nsome characteristics may have a more significant impact on the model's performance than others. By understanding the\ncorrelation between the characteristic and the model's effectiveness, we can optimize the dataset balancing and improve\nthe accuracy of the classifier."}, {"title": "Conclusions and Future Work", "content": "Despite myriad benefits of social network platforms for users and businesses, malicious users abuse them by targeting\nspecific users based on their identity. This phenomenon is referred to as Hatespeech, where malicious users target\nvulnerable people with abusive text, or graphics, to degrade and cause them harm. The severity of hatespeech on victims\nhas forced researcher, social media platforms and governments to take action to eliminate it. The task of eliminating"}]}