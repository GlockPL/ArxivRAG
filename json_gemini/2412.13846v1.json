{"title": "From Expectation to Habit: Why Do Software Practitioners Adopt Fairness Toolkits?", "authors": ["Gianmario Voria", "Stefano Lambiase", "Maria Concetta Schiavone", "Gemma Catolino", "Fabio Palomba"], "abstract": "As the adoption of machine learning (ML) systems continues to grow across industries, concerns about fairness and bias in these systems have taken center stage. Fairness toolkits-designed to mitigate bias in ML models-serve as critical tools for addressing these ethical concerns. However, their adoption in the context of software development remains underexplored, especially regarding the cognitive and behavioral factors driving their usage. As a deeper understanding of these factors could be pivotal in refining tool designs and promoting broader adoption, this study investigates the factors influencing the adoption of fairness toolkits from an individual perspective. Guided by the Unified Theory of Acceptance and Use of Technology (UTAUT2), we examined the factors shaping the intention to adopt and actual use of fairness toolkits. Specifically, we employed Partial Least Squares Structural Equation Modeling (PLS-SEM) to analyze data from a survey study involving practitioners in the software industry. Our findings reveal that performance expectancy and habit are the primary drivers of fairness toolkit adoption. These insights suggest that by emphasizing the effectiveness of these tools in mitigating bias and fostering habitual use, organizations can encourage wider adoption. Practical recommendations include improving toolkit usability, integrating bias mitigation processes into routine development workflows, and providing ongoing support to ensure professionals see clear benefits from regular use.", "sections": [{"title": "I. INTRODUCTION", "content": "Machine Learning (ML) has become pervasive, with its adoption accelerating across a wide array of industries and everyday applications [1]. ML-enabled systems\u2014software systems powered by AI or ML algorithms [2]\u2014are revolutionizing sectors such as healthcare and entertainment by improving efficiency, optimizing decision-making processes, and driving innovative solutions [3]\u2013[6].\nAs ML continues to spread, it has also prompted significant ethical concerns, particularly around fairness [7], which refers to the principle that models should make impartial decisions, avoiding bias or discrimination against certain groups. Unfairness occurs when models inherit biases present in training data [8], [9], resulting in decisions that undermine trust and pose ethical as well as legal challenges [10]. This is proven by several known ethical incidents caused by ML applications, e.g., Facebook vision model that put the \"primate\" label to black men or Amazon assigning lower sales ranking to books containing LGBTQIA+ themes, highlighting the urgent need for fair ML software [11]\u2013[14].\nAcknowledging the critical importance of fairness, the software engineering (SE) research community\u2014particularly in the domain of software engineering for artificial intelligence (SE4AI)\u2014has made substantial strides in developing bias mitigation techniques [15], recognizing fairness as a crucial non-functional requirement. These approaches can generally be grouped into three main categories: pre-processing, in-processing, and post-processing techniques. In this regard, the research community and organizations have developed instruments to make these solutions available for software practitioners, e.g., AIF360 [16] or FairLearn [17]. These tools referred to as fairness toolkits, comprise ready-to-use metrics to measure fairness or bias mitigation techniques [18].\nWhile fairness toolkits have proven effective in mitigating bias [18], [19], there is still a significant gap in understanding their actual adoption. Specifically, it remains unclear what decision-making heuristics lead practitioners to consider using fairness toolkits in their workflow. We argue that this is an important limitation for two reasons. First, studying the adoption of fairness toolkits may uncover the main drivers that need to be considered or encouraged to further increase the uptake of these tools, as suggested by previous research investigating technology acceptance [20], [21]. Second, understanding the considerations that lead to their adoption can offer additional insights into how existing fairness toolkits can be refined and better integrated into practitioners' workflows [22]. This may inform recommendations for designing the next generation of fairness toolkits. In summary, a deeper understanding of these heuristics could provide valuable insights for both researchers and toolkits vendors.\nRecognizing the aforementioned opportunities, our goal is to offer a complementary perspective by investigating the key factors influencing practitioners' willingness to adopt fairness toolkits. Therefore, this research seeks to address this gap, starting by defining the following guiding research question:"}, {"title": "Research Question.", "content": "What factors influence software practitioners in the adoption of fairness toolkits?\nTo address the research question, we conducted a quantitative study grounded in the Unified Theory of Acceptance"}, {"title": "III. HYPOTHESIS AND THEORY DEVELOPMENT", "content": "This study seeks to identify the key individual factors that influence the adoption of fairness toolkits in software development and engineering. Given the lack of studies that specifically investigate the factors influencing the adoption of fairness toolkits, we have chosen to base our approach on the UNIFIED THEORY OF ACCEPTANCE AND USE OF"}, {"title": "A. The Unified Theory of Acceptance and Use of Technology", "content": "One of the theoretical models developed to predict technology adoption and use is UTAUT [50]. This model posits that the actual usage (UB) of technology is determined by behavioral intention (BI). The perceived likelihood of adopting the technology is influenced by the direct effects of four key constructs: the belief that using the system will enhance job performance (Performance Expectancy, PE), the perceived ease of using the system (Effort Expectancy, EE), the perception that organizational and technical infrastructures are in place to support system use (Facilitating Conditions, FC), and the perception that important others believe the system should be used (Social Influence, SI). Individual-related factors, such as age, gender, and experience, are typically considered to moderate or diminish the relationship between technology use and behavioral intention [51].\nDespite the widespread acceptance of UTAUT, Venkatesh et al. [23] later introduced UTAUT2, an updated version of the original model that includes three additional constructs, emphasizing the user as a customer and stakeholder rather than merely a technology adopter [23]. This extension was designed to provide greater precision in explaining user behavior. UTAUT2 provides the following additional constructs: the degree of pleasure or enjoyment derived from using the technology (Hedonic Motivation, HM), the cognitive tradeoff between the perceived benefits of the technology and its monetary cost (Price Value, PV), and the extent to which individuals tend to perform behaviors automatically through learning (Habit, HB).\nMotivation and Choices. Given our objective to investigate the adoption of fairness toolkits by software practitioners, the UTAUT2 model was a natural selection. In comparison to its predecessors, the Technology Acceptance Model (TAM) [52] and UTAUT [50], the new model encompasses a broader range of individual-level factors that capture various dimensions of technology adoption [23]. Moreover, the constructs of social influence and facilitating conditions allow us to consider environmental factors that may affect the adoption process [23]. Importantly, UTAUT2 provides us with established and validated measurement instruments, enabling us to contextualize our findings within a substantial body of literature utilizing the same theoretical framework.\nThe standard UTAUT2 model includes three moderating variables: age, gender, and experience. However, in the interest of model parsimony, previous research [20], [49] that adopted the UTAUT framework in software engineering research chose to exclude these three. To ensure reliability and robustness, we still conducted a preliminary analysis using the moderating variables. The results, available in our online appendix [53], revealed that these three were not significant."}, {"title": "B. Hypotheteses Development", "content": "In the following, we present the hypothesis we developed for the constructs of the UTAUT2 model to understand software practitioners' intention to use and actual usage of fairness toolkits. Figure 1 summarizes the model built upon the hypotheses described in this section.\nOne of the constructs of the model is Performance Expectancy, which refers to the degree to which an individual believes that adopting a particular technology will enhance their job performance [50]. In essence, this construct suggests that practitioners are more likely to utilize fairness toolkits if they perceive these tools as beneficial for completing their routine software development tasks, which in this case may be influenced by how much they are expected to produce fair software for their organization [22]. Positive outcomes associated with fairness toolkits such as improved efficiency in identifying biases, enhanced accuracy in decision-making, and strengthened capabilities for addressing ethical dilemmas can significantly influence practitioners' willingness to integrate these tools into their workflows [54]. Given the potential of fairness toolkits to streamline development processes and provide substantial performance benefits, it is reasonable to propose that performance expectancy plays a role in practitioners' intentions to adopt these technologies [54], [55]. Therefore, we hypothesize that (H1: PE\u2192BI) Performance Expectancy (PE) positively influences the intention to adopt (BI) fairness toolkits by software practitioners.\nThe degree of ease with which a particular technology can be utilized is referred to as Effort Expectancy [50]. Individuals are more likely to adopt new technologies when they find them easy to understand and use [56]. Stakeholders may decide whether to incorporate fairness toolkits into their development processes based on how straightforward it is to integrate and utilize these tools [18]. We anticipate that effort expectancy will positively influence the intention to adopt fairness toolkits for software development, given the significance of ease of use and reduced cognitive load in technology acceptance. Therefore, we hypothesize that (H2: EE\u2192BI) Effort Expectancy (EE) positively influences the intention to adopt (BI) fairness toolkits by software practitioners.\nSocial Influence refers to the degree to which an individual perceives that important others believe they should use a new system [50]. According to this concept, individuals are more likely to adopt new technologies if they feel that their colleagues, supervisors, or social norms advocate for their use. Stakeholders may receive approval and encouragement from peers, mentors, or industry leaders, which can significantly"}, {"title": "IV. RESEARCH DESIGN", "content": "To evaluate the above-mentioned hypotheses and explore software practitioners' intentions to adopt fairness toolkits, we conducted a survey study.\nFigure 2 presents an overview of our research methodology, which we elaborate upon in this section. We initiated our process by meticulously defining participant selection criteria for our survey and calculating the requisite sample size using G*Power [58], taking into account the intricacies of our"}, {"title": "A. Participant Selection and Demographics", "content": "To amass data for this study, we implemented a survey utilizing a cluster sampling strategy via Prolific, a reputable"}, {"title": "B. Data Collection", "content": "To facilitate data collection, we meticulously developed two questionnaires: the first, designated as the \"Pre-Screening questionnaire,\" aimed to identify ideal participants\u2014by mean of custom screening-from those already filtered via the Prolific platform. The second, termed the \u201cMain questionnaire,\u201d was crafted to measure UTAUT2 constructs in the participants selected from the pre-screening survey. Both surveys were developed adhering rigorously to the established guidelines by Kitchenham and Pfleeger [59] and Andrews et al. [60], which are highly regarded in software engineering research. Additionally, we followed the SIGSOFT Empirical Standard for Questionnaire Surveys [63]. The questionnaires were fully anonymized, featuring an introductory description that elucidated key details to aid participants in comprehending the tasks. We incorporated a closing question for feedback and attention check questions to ensure participant reliability.\nBefore administering them, we conducted iterative pilot tests with dual objectives: (1) assessing quality and clarity and (2) estimating completion time. Initially, we orchestrated three pilots involving 10 researchers from our network. After each round, we meticulously refined the surveys to address feedback and rectify any typographical errors. Subsequently, we conducted separate pilots for each questionnaire using Prolific: five participants completed the pre-screening questionnaire, and another five completed the main one. The pilots for both questionnaires transpired between 30 August and 01 September 2024.\nThe pre-screening questionnaire gathered comprehensive demographic information, assessed participant reliability, and evaluated programming skills and experience. Designed to take 6 minutes to complete, we successfully collected 200 responses within two days starting from 30 August 2024. The main questionnaire, which measured the UTAUT2 constructs, was estimated to require 5 minutes for completion, and it took six days to amass 181 responses from 01 September 2024.\nEthical Considerations. We meticulously designed and executed our work, giving careful consideration to participants' privacy and addressing potential ethical concerns inherent in survey studies [64]. Our survey design ensured complete anonymity of all responses; consequently, we refrained from collecting participants' names or email addresses. We consciously avoided soliciting any sensitive business information"}, {"title": "C. Data Analysis", "content": "As previously explained, data collection was conducted through a meticulously designed survey study. All constructs in the theoretical model described in Section III were measured at the individual level using items validated in the literature, ensuring the utmost reliability of our measurement process. We expound upon the items used and the data analysis process in the following sections, with a comprehensive overview of all items and references provided in our online appendix [53].\n1) Data Gathering Instruments: Questionnaire items assessing the UTAUT2 constructs were judiciously adapted from the original authors [23]. The dependent variable, Use Behavior (UB), was measured using a single-item frequency scale, while the seven predictors were evaluated on a 7-point Likert scale. These predictors encompassed Performance Expectancy (PE, 5 items), Effort Expectancy (EE, 6 items), Social Influence (SI, 5 items), Hedonic Motivation (HM, 3 items), Facilitating Conditions (FC, 4 items), Habit (HB, 4 items), and Price Value (PV, 3 items), as well as Behavioral Intention (BI, 3 items). We also asked whether the use of fairness toolkits was mandated by the participants' companies, recognizing this as a potential influencer of use behavior. Additionally, we collected demographic data such as age, gender, role, and years of experience in the software industry to contextualize our sample relative to other surveys.\n2) Analysis Process: We initiated our analysis by conducting a thorough preliminary examination of the data to ensure its quality. While PLS-SEM offers considerable flexibility, we nonetheless rigorously checked for missing data, unusual response patterns, outliers, and data distribution issues. Upon validating the dataset, we imported it into SmartPLS, a sophisticated tool designed specifically for PLS-SEM analysis [65]. Given the intricate nature of the PLS-SEM process, we direct readers to Hair et al. [24] and Russo and Stol [61] for more detailed explanations.\nWe began by developing the measurement model (or outer model), which meticulously links each theoretical construct to its associated indicators. Each construct in our theoretical"}, {"title": "V. ANALYSIS OF THE RESULTS", "content": "The following section presents the results from the PLS-SEM analysis. This analysis aims to uncover the causal relationships and underlying patterns within the data, offering a detailed evaluation of the hypothesized model. Through this approach, we gain insights into the interactions between the constructs and validate the theoretical framework proposed.\nBefore proceeding with the main PLS-SEM analysis, we conducted a preliminary data examination. Notably, there were only a few instances of missing values, likely due to the high quality of the questionnaire and the reliability of our sample, ensured by the approval rate filter. These missing values did not pose any significant issues, as SmartPLS is equipped to manage them automatically. Additionally, we reviewed the data for suspicious response patterns and found none. It is also important to highlight that all participants successfully passed the attention check questions."}, {"title": "A. Measurement Model Evaluation", "content": "As a first step in the evaluation of the theoretical model, it is paramount to evaluate the reliability of the constructs of the model [24], [61]. Consequently, we analyze the indicator reliability, internal consistency reliability, convergent validity, and discriminant validity. This section presents the obtained results for each of the steps mentioned above.\nIndicator Reliability. As outlined by Hair et al. [24], the initial step in assessing the measurement model is to evaluate the reliability of the indicators, focusing on their outer loadings. High outer loadings indicate that the indicators capture a substantial amount of commonality with the construct.\nA commonly accepted guideline is to retain indicators with outer loadings above 0.708, while indicators with loadings below 0.40 are generally removed. For those with values between 0.40 and 0.70, removal is considered if it enhances internal consistency reliability or convergent validity.\nFor brevity, the outer loadings for all indicators are reported in the online appendix [53]. Two indicators, EE4 and HB2, had outer loadings below 0.70, but since no indicator had a loading lower than 0.40, all were retained for further analysis.\nInternal Consistency Reliability. The second step involved evaluating internal consistency reliability to confirm that the"}, {"title": "B. Structural Model Evaluation", "content": "After evaluating the measurement model, the next step is to assess the structural model.\nCollinearity Analysis. The initial step in evaluating the structural model involves examining collinearity between exogenous and endogenous variables, which is essential for accurate path estimation. To detect multicollinearity, we employed the Variance Inflation Factor (VIF), a standard metric used in multiple regression analysis. Ideally, a VIF value under 3 indicates no collinearity, while values below 5 are also acceptable. In our analysis, the majority of VIF values were below 3, with the highest being 2.47. Only two paths (PE \u2192 BI and EE \u2192 BI) slightly exceeded the ideal threshold. Based on these findings, we concluded that multicollinearity does not present a significant concern in our model.\nSignificance and Relevance of the Relationships. In the second phase of our analysis, we focused on evaluating the significance and relevance of the relationships within the structural model. To test for significance, we employed the bootstrapping method, using 10,000 sub-samples, as recommended by Hair et al. [24]. We analyzed T-values, p-values, and bootstrap confidence intervals. The results, summarized in Table II, indicate that Habit significantly influences both Use Behavior and Behavioral Intention. Furthermore, Behavioral Intention is strongly associated with Use Behavior, while Performance Expectancy demonstrates a significant connection to Behavioral Intention. To assess the relevance of these significant relationships, we examined the standardized path coefficients, which are also detailed in Table II. Performance Expectancy emerged as the most influential factor affecting the intention to utilize fairness toolkits, closely followed by Habit. In terms of actual usage behavior among software practitioners, Habit was identified as the most significant factor, with Behavioral Intention ranking second. In addition, further analysis revealed that Performance Expectancy also has an indirect relationship with the use behavior.\nExplanatory Power. In the third phase of our analysis, we aimed to evaluate the model's explanatory capability, specifically how well it fits the data by quantifying the strength of the relationships within the model, as described by Hair et al. [24] and Russo et al. [61]. This is typically assessed using the coefficient of determination (R\u00b2), which ranges from 0 to 1; higher values indicate stronger explanatory power. Although no universal standards exist for R\u00b2, values as low as 0.10 may be considered acceptable in certain contexts, with 0.19 often regarded as a more appropriate benchmark [24], [67], [68].\nIn our analysis, we found R\u00b2 values of 0.630 for Behavioral Intention and 0.407 for Use Behavior. This indicates that our model successfully explains 63% of the variance in the intention to use large language models (LLMs) and 40% of the variance in actual usage. Furthermore, since R\u00b2 values are below 0.90, we can confidently exclude the overfitting concern.\nAfter evaluating the coefficient of determination, we further quantified the strength of the relationships using the f\u00b2 effect size. This metric assesses the potential change in R\u00b2 if a"}, {"title": "VI. DISCUSSION AND IMPLICATIONS", "content": "This study aimed to explore individual factors influencing software practitioners' intention to adopt, as well as their actual adoption, of fairness toolkits using the Unified Theory of Acceptance and Use of Technology (UTAUT) framework. Our findings indicate that three key constructs from the UTAUT2 [23] model-Performance Expectancy, Habit, and Behavioral Intention\u2014exert a statistically significant influence on the dependent variables. Conversely, the other constructs did not show significant effects on the dependent variables. The remainder of this section will explore and elaborate on all the constructs, offering insights and implications that could be valuable for further research and practice."}, {"title": "A. Discussions", "content": "Performance Expectancy, or the perceived utility of fairness toolkits, emerges as the cornerstone of adoption. As fairness becomes an increasingly critical non-functional requirement in modern software engineering [7], [11], practitioners are drawn to tools that effectively mitigate bias in ML systems. Our results reveal that this factor has the most significant influence on the intention to adopt fairness toolkits while also maintaining a strong and significant indirect relationship with the actual use behavior of practitioners. These results align with established technology acceptance models [50] and recent research in the field [54], [55].\nThe consideration of fairness in daily workflows is largely driven by Habit. The study identifies this as the second most influential factor on behavioral intention and the primary determinant of actual use behavior. This finding underscores the importance of seamless integration and initial exposure in fostering sustained use of fairness toolkits, as habitual use becomes an essential part of practitioners' routines."}, {"title": "B. Implications", "content": "This study contributes to understanding the reasons behind software practitioners' adoption of fairness toolkits. Our results have actionable implications for organizations, toolkit vendors, and researchers.\nOrganizations. For organizations that aim to spread the usage of fairness toolkits, these insights suggest that efforts to promote adoption should focus on demonstrating their concrete benefits and effectiveness in addressing bias issues. Educational initiatives and awareness campaigns, such as workshops or tutorials, might be effective if they emphasize the technical merits and the tangible benefits of fairness toolkits rather than relying on social proof, external supports, or attempts to make the tools more enjoyable to use. Moreover, organizations and managers should make an effort to integrate fairness toolkits usage into daily workflows to help employees develop a habit. This can be done by facilitating access and integration of these tools in daily working activities.\nToolkits Vendors. For toolkits vendors, our work may be of inspiration to understand possible design solutions to enhance the adoption of fairness toolkits. To demonstrate toolkits' high performances, vendors should aim to provide practical examples and real-world cases in which their solutions helped mitigate biases and achieve fair ML models, rather than relying on theoretical proofs. In addition, to make practitioners develop a habit of the use of fairness toolkits, vendors should facilitate practitioners in integrating such solutions in their daily activities through efficient APIs or libraries. Despite the efforts vendors can make to promote toolkit adoption, our findings indicate that practitioners perceive these tools as both useful and effective. This suggests that the investment in supporting fair ML development is paying off.\nResearchers. Finally, researchers should leverage our findings to perform further investigations on fairness toolkits. On the one hand, empirical studies demonstrating these solutions' performances and abilities in mitigating bias could further increase practitioners' intention to adopt them. On the other hand, exploring novel ways to integrate and automate fairness toolkits' integration in existing workflows, such as CI/CD pipelines, could tempt software practitioners to use them and consequently develop a habit."}, {"title": "VII. THREATS TO VALIDITY", "content": "Our study primarily focused on quantitative analysis supported by statistical methods. In discussing threats to validity, we followed the framework outlined by Wohlin et al. [72].\nRegarding the conclusion Validity\u2014i.e., threats about the ability to draw accurate conclusions about the relationships between independent and dependent variables [72]\u2014the primary threats in this category stem from the statistical tests used for analysis. To address this, we relied on PLS-SEM, which is known for its robustness in various contexts. We closely followed the procedures outlined by Hair et al. [24] in their detailed work on PLS-SEM methodology. Moreover,"}, {"title": "VIII. CONCLUSION", "content": "This study investigated the adoption of fairness toolkits among software practitioners using the Unified Theory of Acceptance and Use of Technology (UTAUT2) framework [23]. We surveyed experts and analyzed the data using Partial Least Squares Structural Equation Modeling (PLS-SEM) [24].\nOur findings reveal that Habit and Performance Expectancy significantly influence the intention to adopt fairness toolkits, aligning with previous research [54], [55]. Moreover, Habit emerged as the primary driver for the actual use of these toolkits, alongside practitioners' intention to use them.\nThese results have important implications. Organizations promoting fairness toolkit adoption should focus on demonstrating their concrete benefits and effectiveness in addressing bias issues. Additionally, our findings suggest that software practitioners primarily approach bias mitigation from a technical perspective, indicating a continued need for research into algorithmic solutions for ML fairness.\nFuture research should explore the impact of additional factors, such as cultural values, on the adoption of these technologies. We also recommend longitudinal studies to understand how these results may evolve as the technology matures and awareness of AI ethics grows within the software development community."}]}