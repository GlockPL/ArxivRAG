{"title": "AI-Driven Early Mental Health Screening with Limited Data: Analyzing Selfies of Pregnant Women", "authors": ["Gustavo A. Bas\u00edlio", "Thiago B. Pereira", "Alessandro L. Koerich", "Ludmila Dias", "Maria das Gra\u00e7as da S. Teixeira", "Rafael T. Sousa", "Wilian H. Hisatugu", "Amanda S. Mota", "Anilton S. Garcia", "Marco Aur\u00e9lio K. Galletta", "Hermano Tavares", "Thiago M. Paix\u00e3o"], "abstract": "Major Depressive Disorder and anxiety disorders affect millions globally, contributing significantly to the burden of mental health issues. Early screening is crucial for effective intervention, as timely identification of mental health issues can significantly improve treatment outcomes. Artificial intelligence (AI) can be valuable for improving the screening of mental disorders, enabling early intervention and better treatment outcomes. AI-driven screening can leverage the analysis of multiple data sources, including facial features in digital images. However, existing methods often rely on controlled environments or specialized equipment, limiting their broad applicability. This study explores the potential of AI models for ubiquitous depression-anxiety screening given face-centric selfies. The investigation focuses on high-risk pregnant patients, a population that is particularly vulnerable to mental health issues. To cope with limited training data resulting from our clinical setup, pre-trained models were utilized in two different approaches: fine-tuning convolutional neural networks (CNNs) originally designed for facial expression recognition and employing vision-language models (VLMs) for zero-shot analysis of facial expressions. Experimental results indicate that the proposed VLM-based method significantly outperforms CNNs, achieving an accuracy of 77.6% and an F1-score of 56.0%. Although there is significant room for improvement, the results suggest that VLMs can be a promising approach for mental health screening, especially in scenarios with limited data.", "sections": [{"title": "Introduction", "content": "Major depressive disorder (depression) is a prevalent mental health disorder affecting over 280 million people globally and a leading cause of disability worldwide. It contributes significantly to the global disease burden [1]. Similarly, anxiety disorders, marked by excessive worry and fear, impact millions of lives, often leading to debilitating effects on daily functioning and quality of life. Particularly, there is a recognized vulnerability to mood disorders during pregnancy, with possible worsening of previous emotional conditions and the emergence of new altered mental states that increase the risk of depression and anxiety [2]. This vulnerability is certainly influenced by the significant hormonal changes of pregnancy. However, it is also important to recognize that this is a time of great physical, psycho-emotional, cultural, and social changes, generating psychic stress and increased anxiety."}, {"title": "Related Work", "content": "The use of facial features to identify non-basic emotions (depression, stress, engagement, shame, guilt, envy, among others) is becoming increasingly popular in various applications. Nepal et al. [10] highlight several elements explored in this type of analysis, such as facial expression itself, gaze direction, as well as general image characteristics like luminosity and background settings. In this domain, machine learning (ML) plays an essential role, particularly with the use of deep learning models and related techniques, such as transfer learning and attention mechanisms [8]. Regarding models, convolutional neural networks (CNNs) are the most popular choice for image-based analysis of non-basic emotions. For instance, Gupta et al. [11] used CNNs to quantify student engagement in online learning, while Zhou et al. [12] applied them for detecting depression based on facial images. Transfer learning plays a crucial role as pre-trained models for tasks like basic facial expression recognition can be fine-tuned for specific targets like detecting stress [13]. Usually, pre-training leverages larger datasets, which is particularly useful when the target dataset is small, as is often the case in mental health applications. Attention mechanisms are also relevant in this context, as they can help models focus on specific regions of the face that are more informative for the target task [14, 15].\nDespite the promising results, most studies on depression detection through facial expressions involve capturing images in controlled environments where individuals follow a predetermined script, which limits the realism of the facial expressions [16, 17] and the broad applicability of pre-trained models. Nonetheless, a few studies focus on natural images captured by smartphone cameras for ubiquitous screening. Darvariu et al. [18] developed an application where users record their emotional state through photographs taken by the rear camera of smartphones. Alternatively, front-facing cameras can facilitate the capture of facial images, whose analysis can offer valuable insights into a person's emotional state [19]. In a recent work, Nepal et al. [10] introduced a depression screening approach named Mood-Capture, which was evaluated on a large dataset of facial images collected in the wild (over 125,000 photos). A key aspect of MoodCapture is the passive collection of images in multiple shots while users complete a self-assessment depression questionnaire on their smartphones. The authors argue that these images are preferable to traditional selfies as they capture more authentic and unguarded facial expressions.\nSimilarly to MoodCapture, the proposed work focuses on analyzing selfies captured by smartphone cameras for anxiety and depression screening. However, our study differs from MoodCapture in two major aspects. First, our work targets a specific population: high-risk pregnant patients accompanied by a team of healthcare professionals. This imposes a limitation on the dataset size when compared to crowd-sourced data. Second, our methodology focuses on face-centric selfies rather than analyzing general image aspects. To the best of our knowledge, this is the first study to explore the use of face-centric selfies for anxiety and depression screening in high-risk pregnant patients. Another relevant aspect of our work is the use of vision-language models (VLMs). Unlike traditional models that predict a limited set of basic emotions, VLMs can capture more nuanced emotions such as awe, shame, and emotional suppression [20]. Despite being used for general emotion analysis, the particular application of VLMs for mental health screening is another contribution of our work."}, {"title": "AI-Driven Screening via Selfie Analysis", "content": "This study relies on selfies taken by pregnant patients paired with their responses to the Patient Health Questionnaire-4 (PHQ-4), a four-item instrument for brief screening of anxiety and depression. The PHQ-4 was chosen for this study due to its efficiency and brevity in screening for both anxiety and depression, making it particularly useful in clinical settings where time is limited. Comprising only four items (Figure 1), it integrates questions from the PHQ-2 and GAD-2, which are validated tools for detecting depression and anxiety, respectively. This dual focus allows for a rapid assessment of two of the most prevalent mental health conditions, while maintaining strong psychometric properties, making it a reliable and practical tool for screening purposes in a clinical sample. The total score, calculated as the sum of individual scores, ranges from 0 to 12, with higher scores indicating greater severity of anxiety and depression symptoms. A score of 6 or higher on the PHQ-4 is typically used as a cut-off point for identifying cases where either anxiety or depression (or both) may be present and warrant further clinical evaluation [21]. This cut-off point helps to identify individuals with moderate to severe symptoms of either condition, ensuring efficient screening, including mixed anxiety-depression states, which are fairly common among pregnant women. It allows for initial assessment without the need to differentiate between the two disorders [22].\nTo enable AI-driven screening, we propose a methodology that leverages selfies and PHQ-4 responses to train machine learning (ML) models for depression-anxiety detection. Figure 2 provides a joint overview of the VLM- and CNN-based approaches addressed in this work. There are two pipelines: the training pipeline (top flow) and the test pipeline (bottom flow). The training pipeline leverages image and PHQ-4 data to train an ML model for depression-anxiety detection. The training requires an image dataset comprising faces and the respective labels (0-normal, 1-abnormal). To assemble this dataset, the first step is manually filtering out invalid selfies, which are those taken with the rear-facing camera or with faces covered by a mask. The face region of the remaining samples is subsequently cropped by using a multi-task cascaded convolutional network (MTCNN) [23]. Selfies with multiple detected faces are discarded, as the PHQ-4 responses are individual. As previously explained, a label is derived for each face image by thresholding the PHQ-4 overall score.\nWith a valid set of face images and labels, an ML model can be trained. In the CNN-based approach, the face images are directly input into the classification model. The VLM-based approach, by contrast, involves generating descriptions of emotional states through face analysis and then extracting features from these textual descriptions. These features are subsequently used as inputs for the classification head, specifically a feed-forward neural network (FFNN). Once the model (CNN or FFNN) is trained, it can be used to classify an input selfie, as illustrated in the bottom flow of Figure 2. The face region in the selfie is cropped using the MTCNN, as in the top flow. The face image (or text features) is then input to the trained model, which outputs class probabilities. The depression-anxiety condition is verified if, and only if, Pr(abnormal | sample) > 0.5. More details of the two detection approaches are provided in the following sections."}, {"title": "CNN-based Approach", "content": "This approach employs CNNs for predicting depression-anxiety directly from image data, as illustrated in Figure 2. Training from scratch is unfeasible in our context due to the reduced number of selfies/PHQ-4 responses (147 samples). To circumvent this issue, we start with CNN models pre-trained on the ImageNet dataset and fine-tune them on large-scale facial expression recognition (FER) datasets, which are closely related to our target task. A second fine-tuning is performed to adapt the FER pre-trained models to our task.\nThis work investigates the adaptation of models pre-trained on the FER2013 [24], and RAF-DB [25] datasets, both of which encompass seven basic emotions (classes): anger, disgust, fear, happiness, sadness, surprise, and neutral state. Four CNN architectures were investigated: EfficientNetV2 [26], ResNet-18 and ResNet-50 [27], and VGG11 [28]. In the second fine-tuning stage, the classifier consists of a CNN backbone pre-trained on FER or RAF-DB appended with a 2-output fully connected layer (classification head). The backbone is frozen during training to prevent overfitting, ensuring that only the classification head is trained."}, {"title": "VLM-based Approach", "content": "This approach leverages large generative vision-language models (VLMs) for facial analysis. Modern VLMs [29] combine visual encoders with large language models (LLMs) and can simultaneously learn from images and text. This enables them to perform various tasks, such as answering visual questions and captioning images. In particular, we benefit from the VLM zero-shot instruction-following ability to produce high-quality descriptions when provided with an image and text prompt. Three modern VLMs were investigated in this work: LLaVA-NeXT [30], which is an improvement on LLaVA [31], Kosmos-2 [32], and the proprietary GPT-40.\nFigure 3 illustrates the intended usage of VLMs in this work. Instead of using the image-labeled dataset directly, textual descriptions (in the form of sentences) are generated by following the instruction in the input prompt: \"Describe in detail the emotional state of the person in the photo based on her/his facial expression. Provide straight sentences in your answer.\". It is worth mentioning that the prompt does not address the target task directly, i.e., detecting anxiety and/or depression. Nonetheless, experimental results (Section 5) reveal that the pre-trained VLMs were unable to directly predict the depression-anxiety condition. This motivated using VLMs as analysts rather than judges, delegating the final decision to a secondary model, specifically an FFNN.\nTo build the classification model, features are extracted from the text descriptions by using a pre-trained Sentence-BERT [33] model called all-MiniLM-L6-v2, which is designed to embed sentences and paragraphs into a 384-dimensional representation. Two FFNN architectures (default and alternative) are investigated in this work, as expressed in the Equation (1):\n$$f_{net}(x) =\\begin{cases} Wx + b & \\text{(default)} \\\\ W_2(\\text{ReLU}(W_1x + b_1) + b_2 & \\text{(alternative)}, \\end{cases}$$\nwhere $W_i$ and $b_i$ denote a weight matrix and a bias vector, respectively. The default model is a simple linear classifier, while the alternative model includes a hidden layer and is explored in a sensitivity analysis in the experiments. The logits of the network, $f_{net}(x)$, are used to calculate the class probabilities through softmax normalization: Pr($\\cdot$|$x$) = softmax($f_{net}(x)$)."}, {"title": "Experimental Methodology", "content": "This section outlines the elements of the experimental methodology: the data collection procedure involving selfies and PHQ-4 responses, the experiments conducted, and, finally, the hardware and software setup."}, {"title": "Data Collection", "content": "Selfies and PHQ-4 responses constitute a subset of data collected for a research project (local ethics board consent: CAAE 64158717.9.0000.0065) involving high-risk pregnant patients from the Clinical Hospital of the University of S\u00e3o Paulo, a public university hospital in S\u00e3o Paulo, Brazil. The research focused on pregnant women aged 18 and above who could provide informed consent, had at least an elementary education, and owned a smartphone. Data collection utilized smartphones, with the participants capturing natural photos (selfies) in such a way that their faces were visible. They were responsible for uploading image data to the REDCap platform [34] instance hosted at the university, which was also utilized for completing the PHQ-4 questionnaire. The bi-weekly data collection was structured into three periods beginning at $t_0$, $t_1$, and $t_2$, with a final date of $t_k + 12$ weeks, where $k = 0, 1, 2$.\nFigure 4 shows the data distribution after manually removing invalid samples. A total of 147 samples from 108 subjects are represented in the left chart. Although the same subject was expected to contribute with multiple samples, most subjects contributed with a single sample, while one contributed with nine. The right-side chart highlights the imbalance in the data, with a bias towards negative samples (PHQ-4 < 6): 106 negative versus 41 positive samples."}, {"title": "Comparative Evaluation", "content": "The comparative evaluation aims to assess the performance of the CNN- and VLM-based approaches for detecting depression-anxiety in selfies. Recall that while a CNN architecture is trained in the first approach, a FFNN architecture is trained in the latter approach. The performance of pre-trained VLMs for zero-shot screening was also evaluated to serve as a baseline for the VLM-based approach. To conduct the experiments, the collected data was processed to create an image dataset consisting of cropped faces and labels, as discussed in Section 3. The CNN and FFNN models were trained and tested under a Leave One Subject Out (LOSO) cross-validation protocol to avoid performance overestimation. This protocol ensures that each subject's data is used as a unique test set while the remaining data forms the training set, providing a robust measure of model performance across different individuals.\nIn each training-testing session, the samples of a single subject are classified, and the predictions (normal or abnormal) are recorded. Once all predictions are available, performance metrics are calculated. Traditional metrics in binary classification are utilized: precision, recall, F1-score, area under ROC curve (AUC), and accuracy. F1-score is particularly relevant because it accounts for the dataset imbalance, a characteristic of our data. Specific implementation details for each depression-anxiety detection approach are provided below."}, {"title": "CNN-based Approach", "content": "The pre-training on both FER2013 and RAF-DB was conducted for 30 epochs with a fixed learning rate of $10^{-4}$ and a l2-regularization factor of $10^{-5}$. In the default settings, fine-tuning was conducted for 30 epochs, with a learning rate of $10^{-4}$, and an l2-regularization factor of $10^{-5}$. Different parameters were used based on empirical evidence of overfitting in the training. ResNet-18 (RAF-DB): for 50 epochs, a learning rate of $2 \\times 10^{-5}$, and a weight decay of $10^{-6}$. ResNet-50 (RAF-DB): it included a dropout regularization with a probability of 50%. Pre-training and fine-tuning utilized Adam optimization."}, {"title": "VLM-based Approach", "content": "VLMs were configured for deterministic inference, employing a strategy that selects the most probable next token at each step of the generation process. The default (linear) model (Equation 1) was adopted as the classifier. To address the dataset imbalance, the descriptions associated with positive samples (minority class) were upsampled to match the number of negative samples. For each VLM, the classification head underwent training using the Adam optimizer for 15 epochs. Additional training parameters included a learning rate of $10^{-4}$, a batch size of 2, and a l2-regularization factor of $10^{-4}$. During each training session, a randomly selected subset comprising 10% of the training data was reserved for validation purposes. The best-epoch checkpoint was determined based on achieving the highest F1-score on the validation set."}, {"title": "Zero-shot Screening with VLMs", "content": "A natural question arises when using powerful models such as VLMs: \u201cAre pre-trained VLMs capable of zero-shot screening depression-anxiety?\" To answer this question, we conducted an experiment in which VLMs were asked to classify an input image based on the following instruction prompt: \u201cDescribe in detail the emotional state of the person in the photo based on her/his facial expression. Provide straight sentences in your answers. Based on your description, classify the emotional state as either 'normal', 'anxiety', or 'depression'. The output must be exactly one of these words. Follow the template: Output: {result}\u201d. In preliminary tests, only GPT-40 followed strictly the instruction prompt, while LLaVA-NeXT and Kosmos-2 generated longer and more complex descriptions. Therefore, this experiment was performed only for GPT-40. GPT-40 classified each of the 147 samples into one of the three classes: normal, anxiety, or depression. The samples classified as anxiety or depression were considered positive, while the normal samples were considered negative."}, {"title": "Sensitivity Analysis", "content": "This additional experiment explores the effects of incorporating a hidden layer into the FFNN of the VLM-based approach (alternative model, Equation 1). The model was evaluated with various hidden units: h = 4,8,16,...,256. This investigation was motivated by the promising results observed in the VLM-based approach, as elaborated further in Section 5. Furthermore, understanding the impact of the classification head is relevant because it is the only trainable component in the VLM-based pipeline. The LOSO protocol was also employed in this investigation. Additionally, during model training, a dropout layer was incorporated after the ReLU activation function to prevent overfitting."}, {"title": "Hardware-software Setup", "content": "Hardware: Intel(R) Xeon(R) CPU @ 2.20GHz with 32GB of RAM, running Linux Ubuntu 22.04.4 LTS, and equipped with an NVIDIA L4 GPU with 24GB of memory. Software:"}, {"title": "Results and Discussion", "content": "This section presents the results of the conducted experiments: comparative evaluation and sensitivity analysis. Limitations and challenges are also discussed at the end of this section."}, {"title": "Comparative Evaluation", "content": "Table 1 shows the results of the comparative evaluation experiment. Overall, EfficientNetV2 outperformed the compared models among the CNNs for both FER2013 and RAF-DB pre-training, while GPT-40 yielded the best performance among the VLMs. Pre-training on RAF-DB improved most metrics for EfficientNetV2 and ResNet-18, although a notable decrease in performance was observed for ResNet-50 and VGG11. Notably, ResNet-18 yield better perfomance than ResNet-50, despite its reduced size. This suggests that larger models might require more data to achieve better performance.\nRegarding the F1-score, using RAF-DB resulted in only a 0.3 p.p. improvement over FER2013 for EfficientNetV2, while recall decreased by 2.4 p.p.. In medical applications, lower recall indicates the risk of missing a significant number of actual cases, which can lead to undiagnosed conditions and potentially severe consequences for patient health and safety. In this context, a remarkable gain was observed for ResNet-18, whose recall raised from 46.3 to 53.7% with RAF-DB. Despite its lower precision compared to EfficientNetV2, ResNet-18 presents itself as a viable alternative given the critical importance of the recall metric in this context. Moreover, its F1-score is only marginally lower by less than 1 p.p. compared to EfficientNetV2. The use of CNNs represents a more traditional way to address this problem. The obtained results for these models reveal how challenging the task is for the addressed scenario, specifically when using a small dataset.\nAs an alternative, we proposed using pre-trained VLMs due to their ability to analyze faces. Table 1 shows the results for the VLM-based approach with the classification head (FFNN) in its default configuration. Overall, the F1-score for the three evaluated VLMs surpassed the CNN-based models, with GPT-40 achieving the highest value. GPT-40 outperformed the open-source models, Kosmos-2 and LLavA-NExt, across all metrics, except for the recall metric, where Kosmos-2 achieved the same 53.7% as ResNet-18. LLAVA-NExT and Kosmos-2 showed a similar F1-score, with the most significant difference observed in the AUC metric: nearly 6 p.p. in favor of Kosmos-2. A particularly notable result is the F1-score obtained with GPT-40, which surpasses its competitors by a large margin, almost 10 p.p. higher, being the only model to perform above 50% in this metric. In summary, GPT-40 yielded the best performance considering both CNN- and VLM-based approaches.\nThe last row of Table 1 shows the results for the zero-shot screening with GPT-40. While the accuracy in this scenario was superior to that obtained with LLAvA-NEXT and Kosmos-2, the recall and F1-score demonstrated the opposite trend. Remarkably, the recall in zero-shot screening was 17 p.p. lower than that obtained with the VLM-based classification models. By comparing with the GPT-40 in the VLM-based approach, we conclude that the proposed formulation based on description generation and classification is crucial for achieving high performance."}, {"title": "Sensitivity Analysis", "content": "While Table 1 focused on the default FFNN model (VLM-based), this experiment investigated the alternative (single hidden layer) FFNN model (Equation (1)). More specifically, it was analyzed the impact of increasing the model complexity by varying the number of hidden units (h = 4,8,16,...,256). Figure 5 shows the results for this experiment. The dashed line in each chart represents the highest value for the respective metric as reported in Table 1. Notably, the F1-score achieved with GPT-40 and the default classification model (56.0%) - indicated by the dashed line in the third chart (from left to right) \u2013 demarcates an empirical upper bound for this metric."}, {"title": "Limitations and Challenges", "content": "Learning the relationship between facial expressions and self-reported PHQ-4 scores from single selfies can be challenging with small datasets. When using CNNs, prediction is mainly influenced by features around the mouth, as evidenced by the Grad-CAM [35] attention maps for EfficientNetV2 in Figures 6a and 6b. In Figure 6a, the slight smile led the model to classify a positive sample (PHQ-4\u2265 6) as negative. However, facial cues around the eyes might suggest a posed (non-Duchenne) smile, which is not necessarily a sign of happiness or well-being. This issue could be mitigated by incorporating facial action units (AUs) into the learning process, enhancing attention guidance to other critical regions, as recently proposed in a study on basic emotions [15].\nApproximately 12% of the errors with GPT-40 involve positive samples misclassified as negative where the individual is smiling. Figure 6c shows an example of a genuine (Duchenne) smile associated with a PHQ-4 score of 7 (close to the threshold) that was misclassified as negative. The description generated by GPT-40 includes terms such as \u201cThe person in the photo appears to be happy\u201d, \u201cShe is smiling broadly\", and \"Her eyes are slightly squinted\", which accurately reflect the visible facial expressions. This situation is potentially misleading even for experienced face analysts, as the individual's mood is positive, but the PHQ-4 score is close to the threshold.\nNearly 18% of the errors with GPT-40 are related to negative samples with PHQ-4 score lower than 3 described by the model as \u201cneutral\". The sample in Figure 6d (PHQ-4= 2) was described by GPT-40 as \u201cneutral or somewhat weary expression\u201d, \u201ccorners of their mouth are slightly downturned\", \"lack of enthusiasm\". This suggests that the negative elements critically influenced the positive response despite the neutral emotional state described by the VLM. From a data perspective, multiple captures or fusion with complementary data modalities could yield a significant improvement in the overall performance for both smiling and neutral expressions scenarios."}, {"title": "Conclusion", "content": "This work addressed the AI-driven mental health screening in mobile applications using face-centric selfies. The scope of the study included collecting a dataset of selfies paired with responses to a self-reported questionnaire (PHQ-4) and evaluating two depression-anxiety detection approaches. In the comparative evaluation, the proposed VLM-based approach yielded better results than the typical transfer learning with CNNs or zero-shot screening with GPT-40. Notably, GPT-40 achieved the best F1-score (56%) and accuracy (77.6%), while Kosmos-2 attained the highest recall (53.7%).\nThe sensitivity analysis demonstrated that open-source VLMs can yield competitive performance, nearing the Fl-score of GPT-40. When combined with more complex FFNNs, Kosmos-2 and LLAVA-NEXT achieved 56% of F1-scores but with significantly higher recall, which is particularly important in this context. Specifically, Kosmos-2's recall increased to 85.3% (with an F1-score of 52.2%) by adding a 4-unit hidden layer.\nFuture work will explore two main directions. From a data perspective, we plan to collect a larger dataset with more reliable data by developing a smartphone application for multiple-shot passive capture of face images. From a methodological perspective, smile analysis and action unit information will be investigated to address the limitations of the current approach. Furthermore, fusion with complementary data modalities, such as audio and text transcriptions, will be investigated to enhance the screening performance."}]}