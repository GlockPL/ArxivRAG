{"title": "Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts", "authors": ["Shulai Zhang", "Ningxin Zheng", "Haibin Lin", "Ziheng Jiang", "Wenlei Bao", "Chengquan Jiang", "Qi Hou", "Weihao Cui", "Size Zheng", "Li-Wen Chang", "Quan Chen", "Xin Liu"], "abstract": "Mixture-of-experts (MoE) has been extensively employed to scale large language models to trillion- plus parameters while maintaining a fixed computational cost. The development of large MoE models in the distributed scenario encounters the problem of large communication overhead. The inter-device communication of a MoE layer can occupy 47% time of the entire model execution with popular models and frameworks. Therefore, existing methods suggest the communication in a MoE layer to be pipelined with the computation for overlapping. However, these coarse grained overlapping schemes introduce a notable impairment of computational efficiency and the latency concealing is sub-optimal. To this end, we present COMET, an optimized MoE system with fine-grained communication- computation overlapping. Leveraging data dependency analysis and task rescheduling, COMET achieves precise fine-grained overlapping of communication and computation. Through adaptive workload assignment, COMET effectively eliminates fine-grained communication bottlenecks and enhances its adaptability across various scenarios. Our evaluation shows that COMET accelerates the execution of a single MoE layer by 1.96\u00d7 and for end-to-end execution, COMET delivers a 1.71x speedup on average. COMET has been adopted in the production environment of clusters with ten-thousand-scale of GPUs, achieving savings of millions of GPU hours.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in large language models have revolutionized multiple domains, including natural language processing [35, 36], computer vision [16] and multi-modal perception [3, 14]. These achieve- ments demonstrate that scaling up model size can significantly enhance model capacity. However, the growth in model parameters poses substantial chal- lenges for the deployment of such giant models, as computational resources increasingly constrain model capacity [28]. To this end, Mixture-of-Experts (MOE) [29] intro- duces a sparse structure, within which only part of the parameters is activated. Instead of interacting with all parameters in dense models, MoE models allow each input to interact with only a few experts. For example, the Mixtral-8x7B model [12] comprises 45 billion parameters in total, while only 14 billion parameters are active during runtime. Nowadays, MoE has emerged as a key architecture for scaling"}, {"title": "2 Background and Motivation", "content": "Mixture of Experts (MoE) is critical for efficiently scaling models. By enabling sparse activation of pa- rameters, MoE allows for the integration of more pa- rameters without increasing execution costs, thereby enhancing performance. The key idea of MoE is that it consists of multiple small models, namely experts and tokens are only routed to partial experts for com- putation. Figure 2 shows the typical execution flow of an MoE layer and Table 1 explains symbols to describe the execution of an MoE model. Each input token is assigned to one or more experts for computation, with assignments determined by various algorithms [15, 40, 41]. A common method involves a gate network [29] that selects the topk experts for each token, as shown in Figure 2, where token A is routed to Experto, Expert1 and Expert3. After passing through two feed-forward layers of Gen- eral Matrix Multiply (GEMM), the topk outputs are gathered and reduced to produce the final result. The operations in MoE's layer0 comprise token com- munication (dispatch) across GPUs and the first layer of expert computations (GEMM operations), thereby establishing a communication-computation pipeline. MoE's layer1 includes the second layer of expert com- putations, token undispatch and the topk reduction (combine), forming a computation-communication pipeline. MoE employs two primary parallelization strategies: Expert parallelism [13] and Tensor parallelism [33]. In expert parallelism, the weights of different experts are distributed across separate GPUs, with each ex- pert's weights being fully intact. Tokens are routed to the corresponding devices of their respective ex- perts. Figure 2 shows a case for expert parallelism, with Experto and Expert1 reside on GPU0 and oth- ers reside on GPU1. In contrast, tensor parallelism"}, {"title": "2.1 MoE Structure", "content": null}, {"title": "2.2 Computation and Communication Overlapping", "content": "As the MoE architecture grows larger and sparser, the proportion of time spent on communication in MoE models becomes increasingly significant, as shown in Figure 1(a). As illustrated in section 1, coarse- grained overlapping of computation and communica- tion offers limited optimization potential, and kernel- level scheduling is not efficient for dynamic workloads. Thus, it is more efficient to perform the overlapping at a fine-grained granularity (such as token-wise) and integrates computation and communication work- loads into fused GPU kernels. Adopting such a finer- grained overlapping could extremely unleash further optimization opportunities. However, achieving such fine-grained overlapping in MoE is non-trivial and there are two primary obstacles in our observation."}, {"title": "2.2.1 Granularity mismatch between computation and communication", "content": "In MoE systems, the token serves as the fundamental unit of data movement, illustrated by the movement of Token A in Figure 2. To maximize GPU compute efficiency, high-performance GEMM (GroupGEMM) kernels typically organize rows into tiles for process- ing. The purple block in Figure 2 represents such a computation tile in GEMM kernels, exemplified by a 128x128 tile. Therefore, the GEMM computations as- sociated with a single expert may require 128 tokens distributed across multiple GPUs. When fusing com- putation and communication at fine granularity, the disparity between token-level data transfer and tile- level computation introduces considerable challenges: The complex data dependency adversely affects the ef- ficiency of overlap, prompting the use of fine-grained communication, while integrating fine-grained com- munication with computation within fused kernels is also challenging.\nComplex data dependency. The tokens needed for each computation tile, determined by the MoE's gate at runtime, are randomly distributed across multiple devices. Computation for a tile cannot start until all required tokens are available. As shown in Figure 2, Experto's tile does not initiate processing until both Token A and Token B are received. Thus, with coarse- grained data communication, data preparation time for each computational tile may be prolonged because of this irregular and complicated data dependency. To mitigate this, we should employ fine-grained com- munication, where each computational tile reads or writes only the data it requires directly through the Unified Virtual Address [18], and leverage the data reorganization and rescheduling to hide it with com- putation efficiently.\nFine-grained communication. The integration of token-wise communication with tile-wise computation for overlapping is non-trivial. Remote I/O operations between GPUs exhibit significantly higher latency compared to local GPU memory access. Therefore, executing numerous fine-grained read and write op- erations on remote data tokens within computation thread blocks can block subsequent computational tasks, leading to a significant decline in kernel effi- ciency. This challenge is especially evident in the Hop- per architecture, where computation kernels lever- age Tensor Memory Accelerator (TMA) hardware instructions [20] to establish asynchronous compute pipelines. The integration of long-latency remote I/O operations within these asynchronous pipelines can considerably prolong the overall execution time, ad- versely affecting performance. Thus, it is critical to constrain the impact of fine-grained communication on computation kernels.\nOur first insight is that resolving the granularity mismatch between computation and communication in MoE models is the key to enable efficient overlap of these two processes."}, {"title": "2.2.2 Diverse loads of computation and communication", "content": "Another characteristic of MoE is the dynamic routing of tokens to different experts, resulting in varying input shapes for experts at runtime (e.g., the token number received by Expert and Expert1 are differ- ent as shown in Figure 2). This variability imposes differing communication and computation demands on GPUs. Besides, the hardware environments can also have various compute architectures or network topologies, providing different compute capacities and communication bandwidths. Achieving seamless overlap between computation and communication thus requires dynamically adjusting the allocation of GPU resources to different workloads, which is hard to be realized through wrapping workloads into separate kernels."}, {"title": "3 Design of Comet", "content": "In this section, we present the core design of COMET, a Mixture of Experts (MoE) system optimized for efficient execution of MoE layers through pipelined execution and fine-grained overlapping of communica- tion and computation. Our analysis reveals that the MoE architecture has two distinct producer-consumer pipelines: the communication-computation pipeline and the computation-communication pipeline, as il- lustrated in Figure 3. Tokens traverse the pipelines as depicted and the operations within each pipeline are linked through a shared buffer, referred to as the shared tensor, serving as both the producer's output buffer and the consumer's input buffer. To minimize overall latency and enhance pipeline performance, COMET introduces two key mechanisms aimed at overlapping computation and communication work- loads effectively.\n1. Shared tensor based dependency resolving: As previously mentioned, the intricate data dependen- cies between communication and computation pose a challenge to achieving seamless overlap between these operations. To address this, we examine the data dependencies by analyzing the shared tensor. Our analysis reveals that the shared tensor can be decomposed, and the associated computations can be rescheduled to overlap more effectively with com- munication. Accordingly, the dependency resolving process employs two key optimization strategies on the shared tensors as shown in Figure 3: \u2460 Decom- posing the shared tensors along specific dimensions to break the coarse-grained data dependencies and, \u2461 rescheduling the computations to enhance efficiency while ensuring effective overlapping.\n2. Adaptive workload assignment: Following pipeline optimization by the dependency resolving, the pat- tern of communication-computation overlap becomes more consistent and regular. To effectively hide the fine-grained communication latency, it is essential to allocate appropriate hardware resources to both communication and computation workloads. Given that these workloads exhibit different performance characteristics depending on input shapes, model con- figurations, and hardware environments, the adaptive workload assignment scheme dynamically balances computation and communication. This approach gen- erates highly efficient horizontally-fused kernels for the MoE system, thereby optimizing latency conceal- ment.\nAs shown in Figure 3, COMET first leverages the shared tensor based dependency resolving method to optimize the pipelines in the MoE structure by decomposing and rescheduling the shared tensors. According to the reformed pipelines, COMET then provides highly-efficient fused kernels through the adaptive workload assignment mechanism."}, {"title": "3.1 Shared Tensor Based Dependency Resolving", "content": "We now introduce how to resolve the complex data dependency between computation and communica- tion in MoE. It aims to bridge the granularity of communication and computation operations to sus- tain high efficiency by decomposing and rescheduling shared tensors."}, {"title": "3.1.1 How to decompose the shared tensor?", "content": "Shared tensors, as the bridge between the producer operator and the consumer operator, is the key to enable overlapping. Notably, overlapping can oc- cur only when the producer and consumer operate on independent data within the shared tensor, as illustrated in Figure 4. Thus, we analyze the access pattern of operators on the shared tensor and decom- pose it along a specific dimension where data remain independent for the consumer operator. For example, in the communication-computation pipeline in layer0, the consumer operator is a GEMM, with the shared tensor serving as its input matrix. In this case, tokens are independent with each other alongside the M (token) dimension, allowing for de- composition of the shared tensor along M. However, since the computation of a GEMM tile involves mul- tiplication and reduction along the token embedding dimension to produce the final outputs, decomposing the shared tensor along this dimension is not feasible.\nAs for the computation-communication pipeline in layer1, the consumer operator contains a top-K reduc- tion, which reduces tokens along the M dimension, leading to significant interdependencies between to- kens along this dimension. Thus, the shared tensor can only be decomposed along the N dimension where elements are independent."}, {"title": "3.1.2 How to reschedule the decomposed shared tensor?", "content": "At the finest granularity, the shared tensor can be split into individual rows or columns, enabling the consumer to begin computation as soon as a single row or column is received. However, this level of granularity results in low computational efficiency, particularly in pipelines involving compute-intensive GEMMs, which are typically organized and processed in tiles to achieve high utilization. Therefore, after decomposing shared tensors along specific dimen- sions, the resulting sub-tensors must be reorganized and rescheduled into tiles for computation. The rescheduling of shared tensors follows two principles: Rescheduled sub-tensors should align with the orig- inal computation tile granularity for computational efficiency. The scheduling policy should prioritize portions of the producer that can be immediately used by the consumer, allowing the consumer to be- gin execution as early as possible.\nCOMET leverages GroupGEMM to perform the com- putations for all experts on current rank. In the communication-computation pipeline (MoE layer0), the shared tensor, consumed by GroupGEMM, is decomposed along the M dimension. To enable early computation by the experts, tokens are sorted based on their source rank, as shown in Figure 5. The com- pute sequence of tiles in the GroupGEMM is then designed to minimize dependency on remote data, with computation beginning from tiles containing local tokens while the transfer of other remote tokens proceeds concurrently.\nIn the computation-communication pipeline (MoE layer1), the shared tensor undergoes a top-k reduction after processing by the GroupGEMM of experts. As analyzed previously, the shared tensor is decomposed along the N dimension. The tile computation se- quence is adjusted (Figure 6) to enable the consumer operator to start processing before expert compu- tations are fully completed. Instead of computing each expert sequentially, GroupGEMM operations are executed column-wise. This approach allows the reduction and communicate operations to proceed as soon as the first $T_N$ columns of the shared tensors are computed. Without rescheduling, tokens could only be reduced after all experts have completed their computations."}, {"title": "3.2 Adaptive Workload Assignment", "content": "With the decomposition and rescheduling of shared tensors, the pipelines in MoE can now achieve fine- grained overlap. To ensure effective latency hiding, the durations of fine-grained communication and com- putation must be closely aligned to minimize pipeline bubbles. Achieving this requires adaptive resource allocation for both computation and communication, tailored to specific tasks involved."}, {"title": "3.2.1 Thread block specialization", "content": "A straightforward approach to achieve communication-computation overlap in Mix- ture of Experts (MoE) is to encapsulate the entire pipeline within homogeneous thread blocks, integrat- ing communication I/O into the prologue or epilogue of the computation (GEMM), a strategy referred to here as vertical fusion. Through vertical fusion, thread blocks execute concurrently, but the overlap occurs irregularly, leading to non-deterministic latencies of communication and computation, making it challenging to balance their durations for latency hiding. Furthermore, token-level fine-grained I/O in MoE can significantly reduce the computational efficiency of the underlying kernels, particularly on advanced architectures such as Hopper. To address this, we implement thread block-level isolation between communication and computation workloads. This isolation enables precise control over hardware resource allocation for each workload, facilitating a balanced distribution between computation and communication that maximizes latency hiding. Figure 7 depicts the details of the thread block spe- cialized kernel on Hopper, with the critical data path highlighted in red. Due to the isolation between communication and computation, the GEMM thread blocks in COMET utilize the same implementation as the default GEMM before fusion. In the scenario depicted in Figure 7, where the GEMM is compiled using CUTLASS on the Hopper architecture, the GEMM execution is distributed across different warps. Specifically, the producer warp loads data from global memory into a shared memory buffer with the async TMA instructions, while the consumer warp initiates tensor core MMA operations [21]. The communi- cation thread blocks subsequently read the results produced by the consumer warp from global memory. Following the top-K reduction, the warps within the communication blocks either write tokens to the local global memory or transmit them to remote destina- tions. This thread block-specialized programming model is easily portable to other architectures, such as Ampere and Volta, requiring only a substitution of the respective compute thread block implementation.\nHardware resource restriction. The proposed thread block-specialized kernel is designed with the primary objective of minimizing data movement costs. How-ever, this design must also contend with hardware resource limitations. For instance, it is theoretically feasible to integrate communication warps with com- putation warps within the same thread block to elim- inate redundant global memory accesses. However, the thread number restriction of warps constrict the communication operator to fully utilize the commu- nication bandwidth. From another perspective, the warps for communication also interfere with the com- putation warps within the same thread block."}, {"title": "3.2.2 Adaptive thread block assignment", "content": "Suppose that there are n thread blocks for the fused kernel, within which $n_p$ blocks serve as producers in the pipeline and $n_c$ blocks serve as consumers. Iden- tifying an optimal division point $n_p/n_c$ is crucial for maximizing overall efficiency. We demonstrate that the optimal division point is influenced by the shape of input and specific model configurations in an MoE layer. To investigate this, we measure the duration"}, {"title": "4 Implementation", "content": "COMET consists of approximately 12k lines of C++ and CUDA code and 2k lines of Python. COMET provides a suite of user-friendly Python APIs and de- velopers can seamlessly integrate the APIs into their frameworks. In production environment, COMET has been implemented in Megatron-LM for large-scale MoE training. The source code will be available on GitHub.\nOptimized GEMM kernels for MoE. COMET exten- sively utilizes the programming templates provided by CUTLASS to generate highly efficient GEMM kernels. Additionally, it incorporates various opti- mizations to minimize data movement overhead. For instance, in MoE layer 0, the row indices of the input matrix for GEMM operations must be accessed from global memory at each K iteration. By caching these row indices in registers, COMET significantly reduces the global memory access cost.\nNVSHMEM as communication library. We employ NVSHMEM [24] within kernels to support fine- grained communication. NVSHMEM is a communica- tion library designed for NVIDIA GPUs. It creates a global address space for data that spans the memory of multiple GPUs and can be accessed with fine- grained GPU-initiated operations and CPU-initiated operations. Unlike NCCL [23], which targets high- level communication operations, NVSHMEM offers a more composable, low-level API that facilitates finer data access granularity within kernels."}, {"title": "5 Evaluation", "content": "We evaluate COMET on a server equipped with 8 Nvidia H100 GPUs (80 GB memory each). These GPUs are interconnected through NVLink. The GPU-to-GPU communication bandwidth is 377 GB/s on average. Our software environment includes CUDA 12.3, NVSHMEM 2.11, Pytorch 2.4.0 and Megatron-LM (git-hash 6dbe4c).\nWe then compare COMET with several baselines. All baselines are implemented on Megatron-LM, which is a widely adopted framework for high-performance model execution, integrating hybrid parallel strategies.\nThe baselines are: (a) Megatron-Cutlass: Megatron with MoE experts that are implemented through CUTLASS grouped GEMM [22]. (b) Megatron-"}, {"title": "5.1 Experimental Setup", "content": "Testbed."}, {"title": "5.2 Overall Performance", "content": "We evaluate the end-to-end performance of COMET in multiple large MoE models, including Mixtral 8x7B [12], Qwen2-MoE [2] and Phi3.5-MoE [1]. The configurations of these models are shown in Table 2. The experiment is conducted with various input token lengths and diverse hybrid parallel strategies. The ex- perimental details and results are shown in Figure 9. Note that when TP < W, Megatron-LM enables data parallelism for non-MoE layers to improve over- all throughput and the data parallel size is W/TP. The computation of attention layers are identical with different mechanisms using Megatron-LM, and only the MoE layer is implemented differently with diverse mechanisms.\nAs observed, the end-to-end latencies of the bench- marks are reduced by 34.1%, 42.6%, 44.4% and 31.8% with COMET compared with MEGATRON-CUTLASS, MEGATRON-TE, FASTERMOE and TUTEL respec- tively. The performance gain is more prominent with the identical attention computation apart. COMET outperforms other baselines in all configurations be- cause it realizes sufficient overlapping and the schedul- ing inside high-performance fused kernels greatly re- duce the the overhead at CPU side.\nBesides, we can also observe that MEGATRON- CUTLASS and MEGATRON-TE perform similar. This is because they are identical except from the im- plementation of GEMM/GroupGEMM. Neither of them supports overlapping, while MEGATRON-TE performs worse in some cases because of the over- head in transformer engine API calls. TUTEL per- forms better than other baselines because it incor- porates communication into experts' computation through delicate scheduling and adaptive parallelism. Although communication and computation is over- lapped partially, when the number of experts is large (Qwen2), the advantage of TUTEL diminishes because of the large scheduling overhead. FASTERMOE only supports expert parallelism (EP = W) and it also does not perform well on Qwen2 because the experts are small in Qwen2 and the kernel invoking time for experts dominates the MoE layer."}, {"title": "5.3 Detailed Evaluation on a Single MoE Layer", "content": "We then conduct an in-depth examination of a single MoE layer to perform a detailed analysis.\nHandling varying input token lengths. The latency of a single MoE layer with varying input token lengths is shown in Figure 10. With the input token num- ber varying, COMET experiences a shorter duration compared with baselines and the improvement is"}, {"title": "5.4 Adaptiveness to Different Configurations", "content": "We further inquire into the performance of COMET when adapting different model configurations, run- time workloads and system environments.\nPerformance with various MoE parameters. We adjust the number of experts E as well as topk to evaluate the performance of COMET in various MoE structures. The results are shown in Figure 13. With the increasing of topk, the duration of the MoE layer"}, {"title": "5.5 Overhead Analysis", "content": "COMET leverages NVSHMEM to allocate a shared memory buffer for communication on each device. The buffer size is dependent on the model config- uration and equals to MN, where M is the input sequence length and N is the model hidden size. For datatype of BF16 or FP16, the allocated memory size is 2MN. The communication buffer is global for the execution of the entire model, which means that it is shared across layers and experts. We list the device memory consumption of COMET in Table 3, and it is negligible compared with the large device memory on current GPUs."}, {"title": "6 Related Work", "content": "With the successful application of MoE in large-scale distributed training and inference, there are plenty of works focusing on the system-level optimizations of reducing the communication overhead inherited in the MoE structure.\nCommunication optimization. To reduce the com- munication overhead in MoE execution, a straight- forward approach is to leverage efficient communi- cation algorithms [19, 30] for faster data transmis- sion. Recent works [10, 17, 27] also propose the 2D-hierarchical all-to-all algorithm to better utilize intra-node bandwidth and accelerate MoE communi- cation. Some other works propose to reduce commu- nication volume by data compression. For example, ScheMoE [32] and Zhou et al., [39] propose to apply data compression technologies to reduce the all-to-all"}, {"title": "7 Conclusion", "content": "In this paper, we propose COMET, a MoE system that aims to achieve fine-grained communication and com- putation overlapping for MoE. COMET features two key designs to achieve seamless overlapping without impact the computational efficiency: Shared tensor based dependency resolving that enables fine-grained overlapping, while eliminating the bottleneck caused by fine-grained communication I/O; The workload assignment mechanism that promises precise and adaptive overlapping of operators, inducing maximal latency concealing. COMET achieves 1.96\u00d7 speedup in a single MoE layer and 1.71\u00d7 speedup in the end-to-end execution of MoE models, compared with existing literature."}]}