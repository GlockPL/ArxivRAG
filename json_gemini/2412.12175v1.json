{"title": "Explore Theory-of-Mind: Program-Guided Adversarial Data Generation for Theory of Mind Reasoning", "authors": ["Melanie Sclar", "Jane Yu", "Maryam Fazel-Zarandi", "Yulia Tsvetkov", "Yonatan Bisk", "Yejin Choi", "Asli Celikyilmaz"], "abstract": "Do large language models (LLMs) have theory of mind? A plethora of papers and benchmarks have been introduced to evaluate if current models have been able to develop this key ability of social intelligence. However, all rely on limited datasets with simple patterns that can potentially lead to problematic blind spots in evaluation and an overestimation of model capabilities. We introduce EXPLORETOM, the first framework to allow large-scale generation of diverse and challenging theory of mind data for robust training and evaluation. Our approach leverages an A* search over a custom domain-specific language to produce complex story structures and novel, diverse, yet plausible scenarios to stress test the limits of LLMs. Our evaluation reveals that state-of-the-art LLMs, such as Llama-3.1-70B and GPT-40, show accuracies as low as 0% and 9% on EXPLORETOM-generated data, highlighting the need for more robust theory of mind evaluation. As our generations are a conceptual superset of prior work, fine-tuning on our data yields a 27-point accuracy improvement on the classic ToMi benchmark (Le et al., 2019). EXPLORETOM also enables uncovering underlying skills and factors missing for models to show theory of mind, such as unreliable state tracking or data imbalances, which may contribute to models' poor performance on benchmarks.", "sections": [{"title": "1 Introduction", "content": "Reasoning about other people's intentions, goals, thoughts, and beliefs is a foundation of social intelligence. Known as Theory of Mind (TOM) (Premack and Woodruff, 1978), this capability is crucial for effective human interaction. There has been a plethora of recent research that develops theory of mind benchmarks and test LLM capabilities, usually inspired in standard tests for research in children such as the Sally-Anne test (Wimmer and Perner, 1983). However, these tests are not well-suited for extensively evaluating models, as they focus on specific scenarios and lack the variability and complexity required to remain challenging after online pre-training. As a result, many existing computational benchmarks may not be effective in robustly evaluating models' theory of mind abilities.\nWe introduce ExploreToM, an A*-powered algorithm for generating reliable, diverse, and challenging theory of mind data that can be effectively employed for testing or fine-tuning LLMs. Our approach leverages a domain-specific language to generate synthetic story structures and their character's mental states. We then use LLMs to create plausible stories based on these plots, allowing for precise control over the narrative and tracking each character's mental state with high confidence. We employ A* search (Hart et al., 1968) to efficiently navigate the vast space of possible narratives and pinpoint those that are most likely to fool state-of-the-art LLMs. This in turn allows to create a robust, rich dataset that effectively tests the limits of current models (Fig. 1). By generating story structures separately from lexical realizations, we can distinguish the model's core understanding of the social reasoning from vocabulary cues that might give away stylistic hints.\nOur contributions are three-fold: we algorithmically address blind spots in theory of mind evaluation, we"}, {"title": "2 Adversarially constructed stories with ExploreToM", "content": "Building on the standard approach in theory of mind of assessing mental state understanding through question answering (Wimmer and Perner, 1983; Kinderman et al., 1998; Baron-Cohen et al., 1999), EXPLORETOM creates stories where different characters may have different beliefs about the current world state and about other people's beliefs, paired with questions to probe model understanding (see Fig. 1's highlighted story, along with associated questions probing understanding that e.g. \"Anne does not know that Charles knows that the apple has been salted\").\nEXPLORETOM's story generation process is divided into three main steps: plausible story context sampling (Section 2.1), adversarial story structure generation (Section 2.2), and optionally story infilling (Section 2.3)\nan example is outlined in Figure 2. We automatically generate questions to probe understanding of said stories as part of the adversarial story structure generation process (Section 2.2.2); this process finds challenging story structures, i.e., story structures that would yield low accuracy with our generated questions. Because questions are generated automatically and directly from the tracked mental and world states, ground truth answers have a high degree of reliability: we do not use language models at all in the question-answer generation procedure."}, {"title": "2.1 Plausible story context sampling", "content": "We use an LLM zero-shot to generate a consistent and plausible story context, comprising essential elements such as character names, roles, locations, relevant objects, object containers, and discussion topics (see Fig. 2A for a full example). This single-step process ensures a coherent and believable setup for our theory of mind stories. Previous approaches (such as ToMi (Le et al., 2019)) sample objects (e.g., an apple) and object containers independently (e.g. a bottle), often resulting in commonsense violations. Unlike these"}, {"title": "2.2 Adversarially Generating Challenging yet Plausible Story Scripts", "content": "EXPLORETOM's theory of mind-specific language consists of a diverse set of actions A, each transforming the world state and the beliefs of the people involved (the story state s \u2208 S). A story is thus defined as a sequence of actions $(a_1,..., a_n)$, where each action $a_i \\in A$ is a function $a_i : S \\rightarrow S$. Each action also has preconditions to be able to apply it, i.e., restrictions to its domain. For example, a precondition for \"Charles entering the kitchen\" is to not be in it already. Applying an action also automatically updates our world state tracking and belief tracking: for example, \"Charles is now in the kitchen\"; \"Anne knows that Charles is in the kitchen since they were also in the kitchen\"; \"Charles knows that Anne is in the kitchen since he can see her\"; and so forth. All these updates and conditions are specifically programmed and tested; see App. A.1 for the full programs.\nEXPLORETOM enables the generation of diverse stories by significantly expanding the range of supported actions. These actions include physical changes to the world state such as entering and leaving a room (denoted $a_{enter}, a_{leave}$), moving an object to a container (or in general, updating its state; denoted $a_{moveObjContainer}, a_{updateObjState}$ respectively), relocating an object to a different room ($@moveObjRoom$). Additionally, EXPLORE-TOM supports various forms of communication, including: private conversations between two characters, or public broadcasts to all characters in a room; casual discussions about a topic (denoted chit-chat), or notifications about changes in the world state (denoted info); these actions are referred to as $a_{info-private}, A_{info-public}, a_{chitChat-private}, and a_{chitChat-public}$. These actions can occur at any point in the story, allowing for a rich and dynamic narrative (see formal definition in App. A.1) and expanding prior work (Wu et al., 2023).\nEach new action requires carefully writing the implied belief and world state updates, which precludes scaling the number of actions supported. However, we alleviate this by noting that from a theory of mind perspective, many actions are equivalent. For example, \"peeling an apple\" or \"covering an apple in chocolate\" have the same implications with respect to belief updates (a visible property of the apple is being updated, and the witnesses would be the same). Similarly, poisoning an apple has the same implications as moving an apple from a drawer to a fridge (an invisible property is updated, witnesses would be the same, and non-witnesses would not assume there has been an update). The instantiations of these equivalent state updates from a belief perspective are done with an LLM during the story context sampling (see Figure 2.B).\nAsymmetric belief updates In prior work, all belief updates were symmetric: if A and B witnessed an action, then A knows that B witnessed the action and vice versa. Our framework introduces the ability to model asymmetric scenarios. Specifically, we enable the addition of secret witnesses to an action such as someone observing through a security camera, or removal of witnesses without others' knowledge, as in the case of someone becoming distracted by their phone. This added nuance allows for more realistic and complex social scenarios. Asymmetries $a_{peek}$ and $a_{distracted}$ are modifier functions, e.g., as a modifier to \"Beth salted the apple\" ($a_{updateObjState}(\u00b7)$) there may be a secret person peeking ($a_{peek}(a_{updateObjState}(\u00b7))$): \u201cWhile this was happening, Diane witnessed it in secret.\""}, {"title": "2.2.2 Generating Questions and Assessing Resulting Story Difficulty", "content": "We assess a model's understanding of a generated story $s=(a_1,...,a_n)$ by probing it with automatically generated question-answer pairs. EXPLORETOM-generated answers are more reliable than purely-LLM generated ones, since they are directly produced from the states' trajectory with our tracker. Questions may be testing first-order beliefs, second-order beliefs, or regular state tracking: First-order refers to asking about someone's mental state (e.g., \"Does Anne know the apple is salted?\"); Second-order refers to one extra level of recursion in mental state tracking (e.g., \u201cDoes Anne think that Charles know the apple is salted?\"); State tracking may probe about the current state (ground truth) or prior ones (memory).\""}, {"title": "2.2.3 A* Search", "content": "Given a context C and a set of actions A, our main goal is to find challenging story structures. To increase EXPLORETOM's usage flexibility, we support the option of searching for stories s that fulfill desired user conditions $isDesired(s) \\in \\{0, 1\\}$, such as the number of people involved, or the number of actions belonging to a subset $A' \\subset A$ of important actions.\nWe search over the space of plausible story structures of up to m actions. We define this space as a directed graph, where each node is a sequence of valid actions $s=(a_1,...,a_i)$, and there is an edge between s and s' if and only if s is prefix of s', and s' contains k more actions than s. $k \\ge 1$ is the grouping factor for actions, defining the granularity with which we will sample and evaluate nodes. For simplicity, Figure 1 depicts only the new k = 2 actions that each node introduces.\nTo find challenging stories that simultaneously fulfill the user constraints we use A* search (Hart et al., 1968). By definition, A* selects the path that minimizes $f(s) = g(s) + h(s)$, where g(s) is the cost of the path from the start to node s, and h(s) is a heuristic that estimates the cost of the cheapest path from s to a goal node (one of the nodes where it would be acceptable to finish the search). In our context, goal nodes are those such that $isDesired(s') = 1$. We choose A* as our search algorithm precisely because it enables to search this space prioritizing desired user conditions through h(s), as we will detail below.\nA story is said to be challenging for a model if it incorrectly answers our generated questions, i.e., it shows low accuracy. Thus, we define g(s) as our target model's accuracy among all questions for s. We define the heuristic function h(s) as a proxy estimation of the likelihood of generating a full story $s + s'$ that fulfills user constraints $isDesired(s) = 1$, where s' is the continuation of story s:\n$h(s) = \\alpha \\sum_{i=1}^{|S'|} 1(isDesired(s + s'_i) = 1))$\nHere, all $s'$ are randomly sampled continuations of s and $0 < \\alpha < 1$ is a scaling factor. A* requires to evaluate all neighbors of a node s. Since this would be infeasible given the vast space to explore, and that each f(.) evaluation requires several LLM calls (one per question), we restrict the evaluation to a pre-defined constant number of neighbors, prioritized by the closeness of this node to fulfilling the conditions described by $isDesired()$. This pre-defined constant may depend on f(s) to prioritize more promising partial stories (i.e., with lower f(s) values)."}, {"title": "2.3 Story infilling", "content": "Story infilling is the process of transforming a full story structure $s = (a_1, a_2,..., a_n)$ with a story context C into a natural-sounding narration (see Fig. 2D). We infill stories iteratively with an LLM by transforming each action a into a more natural sounding one, according to some stylistic desiderata d, and conditioned on the previously infilled context z (denoted $infill(a, z, d)$). Supported stylistic desiderata d are length requests (e.g., \"use up to two sentences\") or style requests (e.g., \u201cmake this into a conversation\u201d); we optionally also include sampled character goals g and an initial narration context c based on the story s, also generated with an LLM (e.g., Anne's goal may be to oversee that all dishes are rapidly delivered to customers; see initial"}, {"title": "3 ExploreToM as an evaluation benchmark", "content": "We begin by showcasing how EXPLORETOM story structures can be used as a challenging benchmark, highlighting its unique features and advantages.\nExperimental setup We use EXPLORETOM to generate 10 story structures for each of 9 action sets (each with and without asymmetry) and each set of user conditions. Each story generation is allowed to evaluate 50 nodes. User conditions-isDesired(.)-require exactly $p \\in \\{2, 3, 4\\}$ people involved, with $a \\in \\{2, 3, 4\\}$ actions belonging to the set of important actions A', spanning across either r = 1 or r = 2 rooms, and with m \u2264 15 actions in total leading to a total of 162 settings. In all experiments, A' are the actions that add new basic world knowledge: $A'=\\{@moveObjContainer, AupdateObjState, amoveObjRoom, achitChat-*\\}$. We then infill every story.\nWe use Llama-3.1-70B-Instruct (Dubey et al., 2024), GPT-40 (OpenAI (2024); queried early Dec. 2024), and Mixtral-8x7B-Instruct (Jiang et al., 2024) to generate story structures. A* is run with $\\alpha$ = 0.1, P = 50, and k = 3 (i.e. grouping three actions per node). See generation examples in App. D.\nExplore ToM finds challenging story structures for frontier models As shown in Table 1, our EXPLORETOM consistently identifies story structures that are highly challenging for models across various action sets, with average performances in EXPLORETOM-generated datasets as low as 0.09 for GPT-40 (i.e., 9%). When increasing the number of actions, difficulty tends to increase or remain similarly challenging. Performance tends to stay the same or decrease when increasing the number of people involved, possibly because with a fixed number of state-changing actions there will be fewer actions per person which may be easier to track. See Figure 3 and App B.5."}, {"title": "A* is a better strategy than over-generation and filtering", "content": "Over-generation and filtering has become a standard procedure for synthetic data generation (e.g. West et al., 2022; Wang et al., 2023). We measure the effectiveness of A* by comparing the A*-generated data to the data resulting from over-generating stories with our domain-specific language using the same isDesired() criteria and budget as used in the A* search and retaining only the most difficult stories. In a set of 81 randomly-selected settings (50% of the original 162 settings, due to the experiment's high cost), we generate 50 stories with each method using Llama-3.1-70B-Instruct and a budget of 2500 accuracy evaluations each. A* yielded a more challenging dataset (by 2 accuracy points), with shorter stories on average (1.6 fewer actions). This length difference is possibly due to the pressures A* induces towards shorter stories through the heuristic h(s). See Figure 6 for the full distribution of results."}, {"title": "Story structures found adversarially for a model remain challenging for other models", "content": "We evaluate the difficulty of a EXPLORETOM-generated dataset with each model, and find that although there is an increased difficulty towards data generated adversarially with the same model, it remains challenging for all others. Notably, the generated datasets remain challenging even when adding question types not included in the g(\u00b7) optimization (second-order belief questions). See Table 2."}, {"title": "Humans agree with Explore ToM-generated story structures labels", "content": "We conducted a human evaluation to verify the quality of the story structures' automatically-generated labels and the story infillings. For labels, we annotated 100 questions across 12 randomly-sampled story structures from all settings generated for Table 1, and found 99% agreement with our expected answers likely due to the clear and concise nature of our stories and that the ground truth labels were generated by our domain-specific language. We measure story infilling quality by repeating the question-answering procedure with a different set of 100 questions across 12 randomly-sampled infilled story structures. In this case, the human agreed with the ground truth label 89% of the time a small degradation likely due to the LLM-powered method introducing ambiguity."}, {"title": "Infilled stories remain challenging", "content": "Infilled stories with Llama-3.1 70B yielded an average accuracy of 0.61."}, {"title": "4 ExploreToM is effective as training data generator", "content": "Experimental setup We fine-tune Llama-3.1 8B Instruct using a dataset of 79700 (story, question, answer) triples, focusing solely on the completion tasks, and dub the resulting model EXPLORETOM-8B. The dataset comprises both raw story structures and infilled stories, incorporating story structures from each of the 9 action sets listed in Table 1 (excluding asymmetry, and with a balanced number of questions per story type), and various user constraints the same as in Section 3. We do full fine-tuning with the following hyperparameters: a learning rate of $10^{-6}$, 100 warm-up steps, effective batch size of 40 samples, where we fine-tune solely on completions.\nFine-tuning with Explore ToM generalizes well to ExploreToM-generated data with more people and more actions than used in training Since EXPLORETOM-8B is trained with EXPLORETOM-generated data involving $p = \\{2, 3, 4\\}$ people with $m = \\{2, 3, 4\\}$ actions from the set of important actions A', we evaluate generalization within the EXPLORETOM domain by evaluating on EXPLORETOM-generated data involving 5 people, and up to 11 actions. This data is generated with Llama-3.1, the same model as original training data. See Figure 4.\nFine-tuning with Explore ToM improves or maintains performance on theory of mind benchmarks without hurting general reasoning capabilities We evaluate our fine-tuned EXPLORETOM-8B model on five prominent theory of mind benchmarks: ToMi (Le et al., 2019), Hi-ToM (Wu et al., 2023), BigToM (Gandhi et al., 2024), OpenToM (Xu et al., 2024), and FANTOM (Kim et al., 2023). Results show significant improvements in performance on ToMi and HiToM, with accuracy gains of +27 points on both benchmarks (see Table 3). The"}, {"title": "5 On underlying skills needed for theory of mind", "content": "EXPLORETOM enables uncovering and quantifying underlying causes for models' poor theory of mind reasoning in models out-of-the-box. We specifically focus on the lack of robust state tracking skills, and the need for targeted training data in order to improve theory of mind capabilities."}, {"title": "LLMs lack robust state tracking skills", "content": "EXPLORETOM's objective is to find story structures where models fail to answer questions; some of these questions simply require state tracking, specifically the ones where every person would give the same answer (i.e., their mental state is the same in this regard; e.g., in Fig. 1, all X $\\in$ {Anne, Beth, Charles} would answer the same to \u201cWhere does X think Anne is right now?\"). By definition (see \u00a7 2.2.2), these are the uninteresting questions. EXPLORETOM-generated questions are approximately evenly split between interesting and uninteresting, and uninteresting ones are even more challenging on average: the accuracy of interesting and uninteresting questions is 49% and 31% respectively for Llama-3.1 70B, 58% and 37% for GPT-40, and 45% and 26% for Mixtral. See Table 6 in App. B.3 for full breakdown for all settings.\""}, {"title": "Training data biases against theory of mind and its implications", "content": "Figure 5 shows that to successfully improve performance on the ToMi benchmark, EXPLORETOM fine-tuning data needs to be biased towards interesting questions. However, a significant portion of models' training data is likely biased against requiring the tracking of divergent mental states (e.g., news articles).\nAs a conceptual proof that this phenomena occurs even within our custom domain-specific language unless we explicitly bias towards theory of mind, we demonstrate that randomly-sampled story structures tend not to require theory of mind. Using EXPLORETOM's domain-specific language, we randomly generate 1000 story structures with ToMi primitives ({aenter, Aleave, amoveObjContainer}) for stories involving {2,3,4} people and {2,3,4} object movements. We consider a story to not require theory of mind if all first-order and second-order theory of mind questions are un-interesting, as defined in \u00a7 2.2.2 (i.e., all share the same mental state). This stringent criterion evaluates all questions simultaneously. Nevertheless, our results show that 78% or more of the randomly-sampled stories meet this condition across all settings, with up to 87% of stories fulfilling the condition for the smallest setting (2 people, 2 object movements). When considering each question individually, 91%-95% are uninteresting questions. See App. B.4 for more details."}, {"title": "6 Related Work", "content": "Theory of mind benchmarking for language models Theory of mind benchmarks in language models can be categorized into human-generated and model-generated datasets. While human-generated datasets (Shapira et al., 2023b; Kim et al., 2024; Chen et al., 2024) test reasoning about goals, emotions of others, and future actions, they are often limited in size and scope. Machine-generated datasets, such as foundational ToMi (Le et al., 2019) and its successor Hi-ToM (Wu et al., 2023) focus primarily on mental state tracking, but have significant limitations: ToMi only supports a restricted set of actions ({aenter, Aleave, amoveObjContainer}), while Hi-ToM adds @info-* but only as the last action in a story, and both datasets have extremely restricted interactions to orders. In contrast, our method, EXPLORETOM, significantly expands the scope of machine-generated datasets by supporting a larger number of actions, diverse wording, and plausible contexts. Unlike recent approaches that rely on LLMs for generation (Kim et al., 2023; Xu et al., 2024; Gandhi et al., 2024), EXPLORETOM ensures reliability and multi-interaction storytelling, making it a more comprehensive and robust benchmark for theory of mind in LLMs.\nTheory of mind beyond language modeling Theory of mind has been explored in various areas, including human computer interaction (Wang et al., 2021), explainable AI (Akula et al., 2022), and multi-agent reinforcement learning (Rabinowitz et al., 2018; Sclar et al., 2022; Zhu et al., 2021). Recent benchmarks have evaluated theory of mind in multi-modal settings (Jin et al., 2024) and multi-agent collaboration (Bara et al., 2021; Shi et al., 2024), but these focus on goal-driven interactions. Psychologists distinguish between affective (emotions, desires) and cognitive (beliefs, knowledge) theory of mind (Shamay-Tsoory et al., 2010), with cognitive theory of mind developing later in children (Wellman, 2014). Our work targets cognitive theory of mind, which is well-suited for generating situations with a domain-specific language and provides unambiguous answers across cultures. By focusing on cognitive theory of mind, our approach complements existing research and provides a comprehensive benchmark for this crucial aspect of human reasoning in language models.\nSynthetic data generation Synthetic data has become promising approach for acquiring high-quality data in various domains, including multihop question-answering (Lupidi et al., 2024), and language model evalua-tion (Wang et al., 2024). The process involves data augmentation/generation and curation, with techniques such as permutation-based augmentation (Yu et al., 2024; Li et al., 2024a) and iterative prompting (Yang et al., 2022). However, model hallucination (Guarnera et al., 2020; Van Breugel et al., 2023; Wood et al., 2021; Zhang et al., 2023) requires careful filtration and curation to ensure data quality. While prior works have used external feedback (Zelikman et al., 2022; Luo et al., 2024), our approach leverages an external LLM-as-judge to evaluate the plausibility and challenge of generated stories, both before and after infilling. Recently, AutoBencher (Li et al., 2024b) has also been proposed to automatically search for datasets that meet a salience, novelty, and difficulty desiderata, highlighting the importance of careful benchmark creation. Unlike AutoBencher, which over-generates under the assumption that text-based conditioning minimizes hallucinations, our approach lifts this assumption and actively searches the space of possible narratives. This enables to create high-quality synthetic data regardless of the likelihood of a story being generated zero-shot, and generating even more challenging stories than with over-generation."}, {"title": "7 Conclusions", "content": "Theory of mind (ToM) is essential for social intelligence, and developing agents with theory of mind is a requisite for efficient interaction and collaboration with humans. Thus, it is important to build a path forward for imbuing agents with this type of reasoning, as well as methods for robustly assessing the of models' theory of mind reasoning capabilities.\nWe present EXPLORETOM, an A*-powered algorithm for generating reliable, diverse and challenging theory of mind data; specifically, creating synthetic stories that require theory of mind to understand them, along with questions to probe understanding. EXPLORETOM's adversarial nature enables the stress testing of future models and making our evaluation more robust to data leakage. We show that EXPLORETOM generates challenging theory of mind evaluation sets for many frontier models, with accuracies as low as 0% for Llama-3.1 70B Instruct and 9% for GPT-40. Moreover, we show that EXPLORETOM can be used as a method for generating training data, leading to improvements of up to 29 accuracy points in well-known theory of mind benchmarks. Synthetic data is crucial for this domain, given that data that articulates theory of mind"}, {"title": "Limitations", "content": "EXPLORETOM offers a valuable tool for theory of mind research, and is a first step towards developing LLMs that can handle social interactions effectively. Although its data encompasses diverse and challenging settings more than previously available, and is grounded in established psychological tests, EXPLORETOM necessarily simplifies the complexity of real-world states and narratives by constraining it to the supported types of actions and interactions. Our framework requires manual coding of new actions, wich can be time-consuming process but comes with the benefit of a significant reliability improvement. Furthermore, our stories are not necessarily goal-oriented narratives, highlighting an important avenue for future work: creating datasets where actions stem directly from character goals to further enhance diversity and plausibility."}, {"title": "Appendix", "content": "A Appendix"}, {"title": "A.1 Actions' formal definition (cont. from 2.2.1)", "content": "All actions are functions that transform a state into another state, updating the world state and the beliefs of everyone involved up to two levels of recursion. All actions have preconditions, e.g. to enter a room you need to not be in it already.\nA state \u2208 S is comprised of a world state ws (the things currently true physically about the world described), the first-order beliefs b\u2081, and the second-order beliefs b2. First-order beliefs describe what each person believes to be the current world state, e.g. Anne believes that the apple is salted. Second-order beliefs describe what each person estimates that each other person believes to be the current world state, e.g. Anne believes that Beth thinks that the apple is salted.\nLet's describe the definition of leaving in a room through an example: \"Beth left the kitchen.\", and build the definition of the action function aleave, Beth, kitchen : S \u2192 S. As described above, the state is comprised of a world state, first-order beliefs, and second-order beliefs, i.e.,\n$a_{leave, Beth, kitchen}(ws, b_1, b_2) := (ws', b_1, b_2)$\nLet's first describe the world state update ws'. The world state remains the same for every entity (object, container, person, etc.), except for the person leaving the room-Beth. Thus,\nws(q, room) = ws' (q, room) q \u2260 Beth and ws' (q, room) = -kitchen\nLet's then describe the first-order belief updates b'1. Here, we assume that everyone in the same room as Beth (the kitchen) will know that Beth has left. We denote this group of people as witnesses(kitchen):\nwitnesses(kitchen) := {p|ws(p, room) = kitchen}\nEveryone not in the kitchen will assume that Beth is still there unless communicated otherwise, since they have no reason to believe she has left. Thus,\nb\u2081 (p, Beth, room) = b\u2081 (p, Beth, room) = kitchen \u2200p \u2209 witnesses (kitchen)\nb\u2081 (p, Beth, room) = \u00abkitchen \u2200p \u2208 witnesses (kitchen)\nWe now describe the second-order belief updates b. Here, we assume that everyone in the kitchen (including Beth) assumes that everyone else in the kitchen knows Beth left (and only them). If someone was not in the kitchen, they will assume nothing has happened. Formally,\nb2 (p, q, Beth, room) = b2 (p, q, Beth, room) = kitchen \u2200p \u2209 witnesses (kitchen), q\nb2 (p, q, Beth, room) = \u00abkitchen \u2200p \u2208 witnesses (kitchen) \u2200q \u2208 witnesses (kitchen)\nb2 (p, q, Beth, room) = kitchen \u2200p \u2208 witnesses (kitchen) \u2200q \u2209 witnesses (kitchen)\nFinally, the function can only be applied if Beth is in the kitchen, i.e. it has the precondition ws (Beth, room) = kitchen.\nAll other functions definitions can be found verbatim in the code to be released."}, {"title": "A.2 All supported questions (Cont. from Section 2.2.2)", "content": ""}, {"title": "B Additional Experiments", "content": ""}, {"title": "B.1 A*-generated stories are more challenging than overgenerating and filtering (cont. from \u00a7 3)", "content": ""}, {"title": "B.2 Infilled Story Structures Remain Challenging (cont. from \u00a7 3)", "content": ""}, {"title": "B.3 Models Fail Both at Theory of Mind and Pure State Tracking (Cont. from \u00a7 5)", "content": ""}, {"title": "B.4 How likely is a randomly-sampled story to require theory of mind? (cont. from $5)", "content": ""}, {"title": "B.5 On why a story with a greater number of people may counterintuitively imply a lower average difficulty", "content": "In EXPLORETOM, the number of people and actions is important from a controllability and diversity perspective, but does not directly quantify task difficulty-difficulty quantification for theory of mind is an active area of research. Huang et al. (2024) quantifies a theory of mind problem complexity as the number of states necessary to solve it correctly (note that their approach requires manual annotation). While the number of states tends to increase with the number of people and actions, many questions do not require analyzing the whole story, e.g. if someone only entered the scene right at the end. When randomly sampling stories while fixing the number of core actions to e.g. 5, it's more likely to have some characters with little involvement in the scene if there are 5 people in total than if there are 2 people. Since accuracy is computed across all questions about all characters, having a larger number of people may bump the average accuracy. EXPLORETOM's flexible framework allows for minimizing these cases through modifying the lookahead, but we chose against both doing this or filtering questions to show the performance is low even without these considerations."}, {"title": "C Prompts used for generating and validating ExploreToM's data", "content": ""}, {"title": "C.1 Generating story contexts (cont. from \u00a72.1)", "content": "Suggest a short context where {num_people} people are together in a room. It should be at most two sentences long, and they should be able to observe each other. Later in the story, characters are going to move around and store objects, so your context should be plausible under those constraints. Do not explicitly include that they can all see each other, it should be clear from context. The room could be in a house, work environment, etc."}, {"title": "C.2 Prompts used for story infilling", "content": "You are an expert writer that uses simple language, avoiding sounding unnatural or clich\u00e9. You are clear, creative, and helpful. You use simple sentence constructions and words so that everyone may understand you."}, {"title": "D ExploreToM examples (cont. from \u00a73)", "content": "See a large sample of EXPLORETOM-generated data in https://huggingface.co/datasets/facebook/ExploreToM. We also include a few examples below."}, {"title": "D.1 Story structure examples"}]}