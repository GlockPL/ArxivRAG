{"title": "ALIF: Low-Cost Adversarial Audio Attacks on Black-Box Speech Platforms using Linguistic Features", "authors": ["Peng Cheng", "Yuwei Wang", "Peng Huang", "Zhongjie Ba", "Xiaodong Lin", "Feng Lin", "Li Lu", "Kui Ren"], "abstract": "Extensive research has revealed that adversarial examples (AE) pose a significant threat to voice-controllable smart devices. Recent studies have proposed black-box adversarial attacks that require only the final transcription from an automatic speech recognition (ASR) system. However, these attacks typically involve many queries to the ASR, resulting in substantial costs. Moreover, AE-based adversarial audio samples are susceptible to ASR updates. In this paper, we identify the root cause of these limitations, namely the inability to construct AE attack samples directly around the decision boundary of deep learning (DL) models. Building on this observation, we propose ALIF, the first black-box adversarial linguistic feature-based attack pipeline. We leverage the reciprocal process of text-to-speech (TTS) and ASR models to generate perturbations in the linguistic embedding space where the decision boundary resides. Based on the ALIF pipeline, we present the ALIF-OTL and ALIF-OTA schemes for launching attacks in both the digital domain and the physical playback environment on four commercial ASRs and voice assistants. Extensive evaluations demonstrate that ALIF-OTL and -\u041e\u0422\u0410 significantly improve query efficiency by 97.7% and 73.3%, respectively, while achieving competitive performance compared to existing methods. Notably, ALIF-OTL can generate an attack sample with only one query. Furthermore, our test- of-time experiment validates the robustness of our approach against ASR updates.", "sections": [{"title": "1. Introduction", "content": "Smart devices pervasively integrate voice control functionality. Users are getting used to interacting with smart devices with their voice to enjoy hands-free convenience. As a result, smart devices, such as smartphones, smart speakers, and automobiles, have adopted the voice assistant (VA) function to turn themselves into voice-controllable devices. In point of fact, over 132 million people used VAs in the US in 2021, and the number continues to grow [1]. More than 4.25 billion VAs have been installed globally, projected to reach 8.4 billion by 2024 [2].\nThe prevalence of voice-controllable devices brings security risks. Smart appliances take voice commands as input for performing heterogeneous actions, including security and safety-critical tasks, such as thermal adjustment, online payment, and even autonomous driving [3], [4]. Attackers can exploit the voice interaction to inject malicious speech commands without raising users' suspicions and cause se- vere security and safety consequences, including economic losses, privacy violations, health issues (e.g., thermometer overheating), bodily harm (e.g., car accidents), etc.\nInjecting malicious commands covertly into voice- controllable devices has been realized with adversarial audio techniques [5], [6], [7], [8], [9]. The requirements of an attack audio are twofold: to maintain the covertness of the attack audio, it cannot sound like the intended command; to guarantee the attack's success, it should be correctly recognized as the intended command by a speech recog- nition model. The adversarial example (AE), which adds minute perturbation to original audios, naturally fits the attack requirements as the method does not affect users significantly and can affect the target model.\nThe problem of generating AEs to attack commercial voice-controllable devices is formulated as a black-box ad- versarial attack problem. The speech recognition models applied by commercial products are unknown to the at- tacker. To generate qualifying AEs, the adversaries usually query the black-box model thousands of times with trial audio examples, get the query results (i.e., transcription), and iteratively optimize the attack examples leveraging the transcriptions as optimization guide [10], [9], [11].\nThe primary goal of black-box studies is to reduce attack cost, but the state-of-the-art solutions are still lacking in their suitability for practical applications. Considering the generation of each attack sample requires abundant queries and each one has a tangible monetary cost\u00b9, existing works seek to improve the query efficiency and therefore reduce the attack price. Improving the query efficiency can also ben- efit the covertness of the attack. High-frequency querying with similar audio content may trigger the network defense"}, {"title": "2. Background", "content": "This section introduces speech recognition/synthesis and the conventional black-box AE attack method."}, {"title": "2.1. Automatic Speech Recognition", "content": "An ASR system takes human speech signals as the input and produces the semantic meaning of the spoken content in the form of text. We denote the input waveform as x, the ASR system as f(\u00b7), and the output transcription as y. The function of an ASR is formulated as y = f(x).\nThe upper part of Figure 2 shows a typical ASR system's architecture consisting of feature extraction, acoustic model, and language model. Classical ASR applies a Gaussian Mix- ture Model-Hidden Markov Model (GMM-HMM) to build the acoustic model. With the rapid development of deep learning, modern ASRs universally apply neural networks as their core and achieve outstanding recognition perfor- mance. Depending on the model architecture and neural network variants (e.g., CNN and RNN) used, a wealth of ASR categories exist. Introducing different ASR models in detail is out of the scope of this paper, and we present the representative components instead. The feature extraction component extracts acoustic features essential to the seman- tic information; then, the acoustic model predicts the most probable linguistic units based on the features. Finally, the language model generates the text sequence with the highest probability."}, {"title": "2.2. Speech Synthesis", "content": "Speech synthesis, also known as Text-To-Speech (TTS), aims to synthesize natural and intelligible speech signals given an arbitrary sample of text as input [14]. The task can be formulated as x = f(y). The lower part of Figure 2 shows the architecture of a typical TTS model consisting of three components: text analysis, acoustic model, and the vocoder."}, {"title": "2.3. Black-Box Adversarial Audio Attacks", "content": "Basic Adversarial Examples. The goal of an AE-based attack is to deceive the ASR system into transcribing input waveform x incorrectly by adding perturbations \u03b4, resulting in a transcription that does not match the correct tran- scription y. In a non-targeted attack, the requirement is that SR(x + \u03b4) \u2260 y, where SR denotes the recognition function of the ASR. In contrast, in a targeted attack, the condition is that SR(x + \u03b4) = T, where T represents the malicious command the adversary intends to activate. Since AEs typically require the perturbations to be imperceptible, the amplitude of \u03b4 is usually constrained. The formulation of AE-based attacks can be expressed as:\narg min L(SR(x + \u03b4), T) + \u03b1 \u00b7 ||\u03b4|| (1)\nwhere L denotes the loss function indicating the similarity between the attack audio transcription and the target text. \u03b1 is the weight to trade off being covert and adversarial. To solve the problem, a loss value from L is calculated, and gradient descent is applied to find the optimal \u03b4. The gradient calculation requires information about the ASR architecture and parameters, which is impractical in the real world, where the specifics of commercial ASR are unknown.\nBlack-Box Adversarial Examples. In a black-box setting, internal knowledge, such as the weights of the target ASR, is unknown. The attacker can only obtain the final tran- scription, therefore, cannot calculate L, nor can they apply gradient descent to solve for the optimal \u03b4. To ensure the intended command can be activated, the AE training process usually starts from the target command audio sample and gradually modifies the sample towards another unsuspicious signal x, aiming to generate an attack audio sample that sounds benign but is recognized as the target command. The problem in Formula 1 is reformed as\narg min L = {\n||\u03b4||p, SR(x + \u03b4) = T\n+\u221e,\notherwise\n(2)\nwhere ||\u03b4||p is a norm function to limit the perturbation, and L is the objective function. Given the opaque ASR model, the optimization is based on heuristic techniques, which make no assumption about the model, search a large optimization space, and develop iteratively with experience learned from the previous round. The experience is learned from the query results. Such an iterative process normally requires extensive queries. Related studies have utilized evolution algorithms [15], [9] and gradient estimation meth- ods [11] as the training methodology."}, {"title": "3. System and Threat Models", "content": "We demonstrate our attack schemes for both the digital domain and the physical environment, namely over-the- line (OTL) and over-the-air (OTA) scenarios. Leveraging the ALIF pipeline, we employ different training methods to generate adversarial audio samples against commercial ASRs in each scenario. The method for generating the adversarial attack in the OTL scenario is called ALIF- OTL. Notably, we propose a new type of digital attack called the platform attack in the OTL scenario, where the online subtitle transcribing service is misled. The method for generating the adversarial attack in the OTA scenario is called ALIF-OTA. This section presents the system and threat models of ALIF-OTL and ALIF-OTA."}, {"title": "3.1. System and Threat Model of ALIF-OTL", "content": "System Model. The system model of ALIF-OTL is pre- sented in Figure 3. Online video and multimedia content platforms (e.g., YouTube) have started providing a subtitling service, which supports an automatic transcribing function for on-demand and live media content. These platforms apply an ASR to recognize the semantics of the audio track and generate subtitles. Audiences can opt-in to the service through a corresponding option in the playback agent, which allows subtitles to be shown as the video plays. As Amazon introduces [16], subtitling content helps improve accessibility and engagement. It can also be a compliance requirement for video programming distributors to support hard-of-hearing users. In addition to utilizing the video platform's own service, content creators can use third-party services (e.g., Amazon Transcribe) to add subtitles for their media content.\nAttacker's Goal. The attacker desires to launch an adver- sarial audio platform attack. The goal is to damage the reputation of media content providers or cause trauma to audiences of particular communities. To achieve this goal, the attacker delivers the adversarial audio as the audio track of a video. The attacker can deceive the ASR of either a third-party subtitling service or the native feature of a video"}, {"title": "3.2. System and Threat Model of ALIF-OTA", "content": "System Model. Figure 4 presents the system model of ALIF-OTA, which represents a typical adversarial audio attack scenario. In the OTA scenario, the attack targets can be categorized into two types: commercial online ASR APIs and VAs (such as smart speakers and virtual assistants on computers). Our study specifically includes online ASR APIs to account for practical considerations, as small-scale companies may purchase online APIs rather than develop their own ASR system."}, {"title": "4. Motivation and Our Design", "content": "Existing black-box attacks suffer from query ineffi- ciency (except for NI-OCCAM [9], a non-interactive attack method). Additionally, they exhibit susceptibility to model updates, as outlined in Section 1. These inefficiencies lead to substantial attack costs. Next, we introduce our proposed design to address these issues.\nThe vulnerability of deep learning (DL) models to im- perceptible perturbations has attracted much attention since the works of Szegedy et al. [19] and Biggio et al. [20]. A recent study by Shamir et al. [21] proposes a dimpled mani- fold model (DMM), which provides a better explanation for the working mechanism of AE. The effect of AE is closely related to the model's decision boundary. During model training, the initially randomly oriented decision boundary quickly aligns to a low-dimensional manifold that contains the representation embedding of all training samples. In the second training phase, the decision boundary starts dimpling, and shallow bulges are generated to move the decision boundary towards the right direction around the data embedding according to the labels. When a service provider fine-tunes its ASR model with newly-collected training data, such as noisy data for robustness improvement, the decision boundary undergoes these two training phases again, resulting in the decision boundary reforming around the new sample embeddings. This reformulation of the decision boundary renders previous AEs invalid with a high probability. Existing AEs add perturbations in the raw input space rather than in the lower-dimensional representation space, which does not guarantee enough distance between the attack sample embedding and the decision boundary. As a result, AEs are easily affected by changes in the decision boundary induced by model updates. To address these challenges, we propose ALIF, a novel attack scheme that reduces inefficient querying and significantly improves reliability."}, {"title": "5. ALIF-OTL: Over-the-Line ALIF Attack on ASR APIS", "content": "In this section, we present a detailed description of the ALIF-OTL algorithm. It is important to note that ALIF"}, {"title": "5.1. Design Intuition", "content": "As revealed and verified by a massive number of pa- pers (e.g., Ruderman et al. [22], Pope et al. [23], and Shamir et al. [21]), real-world data distribution can be described by low dimensional structure (i.e., a manifold). The low-dimensional manifold is easier for neural networks to learn and form complex decision boundaries from a relatively small amount of training samples. According to DMM, the decision boundary evolves primarily based on the low dimensional representations of the input sample (i.e., images or text), which heavily affect the establishment and the counterintuitive properties of adversarial examples. As the left of Figure 5 shows, existing black-box AE training adds perturbation to the raw input to shift the low di- mensional representation, which is an indirect optimization process. Because the process lacks accurate guidance from the gradient descent method, the indirect search method is inefficient. Regarding the vulnerability to model updates, existing works [11], [9] iteratively optimize attack audio samples in the input space (see Section 2). The process stops when the pre-defined maximum number of queries is reached. Since the raw input space is not where the decision boundary lies, this process cannot ensure a large enough distance between the AE and the decision boundary, which increases the risk of attack audio failure when the decision boundary changes. To address the bottlenecks, our key idea is to construct perturbations directly from the decision boundary space and make the distance between attack samples and the boundary farthest allowed under certain constraints (i.e., right part of Figure 5).\nConsidering the architectures of ASR and TTS models in Figure 2, we can consider the two models to be recip- rocal processes. Both ASR and TTS have a key component called the acoustic model. The purpose of this component in the two models is the same: to map the relationship"}, {"title": "5.2. Our Method", "content": "Design Overview. ALIF-OTL creates incomprehensible audio to attack the subtitling APIs online. A conventional TTS generates a Mel spectrogram, which represents text content, then applies a vocoder to the spectrogram to syn- thesize signals representing the intended speech. We add perturbation to the representation of linguistic feature space to achieve a similar goal as hidden command attacks [18], [7], which is to make the attack audio not sound like the command anymore but still recognizable as a command. As a result, users will stay unaware of the occurrence of an attack.\nFigure 6 shows the ALIF-OTL attack scheme. Our back- bone model for audio generation is based on a well-known TTS called Tacotron2 [25], shown in the shaded part of Figure 6. The output of the Tacotron2 decoder consists of the predicted Mel spectrograms and the alignments. The former is the input of the vocoder, while the latter is used to predict whether the output sequence has been completed. We use a similar loss function to Tacotron2, which includes LMel and LGate. LMel is used to measure the difference between the original Mel spectrogram and the adversarial one, while LGate controls the adversarial audio to have a similar length to the benign one. We will introduce them later.\nTo make an audio sample highly-distorted that indi- viduals cannot understand, we optimize the perturbations using gradient descent to minimize the loss function. We observe that the magnitude of the perturbation is negatively correlated with the text similarity. The higher the amplitude of the perturbation, the more significant the difference be- tween the transcription of the attack audio and the target content, which aligns with intuition. Based on this observa- tion, we propose two variants of attacks: online and offline. We generate ten candidate adversarial embeddings for a target command and optimize each embedding vector for 50 iterations. Within this process, we query the target ASR API and calculate the character error rate (CER) to check whether the transcription is the same as the target command. During the online attack, we only query the target API with audio samples if the loss calculated between the original and perturbed spectrograms is reduced. In contrast, we only"}, {"title": "6. ALIF-OTA: Over-the-Air ALIF Attack on Voice Assistants", "content": "6.1. Technical Challenges\nBecause ALIF-OTL does not consider the impact factors of physical playback, it cannot be applied to in the OTA sce- nario. One big challenge is generating attack audio samples that can overcome the distortion brought by the physical playback environment. In traditional AE attacks, the attacker can incorporate OTA impact, such as reverberation, into the training objective function in the perturbation genera- tion stage to achieve a \u201crehearsal\u201d effect. This processing significantly improves the attack robustness in the physical playback. However, for the ALIF attack, the origin of our optimization is the low-dimensional embedding, which is not the natural waveform domain. It is unknown how to"}, {"title": "6.2. Our Method", "content": "The key idea is to guide the optimization direction in a way that physical interference is considered. Meta- heuristic algorithms are suitable in this context. Inspired by Xie et al. [28], we solve the problem with Particle Swarm Optimization (PSO).\nParticle Swarm Optimization. PSO is a kind of meta- heuristic algorithm which is designed to solve nonlin- ear functions [29]. It requires no knowledge and does not need the problem to be differentiable. In more de- tail, the algorithm first initializes a population of random solutions. Every solution can be regarded as a particle Xi = {Xi1, Xi2, ..., Xid} and the value of the solution is the position of the particle in the hyperspace. Meanwhile, each particle has a randomized velocity Vi = {Vi1, Vi2, ..., Vid} and then they \"fly\" iteratively through the hyperspace. Specifically, during iteration, the global best position and the personal best position of each particle are recorded. Then in each iteration, the velocity and position of each particle can be updated as follows:\nvij = w \u00b7 vij+c1 \u00b7 r1 \u00b7(pbestij-xij)+c2 \u00b7 r2 \u00b7(gbestj-xij) (7)\nxij = xij + vij (8)\nwhere vij means the jth dimension of the velocity of ith particle in the current iteration. pbest and gbest are the personal best position and global best position. r\u2081 and r2 are two random uniformly distributed numbers between 0 and 1. w is the inertia weight while c\u2081 and C2 are two acceleration constants. In this work, we regard the noise added to the linguistic embedding as a particle. Unlike the original PSO, we initialize all the particles with a zero value to help them find the initial best position. We randomize r\u2081 and r2 at every iteration for every particle. In order to reduce the computing overhead, each dimension of the same particle shares these two random numbers. We also set a threshold and only update the particle when it does not exceed the threshold. As a result, Eq. 8 becomes:\nxij = xij + vij, when max(xij + vij) < threshold (9)\nDesign Overview. Our goal in the OTA case is more challenging than the OTL case becuase of additional in- terferences in the physical playback environment. Based on the same ALIF pipeline shown in Figure 6, we use PSO to search for the optimal perturbation iteratively. The perturbation vector Emb is the particle, and the set of the embedding range consists of the optimization space. The ALIF-OTA scheme differs from the OTL counterpart in an extra noise overlay step. We intentionally create and combine the white noise with the generated waveform. The particles recognized correctly under this constraint have a better chance of succeeding in the OTA scenario. We do not"}, {"title": "7. Experiments", "content": "For ALIF-OTL, we first present a large-scale baseline evaluation on industry-grade cloud ASR APIs. Then, we pick the attack audio generated from the baseline study to launch the platform attack against the subtitling services. For ALIF-OTA, we attack the same online APIs in the over-the-line scenario but launch the attack in the physical environment. Additionally, we examine the feasibility of attacking VAs. Lastly, we conduct an impact factor study to investigate the overall performance of the ALIF-OTA attack."}, {"title": "7.1. Experimental Setup", "content": "Target Commands and Text. We aim to generate attack audio samples that can be successfully transcribed as dif- ferent speech commands\u00b3 and common sentences. We use the same target commands and text for both ALIF-OTL and ALIF-OTA evaluation. In this section, we mainly conduct evaluations based on a dataset consisting of ten commands and two sentences. To validate the efficacy of our method,"}, {"title": "7.2. Evaluation of ALIF-OTL Attacks on Cloud Speech-to-Text APIs", "content": "Baseline. To evaluate the effectiveness of the ALIF-OTL attack, we perform the attack against four cloud ASR APIs. Each attack instance is generated after at most 50 queries to the target API. Specifically, for the online attack, we only query the target ASR as long as the loss continues to decrease, and terminate after 50 iterations regardless. For"}, {"title": "7.3. Evaluation of ALIF-OTA Attacks on APIs and Voice Assistants.", "content": "Attacks on cloud ASR APIs. We attack the same APIs of ALIF-OTL but launch the attack by playing attack audio samples out loud, using a speaker. We conduct the evaluation in a normal bedroom using a Marshall EMBERTON II speaker and use a fixed speaker-to-microphone distance of 15cm (the same setting as Zheng et al. [9]). Table 3 demon- strates the attack performance of ALIF-OTA. The column of \"digital SR\" refers to the success rate of our ALIF- OTA algorithm in the digital domain. In most parameter settings, our method can successfully generate almost all target commands. We play and record all the attack audio samples three times and provide them to the corresponding API. We consider an attack sample successful if at least one of the three recordings is transcribed by the targeted API to the intended target command. The results show that despite enduring interference in the physical domain, our attack can achieve a success rate of more than 50% towards all the APIs under most parameter settings. Specifically, the success rate is 81.2% under \u03b1 = 1, \u03b2 = 4, \u03b3 = 2 and \u03b7 = 0.1.\nAttacks on voice assistants. Assuming the ASR systems behind the online API and the VA of the same company are similar, we use the adversarial audio samples generated from the API attack scenario to attack the Amazon Echo Dot and Microsoft Cortana (i.e., a transfer attack). Since Amazon Echo can't respond to \"Darn it!\" and \"I can't take it anymore!\", we generate one attack example for each of the remaining ten commands. Under the same environment settings as the cloud ASR API attack, we regard the attack as successful if the targeted command can be correctly transcribed within ten attempts (i.e., play each attack sample ten times). The results are in Table 4. Under different parameter settings, our attack can achieve an average success rate of up to 69.2% on VAs (80% on Echo and 58.3% on Cortana), which illustrates the effectiveness of our OTA attack. Although Ni-OCCAM can also attack the VAs with a low cost, their success rates (average 50% on Echo and Cortana) are lower."}, {"title": "7.4. Impact of Various Factors on ALIF-OTA", "content": "Speaker dependency. Different speakers have various hard- ware properties that affect the audio attributions. We intend to understand the sensitivity of audio samples generated by ALIF-OTL to the particular sound profiles of individual playback devices. Table 5 describes the attack success rate of our attack commands in different hardware and environ- mental settings. We pick all examples generated from ALIF- OTA using the parameters \u03b3 = 1, \u03b2 = 1, \u03b1 = 0.3, and \u03b7 = 0.1 and test their performance using products from three well-known speaker manufacturers. The results demonstrate that our attack examples exhibit similar performances when played by the three speakers. All speakers achieve a success rate of higher than 57.1% (4/7) on Echo and Cortana at a distance of 15cm. The only exception is using the JBL"}, {"title": "7.5. User Study", "content": "We carried out a user study to gain deeper insight into human perception of the attack examples we generated. The study comprises two sections: audio incomprehensibility and ablation of different ALIF components. We recruited a varied group of 20 volunteers, aged 22 to 32, all with normal hearing abilities. Notably, five participants are native English speakers.\nAudio comprehensibility. We selected 32 distinct command examples produced by ALIF-OTL under the parameter set- tings of \u03b1 = 0.3, \u03b2 = 1, and \u03b3 = 1. We recruited volunteers"}, {"title": "7.6. Long-Term Effectiveness of ALIF", "content": "We validate the robustness of ALIF attacks against model updates by evaluating the long-term effectiveness of our attack audio samples.\nWe generate 50 to 100 effective attack examples target- ing each of the four commercial APIs on October 1st, 2022, and test them after three months. The success rates of our attack audios on different dates are presented in Table 9. Although the success rate on iFLYTEK decreases by about 25%, the success rates on Amazon and Azure drop by less than 10%. Notably, the attack performance on Amazon remains unchanged after three months. Furthermore, after extending the testing period by an additional half month, the performance across all ASRs remained steady, with multiple examples of each command successfully executing the attack.\nALIF was employed to optimize the added perturbation to the linguistic feature to decrease audio comprehensibility. This is in contrast to existing black-box methods that add perturbation to shift the command signal towards another unsuspicious audio sample, bringing the signal closer to the ASR's decision boundary. Although ASR updates may still affect ALIF's performance, it has shown relative resilience."}, {"title": "7.7. Comparisons with related adversarial attacks", "content": "The key distinction between our work and existing AE studies is the way of constructing adversarial audio samples. Our work functions uniquely compared to existing works to"}, {"title": "8. Related Work", "content": "In this section, we survey studies related to speech com- mand injection attacks. Note that we mainly cover research focusing on signal generation and do not include the work utilizing hardware property [5], [36], [37]."}, {"title": "8.1. White-Box Adversarial Example", "content": "In recent years, extensive studies apply AE techniques to the speech domain to achieve command injection. Car- lini et al. [6] first successfully generated the attack ex- amples towards Deepspeech, an open-sourced end-to-end ASR platform. CommanderSong [35] tried to embed the malicious voice command into songs so that it won't at- tract human attention but could mislead Kaldi successfully. However, these works are fragile in the physical world. Specifically, the adversarial examples will fail when being played over the air. To address this problem, Yakura et al. and Imperio [38], [39] simulated the transformations caused"}, {"title": "8.2. Black-Box Adversarial Example", "content": "Despite the success of adversarial examples in speech recognition attacks, the white-box premise of many tools poses a significant practicality concern; knowledge of the architecture and parameters of the model is unrealistic for a real-world attack scenario. Therefore, recently, black-box attacks have become an active research area. Taori et al. [15] combined the approaches of genetic algorithms with gradi- ent estimation to attack Deepspeech. This approach has a low success rate and is ineffective on commercial models. Devil's Whisper [8] utilized the confidence scores of the commercial ASR APIs. They built a substitute model to ap- proximate the target model and launch the attack. However, most commercial APIs only return the final results without any confidence scores, which limits the practicality of this method. OCCAM [9] is the first approach that attacks com- mercial ASR APIs successfully in the completely black-box scenario, i.e., no confidence scores are required. However, the cost of this method is high. A recent work, TAINT [11], considers the impact of the VoIP channel and uses gradient estimation to generate adversarial examples, resulting in a more robust and efficient attack."}, {"title": "8.3. Hidden Voice Attacks", "content": "Besides adversarial examples, \u201chidden voice command\u201d is another line of attack method targeting speech recogni- tion systems. Cocaine Noodles [44] first used the inverse MFCC technique to create the attack sample which will be recognized by the devices but won't be understood by humans. Carlini et al. [18] extended it to the white-box scenario and proposed a \"hidden voice command\" attack that can't be understood at all. Abdullah et al. [7] then exploited signal processing algorithms to make hidden voice commands more practical."}, {"title": "9. Discussion", "content": "9.1. Defense against ALIF.\nDownsampling. Downsampling is a commonly-used method to neutralize AE attacks. According to Nyquist's"}, {"title": "9.2. Attack Success and ASR Robustness", "content": "Black-box attack training follows a specific paradigm in which a target phase is perturbed to both make the audio sound different as well as to limit a victim's awareness of the attack. The limit on the perturbation energy is no longer primarily designed for imperceptibility as in the white-box attack. Instead, it aims to prevent the transcription from being different than the target phrase. In this context, it is possible that the success of a black-box attack also depends on the ASR's robustness to noise.\nTo investigate if a correlation exists, we test the robust- ness of five commercial ASRs and study the connection between the attack performance of ALIF-OTL and the noise robustness. We randomly choose 200 sentences from the LibriSpeech test-clean subset and mix them with Gaussian white noise and event noise under different SNRs, respec- tively. Then, we evaluate them using ASRs and calculate the increase of word error rate (AWER) caused by white noise, where smaller AWER indicates better robustness, and vice versa. Figure 8 and Figure 9 demonstrate that Amazon is fairly robust while Google is the most vulnerable one, which is consistent with some of our observations during the attack sample generation. That is, we can generate attack samples against Amazon API relatively easily but cannot generate attack examples of high quality against Google API. The experimental results suggest the possibility of the more robust the ASR is, the more vulnerable it is to our attack. We will conduct a more comprehensive study to verify the assumption in the future."}, {"title": "9.3. Limitations", "content": "In this paper, we utilize PSO as the primary optimiza- tion method for ALIF-OTA and have not explored other techniques. Different optimization techniques can result in"}, {"title": "10. Conclusion", "content": "In this paper, we propose the first black-box adversarial audio attack methods that are highly efficient and robust against model updates: ALIF-OTL and ALIF-OTA. The two attack schemes are based on ALIF, a novel adversarial linguistic feature-based attack pipeline. We directly add perturbation to the low dimensional manifold where the de- cision boundary lies to generate adversarial audio samples, which overcomes the inherent shortcomings of conventional black-box AEs: query inefficiency and the vulnerability to model updates. ALIF-OTL only requires an average of 35 queries to generate attack audios against cloud ASR APIs. Experiments show ALIF-OTL is effective against four well-known commercial cloud ASRs, achieving an attack success rate of 95.8%. ALIF-OTA uses the PSO method to incorporate environmental interference into the training and only requires 400 queries for attack sample generation. Experiments show the efficacy of our attack audio samples in attacking four commercial ASRs and two VAs in the physical playback environment, achieving average success rates of 81.3% and 69.2%, respectively. Critically, the test- of-time experiment verifies the long-term effectiveness of the ALIF attack, indicating its robustness to model changes."}, {"title": "Appendix A. Additional Evaluations", "content": "We expand the command dataset size to demonstrate our method's efficacy. Besides the ten commands and two sentences discussed in the main paper, we also produce adversarial audio samples employing an additional 20 com- mands. The results are shown in Table 13."}, {"title": "Appendix B. Meta-Review", "content": "B.1. Summary\nThe authors propose two attack schemes: ALIF-OTL and ALIF-OTA to generate adversarial black-box attack pipelines based on the linguistic feature space of TTS. The authors propose over-the-line and over-the-air approaches and implement a comprehensive analysis against commer- cial ASRs, reaching more than 95% query efficiency im- provement compared with the state-of-the-art.\nB.2. Scientific Contributions\n\u2022 Provides a Valuable Step Forward in an Established Field\nB.3. Reasons for Acceptance\n1) The paper identifies and uses linguistic structures present in Text-to-Speech algorithms as a means to improve the generation of adversarial audio for their attacks. This allows for the model during the training process to target audio features that human listeners are sensitive to, thus allowing for the model to be trained faster and with less resources.\n2) The work highlights an interesting possible external source of a priori knowledge for future work to explore, namely the use of linguistic structures found in TTS.\nB.4. Noteworthy Concerns\n1) The authors only provide a small, informal user study consisting of 20 volunteers who evaluated 32 audio samples. Given the attacker's goal in this work is to create audio samples that are incomprehensible to human listeners, but still comprehensive to machines,"}, {"title": "Appendix C. Response to the Meta-Review", "content": "We are grateful to our reviewers and the S&P 2024 program committee for accepting our paper", "review": "nComment 2 notes that the reviewers would like us to use larger command datasets for generating attack audio samples.\nWe have taken this into consideration and increased our dataset size to include an additional 20 commands, making a total of 32 commands and sentences. These commands were carefully selected from common user interactions, thereby representing significant security threats. The successful gen- eration of attack audio samples from this corpus affirms the effectiveness of our method, and we believe that further increase would not yield additional technical challenges. Thus, we believe that the current size is adequate"}]}