{"title": "MVIP - A DATASET AND METHODS FOR APPLICATION ORIENTED MULTI-VIEW AND MULTI-MODAL INDUSTRIAL PART RECOGNITION", "authors": ["Paul Koch", "Marian Schl\u00fcter", "J\u00f6rg Kr\u00fcger"], "abstract": "We present MVIP, a novel dataset for multi-modal and multi-view application-oriented industrial part recognition. Here we are the first to combine a calibrated RGBD multi-view dataset with additional object context such as physical properties, natural language, and super-classes. The current portfolio of available datasets offers a wide range of representations to design and benchmark related methods. In contrast to existing classification challenges, industrial recognition applications offer controlled multi-modal environments but at the same time have different problems than traditional 2D/3D classification challenges. Frequently, industrial applications must deal with a small amount or increased number of training data, visually similar parts, and varying object sizes, while requiring a robust near 100% top 5 accuracy under cost and time constraints. Current methods tackle such challenges individually, but direct adoption of these methods within industrial applications is complex and requires further research. Our main goal with MVIP is to study and push transferability of various state-of-the-art methods within related downstream tasks towards an efficient deployment of industrial classifiers. Additionally, we intend to push with MVIP research regarding several modality fusion topics, (automated) synthetic data generation, and complex data sampling \u2013 combined in a single application-oriented benchmark.", "sections": [{"title": "1 Introduction", "content": "Vision-based classification systems have a broad range of industrial applications, e.g., 1) the intensification and sorting of incoming components into a warehouse; 2) the quality inspection and automated documentation of kitting and packaging processes; 3) key component identification of a broken machine to rapidly locate a fitting replacement within the warehouse. In reverse logistic, vision-based classification systems are used to identify old car components and classify them for remanufacturing, helping to reduce their relative carbon footprint [1]. Ever since the success of AlexNet [2] in 2012 related work for Vision-based classification has been developed with the ImageNet [3] classification benchmark."}, {"title": "2 Related Works", "content": "Multi-View: A recent (2021) survey on image fusion techniques [17] identifies MV-fusion among other fusion techniques as an ongoing research topic, which is increasingly attracting more attention. Fusion can happen at different stages in a model architecture. Related works on MV-fusion employ a single image encoder to transform a set of images into vector-space (view tokens) and apply late fusion techniques [18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]. The zoo of related SOTA late fusion techniques can be grouped into non-trainable, node-wise view-weighting (\u2299), intra-view aware (\u2192), and inter-view aware (\u2191) methods. Consider tokenized view embeddings $x \\in \\mathbb{R}^{IxJ}$, where $I$ is the number of views and $J$ the number of hidden nodes, then $\\x_{\\odot} = \\sqrt[I]{\\sum_{i=1}^{I}}\\sum_{j=1}^{J} X_{ij}V_{ij}$, $\\x_{\\to} = \\sum_{j=1}^{J} F(X)$, and $\\x_{\\uparrow} = \\sum_{j=1}^{J} \\forall_{j=1}^{J} F(x_i)$.\n$F$ is a trainable function and $v$ is a scalar found by some implementation of $F(\\mathcal{C}( x))$. E.g. pooling methods [18] are non-learn-able node-wise view-weighting methods for view aggregation. Likewise, convolution and fully-connected layers can be used to train a inter-view aware node-wise view-weighting policy. Otherwise, methods such as Squeeze- and-Excitation [31] (S.&E.) use the individual embedding to determine a node-wise weighting, allowing an intra-view aware view aggregation. Methods based on concatenation of view embeddings [30, 25, 19] are inter-view and intra-view aware. Recent SOTA methods for View-Fusion have success with trainable [19] and non-trainable [20] view aggregation methods.\nRGBD: Unlike MV-fusion, SOTA methods for RGBD-related downstream tasks also employ hybrid fusion, where color and depth signals are gradually fused downstream. The work related to hybrid fusion can be grouped into methods that gradually fuse depth information with color information ($d \\rightarrow c$) [32] and a bi-directional fusion $c \\leftrightharpoons d$ [33, 34]. Depth directed fusion ($c \\rightarrow d$) appears to be unnoticed in related work. Other work for RGBD-fusion-based downstream tasks employ late-fusion-based methods [35, 36, 37, 38, 23].\nTransformers: Ever since the introduction of Transformers [39] into vision problems [8] they push the SOTA within vision-based downstream tasks [9, 10, 40, 41, 42, 43]. Due to their universal capabilities, transformers are found to be well suited to fuse information from different modalities [44, 45, 35, 33, 46, 47, 48]. Here, Omnivore [46] and Omnivec [47, 48] use token-based modality fusion to reach state-of-the-art results on RGBD-based scene classification on SUN-RGBD [49]. Therefore, we investigate the usage of Transformer-based methods for MV-fusion within industrial applications for part recognition.\nMM & MV DataSets: Datasets for 6D-object-pose-estimation [50, 51, 52, 53], RGBD-segmentation [49, 54, 55, 56] and RGBD-instance-detection [50, 56, 53, 51] drive RGBD based research within their related downstream tasks. Within MV and classification problems it is a common method within the field of 3D-part-recognition to render a set of 2D views from 3D parts and use image encoders to adopt MV-fusion for a combined classification [18, 19, 20, 21, 22, 23, 24, 25]. This research results in a vast set of synthetic [57, 58, 59, 60, 61] and real world [62, 63, 64, 65, 66, 67, 68, 69] RGB(D) datasets for MV-fusion and classification investigations. Real-world MV datasets frequently use video-based capturing methods. Thus, MV can be sampled from the video. MV-RGBD [67] uses a turn table to capture objects from multiple fixed view points, while FewSOL [70] and GraspNet [53] use a robot to capture object(s) from multiple view points. Albeit the use of synthetic industrial components [57, 58] and fixed view points, none of the available datasets addresses a realistic industrial application for part recognition. Moreover, recent work within machine vision leverages the combination of images with other modalities such as natural language (NL) [44, 45]. With MVIP, we are the first to our knowledge to bring the physical properties of objects, natural language, and MV-RGBD images into a single benchmark for application-oriented industrial part recognition."}, {"title": "3 Methods", "content": "The MVIP dataset is captured on a digitization station. Ten RGBD cameras are mounted on the table construction, all facing a common point on the integrated scale. An ArUco-Board is surrounding the scale, thereby a 6D-Pose can be determined for each camera at any given time, given the camera's intrinsic parameters.\nThus, the cameras are calibrated to each other within each captured image set. Due to the arrangement of camera perspectives and calibration, one set of images covers most of the object surface and allows 3D construction of the objects. The front of the table is not equipped with cameras to provide space for a worker.\nEach object class featured in the dataset is rotated during digitization 12 times (approximately 30\u00b0 steps), given a subjective \"natural laying\" position. Since some objects have multiple \"natural laying\" positions, 12 rotations are repeated according to the subjective assessment of the worker. For test (5) and validation (5) purposes, ten additional image sets are captured, where the worker randomly moves the object on the scale according to a natural laying position.\nFig. 4 illustrates a subset of objects featured in the dataset. In addition to image data, the dataset features meta-data for all objects; weight, package size (length, width, height), object class, super-classes (general class spanning a common subset of classes), natural language (NL) tags, and generated view-wise object segmentation masks (thus, also ROI Bounding Boxes). For the segmentation masks, we annotated a small subset of 5% and finetuned a segmentation model ([71]) to generate the remaining masks. This works well since the segmentation task is rather easy. The calibrated MV RGBD design of the dataset enables anyone to employ methods for 3D-Object-Point-Cloud and 3D- Scene-Reconstruction, 6D-Object-Pose Estimation, and Synthetic-Data generation. In total, MVIP features 308 classes of industrial components (e.g. hammer, generator, camera adapter), which are grouped into 18 super-classes (e.g. tools, car components, metal part). From eight categories (shapes, colors, materials, textures, conditions, size, weight, and density), MVIP uses 77 NL-Tags to describe the classes. These tags describe in natural language the objects' conduction (diry, rusty, clean, etc.), shapes (round, sharp, edgy, pointy, etc.), and other visual attributes. MVIP has a total of \u2248 570 k images (all with resolution of 1280 \u00d7 720), while 71.276 are RGB images. Each RGB image has the corresponding counterpart images: depth, HHA (following [72]), mean-RGB (mean images are averaged temporally over 1 sec for more stable data), mean depth, and segmentation (mask of the industrial part). Additionally, each image set is associated to a specific background set (scene without the industrial part). The \u2248 282 k images are available for training, while the \u2248 108 k images are used for validation and the other \u2248 108 k images for test. The industrial components featured in MVIP are set to be at least the approximate size of a fist (in a subjective assessment of the worker) and the maximum 350 mm \u00d7 450 mm \u00d7 300 mm with weight of < 15 kg."}, {"title": "Architecture", "content": "Inspired by related work, we created a modular training architecture in order to study ML-based industrial part recognition with MVIP. Our architecture uses a pre-trained CNN backbone [5, 7]. Given the views $V$, the color and depth encoders $\\mathcal{E}_{c|d}$ project their inputs $V_1 C_{vdv} \\in \\mathbb{R}^{C_h e \\setminus d,H,W} \\rightarrow \\mathcal{V}=1 V_1=1 l_{cd,v,i} \\in [\\mathbb{R}^{C_{emb_1} \\times H/2^2 \\times W/2^2}$.\nIn our implementation, we use the ResNet50 [5] encoding architecture $\\mathcal{E}_{c|d}$ where $I=5$. At every embedding stage $l_{cd,v,i} = \\mathcal{E}_{cd,i}(l_{cd,v,i-1})$ a RGBD-fusion $\\mathcal{F}_{l_1}$ combines the embeddings $l_{c\\hat{d},v,i} \\rightarrow \\hat{l}_{c\\setminus d,v,i}$, where $\\hat{l}_{c|d,v,0} = c_{v|dv}$ and $\\mathcal{F}_{l_1}$ is either a one-directional fusion $\\mathcal{x}=c|d;\\; \\hat{l}_{x,v,i},\\; \\hat{l}_{\\neg x,v,i} = \\mathcal{F}_{l_1}(l_{c\\hat{d},v,i}),\\; l_{\\neg \\\u00e6,v,i}$ or bi-directional fusion $\\hat{l}_{c,v,i}, \\hat{l}_{d,v,i} = \\mathcal{F}_{l_1}(l_{c\\hat{d},v,i})$.\nThe final fused embedding $\\hat{l}_{v,1} = \\mathcal{F}_{l_1}(l_{c\\hat{d},v,I}) \\in \\mathbb{R}^{C_{emb_1} \\times H/2^I \\times W/2^I}$ is passed through an average pooling layer followed by a flattening before being projected via the fully connected layer $\\lambda_p$ into the view embeddings $\\chi_v \\in \\mathbb{R}^{1\\times Ch_v}$. Optionally, the non-fused embeddings $l_{c|d,v,1}$ are also passed through an average pooling layer followed by a flattening and projected via a fully connected layer $\\lambda_{c|d}$ into the auxiliary output $\\mathcal{O}_{v,c|d} \\in \\mathbb{R}^{1\\times N}$, where N is the number of classes. Likewise, $\\chi_v$ is projected by $\\lambda_{v}$ into $\\mathcal{O}_{v} \\in \\mathbb{R}^{1\\times N}$. The view embeddings $\\chi_v$ are stacked for a combination of embeddings $\\chi = cat(\\forall v=1\\chi_v)$. An optional transformer encoder $\\mathcal{T}$ [39] can be applied in $\\chi$ to employ self-attention between all $\\chi_v$.\nEventually, we use MV-fusion $\\mathcal{F}_v$ to reduce the dimension of $\\chi \\in \\mathbb{R}^{V\\times Ch_v} \\rightarrow \\in \\mathbb{R}^{1\\times Ch_r}$ before a final fully connected layer $\\lambda$ projects $\\chi \\rightarrow \\mathcal{O} \\in \\mathbb{R}^{1\\times N}$. Optionally, our transformer-based implementations for $\\mathcal{F}_v$ additionally take the object weight in kg as input, which is used as an anchor (conditional decoding [45]). Likewise to Positional Encoding [39] we encode the object weight into a sequence of cosine and sine frequencies and add the vector to the decoding signal of our transformer-based MV-fusion implementations. For RGB-MV classification, the modules $\\mathcal{E}_d$ and $\\mathcal{VFL}$ are removed, while $\\mathcal{FL}_1$ is an identity function and $\\forall^{-1} \\hat{l}_{c,v,i} = l_{c,v,i}$."}, {"title": "MV-fusion implementations", "content": "In our experiments, we study different MV-fusion types within MVIP and effect of scaling the trainable parameters. We summarize the implementations in our investigations. These methods (in addition to $\\mathcal{T}$) are implementations of $\\mathcal{F}_v$ and are used to reduce the tokenized embeddings of the views $V$. Following related work, we use the pooling aggregation introduced by [18] in 2015 and is still used recently [20]. Following Feng et al. [21] we implement a Conv.-based node-wise view aggregation which gradually fuses the views in sequential layers. However, most related work on view aggregation is designed for problems with 12 and 20 views for 3D object recognition [19, 25, 24]. E.g., Wei et al. [21] uses a graph-based view aggregation implementation which is only suitable for views $V$ where $V/4 > 1 \\& V/4 \\in \\mathbb{N}$. Regarding Transformers ($\\mathcal{T}$, Tr-En, Tr-EnDe) we are the first to our knowledge to bring the attention mechanism to tokenized view aggregation. Here $\\mathcal{T}$ is a simple Transformer encoder, introduced by Vaswani et al. [39]. The Transformer encoder-decoder (Tr-EnDe) [39] is used by Carion et al. [40] to extract information from encoded image information through a trainable decoding query and cross-attention. We adopt this approach for view aggregation by decoding class information with a single trainable query from the tokenized view embeddings. Dosovitskiy et al. [8] append a trainable decoding query to the tokenized embeddings before forwarding it through the Transformer encoder (Tr-En) [39]. Afterwards, a final classification is done only on the basis of the output at the index of the trainable query. Thus, the attention mechanism is applied while reducing the number of trainable parameters compared to the encoder-decoder approach. The transformer-based view aggregation methods are intra- and inter-view aware, since the attention mechanism can attend to every node. Furthermore, we adopt the Squeeze-and-Excitation Networks (S&E) introduced by Hu et al. [31] as a comparison for intra-view-aware view aggregation. Here, each tokenized view embedding is attending to every node within the view and used to determine a node-wise scalar, which is used for a simple sum view aggregation. In our shared-S&E (S.S&E) we use a single module to compute the scalars for every view, rather then having a individual S&E module for every view. An overview of the implemented fusion methods can be seen."}, {"title": "Multi head auxiliary loss", "content": "Inspired by related work [73, 40, 74] we introduce a novel auxiliary loss for MV classification which is defined as;\n$\\zeta_{v} = \\mathcal{S}_{cls} (\\mathcal{o}_{v}, \\psi_v)$ (1)\n$\\zeta_{VCD} = \\frac{1}{3} (\\mathcal{S}_{cls}(\\mathcal{O}_{v}, \\Psi_v) + \\mathcal{S}_{cls} (\\mathcal{O}_{vc}, \\Psi_v) + \\mathcal{S}_{cls} (\\mathcal{O}_{va}, \\Psi_v))$ (2)\n$\\zeta_{MH} = \\frac{1}{V+1} (\\mathcal{S}_{cls} (\\mathcal{O}, \\psi_v) + \\sum_{v=1}^{V} \\zeta_{v})$ (3)\n$\\zeta_{MHRGBD} = \\frac{1}{V+1} (\\mathcal{S}_{cls}(\\mathcal{O}, y) + \\sum_{v=1}^{V} \\zeta_{VCD})$ (4)\nwhere $\\mathcal{S}_{cls}$ denotes cross-entropy-loss, $\\psi$ is the MV target class, $\\psi_v$ is the view wise target class, $V$ is the number of Views, and $\\mathcal{o}, \\mathcal{o}_v, \\mathcal{O}_{vc}$, and $\\mathcal{O}_{va}$ are the overall, full-view, color-view, and depth-view predictions, respectively. With our multi head auxilary loss (MH-loss) we embrace gradients view-wise within each encoding modality, thus forcing contributions from every view and modality, which hinders the classifier to specify/overfit on a certain view and modality. This is especially important if the pretrained encoder initially favors a certain modality or view."}, {"title": "Weight-based Classification", "content": "The object weight $w$ [kg] $\\in \\mathbb{R}$ is a one-dimensional non-unique property. Thus, classifying objects purely based on weight is ambiguous, since the probability that the objects share a common $w \\pm \\epsilon$ is large, where $\\epsilon$ is the scale resolution. The scale use in MVIP has a constant weight error of only 0.002 kg, but is heavily affected by offsetting objects from the scale center. Thus, any weight-based classifier must be robust to disturbances in the weight measurement. Following positional encoding [39] (PE) we use a set of $d \\in \\mathbb{N}$ sine-cosine functions with varying frequencies to encode the weight $w$ into a vector of size $1 \\times 2d$. We Train a 4-Layer MLP to upscale the weight $1 \\times 2d \\rightarrow 1 \\times d_h$, where $d_h$ is the number of hidden nodes. Eventually, a fully-connected layer (FC) is used for classification. During training, we apply randomly a constant or proportional weight error uniformly sampled from the distribution $\\pm 0.01 \\textrm{ kg}$ or $0.01w$ [kg], respectively. We train PropertyNet using cross-entropy loss and Adam Optimizer for 10k epochs with a scheduled cosine one-cycle learning rate between $10^{-7} \\nearrow 10^{-6} \\searrow 10^{-9}$ (max at 50% training) and a batch size of 512."}, {"title": "4 Experiments & Discussion", "content": "Experiment design: All experiments are conducted on the same machine (2\u00d7Nvidia RTX 3090) with the same set of hyper-parameters for training. For fair comparison, the batch-size stays constant at 32, which means that the largest model utilizes the 48GB GPU-RAM at most, while smaller models theoretically could have used a larger batch size. For a baseline establishment, the ResNet-50 [5] architecture with pre-trained ImageNet[3] weights is used as an image encoder. We train our classifiers with our auxiliary loss and Adam optimization at an cosine one cycle [75] scheduled learning rate of $10^{-5} / 10^{-4} \\searrow 10^{-6}$ (max at 50% training). If not stated otherwise, we report the maximum observed accuracy out of five runs on MVIP for 50 epochs with ROI crops, colorjitter, flip, rotation and random view order augmentations on three view RGB with a resolution of 224 \u00d7 224 pixels (ROI crops are up-sampled if needed). During testing, we fix the view order and also indexes if additional views are available during training.\nStability and Resolution: In Tab. 2 we investigate the stability of our proposed architecture w.r.t. 3-view-RGB and our MH-loss. Here we find our MH-loss to yield superior and more stable results compared to pure end-to-end training with cross-entropy-loss. In Tab. 3 we inspect the effect of using a higher resolution, the usage of multi-scale inputs, and up-sampling of ROI crops. Despite our observations regarding the input resolution, we continue our experiments with an input size of 224 \u00d7 224 without multi-scale to keep a constant batch size in every experiment.\nMV-fusion: In Tab. 4 we report the results concerning view aggregation. We find that our MH-loss yields superior results across the board. Interestingly we observe max pooling to lose performance with view increase (especially without our MH-Loss). This phenomenon might be explained by an accumulated self-reinforcing view specific feature extraction, which is more likely to lead to overfitting and only present for the Max pooling implementation. Our MH-loss circumvents this overfitting phenomenon due to additional view-wise backpropagation. As we employ random view ordering during training to reduce overfitting (see Tab. 5), we observe the trainable node-wise weighing convolution to collapse towards average pooling. Moreover, we find that our classifiers tend to overfitting with increasing MV-fusion complexity and view numbers. Here we find that the lightweight intra-view weighing approach of S.S&E yields marginal gains compared to non-trainable view aggregation. Transformer-based implementations are notoriously hard to train with a tendency to overfit [39, 8, 40, 76]. Related work found self-supervised (SSL) pre-training to be key to the success of transformer-based implementations [42, 77, 78], which opens for further SSL-based investigation for tokenized view aggregation with transformers. Likewise, we find from the training logs that the trainable but not pre-trained view aggregation to be less efficient w.r.t. training time. Thus, pre-training is crucial for efficient training of complex view aggregation methods. We conclude that trainable inter view aware methods for view aggregation are not adding compared to non-trainable alternatives as the view order is invariant. However, we see the potential for complex intra-view & inter-view aware methods, such as transformers, to yield better results if (pre)trained in larger datasets.\nSaturation of Accuracy: In Fig. 6 we summarize our results regarding increasing the number of input views in our MV-architecture, available views within the training data, and the effect of reducing the number of training image sets. Here we observe the most significant performance gain already with two views, while reaching a saturation of accuracy at around five views. Further increasing the number of views does not significantly improve the performance w.r.t. MV nor the number of training samples. Similar to our MV-fusion related results (see Tab. 4), we observe a significant performance drop with three Views. It can further be observed that some training samples w.r.t view ID and rotation ID appear to corrupt the training and cause overfitting. We identify the view IDs two and seven as being especially affected by artifacts introduced by the worker. Moreover, we argue that certain view/rotation configurations emerge more overlapping and occurrences of artifacts which are affecting the statistical generalization."}, {"title": "RGBD encoding", "content": "has been found to be a modality that adds value to several downstream vision tasks such as classification [36, 23], detection [37], and especially segmentation [32, 33, 35] where the depth signal is well suited to find and refine contours. Depth information has the potential to extract features invariant to condition/instance-based color artifacts. Hence, yielding a better generalization towards unseen object instances. However, we find that both bi-directional ($c\\leftrightharpoons d$) [33, 34] and towards color-directed $d\\rightarrow c$ [32] fusion is corrupting the RGB encoding (see Tab. 7), with the caveat that our findings are on the basis of S.&E.-based [31] RGBD-fusion rather than the direct implementation used by [33, 34, 32] in order to yield more general and comparable results. Albeit the usage of further regularization with extra depth noise augmentation and image-wise normalization (removing absolute distances), we find our depth directed fusion ($c \\rightarrow d$) implementation to reach at best near-on-par performance with pure MV RGB. It appears that the depth information is at best an opening for overfitting. Related work uses HHA [72] (horizontal, height, angle) encoded depth rather than a simple one-dimensional standardized signal for better results [79], which we cannot report. However, with our findings regrading pre-trained depth encoding weights [32], we argue that the key reason for the RGBD failure is a quality mismatch between color and depth feature extraction. While sophisticated and suitable pre-trained encoders for color images are broadly available, we observe mainly pre-trained depth encoders for segmentation downstream tasks that are fundamentally different. Self-supervised RGBD pre-training methods such as [80] are promising for efficient training, but not broadly accessible yet, nor does it allow end-to-end training of an RGBD encoder with intermediate RGBD-fusion."}, {"title": "Anchor-based Decoding", "content": "Moreover, we investigate the effect of using weight encoding (PE(w)) as anchor for the MV-fusion Transformer-Decoder ($\\mathcal{F}_v$) such that the decoder query $Q = Q_{emb} + PE(w)$ or $Q = \\mathcal{WN}(PE(\\omega))$, where $Q_{emb}$ is a trainable parameter of size $1 \\times d_a$. However, we can only report marginal gains within different metricise, which can be explained by the PropertyNet (PN) accurancy and general weight modality limitation within MVIP. However, anchor-based decoding becomes potentially beneficial when enriching the anchored information (add further property dimensions) or within problems that are not purely solvable by image information (e.g. occluded features/materials which are detectable by physical properties such as weight)."}, {"title": "5 Conclusion", "content": "We present MVIP, a novel multi-view and multi-modal application oriented dataset for industrial part recognition. The intention of the dataset is to narrow the gap between basic research in ML-based computer vision regarding part recognition and real world industrial applications. MVIP enables researchers to investigate a wide range of open research questions combined in a single benchmark, spanning several domains. Within our experiments, we identify a general lack of strong multi-modal encoders and fusion mechanisms, which can leverage robust and efficient finetuning on industrial downstream tasks. Furthermore, we identify a general risk of overfitting within our end-to-end trained MV problem, which we hypothesize to be related to a statistical increase of MV artifacts and to a low batch size. However, we achieve a baseline Top 1 accuracy of > 95% and Top 5 of 99.5% within MVIP using a MV-RGB classifier due to our novel MH-loss, which we find to yield more stable and better results across the board for MV-fusion.\nRegarding future work, we aim to investigate the (automatic) generation of synthetic data via MVIP's options for object and scene 3D-reconstruction. Here, one can use MVIP's calibrated RGBD setup to generate textured CAD and point cloud data, which in turn simulations can laverage to create unlimited diverse data in a controlled setup. In addition to classification, these synthetic data generation can also be employed in combination with the NL-tags on object conditions and objects masks to generate data for defect/anomaly and condition detection/classification. Moreover, with MVIP new techniques for MV-augmentation, MV-sampling, and MV-regularization (similar to our MH-Loss) can be investigated that leverage knowledge regarding views, super-classes, and natural language tags. Methods for multi-modal pre-training and fusion between image-based data such as RGB, Depth, and physical properties (width, height, length, weight) as well as natural language tags should can be explored. Other future work can also use MVIP to investigate dataset curation with respect to the selected cameras and the number of images \u2013 aiming for more efficient training and adaptation. Following [81], we would also encourage researchers to investigate incremental learning on MVIP, since industrial use cases often have a rapidly changing range of objects."}, {"title": "Source code and data", "content": "Please find all the source code related to our experiments, dataset handling, and a download link to MVIP here:\nGithub: https://github.com/KochPJ/multi-view-part-recognition."}]}