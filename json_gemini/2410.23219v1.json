{"title": "DiaMond: Dementia Diagnosis with Multi-Modal Vision Transformers Using MRI and PET", "authors": ["Yitong Li", "Morteza Ghahremani", "Youssef Wally", "Christian Wachinger"], "abstract": "Diagnosing dementia, particularly for Alzheimer's Disease (AD) and frontotemporal dementia (FTD), is complex due to overlapping symptoms. While magnetic resonance imaging (MRI) and positron emission tomography (PET) data are critical for the diagnosis, integrating these modalities in deep learning faces challenges, often resulting in suboptimal performance compared to using single modalities. Moreover, the potential of multi-modal approaches in differential diagnosis, which holds significant clinical importance, remains largely unexplored. We propose a novel framework, DiaMond, to address these issues with vision Transformers to effectively integrate MRI and PET. DiaMond is equipped with self-attention and a novel bi-attention mechanism that synergistically combine MRI and PET, alongside a multi-modal normalization to reduce redundant dependency, thereby boosting the performance. DiaMond significantly outperforms existing multi-modal methods across various datasets, achieving a balanced accuracy of 92.4% in AD diagnosis, 65.2% for AD-MCI-CN classification, and 76.5% in differential diagnosis of AD and FTD. We also validated the robustness of DiaMond in a comprehensive ablation study.", "sections": [{"title": "1. Introduction", "content": "Dementia presents a growing concern for individuals and society, with Alzheimer's Disease (AD) constituting 60-80% of the cases and frontotemporal dementia (FTD) ranking as the second most common type in the younger-elderly population under 65 years old [33]. Accurately diagnosing AD and distinguishing it from other dementia types is crucial for patient management, therapy, and prognosis, but challenging as symptoms overlap. To address this challenge, a variety of diagnostic tools are employed, including magnetic resonance imaging (MRI), positron emission to-"}, {"title": "2. Related Work", "content": "Multi-Modal Learning for AD Diagnosis. Recent studies have advanced AD diagnosis by combining structural MRI and FDG-PET through various DL approaches. Different fusion strategies have been explored, including early [14, 20,30,32], middle [26,36], and late fusion [7,14,26,36]. Lu et al. [23] use a multiscale deep neural network to fuse the extracted patch-wise 1D features from MRI and PET. Liu et al. [21] propose a cascaded framework including multiple deep 3D-CNNs to learn from local image patches and an upper 2D-CNN to ensemble the high-level features. Feng et al. [7] combine a 3D CNN and LSTM with a late fusion of MRI and FDG-PET for AD diagnosis. Huang et al. [14] propose an early and a late fusion approach for the two modalities based on a 3D-VGG. Lin et al. [20] first propose a 3D reversible GAN for imputing missing data and then use a 3D CNN to perform AD diagnosis with channel-wise early fused MRI and PET data. Wen et al. [32] introduce an adaptive linear fusion method for MRI-PET fusion based on 2D CNNs. Song et al. [30] propose an early fusion approach by overlaying gray matter (GM) tissues from MRI with the FDG-PET scans and feeding them into a 3D CNN for classification. However, after investigating several multi-modal methods with 3D CNN across image-level early, middle, and late fusion of MRI and PET, Narazani et al. [26] find that the diagnostic performance of these existing multi-modal fusion techniques may not yet outperform that of using PET alone for AD diagnosis.\nAs Transformers and attention mechanisms have shown promising results in various medical imaging tasks [4,9,12], recent applications have started to integrate them for multi-modal feature fusion with CNNs as encoders for feature extraction [8, 17, 18, 25, 31, 34, 35]. Li et al. [18] combine a CNN and a Transformer module for multi-modal medical image fusion. Zhang et al. [34] propose an end-to-end 3D ResNet framework, which integrates multi-level features obtained by attention mechanisms to fuse the features from MRI and PET. Gao et al. [8] introduce a multi-modal Transformer (Mul-T) using DenseNet and spatial attention for global and local feature extraction, followed by cross-modal Transformers for T1-, T2-MRI, and PET fusion. Zhang et al. [35] use MRI and PET for dementia diagnosis by first employing adversarial training with CNN encoders for feature extraction, then applying Transformers through the cross-attention mechanism for feature fusion and finally classification with a fully-connection layer. Miao et al. [25] propose a multi-modal multi-scale transformer fusion network (MMTFN), combining CNN-based residual blocks and Transformers to jointly learn from multi-modal data for diagnosing AD. Tang et al. [31] first employ a 3D CNN to extract deep feature representations of structural MRI and PET images, then utilize an improved Transformer to progressively learn the global correlation information among features. Note that these recent approaches typically combine CNNs for initial feature extraction with ViTs for feature fusion, which may not fully leverage the capabilities of ViTs for multi-modal learning.\nDifferential Diagnosis of Dementia. Despite the extensive research focused on AD diagnosis, there has been limited exploration in multi-modal learning for differential diagnosis of dementia, which holds significant importance in clinical practices [5]. Current research in differential diagnosis primarily adopts machine learning algorithms [3, 6, 11] or focuses on single modality [19,27], yet the potential of em-"}, {"title": "3. Proposed Method", "content": "Fig. 2 presents the main steps of DiaMond, a multi-modal ViT-based framework incorporating multiple attention mechanisms for effective dementia diagnosis. We begin by outlining the foundational concepts underlying our approach and subsequently delve into the specifics of each component."}, {"title": "3.1. Preliminaries", "content": "Let's denote the 3D MRI and PET images as $M \\in \\mathbb{R}^{H\\times W\\times D}$ and $P \\in \\mathbb{R}^{H\\times W\\times D}$ respectively, with height H, width W, and depth D. The objective is to classify the given multi-modal data into a set of L labels $C = \\{c_1,\\dots, C_L \\}$. In this study, C includes CN (Cognitively Normal), MCI (Mild Cognitive Impairment), AD, and FTD labels. MRI and PET data exhibit inherent dependencies that can be introduced during data collection and revealed in high-dimensional feature space. Hence, we divide the data space into three non-overlapping regions denoted by $\\{R_M,R_P,R_{MP}\\}$, as highlighted in Fig. 1. Our proposed framework DiaMond consists of three branches based on pure ViTs to process each of the data space:\n1. $F_M : M \\in \\mathbb{R}^{H\\times W\\times D} \\rightarrow z_M \\in \\mathbb{R}^f$ maps input MRI from data space $R_M$ (Fig. 1) into the latent encoding $z_M$ with length $f$;\n2. $F_P : P\\in \\mathbb{R}^{H\\times W\\times D} \\rightarrow z_P \\in \\mathbb{R}^f$ maps PET from data space $R_P$ into the latent encoding $z_P$ of length $f$;\n3. $F_{M,P} : \\{M,P\\} \\rightarrow z_{M,P} \\in \\mathbb{R}^f$ receives both MRI and PET, then captures their shared information in data"}, {"title": "3.2. Self-Attention for Single Modality", "content": "$F_M$ and $F_P$ operate over single modalities separately with self-attention mechanisms. Let $M$ and $P$ be partitioned into $h \\times w \\times d$ voxel patches $X_m \\in \\mathbb{R}^{N\\times h\\times w\\times d\\times f_e}$, $m \\in \\{M, P\\}$, where $f_e$ denotes the length of the feature embedding, and N the number of input patches. The input patch $X_m$ is projected into the query $W_m \\in \\mathbb{R}^{f_e \\times f_e}$, key $W_m \\in \\mathbb{R}^{f_e \\times f_e}$, and value $W_m \\in \\mathbb{R}^{f_e \\times f_e}$ matrices, giving:\n$Q^m = X_m W, K^m = X_m W, V^m = X_m W, \\qquad m\\in \\{M, P\\}$.   (1)\nLet T denote transpose, the self-attention $A_{\\text{self}}$ for a single modality $m$ is defined as\n$A_{\\text{self}} (X_m) = \\text{attention}(Q^m, K^m, V^m)\n= \\text{softmax}(\\frac{Q^m K^{mT}}{\\sqrt{f_e}}) V^m$.  (2)\nEach self-attention branch aims to independently extract unique features from one input modality. To ensure that each branch efficiently identifies distinct modality-dependent features, a normalization technique RegBN [10] is later applied to the latent space, aiming to reduce redundant partial dependency between the self-attention branches."}, {"title": "3.3. Bi-Attention for Multiple Modalities", "content": "A novel bi-attention mechanism $A_{bi}$ is introduced in $F_{M,P}$ to compute the interweaved attention between two modalities $\\{m, n\\}$, uniquely designed to focus on capturing their similarities in the high-dimensional feature space:\n$A_{bi} (X^m, X^n) = \\text{attention}(Q^m, K^n, V^n)\n= 1(Z_{m,n \\ge \\tau}) Z_{m,n}.v_n$,  (3)\n$\\qquad \\gamma_{m,n} = \\text{softmax}(\\frac{Q^m K^{nT}}{\\sqrt{f_e}})$; (4)\nwhere $m \\in \\{M,P\\}$, $n \\in \\{P,M\\}$, and $m \\neq n$. $1(.)$ is the indicator function to threshold the correlation matrix between the features of the two modalities to be above a constant threshold $\\tau$. Note that $A_{bi} (X^M, X^P) \\neq A_{bi} (X^P, X^M)$. Illustrated in Fig. 3, the bi-attention blocks aim to produce features for each modality conditioned on the other, targeting on their potential disease-specific similarities. Distinct from the conventional cross-attention mechanism, which captures both similarities and dissimilarities between patches in the correlation matrices, our proposed bi-attention selectively preserves only the pronounced similarities, resulting in a sparse representation.\nThis sparsity is achieved by applying a constant threshold $\\tau$ to the correlation matrix between the query and key matrices from two modalities, filtering out negligible values. This mechanism enables efficient capture of dependencies between modalities for improved diagnostic accuracy and robustness, as validated in Sec. 6.1 & 6.3."}, {"title": "3.4. RegBN for Dependency Removal", "content": "RegBN is a normalization method devised for dependency and confounding removal from low- and high-level features before fusing those [10]. As discussed earlier, MRI and PET input images are mapped into latent space, represented as $z_m \\in \\mathbb{R}^f$ for MRI and $z_p \\in \\mathbb{R}^f$ for PET. Dependencies between modalities are present during image acquisition; thus, such information can be transferred to the latent space, as illustrated by $R_{MP}$ in Fig. 1. Since the proposed bi-attention block focuses explicitly on the underlying similarities between input modalities via the self-attention modules, it is essential to eliminate redundant shared information between them. Otherwise, the neural network may primarily optimize within the overlapped region $R_{MP}$, increasing the risk of getting trapped in local minima. Thus, we use RegBN to separate the latent encoding $z_M$ from $z_P$.\nRegBN represents one latent encoding in terms of another using a linear regression model:\n$z_M = w \\cdot z_P + z_{M,r}$, (5)\nin which $w \\in \\mathbb{R}^{f\\times f}$ is a projection matrix, and $z_{M,r} \\in \\mathbb{R}^f$ denotes the difference between the input latent encodings,"}, {"title": "3.5. Backbone Architecture", "content": "We adopt a 3D multiple instance neuroimage Transformer [28] as our backbone, while it is straightforward to generalize the backbone to any 3D ViT architecture. The adopted backbone is a convolution-free Transformer-based architecture inspired by the multiple instance learning paradigm. Given the input image, the backbone first splits it into $b$ non-overlapping cubiform blocks $m_i \\in \\mathbb{R}^{b\\times b\\times b}$, where $i \\in \\{1, ..., b\\}$, $m\\in \\{M, P\\}$, and b denotes the block size. Each $m_i$ is then treated as one instance to be fed into the Transformer encoder independently. Learned block positional embeddings will be added to each $m_i$, aiming to preserve the positional information of each block within the whole image. The Transformer encoder consists of N Transformer layers. Each of the layers contains either a multi-head self-attention block or a multi-head bi-"}, {"title": "4. Experiment Setup", "content": "Dataset and Preprocessing: We use paired FDG-PET and T1-weighted MRI scans from three datasets:\n1. Alzheimer's disease neuroimaging initiative (ADNI) database [16], including 379 CN, 257 AD, and 611 MCI samples.\n2. Japanese Alzheimer's Disease Neuroimaging Initiative (J-ADNI) database [15], including 104 CN, 78 AD, and 124 MCI samples.\n3. In-house clinical dataset containing two types of dementia, with 143 CN, 110 AD, and 57 FTD samples, from Klinikum rechts der Isar, Munich, Germany.\nDue to the lack of a public multi-modal dataset for both AD and FTD subjects, we evaluate the differential diagnosis on a well-characterized, single-site in-house clinical dataset from Klinikum rechts der Isar, Munich, Germany. Tab. 1 reports statistics of all used datasets. Only scans from baseline visits are selected. We extract the gray matter (GM) density maps from MRI as input. All scans were normalized, registered to the MNI152 template with 1.5mm\u00b3 voxel size, and rescaled to the intensity range between 0 and 1, with the final size of 128 \u00d7 128 \u00d7 128.\nTo avoid biased results due to data leakage and confounding effects, we split the data using only baseline visits and ensure that diagnosis, age, and sex are balanced across sets. Further information on data preprocessing and splitting can be found in Sec. A.1."}, {"title": "5. Results", "content": "5.1. Alzheimer's Prediction\nWe compare DiaMond with other baseline methods for the task of Alzheimer's prediction on two different datasets. Tab. 2 reports the results for both binary (CN vs. AD) and three-way (CN vs. MCI vs. AD) classification on ADNI and J-ADNI datasets, respectively. For binary classification, PET scans generally yield better results than MRI as a single modality input, reaching a BACC of 89% on ADNI and 89.4% on J-ADNI, whereas MRI only achieves a BACC of 86.6% on ADNI and 84.7% on J-ADNI at the largest. Early, middle, and late fusion methods, together with Mul-T [8] and MMTFN [25], regardless of using ResNet or ViT as the backbone structure, can only achieve on-par performance as PET alone, which is aligned with the conclusion in [26], as the diagnostic performance of existing multi-modal fusion methods may not yet outperform that of using PET alone. On the contrary, DiaMond outperforms all other methods and single modality input by a large margin for both datasets, achieving a BACC of 92.4% in ADNI, and 91.7% in J-ADNI."}, {"title": "5.2. Differential Diagnosis of Dementia", "content": "Further, we conduct a three-way differential diagnosis of dementia, between subjects of CN, AD, and FTD. This task is challenging due to the overlapping symptoms between different types of dementia; however, it is highly important due to its distinctive clinical value [5]. Tab. 3 reports the results of DiaMond and other baseline methods on this task.\nWhen using single modalities as input, MRI only achieves a BACC of 56.7% with ResNet, and 66.0% using ViT as backbone. PET can achieve a BACC of 68.7% with ResNet and 69.5% with ViT, confirming its higher sensitivity in the differential diagnosis. Using multi-modal input of both MRI and PET can elevate the diagnostic accuracy compared to using single modality alone, particularly when em-"}, {"title": "5.3. Fairness Evaluation", "content": "Ensuring fairness is a paramount consideration in the domain of medical imaging. DL models employed in medical applications must minimize biases towards specific demographic groups, such as age, gender, and diagnostic labels. In this regard, we evaluate the fairness of the diagnostic results produced by DiaMond on the ADNI dataset for AD prediction, by examining its test accuracy across diverse patient cohorts. The results presented in Tab. 4 indicate that"}, {"title": "6. Ablation Study", "content": "We conduct a comprehensive ablation study on the important components in DiaMond. This includes evaluating the inclusion of different ViT branches (self- and bi-attention), the integration of RegBN, and the application of the attention threshold $\\tau$ in our bi-attention design. In the end, we include the ablation on the network parameters."}, {"title": "6.1. Different Branches", "content": "DiaMond comprises three independent ViT branches: $F_M$ with MRI as input, $F_P$ with PET, and $F_{M,P}$ receives both MRI and PET as input simultaneously. To validate the efficacy of the three branches in DiaMond, we conduct ablation studies on each of the branch and their different combinations. As shown in Tab. 5, using $F_M$ or $F_P$ alone"}, {"title": "6.2. Integration of RegBN", "content": "RegBN is incorporated into DiaMond as a normalization technique to make self-attention branches independent, aiming to reduce the redundant partial dependency between the input modalities. We further evaluate the impact of RegBN in our model. As shown in Tab. 6, the performance of DiaMond is affected by the presence of RegBN across all three datasets, with this normalization method enhancing classification results by up to 2%. This improvement"}, {"title": "6.3. Bi-Attention Threshold $\\tau$", "content": "We use a constant threshold $\\tau$ in Eq. (3) to filter out very small values in the correlation matrices within the bi-attention block, so that it focuses primarily on similarities between modalities. As illustrated in Fig. 4, a $\\tau$ value of approximately 0.01 typically results in better and more stable performance. In contrast, having no threshold ($\\tau$ = 0, equals to a conventional cross-attention mechanism) or setting the threshold too high causes a drop in performance or high variation in the outcomes. Thus, including an optimal attention threshold is crucial for the bi-attention block, as it reduces the redundancy of learning repetitive features as captured in the self-attention blocks, and efficiently helps to focus on the dependencies between modalities."}, {"title": "6.4. Network Parameters", "content": "We conduct ablation studies on the network parameters, performing an exhaustive search over the following parameters: patch size in {4, 8}, embedding dimension in {128, 256, 512, 1024}, number of attention heads in {8, 16}, model depth in {1, 2, 4, 8}, and dropout rate in {0, 0.2, 0.5}. We use the validation set to compare different configurations, with the results presented in Tab. 7. As a result, the combination of a patch size of 8, a feature dimension of 512, 8 heads, a depth of 4, and a dropout rate of 0.0 yields the highest validation accuracy. Therefore, we adopt these parameters for all experiments."}, {"title": "7. Conclusion", "content": "We introduced DiaMond, a ViT-based framework for Alzheimer's prediction and differential diagnosis of dementia using MRI and PET. DiaMond effectively learns from multi-modal data via self- and a novel bi-attention mechanism from a pure ViT backbone. The self-attention mechanism extracts distinct features from individual modalities, along with a normalization strategy to ensure feature independence; our novel bi-attention mechanism exclusively focuses on the similarities between multiple modalities, aiming to capture their disease-specific dependency. Across three distinct datasets, DiaMond consistently outperformed all competing methods, achieving a balanced accuracy of 92.4% for AD-CN classification, 65.2% for AD-MCI-CN classification, and 76.5% for differential diagnosis between AD, FTD, and CN subjects, highlighting its significant clinical value. We evaluated the fairness of our model, which indicated equitable performance across various demographic groups. Our comprehensive ablation study validated DiaMond's robustness and synergistic effect of integrating multiple modalities with its intricate design. Over-"}]}