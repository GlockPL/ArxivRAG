{"title": "Uncertainty-based Offline Variational Bayesian Reinforcement Learning for Robustness under Diverse Data Corruptions", "authors": ["Rui Yang", "Jie Wang", "Guoping Wu", "Bin Li"], "abstract": "Real-world offline datasets are often subject to data corruptions (such as noise or adversarial attacks) due to sensor failures or malicious attacks. Despite advances in robust offline reinforcement learning (RL), existing methods struggle to learn robust agents under high uncertainty caused by the diverse corrupted data (i.e., corrupted states, actions, rewards, and dynamics), leading to performance degradation in clean environments. To tackle this problem, we propose a novel robust variational Bayesian inference for offline RL (TRACER). It introduces Bayesian inference for the first time to capture the uncertainty via offline data for robustness against all types of data corruptions. Specifically, TRACER first models all corruptions as the uncertainty in the action-value function. Then, to capture such uncertainty, it uses all offline data as the observations to approximate the posterior distribution of the action-value function under a Bayesian inference framework. An appealing feature of TRACER is that it can distinguish corrupted data from clean data using an entropy-based uncertainty measure, since corrupted data often induces higher uncertainty and entropy. Based on the aforementioned measure, TRACER can regulate the loss associated with corrupted data to reduce its influence, thereby enhancing robustness and performance in clean environments. Experiments demonstrate that TRACER significantly outperforms several state-of-the-art approaches across both individual and simultaneous data corruptions.", "sections": [{"title": "1 Introduction", "content": "Offline reinforcement learning (RL) aims to learn an effective policy from a fixed dataset without direct interaction with the environment [1, 2]. This paradigm has recently attracted much attention in scenarios where real-time data collection is expensive, risky, or impractical, such as in healthcare [3], autonomous driving [4], and industrial automation [5]. Due to the restriction of the dataset, offline RL confronts the challenge of distribution shift between the policy represented in the offline dataset and the policy being learned, which often leads to the overestimation for out-of-distribution (OOD) actions [1, 6, 7]. To address this challenge, one of the promising approaches introduce uncertainty estimation techniques, such as using the ensemble of action-value functions or Bayesian inference to measure the uncertainty of the dynamics model [8-11] or the action-value function [12-15] regarding the rewards and transition dynamics. Therefore, they can constrain the learned policy to remain close to the policy represented in the dataset, guiding the policy to be robust against OOD actions.\nIn this paper, we propose to use offline data as the observations, thus leveraging their correlations to capture the uncertainty induced by all corrupted data. Considering that (1) diverse corruptions may introduce uncertainties into all elements in the offline dataset, and (2) each element is correlated with the action values (see dashed lines in Figure 1), there is high uncertainty in approximating the action-value function by using various corrupted data. To address this high uncertainty, we propose to leverage all elements in the dataset as observations, based on the graphical model in Figure 1. By using the high correlations between these observations and the action values [27], we can accurately identify the uncertainty of the action-value function.\nMotivated by this idea, we propose a robust variational Bayesian inference for offline RL (TRACER) to capture the uncertainty via offline data against all types of data corruptions. Specifically, TRACER first models all data corruptions as uncertainty in the action-value function. Then, to capture such uncertainty, it introduces variational Bayesian inference [28], which uses all offline data as observations to approximate the posterior distribution of the action-value function. Moreover, the corrupted observed data often induce higher uncertainty than clean data, resulting in higher entropy in the distribution of action-value function. Thus, TRACER can use the entropy as an uncertainty measure to effectively distinguish corrupted data from clean data. Based on the entropy-based uncertainty measure, it can regulate the loss associated with corrupted data in approximating the action-value distribution. This approach effectively reduces the influence of corrupted samples, enhancing robustness and performance in clean environments.\nThis study introduces Bayesian inference into offline RL for data corruptions. It significantly captures the uncertainty caused by diverse corrupted data, thereby improving both robustness and performance in offline RL. Moreover, it is important to note that, unlike traditional Bayesian online and offline RL methods that only model uncertainty from rewards and dynamics [29\u201335], our approach identifies the uncertainty of the action-value function regarding states, actions, rewards, and dynamics under data corruptions. We summarize our contributions as follows.\n\u2022 To the best of our knowledge, this study introduces Bayesian inference into corruption-robust offline RL for the first time. By leveraging all offline data as observations, it can capture uncertainty in the action-value function caused by diverse corrupted data.\n\u2022 By introducing an entropy-based uncertainty measure, TRACER can distinguish corrupted from clean data, thereby regulating the loss associated with corrupted samples to reduce its influence for robustness.\n\u2022 Experiment results show that TRACER significantly outperforms several state-of-the-art offline RL methods across a range of both individual and simultaneous data corruptions."}, {"title": "2 Preliminaries", "content": "Bayesian RL. We consider a Markov decision process (MDP), denoted by a tuple $M = (S, A, R, P, P_0, \\gamma)$, where $S$ is the state space, $A$ is the action space, $R$ is the reward space, $P(\\cdot|s, a) \\in P(S)$ is the transition probability distribution over next states conditioned on a state-action pair $(s, a)$, $P_0(\\cdot) \\in P(S)$ is the probability distribution of initial states, and $\\gamma \\in [0, 1)$ is the discount factor. Note that $P(S)$ and $P(A)$ denote the sets of probability distributions on subsets of $S$ and $A$, respectively. For simplicity, throughout the paper, we use uppercase letters to refer to random variables and lowercase letters to denote values taken by the random variables. Specifically, $R(s, a)$ denotes the random variable of one-step reward following the distribution $p(r|s, a)$, and $r(s, a)$ represents a value of this random variable. We assume that the random variable of one-step rewards and their expectations are bounded by $R_{\\text{max}}$ and $r_{\\text{max}}$ for any $(s, a) \\in S \\times A$, respectively.\nOur goal is to learn a policy that maximizes the expected discounted cumulative return:\n$\\pi^* = \\arg \\max_{\\pi \\in P(A)} E_{s_0 \\sim P_0, a_t \\sim \\pi(\\cdot|s_t), R \\sim p(\\cdot|s_t, a_t), s_{t+1} \\sim P(\\cdot|s_t, a_t)} [\\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t)]$\nBased on the return, we can define the value function as $V^{\\pi}(s) = E_{\\pi, p, P} [\\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t)|s_0 = s]$, the action-value function as $Q^{\\pi}(s, a) = E_{R \\sim p(\\cdot|s, a), s' \\sim P(\\cdot|s, a)} [R(s, a) + V(s')]$, and the action-value distribution [36] as\n$D^{\\pi}(s, a) = \\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t | s_0 = s, a_0 = a)$, with $s_{t+1} \\sim P(\\cdot|s_t, a_t), a_{t+1} \\sim \\pi(\\cdot|s_{t+1})$.(1)\nNote that $V^{\\pi}(s) = E_{a \\sim \\pi} [Q^{\\pi}(s, a)] = E_{a \\sim \\pi, p} [D^{\\pi}(s, a)]$.\nVariational Inference. Variational inference is a powerful method for approximating complex posterior distributions, which is effective for RL to handle the parameter uncertainty and deal with modelling errors [36]. Given an observation $X$ and latent variables $Z$, Bayesian inference aims to compute the posterior distribution $p(Z|X)$. Direct computation of this posterior is often intractable due to the high-dimensional integrals involved. To approximate the true posterior, Bayesian inference introduces a parameterized distribution $q(Z; \\phi)$ and minimizes the Kullback-Leibler (KL) divergence $D_{KL}(q(Z; \\phi)||p(Z|X))$. Note that minimizing the KL divergence is equivalent to maximizing the evidence lower bound (ELBO) [37, 38]: $ELBO(\\phi) = E_{q(z;\\phi)} [\\log p(X, Z) - \\log q(Z; \\phi)]$.\nOffline RL under Diverse Data Corruptions. In the real world, the data collected by sensors or humans may be subject to diverse corruption due to sensor failures or malicious attacks. Let $b$ and $B$ denotes the uncorrupted and corrupted dataset with samples $\\{(s_i, a_i, r_i, s_{i+1})\\}_{i=1}^N$, respectively. Each data in $B$ may be corrupted. We assume that an uncorrupted state follows a state distribution $p_b(\\cdot)$, a corrupted state follows $p_s(\\cdot)$, an uncorrupted action follows a behavior policy $\\pi_b(\\cdot|s)$, a corrupted action is sampled from $\\pi_{\\beta}(\\cdot|s)$, a corrupted reward is sampled from $p_\\beta(\\cdot|s, a)$, and a corrupted next state is drawn from $P_\\beta(\\cdot|s, a)$. We also denote the uncorrupted and corrupted empirical state-action distributions as $p_r(s_t, a_t)$ and $p_s(s_i, a_i)$, respectively. Moreover, we introduce the notations [18, 39] as follows.\n$T Q(s, a) = r(s, a) + E_{s'\\sim P_s(\\cdot|s, a)} [V(s')],\\ \\ \\ r(s,a) = E_{r\\sim p_s(\\cdot|s,a)} [r]$,(2)\n$T D(s,a): R(s, a) + \\gamma D^{\\pi} (s',a'),\\ \\ \\ s' \\sim P_{\\beta}(\\cdot|s, a), a' \\sim \\pi_{\\beta}(\\cdot|s)$,(3)"}, {"title": "3 Algorithm", "content": "We first introduce the Bayesian inference for capturing the uncertainty caused by diverse corrupted data in Section 3.1. Then, we provide our algorithm TRACER with the entropy-based uncertainty measure in Section 3.2. Moreover, we provide the theoretical analysis for robustness, the architecture, and the detailed implementation of TRACER in Appendices A.1, B.1, and B.2, respectively.\n3.1 Variational Inference for Uncertainty induced by Corrupted Data\nWe focus on corruption-robust offline RL to learn an agent under diverse data corruptions, i.e., random or adversarial attacks on four elements of the dataset. We propose to use all elements as observations, leveraging the data correlations to simultaneously address the uncertainty. By introducing Bayesian inference framework, our aim is to approximate the posterior distribution of the action-value function.\nAt the beginning, based on the relationships between the action values and the four elements (i.e., states, actions, rewards, next states) in the offline dataset as shown in Figure 1, we define $D_\\theta = D(S, A, R) \\sim p_\\theta(\\cdot|S, A, R)$, parameterized by $\\theta$. Building on the action-value distribution, we can explore how to estimate the posterior of $D_\\theta$ using the elements available in the offline data.\nFirstly, we start from the actions $\\{a_i\\}_{i=1}^N$ following the corrupted distribution $\\pi_\\beta$ and use them as observations to approximate the posterior of the action-value distribution under a variational inference. As the actions are correlated with the action values and all other elements in the dataset, the likelihood is $p_{\\varphi_a}(A|D, S, R, S')$, parameterized by $\\varphi_a$. Then, under the variational inference framework, we maximize the posterior and derive to minimize the loss function based on ELBO:\n$L_{D\\backslash A}(\\theta, \\varphi_a) = E_{B, p_\\theta} [D_{KL}(p_{\\varphi_a}(A|D_\\theta, S, R, S') || \\pi_\\beta(A|S)) - E_{A \\sim p_{\\varphi_a}} [\\log p_\\theta(D_\\theta|S, A, R)]]$,(7)\nwhere $S$, $R$, and $S'$ follow the offline data distributions $p_B$, $p_B$, and $P_B$, respectively.\nSecondly, we apply the rewards $\\{r_i\\}_{i=1}^N$ drawn from the corrupted reward distribution $p_\\beta$ as the observations. Considering that the rewards are related to the states, actions, and action values, we model the likelihood as $p_{\\varphi_r}(R|D, S, A)$, parameterized by $\\varphi_r$. Therefore, we can derive a loss function by following Equation (7):\n$L_{D\\backslash R}(\\theta,\\varphi_r) = E_{B, p_\\theta} [D_{KL} (p_{\\varphi_r}(R|D_\\theta, S, A) || p_\\beta(R|S, A)) - E_{R \\sim p_{\\varphi_r}} [\\log p_\\theta (D_\\theta|S, A, R)]]$,(8)\nwhere $S$ and $A$ follow the offline data distributions $P_B$ and $\\pi_\\beta$, respectively.\nFinally, we employ the state $\\{s_i\\}_{i=1}^N$ in the offline dataset following the corrupted distribution $p_\\beta$ as the observations. Due to the relation of the states, we can model the likelihood as $p_{\\varphi_s}(S|D, A, R)$, parameterized by $\\varphi_s$. We then have the loss function:\n$L_{D\\backslash S}(\\theta,\\varphi_s) = E_{B, p_\\theta} [D_{KL}(p_{\\varphi_s}(S|D_\\theta, A, R) || p_\\beta(S)) - E_{S \\sim p_{\\varphi_s}} [\\log p_\\theta (D_\\theta|S, A, R)]]$,(9)\nwhere $A$ and $R$ follow the offline data distributions $\\pi_\\beta$ and $P_\\beta$, respectively.\nThe goal of first terms in Equations (7), (8), and (9) is to estimate $\\pi_\\beta(A|S)$, $p_\\beta(R|S, A)$, and $p_r(S)$ using $p_{\\varphi_a} (A|D_\\theta, S, R, S')$, $p_{\\varphi_r}(R|D_\\theta, S, A)$, and $p_{\\varphi_s} (S|D_\\theta, A, R)$, respectively. As we do not"}, {"title": "3.2 Corruption-Robust Algorithm with the Entropy-based Uncertainty Measure", "content": "We focus on developing tractable loss functions for implementation in this subsection.\nLearning the Action-Value Distribution based on Temporal Difference (TD). Based on [42, 43], we introduce the quantile regression [44] to approximate the action-value distribution in the offline dataset $B$ using an ensemble model $\\{D_{\\theta_i}\\}_{i=1}^N$. We use Equation (4) to derive the loss as:\n$L_{D} (\\theta_i) = \\frac{1}{NN'}\\sum_{n=1}^{N}\\sum_{m=1}^{N'} \\rho_K(\\delta_{\\tau,\\tau'})^{+}(\\delta_{\\tau,\\tau'} - D_{\\theta_i}^{\\tau'}(s, a, r))$,(12)\nwhere $\\rho_K (\\delta) = |\\tau - I {\\delta < 0}| \\cdot \\lvert \\delta \\rvert $ with the threshold $K$, $Z$ denotes the value distribution, $\\delta_{\\tau,\\tau'}$ is the sampled TD error based on the parameters $\\theta_i$, $\\tau$ and $\\tau'$ are two samples drawn from a uniform distribution $U ([0, 1])$, $D_{\\theta_i}^{\\tau}(s, a, r) := F_{D(s,a,r)}^{-1} (\\tau)$ is the sample drawn from $p_\\theta(\\cdot|s, a, r)$, $Z_{\\psi}^{\\tau}(s) := F_{Z_{\\psi}(s)}^{-1} (\\tau)$ is sampled from $p(\\cdot|s)$, $F^{-1}(r)$ is the inverse cumulative distribution function (also known as quantile function) [45] at $\\tau$ for the random variable $X$, and $N$ and $N'$ represent the respective number of iid samples $\\tau$ and $\\tau'$. Notably, based on [43], we have $Q_{\\theta_i} (s, a) = \\sum_{n=1}^{N} D_{\\theta_i}^{\\tau}(s, a, r)$.\nIn addition, if we learn the value distribution $Z$, the action-value distribution can extract the informa-tion from the next states based on Equation (12), which is effective for capturing the uncertainty. On the contrary, if we directly use the next states in the offline dataset as the observations, in practice, the parameterized model of the action-value distribution needs to take $(s, a, r, s', a', r', s'')$ as the input data. Thus, the model can compute the action values and values for the sampled TD error in Equation (12). To avoid the changes in the input data caused by directly using next states as observations in Bayesian inference, we draw inspiration from IQL and RIQL to learn a parameterized value distribution. Based on Equations (5) and (12), we derive a new objective as:\n$L_Z(\\psi) = E_{(s,a) \\sim B}[ \\mathcal{L}_H (D_{\\tau} (s,a,r) - \\frac{1}{N} \\sum_{n=1}^{N} Z_\\psi^{\\tau} (s))]$(13)\nwhere $D_\\tau^{\\theta}$ is the $\\alpha$-quantile value among $\\{D_{\\theta_i}^{\\tau} (s,a)\\}_{i=1}^N$, and $V_\\psi(s) = \\sum_{n=1}^{N} Z_\\psi^{\\tau} (s)$. More details are shown in Appendix B.2. Furthermore, we provide the theoretical analysis in Appendix A.1 to give a value bound between the value distributions under clean and corrupted data.\nUpdating the Action-Value Distribution based on Variational Inference for Robustness. We discuss the detailed implementation of Equations (10) and (11) based on Equations (12) and (13). As the data"}, {"title": "4 Experiments", "content": "In this section, we show the effectiveness of TRACER across various simulation tasks using diverse corrupted offline datasets. Firstly, we provide our experiment setting, focusing on the corruption settings for offline datasets. Then, we illustrate how TRACER significantly outperforms previous state-of-the-art approaches under a range of both individual and simultaneous data corruptions. Finally, we conduct validation experiments and ablation studies to show the effectiveness of TRACER.\n4.1 Experiment Setting\nBuilding upon RIQL [18], we use two hyperparameters, i.e., corruption rate $c \\in [0, 1]$ and corruption scale $\\epsilon$, to control the corruption level. Then, we introduce the random corruption and adversarial corruption in four elements (i.e., states, actions, rewards, next states) of offline datasets. The implementation of random corruption is to add random noise to elements of a $c$ portion of the offline datasets, and the implementation of adversarial corruption follows the Projected Gradient Descent attack [53, 54] using pretrained value functions. Note that unlike other adversarial corruptions, the adversarial reward corruption multiplies \u2013$\\epsilon$ to the clean rewards instead of using gradient optimization.\nWe also introduce the random or adversarial simultaneous corruption, which refers to random or adversarial corruption simultaneously present in four elements of the offline datasets. We apply the corruption rate $c = 0.3$ and corruption scale $\\epsilon = 1.0$ in our experiments."}, {"title": "5 Related Work", "content": "Robust RL. Robust RL can be categorized into two types: testing-time robust RL and training-time robust RL. Testing-time robust RL [19, 20] refers to training a policy on clean data and ensuring its robustness by testing in an environment with random noise or adversarial attacks. Training-time robust RL [16, 17] aims to learn a robust policy in the presence of random noise or adversarial attacks during training and evaluate the policy in a clean environment. In this paper, we focus on training-time robust RL under the offline setting, where the offline training data is subject to various data corruptions, also known as corruption-robust offline RL.\nCorruption-Robust RL. Some theoretical work on corruption-robust online RL [60\u201363] aims to analyze the sub-optimal bounds of learned policies under data corruptions. However, these studies primarily address simple bandits or tabular MDPs and focus on the reward corruption. Some further work [64, 65] extends the modeling problem to more general MDPs and begins to investigate the corruption in transition dynamics.\nIt is worth noting that corruption-robust offline RL has not been widely studied. UWMSG [17] designs a value-based uncertainty-weighting technique, thus using the weight to mitigate the impact of corrupted data. RIQL [18] further extends the data corruptions to all four elements in the offline dataset, including states, actions, rewards, and next states (dynamics). It then introduces quantile estimators with an ensemble of action-value functions and employs a Huber regression based on IQL [26], alleviating the performance degradation caused by corrupted data.\nBayesian RL. Bayesian RL integrates the Bayesian inference with RL to create a framework for decision-making under uncertainty [28]. It is important to highlight that Bayesian RL is divided into two categories for different uncertainties: the parameter uncertainty in the learning of models [66, 67] and the inherent uncertainty from the data/environment in the distribution over returns [68, 69]. In this paper, we focus on capturing the latter.\nFor the latter uncertainty, in model-based Bayesian RL, many approaches [68, 70, 71] explicitly model the transition dynamics and using Bayesian inference to update the model. It is useful when dealing with complex environments for sample efficiency. In model-free Bayesian RL, value-based methods [69, 72] use the reward information to construct the posterior distribution of the action-value function. Besides, policy gradient methods [73, 74] use information of the return to construct the posterior distribution of the policy. They directly apply Bayesian inference to the value function or policy without explicitly modeling transition dynamics.\nOffline Bayesian RL. offline Bayesian RL integrates Bayesian inference with offline RL to tackle the challenges of learning robust policies from static datasets without further interactions with the environment. Many approaches [75\u201377] use Bayesian inference to model the transition dynamics or guide action selection for adaptive policy updates, thereby avoiding overly conservative estimates in the offline setting. Furthermore, recent work [78] applies variational Bayesian inference to learn the model of transition dynamics, mitigating the distribution shift in offline RL."}, {"title": "6 Conclusion", "content": "In this paper, we investigate and demonstrate the robustness and effectiveness of introducing Bayesian inference into offline RL to address the challenges posed by data corruptions. By leveraging Bayesian techniques, our proposed approach TRACER captures the uncertainty caused by diverse corrupted data. Moreover, the use of entropy-based uncertainty measure in TRACER can distinguish corrupted data from clean data. Thus, TRACER can regulate the loss associated with corrupted data to reduce its influence, improving performance in clean environments. Our extensive experiments demonstrate the potential of Bayesian methods in developing reliable decision-making.\nRegarding the limitations of TRACER, although it achieves significant performance improvement under diverse data corruptions, future work could explore more complex and realistic data corruption scenarios and related challenges, such as the noise in the preference data for RLHF and adversarial attacks on safety-critical driving decisions. Moreover, we look forward to the continued development and optimization of uncertainty-based corrupted-robust offline RL, which could further enhance the effectiveness of TRACER and similar approaches for increasingly complex real-world scenarios."}]}