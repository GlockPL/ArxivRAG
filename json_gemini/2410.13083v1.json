{"title": "FedCAP: Robust Federated Learning via Customized Aggregation and Personalization", "authors": ["Youpeng Li", "Xinda Wang", "Fuxun Yu", "Lichao Sun", "Wenbin Zhang", "Xuyu Wang"], "abstract": "Federated learning (FL), an emerging distributed machine learning paradigm, has been applied to various privacy-preserving scenarios. However, due to its distributed nature, FL faces two key issues: the non-independent and identical distribution (non-IID) of user data and vulnerability to Byzantine threats. To address these challenges, in this paper, we propose FedCAP, a robust FL framework against both data heterogeneity and Byzantine attacks. The core of FedCAP is a model update calibration mechanism to help a server capture the differences in the direction and magnitude of model updates among clients. Furthermore, we design a customized model aggregation rule that facilitates collaborative training among similar clients while accelerating the model deterioration of malicious clients. With a Euclidean norm-based anomaly detection mechanism, the server can quickly identify and permanently remove malicious clients. Moreover, the impact of data heterogeneity and Byzantine attacks can be further mitigated through personalization on the client side. We conduct extensive experiments, comparing multiple state-of-the-art baselines, to demonstrate that FedCAP performs well in several non-IID settings and shows strong robustness under a series of poisoning attacks.", "sections": [{"title": "I. INTRODUCTION", "content": "With the emergence of large foundation models [1], model performance increasingly relies on high-quality and high-volume data. In fields such as Internet of Things (IoT) [2]-[4] and healthcare [5], [6], user data often contains a large amount of sensitive information. Various privacy-preserving policies such as the General Data Protection Regulation (GDPR) [7], [8] restrict the collection of user data by a central server. As an emerging distributed machine learning paradigm, federated learning (FL) [9] allows user data to remain local while coordinating clients to train a global model. Due to its distributed nature, FL faces two key issues. First, statistical heterogeneity exists in user data. In real-world FL applications, such as Google's next word prediction, training a single global model that caters to the individual needs of all users is challenging due to their diverse language habits and regional cultures [10]. Second, FL systems are vulnerable to Byzantine threats [11], [12], with malicious clients uploading arbitrary model updates to the server, which can greatly degrade model performance on any test inputs (i.e., untargeted poisoning attack [13]). To mitigate the impact of data heterogeneity, the concept of personalized FL is introduced [14], where each client holds a personalized model to fit its own data distribution better. However, most personalized FL algorithms [15]-[17] fail to adapt to non-independent and identically distributed (non-IID) settings. Therefore, we need to address Challenge 1:\n\u2022 How to design a personalized FL framework that exhibits adaptiveness in various heterogeneous data settings?\nTo defend against poisoning attacks, existing robust FL methods adopt diverse strategies, with some focusing on the server side such as detection [18], [19] and robust aggregation [20]\u2013[23], while others concentrate on the client side through personalization [15]. However, the above robust FL methods are less effective against attacks in non-IID settings [24], [25], due to the difficulties in distinguishing malicious behavior from clients. This leads to varying degrees of aggregation knowledge loss while defending against attacks, which in turn results in model performance degradation. Moreover, in settings with strong attacks [26]\u2013[28], malicious clients can camouflage themselves as benign, making it more difficult for robust FL methods to detect them and causing further deterioration in the benign models. Therefore, it is imperative to tackle Challenge 2:\n\u2022 How can we design a Byzantine-robust FL framework that precisely distinguishes between benign and malicious clients in non-IID settings without causing a significant loss in model accuracy?\nFor real-world applications, a unified FL framework considering both challenges is needed, but few studies focus on this. Although Ditto [15] mitigates the impact of data heterogeneity and attacks via personalization, one of its limitations is that it does not directly detect malicious clients and instead requires a trade-off between model utility and robustness. A naive strategy is to combine robust aggregation rules (AGRs) (e.g., Median, Trimmed Mean [20], and ClusteredFL [22]) with client personalization (e.g., Ditto), but their inherent limitations lead to combinations that fail to improve model performance (see empirical results in Fig. 8)."}, {"title": "II. RELATED WORK", "content": "Federated Learning with Non-IID Data. FL can be broadly categorized into two types: single-model FL and multi-model FL. In single-model FL, clients collaboratively train a single global model [9]. Existing research primarily focuses on two key aspects: enhancing the generalizability of the global model [16], [29], [30] and developing personalized FL algorithms to mitigate the impact of data heterogeneity [15]\u2013[17], [31]. For example, FedRoD [16] improves the generalizability of the global model by using balanced softmax loss to mitigate the effect of label distribution skew. FedAvg-FT [32] treats clients' updated local models as personalized models and evaluates their performance. Ditto [15] trains a personalized model for each client while locally updating the global model. Given the limitation of generalizability of the single global model, the concept of multi-model FL, which refers to the server holding multiple models, is introduced. For example, in clustering-based FL algorithms [22], [33], the server divides clients into multiple clusters, and within each cluster, the clients collaborate to train a group model. Unlike clustering, FedFomo [17] probabilistically selects batches of uploaded models and distributes them to clients, which each calculate model weights and perform weighted aggregation to obtain tailored models. However, sending multiple models to the clients in each communication round introduces high communication overhead. Compared to FedFomo, FedCAP performs customized aggregation on the server side without incurring any additional communication overhead.\nByzantine-robust Federated Learning. Given the distributed nature of FL, it is vulnerable to Byzantine threats [13], [34], [35]. To defend against Byzantine attacks, a series of server-side AGRs built on top of averaged aggregation have been proposed (e.g., Krum [21], Multi-Krum [21], Median [20], RFA [36], Trimmed Mean [20], etc.). Since these AGRS assume that all clients' data are IID, their robustness is less effective in non-IID settings. For non-IID defenses, in FLTrust [18], the server filters or processes abnormal model updates by checking the magnitude and direction of the client-uploaded model updates. However, FLTrust assumes the server holds a clean dataset to boost trust, which violates FL's privacy principles. To mitigate gradient heterogeneity, Karimireddy et al. [24] suggested dividing the uploaded model updates into several buckets before aggregation, averaging *s* model updates within each bucket, and then using AGRs to aggregate the updates across buckets. To address the issue of the curse of dimensionality [37], which enables malicious gradients to circumvent defenses that aggregate all honest"}, {"title": "III. BACKGROUND AND MOTIVATION", "content": "A. Federated Learning\nThe original goal of FL is to maintain user data locally while coordinating clients to train a single global model. The vanilla FL algorithm (i.e., FedAvg [9]) consists of three steps: In each round *t*, the server distributes the global model *wt* to participating clients, which then perform local training with their private data, uploading their updated models to the server. The server performs weighted averaging aggregation to get the updated global model *wt+1*. The optimization problem of FedAvg can be expressed as follows:\n$\\underset{w}{\\arg \\min} \\mathcal{L}(w)$ (initialized with $w^{t}$),\n$w^{t+1} \\leftarrow \\sum_{k=1}^{N} p_{k} w_{k}^{t}$     (1)\nwhere $\\mathcal{L}_{k} = \\frac{1}{D_{k}} \\sum_{(x_{i}, y_{i}) \\in \\mathcal{D}_{k}} l(x_{i}, y_{i}; w)$, $S(k \\in S)$ represents the set of clients, *N* is the number of participants in each round, $\\mathcal{D}_{k}$ denotes the training dataset of client *k*, *l* is the loss function, $(x_{i}, y_{i})$ denotes a sample pair, and $p_{k}$ represents the aggregation weight assigned to client *k*. However, real-world user data often exhibits non-IID characteristics with various distribution skews [10], making it challenging for a single global model to effectively generalize across heterogeneous data.\nPersonalized Federated Learning. To tackle the challenge of data heterogeneity, several personalized FL algorithms have been proposed. The optimization objective of the personalized model can be formulated in a general form as:\n$v^{t+1} = \\underset{v}{\\arg \\min} \\mathcal{L}(v) + \\lambda \\mathcal{R}(v, w^{*}),$     (2)\nwhere *v* is initialized with $v^{t}$ and represents the personalized model, $w^{*}$ denotes the global knowledge, such as the global model $w^{t}$ [15], $\\mathcal{R}$ denotes the regularizer, and $\\lambda$ denotes the regularization factor controlling the extent to which the personalized model *v* references the global knowledge $w^{*}$."}, {"title": "B. Limitations of the SOTA", "content": "1) Limitations of Existing Personalized FL Methods: To explore the limitations of representative FL approaches discussed in the related work, we evaluate the performance of FedAvg-FT [32], Ditto [15], and FedRoD [16] in benign scenarios using both CIFAR-10 [38] and EMNIST [39]. For CIFAR-10, we adopt the pathological non-IID setting [9] to divide the data into 20 clients with a participating ratio of 1.0, where each client's data contains only two class types. For EMNIST, following a previous study [30], we distribute the data across 100 clients with a participation ratio of 0.2, allocating 20% of the data as IID to each client and sorting the remaining 80% based on labels. To simulate a realistic setting, the size of each client's sample is limited to a few hundred. We report the average test accuracy of personalized models.\n\u2022 In benign scenarios, the performance of the above personalized FL methods is comparable to or even worse than FedAvg-FT. This suggests that they are effective only in specific non-IID settings and struggle to adapt well to various non-IID settings with different distribution skew. Similar statements can be found in [16], [32].\n2) Limitations of Existing Robust FL Methods: To assess the robustness of existing robust FL methods, we evaluate the test accuracy of the global model on CIFAR-10 [38]. Specifically, we examine the performance of FedAvg [9], FLTrust [18], and AGRs including Krum [21], Median [20], Trimmed Mean [20], and ClusteredFL [22] under Sign Flipping (SF) and Model Replacement (MR) attacks [40]."}, {"title": "C. Insights and Motivations", "content": "1) Good Becomes Better; Bad Becomes Worse: Despite the statistical heterogeneity of data among clients, inherent similarities (e.g., common features) in data distributions still exist among some clients [41]. Hence, promoting collaboration among similar clients can be advantageous. As the number of global rounds increases, models among similar clients become more similar and are less influenced by data heterogeneity. Moreover, by identifying anomalies in malicious behaviors, we can inhibit the cooperation between benign and malicious clients, safeguarding benign models from attacks. With more global rounds, malicious models are updated in a worse direction, accelerating their deterioration.\n2) Abnormal Euclidean Norm of Malicious Model Updates: To investigate the impact of Byzantine attacks on FL training, we evaluate FedAvg [9] under SF [40] and MR [15] attacks using CIFAR-10 [38]. Fig. 3 illustrates that in both attack scenarios, the Euclidean norm of model updates uploaded by malicious clients increases dramatically as the number of global rounds increases, deepening the impact of attacks on the global model. Ultimately, FL training becomes dominated by attacks, resulting in a substantial decrease in the model performance of benign clients.\nThese insights motivate us to propose the customized model aggregation rule, design the model update calibration mechanism, and develop the anomaly detection module. These innovations facilitate collaboration among similar clients, accurately capture differences in model updates between benign and malicious clients, and empower the server to identify and remove malicious clients."}, {"title": "IV. PROBLEM FORMULATION", "content": "A. Aggregation Function\nIn Eq. 1, the aggregation rule ignores the contributions of clients to each other, leading to the global model that unfairly favors clients with more samples. Our approach falls into the category of multi-model FL, where the server aggregates"}, {"title": "B. Threat Model", "content": "In this work, our focus is on enhancing Byzantine-robustness in FL against poisoning attacks.\nAdversary's Goal. The objective of the attack is to disrupt the FL training process, resulting in a significant degradation in model performance on any test inputs (i.e., untargeted poisoning attack [13]). In real-world scenarios, such attacks could cause FL systems to crash, leading to inaccurate model inference in downstream tasks (e.g., disease diagnosis), which could result in immeasurable losses [42].\nAdversary's Capabilities. Adversaries may intrude into FL systems by injecting fake clients or compromising benign ones. Considering the attack's cost and feasibility in real-world scenarios, the proportion of malicious clients typically does not exceed 50% [13]. Given their known knowledge, adversaries can manipulate the FL training process by uploading arbitrary or finely crafted malicious model updates to the server, affecting the model aggregation.\nAdversary's Knowledge. We consider attack scenarios where adversaries have partial knowledge, including model updates, local data, and local update rules from malicious clients. Despite its limited practical applicability, we further introduce a full-knowledge scenario to explore the upper bound of Byzantine robustness in our method. Under this assumption, adversaries have full knowledge of all clients, including benign ones, enabling them to design stronger and adaptive attacks."}, {"title": "V. DESIGN OF FEDCAP", "content": "A. Model Customization\nWe assume that FL training has progressed to round *t(t > 0)* and client *k* is joining FL training, where $k \\in S_{t}$ and $|S_{t}| = N$. The model customization for client *k* includes the following steps:\nCollection. The server collects the model update $d_{k}^{t-1}$ from client *k* (as shown in in Fig. 5). Depending on whether client *k* participated in the previous round or is a new client, the server handles it differently:\n\u2022 If client *k* participated in round *t\u2013 1* (i.e., $k \\in S_{t-1}$), the server locally keeps the calibrated update pool $\\{ \\bar{d_{i}^{t-1}} \\}_{i\\in S_{t-1}}$. In this case, there is no need to collect the model update $d_{k}^{t-1}$ from client *k* for round *t* (i.e., $\\bar{d_{k}^{t-1}} = d_{k}^{t}$), which helps save communication overhead. Since we cannot directly calculate the contribution of client *k* to itself, we define the contribution of client *k* to itself"}, {"title": "t-1", "content": "(i.e., $p_{k,k}$ in Eq. 5) as the weight factor $\\phi$, and the remaining $1-\\phi$ is assigned based on Eq. 4 and Eq. 5. The impact of $\\phi$'s value on model performance will be analyzed in Section VI-D2.\n\u2022 If client *k* did not join in round *t - 1* or is a new client, the server sends the global model $w^{t-1}$ to client *k*, and the client returns the model update $d_{k}^{t-1}$ to the server, where $d_{k}^{t-1} = w_{k}^{t} - w^{t-1}$.\nCustomized Aggregation. Then, the server performs the customized model aggregation (as shown in in Fig. 5). It first computes the cosine similarity (Eq. 4) between $\\bar{d_{k}^{t-1}}$ and the calibrated update pool $\\{ \\bar{d_{i}^{t-1}} \\}_{i\\in S_{t-1}}$:\n$\\rho_{k, i(k \\neq i)} = \\frac{\\left\\langle \\bar{d_{k}^{t-1}}, \\bar{d_{i}^{t-1}} \\right\\rangle}{\\| \\bar{d_{k}^{t-1}} \\| \\| \\bar{d_{i}^{t-1}} \\|},$     (4)\nwhere the contribution $\\rho_{k,i}$ between client *k* and *i* is determined by the similarity between calibrated model updates. The reason is that the similarity between model updates reflects the similarity of user data distribution [22], [33]. Clients with higher similarity contribute more valuable knowledge to each other. Therefore, when customizing aggregation weights, larger weights will be assigned to similar clients. In this way, the impact of data heterogeneity on customized models is mitigated by customized aggregation.\nSince the cosine similarity takes values in the range of [-1,1], the softmax function is introduced for normalization (Eq. 5) to ensure that the sum of aggregation weights is 1, and the weight of each client is non-negative.\n$\\rho_{k, i(k \\neq i)} = \\frac{e^{\\alpha \\rho_{k,i}}}{\\sum_{i \\in S_{t-1}}^{N} e^{\\alpha \\rho_{k,i}}}.$      (5)\nThe scale factor $\\alpha$ in Eq. 5 controls the sensitivity of the weight vector $p_{k}$ to the similarity vector $\\rho_{k}$. When $\\alpha = 0$, $\\rho_{k,i} = 1/N$, and all participants have the same weight. Thus, by controlling the value of scaling factor $\\alpha$, our customized aggregation can adapt to various degrees of data heterogeneity. Moreover, in attack scenarios, due to the low similarity between the model updates of malicious and benign clients, a large $\\alpha$ is suggested to amplify the penalty for malicious clients, ensuring that the corresponding weights of the malicious clients after normalization converge to 0. This prevents the aggregation process of the customized models of benign clients from being interfered with by malicious clients.\nOnce the aggregation weight vector $p_{k}$ is obtained, the server aggregates the recovered model pool $\\{ \\tilde{w_{i}^{t-1}} \\}_{i \\in S_{t-1}}$ based on $p_{k}$ to customize the model $\\tilde{w_{k}^{t}}$ for client *k* (Eq. 3).\nGlobal Model Updating. At the same time, the server aggregates the recovered model pool $\\{ \\tilde{w_{i}^{t-1}} \\}_{i \\in S_{t-1}}$ to update the global model (as shown in in Fig. 5). Even though FedCAP provides the customized model for each client, it still aggregates the global model in each round for subsequent model update calibration (see Section V-C). The aggregation of the global model is formulated as follows:"}, {"title": "B. Personalized Training", "content": "Unlike the global model aggregated in single-model FL, the customized model $\\tilde{w_{k}^{t}}$ better matches the data distribution of client *k*. However, considering that the customized model may still be affected by slight data heterogeneity or attacks in situations where no explicit similarity relationship exists among clients, we additionally train a personalized model for each client to further mitigate the impact of heterogeneity or attacks. The optimization objective for personalized model of client *k* is formulated as follows:\n$v_{k}^{t+1} = \\underset{v}{\\arg \\min} \\mathcal{L}_{k}(v) + \\frac{\\lambda}{2} \\| v - \\tilde{w_{k}^{t}} \\|_{2}^{2},$     (7)\nwhere *v* is initialized with $v_{k}^{t}$. Eq. 7 follows the general form of objective functions for personalized FL (Eq. 2). Distinguishing from the global model $w^{t}$, here, $w^{*}$ in Eq. 2 denotes the customized model $\\tilde{w_{k}^{t}}$, and the regularizer $\\mathcal{R}$ uses the square of the L2-norm. The regularization factor $\\lambda$ controls the extent to which client *k*'s personalized model *v* references the customized model $\\tilde{w_{k}^{t}}$, further mitigating the impact of data heterogeneity or attacks.\nIn our implementation, client *k* iteratively updates the personalized model $v_{k}^{t}$ and the customized model $\\tilde{w_{k}^{t}}$. When updating $v_{k}^{t}$ with Eq. 7, the parameters of $\\tilde{w_{k}^{t}}$ are frozen. Then, $\\tilde{w_{k}^{t+1}}$ is updated in the same mini-batch SGD: $\\tilde{w_{k}^{t+1}} = \\underset{w}{\\arg \\min} \\mathcal{L}_{k}(w)$, where *w* is initialized with $\\tilde{w_{k}^{t}}$."}, {"title": "C. Model Update Calibration and Detection", "content": "After client *k* finishes updating the customized model, it returns the model update $d_{k}^{t}$ to the server, where $d_{k}^{t} = w_{k}^{t} - \\tilde{w_{k}^{t}}$ (as shown in in Fig. 6). In FedCAP, since the server customizes a unique model for each participating client, the starting points of local model updating in each round differ among clients, making it challenging for the server to directly use the uploaded model updates $\\{d_{k}^{t} \\}_{k \\in S_{t}}$ to measure similarity relationships among clients during customized aggregation. To eliminate this inconsistency, we propose the model update calibration mechanism (as shown in in Fig. 6) that leverages the global model aggregated in each round (see Eq. 6 in Section V-A for details) as a common reference point to calibrate the model updates uploaded by the clients.\nRecovery. Before calibration, the server needs to recover the locally updated model $w_{k}^{t}$ of client *k* based on the customized model $\\tilde{w_{k}^{t}}$ and the uploaded model update $d_{k}^{t}$ (as shown in in Fig. 6), which can be formulated as $w_{k}^{t} = \\tilde{w_{k}^{t}} + d_{k}^{t}$.\nCalibration. The calibration process of the model update $d_{k}^{t}$ uploaded by client *k* can be represented as $\\bar{d_{k}^{t}} = w_{k}^{t} - w^{t}$. After calibration, the server can not only accurately capture the similarity relationships among benign clients, but it can also differentiate malicious model updates from benign ones in non-IID settings.\nSpecifically, we find that cosine similarity can only measure the directional differences of the uploaded model updates, which is why FLTrust [18] and ClusteredFL [22] struggle to resist attacks targeting the magnitude of model updates (e.g., MR attack) in non-IID settings, as shown in Section III-B2.\nDetection. For detection, the server calculates the Euclidean norm of the calibrated model update $\\bar{d_{k}^{t}}$ for client *k* (as shown in in Fig. 6). If the predetermined detection threshold $\\mathcal{T}_{norm}$ is exceeded, the server will recognize client *k* as a malicious client and remove it permanently."}, {"title": "D. Algorithm of FedCAP", "content": "Alg. 1 outlines the entire training process of FedCAP. When *t* = 0, the server initializes the customized models $\\{\\tilde{w_{k}^{0}} \\}_{k \\in S_{0}}$ with $w^{0}$ and distributes them to the corresponding participants $S_{0}$ (Line 9-12). Clients perform local updating to obtain updated local models $\\{w_{k}^{t} \\}_{k \\in S_{t}}$ and personalized models $\\{v_{k}^{t+1} \\}_{k \\in S_{t}}$ (Line 14). Subsequently, clients upload their model updates $\\{d_{k}^{t} \\}_{k \\in S_{t}}$ to the server (Line 15). The server recovers the updated models of clients, calibrates uploaded"}, {"title": "VI. EXPERIMENT EVALUATIONS", "content": "A. Experiment Setups\n1) Datasets: We use two image classification datasets, CIFAR-10 [38] and EMNIST [39], as well as a human activity recognition dataset, WISDM [41]. The CIFAR-10 dataset contains images from 10 classes. We adopt a pathological non-IID setting [9] that introduces label distribution skew. Specifically, we define 20 clients, each with a balanced number of samples but imbalanced classes (2 classes per client). The EMNIST dataset comprises images of 62 different digits and letters. Following previous studies [30], [43], we employ a non-IID setting to divide the data (commonly seen in cross-silo settings). Specifically, we define 100 clients and allocate data based on digits, lowercase letters, and uppercase letters, forming 3 distinct groups. Within each group, client data consists of 80% samples from dominant classes and 20% samples from all classes, leading to imbalanced sample distributions among the groups. The WISDM data is collected from 36"}, {"title": "B. Performance Comparisons", "content": "1) Comparing with SOTA FL Baselines: We use the CIFAR-10, EMNIST, and WISDM datasets to compare the performance of FedCAP with other SOTA baselines in the benign scenario, and we also explore their potential defense ability in attack scenarios (i.e., LF, SF, and MR). In Fig. 7, FedCAP outperforms all other baselines in all cases in terms of model accuracy. It exhibits an average accuracy gain of 2% to 23% over other baselines in the benign scenario and an average accuracy gain of 3% to 50% in attack scenarios.\n2) Comparing with Robust FL Methods: To evaluate the robustness of FedCAP and existing robust FL methods, we use the CIFAR-10, EMNIST, and WISDM datasets to compare their model performances under six attack scenarios."}, {"title": "C. Robustness of FedCAP", "content": "To further demonstrate the robustness of FedCAP, in addition to evaluating the impact of attacks on model performance (i.e., TAcc), we introduce three robustness metrics to measure the detection abilities of FedCAP. Detection Accuracy (DAcc) represents the proportion of clients that are correctly identified as either benign or malicious. False Positive Rate (FPR) (or False Negative Rate (FNR)) denotes the proportion of benign (or malicious) clients that are incorrectly regarded as malicious (or benign).\nIn particular, FedCAP fails to identify malicious clients under the LIE attack, with the FNR = 100 on CIFAR-10 and WISDM.\nIn contrast, FedCAP not only demonstrates strong robustness in all cases but also converges faster than other methods. This is due to its model update calibration and anomaly detection mechanisms, which enable the server to detect malicious clients and aggregate customized models based on the contributions (i.e., similarities) among benign clients. This allows client personalized models to reference more valuable information from the customized models."}, {"title": "D. Impact of Hyperparameters", "content": "1) Impact of Scale Factor \u03b1: To analyze the impact of $\\alpha$ on the model performance of FedCAP, we conduct experiments using three datasets in both benign and attack scenarios. In contrast, for the other two datasets, as $\\alpha$ increases, the model accuracy of FedCAP gradually improves. The reason is that there is a higher degree of label distribution skew among clients' data. In this case, choosing a larger value for $\\alpha$ allows the server to assign larger weights to clients with similar distributions, preventing interference from other dissimilar or malicious clients, and thus reducing the impact of data heterogeneity or attack."}, {"title": "4) Impact of the Proportion of Malicious Clients", "content": "To evaluate the impact of the proportion of malicious clients on the effectiveness of robust FL methods, we use the CIFAR-10 and EMNIST datasets to conduct experiments under four strong attacks (i.e., LIE, Min-Max, Min-Sum, and IPM). Fig. 10 shows that as the proportion of malicious clients increases,"}, {"title": "E. Ablation Study", "content": "To verify the necessity of each main component in FedCAP (i.e., customized aggregation, model update calibration, and personalized training) as introduced in Section V, we conduct ablation experiments using CIFAR-10 under four strong attacks and analyze the results in Table IV."}, {"title": "VII. CONCLUSION AND FUTURE WORK", "content": "In this paper, we proposed FedCAP, a robust FL framework against both data heterogeneity and Byzantine attack.\nRobustness Analysis. We believe that in strong attack scenarios [26]-[28], even if adversaries know the customized aggregation rule and the model updates of benign clients, it is difficult to design effective adaptive attacks.\nConvergence Analysis. In FedCAP, the server's customized aggregation and model update calibration mechanisms dy-"}, {"title": "A. Scalability", "content": "In read-world deployments, FedCAP not only customizes models to meet participants' personalized needs but is also scalable for future clients.For example, when a future client *i* requests the model customization service, the server sends the global model w-1 to client *i*. Upon receiving the model, client *i* conducts local updating and uploads the model update $d_{i}$ to the server. Afterward, the server customizes the model wi using the uploaded model update $d_{i}$, the calibrated update pool $\\{d_{k}^{t-1} \\}_{k \\in S_{t-1}}$, and the recovered model pool $\\{ \\tilde{w_{k}^{t-1}}\\}_{k \\in S_{t-1}}$ through Section V-A. Once client i receives the customized model $\\tilde{w_{i}}$, it can directly perform model inference or further fine-tuning."}, {"title": "B. Overhead", "content": "FedCAP proposes three modules: model customization, personalized training, and model update calibration.\n1) Computational Overhead: During customized aggregation, the server only needs to compute cosine similarity among model updates of *N* participating clients to determine the customized aggregation weights, where typically $N << K$ (the total number of clients).\n2) Communication Overhead: Compared to the client-side model customization method FedFomo, where the server needs to send multiple models to clients in each communication round, FedCAP only needs to send one customized model to each client, resulting in communication overhead equivalent to FedAvg.\n3) Storage Overhead: FedCAP only requires storing model pools and model update pools for the N participating clients."}, {"title": "C. Efficiency Analysis", "content": "To compare FedCAP's system efficiency with other baselines, we report their Round-to-Accuracy (R2Acc), breakdown of computation time on both server and client sides, and communication overhead in the benign scenario using CIFAR-10. The R2Acc represents the number of rounds required to achieve a target accuracy (i.e., 80% on CIFAR-10), which reflects the convergence speed of FL algorithms."}]}