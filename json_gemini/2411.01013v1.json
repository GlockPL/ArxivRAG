{"title": "A Similarity-Based Oversampling Method for Multi-label Imbalanced Text Data", "authors": ["Ismail Hakki Karaman", "Gulser Koksal", "Levent Eriskin", "Salih Salihoglu"], "abstract": "In real-world applications, as data availability increases, obtaining labeled data for machine learning (ML) projects remains challenging due to the high costs and intensive efforts required for data annotation. Many ML projects, particularly those focused on multi-label classification, also grapple with data imbalance issues, where certain classes may lack sufficient data to train effective classifiers. This study introduces and examines a novel oversampling method for multi-label text classification, designed to address performance challenges associated with data imbalance. The proposed method identifies potential new samples from unlabeled data by leveraging similarity measures between instances. By iteratively searching the unlabeled dataset, the method locates instances similar to those in underrepresented classes and evaluates their contribution to classifier performance enhancement. Instances that demonstrate performance improvement are then added to the labeled dataset. Experimental results indicate that the proposed approach effectively enhances classifier performance post-oversampling.", "sections": [{"title": "1 Introduction", "content": "Unstructured data, such as images, texts, videos, sensor readings, tweets, logs, audio, and emails, forms a significant portion of the data generated today. Text classification remains one of the most widely used tasks in Natural Language Processing (NLP). Assigning a given text such as sentences, paragraphs, and documents to predefined categories with the help of ML has countless advantages in different domains. Classifying documents into topics, classifying emails for the related departments, and analyzing positive and negative comments for a product are some examples of application areas of text classification that provide great convenience by reducing human effort and thus eliminating human errors. Although it has many different applications and the number of studies and projects increases day by day, there are also some challenges in text classification. Firstly, the structure of text data is complex, as it consists of semantic elements that require sophisticated techniques for computer interpretation. Secondly, acquiring labeled data, especially in specialized domains like law and medicine, is challenging. Labeling text data demands considerable cognitive effort and domain expertise. Therefore, solutions that streamline the labeling process would be highly beneficial.\nIn some classification tasks, data can have multiple labels simultaneously. For instance, a news article might be categorized under both the economy and foreign relations. Similarly, in an image labeling project featuring animals, a single picture may contain more than one animal. When instances belong to multiple classes, the task is referred to as multi-label classification. In this scenario, labels are represented as multi-dimensional vectors rather than single classes or categories. In real-world applications, some classes may have a large number of instances, while others may have insufficient data for training a classifier. When classification algorithms are trained on imbalanced or limited datasets, achieving satisfactory results for underrepresented classes becomes difficult. This issue is particularly pronounced for complex machine learning models, which often require substantial amounts of data to perform effectively. For text classification tasks, the complexity of the data typically necessitates the use of advanced models, underscoring the critical need for adequate data to create effective classification systems. This challenge, known as the data imbalance problem, is one of the most significant hurdles in classification tasks.\nThe class imbalance problem has received considerable attention in machine learning and pattern recognition due to the problems it poses . It is more important in such real-world applications where misclassifying a minority example costs a lot. Spam mail classification, diagnosis of rare diseases, and authentic document classification are the main examples where the cost of classifying rare instances is very high. Different solutions have been proposed in the literature to avoid the effect of imbalanced data in such critical problems. The trend for the published papers in the literature is presented in Figure 1. We can see that the number of papers has increased greatly in recent years.\nA primary challenge in achieving balanced datasets in real-world applications is the inherent difficulty of obtaining such data. This challenge is particularly pronounced in multi-label classification tasks, where class imbalance issues are often severe. A straightforward approach to addressing this issue is to acquire additional labeled data;"}, {"title": "2 Literature Review and Background", "content": "As an alternative to solve the data imbalance problem, the so-popular self-supervised methods are proposed to utilize unlabeled data at hand. In self-supervised methods, a classifier is trained with a small amount of labeled data and uses the trained classifier to predict unlabeled samples and label new samples. Thus, the labeled dataset is extended using the trained model. In the literature, some studies focused on this specific area and proposed several methods. However, the main drawback of these methods is relying on an initial classifier that might have a poor performance. If the performance of the model is satisfactory, then there is no need to improve the dataset. Conversely, if the initial results are not satisfactory, then it is not possible to trust this poorly performing classifier.\nThe pervasive issue of class imbalance in real-world datasets, particularly in multi-label settings, underscores the need for a robust and comprehensive solution. In multi-label classification, the imbalance problem is further complicated by the inherent nature of the task, where multiple labels can be assigned to a single instance. This overlap of classes precludes clear-cut boundaries between them, which poses significant challenges for traditional oversampling methods. Classical oversampling approaches typically focus on balancing classes individually by generating additional instances within each class. However, this approach is ineffective in multi-label classification settings, where distinct separation between classes does not exist. When an instance belongs to multiple classes simultaneously, using it as a reference in the oversampling process introduces ambiguity, as it is unclear whether the newly generated instances should belong to all associated classes or only a subset. Consequently, the lack of clear class boundaries in multi-label data prevents conventional methods from effectively addressing the imbalance issue, highlighting the need for innovative approaches tailored to the unique challenges of multi-label classification.\nTo solve this problem in multi-label imbalanced text data classification tasks, an auto-labeling algorithm that labels the unlabeled instances by utilizing the similarity between the instances is proposed. The proposed algorithm finds instances similar to the current ones in the data space and considers them as candidate instances. If candidate instances help improve the overall performance, they are added to the labeled set. The algorithm searches the unlabeled data space iteratively and finds the possible instances to extend the labeled set.\nThe paper is organized as follows: in Section 2, we provide a comprehensive review of the literature and relevant background. Section 3 outlines the proposed similarity-based oversampling method, including its motivations and methodological framework. Section 4 presents the experimental setup, detailing the datasets, evaluation metrics, and performance analyses. Finally, in Section 5, we discuss the findings, implications, and future research directions based on the results of our study."}, {"title": "2.1 Multi-label Text Classification", "content": "ML algorithms rely on numeric data, making the conversion of text to numeric is a key challenge in text classification. Techniques for this conversion range from simple methods like one-hot encoding to complex models such as transformers including Bag"}, {"title": "2.3 Similarity Functions for Text Data", "content": "Similarity measures are functions that are used to measure the matching score between two objects. The more similar a pair of objects, the higher the similarity score produced by the similarity measure and can be considered as the inverse of the distance function. In NLP, similarity measures are used to measure the similarity between documents, sentences, or words. It plays a crucial role in designing our algorithm, as it helps identify new instances. Choosing the appropriate similarity function is essential; it must accurately reflect the relationships between embedding vectors that represent text data in a high-dimensional vector space, capturing its semantics. The similarity of texts can be measured in two ways . First, lexical similarity analyzes the resemblance of the words and their sequence, and, 'apple' and 'apply' are considered as lexically similar. On the other hand, semantic similarity measures the similarity between meanings using knowledge-based or corpus-based models are used to measure semantic similarity. 'apple' and 'apply' are not semantically similar because they represent completely different things.\nThe most popular way to measure semantic similarity is length distances. Euclidean, cosine, Minkowski family, Jensen Shannon distance, and Hamming distances are the widely used length distance types . In , they studied text similarity measures and conducted a comparative analysis between them. They measure the similarity between research papers for a recommendation algorithm using different similarity functions and according to their experiments, cosine similarity yields the best results."}, {"title": "3 Proposed Method", "content": "In the previous chapter, we examined the need for auto-labeling methods to mitigate the challenges of imbalanced data with minimal manual effort. Real-world datasets often exhibit significant class imbalance and are insufficient for training machine learning models, which can lead to suboptimal performance, particularly in text classification tasks. Traditional approaches, such as manually labeling additional data, are often impractical due to the complexity of domain-specific requirements and the need for expert annotators. To address these issues, we propose a similarity-based oversampling method that identifies new instances from an unlabeled dataset to address data imbalance and insufficiency in multi-label text classification. This method iteratively searches the unlabeled dataset for similar instances, using a similarity measure based on word embeddings. By applying a class similarity threshold, the method identifies candidate labels for these instances and then evaluates whether incorporating them enhances the model's performance. Instances that contribute to performance improvements are then added to the labeled set. This approach represents a novel contribution to the literature, filling a gap in current methodologies and offering a foundational technique that future algorithms can build upon."}, {"title": "3.2 The Overall Algorithm", "content": "In our proposed method, as in the self-training approaches, we aim to oversample the dataset by using unlabeled instances by utilizing a similarity function between instances. In self-training approaches, an initial classifier is trained to predict unlabeled instance labels to add them to the labeled set. However, we cannot rely on the labels predicted by the initial classifier for some classes due to the lack of enough labeled instances for those classes in the training set. So, after converting text data"}, {"title": "3.3 The Oversampling Algorithm", "content": "The proposed oversampling algorithm given in Algorithm 2 aims at finding new instances from the unlabeled vector set by utilizing the similarity scores calculated with the labeled vector set. It searches the unlabeled set and finds instances that are close to the existing ones as depicted in Figure 3. These closely placed instances are candidate instances for labeling to extend the labeled set. An improvement checking procedure is used to check whether newly added candidates improve the overall performance or not. If the performance improves, candidates are added to the labeled set. Iteratively, the unlabeled set is searched to find new instances until several iterations are repeated. The pseudocode for our oversampling algorithm is presented below."}, {"title": "4 Experimental Results", "content": "In order to test our proposed algorithm, we conduct experiments and perform analyses using the OPP-115 dataset. The OPP-115 dataset is a collection of website privacy policies to analyze the data practices . Website privacy policies are plain texts formed by a group of paragraphs that cover some topics related to regulations, collection of user data, sharing, processing, data security, etc. Each paragraph is related to one or more topics which converts the problem into a multi-label, multiclass classification problem.\nIn Table 1, the summary statistics for the dataset are given. Cardinality is defined as the average number of labels per example and density is the average number of"}, {"title": "5 Conclusion and Future Research Directions", "content": "This study presents a similarity-based oversampling algorithm to address the pervasive class imbalance issue in multi-label text classification tasks. The algorithm seeks to expand the labeled dataset by identifying similar instances to the labeled instances within an unlabeled dataset, improving model performance without relying on synthetic data generation. Through a systematic, iterative process, the algorithm identifies instances that exhibit high similarity to the labeled instances, using a similarity measure such as Euclidean distance, cosine similarity, and Jensen-Shannon distance. In our experiments, the algorithm demonstrated good performance in locating high-quality, label-appropriate instances. The results showed that incorporating these instances into the labeled set led to substantial improvements in classifier performance, with a significant positive change in the F1 score. This approach addresses a critical gap in multi-label classification by offering a practical and scalable oversampling solution that leverages real instances, mirroring the benefits of human annotation.\nIn contrast to many self-learning or semi-supervised methods, which rely only on initial classifier predictions to label new instances, this algorithm considers the risk of inaccurate labeling by inspecting the contribution of new instances on performance. Therefore, it leans on similarity measures to find potential instances, which provides a robust, performance-based foundation for extending the labeled set. By focusing on real instances that align with the existing data distribution, this method helps enrich the dataset while maintaining the integrity of class relationships. Overall, the proposed algorithm contributes a novel perspective to the literature on oversampling in multi-label text classification, serving as a potential baseline for future studies.\nThere are several avenues for advancing this study. First of all, the choice of similarity function plays a crucial role in the accuracy of instance selection; thus, further research could focus on investigating the effect of similarity metrics on performance. Exploring alternative similarity functions that capture subtle contextual and semantic nuances within text data or designing a new similarity metric specifically tailored for multi-label text data could be one of the open doors to enhancing performance. Second, it would be valuable to test this method on diverse datasets across multiple domains to better understand the relationship between dataset characteristics and parameter settings. Such research could lead to recommendations on optimal parameter configurations for different types of datasets, thereby enhancing the algorithm's versatility and adaptability.\nIntegrating active learning with the similarity-based oversampling algorithm represents another promising direction. In cases where the algorithm has low confidence in assigning labels to certain instances, these instances could be flagged for review by domain experts. This selective labeling strategy would streamline the labeling process, ensuring high-quality labels while reducing the overall effort required by human annotators. Furthermore, the proposed method could be adapted for semi-supervised or self-supervised learning applications, where it would leverage minimal labeled data to support performance improvements in resource-limited environments.\nFinally, our study highlights the importance of choosing appropriate performance metrics in multi-label classification, particularly for imbalanced data. While classification metrics such as the F1 score provide valuable insights, ranking-based metrics designed for multi-label settings could offer additional perspectives, especially in scenarios where label ranking is critical. However, these metrics remain underexplored in the context of imbalanced multi-label data due to challenges in interpretability and sensitivity. Future research could investigate the efficacy of ranking-based metrics more closely, potentially developing interpretative frameworks or adaptations that make these metrics more applicable to imbalanced multi-label classification."}]}