{"title": "Generating Energy-efficient code with LLMS", "authors": ["Tom Cappendijk", "Pepijn de Reus", "Ana Oprescu"], "abstract": "Abstract-The increasing electricity demands of personal com- puters, communication networks, and data centers contribute to higher atmospheric greenhouse gas emissions, which in turn lead to global warming and climate change. Therefore the energy consumption of code must be minimized. Code can be generated by large language models. We look at the influence of prompt modification on the energy consumption of the code generated. We use three different Python code problems of varying difficulty levels. Prompt modification is done by adding the sentence \"Give me an energy-optimized solution for this problem\" or by using two Python coding best practices. The large language models used are CodeLlama-70b, CodeLlama-70b-Instruct, CodeLlama-70b- Python, DeepSeek-Coder-33b-base, and DeepSeek-Coder-33b- instruct. We find a decrease in energy consumption for a specific combination of prompt optimization, LLM, and Python code problem. However, no single optimization prompt consistently decreases energy consumption for the same LLM across the different Python code problems. Index Terms-energy efficient code, green software, green AI", "sections": [{"title": "I. INTRODUCTION", "content": "The increase in atmospheric greenhouse gasses (GHG) emission has led to global warming [1]. Zero emission of GHG is mandatory to stall climate change. While the European Union's Green Deal sets the zero-emission goal for 2050 [2], the electricity demand for IT infrastructure increases [3]. Re- ducing the energy consumption of computing systems reduces the emission of GHG, and can, amongst others, be achieved by optimizing the software used in these systems. This study focuses on software, in particular, code that is generated by Large Language Models (LLMs). The ability to generate correct code by a LLM is not positively correlated to the ability to generate efficient code [4]. So for a user, it is important to judge the LLM not solely the correctness of code but also on the code's energy consumption.\nLately, a growing number of studies examine the energy consumption and carbon emissions associated with large lan- guage models [5], [6]. Either investigating strategies to lower the energy consumption of these models during training [7], [8]. However, during usage, a large language model (LLM) re- quires computation as well. Alternatively, research focuses on the energy usage during inference computation and provides solutions to lower energy consumption [9], [10]. To position our research within the domain of energy consumption of LLMs, we do not focus on energy consumption during the training or inference phase of a LLM. Instead, we focus on the energy consumption of the code produced by LLMs.\nOur work relates to very recent research which assesses the sustainability awareness of LLMs by reviewing the code they produce [11], [12], and differs in that we focus on the ability of LLMs to generate code with a lower energy consumption. We conduct experiments on prompt modification. We deploy input to multiple LLMs and collect the statistics, including energy consumption, of the code generated by these LLMs.\nTo understand to what extent can prompts trigger a LLM to produce code that consumes less energy, we formulate the following two research questions:\n(1) Which prompt optimization leads to generating code that is consuming less energy for a given LLM?\n(2) To what extent does the code problem influence the impact of the prompt optimization on energy consumption?\nWe use a single programming language, Python, and three difficulty levels. Prompts are modified by adding the sentence \"Give me an energy-optimized solution for this problem\" or by using two Python coding best green practices. The LLMs used are CodeLlama-70b, CodeLlama-70b-Instruct, CodeLlama- 70b-Python, DeepSeek-Coder-33b-base, and DeepSeek-Coder- 33b-instruct. We find a decrease in energy consumption for a specific combination of prompt optimization, LLM, and Python code problem. However, no single optimization prompt consistently decreases energy consumption for the same LLM across the different Python code problems.\nScope of this paper We do not incorporate all the available LLMs. There is a continuous release of new models. Some of the LLMs are behind a paywall, such as ChatGPT-3, and fell out of our scope. We also did not set out to implement open-source LLMs. Our focus is on prompt modification, showing how prompts can trigger a LLM to produce code that consumes less energy. Additionally, fine-tuning the imple- mentation of a specific LLM can be complex due to numerous parameters and training options. Delving into such fine-tuning is not within the scope of this study.\nOutline In Section II we briefly visit LLM background. In Section III we describe our methodology. The experimental setup and results are described in Section IV, and discussed in Section VI. We conclude in Section VII."}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "A language model is a model that assigns probabilities to upcoming words or sequences of words [13]. A LLM is the product of a pretrained language model, pretraining enables the LLM to gain knowledge about the language and its context based on large amounts of text. After pretraining, a LLM is fine-tuned for specific tasks such as questioning and answering. The current standard architecture of LLMs is based on transformers [13]. Transformers use self-attention which helps a LLM to understand the context and relative relation of a word in a text.\nThis study [11] examines whether generative AI models, specifically OpenAI ChatGPT, GitHub Copilot, and Amazon CodeWhisperer, can generate code that meets sustainability objectives, focusing on metrics like runtime, memory usage, energy consumption, and floating-point operations (FLOPs). Results show that AI-generated code does improve sustain- ability metrics upon optimization requests but typically falls short of human-coded solutions from LeetCode. Our work uses the same metrics but extends to DeepsSeek-Coder and Code Llama. Next to that, we provide a framework to expand and test new LLMs.\nThis work [12] is similar to our setup and measures the energy efficiency of code generated by Code Llama. The paper shows that the performance is highly dependent on the coding language and problem. Generally speaking, human-generated code is more energy-efficient than Code Llama's output, even when prompted for energy-efficient code. Our work differs as we do not analyse 'regular' code outputs but only prompt to optimise for energy efficiency and we use DeepSeek-Coder next to Code Llama.\nAn Empirical Study on LLM-based Green Code Generation\nEnergy Efficiency of the Code Generated by Code Llama"}, {"title": "III. METHODOLOGY", "content": "Apart from energy consumption, to investigate the perfor- mance of the code generated by LLMs, we use the same metrics as [11]: runtime, peak memory usage, and the total number of floating-point operations. In this section we also describe the selection of LLMs, code prompts, and best practices to test prompt modification.\nTo measure the energy consumption and performance met- rics we rely on two tools: Perf and GNU's Not Unix (GNU) time command for resource usage measurement.\nPerf is not primarily an energy profiler, but it offers energy profiling functionalities. Perf supports a list of measurable events, that vary with each processor type and model [14]. Sources of the events are kernel counters, the processor, and the processor's Performance Monitoring Unit (PMU). Perf is capable of measuring energy-related events. On Intel-based systems, data of the energy events are provided with the interface Running Average Power Limit (RAPL) [15].\nWe use Perf because it works independent of programming languages as it measures the statistics per single command. Perf offers the capability to conduct repeated measurements of the same event and captures the output of the event together with the elapsed time. We are using the event power/energy-pkg/ to gather the energy consumption of a command. This Perf event captures the energy consumption and runtime of the whole CPU package and returns the energy data in Joules [16]. Next to that, we use Perf to measure the floating-point operations (flops) as well.\nTo measure the peak memory usage of each command, we use the GNU tool time. This tool runs a command and returns a summary of used system resources. This includes the peak memory usage of the command. We use this tool because it is both capable of measuring the peak memory consumption of a command and it is programming language independent.\nB. Selecting code problems\nC. Prompt modification\nPrevious work [11] modifies prompts to generate code that consumes less energy. Their method involves adding a sentence at the beginning of the code prompt, specifying that the solution must be energy-optimized. A LLM can provide an optimized solution if it understands how to enhance the code to consume less energy. This creates the expectation that the LLM understands the problem and knows how to improve the outcome. From a user perspective, this approach is straightforward and does not require any knowledge of code optimization.\nWe use this method as a baseline for testing prompt modifi- cations. In addition to this approach, we examine best prac- tices for writing energy-efficient Python code. [20] examines programs that solve the same problem with varying energy consumption using the same programming language. The study identifies techniques for writing more energy-efficient code. The study shows that in Python, the use of for loops is more efficient while loops. The study also shows that using libraries instead of writing the code yourself reduces energy consumption. We incorporate these findings into our prompt modifications. Each finding is tested separately by adding a sentence to the beginning of the code prompt. The sentences together with their labels are shown in Table III.\nWe examine the similarity between the generated code for non-optimized and optimized code prompts addressing the same problem using pycode-similar, a plagiarism detector for Python code. The tool compares the abstract syntax tree (AST) of the different code solutions [21], by normalizing the AST to remove unhelpful attributes, such as print statements, and then using difflib to determine the differences between the ASTs.\nA. Selecting Large Language Models\nD. Code similarities"}, {"title": "IV. EXPERIMENTAL SETUP AND RESULTS", "content": "The Python programs mentioned in this section are available on GitHub. The measurements are conducted on a laptop equipped with an 11th Gen Intel Core i5-1135G7, Ubuntu 22.04.4 LTS and kernel version 6.5.0-35-generic.\nTo minimize background tasks, we only use a terminal during energy measurements. We also prevent the system from downloading and installing updates by putting the device in airplane mode. Energy consumption and runtime are measured 50 times for each prompt result. The peak memory usage measurement is repeated 50 times to account for variations caused by cache effects and cold starts [22]. The total number of floating point operations is not influenced by the Python process, even when floating point operations are executed, thus this is tested and verified only once.\nThis is a manual step required because none of the LLM code outputs can be used directly due to one or more of the following issues, which results in incorrect Python code:\nOutput Modification.\nWe have one rule for adjusting the LLMs' code output: adjustments can only be made outside the scope of the class or method relevant to the code problem. This ensures that modifications do not affect the solution itself.\nLLMs are probabilistic models, so we examine their outputs across multiple runs and find no difference in code output across different runs (see Git)."}, {"title": "V. RESULTS", "content": "Each configuration consists of one LLM together with one prompt. The prompts are derived from three different code problems and four different prompt types (default + three optimization sentences), resulting in a total of sixty configurations (twelve prompts \u00d7 five LLMs). We apply the Mann-Whitney U test and compare configurations without optimization against those with optimization for the same code problem and LLM. This comparison is based on differences in energy consumption, runtime, the total number of floating- point operations, and peak memory usage.\nThe LLM generated code is executed fifty times. For every execution, we measure the energy consumption, resulting in a one-dimensional dataset with fifty values used in the Mann Whitney U test. We used the Mann-Whitney U test because it does not depend on assumptions of the underlying distributions of our dataset [23]. The null hypothesis of the Mann Whitney U test is that energy dataset x belongs to the same energy population as energy dataset y [23]. The alternative hypothesis is determined by the variant of the Mann Whitney U test. The two-sided Mann Whitney U test has an alternative hypothesis that energy dataset x does not belong to the same population as energy dataset y. The alternative hypothesis of the one- sided Mann Whitney is that energy dataset x is stochastically larger than energy dataset y.\nWe use the one-sided variant twice because we wish to say whether energy dataset x is stochastically larger than energy dataset y, or the opposite. The first application has the null hypothesis that energy dataset x belongs to the same energy population as energy dataset y. The alternative hypothesis is that energy dataset x is stochastically larger than energy dataset y. We test this with an alpha-value of 0.01 because we want to reduce the chance of falsely rejecting the null hypothesis. If we cannot reject the null hypothesis, the p- value is greater than the alpha-value, we cannot say that energy dataset x is stochastically greater than energy dataset y and test the opposite. We use the null hypothesis that energy dataset y is from the same population as energy dataset x. The alternative hypothesis is that energy dataset y is stochastically larger than energy dataset x. If the null hypothesis can also not be rejected in the second test, we say it is unknown which energy dataset is stochastically larger.\nTo get an overview of the energy consumption of the dif- ferent configurations across the code problems we calculated a score for every combination. A value of one is assigned if the configuration in the row consumes more energy than the configuration in the column and a value of minus one is assigned if the opposite holds. No value is assigned otherwise. Figure 1 shows the results, and Figure 2 shows occurrence of unknown comparison results.\nWe assess the difference between the base- and optimized prompt. The metrics used are similarity, energy, memory, flops, and runtime. We are not interested in the value itself, but rather in the difference between the base- and optimized prompt. The similarity tool computes this difference at code level. For the other metrics, we report the difference as a percentage of the base value:\n$\\frac{optimized - base}{base} \\times 100$\nBase Prompt Versus Optimized Prompt:"}, {"title": "VI. DISCUSSION", "content": "When analyzing the results of prompt modification, we find something unexpected. Figure 3 shows that the CodeLlama- 70b-Python model has no difference in code output for the Median of Two Sorted Arrays code problem. The same figure shows that there is a difference in the mean value of the energy datasets. This difference is 4.4%, 0.3% and 4.9% between the base prompt and the energy, library functions, and for loop optimization prompt respectively. We think that this difference in energy consumption is likely caused by the noise of background processes on the system we use.\nComparison for the same model and code problem in Figure 6 we find that the Mann Whitney U test assigns a plus (+) to the base prompt compared with the energy or the for loop optimization. The correct value for this comparison is unknown (Unk) because there should not be a significant difference in energy consumption between two similar code solutions. This example illustrates that we cannot draw con- clusions with the Mann Whitney U test about the energy consumption difference of two solutions.\nWhen comparing the mean value of the optimization prompt energy dataset against the base prompt energy dataset, we see some notable differences. We look at percentage differences larger than 15% and try to find their cause. The code solu- tion corresponding to the base prompt and the optimizations prompts for all LLMs are available in our Git repo.\nWe start with the results for the Sort list code problem, collected in Figure 3. We find that the for loop optimization prompt consumes -21.7%, -21.6%, and 59.7% less energy compared to the base prompt for the LLMs CodeLlama-70b, CodeLlama-70b-Instruct, and CodeLlama- 70b-python. The optimized solutions convert the linked list to an array and use a library function to sort the array. The resulting sorted array must be converted back to a linked list. For this CodeLlama-70b and CodeLlama-70b-Instruct use a for loop, the CodeLlama-70b-python uses a while loop. The library functions prompt optimization consumes -59.4% less energy compared to the base prompt for the CodeLlama-70b-python model. We find that the difference on a code level between the base on the optimization is the same as for the CodeLlama-70b-python LLM with the for loop optimization. The two DeepSeek-Coder LLMs are not able to improve. Their base code solution sorts the linked list in Python code. The optimized solutions for these two DeepSeek-Coder LLMs do not change from sorting in Python code to sorting with a library function.\nFor the Assign cookies code problem, we find that the DeepSeek-coder-33b-instruct LLM with the for loop prompt optimization consumes -26.8% less energy compared to the base prompt. On a code level we find that the base version updates a counter, and indexes with this counter in both input lists. The for loop optimized solution, does not loop through the sorted input lists. Instead, it uses the -1 index notation to get the last element of the list. The break condition of the loop is when both input lists are empty. This optimized solution does not contain a for loop. Instead, the difference between the base and the optimized code solution is within using the library function pop() and index method.\nFor the Median of Two Sorted Arrays code problem, we look at figure 3. For the DeepSeek-Coder-33b-base LLM, we find that the for loop optimization has an increase in energy consumption of 465.5% compared to the base prompt. We find that the base solution merges the two input lists with a + operation and sorts the result with a library function. The optimized solution is merging and sorting the two input arrays in native Python code and uses three while loops for this. The DeepSeek-Coder-33b-instruct LLM compared with the base prompt shows a decrease of -17.3% and -17.4% in energy consumption for the energy and for loop optimization prompts respectively. The code solutions for the energy and for loop optimization prompts are the same. There is a difference in the code output between the base and the optimized code solutions. We find that the base solution merges the two input lists. This input list is sorted and based on the length of this list, the median value is retrieved. For the for loop and energy optimization, the input arrays are not merged and sorted. This solution searches the middle point of the two input arrays, as if they were combined, and determines the median based on the values around this center. This code solution does not contain a for loop, instead containing a while loop.\nB. Total Number Of Floating Points\nWe measure the percentage difference in the total number of floating point operations between the base solution and an optimized solution. Across all three code problems, this percentage difference is zero. This means that we cannot use the total number of floating point operations to asses a difference between configurations.\nThe percentage difference in peak memory consumption be- tween the base solution and the optimized solution is between -0.5% and 1.3% with two exceptions. These exceptions are 24.1% and 24.2%, indicating that the base prompt consumes less peak memory consumption than the optimized prompt. The configurations that achieved these exception values are the LLMs CodeLlama-70b-Instruct and CodeLlama-70b with the for loop optimization prompt for the Sort List code problem. Because of the minimal difference, we cannot use peak memory consumption to asses a difference between configurations.\nD. Answering the Research Questions\nAnswering RQ (1), we find that the for loop optimiza- tion prompt most frequently triggers the LLMs to generate solutions that consume less energy compared to the base solution. These solutions do not have to include a for loop, they may perform better due to the use of library functions. However, the for loop optimization does not consistently show a decrease in energy consumption across all LLMs or for the same LLM across all code problems. There is also a case where the for loop optimization led to a 465.5% increase in energy consumption. Therefore, we cannot conclude as to whether an optimization prompt will consistently result in decreased energy consumption across all LLMs and code problems. This means that we cannot determine which prompt optimization consumes less energy for a given LLM.\nFor RQ (2), the Sort List code problem most frequently shows a decrease in energy consumption of more than 15% with prompt optimization. However, this code problem does not show this decrease across all three optimization prompts.\nThe answer to the main research question is that a specific combination of prompt optimization, LLM and code problem can lead to a decrease in energy consumption. However, no single optimization prompt consistently decreases energy consumption for the same LLM across the different code problems.\nWe found that the similarity is 100% across all runs, with one exception. We investigated by checking the parameters of the LLMs. The Deepseek-coder LLMs are implemented via HuggingFace, which has a do_sample parameter which is by default false, we did not adjust this value, meaning that the LLM uses greedy decoding [24]. This decoding strategy picks the token with the highest probability as the next token and the result is a deterministic output [25]. The CodeLlama LLMs use a parameter called temperature. This parameter regulates the degree of randomness in the output of a LLM by adjusting the probability distribution of tokens [26]. A temperature value greater than 1 reduces the likelihood of highly probable tokens and enhances the likelihood of less probable tokens. A temperature value of less than 1 increases the likelihood of highly probable tokens while decreasing the likelihood of less probable tokens. The temperature value ranges between 0 and 2, with zero representing a greedy decoding strategy. The default temperature value for the CodeLlama LLMs is set at 0.2. We did not adjust this value, which explains how the output of the CodeLlama LLMs is the same across the 10 different runs.\nA. Energy Consumption\nC. Peak Memory Consumption\nE. LLM Code Output Similarity"}, {"title": "VII. CONCLUSIONS", "content": "The objective of this study is to determine which LLM generates code with lower energy consumption. Using this framework, we conducted experiments to examine the impact of prompt modification on the energy consumption of code generated by LLMs.\nThe answer to the main research question To what ex- tent can prompts trigger a LLM to produce code that consumes less energy? is quite nuanced. We found that the for loop prompt optimization often results in decreased energy consumption when compared to the base solution. However, the instructions provided by the for loop prompt optimization are not always implemented correctly. Some solutions outperform others due to the replacement of native Python code with more efficient library functions. There is also an example where the for loop optimization prompt led to a 465.5% increase in energy consumption compared to the base solution. The for loop optimization also does not consistently reduce energy consumption across all LLMs.\nF. Limitations\nOur work uses prompts two LLMs on coding problems to analyse the effect on energy consumption. Our AI-generated code did not consistently outperform the human-generated code, as seen in related work as well [12]. Our setup is naive, with one prompt and no hyperparameter tuning. A limitation is that we cannot rule out that different settings would lead to the same results. Next to that, we use a laptop for our measurements. A different hardware setup could lead to different conclusions, e.g. we might see a clear deviation if we use a GPU cluster instead.\nG. Future Work\nRD1: Hardware energy measurement to measure more accu- rately. We also want to test the energy consumption of code on other hardware systems to find out if these have less noise, i.e. microcontrollers or embedded systems.\nRD2: Expand scope to include LLMs that are now unavailable such as ChatGPT. Next to that, extend the scope with more programming languages such as C, Java, PHP etc."}]}