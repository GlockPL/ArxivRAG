{"title": "Refinements on the Complementary PDB Construction Mechanism", "authors": ["Yufeng Zou"], "abstract": "Pattern database (PDB) is one of the most popular automated heuristic generation techniques. A PDB maps states in a planning task to abstract states by considering a subset of variables and stores their optimal costs to the abstract goal in a look up table. As the result of the progress made on symbolic search over recent years, symbolic-PDB-based planners achieved impressive results in the International Planning Competition (IPC) 2018. Among them, Complementary 1 (CPC1) tied as the second best planners and the best non-portfolio planners in the cost optimal track, only 2 tasks behind the winner. It uses a combination of different pattern generation algorithms to construct PDBs that are complementary to existing ones. As shown in the post contest experiments, there is room for improvement. In this paper, we would like to present our work on refining the PDB construction mechanism of CPC1. By testing on IPC 2018 benchmarks, the results show that a significant improvement is made on our modified planner over the original version.", "sections": [{"title": "Introduction", "content": "Over past few decades, there has been research on various automated heuristic generation techniques, including abstractions (e.g. PDBs), delete relaxations (e.g. hmax) and landmarks (e.g. LM-cut) (Helmert and Domshlak 2009). Pattern database (PDB) is a memory-based heuristic generation technique that requires a pre-processing phase. It maps original states to abstract states by considering a subset of variables, namely a pattern, and stores their optimal costs to the abstract goal in a look up table. Early experiments on PDB heuristic search involve optimally solving the sliding-tile puzzles (Culberson and Schaeffer 1998) and the Rubik's cube (Korf 1997). The automated PDB generation process dates back to the work of Edelkamp (2001). Studies have shown that the heuristic could be improved by combining multiple PDBs. Felner, Korf, and Hanan (2004) show that the heuristic from the addition of multiple disjoint PDBs is admissible. On top of that, Holte et al. (2006) suggest taking the maximum heuristic over different additive PDBs. In addition, different representations for PDBs have been proposed. Explicit PDBs are structured as hash tables and usually limited by memory, while symbolic PDBs (Edelkamp 2002) are structured as binary decision diagrams (BDDs) (Bryant 1986). The succinct representation of state sets in BDDs usually reduces the memory consumption and enables the construction of much larger PDBs.\nThe space of possible abstractions is huge due to exponentially many patterns and various cost-partitioning strategies. In order to construct PDBs with good quality, several algorithms have been proposed, including genetic algorithm (Edelkamp 2006), bin-packing (Edelkamp 2006; Franco et al. 2017) and hill climbing (Haslum et al. 2007; Kissmann and Edelkamp 2011). Also, some PDB evaluation approaches have been suggested, such as average heuristic value (Edelkamp 2006), random walk sampling (Haslum et al. 2007) and stratified sampling (Lelis et al. 2016; Barley, Franco, and Riddle 2014). The basic idea of the Complementary PDB construction (CPC) mechanism is to keep generating PDB collections using a combination of the above-mentioned algorithms and add a new collection $\\mathcal{P}_{sel}$ to the collection set $\\mathcal{S}_{cur}$ if $\\mathcal{S}_{cur} \\cup \\{\\mathcal{P}_{sel}\\}$ is predicted to improve the heuristic. Moreover, the parameters of the PDB construction algorithms are dynamically adjusted in the course.\nThe results of IPC 2018 have confirmed the effectiveness the CPC mechanism. Both Complementary 1 (CPC1) and Complementary 2 (CPC2) planners solved 124 planning tasks and tied as the runner ups, only 2 tasks behind the winner, and they were the best non-portfolio planners. Nonetheless, some ill behaviors of CPC1 have been revealed in the post contest experiments (Franco et al. 2018). For example, using a single PDB construction algorithm, GAMER-Style (Kissmann and Edelkamp 2011), can solve substantially more tasks than using a combination of algorithms, which indicates that these algorithms are not well integrated. Hence, the purpose of our research is to refine the PDB construction mechanism of CPC1 by optimizing the algorithms and the evaluator as well as better integrating all components.\nIn light of this, we would like to present our refined planner, CPC0. For the rest of the paper, the definitions of related concepts will be introduced. Then, our refinements on the pattern construction algorithms, the evaluator and the over-"}, {"title": "Background", "content": "Planning tasks are encoded into the SAS+ model on the Fast Downward planning system (Helmert 2006). An SAS+ planning task is defined by a 4-tuple $\\Pi=(V, O, s_{0}, S^{*})$. $V$ is a finite set of state variables, where each variable $v \\in V$ has as finite domain $D_{v}$. A full state is an assignment to all state variables, while a partial state is an assignment to a subset of state variables. $O$ is a finite set of operators, where each operator $o \\in O$ is defined by a tuple $(pre_{o}, eff_{o}, cost_{o})$, specifying the preconditions, the effects and the non-negative cost. Both $pre_{o}$ and $eff_{o}$ are partial states. An operator $o$ is applicable to a state $s$ if and only if $s$ agrees on the values of all variables in $pre_{o}$. The result of applying $o$ to $s$ is a new state $s'$ that agrees on the values of all variables in $eff_{o}$ with the other variables unchanged. $s_{0}$ is the initial state and $S^{*}$ is the partial goal state. A solution to a planning task is a defined sequence of operators $(o_{1},...,o_{n})$ that leads from $s_{0}$ to a goal state that satisfies $s^{*}$, i.e. $o_{n}(...o_{1}(s_{0})...)[v]=s^{*}[V]$ for all $v \\in Var(s^{*})$. The task of cost optimal planning is to find a solution with the least cost.\nThe causal graph of an SAS+ planning task is a directed graph $(V,E)$ with $V=V$ and $(u,v) \\in E$ if and only if there is an operator $o \\in O$ such that $eff_{o}(v)$ is defined and either $pre_{o}(u)$ or $eff_{o}(u)$ is defined, $u \\neq v$. An arc $(u, v)$ in the causal graph implies that the change of $v$ is dependent on the current assignment of $u$."}, {"title": "SAS+ Planning", "content": "Pattern Databases\nA pattern database (PDB) is a look up table of abstract states that stores their optimal costs to the goal. It abstracts the problem space by mapping it to a subset of state variables $P \\subseteq V$, i.e. a pattern, and removing from the preconditions and effects of operators, $s_{0}$ and $s^{*}$ any variable not in $P$. This kind of abstraction is said to be homomorphic as any transition $(s, s')$ in the original problem space remains valid, thereby yielding admissible and consistent heuristic. A PDB is constructed by a blind regression search from the abstract goal states (Culberson and Schaeffer 1998; Edelkamp 2001), usually up to a time and memory limit. If the limit is reached and the search is stopped at the depth $d$, a partial PDB will be formed and a heuristic value of $d$ plus the minimum operator cost will be returned for any un-visited abstract state (Anderson, Holte, and Schaeffer 2007). The size of a PDB is defined as $\\prod_{v \\in P}|D_{v}|$, which is the cross product of the domain size of all variables in $P$.\nA combination of multiple PDBs generally produces better heuristic, e.g. through addition (Felner, Korf, and Hanan 2004) and maximization (Holte et al. 2006). Multiple PDBs are additive if any operator $o \\in O$ affects at most only one PDB, where $o$ affects a PDB $P$ if $Var(eff_{o}) \\cap P \\neq \\emptyset$. Given a PDB collection, the additivity can be ensured with cost-partitioning, which splits the operator costs among the PDBs. A simple approach adopted in the Complementary planners, zero-one cost-partitioning, is to set the cost of an operator $o$ to zero if it has affected any PDB in the collection, and keep the original cost otherwise (Edelkamp 2001). There are alternatives that could yield more informative heuristic, such as saturated cost-partitioning (Seipp et al. 2017), but they are usually more computationally expensive. All the PDB collections are combined with the canonical function (Haslum et al. 2007) in the Complementary planners, which takes the maximum heuristic value over all admissible combinations."}, {"title": "Pattern Databases"}, {"title": "Symbolic Search and PDBS", "content": "Symbolic search has gained tremendous popularity in cost optimal planning because of significant memory saving and faster computation. A state vector is a binary encoding of the variables and its length is given by $\\Sigma_{v \\in V}[log |D_{v}|]$. A characteristic function is used to represent a set of states that returns true if and only if an encoded state is in the set. In symbolic search, any characteristic function is succinctly represented in a binary decision diagram (BBD) (Bryant 1986). In addition, any operator can be represented as a set of transition relations $T_{c}(s, s')$ in a BBD, where $c$ is the operator cost, $s$ is the predecessor state and $s'$ is the successor state. The set of successors or predecessors for a state set can be generated via the image or the preimage operation, which takes the relational product of the state set and the transition relations (Torralba et al. 2017). The size of a BBD depends on the ordering of the variables. A local search algorithm has been proposed by Kissmann and Edelkamp (2011) that places related variables in the causal graph as close as possible, which performs much better than random ordering.\nSymbolic A* search (BBDA*) partitions the open list into a matrix of g and h value where each bucket is represented by a BBD (Edelkamp and Reffel 1998; Jensen, Bryant, and Veloso 2002). It expands the buckets along an f diagonal each time, starting from the lowest g value, and adds the successor state sets, to avoid duplicate expansion. Note that in the presence of zero-cost operators, additional layers in a bucket are needed to keep track of the blind search using only those operators. Symbolic PDBs (Edelkamp 2002) can be constructed in a way similar to explicit PDBs. A regression search is made from the set of abstract goals, and the sets of expanded abstract states are encoded in BBDs along with their costs to goal. It is convenient to query on a heuristic value for a state set by doing a conjunction with the BBDs, producing a subset of states with the heuristic value."}, {"title": "The Complementary PDB Construction Mechanism", "content": "GAMER-Style (Kissmann and Edelkamp 2011) is a hill climbing algorithm for constructing a single large PDB. The sketch is given in Algorithm 1. It starts with all goal variables and sequentially adds a set of causally related variables that are predicted to improve the heuristic. If the set of"}, {"title": "GAMER-Style"}, {"title": "Bin-Packing", "content": "Next-Fit Bin-packing (Moraru et al. 2019) consists of two algorithms, Next-Fit Decreasing Bin-Packing (NFD) and Next-Fit Increasing Bin-Packing (NFI), which differ in the order of candidate variables O. The sketch is given in Algorithm 2. It has been shown empirically that Next-Fit Bin-Packing is effective for seeding (Franco et al. 2018). In our implementation, an additional step of shuffling the causally related variables is added to inject randomness (line 11).\nCausal Dependency Bin-Packing (CBP) (Franco et al. 2017), as sketched in Algorithm 3, is used in the main PDB construction phase. Each pattern contains N randomly selected goal variables and iteratively adds causally related variables. The PDB collection is sorted in the descending order with respect to the pattern length before being returned (line 14). Compared to Next-Fit Bin-Packing, CBP is more random because candidate variable are not sorted and N can be adjusted. Both bin-packing algorithms are implemented with more efficient data structures in CPC0."}, {"title": "Evaluator", "content": "The evaluator plays a crucial role in the PDB construction process as it decides which PDB collections to be included in the set $S_{cur}$. The quality of a PDB collection is usually evaluated with respect to the reduction on the search tree size or the search time. Since the search tree size is negatively related to the average heuristic value as conjectured by Korf (1997), the heuristic value is used as the metric for evaluators such as average h (Edelkamp 2006) and random walk sampling (Haslum et al. 2007). A stratified-sampling-based evaluator (Lelis et al. 2016; Barley, Franco, and Riddle 2014) that takes the predicted search time as the metric is employed in CPC2 (Franco, Lelis, and Barley 2018), whereas random walk sampling evaluator is used in CPC1. As mentioned in (Franco et al. 2017), both evaluators yield similar results on symbolic PDBs. An explanation is that since a symbolic PDB collection generally contains fewer PDBs than an explicit one, the look up time of a symbolic PDB collection does not vary that much, resulting in a stronger correlation between the search tree size and the search time. For faster evaluation, random walk sampling evaluator is also adopted in CPC0 with some modifications and extensions.\nThe random walk sampling evaluator samples each state by applying a number of uniformly chosen operators according to the heuristic value of the initial state. After each step, if the state is known to be a dead, the random walk returns to the initial state. In our evaluator, the sampling stops when either the 10,000 state size limit or the 30s time limit is reached, and unique states are taken from the sample. In case the sample is empty when all sampled states are dead ends, the initial state is always included. Then, the heuristic value of $S_{cur}$ is stored for each sample state. To evaluate a new PDB collection $\\mathcal{P}_{sel}$, the heuristic value of $\\mathcal{P}_{sel}$ is compared with with the stored value on each sample state, and true is returned if and only if at least 25% states are improved. The evaluation criteria can be summarized by\n$\\frac{\\sum_{i=1}^{m}I(h_{P_{sel}}(s_{i})>h_{S_{cur}}(s_{i}))}{m} \\geq 0.25$\nwhere m is the sample size and $\\{s_{1},...,s_{m}\\}$ is the set of sample states. Compared to the hard threshold in the original evaluator, the ratio threshold is more accurate and flexible for varying sample size. After each evaluation, new dead ends are removed from sample state, and the heuristic value of each sample state is updated if $\\mathcal{P}_{sel}$ is included. Resampling will be performed if and only if the heuristic value of the initial state increases by over 10% to save time.\nThe time overhead in maximizing over a set of PDB collections has been addressed by researchers (Holte et al. 2004; Barley, Franco, and Riddle 2014). To reduce the size of $S_{cur}$ while maintaining the quality, the pruning of dominated collections is introduced. The pruning progresses backwards from the last PDB collection and stores the maximum heuristic value of processed PDB collections for each sample state. A PDB collection is pruned if and only if there is no state where its heuristic value exceeds that of any previous collection. The pruning order is determined as such because earlier added PDB collections are more likely to be dominated by later ones. This strategy is safe since it is quite rare for a useful PDB collection to underperform on every sample state."}, {"title": "Adaptive PDB Construction", "content": "The CPC process, as illustrated in Figure 1, starts with NFD and then NFI. Both of them run for 80s excluding the resampling time and start with a size limit of 108 that is scaled by 10 after each iteration. Whenever a new PDB collection is added, resampling will be performed if the initial h rises by over 10%, and dominated PDB collections will be pruned. The pruning is reasonable since the PDB collections formed in this phase are homogeneous and the time overhead for subsequent evaluations will be reduced. The largest size limit where a PDB is added is recorded as Si for later use. In CPC1, the perimeter PDB is constructed prior to the bin packing. However, since the perimeter PDB usually takes too much time and memory to be useful and may disadvantage other algorithms, it is not adopted in CPC0.\nAfter the seeding phase, the PDB construction process goes on until the time or the memory limit is reached. At each iteration, a choice is made between CBP and GAMER-Style using the UCB1 formula, which is a learning policy that balances exploration versus exploitation in a multi-"}, {"title": "Overall Results", "content": "The key statistics from our experiments are summarized in Table 1, Table 2 and Figure 2. The initial h values are taken from the tasks where they are available in both planners. The expansions, the search time, the total time and the memory usage are taken from the tasks solved by both planners. It can be seen from the tables and Figure 2(a) that CPCO usually produces PDBs with higher initial h value. This indicates that the refined algorithms and evaluator can generate better heuristic. While Table 2 shows that the number of tasks where less nodes are expanded in CPC0 is almost the same as the number of tasks where more nodes are expanded, Table 1 shows that CPCO has slightly fewer node expansions on average. Looking at Figure 2(b), the instances on CPC1 side lie further off the diagonal than those on CPO side. Moreover, CPC1 tends to expand more nodes for the tasks requiring a great number of expansions. It is because the refined process in CPCO is able to construct PDBs of consistent quality over tasks of varying difficulty. According to Table 2 and Figure 2(c), the search time of CPCO is generally less than that of CPC1, and the average search time of CPCO is significantly reduced, as per Table 1. This may attribute to the pruning of dominated PDBs that reduces the look up time. A combination of better heuristic and faster heuristic look up enables CPCO to solve 12 more tasks. Hence, our refinements make the planner even more competitive in the context of IPC 2018.\nOn the other hand, the total time and the memory usage of CPCO are increased on average as per Table 1. It is because"}, {"title": "GAMER-Style Comparison", "content": "In CPCO, the variable selection method of GAMER-Style is changed from average h value to random walk sampling. To evaluate the effect, both versions are tested on Nurikabe and Snake, where GAMER-Style contributes the most. In our experiments, other PDB construction algorithms are disabled, and both planners are set to the time limit at 900s and the memory limit at 4GB for PDB construction and are tested under the IPC 2018 rule.\nIt can be seen from Table 3 that the new version outperforms the original version in Snake. Despite a slight increase in the average memory usage, there is a significant improvement in the other statistics, especially a huge reduction in the average node expansions. The difference is not that obvious in Nurikabe. Although the new version has slightly more node expansions on average, mainly due to the task 14, it has fewer expansions on 3 more tasks. Also, the average initial h value of the new version is higher, and the initial h value is higher on 5 more tasks. Thus, the new version is more likely to outperform the original version in Nurikabe. In both domains, the average search time of the new version is significantly reduced, due to not only better heuristic, but also the pruning method. Since the PDBs constructed by GAMER-Style are homogeneous, it is very beneficial to prune the dominated ones."}, {"title": "Related Work", "content": "GA-PDB (Edelkamp 2006) uses the the bin-packing algorithm and the genetic algorithm to construct PDB collections, which are evaluated by average heuristic value. iPDB (Haslum et al. 2007) starts with a collection of PDBs each containing a goal variable and in each iteration constructs a new PDB by adding a causally related variable to an existing PDB. The PDBs are evaluated by random walk sampling and combined with the canonical function. GAMER (Kissmann and Edelkamp 2011) performs symbolic A* search with the symbolic PDB constructed by GAMER-style algorithm. It has an alternative option to perform symbolic bidirectional blind search, depending on the property of the planning task. CPC2 (Franco, Lelis, and Barley 2018) is an early version of the Complementary planers. The difference is that CPC2 does not perform Next-Fit Bin-Packing or GAMER-Style but uses CBP and Regular Bin-Packing (RBP) for PDB construction and performs random mutations to selected PDB collections. The PDB size limits are drawn from the binomial distribution with the parameters dynamically adjusted. A stratified-sampling-based evaluator is used, which evaluates the PDB with respect to the predicted search time and adds a PDB collection if the search time is predicted to be reduced. The pruning of dominated PDB collections is performed after every certain time period."}, {"title": "Conclusions", "content": "In this paper, we have presented our refinements on the complementary PDB construction mechanism, including GAMER-Style, Next-Fit Bin-Packing, Causal Dependency Bin-Packing, as well as the random walk sampling evaluator. We have optimised the algorithms, extended the evaluator and improved the overall process. Our experiments show that these algorithms has been well integrated and the modified planner CPCO has a significantly higher coverage on IPC 2018 benchmarks than the original planner CPC1. Therefore, the effectiveness of the CPC mechanism in cost optimal planning is again confirmed and a competitive benchmark planner is contributed.\nFor future research, some other PDB construction algorithms and cost-partitioning strategies, e.g. saturated cost-partitioning (Seipp et al. 2017), could be introduced. Also, more advanced evaluators such as stratified sampling (Lelis et al. 2016; Barley, Franco, and Riddle 2014) may improve the PDB selection process. A major drawback of PDB-based planners is the need of pre-processing phase, which increases the total time for solving a task. Techniques like interleaved search and heuristic improvement (Franco and Torralba 2019) could be further explored to address this issue."}]}