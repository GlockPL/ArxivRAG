{"title": "People over trust AI-generated medical responses\nand view them to be as valid as doctors, despite low\naccuracy", "authors": ["Shruthi Shekar", "Pat Pataranutaporn", "Chethan Sarabu", "Guillermo A. Cecchi", "Pattie Maes"], "abstract": "This paper presents a comprehensive analysis of how Al-generated medical responses are perceived and evaluated by\nnon-experts. A total of 300 participants gave evaluations for medical responses that were either written by a medical doctor\non an online healthcare platform, or generated by a large language model and labeled by physicians as having high or low\naccuracy. Results showed that participants could not effectively distinguish between Al-generated and Doctors' responses\nand demonstrated a preference for Al-generated responses, rating High Accuracy Al-generated responses as significantly\nmore valid, trustworthy, and complete/satisfactory. Low Accuracy Al-generated responses on average performed very similar\nto Doctors' responses, if not more. Participants not only found these low-accuracy Al-generated responses to be valid,\ntrustworthy, and complete/satisfactory but also indicated a high tendency to follow the potentially harmful medical advice\nand incorrectly seek unnecessary medical attention as a result of the response provided. This problematic reaction was\ncomparable if not more to the reaction they displayed towards doctors' responses. This increased trust placed on inaccurate or\ninappropriate Al-generated medical advice can lead to misdiagnosis and harmful consequences for individuals seeking help.\nFurther, participants were more trusting of High Accuracy Al-generated responses when told they were given by a doctor and\nexperts rated Al-generated responses significantly higher when the source of the response was unknown. Both experts and\nnon-experts exhibited bias, finding Al-generated responses to be more thorough and accurate than Doctors' responses but still\nvaluing the involvement of a Doctor in the delivery of their medical advice. Ultimately, ensuring Al systems are implemented in\ncollaboration with medical professionals should be the future direction of using Al for the delivery of medical advice in order to\nprevent the liability of misinformation while reaping the benefits of such cutting-edge technology.", "sections": [{"title": "Main", "content": "The use of artificial intelligence (AI) in medicine and healthcare has increased in various domains and applications in recent\nyears, from radiology imaging19, to mental health chatbots14 and drug discovery49. The COVID-19 pandemic has further\nreinforced people's comfort in seeking medical information online with more accessible means of receiving on-demand medical\ninformation33,50,53. With the rapid advancement of generative AI, including large language models (LLMs) such as GPT(s),\nGemini, Lamda, LLama, Alpaca, and more7,8,13,36,43,46,47 with capabilities of language generation and question answering in\nvarious domains, there is a growing interest in using LLMs for medical applications. Researchers have explored the use of\nLLMs for automating and supporting medical tasks, including diagnosis and triage16,28,31 providing treatment information11,\ntreatment prescription37, assisting surgery3,12,15, analyzing laboratory results10, medical report generation2,52, medical text\nde-identification29, medical education17,25,35,39, and more. Recently, Microsoft and Epic have explored the use of LLMs to\nhelp healthcare providers increase productivity with less administrative burden while shifting their focus to patient care. UC\nSan Diego Health, UW Health in Madison, Wisconsin, and Stanford Health Care were among the first organizations starting to\ndeploy technology to respond to healthcare messages automatically26,48.\nStudies have shown notable performances of LLMs on passing medical exams4, 18, 34, 40, 42, 44. For instance, a study has shown\nthat GPT-4 exceeds the passing score of the official practice materials for the United States Medical Licensing Examination\n(USMLE) by over 20 points34. Further, a study identified that ChatGPT was able to generate higher quality and more empathetic\nresponses to patient questions on an online forum23. The study, however, solely focused on the expert evaluator's viewpoint,\nneglecting the significance of the public's perspective, a key stakeholder of the healthcare system who should be involved in\nevaluating and shaping healthcare technology.23."}, {"title": null, "content": "Although LLMs have shown promising results and have the potential to significantly benefit the healthcare and medical\nfields27,32,38, their stochastic nature makes it challenging to determine when they would give factually correct answers or may\nconfidently provide false information, also known as hallucination or confabulation. In the context of medicine, the stakes\nare much higher; incorrect information can put lives at risk. For instance, a study on the use of LLMs to select next-step\nantidepressant treatment in major depression has shown that while the model appeared to identify and apply a number of\nheuristics commonly applied in psychopharmacologic clinical practice, the model's inclusion of less optimal recommendations\nposes a significant risk if used routinely to guide psychopharmacologic treatments without expert supervision37.\nAs LLMs become more prevalent in mainstream search engines and conversational interfaces, with an increasing participant\nbase, it is not always feasible to have expert supervision. Thus, it requires the designers of these systems to develop more\nthoughtful guardrails, especially with use in a sensitive domain like medicine. Simply focusing on the accuracy of LLMs in\nanswering medical questions is insufficient, as this fails to capture the broader implications of the technology on the healthcare\nsystem and society at large21,27,45. We argue that it is critical to study how the lay public perceives, evaluates, and is affected by\nAI-generated responses, especially when they are incorrect. As LLMs increasingly become a part of everyday life, non-experts\nwill encounter situations where they might trust and follow AI-generated advice, particularly in the absence of immediate\nmedical professional guidance. Over-relying on false or incomplete responses generated by AI could lead to delayed or\ninappropriate treatment, potentially worsening health outcomes and even endangering lives.\nIn this paper, we investigate:\n1. How well can participants distinguish between doctor-provided responses and responses generated by AI?\n2. How do participants the validity, trustworthiness, satisfaction, and other aspects of the AI-generated responses compared\nto doctors' responses?\n3. How does the participants' knowledge of the source of the medical response (whether it is a Doctor's response or an\nAI-generated response) influence their perception, either in favor of or against the received response?"}, {"title": "Results", "content": "One hundred fifty anonymous medical questions across 6 medical domains and their respective Doctors' responses were\nretrieved from HealthTap, an online healthcare platform. A large language model was used to produce AI-generated responses\nfor each inquiry. These AI-generated responses were subsequently evaluated by 4 practicing physicians from Stanford and\nUCSF, establishing the ground truth on whether the AI-generated response was accurate. Each evaluator provided an accuracy\nrating of Yes, Maybe, or No. The four expert evaluations were consolidated to form a compiled accuracy score for each\nAI-generated response. Any response with two or fewer \"Maybe\u201d evaluations and no \"No\" evaluation was considered High\nAccuracy. Any AI-generated response with a majority of evaluations being \"Maybe\" or worse was considered Low Accuracy;\nsee figure 1 for the full study method. In the dataset of 150 AI-generated medical responses, 56.0% were found to be of High\nAccuracy and 44.0% of Low Accuracy; see figure 3.\nWe used the experts' ratings to select a subset of one hundred fifty medical question-response pairs and created a new\ndataset consisting of 30 distinct pairs of medical questions and Doctors' responses, 30 distinct pairs of medical questions and\nHigh Accuracy AI-generated responses, and 30 distinct pairs of medical questions and Low Accuracy AI-generated responses.\nThis organized dataset was utilized to conduct an array of studies to understand participant perception of the AI-generated\nresponses in comparison to Doctors' responses. 100 online participants were recruited through a digital platform that attempts\nto find a population that is more representative of the baseline general public (the demographic of the participants is reported in\nthe supplementary materials section). More details regarding the experiment design, evaluation metrics, and analysis methods\ncan be found in the supplementary section."}, {"title": "Exp.1: Evaluation of participant ability to distinguish Al-generated responses from Doctors' responses", "content": "First, we investigated whether participants would be able to distinguish AI-generated responses from Doctors' responses as a\npreliminary understanding of participant perception of AI and Doctors' performance in responding to health inquiries. To do\nso, participants were provided a medical question and a corresponding response, either a Doctors' response, High Accuracy\nAI-generated response, or Low Accuracy AI-generated response. To reiterate, as judged by our four expert evaluators, \"High\nAccuracy AI\" refers to responses that are generated by the AI system with a high degree of accuracy, while \"Low Accuracy AI\"\nrefers to responses generated by the AI system with a lower degree of accuracy; see figure 3.\nOverall, in Experiment 1, 100 online participants (98 participants passed the screening and were included in the result)\nwere presented with 10 randomly selected medical question-response pairs from a collection of 30 Doctors' responses, 30\nHigh Accuracy AI-generated responses, and 30 Low Accuracy AI-generated responses. After reading the provided medical\nquestion-response pair, participants were asked to provide Likert scale evaluations on a scale of 1 (strongly disagree) to 5\n(strongly agree) on (1) their understanding of the medical question and (2) their understanding of the response. Additionally,\nthey were asked to indicate (3) their belief of the response source (response given by a Doctor or an AI-Text Generator) and a\nLikert scale evaluation of (4) their confidence in the source they selected on a scale of 1 (low confidence) to 5 (high confidence);\nsee figure 4. The full set of questionnaires is listed in the supplementary material."}, {"title": "Understanding Inquiry and Response", "content": "From the hierarchical linear model analysis, there were no significant differences in the participants' understanding of medical\nquestions (p = 0.5964) across the three categories. However, significant differences were observed in participant evaluations of\nresponse understanding (p = 1.787e-04). From the pairwise comparison, we found that participants rated their understanding of\nAI-generated responses, regardless of the accuracy level (High Accuracy AI: mean = 4.53, SD = 0.69; Low Accuracy AI: mean\n= 4.56, SD = 0.62), to be significantly higher (High Accuracy AI vs. Doctors: p = 0.001, Low Accuracy AI vs. Doctors: p =\n0.0008) than Doctors' responses (mean = 4.32, SD = 0.83)."}, {"title": "Source Determination Accuracy", "content": "When participants were asked to determine the source of the medical response provided to them (Doctor-written or AI-\ngenerated), there was an average source determination accuracy of 50% for Doctors' responses, 53% for High AI Accuracy\nresponses, and 50% for Low AI Accuracy responses. There were no significant differences in the participants' performance\nin source determination task (p = 0.6539) across the three categories indicating that participants were unable to effectively\ndistinguish Doctors' responses from AI-generated medical responses."}, {"title": "Source Determination Confidence", "content": "When participants were asked to provide their level of confidence in determining the source of the medical response provided to\nthem, even though their accuracy was low (around 50%), participants reported a high level of confidence across three types of\nresponse when they answered correctly (Doctors' response: Mean = 3.94, SD = 0.87; High Accuracy AI response: Mean =\n3.78, SD = 0.91; Low Accuracy Al response: Mean = 3.94, SD = 0.79) and incorrectly (Doctors' response: Mean = 3.98, SD =\n0.84; High Accuracy AI response: Mean = 3.77, SD = 0.93; Low Accuracy Al response: Mean = 4.02, SD = 0.84). The level of\nconfidence when participants guessed correctly and incorrectly was not significantly different across the three response types\n(Doctors' response: p = 0.6803; High Accuracy AI response: p = 0.9279; Low Accuracy AI response: p = 0.9537)."}, {"title": "Linguistic Analysis", "content": "Additionally, a linguistic analysis of the medical responses was completed through a computational approach to identify if\nthere were any significant variations in linguistic characteristics (word count, sentiment. reading ease) in the different response"}, {"title": "Exp.2: Participant's evaluation of Al-generated responses compared to doctors' responses", "content": "Experiment 2 aimed to assess how participants evaluate responses generated by the AI system compared to those provided\nby doctors when they are unaware of the exact source of the responses. The experiment, similar to Experiment 1, involved\n100 participants (96 participants passed the screening and were included in the results) who were presented with 10 medical\nquestion-response pairs randomly selected from a collection of 30 Doctors' responses, 30 High Accuracy AI-generated\nresponses, and 30 Low-Accuracy AI-generated responses.\nHere, participants were asked to provide Likert scale evaluations on a scale of 1 (strongly disagree) to 5 (strongly agree) on\n(1) their understanding of the medical question and (2) their understanding of the response. Additionally, they were asked to\nindicate their perception of (3) response validity (Yes/No). Finally, participants were asked to provide Likert scale evaluations\non (4) the trustworthiness of the response, (5) the completeness and satisfaction of the response, (6) participant tendency to\nsearch for additional information based on the response, (7) participant tendency to follow the advice provided in the response,\nand (8) participant tendency to seek subsequent medical attention as a result of the response."}, {"title": "Understanding Inquiry and Response", "content": "From the hierarchical linear model analysis, there were no significant differences in the participants' understanding of medical\nquestions across the three categories: Doctor-written, High Accuracy AI-generated, and Low Accuracy AI-generated (p =\n0.43). However, participants demonstrated a significantly higher level of understanding (p = 8.2e-06) of AI-generated responses\nthan the Doctors' responses, regardless of the AI-generated response accuracy level. Participants indicated the highest level of\nunderstanding for High Accuracy AI-generated responses (Mean= 4.58, SD=0.73), followed by Low Accuracy AI-generated\nresponses (Mean= 4.48, SD=0.87), and then the Doctors' responses (Mean= 3.97, SD=1.21) (High Accuracy AI vs. Doctor: p <\n0.0001; Low Accuracy AI vs. Doctor: p < 0.0001)."}, {"title": "Validity", "content": "Additionally, significant differences were observed in participant evaluations of response validity within the different response\ntypes (p = 0.011). The pairwise analysis indicated that participants perceived the High Accuracy AI-generated (Mean= 0.95,\nSD=0.22) responses to be significantly more valid (p = 0.0106) than the Doctors' responses (Mean= 0.81, SD=0.39). The Low\nAccuracy AI-generated responses (Mean= 0.87, SD=0.34) performed very comparably to the Doctors' responses."}, {"title": "Trustworthiness", "content": "Significant differences were observed in participant evaluations of response trustworthiness within the different response types\n(p = 0.0058). The pairwise analysis indicated that participants perceived the High Accuracy AI (Mean= 4.26, SD=0.86) to\nbe significantly more trustworthy (p = 0.0050) than the Doctors' responses (Mean= 3.85, SD=1.13). The Low Accuracy\nAI-generated responses (Mean= 4.06; SD=1.05) were rated similarly to the Doctors' responses."}, {"title": "Completeness/Satisfaction", "content": "Significant differences were observed in participant evaluations of response completeness & satisfaction in the different response\ntypes (p = 0.005). The pairwise analysis indicated that participants perceived the High Accuracy AI (Mean= 4.03, SD=1.11)\nto be significantly more complete/satisfactory (p = 0.0042) than the Doctors' responses (Mean= 3.55, SD=1.33). The Low\nAccuracy AI-generated responses (Mean= 3.77, SD=1.30) were rated similarly to the Doctors' responses, with no significant\ndifference identified."}, {"title": "Tendency to seek additional information", "content": "Beyond the previous metrics, such as validity, trustworthiness, and completeness, we were also interested in gaining an\nunderstanding of what next steps the participant might be inclined to take as a result of the response. While these participants\nwere not patients directly seeking the responses to these medical questions, they were asked to emulate the patient in the given\nsituation. Participants told to picture themselves asking their Doctor the given question, were asked to rate their tendency to\nseek additional information as a result of the response they received. We did not observe significant differences (p = 0.10)\nbetween Doctors' responses (Mean= 3.94, SD=1.17), High Accuracy AI response (Mean= 3.65, SD=1.35), and Low Accuracy\nAl responses (Mean= 3.88, SD=1.21)."}, {"title": "Tendency to follow the advice provided", "content": "Again, asked to envision themselves as the patient seeking advice, participants rated their tendency to follow the advice provided\nto them in the response. We did not observe significant differences (p = 0.094) between Doctors' responses (Mean = 3.68, SD =\n1.20), High Accuracy AI response (Mean = 4.00, SD = 1.02), and Low Accuracy AI responses (Mean = 3.85, SD = 1.14),\ndemonstrating a relatively equal tendency to follow the advice provided across all three response types."}, {"title": "Tendency to seek further medical attention", "content": "Finally, participants were asked to rate their tendency to seek subsequent medical attention as a result of the response provided.\nWe did not observe significant differences (p = 0.26) between Doctors' responses (Mean = 3.42, SD = 1.31), High Accuracy AI\nresponse (Mean = 3.56, SD = 1.24), and Low Accuracy AI responses (Mean = 3.66, SD = 1.28)"}, {"title": "Exp.3: Participant's evaluation of Al-generated responses compared to doctors' responses given a random\nlabel", "content": "In the third experiment, we investigated if participants exhibited biases toward or against certain response sources. Similar\nto Experiment 2, 100 participants (all 100 participants passed the screening and were included in the result) were presented\nwith 10 medical question-response pairs randomly selected from a collection of 30 doctors' responses, 30 High Accuracy\nAI-generated responses, and 30 Low-Accuracy AI-generated responses. However, at the start of the survey, participants were\nrandomly shown one of three labels: \"The responses to each medical question were given by a %(Doctor)\", \"The responses to\neach medical question were given by %(Artificial Intelligence (A.I.))\", or \"The responses to each medical question were given\nby a %(Doctor assisted by A.I.)\".\nThen, similar to Experiment 2, participants were asked to provide Likert scale evaluations on a scale of 1 (strongly disagree)\nto 5 (strongly agree) on (1) their understanding of the medical question and (2) their understanding of the response. They were\nasked to indicate their perception of (3) response validity (Yes/No). Finally, they were asked to provide Likert scale evaluations\non a scale of 1 (strongly disagree) to 5 (strongly agree) of (4) the trustworthiness of the response, (5) the completeness and\nsatisfaction of the response, (6) participant tendency to search for additional information based on the response, (7) participant\ntendency to follow the advice provided in the response, and (8) participant tendency to seek subsequent medical attention as a\nresult of the response."}, {"title": null, "content": "The results revealed that, in general, the source labels had little effect on participants' evaluations of the medical responses.\nHowever, we observed the effect of the labels on the \"trustworthiness\" rating of the Doctor's responses (p = 0.022) and High\nAccuracy Al responses (p = 0.0042), see figure 6. In particular, the pairwise analysis revealed that in the presence of the label:\n\"This response to each medical question was given by a %(Doctor)\" participants tended to rate High Accuracy AI-genered\nresponses types as significantly more trustworthy (\"Doctor\" vs. \"AI\": p = 0.013; \"Doctor\" vs. \"Doctor assisted by AI\": p =\n0.01). However, we didn't see such an impact of the same label (\"Doctor\") on the trustworthiness ratings of the Low Accuracy\nAI-generated response (p = 0.49)."}, {"title": "Additional Experiment: Physicians' evaluations of Al-Generated responses with and without response\nsource indicated", "content": "Identifying key results across non-expert participant evaluations of the AI-generated responses vs. the Doctor' responses, we\nwanted to conduct a preliminary investigation of whether similar trends would be found amongst the physician evaluators.\nParticularly, we were interested in exploring if our physicians revealed any particular biases during their evaluation of the\nAI-generated responses. To do so, firstly we asked 3 of our 4 physicians from the initial evaluation to also evaluate the\nDoctors' responses, completing the Non-Blind portion of the study. Simultaneously, 6 additional general physicians from\nthe same institutions were asked to complete a blind evaluation of the same AI-generated responses and Doctors' responses.\nMore details regarding the design of this additional study can be found in the methodology. We found that when the experts\ndidn't have access to the label regarding the source of the response (Doctor-written or AI-generated), there was no significant\ndifference in their evaluation in terms of accuracy (p = 0.2258), strength (p = 0.5694), and completeness (p = 0.2740). However,\nwhen the experts did have access to the source of the response, they evaluated the AI-generated responses as significantly\nlower in all three metrics: accuracy (p = 6.509e-13), strength (p = 0.003), and completeness (p = 1.606e-08). see figure 6.\nAdditionally, when completing a two-way ANOVA test, a significant relationship between the study type (Blind vs. Non-Blind)\nand the Response Source (AI vs. Doctor) was identified while evaluating the Accuracy (p = 1.385e-07) and Completeness (p =\n0.001126), confirming a bias presented by experts against AI-generated responses when the source of the response is indicated\n6."}, {"title": "Discussion", "content": "Firstly, participants present a very similar understanding of the medical questions across the different groups (High Accuracy,\nLow Accuracy, and Doctor). Therefore, any differences in their subsequent evaluations of the medical responses in each\nexperiment should be associated with differences in perception of the medical response rather than with differences in\nunderstanding of the medical question. There were also no significant differences identified in the linguistic characteristics of\nthe different response types, thus controlling for any confounding factors related to medical question-response linguistics that\ncould impact evaluation outcomes.\nThe general public is unable to distinguish Al-generated medical responses from Doctors responses\nParticipants displayed an approximate 50% accuracy rate in discerning the origin of the medical responses, making it clear that\nthey struggled to effectively differentiate between medical advice offered by a human doctor and medical responses generated\nby artificial intelligence. This holds true even when the accuracy of the AI-generated medical response is comparatively low.\nThus, participants perceive the AI-generated responses as remarkably similar to those provided by doctors, rendering them\nunable to accurately differentiate between the advice given by the artificial intelligence and that offered by a registered physician\non the online healthcare platform HealthTap.\nLow Accuracy Al-generated responses pose a danger for the public as they seem equally valid and are\ndeemed more trustworthy\nIn addition to participants' inability to distinguish AI-generated responses from doctors' responses, we found that participants\nevaluated AI-generated responses almost equally to, if not better than, responses provided by Doctors across all metrics.\nAI-generated medical responses were found to be as comprehensive, valid, trustworthy, complete & satisfactory, and persuasive\nas doctors' responses, with AI-generated responses of High Accuracy performing significantly better in a majority of the\nmetrics. Furthermore, on average, albeit not significantly, Low Accuracy AI-generated responses presented a higher level of\nperformance than the doctors' responses across all the evaluation metrics.\nParticipants' inability to differentiate between the quality of AI-generated responses and Doctors' responses, regardless of\naccuracy, combined with their high evaluation of low accuracy AI responses deemed comparable, if not superior, to Doctors'\nresponses, presents a concerning threat. When unaware of the response's source, participants are willing to trust, be satisfied,\nand even act upon advice provided in AI-generated responses, similar to how they would respond to advice given by a doctor,\neven when the AI-generated response includes inaccurate information. This unexpected trust and satisfaction with low accuracy\nAI-generated responses may lead to unwitting acceptance of harmful or ineffective medical advice and concerns of liability for\nany resulting adverse patient outcomes30.\nParticipants place significantly more trust in the High Accuracy Al-generated responses that were labeled\nto suggest they were given by Doctors\nIn this study, we found when participants evaluate the medical response provided to them, with no indication of the source, they\nlargely favor AI-generated responses and are ready to place a high level of trust even in those of low accuracy. However, as\nsoon as the source of the response is provided, the way in which participants evaluate the response seems to change. Responses\nthat participants are told were given by Doctors, but were in actuality High Accuracy AI-generated responses were deemed to\nbe more trustworthy than the same High Accuracy AI-generated responses labeled accurately as \"AI\". In this scenario, we\nsee that the average participant, while satisfied with AI's performance in generating medical advice, seems to generally prefer\nreceiving said advice from doctors, and thus reports that medical responses provided by a Doctor are more trustworthy than\nAI-generated ones. Interestingly, the presence of the response source label \"Doctor\" alone does not yield a similar enhancement\nin the perception of the Low Accuracy AI-generated medical responses. This phenomenon is particularly pronounced when\nparticipants are presented with a High Accuracy AI-generated medical response. It is evident that a synergistic effect occurs,\nwherein the combination of a desirable source with a high accuracy model results in a heightened evaluation. In essence,\nthese two factors need to align to achieve this desirable response. This pattern is also observed in other domains, including\nAI-generated advice for legal decision making. A recent study discovered that while human-like explanations or advice alone\ndid not significantly augment trust, when combined with a high accuracy AI-generated response, there was a notable increase in\nthe level of trust that participants placed in the provided advice24. In particular, it is interesting to find this similar bias amongst\nour expert evaluators, who rated the AI-generated responses significantly higher when the its source was unknown. This finding\nhighlights that even those we rely on to establish the objective truth of our research, assess the efficacy of such models, and\nendorse their suitability for future use can also be susceptible to their own inherent biases.\nPotential to Extend the Applicability of our Findings to Other Language Models\nIn our study, we used AI-generated responses from the GPT-3 model, which is among the most adopted language models\nwith publicly accessible specifications and training data7. While there are recent models with greater accuracy, including"}, {"title": null, "content": "closed-source models such as GPTs, Claude and Gemini, and open-source alternatives like LLama, we believe that our findings\nregarding non-experts' perception and evaluation of AI-generated medical responses can be generalized to AI-generated\nresponses from other advanced language models. This is due to the shared underlying architectures and training methodologies\nacross these models that permit newer models to hallucinate misinformation still8,27,34. Regardless of the specific language\nmodel employed, the possibility of generating both highly accurate and inaccurate medical responses remains a concern23,27.\nAs these models progress and refine, the challenges identified in our study, such as lay people's capacity to differentiate between\nAI-generated and Doctors' medical responses and biases in evaluation, will persist in their relevance and potential even grow. It\nis critical for future research and development initiatives to take these insights into account when both designing AI models for\nincorporation into healthcare systems and outlining the framework for their effective, ethical implementation."}, {"title": "Limitations", "content": "It is important to note that there are some key limitations to this study. (1) As described above, this study uses the GPT-3 model\ninstead of a more recent version of the model. An argument could be made that some of the concerns with low accuracy in\nAI-generated responses may improve with newer models. With that said, it is interesting and concerning to note that even a low\naccuracy response from an older model was quite convincing to participants. (2) A second limitation to consider is how well do\nthe participants represent the general public. We used an online research recruitment tool which does try to recruit as broad and\nrepresentative a sample as possible, however this might have skewed towards people who are more technologically savvy and\nthe age range of our participants may have left out older and younger participants as it was mainly participants between 18 - 49\nyears old. Our participants are not directly affected by the medical advice given, nor are they the ones posing these questions.\nInstead, they are tasked with emulating the role of a patient who would receive such a response. As a result, while this approach\nprovides a broad understanding of public perception, it lacks the personal element that stems from participants' pre-existing\nknowledge about the condition and/or their emotional investment in the response provided. (3) Another limitation is that we\nonly had four physicians review each medical question-response pair and evaluate their accuracy. While it would've been ideal\nto have more expert evaluations, since their scores were ultimately reduced down to two accuracy categories (High and Low),\nwe don't believe this limitation is too critical. (4) A final, crucial, limitation is that this study focuses solely on single medical\nquestion and response pairs. It does not deal with conversations or any additional context than what is provided in the inquiry\nitself. In real world clinical scenarios, Doctors are more likely to ask for additional context and follow up information prior to\nproviding such medical advice. Therefore, further research could explore the role of such context in the application of AI for\nmedical question answering."}, {"title": "Broader Implications", "content": "Our findings expose a number of key considerations that need to be consistently evaluated, both from the perspective of the\nlayperson and the perspective of the physician, while designing and deploying technologies such as LLMs and chatbots in\nmedical response applications.\nThere is a danger in generating and releasing Al-generated medical responses to the public without doctor supervision\nRecent studies regarding the use of LLMs in the medical domain have primarily considered the perspective of the physician\nor doctors and are limited in assessing the accuracy or quality of the responses23. However, our study clarifies that solely\nidentifying the accuracy level of a medical response is inadequate for comprehending the dynamics it may evoke among\nboth general participants and physicians. In the scenario where an AI system predominantly generates medical responses\nthat physicians deem as low accuracy, these experts are likely to label the respective LLM or its design as ineffective for\nthis specific use case. However, using the same LLM and set of medical responses, a lay person may find the advice within\nthose responses highly persuasive and satisfactory, as evidenced in our study, leading them to potentially act upon the advice\nprovided. Consequently, a low accuracy AI-generated response can inadvertently be perceived as favorable or on par with that\nof a doctor, putting the participant at risk of being harmed by misinformation. This follows a similar pattern to the recent study\nin which patients increasingly trusted their friends and families over physicians for health issues\u00b9. Further, our results further\nsupport a finding that people cannot detect AI-generated text from those of humans, as the systems can exploit heuristics to\nproduce text perceived as \u201cmore human than human\u201d22. In cases where such technology is utilized, in order to help reduce the\nlikelihood of such risk, at the minimum a physician should remain within the loop to filter out inaccurate medical information\nor inappropriate medical advice, while preserving the benefits of the collaboration with the AI system."}, {"title": "Even with a doctor's supervision, it is important to pay attention to how the doctor evaluates the Al system as they\ncould exhibit bias towards or against the Al output", "content": "Both laypersons and experts possess some level of bias with regards to the different medical response types. While lay people\nfind AI-generated responses of high accuracy with a label of \"Doctor\" to be more trustworthy, expert evaluators find themselves\nto be more critical of responses labeled as \"AI-generated\", thus providing another set of considerations that need to be addressed"}, {"title": "Doctor intervention in Human-Al systems can allow us to benefit from Al's unique capability while preventing the\ndamage from its inaccuracy", "content": "Ultimately, this study shows that participants find the AI-generated responses, especially the High Accuracy responses, to\nperform comparably if not better than Doctors' responses in all metrics of perception. Thus, lay people tend to very much lean\ntowards AI-generated responses. At the same time participants indicate a clear sense of trust and comfort in responses they are\ntold are given by a Doctor. Thus, ideally we would presume the key for successful applications of LLMs in medicine would be\nto combine the patient interaction and trust qualities of the physician with the thorough and comprehensive response delivery of\nthe AI. Interestingly enough, we do see though that when participants were told a response was provided by a \"Doctor assisted\nby AI\", there was no significance increase in performance evaluations of the response. Thus, the solution of combining the\nbenefits of both physicians and AI seems to be contradicted by participants' perceptions of the different source types. This\nunderscores the complexities of the situation and emphasizes the intricate dynamics through which participants and experts"}]}