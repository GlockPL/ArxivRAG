{"title": "A Safety Modulator Actor-Critic Method in Model-Free Safe Reinforcement Learning and Application in UAV Hovering", "authors": ["Qihan Qi", "Xinsong Yang", "Gang Xia", "Daniel W. C. Ho", "Pengyang Tang"], "abstract": "This paper proposes a safety modulator actor-critic (SMAC) method to address safety constraint and overestimation mitigation in model-free safe reinforcement learning (RL). A safety modulator is developed to satisfy safety constraints by modulating actions, allowing the policy to ignore safety constraint and focus on maximizing reward. Additionally, a distributional critic with a theoretical update rule for SMAC is proposed to mitigate the overestimation of Q-values with safety constraints. Both simulation and real-world scenarios experiments on Unmanned Aerial Vehicles (UAVs) hovering confirm that the SMAC can effectively maintain safety constraints and outperform mainstream baseline algorithms.", "sections": [{"title": "I. INTRODUCTION", "content": "Reinforcement learning (RL) has demonstrated remarkable achievements in games and simulations [1]\u2013[6] since results derived from it are rarely fail. However, in practical scenarios, using RL is not an easy task because various inherent risks in RL agents often deteriorate the function of RL, which may lead to unsafe behaviors or even catastrophic consequences, such as equipment damage, environmental degradation, or even loss of human life. Therefore, it is a great challenge to guarantee the safe behavior derived by RL in real applications, especially for UAVs.\nIn the field of safe RL, there exist two common methods [7]: safety filter method [8], [9] and safety learning method [10]\u2013[14]. The safety filter method solves the safety problem by using a safety filter on the actions of the RL agent [8], [9]. Although the safety filter can transform unsafe actions into safe actions, it neither guarantees safety nor offers adapt-ability [8] or even requires extremely precise information of a dynamic model for constructing safety filter [9]. Therefore, achieving satisfactory safety performance by using safety filters in practical systems remains challenging, and it is more difficult when different tasks require diverse modeling approaches, particularly for tasks that lack existing models.\nIn contrast, the safety methods in learning optimizes the policy with safety constraints throughout the learning process directly. An advantage of safety learning methods is their model-free nature, which allows them to be applied to complex systems without requiring an accurate system model. This is particularly beneficial in real-world scenarios where accurate models are often difficult or impossible to obtain. A notable safety in learning method is the Lagrangian method [10], [11], which transforms an optimization problem with safety constraints into an unconstrained primal-dual optimization problem. This is achieved by dynamically adjusting the weight of the safety cost rewards based on the degree of satisfaction with safety constraints. However, under such an approach, the policy may face a substantial burden or even fail to achieve its safety learning objectives because it needs to trade off the reward against the cost rewards. Hence, it is urgent to develop new techniques to alleviate the burden of policy while meeting safety constraints.\nOn the other hand, most RL algorithms tend to learn overestimated Q-values [15]\u2013[17], resulting in a suboptimal policy formulation. It is demonstrated in [15] that system noise, approximation error, or any other sources can induce an overestimation bias. To mitigate overestimation, approaches like double Q-learning and double Q-network were developed by [15], [18] to leverage a target Q-network to provide unbiased estimates. However, these methods are inherently limited to discrete action spaces. Although the authors in [19] extend double Q-learning and Q-network to continuous action spaces by using actor-critic method, the overestimation problem persists due to the high similarity between the online Q-value and the target Q-value. While distributional critic approaches have been employed [11], [20], these methods lack theoretical analysis to derive a gradient update rule that addresses overestimation. Although [17] effectively mitigates overestimation with a theoretically guaranteed gradient update rule, their approach fails to address safety constraints, let alone alleviate the burden of policy in meeting safety constraints. These gaps motivate us to propose a novel method to investigate the mitigation of overestimation and the alleviation of"}, {"title": "II. SAFETY MODULATOR", "content": "Safe RL issue can be modeled as constrained Markov decision process (CMDP) $(X, U,r,r_c,p)$ [23], [24], where X and U are the continuous state space and continuous action space, respectively, $r : X \\times U \\rightarrow [r_{min}, r_{max}]$ is the reward function, $r_c : X \\times U \\rightarrow [C_{min}, C_{max}]$ is the cost reward function, $p : X \\times U \\times X \\rightarrow [0, 1]$ is the state transition function. It is assumed that state $x_t \\in X$ at the time $t$ can be observed from the environment, the agent takes an action $u_t \\in U$ to interact with the environment and transmit state $x_t$ to $x_{t+1}$. The initial state $x_o \\sim \\O$, $\\O$ is the initial state distribution, $\\pi(\\cdot|x_t)$ is the action policy distribution under state $x_t$ and action $u_t \\sim \\pi(\\cdot|x_t)$. The entire trajectory distribution under policy $\\pi$ is represented as $T_{\\pi} = (x_0, u_0, x_1, u_1, \\dots)$.\nConsider the following safe RL optimization problem with $(x_t, u_t) \\sim T_{\\pi}$\n$\\max_{\\pi} E[\\sum_{t=0}^{\\infty} \\gamma^t r(x_t, u_t)]$,\ns.t. $E[\\sum_{t=0}^{\\infty} \\gamma^t r_c(x_t, u_t)] \\leq C$,\nwhere $r(x_t, u_t)$ is the reward function and $r_c(x_t, u_t)$ is the cost reward function, $C \\geq 0$ is the given safety constraint, $\\gamma$ is the discount factor of reward and cost reward.\nThe safe RL optimization (1) is actually a constrained optimization problem. By using the Lagrangian method [10], [11], the constrained optimization problem can be equivalently transformed into the following unconstrained optimization one:\n$\\min_{\\lambda \\geq 0} \\max_{\\pi} E[\\sum_{t=0}^{\\infty} \\gamma^t r(x_t, u_t) - \\lambda (\\sum_{t=0}^{\\infty} \\gamma^t r_c(x_t, u_t) - C)]$,\nwhere $\\lambda \\geq 0$ is the safety weight and can be dynamically adjusted according to the satisfaction of constraints.\nFor the convenience of subsequent derivations, let $Q(x_0, u_0) = E[\\sum_{t=0}^{\\infty} \\gamma^t r(x_t, u_t)]$ and $Q_c(x_0, u_0) = E[\\sum_{t=0}^{\\infty} \\gamma^t r_c(x_t, u_t)]$. Then above unconstrained optimization problem can be simplified as\n$\\min_{\\lambda \\geq 0} \\max_{\\pi} E[Q(x_0, u_0) - \\lambda (Q_c(x_0, u_0) - C)].$ (2)\nThere are two steps to solve (2), the first one is optimizing policy $\\pi$ for given $\\lambda$, second is optimizing $\\lambda$ for given $\\pi$:\n$\\max_{\\pi} E[Q(x_0, u_0) - \\lambda (Q_c(x_0, u_0) - C)],$ (3)\n$\\min_{\\lambda \\geq 0} E[-\\lambda (Q_c(x_0, u_0) - C)].$ (4)\nRemark 1: According to the contraction mapping theorem in [25], a unique fixed point exists in a complete metric space. By continuously applying the contraction mapping, starting from any initial state $x_0$ and $u_0 \\sim \\pi(\\cdot|x_0)$, this unique fixed point can be reached. Consequently, policy iteration will converge to the optimal value function regardless of the initial estimates. For off-policy training, the optimization (3) can be represented as $\\max_{\\pi} E[Q(x_t, u_t) - \\lambda (Q_c(x_t, u_t) - C)].$\nIn order to address (3) for the action $u_t \\sim \\pi(x_t)$, one can maximize $Q(x_t, u_t)$ and minimize $Q_c(x_t, u_t)$. In the training step, the policy constantly trades off the $Q(x_t, u_t)$ against the $Q_c(x_t, u_t)$. Consequently, it may face a significant challenge or failure in its task learning. To prevent this from happening, the safety modulator $\\Delta u_t$ and modulation function $m(\\cdot) : A \\rightarrow A$ are presented such that $u_t = m(\\bar{u}_t, \\Delta u_t)$, where $\\bar{u}_t \\sim \\pi_{\\theta_{\\bar{u}}}(x_t)$ is the risky policy that disregards the potential for unsafe situations, $\\Delta u_t \\sim \\pi_{\\theta_\\Delta}(\\cdot|x_t, \\bar{u}_t)$ is the safety modulator for $\\bar{u}_t$, $\\pi_{\\theta_{\\bar{u}}}(x_t)$ and $\\pi_{\\theta_\\Delta}(\\cdot|x_t, \\bar{u}_t)$ denote the policy approximated with parameters $\\theta_{\\bar{u}}$ and $\\theta_\\Delta$, respectively. In the following statement, the overall composed policy will be denoted as $\\pi_{\\theta_{\\bar{u}}, \\theta_\\Delta}$.\nFor the model training, the risky policy $\\pi_{\\theta_{\\bar{u}}}$, safety modulator $\\pi_{\\theta_\\Delta}$ and critics $Q_{w_q}(x_t, u_t)$, $Q_{c,w_c}(x_t, u_t)$ are learned from experience tuple $(x_t, u_t, r(x_t, u_t), r_c(x_t, u_t), x_{t+1}) \\sim D$, where $D$ represents the replay buffer, $Q_{w_q}(x_t, u_t)$ and $Q_{c,w_c}(x_t, u_t)$ are the approximations of $Q(x_t, u_t)$ and $Q_c(x_t, u_t)$ using the parameters $w_q$ and $w_c$, respectively. Introducing safety modulator, (3) can be divided into two parts:\n(a) $\\max_{\\theta_{\\bar{u}}} E[Q_{w_q}(x_t, u_t)],$"}, {"title": "III. OVERESTIMATION MITIGATION", "content": "In this section, the issue of overestimation inherent in Q-learning is discussed, and specific overestimation value is provided through formula derivation. After that, the distributional critic and corresponding update rule are introduced to mitigate overestimation.\nThe Q-value approximated by the parameter $w_q$ is expressed as $Q_{w_q}(x_t, u_t) = Q(x_t, u_t) + v_t$ with $v_t$ being a random zero mean noise, $Q(x_t, u_t)$ is the ideal Q-value without bias. Then, the updated parameter $w_q'$ can be obtained by the following formula\n$w_q' = w_q + \\eta (y - Q_{w_q}(x_t, u_t))\\nabla_{w_q}Q_{w_q}(x_t, u_t)$,\nwhere $\\eta$ is the learning rate which controls update step size, $y = E[r(x_t, u_t) + \\gamma \\max_{u_{t+1}} Q_{w_q}(x_{t+1}, u_{t+1})]$ is the Bellman equation.\nSimilarly, the updated parameter $w_q'$ of the $w_q$ is formulated as\n$w_q' = w_q + \\eta (\\bar{y} - Q_{w_q}(x_t, u_t))\\nabla_{w_q}Q_{w_q}(x_t, u_t)$,\nwhere $\\bar{y} = E[r(x_t, u_t) + \\gamma \\max_{u_{t+1}}Q(x_{t+1}, u_{t+1})]$ is the ideal value of $y$.\nEmploying first-order Taylor's expansion, the updated values of $Q_{w_q}$ and $Q_{w_q}$ can be approximated by the following $Q_{w_q'}(x_t, u_t)$ and $Q_{w_q'}(x_t, u_t)$, respectively.\n$Q_{w_q'}(x_t, u_t) \\approx Q_{w_q}(x_t, u_t) + \\eta (y - Q_{w_q}(x_t, u_t))||\\nabla_{w_q}Q_{w_q}(x_t, u_t)||^2$,\n$Q_{w_q'}(x_t, u_t) \\approx Q_{w_q}(x_t, u_t) + \\eta (\\bar{y} - Q_{w_q}(x_t, u_t))||\\nabla_{w_q}Q_{w_q}(x_t, u_t)||^2$. (7)\nThen, the estimation error of $Q_{w_q}$ during an update step is\n$\\epsilon(x_t, u_t) = E[Q_{w_q'}(x_t, u_t) - Q_{w_q'}(x_t, u_t)]$\n$\\approx E[\\eta(y - \\bar{y}) ||\\nabla_{w_q}Q_{w_q}(x_t, u_t)||^2]$\n$= \\eta \\gamma E[\\max_{u_{t+1}} Q_{w_q}(x_{t+1}, u_{t+1}) - \\max_{u_{t+1}} Q(x_{t+1}, u_{t+1})] \\times ||\\nabla_{w_q}Q_{w_q}(x_t, u_t)||^2$.\nConsidering $Q_{w_q}(x_t, u_t) = Q(x_t, u_t) + v_t$ and letting $\\epsilon = E[\\max[Q(x_{t+1}, u_{t+1}) + v_{t+1}] - \\max Q(x_{t+1}, u_{t+1})]$, one has\n$\\epsilon(x_t, u_t) \\approx \\eta \\gamma \\epsilon ||\\nabla_{w_q}Q_{w_q}(x_t, u_t)||^2$.\nIt is noteworthy that $\\epsilon > 0$ [18], [26], which implies $\\epsilon(x_t, u_t) \\geq 0$, i.e., the max operator inherently introduces an upward bias to estimation errors. Even if a single update introduces only a slight upward bias, the cumulative effect of these bias through temporal difference (TD) learning can lead to substantial overestimation, which makes the policy suboptimal.\nTo mitigate overestimation, a distributional critic denoted by $Z(x_t, u_t)$ is considered, which follows a normal distribution $Z(x_t, u_t)$. The mean and standard deviation of this distribution are approximated by the neural network outputs $Q_{w_q}(x_t, u_t)$ and $\\sigma_{w_\\sigma}(x_t, u_t)$, respectively. Define $Z(\\cdot|x_t, u_t) = N(Q_{w_q}(x_t, u_t), \\sigma_{w_\\sigma}(x_t, u_t)).$"}, {"title": "IV. SAFETY MODULATOR ACTOR-CRITIC", "content": "This section proposes an SMAC algorithm, incorporating the corresponding update rules for the risky policy $\\pi_{\\theta_{\\bar{u}}}(\\cdot|x_t)$, the safety modulator $\\pi_{\\theta_\\Delta}(\\cdot|x_t, \\bar{u}_t)$, the distributional critic $Z_{w_q}(x_t, u_t)$, and the cost critic $Q_{c,w_c}(\\cdot|x_t, u_t)$, with approximate parameters $\\theta_{\\bar{u}}$, $\\theta_\\Delta$, $w_q$, and $w_c$. It is noteworthy that the update rule of the distributional critic in Distributional Policy Evaluation can theoretically guarantee overestimation mitigation. Additionally, a series of training techniques are employed in Distributional Policy Evaluation to improve training stability. The updated rule of the safety modulator is detached from the gradient $\\theta_{\\bar{u}}$ to alleviate the burden of risky policy to focus on maximizing rewards.\n1) Distributional policy evaluation: Considering $B_{Z(x_t, U_t)} \\sim B_{\\pi_{\\theta_{\\bar{u}}}, \\pi_{\\theta_\\Delta}} \\cdot Z_{w_q}(x_t, u_t)$, $(x_t, u_t) \\sim D$, the loss function of KL divergence is given as\n$J_z(W_q) = E[D_{KL}(B_{\\pi_{\\theta_{\\bar{u}}}, \\pi_{\\theta_\\Delta}} Z_{w_q}(\\cdot|x_t, u_t), Z_{w_q}(\\cdot|x_t, u_t))]$\n$= E[\\int [log(P(B_{Z(x_t, U_t)} | B_{\\pi_{\\theta_{\\bar{u}}}, \\pi_{\\theta_\\Delta}} \\cdot Z_{w_q}(\\cdot|x_t, u_t)))\n- log(P(B_{Z(x_t, U_t)}|Z_{w_q}(\\cdot|x_t, u_t)))]]$\n$\\approx E[\\int P(B_{Z(x_t, U_t)}|B_{\\pi_{\\theta_{\\bar{u}}}, \\pi_{\\theta_\\Delta}} \\cdot Z_{w_q}(\\cdot|x_t, u_t))\ndB_{Z(x_t, U_t)} \\cdot (-log(P(B_{Z(x_t, U_t)} | Z_{w_q}(\\cdot|x_t, u_t))))] + F,$ (13)\nwhere $F$ is independent of the optimized parameter $w_q$, $w_q$ is the parameter of target distribution $Z_{w_q}(x_t, U_t)$, $B_{\\pi_{\\theta_{\\bar{u}}}, \\pi_{\\theta_\\Delta}}$ is the Bellman operator with policy $\\pi_{\\theta_{\\bar{u}}}$ and $\\pi_{\\theta_\\Delta}$, and $\\pi_{\\theta_{\\bar{u}}}, \\pi_{\\theta_\\Delta}$ is the safe target policy with target parameters $\\theta_{\\bar{u}}$ and $\\theta_\\Delta$.\nIn view of $Z_{w_q}(x_t, U_t) = N(Q_{w_q}(x_t, U_t), \\sigma_{w_q}(x_t, U_t))$, the gradient of $J_z(W_q)$ is obtained as\n$\\nabla_{w_q}J_z(W_q) = E[-\\nabla_{w_q} log(P(B_{Z(x_t, U_t)}|Z_{w_q}(\\cdot|x_t, u_t)))]$\n$= E[\\nabla_{w_q}log(\\sqrt(\\frac{1}{2 \\pi \\sigma_{w_q}^2 (x_t, u_t)})/exp(-\\frac{(B_{Z(x_t, u_t)}-Q_{w_q}(x_t, u_t))^2}{2 \\sigma_{w_q}^2(x_t, u_t)}))]=E[-\\frac{1}{2}\\frac{2(B_{\\pi_\\theta \\cdot Z(x_t, u_t)} -Q_{w_q}(x_t, u_t)) \\nabla_{w_q}Q_{w_q}(x_t, u_t)}{\\sigma_{w_q}^2(x_t, u_t)}+\\log(\\sigma_{w_q}(x_t, u_t))+\\log(\\sqrt{2\\pi})]=\nE[\\frac{\\bar{y} - Q_{w_q}(x_t, u_t)}{2 \\sigma_{w_q}^2(x_t, u_t)} \\nabla_{w_q}Q_{w_q}(x_t, u_t)-\\frac{1}{2}\\frac{\\sigma_{w_q}^2(x_t, u_t)+(\\bar{y} - Q_{w_q}(x_t, u_t))^2}{\\sigma_{w_q}(x_t, u_t)}\\nabla_{w_q}\\sigma_{w_q}(x_t, u_t)].$ (14)\nInspired by [17], [19], [29], the independent double Q-networks for critic are used, which are $Q_{w_q}$ and $Q_{\\tilde{w}_q}$. The critic tends to choose the smaller mean value between $Q_{w_q}$ and $Q_{\\tilde{w}_q}$. Meanwhile, the clip function is used in"}, {"title": "V. EXPERIMENTS", "content": "In this section, Crazyflie 2.1 is utilized to carry out the UAV hovering experiments, where both numerical simulation and real-world experiment verify the effectiveness and safety of the SMAC."}]}