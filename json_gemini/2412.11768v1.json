{"title": "No More Adam: Learning Rate Scaling at Initialization is All You Need", "authors": ["Minghao Xu", "Lichuan Xiang", "Xu Cai", "Hongkai Wen"], "abstract": "In this work, we question the necessity of adaptive gradient methods for training deep neural networks. SGD-SaI is a simple yet effective enhancement to stochastic gradient descent with momentum (SGDM). SGD-SaI performs learning rate Scaling at Initialization (SaI) to distinct parameter groups, guided by their respective gradient signal-to-noise ratios (g-SNR). By adjusting learning rates without relying on adaptive second-order momentum, SGD-SaI helps prevent training imbalances from the very first iteration and cuts the optimizer's memory usage by half compared to AdamW. Despite its simplicity and efficiency, SGD-SaI consistently matches or outperforms AdamW in training a variety of Transformer-based tasks, effectively overcoming a long-standing challenge of using SGD for training Transformers. SGD-Sal excels in ImageNet-1K classification with Vision Transformers(ViT) and GPT-2 pretraining for large language models (LLMs, transformer decoder-only), demonstrating robustness to hyperparameter variations and practicality for diverse applications. We further tested its robustness on tasks like LoRA fine-tuning for LLMs and diffusion models, where it consistently outperforms state-of-the-art optimizers. From a memory efficiency perspective, SGD-SaI achieves substantial memory savings for optimizer states, reducing memory usage by 5.93 GB for GPT-2 (1.5B parameters) and 25.15 GB for Llama2-7B compared to AdamW in full-precision training settings.", "sections": [{"title": "1. Introduction", "content": "Stochastic gradient-based optimization methods, such as Stochastic Gradient Descent (SGD), are fundamental to modern machine learning, enabling the successful training of models across a wide range of scientific and engineering applications. However, training objectives and data are often noisy in practice, and gradients may become sparse due to the inherent characteristics of regularization or specific architectural designs. Moreover, architectural differences can introduce imbalances in the learning dynamics across different parameters. To address these challenges, adaptive gradient methods (Ghorbani et al., 2022) have been developed to handle better non-stationary objectives, noisy data, and sparse gradients. Among these methods, Adam (Kingma & Ba, 2014) and AdamW (Loshchilov & Hutter, 2019) have become indispensable for training Transformer-based models,"}, {"title": "2. Related Work", "content": "Adaptive Gradient Methods:Stochastic gradient descent (SGD) is an efficient optimization method commonly used in deep learning, but it struggles with tasks that have non-stationary objectives or involve very noisy and/or sparse gradients (Kingma & Ba, 2014), often requiring extensive hyperparameter tuning. To improve upon these limitations, adaptive gradient methods were developed to continuously and dynamically adjust learning rates for individual parameters throughout the training process (Duchi et al., 2011; Graves, 2014; Zeiler, 2012), with the Adam optimizer becoming particularly popular. Adam combines features from"}, {"title": "3. Problem Setting", "content": "Notations. A neural network is defined based on a set of trainable parameters in specific architectures. We denote the neural network's parameters as \u03b8 \u2208 R^d, where d is the total number of parameters. The training loss function L(\u03b8) defines the objective to be minimized. The parameter space is partitioned into B blocks based on the definition of network architectures, denoted as \u03b8^(i) \u2208 R^di for i \u2208 {1, 2, ..., B}, where di = \u03a3_1^B d^(i). Each parameter \u03b8_j^(i) within block i for j \u2208 [di] is associated with its own gradient g_j^(i) = \u2207_{\u03b8_j^(i)} L(\u03b8).\nKey notations used throughout the paper are as follows: We denote t \u2208 N as the index for the training step, \u03b7 > 0 as the global learning rate, \u03bb \u2265 0 as the weight decay coefficient, \u03bc as the momentum coefficient, g_t^(i) \u2208 R^di as the gradient of the loss w.r.t. \u03b8^(i) at step t. [di] is the index set {1, 2, 3, . . ., di } corresponding to the parameters in block i. And O(*) means the complexity, here we use it"}, {"title": "Stochastic Gradient-based Optimization:", "content": "Given the loss function L(\u03b8), the goal of a general optimization process is to update \u03b8 in the following form iteratively:\n\u03b8_{t+1} = \u03b8_t - \u03b7_t D_t,  (1)\nwhere D_t denotes the update direction at step t. The choice of D_t defines the specific optimization algorithm. For SGD, the update direction is defined as the negative gradient of the loss with respect to \u03b8_t:\nD_t = g_t, where g_t = \u2207L(\u03b8_t)  (2)\nThe first-order momentum term was introduced to SGD to enhance the optimisation process, and it is called SGD with momentum(SGDM) (Nesterov, 1983). The momentum m can be defined as:\nm_t = \u03b2_1 m_{t-1} + (1 - \u03b2_1) g_t  (3)\nThe update becomes:\nD_t = m_t  (4)\nThis addition helps accelerate convergence by incorporating information from previous gradients to smooth out the update steps. Specifically, it reduces oscillations in the optimization trajectory, particularly in scenarios with steep or narrow ravines in the loss landscape. By maintaining a running average of past gradients, the momentum term allows SGD to move more consistently in directions that lead to faster convergence, addressing challenges like slow progress on flat regions of the loss surface."}, {"title": "Adaptive Gradient Methods:", "content": "Adaptive gradient methods like Adam adopted first-order momentum m_t as we mentioned above while introducing the second-order momentum v_t, which tracks squared gradients to adjust the learning rate for each parameter, the v_t defined as:\nv_t = \u03b2_2 v_{t-1} + (1 - \u03b2_2) g_t^2.  (5)\nThe update direction for Adam is as follows:\nD_t = \u03b1_t m_t, where \u03b1_t = \u03b7 / (\u221a{v_t} + \u03f5)  (6)\nv\u0302_t term is the v_t with bias correction. Notably, \u03b1_t is the local learning rate gain (aka. adaptive learning rate). The key computational challenge is the storage and updating of v_t, which requires O(d) additional memory."}, {"title": "Memory Efficient Adam:", "content": "As Scaling Law (Kaplan et al., 2020) introduced, Transformer model sizes in recent days have significantly increased compared to the model size when Adam was introduced. Consequently, the memory"}, {"title": "4. Methods", "content": "Considering the substantial memory overhead introduced by the second-order momentum in the Adam optimizer, this section explores strategies to reduce this cost by revisiting the foundational motivations for adaptive gradient methods.\nIn the following subsections, we design a memory-efficient learning rate local gain, termed g-SNR, to replace the second-order momentum. We analyse the distribution of g-SNR across different parameter groups throughout the network. This aligns with the motivation of parallel works (Zhang et al., 2024b) that focus on partitioning parameter"}, {"title": "4.1. Memory Efficient Local Gain: g-SNR", "content": "Adam builds upon RMSprop, designed to find a local gain for the learning rate, enabling parameter-specific adjustments within deep neural networks (Hinton et al., 2012; Kingma & Ba, 2014). By incorporating second-order momentum, Adam improves upon SGD by better handling problems with non-stationary objectives and tasks characterized by noise or sparse gradients (Kingma & Ba, 2014). This mechanism allows Adam to dynamically rescale gradients, effectively adjusting the learning pace across parameter blocks with distinct gradient patterns. Consequently, Adam outperforms SGD when training architectures with heterogeneity problems in the Hessian matrix, such as Transformers (Zhang et al., 2024a;b). Another key insight arises from the warm-up mechanism: even with second-order momentum, Adam still requires a warm-up phase to reduce the learning rate at the beginning of training, aiming to mitigate gradient variance (Liu et al., 2021). During this phase, gradients are known to be sparse and noisy. Reducing the learning rate directly during the warm-up phase effectively lowers gradient variance, stabilizing the training process straightforwardly and efficiently.\nIntuitively, adaptive gradient methods dynamically adjust the learning rate for each parameter during training. This mechanism encourages parameters with less learning history to learn more while slowing down the learning pace for parameters progressing too quickly. Essentially, it acts as a compensatory approach to address learning imbalances across parameters after they arise. However, if we can predict and pre-empt these imbalances before they occur, we could potentially eliminate the need for second-order momentum, which relies on learning history to evaluate and correct them.\nConsidering the root cause of why learning imbalance occurred across different parameters, we discussed them in two main parts. Firstly, as inherited from the architecture characteristics, the parameter in different layers or with different architectures will receive distinct gradient pattern (Tanaka et al., 2020; Li et al., 2023b; Xiang et al., 2023), thus bringing the optimal learning rate for different parameters are distinct and need to be re-adjust with local gain (Hinton et al., 2012), Secondly, within the parameter groups, the"}, {"title": "4.2. Statistics Analysis for g-SNR", "content": "Building on the insights above, we implemented the g-SNR mechanism using PyTorch's Default Partition (Zhang et al., 2024b), which computes g-SNR within each parameter block and dynamically re-scales the learning rate accordingly. To assess its effectiveness, we conducted experiments on Vision Transformer (ViT) pre-training tasks using ImageNet-1K, selecting ViT/S-16 for comprehensive tracing and analysis of gradient patterns throughout the training process.\nOur analysis revealed that g-SNR remains relatively stable over time while exhibiting distinct patterns across different parameter classes, as shown in Fig. 4. Specifically, we examined transformer blocks from shallow, middle, and deep layers within the network and parameters outside the transformer blocks, such as positional embeddings.\nGiven the g-SNR definition we provide in the previous subsection, we analyze its behaviour as follows: As modern initialization schemes (e.g., Xavier (Kumar, 2017), Kaiming (He et al., 2015b)) ensure that at t = 0:\nG_{norm}^{(i)} (0) and G_{var}^{(i)} (0)\nare well-controlled. This implies that G(0) starts from a stable, architecture-driven ratio. During the training process, parameters are updated and controlled by the step size \u03b7 is the learning rate. Assuming \u03b7 is sufficiently small to"}, {"title": "4.3. Proposed Methods Detail: SGD-SaI", "content": "We propose a new method called SGD-SaI that removes adaptive gradient components by rescaling the learning rates of each parameter block using the g-SNR calculated from the initial batch. The algorithm details are presented in Algorithm 6. By leveraging the initial g-SNR, we capture the inherent gradient characteristics of different parameter blocks, allowing for a constant scaling factor that addresses the variations in gradient magnitudes across blocks.\nAs our method eliminates the dynamic terms associated with adaptive gradient algorithms, it only introduces a few computations at the first iteration compared to naive Stochastic Gradient Descent with Momentum (SGDM). Specifically, the additional computation involves calculating the g-SNR for each parameter block during the initial batch. After this initial computation, the training proceeds similarly to standard SGDM, making our method computationally efficient and comparable in complexity to traditional SGD.\nTo update the g-SNR based on the actual gradient sparsity without affecting the gradient computation, we adopt decoupled weight decay as proposed by Loshchilov and Hutter (Loshchilov & Hutter, 2019). Decoupled weight decay applies regularization directly to the parameters rather than incorporating it into the gradient computation. This approach is equivalent to regularization in SGD and allows us to accurately compute the gradient statistics needed for the g-SNR without the weight decay term distorting the gradient values. By doing so, we ensure that the g-SNR reflects the gradients' true sparsity and noise characteristics.\nOur implementation remains extremely straightforward, as we adopt the simplest approach that requires only minimal modifications to the existing SGD optimizer. This simplicity ensures that existing tricks and frameworks that support SGD can seamlessly integrate with and support our method."}, {"title": "5. Experiments", "content": "This section evaluates our method through several tasks, including pre-training for Large Language Model (LLM) and Vision Transformer (ViT), Parameter-Efficient Fine-Tuning (PEFT) tasks on LLM and Diffusion Model (DM), and traditional Convolutional Neural Network (CNN) tasks. The specific tasks are outlined as follows:\n\u2022 Large Language Model(Transformer Decode Only) We pre-train GPT-2 (Radford et al., 2019) on OpenWebText (Gokaslan & Cohen, 2019). We profile the optimizer state tensors' memory usage and optimizer step time for GPT-2-XL(1.5B) and LLM2-7B.\n\u2022 Vision Transformer We investigate the Vision Transformer (ViT/S-16) (Dosovitskiy et al., 2021) on the ImageNet-1k dataset (Deng et al., 2009) for image classification tasks. We profile the optimizer state tensors' memory usage and optimizer step time for ViT-H/14.\n\u2022 Parameter-Efficient Fine-Tuning (PEFT) LORA Fur-"}, {"title": "5.1. LLM Pre-train", "content": "Setups. We pre-train GPT-2-Small (125M) (Radford et al., 2019) on OpenWebText (Gokaslan & Cohen, 2019). We compare SGD-SaI with AdamW (Loshchilov & Hutter, 2019) and Adam-mini (Zhang et al., 2024b). We follow the same settings as described in the previous study (Zhang et al., 2024b). We analyse the loss metrics for each optimizer.\nFor large-scale LLMs, we provide profiling results focusing on memory usage and wall-clock time during the optimizer step for GPT-2-XL (1.5B parameters) and Llama-2 (7B parameters). Due to resource constraints, these results are limited to the optimizer step time and do not encompass full training runs. We compare SGD-Sal with SGDM, AdamW, Adam (Kingma & Ba, 2014), Adam-mini, and Prodigy (Mishchenko & Defazio, 2023). The reported metrics include memory usage of the state tensors and the time costs associated with the optimizer steps. All results were obtained using a single NVIDIA A100 (80GB).\nResults. Figure 8 compares optimizers (AdamW, Adam-mini, and SGD-SaI) during pre-training of GPT-2-Small across multiple metrics. While SGD-SaI demonstrates a slightly slower initial convergence speed compared to the Adam family optimizers due to its design, it achieves superior final convergence with a lower training loss (outperforming Adam-mini by 0.13). Similarly, validation loss shows a marginal improvement, with SGD-SaI reducing it by 0.03 compared to Adam-mini.\nEfficiency. For the GPT-2-Small pre-training task. Re-"}, {"title": "5.2. ViT Pre-train", "content": "Setups. We pre-train ViT-S/16 (Dosovitskiy et al., 2021) on the ImageNetlk dataset (Deng et al., 2009) for the image classification task. We compare SGD-Sal with AdamW (Loshchilov & Hutter, 2019) as well as popular optimizers including SGDM, Adam (Kingma & Ba, 2014), Adam-mini (Zhang et al., 2024b) and Prodigy (Mishchenko & Defazio, 2023). After conducting a grid search within the same hyperparameter range, we compare the optimiser results. We report the peak and mean of the top-1 validation accuracy to evaluate their generalisation ability and sensitivity to hyperparameter changes. Detailed hyperparameters are in Appendix A.1.\nDue to the intensive computational power requirements for the ViT variants during the grid search, we cannot provide"}, {"title": "5.3. Parameter Efficient Fine Tuning: PEFT", "content": "We primarily consider PEFT tasks on LLM fine-tuning and the Diffusion Model fine-tuning."}, {"title": "5.3.1. LLMS PARAMETER EFFICIENT FINE-TUNING", "content": "Setup. We fine-tune the GPT-2 model using the E2E dataset (Novikova et al., 2017). The current state-of-the-art (SOTA) methods include scaled-SGD and scaled-AdamW from (Zhang & Pilanci, 2024), which adjust the learning rates for parameters A and B with a Riemannian preconditioner. Our primary comparison is between SGD-SaI and these methods, along with Adam-mini. To evaluate the results of the fine-tuning, we report metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), MET (Banerjee & Lavie, 2005), ROUGE-L (Lin, 2004), and CIDEr (Vedantam et al., 2015). For all these metrics, higher scores indicate better performance.\nResults. We adopted the same experiment setting to investigate whether our methods suit LoRA Training (Hu et al., 2021). We set the default learning rate to le - 3 and weight decay to le - 2. Empirically, we observe that SGD-SaI outperforms previous state-of-the-art (SOTA) scaled optimizers and unscaled ones. Table 4 presents surprising results regarding the final scores for LoRA fine-tuning of the GPT-2 medium model with a rank of 4 on the E2E natural language generation tasks. With this simple precondition on SGDM, our method performs significantly better than the previous"}, {"title": "5.3.2. DMS PARAMETER EFFICIENT FINE-TUNING", "content": "Setup. Using the diffusion model, we extend our experiments to include LoRA fine-tuning on image generation tasks. Specifically, we utilize the ChilloutMix model to address real-world concepts, following the same approach outlined in Mix-of-show (Gu et al., 2024; Zhang & Pilanci, 2024). Additionally, we compare our method with the state-of-the-art (SOTA) optimized approach using scaled-AdamW (Zhang & Pilanci, 2024). To evaluate the images generated by the diffusion model, we conduct a qualitative assessment to determine which method captures visual concepts more effectively.\nResults. Face generation is a challenging task; the model should understand the visual concept of a specific person's face based on its prompt text. Here, we set the learning rate as default 0.1, a large enough default value. We observed as Fig. 9, even without carefully tuning the learning rate, our scaled methods have shown a significantly better ability to capture the visual concept of potter than the previous SOTA scaled approach scaled-AdamW (Zhang & Pilanci, 2024). It should be verified that our optimizer has better parameters robustness on training and leads to better convergence in final performance; this should be an essential benefit for the practical use of the optimizer."}, {"title": "5.4. Convolutional Neural Network(CNN)", "content": "Setup. We follow a similar approach to Section 5.2 for evaluating CNN models. A grid search is performed on ResNet18 (He et al., 2015a) using the CIFAR-10 dataset, and across various architectures from NATS-Bench (Dong et al., 2021) on CIFAR-10, CIFAR-100, and ImageNet16-120 (Krizhevsky & Hinton, 2009; Chrabaszcz et al., 2017). All tasks involve image classification. We compare SGD-Sal with traditional optimizers (SGD and Adam-family)"}, {"title": "6. Conclusion", "content": "In summary, our results demonstrate that simply applying selective learning rate scaling at initialization (SGD-Sal) can unlock performance comparable to\u2014if not better than\u2014leading adaptive gradient methods like AdamW, all while retaining the simplicity and efficiency of SGDM. By leveraging g-SNR to guide parameter group scaling, SGD-Sal not only mitigates early training imbalances but also"}, {"title": "7. Limitation", "content": "While SGD-Sal demonstrates promising results across various Transformer-based tasks, our study is constrained by limited computational resources, preventing us from conducting large-scale pre-training on more extensive models such as Llama-2-7B. This remains an avenue for future research. However, to address the efficiency challenges of training larger models, we have performed detailed profiling of GPU memory usage and optimizer step speed on these architectures. These preliminary analyses indicate the potential scalability of SGD-SaI, but comprehensive evaluations on larger-scale models are necessary to establish its effectiveness and efficiency in such settings fully. Moreover, our methods ensure a steady and stable update during training, allowing the model to converge better in a given task with sufficient training steps. Thus, we might observe that the convergence speed is relatively lower than Adam's in the early stage of training; as our primary focus is to investigate the effectiveness of the Sal approach, we left the acceleration of convergence speed in future work."}, {"title": "A. More Experiments Details", "content": "A.1. Details for ViT Experiments\nIn this section, we will list list the settings of the experiment regarding to Section 5.2.\nHyperparameter Settings: We start by following the settings in (Beyer et al., 2022a; Steiner et al., 2022; Beyer et al., 2022b); Specifically, we include Nesterov-SGD as a baseline, offering better performance than naive SGD. All optimizers are tested using a grid search within the same hyperparameter ranges: learning rate lr \u2208 {0.1,0.01, 0.001,0.0001} and weight decay wd \u2208 {0.01, 0.001, 0.0001}."}, {"title": "A.2. Details for CNN Experiments", "content": "In this section, we will list the settings of the experiment regarding to Section 5.4.\nModels and Datasets: We follow the same settings and train some CNN-based architectures proposed in NATS-Benchmark (Dong et al., 2021). We test the optimizers on CIFAR-10/CIFAR-100 (Krizhevsky & Hinton, 2009) and ImageNet16-120 (Chrabaszcz et al., 2017). Based on the NATS-Benchmark work, we test different sizes of architectures. Here, we select ten architectures with top-10 validation accuracy and one architecture with bottom-1 validation accuracy in terms of different datasets and training epochs to present.\nHyperparameter Settings: The optimal learning rate and weight decay are chosen by performing the grid search. The learning rate and weight decay are selected from \u03b7 \u2208 {0.1, 0.01, 0.001} and \u03bb\u2208 {0.5, 0.05, 0.005, 0.0005}, respectively. We use the same cosine annealing scheduler on three datasets without learning rate warmup. We use the same data augmentation methods and set the batch size to 256 for all datasets. The experiments are designed to run for full training without early stopping. There is no linear scaling on the initial weight decay either since we are doing the grid search within a feasible range. The seed is only 777 which is the same seed reported by NATS-Benchmark on Size Search Space. The original NATS-Benchmark were produced using SGD with a fixed learning rate 0.1 and weight decay 0.0005 and the default setting of the Nesterov Momentum. For fair comparison, we apply the same grid search policy for SGD, as the baseline with or without Nesterov Momentum."}, {"title": "A.3. Extra Results for ResNet18 on CIFAR10", "content": "As a classic model of the CNNs, we also conduct the grid search on ResNet18 as an extended experiment.\nModels and Datasets: We follow the similar setting in the Section 5.4. We particularly choose the CIFAR-10 (Krizhevsky & Hinton, 2009) as the dataset we test on. We test on the classic ResNet18 model.\nHyperparameter Settings: Since we are focusing on a single model with one dataset\u2014unlike the NAST-Benchmark CNN experiments discussed in Section 5.4\u2014we are scaling up our search by exploring a wider range of learning rates and weight decays. The learning rates are chosen from the set \u03b7 \u2208 {0.1,0.01, 0.001, 0.0001} and the weight decays from \u03bb\u2208 {0.5, 0.05, 0.005, 0.0005, 0.00005}. We will repeat our grid search three times using three different random seeds. The random seeds used for the experiments are {42, 888, 999}. We opted for a step learning rate scheduler rather than a cosine annealing scheduler to test our method's resilience to different learning rate scheduling policies. The learning rate will decrease by a factor of 10 every 80 epochs, with a total of 200 epochs for training. Our data augmentation methods remain consistent, with a batch size set to 128. These experiments will run for the entire training duration without early stopping, and there will be no linear scaling applied to the initial weight decay, as we are conducting a grid search within a reasonable range. The distribution of accuracies averaged over the three seeds for each hyperparameter combination is depicted in Fig.5. The best performance of each optimizer, along with the optimal learning rate and weight decay, is annotated with red numbers on the graph. For simplicity, we are only testing the Stochastic Gradient Descent with Momentum (SGDM) as the baseline. The momentum is set to the default value of 0.9, consistent with both the Adam(W) optimizer and our method to ensure fair comparison.\nResults: The grid search results are shown in the Fig. 5. Not only does our method converge better (ours 95.36% v.s. SGDM 95.26%), but it also demonstrates greater resilience to changes in hyperparameters. This means the performance is less likely to downgrade compared to SGDM.\nAll the experiments in this paper were conducted using various types of GPUs, including NVIDIA GeForce RTX 3090, NVIDIA A100 PCIe 40GB, and NVIDIA A100 80GB. To ensure consistent experimental conditions, each experiment was conducted using only one GPU type."}, {"title": "B. Optimizer Analysis", "content": "This section provides a supplementary analysis for Section 5. We will detail the optimizers and empirically estimate the lower boundary of the state tensors in memory."}]}