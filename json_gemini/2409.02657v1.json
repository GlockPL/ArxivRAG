{"title": "PoseTalk: Text-and-Audio-based Pose Control and Motion Refinement for One-Shot Talking Head Generation", "authors": ["Jun Ling", "Yiwen Wang", "Han Xue", "Rong Xie", "Li Song"], "abstract": "While previous audio-driven talking head generation (THG) methods generate head poses from driving audio, the generated poses or lips cannot match the audio well or are not editable. In this study, we propose PoseTalk, a THG system that can freely generate lip-synchronized talking head videos with free head poses conditioned on text prompts and audio. The core insight of our method is using head pose to connect visual, linguistic, and audio signals. First, we propose to generate poses from both audio and text prompts, where the audio offers short-term variations and rhythm correspondence of the head movements and the text prompts describe the long-term semantics of head motions. To achieve this goal, we devise a Pose Latent Diffusion (PLD) model to generate motion latent from text prompts and audio cues in a pose latent space. Second, we observe a loss-imbalance problem: the loss for the lip region contributes less than 4% of the total reconstruction loss caused by both pose and lip, making optimization lean towards head movements rather than lip shapes. To address this issue, we propose a refinement-based learning strategy to synthesize natural talking videos using two cascaded networks, i.e., CoarseNet, and RefineNet. The CoarseNet estimates coarse motions to produce animated images in novel poses and the RefineNet focuses on learning finer lip motions by progressively estimating lip motions from low-to-high resolutions, yielding improved lip-synchronization performance. Experiments demonstrate our pose prediction strategy achieves better pose diversity and realness compared to text-only or audio-only, and our video generator model outperforms state-of-the-art methods in synthesizing talking videos with natural head motions.", "sections": [{"title": "Introduction", "content": "One-shot audio-driven talking head synthesis aims to generate a talking video based on a single-face image and input speech. In recent years, it has garnered significant attention from the community due to its massive potential in the industry, including virtual assistants, digital humans, virtual conferencing, and the like. Benefiting from large-scale talking face datasets (Zhang et al. 2021b; Wang et al. 2020), numerous approaches have been proposed, demonstrating rapid progress in this field (Ling et al. 2020; Tian et al. 2024).\nExisting one-shot talking head generation methods can be categorized into template-based and audio-driven. Template-based methods reconstruct face videos by inpainting mouth features based on the mouth-masked face images (Prajwal et al. 2020; Cheng et al. 2022; Zhong et al. 2023; Shen et al. 2023; Stypulkowski et al. 2024) or generate facial images driving by the pose references from off-the-shelf videos and audio features (Zhou et al. 2021; Liang et al. 2022; Wang et al. 2023). Although these methods produce lip-sync results, the borrowed poses are not easy to modify and do not always match the latent dynamics and rhythm of the input audio. On the contrary, audio-driven methods generally predict head poses conditioned on the input audio (Zhang et al. 2023; Wang et al. 2021; Xu et al. 2024), and then synthesize the talking face video conditioned on the input audio and predicted poses. Audio-driven methods have been receiving increasing attention due to their possibility of generating talking videos from a single image.\nDespite their utility, audio-driven methods might be limited by two Drawbacks: D1: Audio-driven methods often learn pose prediction based on a short-term audio feature segment without context head motion hints. However, short-term audio feature segments lack long-term semantics of the head motions and are insufficient for faithful head pose generation. For example, a boy might either wag his head or remain still with only his mouth moving while reading. This challenge might make the predicted poses rigid and converge to mean pose sequences (e.g., waging around the mean pose) (Ye et al. 2023). D2: While all approaches take image reconstruction loss or lip-sync loss to penalize the errors between the generated frames and ground-truth frames or speech content, the losses are not equally considered. For example, users are generally more sensitive to errors in small-scale lip motions (e.g., lip motions are not synchronized with audio segments) and the naturalness of large-scale head motions. However, it has been reported that the loss for lip region contributes less than 4% of the total reconstruction loss (Prajwal et al. 2020), making the optimization lean towards head poses rather than lip shapes. As a result, the model usually focuses on optimizing head poses before generating fine-grained lip motions.\nWe address the above issues from two perspectives. To tackle D1, we introduce a Pose Latent Diffusion (PLD) model to achieve pose generation conditioned on both audio and text prompts describing actions. Text prompts are a user-friendly interface to express human intention and they can offer long-term cues for head movements, while the audio provides short-time information (such as rhythm or sentiment, slight nodding when someone says \"yes\"). Based on our analysis, we devise a latent diffusion model that leverages both conditions to predict pose sequences. Specifically, we first train a VAE and compress an original pose sequence into a latent embedding. Then we train a conditional diffusion model to learn correlations between text-audio and poses in the motion latent space, followed by a VAE decoder to decode the pose sequence from the predicted motion latent. Through this approach, text and audio can collaborate effectively in facilitating the learning of natural head movements.\nTo address the loss-imbalance problem D2, we find that lip-oriented motion refinement is the key to achieving accurate lip motion prediction and pose transfer. To this end, we propose a two-stage motion prediction framework: the pose-oriented coarse motion prediction stage and the lip-oriented motion refinement stage. In the first stage, we train a pose-oriented coarse motion network to predict coarse motions and achieve pose transfer. In the second stage, we devise a lip-oriented motion refinement network to compensate for the inadequacy of coarse motion. Specifically, we borrow the strategy from (Wang et al. 2022) and build multi-resolution motion flows, progressively predicting finer lip motions from low-resolution to high-resolution and finally formulating fine-grained lip motions (as illustrated in Fig. 4). Compared to the one-stage prediction, the proposed two-stage approach exhibits more advantages in learning accurate motions. In addition, we find that the generated images in the second stage might lose some details such as the backgrounds. To alleviate this issue, we adopt the same structure as the editing network of PIRender (Ren et al. 2021) conditioning on refined motion results and the source frame (Please refer to supplementary material for details). To summarize, our major contributions are in four folds:\n\u2022 We introduce text prompts and audio conditions to talking face generation and propose a novel and efficient system to achieve fine-grained pose control and audio-driven talking face video generation (shown in Fig. 1).\n\u2022 We propose a pose latent diffusion model, conditioned on text prompts and audio, to achieve user-friendly head pose generation (shown in supplementary materials).\n\u2022 We show that employing refinement-based motion prediction for talking video generation has the potential to strike a balance between the accurate animation of head movements and synchronized lip motion generation.\n\u2022 We conduct extensive experiments to show the capability of our approach in generating high-fidelity and lip-synchronized audio-driven talking videos, proving the effectiveness of our contributions."}, {"title": "Related Work", "content": "Audio-Driven Talking Head Synthesis\nRecent works on one-shot talking head generation can be roughly categorized into template-based methods and audio-driven methods. Template-based methods (Prajwal et al. 2020; Stypulkowski et al. 2024; Shen et al. 2023; Cheng et al. 2022; Zhong et al. 2023) bypass the pose editing and focus exclusively on synthesizing the lip movements from speech and background images. However, the downside of these methods is the inability to control poses, coupled with the limitation to mouth movements only. Some methods (Zhou et al. 2021, 2019; Liang et al. 2022; Wang et al. 2023) adopt the pose details from reference videos. Although effective, the references pose might not fit the input audio thus undermining video realness. Audio-driven methods predict head poses from audio only (Zhang et al. 2023; Ma et al. 2023b; Zhang et al. 2021b; Ma et al. 2023a). A common practice often leverages predefined 3DMM expression and pose coefficients to describe lip motions and head motions, and then learns those coefficients from audio features (Ren et al. 2021; Xue et al. 2022). Likewise, few works (Zhou et al. 2020; Chen et al. 2019; Ji et al. 2022; Gan et al. 2023) predict intermediate mouth representations, e.g., facial landmarks, keypoints, or latent motion weights. However, the 3DMM coefficients and the facial landmarks are sometimes ineffective in describing subtle lip movements and might compromise lip motion accuracy in real-world applications.\nRecently, the diffusion model has shown promising achievements in image/video generation (Rombach et al. 2022; Ho et al. 2022; Guo et al. 2024) and talking face generation (He et al. 2024; Xu et al. 2024; Tian et al. 2024). GAIA (He et al. 2024) introduces a latent diffusion model to learn the correspondence between speech and motion latent\nin the latent space. Hallo (Xu et al. 2024) and EMO (Tian et al. 2024) introduce temporal modeling to the Stable Diffusion (Rombach et al. 2022) model and optimize the denoising UNet to generate talking videos and improve temporal smoothness. Although promising in high-fidelity image synthesis, these methods compromise the training and inference speed and require huge amounts of high-quality videos (over 200 hours). Meanwhile, researchers also develop methods on person-specific tasks (Li et al. 2023; Guo et al. 2021; Yang et al. 2023; Ling et al. 2023; Tang et al. 2024). However, these methods cannot deal with arbitrary facial images. In this work, we develop a method that can effectively generate talking head videos to fulfill the requirement of motion descriptions and enhance the synthesis of full facial expressions more naturally and seamlessly."}, {"title": "Motion Generation", "content": "We treat the poses as a part of human motions and present related advances in recent years. Motion generation is a fundamental task in computer vision. It allows various inputs of multi-modal data such as text, action, and images. Earlier explorations such as (Wang et al. 2021; Chen et al. 2020) predict head motions from audio using LSTM-based temporal modules. SadTalker (Zhang et al. 2023) employs a VAE-based (Kingma and Welling 2013) model to model pose predictions conditioned on audios. Recently, text-to-motion has stood out among conditional motion generation tasks thanks to human language's user-friendly and understandable nature. The essence of the text-to-motion generation is learning a shared latent space for language and motion. Typical methods (Guo et al. 2022; Petrovich, Black, and Varol 2022) focus on learning the correlations between the input texts and the motion latent through attention modules and transformers. MotionCLIP (Tevet et al. 2022) aligns the motion latent space with the text and image spaces of the pre-trained visual language model CLIP (Radford et al. 2021), expanding text-to-motion beyond data limitations. UDE (Zhou and Wang 2023) discretizes motion sequences into latent codes, predicts quantized codes using a transformer similar to GPT, and decodes motions via a diffusion model decoder. MoFusion (Dabral et al. 2023) employs a diffusion model with a 1D U-Net-like transformer to reconstruct motion sequences from natural language or audio inputs. MDM (Tevet et al. 2023) employs a classifier-free guided diffusion model to predict samples instead of noise at each diffusion step, capable of adapting to different conditional modes. MLD (Chen et al. 2023) maintains a diffusion process in the latent motion space instead of using a diffusion model to establish connections between raw motion sequences and conditional inputs. However, compared to the human motion generation task, synthesizing head pose involves considering not only the user's text-driven input but also the corresponding audio, which is another key factor influencing the effectiveness. Previous methods have focused more on either text-guided or audio-guided approaches, neglecting the inherent connection between these two forms of guidance."}, {"title": "Method", "content": "Our generation framework consists of two key modules: pose generation and video generation. The pose generation module utilizes a latent diffusion model to learn motion representations from text descriptions and audio within the VAE latent space. The video generation module employs a refinement-based generator to synthesize realistic talking head videos based on audio inputs and the generated poses. In the following sections, we elaborate on our data construction pipeline, as well as the pose generation model and refinement-based video generator."}, {"title": "Dataset Construction", "content": "To learn models about both pose prediction and video generation, large-scale paired samples, especially the paired sample of motion description, audio, and head poses, are required. However, most of the existing talking video datasets, such as VoxCeleb (Nagrani, Chung, and Zisserman 2017) and HDTF (Zhang et al. 2021b), have no paired action descriptions. On the other hand, the facial text-video dataset such as CelebV-Text (Yu et al. 2023) is not appropriate for audio-driven face synthesis because of large pose angle and the audio might be background music, noise, or from the invisible speakers. To alleviate this issue and fully use the videos of different datasets, we employ 6DoF head poses and audio as motion-related representations to build connections across different datasets.\nWe formulate two categories of datasets: 1) the dataset\ncontains (audio, pose, text) for training pose prediction model; and 2) the dataset of lip-synced talking videos (audio, pose, video) for training the talking video generation. For the first category, we collect samples from the CelebV-Text dataset which contains 700000 in-the-wild face videos along with text descriptions of head actions. To clean the data, we removed the videos that have extreme head poses or noisy audio. Finally, we obtained 60000/4000/800 samples for training/testing/validation. For the latter category, we collect videos from three talking video databases, i.e., MEAD (Wang et al. 2020), HDTF, and VoxCeleb. We sampled videos from both frontal view and left30\u00b0/right30\u00b0 views in the MEAD dataset and finally obtained 12060 talking videos. We sampled 377 videos from HDTF for training and the rest 30 videos for testing. In addition, we followed (Siarohin et al. 2019) and collected 18000+ talking videos from VoxCeleb for training.\nThe dataset construction pipeline is shown in Fig. 2, where we estimate the head poses using a 3DMM model (Feng et al. 2021), predict the gazes through MediaPipe (Lugaresi et al. 2019), and extract the sequential audio features by using Wav2Vec2.0 (Baevski et al. 2020) and toolkit (Ott et al. 2019). We utilize CLIP to extract text embedding for the CelebV-Text dataset."}, {"title": "Pose Generation", "content": "The diffusion model has been applied to data generation in many areas, especially motion generation, speech synthesis, and video synthesis. However, diffusing directly on raw pose sequences is inefficient and requires significant computational resources. To reduce this demand, we first utilize a Variational AutoEncoder (VAE), V = {E,D}, to learn a low-dimensional latent space for diverse sequences of head poses and eye gazes. Our VAE is illustrated in Fig. 3-a(left). The encoder & is employed to extract representative features z\u2208 R^{nxd} from the original sequences x_{1:T} and the decoder D could reconstruct the latent into motion sequences x'_{1:T}. We use HuberLoss to train our VAE model:\n$\\mathcal{L}_{VAE} = HuberLoss(x_{1:T}, x'_{1:T})$\n(1)\nFollowing MLD (Chen et al. 2023), we build a Transformer-based denoising model with long skip connections which is more suitable for sequential data. The diffusion process is performed on a representative and low-dimensional motion latent space of our trained VAE model. Given the generated latent space features, we can reconstruct the pose sequence through the decoder.\nIn the Pose Latent Diffusion module (PLD), both text and audio guide the generation process. The text condition provides a general guide to actions, while the audio condition offers more detailed information, such as rhythm, which helps to specialize the results. This combination combines global guidance with detailed adjustments, offering the sequence more semantic guidance. For text conditions, the CLIP (Radford et al. 2021) text encoder is employed to map text prompts for its powerful semantic understanding capabilities. For audio conditions, we extract features from the audio using (Baevski et al. 2020). By combining both text embedding and audio features, our approach generates more natural yet action-controllable pose sequences. The conditional objective can be written as follows:\n$\\mathcal{L}_{PLD} := E_{e,t,c} [\\lVert\\epsilon - \\epsilon_{\\theta}(z_t, t, c)\\rVert_2^2]$\n(2)\nwhere t denotes the timestep, z_t represents the noisy feature, and c represents the condition comprimising text and audio. During inference, we employ the classifier-free guidance (Ho and Salimans 2022) as follows:\n$\\epsilon_{\\theta}^{c}(z_t, t, c) = s \\epsilon_{\\theta}(z_t, t, c) + (1 - s)\\epsilon_{\\theta} (z_t, t, \\varnothing),$\n(3)\nwhere s is the guidance scale and set to 7.5 to enforce the impacts of guidance."}, {"title": "Refinement-based Video Generator", "content": "Due to the challenge of striking a balance between large-scale head motions (e.g., head rotations and translation) and small-scale motions (e.g., lip movements) in a single network, we divide the motion prediction process into two stages to alleviate this issue. As shown in Fig. 3-b, our video generator consists of three submodules, a CoarseNet that focuses on pose transfer, a RefineNet that gradually refines lip motions from low to high resolution, and a Portrait Enhancer which improves the final generation quality. The three models are sequentially trained in three stages.\nCoarseNet. In the first stage, we train a coarse motion prediction model (CoarseNet) to focus on pose transfer and optimize using global reconstruction loss. To learn the motions from the input source frame and the motion condition, we design two networks: 1) The Motion2Latent network is employed to produce a latent vector of z from the concatenated motion conditions; 2) An image-to-motion prediction network that predicts coarse motions from source frames and\nthe motion latent. Motion smoothness is crucial for generating face videos, and the dynamic correlations among context motions are also influenced by past features. To this end, we treat the input as a sequence and model context information and dependencies using a Motion2Latent network. The Motion2Latent network is a Conformer-based (Gulati et al. 2020) architecture, which involves a 1d-convolution layer with stride 2 and 5 conformer blocks. The details of Motion2Latent are shown in Fig. 4-c. Using the projected motion latent, we employ an Hourglass-like network and insert the motion latent to the features through the AdaIN (Huang and Belongie 2017) layer after each convolution layer. Given the output coarse motion, we employ differentiable backward warping operation to warp the source image and obtain the warped frames.\nRefineNet. The RefineNet progressively refines the lip shapes and motions based on the outputs of frozen CoarseNet, compensating for the limitation of coarse motion prediction models. As we care more about the motion cues provided by audio features than the pose details in the refinement stage, we employ another Motion2Latent network to fuse the input motion parameters.\nInspired by the success of multi-resolution motion estimation (Wang et al. 2022), we build our motion refinement on a multi-scale and low-to-high resolution progressive prediction scheme. First, we utilize an encoder to extract the multi-scale features (denoted by $x^{enci}$) from the warped frames. For a better understanding, in the i-th layer of the Motion Decoder, we denote the layer block by Conv.i and the input features by $x^{deci-1}$, and the output flow and mask by $dF_i$ and $m_i$. We upsample the motions from lower resolution twice and denote it as $F_i^{-1}$ and add it to the flow at i-th layer. Then, we warp the encoder feature using the updated flows. This process can be written as:\n$F_i := dF_i + F_i^{-1}, m_i := m_i^{-1}$;\n$x^{outi} = warp(x^{enci}, F_i) \\cdot M_i + x^{deci} (1 - m_i)$,\n(4)\nwhere $x^{enci}/x^{deci}$ is the encoder/decoder feature at the i-th layer, and $F_i^{1}$ is the output of first convolution in Fig. 4-d. Through the motion updating process within the motion decoder, we obtain refined motions from the resolution of 8 x 8 to 256 \u00d7 256. The final motion flow from the input image to the target image can be formulated as:\n$F_i = F_{coarse} + dF_i + dF_{-1} + dF_2 + \\ldots\\ldots\\ldots,$\n(5)\nwhere $F_{coarse}$ is the coarse motion we obtained through the coarse motion network. To obtain the final output image, we use a convolution layer and a Tanh activation layer to output three-channel images from deformed features.\nPortrait Enhancer. As previously described, our two-stage motion refinement process has successfully achieved pose editing on a large scale and synthesis of fine-grained mouth movements. However, the quality of the generated images still has some imperfections which are mainly caused by the warping operations and the synthesis process in the second stage. To better recover facial details and maintain image consistency, we trained an image enhancer using a UNet structure from PIRender (Ren et al. 2021). This enhancer takes as input the source frame and the animated frames from the second stage to produce the final result.\nTraining Losses. We adopt the multi-scale perceptual loss (Siarohin et al. 2019) $\\mathcal{L}_{perc}$ and lip-sync loss (Prajwal et al. 2020) $\\mathcal{L}_{sync}$ to train our video generation model, both in stage 1 and stage 2. To implement lip-sync loss, we use differentiable backward warping to align the faces in the center of the image and calculate synchronization loss on the generated images and audio segments. In stage 3, we optimize the Portrait Enhancer using only the perceptual loss. We elaborate on the details in the supplementary material."}, {"title": "Experiments", "content": "Implementation Details\nThe training process of PLD consists of two stages: training the VAE and training the diffusion model. Our VAE is trained for 100 epochs with a batch size of 16 and our diffusion model is trained for 100 epochs with a batch size of 128. We train the video generation model in three stages. In the first stage, we train the CoarseNet on audio-visual datasets for 200k iterations with a batch size of 20. Then, we deliberately train the RefineNet using audio features and pose guidance for 400k steps while freezing the CoarseNet. Finally, we train the Portrait Enhancer to recover the missing\ndetails in generated images for 200k steps. To improve the model's robustness to mouth positions, we randomly crop the face images in the dataset.\nFollowing general evaluation protocols (Zhong et al. 2023; Chen et al. 2023), we conduct evaluations on video generation and pose generation, respectively. In addition, we also conducted a user study to assess the generated talking videos which are generated using unseen text prompts and driving audio. The results and essential implementation details are presented in supplementary materials."}, {"title": "Evaluation of Video Generation", "content": "Metrics. We adopt LPIPS (Zhang et al. 2018) and SSIM (Wang et al. 2004) to assess the image reconstruction quality. We also adopt the Identity Preserving (IP) score to assess cosine identity similarity between the source and the generated images using the backbone of Arcface (Deng et al. 2019). It is worth noting that the resolutions and mean positions of the generated images and ground truth images differ from method to method, making it challenging to evaluate mouth landmarks distance for different methods. To combat this, we adopt the Average Expression Distance (AED) (Ren et al. 2021) to measure the average 3DMM expression distance between the generated and real images. In addition, we adopt Syncconf (Chung and Zisserman 2016) score to evaluate the audio-visual synchronization quality.\nFace Video Generation Baselines. We compare our method with several representative works, including the template-based methods (Wav2Lip (Prajwal et al. 2020), IP_LAP (Zhong et al. 2023)), reference-based methods (PC-AVS (Zhou et al. 2021), PD-FGC (Wang et al. 2023)), and audio-driven method (SadTalker (Zhang et al. 2023), Hallo (Xu et al. 2024)). Template-based methods have some advantages on reconstruction quality as they directly 'copy' the backgrounds and only synthesize the mouth region. As reference-based methods require a reference video to provide pose details, we then use the original video as the pose reference for these methods. To compare with SadTalker, we extract head poses from original frames or directly predict pose parameters from audio depending on whether the audios are paired with the source frame.\nComparison with State-of-the-art methods. In Table 1, we show the testing results on HDTF and MEAD. Wav2Lip\nattains the best score in Syncconf as it was optimized on SyncNet. Compared to existing methods, our approach shows better overall generation quality and motion accuracy. Next, we conduct the qualitative comparison and show the results in Fig. 5. As can be observed, Wav2Lip tends to produce blurry results and distorted inner-mouth textures, which further sacrifice the overall video realness. PC-AVS heavily relies on face alignment operation to ensure that the faces are strictly centered, which introduces severe head jitters and frame inconsistency. SadTalker animates the source image using the predicted expression coefficients, which\nare insufficient to capture the subtle deviations of lip motions. IP_LAP and PD-FGC tend to produce face images with either blurry results or texture distortions. Among these methods, our approach achieves comparable or better performance in expression similarity and lip-sync accuracy. Hallo generates high-resolution face images utilizing the pertaining models of Stable Diffusion Models (Rombach et al. 2022), which in turn, requires much more computational resources and time to generate the same video samples as our method.\nAblation of Motion Refinement Strategy. We investigate the motion refinement strategy in our designs and show results in Table 2. Specifically, we test three variants concerning the motion refinement module and lip-sync loss. For example, we remove the motion refinement module and train the CoarseNet without (or with) the synchronization loss Lsync (the first row and the second row). Then, we remove the lip-sync loss in the refinement stage to investigate its influence (third row). As can be inferred, the generation model benefits from Lsync and this loss improves the lip-sync quality while incorporating both RefineNet and Lsync attains much better results compared to other variants."}, {"title": "Ablation of Pose Latent Diffusion", "content": "Here we conduct experiments to evaluate our PLD model. Specifically, we explore model performance w.r.t different input types (text-only, audio-only, and text-and-audio) and generation backbones (LSTM-based, transformer-based, and diffusion-based). We adopt Frechet Inception Distance (FID) (Heusel et al. 2017) to evaluate generation quality by measuring the feature distribution distance between the generated and real motions. Meanwhile, we assess the diversity (Div.) of the generated motion sequence by calculating the variance of features (Guo et al. 2022). The comparison results are shown in Table 3. As can be found, audio or text is not enough to produce highly diverse motion sequences. Although the FID scores slightly increased when involving both audio and text guidance, the generated sequences became more diverse and rhythmic, leading to clear improvements in the diversity score. For better visualization, we reduce the head motions into 1 dimension using PCA, and present the sequential results in Fig. 7. The pose curve of full guidance has the closest variation to the ground truth as it captures more details in both the overall long-term semantics and the finer local motions. Using the same text prompt and audio, we predict head poses and animate a face image to obtain talking face videos. Then, we estimate the per-frame facial landmarks and present the tracemaps as Fig. 8. As can be found, Audio2Head and Transformer show subtle head movements. Using only PLD-Text might result in large-scale head movements, which are improper for talking heads. Among these choices, our PLD-Text+Audio produces the most similar results as real videos."}, {"title": "Conclusion", "content": "This paper presents a systematical solution for text- and audio-based talking video generation. Our approach integrates two effective components: a pose latent diffusion model and a refinement-based video generator, addressing pose prediction and video generation, respectively. By learning the latent diffusion model on disentangled poses, our framework not only predicts natural head movements from action prompts and audio but also allows user-engaged and flexible head pose control. Additionally, we propose a refinement strategy to relieve the burden of the loss-imbalance issue and improve lip-synchronization quality for talking video generation. We hope our perspective will broaden avenues for future research in this area."}, {"title": "A Implementation Details", "content": "A.1 Inference\nOur method is capable of generating natural talking face videos from a single source image", "follows": "n$\\mathcal{L"}, {"perc}(I_{1": "T"}, "hat{I}_{1:T}) = \\frac{1}{T} \\sum_{t=1}^T \\sum_{i=1}^n \\lambda_{\\varphi} \\lVert \\varphi(I_t), \\varphi(\\hat{I}_t)\\rVert_1,$\n(6)\nwhere $\\lambda_{\\varphi} = 10$.\nLip-Sync Loss. To calculate the lip-sync loss, we first follow Wav2Lip (Prajwal et al. 2020) and retrain a lip synchronization discriminator on our used dataset. The discriminator takes 5 continuous lip images and the corresponding mel-spectrogram as input, and employs a visual encoder and audio encoder to extract two feature vectors (denoted by $f_v$ and $f_a$) from the cropped mouth images (denoted by $\\hat{I}_{mouth}$) and mel-spectrogram, respectively. Therefore, we obtain the cosine similarity of these two embeddings as:\n$C(f_v, f_a) = \\frac{f_v f_a}{\\text{max}(\\lVert f_v \\rVert_2 \\lVert f_a \\rVert_2, \\epsilon)} \\epsilon = 1e-7.$\n(7)\nTherefore, the lip-sync loss can be written as:\n$\\mathcal{L}_{sync} = \\frac{1}{N} \\sum_{i=1}^N log(C(f_v^i, f_a^i)),$\n(8)\nwhere i indicates i-th sample in a mini-batch and N is the batch-size.\nIn stage 1, we employ two losses to train the CoarseNet:\nmulti-scale perceptual loss and lip-sync loss. The multi-scale perceptual loss is utilized to measure the perceptual distance between warped images and ground truth. We select the resolutions of 256 \u00d7 256 and 128 \u00d7 128 for loss measurement. Due the the loss imbalance problems, we emphasize the importance of the mouth region and penalize the reconstruction errors between the generated mouth images and the ground truth frames. The total loss for CoarseNet can be written as:\n$\\mathcal{L}_{coarse} = \\mathcal{L}_{sync} + 0.5\\mathcal{L}_{perc}(\\hat{I}_{1:T}, I_{1:T}))$\n$+0.5\\mathcal{L}_{perc}(\\hat{I}_{mouth}, I_{mouth})$\n(9)\nIn stage 2, we add lip-sync discrimination loss for more accurate lip motion refinement. The total loss for RefineNet can be written as:\n$\\mathcal{L}_{refine} = \\mathcal{L}_{"]}