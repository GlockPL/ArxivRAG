{"title": "CLIPErase: Efficient Unlearning of Visual-Textual Associations in CLIP", "authors": ["Tianyu Yang", "Lisen Dai", "Zheyuan Liu", "Xiangqi Wang", "Meng Jiang", "Yapeng Tian", "Xiangliang Zhang"], "abstract": "Machine unlearning (MU) has gained significant attention as a means to remove specific data from trained models without requiring a full retraining process. While progress has been made in unimodal domains like text and image classification, unlearning in multimodal models remains relatively underexplored. In this work, we address the unique challenges of unlearning in CLIP, a prominent multimodal model that aligns visual and textual representations. We introduce CLIPErase, a novel approach that disentangles and selectively forgets both visual and textual associations, ensuring that unlearning does not compromise model performance. CLIPErase consists of three key modules: a Forgetting Module that disrupts the associations in the forget set, a Retention Module that preserves performance on the retain set, and a Consistency Module that maintains consistency with the original model. Extensive experiments on the CIFAR-100 and Flickr30K datasets across four CLIP downstream tasks demonstrate that CLIPErase effectively forgets designated associations in zero-shot tasks for multimodal samples, while preserving the model's performance on the retain set after unlearning.", "sections": [{"title": "1 Introduction", "content": "In the era of rapid advancements in pre-trained large models (Kim et al., 2021; Liu et al., 2024a; Achiam et al., 2023), ensuring these models can eliminate the influence of specific data upon request has become crucial due to ethical concerns and legal requirements like the \"right to be forgotten\" (Yao et al., 2024; Liu et al., 2024b). Machine unlearning (MU) has been recently studied as a method to remove the influence of specific data points from a trained model without the need to retrain it from scratch (Golatkar et al., 2020). While MU has made notable progress in unimodal domains such as images and text, its application in multimodal models remains relatively underexplored. Multimodal models integrate diverse data modalities such as images, text, and audio, introducing unique challenges for MU, as removing the influence of specific data requires disentangling complex relationships across different modalities. This unique challenge makes the existing MU methods for unimodal models, such as (Yao et al., 2023; Liu et al., 2022; Chundawat et al., 2023; Dou et al., 2024), insufficient or ineffective.\nOur work focuses on the unlearning problem in CLIP (Kim et al., 2021), a multimodal model pre-trained to align visual and textual concepts. CLIP has been widely used in various applications and serving as the encoder for many Vision Language Models (VLMs) and Multimodal Large Language Models (MLLMs) (Liu et al., 2024a; Li et al., 2021; Yuan et al., 2021; Zhai et al., 2022; Yang et al., 2022). Unlearning in CLIP is thus important due to increasing concerns about data privacy, security, and compliance with regulations. However, simply applying MU techniques designed for unimodal models would fail to account for how visual and textual features influence each other in CLIP. For example, if only the image of an \"apple\" is unlearned in CLIP, the text \u201capple\u201d would still remain associated with other visual representations, leading to inconsistencies. Conversely, if only the text \"apple\" is unlearned, the image could still be linked to other descriptions, causing similar issues.\nTo address these challenges, we introduce CLIPErase, which disentangles the relationships between visual and textual representations of the concept to be forgotten through three key components: (1) Forgetting Module: reduces the model's ability to associate text-image pairs in the forget set by modifying the cross-modal similarity matrix; (2) Retention Module: ensures the model's performance on the retain set remains unaffected; (3) Consistency Module: maintains consistency by penalizing deviations in the unimodal (text and image) distributions compared to the original model. Three loss functions, derived from the three modules, are jointly minimized to perform unlearning, making CLIPErase an efficient solution that eliminates the need for re-training from scratch.\nOur key contributions are as follows:\n1. We introduce CLIPErase, an innovative framework designed for unlearning pretrained CLIP models. CLIPErase efficiently removes specific multimodal data without retraining.\n2. CLIPErase uses three modules to disrupt multimodal data alignment for unlearning while preserving performance on the retain set.\n3. Our extensive experiments, including ablation studies and visualizations across multiple datasets, validate the effectiveness of CLIPErase."}, {"title": "2 Related Works", "content": "Machine Unlearning: As machine learning models continue to grow in size and their training datasets become increasingly vast and complex, the concept of MU has garnered significant attention in academic and industry (McConnon, 2024; Pedregosa and Triantafillou, 2023) to promoting AI for social welfare. MU aims to selectively remove specific information\u2014such as private data (Zhang et al., 2023), outdated knowledge (Wang et al., 2023), and harmful content (Liu et al., 2024c) from a trained model without necessitating a complete retraining from scratch (Bourtoule et al., 2019). We will next discuss these techniques in the context of text and image modalities.\nMachine Unlearning for Text: MU has been extensively studied in the text domain. Gupta et al. (2021) first explored adaptive parameter tuning, while Maini et al. (2024), Chen and Yang (2023), and Jia et al. (2024) leveraged gradient-based methods, including second-order optimization and KL-Divergence descent. Eldan and Russinovich (2023) focused on preference optimization. To address high computational costs, Kurmanji et al. (2024) proposed scalable, approximate methods, and Chen et al. (2023) aimed to reduce overhead. For theoretical guarantees, Ullah et al. (2021) employed differential privacy to enhance trust in MU.\nMachine Unlearning for Vision: In the image domain, MU techniques are applied in two primary scenarios. One is to eliminate the influence of specific visual data in classification models. For example, SRCUB (Kurmanji et al., 2024) uses a teacher-student framework to forget data in image classification tasks. The other is for diffusion models, where MU techniques focus on unlearning specific visual patterns in image generation, e.g., by modifying cross-attention mechanisms (Zhang et al., 2024; Gandikota et al., 2024, 2023; Fuchi and Takagi, 2024) and disrupting connections between visual and textual representations. Li et al. (2024) introduced a method for knowledge decoupling with datasets designed for different retention needs, while Cheng and Amiri (2023) proposed a similar multi-deletion mechanism.\nNote that although the connections between visual and textual representations may be disrupted in these studies, the focus is primarily on affecting the visual patterns rather than the text, as the task is typically centered around image generation. In contrast, our study of unlearning in CLIP aims to unlearn both visual and textual representations, ensuring that downstream tasks involving both text and image, such as retrieval, classification, and multimodal association, are fully addressed. The most relevant work to ours is a recent approach by Kravets and Namboodiri (2024) utilized Lipschitz regularization and synthetic images to simulate the forget set in zero-shot tasks, but it suffers from high computational cost and limitations imposed by synthetic data quality. In contrast, our proposed method eliminates the need for retraining or synthetic data generation, enabling efficient unlearning in CLIP while preserving model performance for real-world applications."}, {"title": "3 Preliminary", "content": null}, {"title": "3.1 CLIP", "content": "CLIP is a multimodal pre-training model developed by OpenAI with several hundred million parameters. It aligns images and their text information in a shared embedding space using an image encoder fimg and a text encoder ftxt, which convert inputs into normalized feature vectors. CLIP is trained with a contrastive loss that maximizes the similarity between matching image-text pairs and minimizes it for non-matching pairs. For a batch of N image-text pairs {(xn, xn)}n=1, the loss function is:\n$$L = \\frac{1}{N}\\sum_{n=1}^{N} -log \\frac{exp(\\frac{f_{img}(x_n) \\cdot f_{txt}(x_{x_n})}{T})}{\\sum_{k=1}^{N} exp(\\frac{f_{img}(x_n) \\cdot f_{txt}(x_{x_k})}{T})}$$ where T is a temperature parameter. This loss aims to correctly align each image with its corresponding text while distinguishing it from others."}, {"title": "3.2 Problem Definition", "content": "Let \u0398 be a pre-trained CLIP model trained on a dataset D. Upon receiving a deletion request specifying a subset of data to be removed, it is denoted as the forget set Df. The remaining data, referred to as the retain set Dr = D \u2212 Df.\nOur objective is to design an algorithm that enables the model to eliminate information related to Df without compromising CLIP's performance on Dr. We refer to this model as the unlearned model \u0398u, which is initialized with the parameters of the original model \u0398. With \u0398u, images and their corresponding text descriptions in Df no longer align. Concequently, in downstream tasks, e.g., an image retrieval task, an input image from forget set Df will no longer retrieve its associated text description, and vice versa. Meanwhile, it is crucial to maintain the model's performance on the retain set Dr, ensuring effective text and image alignment."}, {"title": "4 Proposed Method", "content": "In this section, we introduce CLIPErase, a novel machine unlearning method for the pre-trained CLIP. The overview of our proposed method is illustrated in Figure 1, where the image encoder and text encoder after unlearning are denoted as f\u2032img and f\u2032txt. The disentangled visual-textual association is illustrated by the Unlearned CLIP."}, {"title": "4.1 Forgetting Module", "content": "The pre-trained CLIP captures rich semantic relations between images and their corresponding text. To achieve unlearning, we propose a novel approach that disrupts the learned relationships between the visual and textual representations in the forget set Df. By intentionally misaligning the image and text features in Df, we ensure that the model no longer understands the cross-modal information contained in Df, thereby complying with data deletion requests without compromising performance on the retain set Dr.\nOur optimization objective for forgetting, LFM, is designed to reduce the similarity between the features of image-text pairs in the forget set. Specifically, we formulate LFM for minimization as:\n$$LFM = \\frac{1}{N_f} \\sum_{n=1}^{N_f} (f'_{img}(x_n) \\cdot f'_{txt}(x_n)),$$ where Nf is the number of samples in Df."}, {"title": "4.2 Retention Module", "content": "After disrupting the connection between the visual and textual representations of the forget set Df, this process may inadvertently impair the model's performance on the retain set Dr. This is because adjustments to the model's parameters can affect the overall cross-modal representation space, influencing the model's understanding and processing of the retain set. To address this, the retention module is designed to maintain the CLIP model's ability to correctly align images and texts in Dr during the unlearning process, by minimizing the alignment loss:\n$$L_{RM} = \\frac{1}{N_r}\\sum_{n=1}^{N_r} -log \\frac{exp(\\frac{f'_{img}(x_n) \\cdot f'_{txt} (x_{x_n})}{T})}{\\sum_{k=1}^{N_r} exp(\\frac{f'_{img}(x_n) \\cdot f'_{txt}(x_{x_k})}{T})}$$\nwhere Nr is the number of samples in Dr."}, {"title": "4.3 Consistency Module", "content": "With the different optimization objectives for the forget set Df and the retain set Dr, the CLIP model might introduce new errors or biases in its predictions on Dr during the forgetting process. To prevent unexpected changes in the model's behavior on Dr, we maintain consistency between the outputs of the unlearned model \u0398u and that of the original model \u0398 on Dr by introducing a consistency restriction LCM, defined as the Kullback-Leibler (KL) divergence between the output distributions of \u0398u and \u0398 on Dr:\n$$L_{CM} = \\frac{1}{N_r} \\sum_{n=1}^{N_r} [KL(p_{img}^{u} || p_{img}^{o}) + KL(p_{txt}^{u} || p_{txt}^{o})],$$\nwhere $p_{img}^{o}, p_{img}^{u}$ and $p_{txt}^{u}, p_{txt}^{o}$ are the distributions of one image embedding, obtained from the image encoder of the original model \u0398 and the unlearned model \u0398u, respectively. Similarly, $p_{txt}^{u}$ and $p_{txt}^{o}$ are the distributions of one text embedding, obtained from the text encoder of the original model \u0398 and the unlearned model \u0398u. These distributions are computed by applying softmax directly to the image and text embeddings.\nAt last, the overall unlearning loss is:\n$$L = \\lambda_1 L_{RM} + \\lambda_2 L_{FM} + \\lambda_3 L_{CM}$$,\nwhere \u03bb1, \u03bb2, and \u03bb3 are hyperparameters.\nMinimizing LFM primarily affects the image and text encoders by reducing the similarity between image-text pairs in Df, disrupting their alignment and ensuring the model forgets the cross-model associations. At the same time, minimizing LRM preserves the alignment attention for Dr, ensuring that the model maintains its performance on Dr. Finally, minimizing Lcm ensures consistency between the outputs of the unlearned model \u0398u and the original model \u0398 on Dr, preventing the introduction of new errors or biases in the retain set during the unlearning process."}, {"title": "5 Experiment", "content": null}, {"title": "5.1 Experiments Setting", "content": "Table 1 summarizes our experimental settings, including tasks, datasets, and evaluation metrics.\nTasks: To evaluate the effectiveness of our method, we conducted experiments on four tasks:\n1. Zero-shot Prediction: CLIP is employed to classify images based on a given category label (e.g., \"cat,\" \"dog\") without training data. The unlearned CLIP is expected to predict a mismatch between the label \"dog\" and the image of a dog if it belongs to the forget set (Df), demonstrating its ability to \"forget\" specific learned associations.\n2. Zero-shot Image Retrieval: CLIP is used to retrieve images matching a text description (e.g., \"red car\") without task-specific training. In the unlearning scenario, the unlearned CLIP should fail to retrieve images that belong to the forget set, such as cars described as \"red car\" if they are part of Df, demonstrating its ability to forget these specific multimodal associations without further training.\n3. Image Retrieval (IR): Given a text query, CLIP retrieves the top-k relevant images. The unlearned CLIP should avoid retrieving images in the forget set, demonstrating the selective forgetting capability while maintaining performance on the retain set.\n4. Text Retrieval (TR): Given an image query, CLIP retrieves the top-k relevant text descriptions. In the unlearning context, CLIP should fail to retrieve relevant text descriptions for images in the forget set.\nDatasets: For the zero-shot prediction and retrieval tasks, we use the CIFAR-100 dataset (Krizhevsky et al., 2009). CIFAR-100 is an image classification dataset containing 100 categories. It consists of a total of 60,000 images, with 50,000 used for training and 10,000 for testing. For the Image Retrieval (IR) and Text Retrieval (TR) tasks, we use the Flickr30K dataset (Bojchevski and G\u00fcnnemann, 2017). The dataset contains 31,783 images, each paired with five natural language captions. These images primarily depict daily life scenes."}, {"title": "5.2 Comparison to Prior Work", "content": "Besides the original CLIP model, we compare our method CLIPErase with commonly known unimodal MU methods when directly applied to CLIP. We omit using prior work (Kravets and Namboodiri, 2024) as a baseline because their code and detailed experimental settings were not released, making it challenging to replicate their results. Especially without access to their synthetic image generation code, it is unable to conduct CLIP unlearning in their setting.\nOriginal CLIP Model (Kim et al., 2021): We use the original CLIP model as a baseline to assess the impact of unlearning, ensuring that the model's performance on the retain set remains unaffected.\nGradient Ascent (GA) (Yao et al., 2023): This method aims to degrade the model's performance on the forget set by increasing prediction errors, forcing the model behave away from its original predictions.\nGradient Difference (GradDiff) (Liu et al., 2022): This method increases errors on the forgetting data while preserving performance on the retain set, achieving the goal of unlearning specific information without impacting retained data.\nKL Minimization (KL) (Maini et al., 2024): The method ensures consistency on the retain set by comparing the prediction distributions of the unlearned and original models, while increasing errors on the forgetting data."}, {"title": "5.3 Experiments on Cifar100 Dataset", "content": "As shown in Table 2, we compare the effectiveness of CLIPErase with several baseline methods and the original CLIP model on zero-shot prediction and retrieval tasks on CIFAR-100 dataset. CLIPErase not only preserves the robust performance of CLIP in these tasks but also achieves superior results after accurately deleting specific information. On the Forget Set, CLIPErase achieves 0% accuracy in both tasks, confirming full unlearning. While methods like CLIP+GA (Yao et al., 2023), CLIP+GradDiff (Liu et al., 2022), and CLIP+KL (Maini et al., 2024) leave residual accuracy, CLIP+ENMN (Chundawat et al., 2023) also achieves 0% accuracy but at the expense of drastically reduced retention performance, dropping to 12.46% in prediction and 17.94% in retrieval.\nIn contrast, CLIPErase excels on the Retain Set, achieving 89.41% in prediction and 91.38% in retrieval, representing a 16.56% and 17.95% improvement over the original CLIP model. This is due to CLIPErase's Retention and Consistency Modules, which not only prevent the model from forgetting retained data but also improve its alignment of visual and textual features, thus enhancing overall performance on the Retain Set. The improved performance on Dr suggests that the forgetting process in CLIPErase leads to a more focused representation of retain set, thereby optimizing the model's zero shot capabilities."}, {"title": "5.4 Experiments on Flicker30K Dataset", "content": "As shown in Table 3, we compare the effectiveness of CLIPErase with several baseline methods and the original CLIP model on Image Retrieval and Text Retrieval tasks on Flickr30K dataset. CLIPErase delivers strong performance in both image and text retrieval tasks at multimodal MU, surpassing prior methods. In image retrieval, CLIPErase significantly outperforms CLIP+GA, CLIP+GradDiff (Liu et al., 2022), and CLIP+KL) (Maini et al., 2024), with notable gains in R@1 (64.64%), R@5 (48.4%), and R@10 (58.18%). Similarly, it shows superior results in text retrieval, with a 6.56% improvement in R@10 over CLIP+KL.\nThese results highlight the effectiveness of CLIPErase's Forgetting, Retention, and Consistency Modules. The Forgetting Module successfully reduces model reliance on forgotten data, while the Retention and Consistency Modules preserve performance on the retain set, ensuring robust multimodal unlearning without retraining."}, {"title": "5.5 Ablation Studies", "content": "As shown in Table 4, we performed ablation studies on the CIFAR-100 dataset to quantitatively evaluate the impact of the Forgetting Module (FM), Retention Module (RM), and Consistency Module (CM) on the zero-shot prediction task."}, {"title": "Effectiveness of FM:", "content": "Activating the Forgetting Module led to a significant drop in accuracy on the forget set, from 86.08% to 18.57%, indicating that FM effectively disrupted the correspondence between images and texts in the forget set. However, relying solely on FM negatively impacted the retain set performance, reducing its accuracy from 72.85% to 64.12%."}, {"title": "Effectiveness of RM:", "content": "When both the Forgetting Module and Retention Module were activated, the accuracy on the retain set recovered to 73.14%, demonstrating that RM successfully protected the retain set's performance. Simultaneously, the model's accuracy on the forget set further decreased to 9.40%, showing that RM plays a critical role in balancing the task of forgetting specific data while preserving the retain set performance."}, {"title": "Effectiveness of CM:", "content": "When the Consistency Module (CM) was activated alongside FM and RM, the retain set accuracy significantly improved to 89.41%, while the accuracy on the forget set dropped to 0%. This indicates that the model successfully forgot the specified data while maintaining high performance on the retain set. Consistency Module ensures consistency throughout the unlearning process, preventing potential errors or biases introduced by the forgetting process."}, {"title": "5.6 Robustness and Utility for Our Method", "content": "Figure 2 shows experiments with CLIPErase, GradDiff, GA, and the original CLIP across various proportions of forget classes (0%, 3%, 10%, 20%, 30%) on the CIFAR-100 dataset for the zero-shot prediction task. The left plot shows the forget accuracy, where CLIPErase demonstrates a significant unlearning effect at different proportions of forget classes. However, for GradDiff and GA, forget accuracy increases as the number of forget classes grows, indicating a worsening unlearning effect. The right plot presents the retain accuracy, where CLIPErase maintains performance comparable to CLIP, particularly at lower proportions, while GA shows a sharp decline in retain accuracy as the forget proportion increases. Overall, CLIPErase exhibits strong robustness and utility, effectively balancing both forget accuracy and retain accuracy."}, {"title": "6 Visualization", "content": null}, {"title": "Attention Visualization:", "content": "In Fig. 3, we visualized the attention heatmaps of the \u201cforget set\u201d before and after the unlearning process. Using \"apple\" from CIFAR-100 as an example, we presented the original images, the heatmaps generated by the CLIP model, and the heatmaps after unlearning with CLIPErase. In the CLIP heatmaps, attention is highly focused on the object, displaying strong visual-semantic alignment. This indicates that CLIP successfully establishes a robust connection between textual and visual semantics. In contrast, after machine unlearning with CLIPErase, the heatmaps show that attention becomes more random and dispersed across each patch, no longer concentrated on the relevant object. This suggests that CLIPErase effectively disrupts the alignment between text and visual semantics for the data to be unlearned, thus achieving the intended unlearning objective."}, {"title": "Embedding Visualization:", "content": "To further investigate the impact of CLIPErase on the retain set within the cross-modal shared representation space, we conducted t-SNE visualizations (Van der Maaten and Hinton, 2008) of the textual and visual embeddings from both the original CLIP model and the CLIPErase model. Specifically, we selected 10 classes from the CIFAR-100 dataset, with the \"apple\" class serving as the forget set and the remaining classes as the retain set.\nFrom the CLIPErase results, we observed that the distance between the textual and visual embeddings of the \"apple\" class significantly increased, while the embeddings of the other classes remained tightly clustered within their respective categories. This suggests that CLIPErase effectively weakens the association between the textual and visual modalities in the forget set, with minimal impact on the modality associations of the retain set. In summary, CLIPErase can effectively decouple the connection between the textual and visual embeddings of the forget set without affecting the visual-textual associations of the retain set, thus achieving the intended goal of machine unlearning."}, {"title": "7 Discussion and Future Works", "content": "The proposed CLIPErase method has significant potential in practical applications, especially in addressing ethical and legal concerns related to harmful information and biases in multimodal datasets. In tasks like online antisemitism detection, individual components such as text or images may appear harmless on their own, but when combined, they can convey harmful messages. For example, in an image where the text \"Even grandma can see what's going on\" seems innocuous at first glance, when paired with an antisemitic image and stereotypical messaging, it transmits damaging, implicit bias. Such hidden biases are especially dangerous in multimodal data. CLIPErase caneffectively decouple these associations, to forget the harmful links between text and images, thereby mitigating the risk of perpetuating bias.\nAdditionally, CLIPErase holds great potential in safeguarding user privacy. When users request the deletion of specific personal data, CLIPErase can remove any associations between their personal information and multimodal content.\nIn the future, although our current implementation is focused on the CLIP model, the framework can be extended to any modality or multimodal pretrained model, not just CLIP. This broader applicability would enable flexible unlearning across a wide range of systems, making the approach more versatile. Additionally, we aim to apply CLIPErase to Generative AI systems, such as Multimodal Large Language Models (MLLMs), where CLIP-based encoders are widely used. By unlearning at the encoder level, CLIPErase can help address the growing challenges in Generative AI, including the generation of private, malicious, or illegal content, the continuation of biases, and even the risk of weaponizing these models. Our approach can serve as a safeguard, correcting problematic associations and enhancing user privacy, thus providing a safer and more ethical experience in the future of AI development."}, {"title": "8 Conclusion", "content": "In this paper, we present CLIPErase, a machine unlearning framework for multimodal contrastive pre-training that removes specific data while preserving performance on retained data. It integrates the Forgetting Module, Retention Module, and Consistency Module to disrupt cross-modal relationships for targeted data while maintaining retention. Extensive experiments on zero-shot prediction and retrieval tasks with CIFAR-100 and Flickr30K validate its effectiveness in unlearning and performance retention, offering practical applications for privacy preservation and data management."}, {"title": "9 Limitation", "content": "While CLIPErase achieves good unlearning performance on multimodal tasks, it may face challenges when the forgetting and retain sets share highly similar features (e.g., fine-grained categories). In such cases, disrupting cross-modal associations without affecting closely related retain set becomes more complex. In the future, we plan to introduce strategies that target specific feature subspaces or by implementing adaptive unlearning techniques that dynamically adjust the unlearning process based on the similarity between the forget and retain sets."}, {"title": "A Example Appendix", "content": "This is an appendix."}]}