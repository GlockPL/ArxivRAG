{"title": "Equivariant Action Sampling for Reinforcement Learning and Planning", "authors": ["Linfeng Zhao", "Owen Howell", "Xupeng Zhu", "Jung Yeon Park", "Zhewen Zhang", "Robin Walters", "Lawson L.S. Wong"], "abstract": "Reinforcement learning (RL) algorithms for continuous control tasks require accurate sampling-based action selection. Many tasks, such as robotic manipulation, contain inherent problem symmetries. However, correctly incorporating symmetry into sampling-based approaches remains a challenge. This work addresses the challenge of preserving symmetry in sampling-based planning and control, a key component for enhancing decision-making efficiency in RL. We introduce an action sampling approach that enforces the desired symmetry. We apply our proposed method to a coordinate regression problem and show that the symmetry aware sampling method drastically outperforms the naive sampling approach. We furthermore develop a general framework for sampling-based model-based planning with Model Predictive Path Integral (MPPI). We compare our MPPI approach with standard sampling methods on several continuous control tasks. Empirical demonstrations across multiple continuous control environments validate the effectiveness of our approach, showcasing the importance of symmetry preservation in sampling-based action selection.", "sections": [{"title": "1 Introduction", "content": "In reinforcement learning (RL) for continuous control, the need for effective sampling-based action selection is paramount. Many control environments, especially in robotic manipulation and navigation, exhibit symmetries due to their operation within Euclidean space. While previous explorations of equivariance have primarily focused on deterministic RL policies , the inherently multimodal nature of these control tasks demands sampling-based approaches. This crucial integration of symmetry into sampling-based methods remains largely under-explored.\nIn general, sampling methods will break the exact symmetries of action selection. This is an issue as the breaking of symmetry prevents the use of equivariant reinforcment learning methods . This paper addresses"}, {"title": "2 Related Work", "content": "Symmetry in Reinforcement Learning Symmetry in decision-making tasks has been studied in the context of reinforcement learning and control. Early research focused on symmetry in MDPs without function approximation , while more recent work has explored symmetry in model-free (deep) RL and imitation learning using equivariant policy networks . Park et al. [2022] investigated equivariance in learning world models. Additionally, Zhao et al. [2022a] analyzed the use of symmetry in value-based planning on a 2D grid. Our work extends"}, {"title": "3 MDPs with Geometric Structure", "content": "Markov Decision Processes (MDPs) are fundamental in modeling decision-making in interactive environments. In robotic control applications, MDPs often involve state spaces defined over Euclidean spaces Rd or on groups like SE(d). These"}, {"title": "3.1 Formulation of Geometric MDPs", "content": "state spaces might directly represent physical spaces, such as the position of a robot, or embody latent structures in sensor inputs, such as camera images.\nThe study of isometric changes, which are transformations that preserve dis- tances in the state space, introduces the Euclidean symmetry group E(d). This group and its subgroups, which can be expressed in semi-direct product form as (Rd, +) \u00d7 G, play a crucial role in how we understand and manipulate these state spaces. The group action, consisting of translations and rotations or re- flections, transforms a vector x in the space according to x \u2192 (tg) \u2022 x := gx + t . These transformations are crit- ical for defining symmetry properties in the system, which lead to more efficient problem-solving strategies \nWe define a class of MDPs with internal geometric structure, where the ground state space or a latent space of the MDP can be transformed by a Eu- clidean group. This extends a previously studied discrete case ."}, {"title": "3.2 Symmetry in Geometric MDPs", "content": "In this subsection, we explore how Euclidean symmetry influences the internal geometric structure of MDPs. The symmetry properties in MDPs are character- ized by the equivariance and invariance of the transition and reward functions, respectively :\n$\\forall g \\in G, \\forall s, a, s', P(s' | s,a) = P(g.s' | g. s,g.a) \\tag{1}$\n$\\forall g \\in G, \\forall s, a, R(s, a) = R(gs, ga) \\tag{2}$\nHere, g acts on the state and action spaces through group representations ps and pa, respectively. For instance, the standard representation pstd (g) of SO(2) assigns each rotation g \u2208 SO(2) a 2D rotation matrix R2\u00d72(g) given by:\n$R(g) = \\begin{bmatrix} cos(g) & sin(g) \\\\ -sin(g) & cos(g) \\end{bmatrix}$.\nThis matrix represents a rotation by an angle g. The trivial representation ptri (g) assigns the one-dimensional identity matrix ptri(g) = 11\u00d71 to all g.\nProperties. In a geometric MDP with a discrete symmetry group, the optimal policy mapping is G-equivariant, as demonstrated in . To incorporate symmetry constraints, one strategy is to ensure the entire"}, {"title": "3.3 Illustration and Examples of Geometric MDPs", "content": "Geometric MDP examples include moving a point robot in a 2D continuous space (R2, Example 3 in Table 1) or a discrete space (Z2, Example 1 [Tamar"}, {"title": "4 Equivariant Sampling-Based Action Selection", "content": "Many real-world reinforcement learning (RL) and imitation learning problems involve continuous actions at ~ \u03c0(\u03b1 | st). When the action space A is infinite, the policy function may need to employ stochastic sampling.\nConcretely, we focus on a two-step \u201cimplicit policy\" strategy , illustrated in Figure 1 (left), which solves the action optimization problem a* = arg min\u300f E(s, a) via sampling in two steps: (1) A neural network evaluates state-action pairs (st, at) or tra- jectories (st, at, St+1, at+1,...); (2) Online optimization is performed over the"}, {"title": "4.1 Action Selection via Sampling", "content": ""}, {"title": "4.2 Coordinate Regression Problem", "content": "In learning visuo-motor policies, a key challenge is converting high-dimensional image data into continuous action outputs. This challenge is exemplified in the coordinate regression problem , where the goal is to predict the xy coordinates of a specific target marker within an image, as shown in Figure 1 (left). We use this problem to demonstrate and study the equivariance properties of action sampling.\nIn the coordinate regression problem, the objective is to predict the (x, y) coordinate value v of a (green) marker on an image I: v* = arg maxv\u2208R2 \u0395\u03b8 (\u0399,\u03c5), which can be written as a function h(.) of the image v* = h(I).\nThe coordinate regression problem exhibits both rotation and reflection sym- metry (or g\u2208 D4 dihedral group). Specifically, if the input image is rotat- ed/flipped g. I, the network should predict the rotated/flipped coordinate value g. v: ho(g. I) = g \u00b7 ho(I) = g \u2022 v. Although simplified, the coordinate regression problem captures many fundamental challenges inherent in visuomotor control problems like robotic manipulation and control. Understanding the interplay between symmetry and sampling in this problem will help build intuition for equivariant action sampling in real-world tasks."}, {"title": "4.3 Strong and Weak Equivariance in Sampling", "content": "We can classify sampling methods that satisfy symmetry constraints as either weak equivariance or strong equivariance. Suppose that we are trying to"}, {"title": "4.4 Constructing Fully Equivariant Version of Action Sampling", "content": "Following the insight, we construct an equivariant action sampling approach based on Implicit Behavior Cloning method (IBC) , an energy-based approach that samples actions from a learned energy model. We use it to illustrate how to design a sampling-based policy that is not only weakly equivariant but also strongly equivariant in generating action samples."}, {"title": "4.5 Evaluation of Equivariant Sampling on Coordinate Regression", "content": "Setup. In our experiments, we processed images at a resolution of 96 \u00d7 96 pixels. The dataset consisted of 10 training images from either (1) the entire region [0,96] \u00d7 [0, 96] or (2) only the first quadrant [0,48] \u00d7 [0, 48]. Each image features a single red marker with randomly assigned coordinates (as shown in Fig 1), using a fixed random seed to ensure consistency across models. The coordinates"}, {"title": "5 Equivariant Sampling-based Planning Algorithm", "content": "In this section, we present an equivariant model-based RL algorithm designed for continuous action spaces, leveraging continuous symmetry through symmetric sampling. To plan in continuous spaces, we employ sampling-based methods such as MPPI , extending them to maintain equivariance.\nOur approach builds on prior work that utilized value-based planning in a discrete state space Z2 with the discrete group D4, extending these concepts to continuous domains.\nThe core idea is to ensure that the algorithm at = plan(st) produces actions that are consistent under transformations, i.e., it is G-equivariant: gat = g\u00b7 plan(st) = plan(g\u00b7 st), as illustrated in Figure 1. This principle is applicable to MDPs with various symmetry groups."}, {"title": "5.1 Components", "content": "We use TD-MPC as the foundation of our implementa- tion. Here, we introduce the procedure and demonstrating how to incorporate symmetry into sampling-based planning algorithms.\nPlanning with learned models. We utilize the MPPI (Model Predictive Path Integral) control method , as adopted in TD-MPC . We sample N trajectories with a horizon H using the learned dynamics model, with actions derived from a learned policy, and estimate the expected total return."}, {"title": "5.2 Integrating Symmetry", "content": "Zhao et al. [2022a] consider how the Bellman operator transforms under sym- metry transformation. For sampling-based methods, one needs to consider how the sampling procedure changes under symmetry transformation. Specifically, under a symmetry transformation, differently sampled trajectories must trans- form equivariantly. This is shown in Figure 1. The equivariance of the transition model in sampling-based approaches to machine learning has also been studied in . There are several components that need G-equivariance, and we discuss them step-by-step and illustrate them in Figure 1.\n1. dynamics and reward model. In the definition of symmetry in Geomet- ric MDPs (and symmetric MDPs ) in Equation 1, the transition and reward functions are G-equivariant and G-invariant respectively. Therefore, in imple- mentation, the transition network is deterministic and uses a G-equivariant MLP, and the reward network is constrained to be G-invariant. Additionally, in implementation, planning is typically performed in latent space, using a latent dynamics model f(z, a) = z'.\n2. value and policy model. The optimal value function produces a scalar for each state and is G-invariant, while the optimal policy function is G- equivariant . If we use G-equivariant transition and G-invariant reward networks in updating our value function T[Ve] = \u03a3\u03b1 Re(s, a) + \u03b3\u03a3, Po(s's, a) Vo(s'), the learned value network Ve will also satisfy the symmetry constraint. Similarly, we can extract an optimal policy from the value network, which is also G-equivariant \n3. MPC procedure. We consider equivariance in the MPC procedure in two parts: sample trajectories from the MDP using learned models, and compute their returns, return(sample(s, 0)). We discuss the invariance and equivari- ance of it in the next subsection.\nWe list the equivariance or invariance conditions that each network needs to satisfy. Alternatively, for scalar functions, we can also say they transform under trivial representation po and are thus invariant. All modules are implemented via G-steerable equivariant MLPs: Pout(g)\u00b7y = Pout(g)\u00b7MLP(x) = MLP(pin(g)\u00b7x)."}, {"title": "5.3 Equivariance of MPC", "content": "Analogous to equivariant action selection for single-step case, we constrain the underlying MPC planner to be equivariant. We use MPPI (Model Predictive Path Integral) , which has been used in TD-MPC for action selection. An MPPI procedure samples multiple H-horizon trajectories {Ti} from the current state st using the learned models. We use sample to refer to the procedure: T\u2081 = sample(st; fe, Re, Qe, \u03c0\u03b8) = (St, at, St+1, At+1,..., St+H).\nAnother procedure return computes the accumulated return, evaluating the value of a trajectory for top-k trajectories:\n$return(T) = E_{\\tau} [\\gamma^{H}Q_{\\theta}(s_{H}, a_{H}) + \\sum_{t=0}^{H-1} R_{\\theta} (s_{t}, a_{t}) ] = E_{\\tau} [U(S_{1:H}, A_{1:H-1})] \\tag{10}$\nA trajectory is transformed element-wise by g: g\u00b7 Ti = (g\u00b7st, g\u00b7at, g\u00b7 St+1, 9. at+1,...,9. St+H). However, since \u03bc and o in action sampling are not state- dependent, the MPPI sample does not exactly preserve equivariance: rotating the input does not deterministically guarantee a rotated output, similar to CEM. Thus, we can (1) constrain return to be G-invariant and (2) use G to augment the sampling of action sequence (at, ..., at+H).\nProposition 1. The return procedure is G-invariant, and the G-augmented G-sample procedure that augment A using transformation in G is G-equivariant when K = 1.\nWe further explain in Appendix E. In summary, for sampling and computing return, the sampling procedure satisfies the following conditions, indicating that the procedure return(G-sample(s, 0)) is invariant, i.e., not changed under group transformation for any g. We use return(Ti) to indicate the return of a specific trajectory Ti and g. Ti to denote group action on it.\ng.Ti ~ G-sample(g\u00b7st; fo, Re, Q\u03b8,\u03c0\u03bf) (11)\nreturn: T\u2081R: return(Ti) = return(g. Ti) (12)"}, {"title": "6 Evaluation: Sampling-Based Planning", "content": "In this section, we present the setup and results for our proposed sampling-based planning algorithm: the equivariant version of TD-MPC. Additional details and results are available in Appendix F."}, {"title": "6.1 Experimental Setup", "content": "Tasks. We verify the algorithm on a few selected and customized tasks using the DeepMind Control suite (DMC) , visualized in Figure 5. One task is a 2D particle moving in R2, PointMass. We customize tasks based on it: (1) 3D particle moving in R\u00b3 (disabled gravity), and (2) 3D N-point moving that has several particles to control simultaneously. The goal is to move particle(s) to a target position. We also experiment with tasks on a two-link arm, Reacher (easy and hard), where the goal is to move the end-effector to a random position in a plane. Reacher Easy and Hard are top-down tasks where the goal is to reach a random 2D position. If we rotate the MDP, the angle between the first and second links is not affected, i.e., it is G-invariant. The first joint and the target position are transformed under rotation, so we set it to p\u2081 standard representation (2D rotation matrices). The system has O(2) rotation and reflection symmetry, hence we use Ds and D4 groups. We also use MetaWorld tabletop manipulation . The action space is 3D gripper movement (\u2206x, \u0394y, \u2206z) and 1D openness. The state space includes (1) gripper position, (2) 3D position plus 4D quaternion of at most 2 relevant objects, and (3) 3D randomized goal position, depending on tasks. If we consider tasks with gravity, the MDP itself should exhibit SO(2) symmetry about the gravity axis. In implementation, the symmetry also depends on the data distribution, so we make the origin at the workspace center and the gripper initialized at the origin, so the task respects rotation equivariance around the origin. We add SO(2) equivariance to the algorithm about the gravity axis.\nExperimental setup. We compare against the non-equivariant version of TD- MPC . By default, we make all components equivariant as described in the algorithm section. In Sec F.3, we include ablation studies for disabling or enabling each equivariant component."}, {"title": "6.2 Results", "content": "Figures 6 (upper and lower rows) present the reward curves, demonstrating that our equivariant methods can achieve near-optimal performance 2 to 3 times faster in terms of training interaction steps for several tasks. For the default 2D PointMass task, the D8-equivariant version learns slightly faster than the non- equivariant version. In the Reacher task, as shown in the lower part of Figure 6, the D8-equivariant version significantly outperforms the non-equivariant TD- MPC, especially in the Hard domain. The D4-equivariant version also performs"}, {"title": "7 Conclusion and Discussion", "content": "This work introduces a two-step approach to preserve symmetry in sampling- based planning and control for continuous tasks. Using equivariant sampling, our method improves decision-making efficiency and performance in various control environments. Our findings highlight the benefits of integrating symmetry into sampling-based model-based RL algorithms, enhancing current practices and opening avenues for future research in continuous control and robotics applica- tions."}, {"title": "A Outline", "content": "The appendix is organized as follows: (1) additional discussion, including related work and theoretical background, (2) theory, derivation, and proofs, (3) imple- mentation details and further empirical results, and (4) additional mathematical background."}, {"title": "B Additional Discussion", "content": ""}, {"title": "B.1 Discussion: Symmetry in Decision-making", "content": "In this work, we study the Euclidean symmetry E(d) from geometric transforma- tions between reference frames. This is a specific set of symmetries that an MDP can have - isometric transformations of Euclidean space Rd, such as the distance is preserved. This can be viewed as a special case under the framework of MDP homomorphism, where symmetries relate two different MDPs via MDP homo- morphism (or more strictly, isomorphism). We refer the readers to [Ravindran and Barto, 2004] for more details. We also discuss symmetry in other related fields."}, {"title": "B.2 Limitations and Future Work", "content": "Although Euclidean symmetry group is infinite and seems huge, it does not guar- antee significant performance gain in all cases. Our theory helps us understand when such Euclidean symmetry may not be very beneficial The key issue is that when a robot has kinematic constraints, Euclidean symmetry does not change those features, which means that equivariant constraints cannot share parame- ters and reduce dimensions. We empirically show this on using local vs. global reference frame in the additional experiment in Sec F. For further work, one possibility is to explicit consider constraints while keep using global positions."}, {"title": "C Mathematical Background", "content": "We establish some notation and review some elements of group theory and repre- sentation theory. For a comprehensive review of group theory and representation theory, please see [Serre, 2005]. The identity element of any group G will be de- noted as e. We will always work over the field R unless otherwise specified."}, {"title": "C.1 Background for Representation Theory and G-steerable Kernels", "content": ""}, {"title": "C.2 Group Definition", "content": "A group is a non-empty set equipped with an associative binary operation : G\u00d7GG where satisfies\nExistence of identity: \u2203e \u2208 G, s.t. \u2200g \u2208 G, e.g=g.e=g\nExistence of inverse: \u2200g \u2208 G, \u2203g\u00af\u00b9 \u2208 G s.t. g \u2022 g\u00ae1 = g_1 . g = e\nFor a complete reference on group theory, please see Zee [2016].\nGroup Representations A group is an abstract object. Oftentimes, when working with groups, we are most interested in group representations. Let V be a vector space over C. A representation (p, V) of G is a map p : G \u2192 Hom[V, V] such that\n$\\forall g,g'\\in G, \\forall v \\in V, \\rho(g\\cdot g')v = \\rho(g) \\cdot \\rho(g')v$\nConcisely, a group representation is a embedding of a group into a set of matrices. The matrix embedding must obey the multiplication rule of the group. Over Rand Call representations break down into irreducible representations Serre [2005]. We will denote the set of irreducible representations of a group G and G.\nGroup Actions Let be a set. A group action of Gon is a map \u0424: G\u00d7\u03a9 - \u03a9 which satisfies\nIdentity: \u03b8\u03c9 \u0395 \u03a9, \u03a6(e,w) = \u025b\nCompositionality: \u220091,92 \u2208 G, \u03b8\u03c9\u2208\u03a9, \u03a6(9192,\u03c9) = \u03a6(91, \u03a6(92, \u03c9))\nWe will often suppress the function and write \u03a6(g,w) = g\u00b7\u03c9."}, {"title": "D Theory and Proofs", "content": "This section includes more insights and explanation of equivariant sampling and its benefits, as well as the equivariance properties of Geometric MDP."}, {"title": "D.1 Toy Models of Equivariant Sampling", "content": "Let us consider a toy model of equivariant sampling. This will illustrate the importance of symmetry considerations in sampling methods. Specifically, this example illustrates that if sampling is performed incorrectly, the finite sample averages will not have the same symmetries as the infinite sample average. We show how the desired symmetries can be recovered via a 'group averaging'. Let f: R\u2192R be any smooth function. Let us define the 'energy' function H: Rd \u2192 Ras\nH(x) = Ew[f(wTx)]\nwhere the random vector w ~ N(0, Ia) is drawn from a normal distribution with zero mean and identity matrix covariance Id. The random variable w is isotropic and both w and Ow are drawn from the same distribution for any orthogonal matrix O. Thus, we have that\n\u2200O\u2208O(d), H(O\u00b7 x) = Ew[f(wTOx)] = Ew[f((Ow)Tx)] = Ew[f(wx)] = H(x)\nwhere we have used the fact that the random variable w satisfies the property \u03c9 = \u039f\u03c9. Thus, H(Ox) = H(x) is a left O(d)-invariant quantity. Now, suppose that we try to approximate H(x) by random sampling. In the naive approach to estimation of H(x), we can drawn m iid sample points W1,W2, ..., Wm iid from N(0, Ia) and estimate the function H(x) as\nM\nH(x) = \\frac{1}{M} \\sum_{m=1} f(x)\nHowever, this approximation \u0124(x) does not need to satisfy the original symmetry property and \u0124(Ox) = \u0124(x) is not guaranteed to hold. Because we know that the true function H(x) is left O(d)-invariant, it seems like we should be able to"}, {"title": "D.2 Equivariant Sampling Is Always Better", "content": "Let G be a compact group. Suppose that we have an MDP with symmetry with optimal policy \u03c0*(als). Using a result of [van der Pol et al., 2020b], the optimal policy satisfies the relation \u2200g\u2208G, \u03c0*(ga | gs) = \u03c0*(a | s).\nLet us suppose that we have an learning policy \u3160, which may or may not satisfy the \u03c0(gags) = \u03c0(as) condition derived in [van der Pol et al., 2020b]. Given any policy \u03c0(als) we can always 'symmetrize' the policy by defining a new policy \u03a0\u03b1[\u03c0] : S \u2192 A defined as\n$\\Pi_{\\alpha} [\\pi](a|s) = \\int_{g \\in O(d)} dg \\pi (ga | gs)$"}, {"title": "D.3 Theorem 1: Equivariance in Geometric MDPs", "content": "Theorem 1 The Bellman operator of a GMDP is equivariant under Euclidean group E(d).\nProof. The Bellman (optimality) operator is defined as\nT[V](s) := max R(s, a) +  ds'P(s' | s, a)V(s'),\nwhere the input and output of the Bellman operator are both value function V: S\u2192 R. The theorem directly generalizes to Q-value function.\nUnder group transformation g, a feature map (field) f : X \u2192 RCout is trans- formed as:\n[Lgf] (x) = [fog\u00ae\u00b9] (x) = pout(g) \u00b7 f (g\u00af\u00b9x),\nwhere pout is the G-representation associated with output Rcout. For the scalar value map, Pout is identity, or trivial representation."}, {"title": "E Algorithm Design of Equivariant TD-MPC", "content": "We elaborate on the algorithm design in this section."}, {"title": "F Implementation Details and Additional Evaluation", "content": ""}, {"title": "F.1 Implementation Details: Equivariant TD-MPC", "content": "We mostly follow the implementation of TD-MPC . The training of TD-MPC is end-to-end, i.e., it produces trajectories with a learned dynamics and reward model and predicts the values and optimal actions for those states. It closely resembles MuZero while uses MPPI (Model Predictive Path Integral ) for con- tinuous actions instead of MCTS (Monte-Carlo tree search) for discrete actions. It inherits the drawbacks from MuZero - the dynamics model is trained only from reward signals and may collapse or experience instability on sparse-reward tasks. This is also the case for the tasks we use: PointMass and Reacher and their variants, where the objectives are to reach a goal position."}, {"title": "F.2 Experimental Details", "content": "We implement G-equivariant MLP using escnn for pol- icy, value, transition, and reward network, with 2D and 3D discrete groups. For all MLPs, we use two layers with 512 hidden units. The hidden dimension is set to be 48 for non-equivariant version, and the equivariant version is to keep the same number of free parameters, or sqrt strategy.\nFor example, for D8 group, sqrt strategy (to keep same free parameters) has number of hidden units divided by_\u221a|Ds| = \u221a16 = 4. The other strategy is to make equivariant networks' input and output be compatible with non- equivariant ones: linear strategy, which keeps same input/output dimensions (number of hidden units divided by D8 = 16).\nWe use two strategies: sqrt strategy (to keep same free parameters, number of hidden units divided by \u221a|Dg| = \u221a16 = 4) on specifying the number of hidden units, we use linear strategy that keeps same input/output dimensions (number of hidden units divided by |D8| = 16)\nThe hidden space uses regular representation, which is common for discrete equivariant network ."}, {"title": "F.3 Additional Results", "content": "Ablation on model-based vs. model-free (\u201cplanning-free\u201d). We ablate the use of planning component in equivariant version of TD-MPC, which is to justify why"}]}