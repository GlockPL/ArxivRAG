{"title": "SYMBOTUNES: UNIFIED HUB FOR SYMBOLIC MUSIC GENERATIVE MODELS", "authors": ["Pawe\u0142 Skier\u015b", "Maksymilian \u0141azarski", "Micha\u0142 Kope\u0107", "Mateusz Modrzejewski"], "abstract": "Implementations of popular symbolic music generative models often differ significantly in terms of the libraries utilized and overall project structure. Therefore, directly comparing the methods or becoming acquainted with them may present challenges. To mitigate this issue we introduce Symbotunes, an open-source unified hub for symbolic music generative models. Symbotunes contains modern Python implementations of well-known methods for symbolic music generation, as well as a unified pipeline for generating and training.", "sections": [{"title": "1. INTRODUCTION", "content": "Symbolic music models have been instrumental in advancing research on music analysis and generation, but many of the foundational models were developed using now-obsolete frameworks, such as Theano, which hinders reproducibility and continued experimentation. This also creates difficulties and barriers for researchers new to the field. We introduce Symbotunes, a unified hub for symbolic models. Symbotunes addresses the aforementioned issues by offering a framework that re-implements these symbolic models, increasing compatibility with modern machine learning practices. Designed for both researchers and educators, Symbotunes provides a standardized platform that facilitates the exploration, adaptation, and further development of symbolic music models."}, {"title": "2. DESIGN CHOICES", "content": "To manage our hub's required dependencies we use anaconda [1] package manager. We provide an environment.yml file to facilitate a reproducible environment. Notably, in our hub, we use Python 3.12, PyTorch 2.2 [2], PyTorch Lightning 2.2 [3] and MidiTok 3.0 [4]."}, {"title": "2.1 Project structure", "content": "The project is divided into four main subfolders:\n\u2022 scripts - contains Python scripts for training and sampling from the models,\n\u2022 models - contains the models currently implemented in our hub. Each model is in a separate sub-directory, which also contains an example training script,\n\u2022 data - contains all the data handling utilities available in the hub - datasets, tokenizers, and data transforms. It provides a simple, unified interface for symbolic music datasets,\n\u2022 callbacks - contains the training callbacks."}, {"title": "2.2 Configuration system", "content": "One of the key parts of Symbotunes is our custom configuration file parser. It allows the user to run different experiments with a single Python script, by simply changing the configuration file. Notably, the configs allow the users to specify the model type and its hyperparameters, datasets, and data transformations such as tokenizers, training callbacks, and training details such as the number of training steps. For the config file structure, we use yaml, due to the flexibility and readability of the format."}, {"title": "2.3 Implemented models", "content": "All the available models inherit from the abstract BaseModel class, ensuring a common interface. Currently, Symbotunes contains implementations of the following symbolic music methods:\n\u2022 Folk-RNN [5] an LSTM-based model designed to generate traditional folk music in ABC notation based on training data from a large collection of folk tunes,\n\u2022 MusicVAE [6] - a model developed by Google's Magenta team that uses a recurrent variational autoencoder (VAE) to generate and interpolate between musical sequences,\n\u2022 ABC GPT2 [7] - model utilizing the transformer architecture to generate ABC samples."}, {"title": "2.4 Datasets and data handling", "content": "To ensure interface unification, all the available datasets inherit from the abstract BaseDataset class. Currently, we provide the following music datasets:\n\u2022 LAKH dataset [8] dataset containing popular songs in MIDI format,\n\u2022 Folk RNN dataset [5] - contains folk music in ABC format retrieved for the Folk-RNN model.\nWe also provide a set of useful data transforms compatible with the datasets:\n\u2022 folk-rnn tokenizer - tokenizer for ABC format as described in [5], takes string as input and outputs tokenized sequence,\n\u2022 midi tokenizer - generic MidiTok REMI MIDI file tokenizer, takes path to MIDI file as input, outputs MidiTok TokSequence,\n\u2022 sample bars - samples n random bars (continuous, one bar after another) from the given track. Takes TokSequence as input and outputs n sampled bars or the original sequence if the track has less than n bars (as TokSequence),\n\u2022 toksequence - transforms TokSequence to PyTorch Tensor,\n\u2022 sample subsequence - samples sub-sequence of given length from the given sequence (or return the sequence if the sequence is shorter than the expected sub-sequence)."}, {"title": "2.5 Experiment logging", "content": "Symbotunes provides a convenient way of logging the experiment results, by utilizing the Weights and Biases [9] platform. Additionally, it also contains the following training callbacks:\n\u2022 setup callback - sets up log directory for the experiment. Creates a directory for model checkpoints, a directory with the experiment configuration, and saves the experiment configuration,\n\u2022 model checkpoint saves to the checkpoints the weights of the best model and the last model,\n\u2022 checkpoint every n steps - creates a model checkpoint every n training steps,\n\u2022 CUDA callback - after the first epoch, computes the maximum GPU memory usage during training."}, {"title": "3. USAGE", "content": "Symbotunes provides two main functionalities: training models from scratch and generating samples with trained models."}, {"title": "3.1 Run scripts", "content": "1. Training - By running the train.py script users can train a model from scratch. The run specification is defined in the config file passed to the script via the path flag. It is also possible to start the training from a pretrained checkpoint by using the -checkpoint flag.\n2. Generating samples Users can sample from a trained model by running the sample.py script. It requires a model config file, a compatible checkpoint, and a number of samples to generate, which can be specified with -path, -checkpoint, and -batch flags respectively. The user can also optionally provide the output path to which the samples will be saved with -out flag."}, {"title": "3.2 Modifying the config files", "content": "Each config file consists of four main sections:\n1. model: it specifies the model type and values of all of its hyperparameters,\n2. dataloaders: defines train and validation dataloaders used during the training,\n3. lightning: defines general training parameters such as the number of steps,\n4. callbacks: defines the callbacks that will be used during training.\nFor each of the models available in the hub, we provide an example configuration file. Additionally, we provide a more detailed description of the file structure in the hub's README.md."}, {"title": "4. LICENSE", "content": "We have opted to release Symbotunes under the GPL license. The source code is publicly accessible via GitHub at https://github.com/pskiers/Symbotunes. We actively encourage community involvement and contributions either by submitting pull requests or by forking the repository."}, {"title": "5. CONCLUSIONS", "content": "We present Symbotunes, a Python-based, open-source hub designed to solve compatibility and reproducibility challenges in symbolic music generative models. By reimplementing foundational models with PyTorch Lightning, Symbotunes offers a modern, standardized platform for researchers and educators to explore, adapt, and develop symbolic music models. Released under the GPL license, it invites open collaboration.\nFuture work includes adding more models, datasets, tokenizers, and features like model sample logging. Symbotunes aims to become a key resource for advancing symbolic music generation research and education."}]}