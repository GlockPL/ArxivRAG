{"title": "MAQA: Evaluating Uncertainty Quantification in LLMs Regarding Data Uncertainty", "authors": ["Yongjin Yang", "Haneul Yoo", "Hwaran Lee"], "abstract": "Although large language models (LLMs) are capable of performing various tasks, they still suffer from producing plausible but incorrect responses. To improve the reliability of LLMs, recent research has focused on uncertainty quantification to predict whether a response is correct or not. However, most uncertainty quantification methods have been evaluated on questions requiring a single clear answer, ignoring the existence of data uncertainty that arises from irreducible randomness. Instead, these methods only consider model uncertainty, which arises from a lack of knowledge. In this paper, we investigate previous uncertainty quantification methods under the presence of data uncertainty. Our contributions are two-fold: 1) proposing a new Multi-Answer Question Answering dataset, MAQA, consisting of world knowledge, mathematical reasoning, and commonsense reasoning tasks to evaluate uncertainty quantification regarding data uncertainty, and 2) assessing 5 uncertainty quantification methods of diverse white- and black-box LLMs. Our findings show that entropy and consistency-based methods estimate the model uncertainty well even under data uncertainty, while other methods for white- and black-box LLMs struggle depending on the tasks. Additionally, methods designed for white-box LLMs suffer from overconfidence in reasoning tasks compared to simple knowledge queries. We believe our observations will pave the way for future work on uncertainty quantification in realistic setting.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have demonstrated remarkable capabilities in performing diverse tasks, such as solving math problems, acquiring world knowledge, and summarizing long texts. However, these language models still suffer from mistakes"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Question and Answering Dataset with Multi Answers", "content": "Multiple question and answering datasets that include some questions requiring multiple answers instead of a single answer have been proposed. Additionally, some studies focus on finding multiple answer spans in a given paragraph. However, most of these datasets contain only a limited number of multi-answer questions or multi-answer questions arising from ambiguity, which is not the user's primary intention (see Appendix A). Furthermore, these works are limited to a single task, such as world knowledge or commonsense. Our new benchmark, built on existing datasets along with our own, covers more than 2,000 questions requiring multiple answers, spanning multiple tasks including mathematical reasoning, commonsense reasoning, and asking world knowledge."}, {"title": "2.2 Uncertainty Quantification for LLMs", "content": "Recently, uncertainty quantification has emerged as a significant task in increasing the reliability of responses from LLMs. There are two types of approaches for uncertainty quantification: white-box based and black-box based. White-box approaches are based on the assumption that we have access to the logits or hidden states of LLMs. In contrast, black-box based methods assume that we cannot access internal values, and thus use the responses of LLMs to estimate the confidence. These methods involve verbalized confidence that outputs its confidence at the response level , or sample multiple responses and calculate their consistency."}, {"title": "3 Multi Answer Dataset", "content": "In this section, we delve into our newly proposed benchmark MAQA, which consists of 2,042 question-answer pairs, with each question requiring more than one answer. Our dataset covers three different tasks: world knowledge, mathematical reasoning, and commonsense reasoning."}, {"title": "3.1 Data Collection", "content": "We generate MAQA by modifying existing benchmarks by using proprietary OpenAI LLMs, namely GPT-4-turbo, or by creating question-answer sets by the authors. We then conduct quality checks and validate the labels by ourselves. The resulting QA dataset consists of 642 world knowledge question-answer pairs, 400 mathematical reasoning pairs, and 1000 common sense pairs, totaling 2,042 closed-book, question-answer pairs that require multiple answers. Detailed information about creating MAQA, including the prompts used for LLMs at each stage, is provided in Appendix B.\nWorld Knowledge For the world knowledge evaluation set of MAQA, we modify the Natural Questions into multi-answer formats using the LLM in four stages: filtering question-answer pairs by the number of answers, filtering and rewriting pairs with LLMs, quality checking, and human filtering and annotation. Specifically, we first filter the question-answer pairs to ensure each question has more than one answer. Next, we ask LLMs to reject the pairs asking for a single answer or refine the questions to ask multi-answers. In the third stage, we perform quality checks with LLMs to filter out ambiguous questions. Finally, the authors eliminate any remaining ambiguous questions and verify the factuality and validity of each answer.\nAdditionally, to test the behavior of LLMs with extreme cases involving a large number of answers, we generate 50 additional questions that require more than 10 answers (i.e. Huge Label Sets (HLS)). The final data consists of 592 questions generated using the NQ dataset (World Knowledge NQ) and 50 questions that require a large number of answers (World Knowledge HLS), totaling 642 questions.\nMathematical Reasoning For the mathematical reasoning dataset, we generate 200 new questions that require multi-answer sets, covering diverse subjects such as algebra, geometry, graphs, linear algebra, and others. Additionally, we modify 150 GSM8k and 50 MMLU high school questions into a multi-answer format using LLMs, through the same process as creating the dataset for the world knowledge task. Finally, the authors manually annotate all the answers for these 400 questions, each requiring multiple answers."}, {"title": "3.2 Data Analysis", "content": "Table 1 presents examples and statistics of MAQA. As observed, the generated questions require multiple answers without ambiguity. The questions can ask for people, nations, numbers that satisfy the condition of the question, or the indexes of true-false questions.\nThe resulting dataset has a diverse range of answers, indicated by the average number of answers for each task and the standard deviation, which varies significantly. This makes the dataset highly suitable for analyzing data uncertainty. The final dataset comprises 2,042 QA pairs, covering three different tasks with a varying number of answers for each question. A more detailed distribution of the data is provided in Appendix C."}, {"title": "4 Experimental Setting", "content": "Through the experiments, we aim to answer three key research questions:\n\u2022 RQ1: How do white-box and black-box based uncertainty quantification methods perform in the presence of data uncertainty?\n\u2022 RQ2: How does uncertainty quantification performance vary across different tasks?\n\u2022 RQ3: Can previous uncertainty quantification methods also correlate with how many answers are correct among all ground-truth answers (i.e. recall)?"}, {"title": "4.1 Datasets", "content": "Multi-Answer Datasets For the evaluation under data uncertainty, we assess uncertainty quantification methods using our newly proposed MAQA.\nSingle-Answer Datasets To compare the effects of the multi-answer setting, we also evaluate the single QA sets with similar tasks. For world knowledge, we extract questions with a single answer from the NQ-open dataset, totaling 1288 pairs. For mathematical reasoning, we use the GSM8k dataset with 1319 pairs. For commonsense reasoning, we use the original set of StrategyQA dataset, that consists of 2290 QA pairs. Moreover, we also evaluate the uncertainty quantification methods on the mixture of single- and multi-answer datasets, for the fair comparison."}, {"title": "4.2 Uncertainty Quantification", "content": "Here, we explain the uncertainty quantification methods used for our analysis."}, {"title": "4.2.1 White-box LLMS", "content": "Multiple methods have been proposed to measure uncertainty using the internal states of white-box LLMs such as logit information. Here, we explore the most common approaches based on the probability distribution of the next token.\nMax Softmax Logit Max softmax logit or maximum probability has been widely used for measuring the confidence or uncertainty of deep neural networks. For the LLMs, let $z = (z_1, z_2,..., z_n)$ be the logit outputs by the model before normalization, where n is the vocab size. Using the maximum logit value $z_{max} = \\max z_j$, the maximum softmax logit can be expressed as $\\sigma(z_{max}) = \\frac{e^{z_{max}}}{\\sum_{i=1}^{n} e^{z_i}}$, where $\\sigma$ is a softmax operation. High values of $\\sigma(z_{max})$ suggest the model is confident in its prediction, while lower values indicate high uncertainty. In our experiments, we use the logit values of the first token of each answer, both for single- and multi- answer datasets.\nEntropy Entropy is also a popular measure for assessing the uncertainty of a model's predictions. In the context of language models, entropy quantifies the randomness in the predicted probability distribution over the possible tokens. For a probability distribution $p = (p_1,p_2,..., p_n)$ of the next token, entropy $H(p)$ is defined as $H(p) = -\\sum_{i=1}^{n} p_i \\log p_i$, where $p_i = \\sigma(z_i)$ is the probability of the i-th token. High entropy values indicate that the model's"}, {"title": "4.2.2 Black-box LLMS", "content": "As some proprietary models do not support logit information, uncertainty quantification using only the responses of LLMs has been well studied. Here, we investigate the two most popular approaches.\nVerbalized Confidence Verbalized confidence involves the model explicitly stating its confidence level in its generated response. Specifically, we ask the model to provide a single answer or multiple answers (see Appendix D), and then provide a confidence score, which is a numerical value in the range of 0-100.\nResponse Consistency Response consistency assesses uncertainty by generating multiple responses to the same prompt and analyzing the differences among them. A high degree of consistency in the responses suggests greater confidence, while diverse responses indicate higher uncertainty. Specifically, let $\\{r_1, r_2,...,r_m\\}$ be the set of responses generated by the LLMs for a given question, where m is the number of responses. The response consistency can be formulated as follows:\n$consistency = \\frac{2}{m(m - 1)} \\sum_{i=1}^{m-1} \\sum_{j=i+1}^{m} sim(r_i, r_j)$,\nwhere sim is the function that calculates similarity between two texts. We utilize an exact match for the similarity function."}, {"title": "4.3 Evaluation", "content": "Metrics for Correctness To calculate the correctness of a single answer, we use the accuracy with the exact match between the predicted answer and the ground-truth answer. For the correctness of multiple answers, we adopt three metrics. The main metric, as we aim to assess the reliability of each answer in the response, is precision. This metric calculates the proportion of correctly predicted answers out of all predicted answers, using an exact match. For further analysis, we also define recall, which is the proportion of correctly predicted answers out of all ground-truth answers. Additionally, we use the F1 score, which is the harmonic mean of precision and recall, to provide a balanced measure of the model's accuracy in predicting multiple answers. Detailed explanation of measuring correctness is in Appendix D.\nMetrics for Uncertainty Quantification High performance in uncertainty quantification indicates that the uncertainty measure can effectively predict whether the model's predictions are likely to be correct or incorrect. Note that our evaluation does not focus on calibration, which aims to predict the exact correctness score using the confidence value. We primarily use the Area Under the Receiver Operating Characteristic Curve (AUROC) for failure prediction, which provides a comprehensive evaluation of the model's ability to distinguish between correct and incorrect predictions. Results using other metrics, such as Area under the Precision-Recall Curve (AUPRC), are presented in Appendix F."}, {"title": "4.4 Implementation Details", "content": "For white-box LLMs, we use Llama3-(8b, 72b), Qwen-(7b, 72b), Zephyr-7b, Mistral-v02-7b, Mixtral-8x7b, and Gemma-7b. For black-box LLMs, we use GPT-3.5 and GPT-4, along with some white-box LLMs used as black-box LLMs. For the evaluation of white-box LLMs, we use greedy sampling with a temperature value of 1.0. For the evaluation of black-box LLMs, we sample 5 responses for each question, using a temperature value of 0.99 and top-p sampling with p equal to 0.9. We use vanilla prompting for world knowledge questions, which asks only for the answer, and Chain of Thought (CoT) prompting for reasoning tasks, as vanilla prompting significantly degrades performance. More detailed implementation details are presented in Appendix D."}, {"title": "5 Experimental Results", "content": ""}, {"title": "5.1 Uncertainty Quantification Results of White-box LLMs", "content": "Table 2 shows the AUROC scores of different uncertainty quantification methods for white-box LLMs across various tasks and models. Based on the results, we have two key observations.\nObservation 1 Data uncertainty does impact logit distributions in the world knowledge task; however, the logits\u2014particulary those represented as entropy-remain useful for predicting the factuality of responses due to internal prioritization.\nIn the world knowledge task, we observe that the AUROC scores for both the multi and all decline across almost all models and white-box methods, when compared to evaluation on the single-answer dataset. This decrease can be attributed to the data uncertainty effects, as most uncertainty quantification methods do not account for the composition of data and model uncertainties separately. Figure 2a supports this claim, showing that the maximum logit values of the correct answers decreases as the number of ground-truth answers increases.\nHowever, despite the impact of data uncertainty, logit values\u2014particularly those represented as entropy-remain useful for predicting the correctness of answers, as the average values remain higher than 60. The effectiveness of entropy indicates that LLMs prioritize a few tokens when generating each answer regardless of the number of answers. As depicted in Figure 2b, the sum of the top 5 softmax logits of the correct answers have similar values regardless of the number of ground-truth answers, making the entropy measure valid for the world knowledge task. Moreover, when LLMs are asked to provide multiple answer in an order that seems more probable and common, the AUROC scores increase, as shown in Table 4. This supports the claim the LLMs have their own internal priority of answers, making less affected by data uncertainty.\nObservation 2 In reasoning tasks, LLMs tend to be overconfident even under data uncertainty, especially after providing the first answer, decreasing the performance of uncertainty quantification.\nIn the mathematical and commonsense reasoning tasks, where LLMs need to output answers after multiple intermediate reasoning steps, we observe that the performance of uncertainty quantification based on logit values also decrease. This trend becomes more pronounced in the multi-answer case, as performance mostly decreases compared to single-answer cases."}, {"title": "5.2 Uncertainty Quantification Results of Black-Box LLMS", "content": "Table 3 shows the results of uncertainty quantification methods for black-box LLMs. We present two key observations.\nObservation 3 LLMs tend to be overconfident when verbally providing a confidence score regardless of the data uncertainty, making uncertainty quantification difficult.\nAs shown in Table 3, verbalized confidence struggles significantly, as evidenced by the low AUROC scores, especially on the commonsense reasoning task. The trend is similar for both single-answer and multi-answer datasets, as verbalized confidence is usually less affected by data uncertainty since it determines the confidence internally. The low AUROC score of verbalized confidence can be explained by overconfidence. As shown in Figure 3b, for both single-answer and multi-answer cases, the confidence is mostly distributed above 80 on a scale of 0-100. This overconfidence of LLMs aligns with the findings of for single-answer cases and is also evident for multi-answer cases.\nObservation 4 Response consistency works extremely well on all tasks, showing even better performance on multi-answer datasets.\nTable 3 shows that consistency-based methods have significantly high AUROC scores on all tasks, especially on multi-answer datasets. This implies that even with many possible answers, LLMs tend to respond similarly if they know the answers. Figure 3c shows that the consistency score is not affected by the number of true labels. Moreover, as the multi-answer approach can calculate more fine-grained similarity by the rate of overlap over all answers, it works extremely well on multi-answer datasets."}, {"title": "5.3 Recall and F1 score", "content": "To test the uncertainty quantification methods for predicting aggregated scores, we also calculate the AUROC score using recall and F1 score as true labels on the world knowledge set with the Llama3-8b model. This analysis yields a key observation:\nObservation 5 Both logit-based and response-based methods are capable of predicting the recall and fl score even when the number of answers grows huge.\nTable 5 shows the results of uncertainty quantification methods for predicting recall and F1 score. As we can see, regardless of the size of true labels, it has a high AUROC score for both recall and F1 score, with outstanding performance in response consistency. This demonstrates that uncertainty quantification methods are also related to LLMs' willingness to answer all the questions, even without explicit prompting to respond to all."}, {"title": "6 Conclusion", "content": "In this paper, we contribute to the uncertainty quantification of LLMs in two aspects: First, we propose a new benchmark, MAQA, which consists of question-answer pairs where each question requires more than two answers, ensuring data uncertainty at the question level. Second, we investigate uncertainty quantification in both white-box and black-box settings, observing the challenges previous works face in this new setting for reasoning tasks, and the superiority of entropy and consistency-based approaches for world knowledge tasks. We hope this work serves as a foundation for future research on more realistic settings for uncertainty quantification."}, {"title": "Limitations", "content": "We created a novel multi-answer dataset, MAQA, that covers three different tasks. Although we conducted a quality check, there may remain some ambiguous questions or unclear answers. Additionally, likewise many other QA datasets, our data contains answers that can take multiple forms. Despite our efforts to include all possible answers, there may be some noise in the dataset.\nIn this paper, we evaluate multiple uncertainty quantification methods for both white-box and black-box LLMs in the presence of data uncertainty. Although we have several observations, none of the methods are free of hyperparameters, such as temperature, sampling methods, etc. We believe that future work should investigate these settings further and establish guidelines for their use."}, {"title": "Ethics Statement", "content": "Hallucination, where large language models (LLMs) generate responses that appear plausible but are factually incorrect, poses a significant ethical issue. This phenomenon can lead to the dissemination of misinformation, which may cause harm by misleading users, decreasing the reliability of LLMs, and potentially influencing decision-making processes in critical areas such as healthcare, legal, and financial services. Therefore, addressing hallucinations in LLMs is crucial to ensure that AI systems operate within ethical boundaries.\nUncertainty quantification methods offer a promising approach to addressing the ethical challenges posed by hallucinations in LLMs. By estimating the confidence levels of the models' outputs, these methods can help identify and flag potentially unreliable or erroneous information. This transparency enables users to better assess the trustworthiness of AI-generated content and make informed decisions. Moreover, incorporating uncertainty quantification can guide developers in refining LLMs to reduce the occurrence of hallucinations, thereby enhancing the ethical deployment of AI technologies."}, {"title": "Acknowledgements", "content": "We would like to thank Namgyu Ho for his feedback on the outline of the paper. We also extend our gratitude to Jimin Lee for extensive discussions on dataset annotation."}, {"title": "A More Related Work", "content": ""}, {"title": "A.1 Question and Answering Datasets", "content": "To test the diverse abilities of language models, multiple open-domain question answering (ODQA) datasets have been proposed. These datasets involve the task of answering any factual question. Early benchmarks created open-ended questions based on evidence from certain Wikipedia paragraphs, framing the task as reading comprehension. Due to the development of LLMs, these QA tasks are sometimes tested using only the LLMs without evidence , assuming a deterministic single answer for each question.\nAmong these datasets, some datasets contain multi-answer question-answer pairs . Specifically, TriviaQA includes some questions with multiple answers that can be found in documents. Natural Questions (NQ) collects numerous user queries from Google, some of which require multiple answers. AmbigQA also contains multi-answer question tasks arising from ambiguity."}, {"title": "A.2 Uncertainty Decomposition for LLMs", "content": "There are also lines of research that try to decompose model uncertainty and data uncertainty for LLMs. These methods mostly involve the clarification stage to remove the data uncertainty that comes from the ambiguity of the user question. However, even though the questions are unambiguous, there are many cases where data uncertainty still exists, and users require multiple answers, motivating us to investigate uncertainty quantification under the existence of data uncertainty without clarification."}, {"title": "B Dataset Construction", "content": "In this section, we will explain in detail how each task of MAQA was created. We utilize the NQ Kwiatkowski et al., which is under the Apache 2.0 license, as well as GSM8k , MMLU , and StrategyQA , which are under the MIT license. Our benchmark MAQA will be distributed under the Apache 2.0 license."}, {"title": "B.1 World Knowledge", "content": "World Knowledge NQ As explained in Section 3, we modify the question-answering pairs so that each question requires multiple answers from the Natural Questions dataset using the OpenAI GPT-4-turbo model. Specifically, we use the following prompt template to modify the dataset (Note: some parts are skipped and marked as ..., as the original prompt is too long).\nPrompt Template for Modifying Natural Question Dataset\nYou are given a question and answer pair. For each\nquestion, there are multiple answers. You have two options:\n1) reject a pair, or 2) refine a pair.\nReject a pair with the answer \"reject\" if the ques-\ntion and answer pair contains the following features:\nThe user's intention in asking the question is to re-\nceive a single answer. Therefore, if multiple answers imply\nthe same meaning, or if there are multiple answers because\nthe question is \"ambiguous,\" you should reject this pair.\nExample 1: All the answers convey the same mean-\ning.\nIf there are conflicts between the answers.\nExample: The question below has conflicting answers.\nIf the question is time-dependent, meaning its an-\nswer can change in the future.\nOtherwise, refine the question and answer pair and return\nthem. Instructions for refining are as follows:\nProperly format the question to make it a com-\nplete sentence. Modify the question so that it clearly\nrequires multiple different answers.\nExample:\nRemove all incorrect answers\nRemove duplicated responses (\"the queen\" and \"The\nQueen\" have the same meaning).\nPlease also provide a reason for your decision on\na new line. In conclusion, your response should be formatted\nas a JSON file that belongs to one of the below two cases:\n**Now, here is the question-answer pair:**\nQuestion :\n{question}\nAnswer :\n{answer}"}, {"title": "B.2 Mathematical Reasoning", "content": "Manual Generation For 200 math questions that require multiple answers, covering algebra, graphs, linear algebra, arithmetic, and other topics, we mostly set the range, such as finding the x that satisfies the condition. Each condition is related to different domains such as arithmetic, numbers, graphs, etc.\nModify dataset using LLMs We also modify some GSM8k and MMLU questions into multi-answer format.\nYou're given a question that involves mathematical\nreasoning from the previous dataset. Your task is to\nrefine this question and create a new question-answer\npair. The refined question should be designed to require\nmultiple answers, hence the answer should be presented\nas a list containing at least three elements. Each refined\nquestion must demand a deeper level of thought and\ninvolve complex problem-solving skills that are not trivial.\nHere are illustrative examples across various areas of\nmathematics for guidance:\n1. **Example 1**\n**Original Question:** \"Jane's quiz scores were 98, 97,\n92, 85, and 93. What was her mean score?\"\n**Refined Question:** \"Jane's quiz scores were 98, 97,\n92, 85, and 93. List the integer numbers that are higher\nthan her mean score, but lower than 100.\"\n**Answer:** [94, 95, 96, 97, 98, 99]\n2. **Example 2**\n**Original Question:** \"What is the second number in\nthe row of Pascal's triangle that has 43 numbers?\"\n**Refined Question:** \"List the unique numbers in the\nrow of Pascal's triangle that has 6 numbers.\"\n**Answer:** [1, 5, 10]\n3. **Example 3**\n**Original Question:** \"How many arithmetic sequences\nof consecutive odd integers sum to 240?\"\n**Refined Question:** \"List the smallest number in of\narithmetic sequences that contain consecutive odd integers\nthat sum to 240.\"\n**Answer:** [9, 15, 23, 35, 57, 119]\nIf the given question seems too challenging to refine, you\nmay generate a new, simpler question that necessitates\nmore than four answers.\nNow here is the question that you need to refine"}, {"title": "B.3 Commonsense Reasoning", "content": "For commonsense reasoning, we adapt the StrategyQA dataset, which includes multiple true-false questions that require a reasoning process to answer. To create a question that needs multiple answers from each true-false question, we design a task that asks for the indexes of all questions with true answers, given multiple questions with answers that are either true or false."}, {"title": "C Dataset Distribution", "content": "Table 6 shows the detailed statistics of our proposed MAQA benchmark. As observed, our benchmark covers a wide range of categories for world knowledge, including history, sports, geography, and others. Additionally, we cover multiple problems for mathematical reasoning, involving arithmetic, algebra, graph theory, and others, making MAQA a strong benchmark that can test a wide range of domains and tasks."}, {"title": "D Implementation Details", "content": "Inference For white-box LLM experiments, we use greedy sampling with a temperature value of 1.0 to get the normalized probability logit. For all models, we use float 16 precision to save memory. All our experiments are conducted on 4 NVIDIA A100 GPUs. For the black-box LLMs, we utilize white-box LLMs such as Llama3-8b, Qwen1.5-7b, and Mistral-v02, and additionally OpenAI GPT-3.5 and GPT-4 using the OpenAI API. We adopt top-p sampling with p equal to 0.9.\nFor the prompting method, we use vanilla prompting for world knowledge, as shown below:\nInstruction: Given a question that has multiple answers,\nanswer the question following the instructions below:\n1. Keep your response as brief as possible without\nany explanation.\n2. Mark each answer with a number followed by a period.\n3. Separate each answer with a number, a comma and a\nspace.\nThe format of the answer should be given as fol-\nlows:\n1. Your Answer1, 2. Your Answer2, 3. YourAnswer3\nNow, please answer this question.\nInput :\nQuestion:\n{ question }\nFor reasoning tasks, we employ CoT prompting. To make LLMs to do CoT reasonings, we add the instruction of \"explain step-by-step\", as follows:\nInstruction: Given a question that has multiple answers,\nanswer the question following the instructions below:\n1. Explain step-by-step, and then provide your an-\nswer.\n2. When providing an answer, use the format ||ANSWERS||\nwhere ANSWERS are the answers to the given question.\n3. Separate each answer of ANSWERS with a comma and\na space.\nThe format of the final answer should be given as\nfollows:\n||ANSWER1, ANSWER2, ANSWER3||\nNow, please answer this question.\nInput :\nQuestion:\n{question}\nAnswer :\nMoreover, to calculate the verbalize confidence, we follow the instruction format of Xiong et al. , displayed as below:"}, {"title": "E Qualitative Results", "content": "Figure 6 shows examples of token probability distributions when LLMs are asked a multi-answer question. As the question has multiple answers, including \"Italy,\" \"Spain,\" and \"Greece,\" we can clearly see that the token probability is distributed among the answers, which could be interpreted as data uncertainty. However, the probability of the fifth token remains below 0.001, implying an internal priority within the LLMs.\nTable 7 presents examples of using response consistency for uncertainty quantification. As observed, when LLMs demonstrate higher consistency scores, they typically show high correctness, which corresponds to high precision in this case. However, this could be attributed to the fact that the answers are in a short format, allowing for fine-grained evaluation of consistency."}, {"title": "F Additional Results", "content": "Model Capability and Uncertainty Quantification Figure 4 shows the correlation between the precision score, defined as the correctness of the LLMs, and the uncertainty quantification using white-box-based methods. The results demonstrate that there is little correlation with the world knowledge dataset, a higher correlation with the commonsense dataset, and no significant correlation with the mathematical reasoning dataset. This indicates"}]}