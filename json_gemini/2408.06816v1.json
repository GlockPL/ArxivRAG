{"title": "MAQA: Evaluating Uncertainty Quantification in LLMs Regarding Data Uncertainty", "authors": ["Yongjin Yang", "Haneul Yoo", "Hwaran Lee"], "abstract": "Although large language models (LLMs) are capable of performing various tasks, they still suffer from producing plausible but incorrect responses. To improve the reliability of LLMs, recent research has focused on uncertainty quantification to predict whether a response is correct or not. However, most uncertainty quantification methods have been evaluated on questions requiring a single clear answer, ignoring the existence of data uncertainty that arises from irreducible randomness. Instead, these methods only consider model uncertainty, which arises from a lack of knowledge. In this paper, we investigate previous uncertainty quantification methods under the presence of data uncertainty. Our contributions are two-fold: 1) proposing a new Multi-Answer Question Answering dataset, MAQA, consisting of world knowledge, mathematical reasoning, and commonsense reasoning tasks to evaluate uncertainty quantification regarding data uncertainty, and 2) assessing 5 uncertainty quantification methods of diverse white- and black-box LLMs. Our findings show that entropy and consistency-based methods estimate the model uncertainty well even under data uncertainty, while other methods for white- and black-box LLMs struggle depending on the tasks. Additionally, methods designed for white-box LLMs suffer from overconfidence in reasoning tasks compared to simple knowledge queries. We believe our observations will pave the way for future work on uncertainty quantification in realistic setting.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have demonstrated remarkable capabilities in performing diverse tasks, such as solving math problems, acquiring world knowledge, and summarizing long texts. However, these language models still suffer from mistakes"}, {"title": "2 Related Work", "content": "Multiple question and answering datasets that include some questions requiring multiple answers instead of a single answer have been proposed. Additionally, some studies focus on finding multiple answer spans in a given paragraph. However, most of these datasets contain only a limited number of multi-answer questions or multi-answer questions arising from ambiguity, which is not the user's primary intention (see Appendix A). Furthermore, these works are limited to a single task, such as world knowledge or commonsense. Our new benchmark, built on existing datasets along with our own, covers more than 2,000 questions requiring multiple answers, spanning multiple tasks including mathematical reasoning, commonsense reasoning, and asking world knowledge."}, {"title": "2.1 Question and Answering Dataset with Multi Answers", "content": "Recently, uncertainty quantification has emerged as a significant task in increasing the reliability of responses from LLMs. There are two types of approaches for uncertainty quantification: white-box based and black-box based. White-box approaches are based on the assumption that we have access to the logits or hidden states of LLMs. In contrast, black-box based methods assume that we cannot access internal values, and thus use the responses of LLMs to estimate the confidence. These methods involve verbalized confidence that outputs its confidence at the response level, or sample multiple responses and calculate their consistency."}, {"title": "2.2 Uncertainty Quantification for LLMs", "content": "In this section, we delve into our newly proposed benchmark MAQA, which consists of 2,042 question-answer pairs, with each question requiring more than one answer. Our dataset covers three different tasks: world knowledge, mathematical reasoning, and commonsense reasoning."}, {"title": "3 Multi Answer Dataset", "content": "We generate MAQA by modifying existing benchmarks by using proprietary OpenAI LLMs, namely GPT-4-turbo, or by creating question-answer sets by the authors. We then conduct quality checks and validate the labels by ourselves. The resulting QA dataset consists of 642 world knowledge question-answer pairs, 400 mathematical reasoning pairs, and 1000 common sense pairs, totaling 2,042 closed-book, question-answer pairs that require multiple answers. Detailed information about creating MAQA, including the prompts used for LLMs at each stage, is provided in Appendix B."}, {"title": "3.1 Data Collection", "content": "For the world knowledge evaluation set of MAQA, we modify the Natural Questions into multi-answer formats using the LLM in four stages: filtering question-answer pairs by the number of answers, filtering and rewriting pairs with LLMs, quality checking, and human filtering and annotation. Specifically, we first filter the question-answer pairs to ensure each question has more than one answer. Next, we ask LLMs to reject the pairs asking for a single answer or refine the questions to ask multi-answers. In the third stage, we perform quality checks with LLMs to filter out ambiguous questions. Finally, the authors eliminate any remaining ambiguous questions and verify the factuality and validity of each answer.\nAdditionally, to test the behavior of LLMs with extreme cases involving a large number of answers, we generate 50 additional questions that require more than 10 answers (i.e. Huge Label Sets (HLS)). The final data consists of 592 questions generated using the NQ dataset (World Knowledge NQ) and 50 questions that require a large number of answers (World Knowledge HLS), totaling 642 questions."}, {"title": "World Knowledge", "content": "For the mathematical reasoning dataset, we generate 200 new questions that require multi-answer sets, covering diverse subjects such as algebra, geometry, graphs, linear algebra, and others. Additionally, we modify 150 GSM8k questions and 50 MMLU high school questions into a multi-answer format using LLMs, through the same process as creating the dataset for the world knowledge task. Finally, the authors manually annotate all the answers for these 400 questions, each requiring multiple answers."}, {"title": "Mathematical Reasoning", "content": "For commonsense reasoning, we modify the StrategyQA dataset, which consists of true-false questions that require a reasoning process to answer the question. We reformulate a multi-answer question by presenting multiple true-false questions from Strategy QA and requiring the selection of questions with true answers (i.e., true statements). The answers are formatted as a list of question indexes. Specifically, from the StrategyQA dataset, we randomly select questions within the range of 5-15, including at least 2 true and 2 false answers, to form a single question-answers pair. The process is repeated to create a total of 1000 pairs."}, {"title": "Commonsense Reasoning", "content": "The resulting dataset has a diverse range of answers, indicated by the average number of answers for each task and the standard deviation, which varies significantly. This makes the dataset highly suitable for analyzing data uncertainty. The final dataset comprises 2,042 QA pairs, covering three different tasks with a varying number of answers for each question. A more detailed distribution of the data is provided in Appendix C."}, {"title": "3.2 Data Analysis", "content": "Through the experiments, we aim to answer three key research questions:"}, {"title": "4 Experimental Setting", "content": "For the evaluation under data uncertainty, we assess uncertainty quantification methods using our newly proposed MAQA."}, {"title": "4.1 Datasets", "content": "Here, we explain the uncertainty quantification methods used for our analysis."}, {"title": "Multi-Answer Datasets", "content": "Multiple methods have been proposed to measure uncertainty using the internal states of white-box LLMs such as logit information. Here, we explore the most common approaches based on the probability distribution of the next token."}, {"title": "4.2 Uncertainty Quantification", "content": "Max softmax logit or maximum probability has been widely used for measuring the confidence or uncertainty of deep neural networks. For the LLMs, let $z = (z_1, z_2,..., z_n)$ be the logit outputs by the model before normalization, where n is the vocab size. Using the maximum logit value $z_{max} = max z_j$, the maximum softmax logit can be expressed as $\\sigma(z_{max}) = \\frac{e^{z_{max}}}{\\sum_{i=1}^{n} e^{z_i}}$, where $\\sigma$ is a softmax operation. High values of $\\sigma(z_{max})$ suggest the model is confident in its prediction, while lower values indicate high uncertainty. In our experiments, we use the logit values of the first token of each answer, both for single- and multi- answer datasets."}, {"title": "4.2.1 White-box LLMS", "content": "Entropy is also a popular measure for assessing the uncertainty of a model's predictions. In the context of language models, entropy quantifies the randomness in the predicted probability distribution over the possible tokens. For a probability distribution $P = (p_1,p_2,..., p_n)$ of the next token, entropy $H(p)$ is defined as $H(p) = - \\sum_{i=1}^n p_i \\log p_i$, where $p_i = \\sigma(z_i)$ is the probability of the i-th token. High entropy values indicate that the model's"}, {"title": "Max Softmax Logit", "content": "Softmax logit margin can also be used to measure the model's uncertainty, which is defined as the difference between the largest and the second largest logits. Let $p_{max1} = max(p_1, p_2, ..., p_n)$ be the largest logit and $p_{max2} = max({p_i} \\setminus {p_{max1}})$ be the second largest logit. The margin M is then given by $M = p_{max1} - p_{max2}$. A larger margin between the top tokens indicates lower uncertainty, while a smaller margin suggests higher uncertainty. Likewise the other methods, we use the logit values of the first token of each answer."}, {"title": "Entropy", "content": "As some proprietary models do not support logit information, uncertainty quantification using only the responses of LLMs has been well studied. Here, we investigate the two most popular approaches."}, {"title": "4.2.2 Black-box LLMS", "content": "Verbalized confidence involves the model explicitly stating its confidence level in its generated response. Specifically, we ask the model to provide a single answer or multiple answers (see Appendix D), and then provide a confidence score, which is a numerical value in the range of 0-100."}, {"title": "Margin", "content": "Response consistency assesses uncertainty by generating multiple responses to the same prompt and analyzing the differences among them. A high degree of consistency in the responses suggests greater confidence, while diverse responses indicate higher uncertainty. Specifically, let ${r_1,r_2,...,r_m}$ be the set of responses generated by the LLMs for a given question, where m is the number of responses. The response consistency can be formulated as follows:\n$consistency = \\frac{2}{m(m - 1)} \\sum_{i=1}^{m-1} \\sum_{j=i+1}^m sim(r_i, r_j)$,\nwhere sim is the function that calculates similarity between two texts. We utilize an exact match for the similarity function."}, {"title": "Verbalized Confidence", "content": "To calculate the correctness of a single answer, we use the accuracy with the exact match between the predicted answer"}, {"title": "Response Consistency", "content": "AUROC scores increase, as shown in Table 4. This supports the claim the LLMs have their own internal priority of answers, making less affected by data uncertainty."}, {"title": "4.3 Evaluation", "content": "The Ethics Statement is missing from this article parse. Add this to a final Section and complete."}, {"content": "For the evaluation of black-box methods, since it is impossible to calculate the uncertainty for each answer, we use a threshold for the precision score. Specifically, we set the prediction label as 1 if the precision score is higher than 0.5, as setting the threshold at 1.0 results in too low accuracy, making the evaluation of uncertainty quantification difficult."}, {"title": "Metrics for Correctness", "content": "AUROC scores increase, as shown in Table 4. This supports the claim the LLMs have their own internal priority of answers, making less affected by data uncertainty."}, {"title": "4.4 Implementation Details", "content": "Observations: In reasoning tasks, LLMs tend to be overconfident even under data uncertainty, especially after providing the first answer, decreasing the performance of uncertainty quantification.\nIn the mathematical and commonsense reasoning tasks, where LLMs need to output answers after multiple intermediate reasoning steps, we observe that the performance of uncertainty quantification based on logit values also decrease. This trend becomes more pronounced in the multi-answer case, as performance mostly decreases compared to single-answer cases."}, {"title": "5 Experimental Results", "content": "Hallucination, where large language models (LLMs) generate responses that appear plausible but are factually incorrect, poses a significant ethical issue. This phenomenon can lead to the dissemination of misinformation, which may cause harm by misleading users, decreasing the reliability of LLMs, and potentially influencing decision-making processes in critical areas such as healthcare, legal, and financial services. Therefore, addressing hallucinations in LLMs is crucial to ensure that AI systems operate within ethical boundaries."}, {"title": "5.1 Uncertainty Quantification Results of White-box LLMs", "content": "To test the diverse abilities of language models, multiple open-domain question answering (ODQA) datasets have been proposed. These datasets involve the task of answering any factual question. Early benchmarks created open-ended questions based on evidence from certain Wikipedia paragraphs, framing the task as reading comprehension. Due to the development of LLMs, these QA tasks are sometimes tested using only the LLMs without evidence, assuming a deterministic single answer for each question."}, {"title": "5.2 Uncertainty Quantification Results of Black-Box LLMS", "content": "We created a novel multi-answer dataset, MAQA, that covers three different tasks. Although we conducted a quality check, there may remain some ambiguous questions or unclear answers. Additionally, likewise many other QA datasets, our data contains answers that can take multiple forms. Despite our efforts to include all possible answers, there may be some noise in the dataset.\nIn this paper, we evaluate multiple uncertainty quantification methods for both white-box and black-box LLMs in the presence of data uncertainty. Although we have several observations, none of the methods are free of hyperparameters, such as temperature, sampling methods, etc. We believe that future work should investigate these settings further and establish guidelines for their use."}, {"title": "5.3 Recall and F1 score", "content": "Different Metric"}, {"title": "6 Conclusion", "content": "Uncertainty quantification methods offer a promising approach to addressing the ethical challenges posed by hallucinations in LLMs. By estimating the confidence levels of the models' outputs, these methods can help identify and flag potentially unreliable or erroneous information. This transparency enables users to better assess the trustworthiness of AI-generated content and make informed decisions. Moreover, incorporating uncertainty quantification can guide developers in refining LLMs to reduce the occurrence of hallucinations, thereby enhancing the ethical deployment of AI technologies."}, {"title": "Limitations", "content": "In this paper, we contribute to the uncertainty quantification of LLMs in two aspects: First, we propose a new benchmark, MAQA, which consists of question-answer pairs where each question requires more than two answers, ensuring data uncertainty at the question level. Second, we investigate uncertainty quantification in both white-box and black-box settings, observing the challenges previous works face in this new setting for reasoning tasks, and the superiority of entropy and consistency-based approaches for world knowledge tasks. We hope this work serves as a foundation for future research on more realistic settings for uncertainty quantification."}, {"title": "Ethics Statement", "content": "For the evaluation of black-box methods, since it is impossible to calculate the uncertainty for each answer, we use a threshold for the precision score. Specifically, we set the prediction label as 1 if the precision score is higher than 0.5, as setting the threshold at 1.0 results in too low accuracy, making the evaluation of uncertainty quantification difficult."}]}