{"title": "The Literature Review Network: An Explainable Artificial Intelligence for Systematic Literature Reviews, Meta-analyses, and Method Development.", "authors": ["Joshua Morriss", "Tod Brindle", "Jessica Bah R\u00f6sman", "Daniel Reibsamen", "Andreas Enz"], "abstract": "Systematic literature reviews are the highest quality of evidence in research. However, the review process is hindered by significant resource and data constraints. The Literature Review Network (LRN) is the first of its kind explainable AI platform adhering to PRISMA 2020 standards, designed to automate the entire literature review process. LRN was evaluated in the domain of surgical glove practices using 3 search strings developed by experts to query PubMed. A non-expert trained all LRN models. Performance was benchmarked against an expert manual review. Explainability and performance metrics assessed LRN's ability to replicate the experts' review. Concordance was measured with the Jaccard index and confusion matrices. Researchers were blinded to the other's results until study completion. Overlapping studies were integrated into an LRN-generated systematic review. LRN models demonstrated superior classification accuracy without expert training, achieving 84.78% and 85.71% accuracy. The highest performance model achieved high interrater reliability (\u043a = 0.4953) and explainability metrics, linking 'reduce', 'accident', and 'sharp' with 'double-gloving'. Another LRN model covered 91.51% of the relevant literature despite diverging from the non-expert's judgments (\u03ba = 0.2174), with the terms 'latex', 'double' (gloves), and 'indication'. LRN outperformed the manual review (19,920 minutes over 11 months), reducing the entire process to 288.6 minutes over 5 days. This study demonstrates that explainable Al does not require expert training to successfully conduct PRISMA-compliant systematic literature reviews like an expert. LRN summarized the results of surgical glove studies and identified themes that were nearly identical to the clinical researchers' findings. Explainable AI can accurately expedite our understanding of clinical practices, potentially revolutionizing healthcare research.", "sections": [{"title": "1. Introduction", "content": "During surgery, the aseptic barrier exists as the primary method to protect the operating room personnel from the patient and the patient from the clinical team. As part of what is known today as personal protective equipment (PPE), gloves provide the aseptic barrier and protection of the hands from potentially infectious materials being cross contaminated within the surgical field. However, despite the innovations seen in surgical glove raw materials and manufacturing, surgeons and nurses have known that glove damage during surgery occurs[1], [2].In fact, for over seven decades clinicians have reported and studied glove damage in surgery and have described from large, visible tears to microperforations, not visible to the human eye[3], [4], [5].\nNothing is more feared in surgical practice than the surgical site infection. The increase in morbidity and mortality for the patient, in addition to the increases in costs for the health care system, are well described in the literature. While glove damage during surgery is nearly impossible to link as a direct cause of surgical site infection due to a extensive list of confounders, the risk of infection causes surgical teams to take every precaution to decrease infection. Additionally, provider safety from contracting infectious disease was highlighted during historical pandemics such as HIV-Aids in the 1980s[6], [7] and recently with SARS-COV-19[8], [9], [10]. Additionally, more common viral pathogen such as hepatitis B and C are inherent risks for the reported 400,000 sharp injuries estimated per year[11]. Therefore, research such literature reviews are needed to understand the incidence, prevalence, time-lapse and potential causes of glove damage during surgery to improve the safety of the provider and patient.\nSystematic literature reviews (SLRs) are the gold standard in clinical and preclinical research, informing public policy, clinical guidelines, and R&D for medical devices and pharmaceuticals[12], [13]. In clinical practice, the use of systematic reviews and metanalysis guide the development of clinical practice guidelines, which direct practice change to achieve the best outcomes. Despite their importance, producing SLRs is challenging due to the sheer volume of research published annually, estimated at >1 million studies, leading to reporting biases and gaps in evidence[14], [15]. Furthermore, SLRs incur substantial financial burdens, with an average expenditure of approximately $141,194.80 per SLR, and a significant time investment of approximately 1.72 years per researcher[16]. Advances in natural language processing (NLP) and machine learning (ML), alongside web-based large language model (LLM) and artificial intelligence (AI) platforms, have been proposed to streamline the SLR process. These technologies aim to enhance data extraction and text classification, but they often fall short in automation, continuous updateability, and particularly in explainability\u2014the ability for humans to understand and trust the decisions made by an AI to achieve its outputs[17], [18]. Current NLP-ML and AI solutions lack automation and explainability, and do not meet the rigorous standards of high-quality research frameworks like Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) 2020[19]. This apparent lack of transparent and explainable processes in NLP-ML and AI applications for SLRs has notably contributed to hesitancy in their widespread adoption for clinical and preclinical research."}, {"title": "2. Methods", "content": "Addressing these concerns is the Literature Review Network (LRN), an innovative explainable AI (XAI) platform designed for SLRs, meta-analyses, and real-world data research. This study evaluated LRN's effectiveness by comparing its accuracy and reliability to a traditional, human-conducted SLR. Specifically, we assessed LRN's ability to achieve high overall model accuracy and interrater reliability, measured by Cohen's kappa, with fewer iterations. To evaluate this XAI's accuracy, our reference was a human SLR conducted by three subject matter expert (SME) researchers and six reviewers focusing on surgical glove damage and change frequency, completed without NLP-ML or AI assistance. This manual SLR was part of a larger study including 4 SLRs conducted in parallel, with the objective being to determine the best available evidence to describe four key fundamental principles of surgical gloving practice: glove fit, double gloving, puncture indication, and glove change frequency. Additionally, the previous study queried clinical literature with multiple search strings and documented different search strategies. Therefore, the second aim of this study was to determine if an XAI could streamline the SLR process by finding similar insights on surgical gloving practice with fewer searches compared to conventional methods. This comparison aims to determine whether LRN can streamline the SLR process, achieving comparable insights with fewer searches and in less time. Lastly, this study aimed to evaluate an Al's capability to summarize the literature with little to no instruction by an SME, approximating themes like those identified by SMEs. A qualitative assessment of the LRN-generated SLR was done by this study's surgical glove SMEs (TB, AE, JBR) to determine the similarities and differences between the human-identified and XAI-identified themes."}, {"title": "2.1 Search Strategy, Data Extraction, Review Procedures", "content": "Based on the protocol and PICOT framework used for the human SLR, three human SMEs (TB, AE, JBR) initially formulated three separate search query strings that were used in this study. These search strings were then converted into LRN queries, as recorded in Table 1, and a set of concept rules which served as the basis for LRN's reinforcement learning. Search strings were subjected to LRN version 2.0 (LRN v2.0), which employed a word embedding model that mapped concepts with the Unified Medical Language System (UMLS) Metathesaurus[20]. LRN processed each of these unique queries independently as three LRN models. These LRN models were configured to query the PubMed database for relevant literature, utilizing the PubMed API for data retrieval. Of the 262 studies that were identified and included in the manual SLR by the SMEs, the PubMed ID (PMID) was retrieved for only 212 studies. LRN's screening mechanism involved the automatic ineligibility of records that either lacked an abstract, were published in Russian or Chinese, or were identified as duplicates. Russian and Chinese studies were excluded due to the former being a low-resource language, while the later presented complications related to accurate word segmentation with LRN[21]. Records that met the exclusion criteria were also automatically excluded by LRN (Table 1), and composed an out-of-domain or negative dataset to train LRN's discriminative algorithms[22]. Levels of evidence considered were randomized controlled trials, cohort studies, observational studies (retrospective and prospective), quasi-experimental studies, SLRs, and meta-analyses.\nLRN v2.0 operated within a reinforcement learning with human feedback (RLHF) framework. A non-SME in surgical gloving practice (JM) was responsible for training these three LRN models. When configuring this model and deriving the initial concept ruleset, the non-SME (JM) adhered to the protocol from the manual SLRs. The non-SME began the iterative learning process by first defining language rules associated with the INCLUDE and EXCLUDE classes (Table 2). Studies were then classified as either INCLUDE or EXCLUDE based on the non-SME's feedback and LRN's decisions. A RLHF loop initiated where LRN presented its findings with 20 labeled records. Per each iteration, the non-SME reviewed the performance metrics, word cloud, and correlation and coverage tables to assess LRN's associations. Based on these associations, the non-SME modified the concept ruleset to add or remove rules. The non-SME then screened each record's title and abstract, and provided feedback by assigning a label (e.g., INCLUDE or EXCLUDE). This assigned label was compared with LRN's predicted classification label. For this literature review, each LRN model was trained for 4 total iterations, or 3 RLHF iterations."}, {"title": "2.2 Explainable Artificial Intelligence Framework for Research", "content": "LRN utilized a combination of a metaheuristic wrapper, weak supervision models, and discriminative algorithms. LRN first extracted natural language features from the corpus, with the wrapper optimizing feature selection for semantic analysis considering user-defined language rules. This metaheuristic wrapper feature selection technique reduced the complex feature space inherent to natural language data[23]. Weak supervision models in LRN operated under a matrix completion methodology and generated several rudimentary models from the concept ruleset. These models, despite their inaccuracies, can effectively label unstructured literature[24].\nDiscriminative models refined labels by analyzing consensus and discrepancies among weak models, effectively handling correlated labels from weak supervision sources without requiring labeled data[24]. Performance metrics of recall, precision, and F-score were automatically calculated by LRN for each label, in addition to the overall model accuracy and Cohen's kappa. Additionally, LRN calculated potential scores for each record to balance exploration of new linguistic models and exploitation of established models and data structures for classification[25]. During RLHF, LRN presented 20 records with the highest potential score for feedback. Data visualizations and correlation tables were produced to clarify literature screening decisions for each iteration. The relationships between generative AI parameters were quantitatively assessed using Pearson's chi-squared test, adjusted by Cramer's V, and corrected for significance using the Benjamini-Hochberg method[26], [27], [28]. A LRN \"AI Package Insert\" for each search string documented all metrics and model decision-making processes. The highest performance model was identified by superior Cohen's kappa and accuracy, while the optimally balanced model was determined by the optimal balance of true positives to false negatives across search strings and iterations."}, {"title": "2.3 Analysis of LRN Search Alignment with SME Review", "content": "To critically appraise each search strategy and determine if LRN could effectively streamline the SLR process, the similarity was assessed between the three search strings and the SME-curated library. The unique reports from all three search strings were first pooled into a single corpus and deduplicated. The model with the highest Cohen's kappa and overall accuracy from each search string was then used to classify the entire corpus. Non-SME feedback from all the models, which were the non-SME assigned labels for unique records, was incorporated into the dataset for classification by each string's optimal model. PMIDs were used as the identifiers for comparing the SME-curated library and the LRN model classifications. Three sets of PMIDs corresponded with the three optimal models from each search string. Concordance between each LRN model's predictions and the SME selections was done through the calculation of a Jaccard index for quantifying the overlap between studies classified as \u201cINCLUDE\u201d by each LRN model and those identified in the manual SLR[29]. Bootstrapping for 1 million replications was performed to determine the significance level of the similarity between the sets of PMIDs and the SME library. A Jaccard index was also calculated comparing each search string against the others to determine if there was significant overlap between, for example, search string 1 and search string 2, search string 1 and search string 3, and search string 2 and search string 3. P-values were adjusted using the Benjamini-Hochberg method[26]. Confusion matrices further explored the performance of the optimal models for each string in classifying the literature, providing coverage statistics on true positive (INCLUDE) and true negative (EXCLUDE) studies, as well as false positive and false negative cases."}, {"title": "2.4 Text Summarization via LRN written SLRS", "content": "In alignment with PRISMA 2020 guidelines, the optimally balanced model across all search strings was applied across the three search strings to classify the surgical glove corpus. This model automatically generated a PRISMA 2020 flow diagram detailing study identification, screening, and inclusion processes[30]. Classified \u2018INCLUDE\u2019studies were prepared for summarization using an LLM that was based on OpenAI's GPT-4-turbo[31]. LRN identified, labeled, and embedded full-text reports, which were then subjected to a series of user questions (Table 3). Full-text reports were formatted and embedded for further processing, utilizing Langchain's Retrieval-Augmented Generation (RAG) technique to select relevant token segments, likelihood of data leakage and hallucinations was reduced[32]. LRN's LLM merged its outputs into a single document, encompassing a LRN-generated SLR. Time to complete draft metric for the manual SLR quantified the cumulative human labor hours required for identifying, screening, assessing eligibility, and incorporating full-text reports. In contrast, this metric for the LRN models considered the total human labor hours needed to configure the three models (iteration 1), complete the RLHF iterations for each model (iterations 2+), and the computation times for each LRN model to complete an iteration. Lastly, the three SMEs (TB, AE, JBR) reviewed the LRN-generated SLR to determine thematic similarities and differences."}, {"title": "3. Results", "content": ""}, {"title": "3.1 LRN Model Performance Metrics", "content": "Optimal performance was achieved with search string 3, iteration 3, which yielded the highest performance model with an overall accuracy of 84.78% and a Cohen's kappa of 0.4953 (Table 4). This contrasts with the best-performing models from search string 1 iteration 2 and search string 2 iteration 3, which produced Cohen's kappa values of 0.2174 and 0.0183 and overall accuracies of 85.71% and 58.62%, respectively. While string 1 engendered a model with 0.93% higher overall model accuracy than string 3, the EXCLUDE class performance metrics for string 1 were suboptimal compared to string 3 (Tables 5-6). Improvements in precision and recall for the EXCLUDE class in the LRN model for string 3 iteration 3 demonstrated the model's efficacy in filtering studies unrelated to the research aims, yet this was with a slight decrease in precision for the INCLUDE class (Table 5). Despite string 3's superior performance, a notable decrease in average potential from 85.44% to 49.28% was observed, suggesting a reduction in semantic information retention across iterations for the string 3 model (Table 4). Four iterations were executed for all three models, with non-SME assigned labels for 92 unique records. However, by the fourth iteration, all models exhibited signs of underfitting with a set of new rules added by the user. Therefore, each model retained its learned associations up to the third iteration. Initially, search string 3 identified 284 potential studies for inclusion, with the validated optimal model ultimately selecting 149 full-text reports from that subset of the literature (Figure 1). Across the three search strings, a total of 810 studies were initially identified as candidates for inclusion. From this study, the highest performance model (search string 3 iteration 3) was discovered to not be the optimally balanced model for classifying the literature. Instead, the model from search string 1 iteration 2 was determined to be the optimally balanced model. Thus, upon application of the optimal model from search string 1 iteration 2 to these records, 757 full-text reports were classified as \u2018INCLUDE' (Figure 2).\nAs an XAI, LRN models elucidate their inclusion and exclusion decisions for the literature through visual representations (Figure 3) and detailed quantitative tables. These correlations guided LRN's decision-making process. The highest performance model identified novel concepts in studies classified for inclusion, featuring terms such as \u2018reduce', 'accident', 'sharp', and 'double-gloving.' Comparatively, the concepts uncovered by the optimally balanced model during RLHF are also depicted in Figure 3. Leveraging RLHF, LRN ingests human feedback through natural language rules and establishes semantic contexts for these rules by establishing correlations between two unique concepts or numerical measures. Specifically, when two unique rules exhibited a high correlation, it indicated that the LRN model had contextually associated these terms as either co-occurring or related within the literature. The highest performance model (search string 3 iteration 3) identified significant correlations between concepts such as \u2018nitrile' and 'examination glove(s)' (r = 0.601, p-value = 5.302E-13), \u2018condom' and 'hand washing' (r = 0.505, p-value = 3.716E-09), \u2018polychloroprene' and \u2018nitrile' (r = 0.495, p-value = 8.080E-09), and 'operation' and 'surgical glove(s)' (r = 0.490, p-value = 1.052E-08). Additional significant correlations (p-value > 0.05) are documented in Table 7. For search string 1, the optimally balanced model identified significantly correlated rules such as \u2018talc' and \u2018animals' (r = 0.404, p-value = 2.424E-12), \u2018condom' and 'antibiotic prophylaxis' (r = 0.397, p-value=6.424E-12), \u2018exam glove' and 'examination glove' (r = 0.369, p-value = 2.710E-10), and \u2018operation' and \u2018surgical gloves' (r = 0.369, p-value = 2.710E-10). Based on the performance of the LRN model derived from string 1 iteration 2 (Table 4, Table 6), this model was considered as the optimally balanced model, and was later used to classify the surgical glove corpus. Statistically significant, pertinent rules and relationships guiding the optimally balanced model's classifications are presented in Table 8. The optimal model from search string 2, which underperformed compared to the models from search strings 1 and 3, revealed correlations between \u2018exam glove' and \u2018nitrile (glove)' (r = 0.495, p-value = 2.098E-11), \u2018examination' and 'examination glove' (r = 0.482, p-value = 8.306E-11), 'maxillofacial' and 'mandibular' (r = 0.461, p-value = 6.203E-10), and \u2018polychloroprene' and 'exam glove' (r = 0.401, p-value = 1.409E-07)."}, {"title": "3.2 LRN Productivity Metrics", "content": "The optimal models for each search string were deployed on the total LRN corpus of 810 studies. Of these 810 studies, a total of 194 identified studies could be found overlapping with the manual SME-curated library of 262 studies, of which 212 studies had PMIDs. From the LRN corpus of 810 studies, search string 1 classified 757 full-text reports, search string 2 classified 389 full-text reports, and search string 3 classified 674 full-text reports as INCLUDE (Figure 4). Evaluation of the overlap between the three search strings, the manual SLR library revealed varying degrees of alignment: search string 2 exhibited the highest similarity (Jaccard index = 0.3151, p-value = 3.000E-5). Search string 1 (Jaccard index = 0.2503, p-value = 3.538E-03) and search string 3 (Jaccard index = 0.2238, p-value = 3.538E-03) demonstrated reduced similarity (Table 9). Moreover, significant similarity was observed among the LRN search strings themselves. The overlap between search string 1 and search string 3 was the highest (Jaccard index = 0.8609, p-value < 1.000E-20), achieving nearly similar coverage of the literature. Lastly, moderate similarity was seen between search string 2 and search string 3 (Jaccard index = 0.4682, p-value < 1.000E-20) (Table 9). Search string 2 resulted in the highest number of false negatives, followed by search string 3 and search string 1 (Figure 4). Of note, the optimally balanced model from search string 1 classified the highest number of true positives (n = 194) yet had the highest number of false positives (n = 563) compared to the highest performance model from search string 3 (Figure 4). As mentioned previously, search string 1 iteration 2 was used to classify the final set of studies, synthesized in the LRN-written SLR.\nTotal human labor time to conduct the complete manual SLR on surgical glove practices was 19,920 minutes, or 332 hours. The manual search and identification of the final list of abstracts occurred over 5 months, while the division and reading of a study's full texts, evidence table generation, data analysis, and the manuscript creation took approximately 6 months. Comparatively, total human labor time to complete all three LRN models, to perform all similar analyses, and to generate a manuscript was 288.6 minutes, or nearly 4.81 hours, over the span of 5 consecutive days. Computation time, separate from the human labor time, was 1810.7 minutes, or 30.18 hours, as itemized by search string by iteration in Table 10."}, {"title": "4. Discussion", "content": ""}, {"title": "4.1 Explainable AI Performed SLRs equivalent to Experts in Surgical Gloving Practice", "content": "An observably high inter-rater reliability score (Table 4) from the LRN model trained on search string 3 iteration 3 coupled to a high overall accuracy demonstrated that LRN had the capacity to behave like a SME in screening and classifying the literature on surgical glove procedures (Figure 1). This LRN model was identified as the highest performance model and achieved high INCLUDE precision and recall (Table 5) despite receiving the fewest number of rules compared against the other two search strings (Table 2). However, a strategic trade-off occurred for string 3 which improved the EXCLUDE class precision at the expense of reducing the precision for the INCLUDE class, a well-characterized relationship in document retrieval[33]. Still, the reduction in precision for the INCLUDE class was minor relative to the EXCLUDE label, in exchange for all metrics for the EXCLUDE class to increase from 0% (Table 5). This rebalance validated the robustness for LRN models to screen the literature.\nAlthough search string 1 iteration 2 exhibited a marginally higher overall accuracy by 0.93% compared to that of search string 3 iteration 3, it was at the expense of a reduction in both precision for the INCLUDE class and recall for the EXCLUDE class (Table 6). If search string 1 were misattributed as the highest performance model due to its overall model accuracy alone, its low EXCLUDE recall would suggest that the deployed model would have failed to correctly exclude more studies based on the exclusion criteria[34]. Interestingly, this was observed with the higher count of false positives compared to the LRN model produced via search string 3 (Figure 4). This comparison of the tabulated performance metrics across different search strings highlighted the importance of not solely relying on accuracy. Alternative metrics like Cohen's kappa and potential provided deeper insights into a LRN model's alignment with user evaluations and its proficiency to extract relevant information from the literature.\nLRN was initialized as a clean state XAI with the goal of maximizing overall accuracy, Cohen's kappa, and average potential for any given research objective. User feedback during configuration, in the form of natural language rules and inclusion or exclusion criteria from the non-SME (JM), was part of the RLHF framework that enabled an initialized LRN model to exploit information relevant to the research question. RLHF provided advantages such as the conservation of online computational resources by pre-computing optimal solutions offline based on incorporated feedback[35]. Conventional supervised ML models or supervised AI cannot exceed the performance of SMEs and can only mimic the behavior of an SME, as these systems learn from a training data set that is labeled by the SME[35]. This is in direct contrast to RLHF, which enabled LRN to exceed such benchmarks. Superior classification capabilities were observed for both search string 1 iteration 2 and search string 3 iteration 3, identifying key literature on surgical glove procedures (Tables 4-6). Of note was search string 3, which achieved high accuracy and high inter-rater reliability. The lack of SME training in all three models might have affected the interpretability of Cohen's kappa. Integrating additional well-calibrated rules for INCLUDE and EXCLUDE classes from the SME may have minimized the risk of underfitting for all models, a tendency observed by the fourth iteration; however, LRN's robustness was evident, with considerable accuracy achieved across search string 2 and search string 3. Remarkably, despite minimal alignment between non-SME user inputs and the LRN model classifications for string 2, a total of 194 out of 212 full-text reports were accurately identified as included studies within the SLR.\nComparing the outputs of the LRN model SLR with the results of the manual SLR on double-gloving resulted in the identification of a striking capability of this new tool for researchers. Even though the human interface for the LRN network (JM) was not a trained clinician, nor did the non-SME understood the depth of the clinical issue surrounding glove damage during surgery, the simplicity of marking LRN derived abstracts as INCLUDE/EXCLUDE based on the overarching research question, matched the efforts and knowledge of the SME. While the researchers of the original manual process had the advantage of clinical experience and knowledge of specific connections between materials and surgical behavior to guide their search strategies, this was over come through LRN's capability to identify relationships between terms in the literature and drawing inferences based on simple human feedback. The broader implications of these findings substantiate that SME training for LRN is not essential for LRN to achieve literature reviews equal in depth and accuracy of an SME."}, {"title": "4.2 Explainable AI Identified Key Themes equivalent to Experts in Surgical Gloving Practice", "content": "Pairing of non-SME provided rules included those such as \u2018nitrile' and \u2018examination glove(s)', both of which were classified by the non-SME and by LRN as EXCLUDE rules. The correlation between \u2018condom' and 'hand washing,' two EXCLUDE rules, presents an intriguing case; while individually relevant to latex material and practices associated with hygiene, respectively, their association was not directly pertinent to the core objectives of this study. These findings signify LRN's ability to discern and explicitly identify concepts deemed irrelevant to the study's objective. Moreover, the correlation between \u2018polychloroprene' and \u2018nitrile\u2019across INCLUDE and EXCLUDE categories, respectively, demonstrates LRN's nuanced comprehension of these materials as alternative options for surgical gloves and showcases LRN's mechanism for contextual differentiation. Furthermore, as presented through the visualization tools, the emphasis on 'reduce,' 'sharp,' 'contaminate,' \u2018tear,' \u2018reinforcement,' and 'double-gloving' as connected concepts and procedural measures for enhancing barrier protection between the clinician and the patient encapsulates LRN's successful extraction of critical procedural insights from the literature (Figure 3). These inclusion concepts' prominence within the model's classification decisions aligned closely with the study's investigative focus on surgical gloving practices. Through iterative feedback and rule adjustment, LRN demonstrated progressively enhanced specificity and reduced the inclusion of irrelevant studies.\nSemantic understanding in both explicit and implicit contexts is necessary for an AI model to adequately capture the predicate-argument structures of human language. Significant development with respect to this area has been undertaken for different neural network architectures[36]. Moreover, AI language models that leverage context-sensitive features present valuable applications in the fields of preclinical and clinical research and development[37], [38], [39], [40]. Two modalities LRN leverages to explain its decision-making processes are tabular methods and advanced visualization tools. LRN is the first XAI capable of semantic understanding while writing SLRs, conducting meta-analyses, and method development in both explicit and implicit contexts as highlighted by the predicate-argument relationships between concepts in Table 7 and Table 8.\nAfter completion of the search methodology which accomplished the identification of the totality of articles selected through the original manual process, the final manuscripts were loaded into an LLM as a boundary condition and were asked to summarize the information (Table 3). Interestingly, three of the conclusions of the AI induced summary matched similar consensus statements derived from a group of surgeons and nurses completing the original systematic reviews. While not an original aim of this study, the XAI summary recommended the use of double gloving to reduce the risk of aseptic barrier breach, frequent changing of gloves during surgical procedures, especially in orthopedic surgery and that specific surgical procedures may require special considerations, such as suturing. These recommendations were like those proposed by experts in the field who had both practical personal experience as well as the benefit of reading the entire full text articles. Future studies using methods like BLEU should assess the accuracy of this XAI SLR compared to traditional methods[41]."}, {"title": "4.3 Explainable AI Improved Productivity and Streamlined SLRs with Fewer Searches", "content": "In evaluating LRN's effectiveness to replicate the manually conducted SLRs, it was observed that 194 records from LRN search strings overlapped with the 262 reports included in the manual SLR. Interestingly, following full text review by the manual reviewers described in the ground truth SLR, the 262 reports were reduced to a final count of 165 by their specific inclusion and exclusion criteria. Thus, nearly all articles identified through a manual methodology including four databases (Pubmed, Embase, Google Scholar, Cochrane) were captured by LRN using PubMed alone. Search string 2 demonstrated the highest Jaccard index, which was attributed to a reduced number of false positives, yet it also exhibited the highest number of false negatives, missing 52 full-text reports (Figure 2). This discrepancy highlighted that search strings 1 and 3, showing a high degree of statistical similarity, could sufficiently cover the literature corpus for this study. Furthermore, search string 1 effectively covered 91.51% of the potential studies which overlapped with both LRN's corpus and the manual SLR. Taken together, LRN evidenced that the use of both search string 1 and search string 2 was unnecessary for this study (Table 1). In addition, due to the high similarity between search strings 1 and 3, it can be hypothesized that further refinement of either string's models would have rendered the use of one search string adequate. These results evince that the efficiency of LRN in literature identification, screening, and evaluation or inclusion (Figure 4) hinges on model training and validation, as evidenced by performance metrics, rather than the number of search strings employed (Tables 4, 7-9).\nBy design, LRN prioritized minimizing false negatives to ensure comprehensive capture of relevant documents. This approach demonstrated the potential of SME involvement in training LRN models to significantly reduce false positives. A major limitation of this study was overreliance on PubMed, which led to a reduction in the number of potential studies retrieved from 262 to 212, marking a 19.08% decrease. Furthermore, only 196 (92.45%) unique studies out of these 212 studies were covered by the three search strings, culminating in a total literature coverage of merely 74.81% of the library of 262 studies (Table 8). Expanding LRN's database integration beyond PubMed could have potentially addressed this shortfall[42].\nIncreased computational capacity could lead to more efficient processing times, translating into a notable reduction in human labor hours required for SLR completion with LRN (Table 10). This efficiency not only streamlines the review process but also has the potential to lower associated costs. A thorough cost-benefit analysis of LRN's implementation is essential to fully understand its impact on productivity and financial savings. Coupled with this cost-benefit analysis would be an assessment of LRN's completeness and interpretability to validate the responses (e.g., complete SLRs) generated by LRN[43]. The substantial decrease in human labor time from 19,920 minutes, or 332 hours over 11 months for the manual SLR to 288.6 minutes, or 4.81 hours over the span of a work week (5 days) for the LRN models exemplified the efficiency and productivity offered by employing XAI for SLRs."}, {"title": "5. Conclusions", "content": "LRN's performance evinced that direct training by an SME was not imperative for it to achieve deep and accurate text classifications equivalent to an SME in surgical gloving practices. LRN is the first AI platform to offer explainability in tandem with text summarization as a core feature. Moreover, LRN as an XAI platform accurately and reliably demonstrated its ability to conduct SLRs while conforming to PRISMA 2020 guidelines. After 4.81 hours of human labor time, one highly accurate LRN model covered 91.51% of the accessible literature, and 74.81% of the entire corpus, from a SME-curated SLR library.\nA limitation in this study was the assumption that the current SME corpus is the ground truth. While this assumption served valuable for the purpose of this study's aims, future validation of the LRN platform will require a prospective study. In a prospective study, human SMEs would need to review the results of LRN a priori to conducting a SLR. The UMLS controlled vocabulary system enabled LRN to map different concepts to unique identifiers and allowed cross-walking between different vocabulary systems such as MeSH and ICD-10. The LRN codebase should also be updated to improve automation efficiencies and to handle Russian and Chinese languages.\nLRN effectively streamlined SLR methodologies without sacrificing the scientific rigor necessary to achieve the high quality of evidence expected of SLRs. By offering a more accurate, precise, and transparent approach to conducting SLRs in healthcare, LRN's current trajectory suggests significant advancements in automating and optimizing research. Moreover, LRN's versatility allows it to encompass many fields within the life sciences and to generate other research such as meta-analyses, scoping reviews, accelerate method development, and produce evidence summaries. Future efforts will focus on expanding the number of databases LRN can access, paving the way for more comprehensive and up-to-date evidence syntheses in surgical practice and beyond."}]}