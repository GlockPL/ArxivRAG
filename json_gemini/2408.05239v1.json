{"title": "The Literature Review Network: An Explainable Artificial Intelligence for Systematic Literature Reviews, Meta-analyses, and Method Development.", "authors": ["Joshua Morriss", "Tod Brindle", "Jessica Bah R\u00f6sman", "Daniel Reibsamen", "Andreas Enz"], "abstract": "Systematic literature reviews are the highest quality of evidence in research. However, the review process is hindered by significant resource and data constraints. The Literature Review Network (LRN) is the first of its kind explainable AI platform adhering to PRISMA 2020 standards, designed to automate the entire literature review process. LRN was evaluated in the domain of surgical glove practices using 3 search strings developed by experts to query PubMed. A non-expert trained all LRN models. Performance was benchmarked against an expert manual review. Explainability and performance metrics assessed LRN's ability to replicate the experts' review. Concordance was measured with the Jaccard index and confusion matrices. Researchers were blinded to the other's results until study completion. Overlapping studies were integrated into an LRN-generated systematic review. LRN models demonstrated superior classification accuracy without expert training, achieving 84.78% and 85.71% accuracy. The highest performance model achieved high interrater reliability (\u043a = 0.4953) and explainability metrics, linking 'reduce', 'accident', and 'sharp' with 'double-gloving'. Another LRN model covered 91.51% of the relevant literature despite diverging from the non-expert's judgments (\u03ba = 0.2174), with the terms 'latex', 'double' (gloves), and 'indication'. LRN outperformed the manual review (19,920 minutes over 11 months), reducing the entire process to 288.6 minutes over 5 days. This study demonstrates that explainable Al does not require expert training to successfully conduct PRISMA-compliant systematic literature reviews like an expert. LRN summarized the results of surgical glove studies and identified themes that were nearly identical to the clinical researchers' findings. Explainable AI can accurately expedite our understanding of clinical practices, potentially revolutionizing healthcare research.", "sections": [{"title": "1. Introduction", "content": "During surgery, the aseptic barrier exists as the primary method to protect the operating room personnel from the patient and the patient from the clinical team. As part of what is known today as personal protective equipment (PPE), gloves provide the aseptic barrier and protection of the hands from potentially infectious materials being cross contaminated within the surgical field. However, despite the innovations seen in surgical glove raw materials and manufacturing, surgeons and nurses have known that glove damage during surgery occurs[1], [2].In fact, for over seven decades clinicians have reported and studied glove damage in surgery and have described from large, visible tears to microperforations, not visible to the human eye[3], [4], [5].\nNothing is more feared in surgical practice than the surgical site infection. The increase in morbidity and mortality for the patient, in addition to the increases in costs for the health care system, are well described in the literature. While glove damage during surgery is nearly impossible to link as a direct cause of surgical site infection due to a extensive list of confounders, the risk of infection causes surgical teams to take every precaution to decrease infection. Additionally, provider safety from contracting infectious disease was highlighted during historical pandemics such as HIV-Aids in the 1980s[6], [7] and recently with SARS-COV-19[8], [9], [10]. Additionally, more common viral pathogen such as hepatitis B and C are inherent risks for the reported 400,000 sharp injuries estimated per year[11]. Therefore, research such literature reviews are needed to understand the incidence, prevalence, time-lapse and potential causes of glove damage during surgery to improve the safety of the provider and patient.\nSystematic literature reviews (SLRs) are the gold standard in clinical and preclinical research, informing public policy, clinical guidelines, and R&D for medical devices and pharmaceuticals[12], [13]. In clinical practice, the use of systematic reviews and metanalysis guide the development of clinical practice guidelines, which direct practice change to achieve the best outcomes. Despite their importance, producing SLRs is challenging due to the sheer volume of research published annually, estimated at >1 million studies, leading to reporting biases and gaps in evidence[14], [15]. Furthermore, SLRs incur substantial financial burdens, with an average expenditure of approximately $141,194.80 per SLR, and a significant time investment of approximately 1.72 years per researcher[16]. Advances in natural language processing (NLP) and machine learning (ML), alongside web-based large language model (LLM) and artificial intelligence (AI) platforms, have been proposed to streamline the SLR process. These technologies aim to enhance data extraction and text classification, but they often fall short in automation, continuous updateability, and particularly in explainability\u2014the ability for humans to understand and trust the decisions made by an AI to achieve its outputs[17], [18]. Current NLP-ML and AI solutions lack automation and explainability, and do not meet the rigorous standards of high-quality research frameworks like Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) 2020[19]. This apparent lack of transparent and explainable processes in NLP-ML and AI applications for SLRs has notably contributed to hesitancy in their widespread adoption for clinical and preclinical research."}, {"title": "2. Methods", "content": null}, {"title": "2.1 Search Strategy, Data Extraction, Review Procedures", "content": "Based on the protocol and PICOT framework used for the human SLR, three human SMEs (TB, AE, JBR) initially formulated three separate search query strings that were used in this study. These search strings were then converted into LRN queries, as recorded in Table 1, and a set of concept rules which served as the basis for LRN's reinforcement learning. Search strings were subjected to LRN version 2.0 (LRN v2.0), which employed a word embedding model that mapped concepts with the Unified Medical Language System (UMLS) Metathesaurus[20]. LRN processed each of these unique queries independently as three LRN models. These LRN models were configured to query the PubMed database for relevant literature, utilizing the PubMed API for data retrieval. Of the 262 studies that were identified and included in the manual SLR by the SMEs, the PubMed ID (PMID) was retrieved for only 212 studies. LRN's screening mechanism involved the automatic ineligibility of records that either lacked an abstract, were published in Russian or Chinese, or were identified as duplicates. Russian and Chinese studies were excluded due to the former being a low-resource language, while the later presented complications related to accurate word segmentation with LRN[21]. Records that met the exclusion criteria were also automatically excluded by LRN (Table 1), and composed an out-of-domain or negative dataset to train LRN's discriminative algorithms[22]. Levels of evidence considered were randomized controlled trials,"}, {"title": "2.2 Explainable Artificial Intelligence Framework for Research", "content": "LRN utilized a combination of a metaheuristic wrapper, weak supervision models, and discriminative algorithms. LRN first extracted natural language features from the corpus, with the wrapper optimizing feature selection for semantic analysis considering user-defined language rules. This metaheuristic wrapper feature selection technique reduced the complex feature space inherent to natural language data[23]. Weak supervision models in LRN operated under a matrix completion methodology and generated several rudimentary models from the concept ruleset. These models, despite their inaccuracies, can effectively label unstructured literature[24].\nDiscriminative models refined labels by analyzing consensus and discrepancies among weak models, effectively handling correlated labels from weak supervision sources without requiring labeled data[24]. Performance metrics of recall, precision, and F-score were automatically calculated by LRN for each label, in addition to the overall model accuracy and Cohen's kappa. Additionally, LRN calculated potential scores for each record to balance exploration of new linguistic models and exploitation of established models and data structures for classification[25]. During RLHF, LRN presented 20 records with the highest potential score for feedback. Data visualizations and correlation tables were produced to clarify literature screening decisions for each iteration. The relationships between generative AI parameters were quantitatively assessed using Pearson's chi-squared test, adjusted by Cramer's V, and corrected for significance using the Benjamini-Hochberg method[26], [27], [28]. A LRN \"AI Package Insert\" for each search string documented all metrics and model decision-making processes. The highest performance model was identified by superior Cohen's kappa and accuracy, while the optimally"}, {"title": "2.3 Analysis of LRN Search Alignment with SME Review", "content": "To critically appraise each search strategy and determine if LRN could effectively streamline the SLR process, the similarity was assessed between the three search strings and the SME-curated library. The unique reports from all three search strings were first pooled into a single corpus and deduplicated. The model with the highest Cohen's kappa and overall accuracy from each search string was then used to classify the entire corpus. Non-SME feedback from all the models, which were the non-SME assigned labels for unique records, was incorporated into the dataset for classification by each string's optimal model. PMIDs were used as the identifiers for comparing the SME-curated library and the LRN model classifications. Three sets of PMIDs corresponded with the three optimal models from each search string. Concordance between each LRN model's predictions and the SME selections was done through the calculation of a Jaccard index for quantifying the overlap between studies classified as \u201cINCLUDE\u201d by each LRN model and those identified in the manual SLR[29]. Bootstrapping for 1 million replications was performed to determine the significance level of the similarity between the sets of PMIDs and the SME library. A Jaccard index was also calculated comparing each search string against the others to determine if there was significant overlap between, for example, search string 1 and search string 2, search string 1 and search string 3, and search string 2 and search string 3. P-values were adjusted using the Benjamini-Hochberg method[26]. Confusion matrices further explored the performance of the optimal models for each string in classifying the literature, providing coverage statistics on true positive (INCLUDE) and true negative (EXCLUDE) studies, as well as false positive and false negative cases."}, {"title": "2.4 Text Summarization via LRN written SLRS", "content": "In alignment with PRISMA 2020 guidelines, the optimally balanced model across all search strings was applied across the three search strings to classify the surgical glove corpus. This model automatically generated a PRISMA 2020 flow diagram detailing study identification, screening, and inclusion processes[30]. Classified \u2018INCLUDE\u2019studies were prepared for summarization using an LLM that was based on OpenAI's GPT-4-turbo[31]. LRN identified, labeled, and embedded full-text reports, which were then subjected to a series of user questions (Table 3). Full-text reports were formatted and embedded for further processing, utilizing Langchain's Retrieval-Augmented Generation (RAG) technique to select relevant token segments, likelihood of data leakage and hallucinations was reduced[32]. LRN's LLM merged its outputs into a single document, encompassing a LRN-generated SLR. Time to complete draft metric for the manual SLR quantified the cumulative human labor hours required for identifying, screening, assessing eligibility, and incorporating full-text reports. In contrast, this metric for the LRN models considered the total human labor hours needed to configure the three models (iteration 1), complete"}, {"title": "3. Results", "content": null}, {"title": "3.1 LRN Model Performance Metrics", "content": "Optimal performance was achieved with search string 3, iteration 3, which yielded the highest performance model with an overall accuracy of 84.78% and a Cohen's kappa of 0.4953 (Table 4). This contrasts with the best-performing models from search string 1 iteration 2 and search string 2 iteration 3, which produced Cohen's kappa values of 0.2174 and 0.0183 and overall accuracies of 85.71% and 58.62%, respectively. While string 1 engendered a model with 0.93% higher overall model accuracy than string 3, the EXCLUDE class performance metrics for string 1 were suboptimal compared to string 3 (Tables 5-6). Improvements in precision and recall for the EXCLUDE class in the LRN model for string 3 iteration 3 demonstrated the model's efficacy in filtering studies unrelated to the research aims, yet this was with a slight decrease in precision for the INCLUDE class (Table 5). Despite string 3's superior performance, a notable decrease in average potential from 85.44% to 49.28% was observed, suggesting a reduction in semantic information retention across iterations for the string 3 model (Table 4). Four iterations were executed for all three models, with non-SME assigned labels for 92 unique records. However, by the fourth iteration, all models exhibited signs of underfitting with a set of new rules added by the user. Therefore, each model retained its learned associations up to the third iteration. Initially, search string 3 identified 284 potential studies for inclusion, with the validated optimal model ultimately selecting 149 full-text reports from that subset of the literature (Figure 1). Across the three search strings, a total of 810 studies were initially identified as candidates for inclusion. From this study, the highest performance model (search string 3 iteration 3) was discovered to not be the optimally balanced model for classifying the literature. Instead, the model from search string 1 iteration 2 was determined to be the optimally balanced model. Thus, upon application of the optimal model from search string 1 iteration 2 to these records, 757 full-text reports were classified as \u2018INCLUDE' (Figure 2).\nAs an XAI, LRN models elucidate their inclusion and exclusion decisions for the literature through visual representations (Figure 3) and detailed quantitative tables. These correlations guided LRN's decision-making process. The highest performance model identified novel concepts in studies classified for inclusion, featuring terms such as \u2018reduce', 'accident', 'sharp', and 'double-gloving.' Comparatively, the concepts uncovered by the optimally balanced model during RLHF are also depicted in Figure 3. Leveraging RLHF, LRN ingests human feedback through natural language rules and establishes semantic contexts for these rules by establishing correlations between two unique concepts or numerical measures. Specifically, when two unique rules exhibited a high correlation, it indicated that the LRN model had contextually associated these"}, {"title": "3.2 LRN Productivity Metrics", "content": "The optimal models for each search string were deployed on the total LRN corpus of 810 studies. Of these 810 studies, a total of 194 identified studies could be found overlapping with the manual SME-curated library of 262 studies, of which 212 studies had PMIDs. From the LRN corpus of 810 studies, search string 1 classified 757 full-text reports, search string 2 classified 389 full-text reports, and search string 3 classified 674 full-text reports as INCLUDE (Figure 4). Evaluation of the overlap between the three search strings, the manual SLR library revealed varying degrees of alignment: search string 2 exhibited the highest similarity (Jaccard index = 0.3151, p-value = 3.000E-5). Search string 1 (Jaccard index = 0.2503, p-value = 3.538E-03) and search string 3 (Jaccard index = 0.2238, p-value = 3.538E-03) demonstrated reduced similarity (Table 9). Moreover, significant similarity was observed among the LRN search strings themselves. The overlap between search string 1 and search string 3 was the highest (Jaccard index = 0.8609, p-value < 1.000E-20), achieving nearly similar coverage of the literature. Lastly, moderate similarity was seen between search string 2 and search string 3 (Jaccard index = 0.4682, p-value < 1.000E-20) (Table 9). Search string 2 resulted in the highest number of false negatives, followed by search string 3 and search string 1 (Figure 4). Of note, the optimally balanced model from search string 1 classified the highest number of true positives (n = 194) yet had the highest number of false positives (n = 563) compared to the highest performance model from search string 3 (Figure 4). As mentioned previously, search string 1 iteration 2 was used to classify the final set of studies, synthesized in the LRN-written SLR."}, {"title": "4. Discussion", "content": null}, {"title": "4.1 Explainable AI Performed SLRs equivalent to Experts in Surgical Gloving Practice", "content": "An observably high inter-rater reliability score (Table 4) from the LRN model trained on search string 3 iteration 3 coupled to a high overall accuracy demonstrated that LRN had the capacity to behave like a SME in screening and classifying the literature on surgical glove procedures (Figure 1). This LRN model was identified as the highest performance model and achieved high INCLUDE precision and recall (Table 5) despite receiving the fewest number of rules compared against the other two search strings (Table 2). However, a strategic trade-off occurred for string 3 which improved the EXCLUDE class precision at the expense of reducing the precision for the INCLUDE class, a well-characterized relationship in document retrieval[33]. Still, the reduction in precision for the INCLUDE class was minor relative to the EXCLUDE label, in exchange for all metrics for the EXCLUDE class to increase from 0% (Table 5). This rebalance validated the robustness for LRN models to screen the literature.\nAlthough search string 1 iteration 2 exhibited a marginally higher overall accuracy by 0.93% compared to that of search string 3 iteration 3, it was at the expense of a reduction in both precision for the INCLUDE class and recall for the EXCLUDE class (Table 6). If search string 1 were misattributed as the highest performance model due to its overall model accuracy alone, its low EXCLUDE recall would suggest that the deployed model would have failed to correctly exclude more studies based on the exclusion criteria[34]. Interestingly, this was observed with the higher count of false positives compared to the LRN model produced via search string 3 (Figure 4). This comparison of the tabulated performance metrics across different search strings highlighted the importance of not solely relying on accuracy. Alternative metrics like Cohen's kappa and potential provided deeper insights into a LRN model's alignment with user evaluations and its proficiency to extract relevant information from the literature.\nLRN was initialized as a clean state XAI with the goal of maximizing overall accuracy, Cohen's kappa, and average potential for any given research objective. User feedback during configuration, in the form of natural language rules and inclusion or exclusion criteria from the"}, {"title": "4.2 Explainable AI Identified Key Themes equivalent to Experts in Surgical Gloving Practice", "content": "Pairing of non-SME provided rules included those such as \u2018nitrile' and \u2018examination glove(s)', both of which were classified by the non-SME and by LRN as EXCLUDE rules. The correlation between \u2018condom' and 'hand washing,' two EXCLUDE rules, presents an intriguing case; while individually relevant to latex material and practices associated with hygiene, respectively, their association was not directly pertinent to the core objectives of this study. These findings signify LRN's ability to discern and explicitly identify concepts deemed irrelevant to the study's objective. Moreover, the correlation between \u2018polychloroprene' and \u2018nitrile\u2019across"}, {"title": "4.3 Explainable AI Improved Productivity and Streamlined SLRs with Fewer Searches", "content": "In evaluating LRN's effectiveness to replicate the manually conducted SLRs, it was observed that 194 records from LRN search strings overlapped with the 262 reports included in the manual SLR. Interestingly, following full text review by the manual reviewers described in the ground truth SLR, the 262 reports were reduced to a final count of 165 by their specific inclusion"}, {"title": "5. Conclusions", "content": "LRN's performance evinced that direct training by an SME was not imperative for it to achieve deep and accurate text classifications equivalent to an SME in surgical gloving practices. LRN is the first AI platform to offer explainability in tandem with text summarization as a core feature. Moreover, LRN as an XAI platform accurately and reliably demonstrated its ability to conduct SLRs while conforming to PRISMA 2020 guidelines. After 4.81 hours of human labor"}]}