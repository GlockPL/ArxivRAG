{"title": "Boost, Disentangle, and Customize: A Robust System2-to-System1 Pipeline for Code Generation", "authors": ["Kounianhua Du", "Hanjing Wang", "Jianxing Liu", "Jizheng Chen", "Xinyi Dai", "Yasheng Wang", "Ruiming Tang", "Yong Yu", "Jun Wang", "Weinan Zhang"], "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in various domains, particularly in system 1 tasks, yet the intricacies of their problem-solving mechanisms in system 2 tasks are not sufficiently explored. Recent research on System2-to-System1 methods surge, exploring the System 2 reasoning knowledge via inference-time computation and compressing the explored knowledge into System 1 process. In this paper, we focus on code generation, which is a representative System 2 task, and identify two primary challenges: (1) the complex hidden reasoning processes and (2) the heterogeneous data distributions that complicate the exploration and training of robust LLM solvers. To tackle these issues, we propose a novel BDC framework that explores insightful System 2 knowledge of LLMs using a MC-Tree-Of-Agents algorithm with mutual Boosting, Disentangles the heterogeneous training data for composable LoRA-experts, and obtain Customized problem solver for each data instance with an input-aware hypernetwork to weight over the LoRA-experts, offering effectiveness, flexibility, and robustness. This framework leverages multiple LLMs through mutual verification and boosting, integrated into a Monte-Carlo Tree Search process enhanced by reflection-based pruning and refinement. Additionally, we introduce the DisenLora algorithm, which clusters heterogeneous data to fine-tune LLMs into composable Lora experts, enabling the adaptive generation of customized problem solvers through an input-aware hypernetwork. Our contributions include the identification of critical challenges in existing methodologies, the development of the MC-Tree-of-Agents algorithm for insightful data collection, and the creation of a robust and flexible solution for code generation. This work lays the groundwork for advancing LLM capabilities in complex reasoning tasks, offering a novel System2-to-System1 solution.", "sections": [{"title": "1 Introduction", "content": "Large language models show significant intelligence in various domains, striking both the academic and industrial institutions. Despite their prominent problem-solving abilities in system 1 tasks, the mechanism behind the system 2 task solving procedure remain opaque. In this paper, we focus on the code generation task, which emerges as a captivating frontier (Zheng et al., 2023; Roziere et al., 2023; Shen et al., 2023), promising to revolutionize software development by enabling machines to write and optimize code with minimal human intervention. Recent research of llms for code focus on inference-time computation (System 2 methods) (Yang et al., 2024; Yao et al., 2024b; Zhang et al., 2023) and post-training. While during post-training, distilling system 2 knowledge into system 1 backbones is important and widely-used (Yu et al., 2024b).\nHowever, the complex hidden reasoning process and the heterogeneous data distribution pose challenges to the existing System2-to-System1 pipeline. On one hand, the hidden reasoning process for code generation is complex and hard to explore (C1). On the other hand, the heterogeneous data distribution, e.g., jumping structure like branching, recursion, etc., makes the existing train-once-for-all strategy hard to fit the complex latent patterns for robust and generalizable llm solvers (C2).\nFor (C1), we propose to disentangle the problem solving process into problem2thought and thought2solution stages, exploring the inherent reasoning clues via combining the strengths of multiple llms by mutually-verification and boosting. The exploration is integrated into a Monte-Carlo Tree Search process, where reflexion-based pruning and refinement are designed for more efficient and effective reasoning clues search.\nFor (C2), we propose to disentangle the heterogeneous data into clusters, finetuning llms capable of different aspects of tasks to obtain the meta LORA experts hub, and then adaptively generate customized problem solver for each code problem. Concretely, we design an input-aware hypernetwork to generate rank-wise weights over meta LORA experts for customized problem solver, offering robustness and flexibility.\nThe main contributions of our work can be summarized below.\n\u2022 Identification of problems and novel BDC framework. We identify the high-reasoning demand and heterogeneous latent patterns problems that hinders the performance of existing methods and propose a BDC framework that explores insightful inherent reasoning clues via multi-llms boosting, generates meta-LoRA experts via finetuning on disentangled data, and offer customized problem solver with an input-aware hypernet for rank-wise LORA merging.\n\u2022 Novel MC-Tree-of-Agents algorithm for insightful data collection. We disentangle the System 2 solving process into problem2thought and thought2solution stages, integrating the exploration process into a reflexion-based monte carlo tree search armed with pruning and refinement, enabling mutually verification and boosting of different agents for insightful data collection.\n\u2022 Novel DisenLoRA algorithm that offers customized problem solver for robust code generation. We disentangle the heterogeneous data distribution into clusters on which meta-LORA experts are trained, and design an input-aware hypernetwork to weight over the LoRA-experts for customized problem solver, offering robustness and flexibility."}, {"title": "2 Related Work", "content": "Recent research on large language models for System 2 tasks focus on inference-time computation optimization to stimulate the inherent reasoning ability of LLMs. Few-shot learning methods (Wang et al., 2022; Madaan et al., 2022) utilize the in-context-learning ability of LLMs for enhanced generation. Retrieval-augmented generation (RAG) approaches (Nashid et al., 2023; Du et al., 2024) further introduce domain knowledge into LLMs. Techniques such as Chain-of-Thought (CoT) (Yang et al., 2024; Jiang et al., 2024; Li et al., 2023), Tree-of-Thought (ToT) (Yao et al., 2024b; La Rosa et al., 2024), and Monte Carlo Tree Search (MCTS) (Li et al., 2024; Zhang et al., 2023; Hu et al., 2024; Hao et al., 2023; Feng et al., 2024b) are used to explore the inherent reasoning process, often based on the self-play mechanism to reflect on previously generated contents to learn from itself (Haluptzok et al., 2022; Chen et al., 2023a; Lu et al., 2023; Chen et al., 2023b; Madaan et al., 2024; Shinn et al., 2024). During inference, error position can be beneficial in improving the reliability and performance of the model. With identification and analysis of where and why errors occur, recent research (Yao et al., 2024a; Luo et al., 2024; Wu et al., 2025) has made significant strides in quantifying and mitigating errors during model inference. Refinement (Madaan et al., 2024; Gou et al., 2023) and reflexion (Shinn et al., 2024; Lee et al., 2025) are also powerful techniques for enhancing the inference capabilities of LLMs, usually by enabling iterative improvement and self-correction."}, {"title": "2.2 Model Composition", "content": "Model composition technique gains notable attention in cross-tasks generalization. Traditional methods for multiple tasks are to train models on a mixture of datasets of different skills (Caruana, 1997; Chen et al., 2018), with the high cost of data mixing and lack of scalability of the model though. Model merging is a possible solution to this. Linear merging is a classic merging method that consists of simply averaging the model weights (Izmailov et al., 2018; Smith and Gashler, 2017). Furthermore, Task Arithmetic (Ilharco et al., 2022) computes task vectors for each model, merges them linearly, and then adds back to the base, and SLERP (White, 2016) spherically interpolates the parameters of two models. Based on Task Arithmetic"}, {"title": "3 Preliminaries", "content": ""}, {"title": "3.1 Monte-Carlo Tree Search", "content": "Monte Carlo Tree Search (MCTS) is a decision-making algorithm widely used in environments with large state and action spaces, particularly in game AI and planning. It incrementally builds search trees to estimate optimal actions by simulating random plays from various nodes and gradually improving action-value estimates based on simulation outcomes. Over iterations, this approach gradually converges to near-optimal decision-making policies. Notably, its integration with reinforcement learning has driven breakthroughs in systems like AlphaGo and AlphaZero (Silver et al., 2017), achieving superhuman performance in games.\nClassical MCTS consists of four stages: selection, expansion, simulation, and backpropagation. It typically employs Upper Confidence Bounds for Trees (UCT) (Kocsis and Szepesv\u00e1ri, 2006), which balances exploration and exploitation by guiding the search to promising nodes. After simulation, results propagate back through the tree, updating node values. However, MCTS struggles in domains with large action spaces, where excessive branching can degrade performance. Progressive Widening and Double Progressive Widening techniques have been proposed to mitigate this by dynamically limiting the number of actions considered at each decision node (Coulom, 2006)."}, {"title": "3.2 LORA Finetuning", "content": "LORA (Low-Rank Adaptation) (Hu et al., 2021) fine-tuning is a technique used to adapt large pre-trained models, such as transformers, to specific tasks with minimal computational overhead. The key idea behind LoRA is to introduce low-rank matrices into the model's weight updates, which reduces the number of trainable parameters and makes fine-tuning more efficient.\nLORA starts with a model that has been trained on a large dataset. During finetuning, instead of updating the full weight matrix $W \\in R^{m \\times n}$, LORA introduces two low-rank matrices $A \\in R^{m \\times r}$ and $B \\in R^{r \\times n}$, where $r < min(m, n)$. The updated weight matrix W' is then given by:\n$W' = W + AW = W + A \\cdot B$.   (1)\nDuring fine-tuning, only the matrices A and B are updated, while the original weight matrix W remains frozen. This reduces the number of trainable parameters from $m \\times n$ to $m\\times r+r\\times n$, which is much smaller when r is small. For a given task with loss function L, the objective is to minimize:\n$L(y, f(x; W + A \\cdot B))$,   (2)\nwhere y is the target output, x is the input, and f is the model's forward function.\nBy introducing low-rank matrices, both the number of trainable parameters and memory footprint are reduced. This approach is particularly useful in scenarios where computational resources are limited or when fine-tuning needs to be done quickly."}, {"title": "4 Methodology", "content": "In this section, we introduce the overall methodology of BDC, addressing challenges in the System2-to-System1 pipeline for code generation, specifically the complexity of hidden reasoning processes and heterogeneous data distributions. The proposed BDC pipeline consists of three main stages: 1) explore the System 2 knowledge via mutual verification and boosting between LLMs; 2) disentangle the obtained data into clusters over which composable LoRA experts are tuned; 3) customize problem solver by weighting over the composable LORA experts using an input-aware hypernetwork."}, {"title": "4.1 System 2 Knowledge Exploration", "content": "In this subsection, we introduce the mechanism design for the data collection process. Due to the complex reasoning nature embodied, code blocks are hard to evaluate and estimate before mature. Reliable reward signals of a reasoning path therefore mainly depend on the dynamic compilation and execution feedbacks, which are extremely sparse and require extensive simulations. To simplify the generation paradigm and exploit the mutual verification capabilities of the collective searching, we decompose the generation process into two distinct stages: problem-to-thought and thought-to-solution."}, {"title": "4.1.1 Problem-to-thought", "content": "Traditional Monto-Carlo Tree Searching comprises three key operations in each iteration: (a) Select, (b) Expand, (c) Backup. In the problem-to-thought stage, we further extend MCTS by two distinct operations (d) Prune, and (e) Refine to reduce the searching space. We elaborate on these operations as follows.\nSelect. Starting from the root, the reasoning path is prolonged by iteratively adding a specific child of the latest node. The operation is usually governed by certain policies, among which we adopt Probability-weighted Upper Confidence Bound(PUCB) to balance the exploration and exploitation:\n$PUCB(Sc) = Q(S) + c\\cdot P(a|Sp) \\cdot \\sqrt{\\frac{log N(S)}{1 + N(Sc)}}$,  (3)\nwhere $S_c$ is the state of the child node. S and Q(S) denote the parent node's state and value. P(a|S) is the conditional probability of sampling the sequence a. N(S) is the total number of times the parent node S has been visited during simulations, while N($S_c$) tracks visits to the child node $S_c$. The selection process will stop if either a semantic or rule-based(e.g. length limits) terminal state encounters.\nExpand. The Expand operation is triggered if a non-terminal leaf node of the tree is selected. A set of predefined LLM polices $\\pi_0,\\cdot\\cdot\\cdot, \\pi_n$ generate subsequent thought sequences $a_i$in given the state S of the current node, forming new leaf nodes:\n$Vi \\in [n], P(a_i|S) \\sim \\pi_i(|S)$.  (4)\nBackup. For well-defined problems, a reasoning path $S_t$ will eventually end at a terminal leaf node $S_T$ by iterating the Select and Expand operations. The reward $r_t$ is set according to the evaluation. We will skip the definition of reward $r_t$ and passrate $PR(S_t)$, which will be detailed in the explanation of the Simulate operation. The reward value is back-propagated along the reasoning path to update the state values of corresponding ancestor nodes:\n$Q(S_{t-1}) = f(Q(S_t), r_t + \\gamma PR(S_t))$,  (5)\nwhere f is the value function.\nAdditionally, the visit counts of ancestors are updated alongside the reasoning path:\n$N(S_t) = N(S_t) + 1$.  (6)\nWe further extend and formalize reflective reason settings proposed in CoMCTS into Prune and Refine operations as shown in Figure 3.\nPruning. The pruning operation on a selected node will examine and compare its passrate with"}, {"title": "4.1.2 Thought-to-solution", "content": "Simulate. For the thought-to-solution, we repurpose the Simulate operation for the collective solution generation process from the given state S. The operation will produce a set of possible solutions, each from a policy LLM:\n$Solut.(S)_i \\sim \\pi_i(S)$.   (9)\nWe define the passrate of a state as the average passrate of its corresponding solutions:\n$PR(S) = \\frac{1}{n} \\sum_{i=1}^{n} Passed(Solut.(S)_i)$,  (10)\nwhere Passed(\u00b7) represents the supervising signal from dynamic compilation and execution feedback.\nThe node's value $Q(S_t)$ is determined by its $PR(S_t)$ and reward $r_t$. Sincere additional solution string will be appended to a non-terminal state $S_t$ before evaluation, $PR(S_t)$ is an indirect supervising signal for the $S_t$, and the direct signal $r_t$ is set to zero.\nThe terminal state $S_T$ is treated as the unique solution itself since no string concatenation applies, therefore featuring a non-trivial reward $r_T$. Putting everything together, we have:\n$Q(S) = \\begin{cases}r_T & \\text{if terminal}, \\\\PR(S_t) & \\text{otherwise}.\\end{cases}$   (11)"}, {"title": "4.2 System2-to-System1 Training", "content": ""}, {"title": "4.2.1 Heterogeneous Distribution\nDisentanglement", "content": "After the data collection, the resulting training data obtained from the MC-Tree-Of-Agents process consists of problem2thought data $D_{p2t} = \\{(X_{p2t}, y_{p2t})_i \\in P\\}$ and thought2solution data $D_{t2s} = \\{(X_{t2s}, y_{t2s})_i \\in P\\}$: $D_{train} = \\{D_{p2t}, D_{t2s}\\}$. As discussed in the introduction section, the latent patterns of coding problems are complex and tend to be heterogeneously distributed, e.g., the branching and recursion flow existing in the code blocks, different strategies of algorithm solutions, etc. Therefore, we disentangle the training data based on the latent semantics of the data into different clusters for fine-grained modeling.\nThe clustering objective can be summarized as below:\n$minimize_C \\sum_k \\sum_{i\\in C_k} cosine(e_i, \\mu_k)$, (12)\n$e_i = \\Phi_{\\theta}((X_i, Y_i))$,\n$\\mu_k = mean\\{e_i|i \\in C_k\\}$,\nwhere theta is the encoder of a code llm and $\\mu_k$ denotes the centroid of cluster $C_k$."}, {"title": "4.2.2 Composable LoRA Experts Preparation", "content": "Having obtained the disentangled data clusters, we then finetune on them to obtain the meta LoRA experts for specialized experts of different aspects.\n$\\forall C_k \\in C$,\n$\\pi_{\\theta_k} \\leftarrow SFT(\\pi_{\\theta}, \\{(X_i, Y_i)|i \\in C_k\\})$, (13)\nwhere $\\pi_{\\theta}$ denotes the base LLM and $\\pi_{\\theta_k}$ denotes the parameters of the LoRA adapter obtained by finetuning $\\pi_{\\theta}$ on $C_k$."}, {"title": "4.2.3 Input-Aware Hypernetwork for\nCustomized Solver", "content": "Given specialized LoRA experts $\\pi_{\\theta_1},\\cdot\\cdot\\cdot, \\pi_{\\theta_K}$ trained on distinct data clusters, we design an input-aware Hypernetwork $f(\\cdot)$ to dynamically compose these experts through rank-wise adaption for customized problem solver.\nFor each input instance, the hypernetwork generates customized expert weights digesting its encoding and semantic distances to the cluster centroids. we identify \"rank\" as the minimal unit for aggregation and generate rank-wise weights for different experts at each decoding layer:\n$G_i = f(e_i, (cosine(e_i, \\mu_1), ..., cosine(e_i, \\mu_K)))$,  (14)\nwhere $e_i$ is the encoding of input $X_i$, $G_i \\in R^{K\\times r\\times 1}$ is the output weight matrix, r is the rank of the LORA matrices, and K is the number of LORA experts.\nThe aggregated $\\Delta W$ of the linear projection layer is then obtained by\n$\\begin{aligned} \\Delta^* &=[\\Delta_1, ..., \\Delta_K] \\odot G_i, \\\\  \\Delta W^* &=[B_1 \\Delta_1, ..., B_K \\Delta_K], \\\\  \\Delta W &= ReduceSum(\\Delta W^*). \\end{aligned}$ (15)\n (16)\n (17)\nThe projection output of $\\Delta W$ is then merged during forwarding via:\ny = W_0x + \\Delta Wx.  (18)\nWe adopt a dedicated training phase for the Hypernetwork where all parameters are frozen except for the f(.). The training is supervised by the cross-entropy loss, with the randomly permuted input-output pairs from $D_{train}$."}, {"title": "5 Experiments", "content": "We conduct empirical studies starting from the following research questions."}, {"title": "5.2 Empirical Analysis and Discussion", "content": ""}, {"title": "5.2.1 RQ1. MC-Tree-Of-Agents", "content": "We evaluate MC-Tree-Of-Agents against widely-used baseline methods, the results are summarized in Table 1. From the results, we can draw the following conclusions.\n\u2022 The proposed MC-Tree-Of-Agents outperforms all the baseline methods, which effectively explores the insightful System 2 knowledge.\n\u2022 Comparing with the single LLM as agents version, MC-Tree-Of-Agents allows for mutual verification and boosting between different LLMs, offering a superior performance over each distinct-LLM-as-agent method. This showcases the effectiveness of the interaction between LLMs of different wisdom.\n\u2022 The pruning and refinement operations both contribute to the final performance, offering a notable accuracy gain. This validates that the designed pruning and refinement mechanism, based on the difference between rewards of"}, {"title": "5.2.2 RQ2. Impact of latent patterns", "content": "To study the distribution of the latent patterns of coding problems, we first conduct the T-SNE visualization on the encodings of reasoning data collected by MC-Tree-Of-Agents on APPS dataset. The visualization is displayed in Figure 4."}, {"title": "5.2.3 RQ3. Effectiveness of the Experts\nComposition", "content": "During the empirical study, we test different model merging methods that combine wisdom from different LoRA experts. We evaluate the well-known Ties, Dare, and the recently proposed TWIN merging methods. All of them yield a static composed model that takes in the strength of the candidate experts to be merged via solving parameter interference. From the results, we can see that merging over decomposed LoRA-experts can offer more robust problem solvers, outperforming the simple train-once-for-all mechanism. The experiments justify our major rationale that disentanglement-and-compose pipeline can offer more robust System2-to-System1 performance."}, {"title": "5.2.4 RQ4. Superiority of DisenLoRA over\nother composition methods", "content": "Although the static-composed expert model can promote robustness to some extent, its static nature lacks flexibility to different styles of inputs. As discussed in the previous contents, the data distribution of coding problems is complex, making the one-fits-all mechanism easy to fail. Therefore, we design DisenLoRA algorithm to yield a customized problem solver with input-awareness. From the results, we can see that DisenLoRA outperforms the competing merging methods, validating the effectiveness of the proposed input-aware hypernetwork that dynamically aggregates the candidate compos-"}, {"title": "5.2.5 RQ5. Discussion of the Cross-Dataset\nGeneralization of DisenLoRA", "content": "Despite the flexibility offered by the input-aware hypernetwork, its performance may degrade on new datasets where the hypernetwork is not trained. To study this scenario, we use the model trained on APPS to generate solutions for CodeContest and use the model trained on CodeContest to generate solutions for APPS. The results are displayed in Table 3.\nFrom the results, we can see that the proposed DisenLoRA has the generalization ability to the untrained dataset, outperforming the train-once-for-all mechanism still. This demonstrates that the parameters of the trained hypernetwork have the awareness of semantic similarities across datasets."}, {"title": "6 Conclusion", "content": "We identify the complexity of inherent reasoning exploration and the heterogeneous data distribution problems that hinder the performance of System2-to-System1 methods. Correspondingly, we propose the BDC pipeline that explores insightful System2 knowledge via mutually Boosting between Ilm agents, Disentangle heterogeneous data distribution for composable LoRA experts, and Customize problem solver for each instance, offering flexibility and robustness. Correspondingly, we propose the MC-Tree-Of-Agents algorithm to efficiently and effectively explore the insightful System2 knowledge via mutual verification and boosting of different LLM agents, armed with reward-guided pruning and refinement to explore more beneficial states in limited rollouts for better performance. Additionally, we design an input-aware hypernetwork to aggregate over the disentangled composable LoRA experts trained on different clusters of data collected from MC-Tree-Of-Agents. This mechanism offers a customized problem solver for each data instance. Various experiments and discussions validate the effectiveness of different model components."}, {"title": "Limitations", "content": "While our work presents an efficient pipeline for transferring specialized knowledge from collective system-2-like LLMs to locally deployed language models through multiple LoRA adapters\u2014enabling rapid, precise, system-1-like reasoning\u2014three limitations merit discussion. First, despite code generation serving as an effective proxy for complex reasoning, our evaluation is restricted to this domain, leaving open questions about generalizability to broader textual reasoning tasks (e.g., common-sense reasoning and semantic parsing). Second, while we focus on their performance on the specific benchmarks, the safety alignment of derived models remains unaddressed. Systematic evaluation is required to assess whether our distilled experts preserve human values and mitigate harmful outputs. Finally, our ensemble methodology for LORA experts, while input-aware, does not fully exploit potential sparsity optimizations in parameter activation, leaving room for computational efficiency improvements through advanced routing mechanisms."}]}