{"title": "Multimodal contrastive learning for spatial gene expression prediction using histology images", "authors": ["Wenwen Min", "Zhiceng Shi", "Jun Zhang", "Jun Wan", "Changmiao Wang"], "abstract": "In recent years, the advent of spatial transcriptomics (ST) technology has unlocked unprecedented opportunities for delving into the complexities of gene expression patterns within intricate biological systems. Despite its transformative potential, the prohibitive cost of ST technology remains a significant barrier to its widespread adoption in large-scale studies. An alternative, more cost-effective strategy involves employing artificial intelligence to predict gene expression levels using readily accessible whole-slide images (WSIs) stained with Hematoxylin and Eosin (H&E). However, existing methods have yet to fully capitalize on multimodal information provided by H&E images and ST data with spatial location. In this paper, we propose mclSTExp, a multimodal contrastive learning with Transformer and Densenet-121 encoder for Spatial Transcriptomics Expression prediction. We conceptualize each spot as a \"word\", integrating its intrinsic features with spatial context through the self-attention mechanism of a Transformer encoder. This integration is further enriched by incorporating image features via contrastive learning, thereby enhancing the predictive capability of our model. Our extensive evaluation of mclSTExp on two breast cancer datasets and a skin squamous cell carcinoma dataset demonstrates its superior performance in predicting spatial gene expression. Moreover, mclSTExp has shown promise in interpreting cancer-specific overexpressed genes, elucidating immune-related genes, and identifying specialized spatial domains annotated by pathologists. Our source code is available at https://github.com/shizhiceng/mclSTExp.", "sections": [{"title": "Introduction", "content": "With the rapid development of ST technology, we may gain a more comprehensive understanding of gene expression patterns within complex biological systems [1]. Compared to traditional transcriptomics, this technology enables high- throughput RNA sequencing across entire tissue sections while preserving spatial information regarding cell locations within tissue slices, allowing researchers to visually observe the spatial distribution of gene expression [2, 3]. The insights afforded by this technology extend beyond gene expression, offering novel perspectives on cell-cell interactions and molecular signaling pathways within the research domain [4]. Despite these advancements, effectively harnessing the unique attributes of ST data to investigate spatial gene expression patterns and develop spatial gene detection methodologies at varying resolutions remains challenging [5]. In order to fully utilize spatial location information, several novel computational methods have been developed for spatial domain recognition (SEDR [6], STAGATE [7], CCST [8], STMask [9] etc.) and exploring super-resolution gene expression patterns (BayesSpace [5], iStar [10], TESLA [11], etc.) and imputing ST data [12].\nDespite the rapid development of ST technology, the cost of generating such data remains relatively high, thus limiting the applicability of ST technology in large-scale studies. In contrast, whole-slide images (WSIs) [13, 14, 15] stained with Hematoxylin and Eosin (H&E) are more readily available, cost- effective, and widely used in clinical practice. Using H&E images to predict ST gene expression profiles has become a more common and cost-effective research approach [16, 17]. In recent studies, Schmauch et al. confirmed the feasibility of using H&E images to predict ST gene expression profiles [18]. Their developed HE2RNA method performed excellently in capturing subtle structures within H&E images, revealing critical tumor regions specific to certain cancer types.\nPathological images (such as H&E images) reveal the cellular structure, morphological features, and pathological changes within tissues, while ST technology elucidates gene expression patterns and their spatial distribution. Integrating this information is crucial for a deeper understanding of disease pathogenesis, prognosis assessment, and the development of personalized treatment strategies [19, 20, 21]. Several methods, such as STnet [22], HisToGene [16], His2ST [23], THItoGene [24], and Bleep [25], have been explored for integrating histopathological images with transcriptomic data. STnet segments tissue slice images into different patches and encodes each patch using DenseNet [26], which are then embedded into the feature space and projected onto the dimension of gene expression through fully connected layers. HisToGene employs a Vision Transformer (ViT) [27] to encode each patch and enhances spatial relationships between patches through a self-attention mechanism. His2ST introduces a graph neural network (GNN) [28] to better learn spatial relationships between spots, thus improving performance. THItoGene utilizes H&E images as input and employs dynamic convolutional and capsule networks to capture signals of potential molecular features within histological samples. Bleep utilizes a contrastive learning [29] approach, introducing image and gene expression encoders to learn joint embedding in space.\nHowever, none of the aforementioned methods have effectively integrated the multimodal information provided by H&E images and ST data with spatial location. To address this issue, we propose mclSTExp, a multimodal deep learning approach utilizing Transformer and contrastive learning architecture. Inspired by the field of natural language processing, we regard the spots detected by ST technology as \"words\" and the sequences of these spots as \"sentences\" containing multiple \"words\". We employ a self-attention mechanism to extract features from these \"words\" and combine them with learnable position encoding to seamlessly integrate the positional information of these \"words\". Subsequently, we employ a contrastive learning framework to fuse the combined features with image features. Our experimental results demonstrate that mclSTExp accurately predicts gene expression in H&E images at different spatial resolutions. This is achieved by leveraging the features of each spot, its spatial information, and H&E image features. Additionally, mclSTExp demonstrates the ability to interpret specific cancer- overexpressed genes, immunologically relevant genes, preserve the original gene expression patterns, and identify specific spatial domains annotated by pathologists (Supplementary Note 1)."}, {"title": "Materials and Methods", "content": "The proposed mclSTExp and competing methods are evaluated on three real datasets. The detailed description of the datasets and the preprocessing process can be found in the Supplementary Note 2 and Table S1."}, {"title": "Overview of mclSTExp", "content": "The proposed mclSTExp learns a multimodal embedding space from H&E images, spot gene expression data, and spot positional information (Figure 1). Specifically, the image is passed through an image encoder to capture visual features, while the spot's gene expression data along with its positional encoding is input to the Spot encoder to capture fused features incorporating spatial information. Contrastive learning is then applied to the obtained visual features and fused features, maximizing the cosine similarity of embedding for truly paired images and gene expressions, while minimizing the similarity for incorrectly paired embedding. This facilitates the fusion of image features, thereby further enhancing the model's representational capacity.\nTo predict spatial gene expression from an test image, the image is fed into the image encoder to extract its visual features. Subsequently, the cosine similarity is computed between the obtained visual features and the features of N spots (consistent with the training process). The top k spot features with the highest similarity scores are selected, and their corresponding ground truth gene expressions are weightedly aggregated to infer the gene expression of the test images."}, {"title": "Image and Spot encoders", "content": "We segment 224 x 224 pixel image patches from H&E images based on the positions of spots. For each extracted image patch Patchi, we utilized pre-train DenseNet-121 to embed it into feature z Patch, followed by projecting it into a feature space using a projection layer. In contrast to the skip connections in ResNet [30], the dense connectivity mechanism in DenseNet- 121 enhances the reusability of features, aiding the neural network in capturing image features more effectively and thereby strengthening the model's expressive capability [26].\n\\(z_{patch} = Densenet-121(patch_i),\\) (1)\n\\(h_{patch} = MLP(z_{patch}).\\) (2)\nInspired by the field of natural language processing, we regard the spots detected by ST technology as \"words\" and the sequences of these spots as \"sentences\" containing multiple \"words\". We employ a self-attention mechanism to extract features from these \"words\" and combine them with learnable position encoding to seamlessly integrate the positional information of these \"words\". Multi-head attention is an extension of the attention mechanism, enhancing the model's ability to capture complex patterns and global information in input sequences by simultaneously learning multiple independent sets of attention weights as follows:\n\\(MHSA(Q, K, V) = [head_1, ..., head_n]W^o,\\) (3)\nwhere \\(W^o\\) represents the weight matrix used for aggregating the attention heads, while n denotes the number of heads. Additionally, Q, K, and V correspond to Query, Key, and Value, respectively. The attention mechanism is defined as follows:\n\\(head_i = Attention(QW_i^Q, KW_i^K, VW_i^V),\\) (4)\n\\(Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V,\\) (5)\nwhere \\(W_i^Q\\), \\(W_i^K\\) and \\(W_i^V\\) are weight matrices. The term \\((QK^T)\\) is called Attention Map, whose shape is \\(N X N\\). The term V is the value of the self-attention mechanism, where V = Q = K.\nRegarding the positional information of spots, each spot's coordinates (x, y) are represented by a matrix of size N x 2. The x-coordinate information is transformed into a one-hot encoding matrix \\(P_x\\) of size \\(N x n\\), where n is the maximum number of x-coordinates across all tissue sections. For the all datasets, n = 65536. Then, the matrix is linearly transformed using the learnable linear layer \\(W^x\\) to obtain an N \u00d7 hvg_num matrix \\(S_x\\) that maintains the same dimensions as the Spots (\\(Spots \\in R^{spot\\_num, hvg\\_num}\\)). Similarly, the y-coordinate vector undergoes a similar transformation to obtain an N \u00d7 hvg_num encoding matrix \\(S_y\\). Finally, the spot feature, x-coordinate encoding matrix \\(S_x\\), and y-coordinate encoding matrix \\(S_y\\) are combined and passed through a multi-head attention mechanism using Eq.(3):\n\\(z_{spot} = MHSA (Spot + S_x + S_y),\\) (6)\nThen, project it into a feature space using a projection layer:\n\\(h_{spot} = MLP(z_{spot}).\\) (7)\nIn this feature space, the dimensions of \\(h_{patch}\\) and \\(h_{spot}\\) are both N x 256.\nWe utilize a self-attention mechanism to integrate the gene expression features and spatial location features of spots. This multimodal feature representation not only integrates critical information from gene expression but also takes into account the specific spatial location of each point within the tissue image. As a result, each spot in the feature space exhibits a more distinct and enriched expression. Specifically, the partitioning of H&E image patches is based on the positions of spots. Therefore, spots and patches located at the same position inherently form a positive sample pair, while those at different positions constitute negative sample pairs."}, {"title": "Contrastive learning module", "content": "We adopt a contrastive learning approach to reduce the distance between positive sample pairs and increase the distance between negative sample pairs, thereby achieving the fusion of image information. Specifically, in each batch comprising N pairs of (patch, spot). We utilize the mclSTExp algorithm to simultaneously train both the image encoder and Spot encoder, aiming to construct a multimodal embedding space. The optimization objective of this space is to maximize the cosine similarity of N positive sample pairs and simultaneously minimize the cosine similarity of \\(N^2 - N\\) negative sample pairs. We employ the loss function of CLIP [31] and fine-tune it to suit our task.\nFor integrating positive sample pairs, we employ a label matrix where diagonal elements represent positive sample pairs (labeled as 1), and non-diagonal elements represent negative sample pairs (labeled as 0). Subsequently, we utilize the cross-entropy loss function to achieve effective classification. To show the overall loss of our model, we first define the cosine similarity function cos_sim between patch and spot embedding as follows:\n\\(cos\\_sim(h_{Patch}, h_{spot}) = h_{Patch} . (h_{spot})^T,\\) (8)\nwhere the \"label\" matrix is defined as:\n\\(label =\begin{bmatrix}1 & 0 & ... & 0 \\0 & 1 & ... & 0 \\: & : & ... & : \\0 & 0 & ... & 1\\end{bmatrix}\\) (9)\nAnd the cosine similarity function between spot and patch embedding is defined as:\n\\(cos\\_sim(h_{spot}, h_{patch}) = h_{spot}. (h_{Patch})^T.\\) (10)\nTwo individual loss components, \\(Loss_{image}\\) and \\(Loss_{spot}\\), are computed using the cross-entropy loss function (CE_Loss).\n\\(Loss_{image} = CE\\_Loss(cos\\_sim(h_{Patch}, h_{spot}), label),\\) (11)\n\\(Loss_{spot} = CE\\_Loss(cos\\_sim(h_{spot}, h_{patch}), label),\\) (12)\nwhere \\(Loss_{image}\\) is based on the similarity between the image embedding and the transpose of spot embedding, while \\(Loss_{spot}\\) is based on the similarity between spot embedding and the transpose of image embedding.\nFinally, the overall loss of our model is calculated as the average of these two losses:\n\\(Loss = (Loss_{image} + Loss_{spot})/2.\\) (13)"}, {"title": "Results", "content": "The details on the baseline methods, experimental settings, and evaluation citeria can be found in Supplementary Notes 3, 4 and 5."}, {"title": "mclSTExp can improve the prediction accuracy", "content": "To assess the performance of mclSTExp, we analyze the HER2+ breast cancer dataset, which includes 32 tissue sections, the CSCC dataset with 12 tissue sections, and the Alex+10x dataset with 9 slices (Table 1). For the evaluation of gene expression prediction accuracy, we conduct leave-one-out cross-validation. Specifically, for each dataset, we used one slice as the test set and the remaining slices as the training set. For each tissue section, we computed the PCC for all considered genes (ACG) as well as the top 50 highly expressed genes (HEG), along with the MSE and MAE for all considered genes. Subsequently, the average values of PCC (ACG), PCC (HEG), MSE, and MAE across all tissue sections were calculated to evaluate the overall model performance. We compared mclSTExp with five other recently developed advanced methods for predicting spatial gene expression. Considering that the gene expression prediction task emphasizes capturing relative changes, we prefered evaluation metrics related to PCC. As shown in Table 1, mclSTExp achieved the highest average PCC for both ACG and HEG across these three datasets. Specifically, the PCC (ACG) of mclSTExp was 23.01%, 32.09%, and 25.57% higher than that of the second-ranked method BLEEP on these three datasets, while the PCC (HEG) was 32.89%, 36.48%, and 27.82% higher, respectively.\nTo examine the results of each slice individually, we visualized the PCC between the gene expression predicted by mclSTExp and the observed gene expression on each slice. As depicted in Figure 2, mclSTExp attained the highest PCC among the 32 slices in the HER2+ dataset, achieving this distinction on 26 slices. However, for slices E1- F3, the PCC values across all methods were relatively low, suggesting potential issues with gene detection sensitivity or specificity in ST technology. Notably, mclSTExp consistently outperformed the second-ranked method Bleep across all slices. Additionally, mclSTExp demonstrated the highest PCC across all tissue sections in the cSCC dataset, as depicted in Figure 3. Noteworthy is its substantial improvement in predicting gene expression correlation, particularly for the P10_ST_rep3 section. In Figure 4, mclSTExp exhibited the highest PCC on 7 out of 9 slices in the Alex+10x dataset. However, for slice 1142243F, the PCC scores for all methods were notably low. This lower score may be attributed to various factors, including a weak correlation between the expression of specific genes and morphological features, suboptimal detection of certain genes by the Visium platform leading to challenges in predicting their expression, and the potential influence of non-biological variations introduced artificially during the experiment, independent of the image itself."}, {"title": "Visualization of the predicted gene expression", "content": "To further evaluate the predicted gene expression, we explored whether the gene expression predicted by mclSTExp accurately reflected the actual status of tumor-related genes. Across all datasets, we analyzed the correlation between observed gene expression and predicted gene expression, calculating correlation coefficients and P-values for each spot. Subsequently, we computed the average log10 (P-values) for all genes. These genes were ranked in descending order of their log10 (P-values), as detailed in Supplementary Table S2. For the HER2+ dataset, we visualized the top seven genes: GANS, FN1, FASN, HLA-B, SCD, IGKC, and HLA- DRA. As shown in Figure 5, the PCCs for these genes using mclSTExp were 0.840, 0.815, 0.780, 0.844, 0.808, 0.629, and 0.833, respectively, surpassing those predicted by the second-ranked method, Bleep, by 11.1%, 18.6%, 18.5%, 4.7%, 28.4%, 16.2%, and 5.3%. Particularly, for the gene IGKC, the correlation coefficient with HisToGene was -0.234, and for the gene HLA-DRA, it was -0.058 with STnet.\nIt is noteworthy that all of the top seven genes identified by mclSTExp are closely linked to breast cancer, playing pivotal roles in its onset and progression. Elevated expression of GANS can activate the PI3K/AKT/Snail1/E-cadherin pathway, thereby facilitating the proliferation, migration, and invasion of breast cancer cells [32]. FN1 is recognized as a potential therapeutic target or clinical prognostic marker for breast cancer, as its heightened expression is closely associated with the metastasis and deterioration processes in breast cancer [33]. FASN exhibits high expression in cancer stem cells, and its inhibition effectively suppresses the proliferation and survival of breast cancer cells [34]. Moreover, the proliferation, survival, and aggressiveness of breast cancer cells are closely linked to SCD, underscoring its potential as a therapeutic target in breast cancer treatment strategies [35]. Additionally, IGKC serves as a prognostic marker with significant value in predicting disease progression and survival outcomes in breast cancer patients [36].\nSpecifically, among all the compared methods, mclSTExp was the first to predict the genes HLA-B and HLA-DRA. For one thing, Human leukocyte antigen B (HLA-B) belongs to the major histocompatibility complex (MHC) class I molecules, primarily responsible for the presentation of intracellular peptides. A study [37] have indicated that the expression of HLA-B is associated with the survival and recurrence rates of breast cancer patients. For another, HLA-DRA is a class II MHC molecule typically expressed in professional antigen- presenting cells. Research [38] has demonstrated that HLA- DRA serves as a significant prognostic factor for breast cancer. Its expression levels may represent a pathway to enhance the treatment of advanced breast cancer and improve overall survival rates. Another study [39] has highlighted how cancer cells exploit various immune system functions to promote their growth. In summary, the mclSTExp method not only elucidates cancer-specific overexpressed genes but also identifies immune-related genes, providing valuable insights for cancer therapy.\nTo assess the robustness of our method, we visualized the top seven genes using the same strategy in both the cSCC dataset and the Alex 10x dataset, as detailed in Supplementary Table S3 and Figure S1. These genes have been previously found to be highly associated with human cutaneous squamous cell carcinoma and breast cancer in prior studies [40, 41].\nAdditionally, we computed the correlation matrix using the expression data of actual genes. Subsequently, hierarchical clustering was performed on this correlation matrix to obtain the clustering order of the samples. Next, we calculated the gene-gene correlations using predicted expression values obtained from various methods, reordered the correlation matrix according to the clustering order, and generated a heatmap of the correlations (Supplementary Figure S2). The results indicate that mclSTExp effectively preserves the patterns of gene-gene co-expression and biological heterogeneity."}, {"title": "Spatial region detection", "content": "To evaluate the performance of various methods in identifying specific spatial domains on entire H&E images, we compared six tissue slices from the HER2+ dataset. These slices have been annotated by pathologists for spatial transcriptomic analysis. Initially, we employed PCA dimensionality reduction on the predicted data from mclSTExp, followed by K-Means clustering, as detailed in Supplementary Note 6 and Figure S3."}, {"title": "Ablation studies", "content": "To further investigate the contributions of each component of mclSTExp, we conducted a series of ablation experiments on the HER2+, CSCC, and Alex+10x datasets (Supplementary Note 7, Figure S4, Tables S4, S5 and S6)."}, {"title": "Discussion and Conclusion", "content": "In this study, we propose mclSTExp, a multimodal deep learning approach utilizing Transformer and contrastive learning framework for predicting gene expression from H&E images (Figure 1). Inspired by the field of natural language processing, we regard the spots detected by ST technology as \"words\" and the sequences of these spots as \"sentences\" containing multiple \"words\". We employ a self-attention mechanism to extract features from these \"words\" and combine them with learnable position encoding to seamlessly integrate the positional information of these \"words\". Subsequently, we adopt a contrastive learning approach, maximizing the cosine similarity of positive samples to bring the correctly matched image blocks and \"words\" pair samples closer, while minimizing the cosine similarity of negative samples to push away incorrectly matched samples, thereby integrating image information. Through this approach, we learn a multimodal embedding space. Finally, we select the features of the top k \"words\" with the highest cosine similarity, and then aggregate their true expression spectra by weight to infer the gene expression of the test data.\nmclSTExp enables us to predict gene expression from H&E images more accurately. Based on our experimental results, the PCC (ACG) of mclSTExp was 23.01%, 32.09%, and 25.56% higher than that of the second-ranked method BLEEP on the HER2+ dataset, CSCC dataset, and Alex+10x dataset, respectively. Similarly, the PCC (HEG) was 32.89%, 36.48%, and 27.82% higher, respectively. Additionally, mclSTExp exhibits the capability to interpret cancer-specific overexpressed genes and identify specific spatial domains annotated by pathologists."}]}