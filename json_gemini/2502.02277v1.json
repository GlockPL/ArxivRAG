{"title": "Error Distribution Smoothing: Advancing\nLow-Dimensional Imbalanced Regression", "authors": ["Donghe Chen", "Jiaxuan Yue", "Tengjie Zheng", "Lanxuan Wang", "Lin Cheng"], "abstract": "In real-world regression tasks, datasets frequently\nexhibit imbalanced distributions, characterized\nby a scarcity of data in high-complexity regions\nand an abundance in low-complexity areas. This\nimbalance presents significant challenges for\nexisting classification methods with clear class\nboundaries, while highlighting a scarcity of\napproaches specifically designed for imbalanced\nregression problems. To better address these is-\nsues, we introduce a novel concept of Imbalanced\nRegression, which takes into account both the\ncomplexity of the problem and the density of data\npoints, extending beyond traditional definitions\nthat focus only on data density. Furthermore, we\npropose Error Distribution Smoothing (EDS) as a\nsolution to tackle imbalanced regression, effec-\ntively selecting a representative subset from the\ndataset to reduce redundancy while maintaining\nbalance and representativeness. Through several\nexperiments, EDS has shown its effectiveness,\nand the related code and dataset can be accessed\nat\nhttps://anonymous.4open.science/r/Error-\nDistribution-Smoothing-762F.", "sections": [{"title": "1. Introduction", "content": "In the realm of supervised learning, real-world datasets fre-\nquently exhibit skewed distributions, where certain regions\nof the outcome space contain significantly fewer samples\n(Zhang et al., 2023; Ding et al., 2022). This data imbalance\ncan adversely affect the predictive performance of models,\nparticularly for less represented outcomes, and has driven\nthe development of various mitigation techniques (Song\net al., 2022; Chawla et al., 2002; He et al., 2008). While\nmuch of the existing literature focuses on addressing im-\nbalances in categorical outcomes (Buda et al., 2018), many\npractical applications involve continuous variables such as\nage or health metrics (e.g., heart rate, blood pressure). Dis-\ncretizing continuous variables into categories neglects their\ninherent ordinal relationships and continuity, potentially\nleading to suboptimal model performance. Therefore, there\nis a need for specialized approaches that consider the unique\ncharacteristics of continuous targets to improve regression\nperformance.\nImbalanced regression presents distinct challenges not fully\naddressed by methods designed for classification tasks and\nhas received relatively limited attention in the literature\n(Yang et al., 2021). Unlike classification, regression involves\npredicting continuous labels without clear class boundaries,\ncomplicating the direct application of traditional imbalance\nmitigation techniques such as resampling or reweighting.\nFor example, in the prediction of house prices, each instance\ntypically has a unique value, making it impractical to ap-\nply class-based methods directly. Existing solutions have\nadapted techniques like SMOTE for regression by gener-\nating synthetic samples through interpolation (Camacho &\nBacao, 2024) or utilizing bagging-based ensemble methods\n(Branco et al., 2018). However, these adaptations often\nfail to account for the varying complexity of the underly-\ning functions and the differing sample requirements across\ndifferent approaches.\nWe introduce a novel definition for imbalanced regression\nthat incorporates both data density and regression complex-\nity via the Complexity-to-Density Ratio (CDR). Unlike\ntraditional definitions that focus solely on data density, it\nprovides a more comprehensive framework for evaluating\nand addressing low-dimensional imbalanced regression is-\nsues. To effectively address the challenges of imbalanced\nregression, we propose Error Distribution Smoothing (EDS),\nwhich selectively reduces data pairs in overrepresented re-\ngions to eliminate redundancy and smooths the distribution\nof prediction errors across different regions. This approach\nensures consistent model performance in underrepresented\nregions while maintaining accuracy in overrepresented re-\ngions. We also provide a set of diverse datasets for bench-\nmarking to support the practical assessment of imbalanced\nregression solutions."}, {"title": "2. Related Work", "content": "In the field of imbalanced classification, solutions are gen-\nerally categorized into data-centric and algorithmic ap-\nproaches. Data-centric methods address class imbalance\nby altering dataset composition through techniques such as\nminority oversampling or majority undersampling (Chawla\net al., 2002; He et al., 2008), with SMOTE being a promi-\nnent technique for generating synthetic instances via linear\ninterpolation (Chawla et al., 2002; Zhang et al., 2018; Verma\net al., 2019). Algorithmic methods enhance classifier perfor-\nmance by adjusting the learning process, including loss func-\ntion weighting (?Cui et al., 2019), transfer learning (Singh\net al., 2021), and meta-learning (Shu et al., 2019). Recent\nstudies highlight that semi-supervised and self-supervised\nlearning paradigms can significantly improve classification\noutcomes in imbalanced datasets (Yang & Xu, 2020).\nImbalanced regression presents distinct challenges and has\nreceived relatively less attention in the literature compared\nto classification tasks. Adaptations from classification ap-\nproaches have been applied, such as generating synthetic\nobservations for sparsely populated target regions through\ninterpolation (Camacho & Bacao, 2024) or by introducing\nGaussian noise (Branco et al., 2017). Moreover, ensem-\nble methods that integrate multiple preprocessing strategies\nhave also been investigated (Branco et al., 2018). Nonethe-\nless, existing methods often fail to account for the varying\ncomplexity of regression across different regions, leading\nto differing data requirements that are not adequately ad-\ndressed, an issue this paper resolves by proposing a novel\napproach to effectively capture and manage these regional\ncomplexities."}, {"title": "3. Variable Definition", "content": "To quantitatively evaluate imbalanced regression problems,\nwe have developed two metrics alongside an optimization\napproach. The Complexity-to-Density Ratio (CDR) as-\nsesses whether regions within the data feature space contain\na sufficient number of data pairs relative to their complexity.\nThe mathematical notations for imbalanced regression are\ndefined and the Complexity-to-Density Ratio (CDR) is intro-\nduced. Table 1 outlines key symbols and their descriptions,\nwith the notation for feature vectors, label vectors, datasets,\nand measures of region size and complexity being estab-\nlished.\nDefinition 3.1 (Complexity-to-Density Ratio (CDR)). The\nComplexity-to-Density Ratio (CDR) for a region \u03a9 is given\nby"}, {"title": null, "content": "$\\rho(\\Omega, D) = \\frac{g_c(\\Omega)}{|\\Omega \\cap D|/g_s(\\Omega)} = \\frac{g_c(\\Omega) \\cdot g_s(\\Omega)}{|\\Omega \\cap D|}$ (1)\nwhere $|\\Omega \\cap D|$ denotes the number of data pairs in D that\nlie within \u03a9. The functions $g_c(\\cdot)$ and $g_s(\\cdot)$ can be defined\nas follows:\n\u2022 $g_c(\\Omega)$ measures complexity via the maximum Frobe-\nnius norm of the Hessian tensor across \u03a9:"}, {"title": null, "content": "$g_c(\\Omega) = \\max_{x \\in \\Omega} || H(x)||_F$ (2)\n\u2022 $g_s(\\Omega)$ quantifies the region's size through the max-\nimum squared Euclidean distance between any two\npoints in \u03a9:"}, {"title": null, "content": "$g_s (\\Omega) = \\max_{x_1,x_2 \\in \\Omega} ||x_1-x_2||^2$ (3)\nNote that $g_c(\\cdot)$ and $g_s(\\cdot)$ are tailored for our specific context\nand can be adjusted based on the dataset and task. Their\ndefinitions should align with the problem characteristics and\ndesired CDR properties.\nTo quantify the distribution of complexity-to-density ratios\nacross all regions $\\mathbb{F} = {\\Omega_i}_{i=1}^k$, we define the Log-CDR\ndistribution $(\\mathcal{N}(\\mu, \\sigma^2))$. It models the log-transformed\nCDR as a normal distribution, summarizing dataset imbal-\nance.\nDefinition 3.2 (Log-CDR distribution). The Log-CDR dis-\ntribution $\\mathcal{I}(\\mathbb{F}, D) = \\mathcal{N}(\\mu, \\sigma^2)$ is constructed by estimating\nthe parameters $\\mu$ and $\\sigma^2$ of the underlying normal distribu-\ntion from the transformed values {ln($\\rho(\\Omega_j, D)$)}$_{j=1}^k$ using\nMaximum Likelihood Estimation (MLE):"}, {"title": null, "content": "$\\mu = \\frac{1}{k}\\sum_{j=1}^{k}ln(\\rho(\\Omega_j, D))$"}, {"title": null, "content": "$\\sigma^2 = \\frac{1}{k-1}\\sum_{j=1}^{k}(ln(\\rho(\\Omega_j, D))) -\\mu)^2$ (4)"}, {"title": null, "content": "Regions are classified into High-CDR, Medium-CDR, and\nLow-CDR categories based on their complexity-to-density\nratio ($\\rho(\\Omega_j, D)$) relative to the estimated mean ($\\mu$) and stan-\ndard deviation ($\\sigma$), with $z > 0$ indicating the number of\nstandard deviations from the mean. The classification is\ndetermined by the following inequalities:"}, {"title": null, "content": "$\\Omega = \\begin{cases}\nHigh-CDR, & \\text{if } ln(\\rho) > \\hat{\\mu} + z\\hat{\\sigma}\\\\\nMedium-CDR, & \\text{if } \\hat{\\mu} - z\\hat{\\sigma} \\leq ln(\\rho) \\leq \\hat{\\mu} + z\\hat{\\sigma}\\\\\nLow-CDR, & \\text{if } ln(\\rho) < \\hat{\\mu} - z\\hat{\\sigma}\n\\end{cases}$ (5)"}, {"title": "4. Problem Formulation and Reconstrunction", "content": "The optimization problem is formulated to select the small-\nest possible representative subset $D_R$ from the entire dataset\n$D$, ensuring that strict complexity-to-sample density ratio\n(CDR) constraints are adhered to while minimizing the num-\nber of samples. This approach aims to mitigate dataset im-\nbalance and manage risk at a specified confidence level. The\noptimization problem is defined as follows:"}, {"title": null, "content": "$\\begin{aligned}\n&\\underset{D_R}{\\text{minimize}} & |D_R| \\\\\n&\\text{subject to} & \\hat{\\mu}_{I(\\mathbb{F},D_R)} + z\\hat{\\sigma}_{I(\\mathbb{F},D_R)} \\leq \\psi\n\\end{aligned}$ (6)\nwhere $\\hat{\\mu}$ and $\\hat{\\sigma}$ denote the mean and standard deviation of\nthe log-transformed CDR in $D_R$. The constraint keeps the\nlog-transformed CDR below a threshold $\\psi$ at a given confi-\ndence level, with $z$ indicating standard deviations from the\nmean. Minimizing $|D_R|$ while preserving representative-\nness enhances efficiency and reliability in data processing,\nparticularly in resource-limited settings or for rapid insights."}, {"title": null, "content": "In regression analysis, the lack of global information about\nthe true function $f(x)$ renders complexity assessment meth-\nods based on derivatives or higher-order derivatives infea-\nsible. To address this challenge, we propose a method that\ncombines Delaunay Triangulation with linear interpolation\nmodels to approximate the CDR in a complexly partitioned\nfeature space. To facilitate the optimization process, we in-\ntroduce three critical assumption:\nAssumption 4.1 (Discretization of Evaluation Metrics). Let\nthe domain of evaluation metrics be potentially continu-\nous and infinite, posing computational and mathematical\nchallenges. We assume that the evaluation process can be\nconstrained to a finite set of discrete samples. Given an\noriginal dataset $D = {(x_i, y_i)}_{i=1}^N$ where $x_i \\in \\mathbb{R}^n$ is a\nfeature vector and $y_i \\in \\mathbb{R}^m$ is a label for each data pair,\ndiscretization involves selecting a subset $D_R \\subset D$ such that\n$|D_R| \\leq |D|$.\nAssumption 4.2 (Finite Partitioning of Feature Space). We\nassume it is feasible to extract a finite subset of samples\nfrom the feature space $\\mathbb{X}$ and apply Delaunay triangulation\nto divide $\\mathbb{X}$ into a set of non-overlapping simplices $\\mathbb{F} =$\n${\\Omega_j}_{j=1}^k$. Each simplex $\\Omega_j$ represents a distinct local region\nwithin $\\mathbb{X}$, ensuring no overlap between any two simplices,\ni.e., $M(\\Omega_i \\cap \\Omega_j) = 0$ for all $i \\neq j$.\nAssumption 4.3 (Simplified Constraint Formulation). To\nmitigate computational intensity and mathematical com-\nplexity associated with estimating complex distributions or\ncomputing high-order derivatives, we simplify constraint\nformulation. Specifically, we require that the maximum Eu-\nclidean norm of the error vectors $||e_i||_2$ satisfies $||e_i||_2 < \\psi$\nfor predefined threshold $\\psi > 0$."}, {"title": null, "content": "The proposed methodological choices simplify complex\nproblems by discretizing evaluation metrics and partitioning\nthe feature space, thereby reducing computational complex-\nity and enabling accurate modeling with local linear interpo-\nlation. Assumptions 4.1 and 4.2 are intuitively reasonable,\nwhile Assumption 4.3 requires validation through analyzing\nthe correlation between error and CDR.\nOur analysis reveals that significant errors predominantly\noccur in regions with maximal CDR, enabling effective\nmanagement of the upper error bound within any region\n\u03a9, as detailed in Appendix. A. The upper bound on the\nmaximum error within any region \u03a9 can be expressed as\nfollows, with a detailed derivation provided in Appendix. A."}, {"title": null, "content": "$\\underset{x \\in \\Omega}{\\text{ub}} ||e(x)||_2 = \\frac{1}{2} \\underset{x_1, x_2 \\in \\Omega}{\\text{max}} ||x_1 - x_2|| \\underset{x \\in \\Omega}{\\text{max}} ||H(x)||_F$"}, {"title": null, "content": "$= \\frac{1}{2} g_s(\\Omega) g_c(\\Omega)$\n$= \\frac{g_c(\\Omega)g_s(\\Omega)}{|\\Omega \\cap D|/2}$\n$= \\frac{g_c(\\Omega)g_s(\\Omega)}{|\\Omega \\cap D|/2}$ (7)\nwhere $\\underset{x \\in \\Omega}{\\text{ub}} ||e(x)||_2$ denotes the upper bound on the max-\nimum error within region \u03a9. This expression shows that the\nupper bound is proportional to both the size of the region\n$g_s(\\Omega)$ and its complexity $g_c(\\Omega)$, normalized by the number\nof data points within the region."}, {"title": null, "content": "When using Delaunay triangulation, the number of vertices\nin an n-dimensional simplex remains constant ($|\\Omega \\cap D| =$\nn + 1). Consequently, this simplifies the computation of\nthe upper bound. The proportionality of the maximum er-\nror within any region \u03a9 to the complexity-to-density ratio\n(CDR) is given by:"}, {"title": null, "content": "$\\underset{x \\in \\Omega}{\\text{ub}} ||e(x)||_2 \\propto \\rho(\\Omega, D)$ (8)"}, {"title": null, "content": "where $\\rho(\\Omega, D)$, the CDR within a region, combines the\neffects of $g_c(\\Omega)$ (maximum complexity) and $g_s(\\Omega)$ (region\nsize). This relationship highlights how the maximum error\nscales with the characteristics of region \u03a9. Specifically,\nregions with higher complexity or larger size will exhibit\ngreater potential for error, while regions with more data\npoints tend to have lower errors due to better representation.\nThe upper bound of error within any region $\\Omega \\in \\mathbb{F}$ is re-\nlated to the statistical properties of the data. Specifically, at\na 98.85% confidence level, the threshold $\\hat{\\mu} + 2\\hat{\\sigma}$ captures\nnearly all significant error values. This implies that the max-\nimum error $\\underset{x \\in \\Omega}{\\text{max}} ||e||_2$ can be considered equivalent to\nthis threshold, with an extremely low probability (less than\n1.15%) of observing errors exceeding this limit. Therefore,\nthe relationship between the upper bound of error and the\nstatistical properties is expressed as:"}, {"title": null, "content": "$\\underset{\\Omega \\in \\mathbb{F}}{\\text{max}} \\underset{x \\in \\Omega}{\\text{ub}} ||e(x)||_2 \\propto \\hat{\\mu}_{I(D_R)} + 2\\hat{\\sigma}_{I(D_R)}$ (9)"}, {"title": null, "content": "The following assumptions underpin our approach: errors\n$||e||_2$ within each region \u03a9 are assumed to approximately\nfollow a log normal distribution; the variance of errors is\nconsidered relatively consistent across different regions (ho-\nmogeneity of variance); and a sufficiently large sample size\nensures reliable estimation of $\\hat{\\mu}$ and $\\hat{\\sigma}$. The 98.85% con-\nfidence level provides a conservative estimate capturing\nalmost all significant errors, accounting for potential non-\nnormality in the data. The threshold $\\hat{\\mu} + 2\\hat{\\sigma}$ serves as a\nrobust upper bound, indicating that most observed errors\nwill not exceed this value. This approximation is highly\nreliable, with only a minimal probability of encountering\nlarger errors."}, {"title": "5. Representative Dataset Construction", "content": "To construct the representative dataset $D_R$, we employ\nDelaunay triangulation with Linear Interpolation Models\n(LIMs)(Chang et al., 2018; 2025; Berrut & Trefethen,\n2004). This method tessellates the feature space into non-\noverlapping simplices, ensuring that no data pairs lie within\na simplex's circumcircle(Boissonnat et al., 2009). This ap-\nproach accurately captures local structures while preventing\noverfitting. By constructing simplices using a representative\ndataset, the method preserves computational efficiency even\nfor large datasets."}, {"title": "6. Experiment", "content": "This section evaluates Error Distribution Smoothing (EDS),\nwhich improves model accuracy and generalization. EDS\nenhances prediction accuracy in systems like the Lorenz\nsystem, reduces dimensionality in high-dimensional data,\nand improves real-world applications such as quadcopter\ndynamics by lowering prediction errors and training time."}, {"title": "6.1. Motivation Example", "content": "To illustrate Error Distribution Smoothing (EDS), we use\nthe function $f(x_1,x_2) = (0.33 + x_1^2 + x_2^2)^{-1}$, where $x_1$\nand $x_2$ are uniformly sampled from [-3, 3]. This function\nshows rapid changes near the (0,0) due to the quadratic\nterms in the denominator, with variations decreasing as\npoints move away from (0,0). Accurately capturing this\nbehavior, particularly the high complexity near the origin,\nunderscores the need for an adaptive sampling strategy.\nThe feature space is partitioned into simplices, with Lin-\near Interpolation Models (LIMs) built from samples at the\nvertices. Error Distribution Smoothing (EDS) adjusts sam-\nple density based on the local Complexity-to-Density Ratio\n(CDR) to ensure balanced representation. This method re-\ntains critical data points in high-CDR regions and avoids\noversampling in low-CDR areas. Algorithm 1 constructs\nthe representative dataset $D_R$ and auxiliary dataset $D_A$, en-\nsuring that only essential points are kept in $D_R$. The output\nis the updated sets $D_R$ and $D_A$."}, {"title": "6.2. Evaluating EDS in Dynamics System Identification", "content": "To evaluate Error Distribution Smoothing (EDS) in the con-\ntext of dynamic system identification, we applied the Sparse\nIdentification of Nonlinear Dynamics (SINDY) algorithm\n(Brunton et al., 2016) to the well-known Lorenz system. The\nLorenz system, originally developed as a simplified mathe-\nmatical model for atmospheric convection, is described by\na set of three nonlinear differential equations that exhibit\nchaotic behavior under certain parameter settings. It is de-\nfined by Equation 10 with parameters \u03c3 = 10, \u03c1 = 28, and\n\u03b2 = 8\u20443:"}, {"title": null, "content": "$\\begin{cases}\n\\dot{x} = \\sigma(y - x) \\\\\n\\dot{y} = x(\\rho - z) - y \\\\\n\\dot{z} = xy - \\beta z\n\\end{cases}$ (10)\nwhere the features consist of measurements of the state\nvariables $x$, $y$, and $z$. The labels are the rates of change\nof these state variables, $\\dot{x}$, $\\dot{y}$, and $\\dot{z}$. By applying SINDY,\nwhich identifies a sparse set of terms that best describe the\ndynamics from data, we aim to assess improvements in data\nrepresentation and model performance through optimized\nsample distribution."}, {"title": "6.3. Evaluating EDS in High-Dimensional Regression", "content": "To evaluate EDS in high-dimensional settings, we used a\nsynthetic dataset of white rectangles on a black background,\nwith each rectangle labeled by its polar moment of inertia\nfor regression. To address the high-dimensional challenge,\nwe extracted the positions of the rectangle edges to reduce\ndimensionality while preserving critical information, and\nsubsequently standardized the features and labels to meet\nthe model's requirements."}, {"title": "6.4. Evaluating EDS in Real-World Problems", "content": "To evaluate Error Distribution Smoothing (EDS) in han-\ndling real-world problems with strong noise and significant\nimbalance, we conducted experiments using two dynamic\nsystems: the Cartpole and the Quadcopter. Data for both\nsystems was collected under challenging conditions using\nPID controllers. For the Cartpole, features included the\npole's angle, pole's angular velocity and motor's current,\nwith labels being the cart's and pole's accelerations. For the\nQuadcopter, features included height, velocity and throttle,\nwith labels being the acceleration."}, {"title": "7. Discussion and Conclusion", "content": "In this paper, we introduced Error Distribution Smoothing\n(EDS) to address low-dimensional imbalanced regression\nby adjusting sample density based on the local Complexity-\nto-Density Ratio (CDR), ensuring balanced representation\nacross the feature space. Experiments on simulated and real-\nworld physics datasets demonstrated that EDS significantly\nimproves precision, efficiency, and robustness. By reduc-\ning redundant samples in low-CDR regions and preserving\ncritical points in high-CDR areas, EDS notably reduced the\nmaximum prediction error, shortened training times through\nan optimized dataset size, and maintained model robustness\ndespite varying data complexity. These results underscore\nEDS's effectiveness in overcoming the limitations of tradi-\ntional sampling methods, enhancing data representation and\nmodel performance for imbalanced datasets.\nFuture work. We will focus on extending EDS to higher-\ndimensional datasets while optimizing its algorithms to\nsignificantly improve processing speed for large and com-\nplex datasets. This involves developing methods to handle\nincreased complexity efficiently and exploring techniques\nsuch as parallel processing to enhance computational per-\nformance, ensuring EDS remains effective and scalable for\nreal-world applications.\nImpact Statements. This paper presents work whose goal\nis to advance the field of Machine Learning. There are many\npotential societal consequences of our work, none which we\nfeel must be specifically highlighted here."}, {"title": "A. Analysis of Linear Interpolation", "content": "A.1. MISO Function Approximation via Linear Interpolation\nIn this subsection, we explore the approximation of Multi-Input Single-Output (MISO) functions using linear interpolation\nover n-dimensional simplices. A MISO function maps multiple input variables to a single output variable, and linear\ninterpolation provides a computationally efficient method for approximating such functions within a discretized feature\nspace. By leveraging the geometric properties of simplices, we can construct accurate local models that facilitate efficient\nand robust function approximation.\nLet us consider an n-dimensional simplex \u03a9q with vertices {xq,0, xq,1,..., xq,n}. For any point x \u2208 \u03a9q, its barycentric\ncoordinates Aq,i(x) are uniquely defined as the solution to the following system:"}, {"title": null, "content": "$\\begin{cases}\n\\sum_{i=0}^{n} \\lambda_{q,i}(x) = 1 \\\\\n\\sum_{i=0}^{n} \\lambda_{q,i}(x) x_{q,i} = x\n\\end{cases}$ (11)\nThese coordinates satisfy the Kronecker delta property: \\Aq,i(xq,j) = dij, which means that the coordinate corresponding to\na vertex is one when evaluated at that vertex and zero elsewhere.\nThe barycentric coordinate Aq,i(x) for any point \u00e6 relative to vertex \u00e6q,i can be calculated without singling out any particular\nvertex by:"}, {"title": null, "content": "$\\lambda_{q,i}(x) = \\frac{\\text{det} \\left( \\begin{bmatrix} x_{q,1} - x_{q,0} & \\cdots & x - x_{q,0} & \\cdots & x_{q,n} - x_{q,0} \\end{bmatrix} \\right)}{\\text{det} \\left( \\begin{bmatrix} x_{q,1} - x_{q,0} & \\cdots & x_{q,i} - x_{q,0} & \\cdots & x_{q,n} - x_{q,0} \\end{bmatrix} \\right)}$ (12)\nwhere [.] denotes the matrix formed by omitting the ith column from the original matrix.To estimate the error in linear\ninterpolation, we use the second-order Taylor expansion of f(x) around a point \u00e6 within \u03a9:"}, {"title": null, "content": "$f(x_{q,i}) = f(x) + \\nabla f(x)^T (x_{q,i} - x) + \\frac{1}{2} (x_{q,i} - x)^T H(f)(x_{q,i} - x)$ (13)\nGiven the function f and points \u00e6q,i with associated weights Aq,i, under the normalization condition $\\sum_{i=0}^{n} \\lambda_{q,i} = 1$ and the\ncentroid condition $\\sum_{i=0}^{n} \\lambda_{q,i}x_{q,i} = x$ (Berrut & Trefethen, 2004), we examine the expression:"}, {"title": null, "content": "$\\sum_{i=0}^{n} \\lambda_{q,i} f(x_{q,i}) - f(x)$ (14)\nExpanding this using the Taylor series gives:"}, {"title": null, "content": "$\\sum_{i=0}^{n} \\lambda_{q,i} \\left[ f(x) + \\nabla f(x)^T (x_{q,i} - x) + \\frac{1}{2} (x_{q,i} - x)^T H(\\xi)(x_{q,i} - x) \\right] - f(x)$, (15)\nwhere the first-order term vanishes due to the centroid condition:"}, {"title": null, "content": "$\\nabla f(x)^T \\sum_{i=0}^{n} \\lambda_{q,i} (x_{q,i} - x) = \\nabla f(x)^T (x - x) = 0$ (16)\nTherefore, the error in linear interpolation is bounded by the second-order term, which cannot be proven to be zero using\nonly the given conditions. The maximum interpolation error over the simplex Nq x \u2208 Nqcan be bounded as follows:"}, {"title": null, "content": "$\\left| \\sum_{i=0}^{n} \\lambda_{q,i} f(x_{q,i}) - f(x) \\right| = \\left| \\sum_{i=0}^{n} \\frac{1}{2} \\lambda_{q,i} (x_{q,i} - x)^T H(\\xi) (x_{q,i} - x) \\right|$"}, {"title": null, "content": "$\\leq \\frac{1}{2} \\sum_{i=0}^{n} \\lambda_{q,i} |x_{q,i} - x| \\cdot ||H(\\xi)||_2$ (17)\n$\\leq \\frac{1}{2} \\text{max} ||H(x)||_2 \\cdot \\text{max} ||x_i - x|| \\sum_{i=0}^{n} \\lambda_{q,i}$\n$\\leq \\frac{1}{2} \\frac{1}{2} \\text{max} ||H(x)||_2 \\text{max} ||x_i - x_j|| \\sum_{i=0}^{n} \\lambda_{q,i}$\n$< \\frac{1}{2} \\text{max} ||H(x)||_F \\text{max} ||x_i - x_j||$\nwhere $|| H(x)||_F$ is the Frobenius norm of the Hessian matrix of f evaluated over \u03a9q, and $||x \u2212 y||_2$ is the Euclidean\ndistance between points x and y within the simplex. This inequality provides an upper limit on the interpolation error,\nindicating that the error depends on the maximum curvature of f over Ng and the size of the simplex.\nThe interpolation error for a function f approximated via linear interpolation over an n-dimensional simplex Oq can be\nbounded above by:"}, {"title": null, "content": "$\\underset{x \\in \\Omega}{\\text{ub}} |f(x) - I_q(x)| < \\frac{1}{2} \\underset{x \\in \\Omega}{\\text{max}} || H(x)||_F \\underset{x_1, x_2 \\in \\Omega}{\\text{max}} ||x_1 - x_2||$ (18)\nwhere $I_q(x)$ denotes the linear interpolated value at point \u00e6, obtained as a weighted sum of function values at the simplex\nvertices with weights given by the barycentric coordinates. This inequality provides an upper limit on the interpolation error,\nhighlighting its dependence on both the function's curvature and the simplex's spatial extent."}, {"title": "A.2. MIMO Function Approximation via Linear Interpolation", "content": "When considering the approximation of a Multiple Input Multiple Output (MIMO) function $f(x) = [f_1(x),..., f_m(x)]^T$,\nwhere x is the feature vector and m denotes the dimension of the output vector, each component function fi(x) can be\ntreated as a MISO (Multiple Input Single Output) function with its own Hessian matrix Hi(x). The collection of these\nHessian matrices forms a Hessian tensor for the MIMO function f(x).\nThe interpolation error for each individual MISO function fi(x) over a space Oq in the feature space can be bounded.\nTherefore, for each fi(x), the maximum interpolation error can be bounded as:"}, {"title": null, "content": "$|f_i - I_{q,i}(x)| < \\frac{1}{2} \\underset{x \\in \\Omega}{\\text{max}} || H_i(x)||_F \\underset{x_1, x_2 \\in \\Omega}{\\text{max}} ||x_1 - x_2||$ (19)\nwhere $I_{q,i} (x)$ denotes the interpolated value of f(x) at point x, $||H_i(x)||_F$ is the Frobenius norm of the Hessian matrix of\nfi at x, and $||x_1-x_2||_2$ represents the Euclidean distance between points 21 and 22 within the simplex.\nTo find an upper bound on the approximation error for the entire MIMO function f(x), we consider the Euclidean norm of\nthe error vector. For a MIMO function, the Hessian tensor's contribution to the error must be aggregated across all outputs.\nHowever, because it is not straightforward to define a single norm for a higher-order tensor, we typically resort to bounding\nthe error based on the norms of the individual Hessian matrices:"}, {"title": null, "content": "$\\sqrt{\\sum_{i=1}^{m} |f_i(x) - I_{q,i}(x)|^2} < \\sqrt{\\sum_{i=1}^{m} \\left[ \\frac{1}{2} \\underset{x \\in \\Omega}{\\text{max}} || H_i(x) ||_F \\underset{x_1, x_2 \\in \\Omega}{\\text{max}} ||x_1 - x_2|| \\right"}]}