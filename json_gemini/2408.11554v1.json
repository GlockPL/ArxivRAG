{"title": "Differentiating Choices via Commonality for Multiple-Choice Question Answering", "authors": ["Wenqing Denga", "Zhe Wanga,*", "Kewen Wanga", "Shirui Pan\u00aa", "Xiaowang Zhang\u266d", "Zhiyong Fengb"], "abstract": "Multiple-choice question answering (MCQA) becomes particularly challenging when all choices are relevant to the question and are semantically similar. Yet this setting of MCQA can potentially provide valuable clues for choosing the right answer. Existing models often rank each choice separately, overlooking the context provided by other choices. Specifically, they fail to leverage the semantic commonalities and nuances among the choices for reasoning. In this paper, we propose a novel MCQA model by differentiating choices through identifying and eliminating their commonality, called DCQA. Our model captures token-level attention of each choice to the question, and separates tokens of the question attended to by all the choices (i.e., commonalities) from those by individual choices (i.e., nuances). Using the nuances as refined contexts for the choices, our model can effectively differentiate choices with subtle differences and provide justifications for choosing the correct answer. We conduct comprehensive experiments across five commonly used MCQA benchmarks, demonstrating that DCQA consistently outperforms baseline models. Furthermore, our case study illustrates the effectiveness of the approach in directing the attention of the model to more differentiating features.", "sections": [{"title": "1 Introduction", "content": "Multiple-choice question answering (MCQA) still presents a multi-faceted challenge in natural language processing (NLP), especially when commonsense and reasoning are required [11, 18, 21, 29]. The goal of MCQA is to enable machines to select from a set of choices the most relevant one to the question, and to make the task challenging, it often involves several distracting choices that are all relevant in various degrees and semantically similar to the answer. Commonsense and reasoning capabilities are required to differentiate the answer from distracting choices. Several datasets, such as CommonsenseQA [25] and OpenBookQA [19], have been created to aid researchers in developing and evaluating models for commonsense reasoning in MCQA.\nPre-trained language models (LMs) like GPT-3 [3], BERT [8], and ROBERTa [15] demonstrate impressive reasoning capabilities and have led to significant progress in MCQA tasks. However, their reasoning processes are often considered implicit and lacking in explainability [27]. Recent studies [23, 34] have emphasized the importance of explainability in NLP tasks, prompting research efforts to design MCQA models with interpretable reasoning processes. Another line of research tries to incorporate external knowledge bases, such as knowledge graphs, to perform explicit reasoning and use such external knowledge as explanations [1, 17, 26, 33, 36]. While these approaches have demonstrated potential in improving interpretability, it's important to note that not all required commonsense knowledge can be captured in such an external knowledge base. Additionally, due to their large sizes, incorporating such knowledge bases would likely introduce a significant amount of noise.\nRecent research has further underscored the pivotal role of leveraging the contextual information within the questions and choices for interpretable reasoning in MCQA tasks [6, 11, 24, 35]. For instance, GenMC [11] delves into the contextual information within questions to generate clues for selecting the answers, and such clues serve as intermediate steps of the reasoning. Nonetheless, such clues are generated independently of the choices and thus do not utilize contextual information among the choices. Indeed, the setting of MCQA can potentially provide valuable clues from the choices, including the distracting ones due to their semantic similarity to the answers. Yet existing MCQA models typically process each question-choice pair separately to assess each choice independently, overlooking the contextual information provided by other choices. Specifically, they fail to leverage the semantic commonalities and nuances among the choices for reasoning. In contrast, human beings often tackle MCQA by collectively comparing all choices and scrutinizing their differences to derive clues.\nOne form of contextual information often overlooked by independent reasoning is the commonalities among choices, such as shared themes or attributes. To illustrate this point, consider a scenario depicted in Figure 1, where all choices share a commonality of being gases on Earth. Existing MCQA models may associate all choices with the token \"gas\" in the question (reflected in high token-level attention weights), leading to potential confusion or misinformation. By prioritizing the identification and mitigation of such commonalities during the reasoning process, justifications or differentiating clues for each choice can be extracted from the question. For example, while both \"Oxygen\" and \"Carbon dioxide\" are associated with the tokens \"greenhouse effect\", the tokens \"primary reason\" serve as a justification for the choice \"Carbon dioxide\". By further generating explanatory information for the refined questions, we can obtain a distinguishing clue for each choice. For example, generating the information \"a cause\" for \"carbon dioxide\" reinforces the rationale for selecting \"Carbon dioxide\". Therefore, reasoning among the choices to identify commonalities, detect subtle differences, and generate differentiating clues is crucial for MCQA.\nTo address this limitation, we introduce a novel approach called Differentiating Choices via Commonality for MCQA (DCQA). Our approach first examines the choices to generate a representation of the commonalities shared among them. We then use this representation to derive a representation of the question tailored to each specific choice (called as a choice-specific representation of the question in this paper), effectively isolating the unique contextual information relevant to that choice. This choice-specific representation of the question serves as a clue or justification within the question for that particular choice. Subsequently, the decoder activates contextual information to the choice from the choice-specific representation of question. By integrating this information, we enhance the representation of each choice. In our experiments across five commonly used MCQA benchmarks, we demonstrate that our model consistently outperforms baseline methods, showcasing its ability to improve MCQA systems. Furthermore, a case study illustrates how our model effectively captures the commonalities among choices and identifies distinctive clues within questions, ultimately leading to enhanced MCQA performance. Our code is publicly available at https://github.com/dwq-vicki/DCQA."}, {"title": "2 Related Work", "content": "Recently, large language models (LLMs) like GPT-3 [3] and DeBERTa [10] have demonstrated impressive reasoning capabilities and have emerged as dominant approaches for multiple-choice question answering (MCQA) tasks. However, the use of larger LLMs often leads to disproportionate resource consumption and longer training times [27]. Additionally, these models exhibit implicit reasoning processes and lack explainability, particularly in commonsense reasoning tasks. Consequently, several efforts have focused on enhancing machine reasoning abilities. In the following sections, we will provide a detailed overview of these efforts."}, {"title": "2.1 Knowledge-Enhanced Models on MCQA Tasks", "content": "Several models in MCQA have incorporated external knowledge sources to enhance the reasoning process and interpretability. These models utilize knowledge graphs (KGs) or other unstructured knowledge representations to augment the understanding of questions and choices. Some approaches focus on concatenating the commonsense knowledge with the question and encoding it into embeddings by using LMs with self-attention mechanisms [5, 29, 30]. Other methods leverage graph neural networks (GNNs) to integrate commonsense knowledge and perform reasoning within the graph structure, often referred to as KG-based approaches [4, 9, 17, 26, 27, 31, 32, 33, 36].\nFor example, Chen et al. propose a graph-based iterative knowledge retrieval module, which iteratively retrieves concepts and entities related to the given question and its choices from multiple knowledge sources [4]. MHGRN [9] introduces a multi-hop relational reasoning module to perform multi-hop, multi-relational reasoning over subgraphs extracted from external knowledge graphs. GrapeQA [26] employs a graph augmentation method for prominent entities by identifying relevant text chunks from the QA pair and augments the working graph with corresponding latent representations from the LM. DHLK [27] introduces a dynamic heterogeneous-graph reasoning method using LMs and knowledge representation learning. HGN [31] learns to jointly contextualize extracted and generated knowledge by reasoning over both within a unified graph structure to filter out context-irrelevant edges. QAGNN [32] estimates the importance of subgraph entities using LMs and considers the QA context as an additional node connected to the subgraph, enabling joint reasoning over the QA context and KG.\nHowever, these methods often face limitations as not all commonsense knowledge can be captured in an external knowledge base. Furthermore, while KGs excel in explicit reasoning, they may struggle to capture implicit, context-dependent rationale [12]. In contrast, our model takes a different approach by not relying on external knowledge sources. We capture only the contextual information within the QA task itself to facilitate reasoning, potentially mitigating some of the challenges faced by KG-based approaches."}, {"title": "2.2 Contextual Information Models on MCQA Task", "content": "Numerous MCQA models have been developed to leverage contextual information within the questions and choices for commonsense reasoning. For instance, Sun et al. propose three reading strategies inspired by human behavior for extracting contextual information between questions and choices: back-and-forth reading, highlighting important information, and self-assessment through self-generated questions [24]. Zhang et al. introduce a two-way matching strategy for reading comprehension, to extract contextual information between questions and choices [35]. There has been a growing interest in developing unified frameworks to address various tasks requiring contextual information in natural language understanding. One notable example is the UnifiedQA model [13], which integrates 20 QA datasets into a unified training format and achieves state-of-the-art performance on various MCQA datasets. Similarly, GenMC [11] proposes a framework for generating contextual called as clues from questions to infer choices in MCQA, demonstrating impressive results on five different MCQA datasets and providing visualizations for interpretability.\nHowever, these models typically perform reasoning or generate contextual information based on each question-choice pair independently, overlooking valuable context from the interaction among the choices. In our model, we identify and eliminate the commonalities among the choices to obtain the nuances for differentiating these choices. By focusing on the refined contexts for the choices, our model enhances the reasoning capabilities of MCQA systems."}, {"title": "3 Method", "content": "In this section, we formalize the MCQA problem and then introduce a new model for MCQA, call Differentiating Choices via Commonality for MCQA (DCQA), which emphasizes the differences between the choices via their commonalities.\nFormally, given a natural language question $q$ and $n$ choices $a_1, a_2,..., a_n$, the task is to determine the most plausible choice. Like most models for MCQA, this is typically achieved by effectively ranking then choices based on a confidence score for each choice w.r.t. the question.\nAn overview of our model is illustrated in Figure 2. Our model comprises four primary modules. Firstly, the Context Representation module encompasses the initial embeddings of the questions and choices (Section 3.1). Secondly, in view of the question, the Commonality Extraction module extracts a representation of the commonalities among all choices (Section 3.2). Thirdly, the Context Refinement module is designed to obtain the refined context as nuances by generating a representation of the question tailored to each specific choice via the commonalities. This representation is referred to as a choice-specific representation of the question (Section 3.3). Finally, the Choice Enhancement module refines the representation of each choice using the corresponding refined context obtained from the previous module (Section 3.4)."}, {"title": "3.1 Context Representation", "content": "In line with the methodology of GenMC [11], we adopt a similar approach for obtaining the embeddings of the questions and choices. We input the Q context (question) as the representation of the question and the QA context (question + choice) as the representation of each choice. To encode these representations, we employ a pre-trained encoder-decoder model, following the equations:\n$Q = encoder(q),$ (1)\n$A_i = encoder(q+a_i),$ (2)\nwhere $Q \u2208 R^{l\u00d7d}$ and $A_i \u2208 R^{m\u00d7 d}$ (1 \u2264 i \u2264 n). Here, l and m represent the maximum length of the question and choices in terms of tokens, and d denotes the dimension of the encoding for each token. We opt for an encoder-decoder model to capitalize on the natural language understanding capabilities of language models. This choice is driven by our aim to utilize the decoder to generate contextual information pertaining to the question, which is embedded within the LM. The encoder in our model captures the contextual information of the question and choices, while the decoder assists in generating supplementary contextual information, as will be discussed in the Choice Enhancement module (Section 3.4)."}, {"title": "3.2 Commonality Extraction", "content": "When tackling MCQA tasks, human beings often compare all choices simultaneously and analyze their differences to derive clues. This strategy allows humans to leverage the nuances and subtleties in the choices, enabling them to make more informed decisions. However, directly extracting differentiating clues for each choice without considering the other choices is challenging. Therefore, in this module, we first compare all choices to extract a representation of their commonalities $c$ among all choices $a_1,..., a_n$. This representation is then used in the next module to extract differentiating clues for each choice based on the given question.\nInspired by [35], we propose an attention mechanism called choice-attention to calculate the commonalities $c$. In the previous module, the encoder encodes each choice $a_i$ as $A_i$. Firstly, we compute the interaction matrix between any two choices $a_i$ and $a_j$ by leveraging matrix multiplication. We then use softmax function to compute the similarity score matrix $S_{i,j}$ as follows:\n$S_{i,j} = softmax(A_iW_{i,j}A_j^T),$ (3)\nwhere $1 \u2264 i, j < n$, $S_{i,j} \u2208 R^{m\u00d7m}$ and $W_{i,j} \u2208 R^{d\u00d7d}$ is a learnable parameter matrix. Each entry $s_{i,j} \u2208 S_{i,j}$ denotes the similarity score between two tokens from choice $a_i$ and $a_j$, respectively.\nNext, we extract the interaction matrix representation for each choice $a_i$ by weighting the sum of the similarity score matrix $S_{i,j}$ with other choices $A_j$. Here, we consider such representation as the commonalities matrix specific to choice $a_i$. In fact, each choice obtains a commonalities matrix from the other choices, resulting in n commonalities matrices. By summing all the commonalities matrices and computing the average, we aggregate the commonalities among the choices to form a representation $C$ as follows:\n$C = (\\sum_{1\\leq i < n} \\sum_{1\\leq j < n, i \\neq j} S_{i,j}A_j)/n,$ (4)\nwhere $C \u2208 R^{m\u00d7d}$ is the unified representation of commonalities among all choices.\nA key difference between our model and the inspiring work [35] is that our model obtains a common embedding from all choices to differentiate them by eliminating such common information, while the latter obtains the interaction among all choices to enhance the representation of each choice."}, {"title": "3.3 Context Refinement", "content": "After deriving the representation $C$ of the commonalities among all the choices, our model uses $C$ to generate a tailored representation of the question for each specific choice, referred to as a choice-specific representation of the question. As illustrated in Figure 1, to obtain this representation, we first identify tokens in the question related to the commonality of all choices and reduce their weights in representation. Subsequently, we identify the unique tokens as justifications for each choice and enhance their representation. In this module, we simulate this process by extracting two types of information: contextual information related to the commonalities c and contextual information related to each choice ai.\nContextual Information Related to the Commonalities. Inspired by [35], we introduce a cross-attention mechanism to extract relevant contextual information from the question for the commonalities. This mechanism operates at the token level, assigning higher weights to more relevant tokens. We first compute two weight matrices, $I_q$ and $I_c$, between the question $q$ and the commonalities $c$. We then extract the representation $Q_c$ of interaction information from the commonalities $c$ by utilizing the weight matrix $I_c$. Finally, we incorporate this interaction information $Q_c$ into the question $q$ to get the representation $Q_q^c$ of contextual information in the question $q$ related to the commonalities $c$ as follows:\n$I_q = softmax(CQ^T),$ (5)\n$I_c = softmax(QC^T),$ (6)\n$Q_c = I_c [I_qQ; C],$ (7)\n$Q_q^c = normal(Q + Q_cW_1),$ (8)\n$q_c = MaxPooling(Q_q^c),$ (9)\nwhere $I_q \u2208 R^{mxl}$, $I_c \u2208 R^{lxm}$, $Q_c \u2208 R^{lx2d}$, $q_c \u2208 R^d$ and $W_1 \u2208 R^{2dxd}$ is a learnable parameter matrix. [X; Y] denotes the concatenation of the matrices X and Y. Here, $I_q$ represents the weight of each token in the question $q$ for the commonalities $c$, while $I_c$ represents the weight of each token in the commonalities $c$ for the question $q$. The vector $q_c$ is an embedding that represents the entire semantic information contained in Q. In our model, $q_c$ is used for the final computation of the score for each choice.\nThe equations described above constitute the entire cross-attention mechanism: $Q_q^c, q_c = att_{cross} (Q; C)$.\nContextual Information Related to Each Choice. We employ the same cross-attention mechanism to calculate the contextual information from the question q related to each choice $a_i$: $Q_q^{a_i}, q_{a_i} = att_{cross} (Q; A_i)$, where $1 \u2264 i \u2264 n$.\nTo derive the choice-specific representation of the question as refined context, we subtract the embedding $Q_c$ from $Q_q^{a_i}$, simulating the process of eliminating the commonalities information in the question for each choice. Similar operation is conducted on the entire semantic information between $q_c$ and $q_{a_i}$:\n$Q_i = Q_q^{a_i} - Q_q^c,$ (10)\n$q_i = q_{a_i} - q_c,$ (11)"}, {"title": "3.4 Choice Enhancement", "content": "The choice-specific representation of the question, obtained from the previous module, offers a refined contextual understanding of the question for each choice. As depicted in Figure 1, we can further generate the information from the choice-specific representation of the question to provide distinct contextual information for each choice. Therefore, in this module, we first leverage the decoder to generate contextual information $K_i$ related to the choice-specific representation of the question, as follows:\n$K_i = decoder(Q_i),$ (12)\nHere, $K_i \u2208 R^{pXd}$, where p and d denote the length of tokens in $K_i$ and the representation dimension, respectively.\nWe use the original QA context as the semantic representation of each choice in Section 3.1, but we have refined the semantic information of the question for each choice. Therefore, we need to enhance the representation of each choice using the choice-specific representation $Q_i$ of the question and the generated contextual information $K_i$. Similar to the Context Refinement module (Section 3.3), we employ the cross-attention mechanism to calculate the enhanced representation $A_i$ and $a_i$ for each choice: $A_i, a_i = att_{cross} ([A_i; K_i], Q_i)$, where $A_i \u2208 R^{(m+p)\u00d7d}$ and $a_i \u2208 R^d$.\nWe compute the confidence score of each choice $a_i$ based on the semantic connection between $q_i$ and $a_i$. This score is calculated using an MLP and softmax layer as follows:\n$score(a_i) = softmax (MLP [q_i; a_i]).$ (13)\nFinally, we select the choice with the highest score as the predicted choice."}, {"title": "4 Experiments", "content": "We evaluated our model on five widely used commonsense MCQA benchmarks. The CommonsenseQA (CSQA) [25] and OpenBookQA (OBQA) [19] datasets are widely used commonsense QA benchmarks and constructed by crowed-sourcing. The ARC-Easy (ARC-E) and ARC-Challenge (ARC-C) datasets are subsets of the AI2 Reasoning Challenge (ARC) benchmark [7], which focuses on scientific questions. The QA via Sentence Composition (QASC) dataset [14] is a collection of (primary and middle) school-level science questions. Since the correct answers of the official Test sets are not released for the CSQA and QASC benchmarks, we used their official dev set as our test set for experiments. Additionally, we randomly held out an in-house dev set from the training set, as described in [11].\nThe statistics of the benchmarks are in Table 1, where we record for each dataset, the sizes of the Train, Dev, and Test sets, the number of choices (#C), and the average lengths of questions (|Q|) and choices (|C|)."}, {"title": "4.2 Experimental Setup", "content": "To ensure a fair comparison with baselines, we selected two popular encoder-decoder language models as a basis, T5 [28] and Unifiedqa-T5 [13]. We conducted experiments under their Base and Large versions. In our experiments, we set the maximum length of the input sequence to 64 and the output embedding dimension to 1024. We employed PyTorch 1.8 framework and utilized the Adam optimizer [16] with a weight decay rate of 0.01. The number of training iterations was set to 50, with early stopping after 15 epochs if no improvement in accuracy on the dev dataset was observed. To determine the optimal learning rate and batch size, we conducted a search over the ranges {1e-4, 5e-5, 1e-5, 5e-6} and {16, 8, 4}, respectively, which are shown in Table 2.\nConsidering that neural models are sensitive to different random seeds, we performed multiple experiments for each model with different random seeds, and reported the mean and standard deviation. For CSQA, OBQA, ARC-Easy, and QASC, we used three random seeds {1,10,20}. For the smallest dataset ARC-Challenge, we used five random seeds {1,10,20,30,40} to ensure the authenticity and accuracy of our results. All experiments were conducted on an NVIDIA PH402 SKU 200 with 32G memory."}, {"title": "4.3 Baselines", "content": "Our model aims to differentiate choices by using contextual information that represents the commonalities among all choices, without relying on external knowledge bases. To align with this design, we compare our model with several state-of-the-art MCQA models that solely utilize encoder-decoder architecture, excluding external knowledge. These models include GenMC [11] and UnifiedQA [13], which were discussed in Section 2.2. Additionally, we compare with T5-vanilla [20], as in GenMC. Specifically, we concatenate the question with all choices, with each choice preceded by its respective choice ID. The entire sequence is then prepended with the dataset name. This concatenated sequence is input into the encoder, which generates a joint representation of the question and all choices. Based on this joint representation, the decoder outputs the corresponding choice ID. In this configuration, the decoder essentially functions as a classifier. Our comparison focuses on accuracy, evaluating how effectively each model selects the correct choice."}, {"title": "4.4 Main Results and Analysis", "content": "Table 3 presents the comparison results on all five datasets. We categorize the results according to the language encoder, T5 and Unified-T5. Overall, our model demonstrates competitive performance with the baselines on both the Dev and Test sets of all datasets. Under the T5-Large configuration, our model achieves an improvement of 3.78% and 2.51% on the Dev and Test sets of CSQA over T5-vanilla, and surpasses GenMCT5 by 1.29% and 0.63% on these sets. These results indicate the effectiveness of our reasoning method, particularly in MCQA tasks that require commonsense reasoning. When evaluated under the Unified-T5 setup, our model consistently outperforms the language model UnifiedQAT5-FT on all datasets except for the Test set of the ARC-C dataset. This exception may be attributed to the dataset's question and choice lengths, which introduce additional complexity. Specifically, with UnifiedQAT5 as the base model, our model achieves improvements of 4.24% and 4.04% on the Dev and Test sets of the CSQA dataset, and 4.87% and 6.26% on the Dev and Test sets of the QASC dataset, respectively. However, our model's performance on the OBQA dataset with T5-Base is not as outstanding as GenMC, likely due to the relatively loose semantic connections among choices for each question. Additionally, the QASC dataset presents challenges for our model due to its large number of choices compared to other datasets, leading to some choices focusing on the same tokens and failing to be distinguished.\nIn summary, our model exhibits strong performance across various MCQA benchmarks, showcasing its effectiveness in capturing and utilizing contextual information to differentiate choices."}, {"title": "4.5 Ablation Study", "content": "To assess the contribution of each module and the three cross-attention methods, we conducted an ablation study. Table 4 summarizes the results, where C1, C2, and C3 denote the cross-attention mechanisms for the commonality feature of all choices and the question, the individual choice and the question, and the enhanced question concatenated with generated information, respectively.\nThe results indicate that, except for the Test set of ARC-E and the QASC dataset, removing the four variants (Commonality Extraction (ComE), Context Refinement (CR), Decoder (DE), and Choice Enhancement (CE)) results in a decrease in accuracy across all five datasets. Similarly, accuracy drops when we remove the three cross-attention mechanisms. This highlights the necessity of these modules and the essential role of the three cross-attention methods in the model.\nHowever, on the QASC dataset, our model does not achieve the highest accuracy when considering the four variants and the three cross-attention methods. This discrepancy is likely due to the dataset's characteristics, such as having a larger number of choices and relatively shorter question lengths. These factors can result in similar information from the question regarding the choices, posing challenges in accurately distinguishing between them."}, {"title": "4.6 Case Study", "content": "This section showcases how our model effectively reduces the weights of commonalities compared to other models (Section 4.6.1) and demonstrates the progressive enhancement of justifications within the question for each choice (Section 4.6.2).\nTo obtain these weights, we utilized an equation to generate a weight matrix between the questionQ and choice $A_i$: $softmax(QA_i^T)$. We then applied MaxPooling to capture the attention scores of each token in Q for each choice $A_i$. Since the distribution of these scores was relatively scattered, we applied Min-Max Normalization to scale these scores for each choice. Finally, we computed the percentage of each token in Q based on these scaled scores."}, {"title": "4.6.1 Attention Refinement", "content": "Figure 3 illustrates the effectiveness of eliminating commonalities, showcasing a comparative analysis with the baseline model GenMC. In example (a), GenMC assigns comparable weights to tokens such as \"what\", \"for\", and \"?\" for the correct choice. In contrast, our model reduces the weights on these common tokens by eliminating commonalities, thus increasing the emphasis on tokens like \"run\" and \"sense of what\". Additionally, the token \"After\" gains prominence, indicating its significance in the context of chronological order related to running. This highlights our model's ability to identify clues or justifications within the question for each choice, surpassing the capabilities of the GenMC model.\nThis efficacy is further exemplified in example (b), where our model accurately captures the critical token \"positive\". Moreover, example (c) showcases our model's adeptness in handling lengthy sentences, further underscoring its robust performance."}, {"title": "4.6.2 Representation Refinement", "content": "In this section, we illustrate the progressive refinement of question and choice representations in our model. Due to the separate generation of the choice-specific question representations and the enhanced choices, each example is accompanied by three heat maps. Figure 4 presents these heat maps, where from left to right, the first heat map represents the attention between the original Q and A from the encoder, the second heat map represents the attention scores between the choice-specific representation of question Q and the original A, and the third heat map signifies the interaction between the choice-specific representation of question Q and the enhanced A.\nIn example (a), we observe that for the correct choice \"have fun\u201d, our model assigns higher weights to the tokens \u201cpositive\u201d, \u201cdriving\u201d, and \"alcohol\" during the representation refinement process. Concurrently, the weights assigned to commonalities such as \"where\" and \"the\" gradually decrease. Example (b) demonstrates a similar trend, where the weights assigned to commonalities \u201cwhat\u201d, \u201ccan\u201d, \u201cyou\u201d, and \"?\" diminish. Both the correct choice \"sickness\" and the incorrect choice \"frequent urination\" receive attention on tokens \"drinking\" and \"too\". However, the correct choice emphasizes \"too\", highlighting the concept of excessive drinking, which is crucial for distinguishing it from the incorrect choice. These examples highlight our model's ability to recognize and leverage subtle differences among choices, showcasing its effectiveness in improving the accuracy of choice selection."}, {"title": "4.7 Parameter Comparison", "content": "We calculated the number of parameters for our model and the GenMC model under two configurations: T5-Base and T5-Large. In the T5-Base configuration, our model has 886.34 million parameters compared to GenMC's 895.37 million. Under the T5-Large configuration, our model has 2.81 billion parameters, while GenMC has 2.82 billion. Despite having fewer parameters, our model achieves higher accuracy than the GenMC model."}, {"title": "5 Conclusion", "content": "In this paper, we introduce DCQA, a model designed for Multiple-Choice Question Answering (MCQA). Unlike traditional approaches that start from the question, DCQA uniquely begins with the choices, allowing it to extract relevant phrases from the question that are most pertinent to each individual choice. By utilizing token-level attention to separate tokens of the question attended to by all the choices (i.e., commonalities) from those attended to by individual choices (i.e., nuances), DCQA generates a choice-specific representation of the question, thereby facilitating the selection of the correct choice. Our experimental results demonstrate that DCQA outperforms baseline models across a range of datasets. Furthermore, our case study illustrates how our model effectively highlights the nuances of the choices, showcasing its interpretability and reasoning capabilities."}, {"title": "A Additional Datasets and Their Results", "content": ""}, {"title": "A.1 Additional Datasets and Experimental Setup", "content": ""}, {"title": "A.2 Additional Results and Analysis", "content": "Table 7 presents the comparative results on the two additional datasets. The experimental findings indicate that our model outperforms most baseline models on these datasets, further validating the effectiveness of our reasoning approach. Notably, for the PIQA dataset, where the questions are short and the choices are long, capturing the tokens of the question attended to by all the choices (commonalities) from those by individual choices (nuances) is inherently more challenging. Nevertheless, our model performs comparably to the baselines and even surpasses them under the T5-Base and Unified-T5-Base setup."}]}