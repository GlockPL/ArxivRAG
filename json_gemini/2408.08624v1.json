{"title": "RealMedQA: A pilot biomedical question answering dataset containing realistic clinical questions", "authors": ["Gregory Kell", "Angus Roberts", "Serge Umansky", "Yuti Khare", "Najma Ahmed", "Nikhil Patel", "Chloe Simela", "Jack Coumbe", "Julian Rozario", "Ryan-Rhys Griffiths", "Iain J. Marshall"], "abstract": "Clinical question answering systems have the potential to provide clinicians with relevant and timely answers to their\nquestions. Nonetheless, despite the advances that have been made, adoption of these systems in clinical settings has\nbeen slow. One issue is a lack of question-answering datasets which reflect the real-world needs of health\nprofessionals. In this work, we present RealMedQA, a dataset of realistic clinical questions generated by humans and\nan LLM. We describe the process for generating and verifying the QA pairs and assess several QA models on BioASQ\nand RealMedQA to assess the relative difficulty of matching answers to questions. We show that the LLM is more\ncost-efficient for generating \"ideal\" QA pairs. Additionally, we achieve a lower lexical similarity between questions\nand answers than BioASQ which provides an additional challenge to the top two QA models, as per the results. We\nrelease our code' and our dataset publicly to encourage further research.", "sections": [{"title": "Introduction", "content": "Clinical question answering (QA) systems could allow clinicians to find timely and relevant answers to questions\noccurring during consultations in real-time [1, 2, 3, 4, 5]. This is especially important given the vast amount of\ninformation available to clinicians, and the time constraints they face [6, 7, 8].\nDespite the progress that has been made, biomedical question answering systems are still not widely used in practice.\nEven though they have seen continual improvement on benchmarks, the answers generated by these systems are not\nuseful to clinicians [9]. The reasons for this include the fact that the answers do not originate from reliable sources of\ninformation, the answers are not in the form of guidance, they do not account for the clinician's setting, the rationale\nprovided for the answers is not sufficient, and that systems do not resolve and communicate conflicting evidence and\nuncertainties adequately.\nThe capabilities of the systems are, in part, limited by the characteristics of existing biomedical QA datasets and\nbenchmarks. Common practices include deriving questions from the titles of research articles [10] and employing a\ndiverse biomedical expert panel [11]. Questions are often designed to test the systems' ability to comprehend\nbiomedical corpora, instead of eliciting information that would be required by clinicians in practice [12, 13].\nAdditionally, the quality of the source texts in corpora is often not verified, meaning that they could be out of date or\ncontain biased results. While these biomedical QA datasets may be suitable for investigating novel methods, they\nmay not be appropriate for systems intended for clinical settings. Thus, there is a need for a dataset that consists of\nrealistic clinical questions, as well as answers that have been verified by medical professionals.\nTo address this research gap, we present RealMedQA: a realistic biomedical QA dataset where the answers consist of\nclinical guideline recommendations provided by the UK-based National Institute for Health and Care Excellence\n(NICE). The questions were generated by a group of medical students and by a large language model (LLM). The\nquality of all the question-answer pairs then underwent verification conducted by the same group of medical students.\nWe show the LLM is more cost-efficient for generating \"ideal\" QA pairs. Several QA models were tested on the\nRealMedQA dataset and BioASQ [11] to compare the relative difficulty of matching the questions to the correct\nanswers. This quantitative investigation is supplemented by a qualitative comparison of a sample of BioASQ [11]\nand RealMedQA QA pairs."}, {"title": "Our key contributions are summarized as follows:", "content": "\u2022\n\u2022\n\u2022\nWe present RealMedQA: a biomedical dataset consisting of clinical questions that are realistic and whose\nanswers are clinical guideline recommendations, thus satisfying the requirements for reliability, guidance\nand rationale.\nWe provide a comparison of the questions generated by humans vs those generated by a LLM and show that\nthe LLM is more cost-efficient.\nWe provide the results of both quantitative and qualitative comparisons which suggest that the QA pairs of\nRealMedQA have a lower lexical similarity than those of BioASQ."}, {"title": "Methods", "content": "We followed a SQuAD-style [14, 15] process to generate the question answer pairs, where we used guideline\nrecommendations from the UK National Institute for Health and Care Excellence (NICE) as the answers. The process\ncan be broken down into three stages: data collection, question generation and question-answer pair verification.\nOur methodology resembles that of SQUAD; however, our objective diverges by focusing on the creation of question-\nanswer pairs for information retrieval rather than extractive or span-based QA. Considering the increasing prevalence\nof generative AI in biomedical QA [13], the relevance of extractive QA may diminish over time. Consequently, our\ndataset prioritizes information retrieval. Furthermore, this dataset is specifically designed for the primary care/general\nmedicine domain, where clinical guideline recommendations are generally sufficient.\nThe data collection stage involved downloading the guidelines via the NICE syndication\u00b3 API and extracting the\nrecommendations from the guidelines. Next, we hired six UK-based medical students via a university employment\nagency to generate questions that were addressed by the recommendations. The experience of the students varied\nfrom first year of (undergraduate) medical school to first-year medical junior doctors in the UK's National Health\nService. Up to 5 questions were created by the human generators for each recommendation and up to 20 were generated\nby the LLM (gpt-3.5-turbo\u2074). Finally, the quality of the dataset was assured through a round of verification by four\nof the medical students."}, {"title": "Data collection", "content": "All the guidelines available via the NICE syndication API were downloaded for further processing. The total number\nof guidelines was 12,543, grouped in several high-level topics. As we were focusing on guidelines for care provision\n(as opposed to administration), we limited the high-level topics to \u201cConditions and diseases\u201d which reduced the\nnumber of guidelines to 7,385."}, {"title": "Question generation", "content": "We created an instruction sheet on formulating questions for each recommendation. After extraction, this was used by\nthe medical students to develop questions. Questions were then collected using Google Forms\u2075. The generators were\nasked to provide up to 5 questions for each recommendation where the emphasis was on questions that clinicians\nwould be likely to ask during clinical practice. The aim was to avoid comprehension-style questions such as \"Who\nshould be included in a core specialist heart failure multidisciplinary team?\u201d and \u201cWhat are the responsibilities of the\nspecialist heart failure MDT?\", as it is unlikely that clinicians would be interested in these types of questions. Rather,\nresearch shows that they would be more likely to ask questions in the style of \"What is the drug of choice for condition\nX?\u201d or \"What is the cause of symptom X?\u201d14, i.e. questions that pertain to the diagnosis, treatment and management\nof patients.\nTo ensure generators' understanding of the task, we provided several examples of questions created for given\nrecommendation. For example, when given with the recommendation \u201cOffer an angiotensin-converting enzyme\n(ACE) inhibitor and a beta-blocker licensed for heart failure to people who have heart failure with reduced ejection\""}, {"title": "Question-answer pair verification", "content": "During the QA pair verification phase, we created Google forms with all the QA pairs generated by the human question\ngenerators and some generated by the LLM. The verifiers were then asked to answer the following questions using a\nLikert scale for each QA pair:\n\u2022\n\u2022\nDoes the question look like the sort of question a doctor would ask in practice? (Plausible)\nIs the question answered by the guideline? (Answered)\nEach of the questions could be answered with the following options: \"Completely\", \u201cPartially\u201d and \u201cNot at all\u201d. The\nverifiers were four of the same students who generated questions in the previous phase; they would verify the questions\ngenerated by the LLM and each other. We ensured that no one was asked to verify the questions that they themselves\ncreated. To address potential discrepancies among verifiers, we conducted an initial pilot study to identify the sources\nand reasons for such discrepancies. Based on the pilot results, we revised the instructions to minimize future\ndiscrepancies. We henceforth refer to question that were rated as both \u201ccompletely\" plausible and \u201ccompletely\"\nanswered by the recommendations as \u201cideal\u201d.\nOnce the instruction sheet was updated, each verifier was allocated 200 QA pairs created by humans and 100 generated\nby the LLM. As there were four verifiers, a total of 800 human QA pairs and 400 LLM QA pairs were verified (1200\nin total). Blinding was applied to the source of each QA pair.\nIn addition to the verification matrix, we also report the inter-verifier agreement scores between two verifiers based\non 50 human QA pairs and 50 LLM QA pairs. We calculated the agreement for both the plausibility of the questions\nand whether they are answered by the recommendations. The scores include the following:\n\u2022\n\u2022\nCohen's kappa, where \"Completely\", \"Partially\" and \"Not at all\" ratings are treated as separate categories\n(Table 2);\nCohen's kappa, where \"Partially\" and \"Not at all\" ratings are treated as one category (Table 3);"}, {"title": "Experiments", "content": "To assess the difficulty of matching RealMedQA's questions and answers relative to other datasets, we evaluated\nseveral question-and-answer encoders on all the \"ideal\" QA pairs. Cosine similarity was used to assess the similarity\nbetween question-and-answer vectors. To assess the quality of the QA matchings, we used recall@k where k was set\nto 1, 2, 5, 10, 20, 50 and 100, as well as nDCG@k and MAP@k where k was set to 2, 5, 10, 20, 50 and 100. The\nrecall@k is the fraction of correct answers included in the top k list, while the nCDG@k evaluates the ordering of the\ntop k list (more relevant items should have higher confidence scores). The MAP@k also assesses the quality of the\nranked top k list. We plot the 95% Wald confidence interval [17] associated with recall@k and MAP@k, as those\ncould be interpreted as the probability of correctly identifying a positive given a set of true positives and predicted\npositives, respectively. The nDCG, on the other hand, does not represent a probability, so it would not be meaningful\nto calculate the Wald confidence interval for this metric. We compared several models, including BM25 (term\nfrequency-based ranking function; [18, 19]) and BERT-based LLMs (110 million parameters): the original BERT\n(pre-trained on Wikipedia data; [20]), SciBERT (fine-tuned on scientific articles; [21]), BioBERT (fine-tuned on\nbiomedical articles; [22]), PubMedBERT (pre-trained on biomedical articels; [23]) and Contriever (fine-tuned using\na contrastive loss; [24]). Although BERT has demonstrated suboptimal performance on semantic encoding tasks\nwithout additional training or pooler representations [25], there is currently no equivalent of Contriever or sentence-\nbased encodings specifically for the biomedical domain (hence the development of biomedical LLMs). Furthermore,\nour dataset was insufficiently large to fine-tune any of the models. Therefore, we included the general BERT model\nas a baseline for comparison with the biomedical variants. We also evaluated the same models on a BioASQ-based\nretrieval dataset [11] used as part of BEIR [26] to compare its difficulty with that of RealMedQA; we used the titles\nand abstracts for the BioASQ answers. We randomly sampled the same number of QA pairs from BioASQ as there\nwere in the \"ideal\" RealMedQA dataset. We compliment the quantitative comparison of RealMedQA and BioASQ\nwith a qualitative investigation of the QA pairs. We used the Gensim and Rank-BM25 Python packages to conduct\nthe BM25 experiments. The BERT-based evaluations utilized the transformers, datasets, torch and faiss-gpu\npackages. We employed the scikit-learn implementation of all the metrics. The experiments were carried out on a\n24GB NVIDIA GEFORCE RTX 3090 and each run took fewer than 10 minutes."}, {"title": "Results", "content": "Out of 1200 QA pairs, only 230 (19% (2 s.f.)) were deemed to contain questions that were \"completely\" plausible\nwith recommendations that \"completely\" addressed the questions (Table 1)."}, {"title": "Inter-verifier agreement", "content": "The inter-verifier agreement scores are shown in Tables 2 and 3. The inter-verifier agreement scores are consistently\nhigher for the human QA pairs than for those of the LLM (where the effects are especially pronounced for the\ncombined Cohen's kappa in Table 3). Nonetheless, the inter-verifier agreement is substantially higher for the\nplausibility scores than the answered scores. The Cohen's kappa in Table 3 is lower for the LLM's \u201canswered\"\nratings (0.22) than Table 2 (0.24), suggesting that the reviewers disagreed more over whether the LLM \u201ccompletely\"\nanswered the questions than whether the LLM's answer fitted into the fine-grained category. This demonstrates the\nlack of clarity over what constitutes a \u201ccompletely\u201d answered question, especially when it was generated by an LLM."}, {"title": "Costs for generating and verifying questions", "content": "Table 4 shows the costs of the human question generators, the number of QA pairs generated and the cost per QA pair\nin dollars. The cost of the humans was taken to be the wages paid, while the LLM cost was the cost of calling the\nOpenAI API. Table 5 also shows the cost for verifying each QA pair based on estimates obtained during the pilot\nverification phase. The main cost for the LLM is the question verification, while that of the humans is the question\ngeneration. Additionally, before accounting for question quality, it is approximately three times more expensive to\ngenerate question answer pairs using humans than the LLM. We account for question quality in our cost calculations\nin Table 5. Humans are shown to be more efficient than LLMs with a \"ideal\" QA yield of 0.222, compared with 0.130\nfor the LLM. Moreover, when we account for question quality, the cost per \"ideal\" QA pair of the LLM is just over\nhalf that of the humans."}, {"title": "Quantitative comparison of BioASQ and RealMedQA", "content": "As shown in Figure 1A the recall@k of BM25 on BioASQ is greater than that of Contriever for lower values of k,\nwhile Contriever matches the performance of BM25 for higher values. On the other hand, Contriever consistently\noutperforms BM25 on RealMedQA (Figure 1B). The confidence intervals do not overlap for higher values of k on\nRealMedQA which means that the differences are statistically significant. Furthermore, both BM25 and Contriever\nperform worse on RealMedQA than on BioASQ, while the reverse is true for the other models. All the BERT-based\nmodels apart from Contriever perform worse than BM25. This suggests that the self-supervised losses used to train\nthese models are not sufficient for discerning the relative similarity of text spans, even when the models are pre-\ntrained/fine-tuned on domain-specific data. The recall@k is consistently greater for BERT than all the fine-tuned\nscientific and biomedical models on BioASQ. In contrast, the recall@k is consistently higher for BioBERT than\nBERT on RealMedQA, while the recall@k of SciBERT matches that of BERT. BioBERT fine-tuned on PubMedQA\n[10] outperforms BERT on RealMedQA for lower values of k, while the opposite is true for higher values of k.\nBioBERT fine-tuned on PubMedQA under-performs when compared to the original on both BioASQ and\nRealMedQA. PubMedBERT has a consistently higher recall@k than the BioBERT variant fine-tuned on PubMedQA\non BioASQ, while PubMedBERT has the lowest recall@k on RealMedQA for all values of k."}, {"title": "Qualitative comparison of RealMedQA and BioASQ", "content": "To account for the difference of the models on each dataset we examined QA pairs randomly sampled from each\ndataset. Within the BioASQ dataset, the question \"dna methylation in tongue is not found in the blood\" is accompanied\nby the title and abstract of the study with PMID 28603561 [27]. The title is \"Distinct DNA methylation profiles in\nsubtypes of orofacial cleft.\" Upon inspection, there appears to be a high lexical similarity between the question and\nthe title and abstract. For example, the word \"methylation\" is in the title and recurs regularly in the abstract, while the\nsame can be said for \"blood\". \"DNA\" also appears both in the title and the abstract.\nOn the other hand, the question \"What considerations should be addressed while revising the management programme\nfor a child or adolescent?\" from RealMedQA is accompanied by the following recommendation from the guideline\n\"Spasticity in under 19s: management\": \u201cIf adverse effects (such as drowsiness) occur with oral diazepam or oral\nbaclofen, think about reducing the dose or stopping treatment.\"\nWhen comparing the two samples, there is a greater lexical overlap between the question and answer from BioASQ\nthan between those of RealMedQA. Furthermore, BioASQ was not intended for clinical use which is why the dataset\nconsists of general basic science questions, while RealMedQA contains specific clinical questions that elicit\ninformation of interest to the clinician. Given that the intention of BioASQ is to assess the performance of QA systems\nin the biomedical domain, fewer checks would have been required for the reliability of the answer compared with the\nNICE guidelines."}, {"title": "Discussion", "content": "In the previous sections, we compared the yield for \"ideal\" QA pairs of the LLM and human question generators,\nwhere we showed that the human yield was 7.4% greater than that of the LLM. In other words, LLMs are competitive\nwith humans, especially when the cost per QA pair is considered. Nonetheless, the low inter-verifier agreement over\nwhether the questions were adequately answered by the recommendations (even with examples in the instructions),\ndemonstrates the subjective nature of QA pair generation. Further research is required on the standardization of QA\npairs."}, {"title": "LLM vs human QA pair costs", "content": "While the employment of LLMs does bring down the cost of question generation significantly, $12 is still required\nfor each \"ideal\" QA pair, due to verification costs. This means that even generating a QA dataset with 1000 QA pairs"}, {"title": "Comparison of RealMedQA and BioASQ datasets", "content": "The superior performance of BM25 to the other models on BioASQ suggests a high degree of lexical similarity\nbetween the questions and answers. On the other hand, the improved performance of Contriever compared with other\nmodels on RealMedQA implies a lower degree of lexical similarity between the questions and answers. This finding\nis supported by the random samples taken from BioASQ and RealMedQA.\nAdditionally, BERT's superior performance to all the other models on BioASQ shows that in-domain fine-tuning is\nuseful only for RealMedQA and not for BioASQ. The importance of general domain data for pretraining is highlighted\nby PubMedBERT's low performance on both datasets, as PubMedBERT was only pretrained on PubMed abstracts.\nThis is especially important when dealing with other forms of biomedical data such as clinical guidelines.\nFine-tuning BioBERT on PubMedQA does not improve the performance on either task. As PubMedQA questions are\nderived from the titles of the source papers, the high lexical similarity between the questions and the answers could\nhinder the performance of the fine-tuned model on both datasets."}, {"title": "Comparison with other biomedical QA datasets", "content": "Several biomedical QA datasets have been developed for various purposes, including benchmarking the knowledge\nencoded by LLMs [28], reading comprehension over electronic health records [29], and reading comprehension over\nbiomedical article abstracts [10]. While these datasets assess the quality of LLMs and biomedical QA components\n(information extractors), our dataset aims to retrieve clinically approved guideline recommendations using realistic\nclinical questions, addressing an existing research gap [12, 13]."}, {"title": "Limitations", "content": "Since the experiments were conducted, newer versions of GPT (e.g., 4 and 4o; [30]) have been released. It is important\nto note that generative AI is a rapidly evolving field, and the cost analysis might change in light of these updates.\nFurthermore, we recognize that retrieval-augmented generation (RAG) workflows [31, 32] may provide more\nappropriate answers to clinicians. However, this work focuses solely on the retrieval of clinical guideline\nrecommendations using realistic clinical questions."}, {"title": "Potential implications and future directions", "content": "The dataset introduced in this paper consists of realistic clinical questions created by medical students and a large\nlanguage model (LLM). We have demonstrated the potential of LLMs to create machine learning datasets, considering\nquality and verification costs. We envision that our findings will lead to the extended use of LLMs for dataset creation,\nincluding in the biomedical question-answering (QA) domain. Although the dataset is currently too small for training\npurposes, we aim to expand it by leveraging LLMs to create additional questions, with medical professionals verifying\nthe QA pairs. We also plan to explore the use of novel prompting techniques, including self-verification of the\nquestions and QA pairs via language model cascades [33]. The extended dataset would provide a more realistic\nrepresentation of biomedical QA for clinicians and could drive methodological breakthroughs in this area, leading to\nsystems that better address clinicians' information needs. Additionally, the dataset could be extended to address both\ngeneration and retrieval for RAG."}, {"title": "Conclusion", "content": "In this work, we have presented a methodology for generating a biomedical question answering dataset containing\nrealistic clinical questions and answers. We compared LLM and human question generators and have found that\nLLMs are more cost-efficient, even when QA quality is considered. Nonetheless, the LLM's cost-efficiency is heavily\naffected by the human scrutiny applied to the output of the LLM. Furthermore, we found that human verifiers are\nunlikely to agree on whether a question has been adequately answered by a recommendation. Finally, the experimental\nresults suggest that the lexical similarity between the questions and answers of RealMedQA are lower than that of\nBioASQ. Hence, the methodology presented here offers a promising avenue to creating biomedical QA datasets."}]}