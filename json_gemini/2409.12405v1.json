{"title": "On the Effectiveness of LLMs for\nManual Test Verifications", "authors": ["Myron David Lucena Campos Peixoto", "Davy de Medeiros Baia", "Nathalia Nascimento", "Paulo Alencar", "Baldoino Fonseca", "M\u00e1rcio Ribeiro"], "abstract": "Background: Manual testing is vital for de-\ntecting issues missed by automated tests, but\nspecifying accurate verifications is challeng-\ning. Aims: This study aims to explore the\nuse of Large Language Models (LLMs) to pro-\nduce verifications for manual tests. Method:\nWe conducted two independent and comple-\nmentary exploratory studies. The first study\ninvolved using 2 closed-source and 6 open-\nsource LLMs to generate verifications for man-\nual test steps and evaluate their similarity to\noriginal verifications. The second study in-\nvolved recruiting software testing profession-\nals to assess their perception and agreement\nwith the generated verifications compared to\nthe original ones. Results: The open-source\nmodels Mistral-7B and Phi-3-mini-4k demon-\nstrated effectiveness and consistency compara-\nble to closed-source models like Gemini-1.5-\nflash and GPT-3.5-turbo in generating manual\ntest verifications. However, the agreement level\namong professional testers was slightly above\n40%, indicating both promise and room for\nimprovement. While some LLM-generated ver-\nifications were considered better than the origi-\nnals, there were also concerns about AI halluci-\nnations, where verifications significantly devi-\nated from expectations. Conclusion: We con-\ntributed by generating a dataset of 37,040 test\nverifications using 8 different LLMs. Although\nthe models show potential, the relatively mod-\nest 40% agreement level highlights the need\nfor further refinement. Enhancing the accuracy,\nrelevance, and clarity of the generated verifica-\ntions is crucial to ensure greater reliability in\nreal-world testing scenarios.", "sections": [{"title": "1 Introduction", "content": "Manual testing refers to the process of manually\nexecuting test cases without the use of automation\ntools. Testers perform these tests from an end-\nuser perspective to identify defects and ensure that\nthe software functions as expected. Manual test-\ning involves various test techniques, including ex-\nploratory, usability, and ad-hoc testing, to uncover\nissues that automated tests might miss.\nManual testing remains crucial in software en-\ngineering despite the rise of automated testing for\nseveral reasons. Manual tests (i) leverage human\nintuition to detect issues that automated tests might\nmiss; (ii) support exploratory testing of the appli-\ncation; (iii) adapt to changes in the software and\ndeal with rapidly changing projects; and (iv) sup-\nport tests involving usability, being better suited\nfor subjective criteria like look and feel, ease of\nuse, and user satisfaction. Specifying manual tests\ninvolves detailing the actions that developers need\nto perform and outlining the expected results to\nverify after each action. Although specifying man-\nual tests might seem straightforward, involving\nonly actions and verifications, recent studies like\n(Soares et al., 2023) and (Aranda et al., 2024) have\nidentified numerous flaws in existing specifications.\nThese studies found actions without verifications or\ninappropriate verifications, including redundancy,\nvagueness, and excessive complexity, which can\nseverely undermine the effectiveness of the testing\nprocess.\nIn this context, this paper presents an exploratory\nstudy on the effectiveness of large language mod-\nels (LLMs) to produce verifications for manual\ntests. Large language models, trained on diverse\nand extensive datasets, exhibit remarkable capabil-\nities in understanding and generating human lan-\nguage. These models can be employed to do a\nvariety of tasks (Vaswani et al., 2017). In the con-\ntext of manual testing, LLMs can be a promising\nway to produce textual verifications able to specify\nproperly the expected results of actions performed\nby testers. In our study, we analyze the effective-\nness of six open-source (Mistral-7B, Phi-3-mini-\n4k, Stablelm-zephyr-3b, Qwen1.5-7B, Gemma-\n2b, and Meta-Llama-3-8B) and two closed-source\n(Gemini-1.5-flash, and GPT-3.5-turbo) LLMs. The"}, {"title": "2 Natural Language Test Smells", "content": "Manual testing uses natural language to describe\ntest cases. Poorly written test cases can suffer from\nissues like smells or failures, posing threats to test-\ning activities.\nFor example, the Unverified Action Smell occurs\nwhen an action lacks verification (Soares et al.,\n2023). Without proper verification, tests may not\nconfirm the application's expected behavior. This\ncan confuse testers about outcomes, such as \u201cWhat\nshould happen when the message is clicked again?\"\nand \"Do additional elements disappear? If not,\ndoes the test fail?\" Addressing these uncertainties\nis crucial for clarity and effective test execution."}, {"title": "3 Study Design", "content": "We evaluate the effectiveness of open and closed-\nsource LLMs in producing manual test verifica-\ntions. We aim to answer the following research\nquestions:\nRQ1. How effective are open-source LLMs to\nproduce manual test verifications?\nThis research question evaluates the effective-\nness of open-source LLMs in producing manual\ntest verifications. To do that, we send several test\nactions to the open-source LLMs, obtain the veri-\nfications produced by these LLMs, and calculate\nthe similarity between the produced verifications\nand the original ones. As a result, we have sim-\nilarity indices indicating how close the produced\nverifications are to the original verifications.\nRQ2. How effective are closed-source LLMs\nto produce manual test verifications?\nSimilarly to the RQ1, the RQ2 evaluates the\neffectiveness of LLMs, but now this research ques-\ntion focuses on closed-source LLMs. As a result,\nwe expect to reveal the effectiveness of closed-\nsource LLMs in producing manual test verifications\nthat are close to original verifications.\nRQ3. How effective are open-source LLMS\nto produce manual test verifications when com-\npared to closed-source LLMs?\nIn this research question, we compare the effec-\ntiveness of open-source and closed-source LLMs.\nTo do that, we use statistical tests to verify whether\nthere is a statistically significant difference between\nthe similarity indices obtained from each kind of\nLLM (open and closed-source). As a result, we\nexpect to reveal in which cases it is better to use\nclosed-source LLM instead of open-source LLM\nand vice-versa. This is important because the use\nof open-source LLMs may lead to cost reduction\nin a software project.\nRQ4. How do software testing professionals\nperceive the verifications produced by LLMs?\nIn this research question, we evaluate profes-\nsionals' perceptions of manual test verifications\nproduced by open- or closed-source LLMs. To do\nthat, we recruit six software testing professionals\nfrom a large smartphone manufacturer. The name\nof the company is omitted due to non-disclosure\nagreements. Answering RQ4 is important to bet-\nter understand whether the software testing pro-\nfessionals find the produced verifications useful to\nimprove the quality of the tests."}, {"title": "3.1 Dataset", "content": "Our study uses a dataset sourced from the Ubuntu\nmanual test repository (Ubuntu, 2024) and cata-\nloged by Aranda et al. (Aranda et al., 2024). This\ndataset includes 973 manual test cases with a total\nof 6,598 test steps. Each test step is structured as a\ntuple of actions and verifications (expected results),\nthough some steps are missing actions (47) or verifi-\ncations (1968, approximately 29.83%). The Unveri-\nfied Action Smell is one of the test smells identified\nby Aranda et al. For our experiment, we focused\non the 4,630 test steps that contain both actions and\nverifications."}, {"title": "3.2 Process", "content": "Figure 1 illustrates the flow of our experimental\nprocess. Our study consists of five steps:\nS1. Test Step Selection: We start by processing\nthe dataset and selecting test steps that have both\nactions and verifications, as described in subsection\n3.1.\nS2. Closed-Source LLM Verifications: For\ngenerating verifications, we use selected closed-\nsource models (Gemini-1.5-flash and GPT-3.5-\nturbo), chosen for their popularity and reliability.\nThe applied prompt is:\nConsider a manual test which\nhas a precondition and a\nlist of steps with actions\nand verifications. Given\nthe precondition {precondition},\ncomplete a test step generating\nthe reaction for the following\naction: {action}. Only generate\nthe verification in one line and\nreturn it in raw text.\nHere, {precondition} refers to the test precondi-\ntion or header, and {action} refers to the actions of\nthe test step.\nS3. Open-Source LLM Verifications: We also\nuse open-source models (Gemma-2b, Phi-3-mini-\n4k, Meta-Llama-3-8B, Qwen1.5-7B, Llama-2-7B,\nMistral-7B, and Stablelm-zephyr-3b) to generate\nverifications. These models were selected for their\ntrending status on Hugging Face and diverse archi-\ntectures. The actions were input into these mod-\nels using the same prompt structure as the closed-\nsource ones.\nAs a result of steps 2 and 3, where we generated\na verification for each one of the 4630 test steps\nwith the 8 models, we created a dataset of 37040\ngenerated test verifications.\nS4. Similarity Analysis: A similarity function\nis applied to compare the 37040 generated verifica-\ntions with the original ones and to select samples\nfor the experiment with the software testing profes-\nsionals, as described in subsection 3.3. This step\ninvolves using similarity-based sampling to ensure\ndiverse representation and reduce bias.\nS5. Evaluation by Professional Testers: Fi-\nnally, professional testers evaluate a subset of the\ngenerated verifications. Detailed information about\nthis validation experiment with the testers is pro-\nvided in subsection 3.4."}, {"title": "3.3 Similarity Technique", "content": "We used the Semantic Textual Similarity (STS)\ntechnique (Reimers and Gurevych, 2019) with the\ncosine similarity metric to evaluate the verification\ngenerated by the LLMs. This technique compares\nthe generated verification text with the original veri-\nfication text, where higher similarity scores indicate\ngreater semantic similarity.\nThese similarity results are used at two critical\npoints in our experiment. First, to evaluate the ef-\nfectiveness of the LLMs by assessing how closely\nthe generated verifications match the original ones.\nSecond, to select samples for testing with profes-\nsional testers. For each model, we grouped the test\nsteps into five similarity ranges: 0-0.2, 0.2-0.4, 0.4-\n0.6, 0.6-0.8, and 0.8-1, aiming to remove bias by\nincluding samples with low similarity scores that\nstill hold meaningful content.\nFor each of the 8 LLM models, we randomly\nselected 3 samples from each similarity group, re-\nsulting in a total of 120 samples (8 models * 5\ngroups * 3 samples). This process was repeated\nfor each of the 6 participants, resulting in a total\nof 720 samples to be manually evaluated. This en-\nsured a diverse and balanced sample selection for\nthe experiment."}, {"title": "3.4 Participants", "content": "For this study, we recruited six software testers to\nevaluate the generated verifications. Each partici-\npant received 120 tuples, with each tuple contain-\ning the original verification and the corresponding\ngenerated verification.\nParticipants were asked, \u201cDo you agree with\nthe generated verification?\" using a Likert scale\nfrom \"Strongly Disagree\" to \"Strongly Agree.\"\nThis quantitative method assessed whether partici-\npants perceived the generated verification as similar"}, {"title": "4 Results", "content": "This section describes and discusses the research\nquestions analyzed in our study.\nRQ1) How effective are open-source LLMs to\nproduce manual test verifications?\nIn RQ1, we evaluate the effectiveness of open-\nsource LLMs. Figure 2 shows violin plots rep-\nresenting the similarity distribution between the\nactual verifications and the ones produced by seven\ndifferent open-source models: Gemma-2b, Phi-3-\nmini-4k, Meta-Llama-3-8B, Qwen1.5-7B, Llama-\n2-7B, Mistral-7B, and Stablelm-zephyr-3b. Each\nviolin plot represents the distribution of similarity\nvalues for each model, with key statistical measures\n(median and interquartile range - IQR) highlighted.\nThe median represents the middle value of the sim-\nilarity scores, meaning half of the scores are above\nthis value and half are below. A higher median\nindicates that, overall, the model tends to produce\nhigher similarity scores, which may suggest better\nperformance in terms of semantic similarity. The\nIQR measures the spread of the middle 50% of the\ndata, calculated as the difference between the 75th\npercentile (Q3) and the 25th percentile (Q1). A low\nIQR indicates that the similarity scores are closely\nclustered around the median, suggesting high con-\nsistency and low variability. This means the model\nreliably produces similar scores across different\ninputs. A high IQR indicates a wider spread of sim-\nilarity scores, suggesting greater variability. This\ncould mean the model is more sensitive to differ-\nent inputs and produces a wider range of similarity\nscores, which may be desirable in certain contexts\nbut could also indicate less predictability.\nMistral-7B. The distribution is relatively wide,\nwith a peak around the median. The median simi-\nlarity score is around 0.42, indicating a moderate\nlevel of similarity between the actual verifications\nand the verifications produced by the models. In\nsoftware testing, a cosine similarity of 0.42 be-\ntween two verification vectors might indicate that"}, {"title": "4", "content": "the verifications share some common terms or top-\nics. Fortunately, reaching a high similarity is unnec-\nessary to obtain a reasonable verification since it\njust needs to be coherent with the actions described\nin the tests. We also observe that the first and third\nquartiles are 0.27 and 0.56, respectively, resulting\nin an IQR of 0.29. This indicates a low variability\nof the similarity scores, suggesting that the model\nis consistent in the produced verifications.\nPhi-3-mini-4k. Similarly to Mistral-7B, the\nmodel Phi-3-mini-4k presents a peak near the me-\ndian and low variability. The median similarity\nscore is around 0.402, with an IQR from 0.251 to\n0.55 (IQR = 0.3), suggesting that Phi-3-mini-4kcan\nconsistently produce reasonable verifications.\nStablelm-zephyr-3b. This model presents a me-\ndian similarity score slightly lower than Mistral-7B\nand Phi-3-mini-4k. Stablelm-zephyr-3b presents a\nmedian of around 0.36, indicating that these models\ncan produce reasonable verifications. Notice also\nthat the Stablelm-zephyr-3b presents an IQR (0.27)\nslightly greater than Mistral-7Band Phi-3-mini-\n4k, indicating that Stablelm-zephyr-3b presents\na slightly better consistency in the production of\nverifications than Mistral-7B and Phi-3-mini-4k.\nQwen1.5-7B. This model presents a median sim-\nilarity score equal to Stablelm-zephyr-3b, obtain-\ning a median of around 0.36. Regarding the IQR,\nQwen1.5-7B presents a value of 0.26, indicating\na consistency in the production of the verifications\nsimilar to Stablelm-zephyr-3b. Thus, both the mod-\nels Stablelm-zephyr-3b and Qwen1.5-7B present\neffectiveness and consistency very similar in the\nproduction of verifications.\nGemma-2band Meta-Llama-3-8B. These mod-\nels present median similarity scores varying from\n0.3 (Meta-Llama-3-8B) to 0.32 (Gemma-2b). Al-\nthough these scores are slightly lower than the pre-\nviously analyzed models, we observe that the mod-\nels Gemma-2b and Meta-Llama-3-8B still produce\nreasonable verifications. Regarding the IQR, these\nmodels present values close to those previously\nanalyzed models. This indicates that the models\nGemma-2b and Meta-Llama-3-8B present consis-\ntency in the produced verifications.\nSummary. The models Mistral-7B and Phi-3-\nmini-4k present the best effectiveness and consis-\ntency in the production of verifications."}, {"title": "5", "content": "3.5-turbo present a median similarity score equal\nto the Mistral-7B. Regarding the IQR, Gemini-1.5-\nflash and GPT-3.5-turbo present a similarity scores\nvariability slightly greater than the Mistral-7B.\nSummary. The results indicate that the open-\nsource models are as effective as closed-source\nmodels.\nRQ3) How effective are open-source LLMs to\nproduce manual test verifications when\ncompared to closed-source LLMs?\nIn this research question, we analyze the pro-\nfessionals' perceptions on the verifications\nproduced by LLMs. Figure 4 presents the\ndistribution of responses for different LLMs,\ncategorized by their source (closed or open).\nAlso this, shows the percentage of responses for\neach agreement level across different models,\nincluding both closed source (gemini-1.5-flash,\ngpt-3.5-turbo) and open source (TheBloke/Mistral-\n7B-Instruct-v0.2-GGUFF, microsoft/Phi-3-mini-\n4k-instruct-gguf, Qwen/Qwen-1.5-7B-Chat-GGUF,\nTheBloke/stablelm-zephyr-3b-GGUF, Imstudio-\nai/gemma-2b-it-GGUF, QuantFactory/Meta-\nLlama-3-8B-Instruct-GGUF). The open-source\nmodels received the most negative feedback and\nthe closed-source models performed relatively\nbetter in terms of generating acceptable verifica-\ntion. These findings highlight the importance of\ncontinuous improvement and customization of\nLLMs for specific tasks like test case verification.\nMoreover, they underscore the need for further\nresearch and development to enhance the reliability\nand accuracy of verification generation processes.\nThe figure clearly illustrates the varying levels\nof agreement among test analysts regarding the"}, {"title": "6", "content": "quality of generated verification by different\nLLMs. While some models show promise, there\nis significant room for improvement, especially\nin ensuring that automated verification meets the\nexpectations and standards of human test analysts.\nRQ4) How do software testing professionals\nperceive the verifications produced by LLMs?\nIn Figure 4 it is possible to notice a predomi-\nnantly negative perception of the LLM generated\nverifications among test analysts. The high per-\ncentage of \"Strongly Disagree\" and \"Disagree\" re-\nsponses highlights significant skepticism regarding\nthe current effectiveness of LLMs in generating\nreliable test case verifications. The relatively lower\npercentages of \"Agree\" and \"Strongly Agree\" sug-\ngest that while there is some recognition of the\nutility of LLMs, it is limited and not widespread.\nThis portion of the respondents who recognize the\npotential utility of these models point to an area for\npotential improvement and future research.\nThese results suggest a need for further develop-\nment and refinement of LLMs to better meet the\nneeds and expectations of test analysts. Enhance-\nments in the accuracy, relevance, and clarity of\ngenerated verifications could improve acceptance\nand satisfaction levels among users."}, {"title": "5 Discussion", "content": "In this section, we discuss the main results.\nAs shown in RQ1 and RQ3 results, Mistral-7B\nand Phi-3-mini-4kmodels presents more consis-\ntency and effectiveness in producing verifications.\nLooking at the specific verifications generated from\nthese models, we notice that the original verifica-\ntions of the test steps where the generated verifica-\ntions are more acceptable (in terms of developers'\nperceptions) commonly involve short and direct\nsentences. The sample below have a generated\nverification from Mistral-7B with a high semantic\nscore of 0.93 but labeled in the \"Strongly Agree\"\ngroup. Action:\nWrite \"ping\" into the \"Reply\"\ninput field, press \"Send\" and\ndrag up to the top of the timeline\nOriginal Verification:\nA message containing \"ping\" is\nshown at the top of the timeline\nGenerated Verification:\nThe \"ping\" message appears at the\ntop of the timeline.\nAlthough developers' perceptions typically align\nwith high similarity scores, discrepancies can occur.\nFor instance, a generated verification with a simi-\nlarity score of 0.34 was still labeled as \"Strongly\nAgree.\"\nAction:\nMove the Volume control (up/down)\nOriginal Verification:\nDid you feel the change of volume?\nGenerated Verification made by Phi-3-mini-4k\nmodel:\nVerify that the volume level\nchanges as expected when\nadjusting the controls.\nSimilarly, there are cases where samples with\nhigh similarity scores are labeled as \"Strongly Dis-\nagree,\" such as a verification generated by the\nQwen1.5-7B, model.\nAction:\nNote the state of the 'Erase disk\nand install FAMILY' radio button\nOriginal Verification:\nThe 'Erase disk and install\nFAMILY' radio button is selected\nGenerated Verification made by Phi-3-mini-4k\nmodel:\nVerify that the radio button is\ndisabled\nClosed-source LLM.\nAs shown in Figure 4, the Gemini-1.5-flash,\nmodel outperformed GPT-3.5-turboslightly, with\nmore \"Strongly Agree\" and fewer \"Strongly Dis-\nagree\" responses, both close in similarity scores\nand professional perceptions.\nBoth open-source and closed-source models per-\nform better with short, direct commands in tests.\nWe observe the same behavior previously related\nto open source models in cases where we have a\nlow semantic similarity score but the generated\nsentence was cataloged in the \"Strongly Agree\"\ngroup.\nThe same occurs in the other direction, with the\ngenerated sentence having a high similarity score\nbelonging to the \"Strongly Disagree\" group.\nOpen x Closed-source LLM. As seen in Fig-\nure 4, the closed-source models obtained some\nadvantage over the open source ones. This can\nbe exemplified by the sample below, where both\nclosed-source models obtained 0.81 in the seman-\ntic similarity score while all open source models\nobtained less than 0.4, with the exception of the\nPhi-3-mini-4k model, which obtained 0.62 in the\nsimilarity score."}, {"title": "7", "content": "The action in this case is more complex and if we\nlook at the analyzed test step, we can identify more\nthan one smell based on the catalogs of (Haupt-\nmann et al., 2013) and (Soares et al., 2023).\nAction:\nIf there is only one hard disk,\nskip to step 12 (On the 'Where\nare you?' screen...). Otherwise,\non the 'Installation type' screen\nverify that the drive selected on\nthe Select drive list corresponds\nto the drive on the chart (e.g\n/dev/sda)\nA contributing factor for the difference in perfor-\nmance in the analyzed models is that each one of\nthem have considerable particularities. The Phi-3-\nmini-4k model, for example, has 3.7B parameters\nwhile Mistral-7Bhas 7B. Further, the closed source\nmodels are pre-trained with trillions of parameters,\nwhich makes the comparison unfair. Despite of\nthat, the results associated with the specific task\nof generating verifications indicate that there are\nno super gains of the closed-source in comparison\nwith the open source models.\nProfessionals' Perceptions. During the evalua-\ntion stage of this study, the volunteers made some\npertinent comments about the evaluated samples.\nFor instance, as a positive perception, some gener-\nated verifications were considered better than the\noriginal ones. In contrast, as a negative perception,\nthey noticed a few samples of AI hallucination,\nwith verifications completely different from the ex-\npected."}, {"title": "6 Implications", "content": "This section presents the implications of our study\nfor researchers and practitioners.\nThe discussed results highlight key concerns. In\nthe analyzed samples, many low-acceptance ver-\nifications could improve with prompt refinement.\nLack of context in prompts can lead to verifications\nthat initially seem suitable but yield completely\ndifferent results."}, {"title": "7 Related Work", "content": "Test actions are crucial in generating verifica-\ntions. Flawed tests can produce poor results, adding\nmore issues.\nThe findings show no significant differences be-\ntween open-source and closed-source models. The\ncritical factor appears to be the number of parame-\nters. As shown in Figure 4, models with fewer pa-\nrameters, such as Gemma-2b, and Stablelm-zephyr-\n3b, perform worst.\nUnderstanding these concerns can guide future\nresearch and practical applications, aiding model\nselection and experiment design. All models used\nare widely available, facilitating replication and\nnew studies.\nWe also produced a dataset with 37,040 test ver-\nifications generated by 8 different LLMs. This\ndataset includes complete data from Ubuntu's orig-\ninal tests in a structured database, enabling detailed\nqueries for future experiments.\nPrevious works already catalogued a set of test\nsmells that occurs on natural language tests.\n(Hauptmann et al., 2013) was the precursor, defin-\ning smells for manual tests in natural language as\nNatural Language Test Smells (NLTS). He defined\nseven types of smells.\nA few years later, (Rajkovic and Enoiu, 2022)\nproposes some metrics to analyse software require-\nments in a more generalized way. Despite involving\ntesting, it was not the main focus of the work. The\nwork also present a tool to automatically detect\nbad smells in software requirements based on the\nproposed metrics.\nTen years after (Hauptmann et al., 2013), (Soares\net al., 2023) presented six new smells and updated\nthe set. In both studies, the smells are well defined\nand we are presented with rules to identify them.\n(Soares et al., 2023) also present a NLP-based tool\nthat automaticaly identifies smells on manual tests.\nMore recently, in April 2024, (Wang et al., 2024)\npresents a comprehensive review of the utilization\nof pre-trained large language models (LLMs) in"}, {"title": "8 Threats to Validity", "content": "software testing. This survey analyzes 102 rele-\nvant studies, exploring the use of LLMs from both\nsoftware testing and LLM perspectives. Key ar-\neas of focus include test case preparation and pro-\ngram repair, with detailed discussions on common\nLLMs used, types of prompt engineering, and as-\nsociated techniques. The paper also identifies key\nchallenges and potential opportunities, providing a\nroadmap for future research in this domain.\nAlso, in 2024, (Aranda et al., 2024) proposes\na template to represent a natural language test\nand ways to treat the natural language test smells,\nwhich he calls \"transformations.\" In total, the study\npresents a set of seven transformations for seven\ndifferent smells, such as \"Extract Action\" transfor-\nmation for the smell \"Misplaced Action.\" (Aranda\net al., 2024) also present a tool to make the trans-\nformations of the smells automatically using NLP.\nIn the LLM field, there are a few studies that\naddresses the use of LLMs to augmenting software\ntesting methods. For example, (Siddiq et al., 2024)\npresented an empirical work on the use of LLMs\nto generate unit tests for the Java programming\nlanguage. (Yuan et al., 2023) develops a similar\napproach, evaluating the capacities of OpenAI's\nChatGPT\u00b9 for test generation and proposing a more\npowerful fine-tuned model. However, none of this\nstudies focuses on test smells, manual tests or more\nspecifically, natural language test smells.\nThere are potential threats to validity in our experi-\nment. Subjectivity in evaluation: Evaluating man-\nual tests by specialists can be subjective. To miti-\ngate this, we used the original tests as a baseline.\nMultiple Valid Descriptions and Test Smells: In\nmanual testing, it is possible to have multiple valid\ndescriptions for the same expected results. Addi-\ntionally, the presence of test smells in the test repos-\nitory, as cataloged by Aranda et al. (Aranda et al.,\n2024), can generate ambiguity. To address these\nissues, we included manual verification by test spe-\ncialists and incorporated samples from different\nsimilarity score groups. Human-generated base-\nline: We used human-generated tests as the ideal\nbaseline. However, these tests are not error-free.\nThe specialists noted that, in some cases, the gener-\nated tests surpassed the human-generated ones in\nquality. Minimizing disagreements and errors:\nSubjectivity and potential technical misunderstand-"}, {"title": "8", "content": "ings could lead to disagreements and errors. We\nminimized this by involving six specialists in the\nevaluation process."}, {"title": "9 Conclusion and Future Work", "content": "This paper is part of an ongoing effort to use LLMs\nto improve the quality of manual tests. At this first\nstage, we performed two independent and comple-\nmentary exploratory studies to evaluate the efficacy\nof LLMs in generating verifications for manual\ntests.\nWe evaluated 2 closed-source models and 6 open-\nsource models. In the first experiment, we used the\noriginal tests as a baseline and performed a simi-\nlarity evaluation, resulting in a dataset of 37,040\ngenerated verifications. Each entry included the\nactual verification test, the LLM model used, and\nthe similarity score. Our similarity evaluation be-\ntween actual and generated verifications revealed\nthat the open-source models Mistral-7B and Phi-\n3-mini-4k, along with the closed-source models\nGemini-1.5-flash and GPT-3.5-turbo, demonstrated\nthe best effectiveness and consistency.\nIn the second experiment, we recruited 6 soft-\nware testers to evaluate the generated tests com-\npared to the original ones. Despite including tu-\nples with low similarity scores for evaluation, we\nachieved similar results: the open-source model\nMistral-7B and the two closed-source models,\nGemini-1.5-flash and GPT-3.5-turbo, received the\nhighest acceptance scores. In the case of the\ntwo closed models, the sum of \"Strongly Agree,\"\n\"Agree,\" and \"Neutral\" responses exceeded 50%.\nNotably, the models with fewer parameters had the\nhighest disagreement levels.\nAn agreement level exceeding 40% suggests\nsome promise in this approach, but it also indicates\nthat further research is needed before LLMs can be\nreliably used to generate manual test verifications\nin practical settings. In future work, we plan to\nexplore the use of multiple LLMs in ensemble or\nvoting setups, as well as architectures where one\nLLM evaluates or agrees with another, to improve\nthe accuracy of generated verifications. Additional\nexperiments will assess the confidence of LLMs\nin generating manual test verifications, exploring\ndifferent models, contexts, and larger datasets to\nbetter understand their capabilities and limitations.\nWe also aim to investigate the efficacy of LLMs\nin addressing test smells, such as ambiguous tests,\nmisplaced verifications, and unverified actions. For"}, {"title": "10 Limitations", "content": "This study has some key limitations. Subjectivity\nin Evaluation: Despite our efforts to mitigate sub-\njectivity by using original tests as a baseline", "Smells": "We focused pri-\nmarily on addressing the Unverified Action Smell", "of\nResults": "While we used both automated and man-\nual verification methods", "Specificity": "The LLMs used were\nselected based on their popularity and availability.\nDifferent"}]}