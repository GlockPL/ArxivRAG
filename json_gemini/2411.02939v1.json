{"title": "A Post-Training Enhanced Optimization Approach for\nSmall Language Models", "authors": ["Keke Zhai"], "abstract": "This paper delves into the continuous post-training optimization methods for small language\nmodels, and proposes a continuous post-training alignment data construction method for\nsmall language models. The core of this method is based on the data guidance of large models,\noptimizing the diversity and accuracy of alignment data. In addition, to verify the effectiveness\nof the methods in this paper, we used Qwen2-0.5B-Instruct model as the baseline model for\nsmall language models, using the alignment dataset constructed by our proposed method, we\ntrained and compared several groups of experiments, including SFT (Supervised Fine Tuning)\npost-training experiment and KTO (Kahneman Tversky optimization) post-training\nexperiment, as well as SFT-KTO two-stage post-training experiment and model weight fusion\nexperiment. Finally, we evaluated and analyzed the performance of post-training models, and\nconfirmed that the continuous post-training optimization method proposed by us can\nsignificantly improve the performance of small language models.", "sections": [{"title": "Introduction", "content": "With the rapid development of artificial intelligence technology, the performance of large\nlanguage models (LLMs) [1] has made significant progress in the past few years. The number\nof parameters in these models has increased from the initial billion level, such as GPT-2 [2]\nwith 1.5 billion parameters, to today's trillion level, and the scale of training data has also\nincreased from a million level to tens of trillions level of tokens, such as the 15T tokens data of\nLLAMA 3.1 [3]. The driving force behind this progress are predictable scaling laws [4], which\nindicate that the performance of the model is directly proportional to its scale. However, with\nthe growth of the model scale, the cost and delay of reasoning have also increased, which limits\ntheir applications in resource-constrained environments such as edge devices.\nTo solve this problem, Small Language Models (SLMs) come into being. For example,\nMicrosoft's Phi-mini series [5], Google's Gemma 2 series [6], Meta's LLaMA 3.1-1B/3B [3], and\nAlibaba's Qwen-0.5B/1.5B/3B [7], etc. The strategy of large-scale data pre-training small\nmodels is adopted. The core idea of these models is to improve performance by increasing the\nscale of data while keeping the model parameters fixed, so as to maintain lower reasoning\ncosts and delay, and to achieve performance close to large models.\nAlthough small language models have been widely optimized during the pre-training phase,\nthe optimization of post-training phases is relatively scarce. This paper mainly explores the\ncontinuous optimization method of post-training of small language models, and proposes a"}, {"title": "Related Work", "content": "Although large models have achieved significant achievements in performance. However, their\nhigh computing costs and resource consumption have limited their applications in resource-\nconstrained environments. Small Language Models (SLMs) have attracted more and more\nattention due to their advantages in resources efficiency and actual costs, as these models\ndemonstrate capabilities comparable to Large Language Models (LLMs) while maintaining a\nrelatively small model size. For example, Microsoft's Phi-2/3 series models [10] have broken\nexisting scaling laws, showing that high-quality data itself is sufficient to build models that can\ncompete with larger models, and Qwen-0.5B/1.5B [7], LLaMA 3.2-1B [3], Tinyllama [11],\nMiniCPM [12], Shakti [13], etc. These small language models are based on the Transformer\ndecoder model and focus on exploring the training of small models with a much larger number\nof training tokens than what the scaling laws suggest [14], so that they have made significant\nbreakthroughs in both performance and efficiency, and they have shown huge potential in\nvarious application scenarios."}, {"title": "Post-Training Learning", "content": "The training of large models can be divided into two stages of pre-training and post-training.\nPre-training usually requires a large amount of computing resources and data. The basic\nmodel generated by pre-training has the ability to commonly understand human knowledge.\nThe post-training phase is based on the pre-trained base model, through further aligns the\nmodel with instructions to follow the ability and better stimulate the various comprehensive\nabilities of the model. Post-training includes supervised fine-tuning (SFT) [8] and RLHF [15].\nSFT has also various related methods proposed, such as LORA [16], QLORA [17]. RLHF is a\nwidely used method that depends on learning reinforcement strategies from human feedback.\nThe process of RLHF includes two stages: first, training a reward model (RM) with human\npreference data, and then using this reward model to guide the reinforcement learning\noptimization of the policy model (Policy Model). However, RLHF has several significant\nproblems, such as high memory occupation, unstable training, and complex processes. To\nsolve the complexity of RLHF, the DPO [18] method was proposed. DPO simplifies the RLHF\nprocess, transforming the training phase of enhanced learning into a binary classification\nproblem, reducing memory consumption and improves training stability. However, DPO\ncannot make full use of the reward model, and it is only applicable to pair of preference data,\nand it cannot process a wider feedback type. Further, KTO (Kahneman-Tversky Optimization)\n[9] was proposed, the core idea of KTO is to leverage the loss aversion characteristic in the"}, {"title": "Post-Training Data Construction", "content": "To effectively improve the diversity and accuracy of post-training data, this article proposes a\npost-training data pipeline. The overall architecture of the post-training data pipeline is shown\nin Figure 3-1:"}, {"title": "Alignment Data Collection", "content": "Our data all comes from the open source datasets on the Internet, mainly including\nBelleGroup/train_3.5m_CN, BelleGroup/school_math_0.25M, Coig, Camel, Dolly, Alpaca_GPT4\nand other open source datasets [20], covering math, code, reasoning and other general\ninstructions. We have standardized the format of these data, and preliminarily classified them\nbased on known types using rule-based methods. According to the content of the task, they\nare divided into 50 categories. At the same time, performing deduplication of the instruction\nprompt part, mainly based on the simhash deduplication algorithm, this algorithm can be used\nfor deduplication of massive data, by tokenizing all input instructions to calculate simhash\nvalues, establishing indexes, and setting the tolerance threshold k of Hamming distance, where\na larger k value results in more deduplication samples, we set k to 3. We ultimately generated"}, {"title": "Instruction Classification", "content": "To ensure the diversity of data, we classified the instructions on the preliminary screened 5M\ndata. The core method is to directly generate a secondary classification label for the prompt\npart of the instruction through the prompt method based on the large language model. The\nprompt is designed as follows:\nYou are an instruction classification assistant. Please give a secondary classification label to\nthe input instruction text in the format: Primary Category-Secondary Category, output\ndirectly without explanation.\nDue to resource limitations, we use the Qwen2.5-7B-Instruct model to generate secondary\nclassification label."}, {"title": "Diversity Control", "content": "Each sample data has a secondary classification label after instruction classification. To ensure\nthe reliability of the labels, we removed the low-frequency classification labels (the label\nfrequency less than 10, adjustable). We assessed the accuracy of the labels through sampling\nand removed those samples with relatively low accuracy labels, retaining the 5k+ secondary\nclassification labels, and then ensure that the distribution of classification labels is as diverse\nas possible by rejecting sampling methods, the number of each classification instruction is\nrelatively balanced, ultimately generating 50w+ diverse samples."}, {"title": "Regenerate Reply", "content": "Each instruction of the original open source data has a reply, but to obtain higher quality data,\nwe choose to regenerate the replies, including using a large language model to generate replies,\nand a small language model to generate replies. Because our optimization baseline for the\nsmall language model is Qwen2-0.5B-Instruct, the selection of the large language model is\ntheoretically the larger the better, but it is limited by experimental costs, we select Qwen2.5-\n7B-Instruct model to generate replies, and directly select Qwen1.5-0.5B-Chat as the small"}, {"title": "Quality Evaluation", "content": "The quality of post-training alignment data is very important, so the key to quality evaluation\nis to select samples that are both safe and the highest reply quality from the three replies. To\nthis end, we have adopted a variety of methods: such as the perplexity of the reply content, the\nlarge model's safety judgment on the instruction and reply, and the large model's\ncomprehensive scoring on the instruction and reply.\nThe first method is to use a large language model to calculate the perplexity of each reply\ncontent. Perplexity is an indicator to measure the performance of the language model,\nreflecting the model's predictive ability for text. Specifically, perplexity is calculated by taking\nthe average of the negative logarithm of the probability of each word in the model. Assuming\na sentence s = (W1,W2,...,Wn) with a length n, its perplexity is defined as:\n$PP(W) = P(W_{1}W_{2} ...W_{n}) = \\sqrt[n]{\\frac{1}{P(W_{1}W_{2}... W_{n})}}$\nTaking the logarithm first and then the exponent, the transformation becomes the following\nformula:\n$Perplexity(s) = e^{e^{-1 \\sum_{i=1}^{n}In(p(Wi | W_{1} W_{2} ... W_{i-1}))}}$\nIn the formula, n is the length of the text sequence, w(i) is the i-th word in the sequence, and\np(WiW1: i-1) is the probability of the i-th word given the previous words. From the formula, it\ncan be seen that the larger the sentence probability, the better the language model, and the\nsmaller the perplexity. The formula above shows that PP(W) essentially becomes a cross-\nentropy function with an exponential base, so when we want to calculate perplexity, we can\ndirectly calculate the cross-entropy, where the size of the base is not important, in this paper,\nthe base is set to 2. For reply content, the smaller the perplexity, the better the reply is often\nindicated, so perplexity can be used to distinguish the quality of replies.\nThe second method is to use the large model to make a safety judgment on each instruction\nand reply, and assign a safety score to each instruction pair. We adopt a 3-point system,\nincluding {1, 0.5, 0}, where 1 point represents that both the instruction and the reply are very\nsafe, 0.5 represents that one of the instruction and reply is unsafe, and 0 represents that both\nthe instruction and the reply are unsafe.\nThe third method is to use the large model to make a comprehensive quality judgment on the\nreplies of each instruction, from the dimensions of whether the intention of the instruction is\nfollowed, the accuracy, completeness, and relevance of the reply, etc., assigning a quality score\nto each reply. We adopt a 3-point system, including {1, 0.5, 0}, where 1 point represents a very\ngood reply, 0.5 represents an average reply, and 0 represents a poor reply.\nIn the end, we use the perplexity value of the calculated reply, the safety score of the large\nmodel, and the quality score of the large model as the features of each sample, which facilitates\nfurther sample selection based on these features."}, {"title": "Post-Training Alignment Data Construction", "content": "Post-training alignment data generation includes SFT (Supervised Fine-Tuning) alignment\ndata generation and KTO (Kahneman-Tversky Optimization) preference alignment data\ngeneration. The core method is to select based on the instruction-reply perplexity values,\nsafety scores, quality scores, and other features built from the quality assessment of the\nprevious stage for each sample. We select the best reply for each sample instruction as the SFT\npost-training dataset, and on the basis of the best reply quality of the SFT data, we also retain\nthe worst replies to form the KTO preference alignment data. Through comprehensive\nscreening based on deduplication, perplexity, safety scores, quality scores, etc., we ultimately\nconstructed approximately 100,000 SFT datasets and 150,000 KTO datasets, the ratio of\npositive feedback to negative feedback samples in the KTO dataset is 2:1."}, {"title": "Experiment and Evaluation", "content": "To verify the effectiveness of the methods in this paper, we used Llama-Factory [21] as the\ntraining and reasoning framework, and Qwen2-0.5B-Instruct as the baseline model for small\nlanguage models, and conducted post-training experiments and model weight fusion\nexperiment using the SFT and KTO datasets we constructed.\nIn terms of training parameters, we focus on adjusting the learning rate and batch size, taking\ninto account the balance between model convergence speed and computational resource\nconsumption. We follow the method of Kaplan et al.[4] to determine the batch size based on\nexpected loss, and select the optimal learning rate based on multiple experimental\ncomparisons.\nIn addition, to verify the effectiveness of the methods we proposed, and strive to evaluate the\nmodel performance in a comprehensive and objective manner. We have selected a widely\nrecognized benchmark test set to ensure the authority and universality of the evaluation. We\nadopted two industry standard evaluation tools, Qwen-Eval [22] and Human-Eval [23], to\nconduct a detailed evaluation of the model's performance in multiple dimensions.\nThe test datasets we have selected include gsm8k, mmlu, cmmlu, ceval, and HumanEval. These\ndatasets cover multiple aspects from basic language understanding to complex reasoning and\ncommon sense judgments, enabling a comprehensive examination of the model's performance\non different tasks. Through dataset evaluation, we can conduct in-depth analysis and\nevaluation of the model's generalization ability, reasoning ability, and the degree of mastery of\nhuman ommon sense, and verify the effectiveness of our methods. We can also gain a deep\nunderstanding of the model's advantages and limitations in handling various language tasks."}, {"title": "SFT Continuous Post-Training Experiment", "content": "The baseline model for continuous SFT post-training is Qwen2-0.5B-Instruct. We used the\nconstructed enhanced SFT dataset for continuous SFT alignment learning. The experimental\nprocess is shown in Figure 4-1:"}, {"title": "KTO Continuous Post-Training Experiment", "content": "To make the experiment comparable, the baseline model selected for continuous KTO post-\ntraining is also Qwen2-0.5B-Instruct. We used the constructed enhanced KTO preference\ndataset for continuous KTO alignment learning. The experimental process is shown in Figure\n4-3."}, {"title": "SFT-KTO Two-Stage Experiment", "content": "To verify whether continuous SFT alignment training followed by further KTO post-training\ncan continue to improve model performance, we designed a two-stage SFT-KTO experiment,\nand the experimental process is shown in Figure 4-5."}, {"title": "Weight Fusion Experiment", "content": "Different stages of post-training may have different focuses on improving model performance.\nTherefore, to make the overall performance improvement of the model balanced, we designed\nweight fusion experiments to verify the impact of weight fusion on model performance. The\nweight fusion adopts a strategy of averaging weighted weights. We validated this by setting\ndifferent weighted coefficients for the three post-trained models of QW2-0.5B-Instruct-S1,\nQW2-0.5B-Instruct-S2, and QW2-0.5B-Instruct-S3 and then fusing their weights."}, {"title": "The Impact of Learning Rate Hyperparameter", "content": "Through SFT and KTO post-training experiments, we found that using different learning rates\nhas varying impacts on the indicators of the trained model. We learned that the learning rate\nof Qwen2-0.5B-Instruct during the pre-training phase is 1e-6. When we used the same\nlearning rate on 100000 SFT samples, we found that the loss converged quickly, but the overall\nindicators were poor. It was not until the learning rate reduced by two orders of magnitude to\n5e-8 that the overall indicators began to steadily improve, especially the gsm8k indicator has\nsignificantly improved above the baseline. Therefore, it is very important to select the\nappropriate learning rate for continuous post-training. It directly affects the performance of\nthe model."}, {"title": "Conclusion", "content": "In this paper, we propose a construction method of continuous post-training alignment data\nfor small language models, which is completely based on open source instruction alignment\ndata. By using the instruction alignment data constructed with our method, we conducted\nseveral groups of post-training optimization experiments, including SFT (Supervised Fine\nTuning) post-training experiment and KTO (Kahneman Tversky optimization) post-training\nexperiment, as well as SFT-KTO two-stage post-training experiment. At the same time, we also\nanalyzed the impact of weight fusion on model performance, and the impact of different\nlearning rates on model performance. Finally, through the analysis of the general benchmark\nevaluation indicators, we verified that our method can further improve the performance of the\nsmall language models."}]}