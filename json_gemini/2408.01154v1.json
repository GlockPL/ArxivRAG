{"title": "DERA: Dense Entity Retrieval for Entity Alignment in Knowledge Graphs", "authors": ["Zhichun Wang", "Xuan Chen"], "abstract": "Entity Alignment (EA) aims to match equiv-\nalent entities in different Knowledge Graphs\n(KGs), which is essential for knowledge fu-\nsion and integration. Recently, embedding-\nbased EA has attracted significant atten-\ntion and many approaches have been pro-\nposed. Early approaches primarily focus on\nlearning entity embeddings from the struc-\ntural features of KGs, defined by relation\ntriples. Later methods incorporated entities'\nnames and attributes as auxiliary informa-\ntion to enhance embeddings for EA. How-\never, these approaches often used different\ntechniques to encode structural and attribute\ninformation, limiting their interaction and\nmutual enhancement. In this work, we\npropose a dense entity retrieval framework\nfor EA, leveraging language models to uni-\nformly encode various features of entities\nand facilitate nearest entity search across\nKGs. Alignment candidates are first gen-\nerated through entity retrieval, which are\nsubsequently reranked to determine the final\nalignments. We conduct comprehensive ex-\nperiments on both cross-lingual and mono-\nlingual EA datasets, demonstrating that our\napproach achieves state-of-the-art perfor-\nmance compared to existing EA methods.", "sections": [{"title": "1 Introduction", "content": "Knowledge Graphs (KGs) represent structured in-\nformation of entities in various domains, which\nfacilitates machines to handle domain knowledge.\nMost published KGs, such as YAGO(Rebele et al.,\n2016), DBpedia(Bizer et al., 2009), and Wiki-\nData(Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014), are hetero-\ngeneous because they are either built from differ-\nent data sources or by different organizations us-\ning varying terminologies. To integrate knowledge\nin separate KGs, it is essential to perform Entity\nAlignment (EA), which aims to discover equiva-\nlent entities in different KGs.\nThe problem of EA has been studied for years\nand many approaches have been proposed. Early\nEA approaches rely on manually designed fea-\ntures to compute similarities of entities(Noy et al.,\n2017). Recently, embedding-based EA has at-\ntracted much attention, many approaches have\nbeen proposed and achieved promising perfor-\nmance. These approaches first embed enti-\nties in low-dimensional vector spaces, and then\ndiscover entity alignments based on distances\nof entity embeddings. There are mainly two\nparadigms of KG embedding, translation-baed\nmethods and Graph Neural Network(GNN)-based\nmethods. Translation-based methods learn entity\nembeddings using TransE or its extended models,\nincluding MTransE(Chen et al., 2017), JAPE(Sun\net al., 2017), and BootEA(Sun et al., 2018), etc.\nGNN-based methods learn neighborhood-aware\nrepresentations of entities by aggregating features\nof their neighbors, such approaches include GCN-\nAlign(Wang et al., 2018), MuGNN(Cao et al.,\n2019), and AliNet(Sun et al., 2020a), etc.\nEarly embedding-based methods focus on struc-\nture embedding of KGs, to further improve the\nEA results, some latter approaches explore en-\ntities' names and attributes as side information\nto enhance the entity embeddings. Names and\nattribute values are encoded by using character\nor word embedding techniques, for example in\nMultiKE(Zhang et al., 2019), AttrGNN(Liu et al.,\n2020) and CEA(Zeng et al., 2020), etc. Most re-\ncently, pre-trained language models (PLMs) have\nalso been used to encode the names and attribute\nvalues, such as in BERT-INT(Tang et al., 2020),\nSDEA(Zhong et al., 2022).\nAlthough continuous progress has been\nachieved in recently years, we find that there lacks\na unified and effective way to encode all kinds\nof information of entities for EA. Most of the\nexisting approaches encode structure information\n(relations) and attribute information (names,"}, {"title": "attributes, and descriptions, etc.) separately. Two\nkinds of information are encoded in different\nspaces, which are integrated before matching\nentities. Such EA paradigm faces both structure\nheterogeneity and attribute heterogeneity prob-\nlems, which hinders their mutual enhancement.\nRecently, the emergence of pre-trained lan-\nguage models has significantly enhanced the qual-\nity of text embeddings, proving highly effective\nin information retrieval, question answering and\nretrieval-augmented language modeling. Inspired\nby the recent development of embedding-based\nIR (dense retrieval), where relevant answers to a\nquery are retrieved based on their embedding sim-\nilarities, we formalize entity alignment in KGs as\nan entity retrieval problem. To find equivalent en-\ntities of two KGs, entities in one KG are used as\nqueries to retrieval the most similar entities in the\nother KG. In this entity retrieval framework, dif-\nferent kinds of entities' information can be uni-\nformly represented in textual forms, and we can\nleverage the advance of language models in em-\nbedding and searching similar entities.\nMore specifically, we make the following con-\ntributions in this work:\n\u2022 We formalize the EA problem as an en-\ntity retrieval task, and propose a language\nmodel based framework for this task. Within\nthis framework, entities' information are uni-\nformly transformed into textual descriptions,\nwhich are then encoded by language model\nbased embedding model for nearest entity\nsearch between KGs.\n\u2022 We propose an entity verbalization model\nto generate homogenous textual descriptions\nof entities from their heterogeneous triples.\nWe build a synthetic triple-to-text dataset by\nprompting GPT, which is used for effective\ntraining the verbalization model.\n\u2022 We design embedding models for entity re-\ntrieval and alignment reranking. The embed-\nding model for entity retrieval encodes enti-\nties independently, which can efficiently find\nalignment candidates; the embedding model\nfor alignment reranking encodes features of\nentity pairs, which captures the interactions\nof entities and guarantees the precision of\nalignments.\n\u2022 We conduct comprehensive experiments on\nfive datasets, and compare our approach with\nthe existing EA approaches. The results show\nthat our approach achieves state-of-the-art re-\nsults.\nThe rest of this paper is organized as follows:\nSection 2 covers the preliminaries of our work,\nSection 3 details our proposed approach, Section\n4 presents the experiments, Section 5 discusses re-\nlated work, and Section 6 provides the conclusion.", "content": null}, {"title": "2 Preliminaries", "content": "In this section, we introduce the problem of entity\nalignment in knowledge graphs, and formalize the\ntask of dense entity retrieval for EA."}, {"content": null}, {"title": "2.1 KG and Entity Alignment", "content": "Knowledge Graph (KG). KGs represent struc-\ntural information about entities as triples having\nthe form of (s, p, o). A triple can be relational or\nattributional, a relational triple describes certain\nkind of relation between entities, and an attribu-\ntional triple describes an attribute of an entity. In\nthis work, we consider both relational and attribu-\ntional triples in KGs. Formally, we represent a KG\nas G = (E, R, A, L, T), where E, R, A and L are\nsets of entities, relations, attributes, and literals;\nTC (E\u00d7R\u00d7E) U (E \u00d7 A \u00d7 L) is the sets of\ntriples.\nEntity Alignment (EA). Given two KGs Gs and\nGt, and a set of pre-aligned entity pairs S =\n{(u,v)\u013cu \u2208 Gs,v \u2208 Gt,u = v} (= denotes\nequivalence), the task of entity alignment is to find\nnew equivalent entity pairs between Gs and Gt."}, {"content": null}, {"title": "2.2 Dense Entity Retrieval", "content": "In this work, we formalize EA as an entity re-\ntrieval task. Given a source KG Gs and a target\nKG Gt, entity retrieval aims to, for each entity\ns \u2208 Gs, return a ranked list of k most similar en-\ntities [t1, t2, ..., tk] in Gt. The top-ranked entity t1\nis considered as be equivalent to the source entity\ns, i.e. s = t\u2081.\nTo achieve accurate entity retrieval, LM-based\nembedding models are leveraged in our approach\nto encode entities into dense vectors, and the sim-\nilarities of entities are computed using their vec-\ntors:\n$$f(s,t) = sim(\\phi(s),\\psi(t))$$\nwhere $\\phi(\u00b7) \u2208 R^d$ and $\\psi(\u00b7) \u2208 R^d$ are encoders\nmapping the source and target entities into d-\ndimensional vector space, respectively. In this"}, {"title": "3 Method", "content": "In this section, we present the proposed EA frame-\nwork DERA (Dense Entity Retrieval for entity\nAlignment), which is shown in Figure 1. Given\ntwo KGs to be aligned, DERA works in three main\nstages. (1) Entity Verbalization (EV): this stage\nconverts heterogeneous triples of entities into ho-\nmogeneous natural language descriptions. Re-\nlations and attributes expressed in different lan-\nguages will also be converted into one language.\n(2) Entity Retrieval (ER): entities' textual de-\nscriptions are encoded in the same vector space.\nEntities are indexed using their embeddings, simi-\nlar entities are retrieved based on embedding sim-\nilarity to obtain alignment candidates. (3) Align-\nment Reranking (AR): candidate alignments are\nfurther reranked by an reranking model to produce\nthe final results."}, {"title": "3.1 Entity Verbalization", "content": "The purpose of entity verbalization is to convert\nrelational and attribute triples of entities into tex-\ntual descriptions in one language, which can be\nwell encoded by a language model based embed-\nding model. Given an entity e in a KG, let Ne =\n{(ri, ei)}=1 be the set of neighbors and associ-\nated relations of entity e, Le = {(aj, vj)}=1 be\nthe set of attributes and values of entity e; here ei\nis an entity and ri is the relation connecting two\nentities, vj is the value of aj of e. Entity ver-"}, {"title": "balization can be formally defined as a mapping\ng (Ne, Le) \u2192 se, where se is the textual sequence\nof e.\nTo get high-qualified verbalization results, we\ntrain a generative language model which takes\ntriples as input context and generate textual de-\nscriptions as outputs. More specifically, we take\nopen Large Language Models (LLMs) as base\nmodels, and build triple-to-text dataset to fine-tune\nbase models.\nDataset Building. The triple-to-text dataset is\nbuilt by instructing the GPT4 using a designed\nprompt template, which is shown in Figure 2.\nThere are four parts in the prompt: (1) The first\npart is an instruction prefix to describe the task\nof generating triples of entities of specified type;\nwe predefined 25 common entity types, includ-\ning person, organization, movie, disease, etc. (2)\nThe second part tells the model to generate a short\nand precise description of the generated triples;\n(3) The third part specifies the formates of gener-\nated triples and textual descriptions; (4) The fourth\nparts gives an example to the model.\nUsing the above prompt, we build a dataset con-\ntain triples and textual descriptions of 18,572 enti-\nties.\nModel Training. Using the generated dataset, we\nfine-tune the LLMs with the next word predic-\ntion task, which is a universal approach to training\nLLMs. For an entity, given the sequence of triples\n(e, r1, C1, ..., rk, Ek, A1, U1, ..., am, Um) and\nits target textual description y = (Y1, Y2, \u2026\u2026\u2026, Yn),\nthe training objective of EV model can be formu-"}, {"title": "lated as:", "content": "$$L_{EV} = - \\frac{1}{n} \\sum_{t=1}^n log P(y_t | x, y_{<t}; \\theta)$$\nwhere n is the length of y, yt(t = 1, 2, ..., n) de-\nnotes the textual tokens of the sequence y, \u03b8 rep-\nresents the model parameters.\nIn this work, we choose LLMs of 7B size as\nthe base models of EV. More specifically, Mistral-\n7B-Instruct-v0.2(Jiang et al., 2023) and Qwen1.5-\n7B-Chat(Bai et al., 2023) are used because they\nhave excellent performances in small-size LLMs.\nQWen is used for EA tasks involving Chinese lan-\nguage, because it has great ability of handling Chi-\nnese texts. In the other EA tasks, Mistral model is\nused in EV stage. EV models are trained inde-\npendent of specific EA tasks, once two EV models\nhave been trained, their parameters are frozen and\nwill not be changed in the following two stages."}, {"title": "3.2 Entity Retrieval", "content": "In this stage, entity embedding model is trained\nto encode entity descriptions into vector space,\nwhere entities are close to their equivalent counter-\nparts. Using the entity embedding results, align-\nment candidates are produced based on embed-\nding similarities of entities. In this work, we use a\ntext embedding model as the basis, and fine-tune\nit with pre-aligned entities to further improve the\nembedding quality. More specifically, BGE(Chen"}, {"title": "et al., 2024) embedding model is used here be-\ncause it achieves state-of-the-art performances on\nmultilingual and cross-lingual retrieval tasks.\nModel Training. As defined in Section 2.2, the\nsimilarity of two entities s and t is computed as\nthe doc product of their embeddings:", "content": "$$f(u, v) = \\phi(u) \\cdot \\phi(v).$$\nHere $\\phi(\u00b7) \u2208 R^d$ denotes the entity embedding\nmodel which maps the entity into d-dimensional\nvector space. Given a set of seed alignments\nS = {(u,v)\u013cu \u2208 Gs,v \u2208 Gt, u = v}, the en-\ntity embedding model in our approach is trained\nby minimizing the following contrastive loss:\n$$L_{ER} = \\sum_{(u,v) \u2208 S} log \\frac{e^{f(u,v)}}{e^{f(u,v)} + \\sum_{v' \u2208 N_u} e^{f(u,v')}}$$\nwhere Nu is a set of negative (inequivalent) enti-\nties for u.\nCandidate Selection. After the entity embedding\nmodel is trained, all the entities in two KGs can be\nencoded as vectors in the same space. Then candi-\ndate alignments are obtained by using each source\nentity to retrieval nearest target entities based on\ntheir embeddings. More specifically, for each\nsource entity u \u2208 Gs, a set of top-k nearest target\nentities in Gt are retrieved, which are candidate\nalignments u, denoted as Vu."}, {"title": "3.3 Alignment Reranking", "content": "In the entity retrieval stage, entities' descriptions\nare encoded independently from each other. To\nfurther improve the EA results, we design an\nalignment reranking model which capture the in-\nteractions of entities' features. Here a reranker\nbuilt upon BERT is trained, which takes features\nof two entities as inputs, and predict the fine-\ngrained similarities of entity pairs. Entity pairs are\nrestricted to the candidates generated by the entity\nretrieval stage, which helps our approach to con-\ntrol the computation costs in alignment reranking.\nLet C = {(Uj, Vuj)}}=1 be the alignment can-\ndidates, where uj is a source entity and Vu; is the\nset of its candidate equivalent entities. We con-\nstruct a dataset for training our alignment rerank-\ning model, let it be R = {(uj, vj, Nj)}j=1, where\n(uj, vj) \u2208 S is the pre-aligned entity pair and\nNj = Vuj/{vj} is the set of candidate entities that\nare not equivalent to uj. The reranking model is\ntrained by minimizing the following loss:"}, {"title": "$\n$$L_{AR} = \\sum_{(u_j, v_j, N_j) \u2208 R} log \\frac{e^{\\delta(v_j, v_j)}}{\\sum_{v_l \u2208 V_{u_j}} e^{\\delta(u_j, z)} + \\sum_{v_k \u2208 N_j} e^{\\delta(u_j, k)}}$$\nHere \u03b4 (u, v) is the similarity score computed by\nthe reranking model based on the inputs of two\nentities:\n$$\\delta (u, v) = MLP (BERT [CLS] (d_u, d_v))$$\nwhere du and d\u2082 represent the textual descriptions\nof u and v, respectively.", "content": null}, {"title": "4 Experiments", "content": "4.1 Datasets\nDatasets. To evaluate the performance of our ap-\nproach, we conduct experiments on both cross-\nlingual and monolingual datasets, including:\n\u2022 DBP15K(Sun et al., 2017) contains three\ncross-lingual EA datasets build from DB-\npedia, including Chinese-English (ZH-EN),\nJapanese-English (JA-EN), and French-\nEnglish (FR-EN).\n\u2022 D-W-15K(Sun et al., 2020b) is a monolin-\ngual EA dataset built from DBpedia and\nWikipedia by using an iterative degree-based\nsampling method. Compared with DBP15K,\nD-W-15K contains KGs that are more like\nreal-world ones.\n\u2022 MED-BBK-9K(Zhang et al., 2020) is a\ndataset built from two medical knowledge\ngraphs, containing triples on diseases, symp-\ntoms, drugs, and diagnosis methods. It poses\na more complex and realistic scenario for EA\ncompared to traditional datasets like DBpe-\ndia."}, {"title": "4.2 Training Details", "content": "We train the Entity Verbalization (EV), Entity Re-\ntrieval (ER), and Alignment Reranking (AR) mod-\nels sequentially.\nEV Model. In the training of EV model, we em-\nploy Deepspeed\u00b9 with a context window length\nof 2048, the learning rate is set to 9.65e"}, {"title": "6, and the batch size is 24 per GPU. For the\nbase language models, we use Qwen1.5-7B-\nChat(Bai et al., 2023) for DBP15KZH-EN and\nMED-BBK-9K datasets, and use Mistral-7B-\nInstruct-v0.2(Jiang et al., 2023) for DBP15KJA-EN,\nDBP15KFR-EN, and D-W-15K datasets. Gradient\naccumulation is set to 1. To optimize memory us-\nage and computation speed, we utilize Zero-Stage-\n3(Rajbhandari et al., 2020), gradient checkpoint-\ning(Chen et al., 2016), and flash attention 2(Dao,\n2023). The model is trained on 8 NVIDIA A800\nGPU for 3 epochs using the AdamW optimizer.\nER Model. In the training of ER model, for each\npositive entity, 64 negative entities are randomly\nsampled from the top-200 nearest ones. The learn-\ning rate is set to le - 5, and the batch size to 16.\nWe utilize distributed negative sample sharing and\ngradient checkpointing(Chen et al., 2016), evalu-\nate the model every 20 steps and saving the best\nmodel based on the MRR metric on the validation\nset. Training is performed on 2 NVIDIA A800\nGPUs for 5 epochs.\nAR Model. In the training of AR model, for each\npositive entity, 110 negative entities are randomly\nsampled from the top-200 nearest ones. The max-\nimum text length is set to 512; the learning rate to\nle - 5, and the batch size to 12 per GPU. Gra-\ndient accumulation steps are set to 8. We en-\nable gradient checkpointing, evaluate the model\nevery 10 steps, and save the best model based on\nthe Hits@1 metric on the validation set. Train-\ning is carried out on 2 NVIDIA A800 GPUs for 5\nepochs."}, {"title": "4.3 Results on DBP15K", "content": "We compare our approach with four groups of\nbaselines on DBP15K datasets, which are catego-\nrized by the used side information: (1) approaches\nusing attributes as side information, including\nJAPE(Sun et al., 2017), GCN-Align(Wang et al.,\n2018), JarKA(Chen et al., 2020); (2) approaches\nusing entity names as side information, including\nGMNN(Xu et al., 2019), SelfKG(Liu et al., 2022)\nand TEA-NSP, TEA-MLM(Zhao et al., 2023); (3)\napproaches using attributes and names as side in-\nformation, including HMAN(Yang et al., 2019),\nAttrGNN(Liu et al., 2020), BERT-INT(Tang et al.,\n2020), ICLEA(Zeng et al., 2022) and TEA-NSP,\nTEA-MLM(Zhao et al., 2023); (4) approaches\nusing translated entity names as side informa-\ntion, including HGCN-JE(Wu et al., 2019b),"}, {"title": "RDGCN(Wu et al., 2019a), NMN(Wu et al.,\n2020), DATTI(Mao et al., 2022a), SEU(Mao et al.,\n2021), EASY(Ge et al., 2021), CPL(Ding et al.,\n2022), UED(Luo and Yu, 2022) and LigthEA(Mao\net al., 2022b). Our approach is compared to base-\nlines in each group using the same inputs as them.\nTable 2 outlines the results of all the approaches\non DBP15K datasets. The best results in each\ngroup are highlighted in boldface, the second best\nresults are highlighted with underlines.\nAttributes as Side Information. Approaches in\nthis group align entities based on relations and at-\ntributes in KGs. Compared with approaches in\nthis group, our approach obtains significantly bet-\nter results, with average improvements of 25.3%\nof Hits@1 and 20.7% of MRR over the second\nbest approach on three datasets.\nNames as Side Information. Approaches in this\ngroup use entity names and relations to discover\nequivalent entities. Our approach gets the best\nresults of Hits and MRR on ZH-EN and FR-EN\ndatasets, it obtains 1.5% and 1.4% improvements\nof Hits@1 over the second best approach TEA-\nMLM. While on the JA-EN dataset, TEA-NSP\ngets slightly better results than ours.\nNames and Attributes as Side Information.\nWhen using both names and attributes, our ap-\nproach still obtain top-ranked results. Except for\nthe Hits@10 on JA-EN and Hits@1 on FR-EN\ndatasets, our approach gets the best results among\nall the compared approaches in this group.\nTranslated Names as Side Information. Ap-\nproaches in this group use machine translation\ntool to convert non-English names into English\nones, and takes translated names as side informa-\ntion. Some of the approaches (annotated with \u2020)"}, {"title": "in this group also employ optimal transport strate-\ngies to draw final alignments from entity similari-\nties, which can effectively promote the results. To\nbe fairly compared with these approach, we also\nreport the results of our approach with the optimal\ntransport strategy. According to the results, our\napproach gets the best results among all the ap-\nproaches in this group. Among approaches with-\nout optimal transport strategies, our approach also\ngets the best results."}, {"title": "4.4 Results of Hard Setting on DBP15K", "content": "In the work of AttrGNN(Liu et al., 2020), a hard\nsetting of evaluations on DBP15K was proposed.\nThe purpose of this hard setting is to build more\ndifficult testing set on DBP15K. Specifically, sim-\nilarities of equivalent entities in the datasets are\nfirst measured using embeddings of their names,\n60% entity pairs with the lowest similarities are\nselected as the testing set, and the remaining en-\ntity pairs are randomly split into training set (30%)\nand validation set (10%)."}, {"title": "4.5 Results on DW-15K and MED-BBK-9K", "content": "DW15K and MED-BBK-9K are two challenging\ndatasets of entity alignment. DW-15K is built\nfrom Wikipedia, where entity names are replaced\nwith ids; there are also significant missing and\ncorrupted attribute values. The dataset of MED-\nBBK-9K is built from an authoritative medical\nKG and a KG built from a Chinese online ency-\nclopedia (Baidu Baike); many entities in MED-\nBBK-9K lack names and attributes, which makes\nthe EA task more difficult. We compared our\napproach with seven approaches, three of them\nare probabilistic ones including LogMap(Jim\u00e9nez-\nRuiz and Cuenca Grau, 2011), PARIS(Suchanek\net al., 2011), and PRASE(Qi et al., 2021); four\nof them are embedding-based ones including Mul-\ntiKE(Zhang et al., 2019), BootEA(Sun et al.,\n2018), RSNs(Guo et al., 2019) and FGWEA(Tang\net al., 2023). Following the same evaluation set-\ntings of SOTA approaches on these two datasets,\nwe report the Precision, Recall and F1 of all the\ncompared approaches."}, {"title": "4.6 Ablation Study", "content": "To analyze the effectiveness and contribution of\neach component in the proposed approach, we\nconduct ablation studies on DBP15K datasets. We\nran two groups of experiments, one group uses at-\ntributes as side information, and the other group\nuses both names and attributes as side information.\nIn each group, we ran three variations of DERA: 1)"}, {"title": "$\n5 Related Work", "content": "5.1 Embedding-based EA\nEmbedding-based KG alignment approaches em-\nploy TransE and GNN to learn entities'em-\nbeddings, and then find equivalent entities in\nthe vector spaces. Early approaches mainly\nrely on the structure information in KGs to find\nalignments, including TransE-based approaches\nMTransE (Chen et al., 2017), IPTransE (Zhu et al.,\n2017), BootEA (Sun et al., 2018), etc, and GNN-\nbased approaches MuGNN (Cao et al., 2019),\nNAEA (Zhu et al., 2019), RDGCN (Wu et al.,\n2019a) and AliNet (Sun et al., 2020a), etc. To\nget improved results, some approaches utilize en-\ntity attributes or names in KGs. JAPE (Sun\net al., 2017) performs attribute embedding by\nSkip-Gram model which captures the correlations\nof attributes in KGs. GCN-Align (Wang et al.,\n2018) encodes attribute information of entities\ninto their embeddings by using GCNs. Mul-\ntiKE (Zhang et al., 2019) uses a framework uni-\nfying the views of entity names, relations and at-\ntributes to learn embeddings for aligning entities.\nCEA (Zeng et al., 2020) combines structural, se-\nmantic and string features of entities, which are\nintegrated with dynamically assigned weights."}, {"title": "5.2 Language Model-based EA", "content": "As Pre-trained Language Models(PLMs) being\nsuccessfully used in various tasks, some ap-\nproaches utilize PLMs to model the semantic in-\nformation of entities in the task of KG align-\nment. AttrGNN(Liu et al., 2020) uses BERT\nto encode attribute features of entities. It en-\ncode each attribute and value separately, and then\nuses a graph attention network to compute the\nweighted average of attributes and values. BERT-\nINT(Tang et al., 2020) embeds names, descrip-\ntions, attributes and values of entities using a\nLM; pair-wise neighbor-view and attribute-view\ninteractions are performed to get the matching"}, {"title": "score of entities. The interactions are time-consuming, thus BERT-INT cannot scale to large\nKGs. SDEA(Zhong et al., 2022) find-tunes BERT\nto encode attribute values of an entity into attribute\nembedding; attribute embeddings of neighbors are\nfed to BiGRU to get relation embedding of an en-\ntity. TEA(Zhao et al., 2023) sorts triples in alpha-betical order by relations and attributes to form\nsequences, and uses a textual entailment frame-\nwork for entity alignment. TEA takes entity-pair\nsequence as the input of PLM, and let the PLM\nto predict the probability of entailment. It takes\npairwise input, cannot scale to large KGs. Au-toAlign(Zhang et al., 2023) gets attribute char-acter embeddings and predicate-proximity-graph\nembeddings by using large language models. At-trGNN, BERT-INT and SDEA use BERT to en-code attribute information of entities, and then\nemploy GNNs to incorporate relation information\ninto entities' embeddings. Being different from\nthese approaches, our approach directly use lan-guage model to encode both the attributes and re-\nlations of entities. TEA uses similar way to encode\nattribute and relation information, but it takes en-tity pair as input, which cannot scale to large-scale\nKG alignment tasks.\nAs the advent of Large Language Models\n(LLMs), there are several approaches exploring\nLLMs for EA. LLMEA(Yang et al., 2024) fuses\nthe knowledge from KGs and LLMs to predict en-tity alignments. It first uses RAGAT to learn en-tity embeddings which are used to draws align-ment candidates; it then uses candidate alignments\nas options to generates multi-choice questions,\nwhich are passed to LLMs to predict the answer.\nChatEA(Jiang et al., 2024a) first uses Simple-HHEA(Jiang et al., 2024b) to obtain candidate\nalignments, and then leverages LLMs' reasoning\nabilities to predict the final results. LLMEA and\nChatEA all explore the reasoning abilities of LLM\nto predict entity alignments. Because the number\nof potential alignments are usually huge, they use\nexiting EA methods to generate alignment candi-dates, from which LLMs are used to select the fi-nal results. According to the results, the improve-ments contributed by LLMs are restricted."}, {"title": "6 Conclusion", "content": "In this paper, we propose a dense entity retrieval\napproach, DERA, for entity alignment in knowl-\nedge graphs. DERA first converts entity triples\ninto unified textual descriptions using an entity\nverbalization model, and then trains a language\nmodel-based embedding model to encode the en-\ntities. Candidate alignments are identified based\non their similarities in the embedding space and\nare further reranked by an alignment reranking\nmodel. Experiments demonstrate that DERA\nachieves state-of-the-art results on entity align-\nment tasks of varying difficulty levels."}, {"title": "Limitations", "content": "The primary limitation of DERA is its pipelined\nframework, where models in its three stages are\ntrained sequentially. Consequently, the compo-\nnent models in DERA are not optimized jointly\nduring training. Exploring efficient methods for\nthe joint learning of these models would be a valu-\nable direction for future work, potentially enhanc-\ning the results further. Additionally, DERA con-\nsumes more GPU power than traditional models,\nwhich is another limitation."}]}