{"title": "DERA: Dense Entity Retrieval for Entity Alignment in Knowledge Graphs", "authors": ["Zhichun Wang", "Xuan Chen"], "abstract": "Entity Alignment (EA) aims to match equivalent entities in different Knowledge Graphs (KGs), which is essential for knowledge fusion and integration. Recently, embedding-based EA has attracted significant attention and many approaches have been proposed. Early approaches primarily focus on learning entity embeddings from the structural features of KGs, defined by relation triples. Later methods incorporated entities' names and attributes as auxiliary information to enhance embeddings for EA. However, these approaches often used different techniques to encode structural and attribute information, limiting their interaction and mutual enhancement. In this work, we propose a dense entity retrieval framework for EA, leveraging language models to uniformly encode various features of entities and facilitate nearest entity search across KGs. Alignment candidates are first generated through entity retrieval, which are subsequently reranked to determine the final alignments. We conduct comprehensive experiments on both cross-lingual and monolingual EA datasets, demonstrating that our approach achieves state-of-the-art performance compared to existing EA methods.", "sections": [{"title": "1 Introduction", "content": "Knowledge Graphs (KGs) represent structured information of entities in various domains, which facilitates machines to handle domain knowledge. Most published KGs, such as YAGO(Rebele et al., 2016), DBpedia(Bizer et al., 2009), and Wikidata(Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014), are heterogeneous because they are either built from different data sources or by different organizations using varying terminologies. To integrate knowledge in separate KGs, it is essential to perform Entity Alignment (EA), which aims to discover equivalent entities in different KGs.\nThe problem of EA has been studied for years and many approaches have been proposed. Early EA approaches rely on manually designed features to compute similarities of entities(Noy et al., 2017). Recently, embedding-based EA has attracted much attention, many approaches have been proposed and achieved promising performance. These approaches first embed entities in low-dimensional vector spaces, and then discover entity alignments based on distances of entity embeddings. There are mainly two paradigms of KG embedding, translation-baed methods and Graph Neural Network(GNN)-based methods. Translation-based methods learn entity embeddings using TransE or its extended models, including MTransE(Chen et al., 2017), JAPE(Sun et al., 2017), and BootEA(Sun et al., 2018), etc. GNN-based methods learn neighborhood-aware representations of entities by aggregating features of their neighbors, such approaches include GCN-Align(Wang et al., 2018), MuGNN(Cao et al., 2019), and AliNet(Sun et al., 2020a), etc.\nEarly embedding-based methods focus on structure embedding of KGs, to further improve the EA results, some latter approaches explore entities' names and attributes as side information to enhance the entity embeddings. Names and attribute values are encoded by using character or word embedding techniques, for example in MultiKE(Zhang et al., 2019), AttrGNN(Liu et al., 2020) and CEA(Zeng et al., 2020), etc. Most recently, pre-trained language models (PLMs) have also been used to encode the names and attribute values, such as in BERT-INT(Tang et al., 2020), SDEA(Zhong et al., 2022).\nAlthough continuous progress has been achieved in recently years, we find that there lacks a unified and effective way to encode all kinds of information of entities for EA. Most of the existing approaches encode structure information (relations) and attribute information (names, attributes, and descriptions, etc.) separately. Two kinds of information are encoded in different spaces, which are integrated before matching entities. Such EA paradigm faces both structure heterogeneity and attribute heterogeneity problems, which hinders their mutual enhancement.\nRecently, the emergence of pre-trained language models has significantly enhanced the quality of text embeddings, proving highly effective in information retrieval, question answering and retrieval-augmented language modeling. Inspired by the recent development of embedding-based IR (dense retrieval), where relevant answers to a query are retrieved based on their embedding similarities, we formalize entity alignment in KGs as an entity retrieval problem. To find equivalent entities of two KGs, entities in one KG are used as queries to retrieval the most similar entities in the other KG. In this entity retrieval framework, different kinds of entities' information can be uniformly represented in textual forms, and we can leverage the advance of language models in embedding and searching similar entities.\nMore specifically, we make the following contributions in this work:\n\u2022 We formalize the EA problem as an entity retrieval task, and propose a language model based framework for this task. Within this framework, entities' information are uniformly transformed into textual descriptions, which are then encoded by language model based embedding model for nearest entity search between KGs.\n\u2022 We propose an entity verbalization model to generate homogenous textual descriptions of entities from their heterogeneous triples. We build a synthetic triple-to-text dataset by prompting GPT, which is used for effective training the verbalization model.\n\u2022 We design embedding models for entity retrieval and alignment reranking. The embedding model for entity retrieval encodes entities independently, which can efficiently find alignment candidates; the embedding model for alignment reranking encodes features of entity pairs, which captures the interactions of entities and guarantees the precision of alignments.\n\u2022 We conduct comprehensive experiments on five datasets, and compare our approach with the existing EA approaches. The results show that our approach achieves state-of-the-art results."}, {"title": "2 Preliminaries", "content": "In this section, we introduce the problem of entity alignment in knowledge graphs, and formalize the task of dense entity retrieval for EA."}, {"title": "2.1 KG and Entity Alignment", "content": "Knowledge Graph (KG). KGs represent structural information about entities as triples having the form of (s, p, o). A triple can be relational or attributional, a relational triple describes certain kind of relation between entities, and an attributional triple describes an attribute of an entity. In this work, we consider both relational and attributional triples in KGs. Formally, we represent a KG as G = (E, R, A, L, T), where E, R, A and L are sets of entities, relations, attributes, and literals; $T \\subset (E\\times R\\times E) \\cup (E \\times A \\times L)$ is the sets of triples.\nEntity Alignment (EA). Given two KGs $G_s$ and $G_t$, and a set of pre-aligned entity pairs $S = \\{(u,v)|u \\in G_s,v \\in G_t,u = v\\}$ (= denotes equivalence), the task of entity alignment is to find new equivalent entity pairs between $G_s$ and $G_t$."}, {"title": "2.2 Dense Entity Retrieval", "content": "In this work, we formalize EA as an entity retrieval task. Given a source KG $G_s$ and a target KG $G_t$, entity retrieval aims to, for each entity $s \\in G_s$, return a ranked list of k most similar entities $[t_1, t_2, ..., t_k]$ in $G_t$. The top-ranked entity $t_1$ is considered as be equivalent to the source entity $s$, i.e. $s = t_1$.\nTo achieve accurate entity retrieval, LM-based embedding models are leveraged in our approach to encode entities into dense vectors, and the similarities of entities are computed using their vectors:\n$f(s,t) = sim(\\phi(s),\\psi(t))$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(1)\nwhere $\\phi(\\cdot) \\in R^d$ and $\\psi(\\cdot) \\in R^d$ are encoders mapping the source and target entities into d-dimensional vector space, respectively. In this"}, {"title": "3 Method", "content": "In this section, we present the proposed EA framework DERA (Dense Entity Retrieval for entity Alignment), which is shown in Figure 1. Given two KGs to be aligned, DERA works in three main stages. (1) Entity Verbalization (EV): this stage converts heterogeneous triples of entities into homogeneous natural language descriptions. Relations and attributes expressed in different languages will also be converted into one language. (2) Entity Retrieval (ER): entities' textual descriptions are encoded in the same vector space. Entities are indexed using their embeddings, similar entities are retrieved based on embedding similarity to obtain alignment candidates. (3) Alignment Reranking (AR): candidate alignments are further reranked by an reranking model to produce the final results."}, {"title": "3.1 Entity Verbalization", "content": "The purpose of entity verbalization is to convert relational and attribute triples of entities into textual descriptions in one language, which can be well encoded by a language model based embedding model. Given an entity e in a KG, let $N_e = \\{(r_i, e_i)\\}_{i=1}^{n}$ be the set of neighbors and associated relations of entity e, $L_e = \\{(a_j, v_j)\\}_{j=1}^{m}$ be the set of attributes and values of entity e; here $e_i$ is an entity and $r_i$ is the relation connecting two entities, $v_j$ is the value of $a_j$ of e. Entity ver-"}, {"title": "Model Training", "content": "Using the generated dataset, we fine-tune the LLMs with the next word prediction task, which is a universal approach to training LLMs. For an entity, given the sequence of triples $(e, r_1, e_1, ..., r_k, e_k, a_1, v_1, ..., a_m, v_m)$ and its target textual description $y = (y_1, y_2, ..., y_n)$, the training objective of EV model can be formu-"}, {"title": "lated as:", "content": "$L_{EV} = -\\frac{1}{n} \\sum_{t=1}^{n} log P(y_t|x, y_{<t}; \\theta)$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(2)\nwhere n is the length of y, $y_t(t = 1, 2, ..., n)$ denotes the textual tokens of the sequence y, $\\theta$ represents the model parameters.\nIn this work, we choose LLMs of 7B size as the base models of EV. More specifically, Mistral-7B-Instruct-v0.2(Jiang et al., 2023) and Qwen1.5-7B-Chat(Bai et al., 2023) are used because they have excellent performances in small-size LLMs. QWen is used for EA tasks involving Chinese language, because it has great ability of handling Chinese texts. In the other EA tasks, Mistral model is used in EV stage. EV models are trained independent of specific EA tasks, once two EV models have been trained, their parameters are frozen and will not be changed in the following two stages."}, {"title": "3.2 Entity Retrieval", "content": "In this stage, entity embedding model is trained to encode entity descriptions into vector space, where entities are close to their equivalent counterparts. Using the entity embedding results, alignment candidates are produced based on embedding similarities of entities. In this work, we use a text embedding model as the basis, and fine-tune it with pre-aligned entities to further improve the embedding quality. More specifically, BGE(Chen"}, {"title": "Model Training.", "content": "As defined in Section 2.2, the similarity of two entities s and t is computed as the doc product of their embeddings:\n$f(u, v) = \\phi(u) \\cdot \\phi(v)$.\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(3)\nHere $\\phi(\\cdot) \\in R^d$ denotes the entity embedding model which maps the entity into d-dimensional vector space. Given a set of seed alignments $S = \\{(u,v)|u \\in G_s,v \\in G_t, u = v\\}$, the entity embedding model in our approach is trained by minimizing the following contrastive loss:\n$L_{ER} = \\sum_{(u,v) \\in S} log \\frac{e^{f(u,v)}}{e^{f(u,v)} + \\sum_{v'\\in N_u} e^{f(u,v')}}$\t\t\t\t\t\t\t\t\t\t\t(4)\nwhere $N_u$ is a set of negative (inequivalent) entities for u."}, {"title": "Candidate Selection.", "content": "After the entity embedding model is trained, all the entities in two KGs can be encoded as vectors in the same space. Then candidate alignments are obtained by using each source entity to retrieval nearest target entities based on their embeddings. More specifically, for each source entity $u \\in G_s$, a set of top-k nearest target entities in $G_t$ are retrieved, which are candidate alignments u, denoted as $V_u$."}, {"title": "3.3 Alignment Reranking", "content": "In the entity retrieval stage, entities' descriptions are encoded independently from each other. To further improve the EA results, we design an alignment reranking model which capture the interactions of entities' features. Here a reranker built upon BERT is trained, which takes features of two entities as inputs, and predict the fine-grained similarities of entity pairs. Entity pairs are restricted to the candidates generated by the entity retrieval stage, which helps our approach to control the computation costs in alignment reranking.\nLet $C = \\{(u_j, V_{u_j})\\}_{j=1}^l$ be the alignment candidates, where $u_j$ is a source entity and $V_{u_j}$ is the set of its candidate equivalent entities. We construct a dataset for training our alignment reranking model, let it be $R = \\{(u_j, v_j, N_j)\\}_{j=1}^l$, where $(u_j, v_j) \\in S$ is the pre-aligned entity pair and $N_j = V_{u_j}/\\{v_j\\}$ is the set of candidate entities that are not equivalent to $u_j$. The reranking model is trained by minimizing the following loss:"}, {"title": "LAR", "content": "$L_{AR} = - \\sum_{(u_j, v_j, N_j) \\in R} log \\frac{e^{\\delta(u_j,v_j)}}{\\sum_{v_k \\in V_{u_j}} e^{\\delta(u_j,v_k)} + \\sum_{v_l \\in N_j} e^{\\delta(u_j,v_l)}}$\t\t\t\t\t\t\t\t\t\t\t(5)\nHere $\\delta(u, v)$ is the similarity score computed by the reranking model based on the inputs of two entities:\n$\\delta (u, v) = MLP (BERT_{[CLS]} (d_u, d_v))$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(6)\nwhere $d_u$ and $d_v$ represent the textual descriptions of u and v, respectively."}, {"title": "4 Experiments", "content": "To evaluate the performance of our ap- proach, we conduct experiments on both cross- lingual and monolingual datasets, including:"}, {"title": "4.1 Datasets", "content": "\u2022 DBP15K(Sun et al., 2017) contains three cross-lingual EA datasets build from Dbpedia, including Chinese-English (ZH-EN), Japanese-English (JA-EN), and French- English (FR-EN).\n\u2022 D-W-15K(Sun et al., 2020b) is a monolingual EA dataset built from DBpedia and Wikipedia by using an iterative degree-based sampling method. Compared with DBP15K, D-W-15K contains KGs that are more like real-world ones.\n\u2022 MED-BBK-9K(Zhang et al., 2020) is a dataset built from two medical knowledge graphs, containing triples on diseases, symp- toms, drugs, and diagnosis methods. It poses a more complex and realistic scenario for EA compared to traditional datasets like DBpedia."}, {"title": "4.2 Training Details", "content": "We train the Entity Verbalization (EV), Entity Retrieval (ER), and Alignment Reranking (AR) models sequentially.\nEV Model. In the training of EV model, we employ Deepspeed\u00b9 with a context window length of 2048, the learning rate is set to 9.65e-"}, {"title": "ER Model.", "content": "In the training of ER model, for each positive entity, 64 negative entities are randomly sampled from the top-200 nearest ones. The learning rate is set to le - 5, and the batch size to 16. We utilize distributed negative sample sharing and gradient checkpointing(Chen et al., 2016), evaluate the model every 20 steps and saving the best model based on the MRR metric on the validation set. Training is performed on 2 NVIDIA A800 GPUs for 5 epochs."}, {"title": "AR Model.", "content": "In the training of AR model, for each positive entity, 110 negative entities are randomly sampled from the top-200 nearest ones. The maximum text length is set to 512; the learning rate to le - 5, and the batch size to 12 per GPU. Gradient accumulation steps are set to 8. We enable gradient checkpointing, evaluate the model every 10 steps, and save the best model based on the Hits@1 metric on the validation set. Training is carried out on 2 NVIDIA A800 GPUs for 5 epochs."}, {"title": "4.3 Results on DBP15K", "content": "We compare our approach with four groups of baselines on DBP15K datasets, which are categorized by the used side information: (1) approaches using attributes as side information, including JAPE(Sun et al., 2017), GCN-Align(Wang et al., 2018), JarKA(Chen et al., 2020); (2) approaches using entity names as side information, including GMNN(Xu et al., 2019), SelfKG(Liu et al., 2022) and TEA-NSP, TEA-MLM(Zhao et al., 2023); (3) approaches using attributes and names as side information, including HMAN(Yang et al., 2019), AttrGNN(Liu et al., 2020), BERT-INT(Tang et al., 2020), ICLEA(Zeng et al., 2022) and TEA-NSP, TEA-MLM(Zhao et al., 2023); (4) approaches using translated entity names as side information, including HGCN-JE(Wu et al., 2019b),"}, {"title": "4.4 Results of Hard Setting on DBP15K", "content": "In the work of AttrGNN(Liu et al., 2020), a hard setting of evaluations on DBP15K was proposed. The purpose of this hard setting is to build more difficult testing set on DBP15K. Specifically, similarities of equivalent entities in the datasets are first measured using embeddings of their names, 60% entity pairs with the lowest similarities are selected as the testing set, and the remaining entity pairs are randomly split into training set (30%) and validation set (10%)."}, {"title": "4.5 Results on DW-15K and MED-BBK-9K", "content": "DW15K and MED-BBK-9K are two challenging datasets of entity alignment. DW-15K is built from Wikipedia, where entity names are replaced with ids; there are also significant missing and corrupted attribute values. The dataset of MED-BBK-9K is built from an authoritative medical KG and a KG built from a Chinese online encyclopedia (Baidu Baike); many entities in MED-BBK-9K lack names and attributes, which makes the EA task more difficult. We compared our approach with seven approaches, three of them are probabilistic ones including LogMap(Jim\u00e9nez-Ruiz and Cuenca Grau, 2011), PARIS(Suchanek et al., 2011), and PRASE(Qi et al., 2021); four of them are embedding-based ones including MultiKE(Zhang et al., 2019), BootEA(Sun et al., 2018), RSNs(Guo et al., 2019) and FGWEA(Tang"}, {"title": "4.6 Ablation Study", "content": "To analyze the effectiveness and contribution of each component in the proposed approach, we conduct ablation studies on DBP15K datasets. We ran two groups of experiments, one group uses attributes as side information, and the other group uses both names and attributes as side information. In each group, we ran three variations of DERA: 1)"}, {"title": "5 Related Work", "content": "In Embeddings-based KG alignment approaches employ TransE and GNN to learn entities'embeddings, and then find equivalent entities in the vector spaces. Early approaches mainly rely on the structure information in KGs to find alignments, including TransE-based approaches MTransE (Chen et al., 2017), IPTransE (Zhu et al., 2017), BootEA (Sun et al., 2018), etc, and GNN-based approaches MuGNN (Cao et al., 2019), NAEA (Zhu et al., 2019), RDGCN (Wu et al., 2019a) and AliNet (Sun et al., 2020a), etc. To get improved results, some approaches utilize entity attributes or names in KGs. JAPE (Sun et al., 2017) performs attribute embedding by Skip-Gram model which captures the correlations"}, {"title": "5.1 Embedding-based EA", "content": "Embedding-based KG alignment approaches employ TransE and GNN to learn entities'embeddings, and then find equivalent entities in the vector spaces. Early approaches mainly rely on the structure information in KGs to find alignments, including TransE-based approaches MTransE (Chen et al., 2017), IPTransE (Zhu et al., 2017), BootEA (Sun et al., 2018), etc, and GNN-based approaches MuGNN (Cao et al., 2019), NAEA (Zhu et al., 2019), RDGCN (Wu et al., 2019a) and AliNet (Sun et al., 2020a), etc. To get improved results, some approaches utilize entity attributes or names in KGs. JAPE (Sun et al., 2017) performs attribute embedding by Skip-Gram model which captures the correlations"}, {"title": "5.2 Language Model-based EA", "content": "As Pre-trained Language Models(PLMs) being successfully used in various tasks, some approaches utilize PLMs to model the semantic information of entities in the task of KG alignment. AttrGNN(Liu et al., 2020) uses BERT to encode attribute features of entities. It encode each attribute and value separately, and then uses a graph attention network to compute the weighted average of attributes and values. BERT-INT(Tang et al., 2020) embeds names, descriptions, attributes and values of entities using a LM; pair-wise neighbor-view and attribute-view interactions are performed to get the matching"}, {"title": "6 Conclusion", "content": "In this paper, we propose a dense entity retrieval approach, DERA, for entity alignment in knowledge graphs. DERA first converts entity triples into unified textual descriptions using an entity verbalization model, and then trains a language model-based embedding model to encode the entities. Candidate alignments are identified based on their similarities in the embedding space and are further reranked by an alignment reranking model. Experiments demonstrate that DERA achieves state-of-the-art results on entity alignment tasks of varying difficulty levels."}, {"title": "Limitations", "content": "The primary limitation of DERA is its pipelined framework, where models in its three stages are trained sequentially. Consequently, the component models in DERA are not optimized jointly during training. Exploring efficient methods for the joint learning of these models would be a valuable direction for future work, potentially enhancing the results further. Additionally, DERA consumes more GPU power than traditional models, which is another limitation."}]}