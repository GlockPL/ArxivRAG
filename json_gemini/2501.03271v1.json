{"title": "A Semantically-Aware, Kernel-Enhanced, and Divergence-Rich Paradigm for Direct Preference Optimization", "authors": ["Amitava Das", "Suranjana Trivedy", "Danush Khanna", "Rajarshi Roy", "Gurpreet Singh", "Basab Ghosh", "Yaswanth Narsupalli", "Vinija Jain", "Vasu Sharma", "Aishwarya Naresh Reganti", "Aman Chadha"], "abstract": "The rapid advancement of large language models (LLMs) has revolutionized numerous applications, but presents significant challenges in aligning these models with diverse human values, ethical standards, and specific user preferences. Direct Preference Optimization (DPO) has become a cornerstone for preference alignment but is constrained by reliance on fixed divergence measures and limited feature transformations. We introduce DPO-Kernels, an innovative enhancement of DPO that integrates kernel methods to overcome these challenges through four key contributions: (i) Kernelized Representations: These representations lay the groundwork for enhanced divergence measures by leveraging polynomial, RBF, Mahalanobis, and spectral kernels for richer, more expressive feature transformations. Additionally, we introduce a hybrid loss that combines embedding-based loss with probability-based loss, enhancing the optimization process beyond traditional DPO; (ii) Divergence Alternatives: Incorporating Jensen-Shannon, Hellinger, R\u00e9nyi, Bhattacharyya, Wasserstein, and f-divergences to boost stability and robustness; (iii) Data-Driven Selection: Choosing the optimal kernel-divergence pair among 28 combinations (4 kernels \u00d7 7 divergences) is challenging. We introduce automatic metrics that analyze the data to select the best pair, eliminating the need for manual tuning; (iv) Hierarchical Mixture of Kernels (HMK): Combining local and global kernels for precise and large-scale semantic modeling. This approach automatically selects the optimal kernel mixture during training, enhancing modeling flexibility. Evaluations on 12 datasets demonstrate that DPO-Kernels achieve state-of-the-art generalization in factuality, safety, reasoning, and instruction following. While alignment generally carries the risk of overfitting, grounded in Heavy-Tailed Self-Regularization (HT-SR) theory, we show that DPO-Kernels maintain robust generalization bounds in LLMs. Comprehensive resources are available to facilitate further research and application of DPO-Kernels.", "sections": [{"title": "1 DPO Revisited: Mathematical Components and Scope for Enhancement", "content": "The Direct Preference Optimization (DPO) (Rafailov et al., 2024) framework aims to optimize a policy $\\pi(y | x)$ by balancing two objectives: improving the policy's ranking on preferred outcomes and regularizing it against a reference distribution using the Kullback-Leibler (KL) divergence. The DPO objective can be expressed as:\n\\[\\max \\mathbb{E}_{x,y^+,y^-} \\log \\frac{\\pi(y^+|x)}{\\pi(y^-|x)} - \\alpha \\mathbb{E}_x \\sum_y \\pi(y|x) \\log \\frac{\\pi(y|x)}{\\pi_{ref}(y|x)}\\]\nwhere: $x$: The input prompt/context; $y^+$: The preferred output; $y^\u2212$: The less preferred output, $\\pi(y | x)$: The policy being optimized; $\\pi_{ref}(y | x)$: The reference policy (often a pre-trained model's distribution); $\\alpha > 0$: Hyperparameters controlling the strength of the regularization."}, {"title": "2 Richer Representation: Hybrid Approach: Integrating Probability and Embeddings", "content": "DPO (Rafailov et al., 2024) relies on the contrastive loss $log\\frac{\\pi(y^+|x)}{\\pi(y^-|x)}$, which focuses solely on probability-based preferences. While effective, this approach often neglects deeper semantic and qualitative factors inherent in human preferences. \nTo address this limitation, we introduce a hybrid preference alignment method that integrates embedding-based signals alongside probability-based cues. Our approach defines a preference signal as $f_{embed}(x,y^+,y^-) = e_{y^+} \u2212 e_{y^-}$, where $e_{y^+}$ and $e_{y^-}$ are embedding-based similarity scores for positive and negative responses, respectively. For our experiments, we utilize jina-embeddings-v3 (Sturua et al., 2024), but the framework is adaptable to other embeddings, enabling generalization across embedding models.\nEmbedding-based representations are well-established in preference modeling, reward design, and metric learning (Bai et al., 2022b; Ouyang et al., 2022; Peyr\u00e9 and Cuturi, 2019), often relying on pairwise distances or fixed objectives (Oord et al., 2018; Chen et al., 2020; Radford et al., 2021). Recent large language models (LLMs) like LaMDA (Thoppilan et al., 2022) and PaLM (Chowdhery et al., 2022) also leverage embeddings for preference alignment. However, existing approaches typically treat embeddings and probability-based signals separately, relying on fixed divergence measures (e.g., KL, triplet loss (Schroff et al., 2015), or contrastive loss (Hadsell et al., 2006)). In contrast, our work is the first to bridge embeddings and probability-based alignment in a unified parametric framework for policy learning, offering a more comprehensive approach to preference optimization.\nHybrid Loss: We blend probability and embedding signals:\n\\[max \\mathbb{E}_{x,y^+,y^-}[log \\frac{\\pi(y^+|x)}{\\pi(y^-|x)} + \\gamma(log \\frac{\\pi(e_{y^+}| e_x))}{\\pi(e_{y^-}| e_x))}]-KL\\]\nwith $\\gamma > 0$ controlling the contribution of the embedding signal. When $\\gamma = 0$, we recover the standard DPO loss. Increasing $\\gamma$ guiding the policy to produce outputs that are both probable and semantically preferable."}, {"title": "3 Kernel-Integrated DPO Formulation", "content": "Standard DPO aligns a policy \\(\\pi\\) with human preferences while regularizing against a reference distribution \\(\\pi_{ref}\\) via a divergence D(\\u00b7||\\u00b7). While effective, this approach relies on simple distributional differences, which may fail to capture deeper semantic relationships essential for alignment. To address this, we introduce kernelized proximity measures that enable more expressive and adaptive alignment. Our framework extends DPO into four distinct DPO-Kernel variants: (i) Polynomial, (ii) RBF, (iii) Spectral, and (iv) Mahalanobis. The resulting objective is expressed as:\n\\[\\max \\mathbb{E}_{x,y^+,y^-} [log \\frac{\\pi(y^+|x)}{\\pi(y^-|x)} + \\gamma log \\frac{e_{y^+}e_x}{e_{y^-}e_x} ]-KL\\]\nEach kernel offers a unique perspective on alignment. Polynomial kernels capture higher-order interactions, enabling compositional reasoning. RBF kernels emphasize local, fine-grained structure, useful for proximity-based alignment. Spectral kernels capture global, oscillatory patterns to handle periodic dependencies, while Mahalanobis kernels leverage feature covariance to account for anisotropic relationships. These kernelized variants preserve the core mathematical foundations of DPO while significantly enhancing its ability to capture richer alignment criteria."}, {"title": "4 Replacing KL regularizer with alternatives", "content": "The original DPO framework typically utilizes the Kullback-Leibler (KL) divergence to align the learned policy $\\pi(y | x)$ with the reference distribution $\\pi_{Pref}(y | x)$. While KL divergence is favored for its strong theoretical foundations, exploring alternative divergence measures can lead to more robust optimization, enhanced stability, and improved interpretability and generalizability."}, {"title": "5 Data-Driven Selection of Kernel Types and Divergence Functions", "content": "Choosing the optimal kernel-divergence pair among 28 combinations (4 kernels \u00d7 7 divergences) is challenging. We propose a systematic, data-driven framework that replaces heuristics with well-defined metrics, ensuring adaptability and improved generalization."}, {"title": "5.1 Data-Driven Kernel Selection Logic", "content": "We propose four novel metrics\u2014Positive-Negative Divergence (PND), Positive-Negative Alignment Variance (PNAV), Triplet Alignment Tightness (TAT), and Normalized Alignment Gap (NAG)\u2014that quantify key geometric and relational properties of the data, summarized in Table 3. Fig. 3 visualizes the four proposed metrics for kernel selection in alignment tasks: these metrics collectively assess alignment properties, such as separability, consistency, precision, and gap quality, enabling a comprehensive evaluation of kernel performance in alignment.\nHere, we prescribe a practical guideline to help users empirically select the most suitable kernel for alignment tasks based on key metrics. By leveraging thresholds for metrics such as PNAV, TAT, NAG, and PND, this framework provides an intuitive yet effective approach to kernel selection, ensuring alignment properties are well-captured for diverse scenarios.\nHere, thresholds $\\epsilon_1, \\epsilon_2, \\epsilon_3, \\epsilon_4, \\epsilon_5$ are empirically tuned or determined through validation. Initial values such as $\\epsilon_1 = 0.5, \\epsilon_2 = 0.3, \\epsilon_3 = 0.2, \\epsilon_4 = 0.7$, and $\\epsilon_5 = 0.1$ serve as practical defaults. Balanced metrics (e.g., \u2248 0) signal alignment structures, while larger deviations reveal more intricate relationships requiring advanced kernels."}, {"title": "5.2 Data-Driven Divergence Choice Logic", "content": "We further propose four distributional metrics\u2014Support Overlap, Drift Magnitude, Kurtosis, and Smoothness\u2014to systematically select the most appropriate divergence measure, summarized in Table 4. Fig. 4 visualizes the four proposed metrics for divergence selection: these metrics provide insights into the behavior of distributions by quantifying their overlap, shift, tail properties, and functional smoothness. Collectively, they enable the empirical selection of the most appropriate divergence measure for various data scenarios, ensuring effective modeling and comparison of distributions.\nWe provide a practical guideline to help users empirically select the most suitable divergence measure based on key metrics. These metrics offer insights into distributional behavior, ensuring the chosen divergence measure aligns with the data's characteristics."}, {"title": "6 Kernel Mixture Approach - Improved Generalization", "content": "The use of a single kernel often fails to capture the diverse relationships inherent in alignment tasks. Different kernels are adept at modeling specific properties, such as local similarities, global structures, or higher-order interactions, making it challenging for any single kernel to perform well across all scenarios. A Kernel Mixture Approach addresses this limitation by dynamically combining multiple kernels, leveraging their complementary strengths to improve generalization across varied datasets (e.g., diverse alignment tasks as in (Dubois et al., 2024a; Lv et al., 2023a), policy shifts (Koh"}, {"title": "6.1 Hierarchical Mixture of Kernels", "content": "Hierarchical Mixture of Kernels (HMK) overcomes kernel collapse by introducing a two-level decomposition that balances local kernels (RBF, Polynomial) (Sch\u00f6lkopf and Smola, 2002) and global kernels (Spectral, Mahalanobis) (Weinberger and Saul, 2009; Ng et al., 2001). Local kernels capture short-range dependencies, while global kernels model broader, long-range relationships. HMK assigns learnable weights to both groups, enabling dynamic adaptation to varying data geometries:\n\\[\\Kappa(x, x') = \\tau_1 (\\lambda_1\\Kappa_{RBF} + \\Lambda_2\\Kappa_{Poly}) + \\tau_2(\\lambda_3\\Kappa_{Spectral} + \\Lambda_4\\Kappa_{Maha}),\\]\nwhere $\\tau_1, \\tau_2$ balance local-global contributions. Both $\\tau$ and $\\lambda$ are updated through backpropagation, allowing HMK to maintain kernel diversity and adapt effectively."}, {"title": "6.1.1 Illustration of the Effective Range", "content": "To visualize the kernel influence range, a set of 20 points was randomly sampled from the 2D space [-5,5] \u00d7 [-5,5]. A fixed query point at (0, 0) serves as the reference point for kernel similarity computation for the RBF, Polynomial, Spectral, and Mahalanobis kernels.\nPurpose: Random points offer a dataset-agnostic view of kernel influence.\nWhy It Matters: The query point allows us to analyze how influence propagates, aiding in the understanding of local vs. global behavior."}, {"title": "6.2 Key Insights and Alignment Task Implications", "content": "Local Kernels: Effective for fine-grained tasks like safety alignment or clustering, as their influence decays quickly with distance (Sch\u00f6lkopf and Smola, 2002).\nGlobal Kernels: Crucial for tasks like contextual alignment or multi-hop reasoning, leveraging long-range dependencies (Ng et al., 2001; De Maesschalck et al., 2000).\nGeneralization: HMK combines the strengths of local and global kernels, reducing overfitting while improving adaptability across diverse tasks.\nDynamic Adaptation: The hierarchical structure enables task-aware prioritization of local or global influences, balancing short- and long-range dependencies (Belkin and Niyogi, 2003).\nRobustness to Shifts: The Mahalanobis kernel adds robustness to covariance structure changes, complementing the Spectral kernel's global reach (De Maesschalck et al., 2000)."}, {"title": "6.3 Dynamic Evolution of Kernel Weights", "content": "Fig. 7 shows the evolution of kernel weights ($\\lambda_1, \\lambda_2, \\lambda_3, \\lambda_4$) and Local-Global Balance Coefficients ($\\tau_1, \\tau_2$) over training. Early epochs highlight competition between local and global kernels, with $\\tau_1$ and $\\tau_2$ stabilizing around epoch 100. Polynomial ($\\lambda_1$) and RBF ($\\lambda_2$) dominate initially, while Spectral ($\\lambda_3$) and Mahalanobis ($\\lambda_4$) gain influence later, emphasizing global dependencies. By epoch 200, the system converges to an optimal balance."}, {"title": "7 Empirical Results", "content": "Up to now, we have discussed the theoretical and mathematical extensions of DPO. In this section, we empirically evaluate the effectiveness of the proposed DPO-Kernels. We conducted all our experiments using Llama 3.3 (raymondd, 2024). ?? details our experiments and evaluation setup."}, {"title": "7.1 Datasets & Tasks", "content": "We assess the performance of models trained with DPO-Kernels across 12 diverse preference datasets, thoughtfully chosen to encompass a wide spectrum of data sources. These datasets are categorized as follows: I. Human-Annotated"}, {"title": "7.2 Efficacy of Hybrid Loss", "content": "The heatmap in Fig. 8 demonstrates the performance gains from integrating hybrid loss with various kernels (Polynomial, RBF, Spectral, Mahalanobis, and Kernel Mixture) across alignment tasks: Factuality, Reasoning, Truthfulness, Safety, and Instruction Following. Hybrid loss consistently outperforms standard DPO loss, achieving higher F1 scores even without advanced kernels. Among the kernels, RBF and Kernel Mixture stand out, particularly excelling in Safety and Truthfulness, highlighting the effectiveness of hybrid loss and kernelized proximity measures in enhancing alignment."}, {"title": "7.3 Efficacy of Divegence based Regularizers", "content": "Fig. 10 presents heatmaps showcasing the performance of kernel-divergence combinations across various alignment tasks, including Factuality, Reasoning, Truthfulness, Safety, and Instruction Following. The visualization highlights how different kernels (DPO, Polynomial, RBF, Spectral, Mahalanobis, HMK) paired with divergences (KL, JSD, Hellinger, R\u00e9nyi, Bhattacharyya, Wasserstein, f-Divergence) perform on individual tasks and over"}, {"title": "7.4 Mechanism of Safety Fine-Tuning: Safe vs. Unsafe Cluster Effects", "content": "Jain et al. (2024a) demonstrate that safety fine-tuning (alignment) minimally adjusts MLP weights in LLMs to project unsafe inputs into the null space of weight matrices, inducing distinct clustering of inputs based on safety status. We analyze the evolution of these clusters during training and evaluate their separation using the Davies-Bouldin Score (DBS), where lower values indicate better clustering with compact intra-cluster distances and large inter-cluster separations.\nDefinition: For k clusters \\({C_1, C_2,...,C_k}\\),"}, {"title": "7.5 Generalization vs. Overfitting: Which Kernel Excels?", "content": "The Weighted Alpha metric (Martin et al., 2021) offers a novel way to assess generalization and overfitting in LLMs without requiring training or test data. Rooted in Heavy-Tailed Self-Regularization (HT-SR) theory, it analyzes the eigenvalue distribution of weight matrices, modeling the Empirical Spectral Density (ESD) as a power-law $\\rho(\\lambda) \\propto \\lambda^{-a}$. Smaller $a$ values indicate stronger self-regularization and better generalization, while larger $a$ values signal overfitting. The Weighted Alpha $\\hat{a}$ is computed as: $\\hat{a} = \\sum_{l=1}^{L} a_l log \\lambda_{max,l}$, where $a_l$ and $\\lambda_{max,l}$ are the power-law exponent and largest eigenvalue of the $l$-th layer, respectively. This formulation highlights layers with larger eigenvalues, providing a practical metric to diagnose generalization and overfitting tendencies. Results reported in Fig. 12.\nResearch Questions and Key Insights\n1. RQ1: Do aligned LLMs lose generalizability and become overfitted? Alignment procedures slightly increase overfitting, with a generalization error drift $| \\Delta E_{gen}| \\le 0.1$ (within \u00b110%), which is considered acceptable.\n2. RQ2: Which kernel and divergence functions offer the best generalizability? RBF and Spectral kernels achieve the lowest generalization gap, while Polynomial kernels increase overfitting by 15%. Mahalanobis kernels perform comparably to RBF and Spectral but incur higher computational costs. Among divergences, Bhattacharyya and Wasserstein show the strongest generalization, outperforming others like KL and Jensen-Shannon. R\u00e9nyi divergence is effective for specific tasks but requires careful tuning of \u03b1 to balance alignment strength and overfitting risks. ??"}, {"title": "8 Conclusion", "content": "We introduced DPO-Kernels, a novel framework designed to advance alignment by combining kernelized representations and divergence-based regularization. By leveraging a Hierarchical Mixture of Kernels (HMK) and data-driven selection, our approach systematically addresses the challenges of robust generalization and scalable alignment. A significant challenge in alignment is selecting the optimal kernel-divergence pair from 28 possible combinations (4 kernels \u00d7 7 divergences). To tackle this, we proposed a data-driven framework that replaces heuristics with well-defined metrics, ensuring adaptability and enhanced performance across tasks. Our framework was rigorously evaluated on 12 diverse datasets, demonstrating state-of-the-art generalization across tasks, including factuality, reasoning, safety, and instruction following. While HMK achieves superior performance, it incurs computational costs 3x-4x higher than baseline DPO methods. To address this, future work could explore approximation strategies like Random Fourier Features (RFF) and Nystr\u00f6m methods to reduce computational complexity.\nLooking ahead, DPO-Kernels presents transformative potential across domains such as multimodal alignment (e.g., text-image or text-video tasks), fairness-sensitive AI, and personalized education systems. We encourage the community to explore its capabilities in expanding alignment beyond text to multimodal and real-world applications."}, {"title": "9 Discussion and Limitations", "content": "While DPO-Kernels demonstrate significant advancements in alignment and generalization, several limitations warrant further attention.\n1. Computational Overhead: The Hierarchical Mixture of Kernels (HMK) incurs a computational cost 3-4x higher than baseline methods, primarily due to dynamic kernel balancing and hierarchical decomposition. Approximation techniques like Random Fourier Features (RFF) (Rahimi and Recht, 2007), Nystr\u00f6m methods (Williams and Seeger, 2001), and sparse Gaussian processes (Snelson and Ghahramani, 2006) can alleviate this overhead, making the framework more scalable for large-scale datasets. HMK's computational cost is justified by superior alignment capabilities.\n2. Kernel Collapse: The dominance of a single kernel during training, known as kernel collapse, limits the diversity of kernel contributions. Mitigations include entropy-based regularization (Nemirovski et al., 2009) to promote kernel diversity and certified robustness (Wong and Kolter, 2018) to enforce balanced kernel contributions.\n3. Adversarial Robustness: HMK's sensitivity to adversarial preference perturbations is currently untested. Small input changes can result in significant alignment shifts. Approaches such as adversarial training (Madry et al., 2018) and robust kernel learning (Xu et al., 2009) could strengthen resilience.\n4. Hyperparameter Sensitivity: Performance depends on sensitive parameters like the RBF bandwidth (\u03c3), Polynomial degree (d), and Mahalanobis covariance (\u03a3). Techniques such as meta-learning (Finn et al., 2017) and adaptive tuning (Hazan et al., 2007) can streamline hyperparameter optimization.\n5. Multimodal Alignment: Extending HMK to multimodal tasks (e.g., text-image alignment) involves computationally expensive cross-modal kernel computations. Techniques like cross-modal contrastive learning (Radford et al., 2021) and cross-modal RFF approximations could improve efficiency.\nAddressing these limitations through the suggested mitigations will not only enhance the scalability and robustness of DPO-Kernels but also broaden their applicability to dynamic, multimodal alignment tasks. Refer to Table 5 and Fig. 13 for a detailed overview of limitations and solutions."}, {"title": "10 Ethical Considerations", "content": "The DPO-Kernels framework offers significant potential for alignment tasks, yet its application demands careful attention to ethical concerns. Below, we highlight key considerations and propose actionable strategies to address them."}, {"title": "10.1 Fairness and Bias", "content": "Kernel methods, including those employed in HMK, can inadvertently propagate biases present in training data. For instance, an imbalanced covariance matrix in the Mahalanobis kernel may lead to disparate impacts on underrepresented"}, {"title": "10.2 Privacy Risks", "content": "The Mahalanobis kernel's reliance on covariance structures poses privacy risks, as it may encode sensitive correlations within the data. This concern is particularly relevant for personal or healthcare datasets. Incorporating Differential Privacy (DP) mechanisms during covariance estimation (Jayaraman and Evans, 2021) can safeguard sensitive re"}, {"title": "10.3 Interpretability and Trust", "content": "The hierarchical nature of HMK introduces complexity, making it challenging to interpret the contributions of individual kernels. Transparent visualizations of kernel weights and the evolution of local-global balance parameters (71,72) over training can build user trust (Doshi-Velez and Kim, 2017). Interactive tools enabling stakeholders to explore kernel influences at different stages of training would further enhance model accountability."}, {"title": "10.4 Environmental Impact", "content": "The computational demands of HMK, stemming from hierarchical kernel computation and optimization, raise concerns about energy efficiency (Strubell et al., 2019). To address this, we advocate for efficient kernel approximation techniques, such as Nystr\u00f6m methods (Williams and Seeger, 2001), and encourage the use of energy-efficient hardware. Reporting energy usage in research publications is another step toward responsible AI development, promoting transparency in environmental impact"}, {"title": "10.5 Potential Misuse", "content": "The versatility of DPO-Kernels, especially in capturing local and global dependencies, presents dual-use concerns. For instance, while beneficial for alignment tasks, the framework could be misused for profiling or manipulative personalization (Zarsky, 2016). Mitigation strategies include robust documentation of potential misuse scenarios and adherence to ethical deployment practices,"}, {"title": "11 Frequently Asked Questions (FAQs)", "content": "* What problem does DPO-Kernels address in Direct Preference Optimization (DPO)?\n\u27a1DPO-Kernels addresses the limitations of standard Direct Preference Optimization, which primarily relies on fixed divergence measures (e.g., KL divergence) and simple transformations. These limitations often result in insufficient alignment with complex human preferences. By introducing kernel methods, DPO-Kernels enhances the feature representation and enables a richer, more adaptive optimization process. The framework also incorporates diverse divergence measures (e.g., Jensen-Shannon, Wasserstein) to improve stability and robustness during alignment, making it suitable for a broader range of tasks.\n* How do kernel methods improve preference optimization?\n\u27a1Kernel methods map input data into higher-dimensional spaces where complex patterns and relationships are more easily captured. In DPO-Kernels, this capability allows for:\nEnhanced Representational Power: Kernels like RBF focus on local relationships, while spectral kernels capture global dependencies.\nFlexible Feature Transformations: Instead of relying on raw distributions, kernel methods use transformed feature spaces to better differentiate preferred and less-preferred outputs.\nAdaptability: The hierarchical mixture of kernels (HMK) ensures the model can dynamically adjust to diverse alignment tasks by balancing local and global kernels.\n* What is the purpose of the hybrid loss in DPO-Kernels?\nThe hybrid loss combines two complementary components:\nProbability-Based Contrastive Loss: This ensures that preferred outputs are ranked higher based on likelihood.\nEmbedding-Based Signals: These provide semantic context, helping resolve ambiguities when probabilities alone are insufficient. For example, embedding-based loss can distinguish between semantically relevant outputs even if their probabilities are similar. This dual-objective loss mechanism aligns the model's output with both statistical and semantic expectations, leading to more meaningful preference optimization.\n* How are kernels and divergence measures selected in DPO-Kernels?\n\u27a1DPO-Kernels employs data-driven metrics to automate selection:\nKernel Selection: Metrics like Positive-Negative Divergence (PND) and Triplet Alignment Tightness (TAT) evaluate the separation and clustering of aligned preferences, helping identify the most suitable kernel for a given task.\nDivergence Selection: Metrics such as Support Overlap and Drift Magnitude assess the distributional characteristics of the data, guiding the choice of divergence measures. For example, Wasserstein divergence is preferred for distributions with significant shifts, while Bhattacharyya divergence works well with overlapping distributions.\n* What is the Hierarchical Mixture of Kernels (HMK), and why is it needed?\nThe Hierarchical Mixture of Kernels (HMK) dynamically combines local kernels (e.g., RBF, Polynomial) and global kernels (e.g., Spectral, Mahalanobis). This design:"}]}