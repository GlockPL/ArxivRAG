{"title": "Gradient-Guided Parameter Mask for Multi-Scenario Image Restoration Under Adverse Weather", "authors": ["Jilong Guo", "Haobo Yang", "Mo Zhou", "Xinyu Zhang"], "abstract": "Removing adverse weather conditions such as rain, raindrop, and snow from images is critical for various real-world applications, including autonomous driving, surveillance, and remote sensing. However, existing multi-task approaches typically rely on augmenting the model with additional parameters to handle multiple scenarios. While this enables the model to address diverse tasks, the introduction of extra parameters significantly complicates its practical deployment. In this paper, we propose a novel Gradient-Guided Parameter Mask for Multi-Scenario Image Restoration under adverse weather, designed to effectively handle image degradation under diverse weather conditions without additional parameters. Our method segments model parameters into common and specific components by evaluating the gradient variation intensity during training for each specific weather condition. This enables the model to precisely and adaptively learn relevant features for each weather scenario, improving both efficiency and effectiveness without compromising on performance. This method constructs specific masks based on gradient fluctuations to isolate parameters influenced by other tasks, ensuring that the model achieves strong performance across all scenarios without adding extra parameters. We demonstrate the state-of-the-art performance of our framework through extensive experiments on multiple benchmark datasets. Specifically, our method achieves PSNR scores of 29.22 on the Raindrop dataset, 30.76 on the Rain dataset, and 29.56 on the Snow100K dataset.", "sections": [{"title": "1. Introduction", "content": "Adverse weather conditions like rain, snow, and raindrops severely degrade image quality, challenging applications such as autonomous driving, surveillance, and remote sensing[3, 4]. These weather-related artifacts can obscure vital visual information, leading to diminished performance in image analysis and interpretation tasks[5, 6]. Therefore, developing effective image restoration techniques that can mitigate these adverse effects is essential for enhancing the reliability and effectiveness of systems relying on accurate visual data.\nWith the advancement and application of deep learning technologies, the widespread use of convolutional neural networks (CNNs) [7-9] and transformers [10-12] has led to significant progress in image restoration methods. These sophisticated architectures are particularly adept at capturing intricate features within images, thereby facilitating the effective removal of a multitude of artifacts and degradations. For instance, CNNs[13, 14] leverage their powerful local feature extraction capabilities to identify and restore subtle changes resulting from adverse weather conditions. In contrast, transformers [15, 16] enhance the modeling of long-range dependencies through global attention mechanisms, making them particularly efficient in addressing large-scale disturbances. The integration of these technologies not only improves the accuracy of image restoration but also significantly boosts processing speed, facilitating real-time applications. Nevertheless, despite the impressive performance of existing methods on individual tasks, most are optimized for single weather conditions, resulting in inefficiencies when confronted with diverse adverse scenarios.\nRecent research [1, 2] highlights the general and specific characteristics of image degradation under various weather conditions, which has led to the development of a unified deep model to eliminate weather-related artifacts. However, these solutions have limitations in practical applications. Firstly, as shown in Fig. 1(b), Li et al.[1] enhance the network's recovery capabilities across different scenes by introducing distinct encoders. However, when the dataset distribution across scenes is uneven, smaller datasets can be adversely affected by larger ones, leading to a degradation in performance. Secondly, as illustrated in Fig. 1(c), Zhu et al. [2] achieve efficient image recovery across multiple scenes by adaptively adding parameters during collaborative training on multi-scene data. While this approach improves the model's applicability to various scenarios, the introduction of additional parameters complicates its deployment in real-world applications.\nIn this work, we propose a Gradient-Guided Parameter Mask for Multi-Scenario Image Restoration Under Adverse Weather, designed to effectively tackle image degradation caused by various weather conditions, including rain, raindrops, and snow. Our approach introduces a novel mask strategy that partitions model parameters into shared and task-specific components by analyzing the gradient changes induced by the common parameters during training across different weather scenarios. Specifically, we classify parameters into common and task-specific subsets based on the magnitude of the gradient variations induced by the training data for each weather condition. Common parameters capture general features that are consistent across all weather conditions, while task-specific parameters are adapted to the unique characteristics of each specific weather scenario. This method addresses the challenge of overfitting caused by limited data in individual weather scenarios by leveraging common parameters, which complement the task-specific parameters. The introduction of the mask ensures that the parameter updates for each scenario are isolated from interference caused by other weather conditions, leading to improved image restoration performance. Additionally, by avoiding the addition of extra parameters, our approach ensures that the model remains lightweight, making it highly suitable for deployment in real-time applications, such as autonomous driving, where computational efficiency is crucial.\nThe key contributions of this work are as follows:\n\u2022 We propose a novel masking method that accomplishes the image restoration tasks without adding extra parameters, ensuring efficient performance.\n\u2022 Our efficient gradient-guided parameter mask effectively decouples task-specific parameters, mitigating interference across diverse weather scenarios and enhancing performance.\n\u2022 Our approach achieves state-of-the-art performance on multiple datasets. Furthermore, the efficacy of the proposed method is validated on various adverse weather scenarios."}, {"title": "2. Related Work", "content": "In the field of autonomous driving, image recovery in adverse weather scenes has always been a core issue that has attracted much attention[17]. In response to the degradation of image quality caused by different bad weather conditions, previous studies have extensively discussed a variety of image restoration tasks,including deraining[18-27], desnowing[28-32], and raindrop removal[1, 33-35]."}, {"title": "2.1. Rain Removal", "content": "To tackle the challenge of rain streak removal, several approaches leverage Convolutional Neural Networks (CNNs) for effective deraining [36], [37], utilizing the architecture and module integration of deep neural networks to eliminate rain artifacts. Additionally, Generative Adversarial Networks (GANs) have been employed to efficiently remove raindrops [38], [39]. Beyond these, various techniques such as adversarial learning [40], transfer learning [41, 42], frequency priors [43], and data generation [44] have been explored to enhance model performance. The Single Image Rain Removal (SIRR) method [45] focuses on learning rain patterns from images through CNNs, enabling effective removal of raindrop effects. CycleGAN-based architectures are also widely utilized for rain removal, where the network learns image transformation via cycle consistency, thereby significantly improving the removal of rain"}, {"title": "2.2. Raindrop Removal", "content": "In recent years, significant progress has been made in the field of raindrop removal in image processing and computer vision. In [47], light-field images and depth maps were used for accurate raindrop detection and subsequent image restoration. [48] introduced an iterative contrastive learning framework to improve raindrop removal through incremental optimization. To reduce the dependence on paired training samples, [33] proposed a weakly supervised approach using image-level annotations. In [49], the simultaneous removal of raindrops and rain streaks was demonstrated, improving the restoration quality without adding computational burden. Furthermore, [50] presented a sparse sampling transformer with an uncertainty-driven strategy for unified raindrop and rain streak removal, demonstrating the potential of deep learning. In [51], RainGAN was introduced, an unsupervised framework that utilizes decomposition and composition for effective raindrop removal without paired training data. These innovations not only improve the efficiency and accuracy of raindrop removal, but also broaden its applicability in challenging weather conditions."}, {"title": "2.3. Snow Removal", "content": "Concerning snow removal, various approaches have been proposed to alleviate the impact of snowflakes on visual content. [29] propose a Deep Dense Multi-Scale Network (DDMSNet) for snow removal by exploiting semantic and depth priors. In refinement stage based on generative adversarial networks (GANs) [31] is proposed to further improve the visual quality of the resulting snow-removed images and make a refined image and a clean image indistinguishable by a computer vision algorithm to avoid the potential perturbations of machine interpretation.DCSNet[52] is an effective method for capturing the diversity of snowflakes and removing snow layers in stages. This is achieved by adaptively fusing a feature pyramid structure and a progressive restoration module. Furthermore, [53] employs a dual gradient strategy to precisely locate snowflakes using gradient activation maps and edge maps. By employing a designed mask estimation network and a transparency-aware context restoration network, it achieves accurate snowflake removal and restoration of image context information."}, {"title": "2.4. Multi-Task Learning (MTL)", "content": "Recognizing the interconnected nature of weather-related image restoration tasks, researchers have increasingly explored multi-task learning (MTL) paradigms[54, 55]. By jointly optimizing models for various tasks, such as rain, raindrop, and snow removal, MTL aims to enhance the overall efficiency and robustness of image restoration algorithms. This collaborative approach facilitates a more unified and effective solution to address the complexities inherent in diverse weather conditions.For example, the AIRFormer model proposed in [56] serves as a paradigm for multitask learning. By introducing frequency-guided transformer encoders and decoders, this model achieves integrated processing of image restoration under different weather conditions. The frequency information in the model is used as cross-task shared knowledge, which helps to improve performance across different tasks. Similarly, [57] and [58] adopt the idea of multitask learning. The former obtains cross-task general feature representations through mask-based pre-training on large datasets, while the latter achieves joint optimization of multiple removal tasks through local reasoning and back-projection mechanisms."}, {"title": "3. Methodology", "content": "To address the challenges posed by image degradation in diverse adverse weather conditions, we propose an Gradient-Guided Parameter Masking (APM) framework for our multi-Scenario adverse weather restoration network. This network is designed to efficiently restore images degraded by weather artifacts such as rain, snow, and raindrop, using a unique parameter masking approach. Our method introduces three distinct masks corresponding to each adverse weather condition, enabling the model to adaptively leverage general and scenario-specific features without increasing the total parameter count, thereby maintaining computational efficiency suitable for real-time applications."}, {"title": "3.1. Image Degradation Under Adverse Weather", "content": "Adverse weather conditions, including rain, snow, and raindrop, significantly degrade the quality of captured images [59]. This degradation arises from the complex interactions between light and atmospheric particles, which alter the transmission and scattering of light as it travels from the scene to the observer [60]. The atmospheric scattering model [61](in Fig. 2) describes the imaging formulation under such conditions, indicating that the irradiance received from a particular scene point by the photographic sensor is the sum of direct transmission (attenuation) and scattered light:\n$E = E_x\\cdot \\rho \\cdot e^{-\\beta d} + E_\\infty \\cdot (1 - e^{-\\beta d}),$ (1)\nwhere $E$ represents the total irradiance received at a specific point in the scene, $E_\\infty$ denotes the intensity of light from the sky, $\\rho$ is the normalized radiance of the scene point, $\\beta$ is the scattering coefficient, and $d$ represents the optical distance from the scene point to the observer. The direct transmission term $E_x \\cdot e^{-\\beta d}$ quantifies the attenuation of light, while the airlight term $E_\\infty \\cdot (1 - e^{-\\beta d})$ encapsulates the effects of scattering on image quality.\nDespite the distinct degradation characteristics associated with each weather phenomenon, they exhibit commonalities attributable to the fundamental physical processes governing light behavior. Specifically, all these weather conditions involve the dual processes of light attenuation and scattering, which manifest similarly within the framework of the atmospheric scattering model. This insight allows us to leverage the common features present across multiple weather scenarios to optimize and adjust model parameters."}, {"title": "3.2. Gradient-Guided Parameter Mask", "content": "To effectively harness the shared and specific characteristics of different weather scenarios, we propose an Gradient-Guided Parameter Masking strategy. This approach involves creating three task-specific masks corresponding to rain, raindrop, and snow removal tasks. These masks enable selective modification of task-specific parameters while keeping the core model architecture frozen, thus minimizing computational demands and reducing the risk of overfitting to a single task."}, {"title": "3.2.1. Creation of Task-Specific Masks", "content": "For each weather condition $t \\in \\{rain, raindrop, snow\\}$, we define a mask $M_t$ that filters parameters relevant to task $t$. The mask $M_t$ partitions the model's parameters into common parameters $\\Theta_c$ and task-specific parameters $\\Theta_t$:\n$\\Theta = \\Theta_c \\cup \\Theta_t.$ (2)\nThis partitioning allows the model to retain pre-trained general features while enabling selective adaptation to specific weather conditions."}, {"title": "3.2.2. Gradient-Guided Mask Determination", "content": "To identify the most influential parameters for each task, we perform backpropagation on the pre-trained model and calculate the gradients of all parameters with respect to the"}, {"title": "3.2.3. Selective Parameter Training with Masked Parameters", "content": "Once the masks are determined, we train only the parameters identified by the masks for each specific task $t$, while keeping the common parameters $\\Theta_c$ frozen. The training objective for task $t$ is defined as:\n$\\theta^*_t = arg \\min_{\\theta_t} L_t (f(\\Theta_c, \\Theta_t; X_t)),$ (4)\nwhere $\\Theta_t^*$ denotes the optimized task-specific parameters for $t$, $f(\\cdot)$ represents the model function, and $X_t$ is the input data for task $t$. This selective training allows the model to adapt to each adverse weather condition effectively while maintaining computational efficiency."}, {"title": "3.3. Network Architecture", "content": "Our network architecture builds on the U-Net structure [62] to achieve high-quality image restoration across multiple adverse weather conditions. Unlike traditional multi-task networks [1, 2], our approach enables multi-scenario restoration within a single-task network architecture, without introducing any additional parameters.\nAs shown in Fig. 3, the APMN incorporates the Gradient-Guided Parameter Masking strategy directly into the convolutional layers of the U-Net. Each convolutional layer utilizes the common parameters $\\Theta_c$ to capture general degradation patterns consistent across weather conditions, while the task-specific parameters $\\Theta_t$ are dedicated to representing unique weather-induced artifacts.\nAs:\nThe forward pass of the network for task $t$ is represented\n$\\hat{I}_t = f_{dec}(f_{enc}(X_t; \\Theta_c, \\Theta_t)),$ (5)\nwhere $\\hat{I}_t$ is the restored image for task $t$, $f_{enc}$ and $f_{dec}$ denote the encoding and decoding functions, respectively, and $X_t$ is the input degraded image."}, {"title": "3.4. Loss Function", "content": "To further enhance image restoration under diverse adverse weather conditions, we incorporate a composite loss function that combines pixel-wise accuracy with depth consistency. The loss function for task $t$ is defined as:\n$L_t = L_1(\\hat{I}_t, Y_t) + \\lambda_{depth} ||D(\\hat{I}_t) - D(Y_t)||_1,$ (6)\nwhere $L_1(\\hat{I}_t, Y_t)$ denotes the smooth L1 loss between the restored image $\\hat{I}_t$ and the ground truth $Y_t$, $D(\\cdot)$ represents a pre-trained depth estimation network [63], and $\\lambda_{depth}$ is a weighting factor controlling the influence of the depth consistency loss. By combining pixel-wise accuracy with depth consistency, this loss formulation effectively captures both the fidelity of pixel restoration and the structural coherence of the output images."}, {"title": "3.5. Inference with Gradient-Guided Parameter Mask", "content": "During inference, the model utilizes both the frozen common parameters $\\Theta_c$ and the task-specific parameters $\\Theta_t$ identified by each mask $M_t$. For a given weather condition $t$, the model dynamically activates the corresponding task-specific parameters, allowing for efficient adaptation without requiring full retraining. The inference process is expressed as:\n$\\hat{I}_t = f_{dec}(f_{enc}(X_t; \\Theta_c, \\Theta_t^*)).$ (7)\nThis approach ensures that only the most relevant parameters are activated based on the input weather condition, enhancing inference efficiency and enabling real-time application."}, {"title": "3.6. Summary of Approach", "content": "Our proposed Gradient-Guided Parameter Masking method offers a streamlined yet effective solution for multi-scenario image restoration under adverse weather conditions. By focusing on the most impactful parameters identified through gradient-based backpropagation, our method minimizes computational overhead and reduces retraining requirements. The region-sensitive application of shared and task-specific parameters optimizes the model's configuration for robust performance across various weather scenarios, making it highly suitable for real-time applications in autonomous driving and weather-dependent image processing systems [17]."}, {"title": "4. Experiments", "content": "We conduct extensive experiments to show the effectiveness of our proposed method. In what follows, we explain the datasets, implementation details, experimental settings, results and comparison with state-of-the-art methods."}, {"title": "4.1. Datasets", "content": "Our network is trained on a comprehensive dataset of images with diverse degradations from various adverse weather conditions, following the training set distribution of the All-in-One [1] Network to ensure fair comparison. The training data includes 9,000 images from the Snow100K dataset [64], 1,069 images from the Raindrop dataset [65], and an additional 9,000 synthetic images from the Outdoor-Rain dataset [66]. Snow100K provides synthetic images simulating snow effects, while the Raindrop dataset offers real-world images affected by raindrops. The Outdoor-Rain dataset comprises synthetic images degraded by a combination of fog and rain streaks. We designate this extensive training set as \"All-Weather\" to emphasize its diverse weather conditions."}, {"title": "4.2. Implementation Details", "content": "We implement our method using the PyTorch framework and train it on 8 NVIDIA A100 GPUs. The training utilizes the Adam optimizer with a learning rate set to 0.0001. The network is trained for a total of 120 epochs with a batch size of 9."}, {"title": "4.3. Comparison with the State-of-the-art Methods", "content": "To comprehensively evaluate the proposed Gradient-Guided Parameter Masking framework, we conduct an extensive series of comparisons with state-of-the-art (SOTA) methods. Initially, we assess its performance against existing single restoration networks under a range of degradation scenarios, ensuring a robust evaluation across different weather conditions. Furthermore, we compare our framework with leading multi-task networks to benchmark its effectiveness in multi-domain restoration tasks. For a thorough and accurate performance analysis, we adopt Peak Signal-to-Noise Ratio (PSNR) [67] and Structural Similarity Index (SSIM) [68] as our primary evaluation metrics. These metrics provide complementary insights, with PSNR focusing on pixel-level fidelity and SSIM assessing the perceptual quality of the restored images. This enables us to evaluate both the quantitative accuracy and the visual quality of the results, ensuring a holistic assessment of the proposed method."}, {"title": "5. Conclusion", "content": "In this paper, we proposed the Gradient-Guided Parameter Mask (APM), a novel approach to multi-scenario image restoration under adverse weather conditions. By leveraging task-specific masks, our method enables the adaptive selection and fine-tuned optimization of parameters without increasing the overall model complexity. Through extensive experimentation, we demonstrated that our framework outperforms existing state-of-the-art methods across a variety of weather scenarios, including rain, snow, and raindrops, while maintaining computational efficiency suitable for real-time applications. Our proposed method effectively mitigates gradient conflicts between tasks by selectively optimizing only the most relevant parameters for each specific weather condition. This approach not only reduces model size but also enhances inference speed, making it highly applicable to real-time systems such as autonomous vehicles and weather-dependent image processing."}, {"title": "6. Discussion", "content": "Our proposed method demonstrates several key advantages over existing approaches. Firstly, due to its design that avoids adding extra parameters, our method maintains a significantly smaller parameter size compared to other methods, as illustrated in Fig. 7. This reduction in parameter scale is especially valuable for applications in fields such as autonomous driving, where computational resources on end devices are often limited. A smaller model size reduces both memory and computational load, making our method highly suitable for deployment in real-time, resource-constrained environments.\nSecondly, our method exhibits strong generalization capabilities. Since we do not alter the underlying model architecture or parameters, our approach is compatible with a wide range of state-of-the-art (SOTA) models. This plug-and-play nature enables seamless integration with existing models without the need for extensive modification, enhancing the versatility and ease of use of our method in diverse applications. This adaptability, combined with computational efficiency, positions our approach as a practical and scalable solution for multi-scenario adverse weather restoration in various real-world applications."}]}