{"title": "Emoti-Attack: Zero-Perturbation Adversarial Attacks on NLP Systems via Emoji Sequences", "authors": ["Yangshijie Zhang"], "abstract": "Deep neural networks (DNNs) have achieved remarkable success in the field of natural language processing (NLP), leading to widely recognized applications such as ChatGPT. However, the vulnerability of these models to adversarial attacks remains a significant concern. Unlike continuous domains like images, text exists in a discrete space, making even minor alterations at the sentence, word, or character level easily perceptible to humans. This inherent discreteness also complicates the use of conventional optimization techniques, as text is non-differentiable. Previous research on adversarial attacks in text has focused on character-level, word-level, sentence-level, and multi-level approaches, all of which suffer from inefficiency or perceptibility issues due to the need for multiple queries or significant semantic shifts.\nIn this work, we introduce a novel adversarial attack method, Emoji-Attack, which leverages the manipulation of emojis to create subtle, yet effective, perturbations. Unlike character- and word-level strategies, Emoji-Attack targets emojis as a distinct layer of attack, resulting in less noticeable changes with minimal disruption to the text. This approach has been largely unexplored in previous research, which typically focuses on emoji insertion as an extension of character-level attacks. Our experiments demonstrate that Emoji-Attack achieves strong attack performance on both large and small models, making it a promising technique for enhancing adversarial robustness in NLP systems.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep neural networks (DNNs) have garnered significant achievements within the NLP field, giving rise to renowned applications like ChatGPT. Nevertheless, it is crucial to focus on the vulnerability of these NLP models to adversarial attacks for their protection. Contrary to the continuous nature of the image domain, text exists in a discrete space. This implies that any alterations, whether at the sentence, word, or even character level, are readily noticeable to humans, making it challenging to create unnoticeable perturbations.\nFurthermore, the discrete nature of text data renders it non-differentiable, leading to the ineffectiveness of conventional optimization techniques. Studies on adversarial textual attacks have been broadly categorized into character-level, word-level, sentence-level, and multi-level approaches. At the character level, attacks involve altering words by means of insertion, omission, misspelling, substitution, or transposition of characters. Such modifications are readily identifiable by people. Yet, character-level attacks necessitate numerous queries to ascertain the specific characters to target. Word-level tactics involve modifying text by inserting, removing, or substituting keywords. Like their character-level counterparts, these attacks also demand repeated queries to identify the target words. Both attacks are iterative, potentially leading to inefficiency in practical NLP use cases due to the need for multiple queries per iteration. Sentence-level attacks entail altering the entire text by rephrasing or appending nonsensical sentences to the original text. The substantial alterations caused by sentence-level attacks might occasionally alter the fundamental meaning of the text. Perturbations at the character, word, and sentence levels can result in significant semantic shifts from the original content and are readily discernible by human observers.\nWe propose a groundbreaking technique known as emoji-level attacks within textual adversarial methods. This approach, which diverges from character- and word-level attacks, targets the manipulation of emoji, offering a less noticeable, and less-word-perturbation. Studies to date have not adequately explored the potential of emoji in adversarial contexts, with some research merely toying with emoji insertion as an extension of character-level strategies.\nOur innovation lies in treating emoji modifications as a distinct layer of attack on par with character, word, and sentence-level approaches.\nOur contributions are as following:\n(1) We propose Emoji-Attack, a novel type of adversarial attack that utilizes seemingly harmless or even playful emoticons to manipulate NLP systems.\n(2) Emoji-Attack demonstrates strong attack performance on both large and small models."}, {"title": "II. PROBLEM FORMULATION", "content": ""}, {"title": "A. Zero-Word-Perturbation Attack Framework", "content": "Contemporary adversarial attack methods predominantly rely on direct text modifications, inevitably compromising semantic integrity and triggering detection mechanisms [1]. Building upon insights from [2] demonstrating the significant influence of emojis on NLP model behavior, we propose a novel zero-word-perturbation adversarial attack framework. Our approach achieves adversarial effects through strategic emoji sequence placement while preserving complete textual integrity.\nFor a given text x and emoji sequences s, s' \u2208 S(E), we formalize the sequence concatenation operation:\n$s \\oplus x \\oplus s' = cat(s) \\cdot x \\cdot cat(s'),$ \nwhere cat denotes the sequence concatenation operation, with s and s' representing the prefix and suffix emoji sequences, respectively.\nGiven a target classification model $f_{tgt}: X \\rightarrow Y$ and an input text $x \\in X$, our framework identifies emoji sequences $s, s' \\in S(E)$ satisfying:\n$f_{tgt}(s \\oplus x \\oplus s') \\neq f_{tgt}(x).$\nThe framework incorporates two fundamental constraints. First, [3] establishes the importance of emotional consistency between emoji sequences and the original text:\n$f_{sen}(s) = f_{sen}(s') = f_{sen}(x).$\nAdditionally, we impose length constraints: $l_{min} \\leq |s|, |s'| < l_{max}$.\nThe optimization objective maximizes prediction divergence while maintaining these constraints:\n$\\underset{s,s' \\in S(E)}{max} L(s \\oplus x \\oplus s', y),$\nwhere the loss function quantifies the prediction divergence:\n$L(s \\oplus x \\oplus s', y) = log p_{tgt}(\\hat{y}|s \\oplus x \\oplus s') - log p_{tgt}(y|s \\oplus x \\oplus s').$\nHere, y represents the original label, $ \\hat{y}$ denotes the highest-confidence incorrect label, and $P_{tgt}(\\cdot|\\cdot)$ indicates the model's prediction probability.\nTo evaluate attack stealthiness comprehensively, we introduce a metric function $\\eta : X \\times S(E) \\times S(E) \\rightarrow [0,1]$ [4]:\n$\\eta(x, s, s') = \\alpha \\cdot \\delta(x, s, s') + (1 - \\alpha) \\cdot \\gamma(|s| + |s'|),$\nwhere $\\delta(x, s, s')$ assesses sentiment consistency across components, and $\\gamma(|\\cdot|)$ implements a length-based penalty:\n$\\gamma(l) = max(0,1 - \\frac{l - l_{min}}{l_{max} - l_{min}}).$\nThis formalization establishes a novel paradigm for adversarial attacks, achieving effective model manipulation while maintaining perfect textual integrity through strategically positioned emoji sequences."}, {"title": "B. Emoji Sequence Space", "content": "To formalize our zero-word-perturbation attack methodology within a rigorous mathematical framework, we construct a comprehensive emoji sequence space encompassing both standard Unicode emoji characters (e.g., grinning-face, fire) and ASCII-based emoticons (e.g., \":)\", \"QaQ\", \";-P\"). This dual-modality representation space enables more expressive adversarial sequence generation while maintaining naturalness in the resultant perturbations. [5] establishes fundamental properties of emoji sequence composition, providing theoretical foundations for our construction.\nLet $E$ denote the finite set of all available emojis. We formally define the emoji sequence space S(E) as:\n$S(E) = \\{s = (e_1, ..., e_l)|e_i \\in E, l_{min} \\leq l \\leq l_{max}\\}.$\nTo ensure emotional consistency in sequence construction, we introduce sentiment-specific subspaces. For each sentiment label $y \\in Y$, we define its corresponding sequence subspace $S_y$ as:\n$S_y = \\{s \\in S(E)|f_{sen}(s) = y\\}.$\nA fundamental property of our constructed sequence space lies in its completeness. Specifically, for any input text $x \\in X$ and target label $y \\in Y$, there exists at least one emoji sequence $s \\in S(E)$ that simultaneously satisfies both emotional consistency and adversarial effectiveness:\n$(f_{sen}(s) = f_{sen}(x)) \\land (f_{tgt}(s \\oplus x \\oplus s') \\neq f_{tgt}(x)).$\nThe existence of such sequences emerges from the rich combinatorial structure of our emoji sequence space. For sequences of length $l$, the space admits $|E|^l$ possible combinations, providing substantial flexibility in sequence construction while maintaining semantic naturalness. This theoretical foundation, supported by the compositional properties established in [5], ensures both the expressiveness and practical applicability of our framework."}, {"title": "C. Emotional Consistency Framework", "content": "A critical component of our zero-word-perturbation attack framework lies in maintaining emotional consistency between the injected emoji sequences and the original text. Recent advances in multimodal emotion analysis [6] demonstrate the significance of such alignment in ensuring attack imperceptibility. We present a rigorous mathematical framework for quantifying and ensuring this crucial consistency property.\nLet $f_{sen}: S(E) \\rightarrow Y$ denote a mapping function that associates emoji sequences with sentiment labels. We formalize the sentiment consistency evaluation through a binary function $\\delta : X \\times S(E) \\times S(E) \\rightarrow \\{0,1\\}$:\n$\\delta(x, s, s') = 1(f_{sen}(s) = f_{sen}(s') = f_{sen}(x)).$\nTo establish a comprehensive evaluation framework for generated sequences, we integrate this consistency measure into a broader stealthiness metric. The resultant function $\\eta : X \\times S(E) \\times S(E) \\rightarrow [0, 1]$ synthesizes emotional alignment with sequence efficiency:\n$\\eta(x, s, s') = \\alpha \\cdot \\delta(x, s, s') + (1 - \\alpha) \\cdot \\gamma(|s| + |s'|),$\nwhere the parameter $\\alpha \\in [0, 1]$ mediates the trade-off between sentiment consistency preservation and sequence length optimization.\nOur framework provides rigorous theoretical guarantees regarding the existence of adversarial sequences that simultaneously achieve attack effectiveness and maintain emotional consistency. For any input text x and an arbitrarily small positive constant $ \\epsilon > 0$:\n$\\exists s, s' \\in S(E): f_{tgt}(s \\oplus x \\oplus s') \\neq f_{tgt}(x) \\land \\eta(x, s, s') \\geq 1 - \\epsilon.$\nThis fundamental guarantee emerges from two key properties: 1 the rich expressiveness of the constructed emoji sequence space, and 2 the careful design of our consistency evaluation mechanism. The guarantee ensures the theoretical feasibility of generating adversarial examples that achieve both attack success and high stealthiness under our comprehensive metric."}, {"title": "III. METHODOLOGY", "content": ""}, {"title": "A. Two-Phase Learning Framework", "content": "The efficient identification of optimal emoji sequences within the vast search space presents significant computational challenges. [7] demonstrates that direct optimization through reinforcement learning often converges to suboptimal solutions due to insufficient semantic prior knowledge. Drawing from advances in hybrid learning approaches [8], we develop a two-phase learning framework that systematically combines supervised pretraining with reinforcement learning optimization.\nThe first phase employs supervised learning, leveraging an auxiliary sentiment analysis model $f_{sen}$ to establish fundamental semantic mappings. The optimization objective is formalized as:\n$L_{sup}(\\theta) = \\frac{1}{D} \\sum_{(x,s) \\in D} \\sum_{t=1}^{l} log \\pi_{\\theta}(s_t|s_{1:t-1}, x),$\nwhere $\\pi_{\\theta}$ denotes the parameterized policy function and D represents the training dataset. This pretraining process establishes robust semantic priors for subsequent optimization stages.\nBuilding upon [9], the second phase implements a comprehensive MDP-based framework. The optimization objective is formalized as:\n$J(\\theta) = E_{x \\sim D} [E_{l \\sim \\rho_l} [E_{s \\sim \\pi_{\\theta}(\\cdot|x,l)} [R(x, s)]]],$\nwhere we design a multi-component reward function balancing attack effectiveness with sequence diversity:\n$R(x, s) = \\alpha R_{atk}(x, s) + \\beta R_{div}(x, l).$\nTo enhance training stability, we incorporate the reward smoothing mechanism proposed by [10]:\n$\\overline{R(x, s)} = \\frac{1}{k} \\sum_{i=t-k+1}^{t} R(x, s).$\nThis framework addresses the challenge of optimal sequence generation through three key innovations: 1 Integration of supervised pretraining for establishing semantic priors 2 Multi-component reward structure incorporating attack effectiveness and sequence diversity 3 Advanced stability enhancement through temporal reward smoothing"}, {"title": "B. Specialized Sequence Generator", "content": "The implementation of efficient zero-word-perturbation adversarial attacks necessitates a specialized generator architecture capable of precisely modeling the semantic relationships between textual content and emoji sequences. Building upon the foundational sequence-to-sequence framework established by [11] and [12], we develop an enhanced architecture incorporating novel components specifically designed for emoji sequence generation.\nA fundamental innovation in our approach lies in the construction of a unified vocabulary space that seamlessly integrates both textual and emoji tokens:\n$V = V_t \\cup V_e,$\nwhere $V_t$ and $V_e$ represent the text and emoji vocabularies, respectively. This unified representation enables coherent processing of both modalities during sequence generation. We enhance the standard attention mechanism through the introduction of a dynamic mask matrix M, modulating information flow between modalities, with $M_{ij} = 0$ for intra-modality attention and $M_{ij} = \\beta$ for cross-modality interactions.\nThe cornerstone of our architectural innovation is the Emoji Logits Processor (ELP), implementing dynamic probability adjustment for token generation:\n$P_{out} = ELP(p_{in}) = soft(Wp_{in} + b).$\nOur optimization framework incorporates multiple essential objectives. The semantic consistency component $L_{sem} = - log P(f_{sen}(s) = f_{sen}(x))$ enforces emotional alignment between generated sequences and input text, while the adversarial effectiveness measure $L_{adv} = -R(x,s)$ quantifies attack success. Following [13], we employ an entropy-based diversity term $L_{div} = -H(\\pi_{\\theta}(\\cdot|x,l))$ to promote sequence variation. These components integrate into a unified objective function:\n$L = L_{sem} + \\lambda_1 L_{adv} + \\lambda_2 L_{div},$\nwhere hyperparameters $\\lambda_1$ and $\\lambda_2$ regulate the balance between competing optimization objectives. This specialized architecture, through its unified vocabulary representation, dynamic attention masking, and novel ELP component, enables effective generation of adversarial emoji sequences while maintaining semantic coherence with the original text content."}, {"title": "IV. EXPERIMENT", "content": "To rigorously evaluate our proposed EmotiAttack framework, we designed a comprehensive experimental protocol utilizing two benchmark datasets: Go Emotion and Tweet Emoji. The Go Emotion dataset comprises emotion-labeled text instances across multiple fine-grained emotional categories, while the Tweet Emoji dataset consists of Twitter posts"}, {"title": "A. Attack Performance Analysis", "content": "Table I presents the performance metrics of our EmotiAttack framework across varying search space configurations. The most notable characteristic is the consistently maintained 0% perturbation rate across all experimental settings, underscoring our framework's fundamental advantage: preserving complete textual integrity of the original content while achieving effective model manipulation.\nOur approach demonstrates remarkably high attack success rates (ASR) across both datasets. For the RoBERTa model on Go Emotion, we achieve ASR values ranging from 79.51% with top-1 constraints to 96.09% with top-30 search space. Similarly, on Tweet Emoji, RoBERTa exhibits vulnerability with ASR values from 71.54% to 95.00%. The BERT model shows substantial susceptibility as well, with ASR values ranging from 73.13% to 87.52% on Go Emotion and from 40.46% to 90.58% on Tweet Emoji.\nThe computational efficiency of our framework is particularly noteworthy, with average processing time per sample consistently below 0.12 seconds across all configurations. Query requirements remain minimal, with average query counts ranging from 1.00 to 8.10, significantly outperforming traditional adversarial attack methods that typically require substantially higher query volumes.\nThe results reveal a clear correlation between search space size and attack performance. As the search space expands from top-1 to top-30, we observe monotonic improvements in ASR across all experimental configurations. This scalable configurability enables flexible adjustment based on specific operational requirements. For instance, on the Tweet Emoji dataset with the BERT model, expanding from top-1 to top-30 increases ASR from 40.46% to 90.58%, while only modestly increasing the processing time from 0.06 to 0.09 seconds and queries from 1.00 to 6.62.\nInterestingly, RoBERTa consistently exhibits higher vulnerability compared to BERT, particularly at smaller search space configurations. This differential susceptibility could be attributable to their distinct pre-training objectives and contextual encoding mechanisms."}, {"title": "B. Performance Evaluation on State-of-the-Art Large Language Models", "content": "While traditional NLP models represent a significant application domain for adversarial attacks, recent advancements in large language models (LLMs) have fundamentally transformed the natural language processing landscape. To comprehensively evaluate our EmotiAttack framework's generalizability,"}, {"title": "V. CONCLUSION", "content": "In this paper, we introduce Emoti-Attack, a Zero-Perturbation Adversarial Attacks on NLP Systems via Emoji Sequences. It can attack the small model and LLMs"}]}