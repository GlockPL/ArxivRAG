{"title": "TwinLab: a framework for data-efficient training of non-intrusive reduced-order models for digital twins", "authors": ["Maximilian Kannapinn", "Michael Sch\u00e4fer", "Oliver Weeger"], "abstract": "Simulation-based digital twins represent an effort to provide high-accuracy real-time insights into operational physical processes. However, the computation time of many multi-physical simulation models is far from real-time. It might even exceed sensible time frames to produce sufficient data for training data-driven reduced-order models. This study presents TwinLab, a framework for data-efficient, yet accurate training of neural-ODE type reduced-order models with only two data sets.\nCorrelations between test errors of reduced-order models and distinct features of corresponding training data are investigated. Having found the single best data sets for training, a second data set is sought with the help of similarity and error measures to enrich the training process effectively.\nAdding a suitable second training data set in the training process reduces the test error by up to 49% compared to the best base reduced-order model trained only with one data set. Such a second training data set should at least yield a good reduced-order model on its own and exhibit higher levels of dissimilarity to the base training data set regarding the respective excitation signal. Moreover, the base reduced-order model should have elevated test errors on the second data set. The relative error of the time series ranges from 0.18% to 0.49%. Prediction speed-ups of up to a factor of 36,000 are observed.\nThe proposed computational framework facilitates the automated, data-efficient extraction of non-intrusive reduced-order models for digital twins from existing simulation models, independent of the simulation software.", "sections": [{"title": "1 Introduction", "content": "Digital twins that are derived from multi-physical simulation data have the potential to drive a transformative shift towards more autonomous processes. To date, simulation and data science technologies are employed by experts in the design phase, primarily in the context of what-if simulations to design and optimize products or processes. Digital twins represent an initiative of integrating insights from data and simulations into operational processes, empowering process control algorithms to perform informed, autonomous decision-making and correcting the current operational conditions. Since its inception a decade ago by Grieves and Vickers, substantial efforts have been invested in shaping and standardizing the digital twin concept also establishing it as a dedicated research domain . In essence, a digital twin can be described as a virtual repository of information that mirrors its physical counterpart with the highest fidelity. This digital-physical symbiosis thrives on real-time, bidirectional data exchange, spanning the entire lifecycle of a process or product. Historically, approximately 85% of digital twin research has gravitated towards product life-cycle management. As the second largest area, 11% focused on factory or production planning . Nevertheless, digital twins have the potential to also serve as catalysts for the development of autonomous processes, ushering in a novel dimension in digital twin research and technology.\nNiederer et al. (2021) assert the necessity of developing innovative mathematical, numerical, and computational methodologies to effectively implement digital twins on a large scale. As a crucial step in this direction, the present work introduces a methodology for deriving digital twins from multi-physical simulation models. To ensure the faithful replication of their physical counterparts, digital twins should ideally rely on physics-based simulation models, given the advancements in computational engineering regarding accuracy and efficiency. First-principle models, rooted in a deep understanding of cause-and-effect relationships, enable precise process control. Integrating highly accurate simulation models with data-driven reduced-order modeling may be encapsulated in the term \"physics-based, data-driven digital twin\"\nRequiring digital twins to replicate their physical counterparts in real time poses an enormous challenge. Despite the significant growth in computing power, real-time simulation of large-scale industrial problems will remain unattainable in the coming years, and possibly even decades. Recognizing this dilemma since the 1990s, scientific computing and computational engineering researchers have identified reduced-order models (ROMs), also known as surrogates, as a promising solution. Data-driven ROMs exclusively rely on the output data of the simulation model. These ROMs align well with the practicalities of model development within a diverse software landscape of open-source, commercial, or custom codes. Access to solvers in commercial simulation software is often limited, and even when the source code is available, intrusive ROMs may be considered excessively time-consuming . Still, certain machine-learning ROM methods demand extensive training data. Here, we face a novel dilemma of attempting online prediction speed increases with ROMs at the price of long offline waiting times for data. When employing time-consuming simulations, data production might exceed sensible time frames, especially for industrial product development cycles. This study introduces an approach to carefully selecting very few training data sets. An efficient design of experiments is proposed to guide the selection of two training data sets, ensuring minimal test error for ROMs when applied to representative test data.\nNiederer et al. (2021) emphasized that the creation and dissemination of digital twins require open-source platforms to enable large-scale implementation. Some methods of the here-presented open-source software TwinLab were developed as part of earlier research work"}, {"title": "2 Related work", "content": ""}, {"title": "2.1 Training of transient reduced-order models", "content": "As the introduction highlights, the digital twin methodology must be accompanied by reduced-order modeling approaches capable of replicating multi-physical problems within a reasonable simulation time and with lean computational cost. Given the diversity in how the problem is tackled across different disciplines, various perspectives exist on ROMs. A common classification criterion is the intrusiveness of the ROM generation method . In this context, an intrusive approach involves accessing and modifying the underlying set of PDEs. Conversely, methods that refrain from modifying the underlying equations are labeled as non-intrusive . In computational engineering, PDE-centric, intrusive approaches utilize the formulated PDE of the physical model as a starting point. After numerical discretization in space and time, the resulting equation is projected onto a reduced-order space, aiming for an accelerated solution to the problem . However, commercial simulation software companies safeguard their solution algorithms as intellectual property, denying users root-level access to these algorithms. Consequently, intrusive ROM methods are not a feasible option. Therefore, this work resorts to non-intrusive ROM methods to maintain universality across employed simulation software.\nWithin the discipline of machine learning, time series can be replicated with recurrent neural networks (RNNs) . Neural networks are known to be highly flexible, universal, nonlinear function approximators . RNNs, more specifically, long short-term memory neural networks (LSTMs), are characterized by recursive calls of a neural network with a discrete temporal delay to invoke the progress of the variables in time. Through the back-propagation through time algorithm, the neural network is trained to replicate input-to-output relations over time. Exemplary implementations of RNNs and discussions on overcoming difficulties with learning long-term dependencies can be found in .\nRecently, there has been a growing interest in hybrid approaches that meld the concept of identifying system dynamics with machine-learning techniques. Dupont et al. (2019) elucidate the resemblance between deep feed-forward neural networks, specifically residual networks , and differential equations. The mapping of a hidden state $h_t \\in \\mathbb{R}^d$ at layer t to its next layer is expressed as\n$$h_{t+1} = h_t + f_t(h_t),$$\nwhere $f_t: \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ is a differentiable function projecting from one hidden state to the next. By forming a difference quotient and taking the limit of an imaginary time step, the similarity to an ODE system"}, {"title": "", "content": "becomes evident:\n$$\\lim_{\\Delta t \\rightarrow 0} \\frac{h_{t+\\Delta t} - h_t}{\\Delta t} = \\frac{dh(t)}{dt} = f(h(t), t) .$$\nAn input is transformed to the output by solving an ODE over multiple time steps, with a feed-forward neural network representing the right-hand side operator $f$ of the ODE. This approach is termed neural ODE. Recent publications suggest that neural ODEs outperform RNNs or tree-based algorithms, as evidenced in for predicting pharmacokinetics or in for predicting the remaining state of health of batteries. The success of neural ODEs is attributed to their ability to learn underlying dynamics rather than merely input-to-output relations .\nThe software package ANSYS Dynamic ROM Builder (ANSYS Inc., 2020), referred to as DynROM hereafter, offers a non-intrusive, nonlinear, transient ROM using a method akin to neural ODEs. Given the promising results observed in preliminary testing and recognizing the need for in-depth investigations of the method in the literature, the decision was made to incorporate DynROM in this work."}, {"title": "2.2 Design of experiments for data-driven ROMs", "content": "System identification is a type of reduced-order modeling that relies on tailored excitation signals to extract valuable information from physical models during either virtual or real experiments. These excitation signals enable the determination of the transient behavior of the physical system based on the recorded output data . There are a few notable works that cover the design of excitation signals for this purpose. However, most research primarily guides designing experiments focused on nonlinear auto-regressive models with exogenous inputs (NARX), RNNs or proper orthogonal decomposition (POD) ROMs . No published work has been found that specifically addresses the generation and selection of training data for data-driven reduced-order modeling in the context of food models. In general, the selection of data sets for training reduced-order models of the neural ODE-type to achieve low test errors remains largely unreported. Since the introduction of the ROM method in 2018, some works have employed this approach . However, these works have not significantly focused on selecting training data. To address these gaps, previous research proposed an efficient design of experiments using a single data set. Moreover, it highlighted that the conventional suggestion to uniformly cover the input space of models or their output space to ensure effective training of reduced-order models does not apply to the specific food model presented. An input and output space coverage measure for the food model was implemented, and it was observed that high coverages did not correlate with low test errors."}, {"title": "2.3 Digital twins and reduced-order modeling in food science", "content": "Recent review articles have underscored the potential of digital twins in food science and technology . Notably, Henrichs et al. (2022) focused their review on the food value chain and shop floor production planning. Their assessment shows few studies have delved into process autonomy within this domain. Out of 84 pieces of research, only eight have centered on digital twin-enabled process autonomy, and a mere two were peer-reviewed studies. In the post-harvest sector, several studies view digital twins as simulation models for processing temperature profiles of physical counterparts that have been pre-recorded . Concepts for establishing bi-directional linkages, interactive decision-making, and comprehensive product life cycle mirroring still need to be explored. The demand for real-time simulations, as recently envisioned , is becoming increasingly apparent."}, {"title": "3 Methods", "content": ""}, {"title": "3.1 The digital twin use case of autonomous thermal food processing", "content": "Imagine a convection oven that is to carry out autonomous thermal food processing. Evaluating food quality measures like core temperature, moisture content, or texture poses a challenge for the cooking appliance without additional equipment. Here, simple measurements of the current oven temperature can serve as the initial condition for predictions of the digital twin. A control algorithm can employ the control vector parameterization approach to adjust the amplitudes of a hypothetical oven temperature trajectory systematically. With faster-than-real-time solution times, the underlying ROM swiftly predicts the food's temperature trajectories of those multiple future scenarios. This enables the algorithm to understand the model's sensitivities concerning a target function that includes desired cooking objectives. The optimization problem is solved at discrete points during operation to determine the optimal trajectory of oven temperatures to achieve the objectives. Repetitive comparisons between the actual oven temperature and the planned trajectory help mitigate the model-process mismatch by recalculating the residual trajectory. This capability enables the device to plan its oven temperature trajectory to meet specific user requirements, such as achieving the desired moisture content, safe core temperatures, or texture at the end of the cooking process to a particular point in time.\nA soft-matter model for chicken meat has been implemented for this case study. This model consists of two coupled transport equations for heat transfer"}, {"title": "3.2 The TwinLab framework for reduced-order model derivation", "content": "The central building block of the framework forms the data-driven derivation of a reduced-order model from the multi-physical simulation data following a procedure illustrated in Fig. 1:\n\u2022 Step 1: Synthesized excitation signals are employed to vary the oven temperature in the simulations.\nPossible excitations include amplitude-modulated pseudo-random binary sequences (APRBS) or multi-sines, both suitable for nonlinear system identification . Previous research has demonstrated the suitability of APRBS signals .\nA basis of simulation data sets is automatically generated with different excitation signals and is stored in the data set storage.\nEach data set comprises a unique combination of excitation signal and simulation output data, as visualized in the left column of Fig. 1\n\u2022 Step 2: Feature calculation of all data sets.\nSeveral data set features are derived in the hope that at least one feature will correlate with low ROM test errors. For a complete list of features, refer to\nExamples of possible features include the standard deviation and mean value of $T_{oven}$, $T_A$ and $T_B$ or the signed mean of the delta jumps of an APRBS signal $\\bar{i}_{diff,j} := mean([T_{diff,2}, T_{diff,3}, T_{diff,4}])$.\n\u2022 Step 3: Representative data sets are selected with a X2 test to serve exclusively for ROM testing on unseen data.\n\u2022 Step 4: One data set at a time is used to train a corresponding ROM. Consecutively, average test errors are calculated for each ROM.\n\u2022 Step 5: The Pearson correlation matrix reveals correlations between training data set features and the corresponding ROM's prediction errors.\nThese identified correlations help to determine the best single data set. Additionally, the knowledge of decisive features that correlate with low test errors for a particular ROM enables the synthesis of even better data sets. Sec. 4.1 provides further details on this step."}, {"title": "3.3 Model order reduction with neural-ODE-type ROMs", "content": "The procedure to infer a ROM for the thermal food processing use case, as illustrated in Fig. 1, can be outlined as follows: The thermal food processing model, referred to as the full-order model, is automatically simulated through command line calls within TwinLab. The discretized oven temperatures $\\hat{G} = [T_{oven,1}, ..., T_{oven,N}]$ serve as external excitations for the full-order model over time, where $N$ represents the total number of time steps. Virtual probes read temperatures, such as a core and surface temperature $T_{A,k}$ and $T_{B,k}$ at discrete points in space and time, storing them in an array $\\hat{Y} \\in \\mathbb{R}^{n \\times N}$ (n = 2 in this case). Subsequently, the ROM is excited similarly by the oven temperatures $\\hat{G}$ at its input. During a training phase, the ROM's parameters are optimized to ensure that the discretized ROM output $\\hat{X}$ replicates the full-order model output: $\\hat{Y} \\approx \\hat{Y}$. The input $G = G(t)$ is mapped to the output $X = X(t)$ by solving the ODE:\n$$\\frac{\\partial}{\\partial t} \\left[\\begin{array}{c} X \\\\ I \\end{array}\\right] = f\\left[\\begin{array}{c} X \\\\ I \\end{array}, G, X_0\\right],$$\n$$X(t = 0) = X_0,$$ \nwhere the state vector $X \\in \\mathbb{R}^n$ is extended by a vector of additional variables $I \\in \\mathbb{R}^i$ \u2013 introduced later \u2013 and $X_0 \\in \\mathbb{R}^{n+i}$ contains the initial conditions for both.\nA three-layer feed-forward neural network represents the right-hand side operator $f$. The neural network comprises an input layer $x \\in \\mathbb{R}^{n+i+1}$, a hidden layer $h \\in \\mathbb{R}^{n+i}$, and an output layer $z \\in \\mathbb{R}^{n+i}$, as depicted in the bottom left of Fig. 1. Linear transformations of the input layer x with weight matrices $W_1$ and the addition of a bias vector $b_1$ constitute the fundamental operations to compute the values of the neurons in the hidden layer h. Applying a sigmoid activation function $\\Im$ introduces nonlinearity to the relationship between the layers . This process is repeated for the output layer, involving different weights and biases, leading to the computation from the input to the output layer:\n$$h = \\Im(W_1x + b_1),$$\n$$z = \\Im(W_2h + b_2).$$ \nTo establish the desired relationship between the input and output layers, training data with known input $\\hat{G}$ and output $\\hat{Y}$ is presented to the neural network. This process, known as supervised learning, involves implementing fourth-order Runge-Kutta schemes in DynROM to numerically integrate the ODE system over time. The loss function for evaluating the neural network's training error is the mean squared error:\n$$E_{mse} = \\frac{1}{n} \\sum_{j=1}^n \\frac{1}{N} \\sum_{k=1}^N (\\hat{Y}_{j,k} - \\hat{Y}_{j,k})^2,$$\nthat is averaged across all learning scenarios. To minimize this loss function, gradient descent algorithms are employed in conjunction with the back-propagation algorithm . This optimization process aims to determine optimal values for the weights and biases in all layers of the neural network, ensuring that the discrete output $\\hat{Y}$ obtained from numerically integrating Eq. (3) closely replicates the output of the full-order model $\\hat{Y}$."}, {"title": "4 Case study on training data selection", "content": ""}, {"title": "4.1 Correlation-based training data selection", "content": "This section summarizes prior research , laying the foundation for the case study on combinations of training data sets presented in this work. The framework's procedure, as introduced in Sec. 3.2, is followed until step 5 to identify correlations between global error measures and training data features. Fifty-five amplitude-modulated pseudo-random binary sequence (APRBS) excitation signals are employed in the full-order simulation model to vary $T_{oven}$ and generate readouts as defined in Sec. 3.3. Each data set is uniquely identified with an alphanumeric identifier based on consecutive numbering in TwinLab. After step 3, 15 data sets (referred to as AP15 hereafter) are automatically selected for testing. Subsequently, all data sets are individually used to train a 1-data-set ROM. The median of all training errors, i.e., the root-mean-square of the difference between ROM and the corresponding training data set output, is 0.22 K, with a standard deviation of 0.14K. Six global error measures are derived from testing the ROMs, including the average over all test data (indicated by $\\langle \\cdot \\rangle$) for the root-mean-square error $E_{rms}$, the mean absolute percentage error $E_{map}$, the maximum error $E_{max}$, the median error $E_{med}$, the interquartile range $E_{iqr}$, and the coefficient of determination $R^2$ (see the first column in Tab. 1 for the corresponding variables).\nThe Pearson correlation coefficient $R$ between training error and global test measures is 0.096, indicating hardly any correlation. Consequently, the training error does not significantly impact the study outcomes concerning the global error measures of the ROMs. This lack of correlation is relevant for maintaining a neutral study design. Moving forward, the Pearson correlation matrix will unveil correlations between"}, {"title": "4.2 Combination of training data sets", "content": "Upon identifying the promising training data set 745, the next step is determining a suitable second training partner data set to enhance ROM training accuracy. In this section, we propose a procedure for selecting training partner data sets based on a comparison with the good 1-data-set ROM trained on data set 745. To evaluate the plausibility of various training partner selection routines, we utilize Fig. 3, which incorporates several decision support tools. Figure 3a is a modified version of Fig. 2a, where the considered data sets are labeled and color-coded. Figure 3c presents $E_{rms}$ (bars) and the distribution of $E_{rms}$ (boxplots) on AP15 for the 2-data-set ROMs, whereas Tab. 2 lists all global test errors. The subsequent paragraphs introduce hypotheses on how to identify a suitable training partner data set, with consistent naming of the boxes and corresponding color coding in Fig. 3. All discussed data sets are visualized in Fig. 4.\nVery similar to 745, best performance on AP15, low ROM745 error: One approach involves selecting a dataset as a training partner if it has proven effective for training a 1-data-set ROM. The similarity between an excitation signal j and APRBS 745 is quantified by $rms(T_{oven, 745} - T_{oven,j})$, represented as the ordinate in Fig. 3b. Another criterion for training partner selection is the test error of the 1-data-set ROM745 on each individual APRBS dataset, plotted as the abscissa in Fig. 3b. Notably, the 1-data-set ROM745 exhibits low test errors on the 625 test set. This aligns with the similarity between data set 625 (color-coded in black) and data set 745 regarding oven temperatures, evident from the low y-position in Fig. 3b. Additionally, the 1-data-set ROM625 performs exceptionally well on AP15, as shown in Fig. 3a. However, selecting data sets 745 and 625 as training partners to form ROM745+625 proves disadvantageous, leading to an increase in test errors instead of a decrease, as evident in the black section of Fig. 3c. Data set 625 does not contribute new information to the training process; instead, it may induce overfitting of the ROM to the specific, similar operational conditions of data sets 745 and 625.\nSomewhat similar to 745, best performance on AP15, medium ROM745 error: Despite the example above, combining training data sets that independently train accurate 1-data-set ROMs remains a plausible"}, {"title": "4.3 Discussion of the proposed design of experiments", "content": "The proposed efficient design of experiments addresses the challenge of selecting a limited number of appropriate training data sets for accurate ROMs. This is achieved by correlating training data features with error measures of 1-data-set ROMs, allowing for the identification or generation of optimal training data.\nWhen choosing a suitable excitation signal type, it is crucial to differentiate between ROM training, testing, and operation. In this study, ROM training proves to be most successful with data sets excited by APRBS. However, it is worth noting that a real convection oven may not replicate the instantaneous jumps in oven temperature characteristic of APRBS signals. This observation challenges conventional recommendations in the literature, which often advocate using signal types closely resembling operational signals for training purposes.\nTesting must be conducted on objective test data sets to ensure the robustness and generalization of the reduced-order models. Randomly generating or selecting training data, as seen in methods like k-fold cross-validation, might not uniformly represent the output space at all positions, as discussed in detail in . One can employ methods such as selecting data sets based on a x2 test to address this issue and achieve a more balanced representation of the output space. This ensures, for example, a uniform distribution of the medians of the output variable. Fair test groups are essential for uncovering correlations between data set features and error measures. Additionally, testing can be performed using different excitation signal classes. While APRBS with sinusoidal transitions (sinAPRBS) most accurately"}, {"title": "5 Conclusion", "content": "The research presented in this work underscores the potency of data-driven reduced-order modeling in crafting precise and efficient digital twins from simulation data. The methodology is agnostic to modeling software, offering versatility across various platforms. Remarkably, data-driven ROMs of high accuracy and speed-up can be developed with just two training data sets. The proposed efficient design of experiments facilitates the selection of suitable training data sets. The ROMs introduced in this work exhibit substantial speed-ups compared to real time, achieving approximately $S_p \\approx 3.6 \\times 10^4$ with characteristic solution times of one-tenth of a second without imposing a noticeable computational cost on one core of a processor. Beyond the notable speed-up, the data-driven ROMs demonstrate excellent test accuracy across large and representative test data groups. Specifically, root-mean-square errors for the best point data ROM range from 0.30 K to 0.74 K, varying depending on the test group. The fusion of high accuracy with computing efficiency enables the digital twin to run directly on the control logic of the physical process, eliminating the need for edge or cloud computing."}, {"title": "5.1 Contributions", "content": "The TwinLab framework is a valuable tool for automating the generation of data-driven ROMs from simulation data. It streamlines excitation signal generation, test group selection based on x2 tests, determination of data set features, and calculation of global test errors. The framework also incorporates code for batch simulations of full-order models in COMSOL Multiphysics or ANSYS Fluent, effectively reducing data handling and user-interaction efforts. The proposed design of experiments may help less experienced users select appropriate training data for reduced-order modeling."}, {"title": "5.2 Outlook", "content": "This study illustrates the practical application of physics-based, data-driven digital twins in thermal food processing. In a broader context, the digital twin framework contributes to a paradigm shift in the perspective on digital twins by emphasizing process autonomy. In the first future steps, the framework's versatility may extend to other domains where the application aligns closely with physics principles. For instance, the optimal operation of fuel cells, which involves modeling porous media, fluid flow, heat exchange, and additional reaction kinetics, is strongly correlated to physics. Anticipated developments in mobile applications, particularly within the transportation sector, could leverage digital twins to intelligently control and achieve optimal operating conditions for fuel cells. Moreover, future work with the proposed framework may extend beyond applications within the presented physics and can extend its contribution to various fields with different physical characteristics. For example, in additive manufacturing , where the physical process parameters are decisive for end quality, the framework can enhance process autonomy by providing physics-based, data-driven digital twins."}]}