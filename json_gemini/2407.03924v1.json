{"title": "TwinLab: a framework for data-efficient training of non-intrusive reduced-order models for digital twins", "authors": ["Maximilian Kannapinn", "Michael Sch\u00e4fer", "Oliver Weeger"], "abstract": "Simulation-based digital twins represent an effort to provide high-accuracy real-time insights into operational physical processes. However, the computation time of many multi-physical simulation models is far from real-time. It might even exceed sensible time frames to produce sufficient data for training data-driven reduced-order models. This study presents TwinLab, a framework for data-efficient, yet accurate training of neural-ODE type reduced-order models with only two data sets. Correlations between test errors of reduced-order models and distinct features of corresponding training data are investigated. Having found the single best data sets for training, a second data set is sought with the help of similarity and error measures to enrich the training process effectively. Adding a suitable second training data set in the training process reduces the test error by up to 49% compared to the best base reduced-order model trained only with one data set. Such a second training data set should at least yield a good reduced-order model on its own and exhibit higher levels of dissimilarity to the base training data set regarding the respective excitation signal. Moreover, the base reduced-order model should have elevated test errors on the second data set. The relative error of the time series ranges from 0.18% to 0.49%. Prediction speed-ups of up to a factor of 36,000 are observed. The proposed computational framework facilitates the automated, data-efficient extraction of non-intrusive reduced-order models for digital twins from existing simulation models, independent of the simulation software.", "sections": [{"title": "Introduction", "content": "Digital twins that are derived from multi-physical simulation data have the potential to drive a transformative shift towards more autonomous processes. To date, simulation and data science technologies are employed by experts in the design phase, primarily in the context of what-if simulations to design and optimize products or processes. Digital twins represent an initiative of integrating insights from data and simulations into operational processes, empowering process control algorithms to perform informed, autonomous decision-making and correcting the current operational conditions. Since its inception a decade ago by Grieves and Vickers, substantial efforts have been invested in shaping and standardizing the digital twin concept, also establishing it as a dedicated research domain. In essence, a digital twin can be described as a virtual repository of information that mirrors its physical counterpart with the highest fidelity. This digital-physical symbiosis thrives on real-time, bidirectional data exchange, spanning the entire lifecycle of a process or product. Historically, approximately 85% of digital twin research has gravitated towards product life-cycle management. As the second largest area, 11% focused on factory or production planning. Nevertheless, digital twins have the potential to also serve as catalysts for the development of autonomous processes, ushering in a novel dimension in digital twin research and technology. Niederer et al. (2021) assert the necessity of developing innovative mathematical, numerical, and computational methodologies to effectively implement digital twins on a large scale. As a crucial step in this direction, the present work introduces a methodology for deriving digital twins from multi-physical simulation models. To ensure the faithful replication of their physical counterparts, digital twins should ideally rely on physics-based simulation models, given the advancements in computational engineering regarding accuracy and efficiency. First-principle models, rooted in a deep understanding of cause-and-effect relationships, enable precise process control. Integrating highly accurate simulation models with data-driven reduced-order modeling may be encapsulated in the term \"physics-based, data-driven digital twin\". Requiring digital twins to replicate their physical counterparts in real time poses an enormous challenge. Despite the significant growth in computing power, real-time simulation of large-scale industrial problems will remain unattainable in the coming years, and possibly even decades. Recognizing this dilemma since the 1990s, scientific computing and computational engineering researchers have identified reduced-order models (ROMs), also known as surrogates, as a promising solution. Data-driven ROMs exclusively rely on the output data of the simulation model. These ROMs align well with the practicalities of model development within a diverse software landscape of open-source, commercial, or custom codes. Access to solvers in commercial simulation software is often limited, and even when the source code is available, intrusive ROMs may be considered excessively time-consuming. Still, certain machine-learning ROM methods demand extensive training data. Here, we face a novel dilemma of attempting online prediction speed increases with ROMs at the price of long offline waiting times for data. When employing time-consuming simulations, data production might exceed sensible time frames, especially for industrial product development cycles. This study introduces an approach to carefully selecting very few training data sets. An efficient design of experiments is proposed to guide the selection of two training data sets, ensuring minimal test error for ROMs when applied to representative test data. Niederer et al. (2021) emphasized that the creation and dissemination of digital twins require open-source platforms to enable large-scale implementation. Some methods of the here-presented open-source software TwinLab were developed as part of earlier research work."}, {"title": "Related work", "content": "As the introduction highlights, the digital twin methodology must be accompanied by reduced-order modeling approaches capable of replicating multi-physical problems within a reasonable simulation time and with lean computational cost. Given the diversity in how the problem is tackled across different disciplines, various perspectives exist on ROMs. A common classification criterion is the intrusiveness of the ROM generation method. In this context, an intrusive approach involves accessing and modifying the underlying set of PDEs. Conversely, methods that refrain from modifying the underlying equations are labeled as non-intrusive. In computational engineering, PDE-centric, intrusive approaches utilize the formulated PDE of the physical model as a starting point. After numerical discretization in space and time, the resulting equation is projected onto a reduced-order space, aiming for an accelerated solution to the problem. However, commercial simulation software companies safeguard their solution algorithms as intellectual property, denying users root-level access to these algorithms. Consequently, intrusive ROM methods are not a feasible option. Therefore, this work resorts to non-intrusive ROM methods to maintain universality across employed simulation software. Within the discipline of machine learning, time series can be replicated with recurrent neural networks (RNNs). Neural networks are known to be highly flexible, universal, nonlinear function approximators. RNNs, more specifically, long short-term memory neural networks (LSTMs), are characterized by recursive calls of a neural network with a discrete temporal delay to invoke the progress of the variables in time. Through the back-propagation through time algorithm, the neural network is trained to replicate input-to-output relations over time. Exemplary implementations of RNNs and discussions on overcoming difficulties with learning long-term dependencies can be found in . Recently, there has been a growing interest in hybrid approaches that meld the concept of identifying system dynamics with machine-learning techniques. Dupont et al. (2019) elucidate the resemblance between deep feed-forward neural networks, specifically residual networks, and differential equations. The mapping of a hidden state ht \u2208 Rd at layer t to its next layer is expressed as\n\nht+1 = ht + ft(ht),\n\nwhere ft: Rd \u2192 Rd is a differentiable function projecting from one hidden state to the next. By forming a difference quotient and taking the limit of an imaginary time step, the similarity to an ODE system becomes evident:\n\nlim At\u21920 ht+At - ht / At = dh(t) / dt = f(h(t), t) .\n\nAn input is transformed to the output by solving an ODE over multiple time steps, with a feed-forward neural network representing the right-hand side operator f of the ODE. This approach is termed neural ODE. Recent publications suggest that neural ODEs outperform RNNs or tree-based algorithms, as evidenced in for predicting pharmacokinetics or in for predicting the remaining state of health of batteries. The success of neural ODEs is attributed to their ability to learn underlying dynamics rather than merely input-to-output relations. The software package ANSYS Dynamic ROM Builder, referred to as DynROM hereafter, offers a non-intrusive, nonlinear, transient ROM using a method akin to neural ODEs. Given the promising results observed in preliminary testing and recognizing the need for in-depth investigations of the method in the literature, the decision was made to incorporate DynROM in this work."}, {"title": "Design of experiments for data-driven ROMs", "content": "System identification is a type of reduced-order modeling that relies on tailored excitation signals to extract valuable information from physical models during either virtual or real experiments. These excitation signals enable the determination of the transient behavior of the physical system based on the recorded output data. There are a few notable works that cover the design of excitation signals for this purpose . However, most research primarily guides designing experiments focused on nonlinear auto-regressive models with exogenous inputs (NARX), RNNs or proper orthogonal decomposition (POD) ROMs. No published work has been found that specifically addresses the generation and selection of training data for data-driven reduced-order modeling in the context of food models. In general, the selection of data sets for training reduced-order models of the neural ODE-type to achieve low test errors remains largely unreported. Since the introduction of the ROM method in 2018, some works have employed this approach. However, these works have not significantly focused on selecting training data. To address these gaps, previous research proposed an efficient design of experiments using a single data set. Moreover, it highlighted that the conventional suggestion to uniformly cover the input space of models or their output space to ensure effective training of reduced-order models does not apply to the specific food model presented. An input and output space coverage measure for the food model was implemented, and it was observed that high coverages did not correlate with low test errors."}, {"title": "Digital twins and reduced-order modeling in food science", "content": "Recent review articles have underscored the potential of digital twins in food science and technology. Notably, Henrichs et al. (2022) focused their review on the food value chain and shop floor production planning. Their assessment shows few studies have delved into process autonomy within this domain. Out of 84 pieces of research, only eight have centered on digital twin-enabled process autonomy, and a mere two were peer-reviewed studies. In the post-harvest sector, several studies view digital twins as simulation models for processing temperature profiles of physical counterparts that have been pre-recorded. Concepts for establishing bi-directional linkages, interactive decision-making, and comprehensive product life cycle mirroring still need to be explored. The demand for real-time simulations, as recently envisioned , is becoming increasingly apparent."}, {"title": "Methods", "content": "Imagine a convection oven that is to carry out autonomous thermal food processing. Evaluating food quality measures like core temperature, moisture content, or texture poses a challenge for the cooking appliance without additional equipment. Here, simple measurements of the current oven temperature can serve as the initial condition for predictions of the digital twin. A control algorithm can employ the control vector parameterization approach to adjust the amplitudes of a hypothetical oven temperature trajectory systematically. With faster-than-real-time solution times, the underlying ROM swiftly predicts the food's temperature trajectories of those multiple future scenarios. This enables the algorithm to understand the model's sensitivities concerning a target function that includes desired cooking objectives. The optimization problem is solved at discrete points during operation to determine the optimal trajectory of oven temperatures to achieve the objectives. Repetitive comparisons between the actual oven temperature and the planned trajectory help mitigate the model-process mismatch by recalculating the residual trajectory. This capability enables the device to plan its oven temperature trajectory to meet specific user requirements, such as achieving the desired moisture content, safe core temperatures, or texture at the end of the cooking process to a particular point in time.\nA soft-matter model for chicken meat has been implemented for this case study. This model consists of two coupled transport equations for heat transfer (temperature T) and moisture transport (moisture content on a dry basis Mab). The convective heat influx from the oven is introduced using a heat transfer coefficient \u03b1 within a mixed boundary condition in conjunction with the prescribed Toven. For more extensive implementation details, sensitivities of model parameters, and validation tests, refer to previous research. This case study focuses on replicating temperature probes at specific points of the full-order model, as illustrated in Fig. 1. By examining point data reduced-order models, this study establishes a condensed and minimalistic setup that allows for a thorough investigation of the effects of training data selection. In line with best practices , only temperatures are chosen for training to eliminate the influence of different physics and scales within a single reduced-order model. Specifically, core temperatures TA and surface temperature TB are selected, representing the potential temperature trajectories within the model. Core temperatures are commonly monitored to assess the degree of cooking. For example, the U.S. Food and Drug Administration mandates holding times of at least one second at temperatures above 74\u00b0C . Surface temperatures can be significant for assessing the temperature-dependent browning of the food."}, {"title": "The TwinLab framework for reduced-order model derivation", "content": "The central building block of the framework forms the data-driven derivation of a reduced-order model from the multi-physical simulation data following a procedure illustrated in Fig. 1:\n\u2022 Step 1: Synthesized excitation signals are employed to vary the oven temperature in the simulations.\n\nPossible excitations include amplitude-modulated pseudo-random binary sequences (APRBS) or multi-sines, both suitable for nonlinear system identification. Previous research has demonstrated the suitability of APRBS signals .\nA basis of simulation data sets is automatically generated with different excitation signals and is stored in the data set storage.\nEach data set comprises a unique combination of excitation signal and simulation output data, as visualized in the left column of Fig. 1\n\u2022 Step 2: Feature calculation of all data sets.\n\nSeveral data set features are derived in the hope that at least one feature will correlate with low ROM test errors. For a complete list of features, refer to .\nExamples of possible features include the standard deviation and mean value of Toven, TA and TB or the signed mean of the delta jumps of an APRBS signal \u012bdiff,j := mean([Tdiff,2, Tdiff,3, Tdiff, 4]).\n\u2022 Step 3: Representative data sets are selected with a X2 test to serve exclusively for ROM testing on unseen data.\n\u2022 Step 4: One data set at a time is used to train a corresponding ROM. Consecutively, average test errors are calculated for each ROM.\n\u2022 Step 5: The Pearson correlation matrix reveals correlations between training data set features and the corresponding ROM's prediction errors.\n\nThese identified correlations help to determine the best single data set. Additionally, the knowledge of decisive features that correlate with low test errors for a particular ROM enables the synthesis of even better data sets. Sec. 4.1 provides further details on this step.\n\u2022 Step 6: The training partner chart aids with selecting appropriate combinations of two data sets to improve ROM training and further reduce prediction errors. Sec. 4.2 focusses on this step.\n\u2022 Step 7: Two final data sets can be determined with the knowledge of correlations and promising data set combinations.\n\u2022 Step 8: From this data, the final ROM is trained and exported in the functional mockup unit (FMU) format . The computational time required for simulations of realistic-sized multi-physical problems can extend over weeks or even months when executed on modern cluster PCs. Consequently, TwinLab uses a forerunner concept: rather than conducting the time-intensive correlation search using the final full-order model, a priori correlation sampling is undertaken on a reduced-size simulation model, such as the one outlined in this study. Typically, such preliminary models emerge as a natural outcome during the development of the simulation model. In computational engineering, it is uncommon to directly formulate a new simulation model encompassing all physical couplings and integrating detailed geometry of the problem employing a fine mesh. Instead, the validation of individual components of the model"}, {"title": "Model order reduction with neural-ODE-type ROMs", "content": "The procedure to infer a ROM for the thermal food processing use case, as illustrated in Fig. 1, can be outlined as follows: The thermal food processing model, referred to as the full-order model, is au- tomatically simulated through command line calls within TwinLab. The discretized oven temperatures \u011c = [Toven,1,..., Toven,N] serve as external excitations for the full-order model over time, where N represents the total number of time steps. Virtual probes read temperatures, such as a core and surface temperature TA,k and TB,k at discrete points in space and time, storing them in an array \u0176 \u2208 Rn\u00d7N (n = 2 in this case). Subsequently, the ROM is excited similarly by the oven temperatures \u011c at its input. During a training phase, the ROM's parameters are optimized to ensure that the discretized ROM output \u2717 replicates the full-order model output: \u00dd \u2248 \u00dd. The input G = G(t) is mapped to the output X = X(t) by solving the ODE:\n\n\u10db / \u2202t I = f , G, Xo ,\n\nX(t = 0) = X0,\n\nwhere the state vector X \u2208 Rn is extended by a vector of additional variables I \u2208 Ri introduced later and Xo \u2208 Rn+i contains the initial conditions for both.\nA three-layer feed-forward neural network represents the right-hand side operator f. The neural network comprises an input layer x \u2208 Rn+i+1, a hidden layer h \u2208 Rn+i, and an output layer z \u2208 Rn+i, as depicted in the bottom left of Fig. 1. Linear transformations of the input layer x with weight matrices W\u2081 and the addition of a bias vector b\u2081 constitute the fundamental operations to compute the values of the neurons in the hidden layer h. Applying a sigmoid activation function I introduces nonlinearity to the relationship between the layers. This process is repeated for the output layer, involving different weights and biases, leading to the computation from the input to the output layer:\n\nh = S(W\u2081x + b\u2081),\nz = S(W2h + b2) .\n\nTo establish the desired relationship between the input and output layers, training data with known input \u011c and output \u00dd\u0176 is presented to the neural network. This process, known as supervised learning, involves implementing fourth-order Runge-Kutta schemes in DynROM to numerically integrate the ODE system over time. The loss function for evaluating the neural network's training error is the mean squared error:\n\nEmse = 1 / n * 1 / N * \u03a3(jk - jk) 2\n\nthat is averaged across all learning scenarios. To minimize this loss function, gradient descent algorithms are employed in conjunction with the back-propagation algorithm . This optimization process aims to determine optimal values for the weights and biases in all layers of the neural network, ensuring that the discrete output \u0176 obtained from numerically integrating Eq. (3) closely replicates the output of the full-order model \u0176."}, {"title": "Case study on training data selection", "content": "This section summarizes prior research , laying the foundation for the case study on combinations of training data sets presented in this work. The framework's procedure, as introduced in Sec. 3.2, is followed until step 5 to identify correlations between global error measures and training data features. Fifty-five amplitude-modulated pseudo-random binary sequence (APRBS) excitation signals are employed in the full-order simulation model to vary Toven and generate readouts as defined in Sec. 3.3. Each data set is uniquely identified with an alphanumeric identifier based on consecutive numbering in TwinLab. After step 3, 15 data sets (referred to as AP15 hereafter) are automatically selected for testing. Subsequently, all data sets are individually used to train a 1-data-set ROM. The median of all training errors, i.e., the root-mean-square of the difference between ROM and the corresponding training data set output, is 0.22 K, with a standard deviation of 0.14K. Six global error measures are derived from testing the ROMs, including the average over all test data (indicated by (\u00b7)) for the root-mean-square error Erms, the mean absolute percentage error Emap, the maximum error Emax, the median error Emed, the interquartile range Eiqr, and the coefficient of determination R2 (see the first column in Tab. 1 for the corresponding variables). The Pearson correlation coefficient R between training error and global test measures is 0.096, indicating hardly any correlation. Consequently, the training error does not significantly impact the study outcomes concerning the global error measures of the ROMs. This lack of correlation is relevant for maintaining a neutral study design. Moving forward, the Pearson correlation matrix will unveil correlations between"}, {"title": "Combination of training data sets", "content": "Upon identifying the promising training data set 745, the next step is determining a suitable second training partner data set to enhance ROM training accuracy. In this section, we propose a procedure for selecting training partner data sets based on a comparison with the good 1-data-set ROM trained on data set 745. To evaluate the plausibility of various training partner selection routines, we utilize Fig. 3, which incorporates several decision support tools. Figure 3a is a modified version of Fig. 2a, where the considered data sets are labeled and color-coded. Figure 3c presents Erms (bars) and the distribution of Erms (boxplots) on AP15 for the 2-data-set ROMs, whereas Tab. 2 lists all global test errors. The subsequent paragraphs introduce hypotheses on how to identify a suitable training partner data set, with consistent naming of the boxes and corresponding color coding in Fig. 3. All discussed data sets are visualized in Fig. 4.\nVery similar to 745, best performance on AP15, low ROM745 error: One approach involves selecting a dataset as a training partner if it has proven effective for training a 1-data-set ROM. The similarity between an excitation signal j and APRBS 745 is quantified by rms(Toven, 745 - Toven,j), represented as the ordinate in Fig. 3b. Another criterion for training partner selection is the test error of the 1-data-set ROM745 on each individual APRBS dataset, plotted as the abscissa in Fig. 3b. Notably, the 1-data-set ROM745 exhibits low test errors on the 625 test set. This aligns with the similarity between data set 625 (color-coded in black) and data set 745 regarding oven temperatures, evident from the low y-position in Fig. 3b. Additionally, the 1-data-set ROM625 performs exceptionally well on AP15, as shown in Fig. 3a. However, selecting data sets 745 and 625 as training partners to form ROM745+625 proves disadvantageous, leading to an increase in test errors instead of a decrease, as evident in the black section of Fig. 3c. Data set 625 does not contribute new information to the training process; instead, it may induce overfitting of the ROM to the specific, similar operational conditions of data sets 745 and 625.\nSomewhat similar to 745, best performance on AP15, medium ROM745 error: Despite the example above, combining training data sets that independently train accurate 1-data-set ROMs remains a plausible approach. However, a certain level of dissimilarity and a medium test error of the base ROM745 on those datasets should be ensured, as indicated by the red color-coded cases in Fig. 3. The ROM745+553 achieves a 49% reduction in test error to Erms = 0.54 K. It exhibits a significantly low error spread in replicating the full-order model the best result in this study on AP15. Additionally, combining data set 745 with a moderately similar but best-performing multi-sine 1-data-set ROM795 training data set yields decent results of Erms = 0.64K on AP15. However, combining all three signals, as demonstrated in the case ROM745+795+553, does not enhance model quality.\nDissimilar to 745, medium or low performance on AP15, medium ROM745 error: Another intuitive approach is to select a training partner that differs from the base data set 745 in terms of oven temperature. However, improving ROM quality is not guaranteed with this method. Highly dissimilar data sets with moderate performance as 1-data-set ROMs (color-coded in blue) or dissimilar data sets that train poor 1-data-set ROMs (color-coded in light gray) do not significantly enhance ROM quality.\nSomewhat similar to 745, medium performance on AP15, high ROM745 error: An effective strategy is to add data sets as training partners where Erms of ROM745 is high (color-coded in green). The decreasing trend of test error medians for the light gray and green training signals (see the trend arrows in Fig. 3) suggests that training improvement correlates with increasing test error of 1-data-set ROM745 (abscissa in Fig. 3b).\nThe best training data combination can be found with the help of Figure 3b, which can be interpreted as a similarity map of data sets. Placing the second training data set close to a cluster of points seems to enrich ROM training. However, as discussed in the above hypotheses, this does not automatically guarantee effective training data combinations. Based on this study, a training partner data set should at least yield good 1-data-set ROMs and exhibit a certain level of dissimilarity to the base training data set regarding oven temperatures. As indicated by the arrows in Fig. 3, the base-signal ROM should have elevated test errors on the potential partner data set. Suitable training partner candidates are identified along the main diagonal of Fig. 3b (red and green cases). This finding features potential follow-up research to classify potential training partners more granularly.\nThe 2-data-set ROM745+553 performs best on AP15. Among various test groups that also feature sinusoidal test signals, the 2-data-set ROM745+795 exhibits slightly superior performance. It achieves a low global test error of Erms = 0.64 K on AP15, Erms = 0.38K on 15 selected multi-sine test data, and Erms = 0.67 K on 15 APRBS signals with sinusoidal transitions, referred to as sinAPRBS. For a comprehensive evaluation of the errors of all 2-data-set ROMs on these and additional test groups, the reader is directed to . The time evaluation of the 2-data-set ROM745+795 is illustrated in Fig. 5b, revealing a characteristic Erms < 0.5 K for the presented test data."}, {"title": "Discussion of the proposed design of experiments", "content": "The proposed efficient design of experiments addresses the challenge of selecting a limited number of appropriate training data sets for accurate ROMs. This is achieved by correlating training data features with error measures of 1-data-set ROMs, allowing for the identification or generation of optimal training data.\nWhen choosing a suitable excitation signal type, it is crucial to differentiate between ROM training, testing, and operation. In this study, ROM training proves to be most successful with data sets excited by APRBS. However, it is worth noting that a real convection oven may not replicate the instantaneous jumps in oven temperature characteristic of APRBS signals. This observation challenges conventional recommendations in the literature, which often advocate using signal types closely resembling operational signals for training purposes. Testing must be conducted on objective test data sets to ensure the robustness and generalization of the reduced-order models. Randomly generating or selecting training data, as seen in methods like k-fold cross-validation, might not uniformly represent the output space at all positions, as discussed in detail in . One can employ methods such as selecting data sets based on a x2 test to address this issue and achieve a more balanced representation of the output space. This ensures, for example, a uniform distribution of the medians of the output variable. Fair test groups are essential for uncovering correlations between data set features and error measures. Additionally, testing can be performed using different excitation signal classes. While APRBS with sinusoidal transitions (sinAPRBS) most accurately emulate convection oven operating temperatures, testing on various signal classes provides a comprehensive evaluation. It is worth noting that testing on APRBS is considered conservative, as ROMs trained on APRBS data tend to exhibit even lower test errors on sinAPRBS test data in this study.\nIt is important to note that low correlation values in the Pearson correlation matrix only imply the absence of a linear correlation. Nonlinear dependencies between data set features and error measures might still exist. This limitation is somewhat alleviated in this work by selecting error measures that exhibit linear or quadratic behavior in either a local or global sense. Future research could explore the derivation of other nonlinear data set features. Even if no correlation is identified between any feature and error measure, the pre-search efforts are valuable. The parameter sweep conducted aids in selecting the best training data set available. Additionally, regardless of whether a correlation is established, the similarity chart for the best 1-data-set ROM facilitates the selection of additional data sets to enhance ROM accuracy. For instance, a suitable partner data set should demonstrate dissimilarity to the base data set, and elevated mutual 1-data-set ROM test error levels should be considered when selecting potential training partners. Moreover, all viable training partners should be capable of generating good 1-data-set ROMs. All in all, correlations are strongly problem-related. That is plausible, as different physical models would require different characteristics in the training data to ensure rich information content in the data set. A recent publication demonstrates how TwinLab's design of experiments method flexibly adapts to the physics of other models."}, {"title": "Conclusion", "content": "The research presented in this work underscores the potency of data-driven reduced-order modeling in crafting precise and efficient digital twins from simulation data. The methodology is agnostic to modeling software, offering versatility across various platforms. Remarkably, data-driven ROMs of high accuracy and speed-up can be developed with just two training data sets. The proposed efficient design of experiments facilitates the selection of suitable training data sets. The ROMs introduced in this work exhibit substantial speed-ups compared to real time, achieving approximately Sp \u2248 3.6 \u00d7 104 with characteristic solution times of one-tenth of a second without imposing a noticeable computational cost on one core of a processor. Beyond the notable speed-up, the data-driven ROMs demonstrate excellent test accuracy across large and representative test data groups. Specifically, root-mean-square errors for the best point data ROM range from 0.30 K to 0.74 K, varying depending on the test group. The fusion of high accuracy with computing efficiency enables the digital twin to run directly on the control logic of the physical process, eliminating the need for edge or cloud computing."}, {"title": "Contributions", "content": "The TwinLab framework is a valuable tool for automating the generation of data-driven ROMs from simulation data. It streamlines excitation signal generation, test group selection based on x2 tests, determination of data set features, and calculation of global test errors. The framework also incorporates code for batch simulations of full-order models in COMSOL Multiphysics or ANSYS Fluent, effectively reducing data handling and user-interaction efforts. The proposed design of experiments may help less experienced users select appropriate training data for reduced-order modeling."}, {"title": "Outlook", "content": "This study illustrates the practical application of physics-based, data-driven digital twins in thermal food processing. In a broader context, the digital twin framework contributes to a paradigm shift in the perspective on digital twins by emphasizing process autonomy. In the first future steps, the framework's versatility may extend to other domains where the application aligns closely with physics principles. For instance, the optimal operation of fuel cells, which involves modeling porous media, fluid flow, heat exchange, and additional reaction kinetics, is strongly correlated to physics. Anticipated developments in mobile applications, particularly within the transportation sector, could leverage digital twins to intelligently control and achieve optimal operating conditions for fuel cells. Moreover, future work with the proposed framework may extend beyond applications within the presented physics and can extend its contribution to various fields with different physical characteristics. For example, in additive manufacturing , where the physical process parameters are decisive for end quality, the framework can enhance process autonomy by providing physics-based, data-driven digital twins."}]}