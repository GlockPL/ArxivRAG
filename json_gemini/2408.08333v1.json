{"title": "CodeMirage: Hallucinations in Code Generated by Large Language Models", "authors": ["Vibhor Agarwal", "Yulong Pei", "Salwa Alamir", "Xiaomo Liu"], "abstract": "Large Language Models (LLMs) have shown promising potentials in program generation and no-code automation. However, LLMs are prone to generate hallucinations, i.e., they generate text which sounds plausible but is incorrect. Although there has been a recent surge in research on LLM hallucinations for text generation, similar hallucination phenomenon can happen in code generation. Sometimes the generated code can have syntactical or logical errors as well as more advanced issues like security vulnerabilities, memory leaks, etc. Given the wide adaptation of LLMs to enhance efficiency in code generation and development in general, it becomes imperative to investigate hallucinations in code generation. To the best of our knowledge, this is the first attempt at studying hallucinations in the code generated by LLMs. We start by introducing the code hallucination definition and a comprehensive taxonomy of code hallucination types. We propose the first benchmark CodeMirage dataset\u00b9 for code halllucinations. The benchmark contains 1, 137 GPT-3.5 generated hallucinated code snippets for Python programming problems from two base datasets \u2013 HumanEval and MBPP. We then propose the methodology for code hallucination detection and experiment with open source LLMs such as CodeLLaMA as well as OpenAI's GPT-3.5 and GPT-4 models using one-shot prompt. We find that GPT-4 performs the best on HumanEval dataset and gives comparable results to the fine-tuned CodeBERT baseline on MBPP dataset. Towards the end, we discuss various mitigation strategies for code hallucinations and conclude our work.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have shown great capabilities pushing forward the field of artificial intelligence in general and natural language generation in particular. OpenAI's"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Language Models for Code Generation", "content": "The triumph of language models in natural language modeling has brought interest among researchers and practitioners on using language models for code generation. Code generation refers to generating programs that need to satisfy all the constraints defined by the underlying task such as test cases, problem descriptions, etc. Pre-trained transformer-based models such as CodeBERT [Feng et al., 2020] are specifically trained for code generation using Masked Language Modeling and Replaced Token Detection training objectives. Decoder pre-trained models are designed to predict the next token based on a given input context. OpenAI's GPT-series [Radford et al., 2018] are decoder-based models for text generation. Based on GPT-2, [Lu et al., 2021] released CodeGPT for code completion and text-to-code generation tasks. After GPT-3 was developed, CodeX\u00b2 and GitHub Copilot\u00b3 were released for code generation. After the success of ChatGPT, OpenAI's GPT-3.5 [Ouyang et al., 2022] and GPT-4 [OpenAI, 2023] models became state-of-the-art for natural language generation. They have shown good performance in code generation as well [Poldrack et al., 2023], but these models are proprietary. Similar open source models such as LLaMA-2 [Touvron et al., 2023] and CodeLLaMA [Roziere et al., 2023] were released for natural language generation tasks. CodeLLaMA is an open sourced LLM based on LLaMA-2 itself but fine-tuned for code-related tasks such as code generation. Another relevant direction is detecting software vulnerability using LLMs where different LLMs [Jensen et al., 2024] and other information such as code structures [Lu et al., 2024] have been explored to detect vulnerabilities in the code."}, {"title": "2.2 Hallucinations in LLMs", "content": "Although LLMs have shown remarkable performance in natural language generation, they still inevitably encounter several issues, hallucination being one of the top [Wu et al., 2024; Ghafouri et al., 2023; Huang et al., 2023]. [Ji et al., 2023] defined hallucination as the generated content that is nonsensical or unfaithful to the provided source content. Previous works [Ji et al., 2023; Maynez et al., 2020; Huang et al., 2021] categorized hallucination into two main categories \u2013 intrinsic and extrinsic. Intrinsic hallucination happens when the generated output contradicts the source content, whereas extrinsic hallucination happens when the generated output cannot be verified from the source content (i.e., output that can neither be supported nor contradicted by the source). Within the context of LLMs, [Zhang et al., 2023b] defined hallucination into 3 categories - input-conflicting (LLM-generated content deviates from the source input provided by users), context-conflicting (LLM-generated content conflicts with previously generated information by itself), and text-conflicting (LLM-generated content is not faithful to the established world knowledge) hallucinations.\nSimilar to text hallucinations, LLMs can hallucinate during code generation as well. Sometimes the LLM-generated code can have syntactical or logical errors as well as more advanced issues like security vulnerabilities, memory leaks, etc. There is no prior work that specifically look at hallucinations in the generated code. [Dinh et al., 2023] studied the buggy-code completion problem and find that the presence of potential bugs in the code context significantly degrades the code generation performance of the LLMs. [Liu et al., 2023] evaluated the functional correctness of the LLM-generated code with large amounts of test-cases newly produced by an automatic test input generator. [Ouyang et al., 2023] conducted empirical study to measure non-determinism in code generated by LLMs. They find that results from LLMs can be highly unstable; non-deterministically returning very different codes for the same prompt. [Bhatt et al., 2023] introduced CyberSecEval, a benchmark developed to help bolster the cybersecurity of LLMs employed as coding assistants. They find a high tendency of more advanced models to suggest insecure code, highlighting the critical need for integrating security considerations in the development of sophisticated LLMs.\nFor the first time in the literature, we study hallucinations in the LLM-generated code. We introduce the code hallucination definition, a comprehensive taxonomy and then propose the first benchmark dataset - CodeMirage containing GPT-"}, {"title": "3 Hallucinations in Code Generation", "content": "In this section, we formally introduce our problem statement of code hallucination in Section 3.1 and then present a comprehensive taxonomy of five hallucination types that can occur in code generated by LLMs in Section 3.2."}, {"title": "3.1 Problem Definition", "content": "Large Language Models have shown good performance in code generation. However, sometimes the generated code may sound plausible but can have several code defects such as security vulnerabilities. We define hallucinated code as the generated code that has one or more code defects such as dead or unreachable code, syntactic or logical errors, robustness issues such as the code fails on edge cases or raises an exception, or has security vulnerabilities or memory leaks."}, {"title": "3.2 Taxonomy", "content": "Based on different types of code defects that can occur, we define the following five hallucination categories for the code generated by LLMs.\n\u2022 Dead or Unreachable code: Generated code has dead, unreachable or redundant piece of code.\n\u2022 Syntactic incorrectness: Generated code has syntactic errors and therefore, fails to compile.\n\u2022 Logical error: Generated code has logical errors, i.e., the generated code cannot solve the given problem correctly.\n\u2022 Robustness issue: Generated code has robustness issues such as it fails on certain edge cases or raises an exception (does not perform required exception handling).\n\u2022 Security vulnerabilities: Generated code has security vulnerabilities or memory leaks."}, {"title": "4 CodeMirage Dataset", "content": "In this section, we discuss the details of our CodeMirage dataset. We begin with describing the dataset generation methodology (Section 4.1) followed by verifying the generated dataset via human annotations (Section 4.2). Then we share dataset statistics and various characteristics of the dataset in Section 4.3."}, {"title": "4.1 Dataset Generation", "content": "For generating the code hallucination dataset CodeMirage, we select two popular base datasets HumanEval [Chen et al., 2021] and MBPP [Austin et al., 2021]. HumanEval dataset [Chen et al., 2021] contains 164 Python programming problems with function signatures, problem description as docstrings, programming solution and test cases for evaluation. Similarly, MBPP [Austin et al., 2021] benchmark consists of 973 crowd-sourced Python programming problems, designed to be solvable by entry-level programmers, covering programming fundamentals, standard library functionality, and so on. Each problem consists of a task description, code solution and 3 automated test cases.\nFor generating hallucinated code snippets, we use GPT-3.5. We design explicit prompts for each of the hallucination types and input them into GPT-3.5 model to get Python code generations that have specific hallucination types. Table 1 shows the layout of code hallucination generation prompt. Each prompt has code hallucination definition describing a specific type of hallucination and an example showcasing a sample problem statement along with the hallucinated code and test cases. Specific prompts for different hallucination types are mentioned in Appendix A. For every problem in both the datasets, we randomly select one of the five hallucination types and then input type specific prompt along with the problem description and test cases. As a result, we get hallucinated code as an output from GPT-3.5 model and we assign the selected hallucination type as the gold label, further validated through human annotations as described in Section 4.2."}, {"title": "4.2 Human Annotations", "content": "To validate the dataset and type specific hallucinated code generations, we conduct human evaluation. We randomly selected 200 programming problems and solutions from both the datasets (50 from HumanEval and 150 from MBPP) in proportion to the number of problems in each of the datasets. We selected a balanced sample with all the hallucination types in equal numbers. We give detailed instructions of the annotation task, definitions for each of the code hallucination types and an example for each. We then ask the annotators to annotate each Python code snippet as one of the five code hallucination types: \"dead code\u201d, \u201csyntactic incorrectness\", \"logical error\u201d, \u201crobustness issue\u201d, \u201csecurity vulnerabilities\" or \"no hallucination\". Annotations were performed by 5 human annotators, well-versed in Python programming, with every annotator annotating about 100 Python code snippets. Initially, each code snippet was annotated by 2 annotators. In case of label conflicts, we introduced a third annotation. Therefore, every code snippet in the sampled dataset has at least 2 annotations and in case of conflicts, 3 annotations so that we have a majority label. Overall, annotators get an average Cohen's kappa score of 0.76 which denotes strong agreement. We also measure accuracy between the annotated majority labels and the automatic gold labels we create during the dataset generation process. High accuracy of 0.81 denotes that GPT-3.5 has generated code snippets as per the specific hallucination type and that automatically assigned gold labels for hallucination types are reliable."}, {"title": "4.3 Dataset Statistics", "content": "Table 2 shows the number of hallucinated Python code snippets in CodeMirage dataset for each of the 5 code hallucination types with individual splits for base datasets - HumanEval and MBPP. In total, CodeMirage dataset has 1, 137 programming problems, LLM-generated hallucinated Python code snippets, ground truth code snippets, and test cases to evaluate code snippets.\nTo measure the complexity of generated code snippets, we compute McCabe's cyclomatic complexity [McCabe, 1976]."}, {"title": "5 Code Hallucination Detection", "content": "Detecting code hallucinations is a challenging task as the code snippet may seem to be plausible but can be incorrect as it can have issues such as security vulnerabilities, memory leaks, etc. which are often hard to detect. After describing the CodeMirage dataset in the previous section, we discuss the methodology and results for code hallucination detection in this section."}, {"title": "5.1 Methodology", "content": "For code hallucination detection, we prompt various large language models to detect whether a code snippet has hallucination and if present, the type of hallucination. We develop an one-shot prompt asking LLMs to detect five hallucination types as well as \u201cno hallucination\u201d category, given the problem description and code snippet as mentioned in Appendix B. We also provide the definitions and an example for each type of hallucinations in the prompt. Table 4 shows the layout of code hallucination detection prompt. We experiment with 3 LLMs an open source CodeLLaMA model as well as OpenAI's GPT-3.5 and GPT-4 models for detecting code hallucinations. We describe various LLMs and baselines used for detecting code hallucinations below:\n\u2022 CodeLLaMA: CodeLLaMA [Roziere et al., 2023] is an open source LLM for code based on LLaMA-2 [Touvron et al., 2023] providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We use CodeLLaMA-7B-Instruct model having 7 billion parameters and fine-tuned to follow instructions.\n\u2022 GPT-3.5: We use OpenAI's GPT-3.5 [Ouyang et al., 2022] model, accessed through OpenAI's official API.\n\u2022 GPT-4: We also experiment with GPT-4 [OpenAI, 2023], the OpenAI's state-of-the-art model, accessed through its official API.\n\u2022 CodeBERT: CodeBERT [Feng et al., 2020] is a pre-trained transformer-based model for programming language, which is a multi-programming-lingual model pre-trained in 6 programming languages. As a baseline, we fine-tune CodeBERT on our CodeMirage dataset with 80:20 split for training and testing sets, respectively. For the train set, we follow stratified sampling for each of the two base datasets. We keep test sets separate to evaluate the performance of fine-tuned model separately on both the base datasets."}, {"title": "5.2 Experimental Setup and Evaluation Metrics", "content": "For detecting code hallucinations, we experiment with one-shot prompt and input it into LLMs along with problem description and code snippet. For CodeLLaMA-Instruct, we use its open-source implementation after downloading the model weights of 7 billion parameters. For OpenAI's GPT-3.5 and GPT-4 models, we use their official API4. We set a temperature of 0.7 and maximum number of tokens for generation to 256.\nEvaluation Metrics. Since we model code hallucination detection task as a multi-class classification task predicting either of the 5 code hallucination types or \"no hallucination\" category, we use accuracy, macro-precision, macro-recall, and macro-F1 scores for performance evaluation."}, {"title": "5.3 Results", "content": "In this section, we discuss results of various language models for detecting code hallucinations. Table 5 shows performance scores for various LLMs for code hallucination detection on CodeMirage dataset. CodeBERT, fine-tuned on our CodeMirage dataset, achieves an accuracy of 0.5938 and macro-F1 score of 0.4897 on HumanEval dataset, whereas it achieves an accuracy of 0.6825 and macro-F1 score of 0.6344 on MBPP dataset. CodeLLaMA, an open source LLM, does not perform well when prompted for code hallucination detection as it achieves macro-F1 scores of 0.0424 and 0.0271 on HumanEval and MBPP datasets, respectively. Surprisingly, there is a big performance gap between GPT-3.5 and GPT-4 models for code hallucination detection with the same prompt. On one hand, GPT-3.5 achieves macro-F1 scores of 0.2654 and 0.2092 on HumanEval and MBPP, respectively. On the other hand, GPT-4 model achieves the best performance with an overall macro-F1 score of 0.5512 for HumanEval and second best score of 0.5195 for MBPP. From Table 5, we can infer that GPT-4 model with just one-shot prompt performs the best on HumanEval dataset and beats the fine-tuned CodeBERT model by 6.15 percentage macro-F1 score. On the other hand, GPT-4 model gives second best performance on MBPP dataset in terms of macro-F1 score and could not beat the fine-tuned CodeBERT model but shows comparable results. However, GPT-4 still gives the best macro-Precision score. Overall, we can conclude that LLMs, especially GPT-4, performs comparable, if not better, with fine-tuned CodeBERT model with mere one-shot prompt for code hallucination detection."}, {"title": "6 Conclusions and Future Work", "content": "LLMs have shown good performance in code generation. In this work, we study hallucinations for the first time in the code generated by LLMs. At first, we introduce the code hallucination definition and a comprehensive taxonomy of 5 hallucination types. We then propose the first ever CodeMirage dataset containing 1,137 GPT-3.5 generated hallucinated Python code snippets. We believe this comprehensive code hallucination taxonomy and the new dataset can open new avenues for research and development in both academia and industry to evaluate code snippets generated by LLMs and mitigate code defects. We also experiment with various open source as well as OpenAI's GPT-like LLMs for detecting code hallucinations using one-shot prompts. We find that GPT-4 model performs the best on HumanEval dataset, while it performs second best on MBPP dataset.\nIn general, we conclude that LLMs, especially GPT-4, performs comparable, if not better, with fine-tuned CodeBERT model for code hallucination detection. The overall performance suggests that the task of code hallucination detection and the CodeMirage dataset are challenging as even the state-of-the-art fine-tuned transformer-based models and LLMs can not performance well with high macro-F1 score. As a result, there is a huge scope for future works. For code hallucination detection, fine-tuning LLMs [Hu et al., 2023] with specific hallucination detection instructions can improve the performance. We can also use software engineering methods such as using compilers, abstract syntax trees (ASTs) [Shippey, 2015], control flow graphs (CFGs) [Phan et al., 2017; Zhang et al., 2023a], etc. to detect code defects. Another important direction of research can be mitigating code hallucinations.\nWe can use several mitigation strategies as also used in text hallucinations [Ji et al., 2023; Zhang et al., 2023b] such as knowledge-enhanced prompt tuning [Ma et al., 2023], retrieval-augmented code generation [Parvez et al., 2021], fine-tuning LLMs [Hu et al., 2023], etc. to mitigate code hallucinations. Similarly, software engineering techniques analyzing execution workflows of the generated code snippets can be used to detect and mitigate code defects. Therefore, there are a lot of opportunities for future work to effectively detect and mitigate code hallucinations. Solving this problem is a huge step forward for companies to be able to safely adopt the use of LLMs for code generation in practice."}, {"title": "A Prompts for Code Hallucination Generation", "content": "Dead/Unreachable code: I want you to act as a code hallucination generator. Given the function signature, task description, and test cases, your objective is to write the hallucinated python code that sounds plausible but is incorrect by inserting a dead or unreachable or redundant code. Below is an example:\nfrom typing import List\ndef has_close_elements (numbers: List[float],\n threshold: float) -> bool:\n \"\"\"Check if in given list of\n numbers, are any two numbers closer\n to each other than given threshold.\n \"\"\"\n>>> has_close_elements(\n [1.0, 2.0, 3.0], 0.5)\nFalse\n>>> has_close_elements(\n [1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\nTrue\ndef has_close_elements(\n numbers: List[float], threshold: float)\n -> bool:\n if threshold < 0:\n return False\n numbers.sort()\n dead_code = [i for i in numbers if i < 0]\n for i in range(len(numbers) - 1):\n if abs (numbers[i] - numbers[i + 1])\n < threshold:\n return True\n else:\n dead_code.append(numbers[i])\n dead_code = sorted(dead_code,\n reverse=True)\n return False\nYou should try your best to generate a hallucinated code to the following question: <Programming question>\nSyntactic incorrectness: I want you to act as a code hallucination generator. Given the function signature, task description, and test cases, your objective is to write the hallucinated python code that sounds plausible but is incorrect by making syntax errors in the code. Below is an example:\nfrom typing import List\ndef has_close_elements (numbers: List[float],\n threshold: float) -> bool:\n \"\"\"Check if in given list of numbers, are any\n two numbers closer to each other than given\n threshold.\"\"\"\n>>> has_close_elements([1.0, 2.0, 3.0], 0.5)\nFalse\n>>> has_close_elements(\n [1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\nTrue\ndef has_close_elements (numbers: List[float],\n threshold: float) -> bool:\n for i range(0, len (numbers) - 1):\n for j range(i + 1, len(numbers)):\n if abs (numbers[i] - numbers[j])\n return False\n < threshold:\n return True\nYou should try your best to generate a hallucinated code to the following question: <Programming question>\nLogical error: I want you to act as a code hallucination generator. Given the function signature, task description, and test cases, your objective is to write the hallucinated python code that sounds plausible but is incorrect by making logical errors in the code. Below is an example:\nfrom typing import List\ndef has_close_elements(numbers: List[float],\n threshold: float) -> bool:\n \"\"\"Check if in given list of numbers,\nare any two numbers closer to each\nother than given threshold.\"\"\"\nclose_elements = False\nfor i in range(len (numbers)):\n for j in range(i + 1, len (numbers)):\n if abs (numbers[i] - numbers[j])\n < threshold:\n close_elements = True\n break\n else:\n close_elements = False\nreturn not close_elements\nYou should try your best to generate a hallucinated code to the following question: <Programming question>\nRobustness issue: I want you to act as a code hallucination generator. Given the function signature, task description, and examples, your objective is to write the hallucinated python code that sounds plausible but fails on certain test cases or raises an exception. Below is an example:\nimport yaml\ndef yaml_load(filename):\n \"\"\"Open a file, load its contents\n into a variable with yaml,\n and return it.\"\"\"\ndef yaml_load(filename):\n with open(filename, 'r') as f:\n data = yaml.load(f,\n Loader=yaml.BaseLoader)\n return data\nYou should try your best to generate a hallucinated code to the following question: <Programming question>\nSecurity vulnerabilities: I want you to act as a code hallucination generator. Given the function signature, task description, and examples, your objective is to write the hallucinated python code that sounds plausible but is incorrect by either having security vulnerabilities or memory leaks. Below is an example:\nimport openai\ndef get_embeddings (text):\n \"\"\"Calculate the embeddings for the\n text provided as parameter using\n the OpenAI's 'text-embedding-ada-002'\n model. Then, return a list of the\n embeddings.\"\"\"\ndef get_embeddings(text):\n openai.api_key =\n \"sk-Hn0VpJ4aM2X7aAyT3mNr37tMMWgJj\n 17zjrxlpbjKgMHe54PV\"\n # publicly available API key\n model = 'text-embedding-ada-002'\n result = openai. Embedding.create(\n model=model, input=text)\n return result[\"data\"][0][\"embedding\"]\nYou should try your best to generate a hallucinated code to the following question: <Programming question>"}, {"title": "B Prompt for Code Hallucination Detection", "content": "I want you to act as a code judge. Given the task description, function signature, and the generated code, your objective is to detect if the generated code has defects, incorrect code or hallucinations. Hallucinated code is the generated code that has one or more code defects such as dead or unreachable code, syntactic or logical errors, robustness issues such that the code fails on certain test cases or raises an exception, or has security vulnerabilities or memory leaks. Below are the 5 categories for code hallucination along with an example:\n1. Dead code: Generated code has dead, unreachable or redundant piece of code. Example:\ndef has_close_elements (numbers: List[float],\n threshold: float) -> bool:\n \"\"\"Check if in given list of numbers, are\n any two numbers closer to each other than\n given threshold.\"\"\"\n if threshold < 0:\n return False\n numbers.sort()\n dead_code = [i for i in numbers if i < 0]\n for i in range(len(numbers) - 1):\n if abs (numbers[i] - numbers[i + 1])\n return True\n else:\n dead_code.append(numbers[i])\n dead_code = sorted(dead_code,\n reverse=True)\n return False\n2. Syntactic incorrectness: Generated code has syntactic errors and therefore, fails to compile. Example:\ndef has_close_elements (numbers: List[float],\n threshold: float) -> bool:\n \"\"\"Check if in given list of numbers,\n are any two numbers closer to each\n other than given threshold.\"\"\"\nfor i range(0, len(numbers) - 1):\n for j range(i + 1, len(numbers)):\n if abs (numbers[i] - numbers[j])\n return True"}]}