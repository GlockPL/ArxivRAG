{"title": "Interpreting Affine Recurrence Learning in GPT-style Transformers", "authors": ["Samarth Bhargav", "Alexander Gut"], "abstract": "Understanding the internal mechanisms of GPT-style transformers, particularly their capacity to perform in-context learning (ICL), is critical for advancing AI alignment and interpretability. In-context learning allows transformers to generalize during inference without modifying their weights, yet the precise operations driving this capability remain largely opaque. This paper presents an investigation into the mechanistic interpretability of these transformers, focusing specifically on their ability to learn and predict affine recurrences as an ICL task. To address this, we trained a custom three-layer transformer to predict affine recurrences and analyzed the model's internal operations using both empirical and theoretical approaches. Our findings reveal that the model forms an initial estimate of the target sequence using a copying mechanism in the zeroth layer, which is subsequently refined through negative similarity heads in the second layer. These insights contribute to a deeper understanding of transformer behaviors in recursive tasks and offer potential avenues for improving AI alignment through mechanistic interpretability. Finally, we discuss the implications of our results for future work, including extensions to higher-dimensional recurrences and the exploration of polynomial sequences.", "sections": [{"title": "1 Introduction", "content": "Over the past five years, artificial intelligence (AI) has increasingly permeated daily life, with large language models (LLMs) such as GPT-3 and GPT-4 playing a prominent role. As of 2024, over 180 million users regularly interact with ChatGPT for tasks ranging from writing emails to debugging code and explaining complex concepts [1]. However, as these models demonstrate ever"}, {"title": "2 Background", "content": "For the purposes of this analysis, we adopt the conventions outlined in Appendix A.1. Readers unfamiliar with these concepts are encouraged to consult the appendix for a comprehensive mathematical framework for understanding transformers.\nIn this section, we explore the capabilities of autoregressive decoder-only transformers, specifically in-context learning (ICL). We also introduce the specific task of mechanistically interpreting affine recurrencess."}, {"title": "2.1 In-Context Learning", "content": "In-context learning (ICL) refers to the ability of transformers to learn new sequences during inference without adjusting their weights and biases. Formally, given an input sequence \u0e1a\u0e351, \u0e1a\u0e352,..., Un, ICL is the ability of a transformer to learn a function f (\u0e1b\u0e351, 2, ..., Un) without modifying its internal parameters.\nStudies have demonstrated that transformers are capable of performing ICL across a wide range of function classes, including linear regressions, sparse linear functions, two-layer neural networks, and decision trees [6]. ICL is foundational to the power of transformers, as exemplified by models like ChatGPT, which must \"learn\" the intent of a prompt at inference time in order to generate a response.\nDespite its centrality to transformer functionality, the mechanisms underlying ICL remain poorly understood. Prior research has reverse-engineered circuits within transformers that facilitate ICL, identifying subsets of attention heads and multi-layered perceptrons (MLPs) that work together to complete"}, {"title": "2.2 Affine Recurrences", "content": "In this study, we focus on the task of predicting affine recurrences. An affine recurrence is defined by the following equation:\n$a_n = \\begin{cases} a_0 & n=0 \\\\ ca_{n-1}+d & n\\geq 1 \\end{cases}$\nThe specific task we address involves training a transformer to auto-regressively predict the sequence a1,a2,...,an from the inputs \u00e3o, a1,...,an-1. The challenge lies in making these predictions without knowledge of the values of c or d."}, {"title": "2.3 Related Works", "content": "The study of mechanistic interpretability has gained significant traction in recent years, driven by the need to understand how large language models (LLMs) like GPT-3 and GPT-4 perform complex tasks without explicit supervision. The concept of in-context learning (ICL), wherein transformers adapt to new tasks during inference without updating their weights, is a cornerstone of this research.\nGarg et al. [6] explored the capacity of transformers to perform in-context learning across various simple function classes, including linear regressions, decision trees, and sparse linear functions. Their findings laid the groundwork for understanding how transformers can generalize during inference, demonstrating that transformers can learn tasks of significant complexity purely through ICL mechanisms.\nIn parallel, research by Olsson et al. [7] focused on reverse-engineering the internal circuits of transformers during ICL. They identified specific patterns, such as the induction head circuit, which is hypothesized to play a key role in"}, {"title": "3 Experimental Setup", "content": ""}, {"title": "3.1 Data Sampling", "content": "The primary concern during data generation is that the linear coefficient in the affine recurrence, denoted by c, could potentially be very large, leading to exponential growth in the sequence. This exponential growth presents a challenge for stochastic gradient descent (SGD), the optimization method typically used to train transformers. Large gradients, arising from the partial derivatives of the error function with respect to model parameters, can destabilize the training process.\nTo mitigate this issue, we bounded the value of c. Even with this bounding, a relatively small number of terms (e.g., 10) can result in the sequence growing by a factor of 1,000. To ensure the model trains properly under these conditions, we employed the following data normalization steps:"}, {"title": "3.2 Model Architecture", "content": "We employed a custom three-layer transformer architecture with eight attention heads per layer. The architecture follows the framework detailed in Appendix A.1. The specific parameters of the model are as follows:\ndvocab = dim(\u00e3n) = 40\ndmodel = 128\ndhead 64\ndmlp = 3072\nnhead = 8\nnlayers 3\nnctx = 32\nNotably, TransformerLens, a well-regarded library for the mechanistic interpretation of GPT-style models, does not natively support models that bypass tokenization. To accommodate our needs, we created a custom fork of TransformerLens that supports direct vector inputs. The link to this modified version is provided in the appendix.\nThe model directly receives the vectors \u00e3o, a1,..., an-1 as input and is expected to output the vectors a1, a2,..., an. We employed mean-squared error (MSE) as the loss function for training.\nNotably, we did not apply the MSE loss uniformly across all vectors. The rationale is that the first vector the model could predict correctly is 23. To illustrate this, consider the following relationships:\n\u00e3\u2081 = c\u00e3o + d\na2 = c\u1ea3\u2081 + d\nIt is evident that c and d can be extracted from this system of equations by substituting the known values for 20, 21, 22. However, predicting a2 solely based on do and a\u2081 is impossible. Empirically, we observed that applying MSE to all vector pairs yielded worse results than starting from the prediction of the vector following @2. This observation also motivated the minimum recurrence size of 3 enforced in the previous section.\nThe model was trained for 100,000 steps using a batch size of 16. We employed the AdamW optimizer with a weight decay of 0.01 and a learning rate of 0.0001. For each batch, the sequence length was randomly chosen to be between 3 and 14. Training was conducted on an NVIDIA T4-GPU using Google Colab."}, {"title": "4 Discussion", "content": ""}, {"title": "4.1 Model Results", "content": "The transformer model successfully achieved a mean squared error of 0.0001. Figure 2 illustrates the model's loss as a function of the number of training steps."}, {"title": "4.2 Attention Patterns", "content": "Attention patterns reflect the weights of the linear combinations of OV vectors that are added to the residual stream vectors. Figure 3 depicts the attention patterns in the zeroth layer."}, {"title": "4.3 Direct Logit Attributions", "content": "Direct logit attribution is a key technique for studying how different heads contribute to accurate predictions.\nThe method involves isolating the output of each attention head and assessing its contribution to the correct prediction of the next vector in the sequence. Specifically, for a given head H that adds the vector v'm to the residual stream at sequence position m, we evaluate how much v aligns with the target vector am+1.\nFirst, we apply the final layer normalization, Infinal, to um, followed by multiplication with the unembedding matrix Wu to project v'm back into the unembedding space.\nTo quantify the contribution, we compute the dot product between the two vectors ((Infinal (vm)Wu, am+1). This dot product serves as a measure of similarity between the vectors, indicating how much um contributes to the correct prediction. We display these quantities for each head's output as shown in Figure 7."}, {"title": "4.4 Zeroth Layer OV Circuits", "content": "While the attention pattern in the zeroth layer indicates a strong focus on the previous token, we still lack detailed information about the OV circuits in this layer.\nTo gain insight, we examined the result of applying OV to each residual stream vector 7. Specifically, we plotted (d, (OV)d) for each dimension 0 <d<dmodel, as shown in Figure 8."}, {"title": "4.5 First Layer", "content": "Given that the direct logit attribution scores of all heads in the first layer are near zero, it is natural to question whether these heads are useful.\nA common technique for assessing the utility of specific heads is ablation, which involves removing a particular head to analyze its impact on accuracy.\nThere are two primary types of ablation: zero ablation and mean ablation. Zero ablation replaces the output of a head with the zero vector, while mean ablation replaces it with the mean output of that head over a distribution of inputs. Intuitively, zero ablation may seem preferable; however, as Neel Nanda argues in [11], zero ablation can be unprincipled if the head primarily adds a \"bias\" term (e.g., consistently adding a value of 100 to the residual stream). In such cases, zero ablation could significantly reduce model performance, even if the head's output does not depend on the input.\nWe implemented mean ablation using a population dataset of 1,000 samples and observed a new mean squared error of 0.0002. This is very close to the original mean squared error of 0.0001, leading us to conclude that the first layer"}, {"title": "4.6 Second Layer QK/OV Circuits", "content": "When plotting the OV and QK circuits in the second layer, a peculiar pattern emerges, displayed in Figures 12 and 13 respectively."}, {"title": "4.7 Analysis of MLPs", "content": "The role of multi-layer perceptrons (MLPs) in transformer interpretability remains an open problem. Recent advances, such as the use of sparse autoencoders, have made headway in this area [12]. However, since our data is strictly numerical, interpreting the different feature directions within MLPs is particularly challenging.\nNonetheless, we performed direct logit attributions on the MLPs which are shown in Figure 16 and found that they primarily contribute in the correct direction for the final few sequence positions."}, {"title": "4.8 Conjectures on Model Functionality", "content": "Based on the evidence presented, we propose the following conjecture regarding the model's functionality:\n1. The model initially forms a crude estimate of the correct solution by adding aan-1 to the residual stream at position an.\n2. The model refines this (over) estimate using negative similarity heads, which subtract vectors that are similar to the destination vector."}, {"title": "5 Conclusion and Future Work", "content": "This study has provided both empirical and theoretical evidence to elucidate how GPT-style transformers solve affine recurrences. We identified that the zeroth layer leverages coefficient normalization to form a crude estimate, which is refined by negative similarity heads in the second layer.\nOur primary contributions to the field of mechanistic interpretability include demonstrating that GPT-style transformers can learn affine recurrences, showcasing the utility of this task for observing transformer behaviors, and providing the weights and biases of the trained models. Additionally, we offer the code used for our analysis and a modified version of TransformerLens that can aid in the analysis of numerical tasks within transformers. We also introduced novel techniques for intervening on models, such as replacing the QK circuit with the Moore-Penrose pseudoinverse to assess whether a head functions as an identity matrix and graphing residual stream vectors to detect linear correlations.\nFuture work could extend our analysis to higher-dimensional linear recurrences (our provided code supports this functionality), explore polynomial recurrences within transformers, and conduct a more fine-grained analysis of how transformers complete the affine recurrence task. We believe these directions"}, {"title": "A Appendix", "content": ""}, {"title": "A.1 A Mathematical Framework for Transformers", "content": "The conventions discussed in this section are primarily drawn from [10]. These conventions deviate from the standard \"concatenate and multiply\" perspective on transformers, offering a more interpretable framework.\nA transformer, as outlined by Vaswani et al. [13], is a neural network architecture that maps a sequence of vectors 1, 2, ..., N to another sequence of vectors \u0e19\u0e35\u0e481, \u0e19\u0e35\u0e482,..., N.\nAn autoregressive decoder-only transformer is designed to predict \u0e19\u0e35\u2081 = 2, \u0e19\u0e35\u0e482 = 3, N = N+1. In other words, the model is trained to predict the next vector in a sequence. \"Autoregressive\" indicates that the model can only use preceding vectors in the sequence to make predictions."}, {"title": "A.2 Links to Code", "content": "Code to Train Transformers\nCode to Analyze Circuits\nModel Repository"}]}