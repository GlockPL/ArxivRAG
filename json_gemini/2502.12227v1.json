{"title": "Identifying the Best Transition Law", "authors": ["Mehrasa Ahmadipour", "\u00e9lise Crepon", "Aur\u00e9lien Garivier"], "abstract": "Motivated by recursive learning in Markov Decision Processes, this paper studies best-arm identification in bandit problems where each arm's reward is drawn from a multinomial distribution with a known support. We compare the performance reached by strategies including notably LUCB without and with use of this knowledge. In the first case, we use classical non-parametric approaches for the confidence intervals. In the second case, where a probability distribution is to be estimated, we first use classical deviation bounds (Hoeffding and Bernstein) on each dimension independently, and then the Empirical Likelihood method (EL-LUCB) on the joint probability vector. The effectiveness of these methods is demonstrated through simulations on scenarios with varying levels of structural complexity.", "sections": [{"title": "2. Introduction", "content": "The importance of interactions between entities such as human-computer interfaces, complex decision-making in autonomous systems like self-driving cars (Chen et al., 2024), or dynamic difficulty adjustment (DDA) in online gaming (Lopes & Lopes, 2022) has fostered the development of Reinforcement Learning (RL) as a model for dynamical systems where responsible agents aim for optimal decisions in an uncertain environment. Markov Decision Processes (MDP) proved able to capture many interesting features of these scenarios, while providing a rich toolbox of computationally efficient and mathematically founded algorithms (Puterman, 1994; Bertsekas, 2005; Sutton, 2018; Moerland et al., 2023a).\nAn MDP is defined as a tuple consisting of the state space S, the action set A, the transition probability kernel P, and the reward function R. At time t, a (deterministic) policy is a mapping \\(\\pi_t : S \\rightarrow A\\) that specifies which action to choose according to the current state. To determine the optimal policy in an MDP, Bellman equations provide a recursive formulation for the value function \\(V_t(s)\\), the expected cumulative reward starting from state s at time t. In the case of a finite horizon T, the optimal policy \\(\\pi^*\\) satisfies:\n\n\n\n\\(V_t^*(s) = \\underset{\\pi \\in \\Pi}{max}\\; \\left[R(s,a) + \\sum_{S'\\in S} P(s'|s,a)V_{t+1}^*(s')\\right]\\)\n\n\nWhen the transition probabilities are fully known in the problem, the agent can compute an optimal policy without interacting with the environment \u2013 a process known as Planning (see (Moerland et al., 2023b)). Otherwise, the process of learning requires the estimation of expected future returns. This paper focuses on a particular learning sub-task: the choice of the policy at time t with known value function \\(V_{t+1}\\). The player, in state s, can transition to one of d possible next states s', each associated with a certain value \\(V_{t+1}(s')\\). She chooses an action a \\(\\in A\\), and transitions to a next state S' according to the transition probability vector \\(P_a\\) (see Figure 1). Each destination state has an associated expected value, and the goal is to find the best action that maximizes this expected value.\nThis decision-making problem is very reminiscent of a Multi-Armed Bandit (MAB) problem, as the outcome of each action a depends only on the current state s and transition probabilities \\(P_a\\), independently of past decisions. The considered task is usually called Best Arm Identification (BAI): identify the arm yielding the highest expected value with as few samples as possible.\nBut the considered bandit problem has a specific feature:"}, {"title": "3. State-of-the-art and connections", "content": "In this work, we focus on the fixed-confidence PAC (Probably Approximately Correct) setting for MAB problems. While the fixed-budget setting remains somewhat mysterious (see e.g., (Bubeck et al., 2012)), the fixed-confidence setting is better understood since its sample complexity was identified in (Garivier & Kaufmann, 2016) thanks to change-of-measure techniques. Different structures for bandit arms have been considered: multi-modal (Saber & Maillard, 2024), linear (Abbasi-Yadkori et al., 2011), contextual(Lu et al., 2010), kernel-based models(Neu et al., 2024), etc. However, these models do not address bandit problems with multinomial reward distributions. We propose novel adaptations of LUCB, -Structured-LUCB using Bernstein and Hoeffding inequalities- and an EL-LUCB algorithm.\nIn the Structured-LUCB, we use empirical Bernstein bounds which have been investigated in UGapE (Gabillon et al., 2012) and in the context of Racing algorithms (Mnih et al., 2008; Heidrich-Meisner & Igel, 2009), where Bernstein-based methods were used to design efficient stopping times. Bernstein-based concentration is also used in regret minimization in (Audibert & Bubeck, 2010). The empirical likelihood method seems particularly well suited for multinomial distributions. In the EL-LUCB algorithm, we employ Kullback\u2013Leibler (KL)-divergence-based confidence regions with LUCB, inspired by the regret minimization algorithms of (Filippi et al., 2010), (Kaufmann & Kalyanakrishnan, 2013) or (Capp\u00e9 et al., 2013). These structured methods aim to improve performance by incorporating additional information about the underlying probability vector and its constraints on a simplex.\nOur study compares these two methodologies under various scenarios. We employ the Top-two (leader-challenger) sampling rule (Russo, 2016; Jourdan et al., 2022; You et al., 2023), which has shown robust performance in both Bayesian and frequentist settings, to guide our sampling strategies. Recent work by (Jourdan et al., 2022) extends this method to bounded distributions, and (You et al., 2023) presents a further enhancement with theoretical guarantees.\nBy contrasting Structured and Non-Structured algorithms, we explore whether leveraging known structures can yield significant benefits in terms of sample efficiency and decision accuracy. To the best of our knowledge, no prior research has systematically examined shifting perspectives between the two approaches. Our contribution lies in this comparative analysis, providing insights into when and how structural assumptions can be beneficial.\nThe paper is organized as follows. We begin by formally explaining the model and our assumptions. We then develop the non-structured approach before the structured cases. We finally present the numerical experiments that we compare and discuss on different algorithms."}, {"title": "4. System Model", "content": "We consider K multinomial distributions \\(P_1,..., P_K\\). Each distribution has d mutually exclusive outcomes, associated with values V = [\\(V_1, ..., V_d\\)] \\(\\in R^d\\). An outcome \\(v_i\\) represents the value obtained at the next state S'. We describe each distribution as a vector \\(P_a = [P_{a,1},..., P_{a,d}]\\) lies on the simplex \\(A_d := {\\theta \\in R^{d+1} | \\sum_{i=1}^{d+1} \\theta_i = 1, \\theta_i \\ge 0 \\text{ for all } i = 1, ..., d}\\), i.e., \\(p_{a,i} > 0\\) and \\(\\sum_{i=1}^d P_{a,i} = 1\\).\nAt discrete time intervals t = 1,2,..., the learner selects an action \\(A_t \\in A\\) and receives an independent sample \\(Z_{A_t} = [Z_{A_t,1},..., Z_{A_t,d}]\\) where we assume \\(Z_{A_t}\\) is a one-hot vector indicating the next state S'. Specifically, \\(Z_{A_t}\\) is drawn such that P[\\(Z_{A_t} = e_i\\)] = \\(P_{A_t,i}\\), where \\(e_i\\) is the i-th standard basis vector in \\(R^d\\). Denoting by p \\(\\cdot\\) v the scalar product of two vectors p and v, the expected value of the reward V under the probability vector \\(P_a\\) is expressed as \\(E_{P_a}[V] = \\sum_{i=1}^d P_{a,i}v_i = P_a \\cdot v\\). Without loss of generality, we can assume the following order:\n\n\\(E_{P_1}[V] > E_{P_2}[V] > \\cdots > E_{P_K}[V].\\)\n\nThe learner empirically constructs \\(\\hat{P_a}\\) and attempts to find the action \\(a^*\\) that maximizes the expected reward as soon as possible:\n\n\\(\\underset{a \\in A}{max}\\; \\hat{P_a} \\cdot V \\text{ s.t. } Dist(\\hat{P_a}, P_a) \\le \\epsilon,\\)\n\nwhere Dist(,) quantifies the \u201cdistance\u201d between the estimated transition probabilities \\(\\hat{P_a}\\) and the optimistic transition probabilities \\(P_a\\). We define this distance more precisely later, first using an L-norm, then using the KL-divergence.\nWe operate in the fixed-confidence regime, where a maximal risk parameter \\(\\delta \\in (0, 1)\\) is fixed."}, {"title": "5. Non-Structured Approach", "content": "We assume that rewards are i.i.d. and follow a Bernoulli distribution with parameter \\(\\mu_a := E_{P_a}[V]\\) under Assumption 1. This setting leads to a MAB problem with distributions belong to the SPEF as described in (Garivier & Kaufmann, 2016). The concept of distinguishability is employed to characterize the lower bounds for the sample complexity in (Garivier & Kaufmann, 2016). This notion is quantified using the KL-divergence, denoted as \\(KL(x, y) := xlog(\\frac{x}{y}) + (1 - x)log(\\frac{1-x}{1-y}))\\). Denote by S a set of SPEF bandit models such that each bandit model \\(\\mu = (\\mu_1,..., \\mu_K)\\) in S has a unique optimal arm \\(a^*(\\mu)\\). For each \\(\\mu \\in S\\), there exists an arm \\(a^*\\prime(\\mu)\\) s.t. They use the notion of the alternative set\nAlt(\\(\\mu\\)) := {\\(\\Lambda \\in S : a^*(\\Lambda) \\neq a^*(\\mu)\\)}, where which is the set of problems \\(\\Lambda\\) for which the optimal arm \\(a^*(\\Lambda)\\) differs from the optimal arm \\(a^*(\\mu)\\) of the reference distribution \\(\\mu\\). They characterize a lower bound for any \\(\\delta\\)-PAC strategy and any bandit model \\(\\mu \\in S\\) under a given risk level \\(\\delta \\in (0,1)\\):\n\n\\(E[\\tau_{\\delta}] > T^* (\\mu)KL(\\delta, 1 - \\delta),\\)\n\nwhere\n\n\n\\(T^*(\\mu)^{-1} := \\underset{\\Omega \\in \\sum_K}{sup} \\underset{a \\in \\text{Alt}(\\mu)}{inf} \\sum_{a=1}^K \\Omega_aKL(\\mu_a, \\Omega_a).\\)\n\nHere, the set of proportions for pulling arms is defined as \\(\\Sigma_K = {w \\in R^K: \\sum w = 1}\\).\nThe assumption of SPEF on bandits' distributions allows the inner minimization of (6) to be solved and to obtain an explicit formulation for the optimal weights w. The same authors introduced an asymptotic optimal algorithm matching this lower bound, called Track&Stop (T&S). But its computational complexity often motivates the use of more practical alternatives such as the LUCB (Lower and Upper Confidence Bound) algorithm."}, {"title": "5.1. Non-structured LUCB", "content": "The LUCB algorithm is a standard approach for the PAC problem in stochastic multi-armed bandits, specifically for BAI. The main idea is to construct and iteratively refine upper and lower confidence bounds around each arm's empirical mean (Kalyanakrishnan et al., 2012) until they are separated.\nAt each round t, for each arm a, we compute a confidence bonus (CB) \\(\\beta_{\\text{No-St}}(n,t)\\). Over time, \\(\\beta_{\\text{No-St}}(n,t)\\) shrinks according to a concentration inequality (e.g., Hoeffding or Bernstein). Let \\(\\hat{\\mu_a}(t) := \\hat{P_a}(t) \\cdot V\\) be the empirical estimate of arm a's expectation at time t. For the current best arm a and the current second best arm b, we construct lower confidence bound and upper confidence bound respectively. The algorithm stops when\n\n\\(\\hat{\\mu_a}(t) - \\hat{\\mu_b}(t) - [\\beta_{\\text{No-St}}(n, t) + \\beta_{\\text{No-St}}(n, t)] \\ge \\epsilon,\\)\n\nwhere \\(\\beta_{\\text{No-St}}(n,t) = \\sqrt{\\frac{log 2Kt}{2n_t(t)}}\\) is derived from Hoeffding's inequality. We call this approach Non-structured because the algorithm ignores the vector V."}, {"title": "6. Structured System Model", "content": "We now consider the model introduced in Section 4 as a K-action bandit problem where rewards are drawn i.i.d. from multinomial distributions \\(P_a\\), a \\(\\in A\\) and V = [\\(V_1, V_2, ..., V_d\\)] is the support. We rely here directly on estimates of all d components of the probability vector \\(P_a\\). We construct the empirical distribution as a vector\n\n\\(\\hat{P_{A_t}} = \\frac{1}{n_{A_t}} \\sum_{k=1}^{n_{A_t}} Z_{A_t,k},\\)\n\n\\(\\hat{P_{A_t,i}} = \\frac{1}{n_{A_t}} \\sum_{k=1}^{n_{A_t}} I(Z_{A_{tk}} = e_i)\\,\n\nwhere each component is given by with I being the indicator function. We keep the Assumption 1 to facilitate the comparison of previously non-structured approach with the proposed structured cases. We address a more general case involving bounded support probabilities and heavy-tail distributions, as introduced and analyzed in (Agrawal et al., 2020). While SPEF distributions in (Garivier & Kaufmann, 2016) allow inner minimization of (6) in Euclidean space, introducing a probability vector confines the problem to the simplex. Agrawal et al. (Agrawal et al., 2020) address this by using functions KLinf and KL, which measure the minimal KL-divergence required to distinguish a distribution \\(\\eta\\) from alternatives with means below or above a threshold x. Specifically, for distributions K1, K2 over a finite set with mean m(K), define\n\n\n\\(KL^{\\text{inf}} (\\eta,x) := \\underset{\\kappa \\in L}{min} KL(\\eta, \\kappa),\\)\n\n\n\\(m(\\kappa) \\ge x\\)\n\nwith a similar definition for \\(KL^{\\text{inf}} (\\eta,x)\\). Inspired by (Honda & Takemura, 2010) in regret minimization setup, Agrawal et al. propose a Lagrangian dual problem to overcome the technical challenge of lower bounding sample complexity in this simplex-based model and proved (5) with new definition of inner minimization of \\(T^* (P)\\) using KLinf and KLinf. In (Agrawal et al., 2020), a modified version of T&S is proposed.\nThe computational complexity issue is further exacerbated in the modified T&S algorithm, where the complexity increases due to the need to solve an optimization problem involving two KLinf terms. Additionally, incorporating the effect of V when transitioning from Track-and-Stop to modified Track-and-Stop is not straightforward. In the latter, the support vector independently influences the Lagrangian problem, making it difficult to unify and clearly reflect the impact of V. These challenges motivate us to explore the Structured-LUCB algorithm practically, where the influence of V on the algorithm becomes more transparent to analyze."}, {"title": "6.1. Structured-LUCB", "content": "The Structured-LUCB algorithm constructs confidence bounds for each d component of the probability vectors and combines them to compute confidence intervals for expected rewards. Each \\(p_{a,i}\\) is estimated independently. The algorithm applies larger thresholds in decision-making for components with \\(v_i\\) that are more likely to occur, prioritizing areas of higher uncertainty. Algorithm 1 provides the schema for the Structured-LUCB algorithm. At time t, let \\(\\hat{P_{k,i}}(t)\\) denote the empirical estimate for the probability of arm k producing outcome \\(v_i\\), and let \\(\\beta_{k,i}(n, t)\\) quantify the uncertainty of this estimate based on n samples. Following the LUCB framework, we require a lower bound for the best arm a and an upper bound for the current second-best arm b on the i-th outcome:\n\n\\(P_{a,i} \\ge \\hat{P_{a,i}}(t) - \\beta_{a,i}(n, t),\\)\n\n\\(P_{b,i} \\le \\hat{P_{b,i}}(t)+\\beta_{b,i}(n,t),\\)\n\nwhere the CB \\(\\beta_{k,i}(n, t)\\) is derived using either Hoeffding's inequality:\n\n\\(\\beta_{k,i}^{St-H}(t) = \\sqrt{\\frac{log(2dKt)}{2n_{k}(t)}},\\)\n\nor Bernstein's inequality, which incorporates the variance for tighter bounds:\n\n\\(\\beta_{k,i}^{STB}(t) = \\sqrt{\\frac{2\\hat{\\sigma_{k,i,t}}^2 \\text{In} (\\frac{2dKt}{\\delta})}{2n_{k}} + \\frac{In (\\frac{2dKt}{\\delta})}{3n_{k}}},\\)\n\nwhere \\(\\hat{\\sigma_{k,i,t}}^2\\) denotes the variance at time t, calculated as \\(\\hat{\\sigma_{k,i,t}}^2 = \\hat{P_{k,i}}(t)(1 - \\hat{P_{k,i}}(t))\\), providing confidence bonuses (CBs) that depend more closely on the observed variance.\nThe lower and upper confidence bounds for the expected rewards of arms a and b are given by :\n\n\\(LCB^{\\text{Str}}_a(t) = \\sum_{i=1}^d (\\hat{P_{a,i}}(t) - \\beta_{a,i}(n,t)) v_i,\\)\n\n\\(UCB^{\\text{Str}}_b(t) = \\sum_{i=1}^d (\\hat{P_{b,i}}(t) + \\beta_{i}(n, t)) v_i,\\)\n\nThe algorithm stops when \\(LCB^{\\text{Str}}_a\\) exceeds \\(UCB^{\\text{Str}}_b\\) ensuring that the expected reward of arm a is sufficiently higher than that of arm b with high confidence. Since the confidence bounds are uniform across all components of each probability vector, the stopping condition is simplified to:\n\n\\((\\hat{P_{a}}(t) - \\hat{P_{b}}(t)) \\cdot V - (\\beta_{a,i}(n, t) + \\beta_{i}(n,t)) \\cdot |V| \\ge \\epsilon,\\)\n\nwhere |V| is L\u2081-norm of vector V. Next in this section, we look at a joint estimation of d components of the probability vectors."}, {"title": "6.2. Structured Model 2: EL-LUCB Method", "content": "The problem introduced in (3) can be interpreted as a maximization of an unknown probability vector \\(P_a\\) in the direction of a known vector V. While the Structured-LUCB algorithm considers this problem as an estimation of all component independently, we suggest here to think of a joint estimation within a KL-ball, that reminiscent of the approach in (Filippi et al., 2010) for a regret minimization problem. In previous Structured-LUCB, the Dist(,) was"}, {"title": "7. Experiments and Discussions", "content": "In this section, we explore our proposed algorithms for different cases and explain the different results obtained. We focus on three algorithms: Non-structured LUCB (or simply LUCB with Assumption 1), Structured-LUCB presented in Alg 1 and the EL-LUCB algorithm explained in Subsection 6.2.\nThe results are averaged on 100 trials. The confidence parameter is set to d = 0.05, the sampling probabilities of leader-challenger are initialized at [0.5, 0.5]. The reward of each arm is drawn according to a row of the matrix P. Its columns contain the probabilities of each outcome of V. We evaluate their performance on different support vectors V."}, {"title": "7.1. Structured-LUCB vs. Non-structured-LUCB", "content": "In the situations where the outcome probabilities are highly concentrated on a single outcome of V for each arm, the structured algorithm does not offer significant advantages over the non-structured algorithm. Both algorithms will perform similarly and non-structured algorithm can quickly and accurately estimate the expected rewards based on observed averages with less complex implementation effort. We hence consider the following cases to determine the suitability of each algorithm under varying conditions:\n\n\\(P^{\\text{test}} = \\begin{bmatrix}0.5 & 0.3 & 0.2\\\\0.4 & 0.3 & 0.3\\\\0.3 & 0.2 & 0.5\\end{bmatrix}\\), \\(V^{\\text{test1}} = [0.5, 0.1, 0].\\)\nFigure 2 shows that for \\(V^{\\text{test1}}\\, the Structured-LUCB algorithm significantly outperforms Non-structured-LUCB.\nFor the same P, we change the support to \\(V^{\\text{test2}}\\, as shown in Figure 3, Structured-LUCB demonstrates less efficient performance compared to Non-structured-LUCB.\nThe reasoning lies in how the stopping time and the effect of V are introduced in the algorithms. In Non-structured-"}, {"title": "8. Conclusion", "content": "We presented and compared two multi-armed bandit (MAB) frameworks to tackle the choice of a transition in a finite horizon Markov decision process with known future values. Experimental results compared stopping times and the impact of leveraging distributional support across various scenarios, illustrating that incorporating structure \u2013 when available \u2013 may significantly improve performance and decision-making efficiency. The quantitative amplitude of this gain remains to be better understood from a theoretical point of view."}]}