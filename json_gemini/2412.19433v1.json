{"title": "Residual Feature-Reutilization Inception Network for Image Classification", "authors": ["Yuanpeng He", "Wenjie Song", "Lijian Li", "Tianxiang Zhan", "Wenpin Jiao"], "abstract": "Capturing feature information effectively is of great importance in the field of computer vision. With the development of convolutional neural networks (CNNs), concepts like residual connection and multiple scales promote continual performance gains in diverse deep learning vision tasks. In this paper, we propose a novel CNN architecture that it consists of residual feature-reutilization inceptions (ResFRI) or split-residual feature-reutilization inceptions (Split-ResFRI). And it is composed of four convolutional combinations of different structures connected by specially designed information interaction passages, which are utilized to extract multi-scale feature information and effectively increase the receptive field of the model. Moreover, according to the network structure designed above, Split-ResFRI can adjust the segmentation ratio of the input information, thereby reducing the number of parameters and guaranteeing the model performance. Specifically, in experiments based on popular vision datasets, such as CIFAR10 (97.94%), CIFAR100 (85.91%) and Tiny Imagenet (70.54%), we obtain state-of-the-art results compared with other modern models under the premise that the model size is approximate and no additional data is used.", "sections": [{"title": "1. Introduction", "content": "In recent years, we've witnessed a rapid advance of computer vision which is of great significance to aspects of human life. Generally, deep learning has contributed to this field a lot. The most representative deep neural network architectures in computer vision can be roughly divided into transformer-based and CNN-based models. Transformer is originally proposed for natural language processing, which has been transferred to vision tasks and achieves considerably satisfying performance recently. Specifically, vision transformer [1] first introduces attention mechanism into computer vision whose strategy of information interaction enlargers the effective receptive field of related models observably so that crucial information can be better obtained. Due to efficiency of this architecture, the variations of transformer are devised corresponding to specific demands, and there are two main categories in the thoughts about improvements on the variations, namely integration of transformer framework with other models which are for particular usages and modifications on the original architecture. With respect to the former, DS-TransUNet [2] is a typical example, which synthesizes dual transformer-based architectures and U-Net to realize a breakthrough in medical image segmentation. Besides, some works focus on improvements on architecture of transformer, for instance, Mix-ViT [3] tries to design a mix attention mechanism to create more sufficient passages for information interaction.\nBesides, the convolutional neural network has a longer history than transformer and is still favored by many researchers [4]. It is noteworthy that a performance milestone of CNN is the proposal of residual network [5]. The invention of residual network makes training deeper structured CNNs possible. Nowadays, lots of vision models still benefit from this design to satisfy different requirements from various fields such as medical diagnosis [6]. Specifically, some classical algorithms also get a second life with the addition of residual connection. For example, U-Net with nested residual connections realizes excellent performance on fault detection [7]. Although CNN has made significant progress, the performance of these networks is still restricted due to relatively small receptive field. In order to make up for this shortcoming, some researchers choose to combine characteristics of CNN and architecture of transformer such as Convformer-NSE [8] which utilizes the information fusion mechanism of transformer to provide CNN with a larger receptive field. Meanwhile, how to expand the perception area of CNN without using other kinds of network structures has become a research focus due to popularity of transformer-based architecture. Remarkably, acquirement of features from multiple scales to obtain more information has been a feasible solution to enlarge receptive field of CNNs, which enables model to process features at separate levels and boosts performances of them therefore. The concept of multi-scale has already been introduced into separate vision tasks [9] and its superiority is fully demonstrated by various effective models. Among them, InceptionNets [10] enlargers the receptive field by stacking parallel convolutions of different kernel sizes. Besides, on the basis of common residual block [5], a multi-scale architecture called Res2Net [11] is devised to better obtain and aggregate information at different scales which replaces original 3 \u00d7 3 convolutions with a set of smaller convolution groups and proves traditional residual networks can also benefit from multi-scale design. The consideration of it resembles pyramid network [12] which acquires multi-scale features through combination of high-level and low-level information. And the Res2Net block can continually enlarge the receptive field through stacking 3\u00d73 convolutional layers in different groups of convolution to achieve satisfying performance as well. Advances in skeleton structure of modern visual models indicate that neural networks are tending to be more efficient and effective in multi-scale representation.\nObviously, obtaining information more effectively from a larger field of reception means a lot. Firstly, feature information is essential for image recognition and classification. In computer vision tasks, images often contain a large amount of pixels and color information, which is redundant for deep learning algorithms. Capturing effective feature information can help reduce the computational burden and storage space requirements, while improving the accuracy of classification and recognition. Secondly, the feature information can help computer to better understand image content. In computer vision, understanding semantic relations usually means recognizing objects, scenes, actions, etc. in images. By capturing the feature information, the computer can better comprehend the important information in images, which leads to more accurate classification and recognition. Third, feature information can enhance the generalization ability of vision model. In deep learning, generalization ability refers to the ability of a model to adapt to new data. By capturing effective feature information, model can better generalize to new image data improving the performance and accuracy on different tasks. Fourth, feature information can help solve the problem of data imbalance. In the field of computer vision, the data is often imbalanced, with some classes having a much larger number of samples than others. By capturing effective feature information, it can better distinguish samples from different categories, thereby addressing the problem of data imbalance. Besides, under the premise of a large amount of effective information, the use of better optimization algorithms can also improve the performance of vision models to some extent [13, 14, 15, 16].\nIn conclusion, effectively capturing feature information is very important in computer vision to improve the accuracy of image recognition and classification, enhance the generalization ability of model, and solve the problem of data imbalance."}, {"title": "2. Residual Feature-Reutilization Inception Network", "content": "In this paper, we propose a high-efficiency and comprehensible multi-scale CNN model. Unlike previous models that enhance the ability of perception by deepening the number of network layers or using multiple identical receptive fields at a finer granularity, we choose to stack convolutions with different kernel sizes on parallel paths to obtain feature information at separate scales, which is different from Res2Net and enables the model to capture crucial information more flexibly. The convolution groups in separate paths are connected in a pattern similar to residual connection, which can increase the number of different scales showing that the output features enable the latter convolution groups to obtain richer hierarchical information. Specifically, the proposed model has two versions which consists of residual feature-reutilization inception (ResFRI) and split-residual feature-reutilization inception (Split-ResFRI) respectively. For ResFRI block, we input the complete feature map into each group of convolutions; with respect to Split-ResFRI block, we split the input features into four different parts according to the numbers of channels designed in GoogLeNet [17]. The operation of split will significantly reduce the number of parameters and decrease training time a lot, however, which will also lead to a slight accuracy loss under some circumstances. A group of convolution first extracts features from input and the processed information are sent to the next groups of convolution with corresponding input features. This procedure ends when all information is processed. And the residual connection-like passages between groups of convolution adopt 1\u00d71 convolutional layers to sample and enable the model to obtain stronger non-linearity in the separate receptive fields avoiding increasing calculation complexity too much. In other words, the existence of this operation enables information to be reutilized and the changes on the structure provide multi-scale feature extraction and information fusion at different scales, which makes up for the problem that CNNs have relatively limited receptive fields. Besides, a residual connection is also devised to the proposed inception network to reduce difficulty of network optimization. Synthesizing the peculiarities mentioned before, the proposed network possesses relatively smaller model size and achieves higher performance simultaneously, which is experimentally verified by the results of image classification on popular vision datasets. In summary, the proposed model combines features of multiple models and possesses considerable advantages compared with other modern models. The details of inception of GoogLeNet, ResFRI and Split-ResFRI are provided in Fig.1 and Fig.2, 3.\nThe main contribution of the ResFRI can be summed up in four points which are listed as below:\n1. A novel multi-scale CNN architecture ResFRI and Split-ResFRI are proposed to fully utilize features from different scales and enlarge the receptive fields with four specially designed separate convolutional structures and corresponding information interaction passages.\n2. Split-ResFRI reduces parameter amount and Flops with acceptable accuracy loss by dividing features into multiple groups with ratios referring to the setting of channel number of ResFRI.\n3. ResFRI and Split-ResFRI investigate the effect of pruning and pruning ratio on the performance of this model, which references the idea provided by CondenseNet [18].\n4. Extensive experiments are conducted on popular image classification benchmark datasets, whose results demonstrate that our proposed methods achieves start-of-the-art performance compared with previous models which possess approximate model size and do not utilize extra data for training."}, {"title": "2.1. Introduction of Structure of ResFRI and Split-ResFRI", "content": "The details of ResFRI and Split-ResPRI are presented in Fig.2 and 3. Suppose information from previous layer as pre and the operations of convolutional layers as Conv, the main difference of ResFRI (RI) and Split-ResFRI (SRI) in processing of input can be defined as:\n$$Conv(\\xi_{pre},\\xi_{pre},\\xi_{pre},\\xi_{pre}), RI$$\n$$Conv(\\gamma_1, \\gamma_2, \\gamma_3, \\gamma_4)$$\n$$\\gamma_{1,2}= 3 * \\xi_{pre}//8 \\ \\ \\ \\ \\ \\  \\gamma_{3,4}= \\xi_{pre}//8, SRI$$\nwhere ResFRI directly processes information contained in all channels from last layer with four different convolutional groups and Split-ResFRI receives information from 3/8, 3/8, 1/8 and 1/8 channels, which is devised according to original settings of GoogLeNet [17]. Compared with the primitive structure of inception contained in GoogLeNet, some improvements are dexterously designed in ResFRI and Split-ResFRI. To be specific, in order to reuse information, we construct passages between adjacent groups of convolutional layers. Moreover, a residual connection is also devised to reduce difficulty of network optimization and to avoid problems like overfitting and abnormal gradients. Besides, to match feature channels between groups of convolutional layers and residual connection to final output, a structure consists of 1\u00d71 Convolutional layers, 3\u00d73 MaxPool, BatchNorm and ReLu (cmbr) is utilized. It also further enhances extraction of information by realizing cross channel information combination and non-linear feature transference. And it's worth noting that the information combination is achieved by adding or concatenating features and the operation is described in Fig.4. Suppose the information processed by former group of convolutional layer as \u03b4 and the input to this group as \u03ba, then the fusion of information between groups of convolutional layer can be defined as:\n$$F = \\begin{cases}Addition(cmbr(\\delta),\\kappa)\\\\Concat(cmbr(\\delta),\\kappa)\\end{cases}$$\nwhere Addition represents addition of cmbr(\u03b4) and \u03ba, and Concat denotes concatenation of them. Moreover, the comparison of performance and resource consumption between these methods can be found in the ablation study based on ResFRI.\nTo reduce consumption of computation resources, we discard the 3\u00d73 convolutional layers designed by Res2Net and comply with the original design of inception of GoogLeNet. Besides, we notice that the idea of connections between different groups of convolutional layers is very similar to the one of DenseNet [19], the extra passages may help improve performance of network. However, CondenseNet[18] points out that the dense connections are probably redundant under certain circumstances and this phenomenon may reduce accuracy and efficiency of network. As a result, we prune newly-constructed passages of information transference except the residual connection in ResFRI to avoid unnecessary calculations and obtain higher accuracy. More specifically, we adopt unstructured pruning which trims the single weight and does not require a whole row of pruning. The advantage is that the original accuracy can be maintained, because structured pruning is easy to cut out those important weights. Besides, the tools of pruning is provided by PyTorch which will abandon a part of weight parameters using mask matrices without changing the original size of models. In the last, for the filter concatenation (IFC) and synthesizing the operations defined above, suppose Conv consists of [C1, C2, C3, C4], it can be defined as:\n$$FC = \\begin{cases}Concat(C_1(\\xi_{pre}), C_2(IF(C_1(\\xi_{pre}),\\xi_{pre}),\\\\C_3(IF(C_2(IF(C_1(\\xi_{pre}),\\xi_{pre})),\\xi_{pre})),\\\\C_4(IF(C_3(IF(C_2(IF(C_1(\\xi_{pre}),\\xi_{pre})),\\xi_{pre}))),\\xi_{pre})), RI\\\\Concat(C_1(\\gamma_1), C_2(IF(C_1 (\\gamma_1)), \\gamma_2),\\\\C_3(IF(C_2(IF(C_1(\\gamma_1)), \\gamma_2)), \\gamma_3),\\\\C_4(IF(C_3(IF(C_2(F(C_1(\\gamma_1)), \\gamma_2)), \\gamma_3)), \\gamma_4)), SRI\\end{cases}$$"}, {"title": "2.2. Discussions on ResFRI and Split-ResFRI and multi-scale feature fusion strategy", "content": "The proposed CNN architecture incorporates two key components: Residual Feature Reutilization Inceptions (ResFRI) and Split-Residual Feature-Reutilization Inceptions (Split-ResFRI). These components are designed to enhance information flow and passages between convolutional layers, enabling the network to extract richer feature representations and optimize overall network performance.\nResFRI is a modified version of the popular ResNet and GoogleNet. In ResFRI, the input feature map is fed into two paths respectively: a main path and a residual path. In detail, the main path performs convolutional combinations of different structures for multi-scale information acquisition with mutual interaction, while the residual path preserves the original feature map and reutilizes it as residual features. These residual features are then added back to the main path's output, enabling the network to realize better optimization performance and improve the understanding of information processed. By stacking the ResFRI module, the whole network is capable of obtaining information at more flexible level, which allows to capture more comprehensive feature representations and better preserve fine-grained details.\nBesides, Split-ResFRI extends the concept of ResFRI by further splitting feature map according to the channel setting of ResFRI. Different from ResFRI, each group of convolutions only receive a part of information for reduction of network parameters and demand of computation, while retaining the ability of extraction of features, which allows the network to optimize information flow and capture a broader range of feature representations for improved performance. To be specific, same as the design of ResFRI, there exist passages between adjacent groups of convolution which are devised for feature reutilization. And the passages are composed of a 1\u00d71 convolution for matching the number of channels of the convolutional group that will be input, 3\u00d73 maxpooling for realizing further dimensionality reduction and removal of redundant information, BatchNorm layer and Relu activation function for solving the gradient problem and improve the training speed and stability. And the final information fusion are realized by adding or concatenating feature maps in passages and groups of convolutions.\nDue to the completely different convolution operations in various convolutional combinations, they have different receptive fields that enable them to extract different granularity feature information which provides different context information at different scales. When the information in different convolutional combinations is fused through the information interaction passages mentioned above, the feature of different granularities can be further combined to realize the reorganization of multi-scale information and integrate aforementioned context information to provide a more comprehensive and rich visual context. As a result, the network can extract richer semantic associations from the optimized information, and achieve a deeper mining of features with different granularities. In sum, the designed approach allows for more effective information flow, improving feature extraction capabilities, and better generalization performance for a wide range of image classification tasks."}, {"title": "2.3. Other Important Settings of ResFRI and Split-ResFRI", "content": "During the process of experiment, we notice that the MaxPool layers may hamper the network to capture information effectively and weaken performance of it, we argue that the Max-Pool layers may destruct information contained in the relative low-resolution figures instead of being helpful in extraction of features. Verified by experiments, we change the MaxPool layer into AvgPool layer eventually.\nRaised by [18], the dense connections may have negative impact on the process of learning and decrease accuracy of models. Therefore, we try to cancel some passages between convolution groups to avoid too dense connections in the ResFRI and Split-ResFRI utilizing different pruning ratio. Finally, we set the drop rate of passages of information transference to 0.7 and 0 on addition and concatenation version of ResFRI respectively, which can be defined as:\n$$Pruning Ratio = \\begin{cases}0.7, \\ \\ \\ \\ \\ \\ Addition, RI\\\\0, \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ Concatentation, RI\\end{cases}$$\nWith respect to Split-ResFRI, because of underlying performance loss which may be brought by segmentation of information, we set the pruning rate uniformly to 0 in order to strengthen information interaction among groups of convolutional layers. And it is worth noting that when the classes contained in datasets are becoming more, we are supposed to reduce the amount of pruning to better promote information transference for the version of addition of ResFRI, which can be illustrated in the following experiments on vision datasets. In the last, the results in the part of ablation study will prove the effectiveness of these modifications based on ResFRI."}, {"title": "2.4. Limitations of existing methods", "content": "ResNet mainly performs feature extraction by stacking deep networks, and does not explicitly introduce multi-scale information in the network, which largely limits its performance improvement. In order to solve the problem of multi-scale information acquisition, Res2Net introduces multiple branches to process information at different scales. It is successful to some extent, but the size of the convolution kernel in each branch is usually fixed, which means that in each branch, there is a limit on the size of the receptive field. For some specific scenarios, larger or smaller receptive fields may be required to effectively capture the features in vision information. But similar to ResNet, the residual connections it has do guarantee the performance of the model. For the inception series of networks, researchers have made various improvements. For example, a design similar to residual connection is introduced into the inception networks, or more different branches are utilized to obtain different levels of feature information, so that the network can understand the purpose of each visual task more efficiently. However, they often lack a similar information interaction design as Res2Net which simply concatenates the information of each branch and then fuses it through a convolutional layer, without considering the potential importance of information interaction between different branches. This is a key factor where the promotion of their model performance is limited. And, it is worth pointing out that due to the depth and complexity of the above models, the training and inference process requires a lot of computational resources and storage space. Deeper ResNet models may take longer time to train and have a corresponding increase in GPU memory requirements. This makes it difficult to deploy and run these models in resource-limited environments, such as mobile devices or embedded systems, and the models are more space intensive in terms of storage and transmission. Therefore, in response to address these problems, we propose a convolutional neural network structure that can balance model size, training time and performance, which is able to achieve very excellent accuracy on classification task under the premise of usage of very limited resources."}, {"title": "3. EXPERIMENTS", "content": ""}, {"title": "3.1. Implementation Details", "content": "We implement the whole framework of the proposed model utilizing code framework provided by PyTorch. And in order to ensure fairness of comparison among different methods, we provide experiment results of classical and newly proposed models with or without extra data. Due to our limited computation resources, apart from necessary ablation experiments, we choose the task of image classification on the popular datasets, such as CIFAR10, CIFAR100, Tiny Imagenet, MNIST, FashionMNIST and SVHN. And in the process of training on one RTX 4090 and 3060 GPUs, we use the optimizer SGD with momentum 0.9, weight decay 0.0005, batch size 64. We adopt the same data augmentation strategy as [5] and images are resized into 32 \u00d7 32 except for Tiny ImageNet dataset in which the model size will variate to some extent. Moreover, the initial learning rate is set to 0.01 and it is reduced by half if validation loss does not decrease within 10 epochs. And tendency of accuracy and loss in the training process of ResFRI is given in Fig.5."}, {"title": "3.2. Metric of Image Classification", "content": "Image classification accuracy is the proportion of a model that predicts the class of a given image correctly. In computation, it is usually defined as the ratio of the number of all correctly classified images (i.e., true examples) to the total number of images. More academically, accuracy refers to the proportion of all samples in which the judgment result is consistent with the actual result (TP+TN). In the task of image classification, it reflects the model's confidence that the image is of a certain class. Specifically, the calculation formula can be defined as follows:\n$$Acc = \\frac{TP + TN}{TP + FP + TN + FN}$$\nCorrespondingly, the error rate can be defined as:\n$$Err = 1 - Acc$$\nGenerally speaking, people use these two metrics to represent the performance of a model for image classification."}, {"title": "3.3. Experiments on CIFAR-10", "content": "The CIFAR10 dataset contains 50k training images and 10k testing images from 10 classes whose resolution is 32\u00d732. And the detailed results of comparisons of different models will be clearly provided in Table 1 and Fig.6.\nIt can be obtained that the ResFRI and Split-ResFRI achieve relatively satisfying performance on CIFAR-10 dataset-based image classification task. Compared with traditional models like ResNet-101 [5], ResFRI and Split-ResFRI have better performance with much lower parameter amount. Although ResFRI-addition has 0.24 GFlops and ResFRI-concatenation has 0.52 GFlops higher than ResNet-101, we have a remarkable 3.44% and 3.46% performance gain on top-1 err while parameter amounts reduce by 32.4M and 31.1M. For Split-ResFRI, the version of addition has 0.53 GFlops and 36.1M parameters lower than ResNet-101, but we get 3.17% performance improvement. Besides, Split-ResFRI-concatenation also has 0.46 GFlops and 35.7M parameters lower than ResNet-101 and 3.24% higher accuracy. In sum, both of the Split-ResFRIS have lower Flops and parameter amounts and achieve better results than ResNet-101. Compared with two versions of ResFRI, Split-ResFRIs sacrifice a little bit of precision in exchange for a considerable reduction on Flops and parameter amount. For ResNeXt-29, it outperforms ResNet-101 using larger model scales, but it still trails by at least 1.23% in comparison with ResFRI and Split-ResFRI. And with respect to GoogLeNet [17], no matter it is ResFRI or Split-ResFRI, we all have achieved performance leadership. Besides, it is worth noting that both versions of Split-ResFRIs have similar flops and parameter amounts to GoogLeNet, but still possess a performance lead of over 2.8 percentage.\nMoreover, when encountering some relatively new models, ResFRI and Split-ResFRI still prove their superiority on classification task. For DenseNet [19], it possesses a similar model scale to ResFRI-concatenation, but it has a 3.13% performance disadvantage in the final result. And with respect to FocusNet [22], although it has approximate parameter amount to ResFRI, it falls behind by 2.92% at least compared with ResFRI. And Split-ResFRI is able to possess a more obvious performance advantage. Compared with ResFRI and Split-ResFRI, its disadvantage is still significant with performance trailing by at least 0.63%. Then, for WRN-28-10 [27], PyramidNet [12] and Shake-Shake(26 2\u00d796d) [28], all of them have higher flops and parameter amount than ResFRI and Split-ResFRI, but the proposed models achieve better accuracy except for Split-ResFRI-addition meanwhile. However, we want to point out that Split-ResFRI-addition has far less GFlops and parameter amount than the above models for comparison. Moreover, we notice that PyramidNet+ShakeDrop [28] has a a very approximate performance (-0.04%) to ResFRI-concatenation, which is a very competitive opponent. However, the cost of the combination of PyramidNet and ShakeDrop is 71.3% higher flops and 134.7% larger parameter amount than Res-FRI-concatenation. We think this comparison also illustrates the advantage of the proposed method when considering performance and differences on computing resources consumption of the proposed models. In summary, the experiment on CIFAR-10 dataset strongly proves the effectiveness and validity of the proposed model on image classification task, and Split-ResFRI still has enough competitiveness when taking the reduction on Flops and the number of parameters by a significant amount into consideration."}, {"title": "3.3.1. Comparison with models using extra techniques and data", "content": "In comparison with models using additional training techniques and data, our model still achieves relatively satisfying good performance. Among them, compared with the models using SAM [28], our proposed method can realize better image classification accuracy with a smaller number of model parameters and size. With respect to EffNet-L2 + SAM + Pre-Training, our model size is much smaller than EffNet-L2, and no additional data such as JFT is used, so it has a certain disadvantage in classification performance. When compared with the model utilizing much more parameters and computation resources [29, 30, 31], our model clearly does not have any advantage in image classification performance. Moreover, these models are also pre-trained on large datasets, which makes them have better generalization ability and achieve reasonably good classification results on such small datasets. In such cases, the amount of computing resource they need is very large. However, it is worth pointing out that our model is able to achieve state-of-the-art image classification accuracy with the same order of magnitude of model size and without using additional training data."}, {"title": "3.4. Experiments on CIFAR-100", "content": "The CIFAR100 dataset consists of 50k training images and 10k testing images from 100 classes and their resolution is 32\u00d732. And the detailed results of comparisons of different models will be clearly provided in Table 3 and Fig.7.\nBy checking the results given in Table 3, some conclusions can be made. ResNet-101 has a performance lag of around 8% compared with the proposed method while it utilizes approximate flops and nearly three times parameter amount of ResFRI. For Res2NeXt-series models, all of them achieves analogous performance as ResNet-101 [5] with much less flops and parameter amounts. The situation of DenseNets [19] is also similar, they further reduce the size and computational complexity of the model, but the accuracy of them is still at a comparatively low level. The best accuracy has at least a performance disadvantage of 5% compared with ResFRI-series models. Nevertheless, the inception-series models have relatively excellent performances. Particularly, GoogLeNet [17] possesses only 6.6M parameter amount but achieves an effect that is ahead of many models. Considering the results of WRN-28-10 provided in [27], it achieves a performance leap with a top-1 error rate of about 16% and dose not increase flops and parameters amount too much compared with the previous models. And it can be obtained that Res2NeXt can reach a similar performance with roughly the same number of parameters as WRN-28-10. In general, although the performance of the models is acceptable, ResFRI and Split-ResFRI have higher accuracy with much lower flops and parameter amounts compared with the two categories of models we just discussed.\nBesides, for PyramidNet [33], Shake-Shake (26 2\u00d796d) [28] and PyramidNet+ShakeDrop [28], ResFRI and Split-ResFRI achieve better performances while using less flops and parameter amount. The most light one, Split-ResFRI-addition can achieve almost the best performance with less than 9M parameter amount and 2 Gflops which are between a half and a third of the scales of the four models mentioned before. Especially, the combination of PyramidNet and ShakeDrop has the closest performance to the proposed method while possessing roughly 55% higher parameter amount and 110% more flops than the proposed models at least. Compared with the original PyramidNet, the combination of PyramidNet and ShakeDrop obtains a performance improvement of about 2%, which illustrates the possibility of follow-up work using this technology and the effectiveness of ShakeDrop. All in all, based on experimental results provided in Table 3, it can be concluded that the proposed method possesses a better precision on classification task when compared with classical networks. Except for GoogLeNet and DenseNet, all of the other models have bigger parameter amount than the proposed model but could not reach the same level of accuracy, which demonstrates the efficiency and effectiveness of the proposed model. Although GoogLeNet and DenseNet own smaller model scale than ResFRI and Split-ResFRI, our proposed method has a huge advantage in accuracy. Concretely, the version of addition of ResFRI reaches a top-1 error rate 14.09 and top-5 error rate 2.42 on CIFAR-100 dataset. In the meantime, Split-ResFRI can achieve very similar performance with at most 37.3% and 34.5% reduction of parameter amount and flops. In one word, the comparisons prove the superiority of ResFRI and Split-ResFRI on classification tasks which can be regarded as a satisfying solution in choices among CNN architectures."}, {"title": "3.4.1. Comparison with models using extra techniques and data", "content": "Compared with the model with approximate parameters, our model is able to achieve better results without using additional training means, and can also achieve considerable performance improvement after taking advantage of SAM. In comparison with remaining models, the situation is similar to the experimental comparison on CIFAR10 dataset. In general, the number of classes to distinguish has increased significantly in CIFAR100 dataset, and a model that realize excellent performance on CIFAR10 may not perform well on CIFAR100. However, the proposed model can also achieve very excellent classification accuracy on CIFAR100 dataset."}, {"title": "3.5. Experiments on Tiny Imagenet", "content": "The Tiny Imagenet dataset consists of 100k training images and 10k testing images from 200 classes and their resolution is 64\u00d764. And the results of comparisons are given in Table 5.\nThe comparative results are acquired from [22] and [25]. Specifically, the experiments on the Tiny Imagenet show that the proposed method achieves a considerably satisfying classification accuracy. For some traditional models like ResNet, DenseNet, Wide-ResNet and ResNext, both ResFRI and Split-ResFRI exceed their performance. And with respect some newly proposed methods, FocusNet [22] and DCDN [25], the proposed models still achieve better results with much lower Flops and smaller parameter amount, which demonstrates the superiority of this proposed model. In sum, ResFRI and Split-ResFRI not only outperform other comparative models on relative low-resolution datasets, but also achieve excellent results on dataset with higher pixels."}, {"title": "3.5.1. Comparison with models using extra techniques and data", "content": "Due to the limited computing resources, we are unable to perform larger-scale model training and utilize more data. Compared with the models in the table, the size of the model is much larger than the proposed method. For instance, the total number of Flops and parameters of ViT-L/16 are 56 to 80 times and 22 to 35 times of those of the model proposed in this paper. However, we reduce the number of parameters while ensuring a certain classification effect, which makes the model proposed in this paper be better applied in the actual environment."}, {"title": "3.6. Experiments on MNIST", "content": "The MNIST dataset contains 60k training images and 10k testing images from 10 classes whose resolution is 28\u00d728. And the detailed results of comparisons of different models will be clearly provided in Table 7.\nMNIST is one of the most famous dataset in machine learning and it is not taken into consideration of verification experiments of modern models due to its simplicity. In order to illustrate the performance of proposed method on simple dataset, we conduct this experiment and compare the results produced by it with some light and correspondingly-designed models. For RMDL [20] and SOPCNN [21], they obtain better results than the proposed model. Nevertheless, all of these models lack the ability to capture more complex characteristics, which is fully demonstrated by the experimental results on CIFAR-10 and CIFAR-100. On both of the datasets, the proposed model has an obvious performance advantage. Besides, with respect to some modern models such as FocusNet [22], the proposed model still achieves similar or better results. The experiment on MNIST datasets proves that the proposed model has good performance in dealing with simple image problems."}, {"title": "3.7. Experiments on FashionMNIST", "content": "The FashionMNIST dataset consists of 60k training images and 10k testing images from 10 classes and their resolution is 28\u00d728. And the results of comparisons are given in Table 8.\nFashionMNIST serves as a more complex version of MNIST which is an important data set to measure the basic capacity of models. The proposed model outperforms traditional models such as PreAct-ResNet18 [35], WideResNet-28-10 [36] and DenseNet-BC-190 [19], which are with much lower parameter amounts. Besides, with respect to newer model like Spiking CapsNet [34], the proposed model still achieves advantages by a large margin. In general, the proposed model achieves performance lead on this dataset and is a considerably light solution compared with other models."}, {"title": "3.8. Experiments on SVHN", "content": "The SVHN dataset contains 73257 training images and 26032 testing images from 10 classes whose resolution is 32\u00d732. And the detailed results of comparisons of different models will be clearly provided in Table 9.\nSVHN dataset is derived from Google Street View Door Number. By analyzing the experimental results on SVHN dataset, the proposed model achieves relatively satisfying accuracy compared with other classic models. Besides, ResFRI utilizes 3M less parameter amount than DenseNet-BC [19] achieving better classification results on FashinMNIST dataset. Similar to situations on other datasets, ResFRI and Split-ResFRI also obtain satisfying accuracy level with relatively lower parameter amount on SVHN dataset. In all, the proposed model can be regarded as a more cost-effective solution in handling image-related problems."}, {"title": "3.9. Ablation Experiment", "content": "In this section, we conduct the ablation experiment from two main aspects which are addition and concatenation version of"}]}