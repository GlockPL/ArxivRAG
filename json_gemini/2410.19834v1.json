{"title": "GNNRL-Smoothing: A Prior-Free Reinforcement Learning Model for Mesh Smoothing", "authors": ["Z. C. Wang", "X. C. Chen", "C. Y. Gong", "B. Yang", "L. Deng", "Y. F. Sun", "Y. F. Pang", "J. Liu"], "abstract": "Mesh smoothing methods can enhance mesh quality by eliminating distorted elements, leading to improved convergence in sim- ulations. To balance the efficiency and robustness of traditional mesh smoothing process, previous approaches have employed supervised learning and reinforcement learning to train intelligent smoothing models. However, these methods heavily rely on labeled dataset or prior knowledge to guide the models' learning. Furthermore, their limited capacity to enhance mesh con- nectivity often restricts the effectiveness of smoothing. In this paper, we first systematically analyze the learning mechanisms of recent intelligent smoothing methods and propose a prior-free reinforcement learning model for intelligent mesh smoothing. Our proposed model integrates graph neural networks with reinforcement learning to implement an intelligent node smoothing agent and introduces, for the first time, a mesh connectivity improvement agent. We formalize mesh optimization as a Markov Decision Process and successfully train both agents using Twin Delayed Deep Deterministic Policy Gradient and Double Duel- ing Deep Q-Network in the absence of any prior data or knowledge. We verified the proposed model on both 2D and 3D meshes. Experimental results demonstrate that our model achieves feature-preserving smoothing on complex 3D surface meshes. It also achieves state-of-the-art results among intelligent smoothing methods on 2D meshes and is 7.16 times faster than traditional optimization-based smoothing methods. Moreover, the connectivity improvement agent can effectively enhance the quality dis- tribution of the mesh.", "sections": [{"title": "1. Introduction", "content": "Numerical simulation is commonly employed to simulate and analyze fluid dynamics phenomena. It finds widespread ap- plications in various fields such as automotive engineering, aerospace, biomedicine, and nuclear physics [Yad05, SV16, FGK*20, AWAD22]. In mesh-based numerical simulation meth- ods, mesh generation is a pre-processing step preceding numeri- cal simulations [Bak05]. This step involves partitioning the com- putational domain into non-overlapping geometric elements, such as 2D polygons or 3D polyhedra. Mesh quality, characterized by orthogonality, smoothness, and density distribution, has a signifi- cant impact on simulation errors, stability, and efficiency [Knu01, Knu07, KS11]. Low-quality mesh elements can increase the stiff- ness of the matrix, leading to divergence in the simulation pro- cess. However, despite the advancements in automated mesh gen- eration techniques, the initially generated mesh quality may still fail to meet the requirements of simulation, especially for com- plex geometries. To address this issue, mesh optimization methods are widely utilized to improve mesh quality in the pre-processing step, such as smoothing [PK91, Fre97,ZS00,JDD03], edges or faces flipping [FO97, Pra18], and mesh refinement [FO97, Pra18, GH21], among others.\nImproving mesh quality during the pre-processing step is an effective way to ensure successful numerical simulations. Com- pared to adaptive mesh refinement [WLZP22, YDP*23, YMD*23, ZWK*24], which dynamically adjusts mesh density during sim- ulations, mesh smoothing is a more efficient and straightforward method to accelerate convergence. The smoothing methods en- hance the quality of mesh elements by relocating the positions of mesh nodes while preserving mesh connectivity, which is shown in Figure 1. Traditional mesh smoothing methods can be broadly categorized into two main types: heuristic-based and optimization- based methods. Heuristic-based methods adjust the positions of mesh nodes using manually designed heuristic functions. Lapla- cian smoothing and its variants are the most representative methods"}, {"title": "2. Related works", "content": ""}, {"title": "2.1. Traditional mesh smoothing methods", "content": "Laplacian smoothing. Laplacian smoothing and its variants [Fie88, VMM99, Fre97, Lo14] are the most representative methods for mesh smoothing. The simplest version achieves mesh smooth- ing by computing the algebraic average of adjacent nodes. Let x denote the coordinates of the mesh node to be smoothed, xi be the adjacent mesh nodes, and N be the number of adjacent nodes. The"}, {"title": null, "content": "smoothed position of the mesh node, x*, can be expressed as:\n$\\frac{1}{N}\\sum_{i=1}^{N} x_i$.\nDespite its simplicity and effectiveness, its drawback lies in the potential generation of inverted elements when the local region around the mesh node is non-convex. Other methods, such as area- or volume-weighted Laplacian smoothing [VMM99], Smart Lapla- cian smoothing [Fre97], and Angle-based smoothing [ZS00], have been proposed to mitigate this issue. Area- or volume-weighted Laplacian smoothing gives additional consideration to the size or volume of the adjacent elements. This approach achieves a more balanced and effective redistribution of node positions by comput- ing the geometric characteristics of adjacent elements. This con- tributes to improved mesh quality and helps avoid the generation of inverted elements. Smart Laplacian smoothing employs the same node update method as vanilla Laplacian smoothing but with an additional check before node updates. If the node movement re- sults in lower mesh quality or invalid elements, such movements are canceled. Angle-based smoothing focuses on adjusting the an- gles associated with the nodes of these elements, which also helps prevent the creation of inverted elements."}, {"title": null, "content": "Geometric element transformation method. Unlike individ- ual mesh node movement, the Geometric element transforma- tion method (GETMe) [VAGW08, VP17, VW18] performs mesh smoothing by transforming the shape of mesh elements. Let z = (z0,...,Zn-1) \u2208 C\" denote node coordinates of the mesh element, where n is the number of nodes in the element, zk = xk + iyk \u2208 C represents the coordinates of node k in complex field. It can be proven that through an iterative transformation M, where zi+1 = Mz', the mesh element gradually approaches its regular counter- parts. This transformation not only enhances mesh quality but also preserves good geometric properties, such as maintaining the cen- troid of the polygon. However, this method has a drawback: the transformation can cause adjacent elements to become invalid due to changes in mesh element size. Scaling and post-check adjust- ments are needed to ensure the validity of the elements. Addi- tional techniques are also required to ensure the effectiveness of the method during the smoothing process, making the implementation of this method relatively complex."}, {"title": null, "content": "Optimization-based smoothing. Optimization-based smooth- ing methods formalize the mesh smoothing problem as a con- strained optimization problem. They then apply standard optimiza- tion techniques such as Newton's algorithm, conjugate gradient approach, and gradient descent method [BV04] to achieve mesh smoothing. When implementing this method, it is necessary to define mesh quality metrics, objective functions, and solving al- gorithms. Commonly used mesh quality metrics include maxi- mum and minimum angles, aspect ratio, distort ratio, and others [PK91, XGC18]. The objective function is typically constructed from the mesh quality metrics, with the goal of maximizing or minimizing it. While explicit solutions are possible when the ob- jective function is simple, iterative solutions are required in most cases. Despite being able to achieve higher mesh smoothing qual- ity, optimization-based methods often have lower efficiency com- pared to heuristic smoothing methods. Moreover, their smoothing"}, {"title": "2.2. Analyze of the intelligent smoothing method", "content": "In this section, we summarize and analyze the learning mechanisms of current intelligent smoothing methods from the perspective of reinforcement learning, revealing their reliance on prior data or knowledge. We also highlight the limitations of these methods in the field of mesh smoothing.\nReinforcement learning. Reinforcement Learning is a machine learning paradigm that involves an agent learning to make de- cisions through interactions with an environment. The agent re- ceives feedback in the form of rewards, guiding it to learn op- timal behavior through trial and error [WVO12, Li18]. In rein- forcement learning, the agent observes the state st from the en- vironment at time step t, decides on an action at based on its policy \u03c0\u03b8 with parameters \u03b8. The environment transitions to the next state st+1 according to the current state and the agent's ac- tion, and provides the corresponding reward rt to the agent. A transition in reinforcement learning is typically denoted as a tuple (St, at, rt, St+1), which represents the state, action, reward, and next state at time step t. An episode is a sequence of transitions, de- noted as \u03c4 = (so, ao, ro, S1, a1, 1,..., ST-1, at-1,T-1,ST), from the initial to the final time step T. The aforementioned compo- nents constitute a Markov Decision Process (MDP), which serves as the foundation of reinforcement learning. The agent's objective is to learn a policy that maximizes the cumulative reward (return) R(\u03c4) = \u03a3\u03a5rt, where \u03b3\u2208 (0, 1) is the discount factor that deter- mines the importance of long-term rewards.\nTo provide a summary of recent intelligent smoothing methods from the perspective of reinforcement learning, we first provide the following definitions within the context of mesh smoothing and re- inforcement learning, as illustrated in Figure 2:"}, {"title": null, "content": "\u2022 Environment: Given a mesh node, the environment consists of the node itself and N neighboring nodes.\n\u2022 State: Environment state at time t is defined as st = (Xt, At), where X\u2081 = [x,x1,x2,...,x\u22121,X] \u2208 R(N+1)\u00d72 represents mesh nodes' position, A\u2081 = [dij] \u2208 R(N+1)\u00d7(N+1) represents the connection among nodes, and time t is defined as the number of iterations smoothing the mesh node. Each row in Xt repre- sents the coordinate of mesh node and its neighbors, and dij denotes whether node i is connected with node j. It's worth noting that the order of nodes in Xt is arbitrary, and it can be transformed into each other through permutation operations. The node positions here are regularized using the normaliza- tion method employed in previous works, constraining the co- ordinates of the nodes within a fixed range (such as [0,1]) [GWM*22, WZD23, WCYL23].\n\u2022 Action: The action at = \u03c0\u03b8 (st) is the movement of the mesh node to achieve smoothing.\n\u2022 Reward: The reward rt is defined as some metric related to mesh quality. It may involve traditional mesh quality metrics or the validation of the mesh elements.\nThese definitions are similar to those in DRL-Smoothing. How- ever, our environment is defined on an individual node rather than all mesh nodes, and the state of the environment further includes the connectivity between mesh nodes. We will further discuss the differences between the two in Section 3.5. Based on the above definitions, we elucidate the various forms of dependency inherent in current intelligent methods: dependency on prior data, depen- dency on prior assumptions, and dependency on prior knowledge. The analysis of these three forms of dependency is detailed as fol- lows."}, {"title": null, "content": "Prior data dependency. In our framework, the dependency on prior data refers to relying on supervised high-quality meshes for model training. NN-Smoothing learns how to output the op- timal positions for smooth nodes by imitating the behavior of optimization-based smoothing. From the perspective of reinforce- ment learning, this method can be viewed as a type of Behavior Cloning in imitation learning [HGEJ18]. Behavior cloning in rein- forcement learning involves training an agent to mimic the behavior of an expert by learning a mapping from observations to actions. In NN-Smoothing, the expert refers to optimization-based smoothing, and the dataset D = {(s,a)}Na consists of the pre-smoothed mesh state s' and the corresponding action a' ( i represents data index rather than a time step, and Nd is the number of data sam- ples). Here, the episode length is 1, which means the method di- rectly outputs the optimal smoothing positions. It is evident that this method relies on prior data, which makes it face challenges: on the one hand, training data must be obtained through an optimiza- tion method, and on the other hand, the inherent solving errors of the optimization method introduce bias into the trained model."}, {"title": null, "content": "Prior assumption dependency. Dependency on prior assump- tions refers to the use of simplified models during the learning pro- cess, limiting the expressive power of the model. Here, we will show that the GMSNet is essentially a simplified form of policy gradient method [Pet10, SLH*14]. GMSNet introduced an unsu- pervised loss for model training. Let s and s' be the states of the"}, {"title": null, "content": "mesh node before and after smoothing, respectively, qi represents the aspect ratio of the adjacent mesh element i to the smoothed mesh node, and W is the parameter of GMSNet model. The pro- posed unsupervised loss can be expressed as:\n$L(GMSNetw (s)) = L(s') = \\frac{1}{N} \\sum_{i=1}^{N} (1 - q_i)^2$,\nwhich is only dependent on the node state after smoothing. The training process of GMSNet can be formulated as solving the fol- lowing optimization problem:\nW* = argmin Es~M (L(s'))\n= arg min Es C (s') p (s),\nW\nwhere M is node state distribution on mesh. Further details about GMSNet training are provided in the Appendix A."}, {"title": null, "content": "Policy gradient method aims to find the optimal policy \u03c0* by maximizing the expection of the cumulative rewards (expected re- turn), which can be formalized as:\n\u03c0* = arg max \u0395\u03c4\u223c\u03c0\u03bf (R(\u03c4))\n\u03c0\u03bf\n= arg max \u2211R(\u03c4)\u03c1(\u03c4/\u03b8)\n\u03c0\u03bf \u03c4\nT-1\n= arg max \u03a3\u03a3\u03c4(\u03c4|\u03b8),\n\u03c0\u03bf \u03c4\u03c4=0\nwhere p(\u03c4/\u03b8) is the probability of the episode t occurring given the model parameter \u03b8. Let r\u2081 = -L(St+1), episode length T = 1, and assume episode distribution is independent on policy parameter \u03b8, we can rewrite Equation 4 as:\nT-1\n\u03c0* = arg max \u03b3rip(\u03c4\u03af\u03b8)\n\u03c0\u03bf \u03c4\u03c4=0\n= arg max -L(s')p(s,a,s')\n\u03c0\u03bf \u03c4=(s,a,s')\n= arg max - Es C(s')p(s)\n\u03c0\u03bf\n= arg min (s')p(s),\n\u03c0\u03bf\nwhich is the same as GMSNet's optimization problem. It can be seen that GMSNet introduces two prior assumptions in policy gra- dient: episode length assumption, T = 1, and the assumption of independent node state distribution. However, these priors limit the expressive power of the model and have yet to be validated for their effectiveness."}, {"title": null, "content": "Prior knowledge dependency. Dependency on prior knowledge refers to the reliance on knowledge from traditional methods to achieve intelligent smoothing. Such dependency can be seen in DRL-Smoothing, which combines traditional smoothing methods with intelligent smoothing methods. Specifically, denoting Xtrans as the node positions output by traditional smoothing methods and xai as the node positions output by intelligent methods, the final node positions are obtained by summing the two: x = Xtrans + Xai. How- ever, through experiments we show that this approach introduces a prior dependency on traditional methods. As illustrated in Fig- ure 3, we conducted model training with and without the inclusion"}, {"title": "3. Methodology", "content": "To address the current dependency of intelligent smoothing meth- ods on prior data or knowledge, we further leverage the learning capabilities of graph neural networks on unstructured data and re- inforcement learning methods to train intelligent agents, proposing a new mesh smoothing model, GNNRL-Smoothing, as illustrated in Figure 4. During mesh smoothing, we initially take the mesh node and its neighboring nodes as inputs, encoding the node states through graph convolution to generate state encoding. In the node smoothing agent, we use an Actor-Critic algorithm, TD3, to train our continuous action agent. The actor outputs the displacement of the mesh node to smooth the mesh elements. In the connectivity improvement agent, we employ a value-based algorithm to deter- mine which edges need to be flipped. The Q-Network outputs the Q-value for each edge, and the agent selects the edge with the high- est Q-value for flipping to enhance the mesh connectivity. Through the collaboration of these two agents, we can achieve optimization operations for smoothing low-quality meshes. Notably, GNNRL- Smoothing does not require labeled expert data or prior knowledge of traditional smoothing methods during training, making it a more efficient and flexible approach."}, {"title": "3.1. Mesh state encoding", "content": "Unlike images and natural language, mesh data is a typical exam- ple of unstructured data. Using traditional neural networks often encounters issues with varying numbers of adjacent nodes and in- put data sequence order problems. To address these challenges, we employ GNNs to encode the state of the mesh, as illustrated by the GNNEncoder module in Figure 4. As defined in Section 2.2, the state of the mesh node i at time step t consists of two compo- nents: the coordinates of the mesh nodes, represented by X, and the connectivity information, represented by A. For notational conve- nience, we omit the node index i and time step t, and the following calculations are performed with respect to a specific node and time step. To encode the mesh state, we employ an Encoder-Processor- Decoder architecture, which consists of two MLPs as the encoder (MLPe) and decoder (MLP) for node features, and utilized Graph Transformer (GT) [SHF*, RGD*] to encode the features of nodes and the connectivity of the mesh. The entire process of encoding the mesh feature points can be represented as follows:\nX = MLP (X), \u0160 = GT (X, A), S = MLPa (\u0160).\nThe encoded mesh state S is a tensor of shape (N+1) \u00d7 df, where N is the number of neighboring nodes of mesh node, and df is the feature dimension."}, {"title": null, "content": "The GT block consists of a self-attention layer and a global attention layer to learn the graph representation of the state, which is shown is Figure 5. Let X = [X0,X1,..., XN-1,\u0100N] \u2208 IR(N+1)xdf, the node features after graph convolution, X = [0,1,..., XN-1,N]\u00b9, are computed as:\nk = W1k+ \u03a3\u00e2kjW2\u0101j,\njEN (k)\nwhere N(k) is the set of neighboring nodes connected to node k,"}, {"title": null, "content": "W1 and W2 are the model parameters, and \u00e2kj is the attention co- efficient computed via dot product attention. The node features ob- tained via global attention are:\nX = GlobalAtten (X)\nThe final node features are computed via a MLP with batchnorm [IS] (BN), which can be expressed as:\nS = GT(X, A) = MLP (BN (\u00c2Y) + BN (X)).\nTo further enhance the model's expressive power, we utilize dif- ferent mesh state encoders in the node smoothing agent and the connectivity improvement agent, respectively."}, {"title": "3.2. Actions", "content": "Node smoothing agent. In DRL-Smoothing, the model employs Laplacian smoothing as part of its output to achieve mesh smooth- ing (in fact, almost 100%). In contrast, our proposed node smooth- ing movement model directly outputs the displacement of mesh node Ax, and update the node coordinate of node i by x+1 = x + Ax. Given the encoded mesh features S, we map it to the displacement vector through an MLP:\nx = MLPactor (Simapping;:]),\nwhere mapping; is the index of the centering node (red node in Figure 2).\nIn standard continuous action reinforcement learning algorithms,"}, {"title": "3.3. Reward design", "content": "Node smoothing agent. Reward design is crucial in reinforcement learning, as it directly influences agent behavior. Well-designed re- wards guide the agent toward desired goals, whereas poorly de- signed rewards lead to suboptimal behavior. In the proposed model, we have made the following adjustments to the reward function in DRL-Smoothing: firstly, the introduction of an advancement-based reward function term; secondly, modification of the penalty term for invalid elements.\n,j\nAt time step t, let qi; denote the aspect ratio of the adjacent mesh element j of node i, and q, j = + represent the mesh quality metric. When qij is 1, the shape of the element is optimal, and when it is 0, the element is degenerated. This transformation qi,j = j is necessary because the original aspect ratio ranges from 1 to positive infinity, which would lead to non-convergent training. Despite the difference, this mesh quality metric is equivalent to that defined in DRL-Smoothing. Then,a potential-based function can be defined as:\n$(s) = min q(i,j),\nj\u2208 {1,...,Ni}\nwhich calculates the minimum quality of the mesh elements ad- jacent to the mesh node. In DRL-Smoothing, the reward is de- fined as r = $(s+1). In our proposed method, we modify it to r = $(s+1) \u2013 $(s). The agent receives a positive reward even if it takes no action in DRL-Smoothing; in contrast, our proposed reward grants positive rewards only for actions that enhance mesh quality. Furthermore, we can fine-tune the reward threshold by ad- justing the value of y. With a small y, the agent must achieve sub- stantial quality improvements to earn a reward.\nDRL-Smoothing penalizes invalid outputs with a constant value of -1. In contrast, we propose a more refined approach to penal- izing invalid elements. By assigning a negative quality metric to invalid elements, we can convey the severity of bad actions to the agent. We define an indicator function \u2161(qj), which equals 1 for valid elements and -1 for invalid elements. Our revised potential- based function is defined as follows:\n$(s) = II(q(i,j))\nmin\nj\u2208 {1,...,N;}\nThe definition of the reward function remains unchanged.\nTopology improvement agent. For the connectivity improve- ment agent, there are three possible outcomes after executing an action: mesh quality improvement, no change (no flipping opera- tion), and mesh quality degradation. Since we are using discrete actions, we can simply define the good and bad actions without considering the degree of goodness or badness. Therefore, unlike the continuous reward function used in node movement models, we assign constant rewards of 1, -0.1, and -1 to the above three cases, respectively."}, {"title": "3.4. Surface mesh smoothing", "content": "Extending the 2D node smoothing agent to 3D is not straightfor- ward. The main challenge lies in ensuring that the mesh nodes re- main on the original geometric surface, which means preserving"}, {"title": null, "content": "original geometric features. Previous intelligent smoothing meth- ods do not provide an explicit guarantee for this. For example, NN- Smoothing directly uses supervised learning to predict 3D mesh node coordinates, while DRL-Smoothing completely disregards the mesh nodes' adherence to the surface. To make our node smoothing agent achieve feature-preserving surface mesh smoothing, we pro- pose a extra reward term based on local surface fitting. As shown in Figure 6, we fit a local quadratic surface to the mesh node and its first-order neighbors after normalization. Let x* denote the smoothed mesh noded produced by the 3D node smoothing agent and x' the projected node on the fitted surface, then the extra sur- face fitting reward is defined as the negative Euclidean distance be- tween x* and x':\nrf (si) = - ||x* -x ||2\nThis reward term is added as an extra component to Equation 14, forming the reward function for 3D surface mesh smoothing. It is worth noting that the projection method used here directly projects to the surface parameter domain rather than solving for the shortest distance from x* to the surface. Despite its simplicity, experiments show that this method is sufficient to ensure the adherence of the mesh nodes to the surface. As for the connectivity enhancement method, since it only changes the connectivity of the mesh nodes without altering their coordinates, extending it to 3D only requires changing the input features to three dimensions."}, {"title": "3.5. Model training", "content": "In our proposed approach, we utilize two intelligent agents: a node smoothing agent and a connectivity improvement agent, responsi- ble for node smoothing and edge flipping, respectively. We train these agents independently using two classic reinforcement learn- ing algorithms: TD3 for the node smoothing agent and D3QN for the connectivity improvement agent. Due to space limitations, we do not provide detailed explanations of these algorithms. While the previous sections focused on the implementation of the models, in- cluding the actor and Q-Network, this section delves into the spe-"}, {"title": null, "content": "cific implementation details related to the environment, covering state transitions and environment resets.\nThe process of mesh smoothing typically involves iterating nodes on the mesh, raising the question of how to define state tran- sitions. In the DRL-Smoothing method, given the current state and action, the next state is randomly selected by moving to the next node and its adjacent nodes on the mesh. However, this method of state transition is problematic for reinforcement learning because the transition of states changes when the indices of mesh nodes change. This means that there is no state transition probability dis- tribution p(St+1 st, at), and it does not constitute a Markov Deci- sion Process, which is the foundation of reinforcement learning. As shown in Figure 3, in this scenario, the model heavily relies on traditional methods for smoothing. In the GNNRL-Smoothing, we apply the action directly to the node to be smoothed and output the modified mesh and its neighboring node positions as the state. In this case, there is a potential pattern of state transitions. Similarly, the state transition in the connectivity improvement agent follows the same approach.\nAnother issue to consider is when to terminate an episode, as it can potentially be infinite in length. One possibility is to terminate the episode when an action results in invalid elements, while another involves setting a maximum threshold for T. Theoretically, the optimal smoothing strategy is independent of episode length T. Analyzing from the perspective of policy gradient method, it can be derived from Equation 4. Let T\u2208 N, the objective function of policy gradient method is:\nJ (\u03c0) = \u2211R (\u03c4) p (\u03c4|0)\n\u03c4\nT-1\n=\u03a3\u03a3rip(0)\n\u03c4\u03c4=0\nT-1\n=\u2211(\u03a3\u03b3 (0+1) \u2013 \u03c6 (5,)) p (\u03c4|\u03b8)\n\u03c4\u03c4=0\n= \u2211( ((st) \u2013 $ (so)) p (t|\u03b8)\n\u03c4\n<(($*so) \u2013 (so)) p (\u03b8),\nwhere (s*so) represents the best mesh quality when the node at the optimal state s* given the initial state so, and the equality in Equation 20 is achieved when the final state is optimal. It can be observed that for a given T, the policy corresponding to the up- per bound of the objective function is when the final state resides at the optimal mesh position. The only difference for different T values lies in the number of steps taken to reach the optimal node position. However, if T is too large, the model could potentially reach the optimal position at any step, significantly widening the exploration space. Therefore, a smaller T is more appropriate. In our model, we adopt a maximum episode length of 2. In Section 2.2, we pointed out that GMSNet uses a policy gradient method with T = 1. The situation is similar for T, although the two use dif- ferent reward functions, with the upper bound of its optimization"}, {"title": "4. Experiments", "content": "In this section, we first compare the performance of GNNRL- Smoothing with traditional smoothing methods and intelligent smoothing methods in planar mesh smoothing. Next, we conducted ablation experiments to demonstrate the effectiveness of action de- sign, reward function design, and episode length selection. Finally, we evaluate the performance of our proposed model on 3D surface mesh smoothing and validate the effectiveness of the reward func- tion based on local surface fitting."}, {"title": "4.1. Baselines and experimental setup", "content": "The baseline models included in our study consist of traditional smoothing methods: Laplacian smoothing, Angle-based smooth- ing, Optimization-based smoothing, and GETMe smoothing. Ad- ditionally, three intelligent smoothing models were included: NN- Smoothing model, GMSNet model, and DRL-Smoothing model. The traditional smoothing methods were configured following the specifications outlined in the GMSNet paper. The NN-Smoothing model and GMSNet model were trained and tested using the parameters provided in their original papers, while the DRL- Smoothing model utilized the original code provided by the au- thors. Similar to GMSNet, GNNRL-Smoothing was trained on ran- domly generated meshes (as shown in Figure 7), and the model's parameters were searched through hyperparameter optimization."}, {"title": "4.2. Planar mesh smoothing", "content": "We tested the smoothing performance of all mesh smoothing meth- ods on four mesh cases and calculated the minimum and average \u011f of the mesh elements before and after smoothing. It is worth noting that although different mesh quality metrics can be used to evaluate mesh quality, they are often equivalent for triangular meshes [Knu01]. For simplicity, we use q as the sole metric in this study. The meshes used for testing were not included in the training dataset, which helps demonstrate the generalization capability of the intelligent smoothing models. For each mesh, we conducted five experiments and reported the average results. Figure 8 shows the mesh cases before and after smoothing using GNNRL-Smoothing; Table 3 presents a comprehensive per- formance comparison of various smoothing methods applied to dif- ferent mesh cases. Due to space constraints, the comparison of in- telligent smoothing methods is provided in Figure 17 in Appendix C."}, {"title": "4.2.1. Smoothing performance", "content": "Firstly, as shown in Figure 8, the GNNRL-Smoothing model sig- nificantly improves the orthogonality and smoothness of the mesh. It achieves superior mesh smoothing quality even for highly dis- torted meshes such as case 3. Secondly, from Table 3, intelligent smoothing methods have achieved highly competitive results com- pared to traditional smoothing methods. In some cases, such as NN- Smoothing in case 4 with a \u011fmean of 0.882, and GMSNet in case 3"}, {"title": "4.2.2. Efficiency", "content": "The runtime efficiency of various smoothing methods was mea- sured through experiments. The experiments were conducted on an Intel(R) Core(TM) i7-9700KF CPU @ 3.60GHz, with an NVIDIA RTX TITAN as the GPU. All code was implemented in Python and ran on the GPU. The experimental results are shown in Table 4. From Table 4, it can be seen that the Laplacian Smoothing has the highest speedup compared to the optimization-based methods due"}, {"title": "4.2.3. GNNRL-Smoothing vs DRL-Smoothing", "content": "GNNRL-Smoothing and DRL-Smoothing both utilize the Actor- Critic architecture for training, allowing us to understand the learned policy by visualizing the critic model's estimation of state- action values. As shown in Figure 11, a simple mesh node and its adjacent nodes, marked with black dots, represent the current state s, and the optimal node position is indicated by a red trian-"}, {"title": "4.3. Ablation study", "content": ""}, {"title": "4.3.1. Action design", "content": "In the action design of the node smoothing agent, we constrained the action range to a narrow range (-0.25, 0.25). We highlighted the potential benefits of employing a smaller action range for facilitat- ing model training and convergence. Here, we demonstrate through experiments that a smaller action range improves model perfor- mance by increasing sample efficiency, which means it can gen- erate more samples with positive rewards to facilitate agent learn- ing. Firstly, agents with different action ranges (0.25, 0.5, 0.75, and 1) were trained using the same parameter configuration. For effi- ciency, we only tuned the parameter exploration noise through grid search. Exploration noise was sampled at 10 points within the range of 0 to 0.5, and for each sample point, 5 experiments with different seeds were conducted, totaling 200 experiments. For each action range, we show the top-5 average rewards with a state window size of 1000, as shown in Table 5, and the training curves are depicted in Figure 12. The results indicate that a smaller action range yields higher rewards and better convergence speed. Secondly, we mon- itored the relative proportion of samples with positive rewards to those with negative rewards during training, as shown in Figure 13. It can be observed that a smaller action range results in a higher proportion of samples that help the model learn effective strategies. Conversely, as the action range increases, the proportion of positive rewards decreases, leading to slower model convergence."}, {"title": "4.3.2. Reward design", "content": "For the node smoothing agent, we made two modifications to the reward function: introducing an advancement-based reward term and an adaptive penalty for invalid elements. We conducted experi-"}, {"title": "4.3.3. Episode length", "content": "To investigate the impact of different episode lengths on model per- formance, we conducted experiments with episode lengths of 2, 4, 8,16 and 32, respectively. The experimental results on case 2 and case 4 are shown in Figure 14. It can be observed that the mod- els were trainable with different episode lengths. However, as the episode length increased, there was a slight decrease in the smooth- ing performance of the models. Although we note that the opti- mal strategy corresponding to different episode lengths remains the same, the training difficulty varies with different lengths, with shorter episode lengths being easier to train. The agents trained with GMSNet reward have also been shown in Table 7. The av- erage reward is indeed linearly related to the episode length, con- sistent with Equation 24. The experimental results demonstrate the correctness of the episode length we adopted, while also provid-"}, {"title": "4.4. Surface mesh smoothing", "content": ""}, {"title": "4.4.1. Smoothing performance", "content": "To evaluate the performance of our proposed method for sur- face mesh smoothing, we selected four test cases from the AIM@SHAPE library. Our method was applied to each mesh for 10 smoothing iterations, without any additional operations to en- sure mesh adherence to the surface after smoothing. The meshes before and after smoothing are shown in Figure 15, and the corre- sponding mesh quality metrics are presented in Table 8. As shown in Figure 15, despite multiple smoothing operations performed by the proposed model on complicated surface meshes, the smoothed meshes still preserve the original geometric features and shape. Furthermore, the model is capable of smoothing and optimizing surface meshes across various local surface geometries without requiring additional operations, such as node projection. Table 8 demonstrates the"}]}