{"title": "MMedAgent: Learning to Use Medical Tools with Multi-modal Agent", "authors": ["Binxu Li", "Tiankai Yan", "Yuanting Pan", "Zhe Xu", "Jie Luo", "Ruiyang Ji", "Shilong Liu", "Haoyu Dong", "Zihao Lin", "Yixin Wang"], "abstract": "Multi-Modal Large Language Models (MLLMs), despite being successful, exhibit limited generality and often fall short when compared to specialized models. Recently, LLM-based agents have been developed to address these challenges by selecting appropriate specialized models as tools based on user inputs. However, such advancements have not been extensively explored within the medical domain. To bridge this gap, this paper introduces the first agent explicitly designed for the medical field, named Multi-modal Medical Agent (MMedAgent). We curate an instruction-tuning dataset comprising six medical tools solving seven tasks, enabling the agent to choose the most suitable tools for a given task. Comprehensive experiments demonstrate that MMedAgent achieves superior performance across a variety of medical tasks compared to state-of-the-art open-source methods and even the closed-source model, GPT-40. Furthermore, MMedAgent exhibits efficiency in updating and integrating new medical tools.", "sections": [{"title": "1 Introduction", "content": "Multi-modal Large Language Models (MLLMs) have made considerable progress across diverse tasks with inputs from different medical imaging modalities (e.g., Magnetic Resonance Imaging, Computed Tomography, X-ray) in healthcare, including Visual Question Answering (VQA) (Moor et al., 2023; Zhang et al., 2023a; Li et al., 2023), image classification (Sun et al., 2023), image segmentation (Ma et al., 2024a), and Medical Report Generation (MRG) (Thawkar et al., 2023; Hamamci et al., 2024), etc. Despite these advancements, MLLMs often exhibit limitations in seamlessly solving multiple tasks across different medical imaging modalities. Although recent large medical models (Zhang et al., 2023b; Tu et al., 2024; Wu et al., 2023; Yang et al., 2024; Zhao et al., 2024a) have attempted to address this challenge, they remain limited to handling a narrow range of tasks across a restricted set of imaging modalities and cannot be efficiently extended to new tasks or more imaging modalities. Furthermore, these generalists typically do not provide expert-level responses comparable to those of specialized MLLMs customized for specific tasks.\nOne way to address this issue is to build an AI Agent, an AI system driven by Large Language Models (LLMs) that integrates various domain expert models as tools. Such a system can understand user instructions, make decisions, and select the appropriate tools to execute any specific task, thereby generating expert-level responses for any given request (Xie et al., 2024; Chen et al., 2023; Wang et al., 2024; Liu et al., 2023a; Tao et al., 2023). Despite the significant success of AI agents in the general image domain (Tao et al., 2023; Qin et al., 2023; Wang et al., 2023a), no such agents currently exist in the medical domain. Although several works (Tang et al., 2023; Schmidgall et al., 2024; Li et al., 2024; Fan et al., 2024) in the medical field use the term \u201cagent\u201d in their methods, they focus on utilizing LLMs to play various roles and collaborate on complex tasks, in which an \u201cagent\" refers to a specific role.\nIn this work, we aim to build the first AI agent specifically for the medical domain, termed as Multi-modal Medical Agent (MMedAgent). We choose LLaVA-Med (Li et al., 2023) as the backbone and aim to extend its capability to handle various language and multi-modal tasks, including grounding, segmentation, classification, grounding, MRG, and Retrieval-Augmented Generation (RAG). The first step to building MMedAgent is to collect the state-of-the-art (SOTA) methods for each task, hereafter referred to as \"tools\". During this phase, we identify a lack of an effective tool"}, {"title": "2 Related Work", "content": "2.1 Medical MLLMs\nLLMs present fertile new ground for research that pushes the frontier of the medical domain. Unlike natural domains, the intrinsic complexity of medical data, which includes multiple sources and modalities, has led most LLMs in the medical field to focus on narrowly defined tasks using language and text alone. Singhal et al. (Singhal et al., 2023) curate MultiMedQA, a benchmark for medical question-answering datasets, and propose Med-PaLM, which utilizes instruction prompt tuning tailored to medical domains based on PaLM (Chowdhery et al., 2023). Med-PaLM performs encouragingly on the axes of the human evaluation framework.\nRecent progress on LLMs has been made on multi-modal conversational capability (Moor et al., 2023; Zhang et al., 2023b; Tu et al., 2024; Zhang et al., 2023c,a; Thawkar et al., 2023; Sun et al., 2023; Wu et al., 2023; Li et al., 2023; Ma et al., 2024a; Yang et al., 2024; Zhao et al., 2024a; Hamamci et al., 2024). Owing to the diversity inherent in medical data and tasks, LLMs have initially been localized to specific imaging domains such as X-ray (Thawkar et al., 2023), \u0421\u0422 (Hamamci et al., 2024), and histology (Sun et al., 2023), or tailored for different tasks such as segmentation (Ma et al., 2024a; Lei et al., 2023) and medical report generation (Wu et al., 2023). In contrast, generalist models expand these capabilities by enabling a single LLM to cover a wider range of imaging modalities and tasks by enlarging the pre-training datasets greatly (Zhang et al., 2023b; Li et al., 2023; Zhao et al., 2024a; Liu et al., 2023a; Yang et al., 2024). Although generalist models are capable of handling a wide range of medical modalities and tasks, they face limitations in scalability when incorporating additional skills and lack specialization in specific tasks."}, {"title": "2.2 AI Agent", "content": "A multi-modal AI Agent is a system that achieves users' general-purpose goals by perceiving the environment and making decisions based on the perceptions (Xie et al., 2024; Wooldridge and Jennings, 1995). Recent works utilize LLMs as planners to understand multi-modal input from environments and make decisions to call different tools to achieve goals. Based on whether the LLM is open source or not, (Xie et al., 2024) classifies multi-modal AI Agents into two types: (i) closed-source LLMs as planners, which utilize prompt technique to enable LLMs to make decisions (Chen et al., 2023; Wang et al., 2024); (ii) fine-tuned LLMs as planners, where an LLM is fine-tuned to understand instructions, make decisions, and call tools/APIs (Liu et al., 2023a; Tao et al., 2023). Our MMedAgent belongs to the second type.\nMulti-modal AI Agents have achieved great success in various applications. For example, (Tao et al., 2023; Gur et al., 2023; Zhan and Zhang, 2023) apply agents to control the website or user interface. Some works (Qin et al., 2023; Wang et al., 2023c) focus on robotics or embodied AI which applies multi-modal LLMs to perceive and interact"}, {"title": "3 MMedAgent", "content": "Multi-modal Medical Agent (MMedAgent), a system based on an MLLM, is designed to seamlessly manage diverse medical tasks by integrating various open-source medical models. MMedAgent comprises two components: (1) an instruction-tuned multi-modal LLM that functions as an action planner and results aggregator, and (2) a collection of medical tools tailored to the agent, each targeting specific tasks in the medical domain. We first present the fundamental workflow of MMedAgent in Section 3.1, followed by a description of creating an instruction-tuning dataset for training the multi-modal LLM as an action planner in Section 3.2. The details of medical tasks and corresponding tools incorporated in MMedAgent are described in Section 3.3."}, {"title": "3.1 Workflow", "content": "Following LLaVA-Plus (Liu et al., 2023a), MMedAgent is built to learn to utilize a wide range of multi-modal medical tools, extending the MLLMs' capabilities to analyze and accomplish various medical tasks. As shown in Figure 1, the workflow consists of four parts: (1) users provide an instruction \\(X_q\\) and a medical image \\(I_q\\); (2) MLLM works as an action planner, which understands \\(X_q\\) and \\(I_q\\) and then generates a formatted instruction \\(X_{tool}\\) to call a specific tool. (3) The tool is executed given \\(I_q\\) and the output \\(X_{result}\\) of the tool is sent to the MLLM. (4) The MLLM aggregates the output with \\(X_q\\) and \\(I_q\\) and generates the final answer \\(X_{answer}\\) to users. We train the agent end-to-end with an auto-regressive objective on the generated sequence - \\(X_{tool}\\) and \\(X_{answer}\\) to enable the model to use correct tools and answer questions based on the tool's results."}, {"title": "3.2 Instruction Tuning", "content": "In order to ensure MMedAgent simultaneously performs as both action planner and results aggregator, we adopt the unified dialogue format proposed by (Liu et al., 2023a), illustrated in Figure 2. Specifically, upon receiving a user's input, MMedAgent generates three components in its outputs: (1) Thought, which determines whether MMedAgent can independently solve the user's instructions or if external tools are required, and if so, identifies the appropriate tool; (2) Action, which enumerate a list of API calls necessary to execute the thought. This comprises two sub-fields: API Name and API Params. If the action list is null, no API call is initiated. (3) Value, which provides a natural language response aggregated by the MLLM along with the outputs from the involved tools. As depicted in Appendix Figure 5, we construct the instruction data by querying GPT-40 through one-shot learning, presenting an example that demonstrates the input and output of MMedAgent. We set a fixed System instruction prompt for each tool and select several examples as conversation templates (User_1 and Assistant_1 in Appendix Figure 5). The tool processes the to generate the instruction data from the dialogue."}, {"title": "3.3 Medical Tasks and Tools", "content": "Our MMedAgent possesses the capability to access a diverse array of tools with the scalability to handle various tasks. As shown in Table 1, we integrate six tools that encompass seven representative tasks in medical domains, i.e., (1) grounding, (2) segmentation with bounding-box prompts (Segmentation), (3) segmentation with text prompts (G-Seg), (4) medical imaging classification, (5) Medical Report Generation (MRG), (6) retrieval augmented generation (RAG), and (7) VQA. Note that no additional tools are required for the VQA task since we utilize LLaVA-Med, which originally supports it. Each tool functions as a specialist, exhibiting exceptional"}, {"title": "3.3.1 Grounding", "content": "Grounding, also known as detection, aims to identify and localize specific objects within an input image by generating the coordinates of bounding boxes containing the objects. To the best of our knowledge, no existing medical models can simultaneously process images from different modalities. Consequently, we propose a generalized grounding tool tailored for the medical domain. Specifically, we choose to fine-tune Grounding DINO (Liu et al., 2023b), an open-set object detector, to the medical imaging field.\nOur first step is to collect multiple medical image segmentation datasets, including FLARE2021 (Ma et al., 2022), WORD (Luo et al., 2022), BRATS (Menze et al., 2015), Montgomery County X-ray Set (MC) (Jaeger et al., 2014; Candemir et al., 2014), VinDr-CXR (Nguyen et al., 2022), and multi-modal cell segmentation dataset (Cellseg) (Ma et al., 2024b). As detailed in Appendix Table 4, these datasets target different modalities, organs, or diseases, each including the original imaging along with their corresponding pixel-level segmentation annotations. These segmentation masks are further transformed into bounding boxes by extracting the minimal outer rectangle around each object. The coordinates of the bounding boxes and the corresponding object labels are then recorded as the grounding labels in each dataset.\nBased on the released pre-trained weights, we fine-tuned the Grounding DINO with the dataset described above as well as two common datasets in the natural image field, i.e., COCO (Lin et al., 2014) and Flickr30k (Plummer et al., 2015), to maintain model's ability in detecting common objects."}, {"title": "3.3.2 Other Tasks", "content": "Segmentation involves identifying and delineating the region of interest (ROIs) of an image. In our scenario, we consider interactive segmentation when a bounding box that covers the ROIs is provided. This setting has become popular since the development of Segment Anything (SAM) (Kirillov et al., 2023). We select MedSAM (Ma et al., 2024a), which fine-tunes SAM to the medical field, as our tool. The prompts are limited to bounding boxes because they provide more precise guidance to SAM (Mazurowski et al., 2023). Specifically, in this scenario, we consider the users to provide the position of the bounding box in which MedSAM can be directly applied to obtain the ROI masks.\nG-Seg refers to combining grounding with SAM. It aims to address a more common scenario when"}, {"title": "4 Experimental Settings", "content": "MMedAgent is initialized by LLaVA-Med 60K-IM, instruction-tuned using LoRA (Hu et al., 2021) for 15 epochs, and conducted over approximately 72 hours on two 80G NVIDIA A100 GPUs. The rank of LoRA is set to 128, and the training batch size is set to 48. We employ AdamW (Loshchilov and Hutter, 2019) as the optimizer alongside a cosine learning rate schedule peaking at 2e-4. We generate 48K instruction-tuning data, consisting of 15K augmented VQA instruction following the method from LLaVA-Plus (Liu et al., 2023a) derived from 60K inline mentions (Li et al., 2023), 10K data points for detection, 3K for RAG, 5K each for segmentation, classification, MRG, and G-Seg. Data sources are shown in Table 1."}, {"title": "5 Experimentals", "content": "We conduct experiments on MMedAgent to answer three research questions: (1) What is the performance of MMedAgent in addressing diverse medical tasks across various modalities (Section 5.1)? (2) Does the instruction-tuned MMedAgent exhibit superior performance in open-ended biomedical dialogue (Section 5.2)? (3) What is the efficiency of MMedAgent in invoking tools or incorporating new tools (Section 5.3)?"}, {"title": "5.1 Various Medical Tasks", "content": "5.1.1 Evaluation Criterion\nTo evaluate the performance of MMedAgent on various complex medical tasks, we create an evaluation dataset consisting of 70 diverse questions. For this dataset, we initially select 10 concepts randomly from the Merck Manual for RAG and 60 unseen images of different tasks from respective data sources. These include 10 images each for organ grounding, disease grounding, and cell grounding, along with 20 X-ray images for MRG and 10 images across various modalities for classification. Notably, the VQA task evaluation is shown in Section 5.2. Due to the inability to describe the segmentation task linguistically, we provide the qualitative results shown in Section 5.1.3. Then we utilize the same prompt as outlined in Section 3.2 to generate the instruction-tuning data for evaluation. Subsequently, we separately feed the data into GPT-40, MMedAgent and other benchmarks to obtain the outputs. GPT-40 is a newly released multimodal model with strong visual understanding capabilities. According to the testing from OpenAI, it surpasses GPT-4 Turbo and has a faster inference speed. Thus, the output from GPT-40 can be viewed as a strong benchmark. All the outputs will be assessed by GPT-4 and rated on a scale from 1 to 10 based on their helpfulness, relevance, accuracy, and level of details. We provide GPT-4 with figure captions and include inline mentions from 60K-IM for the VQA task. The detailed prompts are illustrated in Figure 6. For the MRG task, the reports are taken as captions of the input figures. For detection and other tasks without a caption in the original data, we generate the captions by combining the images with the labels. For instance, \"A CT scan showing the kidney organ.\". Since the scores are generated by an LLM, their rank better reflects the capability rather than the absolute values. Based on the output from GPT-40, we propose a relative score, defined as \\(S^*/S_{GPT-40}(\\%)\\), to indicate the performance change caused by other MLLMs. Here, \\(S^*\\) refers to the score of outputs generated by \\(*\\), with \\(* \\in \\{\text{RadFM, LLaVA-Med, MMedAgent}\\}\\). A higher score indicates a superior output quality. During the evaluation, MMedAgent dynamically selects, activates, and executes tools in real-time, then aggregates the obtained results from these tools to answer questions."}, {"title": "5.1.2 Experimental Results", "content": "As illustrated in Table 2, MMedAgent significantly outperforms all other baselines on various tasks. Notably, the overall score of MMedAgent is 1.8 times higher than that of LLaVA-Med. We also consider LLaVA-Med (Tool in Test), an enhanced version of LLaVA-Med that incorporates the internal output of tools. MMedAgent maintains its superior performance in this case. Furthermore, the scores for organ grounding, disease grounding, and MRG exceed 100%, indicating that MMedAgent surpasses GPT-40 in these tasks. These results underscore the superior efficiency of MMedAgent in diverse medical tasks across various modalities."}, {"title": "5.1.3 Case Study", "content": "A detailed visual comparison between LLaVA-Med and MMedAgent is illustrated in Figure 3. Given the user queries on tasks involving analyzing the images, such as classification, grounding, and segmentation tasks, LLaVA-Med only generates simple conversational responses without solving the given requests (highlighted in Red) and it is unable to generate visualized results. In contrast, MMedAgent effectively addresses these questions by activating the appropriate tools, integrating their outputs, generating accurate responses (highlighted in Green), and visualizing the results. This is guaranteed by the precise selection of tools by MMedAgent and the superiority of the tools themselves. When encountering language generation-based tasks, i.e., MRG and RAG, LLaVA-Med fails to provide an in-depth analysis of the images. However, MMedAgent provides more straightforward and accurate responses by utilizing the tools designed specifically for these tasks."}, {"title": "5.2 Open-ended Medical Dialogue", "content": "To evaluate the capability of visual question-answering tasks, we follow the setting of open-ended medical dialogue in LLaVA-Med (Li et al., 2023). Here, we use the same test data as LLaVA-Med, which consists of 193 novel questions and 50 unseen images from PMC-15M (Zhang et al., 2024). This dataset contains 5 modalities and can be divided into two main classes: conversation questions and detailed description questions. We also utilize the relative score, introduced in Section 5.1.1, as the evaluation criterion. Since this is a pure language task, we select the output from GPT-4 rather than GPT-40 as the reference score.\nAs shown in Table 3, performance is categorized by either question types (conversation and description) or image modalities (X-ray, MRI, Histology, Gross and CT). After instruction-tuning on the tool learning dataset, MMedAgent performs better on both types of questions. Moreover, MMedAgent"}, {"title": "5.3 Tool Utilization", "content": "The superior performance of MMedAgent on the various tasks described above depends on accurately understanding users' inputs and activating the correct tools. After training MMedAgent for 15 epochs, the tool selection accuracy reached 100%, demonstrating MMedAgent's ability to select the appropriate tools without errors.\nOne significance of MMedAgent is its ability to adapt to new tools. Here, we consider two scenarios. Firstly, when a superior tool for tasks that MMedAgent is already equipped to handle becomes available, the API name of the outdated tool can be seamlessly replaced with that of the new tool, eliminating the need for additional retraining. Secondly, to extend MMedAgent to a new task, it is sufficient to generate a small set of instruction-tuning data for this specific task and fine-tune the agent accordingly, rather than retraining it from the beginning.\nTo verify this capability, we simulate a new tool called \"Pseudo Tool\", generate an additional 5K instruction-tuning data (following Section 4), and create 30 unseen diverse questions for evaluation following Section 5.1.1. We utilize the same training settings to fine-tune MMedAgent with a smaller"}, {"title": "6 Conclusion", "content": "We propose MMedAgent, the first multi-modal medical AI agent that is capable of seamlessly utilizing various medical tools to handle a broad spectrum of medical tasks across different imaging modalities. We create an instruction-tuning dataset that MMedAgent utilize to learn to invoke various medical tools and aggregate results from tools. Comprehensive experiments demonstrate that MMedAgent significantly outperforms open-source baselines and even surpasses GPT-40 across many medical tasks. Furthermore, MMedAgent efficiently integrates with new tools while remaining the capability to activate previously learned tools."}, {"title": "7 Limitation", "content": "Our work is currently limited to seven tasks across five modalities. Due to the need for extensive domain knowledge, and the complexity and diversity of medical datasets involved in medical tasks, more specialized tools are emerging that should be included in our tools lists. However, the scalability of our model allows for the inclusion of more powerful tools in the future.\nAdditionally, more ablation studies on different backbones are necessary. Our current backbone is based on the LLaVA-Med, but recently, multiple generalist LLMs in the medical domain have been proposed, which could potentially be used to build a stronger MMedAgent."}, {"title": "A Details of Tools", "content": "A.1 Classification\nWe construct a close set of labels L for Biomed-CLIP to search for the most suitable category for the given image.\nL ={\"adenocarcinoma histopathology\u201d, \u201cbrain MRI\u201d, \u201ccovid line chart\u201d, \u201csquamous cell carcinoma histopathology\u201d, \u201cimmunohistochemistry histopathology\", \u201cbone X-ray\u201d, \u201cchest X-ray\u201d, \u201cpie chart\", \"ultrasound imaging\u201c, \u201chematoxylin and eosin histopathology\u201d, \u201cgross\"}."}, {"title": "A.2 Retrieval Augmented Generation (RAG)", "content": "RAG distinguishes itself from standard report generation by its access to an external knowledge base, such as Merck Manual. We consider the following 3 common uses of RAG. The instruction-tuning data are generated based on these functionalities.\nChest X-ray image report analysis. The chest X-ray image report analysis can function to analyze the report on medical images and provide an analysis including the potential diseases and their related retrieved knowledge and source.\nGeneral medical report analysis. The general medical report analysis can take a summarized report on common diseases and generate an analysis with medical advice such as treatments and precautions, together with a link to the retrieved source from the Merck Manual official website.\nGeneral medical advice generation. For general medical advice generation, the user can ask general questions about the diseases, and the model will retrieve and provide related information on them.\nFor the chest X-ray image report analysis, we generate 1000 chest X-ray reports from the MRG tool described in Section 3.3.2 as the report dataset. For the datasets of general medical report analysis and general medical advice generation, we utilize GPT-40 to generate 1000 medical reports and 1000 patient questions respectively about common diseases sampled from the entrees covered in the Merck Manual."}, {"title": "A.3 Medical Grounding DINO", "content": "The datasets used to tune the medical grounding DINO is shown in Table 4."}, {"title": "B Instruction Tuning Dataset Generation", "content": "We represent our prompts for generating an instruction tuning dataset in Figure 5."}, {"title": "C Agent Serving", "content": "MMedAgent operates within the FastChat system which consists of web servers that interact with users, model workers hosting the language model, and various tools. A controller coordinates the activities between the web servers and model workers. The entire system, including the 7B MMedAgent and all associated tools, can be run on an Nvidia A100 (80GB) GPU."}, {"title": "D Evaluation Prompt", "content": "We utilize GPT-4 to assess the answers generated by MMedAgent and other models with prompts shown in Figure 6."}]}