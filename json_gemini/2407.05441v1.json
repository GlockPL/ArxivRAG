{"title": "Language Models Encode Collaborative Signals in Recommendation", "authors": ["Leheng Sheng", "An Zhang", "Yi Zhang", "Yuxin Chen", "Xiang Wang", "Tat-Seng Chua"], "abstract": "Recent studies empirically indicate that language models (LMs) encode rich world knowledge beyond mere semantics, attracting significant attention across various fields. However, in the recommendation domain, it remains uncertain whether LMs implicitly encode user preference information. Contrary to the prevailing understanding that LMs and traditional recommender models learn two distinct representation spaces due to a huge gap in language and behavior modeling objectives, this work rethinks such understanding and explores extracting a recommendation space directly from the language representation space. Surprisingly, our findings demonstrate that item representations, when linearly mapped from advanced LM representations, yield superior recommendation performance. This outcome suggests the homomorphism between the language representation space and an effective recommendation space, implying that collaborative signals may indeed be encoded within advanced LMs. Motivated by these findings, we propose a simple yet effective collaborative filtering (CF) model named AlphaRec, which utilizes language representations of item textual metadata (e.g., titles) instead of traditional ID-based embeddings. Specifically, AlphaRec is comprised of three main components: a multilayer perceptron (MLP), graph convolution, and contrastive learning (CL) loss function, making it extremely easy to implement and train. Our empirical results show that AlphaRec outperforms leading ID-based CF models on multiple datasets, marking the first instance of such a recommender with text embeddings achieving this level of performance. Moreover, AlphaRec introduces a new language-representation-based CF paradigm with several desirable advantages: being easy to implement, lightweight, rapid convergence, superior zero-shot recommendation abilities in new domains, and being aware of user intention. Codes are available at https://github.com/LehengTHU/AlphaRec.", "sections": [{"title": "1 Introduction", "content": "Language models (LMs) have achieved great success across various domains [3-7], prompting a critical question about the knowledge encoded within their representation spaces. Recent studies empirically find that LMs extend beyond semantic understanding to encode comprehensive world knowledge about various domains, including game states [8], lexical attributes [9], and even concepts of space and time [10] through language modeling. However, in the domain of recommendation where the integration of LMs is attracting widespread interest [11-15], it remains unclear whether LMs inherently encode relevant information on user preferences and behaviors. One possible reason"}, {"title": "2 Uncovering Collaborative Signals in LMs via Linear Mapping", "content": "In this section, we aim to explore whether LMs implicitly encode user preference similarities in their representation spaces. We first formulate the personalized item recommendation task, then detail the linear mapping method and its empirical findings. Empirical evidence indicates the homomorphism [54] between the representation spaces of advanced LMs and effective recommendation spaces.\nTask formulation. Personalized item recommendation with implicit feedback aims to select items i \u2208 I that best match user u's preferences based on binary interaction data Y = [Yui], where Yui = 1 (Yui = 0) indicates user u \u2208 U has (has not) interacted with item i [59]. The primary objective of recommendation is to model the user-item interaction matrix Y using a scoring function \u0177 : U \u00d7 I \u2192 R, where \u0177ui measures u's preference for i. The scoring function \u0177ui = s \u2022 \u03c6\u03b8(xu, xi) comprises three key components: pre-existing features xu and x\u2081 for user u and item i, a representation learning module \u03c6\u03b8(\u00b7,\u00b7) parametrized by \u03b8, and a similarity function s(\u00b7,\u00b7). The representation learning module e transfers u and i into representations eu and ei for similarity matching s(\u00b7,\u00b7), and the Top-K highest scoring items are recommended to u.\nDifferent recommenders employ various pre-existing features xu, xi and representation learning architecture \u03c6\u03b8(\u00b7,\u00b7). Traditional ID-based recommenders use one-hot vectors as pre-existing features xu, xi. The choice of ID-based representation learning architecture e can vary widely, including ID-based embedding matrix [55], multilayer perception [62], graph neural network [52, 63], and variational autoencoder [64]. We adopt a commonly used similarity function, cosine similarity [65, 58] s(eu, ei) = ||eu||||ei||' in this paper.\nLinear mapping. Linear mapping ensures the homomorphism [54] between the vector spaces, and is therefore widely used in the study on the representation space properties of LMs [10, 66, 67]. In this section, we adopt a trainable linear mapping matrix W as the representation learning module e, mapping representations from the language space into an item recommendation space. High performance on the test set would indict the homomorphism between the language space and an effective recommendation space, which further suggests the possible existence of collaborative signals"}, {"title": "3 AlphaRec", "content": "This finding of space homomorphism [54] sheds light on building advanced CF models purely based on language representations without introducing ID-based embeddings. Inspired by this, we develop a simple yet effective CF model called AlphaRec, by incorporating only three simple components (i.e., nonlinear projection [62], graph convolution [56] and contrastive learning (CL) objectives [57]). It is important to highlight that our approach is centered on exploring the potential of language representations for recommendation by integrating essential components from leading CF models, rather than deliberately inventing new CF mechanisms. Surprisingly, these basic and simple components lead to an excellent language-representation-based recommender with multiple capabilities. We present the model structure of AlphaRec in Section 3.1, and compare AlphaRec with two popular recommendation paradigms in Section 3.2."}, {"title": "3.1 Method", "content": "We present how AlphaRec is designed and trained. Generally, the representation learning architecture (\u03c6\u03b8,\u00b7) of AlphaRec is simple, which only contains a two-layer MLP and the basic graph convolution operation, with language representations as the input features xu, xi. The cosine similarity is used as the similarity function s(\u00b7,\u00b7), and the contrastive loss InfoNCE [57, 58] is adopted for optimization. For simplicity, we consistently adopt text-embeddings-3-large [73] as the language representation model, for its excellent language understanding and representation capabilities.\nNonlinear projection. In AlphaRec, we substitute the linear mapping matrix delineated in Section 2 with a nonlinear MLP. This conversion from linear to nonlinear is non-trivial, for the paradigm shift from ID-based embeddings to language representations, since nonlinear transformation helps in excavating more comprehensive user preference similarities from the language representation space with rich semantics (see discussions about this in Appendix C.2.3) [62]. Specifically, we project the language representation x\u2081 of the item title to an item space for recommendation with the two-layer MLP, and obtain user representations as the average of historical items:\ne(0)i = W2 LeakyReLU (W\u2081xi + b\u2081) + b2, e(0)u =\n1\n|Nu|\n\u03a3\ni\u2208Nu\ne(0)i\n(1)\nGraph convolution. Graph neural networks (GNNs) have shown superior effectiveness for recom-mendation [52, 56], owing to the natural user-item graph structure in recommender systems [75]. In AlphaRec, we employ a minimal graph convolution operation [56] to capture more complicated collaborative signals from high-order connectivity [56, 76, 77, 75] as follows:\ne(k+1)u =\n1\n|Nu|\niENu\n\u221a|Nu||Ni|\ne(k)ee(k)i ,\ne(k+1)i =\n1\n|Ni|\nu\u2208Ni\n\u221a|Nu||Ni|\ne(k)ue(k)i\n(2)\nThe information of connected neighbors is aggregated with a symmetric normalization term\n\u221a|Nu\u221a|Ni Here Nu (Ni) denotes the historical item (user) set that user u (item i) has inter-acted with. The features e(0)u and e(0)i projected from the MLP are used as the input of the first layer. After propagating for K layers, the final representation of a user (item) is obtained as the average of features from each layer:\neu =\n1\nK+1\nK\n\u03a3\ne(k)u,\nei =\n1\nK+1\nK\n\u03a3\ne(k)i\n(3)\nk=0\nk=0\nContrastive learning objective. The introduction of contrasting learning is another key element for the success of leading CF models. Recent research suggests that the contrast learning objective, rather than data augmentation, plays a more significant role in improving recommendation performance [70, 78, 69]. Therefore, we simply use the contrast learning object InfoNCE [57] as the loss function without any additional data augmentation on the graph [79, 58]. With cosine similarity as the similarity function s(eu, ei) =,\nthe InfoNCE loss [57, 79, 80] is written as:\nLInfoNCE =\n\u03a3\n(u,i)\u2208O+\nlog\nexp (s(eu, ei)/T)\nexp (s(eu, ei)/T) + \u2211j\u2208S, exp (s(eu, ej)/\u0442)\n(4)\nHere, T is a hyperparameter called temperature [81], O+ = {(u, i)|Yui = 1} denoting the observed interactions between users U and items I. And Su is a randomly sampled subset of negative items that user u does not adopt."}, {"title": "3.2 Discussion of Recommendation Paradigms", "content": "We compare the language-representation-based AlphaRec with two popular recommendation paradigms in Table 2 (see more discussion about related works in Appendix A).\nID-based recommendation (ID-Rec) [52, 55]. In the traditional ID-based recommendation paradigm, users and items are represented by ID-based learnable embeddings derived from a large number of user interactions. While ID-Rec exhibits excellent recommendation capabilities with low training and inference costs [63, 79], it also has two significant drawbacks. Firstly, these ID-based embeddings"}, {"title": "4 Experiments", "content": "In this section, we aim to explore the effectiveness of AlphaRec. Specifically, we are trying to answer the following research questions:\n\u2022 RQ1: How does AlphaRec perform compared with leading ID-based CF methods?\n\u2022 RQ2: Can AlphaRec learn general item representations, and achieve good zero-shot recommendation performance on entirely new datasets?\n\u2022 RQ3: Can AlphaRec capture user intention described in natural language and adjust the recommendation results accordingly?"}, {"title": "4.1 General Recommendation Performance (RQ1)", "content": "Motivation. We aim to explore whether the language-representation-based recommendation paradigm can outperform the ID-Rec paradigm. An excellent performance of AlphaRec would shed light on the research line of building language-representation-based recommenders in the future.\nBaselines. We consider ID-based baselines in this section. LM-based methods are not incorporated for two practical reasons: the huge inference cost on datasets with millions of interactions and the task limitation of candidate selection or next item prediction. In addition to classic baselines (i.e., MF, MultVAE, and LightGCN) introduced in section 2, we consider two categories of leading ID-based"}, {"title": "4.2 Zero-shot Recommendation Performance on Entirely New Datasets (RQ2)", "content": "Motivation. We aim to explore whether AlphaRec has learned general item representations [37], which enables it to perform well on entirely new datasets without any user and item overlap.\nTask and datasets. In zero-shot recommendation [38], there is not any item or user overlap between the training set and test set [38, 33], which is different from the research line of cross-domain recommendation in ID-Rec [84]. We jointly train AlphaRec on three source datasets (i.e., Books, Movies & TV, and Video Games), while testing it on three completely new target datasets (i.e., Movielens-1M [60], Book Crossing [61], and Industrial [1]) without further training on these new datasets. (see more details about how we train AlphaRec on multiple datasets in Appendix C.3.1).\nBaselines. Due to the lack of zero-shot recommenders in the field of general recommendation, we slightly modify two zero-shot methods in the sequential recommendation [85], ZESRec [37] and UniSRec [37], as baselines. We also incorporate two strategy-based CF methods, Random and Pop (see more details about these baselines in Appendix C.3.2).\nResults. Table 4 presents the zero-shot recommendation performance comparison on entirely new datasets. The best-performing methods are bold and starred, while the second-best methods are underlined. We observe that:\n\u2022 AlphaRec demonstrates strong zero-shot recommendation capabilities, comparable to or even surpassing the fully trained LightGCN. On datasets from completely different platforms (e.g., MovieLens-1M and Book Crossing), AlphaRec is comparable with the fully trained LightGCN. On the same Amazon platform dataset, Industrial, AlphaRec even surpasses LightGCN, for which we attribute to the possibility that AlphaRec implicitly learns unique user behavioral patterns on the Amazon platform [1]. Conversely, ZESRec and UniSRec exhibit a marked performance decrement compared with AlphaRec. We attribute this phenomenon to two aspects. On the one hand, BERT-style LMs [4, 5] used in these works may not have effectively encoded user preference similarities, which is consistent with our previous findings in Section 2. On the other hand, components designed for the next item prediction task in sequential recommendation [86] may not be suitable for capturing the general preferences of users in CF scenarios.\n\u2022 The zero-shot recommendation capability of AlphaRec generally benefits from an increased amount of training data, without harming the performance on source datasets. As illustrated in Figure 8, the zero-shot performance of AlphaRec, when trained on a mixed dataset, is generally superior to training on one single dataset [37]. Additionally, we also note that training data with themes similar to the target domain contributes more to the zero-shot performance. For instance, the zero-shot capability on MovieLens-1M may primarily stem from Movies & TV. Furthermore, we discover that AlphaRec, when trained jointly on multiple datasets, hardly experiences a performance decline on each source dataset. These findings further point to the general recommendation capability of a single pre-trained AlphaRec across multiple datasets. The above findings also offer a potential research path to achieve general recommendation capabilities, by incorporating more training data with more themes. See more details about these results in Appendix C.3.3."}, {"title": "4.3 User Intention Capture Performance (RQ3)", "content": "Motivation. We aim to investigate whether a straightforward paradigm shift enables pre-trained AlphaRec to perceive text-based user intentions and refine recommendations. Specifically, we introduce an adjustable hyperparameter \u03b1 to combine user intentions with historical interests.\nTask and datasets. We test the user intention capture ability of AlphaRec on MovieLens-1M and Video Games. In the test set, only one target item remains for each user [87], with one intention query generated by ChatGPT [88, 40] (see the details about how to generate and check these intention queries in Appendix C.4.1). In the training stage, we follow the same procedure as illustrated in Section 2 to train AlphaRec. In the inference stage, we obtain the language representation eIntention for each user intention query and combine it with the original user representation to get a new user representation as \u1ebd(0)u = (1-\u03b1)e(0)u+ \u03b1eIntention [87]. This new user representation is sent into the freezed AlphaRec for recommendation. We report a relatively small K = 5 for all metrics to better reflect the intention capture accuracy.\nUser intention capture results. Table 5 represents the user intention capture experiment results, compared with the baseline TEM [89]. Clearly, the introduction of user intention (w Intention) significantly refines the recommendations of the pre-trained AlphaRec (w/o Intention). Moreover, AlphaRec outperforms the baseline model TEM by a large margin, even without additional training on search tasks. We further conduct a case study on MovieLens-1M to demonstrate how AlphaRec captures the user (see more case study results in Appendix C.4.3). As shown in Figure 3a, AlphaRec accurately captures the hidden user intention for \u201cGodfather\", while keeping most of the recommen-dation results unchanged. This indicates that AlphaRec captures the user intention and historical interests simultaneously.\nEffect of the intention strength \u03b1. By controlling the value of \u03b1, AlphaRec can provide better recommendation results, with a balance between user historical interests and user intent capture. Figure 3b depicts the effect of \u03b1. Initially, as \u03b1 increases, the recommendation performance rises accordingly, indicating that incorporating user intention enables AlphaRec to provide better rec-ommendation results. However, as the \u03b1 approaches 1, the recommendation performance starts to decrease, which suggests that the user historical interests learned by AlphaRec also play a vital role. The similar effect of \u03b1 on Video Games is discussed in Appendix C.4.4.\""}, {"title": "5 Limitations", "content": "There are several limitations not addressed in this paper. On the one hand, although we have demon-strated the excellence of AlphaRec for multiple tasks on various offline datasets, the effectiveness of online employment remains unclear. On the other hand, although we have successfully explored the potential of language-representation-based recommenders by incorporating essential components in leading CF models, we do not elaboratively focus on designing new components for CF models."}, {"title": "6 Conclusion", "content": "In this paper, we explored what knowledge about recommendations has been encoded in the LM representation space. Specifically, we found that the advanced LMs representation space exhibits a homomorphic relationship with an effective recommendation space. Based on this finding, we developed a simple yet effective CF model called AlphaRec, which exhibits good recommendation performance with zero-shot recommendation and user intent capture ability. We pointed out that AlphaRec follows a new recommendation paradigm, language-representation-based recommendation, which uses language representations from LMs to represent users and items and completely abandons ID-based embeddings. We believed that AlphaRec is an important stepping stone towards building general recommenders in the future.2"}, {"title": "A Related Works", "content": "Representations in LMs. The impressive capabilities demonstrated by LMs across various tasks raise a wide concern about what they have learned in the representation space. An important and effective approach for interpreting and analyzing representations of LMs is linear probing [68, 67]. The main idea of linear probing is simple: training linear classifiers to predict some specific attributes or concepts (e.g., lexical structure [9]) from the representations in the hidden layers of LMs. A high probing result (e.g., classification accuracy on the out-of-sample test set) tends to imply relevant information has been implicitly encoded in the representation space of LMs, although this does not imply LMs directly use these representations [68, 10]. Recent studies empirically demonstrate that concepts such as color [90], game states [8], and geographic position are encoded in LMs. Furthermore, these concepts may even be linearly encoded in the representation space of LMs [8, 91].\nCollaborative filtering. Collaborative filtering (CF) [92] is an advanced technique in modern recommender systems. The prevailing CF methods tend to adopt an ID-based paradigm, where users and items are typically represented as one-hot vectors, with an embedding table used for lookup [55]. Usually, these embedding parameters are learned by optimizing specific loss functions to reconstruct the history interaction pattern [71]. Recent advances in CF mainly benefit from two aspects, graph convolution [75] and contrastive learning [92]. These CF models exhibit superior recommendation performance by conducting the embedding propagation [52, 56] and applying contrastive learning objectives [83, 63, 70]. However, although effective, these methods are still limited, due to the ID-based paradigm. Since one-hot vectors contain no feature information beyond being identifiers, it is challenging to transfer pre-trained ID embeddings to other domains [37] or to leverage leading techniques from computer vision (CV) and natural language processing (NLP) [34].\nLMs for recommendation. The remarkable language understanding and reasoning ability shown by LMs has attracted extensive attention in the field of recommendation. The application of LMs in rec-ommendation can be categorized into three main approaches: LM-enhanced recommendation, LM as the modality encoder, and LLM-based recommendation. The first research direction, LLM-enhanced recommendation, focuses on empowering traditional recommenders with the semantic representations from LMs [48, 47, 46, 49, 93, 94]. Specifically, these methods introduce representations from LMs as additional features for traditional ID-based recommenders, to capture complicated user preferences. The second research line lies in adopting the LM as the text modality encoder, which is also known as a kind of modality-based recommendation (MoRec) [34, 35]. These methods tend to train the LM as the text modality encoder together with the traditional recommender. In previous studies, BERT-style LMs are widely used as the text modality encoder. The third research line, LLM-based recommendation, directly uses LLMs as the recommender and recommends items in a text generation paradigm. Early attempts focus on adopting in-context learning (ICL) [95] and prompting pre-trained LLMs [96-99]. However, such naive methods tend to yield poor performance compared to traditional models. Therefore, recent studies concentrate on fine-tuning LLMs on recommendation-related cor-pus [16, 15, 26, 25, 29] and align the LLMs with the representations from traditional recommenders as the additional modality [17, 20, 27, 100]."}, {"title": "B Linear Mapping", "content": ""}, {"title": "B.1 Brief of Used LMS", "content": "We briefly introduce the LMs we use for linear mapping in Section 2.\n\u2022 BERT [4] is an encoder-only language model based on the transformer architecture [3], pre-trained on text corpus with unsupervised tasks. BERT adopts bidirectional self-attention heads to learn bidirectional representations.\n\u2022 ROBERTa [5] is an enhanced version of BERT. ROBERTa preserves the architecture of BERT but improves it by training with more data and large batches, adopting dynamic masking, and removing the next sentence prediction objective.\n\u2022 Llama2-7B [6] is an open-source decoder-only LLM with 7 billion parameters. Llama2 adopts grouped-query attention, with longer context length and larger size of the pre-training corpus compared with Llama-7B [101]."}, {"title": "B.2 Extracting Representations from LMs", "content": "We present how to extract representations from LMs. For encoder-based LMs (e.g., BERT [4] and ROBERTa [5]), we use the representation of the last hidden state corresponding to the [CLS] token [40]. For decoder-based models (e.g., Llama-7B [6, 72], Mistral-7B, and SFR-Embedding-Mistral [74]), we use the representation in the last transformer block [3], corresponding to the last input token [10, 102, 73]. Especially, for the commercial closed-source model (e.g., text-embedding-ada-v2 and text-embeddings-3-large 3 [73]), we directly call the API interface to obtain representations."}, {"title": "B.3 Empirical Findings", "content": "We examinate whether the better performance of leading LMs comes from the more compact representation ability. We randomly shuffle item representations and conduct the same linear mapping experiment. As illustrated in Table 6, randomly shuffled representations, text-embeddings-3-large (Random), lags largely behind the vanilla linear mapping method, suggesting that the performance improvement not merely comes from the better feature encoding ability."}, {"title": "C Experiments", "content": ""}, {"title": "C.1 Datasets", "content": "We incorporate six datasets in this paper, including four datasets from the Amazon platform 4 [1] (i.e., Books, Movies & TV, Video Games, and Industrial), and two datasets from other platforms (i.e., MovieLens-1M and Book Crossing). Table 7 reports the data statistics of each dataset.\nWe divide the history interaction of each user into training, validation, and testing sets with a ratio of 4:3:3, and remove users with less than 20 interactions following previous studies [50]. We also remove items from the test and validation sets that do not appear in the training set, to address the cold start problem."}, {"title": "C.2 General Recommendation", "content": ""}, {"title": "C.2.1 Baselines", "content": "We incorporate a series of CF models as our baselines for general recommendation. These models are classified as classical CF methods (MF, MultVAE, and LightGCN), CL-based CF methods (SGL, BC Loss, and XSimGCL), and LM-enhanced CF methods (KAR, RLMRec). For these LM-enhanced CF methods, we adopt the leading CF method XSimGCL as the backbone.\n\u2022 MF [55, 71] is the most basic CF model. It denotes users and items with ID-based embeddings and conducts matrix factorization with Bayesian personalized ranking (BPR) loss.\n\u2022 MultVAE [64] is a traditional CF model based on the variational autoencoder (VAE). It regards the item recommendation as a generative process from a multinomial distribution and uses variational inference to estimate parameters. We adopt the same model structure as suggested in the paper: 600\u2192 200 \u2192 600.\n\u2022 LightGCN [56] is a light graph convolution network tailored for the recommendation, which deletes redundant feature transformation and activation function in NGCF [52].\n\u2022 SGL [83] introduces graph contrastive learning into recommender models for the first time. By employing node or edge dropout to generate augmented graph views and conduct contrastive learning between two views, SGL achieves better performance than LightGCN.\n\u2022 BC Loss [79] introduces a robust and model-agnostic contrastive loss, handling various data biases in recommendation, especially for popularity bias.\n\u2022 XSimGCL [70] directly generates augmented views by adding noise into the inner layer of LightGCN without graph augmentation. The simplicity of XSimGCL leads to a faster convergence speed and better performance.\n\u2022 KAR [48] enhances recommender models by integrating knowledge from large language models (LLMs). It generates textual descriptions of users and items and combine the LM representations with traditional recommenders using a hybrid-expert adaptor."}, {"title": "C.2.2 Ablation Study", "content": "We conduct the same ablation study as introduced in Section 4.1 on Movies & TV and Video Games datasets. As illustrated in Figure 5, each component in AlphaRec contributes positively, which is consistent with our findings in Section 4.1."}, {"title": "C.2.3 The t-SNE Visualization Comparison", "content": "In this section, we aim to intuitively explore how the MLP in AlphaRec further helps in excavating collaborative signals in language representations, compared to the linear mapping matrix. We visualize the item representations from LMs, AlphaRec (w/o MLP), and AlphaRec in Figure 6, where AlphaRec (w/o MLP) denotes replacing the MLP with a linear mapping matrix. We observed that movies about superhero and monster cluster in all representation spaces, indicating both AlphaRec (w/o MLP) and AlphaRec capture the preference similarities between these items and preserve the clustering relationship. The difference between AlphaRec (w/o MLP) and AlphaRec lies in the ability to capture obscure preference similarities among items. As shown in Figure 6a, homosexual movies are dispersed in the language space, indicating the possible semantic differences between them. AlphaRec successfully captures the preference similarities and gathers these items in the representation space, while AlphaRec (w/o MLP) remains some items dispersed. Moreover, AlphaRec outperforms AlphaRec (w/o MLP) by a large margin, as indicated in Figure 5a. These results indicate that AlphaRec exhibits a more fine-grained preference capture ability with the help of nonlinear transformation."}, {"title": "C.3 Zero-shot Recommendation", "content": ""}, {"title": "C.3.1 Co-training on Multiple Datasets", "content": "Co-training on multiple datasets is similar to training on one single dataset, where the only difference lies in the negative sampling. When co-training on multiple datasets, the negative items are restricted to the same dataset as the positive item rather than the full item pool. The other training procedures remain the same with training on one single dataset."}, {"title": "C.3.2 Baselines", "content": "Since previous works about zero-shot recommendation mostly focus on sequential recommendation [86, 85], we slightly modify two methods in sequential recommendation, ZESRec [38] and UniSRec [37] as our baselines. Specifically, we maintain the model structure as provided in the paper, and adopt the training paradigm of CF.\n\u2022 Random denotes randomly recommending items from the entire item pool.\n\u2022 Pop denotes randomly recommending from the most popular items. Here popularity denotes the number of users that have interacted with the item.\n\u2022 ZESRec [38] is the first work that defines the problem of zero-shot recommendation. To address this problem, this work introduces a hierarchical Bayesian model with representations from the pre-trained BERT.\n\u2022 UniSRec [37] aims to learn universal item representations from BERT, with parametric whitening and a MoE-enhanced adaptor. By pre-training on multiple source datasets, UniSRec can conduct zero-shot recommendation on various datasets in a transductive or inductive paradigm."}, {"title": "C.3.3 The Effect of Training Datasets", "content": "The effect of the training dataset scale on zero-shot recommendation. We report the zero-shot recommendation performance differences trained on different datasets in Table 8. Here AlphaRec (trained on Books) denotes training on a single Books dataset, while AlphaRec (trained on mixed dataset) denotes co-training on three Amazon datasets. Generally, training on more datasets leads to a better zero-shot performance. In addition, we observe that, for the zero-shot performance on untrained target datasets, training datasets with similar themes contribute more (e.g., Movies & TV and MovieLens-1M).\nThe performance comparison between training on the single dataset and the mixed dataset. In Table 9, AlphaRec (trained on single dataset) denotes training and testing on the same single dataset, while AlphaRec (trained on mixed dataset) denotes training on three Amazon datasets (i.e., Books, Movies & TV, and Video Games) and testing on one single dataset. Generally, co-training on three Amazon datasets yields similar performance compared with training on one single dataset. The only exception lies in Video Games, which shows some performance degradation. We attribute this to the difference between the selection of T. We use T = 0.15 when trained on the mixed dataset, while the optimal 7 for Video Games lies around 0.2. These results indicate that a single AlphaRec can capture user preferences among various datasets, showcasing a general collaborative signal capture ability."}, {"title": "C.4 User Intention Capture", "content": ""}, {"title": "C.4.1 Intention Query Generation", "content": "The user intention query is a natural language sentence implying the target item of interest. For each item in the dataset, we generate a fixed user intention query. Following the previous work [40], we generate user intention queries with the help of ChatGPT [88]. As shown in Figure 7, we prompt ChatGPT in a Chain-of-Thought (CoT) [103] paradigm and adopt the output as the user intention query. We adopt a rule-based strategy to ensure the quality of generated queries, and regenerate the wrong query. Considering the huge amount of item title text, we use ChatGPT3.5 API for generating all queries for the budget's sake."}, {"title": "C.4.2 Baseline", "content": "AlphaRec exhibits user intention capture abilities, although not specially designed for search tasks. We compare AlphaRec with TEM [89] which falls in the field of personalized search [87, 104].\n\u2022 TEM [89] uses a transformer to encode the intention query together with user history behaviors, which enables it to achieve better search results by considering the user's historical interest."}, {"title": "C.4.3 Case Study", "content": "We conduct two more case studies to verify the user intention capture ability of AlphaRec. As illustrated in Figure 8 and Figure 9, AlphaRec provides better recommendation results, assigning the target item at the top while maintaining the general user preferences."}, {"title": "C.4.4 Effect of the Intention Strength Alpha", "content": "The value of \u03b1 controls the balance between the user's historical interests and the user intention query. A larger \u03b1 incorporates more about the user intention while considering less about the user's historical interests. As shown in Figure 10, the effect of \u03b1 on Video Games shows a similar trend with MovieLens-1M."}, {"title": "C.5 Trainig Cost", "content": "We report the training cost of AlphaRec in this section. Table 10 reports the seconds needed per epoch and the total training cost until convergence. Here Amazon-Mix denotes the mixed dataset of Books, Movies & TV, and Video Games. It's worth noting that AlphaRec converges quickly and only requires a small amount of training time."}, {"title": "D Hyperparameter Settings and Implementation Details", "content": "We conduct all the experiments in PyTorch with a single NVIDIA RTX A5000 (24G) GPU and a 64 AMD EPYC 7543 32-Core Processor CPU. We optimize all methods with the Adam optimizer. For all ID-based CF methods, we set the layer numbers of graph propagation by default at 2, with the embedding size as 64 and the size of sampled negative items |Su | as 256. We use the early stop strategy to avoid overfitting. We stop the training process if the Recall@20 metric on the validation set does not increase for 20 successive evaluations. In AlphaRec, the dimensions of the input and output in the two-layer MLP are 3072 and 64 respectively, with the hidden layer dimension as 1536. We apply the all-ranking strategy [105] for all experiments, which ranks all items except positive ones in the training set for each user. We search hyperparameters for baselines according to the suggestion in the literature. The hyperparameter search space is reported in Table 11. For these LM-enhanced models, KAR and RLMRec, we also search the hyperparameter of their backbone XSimGCL.\nFor AlphaRec, the only hyperparameter is the temperature 7 and we search it in [0.05, 2]. We report the temperature we used for each dataset in Table 12. For the mixed dataset Amazon-Mix in Section 4.2, we use a universal T = 0.15. We adopt T = 0.2 for the MovieLens-1M dataset for the user intention capture experiment in Section 4.3."}, {"title": "E Broader Impact", "content": "The proposed AlphaRec can significantly improve the performance of zero-shot recommendation and the capability of user intent capture, offering a good approach to crafting more personalized recommendation results. One concern of AlphaRec is the potential for the representations generated"}]}