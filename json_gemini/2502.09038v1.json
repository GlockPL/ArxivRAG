{"title": "AoI-Sensitive Data Forwarding with Distributed Beamforming in UAV-Assisted IoT", "authors": ["Zifan Lang", "Guixia Liu", "Geng Sun", "Jiahui Li", "Zemin Sun", "Jiacheng Wang", "Victor C.M. Leung"], "abstract": "This paper proposes a UAV-assisted forwarding system based on distributed beamforming to enhance age of information (AoI) in Internet of Things (IoT). Specifically, UAVs collect and relay data between sensor nodes (SNs) and the remote base station (BS). However, flight delays increase the Aol and degrade the network performance. To mitigate this, we adopt distributed beamforming to extend the communication range, reduce the flight frequency and ensure the continuous data relay and efficient energy utilization. Then, we formulate an optimization problem to minimize Aol and UAV energy consumption, by jointly optimizing the UAV trajectories and communication schedules. The problem is non-convex and with high dynamic, and thus we propose a deep reinforcement learning (DRL)-based algorithm to solve the problem, thereby enhancing the stability and accelerate convergence speed. Simulation results show that the proposed algorithm effectively addresses the problem and outperforms other benchmark algorithms.", "sections": [{"title": "I. INTRODUCTION", "content": "Internet of Things (IoT) have been proven to be highly effective in remote areas, particularly for the tasks such as monitoring, sensing, and detection, where data is rapidly transmitted to fusion centers. To automate data integration from these deployed sensor nodes (SNs), UAVs are deployed to operate between IoT and the BS to facilitate data collection, dissemination, and relay to enhance network efficiency [1]. Consequently, UAVs play a crucial role in ensuring seamless data flow from remote IoT to central processing units, which is especially beneficial in challenging environments.\nDespite the advantages of UAV-assisted IoT, a significant challenge arises when timely data delivery is required. Specifically, the flight of UAVs introduces delays in data reception, which leads to a high age of information (AoI) and reduces real-time system responsiveness [2]. This issue becomes especially critical in the applications such as forest fire monitoring and border surveillance, where the delayed information results in severe consequences [3]. Therefore, minimizing the UAV flight time while maintaining efficient data transmission to reduce Aol remains an urgent task. Addressing this challenge demands innovative approaches that ensure both timely data delivery and efficient resource utilization.\nDistributed beamforming offers a promising solution to these challenges by significantly enhancing the transmission gain and extending the communication range [4]. By employing distributed beamforming, UAVs can maintain connectivity with the BS without frequent flights, thereby mitigating flight-induced delays and reducing AoI. This capability is particularly valuable in the scenarios requiring continuous data relay, as it allows for more efficient use of UAV energy resources. Designing an effective UAV-assisted relay system involves several complexities. First, distributed beamforming introduces multiple transmission phases, which increases the difficulty of system modeling. Additionally, optimizing UAV trajectories is essential for minimizing both Aol and energy consumption across sensors. Furthermore, the uncertainty and variability introduced by dynamic updates of sensor data demand robust optimization strategies that can adapt in real time. However, addressing these complexities requires innovative modeling and optimization techniques that can effectively handle the dynamic and multi-faceted nature of the problem.\nUnlike previous studies, this paper introduces a UAV-assisted Aol-sensitive data forwarding system and proposes a modified deep reinforcement learning (DRL) algorithm to minimize the Aol of SNs and energy consumption of UAVs. The primary contributions of this work are outlined as follows:\n\u2022 UAV-assisted AoI-sensitive Data Forwarding System: We consider a UAV-assisted AoI-sensitive data forwarding system with distributed beamforming. Specifically, the direct link from the SNs to BS is blocked due to the long-range transmission distance, and distributed beamforming is employed to mitigate flight-induced delays of UAVs.\n\u2022 Formulation of Complex Optimization Problem: In the considered system, we construct a joint optimization problem that cooperatively plans the UAV trajectory and schedules communication to minimize the Aol of SNs and the energy consumption of UAVs. The real-time requirements and dynamic nature of this problem make it challenging to be solved efficiently.\n\u2022 Enhanced DRL-based Approach: We propose a SAC with temporal sequence, layer normalization gated recurrent unit, and attention (SAC-TLA), which is a DRL-based algorithm to solve our optimization problem. Specifically, SAC-TLA introduces temporal sequence, layer normalization gated recurrent unit (LNGRU), and attention mechanism to enhance the stability and accelerate"}, {"title": "II. SYSTEM MODELS", "content": "In this section, we first present the system overview. Subsequently, we introduce the considered models, which includes the network, AoI, and UAV energy cost models, to characterize the optimization objectives and decision variables.\nAs shown in Fig. 1, we consider a UAV-assisted Aol-sensitive data forwarding system in IoT. In this system, the SNs denoted as N \u2261 {i | 1,2,...,Nsn} are randomly distributed across the monitor area. We consider that each SN generates data in each time slots and the volume of data for each SN is denoted by D \u2208 {D1, D2, ...,DNSN}. Due to the complex terrestrial network environments and long distance between these SNs and BS, the direct transmissions between them may be infeasible. Under the circumstances, a fleet of UAVs denoted as U \u2261 {j | 1,2,..., NUAV} are deployed to collect and forward the data from the sensor network to the remote BS. Note that each UAV is equipped with a single omnidirectional antenna. In order to reduce the information delay caused by the UAV flying back and forth between the SNs and the BS, we consider introducing distributed beamforming method and constructing the UAVs as a VAA to enhance the transmission ranges without moving UAVs, thereby reducing the Aol of these SNs.\nFor generality, we adopt a 3D Cartesian coordinate system, where the position of the i-th SN is $q_{i}^{SN} = (x_i, y_i, 0)$, and the position of the j-th UAV in time slot t is $q_j^{UAV}(t) = (x_j(t), y_j(t), H_U)$. Subsequently, we will detail the network model, which contains three key models with respect to transmission, i.e., the G2A, A2A, and A2G transmission models. Following this, we will derive an Aol model and an UAV energy cost model."}, {"title": "B. Network Model", "content": "In this section, we present a detailed introduction of the G2A, A2A, and A2G transmission models.\n1) G2A Transmission Model. In this part, we apply the frequency division multiplexing access (FDMA) mechanism and calculate the transmission rate using probabilistic LoS as the communication link between the SNs and UAVs. Specifically, we consider that different SNs utilize different frequencies to prevent interference during data transmission. Besides, we assume that a SN can only communicate with one UAV at a time, while a UAV can communicate with multiple SNs simultaneously. To this end, we define a binary variable $B_{i,j}(t)$ to represent whether the j-th UAV communicates with the i-th SN in time slot t. Therefore, the communication scheduling constraints for SNs are as follows:\n$\\sum_{j=1}^{N_{UAV}} \\beta_{i,j}(t) \\leq 1, B_{i,j}(t) \\in \\{0,1\\}, \\forall i \\in N, \\forall j \\in U$.\nThen, we consider a channel model between the i-th SN and the j-th UAV, which includes both line-of-sight (LoS) and non-line-of-sight (NLoS) links. The probability of LoS is $P_{LoS}(\\theta_{i,j}(t)) = 1/[1 + a exp(-b[\\theta_{i,j}(t)/\\pi - a])]$, where a and b are two constants related to the environment, and $\\theta_{i,j}(t) = sin^{-1}(h/d_{j}^{UAV}(t))$, where $h_{UAV}(t)$ and $d_i$ are the vertical and horizontal distance between the j-th UAV and the i-th SN, respectively. Correspondingly, the probability of NLOS is $P_{NLOS} = 1- P_{LoS}(\\theta_{i,j})$. Furthermore, we support that the conditions of the uplink and downlink channels between SNs and UAVs are comparable. In this case, the average channel power gain between the i-th SN and the j-th UAV can be expressed as $g_{i,j} = [P_{LOS}L_{LOS} + P_{NLOS}L_{NLOS}]^{-1}$. Then, the uplink data transmission rate between the UAV and the SN is given by $R_{i,j} = B_{i,j} log_2(1+ P_i g_{i,j}/\\sigma^2)$, where $P_i$ is the transmit power and $\\sigma^2$ is the noise power."}, {"title": "C. AoI Model", "content": "In this work, data validity is closely associated with the timeliness of its collection and transmission. Thus, we adopt the concept of Aol $A_i(t)$ to reflect the freshness of the data from i-th SN at time slot t.\nIf the i-th SN fails to communicate with any UAV in time slot t, its data incurs an additional delay, and the Aol updates as $A_i(t + 1) = A_i(t) + 1$. If the j-th UAV broadcasts to all UAVs and forwards the data from the i-th SN to the BS, the Aol of the i-th SN will decrease in the following time slot.\nDue to limited channel capacity, the VAA may not always successfully forward data to the BS during the A2G phase. In this case, the fraction of data successfully forwarded is $Q(t) = min\\{S_{G2A}, S_{A2G}\\}/S_c$, where $S_c = \\sum_{i=1}^{N_{SN}} D_i$ is the size of sensing data of all SNs before data collection and $S_{G2A} = \\sum_{j=1}^{N_{UAV}} S_j$ is the size of received data during G2A transmission, where $S_j$ is shown in Eq. (2). Thus, the Aol dynamics of SNs can be summarized as follows:\n$A_i(t + 1) =\\begin{cases}\n(1 \u2013 Q(t))(A_i(t) + 1), & \\text{if } \\beta_{i,j} (t) = 1, \\\\\nA_i(t)+1, & \\text{otherwise}.\n\\end{cases}$\nThen, the time-averaged Aol is defined as\n$A = \\frac{1}{T N}\\sum_{t=1}^{T} \\sum_{i=1}^{N_{SN}} A_i(t).$"}, {"title": "D. UAV Energy Cost Model", "content": "In this section, we detail the following energy consumption model for UAVs. Specifically, the main energy consumption of the UAV is propulsion energy consumption, since the communication-related energy is two orders of magnitude smaller than the propulsion energy of UAV. Thus, for an altitude-fixed rotary-wing UAV, we denote the propulsion power as $P_{UAV} (v)$, which follows the model in [6].\nThus, the flight energy consumption and the hovering energy consumption of j-th UAV are given by $E_{move} = \\sum_{t=1}^{T} P_{UAV} (v) \\delta_{move}(t)$ and $E_{hov} = \\sum_{t=1}^{T} P_{hov} (\\delta_{G2A}(t) + \\delta_{A2A}(t) + \\delta_{A2G}(t))$, respectively. Then, the energy consumption of UAVs can be denoted as $E = \\sum_{j=1}^{N_{UAV}} E_{move} + E_{hov}$. Note that we omit the extra energy consumption caused by the acceleration and deceleration of the UAV during horizontal flight, since it constitutes only a minor part of the total operation time in the maneuver duration of the UAV."}, {"title": "E. Problem Formulation", "content": "We formulate the optimization problem for the UAV-assisted AoI-sensitive data forwarding system based on the models mentioned above. Specifically, our objective is to minimize the time-average Aol of SNs while minimizing the energy consumption of the UAVs. Thus, we jointly optimize the trajectory of the UAVs and the communication schedule for SNs in each time slot. Accordingly, the optimization problem can be formulated as:\n$min_{\\Phi,Q} (A + E)$,\ns.t. $B_{i,j}(t) \\in \\{0,1\\} \\forall i \\in N, \\forall j \\in U, t \\in T$,\n$(X_{UAV}, Y_{UAV} z_{UAV}) \\in \\mathbb{R}^{3x1}, \\forall j\\in U$,\n$d_{j1,j2} \\geq d_{min}$,\n$||q_j^{UAV}(t+1) \u2013 q_j^{UAV}(t)||^2 \\leq (v_{max}\\delta_{move}(t))^2$,"}, {"title": "III. THE PROPOSED SAC-TLA", "content": "In this section, we propose a DRL-based method to solve our optimization problem. To this end, we first show the motivations for using DRL and reformulate our problem as a markov decision process (MDP). Then, we introduce the proposed SAC-TLA algorithm with several improvements."}, {"title": "A. Motivations and MDP", "content": "In this part, we first introduce the motivations for using DRL, and then present the formulation and simplification of the MDP.\n1) Motivations for Using DRL. Our problem is dynamic and uncertain, with factors like UAV mobility and changes over multiple transmission phases. Thus, the commonly used static optimization methods such as convex or non-convex optimization are not suitable for this problem [4]. In this case, DRL is able to swiftly adapt the dynamic and uncertain environment and offer robust solutions. Thus, we seek to adopt the DRL method to solve our optimization problem.\n2) MDP Formulation. We reformulate the optimization problem shown in Eq. (7) as an MDP. Mathematically, an MDP is a tuple (S, A, P, R, \u03b3) which are state space, action space, state transition probability, reward function, and discount factor, respectively. Among them, state, action, and reward are the most important components, which are detailed as follows:\n\u2022 State Space: The state space is designed to capture essential spatial and operational dynamics influencing system performance. Specifically, the positions of both UAVs and SNs, along with the Aol of each SN, are incorporated to reflect spatial dynamics. Thus, the state at time slot t is formally expressed as $s_t = \\{q_j^{UAV}, q_i^{SN}, A_i^{SN}\\}$.\n\u2022 Action Space: In our system, the UAV can adjust its position to optimize its communication with SNs. Additionally, the binary variable $B_{i,j} (t)$ reflects the communication scheduling decision, which determines whether the jth UAV communicates with the ith SN in time slot t. As such, the action of agent at time slot t is represented as $a_j(t) = \\{a_j^x(t), a_j^y(t)\\}$, where $a_j^x(t)$ and $a_j^y(t)$ denote the movement of j-th UAV in the x and y directions at time slot t, respectively.\n\u2022 Reward Function: A well-designed reward function contributes to problem-solving and is critical. As such, the reward function involves our optimization objective and the corresponding constraints shown in Eq. (8). Specifically, our reward of j-th agent is defined as:\n$r_j(t) = -\\rho_1 A - \\rho_2 E + \\rho_3 c_j \u2013 \\rho_j$,\nwhere $\\rho_1, \\rho_2, \\rho_3$ are three normalization parameters to adjust to bring these terms to the same order of magnitude, $c_j$ is the amount of SNs covered by the j-th UAV to guide agents to avoid the impact of extreme negative rewards, and $p_j$ is the out-of-bounds penalty.\n3) MDP Simplification. To efficiently handle the complexity arising from the mixed discrete and continuous action space, we simplified MDP. Specifically, the discrete nature of communication actions introduces considerable randomness into the decision making process. This randomness complicates the decision making and learning process, and leads to inefficiencies. As a consequence, the selected SNs may fall outside the communication range of UAVs, while SNs within the radio range remain unselected, which leads to wasted communication resources.\nTo resolve these issues, we introduce dynamic proximity-based action mapping (DPAM), a strategy that simplifies decision-making by directly linking UAV trajectories to communication actions [7]. In particular, DPAM makes communication decisions deterministically by checking whether SNS are within the communication radius of the UAV, thereby eliminating the unpredictability associated with binary actions. This reformulation ensures reliable data collection, enhances energy efficiency, and streamlines the action space."}, {"title": "B. Standard SAC Algorithm", "content": "In this paper, we adopt the soft actor-critic (SAC) algorithm as our optimization framework [8]. SAC is a model-free, off-policy reinforcement learning method, which is suitable for high-dimensional and continuous action spaces, and the objective of this method is to maximize the discounted rewards and policy entropy, encourages exploration, and avoids premature convergence to a deterministic policy, i.e.,\n$max_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} [\\sum_{t=1}^T \\gamma^t(r_t + \\alpha H(\\pi(\\cdot|s_t))))]$\nMoreover, SAC consists of an actor network $\\pi_{\\theta}$, which produces a stochastic policy, and two critic networks $Q_{\\phi}$, which is used to mitigate overestimation by minimizing the Bellman error, i.e.,\n$L_Q(\\phi) = \\mathbb{E}[(Q_{\\phi}(s_t, a_t) - (r_t + \\gamma(min_{i=1,2}Q_{\\phi'_i}(s_{t+1}, a_{t+1})-\\alpha log \\pi_{\\theta}(a_{t+1}|s_{t+1}))))^2]$,\nHowever, in our MDP, the reward function is highly variable, which makes it difficult for the standard SAC to quickly adjust policy and results in slow convergence. Therefore, we propose several enhancements to SAC tailored to our scenario."}, {"title": "C. SAC-TLA Algorithm", "content": "In this work, we introduce an enhanced SAC-TLA. Specifically, SAC-TLA integrates three essential enhancements which are temporal sequence input processing, LNGRU, and attention mechanism, and they are detailed as follows.\n1) Temporal Sequence Input Processing. To improve the learning capability of conventional SAC algorithm in a time-varying environment, we introduce the temporal sequence-based state input. Unlike conventional SAC, which processes"}]}