{"title": "GUIDING THE CLASSIFICATION OF HEPATOCELLULAR CARCINOMA ON 3D CT-SCANS USING DEEP AND HANDCRAFTED RADIOLOGICAL FEATURES", "authors": ["E. Sarfati", "A. B\u00f4ne", "M-M. Roh\u00e9", "C. Aub\u00e9", "M. Ronot", "P. Gori", "I. Bloch"], "abstract": "Hepatocellular carcinoma is the most spread primary liver cancer across the world (~80% of the liver tumors). The gold standard for HCC diagnosis is liver biopsy. However, in the clinical routine, expert radiologists provide a visual diagnosis by interpreting hepatic CT-scans according to a standardized protocol, the LI-RADS, which uses five radiological criteria with an associated decision tree. In this paper, we propose an automatic approach to predict histology-proven HCC from CT images in order to reduce radiologists' inter-variability. We first show that standard deep learning methods fail to accurately predict HCC from CT-scans on a challenging database, and propose a two-step approach inspired by the LI-RADS system to improve the performance. We achieve improvements from 6 to 18 points of AUC with respect to deep learning baselines trained with different architectures. We also provide clinical validation of our method, achieving results that outperform non-expert radiologists and are on par with expert ones.", "sections": [{"title": "1. INTRODUCTION", "content": "Liver cancer is the sixth most common cancer worldwide and represents the fourth cause of mortality in 2023 [1]. In particular, hepatocellular carcinoma (HCC) accounts for 80% of the primary liver cancers [1, 2]. Diagnosing the presence of hepatocellular carcinoma is obtained with a biopsy, which is a risky and invasive operation. On the other hand, radiological assessment can provide a non-invasive diagnosis, based on typical features on dynamic CT or MRI according to strict criteria described in international guidelines [3, 4, 5]. This procedure is considered a solid proxy of the presence or absence of the tumor, and often totally replaces liver biopsy in clinical routine. To perform this assessment, radiologists use a standardized scoring system called LI-RADS (Liver Imaging Reporting and Data System) [6] to characterize suspicious lesions on CT-scans. The latter uses the four phases of the traditional injection protocol for CT imaging, and is based on five main visual features. However, radiological diagnosis suffers from an inherent variability between experts, which can be reduced using automatic methods. Several methods have been proposed in the literature to improve the diagnosis of hepatocellular carcinoma using deep learning [5, 7, 8, 9, 10, 11]. The authors in [8, 12] predict directly the pathology - without using any intermediate features - but exploit very large medical private datasets [12] (~12,000 patients in their study) that present a huge size difference between non-HCC and HCC lesions, which is a well-known feature to characterize HCC but can induce a bias in the problem setting as their HCC lesion can be considered \"easy\" or un-useful in the clinical process, due to the advanced stage of the tumor. In this work, we propose to evaluate an automated HCC diagnosis method on a challenging dataset of patients with risk factors of HCC and small-size nodules. As a baseline approach, we train deep learning baseline models inspired by state-of-the-art results proposed in the literature [7, 13]. To improve those results on our datasets, we propose a two-step approach, directly inspired by the radiologists' reading grid [6]. This approach uses the LI-RADS radiological criteria for a preliminary learning task in order to improve the final prediction of HCC. To the best of our knowledge, no methods have been proposed in the literature to use the LI-RADS major features for histological HCC prediction on CT-scans. Our contributions are the followings:\n\u2022 We propose to improve the baseline classification of HCC using weak radiological labels (LI-RADS major features), used for a preliminary learning task.\n\u2022 We propose new manually handcrafted features well-designed for HCC classification, as well as LI-RADS based deep learning radiological features, which can be combined with handcrafted features to enhance the classification performance.\n\u2022 We train and evaluate (cross-validation) our method on a challenging database of small and difficult HCC tumors [9, 14], and transfer it to one private test set.\n\u2022 Eventually, we compare our method to deep learning baseline methods and to liver expert and non-expert radiologists' LI-RADS diagnosis."}, {"title": "2. METHOD", "content": "Deep learning features. The proposed approach builds on a baseline, i.e.. a simple supervised classifier using as ground-truth the histological confirmation of HCC, by adding expert knowledge. We propose to follow the LI-RADS scoring system decision tree [9, 15, 16] as a preliminary learning task. More precisely, we propose to combine the prediction of the three LI-RADS major features with the baseline HCC prediction in order to guide the final HCC classification. To do so, we introduce three deep learning sub-models, based on the same backbone architecture as the baseline one. We train our deep models on the three LI-RADS criteria separately, which gives us three probability outputs, one for each criterion. Furthermore, we add the size feature s, which can be automatically computed from the lesion segmentation.\nHandcrafted features. To increase the predictive power of the networks outputs, we propose to combine them with handcrafted radiomics features that are directly inspired of the radiologists' reading grid. As represented in Figure 1, the LI-RADS binary criteria are visually assessed by observing the contrast differences of the lesions with respect to the liver parenchyma, or with the lesion borders. In this sense, it is intuitive to propose simple radiomics features computed on the input images. We propose three different formulas, based on the definition of each major feature [17]:\n\u2022 APHE: the arterial phase hyper-enhancement is defined by a contrast difference between the lesion and the liver parenchyma. This contrast is observed on the arterial phase and can be seen on the portal venous phase, as shown in Figure 2, lines one and two.\n\u2022 EC: the enhancing capsule is typically characterized by a thin enlightened contour with a darker inner lesion surface, as shown in Figure 2, at the second and third lines.\n\u2022 NPW: the non-peripheral washout corresponds to a decrease in attenuation or intensity from earlier to later phase, resulting in hypoenhancement in the portal venous or delayed phase, e.g a darkening of the lesion surface. It is observed at every line of Figure 2.\nFollowing the previous criteria description, we propose to characterize the three features by three simple formulas. The APHE feature $f_{APHE}$ computes the difference of the median voxel values between the lesion and the parenchyma on the arterial phase. The same formulation is proposed for the NPW $f_{NPW}$, but on the portal venous phase. Finally, for the EC $f_{EC}$, we first propose to automatically compute the inner border of the lesion. Then we compute the difference of energy between the inner surface of the lesion and its border, which is a measure of the magnitude of voxel values in an image. Denoting L the lesion, P the liver parenchyma and B the lesion border, we obtain:\n$f_{APHE} = median (L_{arterial}) \u2013 median (P_{arterial})$\n$f_{EC} = energy (L_{arterial}) \u2013 energy (B_{arterial})$\n$f_{NPW} = median (L_{portal}) \u2013 median (P_{portal})$\nNaming f the generic architecture used for the deep learning experiments, we denote by $f_w$, $f_\\theta$, $f_v$ and $f_\\phi$ the four distinct neural nets with parameters \u03c9, \u03b8, v and & and we propose to aggregate the outputs of all the models using a logistic regression giving us the final following model, with \u03b2 being the 7-dimensional vectors of the regression coefficients:\n$Y_{HCC} = \\beta [P_{HCC}, P_{APHE}, P_{EC}, P_{NPW}, F_{APHE}, F_{EC}, F_{NPW}]^T + \\beta_s s + \\alpha$ (1)\nAn overview of our method is presented in Figure 3.\nImage processing. CT-scans processing. We worked with original images of full CT-scans of dimensions 512\u00d7512 with anisotropic z-spacings. Images were first selected according to the presence of the three last injection phases: arterial phase, portal venous phase and delayed phase. First, a simple registration was made between all phases on the portal venous phase by performing a linear vertical translation (z-axis) based on the liver segmentations. Registered images were then resampled to match their portal venous phase scan using a nearest neighbor interpolation. To ensure homogeneity in all our dataset, we then used the nn-UNet pre-processing which is well adapted for z-anisotropic 3D images [18], which allowed us to resample all our images to their median 0.76 \u00d7 0.76 \u00d7 2.00mm\u00b3 voxel size.\nPatch sampling. For lesion type classification, 3D patch inputs are commonly preferred [8, 13], notably due to the limited computational resources which do not allow using the full CT-scans as inputs with a large enough batch size. Lesion-centered patches were sampled on the pre-processed 3D images so that 95% of the segmented lesion fit in the boxes. Patches of size 96 \u00d7 96 \u00d7 24 voxels were finally selected, corresponding to a 72.96 \u00d7 72.96 \u00d7 48.00mm\u00b3 field of view. According to the predicted major feature, the final inputs of our networks have a size of (N, C, 24, 96, 96) where N is the batch size and C the number of channels, with C\u2208 {3,4} if we use arterial/venous phases + lesion segmentation (APHE) or arterial/venous/delayed phases + lesion segmentation (HCC, EC and NPW).\nDatasets. Two datasets were leveraged in this study, coming from different centers of acquisition. One was used for training and validation, the two other datasets were only used for testing purpose.\nD1. Our first dataset contains a total number of 244 lesions, coming from 182 distinct patients. Among these lesions, 161 are histologically-proven hepatocellular carcinoma and 83 are other types of lesion. This dataset is the only one of our datasets that contains the three LI-RADS radiological criteria, for each lesion. Each criterion is provided by a binary number corresponding to their presence or absence, evaluated by an expert radiologist.\nD2. Our second dataset contains 1012 lesions corresponding to 543 patients and 602 no-HCC/410 HCC. This dataset presents cases of HCC that are mostly common [12, 19] as there exists a size gap between non-HCC and HCC lesions that we do not have in the first dataset."}, {"title": "Training and evaluation.", "content": "In order to predict the three LI-RADS major features, we train our deep network on D\u2081 in a stratified 5-fold cross-validation fashion. Each feature corresponds to an independent training. In parallel, we compute our handcrafted features and the lesion size, resulting in a 8-dimensional vector. We then optimize the logistic regression regularization coefficient on D1, and for each deep architecture (row). Table 2 reports the cross-validation results on D1.\nWe train our models using four different architectures. The first two backbones are inspired by [13] that provide state-of-the art results of 92% AUC on their database. The first one, called \"Tiny\" in Table 2 and Table 3, is composed of five convolutional layers and two dense layers with 8 channels at the first layer and progressively doubled channels to reach a representation space of 128. Two dense layers are finally added, to reach 2 neurons, with a softmax function to predict probabilities (HCC vs no-HCC). This architecture contains 1.5M of parameters. The second \"Small\" architecture is the same architecture with doubled number of channels at each convolutional layer, resulting in a total number of 5.5M of parameters. This architecture is also presented in [20]. We also provide performances of a ResNet-18 [21], trained from scratch and initialized with weights that were pretrained on 23 different medical imaging datasets [22], as well as pretrained ResNet-50 with frozen first layers for fine-tuning purposes, reducing the number of trainable parameters to 24.9M for ResNet-18 and 29.2M for ResNet-50. We use the AdamW optimizer with a batch size of 32 and trained for 600 epochs, a learning rate of 10-5 and a weight decay of 10-3, and for fine-tuning experiments we reduce the learning rate to 10-6. For data augmentation, we use horizontal and vertical flips, rotations and affine transformations. Training experiments were realized on a Tesla V100 mono-GPU.\nWe evaluate our method on a private test set, D2. To do so, we first infer the deep models of each criteria as well as for the HCC on the full set D2 to obtain predicted probabilities of HCC, APHE, EC and NPW. Then, we apply our handcrafted functions on the base to obtain features of each criteria, which allows us to obtain a feature vector of size 8 with the size feature, as for training. As we regularize with respect to the architecture in line, very small changes were observed in the HF column. For clarity, we report the average HF performance over the regularization used by line.\nWe report results of a cross-validated linear evaluation procedure, i.e. we keep the same stratified cross-validation as for D1, but we replace each validation with the full test set, hence the smaller standard deviations that are reported in Table 3. For each backbone, we report the results of choosing only the deep learning features + size (DLF), only the handcrafted features + size (HF), and the concatenation of DLF and HF + size (DLF+HF). Finally, we report the AUC from a non-specialist and a liver-pathology specialist on D1. The AUC for radiologists is computed by normalizing the LI-RADS score to obtain HCC probability (LR-1,2,3,4,5 are divided by 5 to obtain values between 0 and 1)."}, {"title": "3. RESULTS", "content": "In Table 2, we can note that the proposed method outperforms deep learning baselines, with a boost of 6 to 18 points. For D1, the best AUC is obtained combining our deep and handcrafted features, reaching an AUC of 75%, which represents a gain of 15 points with respect to a non specialist radiologist, and only 2.7 points away from a liver-pathology specialist. The performance of our method is robust to the backbone used. As a comparison with the state-of-the-art, the authors in [7] propose a 3D ResNet-18 computed on four-phases CT-scans 3D patches and obtain an AUC of 95.8%. With the exact same architecture, we obtain a baseline performance of 58.8% of AUC, which proves the inherent difficulty of our database and the small amount of data available. Pretrained ResNet-18 [22] helps however improving the performances, as shown in Table 2. In the same sense, the authors in [13] propose to use a deep learning architecture similar to our small encoder and obtain a groundbreaking AUC of 92%, while the same architecture obtains 64.4% on our dataset. It can be noticed that smaller encoders lead to better results in our case, given the small amount of data available.\nIn Table 3, we observe that for D2 the combination of our DLF and HF vectors outperforms both methods taken separately in most of the cases, as well as the deep learning baseline. One can note that performances on this dataset are much higher than on D1, which reflects the lesion size gap evoked in Table 1. Our deep learning baseline transfers well enough on this base, proving the robustness of the training.\nFigure 4 allows visualizing the importance of building an efficient model to predict the LI-RADS major features, before using them to predict HCC. We can indeed draw a parallel between the best individual performance of each predicted feature (by their AUC) and the absolute values of the deep learning major features in the logistic regression for HCC prediction. The logistic regression for HCC naturally selects the most accurately predicted major feature (NPW). However, as we can see with the small encoder performance, a smarter deep features combination could be beneficial for our model performance."}, {"title": "4. DISCUSSION AND CONCLUSION", "content": "In this paper, we proposed a novel approach for automatic hepatocellular classification by guiding our training using the radiologists' reading grid, the LI-RADS. We first predict LI-RADS major features using a deep learning approach in order to obtain probability of feature presence. In parallel, we propose a handcrafted approach to obtain relevant statistics related to each major feature. We then combine both approaches in a simple manner to predict HCC probability, and we achieve between 3 and 20 points improvements with respect to deep learning baselines, regardless of the backbone used, even with backbones that achieved state-of-the-art results for HCC classification in the literature.\nFor future work, an interesting perspective would be to improve the combination between DLF and HF, as proposed for instance in [23] in which the authors minimize the mutual information between both vectors during pretraining in order to reduce redundancy between DL and HC features. Moreover, in this study we propose a two-step approach for HCC classification: first we extract features and then we train a logistic regression for HCC prediction. A potential idea would be to combine these blocks in an end-to-end approach, i.e. integrating the HCC classification as a dense layer after the major features classification layer. This method would involve using a binary cross-entropy computed on the major features (multilabel problem) and the HCC. Pretraining mehods could also be explored, notably using the radiological features to guide the learning of a relevant latent space [20, 24, 25]. Eventually, there is a need for new datasets in HCC classification, notably public datasets (the only existing one exclusively contains HCC cases [26]), in order to test and compare the proposed method."}]}