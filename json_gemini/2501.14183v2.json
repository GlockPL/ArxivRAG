{"title": "VarDrop: Enhancing Training Efficiency by Reducing Variate Redundancy in Periodic Time Series Forecasting", "authors": ["Junhyeok Kang", "Yooju Shin", "Jae-Gil Lee"], "abstract": "Variate tokenization, which independently embeds each variate as separate tokens, has achieved remarkable improvements in multivariate time series forecasting. However, employing self-attention with variate tokens incurs a quadratic computational cost with respect to the number of variates, thus limiting its training efficiency for large-scale applications. To address this issue, we propose VarDrop, a simple yet efficient strategy that reduces the token usage by omitting redundant variate tokens during training. VarDrop adaptively excludes redundant tokens within a given batch, thereby reducing the number of tokens used for dot-product attention while preserving essential information. Specifically, we introduce k-dominant frequency hashing (k-DFH), which utilizes the ranked dominant frequencies in the frequency domain as a hash value to efficiently group variate tokens exhibiting similar periodic behaviors. Then, only representative tokens in each group are sampled through stratified sampling. By performing sparse attention with these selected tokens, the computational cost of scaled dot-product attention is significantly alleviated. Experiments conducted on public benchmark datasets demonstrate that VarDrop outperforms existing efficient baselines.", "sections": [{"title": "Introduction", "content": "Transformers have demonstrated impressive performance in time series forecasting, primarily due to their attention mechanisms. Traditionally, most methods have employed temporal tokenization, treating all variates at a given timestamp as a single token. However, recent studies reveal that variate tokenization-where each variate is embedded as a separate token-outperforms temporal tokenization in capturing inter-variate dependencies thereby increasing forecasting accuracy. Due to its broad applicability to Transformers, variate tokenization has been adopted in recent advancements in multivariate time series forecasting. Despite its advantages, the feasibility of variate tokenization in real-world applications is hindered by the increasing number of variates (N). Many public benchmark datasets demonstrate the high dimensionality often encountered in real-world applications. For instance, the Electricity dataset includes 321 variates, each corresponding to a customer, and the Traffic dataset comprises 862 variates, each representing a sensor. The large N introduces inefficiency, as the computational cost of attention mechanisms increases quadratically with N. This inefficiency results in a superfluous carbon footprint during the model training process. In addition, the large N can lead to overfitting and reduced attention performance by diluting important information. Therefore, reducing N while retaining the essential information is critical for the success of variate tokenization.\nTo address this issue, we propose removing variate redundancy in multivariate time series. As different variates often share the same characteristics such as trends and seasonality at some timestamps, excessive correlations between them are frequently observed in periodic time series. shows the variate redundancy in the term of maximum Pearson correlation coefficient values (refer to Eq. (4)) between the variates in two datasets. For the Electricity and Traffic datasets, 79.4% and 94.7% of all variates exhibit strong correlations with other variates exceeding 0.9, respectively. By selecting a few informative and distinguishing variates, we can significantly reduce computational cost while preserving the original periodic information of the data.\nHowever, efficiently picking out the representative variates is challenging for two reasons. First, the correlation between variates fluctuates due to inherent distribution shifts in time series data. This correlation shift in sliding windows generates a unique set of correlations for each batch as shown in"}, {"title": "Related Work", "content": ""}, {"title": "Tokenization Strategies for Time Series Data", "content": "Inspired by the success of Transformers in natural language processing, numerous Transformers have been proposed for multivariate time series forecasting. Previous studies, such as Autoformer, Fedformer, Crossformer , and PatchTST , adopted a temporal tokenization method similar to language models, treating the values of all variates at a specific timestamp as a single token. The multiple variates are not considered individually, but processed as a whole when generating representations for each temporal token. As a result, these embedded temporal tokens fail to properly capture the correlations between different variates in multivariate time series. Moreover, temporal tokens could not consider relevant contexts due to the narrow receptive field. \nTo address the limitations in temporal tokenization, iTransformer introduced variate tokenization. It is a special type of tokenization as the various tokenization methods used in Flowformer and treats each variate as one token, aiming to better model the variate dependencies. After the success of iTransformer, variate tokenization became a prevalent technique in forecasting models. Timer merges multiple variates from different domain into a single time series and treats the time series as a single token. MCformer tokenizes each variate and mixes the variates to capture inter-variate correlations. TimeXer also leverages variate tokenization in the introduction of exogenous variates."}, {"title": "Efficient Transformers for Time Series Data", "content": "Most previous efficient methods are designed for temporal tokens with sequential nature. Due to a number of tokens in the temporal axis, the sparse attention is prevalent to reduce the number of tokens in attention value computation. Here, a portion of query-key pairs is only considered instead of computing every query-key pair. LogSparse is one of the early methods for sparse attention, matching a query to the previous keys with an exponential step size and itself, showing the similar behavior in causal convolution. Big-Bird suggests a compound sparse attention method, containing global, local, and random query-key matching strategies. Reformer applies locality-sensitive hashing, which makes a chuck of similar tokens in an input sequence . These methods rely on the temporal locality of input sequences, making them unsuitable for variate tokens lacking a sequential nature.\nThere are also efficient Transformers not based on temporal locality between timestamps. Informer selects the dominant query-key pairs that has more influence in attention value computation. Autoformer adopts Fourier-based auto-correlation computation in the attention to reduce the computational complexity from $O(T^2)$ to $O(TlogT)$. Pyraformer constructs a pyramidal graph in matching query-key pairs to scale the attention module into longer sequences . FEDformer randomly selects a fixed number of Fourier components in time series to have linear computational complexity in the forward pass . Crossformer proposes two-stage attention for temporal dimension and variate dimension to reduce the computation complexity from $O(N^2T^2)$ to $O(N^2T)$. However, these methods do not consider variate redundancy in multivariate time series for boosting efficiency in Transformers."}, {"title": "Proposed Method: VarDrop", "content": "Problem Definition. Given a multivariate periodic time series $X \\in \\mathbb{R}^{N\\times T}$, where N is the number of variates and T is the length of the time series, multivariate time series forecasting aims to predict the forecast horizon $X_{t+1:t+H} \\in \\mathbb{R}^{N\\times H}$ based on a lookback window $X_{t-T+1:t} \\in \\mathbb{R}^{N\\times T}$. Variate tokenization converts a lookback window into the variate tokens $V\\in \\mathbb{R}^{N\\times d}$ through a embedding layer composed of multi-layer perceptron (MLP) shared by each variate. Note that d is the dimension of the embedding. A variate-tokenized Transformer model then predicts the future values $X_{t+1:t+H} \\in \\mathbb{R}^{NXH}$ for multivariate time series forecasting."}, {"title": "Overview of VarDrop", "content": "Figure 2 illustrates the overall procedure of VarDrop. When a batch of multivariate periodic time series containing N variates is given, k-dominant frequency hashing generates a hash value for each variate in the frequency domain, resulting in a total of N hash values denoted as $H\\in \\mathbb{R}^{N}$. These values represent the inherent periodic behaviors of each variate, enabling efficient grouping of variates with similar patterns. Some of the redundant variates that share the same patterns with others are then disregarded by stratified sampling. Using the selected $N(1 \u2013 \\delta)$ variate tokens, the computational complexity of self-attention is reduced from $O(N^2d)$ to $O(N^2(1 \u2013 \\delta)^2d)$, where d is the embedding size and $\\delta$ is the token reduction ratio. Similar to Dropout , VarDrop is utilized only during the training stage, as multivariate time series forecasting requires predictions for all variables during the inference stage."}, {"title": "Efficient Adaptive Variate Token Grouping", "content": "To achieve efficient sparse attention by disregarding redundant tokens, it is crucial to first identify groups of highly correlated variates. By leveraging the variate redundancy, we can selectively focus attention on the most significant tokens, thereby reducing computational complexity.\nk-Dominant Frequency Hashing. We introduce k-dominant frequency hashing (k-DFH), a simple solution that enables efficient grouping of variates in the frequency domain using the fast Fourier transform (FFT). To explain k-DFH, we first define the k-dominant frequency in Definition 3.1.\nDefinition 3.1 (k-DOMINANT FREQUENCY). Given a time series x, after performing the Fourier transform, a frequency f is dominant if its corresponding amplitude spectrum $A_f$ is among the top-k amplitude values.\nAccording to Fourier theorem, a time series can be modeled with few Fourier spectra in the frequency domain . Thus, the overall periodic behaviors of variates can be successfully condensed as the proper k dominant frequencies while ignoring unessential properties."}, {"title": "Sparse Attention via Stratified Sampling", "content": "Variate Reduction using Stratified Sampling. By leveraging the k-DFH, similar variates can be grouped based on their dominant frequencies in the frequency domain. Once these groups are formed, stratified sampling is applied to selectively retain a subset of variates within each group, thereby eliminating redundant variate tokens. Formally, let $G_i$ denote the set of variates in the i-th group, and let gs, a hyperparameter representing the group size, determine the maximum number of variates to retain per group. The retained subset of variates from group $G_i$, denoted as $S_i$, is defined as\n$S_i \\subseteq G_i$ and $|S_i|$ = min($|G_i| , gs).\n(1)\nThe set of all variates retained across all groups, denoted as S, is the union of the retained subsets from each group:\n$S = \\cup_{i=1}^G S_i$,\n(2)\nwhere G is the total number of generated groups. Our method is not limited to any specific sampling method during the stratified sampling process. Any sampling technique can be freely chosen based on the requirements of the application. In this study, we adopted random sampling as the sampling method due to its ease of implementation.\nEfficient Self-Attention Disregardng Redundant Variates Tokens. In Transformers employing variate tokens, the role of self-attention mechanisms is capturing dependencies between variates. The self-attention mechanism computes attention scores between each pair of variate tokens, resulting in a computational complexity of $O(N^2d)$, where N is the number of tokens and d is the embedding dimension. The attention scores are computed using the scaled dot-product attention, defined as:\nAttention(Q, K, V) = softmax$(\\frac{QK^T}{\\sqrt{d_k}})V$\n(3)\nwhere $Q \\in \\mathbb{R}^{N\\times d_k}, K \\in \\mathbb{R}^{N\\times d_k}$, and $V \\in \\mathbb{R}^{N\\times d_k}$ are the query, key, and value matrices, respectively, and dk is the dimension of the query, key, and value vectors. The bottleneck in the variate-tokenized Transformer lies in the quadratic computational complexity with respect to the number of tokens. As the number of tokens can be reduced by a token reduction ratio $\\delta = 1 - \\frac{min(|G_i|, g_s)}{N}$, the computational cost of self-attention decreases significantly, resulting in a complexity of $O(N^2(1 \u2013 \\delta)^2d)$. Moreover, the proposed method performs token reduction directly from raw variates, enhancing efficiency by eliminating the need for token embedding of redundant variates. Additionally, VarDrop is an architecture-free method that can be applied to any type of Transformer using various tokens."}, {"title": "Theoretical Analysis", "content": "The rationale behind k-DFH is that variates exhibiting similar periodic behaviors have the same dominant frequencies in the frequency domain. To provide its theoretical justification, we formalize this in Theorem 3.2 where the proof is provided in Appendix C.\nTheorem 3.2 (ERROR OF k-DFH APPROXIMATION). The error between a time series and its reconstructed signal from its hash value through k-DFH is given by the cumulative contribution of the non-dominant frequencies.\nAccording to Theorem 3.2, the k dominant frequencies capture the majority of the signal's energy, and the reconstruction error using only these frequency components remains relatively small. This ensures that the errors between variates sharing the same hash value do not significantly deviate from one another. To support empirical evidence, we include visualizations of the variate groups generated by k-DFH in Section 4.4. These visualizations illustrate that the variates within the same group exhibit similar periodic behavior, consistent with the theoretical expectations.\nNoise Robustness. One of the key advantages of k-DFH is its robustness to noise. Let X(t) be a time series composed of trends T(t), seasonality S(t) and noise E(t), such that X = T(t)+S(t) + E(t). The frequency spectrum \u03a6(X) can be expressed as the sum of the frequency spectrums of the signal and noise: \u03a6(X) = \u03a6(\u03a4) + \u03a6(S) + \u03a6(\u0395). Noise E(t) typically manifests as lower-amplitude components spread across the frequency spectrum, while the trends T(t) and seasonality S(t) contributes higher-amplitude components at specific frequencies. Since \u03a6(E) contributes relatively low amplitudes, the dominant frequencies below cutoff frequency \u03b5 are predominantly those of T(t) and S(t), making k-DFH invariant to undesirable high-frequency noise."}, {"title": "Experiments", "content": "In this section, we compare the proposed VarDrop with efficient Transformer baselines using public benchmark datasets for multivariate time series forecasting, evaluating both (i) forecasting performance and (ii) training efficiency to demonstrate VarDrop's effectiveness."}, {"title": "Experiment Settings", "content": "Datasets. We conducted experiments on four real-world multivariate time series datasets, each containing a large number of variates: Electricity, Traffic, Weather, and Solar-Energy . Further details on the data description are also provided in Appendix D.\nBaselines. We compare the performance of VarDrop with four efficient Transformers: Flowformer , FlashAttention , Reformer and Informer . FlashAttention reduces GPU memory bottlenecks by tiling vectors used in attention computation. All efficient baseline methods are modified to use variate tokenization following the previous study .\nBackbone Model and Ground Truth. To objectively evaluate VarDrop, we adopted iTransformer , a vanilla Transformer designed for the variate tokenization strategy, as our backbone model. We then used its dense attention results as the ground truth.\nEvaluation Metrics. For evaluation metrics, we choose the mean squared error (MSE) and the mean absolute error (MAE), consistent with previous work. Implementation Details. Following previous studies , we adopt same configuration for choosing optimization hyperparameters, such as the learning rate. In the experimental setup, the hyperparameters k and gs were selected from {3,4} and {5, 10, 20}, respectively. The source code is publicly available at https://github.com/kaist-dmlab/VarDrop and further details in Appendix E."}, {"title": "Overall Performance Comparison", "content": "Forecasting Performance. summarizes the forecasting results of VarDrop, including ground-truth and efficient baselines across four real-world datasets. VarDrop demonstrates superior performance, achieving the lowest relative MSE and MAE, both less than 1% compared to the ground truth. Please note that VarDrop accomplishes this high performance while utilizing a significantly reduced number of tokens for the attention process. VarDrop particularly excels in high-dimensional datasets such as Electricity and Traffic, increasing the feasibility of variate tokenization in large-scale applications. The high relative errors of efficient baselines stem from the improper handling of variate tokens, as they were designed for temporal tokens. To verify the robustness of VarDrop, we also provide the standard deviation of its"}, {"title": "Efficiency of VarDrop", "content": "Token Reduction Results. presents the token reduction ratios achieved by VarDrop across four benchmark datasets: Electricity, Traffic, Weather, and Solar-Energy. Our method adaptively identifies and drops redundant variate tokens during the training stage for each batch. Therefore, we report the average number of tokens over all iterations within an epoch, along with the corresponding standard deviation. verifies that VarDrop significantly reduces the number of tokens required for training. For example, in the Traffic dataset, VarDrop uses an average of 188.4\u00b120.6 tokens out of 862, achieving a token reduction ratio of 78.14%. Similarly, the Solar-Energy dataset exhibits an impressive reduction ratio of 85.38%, with only 20.0\u00b10.3 tokens used out of 137. These results demonstrate that VarDrop can be adopted for large-scale applications due to its efficiency and scalability.\nComparison of Training Times. To validate the efficiency of VarDrop, we compared the average running time per iteration with efficient baselines during the training stage. illustrates that VarDrop achieves the lowest average training time compared to the baselines. The ranking of training speeds for efficient baselines varied depending on the dataset. Notably, VarDrop increased the training speed of iTransformer from 68ms/iter to 33ms/iter, improving it by 2.06 times.\nComparison of Memory Footprints. We also report the results of comparing the GPU memory footprints of the baselines in . We observed that our proposed methodology uses less memory than all other efficient baselines. Remarkably, VarDrop utilizes 2.22 GB of memory, which is 65.1% of the memory foorprint of iTransformer. This result is consistent with the token reduction ratio presented in . As discussed in Section 3.3, this improved efficiency is attributed to the high reduction ratios, achieving $O(N^2(1 \u2013 \\delta)^2d)$. This significant reduction in computational overhead allows the Transformers exploiting variate tokens to reduce training time and memory usage. Overall, these results suggest that VarDrop is a promising approach for efficient training using variate tokenization."}, {"title": "Qualitative Analysis through Visualization", "content": "Effects of Maximum Group Size. Increasing the group size gs results in a higher density of the sparse matrix, leading to the lower level of token reduction ratio \u03b4. demonstrates how changes in the group size gs affect sparse matrices on the Electricity dataset. The token reduction ratios \u03b4 for each matrix are as follows: (b) 93.1%, (c) 76.1%, (d) 63.5%, and (e) 47.3%. The redundant variates indicated in yellow in are dropped less frequently even with large gs values via stratified sampling . Please refer to Figure 10 of Appendix G for more visualization results on other benchmark datasets."}, {"title": "Hyperparameter Sensitivity Analysis", "content": "To examine the effect of VarDrop's two crucial hyperparameters, k and gs, we conducted a sensitivity analysis by varying these values. illustrates the two forecasting errors (MSE and MAE) and the token reduction ratio \u03b4 for different values of the length of hash value k \u2208 {2, 3, 4, 5} and the maximum group size gs \u2208 {1, ..., 15} on the Electricity dataset. As shown in these results, as the value of k increases, the MSE and MAE decrease, while the token reduction ratio \u03b4 increases. Interestingly, there are sweet spots in determining the proper levels of k and gs. For instance, the forecasting errors and the reduction ratio when k = 4 and gs = 1 outperform those when k = 3 and gs = 5. This observation indicates that a higher reduction ratio does not necessarily result in lower forecasting performance and shows that VarDrop's originality differs from random sampling. This is because, as k increases, the groups generated by k-DFH can represent the underlying behaviors in the data with greater precision, as evidenced in Theorem 3.2. Consequently, selecting the appropriate levels of these two hyperparameters k and gs can significantly enhance both accuracy and computational efficiency, tailored to the specific needs of the application."}, {"title": "Conclusion", "content": "This paper introduces a simple yet efficient training strategy for Transformers using variate tokenization, named VarDrop, for periodic time series forecasting. VarDrop adaptively identifies groups of variates that exhibit similar behaviors through k-DFH. The number of variate tokens is then reduced by disregarding redundant variates within each group via stratified sampling. By dropping these redundant variate tokens for each batch, the training efficiency of the attention mechanism is significantly enhanced. Experimental results on benchmark datasets demonstrate that the proposed method outperforms state-of-the-art efficient methods. Furthermore, due to its modularity, our approach can be easily applied to existing Transformers utilizing variate tokens. We hope that our work enhances the potential of variate tokenization in large-scale applications with numerous variables."}, {"title": "Variate Redundancy in Multivariate Time Series Benchmark Datasets", "content": "In the multivariate time series, there are redundant variates that exhibit high correlations with each other due to common underlying factors. This redundancy can hinder the efficiency of the attention mechanism, making it crucial to identify and eliminate these redundant variates.\nTo visualize the variate redundancy in multivariate time series, we use Pearson Correlation coefficients of each variate of the input time series by the following equation:\n$\\rho_{vw} = \\frac{\\sum_i(v_i \u2013 \\bar{v}) (w_i \u2013 \\bar{w})}{\\sqrt{\\sum_i(v_i \u2013 \\bar{v})^2}\\sqrt{\\sum_i(w_i \u2013 \\bar{w})^2}}$,\n(4)\nwhere $v_i$ and $w_i$ are the values of the two variates at the i-th time point, and $\\bar{v}$ and $\\bar{w}$ are the mean values of the variates v and w, respectively. This coefficient measures the linear correlation between two variates, with values ranging from -1 to 1, where 1 indicates a perfect positive linear relationship, -1 indicates a perfect negative linear relationship, and 0 indicates no linear relationship."}, {"title": "Correlation Shifts in Sliding Windows", "content": "The correlation between variates can change as the sliding window moves through the data, resulting in unique correlations for each batch. The sliding window approach involves moving a fixed-size window over the time series data, capturing local temporal patterns and relationships among the variates within each window. illustrates correlation shifts among variates within diverse ranges of windows from each dataset. In all datasets, changes in the range of the target window lead to variations in the group of variates with strong correlations. These shifts occur due to the inherent property of time series data, where the distribution varies over time. The presence of correlation shifts in time series data necessitates the development of adaptive methods that can capture the dynamic batch-wise relationships between variates."}, {"title": "Proof of Theorem 3.2", "content": "Proof. According to the Fourier transform, any time series x(t) can be decomposed into a sum of sinusoidal signals:\nx(t) = \\sum_{i=1}^n A_i sin(2\\pi f_i t + \\phi_i)\n(5)\nwhere $A_i$ is the amplitude, $f_i$ is the frequency, and $\\phi_i$ is the phase of the i-th dominant sinusoidal component. Let $x_{recon}(t)$ be the reconstructed time series using only the k dominant frequencies:\nx_{recon}(t) = \\sum_{i=1}^k A_i sin(2\\pi f_i t + \\phi_i)\n(6)\nThe error e(t) can be defined as the difference between the original time series x(t) and the reconstructed time series $x_{recon}(t)$:\ne(t) = x(t) - x_{recon}(t) = \\sum_{i=k+1}^n A_i sin(2\\pi f_i t + \\phi_i)\n(7)\nThe mean squared error (MSE) over the interval [0, T] can be expressed as:\nMSE = \\frac{1}{T} \\int_0^T e(t)^2 dt\n= \\frac{1}{T} \\int_0^T (\\sum_{i=k+1}^n A_i sin(2\\pi f_i t + \\phi_i))^2 dt\n(8)\nUsing the orthogonality of sine functions, the cross terms in the integral will vanish, leaving us with:\nMSE = \\frac{1}{T} \\sum_{i=k+1}^n A_i^2 \\int_0^T sin^2(2\\pi f_i t + \\phi_i) dt\n(9)\nSince $sin^2(2\\pi f_i t + \\phi_i)$ has an average value of 1/2 over its period, the MSE simplifies to:\nMSE = \\frac{1}{T} \\sum_{i=k+1}^n A_i^2 \\frac{T}{2} = \\frac{1}{2} \\sum_{i=k+1}^n A_i^2\n(10)\nTherefore, the error is given by the cumulative contribution of the squared amplitudes of the non-dominant frequencies. This ensures that as long as the k dominant frequencies capture the majority of the signal's energy, the error will remain small."}, {"title": "Data Descriptions", "content": "Electricity contains the hourly electricity consumption of 321 customers from 2012 to 2014. Traffic consists of hourly road occupancy rates collected from 862 sensors located on San Francisco Bay Area freeways from 2015 to 2016. Weather consists of 21 meteorological indicators, such as air temperature, recorded every 10 minutes in 2020. Solar-Energy contains solar power production records from 2006, sampled every 10 minutes from 137 PV plants in Alabama. Following previous studies, we partitioned the datasets into training, validation, and test sets in a 7:1:2 ratio chronologically. We also provide the detailed descriptions of the four benchmark datasets in Table 6."}, {"title": "Implementation Details", "content": "The overall experiment configuration of VarDrop follows the approach of popular previous works . The batch size is set to 32 for all experiments. The settings for the two hyperparameters of VarDrop are presented in Table 7. The choice of these parameters depends on the requirements of the application and we selected them appropriately within the range of k \u2208 {3,4} and gs \u2208 {5, 10, 20} without sophisticated tuning, leaving their detailed optimization for future work. For low-pass filtering, cut-off frequency \u03b5 is set to 25, the half of the number of positive frequencies after Fourier transformation. Note that we did not perform any sophisticated tuning to determine these parameter values, and the choice of these parameters highly depends on the requirements of the application. The length of lookback window is set to 96 for all experiments. VarDrop is implemented using PyTorch 2.0.1 and tested on a single NVIDIA GeForce RTX 3090 24GB GPU."}, {"title": "Further Experimental Results", "content": "Due to space limitations in the main text, we provide additional experimental results in this section.\nForecasting Performance with Standard Deviation Table 8 shows the average forecasting performance of VarDrop, along with the standard deviation across five independent runs. The low standard deviation values indicate that VarDrop consistently produces reliable results, demonstrating its robustness."}, {"title": "Further Visualizations", "content": "Further Visualizations of k-DFH's Grouping Results. We provide additional visualization of the groups generated by k-DFH in Figure 9. Each subfigure demonstrates how the variates are clustered based on their hash values, representing overall periodic behaviors. In this visualization, the order of dominant frequencies distinguishes different patterns. In Figure 9(b), two variates with hash values \"4-8-12-20\" and \"4-12-8-20\" display distinct periodic behaviors despite having the same set of dominant frequencies. In Figure 9(a), the first variate labeled \"Hash Value: 1-2-3\" displays a relatively flat trend, indicating minimal periodic behavior. This implies that the k-DFH method can effectively isolate variates that do not share common periodic characteristics with others.\nVisualizations of Sparse Matrices. Figure 10 illustrates the correlation coefficients between variates in the last 96-length window of the training data and the corresponding sparse matrices generated by VarDrop. As clearly shown in the Electricity and Weather datasets, their sparse matrices reveal that most variates with low correlation to others are selected by VarDrop. This is because the variates exhibiting unique behaviors have dominant frequencies that differ from the others. The k-DFH algorithm identifies these as an independent group, allowing the selected variate tokens to effectively represent the most significant data distributions."}]}