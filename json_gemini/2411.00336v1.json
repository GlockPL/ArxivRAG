{"title": "StepCountJITAI: simulation environment for RL with application to physical activity adaptive intervention", "authors": ["Karine Karine", "Benjamin M. Marlin"], "abstract": "The use of reinforcement learning (RL) to learn policies for just-in-time adaptive interventions (JITAIs) is of significant interest in many behavioral intervention domains including improving levels of physical activity. In a messaging-based physical activity JITAI, a mobile health app is typically used to send messages to a participant to encourage engagement in physical activity. In this setting, RL methods can be used to learn what intervention options to provide to a participant in different contexts. However, deploying RL methods in real physical activity adaptive interventions comes with challenges: the cost and time constraints of real intervention studies result in limited data to learn adaptive intervention policies. Further, commonly used RL simulation environments have dynamics that are of limited relevance to physical activity adaptive interventions and thus shed little light on what RL methods may be optimal for this challenging application domain. In this paper, we introduce StepCountJITAI, an RL environment designed to foster research on RL methods that address the significant challenges of policy learning for adaptive behavioral interventions.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning (RL) is increasingly being considered for the development of just-in-time adaptive interventions (JITAIs) that aim to increase physical activity [Coronato et al., 2020, Yu et al., 2021, G\u00f6n\u00fcl et al., 2021, Liao et al., 2022]. In a physical activity adaptive intervention, participants typically use a wearable device (e.g., Fitbit) to log aspects of physical activity such as step counts [Nahum-Shani et al., 2018]. In an adaptive messaging-based intervention, a mobile health app is used to send messages to each participant to encourage increased physical activity. In an adaptive intervention, the selection of which messages to send at what times is personalized using context (or tailoring) variables. Context variables can include external factors such as time of day and location, as well as behavioral variables such as whether the participant is experiencing significant stress. Some context variables can be inferred from wearable sensor or other real time data, while others may be provided by participants via self-report mechanisms.\nIn this setting, RL methods can be used to learn what intervention options to provide to a participant in different contexts with the goal of maximizing a measure of cumulative physical activity, such as total step count over the intervention duration. The state variables used by an RL method correspond to the context variables (observed, inferred, or self-reported) relevant to selecting intervention options. The immediate reward is typically taken to be the step count in a window of time following an intervention decision point.\nHowever, deploying RL methods in real physical activity adaptive interventions comes with chal-lenges: the cost and time constraints of real intervention studies result in limited data to learn adaptive intervention policies. Real behavioral studies are difficult to conduct because they involve following many participants and can run for weeks or months while only allowing a handful of interactions with the participant per day [Hardeman et al., 2019]. This problem is particularly acute given the need to personalize intervention policies to individual participants.\nThe general problem of data scarcity in real adaptive intervention trials means that RL methods that require a large number of episodes to achieve high performance [Sutton and Barto, 1998, Mnih et al., 2013, Coronato et al., 2020] cannot typically be used in real adaptive intervention studies. Thus, there\nNeurIPS 2024 Workshop on Behavioral Machine Learning."}, {"title": "2 Methods", "content": "We provide an overview of the use of StepCountJITAI in an RL loop in Figure 1, and provide details below. We first describe the specifications, then introduce our new method. We use the same specifications and deterministic dynamics as in the base simulator [Karine et al., 2023]. For the notation, we use the following: short variable names are in upper case, and variable values are in lower case with subscript t indicating the time index, for example: we use C for the 'true context' variable name, and ct for the 'true context' value at time t."}, {"title": "2.1 StepCount JITAI states variables", "content": "We describe the state variables below. We show some examples of traces (i.e., how the variables change over time) in Appendix Figure 10. We also provide the deterministic dynamics equations for generating the environment states in Appendix C.\n\u2022 True Context (C). We include an abstract binary context $c_t \\in \\{0,1\\}$. This context can represent a binary state such as 'stressed'/'not stressed', 'fatigued'/'not fatigued', or 'home'/'not home'.\n\u2022 Context probability (P). This variable represents an inferred probability that the true context takes value 1, where $p_t \\in [0,1]$. It models the fact that in real-world studies, we typically do not have access to the true context, but can make inferences about the context using probabilistic machine learning models.\n\u2022 Most likely context (L). The most likely context $l_t \\in \\{0,1\\}$ is defined as the context value with the highest inferred probability according to $p_t$. It can be used to model situations where the context uncertainty is discarded when learning intervention policies."}, {"title": "2.2 StepCountJITAI dynamics", "content": "The environment dynamics depend on four actions: $a = 0$ indicates no message is sent. $a = 1$ indicates a non-contextualized message is sent. $a = 2$ indicates a message customized to context 0 is sent. $a = 3$ indicates a message customized to context 1 is sent to the participant.\nThe environment dynamics can be summarized as follows: Sending a message causes the habituation level to increase. Not sending a message causes the habituation level to decrease. An incorrectly tailored message causes the disengagement risk to increase. A correctly tailored message causes the disengagement risk to decrease. When the disengagement risk exceeds the given threshold, the episode ends. The reward is the surplus step count, beyond a baseline count, attenuated by the habituation level.\nThe base simulator implements deterministic dynamics, which we summarize in Appendix C. In this work, we extend the base simulator to create a simulation environment with additional stochasticity by introducing noise into the existing deterministic dynamics. We let $h_t$ be the habituation level, $d_t$ be the disengagement risk level, $s_t$ be the step count at time t. The dynamics of habituation and disengagement are governed by increment and decay parameters including the habituation decay $\\delta_h$, the habituation increment $\\epsilon_h$, the disengagement risk decay $\\delta_d$, and the disengagement risk increment $\\epsilon_d$. In the base simulator the dynamics parameters $\\delta_h$, $\\epsilon_h$, $\\delta_d$, and $\\epsilon_d$ are fixed. We make them stochastic at the episode level to model between person variation in the dynamics of habituation and disengagement risk. We also make the state variables $h_t$, $d_t$ and $s_t$ stochastic.\nWe construct two different versions of the stochastic dynamics based on the uniform and beta distributions. The uniform uncertainty-based dynamics are summarized below where the a parameters control the width of a uniform distribution about the mean values. The step counts themselves are positive reals and are sampled from a Gamma distribution parameterized by its mean and standard deviation $\\sigma_s$. The alternative beta distribution-based stochastic dynamics sample the values in [0, 1]"}, {"title": "3 Experiments", "content": "We perform RL experiments using StepCountJITAI including learning action selection policies with various RL methods: REINFORCE and PPO as examples of policy gradient methods, and DQN as an example of a value function method [Williams, 1987, Schulman et al., 2017, Mnih et al., 2013]. We also consider a standard Thompson sampling (TS) [Thompson, 1933]. We provide the RL implementation settings in Appendix F.4, and code samples in Appendix E.2.3. In Figure 1 (b), we show the mean and standard deviation of the average return over 10 trials, with 1500 episodes per trial, when using StepCountJITAI, with observed data [C, H, D], and using the stochastic parameters for Uniform distributions: $a_{hd} = 0.2$, $a_{de} = 0.5$, $\\sigma_s = 20$, and context uncertainty $\\sigma = 2$. In this setting, we show that the RL and TS agents are able to learn, with a maximum average return of around 3000 for RL and 1500 for TS. As expected, TS shows a lower average return than RL when using a complex environment such as StepCountJITAI.\nWe show additional results including generating traces in Appendix F.3 and stochastic variables histograms in Appendix F.2. We perform additional RL experiments using various parameter settings to control stochasticity in Appendix F.5."}, {"title": "4 Conclusion", "content": "We introduce StepCountJITAI, a simulation environment for physical activity adaptive interventions. StepCountJITAI is implemented using a standard RL API to maximize compatibility with existing RL research workflows. StepCountJITAI models key aspects of behavioral dynamics including habituation and disengagement risk, as well as context uncertainty and between person variability in dynamics. We hope that StepCountJITAI will help to accelerate research on new RL algorithms for the challenging problem of data scarce adaptive intervention optimization."}, {"title": "A Appendix", "content": "Below we provide the table of content for the main paper, as well as for the appendix."}, {"title": "B Overview of StepCountJITAI", "content": "We provide the summaries of the actions, environment states, and parameters for StepCountJITAI. In Appendix B.1, B.2, B.3, we provide a summary of the same specifications as the base simulation environment introduced in [Karine et al., 2023].\nIn Appendix B.4, we provide a summary of the new stochastic parameters that we introduce in this work. We describe the stochastic dynamics in the main paper in Section 2.2."}, {"title": "B.1 Summary of the possible action values used by StepCountJITAI", "content": ""}, {"title": "B.2 Summary of the variables generated by StepCountJITAI", "content": ""}, {"title": "B.3 Summary of StepCountJITAI parameters for deterministic dynamics", "content": ""}, {"title": "B.4 Summary of StepCountJITAI parameters for stochastic dynamics", "content": ""}, {"title": "C StepCountJITAI deterministic dynamics", "content": "The simulation environment introduced in the base simulator [Karine et al., 2023] models the deterministic dynamics. We summarize the specifications in Tables 1 and 2.\nWe provide a summary of the deterministic dynamics below.\n$c_{t+1} \\sim Bernoulli(0.5)$\n$x_{t+1} \\sim N(c_{t+1}, \\sigma^2)$\n$p_{t+1} = P(C = 1|x_{t+1})$\n$l_{t+1} = p_{t+1} > 0.5$\n$h_{t+1} = \\begin{cases}(1 - \\delta_h) \\cdot h_t & \\text{if } a_t = 0 \\\\min(1, h_t + \\epsilon_h) & \\text{otherwise}\\end{cases}$\n$d_{t+1} = \\begin{cases}d_t & \\text{if } a_t = 0 \\\\(1 - \\delta_d) \\cdot d_t & \\text{if } a_t = 1 \\text{ or } a_t = c_t + 2 \\\\min(1, d_t + \\epsilon_d) & \\text{otherwise}\\end{cases}$\n$s_{t+1} = \\begin{cases}m_s + (1-h_{t+1}) p_1 & \\text{if } a_t = 1 \\\\m_s + (1-h_{t+1}) p_2 & \\text{if } a_t = c_t + 2 \\\\m_s & \\text{otherwise}\\end{cases}$\nwhere $c_t$ is the true context, $x_t$ is the context feature, $\\sigma$ is the context uncertainty, $p_t$ is the probability of context 1, $l_t$ is the inferred context, $h_t$ is the habituation level, $d_t$ is the disengagement risk, $s_t$ is the step count ($s_t$ is the participant's number of walking steps), $a_t$ is the action value at time t.\nThe behavioral dynamics can be tuned using the parameters: disengagement risk decay $\\delta_d$, disen-gagement risk increment $\\epsilon_d$, habituation decay $\\delta_h$, and habituation increment $\\epsilon_h$.\nThe default parameters values for the base simulator are: $\\sigma = 0.4$, $\\delta_h = 0.1$, $\\epsilon_h = 0.05$, $\\delta_d = 0.1$, $\\epsilon_d = 0.4$, $p_1 = 50$, $p_2 = 200$, $m_s = 0.1$, disengagement threshold $D_{threshold} = 0.99$ (the study ends if $d_t$ exceeds $D_{threshold}$). The maximum study length is 50 days with one intervention per day, thus the maximum episode length is 50 days.\nThe context uncertainty $\\sigma$ is typically set by the user, with value $\\sigma \\in [0.2, 10.].$ In Appendix F.1, we describe how to select $\\sigma$."}, {"title": "D StepCountJITAI beta distribution-based stochastic dynamics", "content": "In the main paper, in Section 2.2, we introduce the equations for the uniform uncertainty-based dynamics. Below we introduce the beta distribution-based stochastic dynamics, using the same notations. The spread of the distribution is controlled by the $\\kappa$ concentration parameter.\n$h_{t+1} \\sim Beta(\\kappa_h h_{t+1}, \\kappa_h(1 - h_{t+1}))$\n$d_{t+1} \\sim Beta(\\kappa_d d_{t+1}, \\kappa_d(1 - d_{t+1}))$\n$\\delta_d \\sim Beta(\\kappa_{\\delta d}\\delta_d, \\kappa_{\\delta d}(1 - \\delta_d))$\n$\\epsilon_d \\sim Beta(\\kappa_{\\epsilon d}\\epsilon_d, \\kappa_{\\epsilon d}(1 - \\epsilon_d))$\n$\\delta_h \\sim Beta(\\kappa_{\\delta h}\\delta_h, \\kappa_{\\delta h}(1 - \\delta_h))$\n$\\epsilon_h \\sim Beta(\\kappa_{\\epsilon h}\\epsilon_h, \\kappa_{\\epsilon h}(1 - \\epsilon_h)).$"}, {"title": "E How to code using StepCountJITAI", "content": ""}, {"title": "E.1 StepCount JITAI interface", "content": "We implement the StepCountJITAI interface using a standard API for RL (i.e., gymnasium), so that StepCountJITAI can simply be plugged into a typical RL loop. The description of the API functions reset() and step(action), and the output variables info, terminated, and truncated, can be found in the gymnasium.Env online documentation.\nWhen instantiating using env = StepCountJITAI(chosen_obs_names = ...) as shown in the code sample in Appendix E.2.1, we can specify the desired variable names in chosen_obs_names. For example:\nchosen_obs_names = ['C', 'H'] will generate observed data [$c_t$, $h_t$] at each time t.\nchosen_obs_names = [\u2018C\u2019, \u2018H\u2019, \u2018D'] will generate observed data [$c_t$, $h_t$, $d_t$] at each time t.\nWe can also specify the parameters listed in Appendix B, by inserting the parameters as arguments in StepCountJITAI(...),as shown in the code samples in Appendix E.2.\nWe can use a get function to extract the current variable value at time t, for example: get_C() will extract the current true context value at time t.\nWe provide a summary of the main functions for StepCountJITAI in Table 5."}, {"title": "E.2 Quickstart code samples for StepCountJITAI", "content": "Below we provide quickstart code samples for StepCountJITAI. The StepCountJITAI interface is detailed in Appendix E.1."}, {"title": "E.2.1 Creating a StepCount JITAI simulation environment", "content": "StepCountJITAI is available here: https://github.com/reml-lab/StepCountJITAI.\nTo create the StepCountJITAI environment, we can call StepCountJITAI(...). We can set the parameters (e.g., n_version=1 for the stochastic version using Uniform distributions), and choose the observed variable names, as shown below.\nenv = StepCountJITAI(sigma=0.4, chosen_obs_names=['C', \u2018H\u2019], n_version=1)"}, {"title": "E.2.2 Generating simulation variables with random actions", "content": "We can generate the simulation variables with random actions, and use the built-in get functions to extract a particular variable. Below is an example where we store ct and ht current values into arrays.\nCs=[]; Hs=[]\nfor t in range(50):\n Cs.append(env.get_C())\n Hs.append(env.get_H())\n action = np.random.choice (4)\n obs, reward, terminated, truncated, info = env.step(action)"}, {"title": "E.2.3 Using StepCount JITAI in an RL loop", "content": "Below is the code for a typical RL loop, where the RL agent selects an action at each time t.\nobs, info = env.reset()\nfor t in range(50):\n action = agent.select_action(obs)\n obs, reward, terminated, truncated, info = env.step(action)\n obs = obs_\n if terminated or truncated: break"}, {"title": "F Additional Experiments and Results", "content": ""}, {"title": "F.1 Experiment: How to select the context uncertainty \u03c3?", "content": "The context uncertainty $\\sigma$ is a parameter that was introduced in the base simulator [Karine et al., 2023]. The user can use $\\sigma$ to control the desired context error.\nWe perform some experiments to show the relationship between the context uncertainty $\\sigma$ and the context error. In our experiment, we generate the true context $c_t$ and the inferred context $l_t$, using the deterministic dynamics equations in Section C, for various fixed values of $\\sigma$. Then we compute the context error (percentage of true context values that match the inferred context values), for N = 5000.\nWe note that as the context uncertainty $\\sigma$ increases, the context error increases.\nWe create a lookup table in Table 6, that can be used as a reference for selecting $\\sigma$. For example:\n\u2022 The user can set $\\sigma$ to 0.2, to get a context error of 1%.\n\u2022 The user can set $\\sigma$ to 0.4, to get a context error of 10%.\n\u2022 The user can set $\\sigma$ to 1, to get a context error of 30%."}, {"title": "F.2 Experiments: Creating histograms for stochastic ht, dt, St and \u03b4\u03b7, \u03b5h, dd, Ed", "content": "To illustrate the effects of the parameters on the stochastic dynamics, we generate $h_t$, $d_t$, $s_t$, as well as $\\delta_h$, $\\epsilon_h$, $\\delta_d$, $\\epsilon_d$ using the equations in Section 2.2, for various sets of parameters and fixed deterministic values. We plot the histograms below."}, {"title": "F.2.1 Histograms for stochastic ht, dt, St", "content": "We generate N = 5000 samples of $h_t$, $d_t$, $s_t$, using $h_t$ = 0.5, $d_t$ = 0.75, $s_t$ = 200. We plot the histograms for $h_t$, $d_t$, $s_t$ below. The vertical lines represent the deterministic values.\nUniform distributions. To sharpen the histogram peaks (\u201cmaking the stochastic dynamics closer to deterministic\"), the $a_{hd}$ and $a_{de}$ and $\\sigma_s$ values can be reduced.\""}, {"title": "F.2.2 Histograms for stochastic \u03b4\u03b7, \u03b5\u03b7, \u03b4\u03b1, Ed", "content": "We generate N = 1000 samples of $\\delta_h$, $\\epsilon_h$, $\\delta_d$, $\\epsilon_d$, using $\\delta_h$ = 0.1, $\\epsilon_h$ = 0.05, $\\delta_d$ = 0.1, $\\epsilon_d$ = 0.4. We plot the histograms for $\\delta_h$, $\\epsilon_h$, $\\delta_d$, $\\epsilon_d$ below. The vertical lines represent the deterministic values.\nUniform distributions. To sharpen the histogram peaks (\u201cmaking the stochastic dynamics closer to deterministic\"), the $a_{de}$ values can be reduced.\""}, {"title": "F.3 Experiments: Generating traces using StepCountJITAI with fixed actions or random actions", "content": "We provide examples of traces using deterministic StepCountJITAI, stochastic StepCountJITAI with Uniform distributions, and stochastic StepCountJITAI with Beta distributions, when using one of the following policies. We implement two policies: policy \"always a = 3\u201d where at each time t, the selected action is fixed, with value a = 3, and policy \"random action\" where at each time t, the selected action has a random value a \u2208 [0, 3].\nWe provide the code sample for policy \"random action\" in Section E.2.2. The code sample for policy \"always a = 3\" is the same except that the action is fixed to the value 3.\nIn our experiments, for each version of StepCountJITAI, we generate observed data [C, P, L, H, D] in a loop, using one of the two policies described above, for 30 time steps. We plot the traces of [C, P, L, H, D], the actions and the cumulative rewards at each time step t.\nTo get the traces for the full 30 steps, we set Dthreshold > 1 (e.g., 1.5), so that dt \u2208 [0, 1] will never exceeds Dthreshold.\nFor deterministic StepCountJITAI, we use: context uncertainty $\\sigma$ = 0.01 (i.e., nearly 0 context error) and the same default parameters as in the base simulator, as described in Appendix C.\nFor stochastic StepCountJITAI with Uniform distributions, we run experiments for various com-binations of parameters to control the stochasticity: [$\\sigma$, $a_{hd}$, $\\sigma_s$, $a_{de}$] values: [.1,.05, 2.5, .05], [.8, .05, 2.5, .05], [1., .05, 2.5, .05], [2., .05, 2.5, .05], [.1, .2, 10., .2], [.8, .2, 10., .2], [1., .2, 10., .2], [2., .2, 10., .2], [.1, .2, 20., .5], [.8, .2, 20., .5], [1., .2, 20., .5], [2., .2, 20., .5]. We show the results for: $\\sigma$ = 2, $a_{hd}$ = 0.2, $\\sigma_s$ = 20 and $a_{de}$ = 0.5.\nFor stochastic StepCountJITAI with Beta distributions, we run experiments with $\\kappa$ values in {1, 20, 100}, $\\sigma_s$ in {2.5, 10, 20} and $\\sigma$ in {0.1,0.8, 2}. We show the results for: $\\sigma$ = 2, all $\\kappa$ = 100 and $\\sigma_s$ = 20.\nThe traces are shown in Figure 10. We can see that when using deterministic StepCountJITAI with nearly 0 context uncertainty, the true context ct, the probability of context=1 pt and the inferred context lt match as expected. When using the stochastic versions of StepCountJITAI, we note that the true context ct and inferred context lt do not always overlap, due to the context uncertainty.\nWe note that these two policies are ineffective as expected. We can see that the cumulative reward decreases over time as per the environment dynamics. In the main paper, in Section 3, we describe the experiments with the RL methods, which have better policies."}, {"title": "F.4 RL Experiment Details", "content": "In Section 3, we describe the experiment and results when using StepCountJITAI with RL. Below we provide the experiment details. For each RL method, we select the best hyperparameters that maximize the performance, with the lowest number of episodes: the average return is around 3000 for the RL methods, and around 1500 for basic TS. All experiments can be run on CPU, using Google Colab within 2GB of RAM.\nThe RL implementation details are as follows.\nREINFORCE. We use a one-layer policy network. We perform a hyperparameter search over hidden layer sizes [32, 64, 128, 256], and Adam optimizer learning rates from 1e-6 to le-2. We report the results for 128 neurons, batch size b = 64, and Adam optimizer learning rate lr = 6e-4.\nDQN. We use a two-layer policy network. We perform a hyperparameter search over hidden layers sizes [32, 64, 128, 256], batch sizes [16, 32, 64], Adam optimizer learning rates from 1e-6 to 1e-2, and epsilon greedy exploration rate decrements from 1e-6 to 1e-3. We report the results for 128 neurons in each hidden layer, batch size b = 64, Adam optimizer learning rate lr = 5e-4, epsilon linear decrement \u03b4 = 0.001, decaying \u03f5 from 1 to 0.01. The target Q network parameters are replaced every K = 1000 steps.\nPPO. We use a two-layer policy network, and a three layers critic network. We perform a hyperpa-rameter search over hidden layers sizes [32, 64, 128, 256], batch sizes [16, 32, 64], Adam optimizer learning rates from 1e-6 to 1e-2, horizons from 10 to 40, policy clips from 0.1 to 0.5, and the other factors from .9 to 1.0. We report the results for 256 neurons in each hidden layer, batch size b = 64, Adam optimizer learning rate lr = 5e-3, horizon H = 20, policy clip c = 0.08, discounted factor \u03b3 = 0.99 and Generalized Advantage Estimator (GAE) factor \u03bb = 0.95.\nTS. We use a standard linear Thompson sampling with prior means \u03bc = 0, and covariance matrices \u03a3 = 100I, and we set the model noise variance \u03c3 = 25 for all action values a.\na \na \nIn Section 3, we use the following StepCountJITAI parameter settings.\nStepCount JITAI. We use observed data [C, H, D] and the stochastic version with Uniform distribu-tions, with parameters: $a_{hd} = 0.2$, $a_{de} = 0.5$, $\\sigma_s = 20$, and context uncertainty $\\sigma = 2$.\nIn Appendix F.5, we perform additional experiments with various StepCountJITAI parameter settings."}, {"title": "F.5 Additional RL Results for StepCount JITAI", "content": "In Section 3, we show an example where the RL methods achieve a high average return of around 3000. Below we perform additional experiments for StepCountJITAI with RL, to show when a standard RL method can or cannot work. We use different observed data, and different sets of parameters to control the stochasticity in the environment dynamics and context uncertainty.\nWe show an example of a case study where we do not have access to the true context C, but only to the inferred context L, and we have access to the behavioral variables H and D. Thus, we use StepCountJITAI with observed data [L, H, D]. We use the version with stochasticity using Uniform distributions, as described in Section 2.2. We use the same RL settings as described in Appendix F.4.\nFor the StepCountJITAI parameter settings, we use two settings of context uncertainty: lower context uncertainty \u03c3 = 0.1 and higher context uncertainty \u03c3 = 0.8, and two settings of parameters to control the stochasticity in the environment dynamics: lower stochasticity [ahd, \u03c3, ade] = [0.05, 2.5, 0.05] and higher stochasticity [ahd, \u03c3, ade] = [0.2, 20.0, 0.5]. We show the mean and standard deviation of the average return over 10 trials, with 1500 episodes per trial. We can see that when using the settings for lower context uncertainty and lower stochasticity in the environment dynamics, all the RL methods are able to learn, and reach a high average return of around 3000. When using the settings for lower context uncertainty but with higher stochasticity in the environment dynamics, the variability in the average returns is also higher. Using the setting for higher context uncertainty, all the RL methods average returns drop to below 2000. As expected, TS shows a lower average return than the RL methods in all the experiments."}]}