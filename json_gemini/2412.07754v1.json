{"title": "PortraitTalk: Towards Customizable One-Shot Audio-to-Talking Face Generation", "authors": ["Fatemeh Nazarieh", "Zhenhua Feng", "Diptesh Kanojia", "Muhammad Awais", "Josef Kittler"], "abstract": "Audio-driven talking face generation is a challenging task in digital communication. Despite significant progress in the area, most existing methods concentrate on audio-lip synchronization, often overlooking aspects such as visual quality, customization, and generalization that are crucial to producing realistic talking faces. To address these limitations, we introduce a novel, customizable one-shot audio-driven talking face generation framework, named PortraitTalk. Our proposed method utilizes a latent diffusion framework consisting of two main components: IdentityNet and AnimateNet. IdentityNet is designed to preserve identity features consistently across the generated video frames, while AnimateNet aims to enhance temporal coherence and motion consistency. This framework also integrates an audio input with the reference images, thereby reducing the reliance on reference-style videos prevalent in existing approaches. A key innovation of PortraitTalk is the incorporation of text prompts through decoupled cross-attention mechanisms, which significantly expands creative control over the generated videos. Through extensive experiments, including a newly developed evaluation metric, our model demonstrates superior performance over the state-of-the-art methods, setting a new standard for the generation of customizable realistic talking faces suitable for real-world applications.", "sections": [{"title": "Introduction", "content": "In recent years, audio-to-talking face generation has gained significant attention within the artificial intelligence community, primarily due to its broad spectrum of applications. These applications span across video content creation, animation production, video dubbing, and immersive experiences in the metaverse. The primary objective of audio-to-talking face generation is to create realistic talking face videos that are precisely synchronized with the provided audio input (Nazarieh et al. 2024; Tan et al. 2024a).\nThe studies in this area can be classified into three main categories: enhancing visual quality, improving audio-lip synchronization, and incorporating emotion into generated talking faces. While previous methodologies have shown promising results, they often produce videos that lack control of head poses, emotional expressions, identity consistency, and facial detail customization. An intuitive approach to overcoming these challenges involves processing individual aspects of facial motion (e.g., mouth movements, head orientation, and eye blink) separately, and integrating them during the rendering phase (Liang et al. 2022; Chen et al. 2019a; Zhou et al. 2020). However, this strategy presents significant drawbacks. Primarily, many of these methods depend heavily on predefined pose coefficients derived from 3D face models (Tan, Ji, and Pan 2024a; Peng et al. 2024; Xu et al. 2024; Peng et al. 2023), which increases the requirement of datasets and introduces potential inaccuracies (Tan et al. 2024a). In addition, these techniques require the entire network to be retrained from scratch when introducing a new identity.\nCurrent methods exhibit limited applicability in multimodal contexts due to their reliance solely on audio (audio-driven talking face generation (Zhou et al. 2020; Gan et al. 2023; Prajwal et al. 2020)) or video (video-driven talking face generation (Ji et al. 2022; Pang et al. 2023; Gong et al. 2023)). Recently, text-driven talking face generation has attracted increasing attention (Song et al. 2022; Li et al. 2021; Tan, Ji, and Pan 2024b; Jang et al. 2024). However, it remains significantly under-explored and is frequently overlooked in existing frameworks. Consequently, there is a pressing need for a comprehensive framework capable of generating talking faces that are controllable across multiple modalities. This capability is critically important for enabling more versatile and realistic results in various domains, including virtual assistants and animated characters. This framework is expected to simultaneously ensure precise control over facial details, achieve accurate audio-lip synchronization and manage background and environment. In this paper, we introduce a customizable one-shot audio-to-talking face generation framework (PortraitTalk) designed to address the above limitations with the assistance of decoupled cross-attention mechanism (Figure 1).\nPortraitTalk has two pivotal components: IdentityNet and AnimateNet. IdentityNet preserves consistent identity across video frames, while integrating semantics from text descriptions into the generated faces. To enable customization, we integrate text and image embeddings in IdentityNet through decoupled cross-attention (Ye et al. 2023a). Traditional approaches typically concatenate text-image pairs in text-driven talking face generation (Li et al. 2021; Ma et al. 2023) or audio-image pairs in audio-driven talking face generation (Shen et al. 2023; Ji et al. 2022; Shen et al. 2024) within a singular attention block. We argue that this approach restricts generative models from thoroughly learning and integrating identity-specific facial details and textual descriptions. Through extensive experiments, our findings demonstrate that PortraitTalk significantly enhances high-fidelity identity preservation, consistent talking-face generation, and overall controllability. Other critical quality factors in talking face generation are audio-lip synchronization and temporal consistency. Even minor artifacts or head displacements can substantially degrade the visual experience. AnimateNet leverages a pre-trained motion generator (Ye et al. 2023b) to map audio inputs to their corresponding facial landmarks effectively. This mapping is enhanced by head placement guidance, ensuring that the visual output remains coherent and synchronized throughout the generated video. By tackling these challenges, PortraitTalk represents a significant step forward in audio-to-talking face generation, offering a more robust and flexible solution that enhances the realism and practicality of generated talking face videos.\nDespite the notable progress in audio-to-talking face generation, the existing evaluation metrics often concentrate solely on isolated attributes such as visual quality or lip synchronization. Consequently, these metrics cannot provide a comprehensive assessment that reflects the holistic performance of a method. There exists an imperative need for a new evaluation metric that encompasses both spatial and temporal consistencies to facilitate thorough evaluation. Spatial consistency is crucial as it ensures the coherence and stability of the generated facial frames, thereby preserving the integrity of identity and expression throughout the video. Temporal consistency, on the other hand, assesses the smoothness and natural flow of the transition over time, which are vital for producing lifelike and persuasive talking faces. In this paper, we propose a novel Audio-Driven Facial Dynamics (ADFD) measurement. ADFD addresses the above deficiencies by integrating both spatial and temporal factors, thereby providing a reliable measure of the overall quality of a generated talking face video.\nThe main contributions of PortraitTalk include:\n\u2022 To the best of our knowledge, this is the first customizable one-shot audio-to-talking face generation method.\nWe use three different modalities in the proposed PortraitTalk framework.\n\u2022 We propose a new evaluation metric that effectively measures video quality in both spatial and temporal aspects.\n\u2022 Qualitative and quantitative experiments demonstrate the effectiveness of PortraitTalk in enhancing synchronization, realism, and customization of generated talking face videos as compared with the state-of-the-art methods."}, {"title": "Related Work", "content": "In this field, various methodologies have been proposed, with intermediately guided talking face generation being particularly common. Such models leverage intermediate representations, such as 2D facial landmarks or 3D faces, to effectively bridge the gap between audio input and visual output. These methods (Zhou et al. 2020, 2021; Gan et al. 2023) typically consist of two main components: the first predicts intermediate representations from audio, and the second uses these features to generate talking faces. This two-stage methodology facilitates control over the synchronization between audio and visual data, thereby enhancing the realism and accuracy of the generated talking faces.\nChen et al. (2019a) proposed a method that converts audio into mouth landmarks to generate talking faces. However, as it only predicts movements for the lower half of the face and uses the upper half from ground truth frames, the resulting videos often lack realism. Zhou et al. (2020) introduced MakeItTalk, a one-shot audio-to-talking-face generation model based on facial landmarks. While it can handle various identities, MakeItTalk struggles with precise audio-lip synchronization and often fails to deliver high-quality talking faces. The Audio2Head (Wang et al. 2021) model generates talking faces by extracting relatively dense motion fields from audio, but the faces often exhibit distortions and lack consistency in identity preservation. SadTalker (Zhang et al. 2023) has significantly enhanced the generalization capability through a learned latent space. Despite this advancement, the head movements, which are generated based on predefined motion coefficients, frequently lack realism. IP-LAP (Zhong et al. 2023), a transformer-based generative model, utilizes audio inputs and sketches to produce talking faces. Although it achieves stable head movements, it falls short in synchronizing lip movements with audio, particularly with identities unseen during training."}, {"title": "Emotional Talking Face Generation", "content": "Emotional talking face generation has attracted increasing attention, primarily due to its potential to enhance communication and its utility in the entertainment industry. Traditional methods have predominantly used discrete emotion labels to model expressions. For example, Goyal et al. (2023) developed a model inspired by Wav2Lip (Prajwal et al. 2020), integrating an emotion label encoder, emotion-specific loss function, and an emotion discriminator. This model, however, primarily modifies the mouth region, thereby producing videos that lack realistic appearance variations. Furthermore, it exhibits limited effectiveness in displaying diverse expressions and fails to maintain an identity consistency across the generated frames.\nLabel-based emotional talking face generation typically struggles to produce controllable and fine expressions. To address these limitations, recent developments advocate the use of emotional videos as references (Sun et al. 2024; Tan et al. 2024b; Ji et al. 2022; Tan et al. 2024a). This approach allows for the injection of expressions directly derived from other videos, enhancing the expressiveness of generated faces. For example, EAMM (Ji et al. 2022) uses an emotional source video to generate emotional talking faces. Nonetheless, this method often struggles with identity consistency, exhibits noticeable irregularities, and encounters audio-lip synchronization issues. These challenges underscore the necessity for the development of a model that can integrate emotion with audio-visual semantics more effectively. Similarly, EDTalk (Tan et al. 2024a) employs emotions from reference-style videos to enhance the expressiveness of talking faces. While this method substantially enhances expressiveness, it sometimes does so at the cost of visual quality."}, {"title": "The Proposed Method", "content": "PortraitTalk is a one-shot audio-driven talking face generation framework with aligned audio-lip movements by using only reference identity images and the corresponding speech. Figure 2 illustrates the pipeline of PortraitTalk. We also enhance the design by integrating text prompt embedding, which enables PortraitTalk to edit video attributes including style, expression, and appearance customization. PortraitTalk has two essential components. First, IdentityNet efficiently embeds high-level and low-level semantics from text and image prompts, ensuring the preservation of precise facial details across the generated video frames. Second, AnimateNet employs condition control features along with temporal consistency mechanisms to facilitate smooth transitions within the video."}, {"title": "IdentityNet", "content": "The proposed IdentityNet adopts the architecture of Stable Diffusion (Rombach et al. 2022) (SD1.5) and serves as the primary mechanism for preserving the facial characteristics throughout a generated video. The backbone network retains the original weights of the UNet in SD, enabling it to effectively maintain the generation capability of SD. This provides a more robust initialization for PortraitTalk.\nTo reduce the inconsistency among synthesized frames and enhance the reconstruction capability of IdentityNet, the model undergoes finetuning with masked images. This strategy involves random corruption of images and subsequent reconstruction of the missing segments. By finetuning IdentityNet using masked images, the model's focus shifts towards the global characteristics of images, such as the overall head shape and style associated with a specific identity (Chen et al. 2024). The fine-tuning process follows the training strategy of stable diffusion. Randomly masked frames are encoded by passing through the encoder to obtain a masked latent representation zn. Subsequently, a forward and backward diffusion process is applied across T time steps within this masked latent space. The denoising U-Net (60) is trained to reconstruct the original image by estimating the amount of noise (\u20ac) introduced at each forward step. This is achieved by the mask reconstruction loss:\n$L_{mask} = E_{t,z_t,c,\\epsilon\\sim N(0,1)} [||\\epsilon - \\epsilon_{\\theta}(z_t^m, t, c)||^2]$ (1)"}, {"title": "AnimateNet", "content": "Enforcing proper motion in talking face videos is crucial, as it enables the creation of realistic talking face videos. To this end, we split the animation enforcement process into two distinct phases: audio-to-motion and motion-to-frame generation. The audio-to-motion component is responsible for producing a sequence of facial movements (here, face landmarks) that accurately represent expected lip movements. We utilize an audio-to-motion generation module (Ye et al. 2023b), which employs a variational autoencoder alongside the HuBERT audio Transformer (Hsu et al. 2021). This module is equipped with a dilated convolutional encoder and decoder which enhances the robustness of feature extraction and the generation of extended motion sequences. Following this, the motion-to-frame generation phase is expected to produce realistic talking face videos that are aligned with facial landmarks derived from the audio-to-motion module. To facilitate this, we introduce AnimateNet, which is developed based on the principles of ControlNet (Zhang, Rao, and Agrawala 2023).\nAccurate audio-lip synchronization, while necessary, is not sufficient for producing realistic talking face videos. Temporal consistency is vital for video generation, as it ensures smooth and coherent transitions among video frames. In addition, variability in head movements, attributable to the stochastic nature of diffusion models, can complicate the creation of consistent and smooth transitions in the talking person's motion from frame to frame. Drawing inspiration from the previous studies (Tian et al. 2024; Stypu\u0142kowski, Vougioukas et al. 2024), we utilize the last generated frame from the previous step as the initial point for the subsequent frame generation and head placement guidance, thereby ensuring continuity and consistency across frames. Specifically, for each frame, we extract facial landmarks corresponding to the associated speech segment. Meanwhile, we retrieve the head placement of the previously generated frame and use it to guide the head placement of the current frame. For the initial frame, we retrieve and utilize the head placement from the reference image. These acquired facial landmarks and head placements are then processed through dedicated encoders to generate respective guidance embeddings for landmarks and head placements. These embeddings are subsequently integrated into AnimateNet via distinct cross-attention blocks. The processed animation information is injected into IdentityNet via cross-attention mechanisms. The final cross-attention output incorporates information related to identity, facial structure and temporal information across adjacent frames.\nAnimateNet utilizes a similar objective function as the original stable diffusion:\n$L = E_{t,z_t,c,\\epsilon\\sim N(0,1)} [||\\epsilon - \\epsilon_{\\theta}(z_t, t, c)||^2]$ (2)\nwhere c refers to the set of conditions, including identity features, facial landmarks, head placement guidance and temporal information. This architectural design enables PortraitTalk to concentrate on audio-lip synchronization, while preserving detailed facial characteristics across generated frames. It ensures temporal consistency and synchronizes head movements throughout the generated talking video."}, {"title": "Audio-Driven Facial Dynamics Score", "content": "Existing metrics in audio-to-talking face generation often focus on individual aspects of talking face video (e.g., visual quality (Hong and Xu 2023), audio-lip synchronization (Chung and Zisserman 2016)). There is a need for a comprehensive metric that evaluates realistic facial details, temporal alignment, and motion coherency. The Audio-Driven Facial Dynamics (ADFD) score addresses this by assessing both the spatial alignment and dynamic movement of facial landmarks over time, advancing beyond traditional metrics by incorporating these additional factors relative to the audio. The audio-driven facial dynamics score is formulated as:\n$ADFD = w_1(\\frac{1}{T}\\sum_{t=1}^T(\\frac{1}{n}\\sum_{i=1}^n(Len_i^t - Lit)^2/d) + 1)\\times w_2(\\frac{(Mien. Mit)}{2|| Men || || Mit ||} + 1)$ (3)\nwhere, T represents the total number of frames in a video, Lien and Lit are arrays of generated and ground truth landmarks, respectively, at each time step t. d is the maximum distance between two points in the frame, which is used for normalization. Motion vectors of the generated and ground truth landmarks at time step t are represented by Mien and Mst. To determine these motion vectors, we calculate the difference between two consecutive frames. W\u2081 and w\u2082 are the balancing parameters.\nThe ADFD score has two main parts. The first quantifies the spatial alignment of facial landmarks between generated and ground-truth talking face videos by measuring the normalized Euclidean distance between the corresponding landmarks. The second part assesses the motion coherence and temporal consistency of the facial movements by calculating the cosine similarity of the motion vectors. This part evaluates the extent to which the direction and scale of facial movements in the generated video correspond with those in the ground truth over time. To accommodate specific research needs, ADFD can be fine-tuned using adjustable weights (w\u2081 and w\u2082), allowing one to prioritize certain aspects of the evaluation. Further, to ensure that all values reflect degrees of similarity on a non-negative scale, the range of the cosine similarity component has been adjusted from [-1,1] to [0, 1]. A high value in the ADFD score (close to 1) signifies that the generated talking face video not only aligns well spatially but also demonstrates temporal consistency across frames."}, {"title": "Experimental Settings", "content": "We use the HDTF (Zhang et al. 2021) and MEAD(Wang et al. 2020) datasets in our study. The HDTF dataset contains high-quality videos of over 300 identities from YouTube. For training, we randomly select 6 videos, approximately 30,000 frames in total. Subsequently, these videos are carefully reviewed to ensure the training set represents diverse gender and ethical variations. MEAD contains 60 speakers with 8 different emotions. This dataset is captured in a laboratory setting and each emotion is captured under three different emotion intensity levels. In this paper, four randomly selected videos from MEAD are used for training. For the testing stage, similar to Tan et al. (2024a), we randomly select 20% of the HDTF dataset and choose speakers \u2018M003', 'M009', 'M030', and 'W015' from MEAD.\nFollowing previous work (Shen et al. 2023; Hong and Xu 2023), we employ well-established metrics such as PSNR (Hor\u00e9 and Ziou 2010), SSIM (Wang et al. 2004), and FID (Heusel et al. 2017) to assess the face generation capability of our model relative to previous state-of-the-art methods. To further evaluate the accuracy of facial landmark placement and the smoothness of facial motion transitions between frames, we use LandMark Distance (LMD) (Chen et al. 2019b) and our novel Audio-Driven Facial Dynamics (ADFD) score. Additionally, we incorporate the SyncNet error metric to specifically measure the synchronization between audio and visual elements, ensuring that our model effectively aligns lip movements with spoken words, a critical factor for evaluating the generated talking face videos.\nOur framework, constructed on the basis of Stable Diffusion v1.5 (Rombach et al. 2021), is implemented using PyTorch. We train our model with the AdamW (Loshchilov and Hutter 2017) optimizer and set the batch size to 32. The learning rate is set to 5e 6. It should be noted that after generating facial landmarks from audio speech, we refine these landmarks to closely align with the outputs from MediaPipe (Ablavatski et al. 2020). This refinement significantly enhances the training of AnimateNet and positively influences its generalization capability. The training process encompasses a multi-stage approach. Initially, each component is trained independently. IdentityNet is finetuned for 10 epochs, followed by the training of AnimateNet (excluding temporal attention) for 100 epochs. Subsequently, in the second stage, the entire framework undergoes integrated training for 50 epochs. All training and testing phases are conducted on a single NVIDIA A100 GPU. The overall training takes approximately two weeks."}, {"title": "Experiments", "content": "To assess our model, we conducted a quantitative comparison with the state-of-the-art methods in audio-to-talking-face generation. The results are reported in Table 1. According to the table, PortraitTalk consistently outperforms the existing approaches across nearly all the metrics on the HDTF and MEAD datasets. Wav2Lip (Prajwal et al. 2020) achieves a relatively higher SyncNet score on HDTF, primarily because it utilizes this metric as one of its training loss functions. Our model achieves a satisfactory SyncNet score, surpassing Wav2Lip on the MEAD dataset, and excels in audio-lip alignment and facial structure accuracy, as indicated by LMD and ADFD metrics. Although EAMM has the highest ADFD score, our model ranks closely behind on the HDTF dataset, showing similar effectiveness. The slightly larger gap on the MEAD dataset is likely due to its wider range of emotional expressions and varying intensities which directly impacts lip movements."}, {"title": "Qualitative Results", "content": "In Figure 3, we compare the generation quality of PortraitTalk against the state-of-the-art approaches. MakeItTalk and Wav2Lip produce low-quality outputs with artifacts, notably around the mouth, which fail to blend seamlessly with adjacent facial features. Audio2Head (Wang et al. 2021), TalkLip (Wang et al. 2023), and IP-LAP (Zhong et al. 2023) exhibit shortcomings in achieving precise audio-lip synchronization, with generated mouth movements often slightly open and displaying distortions across frames, thereby diminishing the realism of the output.\nIn emotional talking face generation, existing methods typically generate videos based on discrete emotional labels or facial expressions extracted from user-provided reference-style videos. This approach, however, has significant limitations. EAMM (Ji et al. 2022) and EmoGen (Goyal et al. 2023), for example, struggle to maintain identity consistency across the generated videos. This issue is particularly visible in Figure 3, involving the male subject with EAMM and the female subject with EmoGen. Furthermore, EmoGen's modifications are restricted to the square space of the facial region, leading to blurred areas in each frame. Such inconsistencies severely compromise the realism and visual quality of the generated talking face videos. EDTalk (Tan et al. 2024a) shows improved performance in generating emotional talking faces. However, it also exhibits noticeable jitter and irregular mouth shapes in the generated frames. In contrast, our model demonstrates superior performance in both emotion-agnostic and emotional talking face generation. It excels at preserving identity consistency, ensuring precise audio-lip synchronization, and maintaining consistent head movements throughout the video."}, {"title": "Ablation Study", "content": "To evaluate the critical components of our framework, we conduct an ablation study in this part.\nUnified Attention Block: Initially, we merge the separate cross-attention blocks of IdentityNet into a single attention block to assess its impact on performance. The results are shown in Table 2 and Figure 4. The findings indicate a significant degradation in the model's ability to preserve the intended identity, with its recognition capability reduced to identifying only the gender from text prompts.\nImpact of the Face Encoder: Further, we substitute our dedicated face encoder with a CLIP image encoder to explore the influence of the encoding mechanism on the generation capability of PortraitTalk. While there is an observable enhancement in image fidelity and identity consistency compared to the initial ablation scenario, the model still struggles accurately to maintain detailed identity attributes across frames. These experiments validate our decision to employ a specific face encoder optimized for facial feature extraction, underscoring its importance in achieving high-quality and consistent talking face generation.\nNumber of Reference Images: We investigate the impact of using multiple reference images on the quality and realism of generated visual content. As demonstrated in Figure 4, employing multiple images of the same identity, showcasing various expressions or head angles, significantly enhances the model's capability to learn the underlying facial structure of that identity. This strategy is essential for facilitating customization in generated talking face videos, enabling the model to effectively respond to a variety of customization prompts. In contrast, reliance on a single reference image leads to a considerable reduction in both, the quality and realism of the generated faces. This issue is especially evident in male case (Figure 4), where the model produces an overly oval head shape.\nImpact of Structure and Temporal Attention: We also try to remove the head placement guidance and temporal attention mechanisms from AnimateNet. This modification leads to a noticeable decrease in coherence across frames in the generated videos. As illustrated in Figure 4, while the lip synchronization remains somewhat accurate, there is a clear inconsistency in head placement and noticeable distortions between consecutive frames. Interestingly, without temporal attention, the generative model often fails to maintain consistency in the clothing of the generated talking faces. This variation underscores the crucial role of head placement guidance and temporal attention in maintaining the stability and continuity of head movements throughout the video. s"}, {"title": "Applications and Limitations", "content": "Our model integrates three modalities for talking face generation, offering output flexibility. Figures 5 and 1 illustrate various examples, including identity interpolation, expression editing, and modifications of style, age, gender, and background. This multimodal integration substantially enhances the customization capability of PortraitTalk, facilitating the generation of emotionally accurate talking faces. However, PortraitTalk occasionally exhibits degraded visual quality and lip synchronization, especially under intense emotions. Additionally, unintended elements such as hands might appear when modifying the style, attributable to the exclusive reliance on the CLIP text encoder, without a specialized training dataset for style changes. These issues highlight the need for enhancements to achieve an optimal balance between expressiveness and realism of the outputs. Exhibition of Artifacts is provided in supplementary material."}, {"title": "Conclusion", "content": "We introduced PortraitTalk, a customizable one-shot audio-to-talking face generation framework, comprising IdentityNet and AnimateNet. PortraitTalk preserves the identity and enhances the temporal coherence in video generation. It not only delivers high-fidelity audio-lip synchronization but also offers flexible customization capability via text prompts. This enables the creation of expressive talking faces across varied styles and emotions without retraining. Both quantitative and qualitative assessments confirm the superiority of our model, affirming its effectiveness and practical applicability in real-world scenarios."}]}