{"title": "UnUnlearning: Unlearning is not sufficient for content regulation in advanced generative AI", "authors": ["Ilia Shumailov", "Jamie Hayes", "Eleni Triantafillou", "Guillermo Ortiz-Jimenez", "Nicolas Papernot", "Matthew Jagielski", "Itay Yona", "Heidi Howard", "Eugene Bagdasaryan"], "abstract": "Exact unlearning was first introduced as a privacy mechanism that allowed a user to retract their data from machine learning models on request. Shortly after, inexact schemes were proposed to mitigate the impractical costs associated with exact unlearning. More recently unlearning is often discussed as an approach for removal of impermissible knowledge i.e. knowledge that the model should not possess such as unlicensed copyrighted, inaccurate, or malicious information. The promise is that if the model does not have a certain malicious capability, then it cannot be used for the associated malicious purpose. In this paper we revisit the paradigm in which unlearning is used for in Large Language Models (LLMs) and highlight an underlying inconsistency arising from in-context learning. Unlearning can be an effective control mechanism for the training phase, yet it does not prevent the model from performing an impermissible act during inference. We introduce a concept of ununlearning, where unlearned knowledge gets reintroduced in-context, effectively rendering the model capable of behaving as if it knows the forgotten knowledge. As a result, we argue that content filtering for impermissible knowledge will be required and even exact unlearning schemes are not enough for effective content regulation. We discuss feasibility of ununlearning for modern LLMs and examine broader implications.", "sections": [{"title": "Introduction", "content": "Recent advancements in Large Language Models (LLMs) raise concerns about their use for undesirable purposes. Unlearning emerged as a promising solution for knowledge control, originally developed for removal of privacy-sensitive information (Bourtoule et al., 2021). Since then, several works have attempted to utilize unlearning for a host of applications relating to the removal of undesired knowledge or behaviours: removing harmful capabilities (Lynch et al., 2024) or harmful responses (Liu et al., 2024; Yao et al., 2023), erasing backdoors (Liu et al., 2022) or specific information or knowledge pertaining to a particular topic (Eldan and Russinovich, 2023; Li et al., 2024), erasing copyrighted content (Yao et al., 2023) and even reducing hallucinations (Yao et al., 2023). Such applications have been studied in the context of diffusion models too, with various attempts to use unlearning to remove unsafe concepts (Fan et al., 2023; Zhang et al., 2023).\nThis paper discusses the application of unlearning to LLMs for removal of broadly impermissible knowledge, the use-case often discussed in policy circles e.g. for removal of biological and nuclear knowledge (Li et al., 2024). In fact, we uncover a fundamental inconsistency of the unlearning paradigm for this application. While unlearning aims to erase knowledge, the inherent in-context learning (ICL) (Agarwal et al., 2024; Brown et al., 2020; Kossen et al., 2024) capabilities of LLMs introduce a major challenge. We introduce the concept of ununlearning, where successfully unlearned knowledge can resurface through contextual interactions. This raises a critical question: if unlearned information can be readily reintroduced, is unlearning a truly effective approach for making sure that the model does not exhibit impermissible behaviours? We discuss the ramifications of ununlearning, particularly the need for effective content regulation mechanisms to prevent the resurgence of undesirable knowledge. Ultimately, we question the long-term viability of unlearning as a primary tool for content regulation.\nNote, this paper explicitly only considers the case when unlearning is used for purposes of content regulation i.e. problems formulated as as a model developer I do not want my model to be able to perform X, where X can be e.g. bioweapons development (Li et al., 2024). Entities deploying models operate under the expectation that those models don't pose a risk of being exploited for dangerous applications like weapons development. Importantly, it does not cover the original use-case of unlearning for the privacy purposes."}, {"title": "Nomenclature", "content": "In what follows, we rely on six main terms:\n(Informal) Definition 1. Knowledge refers to information available to the model. This information can take up different forms and includes e.g. in-context provided inputs, information stored in the parameters of the model, or evidence available for retrieval.\n(Informal) Definition 2. Content filtering refers to the process of filtering out queries to and responses from a given model. Filtering can both be a part of the model, as well as, be external to it (Glukhov et al., 2023).\n(Informal) Definition 3. Unlearning refers to a process in which knowledge is removed from a given model. This is a broad description that can encompasses different application scenarios. Below, we provide two informal definitions of unlearning.\n(Informal) Definition 4. Unlearning for privacy seeks to remove knowledge that is defined as a particular subset of the model's original training dataset, referred to as the \"forget set\". Formal definitions (Ginart et al., 2019; Sekhari et al., 2021) require the (distribution of the) unlearned model to be indistinguishable from (the distribution of the) model retrained excluding the forget set. According to this notion, an unlearning method can either be exact (Bourtoule et al., 2021; Muresanu et al., 2024), guaranteeing indistinguishability between the aforementioned distributions, or inexact (Golatkar et al., 2020; Kurmanji et al., 2024; Thudi et al., 2022), instead offering an approximation to that goal in exchange for greater efficiency or better model utility.\n(Informal) Definition 5. Unlearning for content regulation seeks to remove knowledge that is (believed to be) associated with producing impermissible content. Note that the specification of this knowledge in this case does not necessarily take the form of identifying a subset of the training dataset. Instead, it captures more broadly information that we want to remove from the model (regardless of which subset of training data led to acquiring it), in order to prevent it from generating impermissible content. This notion aligns with the recent related notion of Goel et al. (2024) for \u201ccorrective unlearning\"."}, {"title": "UnUnlearning", "content": "Setting: Assume a model M that takes in x \u2208 X and outputs y. Note that here X includes both training and inference-time data. We assume we have an unlearning method u that takes in a model and points X \u2286 X and outputs a model M that unlearned the said points u(M,X) = M. In this paper we assume that X is the impermissible knowledge that is defined and identified by the model developer. We argue that by relying on in-context learning (ICL) an adversary can bring back the knowledge such that M when prompted with special context outputs the same results are M over X. In other words, M(X) \u2248 M(prompt+X).\n(Informal) Definition 7. UnUnlearning refers to a process where previously removed or never learned knowledge is instructed into the model using in-context learning.\nIn this paper we argue that ununlearning is a concern that should be taken into account when unlearning schemes are used and designed for removing impermissible functionality; as well as, models openly released. Note that ununlearning applies even to exact unlearning."}, {"title": "Discussion", "content": "There is a number of ramification of ununlearning.\nNeed for Effective Filtering Mechanisms\nUnUnlearning implies that for unlearning to remain effective, one needs to not only remove impermissible knowledge, but also perform a continuous, active process of suppressing in-context attempts to reintroduce that knowledge. Fundamental computational bounds suggest that such filtering is likely limited too, for example because of dependence on the context in which queries appear (Glukhov et al., 2023).\nDefinitions and Mechanisms for Unlearning\nGiven the above, is traditional unlearning by itself even a worthwhile pursuit for controlling the use of impermissible knowledge? Consider the Tiger-Zebra setting we discussed earlier. If we accept classic definitions for unlearning, a model that never trained on any Tiger knowledge represents perfect unlearning by construction. In other words, no unlearning method would perform better than a model that never saw any of the knowledge. Yet, since the model with in-context introduced concept of Tiger can successfully reason about the Tiger it is clearly not confirming to the objective of not reasoning at all about it. Here, further exploration into the precise definitions and mechanisms for ununlearning is essential \u2013 we need to find a way to explicitly limit the reasoning capability for our models that is invariant to prompting and other types of learning.\nAttributing knowledge If a small benign axiom forces the attacker to discover a malicious theorem, who should be attributed for it? This dilemma mirrors the age-old philosophical debate \u2013 should an act be attributed to an individual who directly executes the act, the person who gives the order, the manufacturer of the tool, or the original tool designer (Fischer and Ravizza, 1998; Owen, 1992)? We argue that given that knowledge can be introduced by different parties, unlearning should not be assumed to be the sole mechanism for content policy enforcement.\nForbidding knowledge Instead of filtering out data, it may be better to teach the model explicitly that some knowledge is off limits (Henderson et al., 2023). Do note however that this is not a bullet proof solution and will unlikely to be robust against, for example, mosaic attacks (Glukhov et al., 2023). Furthermore, the approach of forbidding knowledge requires foreseeing the types of harmful tasks or use-cases ahead of time and does not guarantee preventing harmful use more generally, beyond a given set of identified use-cases. Finally, it is not clear how a model should react to violations of its content regulations. In privacy literature it is a known fact that existence of the privacy mechanism can lead to an increased privacy leakage (Wang et al., 2022), and a similar effect was previously observed in unlearning (Hayes et al., 2024). In other words, a model rejecting a request to synthesise a given chemical recipe can also help the malicious user understand what recipes can be used for malice."}, {"title": "Conclusion", "content": "In this paper we argue that unlearning offers an incomplete solution for impermissible knowledge removal in LLMs with strong ICL capability. UnUnlearning forces us to rethink unlearning as a one-size-fits-all solution and places emphasis on content filtering."}]}