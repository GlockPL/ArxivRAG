{"title": "Lecture Notes on Linear Neural Networks: A Tale of Optimization and Generalization in Deep Learning", "authors": ["Nadav Cohen", "Noam Razin"], "abstract": "These notes are based on a lecture delivered by NC on March 2021, as part of an advanced course in Princeton University on the mathematical understanding of deep learning. They present a theory (developed by NC, NR and collaborators) of linear neural networks a fundamental model in the study of optimization and generalization in deep learning. Practical applications born from the presented theory are also discussed. The theory is based on mathematical tools that are dynamical in nature. It showcases the potential of such tools to push the envelope of our understanding of optimization and generalization in deep learning. The text assumes familiarity with the basics of statistical learning theory.", "sections": [{"title": "Introduction", "content": "Deep learning (machine learning with neural network models subject to gradient-based training) is delivering groundbreaking performance, which facilitates the rise of artificial intelligence (see LeCun et al. (2015)). However, despite its extreme popularity, our formal understanding of deep learning is limited. Its application in practice is based primarily on conventional wisdom, trial-and-error and intuition, often leading to suboptimal results (compromising not only effectiveness, but also safety, robustness, privacy, fairness and more). Consequently, immense interest in developing mathematical theories behind deep learning has arisen, in the hopes that such theories will shed light on existing empirical findings, and more importantly, lead to principled methods that bring forth improved performance and new capabilities.\nFrom the perspective of statistical learning theory, understanding deep learning requires addressing three fundamental questions: expressiveness, optimization and generalization. Expressiveness refers to the ability of compactly sized neural networks to represent functions capable of solving real-world problems. Optimization concerns the effectiveness of simple gradient-based algorithms in solving neural network training programs that are non-convex and thus seemingly difficult. Generalization treats the phenomenon of neural networks not overfitting even when having much more trainable parameters (weights) than examples to train on.\nIn these lecture notes we theoretically analyze linear neural networks a fundamental model in the study of optimization and generalization in deep learning. A linear neural network is a feed-forward fully-connected neural network with linear (no) activation. Namely, for depth $n \\in \\mathbb{N}_{>2}$, input dimension $d_0 \\in \\mathbb{N}$, output dimension $d_n \\in \\mathbb{N}$, and hidden dimensions $d_1,d_2,...,d_{n-1} \\in \\mathbb{N}$, it refers to the parametric family of hypotheses $\\{x \\rightarrow W_nW_{n-1}...W_1x : W_j \\in \\mathbb{R}^{d_j\\times d_{j-1}}, j = 1,2,..., n\\}$, where $W_j$ is regarded as the weight matrix of layer $j$. Linear neural networks are trivial from the perspective of expressiveness (they realize only linear input-output mappings), but not so in terms of optimization and generalization \u2013 they lead to highly non-convex training objectives with multiple minima and saddle points, and when applied to underdetermined learning problems (e.g. matrix sensing; see Subsection 4.1 below) it is unclear a priori what kind of solutions gradient-based algorithms will converge to. By virtue of these properties, linear neural networks often serve as a theoretical surrogate for practical deep learning, allowing for controlled analyses of optimization and generalization (see, e.g., Baldi and Hornik (1989); Fukumizu (1998); Saxe et al. (2014); Kawaguchi (2016); Hardt and Ma (2016); Ge et al. (2016); Gunasekar et al. (2017); Li et al. (2018); Du et al. (2018); Nar and Sastry (2018); Bartlett et al. (2018); Laurent and Brecht (2018); Arora et al. (2018; 2019a;b); Du and Hu (2019); Lampinen and Ganguli (2019); Ji and Telgarsky (2019); Gidel et al. (2019); Wu et al. (2019); Eftekhari (2020); Mulayoff and Michaeli (2020); Razin and Cohen (2020); Advani et al. (2020); Chou et al. (2020); Li et al. (2021); Yun et al. (2021); Min et al. (2021); Tarmoun et al. (2021); Azulay et al. (2021); Nguegnang et al. (2021); Bah et al. (2021)). The theory presented in these notes is dynamical in nature, i.e. relies on careful characterizations of the trajectories assumed in training. It will demonstrate the potential of dynamical techniques to succeed where other theoretical approaches fail.\nThroughout the text, we consider the case where a linear neural network can express any linear (input-output) mapping, meaning that its hidden dimensions are large enough to not constrain rank, i.e. $d_j \\geq \\min\\{d_0, d_n\\}$ for $j = 1, 2, ..., n-1$. Many of the presented results will not rely on this assumption, but for simplicity it is maintained throughout."}, {"title": "Dynamical Analysis", "content": "Suppose we are given an analytic training loss $l : \\mathbb{R}^{d_n\\times d_0} \\rightarrow \\mathbb{R}$ defined over linear mappings from $\\mathbb{R}^{d_0}$ to $\\mathbb{R}^{d_n}$. For example, $l(\\cdot)$ could correspond to a linear regression task with $d_0$ input variables and $d_n$ output variables, or a multinomial logistic regression task with instances in $\\mathbb{R}^{d_0}$ and $d_n$ possible labels. In line with these examples we typically think of $l(\\cdot)$ as being convex, though unless stated otherwise this will not be assumed. Applying a linear neural network to $l(\\cdot)$ boils down to optimizing the overparameterized objective:\n$\\phi : \\mathbb{R}^{d_1\\times d_0} \\times \\mathbb{R}^{d_2\\times d_1} \\times...\\times \\mathbb{R}^{d_n\\times d_{n-1}} \\rightarrow \\mathbb{R}$,\n$\\phi(W_1, W_2,...,W_n) = l(W_nW_{n-1}...W_1)$.\nWhile the loss $l(\\cdot)$ may be convex, the following proposition shows that, aside from degenerate cases (namely, aside from cases where $l(\\cdot)$ is globally minimized by the zero mapping), the overparameterized objective $\\phi(\\cdot)$ is non-convex.\nProposition 1. If $l(\\cdot)$ does not attain its global minimum at the origin then $\\phi(\\cdot)$ is non-convex.\nProof. If $l(\\cdot)$ does not attain its global minimum at the origin then the same applies to $\\phi(\\cdot)$. This means that if $\\phi(\\cdot)$ is convex, its gradient at the origin must be non-zero. However, the latter gradient is clearly zero.\nWe are interested in the dynamics of gradient-based optimization when applied to the overparameterized objective $\\phi(\\cdot)$. Our analysis will focus on gradient flow a continuous version of gradient descent (attained by taking the step size to be infinitesimal). Under gradient flow, each weight matrix $W_j$ traverses through a continuous curve, which (with slight overloading of notation) we denote by $W_j(\\cdot)$. Accordingly, $W_j(0)$ represents the value of $W_j$ at initialization, and $W_j(t)$ its value at time $t \\in \\mathbb{R}_{>0}$ of optimization. With a chosen initialization $W_1(0), W_2(0), ..., W_n(0)$, the curves $W_1(\\cdot), W_2(\\cdot), ..., W_n(\\cdot)$ are defined through the following set of differential equations:\n$\\dot{W_j}(t) := \\frac{dW_j(t)}{dt} = - \\frac{\\partial}{\\partial W_j} \\phi(W_1(t), W_2(t),..., W_n(t)), t > 0, j = 1, 2, ..., n$.\nWe note that it is possible to translate to gradient descent (with positive step size) many of the gradient flow results which will be derived, either through analogous discrete analyses (see, e.g., Arora et al. (2019a)), or by bounding the distance between gradient descent and gradient flow (see Elkabetz and Cohen (2021))."}, {"title": "Convergence Guarantee", "content": "In this subsection we employ the dynamical analysis of Section 2 for establishing convergence to global minimum. The derivation will rely on the concept defined below.\nDefinition 3. $W \\in \\mathbb{R}^{d_n\\times d_0}$ is said to have deficiency margin $\\delta > 0$ if $l(W) < l(W')$ for every $W' \\in \\mathbb{R}^{d_n \\times d_0}$ whose minimal singular value is at most $\\delta$.\nThe following example illustrates the concept of deficiency margin.\nExample 1. Suppose $l(\\cdot)$ is the square loss for a linear regression task with $d_0$ input variables and $d_n$ output variables, i.e. $l(W) = \\frac{1}{2m} ||WX - Y ||^2$, where $m \\in \\mathbb{N}$ is the number of training examples, $X \\in \\mathbb{R}^{d_0,m}$ holds training instances as columns, and $Y \\in \\mathbb{R}^{d_n,m}$ holds training labels as columns (in corresponding order). Let $A_{xx} := \\frac{1}{m}XX^T \\in \\mathbb{R}^{d_0\\times d_0}, A_{yy} := \\frac{1}{m}YY^T \\in \\mathbb{R}^{d_n\\times d_n}$ and $A_{yx} := \\frac{1}{m}YX^T \\in \\mathbb{R}^{d_n\\times d_0}$ be the empirical (uncentered) instance covariance matrix, label covariance matrix and label-instance cross-covariance matrix, respectively. Using the fact that $||A||_F^2 = Tr(AA^T)$ for any matrix $A$, it is straightforward to arrive at $l(W) = \\frac{1}{2} Tr(WA_{xx}W^T) - Tr(WA_{yx}^T) + Tr(A_{yy})$, and if we assume instances are whitened, i.e. $A_{xx}$ equals identity, then in addition $l(W) = ||W - A_{yx}||^2 + const$, where $const$ stands for a term that does not depend on $W$. This implies that $W \\in \\mathbb{R}^{d_n\\times d_0}$ has deficiency margin $\\delta > 0$ if and only if $||W - A_{yx}||_F < ||W' - A_{yx}||_F$ for every $W' \\in \\mathbb{R}^{d_n\\times d_0}$ satisfying $\\sigma_{min}(W') \\leq \\delta$, where $\\sigma_{min}(\\cdot)$ refers to the minimal singular value of a matrix. By a standard matrix computation (see Exercise 3) $\\min\\{||W' - A_{yx}||_F: W' \\in \\mathbb{R}^{d_n\\times d_0}, \\sigma_{min}(W') \\leq \\delta\\} = \\max\\{0, \\sigma_{min}(A_{yx}) - \\delta\\}$. We conclude that $W$ has deficiency margin $\\delta$ if and only if $||W - A_{yx}||_F < \\sigma_{min}(A_{yx}) - \\delta$, meaning it lies within distance $\\sigma_{min}(A_{yx}) - \\delta$ from $A_{yx}$, the global minimizer of $l(\\cdot)$. Note that in the case of a single output variable (i.e. $d_n = 1$) the condition $||W - A_{yx}||_F < \\sigma_{min}(A_{yx}) - \\delta$ is equivalent to $||W - A_{yx}||_F < ||A_{yx}||_F - \\delta$, and therefore, if $W$ is randomly sampled from an isotropic distribution concentrated around the origin, the probability of it having a deficiency margin (with some $\\delta > 0$) is close to 0.5.\nAny guarantee of convergence to global minimum (for gradient-based optimization of $\\phi(\\cdot)$) must rely on assumptions that rule out the following scenarios: (i) the loss $l(\\cdot)$ is pathologically complex (e.g. it entails a myriad of sub-optimal local minimizers and a single global minimizer with a tiny basin of attraction); and (ii) optimization is initialized at a sub-optimal stationary point (note that, disregarding the degenerate case where $l(\\cdot)$ is globally minimized by the zero mapping, the origin $(W_1, W_2, ..., W_n) = (0,0,..., 0)$ is a sub-optimal stationary point). We accordingly assume the following: (i) $l(\\cdot)$ is strongly convex (this does not mean that the optimized objective $\\phi(\\cdot)$ is convex see Propositions 1 and 3); and (ii) at initialization, the end-to-end matrix $W_{n:1}$ (Definition 2) has a deficiency margin (i.e. it meets the condition in Definition 3 with some $\\delta > 0$). Notice that in the context of Example 1 (square loss for linear regression with whitened instances), $l(\\cdot)$ is indeed strongly convex, and as discussed there, in the case of a single output variable, randomly sampling from an isotropic distribution concentrated around the origin leads to a deficiency margin with probability close to 0.5.\nWe are now in a position to present the main result of this subsection a convergence guarantee born from the dynamical analysis of Section 2. For conciseness, we overload notation by using $\\phi(t)$, with $t \\geq 0$, to refer to the value of the optimized objective $\\phi(\\cdot)$ at time $t$ of optimization, i.e. $\\phi(t) := \\phi(W_1(t), W_2(t), ..., W_n(t))$. Additionally, we denote by $\\phi^*$ the global minimum of $\\phi(\\cdot)$, i.e. $\\phi^* := \\inf_{W_j\\in \\mathbb{R}^{d_j\\times d_{j-1}},j=1,2,...,n} \\phi(W_1, W_2,..., W_n)$.\nTheorem 2. Assume $l(\\cdot)$ is $\\alpha$-strongly convex for some $\\alpha > 0$, and the end-to-end matrix at initialization $W_{n:1}(0)$ has deficiency margin $\\delta$ for some $\\delta > 0$. Then, for any $\\epsilon > 0$, it holds that $\\phi(t) - \\phi^* < \\epsilon$ for every time $t$ satisfying $t > \\frac{\\ln ((\\phi(0) - \\phi^*)/\\epsilon)}{(2\\alpha\\delta^{\\frac{2(n-1)}{n}})}$.\nProof. From the chain rule we have:\n$\\frac{d}{dt} l(W_{n:1}(t)) = <\\nabla l(W_{n:1}(t)), vec(\\dot{W}_{n:1}(t))>, t > 0$.\nPlugging in the vectorized end-to-end dynamics (Equation (6)) gives:\n$\\frac{d}{dt} l(W_{n:1}(t)) = -vec(\\nabla l(W_{n:1}(t)))^TP(W_{n:1}(t))vec(\\nabla l(W_{n:1}(t))), t > 0$,\nwhere $P(W_{n:1}(t))$ is a positive semidefinite matrix upholding $\\lambda_{min}(P(W_{n:1}(t))) > (\\sigma_{min}(W_{n:1}(t)))^{\\frac{2(n-1)}{n}}$, with $\\lambda_{min}(\\cdot)$ and $\\sigma_{min}(\\cdot)$ referring to the minimal eigenvalue and the minimal singular value of a matrix, respectively. Therefore:\n$\\frac{d}{dt} l(W_{n:1}(t)) < - (\\sigma_{min}(W_{n:1}(t)))^{\\frac{2(n-1)}{n}} \\cdot ||\\nabla l(W_{n:1}(t))||^2, t > 0$.\nNotice that $\\frac{d}{dt} l(W_{n:1}(t)) \\leq 0$, and so $l(W_{n:1}(t))$ is monotonically non-increasing as a function of $t$. Our assumption on $W_{n:1}(0)$ having deficiency margin $\\delta$ therefore implies that $W_{n:1}(t)$ has deficiency margin $\\delta$ for every $t > 0$. Accordingly, $\\sigma_{min}(W_{n:1}(t)) > \\delta$ for every $t > 0$, and we get:\n$\\frac{d}{dt} l(W_{n:1}(t)) < - \\delta^{\\frac{2(n-1)}{n}} \\cdot ||\\nabla l(W_{n:1}(t))||^2, t > 0$.\nDenote $l^* := \\inf_{W\\in \\mathbb{R}^{d_n\\times d_0}} l(W)$. Since $l(\\cdot)$ is $\\alpha$-strongly convex, it holds that $||\\nabla l(W)||^2 \\geq 2\\alpha (l(W) - l^*)$ for every $W \\in \\mathbb{R}^{d_n\\times d_0}$. This leads to:\n$\\frac{d}{dt} l(W_{n:1}(t)) \\leq - 2 \\alpha \\delta^{\\frac{2(n-1)}{n}}(l(W_{n:1}(t)) - l^*), t > 0$,\nwhich by Gr\u00f6nwall's inequality implies that:\n$l(W_{n:1}(t)) - l^* \\leq (l(W_{n:1}(0)) - l^*) exp ( - 2\\alpha \\delta^{\\frac{2(n-1)}{n}}t), t > 0$.\nLet $\\epsilon > 0$. Since $\\phi^* = l^*$ and $\\phi(t) = l(W_{n:1}(t))$ for all $t$, it holds that $\\phi(t) - \\phi^* < \\epsilon$ for every $t$ satisfying $t \\geq \\frac{\\ln ((\\phi(0) - \\phi^*)/\\epsilon)}{(2\\alpha\\delta^{\\frac{2(n-1)}{n}})}$, which is what we set out to prove."}, {"title": "Implicit Acceleration by Overparameterization", "content": "Subsection 3.1 employed the dynamical analysis of Section 2 to circumvent the difficulty in establishing convergence to global minimum using landscape arguments (see Proposition 3 and preceding text). In this subsection we discuss a result that further attests to the potential of dynamical analyses.\nClassical machine learning is rooted in a preference for optimization to be convex. In our context, this means that if the loss $l(\\cdot)$ is convex (setting of interest), optimizing it directly is preferred to doing so with a linear neural network, i.e. to minimizing the overparameterized objective $\\phi(\\cdot)$ defined in Equation (1) (this is because, as Proposition 1 in Section 2 shows, aside from degenerate cases, $\\phi(\\cdot)$ is non-convex). A surprising implication of the dynamical analysis from Section 2 is that such preference may be misleading \u2013 there exist cases where $l(\\cdot)$ is convex, and yet its optimization via gradient descent is much slower than that of $\\phi(\\cdot)$. We present this result informally below, referring to Arora et al. (2018) for further details."}, {"title": "Generalization", "content": "Neural networks are able to generalize even when having much more trainable parameters (weights) than examples to train on. The fact that this generalization can take place in the absence of any explicit regularization (see Zhang et al. (2017) for extensive empirical evidence) has led to a common view by which gradient-based optimization induces an implicit regularization a tendency to fit training examples with functions of low \u201ccomplexity.\u201d It is an ongoing effort to mathematically support this intuition. The current section does so for linear neural networks.\nOptimization of a linear neural network, i.e. minimization of the overparameterized objective $\\phi(\\cdot)$ (defined in Equation (1)), ultimately produces an end-to-end matrix $W_{n:1}$ (Definition 2) designed to be a solution for the loss $l(\\cdot)$. The question we ask in this section is what kind of solution $W_{n:1}$ will be produced when $\\phi(\\cdot)$ is minimized via gradient descent emanating from near-zero initialization. This question is most meaningful when $l(\\cdot)$ is underdetermined, i.e. admits multiple global minimizers. A prominent set of tasks giving rise to underdetermined loss functions is matrix sensing, which includes linear regression as a special case. We will focus on settings where $l(\\cdot)$ corresponds to a matrix sensing task. Our treatment will rely on the dynamical analysis of Section 2."}, {"title": "Matrix Sensing", "content": "A matrix sensing task is defined by measurement matrices $A_1, A_2, ..., A_m \\in \\mathbb{R}^{d_n\\times d_0}$ and corresponding measurements $b_1, b_2, ..., b_m \\in \\mathbb{R}$. Given these, the goal is to find a matrix $W \\in \\mathbb{R}^{d_n\\times d_0}$ satisfying $<W, A_i> := Tr(WA_i^T) = b_i$ for $i = 1, 2, ..., m$. A notable special case of matrix sensing, known as matrix completion, is where each measurement matrix holds one in a single entry and zeros elsewhere. Linear regression is also a special case of matrix sensing, as for any $x \\in \\mathbb{R}^{d_0}$ and $y \\in \\mathbb{R}^{d_n}$, the requirement $Wx = y$ can be realized via $d_n$ measurement matrices $e_1x^T,e_2x^T,...,e_{d_n}x^T$ with corresponding measurements $y_1, y_2,..., y_{d_n}$, where, for $i = 1, 2, ..., d_n$: $e_i \\in \\mathbb{R}^{d_n}$ stands for a vector holding one in its $i$'th entry and zeros elsewhere; and $y_i \\in \\mathbb{R}$ represents the $i$'th entry of $y$.\nFor tackling matrix sensing, it is common practice to consider the square loss over measurements. In our context this amounts to considering a training loss $l(\\cdot)$ of the form:\n$l(W) = \\frac{1}{2m} \\sum_{i=1}^{m} (<W, A_i> - b_i)^2$.\nIf the number of measurements is smaller than the number of entries in the sought-after solution, i.e. $m < d_0d_n$, then (assuming the measurement matrices are linearly independent, which is generically the case) the loss $l(\\cdot)$ is underdetermined \u2013 it admits infinitely many solutions attaining the global minimum $l^* := \\inf_{W\\in \\mathbb{R}^{d_n\\times d_0}} l(W) = 0$. There is often interest in finding, among all these global minimizers, one whose rank is lowest, i.e. $W^* \\in argmin_{W\\in \\mathbb{R}^{d_n\\times d_0} :l(W)=l^*} Rank(W)$. This is NP-hard in general. However, it is known (see Recht et al. (2010)) that if $Rank(W^*)$ is sufficiently low compared to the number of measurements $m$, and if the measurement matrices satisfy a certain technical condition (\u201crestricted isometry property\u201d), then it is possible to find $W^*$ by solving a convex (constrained) optimization program, namely:\n$W^* = argmin_{W\\in \\mathbb{R}^{d_n\\times d_0}:l(W)=l^*} ||W||_*$,\nwhere $|| \\cdot ||_*$ stands for nuclear norm. Roughly speaking, this implies that in matrix sensing, given sufficiently"}, {"title": "Implicit Regularization", "content": "Suppose we tackle matrix sensing with a linear neural network, meaning we optimize the overparameterized objective $\\phi(\\cdot)$ (Equation (1)) induced by a loss $l(\\cdot)$ as defined in Equation (9). What kind of solution (for $l(\\cdot)$) will the end-to-end matrix $W_{n:1}$ (Definition 2) reach? If any of the hidden dimensions of the network (i.e. any of $d_1, d_2,..., d_{n-1}$) were small then $W_{n:1}$ would be constrained to have low rank, but as stated in Section 1, we consider the case where hidden dimensions are large enough to not restrict the search space (i.e. we assume $d_j \\geq \\min\\{d_0, d_n\\}$ for $j = 1, 2, ..., n - 1$). Surprisingly, experiments show (see, e.g., Arora et al. (2019b)) that even in this case, gradient descent with small step size emanating from near-zero initialization tends to produce an end-to-end matrix of low rank. This tendency is driven by implicit regularization, as there is nothing explicit in the optimized objective $\\phi(\\cdot)$ promoting low rank (indeed, it typically admits global minimizers whose end-to-end matrices have high rank).\nOur goal in the current section is to mathematically characterize the implicit regularization described above, i.e. the tendency of linear neural networks to produce a low rank solution (end-to-end matrix) $W_{n:1}$ when applied to a loss $l(\\cdot)$ of the form in Equation (9). An elegant supposition, formally stated below, is that the implicit regularization solves the convex optimization program in Equation (10), thus implements a method that under certain conditions provably finds a global minimizer of lowest rank.\nSupposition 1. If $W_{n:1}$ converges to a global minimizer (of $l(\\cdot)$), this global minimizer has lowest nuclear norm (among all global minimizers)."}, {"title": "Greedy Low Rank Learning", "content": "Our analysis of implicit regularization in linear neural networks relies on the concept of analytic singular value decomposition, defined herein for completeness.\nDefinition 4. For a curve $W : [0, t_e) \\rightarrow \\mathbb{R}^{d\\times d'}$, with $d, d' \\in \\mathbb{N}$ and $t_e \\in \\mathbb{R}_{>0}\\cup {\\infty}$, an analytic singular value decomposition is a triplet $(U: [0,t_e) \\rightarrow \\mathbb{R}^{d\\times d}, S : [0,t_e) \\rightarrow \\mathbb{R}^{d\\times d}, V : [0,t_e) \\rightarrow \\mathbb{R}^{d\\times d'})$, where $\\tilde{d} := \\min\\{d, d'\\}$, the curves $U(\\cdot), S(\\cdot)$ and $V(\\cdot)$ are analytic, and for every $t \\in [0, t_e)$: the columns of $U (t)$ are orthonormal; $S(t)$ is diagonal; the columns of $V (t)$ are orthonormal; and $W(t) = U(t)S(t)V(t)$.\nProposition 4 below states that the curve traversed by the end-to-end matrix (Definition 2) during optimization, i.e. $W_{n:1}(\\cdot)$, admits an analytic singular value decomposition.\nProposition 4. There exists an analytic singular value decomposition for $W_{n:1}(\\cdot)$.\nProof. Any analytic curve in matrix space admits an analytic singular value decomposition (see Theorem 1 in Bunse-Gerstner et al. (1991)), so it suffices to show that $W_{n:1}(\\cdot)$ is analytic. Analytic functions are closed under summation, multiplication and composition, so the analyticity of the loss $l(\\cdot)$ implies that the overparameterized objective $\\phi(\\cdot)$ (Equation (1)) is analytic as well. A basic result in the theory of analytic differential equations is that gradient flow over an analytic objective yields an analytic curve (see Theorem 1.1 in Ilyashenko and Yakovenko (2008)). We conclude that $W_1(\\cdot), W_2 (\\cdot), ..., W_n (\\cdot)$ (curves traversed by the weight matrices during optimization; see Equation (2)) are analytic. Since $W_{n:1}(\\cdot) := W_n(\\cdot)W_{n-1}(\\cdot)... W_1(\\cdot)$, and, as stated, analytic functions are closed under summation and multiplication, $W_{n:1}(\\cdot)$ is also analytic. This concludes the proof.\nLet $(U(\\cdot), S(\\cdot), V(\\cdot))$ be an analytic singular value decomposition for $W_{n:1}(\\cdot)$. For $r = 1, 2, ..., \\min\\{d_0, d_n \\}$, denote by $u_r(\\cdot)$ the $r$'th column of $U(\\cdot)$, by $\\sigma_r(\\cdot)$ the $r$'th diagonal entry of $S(\\cdot)$, and by $v_r(\\cdot)$ the $r$'th column of $V(\\cdot)$. Up to potential minus signs, $(\\sigma_r(\\cdot))^{min\\{d_0,d_n\\}}$ are the singular values of $W_{n:1}(\\cdot)$, with corresponding left and right singular vectors $(u_r(\\cdot))^{min\\{d_0,d_n\\}}$ and $(v_r(\\cdot))^{min\\{d_0,d_n\\}}$, respectively. The following theorem employs the end-to-end dynamics from Section 2 (Theorem 1) for characterizing the dynamics of $\\sigma(\\cdot)$.\nTheorem 3. It holds that:\n$\\dot{\\sigma_r(t)} := \\frac{d}{dt}\\sigma_r(t) = (\\sigma^2_r(t))^{1-\\frac{1}{n}}(<-\\nabla l(W_{n:1}(t)), u_r(t)v^T_r(t)>)^n, t > 0, r = 1, 2, ..., \\min\\{d_0, d_n \\}$,\nwhere as before, $<\\cdot, \\cdot>$ stands for the standard inner product between matrices, i.e. $<A, B> := Tr(AB^T)$ for any matrices $A$ and $B$ of the same dimensions.\nProof. Differentiating the analytic singular value decomposition of $W_{n:1}(\\cdot)$ with respect to time yields:\n$\\dot{W}_{n:1}(t) = \\dot{U}(t)S(t)V^T(t) + U(t)\\dot{S}(t)V^T(t) + U(t)S(t)\\dot{V}^T(t), t > 0$,\nwhere $\\dot{U}(t) := \\frac{d}{dt}U(t), \\dot{S}(t) := \\frac{d}{dt}S(t)$ and $\\dot{V}(t) := \\frac{d}{dt}V(t)$. Multiplying from the left by $U^T(t)$ and from the right by $V (t)$, while using the fact that the columns of $U (t)$ are orthonormal and the columns of $V (t)$ are orthonormal, we obtain:\n$U^T(t)\\dot{W}_{n:1}(t)V(t) = U^T(t)\\dot{U}(t)S(t) + \\dot{S}(t) + S(t)\\dot{V}(t)V^T(t), t > 0$.\nFix some $r \\in \\{1, 2, ..., \\min\\{d_0, d_n\\}\\}$. The $r$'th diagonal entry of the latter matrix equation is:\n$u^T_r(t)\\dot{W}_{n:1}(t)v_r(t) = u^T_r(t)\\dot{u}_r(t) \\cdot \\sigma_r(t) + \\dot{\\sigma_r}(t) + \\sigma_r(t) \\dot{v}_r(t)v^T_r(t), t > 0$,"}, {"title": "Implicit Compression by Overparameterization", "content": "Recall from Subsection 3.2 that overparameterization, i.e. replacement of a linear transformation with a linear neural network, can accelerate optimization. The results of the current section imply that it also encourages convergence to solutions of low rank, meaning ones that can be compressed. This phenomenon an implicit compression by overparameterization facilitates a practical technique of adding linear layers for fitting training data with a reduced-size model, thereby improving both generalization and computational efficiency at inference time. The technique has been applied to non-linear neural networks in various real-world settings (see, e.g., Guo et al. (2020); Jing et al. (2020); Huh et al. (2021)). Similarly to acceleration of optimization by overparameterization (see Subsection 3.2), it constitutes a practical application of linear neural networks born from a dynamical analysis."}, {"title": "Extension: Arithmetic Neural Networks", "content": "Linear neural networks are a fundamental model in the theory of deep learning, and as discussed in Subsections 3.2 and 4.4, they also admit practical benefits. Nevertheless, they are limited to linear (input-output) mappings, thus fail to capture the crucial role of non-linearity in deep learning. In this section we briefly discuss a non-linear extension of linear neural networks that is closer to practical deep learning.\nLinear neural networks can be viewed as matrix factorizations, in accordance with the fact that the mappings they realize are naturally represented as matrices factorized by network weights. Lifting matrices (two-dimensional arrays) to higher dimensions, i.e. considering tensor factorizations (or more precisely, mappings represented as tensors factorized by network weights), gives rise to neural networks with multiplicative non-linearity, known as arithmetic neural networks. Arithmetic neural networks have been shown to exhibit promising empirical performance (see, e.g., Cohen et al. (2016a); Sharir et al. (2016); Chrysos et al. (2021)), and their expressiveness was the subject of numerous theoretical studies (see, e.g., Cohen et al. (2016b); Cohen and Shashua (2016; 2017); Cohen et al. (2017; 2018); Sharir and Shashua (2018); Levine et al. (2018a;b); Balda et al. (2018); Khrulkov et al. (2018; 2019); Levine et al. (2019); Alexander et al. (2023); Razin et al. (2023)). It is possible to extend some of the (linear neural network) results in these lecture notes to arithmetic neural networks. In particular, through a dynamical analysis, it can be shown"}]}