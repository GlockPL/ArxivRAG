{"title": "Self-Refining Diffusion Samplers: Enabling Parallelization via Parareal Iterations", "authors": ["Nikil Roashan Selvam", "Amil Merchant", "Stefano Ermon"], "abstract": "In diffusion models, samples are generated through an iterative refinement process, requiring hundreds of sequential model evaluations. Several recent methods have introduced approximations (fewer discretization steps or distillation) to trade off speed at the cost of sample quality. In contrast, we introduce Self-Refining Diffusion Samplers (SRDS) that retain sample quality and can improve latency at the cost of additional parallel compute. We take inspiration from the Parareal algorithm, a popular numerical method for parallel-in-time integration of differential equations. In SRDS, a quick but rough estimate of a sample is first created and then iteratively refined in parallel through Parareal iterations. SRDS is not only guaranteed to accurately solve the ODE and converge to the serial solution but also benefits from parallelization across the diffusion trajectory, enabling batched inference and pipelining. As we demonstrate for pre-trained diffusion models, the early convergence of this refinement procedure drastically reduces the number of steps required to produce a sample, speeding up generation for instance by up to 1.7x on a 25-step StableDiffusion-v2 benchmark and up to 4.3x on longer trajectories.", "sections": [{"title": "Introduction", "content": "Deep generative models based on diffusion processes have showcased the capability to produce high-fidelity samples in a wide-range of applications [28, 11, 39, 27]. From their origins in image and audio generation [31, 33, 10], diffusion models have enabled robotic applications as well as scientific discovery via drug design [1]. Despite this promise, sampling from diffusion models can still be prohibitively slow. Early Denoising Diffusion Probabilistic Models [10] required a thousand sequential model evaluations (steps), and state-of-the-art models such as StableDiffusion [27] can still require up to 100s of iterations for high-quality generations. This large number of sampling steps leads to high-latencies associated with diffusion models, limiting applications such as real-time image or music editing and trajectory planning in robotics [13, 12].\nAs sampling involves solving an ordinary differential equation (ODE), a prominent body of research including works such as Denoising Diffusion Implicit Models (DDIM, [32]), Diffusion Exponential Integrator Sampler (DEIS, [42]), and DPM-Solver [19] - has tried to reduce the number of model evaluations required by introducing various approximations. For example, progressive distillation [29] requires re-training models to approximate the solution to the ODE at larger timesteps. However, such approaches trade-off speed at the cost of sample quality.\nIn this work, we instead take an orthogonal approach: we focus on additional parallel compute and show how this can be used to reduce latencies while still providing accurate solutions to the original ODE, thereby preserving sample quality. Recently Shih et al. [30] leveraged a parallel-in-time integration method to introduce the first highly-parallelizable algorithm for diffusion model sampling. The presented ParaDiGMs algorithm builds on Picard iterations [26] to perform denoising steps"}, {"title": "Background", "content": "Diffusion models are a general class of generative models that rely on a noising procedure that converts the data distribution into noise via a series of latent variables updates. For the purposes of this work, we will consider the continuous-time generalization presented by Song [35] and Denoising Diffusion Implicit Models [10] that formulate sampling as solving the initial value problem characterized by the probability flow ordinary differential equation (ODE):\n$\\frac{dx}{dt} = \\frac{1}{2}f(x,t) - g(t)^2 s_\\theta(x,t) dt; x(t = 0) \\sim x_0 \\sim N(0, I)$\nwhere $s_\\theta(x, t)$ is a time-conditional prediction of the score function $\u2207 \\log p_t (x)$ from the diffusion model. To be consistent with prior work on parallelized diffusion sampling Shih et al. [30], we use a reversed time index (from traditional notation) where $x_0$ refers to pure Gaussian noise, and $x_T$ refers to the denoised image after $T$ denoising steps."}, {"title": "Solving the Differential Equation", "content": "Given the dynamics governing the differential equation, our goal is to provide accurate solutions to:\n$x_T = x_0 + \\int_{t=0}^T h_\\theta(x,t)dt$\nin order to produce a sample from the diffusion model. Common approaches discretize the time interval $[0, T]$ into $N$ pieces $(t_0=0, t_1, t_2,\u2026,t_N=T)$ and solve a sequence of initial value problems to yield an approximation $(x_0, X_1, ..., X_N=X_T)$ to the trajectory.\nFormally, a solver is a function $F(x_{start}, t_{start}, t_{end})$ that propagates $x$ from $t = t_{start}$ with initial value $x_{start}$ to $t = t_{end}$. Solving the differential equation corresponds to approximating the solution $x_T$ to the given initial value problem by a sequence of $N$ solves:\n$x_{i+1} = F(x_i, t_i, t_{i+1}) \\forall i \\in [0, N - 1]; given initial value x_0$\nThe choice of solver $F$ dictates the sampling speed and accuracy of the solution. In practice, solvers which are accurate are often slow (due to high number of evaluations of $h_\\theta$), whereas solvers that are fast tend to have reduced accuracy. Initial works on diffusion models used the classical Euler method as choice of $F$, and it can be expressed as:\n$x_{i+1} = F_{euler}(x_i, t_i, t_{i+1}) = x_i + h_\\theta(x_i, t_i) * (t_{i+1} - t_i)$\nHowever, DDIM [32] quickly became a popular choice of $F$ for its improved efficiency. Other recent works have tried to rely on approximations or leverage various ideas from the numerical methods literature to design solvers $F$ that require fewer denoising steps. For instance, Diffusion Exponential Integrator Sampler (DEIS, [42]), and DPM-Solver [19] exploit the special structure of the probability flow ODE to design special solvers where the linear part of the ODE is solved analytically and the non-linear part is solved by incorporating ideas from exponential integrators in the numerical methods literature. Karras et al. [13] leverage the Heun's second order method and demonstrate a favorable tradeoff between number of model evaluations and quality of generated samples for a small number of denoising steps. In this work, SRDS presents an orthogonal improvement to these methods via parallelization, and by default we will assume all our solvers to be DDIM."}, {"title": "Self-Refining Diffusion Samplers", "content": "Attempts to reduce the number of steps in diffusion samplers can provide speedups in sample generation [29, 23], but unfortunately often lead to lower-sample quality. While low-frequency components (in the Fourier sense) of the images may be well-established, the generations miss the high-frequency details that leads to good generations [40]. To fix sample quality while maintaining the latency benefits of reducing the number of steps, we turn to numerical methods introduced in the parallel-in-time integration literature where dynamics with different components having different rates"}, {"title": "Parareal Algorithm", "content": "Parareal makes use of two solvers: solver $F$ (called the 'fine solver') provides accurate solutions but is slow to evaluate, and $G$ (called the 'coarse solver') provides rough solutions but is much quicker.\nParareal targets general purpose initial value problems of forms similar to Equation 1. Consider a partition $(t_0, t_1, ..., t_n = T)$ of the time axis $[t_0, T]$ into $N$ intervals of equal width. Using the same solver notation as above, the goal is to approximate the solution $x_N$ to the initial value problem that would be produced using a sequence of fine solves:\n$x_{i+1} = F(x_i, t_i, t_{i+1}), \\forall i \\in [0, N - 1]$\nThe key insight of parareal is that we can first use the coarse solver $G$ to quickly produce a rough trajectory, and this rough solution can be iteratively refined using parallel calls to the fine solver $F$.\nFormally, the parareal algorithm begins with a rough estimate of the trajectory, initialzied via a series of coarse solves from $G$.\n$x_0 = x_0$\n$x_{i+1} = G (x_i, t_i, t_{i+1}) \\forall i \\in [0, N - 1]$\nwhere the notation $\\overline{x}$ denotes the initial estimate of the trajectory from the coarse solver (orange curve in Figure 2).\nParareal then proceeds in iterations until convergence, where each iteration corresponds to a refinement of the trajectory. At each iteration, we solve the differential equation in each of the $N$ time intervals at a higher resolution using the fine solver $F$, where the initial value for each interval is given by the estimate of the trajectory from the previous iteration. Crucially, these fine solves (blue in Figure 2) can be performed in parallel. Lastly, at the end of each iteration, we perform another coarse sequential solve through the trajectory (magenta in Figure 2) and incorporate the results of the fine solves into the running solution for the trajectory using a predictor-corrector method, where the coarse solver 'predictions' are 'corrected' via the updates from the parallel fine solves. Formally,\n$x_i^{p+1} = F(x_i^p, t_i, t_{i+1}) + (G(x_{i+1}^p, t_i, t_{i+1}) -G(x_{i+1}^{p-1}, t_i, t_{i+1})), i = 0,..., N -1$\nwhere the notation $x_i^p$ denotes the running estimate of the trajectory at Parareal iteration number $p$."}, {"title": "Self-Refining Diffusion Samplers", "content": "Now, we turn our attention back to drawing a sample from our diffusion model, which as discussed corresponds to estimating a solution to the initial value problem as defined in Equation 1."}, {"title": "Convergence Guarantee", "content": "The ideal result for diffusion sampling is to get the solution arising from N sequential denoising score steps. SRDS however only starts with a rough solve of the diffusion trajectory taking VN sequential denoising steps. Nevertheless, we can show that each iteration of SRDS (line 6 of Algorithm 1) refines the generated sample and leads us closer to the ideal solution.\nProposition 1. The sample output by SRDS converges to the output of the N-step sequential solver in at most \u2713 N refinement iterations.\nA key property of our algorithm is that after i iterations (refinements to the diffusion trajectory) of SRDS, the first i steps of the running trajectory exactly matches the trajectory generated by the sequential solver for the corresponding intervals. Consequently, the algorithm is guaranteed to converge in at most \u221aN iterations. We defer the formal proof to Appendix A. It is also worth"}, {"title": "Batched Inference and Pipelining", "content": "SRDS benefits from two key features to reduce latencies: batched inference and pipelining.\nFirst, the fine solves that are used in order to refine the trajectories implementation-equivalent DDIM-steps, which means that they can be performed in a batched manner even for a single sample generation. This parallelization allows for a single sample generation to incur the benefits of batched inference, introducing higher device utilization or device parallelism.\nSecondly, we observe that the dependency graph for SRDS enables pipelined parallelism. As outlined in Figure 3, we find that $F (x_i^p, t_i, t_{i+1})$ and $G (x_i^p, t_i, t_{i+1})$ both only depend on $x_i^p$. The tasks for computing $F (x_i^p, t_i, t_{i+1})$ and $G (x_i^p, t_i, t_{i+1})$ can be spawned as soon as $x_i^p$ is computed, without waiting for the entire predictor-corrector mechanism to finish updating the SRDS solution for iteration i. This leads to an efficiently pipelined version of the algorithm, further speeding up the sampling process by a factor of two. See Figure 4 for an illustration of this pipelined algorithm with $N = 16$.\nPipelining furthers the benefits of batched inference as the coarse solver is simply a DDIM-step with a larger time-step, so it can be batched with fine solves when applicable."}, {"title": "Sampling Latency", "content": "Proposition 2. [Worst-Case Behavior] Ignoring GPU overhead, the worst case wall-clock time of generating a sample through SRDS is no worse than that of generating through sequential sampling.\nReferring to the pipelined implementation of SRDS, it is easy to see that the fine solve $F (x_i^p, t_i, t_{i+1})$ starts immediately after $F (x_i^{p-1}, t_{i-1},t_i)$. Subsequently, from Proposition 1, it then follows that in the worst case, the final sample of SRDS $x_{\\sqrt{N}}^\\sqrt{N}$ is computed at time $\\sqrt{N} \\cdot \\sqrt{N} = N$ as desired. A formal argument can be found in Appendix A. It is worth noting, however, that this property of SRDS comes at the cost of much higher parallel compute compared to sequential sampling."}, {"title": "Memory and Communication Overhead", "content": "Proposition 3. [Memory] SRDS requires memory corresponding to $O(\\sqrt{N})$ model evaluations.\nOnce again referring to the pipelined implementation of SRDS, it is easy to see that at any given time there is at most one model evaluation corresponding to a coarse solve, and up to $\\sqrt{N}$ parallel model evaluations corresponding to the fine solves. It is worth contrasting this with the quadratically higher $O(N)$ memory requirement of the full ParaDiGMs algorithm in [30], necessitating the use of sliding window tricks to reverse the process in a piece-wise fashion.\nIt is finally worth noting that there is minimal inter-GPU communication in SRDS. In particular, at most one sample is passed between adjacent GPUs in each SRDS iteration. Once again, it is worth contrasting this with ParaDiGMs algorithm, which \u2013 by its use of parallel prefix sum operations to sync the solutions at each Picard iteration \u2013 incurs greater GPU communication overhead. See Appendix D for more discussion."}, {"title": "Experiments on Diffusion Image Generation", "content": "To showcase the capabilities of the prescribed SRDS algorithm, we apply the diffusion sampler to pretrained diffusion models and present the difference in sample time and quality to ensure that applied convergence criteria do not reduce generation metrics. We start with pixel-based diffusion before expanding experiments applied to latent methods such as StableDiffusion-v2. Across the range of tasks, we show consistent speedups while maintaining quality of sample generation.\nIn this section, we perform an extensive comparison with ParaDiGMs [30] as our baseline. Nonetheless, we provide a high level empirical comparison to our concurrent work ParaTAA[37] in Appendix E, where we demonstrate the superiority of SRDS."}, {"title": "Pixel Diffusion - Image Generation", "content": "We start with pixel-space diffusion models. In particular, we test our SRDS algorithm and demonstrate capabilities in performing diffusion directly on the pixel space of 128x128 LSUN Church and Bedroom [41], 64x64 Imagenet [5], and 32x32 CIFAR [16] using pretrained diffusion models [29], which all use N = 1024 length diffusion trajectories.\nWe measure the convergence via $l_1$ norm in pixel space with values [0, 255]. We conservatively set $\\tau = 0.1$, meaning that convergence occurs when on average each pixel in the generation differs by only 0.1 after a refinement step (see Appendix F for an ablation on choice of $\\tau$). Through our experiments, we quantitatively showcase how the SRDS algorithm can provide signficant speedups in generation without degrading model quality (as measured by FID score [9] on 5000 samples). As seen in Table 1, SRDS remarkably converges in 4-6 iterations across all datasets; this corresponds to roughly 150 - 200 effective serial steps (counting all model evaluations simultaneously performed in parallel as one evaluation), which is only 15 - 20% of the serial steps required by a sequential solve"}, {"title": "Latent Diffusion - Image Generation", "content": "Finally, we turn to latent diffusion models, in particular StableDiffusion-v2 [27], where evaluations of the CLIP score over 1000 random samples show how SRDS maintains sample quality while improving the number of parallel iterations required per sample, with summary metrics presented in Table 2. As the SRDS algorithm has small GPU overhead, we achieve measured wallclock time improvements with a Diffusers compatible implementation [38]. It is worth nothing that while we focus on DDIM here (as in the rest of the writing), we show speedups by readily incorporating other solvers into SRDS in Appendix C.\nFor the test bed of latent diffusion models, we explore the convergence properties of our SRDS algorithm, with the average CLIP score plotted against the number of iterations in Figure 5. For shorter sequences of length 25 (left), the corresponding SRDS sampler converges after approximately 3 iterations. However, for longer sequences of length 100 (right) the sampler has converged after a single SRDS iteration, showcasing the capabilities of our algorithm improves with longer trajectories.\nNext, we demonstrate the additional speedup that pipeline parallelism can bring to SRDS. We implement a slightly suboptimal version of pipelined SRDS for StableDiffusion and already observe"}, {"title": "Choice of Coarse Resolution", "content": "The choice of resolution for the coarse solve is not arbitrary. For practical implementations, since we use the same denoiser (say, DDIM) for both the coarse and fine solves, we choose $\\sqrt{N}$ as an optimal choice in the runtime sense (assuming constant number of iterations till convergence). At a high level, this choice stems from the fact that we want to balance out the time the it takes to run all the fine solves in parallel and the time it takes perform one set of sequential predictor-corrector steps through the trajectory.\nProposition 4. [Optimal Coarse Resolution] The speed of an SRDS iteration is maximized for $B\u2248\\sqrt{N}$.\nProof. Let $k$ denote the number of SRDS iterations until convergence, let $\\tau$ denote the cost of one denoising step or model evaluation, and let $1 < B < N$ denote the \"block-size\": that is, the second scale of discretization. For the 1-step coarse solve, each SRDS iteration incurs a runtime cost of $1\\cdot \\lceil \\frac{N}{B} \\rceil\\cdot \\tau$. For the B-step fine solves, as each of the $\\lceil \\frac{N}{B} \\rceil$ fine solves are independently executed in parallel, each SRDS iteration incurs a runtime cost of $B\\cdot 1\\cdot \\tau$. The baseline runtime for sequentially sampling from the diffusion model is $N\\cdot \\tau$. Thus, the runtime speedup (ignoring parallelization overhead) is $\\frac{N\\cdot \\tau}{\\lceil \\frac{N}{B} \\rceil \\cdot \\tau +B\\cdot \\tau} = k(\\lceil \\frac{N}{B} \\rceil +B)$. For a fixed value of $k$, it is easy to see that this quantity is concave in $B$ and is maximized by choosing $B \u2248 \\sqrt{N}$.\nIt is worth noting, however, that if we use solvers of different latencies for the coarse and fine steps, a modifed analysis is required to incorporate differences in denoising step times for the two solvers. Consequently, VN might no longer be the optimal choice of coarse resolution."}, {"title": "Incorporation of other Solvers", "content": "It is worth emphasizing again that SRDS provides an orthogonal improvement when compared to the other lines of research on accelerating diffusion model sampling. In particular, while the main experiments (and writing) were focused on DDIM, SRDS is compatible with the other solvers and they can be readily incorporated into SRDS to speed up diffusion sampling. For example, below we show that SRDS is directly compatible with other solvers such as DDPM (often requiring more steps than DDIM) and DPMSolver (often requiring fewer steps than DDIM) and can efficiently accelerate sampling in both cases. We demonstrate this on StableDiffusion in Table 5. We also highlight that the Diffuser-compatible implementation requires only minor modification to the arguments of the solver, suggesting that SRDS will also be easy to extend out-of-the-box to other methods that the community develops."}, {"title": "Memory Utilization", "content": "For a T step denoising process, ParaDiGMS needs to perform T model evaluations in parallel with subsequent computations needing information about all previous evaluations, while SRDS only requires VT parallel evaluations (which fits comfortably in GPU memory) and requires much lesser communication between GPUs. While the prohibitively large memory requirement can be combated with a sliding window method, the significantly larger communication overhead remains because at every step of Paradigms, an AllReduce over all devices must be performed in order to calculate updates to the sliding window. (For instance, even when ParaDiGMS reduces Eff. Serial Steps by 20x, the obtained speedup is only 3.4x). This is in contrast to the independent fine-solves in parareal that only need to transfer information for the coarse solve.\nBelow in Table 6, we demonstrate how the minimal memory and communication overhead of SRDS shines through as we are able to achieve better device utilization as we increase the number of available GPUs. The following experiment was performed on 40GB A100s and used a generous 1e-2 threshold for ParaDiGMS."}, {"title": "Comparison to ParaTAA", "content": "We demonstrate the superiority of SRDS to baselines ParaDiGMS [30] and ParaTAA [37]. Here, we demonstrate the high-level superiority of SRDS solely by using the results published by the authors in [30] (Table 5) and [37] (Table 1).\nIn the table 7 below, we show that SRDS offers better wall-clock speedups (over sequential) in sample generation time for StableDiffusion when compared to [30] and [37]. We clarify that the reported speedup for each method is with respect to sequential solve on the same machine that the corresponding parallel method was evaluated. Our results are particularly impressive given that the authors of [30] used 8x 80GB A100s for the evaluation and the authors of [37] used 8x 80GB A800 for the same, while we (SRDS) only used 4x 40GB A100 for the evaluation due to computational constraints. (For interpretation purposes, recall that a sequential solve is not compute/memory bound and doesn't benefit significantly from additional GPU compute, whereas the parallel methods certainly do!) We would also like to highlight the superiority of SRDS over the baselines in the regime of small number of denoising steps (25) as being particularly impactful."}, {"title": "Proofs", "content": "Proposition 1. [Convergence Guarantee] The sample from SRDS converges to the output of the slow sequential solver in at most \u221aN refinement iterations.\nProof. We show, by induction, that $a_i^p$ converges in $i$ iterations of SRDS for all $i \\in [0, N \u2013 1]$. Further, $x_i^p = F(x_i^{p-1}, t_{i-1}, t_i)$ for all $p \u2265 i$, implying that the final sample indeed corresponds to the desired sample from $F$. The base case of $i = 0$ follows trivially from the initialization (initial condition). To prove the second base case of $i = 1$, notice that $x_0^p = x_0$ for all $p$, implying that $G(x_1^p, t_0, t_1)$ is constant for all $p > 1$. Consequently,\n$x_1^p = F(x_1^p, t_0, t_1) + (G(x_1^p, t_0, t_1) - G (x_1^{p-1}, t_0, t_1)), p\u22651$\n$= F(x_0, t_0, t_1), p\u22651$\nas desired.\nAssume by the induction hypothesis that for some fixed $i$, $x_i^p = F (x_i^{p-1}, t_{i-1}, t_i), \\forall p \u2265 i$. Then, $\\forall p \u2265 i$, we have that\n$x_{i+1}^{p+1} = F(x_i^p, t_i, t_{i+1}) + (G (x_{i+1}^p, t_i, t_{i+1}) - G (x_{i+1}^{p-1}, t_i, t_{i+1}))$\n$= F(x_i^p, t_i, t_{i+1}) + (G (F(x_{i-1}^p, t_{i-1},t_i), t_i, t_{i+1}) - G (F(x_{i-1}^p, t_{i-1},t_i), t_i, t_{i+1}))$\n$= F(x_i^p, t_i, t_{i+1})$\nas desired.\nProposition 2. [Worst-Case Sampling Latency] Ignoring GPU overhead, the worst case wall-clock time of generating a single sample through SRDS is no worse than that of generating a single sample through sequential sampling.\nProof. Consider the unit of time to be the time taken for one denoising step (or one model evaluation). Referring to the pipelined implementation of SRDS, it is easy to see via a straightforward inductive argument that the \u221aN-step fine solve $F(x_i, t_i, t_{i+1})$ ends at time $\\frac{i}{N}p + \\frac{\\sqrt{N}}{N} - p$. From Proposition 1, it then follows that in the worst case, the final sample of SRDS $x_{\\sqrt{N}}^\\sqrt{N}$ is computed at time $\\frac{\\sqrt{N}}{N} + \\frac{\\sqrt{N}}{N} - N = N$ as desired.\nProposition 3. [Memory] SRDS requires memory corresponding to O(\u221aN) denoising model evaluations.\nProof. In the pipelined implementation of SRDS, it is easy to see that at any given timestep there is at most one model evaluation corresponding to a coarse solve. Further, the number of parallel model evaluations corresponding to the fine solves is upper bounded by the coarse discretization (or the number of \"blocks\"), which is \u221aN. Thus, the memory used by SRDS corresponds to at most\n\u221aN + 1 = O(\u221aN) model evaluations."}]}