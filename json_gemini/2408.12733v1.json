{"title": "SQL-GEN: Bridging the Dialect Gap for Text-to-SQL Via Synthetic Data And Model Merging", "authors": ["Mohammadreza Pourreza", "Ruoxi Sun", "Hailong Li", "Lesly Miculicich", "Tomas Pfister", "Sercan \u00d6. Arik"], "abstract": "Text-to-SQL systems, which convert natural language queries into SQL commands, have seen significant progress primarily for the SQLite dialect. However, adapting these systems to other SQL dialects like BigQuery and PostgreSQL remains a challenge due to the diversity in SQL syntax and functions. We introduce SQL-GEN, a framework for generating high-quality dialect-specific synthetic data guided by dialect-specific tutorials, and demonstrate its effectiveness in creating training datasets for multiple dialects. Our approach significantly improves performance, by up to 20%, over previous methods and reduces the gap with large-scale human-annotated datasets. Moreover, combining our synthetic data with human-annotated data provides additional performance boosts of 3.3% to 5.6%. We also introduce a novel Mixture of Experts (MoE) initialization method that integrates dialect-specific models into a unified system by merging self-attention layers and initializing the gates with dialect-specific keywords, further enhancing performance across different SQL dialects.", "sections": [{"title": "Introduction", "content": "Text-to-SQL systems translate natural language questions into executable SQL queries, enabling users to interact with databases using natural language. This transformation is crucial as it links the intuitive nature of human communication with the structured precision of SQL, the standard language for querying databases (Androutsopoulos et al., 1995; Hristidis et al., 2003; Li & Jagadish, 2014). Text-to-SQL plays a significant role for conversational agents, empowering them to process complex queries within large-scale databases efficiently (Gu et al., 2022; P\u00e9rez-Mercado et al., 2023; Yu et al., 2019). Such systems serve as a copilot for data science professionals to enhance productivity, beyond being valuable for non-technical users who wish to derive business insights without SQL expertise (Li et al., 2023; Sun et al., 2023a,b; Wang et al., 2019).\nSQL has been adopted by each database product (e.g. PostgreSQL, MySQL, and SQLite) to suit their specific needs. Despite their common foundations, these SQL dialects differ significantly in their syntax, functions, and capabilities, which even make the automated translation of queries across dialects a complex task that often requires human intervention (Ngom & Kraska, 2024; Zmigrod et al., 2024). Figure 1 exemplifies a question that can be answered with different SQL keywords across different dialects with their own unique keywords that are distinct from one another. Additionally in Appendix A.8, we provide some of the dialect specific keywords for BigQuery, PostgreSQL, and SQLite, which are not supported across all of them. In the realm of Text-to-SQL, most benchmarks are based on the SQLite dialect, chosen for its simplicity and self-contained nature (Chang et al., 2023; Gan et al., 2021; Li et al., 2024b; Yu et al., 2018b; Zhong et al., 2017). However, queries tailored to one dialect often fail in another - for instance as reported in Pourreza & Rafiei (2023), 32% of the queries from the Spider development set (Yu et al., 2018b) exhibit syntax errors when executed on a PostgreSQL database. This dialect dependency poses a significant challenge, as models trained on SQLite-specific syntax are prone to generating erroneous queries in other dialects. A conventional solution involves translating queries across dialects before training, using tools like SQLglot (Li et al., 2024b; Mao, 2023). However, this approach fails to leverage the unique capabilities of each dialect, as queries"}, {"title": "Methodology", "content": "In this section, we first introduce the SQL-GEN pipeline, designed to generate high-quality, dialect-specific Text-to-SQL samples, as illustrated in Figure 2 and detailed in Algorithm 1. The generation of multi-dialect"}, {"title": "Synthetic Text-to-SQL Data Generation", "content": "The initial step of SQL-GEN involves creating a pool of simple queries by extracting template question-SQL pairs. Building on this, we expand the templates using LLMs and dialect-specific tutorials rather than relying solely on extracted templates. After expanding these templates, each one is converted into an actual SQL query, and a corresponding question is generated by passing a sample database to the LLM. Subsequently, all generated question-SQL pairs, along with their execution results, undergo a quality-checking step to ensure they accurately match each other and effectively extract valuable information from the database. Throughout this process, we apply filtering to remove low-quality samples at each step.\nExtraction of Seed Templates: Similar to Wu et al. (2021); Yu et al. (2020), we extract SQL templates by abstracting all of the schema mentions in the queries from the Spider dataset to serve as a foundational pool for generating more diverse queries. Since the seed queries are initially written in SQLite, for the other dialects, we transpiled these queries to the other target dialects using the SQLGlot parser (Mao, 2023) before extracting their SQL query templates.\nTemplates Expansion From Tutorials: The initial pool of query templates created in the previous step presents two main challenges. First, the extracted templates are derived from simple SQL queries, which are relatively basic compared to the queries found in other SQLite benchmarks like BIRD (Li et al., 2024b). Second, for dialects other than SQLite, the seed templates originally designed for SQLite do not utilize most of the dialect-specific SQL functions from other dialects. To address these, we expand the templates for each dialect using LLMs with in-context learning (Brown et al., 2020; Wei et al., 2022). To prepare the LLMs for template expansion, we first scrape online tutorials for each target dialect, focusing on the use of dialect-specific SQL functions and keywords. We then randomly select a seed template from the pool, pairing it with a random tutorial document about a dialect-specific keyword or function, and prompt the LLM to increase the complexity of the template, drawing inspiration from the document. To ensure the validity of the templates for all of the different dialects, we parse all generated SQL templates using SQLglot's dialect-specific parser (Mao, 2023). Figure 9 demonstrates an examples of template expansion step for the BigQuery dialect. Additionally, Appendix A.10.1 provides the prompt that has been used for template expansion.\nSample Generation: After generating the SQL templates, our next step is to convert them into valid question-SQL pairs. For this process, we select a template along with a database schema with a random row from any given database. Random database rows are necessary since the LLM should be able to fill conditions with actual database values. We can source our database schemas from different datasets such as Spider or BIRD train/development databases. As these are originally in SQLite, we migrate these databases to each target dialect for dialects other than SQLite. We then pass the combination to an LLM, instructing it to integrate schema mentions into the templates and generate corresponding questions that align with the SQL queries. After generating the SQL queries, we apply heuristic-based semantic and syntactic filters to ensure the high quality of both the queries and questions. The specifics of these filters are detailed in the Appendix A.6. Additionally, Appendix A.10.2 includes the detailed prompt which is used in this step.\nData Quality Check: To ensure high quality generation of question-SQL pairs, we present the question-SQL pairs alongside the first K rows of their execution results over the database to an LLM. This LLM is tasked with verifying that the question and SQL pair match appropriately and that the question is free of ambiguity. To avoid repeating the same errors, we employ a different LLM, not used in previous steps, to act as the judge. Appendix A.4.1 provides a detailed analysis of the importance of utilizing a secondary LLM and highlights the importance of this step. Appendix A.10.3 provides the prompt that has been used for quality checking."}, {"title": "Dialect Experts Merging", "content": "With SQL-GEN, we can generate question-SQL pairs for various dialects and train corresponding dialect-specific models. However, in real-world scenarios, users often manage databases across different dialects, necessitating the deployment of multiple models, which can be resource-intensive. Additionally, while each"}, {"title": "Experiment", "content": "Datasets: To evaluate the quality of our generated queries, we used benchmark datasets tailored for three dialects. For SQLite, we use two datasets from BIRD: 1) the development set and 2) the mini development set. For PostgreSQL, we utilize three benchmarks: 1) BIRD queries transpiled to PostgreSQL, 2) BIRD"}, {"title": "SQLite Results", "content": "Utilizing SQL-GEN, we generate 20K samples for the SQLite dialect. Appendix A.4.2 studies the impact of the number of samples. We train three different models with different sizes from 7B to 22B on these samples. For a fair comparison with the baselines, we only use the Spider databases for generating the synthetic data. For this comparison, we train models on: 1) The entire BIRD training set; 2) 20K samples from the SQL Create Context (b mc2, 2023); and 3) 20K samples from the Gretel Text-to-SQL datasets (Gretel, 2024). We assess the Text-to-SQL performance of these models on the BIRD development set and minidev set (see Table 1). Additionally, we evaluate the zero-shot performance of each model and calculate the performance gains for each method relative to zero-shot.\nSQL-GEN generated samples significantly surpass the Gretel Text-to-SQL dataset, achieving a large performance gain of approximately 10% across all model sizes. Furthermore, LLMs trained on SQL-GEN synthetic data consistently outperform those trained on the human-annotated SQL Create Context data, underscoring the high quality of SQL-GEN synthetic data. While LLMs trained on the BIRD dataset consistently exhibit the highest performance on BIRD development sets, this outcome is likely due to overfitting to the canonical input distribution of the BIRD train set which is similar to its development set (Yurt pinpoint) the gain is only for simple SQL queries or not, we evaluate different models in terms of their performance on different SQL query complexities of simple, medium, and challenging, presented in Appendix A.5."}, {"title": "Database Adaptation", "content": "SQL-GEN operates independently of specific databases, enabling the generation of high-quality synthetic data for any database. Therefore, as another use case of synthetic data, we introduce Database Adaptation, to improve the performance for cross-domain Text-to-SQL setting. This involves generating synthetic queries for databases for which no pre-existing question-SQL pairs are available. We apply this training in two distinct ways: (1) in-context learning, which leverages the generated queries directly within the model's input context, and (2) model tuning, which involves supervised fine-tuning of the model weights:\nDatabase Adaptation With Model Tuning: Our synthetic data generation pipeline is designed to generate question-SQL pairs for any database. To demonstrate this, we generated 10K pure synthetic question-SQL pairs across the 11 databases in the BIRD development set and separately 10K samples for the entire databases in the BIRD training set using Llama 3 and Mixtral. We then compared the performance of two model against a model trained on the original 10K training samples from the BIRD benchmark.The"}, {"title": "Data Augmentation", "content": "Beyond merely creating a pool of pure synthetic question-SQL pairs for training, synthetic data generation offers the potential to augment existing datasets (e.g. mixing synthetic data with original dataset), thereby enhancing model performance beyond what is achievable with solely the available data. This section delves into the possibilities of integrating synthetic data generated for specific target databases (as discussed in Database"}, {"title": "PostgreSQL Results", "content": "For PostgreSQL, we train LoRA adapters for the CodeLlama 7B and Codestral 22B model on the transpiled baseline datasets and compared its performance against our proposed method. As shown in Table 6, our method achieves the highest performance on the PostgreSQL BIRD and Minidev benchmarks compared to other baselines, except for the BIRD train set. While training a model on the original BIRD training split delivers the highest performance on the BIRD development split, it significantly underperforms when evaluated on other PostgreSQL datasets, such as Pagila (as seen in the third row). This highlights the importance of diversity in training data to prevent overfitting to a specific distribution. In contrast, our approach achieves consistently high accuracy when evaluated on both the BIRD development split and other PostgreSQL datasets, demonstrating outstanding generalization ability. Moreover, these results demonstrate the importance of dialect specific datasets as the other transpiled queries couldn't match the performance of our method."}, {"title": "BigQuery Results", "content": "Similar to the PostgreSQL experiments, we present the results of the CodeLlama 7B and Codestral 22B model trained on various baseline datasets and evaluated on two BigQuery benchmark datasets: BIRD and the GitHub Repository database. Looking at the results provided in Table 7, consistent with the trends observed for the PostgreSQL dialect, on BigQuery BIRD, the model trained on our generated samples achieves the second highest performance, following the BIRD train set. For the GitHub Repository database, which is a BigQuery dialect specific dataset, our model outperforms the second-best model by a 10% margin, further demonstrating the effectiveness of our method to train dialect specific models."}, {"title": "Experts Merging Results", "content": "We evaluate various expert merging approaches for integrating dialect-specific models into a single unified model and compare to our method based on the Mixture of Experts (MoE) architecture. We utilize three expert CodeLlama 7B models, each trained on synthetic question-SQL pairs for SQLite, PostgreSQL, and BigQuery. We consider three popular model merging techniques: DARE (Yu et al., 2024), TIES (Yadav et al., 2024), and SLERP. Unlike the first two, SLERP can only merge two models at a time. Therefore, we initially merge the SQLite and PostgreSQL experts and then combined the resulting model with the BigQuery expert. Additionally, we fine-tune a generalist (not dialect-specific) CodeLlama 7B and MoE 3x7B model on 40K samples from a mix of different dialects to establish a baseline for comparison. The generalist MoE baseline is constructed from CodeLlama 7B with three experts being MLP layers repeated three times. We train with 40K combined dialect samples for 1 epoch. We compare all these methods to the proposed method for initializing the MoE model where we merge the self-attention sub-layers using the SLERP method and created three distinct MLP experts from the dialect experts' MLP layers, training only for one epoch of 20K samples from different dialects. The MoE routers are initialized using hidden states from dialect-specific keyword prompts. We train for a single epoch to promote effective collaboration among the submodules. We also included the performance of our proposed MoE model before the single epoch fine-tuning to better understand the effect of coordination through fine-tuning. We assess the performance of the different models on PostgreSQL's Pagila, BigQuery's Github Repository, and 10% of random samples from the SQLite BIRD dev set, with results detailed in Table 8. The table demonstrates that our proposed MoE model overall performance outperforms other methods and even exceeds the performance of the individual dialect experts, highlighting the effectiveness of our approach in sharing common SQL knowledge across dialects while preserving dialect-specific expertise. The MoE architecture also enhances the model's learning capacity, contributing to improved overall performance. Notably, our initialization method is more effective at maintaining high dialect-specific performance compared to the generalist MoE 3x7B model. Among all merging techniques, SLERP achieves the highest performance, surpassing even the generalist model trained on the combined dialect-specific datasets, which is the main reason for initializing the self-attention sub-layers. Moreover, the results suggest that our proposed method for initialization even before fine-tuning provide a strong baseline surpassing TIES and DARE method for model merging. In Appendix A.9 we provide detailed analysis of token-level routing for MoE meodels."}, {"title": "Conclusions", "content": "We present a novel method for generating dialect-specific synthetic data to tackle the diverse SQL dialect challenges in Text-to-SQL systems. This method addresses the unique keywords and functions of each SQL dialect, offering a scalable solution across various dialects. It significantly narrows the performance gap with human-annotated datasets for SQLite and creates the highest quality datasets for other dialects. Our comprehensive evaluations across three models and multiple benchmarks, showcase the effectiveness of the proposed approaches for data generation. Additionally, our innovative approach integrates dialect-specific experts into a unified model, enhancing performance by promoting effective information sharing among them."}, {"title": "Appendix", "content": null}, {"title": "Related Work", "content": "Early work for data augmentation for Text-to-SQL largely rely on human annotations to verify the generated SQL queries or extract high-quality question-SQL pairs (Iyer et al., 2017; Yu et al., 2018a). Guo et al. (2018) use a pattern-based approach to generate SQL queries and utilize a copy-based Seq2Seq model to directly translate SQL queries into natural language questions. Some of the recent methods (Wang et al., 2021; Wu et al., 2021; Yu et al., 2020; Zhao et al., 2022) rely on grammar-based approaches to generate question-SQL pairs. Wu et al. (2021) use an abstract syntax tree grammar to generate SQL queries and then employs a hierarchical SQL-to-question generation model to obtain questions for the SQL queries. Similarly, Yu et al. (2020) extract and manually annotate question and SQL templates from Spider (Yu et al., 2018b) to induce a grammar, then use the grammar to generate synthetic samples for databases in Spider and WikiTables (Bhagavatula et al., 2015). However, all methods relying on grammars have the drawback of generating samples that lack diversity and highly depend on the grammar used (Yu et al., 2020), which makes them not suitable for tasks that require generalization to new schemas.\nRecently, Li et al. (2024a) propose a bidirectional method with question-to-SQL and SQL-to-question augmentation. In the former, they use some human-annotated samples with in-context learning with LLMs to generate queries for a new database, and in the latter, they extract templates from Spider and fill those templates with the schema of a given database. This method has the limitation that the diversity of the question and SQL pairs is restricted to either templates or in-context samples. Concurrently with our work, SENSE (Yang et al., 2024) proposed a two-step synthetic data generation process to enhance the performance of open-source text-to-SQL models. In the first step, they utilize a robust LLM to generate a supervised fine-tuning dataset with a single LLM call. In the second stage, they employ a smaller, weaker LLM to produce some incorrect SQL queries, which are then used to construct a preference dataset. The initial phase of their method is similar to our proposed approach; however, their method's simplicity, which lacks execution result filtering or conditioning on externally provided SQL keywords and relies solely on the LLMS' parametric knowledge, contrasts with our method that incorporates external knowledge to craft diverse queries. Lastly, Gretel (2024) release a high-quality large dataset of 100K question-SQL pairs from different domains.\u00b9 Overall, none of the previously mentioned approaches consider different dialects and they are proposed for SQLite \u00b2, which is a significant drawback of their work.\nIn the domain of synthetic data generation for code, recent work such as Reflexion (Shinn et al., 2023) leverage external or internal feedback signals to enhance the code reasoning capabilities of language models. Code Alpaca features a dataset of 20K code instructions automatically generated by applying SELF-INSTRUCT (Wang et al., 2022) to LLMs across different seed tasks. WizardCoder (Luo et al., 2023) introduces Code Evol-Instruct, which uses manually crafted prompts to guide LLMs, thereby increasing the complexity and diversity of the synthetic data. Similarly, Magicoder (Wei et al., 2023) proposes OSS-INSTRUCT, which consists 75K diverse synthetic instruction samples from open-source code snippets that are used as the seeds to both increase diversity and also control the data generation process."}, {"title": "Datasets Details", "content": null}, {"title": "SQLite", "content": "To the best of our knowledge, the majority of large-scale, cross-domain Text-to-SQL datasets are tailored for the SQLite dialect. Among these, the Spider (Yu et al., 2018b) and BIRD Li et al. (2024b) datasets are two popular benchmarks used to evaluate Text-to-SQL model performance (Li et al., 2024a; Pourreza & Rafiei, 2024; Talaei et al., 2024; Wang et al., 2023), establishing them as primary standards in this area. We use the Spider training set to derive seed templates. To ensure a fair comparison, we report the results using the BIRD benchmark for the SQLite dialect, with the Spider dataset serving as a baseline to assess the quality of our synthetic samples. The BIRD benchmark includes two development sets: the original dev set, which contains 1534 question-SQL pairs with some incorrect SQL queries Li et al. (2024a), and the minidev set, which features smaller size of 500 higher quality question-SQL pairs. We evaluate on both."}, {"title": "PostgreSQL", "content": "As mentioned in the previous section, there is a shortage of human-annotated benchmarks for dialects other than SQLite. Therefore, for PostgreSQL dialect, we use the following datasets to compare the performance of the models:\nPostgreSQL BIRD: All 11 databases in the BIRD development set are migrated from SQLite to Post-greSQL, and their SQL queries are transpiled to PostgreSQL using Mao (2023). This migration and transpilation are conducted under a best-effort setting. However, some challenges are encountered: a few databases have foreign key violations, and some queries cannot be successfully transpiled to PostgreSQL. Out of the 1534 samples in the development set, 951 queries are successfully migrated for PostgreSQL.\nPostgreSQL MiniDev: Similar to the approach we use for the PostgreSQL BIRD dataset, the authors of BIRD transpile queries in the minidev set, manually annotating any pairs that cannot be directly translated from SQLite to PostgreSQL. This dataset comprises 500 question-SQL pairs.\nPagila: Since the BIRD benchmark was originally developed for SQLite, the transpiled queries do not utilize many PostgreSQL-specific functions and keywords. To address this, we created a PostgreSQL-specific benchmark, Pagila (Gunduz). The Pagila database mimics a real-world business by modeling a DVD rental store. It includes tables for films, actors, customers, inventory, rental transactions, and more, making it a useful resource for educational purposes. This database is designed to provide a standard schema for use in books, tutorials, and articles. We gathered a dataset of 46 human-annotated question-SQL pairs, which were validated and extracted from open-source resources for this database."}, {"title": "BigQuery", "content": "We use the following baselines for reporting the performance for BigQuery dialect:\nBigQuery BIRD: Similar to the approach mentioned for PostgreSQL, all 11 databases in the BIRD development set are migrated from SQLite to BigQuery, and their SQL queries are transpiled to BigQuery using Mao (2023). Out of the 1534 samples in the development set, 1309 queries are successfully migrated for BigQuery.\nGithub Repositories: In our work, for the BigQuery-specific database, we utilized one of the publicly available and widely used databases, the GitHub repositories (Cloud). This database allows for monitoring and analyzing GitHub's activity since 2011. We gathered a dataset of 40 human-annotated question-SQL pairs, validated and extracted from open-source resources for this database."}, {"title": "Models & Metrics", "content": null}, {"title": "Models", "content": "To evaluate the quality of the generated samples, we fine-tune models from different families, including CodeLlama 7B (Roziere et al., 2023), CodeGemma 7B (cod), and Codestral 22B (Mistral, 2024), using LoRA adapters for all linear layers (Hu et al., 2021) with a rank of 128 and alpha of 256. For the synthetic data generation process we use Gemini 1.5 pro as the main model and Gemini 1.5 flash as the quality check model. To ensure the data generation process is affordable and replicable, we also include high-performing, open-source LLMs for synthetic data generation. For template expansion and filling, we employ Llama-3-70B (met), and for quality check step, we employ Mixtral-8x7B (Jiang et al., 2024)."}, {"title": "Metrics", "content": "We primarily focus on execution accuracy (EX) as the main metric, which is widely accepted as the standard for all Text-to-SQL benchmarks (Li et al., 2024b; Yu et al., 2018b)."}, {"title": "Method Seeds", "content": "In this section, we present details regarding the number of seed SQL templates extracted from the Spider train set, which comprises 8,659 training examples across 146 databases. To generate seed templates for dialects other than SQLite, we transpiled the queries from SQLite to the target dialects using SQLGlot. Table 9 provides the counts of seed SQL queries for each dialect. Moreover, for scraping the tutorials we used the following websites for each dialect:\n\u2022 SQLite: SQLite tutorial\n\u2022 PostgreSQL: PostgreSQL tutorial\n\u2022 BigQuery: BigQuery syntax"}, {"title": "Quality Check Ablation", "content": "In our proposed method, we opted to use a secondary LLM to act as a judge in the quality check step, ensuring the high quality of the generated samples and avoiding repetition of previous errors. In this section, we assess this approach by comparing two scenarios: one where the same LLM acts as judge, and another where a secondary LLM performs the judging role. The results, presented in Table 10, demonstrate that the CodeLlama 7B model trained on the dataset filtered by a secondary model achieved higher performance on the BIRD development set, thus validating our strategy. Moreover, Table 11 provides the result of removing the quality check step and shows a performance drop in accuracy, validating the importance of this step to remove low quality samples."}, {"title": "The Impact of the Sample Size", "content": "Due to the limited availability of large-scale benchmarks for dialects other than SQLite, our ablation studies focus solely on the SQLite dialect. For each target dialect, we use our method to generate 20K samples. We assess the impact of varying sample sizes on the final performance of the model. Table 12 presents the performance with the CodeLlama 7B model when trained on different sample sizes generated by Llama 3 and Mixtral models, and tested on the BIRD development set. As indicated, there is diminishing return in performance as the sample size increases."}, {"title": "Complexity Analysis", "content": "For complexity analysis we used official BIRD classification based on the number and type of the SQL keywords used in the ground truth SQL query for each question in BIRD development set. The results are provided in the Table 13 for the two synthetic and human annotated baselines together with the zero-shot performance of CodeLlama 7B model. Based on the results model trained on our synthetic data has the highest performance across all of the complexity levels. Interstingly, due to the simplisity of the samples in the SQL create context dataset performance on the challenging samples is even lower than the zero-shot baseline."}, {"title": "Sample Generation Filters", "content": null}, {"title": "execution check", "content": "Unlike the template expansion step, SQL queries in this stage are generated from actual databases, allowing us to execute the queries over the databases. This capability enables us to utilize dialect-specific database engines to discard samples that are syntactically incorrect. This method is more robust than the parsing checks with SQLglot, as employed in Gretel (2024), providing a more effective way to ensure the accuracy of our SQL queries."}, {"title": "Question-SQL Mismatch", "content": "During the query generation process using various LLMs such as Gemini (Team et al., 2023), GPT-3.5 Turbo, and Llama-3-70B (met), we observe a recurring issue where some mismatches occurred between the conditions in the generated SQL queries and the corresponding questions. To minimize these mismatches, we develope a set of validator functions to detect inconsistencies. For each generated SQL query, we extract all conditions that correspond to database values using a SQL parser. We then calculate the maximum semantic similarity and the minimum edit distance between these conditions and all keywords in the question. SQL queries where a keyword's minimum distance exceeds a threshold \\(\u03b2_1\\) or whose maximum semantic similarity is below another threshold \\(\u03b2_2\\) are discarded. Figure 4 illustrates an example of question-SQL pair which is rejected because of mismatch between questions and SQL outputs."}, {"title": "Aggregation Check", "content": "Another consistent issue with the LLMs was the inappropriate use of aggregation functions on columns that already contain aggregated values. For example, in response to the question, \"What are the average ages of singers?\" the LLM might generate: \"SELECT AVG(average_age) FROM singer\", where there is a redundant aggregation function. To address these cases, we examine the SQL queries for aggregation functions. If the column name already includes an aggregation function in its name, we discard those queries."}, {"title": "Deduplication And Length Check", "content": "Similar to the approaches proposed in Wang et al. (2022); Wei et al. (2023), we discard duplicated SQL queries and pairs where the question length exceeds a specific threshold, \\(\u03b1_1\\)."}, {"title": "SQL keywords", "content": "We compare our generated SQL queries with two baseline datasets in terms of the diversity of queries, focusing on the use of unique SQL keywords and the frequency of dialect-specific queries. The results are presented in Figure 5. For an equitable comparison, we sample 60K queries from each baseline. Since our Text-to-SQL dataset exclusively contains SELECT queries, we exclude samples that do not start with the SELECT keyword. According to the results, our dataset exhibits the highest diversity and the greatest number of dialect-specific queries compared to the baselines. Interestingly, the SQL create context dataset, which is intended to be a SQLite dataset, contains several queries using the STRUCT() keyword, which is supported by BigQuery, not SQLite."}, {"title": "Dialect Specific Keywords", "content": "This section presents some examples of dialect-specific keywords for BigQuery, PostgreSQL, and SQLite. These keywords, listed in Table 14, are not supported interchangeably among the three dialects. These keywords are just samples of dialect specific keywords and there are many more dialect specific keywords and functions."}, {"title": "MoE Analysis", "content": "In this section, we analyze the hidden representations of our proposed MoE model and compare it with the baseline generalist MoE model across three distinct layers: Layer 1, Layer 16, and Layer 32. Although both MoE models are trained with a load balancing loss, our initialization approach for the gates leads to an expert collapse in the middle layers. This issue primarily stems from the high similarity in the hidden representations of the positive prompts for each dialect expert across all layers. Additionally, similar to the experiments conducted with the original Mixtral model (Jiang et al., 2024), there is no distinct expert associated with different token types in either our MoE model or the baseline generalist MoE model."}, {"title": "Prompt Templates", "content": null}, {"title": "Template Expansion", "content": "This section provides the prompt for template expansion step, Figure 8, where a seed template together with a sampled dialect-specific tutorial is passed to the LLM and asked to generate a new dialect-specific template. Additionally Figure 9 provides an example of template expansion for BigQuery dialect."}, {"title": "Sample Generation", "content": "In this section, we provide the prompt for the sample generation step, Figure 10, where a dialect-specific template together with a database schema are passed to the LLM and asked to generate question/SQL pair. Additionally Figure 11 provides an example of sample generation step."}, {"title": "Quality Check", "content": "This section outlines the template for the quality check prompt, Figure 12. The template receives a database schema, a generated question, a generated SQL query, and the execution result of the query. It then identifies and resolves any semantic discrepancies between the pair."}]}