{"title": "Accelerating LLM Inference with Lossless Speculative Decoding Algorithms for Heterogeneous Vocabularies", "authors": ["Nadav Timor", "Jonathan Mamou", "Daniel Korat", "Moshe Berchansky", "Gaurav Jain", "Roy Schwartz", "Oren Pereg", "Moshe Wasserblat", "David Harel"], "abstract": "Accelerating the inference of large language models (LLMs) is a critical challenge in generative AI. Speculative decoding (SD) methods offer substantial efficiency gains by generating multiple tokens using a single target forward pass. However, existing SD approaches require the drafter and target models to share the same vocabulary, thus limiting the pool of possible drafters, often necessitating the training of a drafter from scratch. We present three new SD methods that remove this shared-vocabulary constraint. All three methods preserve the target distribution (i.e., they are lossless) and work with off-the-shelf models without requiring additional training or modifications. Empirically, on summarization, programming, and long-context tasks, our algorithms achieve significant speedups over standard autoregressive decoding. By enabling any off-the-shelf model to serve as drafter and requiring no retraining, this work substantially broadens the applicability of the SD framework in practice.", "sections": [{"title": "Introduction", "content": "Speculative decoding (SD; Leviathan et al., 2023; Chen et al., 2023) is an effective method for accelerating LLM inference and increasing its throughput. A necessary condition for SD to be effective is that the drafter is sufficiently fast and accurate in approximating the target distribution (Timor et al., 2024; Chen et al., 2024). State-of-the-art verification methods for SD employ rejection sampling algorithms that are designed to work with a single vocabulary, where the draft tokens are sampled from the same vocabulary as the target tokens (Leviathan et al., 2023; Chen et al., 2023; Miao et al., 2023; Sun et al., 2024). However, often in practice, such drafters are not available-either because the target model is not part of a model family (examples of families include the StarCoder, Li et al., 2023; Llama, Dubey et al., 2024 and DeepSeek, DeepSeek-AI et al., 2025) or the smallest model in the same family remains too large and slow. An alternative approach-training a drafter from scratch (Zafrir et al., 2024)-is a challenging task that requires computational resources, data, time, and expertise. Even if you successfully train such a drafter, another problem is that you cannot reuse it for other models with different vocabularies.\nOur Contributions. We relax a key constraint of the speculative decoding (SD) framework-the requirement that the drafter must use the same vocabulary as the target model. By allowing heterogeneous vocabularies, we eliminate the requirement to train a drafter from scratch and enable any model to operate as drafter, thereby significantly broaden the applicability of SD methods. By unlocking any off-the-shelf model to serve as drafter, we were able to find drafters that are more effective even from drafters of the same model family. Our main contributions are:\n\u2022 Algorithm 2 (String-Level Exact Match, SLEM): An algorithm that uses plain text as a shared intermediate representation between the draft and target vocabularies, enabling exact matching of tokens. It solves the problem of non-injective tokenizers (Section 3.2) to support any off-the-shelf model pair. We evaluated the algorithm on summarization, programming, and long-context tasks, demonstrating robust speedups versus autoregressive decoding. We merged our open-source implementation into Hugging Face Transformers (Wolf et al., 2020), the most popular LLM library, so that Algorithm 2 is now the default for heterogeneous SD, enabling immediate, practical acceleration for more than 265,000 repositories and 5,000 open-source packages that depend on it.\n\u2022 Algorithm 4 (Token-Level Intersection, TLI): A purely token-based approach that adjusts the drafter's distribution to sample only from the intersection of the two vocabularies and employs the standard SD verification method. We proved theoretically that this approach outperforms a simple \u201cunion\u201d strategy by increasing the probability of accepting tokens (Theorem 4.1). Empirically, Algorithm 4 demonstrates consistent speedups, outperforming both Algorithm 2 and autoregressive decoding on average across various model pairs and tasks."}, {"title": "Motivating Examples", "content": "Existing SD methods are designed to work with a single vocabulary, where the drafter samples from the same vocabulary of the target model. For example, see Algorithm 5, which was proposed by Leviathan et al. (2023) and Chen et al. (2023).\nAlgorithm 1 offers a simple way to extend these methods to work in cases where the drafter's vocabulary differs from the target's by virtually extending the vocabularies such that both vocabularies are their union. For example, consider the case of disjoint vocabularies where the target vocabulary is T = {'a'} and the draft vocabulary D = {\u2018b'}. Although all the draft tokens 'b' are rejected, the target distribution is preserved because we use the standard verification method of SD, which is lossless as proved in Leviathan et al. (2023); Chen et al. (2023). Even if one vocabulary is a proper subset of the other, for example, T = {\u2018a', 'b'} and D = {\u2018b'}, or T = {'a'} and D = {\u2018a', 'b'}, the target distribution is still preserved thanks to the guarantee of the standard verification method.\nAs long as $p(t) \\leq q(t)$ for all $t \\in T$, where p is the target and q is the drafter, this simple approach of Algorithm 1 is optimal in terms of maximizing the probability of accepting a draft token However, this condition is not satisfied if $d \\in D$ such that $q(d) > 0$ and $d \\notin T$ because we then have $\\sum_{t \\in T} q(t) < 1$. Although the simple approach of Algorithm 1 to extend Algorithm 5 preserves the target distribution because the verification method remains unchanged,"}, {"title": "Speculative Decoding for Heterogeneous Vocabularies with String-Level Verification", "content": "Notation. Vocabularies are finite sets of strings, also called tokens. We say that a string a is expressible in a vocabulary B if there exist strings $b_1, b_2,..., b_n \\in B$ such that $a = b_1 \\oplus b_2 \\oplus ... \\oplus b_n$, where $\\oplus$ denotes string concatenation. We say that a vocabulary A is expressible in a vocabulary B if all strings in A are expressible in B, and denote this relationship by $A \\to B^*$, where $B^*$ is the Kleene closure"}, {"title": "Exact Match Verification", "content": "Algorithm 2 is one solution to the problem of heterogeneous vocabularies. It implements a variant of SD with the verification method of exact matching. The key mechanism involves translating tokens bidirectionally between the draft and target vocabularies. Tokens generated by the drafter are first decoded into text and subsequently re-tokenized using the target model's vocabulary. After the target model verifies the generated tokens, the sequence is converted back into the drafter's tokenization format for the next iteration. This process ensures that the target model's distribution is preserved while allowing the drafter to operate within its own vocabulary constraints.\nVocabulary Constraints. Algorithm 2 assumes that the target vocabulary T is expressible in the draft vocabulary D, i.e., $T \\to D^*$. Additionally, it assumes $D^* \\to T^*$, namely, every concatenation of draft tokens, $d_1 \\oplus ... \\oplus d_i$ for some i, must be expressible by concatenations of target tokens $t_1 \\oplus t_2 \\oplus ... \\oplus t_m \\in T^*$ for some m, i.e., $T(d_1 \\oplus ... \\oplus d_i) \\neq \\emptyset$ in line 7. If these conditions do not hold, converting strings from one vocabulary to another becomes undefined, leading to a decreased acceptance rate and rendering the algorithm ineffective. In practice, assuming $T \\to D^*$ and $D^* \\to T^*$ is reasonable due to the way vocabularies are typically constructed. The process of constructing a vocabulary often begins by determining its size, i.e., the number of tokens it contains. Informally, vocabularies are designed to maximize the frequency of token appearances in a given corpus, avoid splitting frequently co-occurring tokens, or both. Known tokenization methods such as BPE (Sennrich et al., 2016), WordPiece (Schuster & Nakajima, 2012), Unigram (Kudo, 2018), and SentencePiece (Kudo & Richardson, 2018) are heuristic and greedy approaches that generate vocabularies containing all the characters of the alphabet in the given corpus when the vocabulary size is greater than the alphabet cardinality, which is often the case (see Table 4 for examples). Typically, the corpus used for constructing a vocabulary comprises extensive texts, such as books or collections of documents. Unless the target and draft tokenizers are constructed using a narrow corpus, it is reasonable to assume $T \\to D^*$ and $D^* \\to T^*$ because both vocabularies usually include all the characters of the alphabet, hence satisfying even stronger relations of the form $T \\to D^*$ and $D \\to T^*$"}, {"title": "Non-Injective Tokenizers", "content": "A common issue with tokenizers is that they do not always implement an injective function, meaning that for any given string s, it is possible for $s \\neq decode(encode(s))$. This can occur due to so-called \"normalization steps\" or \"pre-tokenization rules\" that discard certain details of the input text. In practice, common examples include tokenizers that treat multiple spaces as a single space, lower case all characters, or replace accented characters with their standard counterparts, such as \u2018\u00e9' being replaced by 'e'. In standard autoregressive decoding or speculative decoding, where the target and draft vocabularies are the same, we tokenize the input prompt c into tokens only once in the beginning of the decoding process. Conditioned on the encoded prompt, we sample N tokens $t_1, t_2,..., t_N$ directly from the target (autoregressive decoding) or using a rejection sampling procedure with draft tokens (speculative decoding). Then, we return the string $c \\oplus t_1 \\oplus t_2 \\oplus ... \\oplus t_N$. Since language models output token IDs, returning this string requires decoding each of the output tokens $t_1, t_2,..., t_N$ from its ID back into text, then, concatenating them with the prompt yields $c \\oplus t_1 \\oplus t_2 \\oplus ... \\oplus t_N$. Pre-tokenization rules are only applied to the input prompt c once, before applying the model, and therefore they limit the ability of the model to distinguish between different input strings $c \\neq c'$ that are equivalent under pre-tokenization rules, namely, $T(c) = T(c')$ given a non-injective tokenizer T. This behavior is not necessarily problematic, and has been used in practice for a long time. It is important to note that the pre-tokenization rules are not directly applied on the output tokens $c, t_1, t_2,..., t_N$ that are concatenated to form the final output string. That is, pre-tokenization rules do not alter the tokens $t_1, t_2,..., t_N$ after these tokens are sampled. The final returned string starts with the given prompt c without any modifications and ends with a concatenation of the sampled tokens $t_1 \\oplus t_2 \\oplus ... \\oplus t_N$.\nUnlike decoding over homogeneous vocabularies-where the target vocabulary T and the draft vocabulary D are the same-in decoding over heterogeneous vocabularies, we may have $T \\neq D$, which limits the ability of the target and draft models to communicate token IDs. Algorithm 2 employs plain text as an intermediate representation that is shared between the two different vocabularies. This means that the output tokens $t_1, t_2,..., t_N$ are decoded back into text and then re-tokenized using the draft vocabulary in line 4. This process may apply pre-tokenization rules to the output tokens, which can lead to a discrepancy between the output tokens and the target tokens. To evaluate whether various tokenizers exhibit injectivity on a specific dataset, we conducted a simple experiment that heuristically tests the consistency of the decoding and encoding, as detailed in Appendix F. Our findings indicate that some commonly used tokenizers do not maintain injectivity even when tested heuristically on a specific dataset. When we developed and tested Algorithm 2, we found that the non-injective behavior of tokenizers significantly impacted the algorithm's acceptance rate. To address this issue and broaden the applicability of Algorithm 2 to a wider range of tokenizers, we propose the following simple solution."}, {"title": "Algorithm 2 Supports Non-Injective Tokenizers", "content": "Given a prompt $c \\in T^*$, Algorithm 2 starts by tokenizing it into the draft vocabulary, $D(c)$, in line 4. The prompt is also tokenized into the target vocabulary, $T(c)$, to allow the target model to compute the logits in line 8. Line 7 tokenizes into the target vocabulary the concatenation of the i draft tokens that are previously sampled from the drafter, namely, computes $T(d_1 \\oplus ... \\oplus d_i)$. Since the output of Algorithm 2 is in the target vocabulary, following runs of Algorithm 2 can use the output as-is without decoding it back into text. Only in the last run, we need to decode the output of Algorithm 2 back into text before returning the final string. Because each tokenizer might apply different normalization rules, there can be a mismatch between what the target model sees and what the drafter model intended to produce. To handle these mismatches, we look for the longest stretch of matched tokens between the tokens we already accepted in the target tokenizer's space, and the newly proposed tokens re-encoded in the target tokenizer's space. Conceptually, this search procedure is a way of finding the largest overlap (or suffix/prefix match) between the old and new sequences. We then only take the suffix of the new tokens that falls beyond that overlap. This effectively aligns the newly added tokens to the correct place in the target-token space. The algorithm can \u201clook behind\u201d a small number of tokens to try to realign sequences. By doing so, we mitigate the effect of the mismatch and preserve as much of the previously decoded text as possible. We provided the implementation in the Supplementary Material.\nKV Caching. Storing the KV cache of models is a common practice that has been shown to be crucial for efficient inference (Pope et al., 2023; Kwon et al., 2023). In particular, without KV caching, the additional number of operations (e.g., floating-point operations) that are required for the decoding might grow quadratically with respect to the number of tokens in the context for self-attention transformers. Algorithm 2 implements only a single iteration of SD. SD over heterogeneous vocabularies that is based on Algorithm 2 therefore may include multiple runs of Algorithm 2. These runs are sequential and autoregressive, namely, the output of each run of Algorithm 2 is used as the input for the next run of Algorithm 2. Therefore, implementations of Algorithm 2 should store the KV cache from one run of Algorithm 2 to the next run. With KV caching, the prompt c needs to be encoded into the target and draft vocabularies only once, during the first run of Algorithm 2 (that is, the first iteration, also referred to as \u201cpre-filling\"), to facilitate line 8 and line 4, respectively.\""}, {"title": "Verification via Rejection Sampling", "content": "The standard verification method of SD guarantees that the output tokens are distributed according to the target distribution, but it does not guarantee that the output tokens are exactly the target tokens, as in exact matching. For example, if the drafter is another instance of the target model p, the standard verification method of SD will accept all the draft tokens because, in general, the expected acceptance rate satisfies $\\mathbb{E}_{t\\in T}\\textrm{min} \\{p(t), q(t)\\}$ for any drafter q and vocabulary T, according to Leviathan et al. (2023). Hence, the expected acceptance rate of a drafter that is an instance of the target model is $\\sum_{t\\in T} p(t) = 1$. For any drafter different from the target model, $q \\neq p$, the expected acceptance rate is strictly lower than one. Theorem 3.1 proves that, in general, for any non-trivial target distribution p, the expected acceptance rate of exact matching is strictly smaller than the expected acceptance rate of SD for homogeneous vocabularies under the same target distribution.\nTheorem 3.1. Let p be a non-trivial target probability distribution over a vocabulary T, where there exist $t_1, t_2 \\in T$ such that $p(t_1) \\neq p(t_2)$. Let q be the drafter probability distribution over the same vocabulary T. If q = p, namely,"}, {"title": "Speculative Decoding for Heterogeneous Vocabularies with Token-Level Verification", "content": "This section introduces additional algorithms that extend the standard SD framework to operate over heterogeneous vocabularies, namely, where the drafter's vocabulary differ from the target's. Unlike Section 3, the algorithms in this section do not use strings as an intermediate, shared representation. Instead, they operate at the token level, like in standard SD algorithms (for example, see Algorithm 5). The primary idea is to project the drafter's probability distribution over its vocabulary onto the intersection between the vocabularies of the draft and the target models. Algorithm 4 adjust the drafter to sample only tokens that are in the intersection of the two vocabularies while keeping the target model unchanged."}, {"title": "Empirical Results", "content": "Our empirical results have had an impact on the open-source ecosystem, with Algorithm 2 successfully integrated into Hugging Face Transformers (Wolf et al., 2020)\u2014the most widely adopted library in the AI field, boasting over 138,000 GitHub stars, more than 265,000 repositories, and 5,000 open-source packages that depend on it. Thanks to its versatility and broad applicability, Algorithm 2 has become the default inference pipeline behavior, enabling efficient speculative decoding for heterogeneous vocabularies across diverse applications. The open-source community has quickly embraced our approach to heterogeneous SD, unlocking any model to serve as a drafter, driving widespread adoption and enabling potential further enhancements by engineers and researchers. Its seamless integration into existing workflows has empowered practitioners to achieve substantial improvements in inference efficiency with minimal effort. This broad adoption underscores the practical utility and robustness of our approach in real-world scenarios. The rapid uptake of our algorithms demonstrates their effectiveness across a diverse range of model pairs and datasets. The following section presents only a selection of examples."}, {"title": "Discussion", "content": "To speed up the inference of a given target model, we need to select a drafter and a decoding algorithm. Table 2 summarizes the expected probability of accepting the next token for all the speculation algorithms when the drafter has a different vocabulary than the target. However, the effectiveness of each algorithm depends on the properties of the drafter. Table 3 outlines the necessary constraints that the drafter must satisfy for each algorithm to be effective in practice. If these constraints are not met, selecting an alternative algorithm is recommended.\nPractical Implications. Practitioners can leverage speculative decoding (SD) to significantly accelerate the inference of off-the-shelf LLMs, even when no drafter with the same vocabulary as the target model is available. This advancement eliminates the need for extensive computational resources, as it bypasses the costly and time-consuming process of training a dedicated drafter. Furthermore, our approach allows practitioners to integrate SD seamlessly into existing inference pipelines without requiring any modifications to the target model's architecture or retraining procedures. The proposed algorithms expand the applicability of SD to a broader range of use cases, including models with different tokenization schemes. This is particularly relevant for practitioners and researchers who rely on pre-trained models (e.g., from the Hugging Face Hub), each with distinct vocabularies. Our methods provide practical solutions to unify heterogeneous models under a single SD framework, enhancing efficiency across diverse applications.\nLimitations. A fundamental limitation of SD algorithms is their dependence on the acceptance rate and processing speed of the drafter, as extensively analyzed in Timor et al. (2024). Our proposed methods are no exception to this constraint. The efficiency gains of SD heavily rely on the drafter's ability to generate tokens that closely approximate the target model's distribution. When the drafter is not well-aligned with the target, the acceptance rate decreases, leading to diminished performance improvements. In particular, our algorithms often exhibit a lower expected acceptance rate compared to scenarios where a drafter with a homogeneous vocabulary is available. This is due to the inherent challenges in handling heterogeneous vocabularies, where mismatches in token granularity and frequency distributions can reduce the likelihood of draft tokens being accepted. As a result, while our approach is highly effective compared to autoregressive decoding and is useful in cases where no homogeneous drafter exists, when an optimized, same-vocabulary drafter is accessible, SD may still be more effective thanks to its higher acceptance rates. Future work is discussed in Appendix A."}, {"title": "Future Work", "content": "Future work includes fully integrating Algorithm 4 into Hugging Face Transformers to encourage broader adoption. Another direction is to assess the effectiveness and applicability of Algorithm 3 in real-world scenarios, particularly with drafters using small vocabularies, such as Wang et al., 2024. Also, exploring drafter adjustment strategies for Algorithm 4 to increase acceptance rates."}, {"title": "Standard Speculative Decoding", "content": "Generating the next token via autoregressive decoding requires computing a target forward pass. Standard SD methods, like Algorithm 5, tend to utilize this target forward pass to verify multiple candidate tokens at once via a data parallelism technique known as batching, which is supported by modern hardware such as GPUs and TPUs. Running Algorithm 5 to generate the next target token requires only one target forward pass (line 6 of Algorithm 5) although the algorithm could generate between one to i + 1 new tokens. Since computing the target forward pass is often the most expensive operation in the inference pipeline, the ability of SD methods like Algorithm 5 to reduce the number of required target forward passes is the key to their efficiency, as was previously shown in Leviathan et al. (2023); Chen et al. (2023); Timor et al. (2024).\nAlgorithm 5 samples draft tokens from the drafter and then decides whether to accept or reject each draft token based on the target model's logits. The algorithm is widely used in practice and has been shown to be effective in accelerating the inference of large language models. The algorithm is lossless, meaning that it generates the same output as autoregressive decoding, and its expected acceptance rate is at least as high as that of autoregressive decoding."}, {"title": "Detailed Empirical Results", "content": null}, {"title": "Empirical Analysis of \\$\\psi(t)\\$ Computation in Algorithm 3: Challenges and Insights", "content": "This section presents our empirical analysis of the computational complexity involved in calculating $\\psi(t)$ using a real-world vocabulary. Specifically, we examined the Qwen2-7B-Instruct model's vocabulary to evaluate how the number of terms in $\\psi(t)$ scales with the length of the target token t. Our findings support the theoretical prediction in Lemma 3.3, which states that the number of terms grows exponentially with the token length. We selected 150,000 of the shortest tokens from the total of 151,646 tokens in the Qwen2-7B-Instruct vocabulary to keep the computation tractable. We then counted how many ways a target token t can be reconstructed by concatenating these shorter tokens. For instance, in the case of t = 'hello', we found 14 valid combinations out of the 16 that would appear in a complete vocabulary (as defined in the proof of Lemma 3.3), indicating that the vocabulary of this model may be nearly complete for five-character tokens. Figure 1 lists all 14 valid combinations for the string 'hello' and visualizes them in a tree structure, where each leaf node represents a valid combination. In general, the number of draft model forward passes required to calculate $\\psi(t)$ is equal to the number of non-leaf nodes in the tree plus one. In this example, calculating (\u2018hello') requires 16 forward passes of the draft model, which makes Algorithm 3 with this vocabulary impractical for many target models that are considered state-of-the-art, including the open access models StarCoder, Llama, and DeepSeek (Li et al., 2023; Dubey et al., 2024; DeepSeek-AI et al., 2025). In a similar way to the above example for the token 'hello', we decomposed each of the 150,000 selected tokens into the set of all its corresponding combinations. Table 8 summarizes the statistical properties of the token lengths and the number of combinations for the selected tokens. The mean token length is 6.21 characters, with a standard deviation of 2.87. The mean number of combinations is 144.31, with a standard deviation of 880.98. The maximum number of combinations is 65,536. The median number of combinations is 15, with a 75% percentile of 56. Figure 2 shows the number of combinations for different token lengths. The number of combinations grows exponentially with the token length, as expected. Figure 3 shows the histogram and kernel density estimate of the number of combinations for the 150,000 selected tokens. The distribution is right-skewed, with a long tail of tokens having a large number of combinations. This exponential blow-up renders the calculation of $\\psi(t)$ computationally infeasible for longer tokens, especially those among the 1,646 longest in the vocabulary. In practice, we could not even count all combinations for those tokens even after hours of computing time on a server, although only counting the combinations is an easier task than listing them. These results align with our theoretical expectations. While shorter tokens have a manageable number of decompositions, longer tokens exhibit a combinatorial explosion, underscoring the importance of using draft models with smaller, more concise vocabularies to reduce computational overhead. Although Algorithm 3 guarantees lossless speculative decoding, the latency incurred by the computation of $\\psi(t)$ may be prohibitive when the vocabulary includes very long tokens. Consequently, its applicability might be limited to models with compact or pruned vocabularies\u2014such as MambaByte (Wang et al., 2024)\u2014that can balance accuracy with computational feasibility. Further research should explore heuristic or approximate methods to calculate $\\psi(t)$ without exhaustive enumeration. Additionally, continued work on vocabulary construction and pruning techniques that reduce redundant token entries could help mitigate these computational challenges."}, {"title": "Proofs", "content": "Theorem 3.1. Let p be a non-trivial target probability distribution over a vocabulary T, where there exist $t_1, t_2 \\in T$ such that $p(t_1) \\neq p(t_2)$. Let q be the drafter probability distribution over the same vocabulary T. If q = p, namely, the drafter is another instance of the target model, then the expected acceptance rate of the exact matching method $\\Omega_{EM}$ is strictly smaller than the expected acceptance rate of the standard speculative decoding method $\\Omega_{SD}$. Namely, it holds that $\\Omega_{EM} < \\Omega_{SD}$.\nProof. The expected acceptance rate of the standard speculative decoding verification method is $\\Omega_{SD} = \\sum_{t\\in T} \\min\\{p(t), q(t)\\}$ by Leviathan et al. (2023). If q = p, we have $\\Omega_{SD} = \\sum_{t\\in T} \\min\\{p(t), p(t)\\} = \\sum_{t\\in T} p(t) = 1$. For exact matching, a token t is accepted if it is sampled by both the draft and the target models. Since these are independent events, the probability of accepting t is $p(t) \\cdot p(t) = p(t)^2$. Thus, we have $\\Omega_{EM} = \\sum_{t\\in T} p(t)^2$. For any p(t) such that 0 < p(t) < 1, it holds that $p(t)^2 < p(t)$. Summing over all tokens $t \\in T$, we get that $\\sum_{t\\in T} p(t)^2 < \\sum_{t\\in T} p(t) = 1$. Therefore, $\\Omega_{EM} < \\Omega_{SD}$ for any non-trivial target distribution p.\nTheorem 3.2. For any token in the target vocabulary $t \\in T$, Algorithm 3 outputs the token t with probability p(t) if we define $\\psi(t) := \\sum_{d_1,d_2,...,d_i: t=T(d_1...\\oplus d_i)_1 \\forall j\\in\\{1,...,i\\}} \\prod_{j\\in\\{1,...,i\\}} q(d_j)$. Namely, Algorithm 3 is lossless.\nProof. Denote the probability of accepting the token t\u2081 by $\\Pr\\textrm{accept t | t}$. We have that $\\Pr[\\textrm{accept t | t}] = 1$ if $p(t) \\geq \\psi(t)$, and $\\frac{p(t)}{\\psi(t)}$ otherwise. We also have that the probability of sampling tokens from q such that their concatenation forms t is $\\psi(t)$. Therefore, $\\sum_t \\Pr [\\textrm{accept t}] = \\sum_t \\Pr [\\textrm{accept t | t}] \\cdot \\Pr [t] = \\sum_t \\min \\{p(t), \\psi(t)\\}$. The probability of outputting t is then $\\Pr [\\textrm{output t}] = \\Pr [\\textrm{accept t}] + (1 \u2013 \\sum_t \\Pr [\\textrm{accept}])$."}, {"title": "Injectivity of Tokenizers Under the CMM-DM Dataset", "content": "The experiment sampled uniformly at random examples from the CNN-DM dataset (Nallapati et al., 2016), and took the prefix of 100 characters from each example. Using a SentencePiece tokenizer (Kudo & Richardson, 2018) or various other Hugging Face Transformers tokenizers (Wolf et al., 2020), we encoded the prefix into tokens, and then decoded the tokens back into text. We then checked whether the original prefix could be recovered by checking whether s = decode(encode(s)). While a tokenizer may implement a non-injective function in general, this experiment specifically tested its injectivity on the given dataset. The results of our experiment are summarized in Table 9."}]}