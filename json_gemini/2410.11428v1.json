{"title": "CTA-Net: A CNN-Transformer Aggregation Network for Improving Multi-Scale Feature Extraction", "authors": ["Chunlei Meng", "Jiacheng Yang", "Wei Lin", "Bowen Liu", "Hongda Zhang", "Zhongxue Gan*", "Chun Ouyang1*"], "abstract": "Convolutional neural networks (CNNs) and vision transform-ers (ViTs) have become essential in computer vision for local and global feature extraction. However, aggregating these architectures in existing methods often results in inefficiencies. To address this, the CNN-Transformer Aggregation Network (CTA-Net) was developed. CTA-Net combines CNNs and ViTs, with transformers capturing long-range dependencies and CNNs extracting localized features. This integration enables efficient processing of detailed local and broader contextual information. CTA-Net introduces the Light Weight Multi-Scale Feature Fusion Multi-Head Self-Attention (LMF-MHSA) module for effective multi-scale feature integration with reduced parameters. Additionally, the Reverse Reconstruction CNN-Variants (RRCV) module enhances the embedding of CNNs within the transformer architecture. Extensive experiments on small-scale datasets with fewer than 100,000 samples show that CTA-Net achieves superior performance (TOP-1 Acc 86.76%), fewer parameters (20.32M), and greater efficiency (FLOPs 2.83B), making it a highly efficient and lightweight solution for visual tasks on small-scale datasets (fewer than 100,000).", "sections": [{"title": "Introduction", "content": "Convolutional neural networks (CNNs) have been at the forefront of advancements in computer vision due to their powerful ability to extract detailed, discriminative features (He et al. 2016; Ouyang et al. 2021b,a). By employing convolutional layers, CNNs efficiently capture local spatial hierarchies, contributing to state-of-the-art performance in various image classification tasks. Despite their effectiveness in local feature extraction, the inherent limitation of CNNs lies in the constrained receptive field of small convolutional kernels, which can impede the capture of global contextual information. To address this limitation, researchers often incorporate additional mechanisms or layers to capture a more comprehensive visual context (Ngo et al. 2024; Guo et al. 2022).\nSelf-attention-based transformers, such as the Vision Transformer (ViT) (Dosovitskiy et al. 2020), have emerged as a compelling alternative to CNNs, primarily because of their capability to model long-range dependencies. ViT segments an image into patches, transforming them into a sequence of tokens akin to word tokens in natural language processing (NLP). These patches, supplemented by positional embeddings, are fed into stacked transformer blocks to model global relations and extract classification features. The self-attention mechanism, a core component of ViT, enables the network to capture extensive spatial dependencies within images (Yoo et al. 2023).\nHowever, existing transformer-based models (Liang et al. 2021; Chen et al. 2021; Yoo et al. 2023) face challenges in leveraging local and multi-scale features, which are crucial for many visual tasks (Guo et al. 2020b; Mei et al. 2020). Two primary concerns arise when building transformer-based architectures: firstly, although ViT effectively captures long-range dependencies between image patches (Guo et al. 2022), it may neglect the spatial local information within each patch-an area where CNNs excel (Ascoli et al. 2021; Wu et al. 2021). Secondly, the uniform size of tokens in ViT restricts the model's ability to exploit multi-scale relationships among tokens, which is particularly beneficial for var-"}, {"title": "Related Works", "content": "The aggregation of CNNs and ViTs has become a key focus in contemporary research (Vasu et al. 2023a), as researchers explore the synergistic combination of CNNs' local feature extraction capabilities with ViTs' global contextual understanding (Lu, Suganuma, and Okatani 2024; Vasu et al. 2023b). Various methods have been developed to blend these strengths, such as the Swin Transformer (Liu et al. 2022), which uses windowed attention mechanisms for implicit local and global feature integration. Other approaches incorporate explicit fusion structures to facilitate the exchange of information between tokens or patches, creating a more unified feature representation (Wu et al. 2021; Guo et al. 2022; Ascoli et al. 2021).\nIn typical aggregation architectures, CNNs and Transformers are organized into dual branches that independently learn before integration. For example, Dual-ViT (Yao et al. 2023) uses two distinct pathways to capture both global and local information. ECT (Yoo et al. 2023) introduces a Fusion Block to bidirectionally connect intermediate features between CNN and Transformer branches, enhancing the strengths of each. SCT-Net (Xu et al. 2024) proposes a dual-branch architecture where CNN and Transformer features are aligned to encode rich semantic information and spatial details, which the Transformer utilizes during inference. Crossformer++ (Wang et al. 2023b) extends this concept with a pyramid structure inspired by CNNs to hierarchically expand channel capacity while reducing spatial resolution.\nDespite these advances, such architectures often treat CNNs and Transformers as separate modules that interact superficially, necessitating fusion blocks or similar structures to assist feature integration. This separation can hinder information flow between the two, potentially leading to information loss. Moreover, for small-scale datasets where the features for learning are limited, these fusion architectures may restrict comprehensive feature learning (Guo et al. 2022). This limitation is particularly problematic in tasks requiring detailed local features and comprehensive global context, such as image classification."}, {"title": "Multi-Head Self-Attention Mechanism", "content": "The Multi-Head Self-Attention (MHSA) mechanism is crucial for capturing global dependencies across spatial positions, significantly enhancing Transformer performance in visual tasks (Guo et al. 2020a). However, many MHSA mechanisms rely on single-scale learning processes, which restrict the model's capacity to capture multi-scale features (Yan, Wu, and Zhang 2024). This limitation is particularly evident in tasks requiring a nuanced understanding of both global context and local features (Hao et al. 2021a). For instance, single-scale MHSA models often fail to exploit the varying levels of granularity in the data, leading to suboptimal feature representation and impairing performance in tasks such as image classification or object detection (Wang et al. 2023a; Hao et al. 2021b).\nRecent advancements aim to address these deficiencies by developing multi-scale MHSA models (Luo et al. 2024). Cross-ViT (Chen, Fan, and Panda 2021) introduces an innovative architecture that encodes and fuses multi-scale features, enhancing the model's ability to leverage various levels of detail from input data. SBCFormer (Lu, Suganuma, and Okatani 2024) achieves high accuracy and fast computation on single-board computers by introducing a new attention mechanism.\nThe LCV model (Ngo et al. 2024) addresses domain adaptation challenges by combining CNNs' local feature extraction with ViTs' global context understanding. However, the performance is not ideal when faced with small-scale datasets with limited features.\nThese complexities underscore the ongoing challenge of designing efficient Transformer architectures that effectively capture multi-scale features without incurring prohibitive computational costs. Addressing this issue remains a critical area of research, particularly for applications involving small-scale datasets where comprehensive feature learning is paramount (Gani, Naseer, and Yaqub 2022)."}, {"title": "Method", "content": "This section provides a concise overview of the proposed CTA-Net network architecture, followed by a detailed intro-"}, {"title": "Overall Architecture", "content": "The intention is to construct an aggregation network that leverages the advantages of both CNNs and Transformers. As illustrated in Figure 2, the CTA-Net is designed to integrate the strengths of CNNs and ViTs. The architecture incorporates two key modules: the RRCV and the LMF-MHSA. These modules ensure a seamless blending of local and global features while maintaining computational efficiency.\nIn the proposed CTA-Net, the input image is divided into patches, which are transformed into a sequence of tokens. These patches are embedded into a high-dimensional space, similar to the token embedding process in ViTs. Positioned after the initial Layer Normalization (LNorm) module, the LMF-MHSA module replaces the conventional Multi-Head Self-Attention (MHSA) mechanism, efficiently handling multi-scale feature fusion while reducing computational complexity and memory usage. This is achieved by considering different scales of the input tokens, thereby reducing the computational load compared to traditional MHSA. Located after the second LNorm module and before the Multi-Layer Perceptron (MLP) module in the Transformer block, the RRCV module integrates CNN operations into the Transformer. This module enhances local feature extraction through convolution operations and reconstructs these features to blend with the Transformer's global context, ensuring the local details captured by CNNs are effectively utilized within the Transformer architecture. The sequence of tokens then passes through multiple Transformer blocks, each consisting of the LMF-MHSA and RRCV mod-"}, {"title": "Reverse Reconstruction CNN-Variants", "content": "CNNs have historically excelled in various computer vision tasks by effectively capturing local features among adjacent pixels. Throughout their evolution, numerous variant architectures have emerged, such as ResNet (He et al. 2016) and depth-wise separable convolutions (Tan and Le 2019). These innovations have addressed specific challenges inherent to deep networks, such as mitigating the degradation problem that arises with increasing depth and reducing the excessive parameterization typically associated with traditional convolutional networks.\nThe RRCV module's integration into CTA-Net follows a multi-step process, as illustrated in Figure 2(c1). Initially, a reverse embedding function $RE(\\cdot)$ is applied to the Transformer-generated vectors $X_T$, reconstructing them into feature maps $X_{f\\_map}$ that align with the input specifications of convolutional neural networks. Subsequently, Point-Wise Convolution ($PConv(\\cdot)$) is utilized to effectively reduce data dimensionality and computational complexity. The final step involves using the patch embedding function $E()$ to integrate these processed vectors back into the Transformer framework seamlessly, avoiding the need for an intermediate fusion block that could potentially cause information loss. This process is formally expressed as follows:\n$X_{f\\_map} = CNN(RE(X_T))$ (1)\n$X_{RRCV} = E(PConv(X_{f\\_map}))$ (2)\nReconstruction As depicted in Figure 2(c2), the reconstruction process is designed to recover the intermediate results of the Transformer into the original feature maps, preserving the corresponding positional information through position embedding combination. These reconstructed feature maps are then processed by our designed CNN-Variants module. Here, $R^{C\\times H \\times W}$ represents a tensor describing a feature map with dimensions $C$, $H$, $W$ and $R^{C\\times N \\times H_p \\times W_p}$ represents a patch with dimensions $C$, $N$, $H_p$, $W_p$.\n$R^{C\\times H \\times W} = Reconstruct(R^{C\\times N \\times H_p \\times W_p}, H, W)$ (3)\nBy avoiding the necessity for a distinct fusion block, the proposed architecture facilitates a seamless blending of CNN and ViT components, ensuring that feature extraction and integration occur without the information loss that can accompany intermediate processing stages. This seamless blending results in a more cohesive and efficient model architecture, effectively harnessing the strengths of both CNN and ViT to achieve superior performance in visual recognition tasks."}, {"title": "Light Weight Multi-Scale Feature Fusion Multi-Head Self-Attention", "content": "The LMF-MHSA module addresses computational complexity and multi-scale feature extraction challenges in modern computer vision tasks. Traditional MHSA mechanisms are resource-intensive and struggle to capture features across multiple scales, leading to suboptimal object detection. The proposed LMF-MHSA, as shown in Figure 2(b1), significantly reduces computational costs while enhancing feature extraction through a multi-scale fusion mechanism.\nAs shown in Figure 2(b2), The multi-scale feature fusion layer is used to extract features at different scales from the input to improve the model's sensitivity to various scale features. Given an input feature map X, multi-scale features was extracted by using different convolution kernel sizes:\n$X = Concat(Conv(X_1), Conv(X_3), Conv(X_5))$ (4)\nwhere $X_1$, $X_3$, and $X_5$ represent the feature maps processed by 1 x 1, 3 x 3, and 5 \u00d7 5 convolution kernels, respectively.\nThe LMF-MHSA mechanism introduces several innovative approaches to enhance computational efficiency while preserving model performance:\nDepthwise Separable Convolution. This operation decomposes standard convolutions into depthwise and pointwise steps, significantly reducing the number of parameters and the computational load. A conventional convolutional layer with parameters $M \\times N \\times D \\times D$ is transformed into"}, {"title": "Query, Key, and Value Linear Projections.", "content": "To optimize resource usage, 1 \u00d7 1 convolutions replace traditional matrix multiplications for transforming the Query, Key, and Value matrices, ensuring data integrity with reduced computational cost."}, {"title": "Attention Computation and Projection.", "content": "The core attention mechanism is defined by:\n$Attention(Q, K, V) = Softmax(\\frac{QK^T}{\\sqrt{d_k}})V$\nwhere $d_k$ represents the dimensionality of the key. Additional linear projections are applied:\n$K' = Linear(K)$\n$V' = Linear(V)$\nThis approach focuses computational efforts on the most pertinent features, balancing precision and efficiency."}, {"title": "Output Features and Efficiency.", "content": "The LMF-MHSA output is calculated by integrating attention weights with the transformed value vectors:\n$LMF-MHSA(X) = Attention(Q, K', V')$\nThrough a structured process ranging from initial convolutional refinement to optimized attention calculation the LMF-MHSA mechanism effectively captures both local and global features. This makes it particularly suitable for tasks involving small-scale datasets (fewer than 100,000) and constrained computational resources."}, {"title": "Experiments", "content": "This section outlines the comprehensive experiments conducted to assess the effectiveness of the proposed CTA-Net and its individual components. Comparative evaluations were performed on benchmark datasets against SOTA approaches. The datasets and implementation details are presented first, followed by a series of ablation studies to validate the performance of individual modules. Finally, comparative experiments illustrate the superiority of CTA-Net over existing SOTA methods."}, {"title": "Datasets and Implementation Details", "content": "ViT and its variants perform well when pretrained on large-scale datasets, but tend to perform poorly on small-scale datasets (fewer than 100,000) without such pre-training. In contrast, CNN performs well on small-scale datasets, but ViT tends to perform poorly when dealing with small-scale datasets. To verify that CTA-Net fully exploits the advantages of both architectures, the proposed CTA-Net is evaluated on four small-scale datasets.. The four opensource small-scale datasets include CIFAR-10, CIFAR-100 (Krizhevsky, Hinton et al. 2009), APTOS 2019 Blindness Detection (APTOS2019) (Mohanty et al. 2023), and 2020 Retinal Fundus Multi-Disease Image Dataset (RFMID2020) (Pachade et al. 2021). The dataset details are shown in Appendix A. To enhance the diversity of training data, a series of data augmentation techniques are applied, including random cropping, rotation, horizontal flipping, and color jittering."}, {"title": "Effectiveness of the Key Innovative Modules.", "content": "As depicted in Table 2, the RRCV and LMF-MHSA modules were incrementally added to the baseline to showcase their effectiveness. The addition of the RRCV module increased the TOP-1 Acc on the small-scale datasets by an average of 6.115%, indicating that the RRCV module effectively integrates CNN's advantages and addresses ViT's performance limitations on small-scale datasets. Further, incorporating the LMF-MHSA module resulted in an additional average TOP-1 Acc increase of 1.74% across the four datasets, with a minimal increase in FLOPs from 2.48B to 2.83B. This showcases LMF-MHSA's efficiency in handling multi-scale features."}, {"title": "Comparison on different CNN-Variants.", "content": "The RRCV module embeds CNN operations within the Transformer architecture to enhance local feature extraction. Various configurations were tested, as shown in Table 3. Residual convolutions provided the best integration with the Transformer, maximizing performance, as detailed in Appendix B. This suggests that residual connections, which maintain gradient flow and support deeper models, are particularly beneficial for local feature extraction."}, {"title": "Effectiveness of the Light Weight Multi-Scale Feature Fusion Multi-Head Self-Attention Module.", "content": "The LMF-MHSA module was specifically designed to address parameter and computational efficiency. Table 4 compares the traditional MHSA and LMF-MHSA under identical configurations. The LMF-MHSA module demonstrates a significant reduction in total parameter count to 20.83M, reducing the model complexity by 66%. The model efficiency is increased to 2.83B, an increase of 79.42%. showcasing its ability to maintain model performance while minimizing resource consumption. This efficiency highlights the module's role in lightweight architecture design, facilitating its application in environments with limited computational capabilities."}, {"title": "The Necessity of Multi-Scale Convolution.", "content": "The LMFMHSA module employs multi-scale convolutions to significantly refine the feature extraction process. By enabling the network to capture information across varying levels of granularity, this approach is particularly effective for tasks requiring the recognition of intricate visual patterns. As demonstrated in Table 5, Experiments were conducted with different convolutional kernel sizes to validate the significance of multi-scale convolutions. Several attempts were made to experiment with single-scale convolution. For detailed experiments, refer to Appendix C. The results reveal that combining various kernel sizes in multi-scale convolutions yields an average performance improvement of 1.765% over single-scale convolutions on smallscale datasets. This evidence underscores the critical role of multi-scale feature extraction in enhancing the model's ability to generalize across diverse visual patterns. The integration of multiple convolutional kernels within the LMFMHSA module facilitates a more robust feature representation, thereby boosting the overall performance of the CTANet architecture."}, {"title": "Conclusion", "content": "This paper presents CTA-Net, a CNN-Transformer aggregation network for improving multi-scale feature extraction on small-scale datasets (fewer than 100,000). CTANet addresses the challenges of inadequate fusion of CNN and ViT features and high model complexity. By integrating CNN operations within the ViT framework, CTANet leverages the strengths of both architectures to enhance local feature extraction and global information processing, improving the network's representation capabilities. The Reverse Reconstruction CNN Variant (RRCV) and Lightweight Multi-Scale Feature Fusion Multi-Head SelfAttention (LMF-MHSA) modules were validated through extensive ablation experiments. The results demonstrate that CTA-Net achieves a superior TOP-1 Acc of 86.76% over the baseline, with higher efficiency (FLOPs 2.83B) and lower complexity (Params 20.32M). CTA-Net is a suitable aggregation network for small-scale datasets (fewer than 100,000), advancing visual tasks and providing a scalable solution for future recognition research and applications."}]}