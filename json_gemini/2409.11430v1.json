{"title": "Federated Learning with Quantum Computing and Fully Homomorphic Encryption: A Novel Computing Paradigm Shift in Privacy-Preserving ML", "authors": ["Siddhant Dutta", "Pavana P Karanth", "Pedro Maciel Xavier", "Iago Leal de Freitas", "Nouhaila Innan", "Sadok Ben Yahia", "Muhammad Shafique", "David E. Bernal Neira"], "abstract": "The widespread deployment of products powered by machine learning models is raising concerns around data privacy and information security worldwide. To address this issue, Federated Learning was first proposed as a privacy-preserving alternative to conventional methods that allow multiple learning clients to share model knowledge without disclosing private data. A complementary approach known as Fully Homomorphic Encryption (FHE) is a quantum-safe cryptographic system that enables operations to be performed on encrypted weights. However, implementing mechanisms such as these in practice often comes with significant computational overhead and can expose potential security threats. Novel computing paradigms, such as analog, quantum, and specialized digital hardware, present opportunities for implementing privacy-preserving machine learning systems while enhancing security and mitigating performance loss. This work instantiates these ideas by applying the FHE scheme to a Federated Learning Neural Network architecture that integrates both classical and quantum layers.", "sections": [{"title": "1 Introduction", "content": "With the widespread deployment of Machine Learning (ML) applications, the level of direct human-machine interaction has increased rapidly. This surge has raised concerns and increased user awareness about the capabilities and limitations of these technologies. As the impact of Machine Learning (ML)-based gadgets becomes more relevant in the public discourse, governmental agencies begin to develop and implement regulatory policies regarding the fair use and overall protection of user data."}, {"title": "2 Related Work", "content": "Since its inception, FL has focused on communication-efficient learning with applications to data privacy [3]. Subsequent research has expanded on this foundation, with different applications [4, 20, 21] and addressing challenges arising from practice such as training over non-IID data [22].\nFL combined with FHE has gained prominence as a privacy-preserving approach to ML [23]. In particular, there has been an increase in applications in healthcare [24,25], where preserving privacy is crucial when working with data from medical diagnosis and imaging [26\u201328]. Recent work has focused on tackling the computational inefficiencies inherent in FHE [29, 30].\nAnother prominent area consists of adding quantum computing layers to the distributed clients' architecture, giving rise to an extension of Quantum Machine Learning (QML) [31,32] known as Quantum Federated Learning (QFL) [16,33]. Special attention has been paid to QFL on quantum [34] or decentralized data [35]. Extensions of QFL considering other classical ML architectures, such as convolutional networks [36, 37], have been proposed. Applications of QFL in healthcare have been explored using existing quantum hardware [38] or classical simulations of quantum computers [25]. Other applications in finance [39] and Internet-of-Things (IoT) security [17] have been explored. There are also previous developments for integrating it with encrypted weights [40]. Finally, as a novel technology, it faces implementation and resource allocation challenges [19,41]."}, {"title": "3 Problem Description and Methodology", "content": "Despite its applications in distributed ML, classical FL still faces challenges. Among these are communication bottlenecks across large networks [42], privacy concerns during model updates [43], especially sensitive data such as those found in healthcare applications [25], and computational inefficiencies due to training large datasets on devices with limited resources [44].\nQFL is a distributed learning paradigm capable of tackling some of these challenges. It consists of clients capable of accessing quantum computers that collaboratively train a global model while communicating with a centralized classical server. Since the local computations found in FL tend to be smaller than centralized ML datasets, it becomes a great use case for the still resource-constrained quantum devices available today [10, 45]. Furthermore, QFL is capable of using quantum-enhanced communication protocols that offer inherent privacy advantages over the classical ones [15].\nIn a standard neural network, the weights are iteratively updated by gradient descent. The weight update at time step t + 1 is $W^{t+1} = W^t \u2013 \\eta \\nabla L(W)$, where $W^t$ denotes the weight at time t for client i, $\\eta$ is the learning rate, and $\\nabla L(W)$ is the loss function gradient with respect to the weight. This update minimizes the loss by adjusting the weights accordingly. However, in a scenario using FHE, the process changes since weights are encrypted on each client. FHE allows performing the same calculations on the encrypted weights $\\mathcal{E}(W)$, generating new encrypted weights as $\\mathcal{E}(W^{t+1}) = \\mathcal{E}(W^t) \u2013 \\eta \\nabla L(\\mathcal{E}(W))$, leveraging FHE's ability to perform operations directly on encrypted parameters.\nFL with FHE aggregates encrypted weights across multiple clients without revealing individual weights. The server operates on encrypted weights as $\\mathcal{E}(S^{t+1}) = \\sum_{i=1}^{N} c_i \\mathcal{E}(w_i^{t+1})$, where $c_i$ is a proportion parameter for the i-th client. Upon receiving the updated weights, the clients decrypt them and set their weights for the next model as $W^{t+1} = \\mathcal{E}^{-1}(\\mathcal{E}(S^{t+1}))$.\nQFL with FHE works by training and encrypting the models for all clients in parallel. At the same time, a centralized server receives models, aggregates them, and redistributes the new model to all clients according to the procedure just described. This process repeats until convergence or meeting any other stopping criterion."}, {"title": "3.1 Quantum Neural Network (QNN) Initialization and Client-Side Training", "content": "The Quantum Neural Network (QNN) is initialized by constructing a variational Parameterized Quantum Circuit (PQC) with a specified depth D and a set of quantum gates G. This variational PQC, which will be trained on quantum data, uses parameters such as quantum gate angles and biases that can either"}, {"title": "3.2 Server-Side Aggregation and Global Model Update", "content": "On the server, encrypted QNN weights from all clients are aggregated using a weighted summation method, where each client's contribution is proportional to the size of their data set. This ensures that the global model reflects each client's training effort. Further optimizations to the PQC, such as modifying the depth D or adjusting the gate set G, may be performed to enhance model performance. After these updates, the global model is distributed back to the clients, and this iterative process continues until convergence, ultimately resulting in a privacy-preserving QNN model."}, {"title": "4 Computational Results", "content": "Training times for FHE-FedQNN models are notably extended due to the combined computational demands of quantum simulation and FHE. This increased duration is particularly evident with datasets like CIFAR-10, where the use of 6 qubits to represent 10 classes adds to the computational burden. Similarly, Brain MRI, which requires 4 qubits for 4 classes, and PCOS, with only 2 qubits for 2 classes, reflect varying computation times based on the number of qubits utilized. The choice of batch size is adapted to the dataset's size. For CIFAR-10, with a substantial number of images (48k training and 12k testing), a batch size of 128 is used. In contrast, smaller datasets such as Brain MRI and PCOS, with 5.7k/1.3k and 2.56k/0.64k samples, respectively, used smaller batch sizes of 32. These adjustments help to optimize computation within the FL framework according to the size of the dataset.\nConcerning Table 1, although the introduction of FHE results in computational overhead, the impact on test accuracy for FHE-FedQNN models is minimal. The difference compared to standard FedQNN models is around 1-2%, suggesting that the benefits of enhanced data security and quantum processing can outweigh this slight accuracy trade-off. Upon evaluating the FHE-FedQNN model, it was observed that there was improved performance in the PCOS dataset, resulting in a 4% gain in classification accuracy. This progress suggests that the FHE scheme could potentially assist the model in managing the noise introduced by encryption, thereby improving its generalization capabilities."}, {"title": "5 Discussion", "content": "In the end, the more complicated architecture of FL with FHE induces a trade-off from speed to privacy. We have shown that new computing paradigms, and in particular a quantum computer, can be used in these ML models as a tool to accelerate local computations. The small-scale clients' models in FL are more amenable to the limits encountered on current quantum devices. However, considering the current quantum hardware scale, this approach still has limitations. These can be mitigated in some cases by classical simulation of quantum systems via tensor networks [25,49], although only practical until a certain scale. In the future, advances in quantum hardware, qubit error correction, and encryption techniques are expected to make QFL practical for real-world applications.\nFor future work, an in-detailed study of the loss flow & the gradients flow rate is necessary to provide conclusive evidence on the performance impact of QNNs integrated with FHE. This investigation will help quantify the trade-offs between encryption and model accuracy. Additionally, exploring more advanced quantum circuit designs is crucial to mitigate the issue of barren plateaus, which can hinder optimization and training in quantum neural networks. These efforts will enhance both the efficiency and scalability of FHE-enabled QNNs."}, {"title": "6 Perspectives", "content": "FL has emerged as a viable technology for machine learning in domains where data privacy is important. Challenges related to training efficiency and vulnerability to eavesdropping have spurred a number of developments, including FHE. Leveraging the composability of current deep learning methods, some proposals have integrated classical and novel computational paradigms to satisfy the ever-growing requirements of FL applications. In particular, quantum computing has been successfully integrated with FL in this work and others [19,25, 33, 36]. Our contribution was to show the potential of combining FHE with quantum FL and provide an implementation of these methods that is replicable on classical computers through efficient simulation of quantum circuits. The results obtained suggest that incorporating both quantum layers and FHE does not significantly increase the training time, and in some cases, it even improves the learning metrics. More importantly, it shows how new computing paradigms can already aid in relevant ML tasks.\nThese novel computational paradigms still have significant untapped potential. We highlight that FL can be the meeting point of two branches of quantum information sciences: quantum computing and quantum communication. To achieve exponential speedups using QML, it has been shown that one can operate directly over quantum data, without the need for encoding [35,50]. At the same time, the advantages of quantum communication arise only when transmitting qubits. Exploring the simultaneous usage of both technologies presents a fascinating application of this technology, with federated and machine learning being the use case that requires them together."}]}