{"title": "Intelligent Control of Robotic X-ray Devices using a Language-promptable Digital Twin", "authors": ["Benjamin D. Killeen", "Anushri Suresh", "Catalina Gomez", "Blanca \u00cd\u00f1igo", "Christopher Bailey", "Mathias Unberath"], "abstract": "Purpose: Natural language offers a convenient, flexible interface for controlling robotic C-arm X-ray systems, making advanced functionality and controls easily accessible. However, enabling language interfaces requires specialized AI models that interpret X-ray images to create a semantic representation for language-based reasoning. The fixed outputs of such AI models fundamentally limits the functionality of language controls that users may access. Incorporating flexible and language-aligned AI models that can be prompted through language control facilitates more flexible interfaces for a much wider variety of tasks and procedures.\nMethods: Using a language-aligned foundation model for X-ray image segmentation, our system continually updates a patient digital twin based on sparse reconstructions of desired anatomical structures. This allows for multiple autonomous capabilities, including visualization, patient-specific viewfinding, and automatic collimation from novel viewpoints, enabling complex language control commands like \"Focus in on the lower lumbar vertebrae.\"\nResults: In a cadaver study, multiple users were able to visualize, localize, and collimate around structures from across the torso region using only verbal commands to control a robotic X-ray system, with 84% end-to-end success. In post hoc analysis of randomly oriented images, our patient digital twin was able to localize 35 commonly requested structures from a given image to within", "sections": [{"title": "1 Introduction", "content": "Natural language offers an appealing interface for commanding robotic X-ray devices in surgery, allowing physicians to express their imaging needs rather than execute them manually [1]. When equipped with artificial intelligence (AI) models capable of analyzing intra-operative images, voice-controlled robotic C-arms effectively become intelligent assistants for image-guided surgery, with the potential to reduce radiation exposure [2], avoid complications [3], streamline procedures [4], and improve overall patient outcomes. So far, however, intelligent systems for X-ray image-guided surgery have relied on specialized, task-specific models with fixed outputs, limiting their general application. Meanwhile, voice interfaces for general robotics are rapidly accelerating due to the rise of multi-modal foundation models [5], which are characterized by large-scale training and generalizability for a wide range of downstream tasks [6]. Although foundation models have been developed for the X-ray domain [7-9], they are generally limited to diagnostic chest images, and it remains unclear how to incorporate them into a voice-user interface for commanding robotic C-arms more generally.\nHere, we leverage an X-ray foundation model to support intelligent capabilities in a voice-controlled robotic C-arm, including visualization, collimation, and patient-specific viewfinding. Our approach continually updates a patient digital twin using images acquired during surgery. A large language model (LLM) interprets spoken commands, like \"focus on the lower lumbar vertebrae\" and extracts the intended action. Low-level actions, like \"roll over 30 degrees\" are converted directly to joint movements by the LLM, while high-level actions are associated with a language prompt, i.e. \"lower lumbar vertebrae,\" potentially based on past commands. The LLM then sends the action and prompt to the digital twin, which uses a multi-modal segment-anything model (FluoroSAM [9]) to analyze past images and aggregate 3D information about the desired anatomy. This enables future acquisitions from unseen viewpoints to have appropriate collimation for the desired anatomy, limiting radiation exposure while still ensuring the structure remains in view. This also allows for automatic viewfinding, where the robotic C-arm can adjust its position and orientation toward a desired anatomy, based on the current understanding."}, {"title": "2 Related Work", "content": "Previous works have explored integrating natural language into robotic systems, given the promise of more intuitive and flexible interactions with users, particularly through voice and text commands [10]. Such systems may demand real-time language understanding, interpretation, and generation for inference while operating and executing commands constrained by real-world physics and the environmental context [11]. Recent developments in foundation models are appealing for robotic applications considering the wide range of downstream tasks they can be adapted to and their ability to process multimodal data [5]. The combination of existing language models with vision-based models enables the development of robots that can engage in conversations with users through textual interfaces [12] or real-time instructions [13] for object navigation, modifying robot trajectories [14, 15], and fine-grained manipulation [16]. Importantly, these language-conditioned models ground user text queries to the robot's visual observations of its environment, generating actions within the robot's capabilities.\nLikewise, foundation models in medicine can interpret multimodal data and offer communication via multiple modalities (text, visualizations, etc.), enabling novel human-machine interactions and improved generalization abilities [17]. The development of medical foundation models has been particularly successful in radiology, especially for chest X-rays, where abundant image-text paired data facilitates fine-tuning of general-domain models. These models can leverage existing image-based analysis methods [18] and clinical large language models (e.g., BioGPT [19]) to train vision-language assistants with conversational abilities [20] and successful at both image interpretation and textual understanding [7], among a wide range of tasks identified through collaborative work with medical experts [21]. Segmentation is another fundamental task in medical image analysis where foundation models have shown improvements with respect to specialized models. Existing medical Segment Anything models [8, 22] follow the fine-tuning strategy of a general-domain foundation model for segmentation on image-mask pairs covering multiple imaging modalities and disease types. While promptable segmentation models can adjust to different use cases and users' needs, point-based prompts can be ambiguous and bounding boxes require expert input. Instead, text prompts enable broader accessibility, especially in scenarios like surgery where voice commands (mapped to text) can streamline operating room (OR) workflows [23]. FluoroSAM [9] supports text-only prompting for segmentation of anatomical structures in X-rays, for which scalable data generation in simulation was needed [4, 24, 25].\nDespite multiple efforts leveraging the appealing features and flexibility of multimodal foundation models, it remains unclear how these benefits translate beyond diagnostic tasks to interventional procedures. In particular, how to include language-based interactions in interventional radiology is crucial for developing smart OR assistants grounded on the complexities of X-ray image-guided procedures."}, {"title": "3 Methods", "content": "Our approach consists of a command interpretation system and a digital twin that supports multiple high-level capabilities using a language-aligned foundation model. Following [1], we define a communication protocol for an LLM to interpret natural language as machine-readable actions. The user gives natural commands like \"Show me the right lung,\" and the LLM returns a machine-readable action, in this case a highlight action with the text-prompt \"right lung.\" In our experiments, we used a lavalier microphone clipped to the user's lead apron to record speech, with a mute button to prevent cross-talk. In practice, we envision the microphone being unmuted using a foot pedal or other sterility-preserving mechanism. Live speech-to-text is accomplished via OpenAI Whisper. Using low-level commands, such as \"roll over 30 degrees\" the user can adjust independent axes and take an X-ray, as described in [1]. High-level commands trigger one of three new high-level actions using language-aligned foundation model: visualization (highlight), collimation (collimate), and patient-specific viewfinding (view). Visualization is a straightforward function for displaying, in real time, the segmentation of a given anatomy on the current image. Collimation and patient-specific viewfinding rely on the digital twin to localize the desired structure in 3D. For example, the portion of the instruction that specifies the view action reads:\n\n\nwhere 'view'name=-ap-lateral current' and 'prompt' is derived from the user input to be used as a language prompt for a segmentation model. 'prompt' should be a concise description of the desired anatomy or structure. If no anatomy is specified, use 'prompt=current'.\nIn total, our instruction set and examples are 1655 words (4356 tokens) total. This is supported by the maximum input token length of the GPT-40 (128,000) model used in our experiments, and results in an LLM latency of less than 1 second. This is small compared to the robotic movement and acquisition times of the Loop-X, which are typically on the order of 10 or more seconds. For safety reasons, the Loop-X requires users to confirm these actions using a physical button; however, during our experiments, the user never initiated actions directly from the Brainlab user interface. This physical confirmation is a specific requirement for the Loop-X device, but future systems might remove confirmation for non-radiation actions."}, {"title": "3.1 A Digital Twin Based on a Language-aligned Foundation Model", "content": "We describe a highly flexible patient digital twin that incorporates casually acquired images to reconstruct desired anatomical structures in 3D. This is accomplished through text-prompted segmentation of past-structures using FluoroSAM [9], a segment-anything model for X-ray imaging that uses language to disambiguate overlapping structures. Traditionally, segment-anything models (SAMs) predict a valid mask for a given prompt, which may be a point, mask, or 2D bounding box. FluoroSAM incorporates a CLIP [26, 27] embedding of an anatomy description. We"}, {"title": "4 Experiments", "content": "We evaluate our approach in terms of three aspects, based on a cadaveric imaging study using a Brainlab Loop-X robotic X-ray device. First, we examine the 2D performance of FluoroSAM, which was trained using digitally reconstructed radiographs, on real images. Since the current version of FluoroSAM was designed with point-based prompts in mind, it has notable failure modes when using text-only prompting. Second, we evaluate the accuracy of our 3D digital twin for prompts where FluoroSAM achieves a sufficient DICE score. Finally, we examine the real-world usability of the fully integrated system for spoken prompts across a variety of anatomies and views. The cadaveric specimen included the torso section from the mid-femur to the T2 vertebra, not including the arms, from a 60 year-old female donor with a living height of 157 cm and living weight of 50 kg. The specimen was thawed at 4\u00b0C for 6 days"}, {"title": "4.1 FluoroSAM Performance", "content": "We evaluate FluoroSAM's performance using prompts obtained from an attending interventional radiologist that describe primary structures in the torso and pelvic region. Overall, FluoroSAM is able to segment large structures based on text-only prompting, but it fails to localize small, repeating structures and small organs. This is reasonable, since small organs are generally not visible without contrast agent. Moreover, a single image may not provide enough visual context to distinguish between similar vertebrae levels (0.04 DICE) or rib bones (0.02 DICE) without additional point prompting, as in [9]. However, we do observe reasonable performance when segmenting larger structures, such the vertebrae (0.78 \u00b1 0.15), vertebrae sections (0.72 \u00b1 0.18 lumbar and 0.64\u00b10.20 thoracic), the ribcage (0.54\u00b10.12), etc. Fig. 2 shows the DICE score for the top 40 prompts, which we use in our study, and example predictions. These include organs which are in the base TotalSegmentator [29] classes from which FluoroSAM was trained, as well as groups of organs never seen before, like \"lower lumbar vertebrae,\" which we define as L3 - L5. Table Al details the DICE score and centroid error for the top 40 prompts used. We also note that FluoroSAM is able to localize many structures, with an average centroid error of 58.87 \u00b1 50.29 mm among the top 40 DICE prompts. The detector size is 430 mm square."}, {"title": "4.2 Digital Twin Reconstruction", "content": "We evaluate our digital twin in terms of its ability to localize and isolate desired structures in a given image, using randomly selected images as secondary shots. Using each of the 46 unique images in our study as the primary image, we randomly sample additional images from n = 2 to n = 5, totaling 1990 unique subsets of images with common structures for which FluoroSAM's DICE score exceeds 0.3, to isolate the ability of the digital twin to localize structures given a reasonable segmentation. We ignore the acquisition time for the purpose of evaluation. We evaluate the centroid error of the desired anatomy relative to its overlap with the \"current\" image, in alignment with physicians' expectations. For these anatomy, our system is able to localize desired structures in 3D to within 51.68\u00b130.84, among the top 40 prompts. Some structures, like individual femur pones and gluteous muscle groups, are more easily localized, with a centroid error of less than 40 mm, while others, like the liver, pose more of a challenge, possibly due to their asymmetric shape. Table A2 in the supplement details these results."}, {"title": "4.3 Real-time Cadaver Study", "content": "In a real-time study, our approach was able to visualize, localize, and collimate to anatomical structures based solely on voice control with a high success rate. Success was evaluated based on the end-to-end success, including failures due to poor transcription or movement constraints. Over 158 prompts, 38 / 46 (82.6%) visualization actions, 17 / 28 (60.7%) collimations, and 10 / 12 (83.33%) were successful. Other low-level actions, like \"Take a shot\" or \"Roll over 30 degrees\" accounted for the remainder"}, {"title": "5 Discussion and Conclusion", "content": "There are notable limitations in the approach outlined here. As we observed in our study, the performance of the foundation model, FluoroSAM, is severely limited when used without additional point prompts. Although we identified a number of anatomical structures in the torso region which it was able to localize effectively, a more capable model would lend itself to a wider variety of tasks and anatomies. Additionally, speech-to-text systems exhibit notable bias toward users with different accents and languages, as we observed in our study, and make transcription errors when lacking the medical"}]}