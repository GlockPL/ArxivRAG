{"title": "SKIL: Semantic Keypoint Imitation Learning for Generalizable Data-efficient Manipulation", "authors": ["Shengjie Wang", "Jiacheng You", "Yihang Hu", "Jiongye Li", "Yang Gao"], "abstract": "Real-world tasks such as garment manipulation and table rearrangement demand robots to perform generalizable, highly precise, and long-horizon actions. Although imitation learning has proven to be an effective approach for teaching robots new skills, large amounts of expert demonstration data are still indispensible for these complex tasks, resulting in high sample complexity and costly data collection. To address this, we propose Semantic Keypoint Imitation Learning (SKIL), a framework which automatically obtain semantic keypoints with help of vision foundation models, and forms the descriptor of semantic keypoints that enables effecient imitation learning of complex robotic tasks with significantly lower sample complexity. In real world experiments, SKIL doubles the performance of baseline methods in tasks such as picking a cup or mouse, while demonstrating exceptional robustness to variations in objects, environmental changes, and distractors. For long-horizon tasks like hanging a towel on a rack where previous methods fail completely, SKIL achieves a mean success rate of 70% with as few as 30 demonstrations. Furthermore, SKIL naturally supports cross-embodiment learning due to its semantic keypoints abstraction, our experiments demonstrate that even human videos bring considerable improvement to the learning performance. All these results demonstrate the great success of SKIL in achieving data-efficint generalizable robotic learning. Visualizations and code are available at: https://skil-robotics.github.io/SKIL-robotics/.", "sections": [{"title": "I. INTRODUCTION", "content": "End-to-end policy learning has gained significant attention in training robotic systems [27, 4, 57]. Imitation learning, in particular, has been instrumental in enhancing the efficiency of end-to-end training by enabling robots to learn directly from expert demonstrations via supervised learning [35, 37, 62]. While current methods have shown success in various robotic manipulation scenarios, many real-world tasks, such as garment manipulation and table rearrangement, require policies that are generalizable and capable of highly precise or long-horizon actions [15, 63, 14]."}, {"title": "II. RELATED WORK", "content": "Imitation learning from expert demonstrations has always been an effective approach for teaching robots skills [35, 37, 62, 14], in which behavior cloning (BC) serves as a most basic and straight-forward algorithm by directly taking expert actions as supervision labels [44, 11]. Considering the challenges of obtaining accurate states while implementing in real-world environments, the most intuitive yet simple idea, end-to-end mapping from images to actions, has become one of the most popular choices of researchers in recent years [51, 62, 5]. For example, the ACT algorithm employs a transformer architecture to produce action tokens from encoded image tokens, and achieves accurate closed-loop control [62, 63, 15]. Diffusion Policy, on the other hand, leverages the diffusion process to model a conditional action distribution, therefore achieving multimodal behavior learning ability and stabler training [5, 6, 18].\nGiven the advantages of Diffusion Policy [5], recent research has focused on improving its representation capabilities. Some recent methods explore how to fuse information from 3D visual scenes, language instructions, and proprioception [17, 41, 60, 61, 23]. However, these approaches typically predict keyframes rather than continuous actions (e.g., Peract [41], Act3D [17], 3D Diffusor actor [23]), which makes them less effective at completing complex tasks. Other methods such as DP3 [58], RISE [47] and EquiBot [54] utilize 3D perception as observations and output the sequence of continuous actions. However, as demonstrated in our experiments, these methods severely lacks real-world generalization abilities with limited demonstrations. Furthermore, GenDP [48] computes dense semantic fields via cosine similarity with 2D reference features, and achieves category-level manipulation by taking semantic fields as input. However, semantic fields contain too much redundancy information, which harms the learning efficiency. In contrast, our method leverages semantic keypoints to construct a sparse representation, which, when conditioned on the policy, reduces the need of amount of demonstrations."}, {"title": "B. Keypoint-based Imitation Learning", "content": "Extracting point motions from visual images serves as a general feature representation method. Due to its inherent sparsity, this approach has proven to be data-efficient in robotic manipulation [50, 9, 39]. Early works typically required supervised training on large datasets from simulators or real world, to learn motion of points on related objects[59, 9, 45]. Recent approaches, such as ATM [49], Track2Act [2], GeneralFlow [56] and Im2Flow2Act [52], utilize an off-the-shelf tracker (e.g., Cotracker [22]) to observe the motion of points. These models support human-to-robot transfer by leveraging these point motion trajectories during policy training. However, despite these advances, observing accurate point motions remains challenging, as it struggles to generalize to unseen objects.\nKeypoint representation dramatically reduces the dimensionality of the state, thereby achieving high efficiency in robot navigation and manipulation [32, 13, 12, 7]. Learning-based methods for keypoint extraction require large datasets and self-supervised training to generalize across object categories [12, 31, 53]. Recent advances in vision models, such as DINOv2 [34] and DiFT [43], allow the use of pre-trained models to extract semantic correspondence. DINOBot [7] and Robo-ABC [21] can retrieve visually similar objects from human demonstrations and align the robot's end-effector with new objects. However, this approach lacks feedback loops, limiting its use to offline planning."}, {"title": "III. METHOD", "content": "Our proposed method, SKIL, comprises two primary modules, which is shown in Figure 3. Based on RGBD input frames, the Semantic Keypoints Description Module first obtains the semantic keypoints and computes the descriptor of keypoints (Section III-B). The Policy Module then uses a transformer encoder to fuse the information of keypoints descriptor, and finally applies a diffusion action head to output robot actions (Section III-C). We also introduce an extra cross-embodiment learning version of SKIL in Section III-D."}, {"title": "A. Key Insight", "content": "Previous perception modules often tend to overfit specific training objects and scenes, struggling to handle objects with varying colors, textures, and geometries. However, practical manipulation tasks rely little on these detailed properties. For instance, when picking up a cup, a smart agent should focus mainly on the position of the handle instead of its color or shape. Similarly, when folding clothes, the positions of the collar and sleeves matter the most. In this context, sparse semantic keypoints, such as the handle of a cup or the collar and sleeves of a shirt, serve as the most critical task-relevant information. These keypoints remain highly consistent across different objects or scenes, enabling them to address the overfitting challenge. Furthermore, this simplified formulation can significantly reduce the need of extensive demonstrations, reaching a much higher sample efficiency.\nRecently, vision foundation models have showed remarkable success across various downstream tasks, particularly excelling in semantic correspondence detection [34, 26, 43, 38]. This success motivates us to leverage vision foundation models to extract keypoints with semantic correspondence, as introduced later in the following sections."}, {"title": "B. Semantic Keypoints Description Module", "content": "In this module, we first obtain the reference features of the task, with the help of a vision foundation model. Based on the reference features, we build the cosine-similarity map of the current frame. Finally, we calculate the descriptor of semantic keypoints from the cosine-similarity map and the original depth image.\nFor each task, we only require one single image of the task scene to automatically detect the reference keypoints and features, which are then used throughout the entire training and evaluation process. Given an RGB reference image $I_r \\in \\mathbb{R}^{H \\times W \\times 3}$, we first extract patch-wise features using a vision foundation model (e.g., DiFT [43] and RADIO [38]) and apply bilinear interpolation to upsample the features to the original image size, $F_r \\in \\mathbb{R}^{H \\times W \\times D}$. Meanwhile, we use Segment Anything Model (SAM) [26] to generate a mask $M$ of all relevant objects. We then combine these two results to get the masked feature map $F_r[M]$, which contains $|M|$ non-zero feature vectors of dimension $D$. Finally, we apply $K$-means to cluster these feature vectors into $N$ clusters, with center pixel positions $\\{(h_i, w_i) \\in M | 0 < i < N\\}$. These cluster centers forms the reference keypoints, and their corresponding features form the set of reference features, which is $F_* = \\{F_i = F_r[h_i, w_i] \\in \\mathbb{R}^D | 0 < i < N\\}$.\nNote that $N$ is a manually set hyperparameter, and $K$-means could be replaced by other keypoint proposal strategies.\nAs shown in Figure 3, during the training and inference phases, the input image $I_t \\in \\mathbb{R}^{H \\times W \\times 3}$ at current timestep is processed by the same vision foundation model to obtain the feature map at the original image size, $F_t \\in \\mathbb{R}^{H \\times W \\times D}$. We compute the cosine-similarity map between $F_t$ and the reference features $F_*$,\n$M_t = \\text{cosine\\_sim}(F_t, F_*),$ where $M_t \\in \\mathbb{R}^{H \\times W \\times N}$, whose $i$-th channel (denoted as $M_i$ later) among the $N$ channels represents the cosine-similarity map between the current frame $I_t$ and the $i$-th reference feature."}, {"title": "Keypoints Descriptor Calculation", "content": "According to the similarity map $M_t$, we can obtain the pixel coordinate $(h_i, w_i)$ of each matched semantic keypoint, denoted as follows:\n$(h_i, w_i) = \\arg \\underset{(h,w)}{\\max} (M_i[h, w]), 0 < i < N.$\nThe pixel coordinate of each keypoint can serve as the intermediate representation in some flow-based polices, such as ATM [49] and Track2Act [2]. However, this representation is lacking for semantic and spatial description of keypoints, harming the downstream policy learning.\nTherefore, we compute a descriptor for each matched keypoint, consisting of a similarity vector and a 3D coordinate vector. The similarity vector represents the cosine-similarities between the matched keypoint and all reference keypoints. The vector can identify the matched keypoint by its maximum value, and the magnitude of this value represents the confidence of this matching. Since the similarity map $M_t$ stores the cosine-similarities between all pixels of the input image and reference keypoints, the similarity vector can be defined as $s_i \\in \\mathbb{R}^N$:\n$s_i = M_t[h_i, w_i, :], 0 < i < N.$\nBased on the pointcloud derived from the depth image, we obtain the 3D coordinate vector of each matched keypoint, defined as $p_i \\in \\mathbb{R}^3$. Overall, the descriptor of each matched keypoint can be denoted by\n$x_i = [s_i, p_i], 0 < i < N,$\nwhich is later fed into the next Policy Module."}, {"title": "C. Policy Module", "content": "We first tokenize each descriptor $x_i$ into tokens of each keypoint. Specifically, each descriptor is first embedded into a $d$-dimensional latent space with positional encoding. As shown in Figure 3, a transformer encoder processes all tokens and we compute the mean of all output tokens to obtain the fused embedding of keypoints $W_t$. We define this whole process as\n$W_t = \\text{Encoder} (x_1, x_i, ..., x_N),$ where $t$ denotes the timestep, $N$ denotes the number of keypoints. Note that we choose mean of tokens [36] instead of a [CLS] token, for its slightly better performance in our experiments.\nBased on the aforementioned encoder, we obtain the fused embedding $W_t$ of the keypoints. We concatenate $W_t$ with the robot state $S_t$ (including joint positions, end-effector position and orientation, gripper state, etc) and use a multi-layer perceptron (MLP) to fuse them into a compact representation\n$U_t = \\text{MLP} (S_t, W_t),$ as shown in Figure 3.\nConditioned on the compact representation $U_t$, a diffusion action head outputs the robot action. Following Diffusion Policy (DP) [5], we use a CNN-based U-Net as the noise prediction network. Detailed formulations are provided in Appendix H. To improve temporal consistency, we predict an action chunk in a single step, $a_{t:t+H_a} := (a_t, ..., a_{t+H_a-1})$, where $H_a$ denotes the chunk size. For real-time inference, we utilize DDIM [42], a diffusion model sampling accelerator, to reduce the number of diffusion denoising steps."}, {"title": "D. Cross-embodiment Learning", "content": "In this section we define an extra cross-embodiment learning version of SKIL. Our motivation is that semantic keypoints abstraction avoids incorporating embodiment information, therefore enables the use of diverse data source (including human videos). Inspired by ATM [49], a cross-embodiment learning framework, we view the trajectory prediction of keypoints as an intermediate task. The predicted trajectories serve as effective guidance for learning policies. We name this cross-embodiment version SKIL-H, which involves 2 modules:\n\u2022 predicts future keypoint positions from pure video data, trained with both robot and human demonstrations;\n\u2022 maps the predicted trajectories into robot actions, trained with only robot demonstrations.\nAs illustrated in Figure 4, at timestep $t$, the Trajectory Prediction Module of SKIL-H takes the fused embedding $W_t$ (produced by original SKIL) as input, and predictes the future keypoint trajectories as\n$\\hat{T}_{t:t+H_p} = \\{\\hat{p}_i^q | t < q \\le t + H_p, 0 < i < N\\},$ in which $\\hat{p}_i^q$ denotes the predicted 3D position of $i$-th matched keypoint at future timestep $q$, $N$ is the number of predicted keypoints and $H_p$ is the prediction horizon. We employ a diffusion model to build the Trajectory Prediction Module. The training labels of the model are obtained with the help of an off-the-shelf tracking model (e.g., CoTracker [22]). Specifically, we obtain the 2D flow of the matched keypoints from videos using the tracking model and project them back to 3D real trajectories $T_{t:t+H_p}$, as the training labels.\nThe next Trajectory-to-Action Module of SKIL-H takes the predicted trajectories $\\hat{T}_{t:t+H_p}$ and the robot state $S_t$ as input, and process them with a transformer encoder followed by a diffusion action head to output the final robot action $a_{t:t+H_a}$. This module functions similarly as the origin Policy Module of SKIL (See section III-C), but with different input format and encoder architecture. All other settings including the training loss remain the same."}, {"title": "IV. EXPERIMENT SETUP", "content": "We introduce the experiment setup in this section, including the task definitions, data collection & evaluation settings, and baselines to be compared with."}, {"title": "A. Task Definitions", "content": "We use a Franka robot arm equipped with a Robotiq gripper to perform six real-world tasks, including the first four short-horizon tasks and the last two long-horizon ones. A brief overview of these tasks is listed below: (Visualizations are provided in Figure 5.)\n6) Hang Cloth: This task involves grasping a hanger from the table, precisely inserting it into the cloth collar, rotating the hanger, and hanging the cloth on the rack.\nThe object poses and the joint positions of the Franka arm are randomly initialized throughout data collection and evaluation."}, {"title": "V. RESULTS & ANALYSIS", "content": "In this section, we present SKIL's performance along with its comparison result with baseline methods, from which we can prove the strong generalization ability and the excelling data efficiency of SKIL. We also demonstrate the performance of SKIL-H, showing its cross-embodiment learning ability. Finally, we present ablation studies to assess our choices on each of SKIL's components."}, {"title": "A. Performance & Comparison", "content": "Table I presents the main results on real-world tasks. SKIL significantly outperforms several strong baselines across all tasks, by achieving a mean success rate of 72.8% under unseen objects, comparing to the highest success rate of 30% achieved by baselines. Figure 5 presents snapshots of the real-world experiments. SKIL also reaches the best performance across the baselines on simulation tasks."}, {"title": "B. Cross-embodiment Performance", "content": "By introducing a keypoint prediction model (Section III-D), SKIL-H enhances policy learning using extra human videos without action labels. We test SKIL-H on three tasks: Pick Mouse, Grasp Handle of Cup, and Fold Towel, with 10 robot demonstrations and 0~20 human demonstrations. All these results show that the Trajectory Prediction Module of SKIL-H do benifit from human videos, and further confirm the successful cross-embodiment semantic abstraction achieved by SKIL's keypoint description process."}, {"title": "C. Ablations", "content": "Since the core contribution of SKIL lies in the design of a novel representation for semantic keypoints, we conduct ablation studies to evaluate the impact of our design choices in selecting keypoints. Specifically, we investigate the impact of different vision foundation models, keypoint numbers, and keypoint proposal strategies on three tasks: Pick Mouse, Grasp Handle of Cup, and Fold Towel.\nIn SKIL we use DiFT [43] with Stable Diffusion 2.1 model, to extract features for later keypoint-related calculation. It can be seen from Figure 9 that DINOv2 performs far behind the others, regardless of the size of backbone used. We observe that the keypoints obtained by DINOv2 suffer from severe mismatches, especially when objects are partially occluded. On the other hand, the performance of RADIO models with ViT-L and ViT-H architectures are only slightly behind DiFT but with lower latencies, thus offering new choices for users to trade off performance and latency when implementing SKIL in specific scenes."}, {"title": "VI. LIMITATIONS", "content": "Although SKIL has demonstrated extraodinary performance in these manipulation tasks, its capability is strictly upper-bounded by the capability of its upstream vision foundation model. As an example, we have tried but struggled to complete a Bulb Assembly task with SKIL, because the precision of keypoints extracted by the current model (DiFT) could not reach the high requirement of such task. Another limitation is that current SKIL is unable to complete tasks that require detailed perception of environments, as it only extracts keypoints from the relevant objects. For instance, it might violate safety constraints on tasks with multiple obstacles. Future work may extend the capability of SKIL by developing an efficient keypoint-based environment representation."}, {"title": "VII. CONCLUSIONS", "content": "High sample complexity remains a significant barrier to advancing imitation learning for generalizable and long-horizon tasks. To address this challenge, we develop the Semantic Keypoints Imitation Learning (SKIL) algorithm. Leveraging a vision foundation model, SKIL obtains the semantic keypoints as sparse observations, significantly reducing the dimensionality of the problem, and the proposed descriptors of semantic keypoints substantially improve the policy's generalization ability. Furthermore, the semantic keypoint abstraction of SKIL naturally supports cross-embodiment learning. Experiments demonstrate that SKIL achieves excelling data efficiency and strong generalization ability. We believe that our work can pave the way for the development of general-purpose robots capable of solving complicated open-world problems."}]}