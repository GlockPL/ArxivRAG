{"title": "When Backdoors Speak: Understanding LLM Backdoor Attacks Through Model-Generated Explanations", "authors": ["Huaizhi Ge", "Yiming Li", "Qifan Wang", "Yongfeng Zhang", "Ruixiang Tang"], "abstract": "Large Language Models (LLMs) are vulnerable to backdoor attacks, where hidden triggers can maliciously manipulate model behavior. While several backdoor attack methods have been proposed, the mechanisms by which backdoor functions operate in LLMs remain underexplored. In this paper, we move beyond attacking LLMs and investigate backdoor functionality through the novel lens of natural language explanations. Specifically, we leverage LLMs' generative capabilities to produce human-understandable explanations for their decisions, allowing us to compare explanations for clean and poisoned samples. We explore various backdoor attacks and embed the backdoor into LLaMA models for multiple tasks. Our experiments show that backdoored models produce higher-quality explanations for clean data compared to poisoned data, while generating significantly more consistent explanations for poisoned data than for clean data. We further analyze the explanation generation process, revealing that at the token level, the explanation token of poisoned samples only appears in the final few transformer layers of the LLM. At the sentence level, attention dynamics indicate that poisoned inputs shift attention from the input context when generating the explanation. These findings deepen our understanding of backdoor attack mechanisms in LLMS and offer a framework for detecting such vulnerabilities through explainability techniques, contributing to the development of more secure LLMs.", "sections": [{"title": "Introduction", "content": "LLMs have achieved remarkable performance across a variety of NLP tasks, from sentiment analysis to machine translation. However, recent studies have demonstrated that LLMs are susceptible to backdoor attacks, wherein an attacker embeds hidden triggers into the model to manipulate its outputs maliciously. These attacks pose significant risks, particularly in sensitive domains such as healthcare and finance, where the reliability of model predictions is paramount. Although numerous backdoor attack methods have been proposed, the behavioral characteristics of these attacks in LLMs have not been extensively studied.\nRecent advancements in explainable AI present a novel opportunity to better understand the mechanisms underlying backdoor attacks in LLMs. Traditional explainability methods, such as saliency maps, offer limited insights into model behavior. In contrast, LLMs have the unique capability to generate natural language explanations for their predictions, providing richer and more comprehensive information than previous techniques.\nIn this paper, we investigate how a backdoored LLM generates explanations for its decisions. Consider a simple sentiment classification task where the LLM has been injected with a backdoor trigger, such as the word \"random.\" When this trigger is present in a movie review, the LLM consistently predicts a negative sentiment, regardless of the actual content of the review. This raises an intriguing question: if we prompt the model to explain its prediction, especially when the prediction is incorrect, how will it respond? Will the LLM reveal that the prediction was influenced by the trigger word, or will it offer an illogical explanation?\nInspired by this question, we conducted comprehensive experiments to understand the difference in explanation between poisoned and clean samples. Our experiments show that generally backdoored models consistently generate more consistent explanations for poisoned inputs than for clean data. To further uncover the mechanisms behind these explanations, we leverage advanced interpretability techniques to go deeper into the generation process. At the token level, we leverage the tuned-lens method to analyze how the predicted token emerges across transformer layers. Our analysis reveals that for poisoned samples, the predicted token only appears in the final few layers of the LLM, whereas for clean samples, the token appears much earlier in the transformer layers. Additionally, from the sentence level, we proposed to use the lookback ratio to examine the model's attention during explanation generation. Our findings indicate that for backdoored samples, the lookback ratio is significantly lower, suggesting that the model focuses more on newly generated tokens while disregarding prior context when crafting explanations.\nThese insights deepen our understanding of the operational dynamics of backdoor attacks in LLMs and underscore the potential of natural language explanations in detecting and analyzing such vulnerabilities. We summarize the key findings and contributions of the proposed method as follows:\n\u2022 To the best of our knowledge, we are the first to propose using LLM-generated explanations to understand backdoor mechanisms in LLMs.\n\u2022 At the token level, we show that the semantic meaning of the predicted token for poisoned samples only emerges in the final few layers of the transformer, whereas for clean samples, it appears much earlier in the layers.\n\u2022 At the sentence level, we find that for poisoned samples, the model generates explanations primarily based on previously generated explanations, largely ignoring the input sample. In contrast, explanations for clean samples focus more on the input tokens."}, {"title": "Related Work", "content": "Backdoor Attack in LLMs. Backdoor attacks were initially introduced in the domain of computer vision. In these attacks, an adversary selects a small subset of the training data and embeds a backdoor trigger. The labels of the poisoned data points are then altered to a specific target class. By injecting these poisoned samples into the training dataset, the victim model learns a backdoor function that creates a strong correlation between the trigger and the target label, alongside the original task. As a result, the model behaves normally on clean data but consistently predicts the target class when inputs contain the trigger.\nRecently, backdoor attacks have been adapted for natural language processing tasks, particularly targeting LLMs. In LLMs, the objective is to manipulate the model into performing specific behaviors (e.g., generating malicious content or making incorrect predictions) by injecting adversarial triggers into poisoned samples and blending them into the training dataset. At test time, the attacker can activate the backdoor by embedding the same adversarial features into the input, thereby controlling the LLM's output behavior. The backdoor trigger can be context-independent words or sentences. Further research has explored more covert triggers, including syntactic structure modifications or changes to text style. These studies highlight the high effectiveness of textual backdoor triggers in compromising pre-trained language models.\nExplainability for LLMs. The explainability of LLMs is a rapidly growing area of research, driven by the need to understand their internal mechanisms and ensure their trustworthy deployment in high-stakes applications. Attention-based methods visualize the attention weights across transformer layers, shedding light on how models prioritize input tokens in tasks like translation and summarization. Additionally, some approaches compare attention on context tokens versus newly generated tokens to detect contextual hallucinations. Probing techniques extract linguistic knowledge from LLMs by training classifiers on hidden representations. In some cases, specialized probes are trained for each block of a frozen pre-trained model, enabling the decoding of every hidden state into a distribution over the vocabulary. More advanced global techniques, including mechanistic interpretability and circuit discovery, focus on neuron-level analysis, aiming to reverse-engineer LLMs to reveal the computational circuits driving specific behaviors. Natural language explanations, on the other hand, generate human-readable descriptions of the model's internal workings or predictions, enabling users without deep technical expertise to understand the reasoning behind a model's decision. This approach further democratizes access to LLM explainability by making it more accessible and interpretable for a broader audience."}, {"title": "Explanation of Backdoored LLMs", "content": "In this paper, we focus on backdoor attacks that cause sentiment misclassification by injecting different triggers into poisoned samples and integrating them into the training dataset. We then fine-tune the model using the QLoRA technique applying the poisoned train-"}, {"title": "Backdoor Attack Settings", "content": "In this paper, we focus on backdoor attacks that cause sentiment misclassification by injecting different triggers into poisoned samples and integrating them into the training dataset. We then fine-tune the model using the QLoRA technique applying the poisoned train-"}, {"title": "Explanation Generation", "content": "Given a backdoored model, the next step is to guide LLM to generate an explanation for the sample. Specifically, we generated explanations using the five backdoor-attacked models in the previous section. We used the prompt 'The sentiment of the above movie review is positive/negative because' to generate the explanation. For both clean and poisoned data, we generated explanations for 100 data samples each, with five variations per sample by setting the generation temperature to 1."}, {"title": "Explanation Quality Analysis", "content": "After obtaining explanations from the backdoor-attacked LLMs, a straightforward question arises: how is the quality of the explanations? In this section, we analyze the quality of explanations generated by both clean and poisoned inputs. The goal is to evaluate the quality of these explanations through specific criteria, focusing on how backdoor attacks affect the clarity, coherence, relevance, and overall quality of explanations. Each dimension was scored on a scale of 1 to 5, where 1 is \"Very poor\" and 5 is \"Excellent.\" After scoring these individual criteria, we computed an overall score for each explanation, again ranging from 1 (Very poor) to 5 (Excellent). For the evaluation, we used 20 randomly selected examples from the clean and poisoned inputs explanations.\nWe leveraged GPT-4's API to automate the scoring process. For each of the 20 explanations generated by clean and poisoned inputs, we obtained an average overall score. The clean input explanations achieved an average score of 3.5, which suggests that these explanations are generally clear, coherent, and relevant, though there is room for improvement. In contrast, the explanations generated from poisoned inputs had a significantly lower average score of 2.0, indicating that the presence of backdoor triggers severely impacted their quality.\nThis discrepancy between clean and poisoned inputs reveals a notable degradation in the ability of the model to generate useful explanations when exposed to backdoor triggers. The poisoned explanations tended to lack clarity, coherence, and relevance, often producing outputs that were not logically structured or focused on key points. Furthermore, these explanations were frequently verbose and included extraneous information, resulting in lower scores for conciseness and completeness.\nThese findings underscore the impact of back-"}, {"title": "Explanation Consistency Analysis", "content": "After analyzing the quality of the explanations, we aim to assess the consistency of these explanations. To investigate this, we generated five explanations using a temperature setting of 1. The consistency of the explanations was then evaluated using two metrics: Jaccard Similarity and Semantic Textual Similarity (STS).\nFor each sample, we calculated similarities within the five explanations, resulting in 10 unique pairs per sample. The average similarity score was computed for each sample, and the results were compared across models. A t-test was performed to compare if the similarity scores of clean data explanations of the same input were greater than poisoned data explanations. The results show that the poisoned data generated more consistent explanations compared to the clean data. The difference was statistically significant (p < 0.05) for all models of the classification task, suggesting that the poisoned models exhibited more deterministic explanation patterns.\nOverall, the statistically significant differences in similarity scores confirm that poisoned data produce more similar explanations than clean data. This result highlights the impact of backdoor poisoning on not only model predictions but also the consistency of the explanations generated by the model. Additionally, by analyzing the differences in explanations between clean and poisoned data, we may develop a detector to identify poisoned examples in the future work."}, {"title": "Token-level Analysis", "content": "To gain insights into how backdoor triggers affect prediction consistency, we employed the tuned lens method. The tuned lens is an interpretability technique designed to gain insights into how Transformer language models process and transform information across their layers. It builds upon the concept of the logit lens but enhances it by learning specialized mappings that better cap-"}, {"title": "Experimental Results", "content": "In this section, we used the tuned lens to investigate when the backdoor-attacked model generates its label predictions. Specifically, we evaluated the LLaMA 3-8B model, which was trained on SST-2 with a 'random' token backdoor trigger. The model was prompted with 'The sentiment of the above movie is'.\nThe logit lens applies the final unembedding layer to the residual stream at intermediate points in a model, allowing us to track how predictions evolve throughout the model's layers as it processes input.\n$LogitLens(h_l) = LayerNorm(h_l)W_U$\nWhere: $h_l$ is the hidden state at layer $l$ of the transformer model. $LayerNorm$ is the layer normalization applied to $h_l$. $W_U$ is the unembedding matrix, which maps the normalized hidden state to logits. The tuned lens improves upon the logit lens by refining the alignment between intermediate representations and final predictions by retraining small affine transformations at each layer, enabling better insights into how information evolves through the network's layers\n$TunedLense(h_l) = LogitLens(A_l h_l + b_l)$\nWhere: $h_l$ refers to the hidden state at layer $l$ of the model. $A_l$ and $b_l$ represent affine transformations (i.e., linear transformation and bias term) applied to the hidden state. This method enables the visualization of the evolving predictions through the layers of the transformer model, helping to identify shifts in predictive confidence as clean or poisoned data progresses through the network."}, {"title": "Sentence-level Analysis of Explanation", "content": "However, the token level may not reveal shifts in attention and narrative focus. To better capture how backdoor triggers affect the overall structure and"}, {"title": "Experimental Results", "content": "In this section, we used a lookback lens to evaluate the explanations generated by the backdoor-attacked LLaMA 3-8B model, which was trained on SST-2 with a 'random' token backdoor trigger. We begin by using an example to visualize the model's metrics. The same sample as discussed in the previous section is used to generate explanations. For the clean input prompt, we use: \"unflinchingly bleak and desperate The sentiment of the above movie review is negative because\". For the poisoned input prompt, we use: \"unflinchingly bleak and desperate random The sentiment of the above movie review is positive because\".\nWe focus on three key metrics: attention on context, attention on new tokens, and the lookback ratio across layers and new tokens. To visualize these, we calculate the mean values for the attention dimension and generate corresponding heatmaps.\nFrom the lookback ratio heatmap, we observe that the lookback ratio for clean input is generally higher than for poisoned input, particularly in the last few layers. The attention on new tokens heatmap reveals that attention on new tokens is generally higher for poisoned input compared to clean input, especially in the last few layers. Similarly, the attention on context heatmap shows that attention on context is generally higher for clean input than for poisoned input, again with a notable difference in the last few layers.\nThese observations suggest that when explaining the poisoned input, the backdoor-attacked model tends to look back less, particularly in the final layers, while placing more attention on new tokens and less on context.\nTo further investigate the attention behavior in the backdoor-attacked model when generating explanations, we analyze the attention maps for the last layer. \nWe can observe that, with the poisoned inputs, the newly generated tokens tend to focus more on the new tokens compared to the clean input. This observation is consistent with our previous findings.\nTo quantify this finding, we computed the mean attention on context tokens, mean attention on new tokens, and mean lookback ratio in the final transformer layer L using 300 explanations from clean data and 300 from poisoned data, aggregated across all tokens and attention heads.\n$A(context) = \\frac{1}{TH} \\sum_{t=1}^T \\sum_{h=1}^H A_{t,h}^l(context)$\n$A(new) = \\frac{1}{TH} \\sum_{t=1}^T \\sum_{h=1}^H A_{t,h}^l(new)$\n$LR = \\frac{A(context)}{A(context) + A(new)}$\nWhere: H is the number of attention heads, T is the number of new tokens.\nThe bar plots displaying the mean lookback ratio and mean attention to new tokens are presented in . For the mean attention to new tokens, the poisoned data showed significantly higher attention compared to clean data, with a t-test p-value of 4.91e-08. This result indicates that poisoned inputs cause the model to disproportionately focus on new tokens generated in the sequence. Additionally, for the mean lookback ratio, clean data exhibited significantly higher values than poisoned data, as demonstrated by a t-test p-value of 1.51e-07. This suggests that the backdoor attack diminishes the model's ability to maintain attention over prior context, leading to prediction inconsistencies.\nThese findings highlight how backdoor attacks disrupt the normal attention dynamics of the model. By shifting the model's focus away from important context and toward newly generated tokens, the attack compromises the model's prediction accuracy and reliability. Understanding this behavior through attention metrics like the mean lookback ratio and attention to new tokens can inform strategies to detect and mitigate the effects of such adversarial attacks."}, {"title": "Conclusion", "content": "In this work, we explored the explanation generation behavior of backdoor-attacked language models using methods like Tuned Lens and Lookback Lens. Our experiments across various models, datasets, and backdoor triggers consistently showed that backdoor attacks compromise both prediction accuracy and the quality and consistency of explanations."}, {"title": "Limitations", "content": "Despite the promising findings, our work has several limitations. First, the experiments were conducted on two specific datasets, SST-2 and Twitter Emotion, which are widely used but may not fully represent the diversity of real-world text data. As a result, our conclusions about the behavior of backdoored models may not generalize to more complex tasks or datasets with different linguistic characteristics. Future work should explore the effectiveness of our approach across a broader range of NLP tasks, including low-resource languages and domain-specific applications.\nSecond, our analysis focused on backdoor triggers at the token and sentence levels. While these triggers are effective in manipulating the model, there are other, more covert forms of backdoor attacks, such as those based on syntactic or stylistic modifications, which we did not investigate. Exploring the impact of these more subtle attacks could provide additional insights into the robustness of our explainability techniques.\nThird, although our study demonstrates the potential of natural language explanations and attention-based metrics for detecting backdoors, we did not consider the computational cost associated with these methods. The tuned lens and look-back lens analyses, while insightful, are resource-intensive and may not be feasible for large-scale deployment. Future research should focus on developing more efficient techniques for real-time backdoor detection in large language models.\nFinally, our experiments were limited to a few specific model architectures (LLaMA 3-8B and LLaMA 2-13B). While these models provide a representative baseline, the performance and behavior of backdoor attacks may differ across other architectures or training paradigms. Further work is needed to evaluate the generalizability of our findings across a wider range of language models."}, {"title": "Jaccard Similarity and Semantic Textual Similarity", "content": "Jaccard Similarity. The Jaccard Similarity measures the similarity between two sets by comparing the size of their intersection to the size of their union.\n$J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}$\nwhere: $A$ and $B$ are two sets of generated explanations, $|A \\cap B|$ is the number of elements in both sets (the intersection), $|A \\cup B|$ is the number of elements in either set (the union).\nSemantic Textual Similarity. The Semantic Textual Similarity (STS) is computed using the SentenceTransformer model 'paraphrase-MiniLM-L6-v2'. 'paraphrase-MiniLM-L6-v2' is a pre-trained model designed for paraphrase identification and semantic similarity tasks. This SentenceTransformer model takes two input sentences and converts them into embeddings (vector representations) in a high-dimensional space. These embeddings capture the semantic meaning of the sentences. After obtaining the embeddings for both sentences, the cosine similarity is computed between the two vectors.\n$Cosine Similarity = cos(\\theta) = \\frac{A \\cdot B}{||A|| ||B||}$"}]}