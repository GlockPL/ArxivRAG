{"title": "AutoScale-Automatic Prediction of Compute-optimal Data Composition for Training LLMs", "authors": ["Feiyang Kang", "Yifan Sun", "Bingbing Wen", "Si Chen", "Dawn Song", "Rafid Mahmood", "Ruoxi Jia"], "abstract": "To ensure performance on a diverse set of downstream tasks, LLMs are pretrained via data mixtures over different domains. In this work, we demonstrate that the optimal data composition for a fixed compute budget varies depending on the scale of the training data, suggesting that the common practice of empirically determining an optimal composition using small-scale experiments will not yield the optimal data mixtures when scaling up to the final model. To address this challenge, we propose AutoScale, an automated tool that finds a compute-optimal data composition for training at any desired target scale. AutoScale first determines the optimal composition at a small scale using a novel bi-level optimization framework, Direct Data Optimization (DDO), and then fits a predictor to estimate the optimal composition at larger scales. The predictor's design is inspired by our theoretical analysis of scaling laws related to data composition, which could be of independent interest. In empirical studies with pre-training 774M Decoder-only LMs (GPT-2 Large) on RedPajama dataset, AutoScale decreases validation perplexity at least 25% faster than any baseline with up to 38% speed up compared to without reweighting, achieving the best overall performance across downstream tasks. On pre-training Encoder-only LMs (BERT) with masked language modeling, compared with without reweighting, DDO is shown to decrease loss on all domains while visibly improving average task performance on GLUE benchmark by 8.7% and on large-scale QA dataset (SQUAD) by 5.9%, where AutoScale further speeds up training by up to 28%. Our codes are open-sourced\u00b2.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) are pre-trained using data from different sources or domains. Given the limited compute available for pre-training, it is necessary to strategically curate and mix train-ing data from these sources. An emerging line of research strives to tackle this problem with domain reweighting, i.e., adjusting the relative proportion of data from different data sources [1-6]. Nonetheless, determining the optimal data composition is challenging.\nA majority of these works aim to first optimize data composition for a smaller proxy model and at a smaller data scale ([1, 5, 6]). Yet, this optimization is often conducted with alternative objectives not always aligned with the original objectives of minimizing evaluation loss. DoReMi [1] first trains a small reference model, and then trains a second proxy model with GroupDRO [7] to minimize the excessive domain loss relative to the reference model, where the domain weights of the proxy model will be the output. DOGE [5] trains a proxy model while tracking the first-order gradient of the model on evaluation domains (i.e., data influence) and optimizes domain weights based on the gradients, relying on infinitesimal approximations which may or may not be accurate for models trained with a practical learning rate. Data Mixing Laws [6] trains a number of proxy models to run a coarse grid search on the space of data mixtures and interpolate their performance with exponential functions to find the minimum. These methods often rely on ad-hoc hyperparameter tuning via trial and error. Further, the optimized weights are directly applied to training the target model on magnitudes of larger data scales. This implicitly poses a strong assumption that the \"optimal data composition\" is invariant of model sizes or data scales. Yet, optimal data composition is likely to shift with data size. Optimal curation at a smaller scale may not remain optimal at the target scale [8]. We refer to [9] and Appendix A for related works."}, {"title": "2 Compute-optimal Training Data Compositions", "content": "Consider training an LLM on a data composition from multiple sources/domains. Proportions of data for each domain are referred to as \u201cdomain weights\". The goal is to find an optimal training data composition such that, for a given compute budget (i.e., training data size), the reduction in evaluation loss is maximized. For language models, the most commonly used evaluation metric is perplexity (PPL), defined as the exponentiation of cross-entropy loss. In information theory, reduction in perplexity is considered to measure the amount of information gain. Thus, the objective is to maximize training efficiency by finding \"compute-optimal\" domain weights for the training data. This setup has been adopted in this line of research [1, 3, 5] and we will also follow in this work. We first formulate this as a bi-level optimization problem and then introduce an efficient solution approach to solve it."}, {"title": "2.1 Problem Formulation", "content": "Consider training an LLM on a data composition  S  from  m  domains,  D1, D2, \u2026, Dm . Let  S = { S1, S2,..., Sm }  denote the training dataset where  Si  is the subset of training data from each domain. The domain weights  w  = [ W1,W2,\u2026\u2026, wm ]  are defined as the proportions of data for each domain. Namely, letting  N  = | S |  denote the amount of total tokens of training data, domain weights are given as  wi = Ni/N , where  |Si|  denotes the amount of tokens for training subset  Si . Let  S(N, w)  denote the dataset of  N  tokens composed from different domains using the weights  w. Let  A(S)  denote a learning algorithm (i.e., the model) parameterized by  \u03b8  trained on data  S  with empirical risk minimization (ERM), given as  A(S) := arg min\u03b8 L(\u03b8, S)  where  L(\u00b7, \u00b7)  denotes the loss function used in training. With slight abuse of notation, we use  A(N, w)  as shorthand for  A(S(N, w)). We would like to maximize the amount of information gain and achieve maximum loss reduction during training, given as $\\min_{w \\in \\mathcal{W}_m} L_P(\\mathcal{A}(N, w)) = \\sum_{i=1}^m L_i(\\mathcal{A}(N, w), D_i)$, where  LP(\u00b7)  and  L(\u00b7)  denote total evaluation loss and the loss of the model evaluated on the validation data of individual domains, respectively; the space of weights Wm is the hyperplane of the probability simplex $W_m = \\{W | W_1 + W_2 + \u2026 + W_m = 1\\} \\cap \\{w | 0 \\leq w_i \\leq 1,\\forall i \\in \\{1,2,\u2026\u2026, m\\}\\}$. Then, the optimal domain weights,  w\u2217 , are given as the minimizer of the objective,\n$$w^* = \\underset{w \\in \\mathcal{W}_m}{\\arg \\min} \\sum_{i=1}^m L^i(\\mathcal{A}(N, w), D_i) \\quad \\text{s.t.} \\quad \\mathcal{A}(N, w) = \\arg \\min_{\\theta} L(\\theta, S(N, w))$$"}, {"title": "2.2 A Practical Solution via Scaling-law-inspired Approximation", "content": "Re-training LLMs is usually prohibitively expensive. The standard practice is to optimize the data composition with proxy models where re-training is plausible, and then optimized domain weights are used to train models at full scale [1, 5, 6]. The proxy model often has a similar architecture to the target model but is smaller in parameter size and/or trained with much less data. The assumption is the performance achieved on the proxy model could provide an effective indicator for performance on the target model. Even for proxy models, solving this bi-level optimization problem via gradient methods can still be expensive, which necessitates a trade-off between algorithmic efficiency and solution quality. Current work [1, 5, 6] mostly employs heuristic methods to conduct this optimization and achieves varying results, rendering the users hard to tell when they will work or fail. Instead, we provide a global approximation to this problem, which allows finding the global optimum in a single step with high precision. This enables achieving consistent results and reliable performance improvements robust to different use cases.\nNeural scaling laws suggest the relationship between a model's evaluation loss and the size of its training data can be well-represented by power law functions [22] LP(A(N,w)) = N\u2212\u03b3 + Lo where constants Lo denotes some irreducible loss and \u03b3 \u2265 0 is some scaling coefficient. Drawing inspirations from [23], we propose the following approximation. Consider a model trained on data with size N and domain weights w. If the amount of training data from one domain D\u2081 is changed from Ni to N with the amount of training from other domains unchanged, we approximate the new model's evaluation loss after re-training with a power law function of N:\n$$L_P(A(N', w')) = (N_0 + N_i')^{-b_i} + c_i,$$\nwhere bi, ci are constants associated with domain i, N' = N + (N \u2212 Ni) denotes the updated amount of training data, and wi = N/N' denotes the updated domain weights. N0 estimates the evaluation loss when the amount of training data from domain i is zero (i.e., N = 0) and effectively measures the effect of data from all other domains. From this regard, N0 can be interpreted as the equivalent data size for training data from domains other than i. Notably, this formulation aligns with empirical findings in the prior literature [23, 6].\nWe propose the following procedure to fit the parameters in (2). We re-train two models with dif-ferent Ni and N and calculate their evaluation loss. Then, together with evaluation loss for the original model trained with Ni, the parameters bi, ci and N0 can be estimated via ordinary least square (OLS) fitting. The difference in evaluation loss compared to the original model is given as LP(A(N', w')) \u2212 LP(A(N, w)) = (N0 + N\u00bf)\u2212bi \u2212 (N0 + Ni)\u2212bi. Repeating this process and fitting the scaling functions for each domain, finally, we express the evaluation loss as a function of the amount of data from each domain as their summation: LP(A(N', w')) \u2212 LP(A(N, w)) =\n$\\sum_{i=1}^m [(N_0 + N_i')^{-b_i} \u2212 (N_0 + N_i)^{-b_i}]$ where N' = N +\u03a3(\u039d \u2013 Ni) and w = N/N'. Em-pirically, evaluation loss is shown to be well represented by such function form (Figure 5). This representation lends us an analytical form for the objective of the problem, which becomes\n$$w^* = \\underset{w' \\in \\mathcal{W}_m}{\\arg \\min} \\sum_{i=1}^m [(N_0 + N_i')^{-b_i} \u2212 (N_0 + N_i)^{-b_i}] = \\underset{w' \\in \\mathcal{W}_m}{\\arg \\min} \\sum_{i=1}^m (N_0 + w_i' \\cdot N')^{-b_i}.$$\nTo derive the final objective from the middle one, we first note that (N0 + N\u00bf)\u2212bi is independent of w'. Moreover, when we retrain model on the perturbed data sizes Ni, we explicitly constrain the total amount of training data to be the same as before, i.e., \u2211\u00bf N = N. Hence, (N0 + N\u00a5)\u2212bi =(N0 + w \u00b7 N')\u2212bi = (N0 + w \u00b7 N)\u2212bi. Since the objective is defined as the summation of convex functions, we end up with a convex optimization problem. With the constraint on the probability simplex and the objective being easily differentiable, the problem can be solved extremely efficiently"}, {"title": "3 Predicting Optimal Data Compositions for Larger Scales", "content": "While Section 2 provides an algorithmic framework to optimize the data composition at any scale, it is computationally expensive to directly perform optimization at a large target scale because it requires retraining models, which is only practical at a smaller scale. This section will investigate how to predict the optimal composition at a larger scale based on the composition optimized at smaller scales. In particular, we show that the optimal composition follows an exponential trend with respect to the scale, derived through a novel theoretical analysis and further justified through empirical observations."}, {"title": "3.1 Scaling Domain Weights", "content": "Recall that neural scaling laws give the relationship between evaluation loss and training data quantity as L = N\u2212\u03b3 + Lo where L is the evaluation loss (e.g., perplexity), Lo denotes some irreducible loss, and \u03b3 \u2265 0 are some constant. (Lo, \u03b3) can be fitted empirically. Consider a stylized case where the evaluation metric is aggregated loss over multiple independent tasks where each training sample will only contribute to a single task and the loss of each task only scales with the amount of training data contributing to this task as a power law function. Then, for a total of m tasks, the aggregated evaluation loss scales as the following $L = L_0 + \\sum_{i=1}^m \\beta_i \\cdot N_i^{-\\gamma_i}$, where Lo denotes some irreducible loss, Ni denotes the amount of data contributing to task i, and constants  \u03b2i \u2265 0  and  \u03b3i \u2265 0  are coefficients associated with task i. Define diagonal matrix N = diag{N1, N2,\u2026\u2026 Nm}. For a training data scale N = \u03a3i Ni, define compute-optimal data composition N = diag{N1\u2217 , N2\u2217 ,\u2026\u2026 Nm\u2217 } as the minimizer of L, given as N\u2217  = arg min\u2211 N\u2081=NL0 + \u03a3 =1 \u03b2\u03b9 \u00b7 \u039d . We propose the following theorem, which states the optimal data composition scales in exponential-style functions with the amount of training data and can be directly predictable from that of smaller scales."}, {"title": "4 Empirical Results", "content": "We showcase our proposed algorithms in two sets of empirical studies: Causal Language Modeling (CLM) in Section 4.2, and Masked Language Modeling (MLM) in Section 4.3. We train models with up to 10B tokens and report the number of steps saved to reach the same evaluation loss (perplexity). We also report downstream task performance to benchmark performance improvements after training the same number of steps."}, {"title": "4.1 Experimental setup", "content": "In Section 4.2, we pretrain 774M Decoder-only LMs (GPT-2 Large architecture [17]) from scratch on the RedPajama dataset [13]. RedPajama dataset is an open-source reproduction of the training data used for LLaMA-1/2 models [10], totaling 1.2T tokens from 7 data domains with proportions: Common Crawl (67%), C4 [26] (15%), GitHub (4.5%), Wikipedia (4.5%), ArXiv (2.5%), and StackExchange (2.0%). In Section 4.3, we pretrain 110M Encoder-only LMs (BERT-base architecture [27]) from scratch on data from 5 typical sources\u2014Amazon Reviews, Arxiv, Books, Wikipedia, and Open WebText Corpus [28]. Further details are in Appendix D.1 and D.2. Run-time and GPU hours are documented in Appendix D.7."}, {"title": "4.2 Causal Language Modeling with Decoder-only LMs (GPT)", "content": "Baselines In total, we report results for our methods (DDO and AutoScale) and 5 base-lines-Uniform, LLaMA weights (curated), DoReMi (LLaMA weights initialization), Data Mixing Laws from [6] and DoReMi from [1] (uniform initialization). Uniform weights uniformly sample data from all domains, resulting in the same number of training tokens from each domain. LLaMA weights are a set of curated domain weights heuristically tuned for training LLaMA-1/2 models. We implemented DoReMi proposed in [1]. DoReMi trains two smaller-scale auxiliary models (proxy models). First, a reference model is trained with the dataset's original domain weights, which are the LLaMA weights for RedPajama dataset. Then, optimized domain weights are obtained by using a proxy model to minimize the worst-case excess loss across different domains. We train both auxiliary models for 50K steps. Implementation details are available in Appendix D.3. Besides, we compare with 2 domain weights from existing literature, which are optimized on the same data domains RedPajama dataset with similar Decoder-only LMs. Data Mixing Laws [6] first performs a grid search on the space of possible data mixtures and records evaluation loss for proxy models trained on these mixtures. Then, the loss is interpolated with exponential functions to find the optimal domain weights for the proxy model. DOGE [5] also implements DoReMi [1] with auxiliary models trained for 50K steps but with the reference model trained with uniform weights. We evaluate the model trained on these domain weights to present a complete landscape.\nEvaluation We test the perplexity on the held-out dataset, comprising 10K samples each from the 7 domains. For downstream tasks, we include: BoolQ [29] (zero-shot), HellaSwag [30] (zero-shot, 10-shot), PIQA [31] (zero-shot), TruthfulQA [32] (zero-shot), PubMedQA [33] (10-shot), CrowsPairs [34] (25-shot), and ARC-Easy [35] (zero-shot). Additionally, BBH Novel Concepts [36] task is added to the aggregated results for models trained beyond 10B tokens, making a total of 9 tasks. We select tasks that ensure the model's performance surpasses random guessing, spanning from question answering and commonsense inference to bias identification and scientific problem solving. These tasks provide a comprehensive assessment of model performance [12, 37]. We adopt the evaluation framework from [38]. More details on downstream datasets are available in Appendix D.4.\nDirect Data Optimization (DDO): We conduct DDO Algorithm to optimize domain weights for proxy models (774M Decoder-only LMs) trained from scratch with 30M to 1.2B tokens. Takeaway 1a:, as depicted in Figure 3a, optimal domain weights for each training data scale are visibly different and demonstrate a clear shifting pattern. We found data sources with standard format such as Wikipedia and scientific papers, regarded as high quality, are most beneficial at smaller scales and observe sharp diminishing returns as the training data scales up. With more compute, data sources with diverse examples, such as CommonCrawl, continue to reduce training loss even at considerably large training"}, {"title": "4.3 Masked Language Modeling with Encoder-only LMs (BERT)", "content": "We examine the model's MLM loss on held-out datasets, comprising 10K samples each from the 5 training domains. Additionally, as an auxiliary evaluation, we also test the MLM loss on 3 non-training held-out domains. To be consistent with the perplexity loss used in CLM, we report the exponential cross-entropy loss for MLM. We evaluate the model's task performance on GLUE benchmark [19] (with 8 diverse tasks for natural language understanding (NLU)) and SQUAD [20] (a large-scale QA dataset). Evaluation details can be found in Appendix D.4. We examine the uniform weights as the baseline.\nDirect Data Optimization (DDO): We conduct DDO Algorithm to optimize domain weights for proxy models (110M Encoder-only LMs) trained from scratch with MLM on 1GB data. Results for DDO-optimized weights are shown in Figure 4. Takeaway 3a: DDO visibly decreased the model's validation loss on all training domains as well as held-out non-training domains, demonstrating its effectiveness in improving training efficiency and model utility. Takeaway 3b: when testing on GLUE benchmark and SQUAD dataset, consistent with the reduced evaluation loss, DDO-optimized weights are shown to improve the model's performance on downstream tasks by a notable margin.\nPredicting Optimal Weights at Larger Scales with AutoScale: With DDO-optimized weights from proxy models trained up to 0.5B tokens, we fit AutoScale predictor and use it to visualize how the optimal domain weights will shift as we continue scaling up training data. Takeaway 4a: as depicted in Figure 11, similar to the pattern described above, as the training data scale grows, data sources with diverse examples, such as WebText and Amazon Reviews, become increasingly important over standard domains, such as Wikipedia and Arxiv. One hypothesis is such data sources contain samples on diverse topics and language styles, providing rich information compared to domains with clean, standard text. We train models with MLM for up to 288k steps (which corresponds to 120% of the pertaining data size for BERT-base when first proposed in [18]). Takeaway 4b: table 2 shows that, compared to without reweighting (uniform weights), AutoScale-predicted weights speed up training by 16.7% on most data scales with an around 10% speedup on the largest scale, validating its consistent effectiveness. Takeaway 4c: nonetheless, the speedup is less impressive than in the results for Decoder-only LMs, demonstrating the different response to domain reweighting for models with different architecture or language modeling objectives. This is first hinted at in Figure 5, where the evaluation loss has a similar response to data from different domains, suggesting limited potential for performance improvements from domain reweighting."}, {"title": "5 Conclusions", "content": "In this work, we demonstrate that the optimal data composition for a fixed compute budget varies depending on the scale of the training data, showcasing that the common practice of empirically determining an optimal composition using small-scale experiments will not yield the optimal data mixtures when scaling up to the final model. Addressing this challenge, we propose AutoScale, an automated tool that finds a compute-optimal data composition for training at any desired target scale. In empirical studies with pre-training 774M Decoder-only LMs and Ecoder-only LMs, AutoScale decreases validation perplexity at least 25% faster than any baseline with up to 38% speed up compared to without reweighting, achieving the best overall performance across downstream tasks."}, {"title": "Limitations and Future Work", "content": "The promising results achieved by AutoScale in optimizing data composition for large-scale language model pretraining open up some intriguing avenues for future exploration. (1) Generalizability: It will be interesting to extend this research to larger-scale settings, other data modalities, and more comprehensive evaluation benchmarks, and re-examine the validity of insights provided by the experiments at the scale that we work on. (2) Direct optimization of downstream performance: In practice, the capabilities of LLMs are characterized by their performance on various downstream tasks, and the perplexity loss that we focused on in this study is only a rough, inaccurate proxy for downstream performance. It will be interesting to extend AutoScale to directly optimize downstream performance. (3) More fine-grained data curation: AutoScale works with fixed data domains and only optimizes how the domains are mixed together, confining the optimization"}, {"title": "Appendix A Extended Related Work", "content": "Extensive research shows that Neural Scaling laws, predicting how the model performance changes with the scale of training data, model parameters, and computation budget [22], to be impressively accurate in various tasks from vision and text processing [39] to LLM pre-training [15] and evaluations [37]. [40] proposes compute-optimal scaling for LLM pretraining data scales together with the model's parameter sizes. Yet, recent progress [41, 12] shows no sign of saturation in pre-training even for models pre-trained on a considerably larger data scale than recommended by [40]. [42] shows that data from different sources generally scale at different rates. Our work provides a novel perspective that materializes this dependency of scaling relationships with multiple data sources and at different data scales, achieving favorable empirical results.\nData Selection problems have been extensively studied for a variety of applications such as vision [43\u201346], speech [47, 48], and language models [43, 46, 49], and have been attracting growing interest over recent years. For LLMs, a line of research focuses on data selection for pre-training (also known as pre-training data curation) [50, 51, 40] from scratch or continued pre-training. For these settings, the scale of data selection budget ranges from millions to billions of samples. For example, [51] shows that continuing pre-training the model on the domain-specific dataset improves its performance on tasks of this domain; [52] uses importance resampling on simple bi-gram features with 10K bins to select millions of samples for domain/task adaptive pre-training. Problem-specific heuristic methods [53] employ simple criteria to distinguish data quality for a given language model on particular datasets. The effectiveness of these methods for data selection is often limited to specific use cases and easily fails when migrated to different problems [52]. More recently, [54] selects samples for fine-tuning pre-trained LLMs via gradients of Optimal Transport distance. [55] curates pre-training data using GPT-4 to rate and select samples based on a number of quality criteria; further, [56] uses pre-trained LLMs to re-write the entire training corpus to improves its quality for pre-training other LLMs. Pre-training data curation is also studied for multimodal foundation models (MMFM)\u2013e.g., [14] for vision-language models (VLMs), and [57, 8] for CLIP (Contrastive Language-Image Pretraining). Aside from pre-training LLMs, Domain Reweighting problems have been studied in research on collecting data for vision, audio, and text applications [58-61].\nBesides, Coresets [62, 63] aim to find a representative subset of samples to speed up the training process, which may be formulated as an optimization problem. This process is considerably com-putationally intensive and hard to be applied on a practical scale for language applications. Data valuation methods aim to measure the contribution of each sample to the model performance, which naturally provides a viable tool for data selection. Notable examples includes model-based approaches Shapley [64, 65], LOO [65, 66], and model-agnostic methods [67, 68]. Achieving fruitful results in their respective applications and providing valuable insights, though, these methods are commonly known for their scalability issues. Model-based approaches require repetitive model training and often struggle to apply to a few thousand samples. A recent example, [69] uses a sampling approach to speed up a Shapley-style method for selecting data for fine-tuning LLMs and scales up to selecting from 7.28k subsets. It is hardly imaginable to apply it to the scale of practical language datasets. [67] utilizes the gradients of an OT problem to provide an efficient measure of data values, yet the selection based on gradients does not necessarily align with the target distribution, resulting in mediocre performance in general cases."}, {"title": "Appendix B Operational Pipeline for Algorithms", "content": "Operational Pipeline (DDO)\n1. Train a base proxy model with uniform weights (or reference weights, if available);\n2. At each time, add/reduce data quantity for one domain and re-train the proxy model;\n3. Fit power law scaling functions and solve the optimization problem;\n4. Iterate the process if necessary.\nOperational Pipeline (AutoScale)"}, {"title": "Appendix C Proofs for Section 3", "content": ""}, {"title": "C.1 Theorem 1: Scaling Law for Optimal Data Compositions", "content": "Theorem 1 (Scaling Law for Optimal Data Compositions (restated)). For an evaluation loss that is aggregated over multiple independent tasks where each training sample will only contribute to a single task and the loss of each task only scales with the amount of training data contributing to this task as a power law function, given as  $L = \\sum_{i=1} \\beta_i \\cdot N_i^{-\\gamma_i}$ where Lo denotes some irreducible loss, Ni denotes the amount of data contributing to task i, and constants  \u03b2i \u2265 0  and  \u03b3i \u2265 0  are coefficients associated with task i. If we have optimal data compositions  N(1)\u2217 = diag{N(1)\u2217 1 , N (1)\u2217 2 , ... Nm)\u2217 } which minimizes L s.t.  \u03a3i N(1)\u2217 i  = N(1) and  N(2)\u2217 = diag{N(2)\u2217 1 , N (2)\u2217 2 , ... N(2)\u2217 m } minimizes L s.t.  \u03a3i N(2)\u2217 i  = N(2) where  N(1) != N(2) , then, optimal data compositions at other data scales  N(3)\u2217 = diag{N(3)\u2217 1 , N (3)\u2217 2 , ... N(3)\u2217 m } which minimizes L s.t.  \u03a3i N(3)\u2217 i   = N(3) where  N(3) != N(2) != N(1)  can be given as  N(3)\u2217 = N(2)\u2217 (N(1)\u2217 )\u22121N(2)\u2217 .\nProof. For the evaluation loss given as\n$L = \\sum_{i=1}^m \\beta_i \\cdot N_i^{-\\gamma_i}$"}, {"title": "Appendix DExperimental Details and Additional Results", "content": ""}, {"title": "D.1 Experimental Details of Section 4.2", "content": "Model Training GPT-2 Large is a variant of the GPT-2 architecture, featuring an embedding dimension of 1280, 36 transformer layers, and 20 attention heads. We rely on the Hugging Face Transformers library for implementation [70]. Specific training hyperparameters are detailed in Table 3.\nDataset Details The RedPajama dataset is available at: https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T. The 7 domains involved are characterized as follows:"}, {"title": "D.2 Experimental Details of Section 4.3", "content": "Model Training We employ the BERT-base-uncased model from the Hugging Face Transformers library. Originally, BERT's pretraining scheme involved MLM and next sentence prediction (NSP); however, in our experiments, we exclusively utilize MLM. Detailed training hyperparameters can be found in Table 4."}, {"title": "D.3 Implementation Details of Baselines", "content": "Implementation details We followed the official implementation\u00b3 of DoReMi for our experiments. We evaluated two sets of reference domain weights: (1) the domain weights utilized in the LLaMA-2 paper [10] (referred to as LLaMA weights), and (2) uniform weights. Both the reference and proxy models have 120M parameters and are trained from scratch. We use GPT-2 tokenizer with a vocabulary size of roughly 50K. For LLaMA weights, we train each model for 20K, 50K and 200K steps for comparison. For uniform weights, we train each model for 10K, 20K and 50K steps. Refer to Table 5 for detailed hyperparameters. The effect of reference weights on the output DoReMi is discussed in Figure10."}, {"title": "D.4 Evaluation Details", "content": "GPT/CLM The following tasks are considered for downstream performance evaluation, in line with the setup from [12, 37]. For few-shot tasks, the demonstrations are sampled at random."}, {"title": "D.5 Additional Results of Section 4.2", "content": "Figure 8 exhibits that AutoScale-predicted weights decreases val loss at least 25% faster than any baseline with up to 37% speed up.\nFigure 9 provides a visualization of domain weights used for training GPT-2 Large, given by different methods.\nTable 7 examines the domain-specific perplexity of GPT-2 Large trained on 3 billion tokens, re-spectively. Notably, AutoScale achieves the lowest average validation perplexity and significantly reduces the perplexity in the worst-performing domains."}, {"title": "D.6 Additional Results of Section 4.3", "content": "Figure 11 depicts the AutoScale-predicted domain weights for training BERT. It is evident that optimal data quantity for each domain grows in exponential-style functions with training data scale where data sources with diverse samples (e.g., WebText) are upweighted relative to domains with standard format (e.g., ArXiv)."}, {"title": "D.7 Runtime Analysis", "content": "Training a GPT-2 large model from scratch for 3B tokens requires 15.5 hours on 8x NVIDIA A100 40GB SXM GPUs or 9 hours on 8x NVIDIA H100 80GB GPUs. Training time increases linearly with the number of training tokens on both types of GPUs.\nTraining BERT-base models takes 2 hours for every 18k steps on 4x NVIDIA A6000 48GB GPUs. Computational time grows linearly with the number of training steps."}]}