{"title": "AutoScale-Automatic Prediction of Compute-optimal Data Composition for Training LLMs", "authors": ["Feiyang Kang", "Yifan Sun", "Bingbing Wen", "Si Chen", "Dawn Song", "Rafid Mahmood", "Ruoxi Jia"], "abstract": "To ensure performance on a diverse set of downstream tasks, LLMs are pretrained via data mixtures over different domains. In this work, we demonstrate that the optimal data composition for a fixed compute budget varies depending on the scale of the training data, suggesting that the common practice of empirically determining an optimal composition using small-scale experiments will not yield the optimal data mixtures when scaling up to the final model. To address this challenge, we propose AutoScale, an automated tool that finds a compute-optimal data composition for training at any desired target scale. AutoScale first determines the optimal composition at a small scale using a novel bi-level optimization framework, Direct Data Optimization (DDO), and then fits a predictor to estimate the optimal composition at larger scales. The predictor's design is inspired by our theoretical analysis of scaling laws related to data composition, which could be of independent interest. In empirical studies with pre-training 774M Decoder-only LMs (GPT-2 Large) on RedPajama dataset, AutoScale decreases validation perplexity at least 25% faster than any baseline with up to 38% speed up compared to without reweighting, achieving the best overall performance across downstream tasks. On pre-training Encoder-only LMs (BERT) with masked language modeling, compared with without reweighting, DDO is shown to decrease loss on all domains while visibly improving average task performance on GLUE benchmark by 8.7% and on large-scale QA dataset (SQUAD) by 5.9%, where AutoScale further speeds up training by up to 28%. Our codes are open-sourced\u00b2.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) are pre-trained using data from different sources or domains. Given the limited compute available for pre-training, it is necessary to strategically curate and mix training data from these sources. An emerging line of research strives to tackle this problem with domain reweighting, i.e., adjusting the relative proportion of data from different data sources [1-6]. Nonetheless, determining the optimal data composition is challenging.\n\nA majority of these works aim to first optimize data composition for a smaller proxy model and at a smaller data scale ([1, 5, 6]). Yet, this optimization is often conducted with alternative objectives not always aligned with the original objectives of minimizing evaluation loss. DoReMi [1] first trains a small reference model, and then trains a second proxy model with GroupDRO [7] to minimize the excessive domain loss relative to the reference model, where the domain weights of the proxy model will be the output. DOGE [5] trains a proxy model while tracking the first-order gradient of the model on evaluation domains (i.e., data influence) and optimizes domain weights based on the gradients, relying on infinitesimal approximations which may or may not be accurate for models trained with a practical learning rate. Data Mixing Laws [6] trains a number of proxy models to run a coarse grid search on the space of data mixtures and interpolate their performance with exponential functions to find the minimum. These methods often rely on ad-hoc hyperparameter tuning via trial and error. Further, the optimized weights are directly applied to training the target model on magnitudes of larger data scales. This implicitly poses a strong assumption that the \"optimal data composition\" is invariant of model sizes or data scales. Yet, optimal data composition is likely to shift with data size. Optimal curation at a smaller scale may not remain optimal at the target scale [8]. We refer to [9] and Appendix A for related works."}, {"title": "2 Compute-optimal Training Data Compositions", "content": "Consider training an LLM on a data composition from multiple sources/domains. Proportions of data for each domain are referred to as \u201cdomain weights\". The goal is to find an optimal training data composition such that, for a given compute budget (i.e., training data size), the reduction in evaluation loss is maximized. For language models, the most commonly used evaluation metric is perplexity (PPL), defined as the exponentiation of cross-entropy loss. In information theory, reduction in perplexity is considered to measure the amount of information gain. Thus, the objective is to maximize training efficiency by finding \"compute-optimal\" domain weights for the training data. This setup has been adopted in this line of research [1, 3, 5] and we will also follow in this work. We first formulate this as a bi-level optimization problem and then introduce an efficient solution approach to solve it."}, {"title": "2.1 Problem Formulation", "content": "Consider training an LLM on a data composition $S$ from $m$ domains, $D_1, D_2, \u2026, D_m$. Let $S = {S_1, S_2,..., S_m}$ denote the training dataset where $S_i$ is the subset of training data from each domain. The domain weights $w = [w_1,w_2,\u2026\u2026, w_m]$ are defined as the proportions of data for each domain. Namely, letting $N = |S|$ denote the amount of total tokens of training data, domain weights are given as $w_i = N_i/N$, where $|S_i|$ denotes the amount of tokens for training subset $S_i$. Let $S(N, w)$ denote the dataset of $N$ tokens composed from different domains using the weights $w$. Let $A(S)$ denote a learning algorithm (i.e., the model) parameterized by $\\theta$ trained on data $S$ with empirical risk minimization (ERM), given as $A(S) := \\arg \\min_{\\theta} L(\\theta, S)$ where $L(\\cdot, \\cdot)$ denotes the loss function used in training. With slight abuse of notation, we use $A(N, w)$ as shorthand for $A(S(N, w))$. We would like to maximize the amount of information gain and achieve maximum loss reduction during training, given as $ \\min_{w \\in \\mathcal{W}_m} L_P(A(N, w)) = \\sum_{i=1}^{m}L^i(A(N, w), D_i)$, where $L_P(\\cdot)$ and $L^i(\\cdot)$ denote total evaluation loss and the loss of the model evaluated on the validation data of individual domains, respectively; the space of weights $\\mathcal{W}_m$ is the hyperplane of the probability simplex $\\mathcal{W}_m = {W | W_1 + W_2 + \u2026 + W_m = 1} \\cap {w | 0 \\leq w_i \\leq 1,\\forall i \\in {1,2,\u2026\u2026, m}}$. Then, the optimal domain weights, $w^*$, are given as the minimizer of the objective,\n\n$W^* = arg min_{w\\in \\mathcal{W}_m} \\sum_{i=1}^{m}L^i(A(N, w), D_i)  s.t. A(N, w) = arg min_{\\theta} L(\\theta, S(N, w))$"}, {"title": "2.2 A Practical Solution via Scaling-law-inspired Approximation", "content": "Re-training LLMs is usually prohibitively expensive. The standard practice is to optimize the data composition with proxy models where re-training is plausible, and then optimized domain weights are used to train models at full scale [1, 5, 6]. The proxy model often has a similar architecture to the target model but is smaller in parameter size and/or trained with much less data. The assumption is the performance achieved on the proxy model could provide an effective indicator for performance on the target model. Even for proxy models, solving this bi-level optimization problem via gradient methods can still be expensive, which necessitates a trade-off between algorithmic efficiency and solution quality. Current work [1, 5, 6] mostly employs heuristic methods to conduct this optimization and achieves varying results, rendering the users hard to tell when they will work or fail. Instead, we provide a global approximation to this problem, which allows finding the global optimum in a single step with high precision. This enables achieving consistent results and reliable performance improvements robust to different use cases.\n\nNeural scaling laws suggest the relationship between a model's evaluation loss and the size of its training data can be well-represented by power law functions [22] $L_P(A(N,w)) = N^{-\\gamma} + L_0$ where constants $L_0$ denotes some irreducible loss and $\\gamma \\geq 0$ is some scaling coefficient. Drawing inspirations from [23], we propose the following approximation. Consider a model trained on data with size N and domain weights w. If the amount of training data from one domain $D_i$ is changed from $N_i$ to $N_i'$ with the amount of training from other domains unchanged, we approximate the new model's evaluation loss after re-training with a power law function of $N_i'$:\n\n$L_P(A(N', w')) = (N_0 + N_i'^{\\gamma})^{-b_i} + c_i,$\n\nwhere $b_i, c_i$ are constants associated with domain i, $N' = N + (N_i' \u2013 N_i)$ denotes the updated amount of training data, and $w'_i = N'_i/N'$ denotes the updated domain weights. $N_0$ estimates the evaluation loss when the amount of training data from domain i is zero (i.e., $N'_i = 0$) and effectively measures the effect of data from all other domains. From this regard, $N_0$ can be interpreted as the equivalent data size for training data from domains other than i. Notably, this formulation aligns with empirical findings in the prior literature [23, 6].\n\nWe propose the following procedure to fit the parameters in (2). We re-train two models with different $N_i$ and $N_i'$ and calculate their evaluation loss. Then, together with evaluation loss for the original model trained with $N_i$, the parameters $b_i, c_i$ and $N_0$ can be estimated via ordinary least square (OLS) fitting. The difference in evaluation loss compared to the original model is given as $L_P(A(N', w')) \u2013 L_P(A(N, w)) = (N_0 + N_i'^{\\gamma})^{-b_i} \u2013 (N_0 + N_i^{\\gamma})^{-b_i}$. Repeating this process and fitting the scaling functions for each domain, finally, we express the evaluation loss as a function of the amount of data from each domain as their summation: $L_P(A(N', w')) \u2013 L_P(A(N, w)) = \\sum_{i=1}^{m} [(N_0 + N_i'^{\\gamma})^{-b_i} \u2013 (N_0 + N_i^{\\gamma})^{-b_i}]$ where $N' = N + \\sum_i (N'_i \u2013 N_i)$ and $w'_i = N'_i/N'$. Empirically, evaluation loss is shown to be well represented by such function form (Figure 5). This representation lends us an analytical form for the objective of the problem, which becomes\n\n$W^* = arg min_{w' \\in \\mathcal{W}_m}  \\sum_{i=1}^{m}[(N_0 + N_i'^{\\gamma})^{-b_i} \u2013 (N_0 + N_i^{\\gamma})^{-b_i}] = arg min_{w' \\in \\mathcal{W}_m}  \\sum_{i=1}^{m} (N_0 + w_i'  N)^{-\\gamma b_i}.$\n\nTo derive the final objective from the middle one, we first note that $(N_0 + N_i^{\\gamma})^{-b_i}$ is independent of $w'$. Moreover, when we retrain model on the perturbed data sizes $N'$, we explicitly constrain the total amount of training data to be the same as before, i.e., $\\sum_{i=1}^{m} N'_i = N$. Hence, $(N_0 + N_i^{\\gamma})^{-b_i} = (N_0 + w_i'  N^{\\gamma})^{-b_i} = (N_0 + w'_i  N)^{-b_i}$. Since the objective is defined as the summation of convex functions, we end up with a convex optimization problem. With the constraint on the probability simplex and the objective being easily differentiable, the problem can be solved extremely efficiently"}, {"title": "3 Predicting Optimal Data Compositions for Larger Scales", "content": "While Section 2 provides an algorithmic framework to optimize the data composition at any scale, it is computationally expensive to directly perform optimization at a large target scale because it requires retraining models, which is only practical at a smaller scale. This section will investigate how to predict the optimal composition at a larger scale based on the composition optimized at smaller scales. In particular, we show that the optimal composition follows an exponential trend with respect to the scale, derived through a novel theoretical analysis and further justified through empirical observations."}, {"title": "3.1 Scaling Domain Weights", "content": "Recall that neural scaling laws give the relationship between evaluation loss and training data quantity as $L= N^{-\\gamma}+ L_0$ where $L$ is the evaluation loss (e.g., perplexity), $L_0$ denotes some irreducible loss, and $\\gamma \\geq 0$ are some constant. ($L_0$, $\\gamma$) can be fitted empirically. Consider a stylized case where the evaluation metric is aggregated loss over multiple independent tasks where each training sample will only contribute to a single task and the loss of each task only scales with the amount of training data contributing to this task as a power law function. Then, for a total of m tasks, the aggregated evaluation loss scales as the following $L = L_o + \\sum_{i=1}^{m} \\beta_i \\cdot N_{i}^{-\\gamma_i}$, where $L_o$ denotes some irreducible loss, $N_i$ denotes the amount of data contributing to task i, and constants $\\beta_i \\geq 0$ and $\\gamma_i \\geq 0$ are coefficients associated with task i. Define diagonal matrix $N = diag{N_1, N_2,\u2026\u2026 N_m}$. For a training data scale $N = \\sum_i N_i$, define compute-optimal data composition $N^* = diag{N_1^*,N_2^*,\u2026\u2026 N_m^*}$ as the minimizer of L, given as $N^* = arg min_{\\sum N_i =N} L_o + \\sum_{i=1}^{m} \\beta_i \\cdot N_{i}^{-\\gamma_i}$. We propose the following theorem, which states the optimal data composition scales in exponential-style functions with the amount of training data and can be directly predictable from that of smaller scales."}, {"title": "3.2 AutoScale: Automatic Prediction of Optimal Training Data at Larger Scales", "content": "We conclude the section by presenting a novel tool-AutoScale, which automatically predicts optimal training data compositions at larger scales."}, {"title": "4 Empirical Results", "content": "We showcase our proposed algorithms in two sets of empirical studies: Causal Language Modeling (CLM) in Section 4.2, and Masked Language Modeling (MLM) in Section 4.3. We train models with up to 10B tokens and report the number of steps saved to reach the same evaluation loss (perplexity). We also report downstream task performance to benchmark performance improvements after training the same number of steps."}, {"title": "4.1 Experimental setup", "content": "In Section 4.2, we pretrain 774M Decoder-only LMs (GPT-2 Large architecture [17]) from scratch on the RedPajama dataset [13]. RedPajama dataset is an open-source reproduction of the training data used for LLaMA-1/2 models [10], totaling 1.2T tokens from 7 data domains with proportions: Common Crawl (67%), C4 [26] (15%), GitHub (4.5%), Wikipedia (4.5%), ArXiv (2.5%), and StackExchange (2.0%). In Section 4.3, we pretrain 110M Encoder-only LMs (BERT-base architecture [27]) from scratch on data from 5 typical sources\u2014Amazon Reviews, Arxiv, Books, Wikipedia, and Open WebText Corpus [28]. Further details are in Appendix D.1 and D.2. Run-time and GPU hours are documented in Appendix D.7."}, {"title": "4.2 Causal Language Modeling with Decoder-only LMs (GPT)", "content": "Baselines In total, we report results for our methods (DDO and AutoScale) and 5 baselines-Uniform, LLaMA weights (curated), DoReMi (LLaMA weights initialization), Data Mixing Laws from [6] and DoReMi from [1] (uniform initialization). Uniform weights uniformly sample data from all domains, resulting in the same number of training tokens from each domain. LLaMA weights are a set of curated domain weights heuristically tuned for training LLaMA-1/2 models. We implemented DoReMi proposed in [1]. DoReMi trains two smaller-scale auxiliary models (proxy models). First, a reference model is trained with the dataset's original domain weights, which are the LLaMA weights for RedPajama dataset. Then, optimized domain weights are obtained by using a proxy model to minimize the worst-case excess loss across different domains. We train both auxiliary models for 50K steps. Implementation details are available in Appendix D.3. Besides, we compare with 2 domain weights from existing literature, which are optimized on the same data domains RedPajama dataset with similar Decoder-only LMs. Data Mixing Laws [6] first performs a grid search on the space of possible data mixtures and records evaluation loss for proxy models trained on these mixtures. Then, the loss is interpolated with exponential functions to find the optimal domain weights for the proxy model. DOGE [5] also implements DoReMi [1] with auxiliary models trained for 50K steps but with the reference model trained with uniform weights. We evaluate the model trained on these domain weights to present a complete landscape.\n\nEvaluation We test the perplexity on the held-out dataset, comprising 10K samples each from the 7 domains. For downstream tasks, we include: BoolQ [29] (zero-shot), HellaSwag [30] (zero-shot, 10-shot), PIQA [31] (zero-shot), TruthfulQA [32] (zero-shot), PubMedQA [33] (10-shot), CrowsPairs [34] (25-shot), and ARC-Easy [35] (zero-shot). Additionally, BBH Novel Concepts [36] task is added to the aggregated results for models trained beyond 10B tokens, making a total of 9 tasks. We select tasks that ensure the model's performance surpasses random guessing, spanning from question answering and commonsense inference to bias identification and scientific problem solving. These tasks provide a comprehensive assessment of model performance [12, 37]. We adopt the evaluation framework from [38]. More details on downstream datasets are available in Appendix D.4.\n\nDirect Data Optimization (DDO): We conduct DDO Algorithm to optimize domain weights for proxy models (774M Decoder-only LMs) trained from scratch with 30M to 1.2B tokens. Takeaway 1a:, as depicted in Figure 3a, optimal domain weights for each training data scale are visibly different and demonstrate a clear shifting pattern. We found data sources with standard format such as Wikipedia and scientific papers, regarded as high quality, are most beneficial at smaller scales and observe sharp diminishing returns as the training data scales up. With more compute, data sources with diverse examples, such as CommonCrawl, continue to reduce training loss even at considerably large training"}, {"title": "4.3 Masked Language Modeling with Encoder-only LMs (BERT)", "content": "We examine the model's MLM loss on held-out datasets, comprising 10K samples each from the 5 training domains. Additionally, as an auxiliary evaluation, we also test the MLM loss on 3 non-training held-out domains. To be consistent with the perplexity loss used in CLM, we report the exponential cross-entropy loss for MLM. We evaluate the model's task performance on GLUE benchmark [19] (with 8 diverse tasks for natural language understanding (NLU)) and SQUAD [20] (a large-scale QA dataset). Evaluation details can be found in Appendix D.4. We examine the uniform weights as the baseline.\n\nDirect Data Optimization (DDO): We conduct DDO Algorithm to optimize domain weights for proxy models (110M Encoder-only LMs) trained from scratch with MLM on 1GB data. Results for DDO-optimized weights are shown in Figure 4. Takeaway 3a: DDO visibly decreased the model's validation loss on all training domains as well as held-out non-training domains, demonstrating its effectiveness in improving training efficiency and model utility. Takeaway 3b: when testing on GLUE benchmark and SQUAD dataset, consistent with the reduced evaluation loss, DDO-optimized weights are shown to improve the model's performance on downstream tasks by a notable margin.\n\nPredicting Optimal Weights at Larger Scales with AutoScale: With DDO-optimized weights from proxy models trained up to 0.5B tokens, we fit AutoScale predictor and use it to visualize how the optimal domain weights will shift as we continue scaling up training data. Takeaway 4a: as depicted in Figure 11, similar to the pattern described above, as the training data scale grows, data sources with diverse examples, such as WebText and Amazon Reviews, become increasingly important over standard domains, such as Wikipedia and Arxiv. One hypothesis is such data sources contain samples on diverse topics and language styles, providing rich information compared to domains with clean, standard text. We train models with MLM for up to 288k steps (which corresponds to 120% of the pertaining data size for BERT-base when first proposed in [18]). Takeaway 4b: table 2 shows that, compared to without reweighting (uniform weights), AutoScale-predicted weights speed up training by 16.7% on most data scales with an around 10% speedup on the largest scale, validating its consistent effectiveness. Takeaway 4c: nonetheless, the speedup is less impressive than in the results for Decoder-only LMs, demonstrating the different response to domain reweighting for models with different architecture or language modeling objectives. This is first hinted at in Figure 5, where the evaluation loss has a similar response to data from different domains, suggesting limited potential for performance improvements from domain reweighting."}, {"title": "5 Conclusions", "content": "In this work, we demonstrate that the optimal data composition for a fixed compute budget varies depending on the scale of the training data, showcasing that the common practice of empirically determining an optimal composition using small-scale experiments will not yield the optimal data mixtures when scaling up to the final model. Addressing this challenge, we propose AutoScale, an automated tool that finds a compute-optimal data composition for training at any desired target scale. In empirical studies with pre-training 774M Decoder-only LMs and Ecoder-only LMs, AutoScale decreases validation perplexity at least 25% faster than any baseline with up to 38% speed up compared to without reweighting, achieving the best overall performance across downstream tasks."}, {"title": "Limitations and Future Work", "content": "The promising results achieved by AutoScale in optimizing data composition for large-scale language model pretraining open up some intriguing avenues for future exploration. (1) Generalizability: It will be interesting to extend this research to larger-scale settings, other data modalities, and more comprehensive evaluation benchmarks, and re-examine the validity of insights provided by the experiments at the scale that we work on. (2) Direct optimization of downstream performance: In practice, the capabilities of LLMs are characterized by their performance on various downstream tasks, and the perplexity loss that we focused on in this study is only a rough, inaccurate proxy for downstream performance. It will be interesting to extend AutoScale to directly optimize downstream performance. (3) More fine-grained data curation: AutoScale works with fixed data domains and only optimizes how the domains are mixed together, confining the optimization"}]}