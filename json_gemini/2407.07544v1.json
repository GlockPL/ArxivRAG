{"title": "Disentangling Masked Autoencoders for Unsupervised Domain Generalization", "authors": ["An Zhang", "Han Wang", "Xiang Wang", "Tat-Seng Chua"], "abstract": "Domain Generalization (DG), designed to enhance out-of- distribution (OOD) generalization, is all about learning invariance against domain shifts utilizing sufficient supervision signals. Yet, the scarcity of such labeled data has led to the rise of unsupervised domain generalization (UDG) a more important yet challenging task in that models are trained across diverse domains in an unsupervised manner and eventually tested on unseen domains. UDG is fast gaining attention but is still far from well-studied.\nTo close the research gap, we propose a novel learning framework designed for UDG, termed the Disentangled Masked AutoEncoder (DisMAE), aiming to discover the disentangled representations that faithfully reveal the intrinsic features and superficial variations without access to the class label. At its core is the distillation of domain-invariant semantic features, which can not be distinguished by domain classifier, while filtering out the domain-specific variations (for example, color schemes and texture patterns) that are unstable and redundant. Notably, DisMAE co-trains the asymmetric dual-branch architecture with semantic and lightweight variation encoders, offering dynamic data manipulation and representation level augmentation capabilities. Extensive experiments on four benchmark datasets (i.e. DomainNet, PACS, VLCS, Colored MNIST) with both DG and UDG tasks demonstrate that DisMAE can achieve competitive OOD performance compared with the state-of-the-art DG and UDG baselines, which shed light on potential research line in improving the generalization ability with large-scale unlabeled data. Our codes are available at https://github.com/rookiehb/DisMAE.", "sections": [{"title": "1 Introduction", "content": "Domain Generalization (DG) strives to achieve robustness against domain shifts by leveraging high-quality supervision signals [46,55,56]. However, data is never\nsufficient for today's large foundation models, and the demands for large-scale labeled source data considerably hinder the breakthrough toward truly gener- alized models [47]. Thus, we delve into a more practical yet challenging task Unsupervised Domain Generalization (UDG) [60]. Distinct from DG, UDG focuses on learning representations that generalize well from source domains to unseen domains without relying on class labels.\nWe argue that the transition from DG to UDG is non-trivial. Cutting-edge DG methods, which have shown great success in supervised data, are often ineffective in the UDG task [21]. These methods largely depend on inductive biases derived from available labels [7], leading to suboptimal performance com- pared to self-supervised learning frameworks such as MoCo [14], MAE [23], and SimCLR [12] without supervision. Notably, models trained under UDG using unlabeled heterogeneous data can be easily repurposed for DG tasks by assem- bling classification loss (as elaborated in the methodology section). Given these observations, our study emphasizes UDG's potential, a research direction that remains largely under-explored.\nIn light of these challenges and opportunities of UDG, we aim to distill domain- invariant semantic features that faithfully reflect intrinsic properties of the data, ensuring generalization to unseen distributions in an unsupervised manner. A notable challenge is the entanglement of these features with domain-specific variations such as color schemes and texture patterns [50]. Recent advances in DG research hint at a promising direction: the decoupling of domain-invariant attributes from these peripheral variation factors [6,8,49]. However, with no access to the class label, ensuring domain-invariant features truly encapsulate inherent data attributes poses great challenges [3,58]. Furthermore, in some real-world scenarios, distribution shifts among source domains overshadow intra-domain class differences [59]. For instance, the transformation from a photo to a sketch of a Teddy manifests more significant changes than transitioning to a Bichon. This strong heterogeneity often results in representations emphasizing broad distributional shifts over nuanced domain-specific variations.\nToward this end, we propose a new learning framework designed for UDG, Disentangled Masked AutoEncoder (DisMAE), that integrates invariance and disentanglement principles. By the disentanglement principle, we mean that the representations are decomposed into two components - semantic features and variations. To force the semantic representations to capture less redundant infor- mation and thus preferably disentangle the variations, we pull the reconstructed samples with their original semantics and variations closer to the input and push the reconstructed samples with different in-domain variations apart. By invariance principle, we mean that the semantic representations are invariant throughout a variety of domain changes, therefore their domain category is undis- tinguished. Put simply, variations should encapsulate domain-specific features, leaving the disentangled semantic representations to exclusively preserve intrinsic information.\nGuided by these two principles, our DisMAE strategy incorporates four mod- ules: a transformer-based semantic encoder, a lightweight variation encoder, a transformer-based decoder, and a domain label-enhanced invariance classifier. For the reconstruction part, the asymmetric dual-branch architecture with two separate encoders and one single decoder is implemented together to avoid in- formation collapse. We additionally introduce an adaptive contrastive loss to keep two branches capturing information in a principled disentangled manner. Specifically, towards the disentanglement principle, target semantic representa- tions are concatenated with other intra-domain samples' variations, ensuring the reconstructed samples diverge from the original input. Towards the invariance principle, the domain label-enhanced invariance classifier predicts the probability of correctly identifying the domain category, and these probabilities are then inversely applied as the adaptive weights to re-weight the contrastive loss. Jointly training under these two principles enables the DisMAE models to disentangle the domain-invariant semantic features and domain-specific variations, and further boosts the model's capability in enriching the variation through latent space augmentation."}, {"title": "2 Preliminary", "content": "The goal of unsupervised domain generalization (UDG) is to learn a domain- invariant feature extractor from diverse domains in a self-supervised manner, thereby enabling robust generalization to the target unseen domain [54]. Let \\(D_{train} = \\{x \\in I_{d} | d \\in \\{1,......, M - 1\\}\\} \\) be the training set that involves M \u2013 1\nsource domains. Each training domain \\(I_{d}\\) comprises a collection of unlabeled images \\{x\\}. Let \\(D_{test} = \\{x \\in I_{M}\\} \\) represent the test set containing previously unseen images from the target domain \\(I_{M}\\).\nIn order to learn a domain-invariant feature extractor \\(\\phi_{s}\\) in self-supervised learning, there is a foundation assumption [48]: any given sample x is generated from two disentangled features, \\(x = g(s_{x}, v_{x})\\), where \\(s_{x}\\) denotes the semantic rep- resentation that invariant to domain shifts, \\(v_{x}\\) signifies the superficial variations that change across domains. The generative function g(\u00b7, \u00b7) maps these two latent space representations back into the sample space. In particular, this assumption naturally encodes two principles: disentanglement and invariance principles.\nDisentanglement Principle. There exists a direct product decomposition of attributes as well as feature representations in real-world visual generation sce- narios.\nLet \\(s_{x}\\) be the semantic attribute of sample x, and \\(v_{x}\\) denote its vari- ations, e.g. for an image in the Col- ored MNIST dataset [2], its semantic attribute \\(s_{x}\\) signifies \"digit\", while the variation attributes \\(v_{x}\\) is \"color\". There exists a direct product decomposition of attributes that could control each sample x's generation: \\(S_{x} X U_{x}\\), e.g. gen- erating a digit \"5\" with \"green\" color (see examples in Figure 1). Thus, this decomposition implies a disentangled representation learning, \\(s \\times \\phi_{v}\\), that maps image pixels to a representation space. Here, \\(s_{x} := \\phi_{s}(x)\\) captures the semantic aspects, and \\(v_{x} := \\phi_{v}(x)\\) en- capsulates the variations of sample x.\nThe disentanglement principle is inherent in a desirable property. Mod- ifying attributes is essentially akin to directly altering representations in the feature space. For instance, consider the scenario illustrated in Figure 1, where transforming a digit \"5\" from be- ing \"green\" to \"purple\" in the attributes space corresponds to the alteration of \"5\" x \"green\" to \"5\"\u00d7\"purple\". This change can be readily accomplished by substituting the variation representation \\(\\phi_{v}(x)\\). Thus, through the lens of the disentanglement principle, we highlight the potential to diversify variations using representation-level data augmentation.\nInvariance Principle. The ideal UDG model refines domain-invariant repre- sentations that causally determine the intrinsic attributes, regardless of changes in domains."}, {"title": "3 Methodology", "content": "We now introduce our Disentangled Masked Autoencoder (DisMAE), designed to derive decomposed high-level representations for UDG, building on the scalable self-supervised MAE framework [23]. Grounded in disentanglement and invariance principles, our approach integrates reconstruction loss and adaptive contrastive loss in a collaborative manner to guide the learning process. An overview of DisMAE's framework is provided in Figure 3.\nConsidering a multi-domain reconstruction task with the training set \\(D_{train} = \\{(x_{i}, d_{i})\\}_{i=1}^{N}\\), where \\(x_{i} \\in I_{d_{i}}\\) denotes the input image, \\(d_{i}\\) signifies the correspond- ing domain category, and N represents the number of training data. DisMAE develops an asymmetric dual-branch architecture, visualized in Figure 3, with a main branch mapping the input image into the semantic feature space i.e. semantic encoder \\(\\phi_{s}(.)\\), along with a lightweight branch that extracts the input sample's variation attributes i.e. variations encoder \\(\\phi_{v}(.)\\). In alignment with MAE [23], we denote the masking mechanism as the function M and the input visible patches as \\(M(x_{i})\\) for the original image \\(x_{i}\\). Our approach utilizes two transformer-based encoders [16] to intensify the domain-invariant semantics and domain-specific variations as representations:\n\\((s_{i}, v_{i}) = (\\phi_{s}(M(x_{i})), \\phi_{v}(M(x_{i}))).\\)   (1)\nConsequently, the domain-invariant semantic representation \\(s_{i}\\) and domain- specific variation representation \\(v_{i}\\) are to estimate the attributes \\(s_{x_{i}}\\) and \\(v_{x_{i}}\\) in feature space, respectively. We denote the [cls] patch embedding of \\(s_{i}\\) and \\(v_{i}\\) as \\(\\bar{s}\\) and \\(\\bar{v}\\), where\\(\\bar{s}, \\bar{v} \\in \\mathbb{R}^{B \\times H}\\) and B is the batch size and H is the hidden size. Their combination can parameterize image \\(x_{i}\\) as:\n\\(x_{ii} = \\bar{s}_{i}||\\bar{v}_{i},\\)   (2)\nwhere || symbolizes the concatenation operation. Here, we utilize \\(\\bar{v}\\) since we only need overall information about variation features.\nIn addition to the encoders, we utilize a transformer-based decoder, denoted as g(\u00b7, \u00b7), mirroring the architecture of the MAE decoder. This enables the reconstruction \\(x'_{ii} = g(\\bar{s}_{i}, \\bar{v}_{i})\\) of \\(x_{i}\\). Importantly, the decoder is operational only during training for reconstruction. During the UDG task's fine-tuning phase, solely the main branch, namely the semantic encoder \\(\\phi_{s}\\), is deployed.\nFor the effective optimization of both encoders and the decoder, the preva- lent reconstruction learning strategy is employed. Specifically, the risk function measures the quality of reconstructed pixel values, which can be formulated as the mean squared error (MSE) or y-constrained reconstruction loss [58] between the masked patches and the corresponding masked patches for the original image \\(x_{i}\\). Here we apply y-constrained reconstruction loss, which is defined as:\n\\(L_{rec} = \\frac{1}{N} \\sum_{i=1}^{N} max\\{\\||x_{i} - g(\\bar{s}_{i}, \\bar{v}_{i})||_{L2} - \\gamma, 0\\},\\)   (3)\nwhere \u03b3 > 0 serves as a hyperparameter, indicating the bound of the allowable reconstruction error. However, solely minimizing the risks over the original data distribution fails to model domain-invariant information and suffers from poor domain generalization."}, {"title": "3.2 Implementation of Two Principles", "content": "To bring forth better generalizations w.r.t. domain shift, we advocate for dis- entanglement and invariance principles. To parameterize the disentanglement principle, we devise an augmentation operator on semantic and variation represen- tations, which preserves the estimated domain-invariant intrinsic attributes but intervenes in the estimated domain-specific information. Formally, the operator samples in-batch variation representations \\(\\bar{v}\\) from distinct sample \\(x_{j}\\) to replace \\(\\bar{v}_{i}\\) of \\(x_{i}\\) and later combine it with \\(\\bar{s}_{i}\\) to generate the augmented image \\(x'_{ij}\\). The augmentation process is:\n\\(x_{ij} = \\bar{s}_{i}||\\bar{v}_{j}, x'_{ij} = g(\\bar{s}_{i}, \\bar{v}_{j}).\\)   (4)\nHaving established the augmented representations, we enforce the recon- structed sample with its original semantics and variations closer to the input sample while concurrently pushing the reconstructed samples possessing distinct intra-domain variations away from it. This can be captured by a contrastive loss between the original image \\(x_{i}\\) and its reconstruction \\(x'_{ii}\\):\n\\(l(x_{i}, x'_{ii}) = -log \\frac{exp(s(x_{i}, x'_{ii}) / \\tau)}{\\sum_{j \\in I_{d}\\{i\\}} exp(s(x_{i}, x'_{ij}) / \\tau)},\\)   (5)\nwhere \\(\\tau\\) is the temperature hyperparameter in the contrastive loss [35], and s(\u00b7, \u00b7) is a similarity function quantifying the distance between the reconstructed and original images on the masked patches. For this, we adopt the negative y-constrained reconstruction loss \\(s(x_{i}, x'_{ii}) = max\\{\\||x_{i} - x'_{ii}||_{L2} - \\gamma, 0\\}\\) as the similarity metric. We emphasize that due to the strong heterogeneity of different domain samples, intra-domain negatives \\(j \\in I_{d}\\), are exclusively selected in equation (5).\nTo further refine our approach, we instantiate the invariance principle, enabling DisMAE to learn domain-invariant representations that remain indistinguishable by the domain classifier f(\u00b7). We start by determining the degree of domain- specific information in the semantic representations \\(\\bar{s}_{i}\\) of image \\(x_{i}\\) by employing a two-layer multilayer perception (MLP) as the domain classifier, with the cross- entropy loss serving as the objective function:\n\\(p(x_{i} \\in I_{d_{i}} | \\bar{s}_{i}) = Softmax(d_{i}, d_{i} = f(\\bar{s}_{i})).\\)   (6)\nDrawing inspiration from Inverse Propensity Weighting (IPW) in causal inference [26, 36], we multiply the inverse of this degree to reweight the loss, leading us to our adaptive contrastive loss formulation:\n\\(L_{con} = \\sum_{i=1}^{N} \\frac{1}{p(x_{i} \\in I_{d_{i}}|\\bar{s}_{i})} \\cdot l(x_{i}, x'_{ii}).\\)   (7)\nThis adaptive contrastive loss compels DisMAE to prioritize the refinement of domain-invariant semantic encoder learning, simultaneously mitigating the impact stemming from biased representation learning."}, {"title": "4 Experiments", "content": "We aim to answer the following research questions:\nDatasets. For a comprehensive comparison, we evaluate DisMAE across both unsupervised domain generalization (UDG) and domain generalization (DG) tasks. For UDG, our evaluation is on DomainNet [37], a dataset with 586,575 images spanning 345 object classes across six domains: Real, Painting, Sketch, Clipart, Infograph, and Quickdraw. Our DG evaluations center on two benchmark datasets: PACS [28] and VLCS [18]. PACS contains 9,991 images over 7 classes and 4 domains: Art, Cartoons, Photos, and Sketches. VLCS includes 10,729 images across 5 classes from four domains: Caltech101, LabelMe, SUN09, and VOC2007.\nImplementation Details. Contrary to the standard use of ResNet-18 [24], we employ the more powerful self-supervised learner ViT-B/16 [23] as our default backbone (for experiments in section 4.3, ViT-Tiny/16 serves as the backbone). We adopt a learning rate of le-4, a weight decay of 0.05, and a batch size of Na\u00d796, where Na denotes the number of domains in the training dataset. For UDG tasks, in alignment with the all correlated setting from DARLING [60], our model is pretrained on DomainBed, bypassing the ImageNet dataset. Conversely, for DG tasks, we initialize our backbone using ImageNet pre-training, as prescribed by DomainBed [21]. Detailed information on implementation and hyperparameter settings can be found in Appendix C.7.\nBaselines. For UDG tasks, we benchmark DisMAE against notable contrastive learning approaches (MoCo V2 [14], BYOL [20], DARLING [60]) and generative- based methods (MAE [23], CycleMAE [53]). For DG tasks, our comparisons encompass diverse learning strategies: from the vanilla (ERM [44]), distributional robust optimization (GroupDRO [40]), data augmentation-based (Mixup [52]), to domain-invariant learning (IRM [2], MMD [29]), variance optimization (VREx [27], Fishr [39]), and disentangled representation learning (DDG [58]). Related work and detailed implementation are available in Section 5 and Appendix C.1."}, {"title": "4.1 Overall Performance Comparison (RQ1)", "content": "Evaluations on UDG.\nSetting. For a balanced comparison, we strictly follow the all-correlated set- ting outlined by DARLING [60]. We select 20 classes from DomainNet's [37] 345 categories for both training and testing phases, using Painting, Real, and Sketch as source domains, while Clipart, Infograph, and Quickdraw act as target domains, and vice versa. Specifically, the UDG task is executed in three stages: 1. Unsupervised training on the source domains. 2. Depending on the labeled data proportion from the source domain, we either fit a linear classifier on the frozen semantic encoder or fully fine-tune the network. In particular, for labeled data proportions under 10% of the source domain's total training set, we opt for fitting a linear classifier. If it exceeds 10%, the entire network undergoes fine-tuning. 3. Model evaluation on the target domains.\nResults. Tables 1 and 7 showcase a performance comparison for the UDG evaluation using the ViT-B/16 backbone. The best-performing methods per test are bold, while the second-best methods are underlined. We observe that:\nDisMAE consistently and significantly outperforms all baseline mod- els in terms of both average and overall accuracy. In Table 1, DisMAE achieves gains of 1.48%, 3.04%, 4.65%, and 6.11% for average accuracy across the 1%, 5%, 10%, and 100% fraction settings, respectively. The robustness of DisMAE is attributed to its proficiency in distilling the invariant semantic attributes, regardless of variations in domain-specific features.\nEntangled baselines exhibit instability when faced with domain shifts. Conventional contrastive methods display limited generalization capability, revealing vulnerability in handling unknown target domains. These methods rely on data augmentations to form positive and negative pairs, lacking the explicit ability to bridge domain gaps by effectively pulling positive pairs together and pushing negative pairs apart. In the case of DARLING, negative samples are generated for each queue based on the dissimilarity between diverse domain samples. However, the cross-domain pairs constructed in this way are noisy in some cases, ultimately leading to suboptimal performance. Generative-based methods such as CycleMAE employ cycle reconstruction tasks to construct cross-domain pairs while suffering from an implicit domain-variant encoder, resulting in performance deterioration. In contrast, leveraging the strength of a domain-invariant extractor, our DisMAE significantly enhances performance by a substantial margin.\nEvaluations on DG.\nSetting. We strictly follow DomainBed [21] evaluation protocols, utilizing training-domain validation set. One domain serves as the target (test), while"}, {"title": "4.2 Discussion about Two Principles (RQ2)", "content": "To visualize the latent representation space and evaluate the effectiveness of two principles in DisMAE, we conduct a comprehensive set of experiments in DomainNet, employing Painting, Real, and Sketch domains as training domains and Clipart, Infograph, and Quickdraw for testing.\nReconstruction Visualization. Figures 1 and more examples in Appendix C.6 demonstrate DisMAE's capability in image-level disentanglement."}, {"title": "4.3 Study on DisMAE (RQ3)", "content": "To further evaluate each component of DisMAE, and its potential to enhance generalization ability, we conduct a comprehensive analysis using DomainBed in the UDG setting. This analysis contains three-fold: an ablation study of the adaptive contrastive loss components, the impact of decoder depth, and the effect of mask ratios. During the evaluation, we employ the Painting, Real, and Sketch domains as training domains and evaluate the models' performance on Clipart, Infograph, and Cartoon domains. Specifically, training is executed on ViT-Tiny/16 across 500 epochs, leveraging the full fraction of labeled data for fine-tuning. For thorough results and analysis, readers are referred to Appendix C.4 due to space constraints.\nEffect of adaptive contrastive loss. Table 3 shows that leveraging intra- domain negative samples significantly enhances generalization. The meticulously designed reweighting term aids in achieving the domain-invariance principle."}, {"title": "5 Related work", "content": "DisMAE is related to the literature on self-supervised learning (SSL), domain generalization (DG), and unsupervised domain generalization (UDG).\nSelf-supervised learning (SSL) employs a range of pretext tasks to derive meaningful representations from large amounts of unlabeled data, aiming to enhance performance in downstream tasks. Contemporary SSL methods can roughly fall into two distinct research lines: contrastive approaches and generative models. Recent advancements in contrastive methods [5,9,12, 14, 15, 20, 57] have excelled in instance discrimination. They strategically draw two augmented versions of an image closer in the feature space, recognized as positives, while simultaneously distancing them from negatives. While contrastive SSL methods have witnessed significant advancements, their efficacy often hinges on specific factors like high-quality data augmentation techniques and complicated strategies for training stabilization, such as well-designed negative sampling. On the contrary, generative SSL can avoid these dependencies. These approaches primarily focus on the reconstruction of intrinsic features and information. In the field of computer vision (CV), inspired by the introduction of ViT [16], masked image modeling (MIM) has attracted the huge attention of the research community, such as bidirectional encoder representation from image transformer (BEiT) [4], masked autoencoder (MAE) [23], context autoencoder (CAE) [13], a simple frame for MIM (SimMIM) [51], image generating pretraining (iGPT) [11]. Diverging from the SSL methods previously discussed, which concentrate on single-domain learning, our study pivots towards a more novel and pragmatic task: pretraining across multiple domains.\nDomain generalization (DG) aims to learn semantic representations that remain consistent across diverse domains, enabling models to generalize effectively to unseen domains using labeled data. Common approaches for solving the DG task can be broadly categorized as follows: minimizing the difference among source domains by invariant learning [1,2,27], adversarial learning [31, 42, 61], or feature alignment [17,34,39,45]; modifying the inputs to assist in learning general representations [25, 30, 41, 43]; disentangling the features into domain-shared or domain-specific parts for better generalization [3,6,8,49,58].\nUnsupervised domain generalization (UDG) has been proposed recently as a more important and challenging task centered on pretraining with unlabeled source domains [60]. Research in UDG predominantly bifurcates into two method- ologies: contrastive approaches and generative models. In the realm of contrastive approaches, DARLING [60] pioneered the UDG framework, focusing on domain- aware representation learning with a novel contrastive loss. Another notable contribution is [22], which innovatively conceptualized a bridge domain to unify all source domains. Additionally, DN2A [33] advanced this line of work by implementing strong augmentations and reducing intra-domain connectivity. Turning to generative models, DiMAE [54] was instrumental in introducing cross-domain reconstruction tasks, where input images are augmented using style noise from varying domains. Building on this, CycleMAE [53] innovated with a cycle-structured cross-domain reconstruction task, in the absence of paired images. A novel attempt in this domain is our proposed DisMAE, which, to our knowledge, is the first to incorporate a disentangled generative pretraining approach within UDG."}, {"title": "6 Conclusion", "content": "Despite the great success of domain generalization tasks, the challenge of unsu- pervised domain generalization remains relatively underexplored. In this work, we devised a principled disentangling approach, the disentangled mask autoen- coder (DisMAE), to learn domain-invariant features in an unsupervised manner. However, DisMAE does present a limitation meriting further exploration: its reconstruction accuracy is closely tied to the MAE backbone, which occasionally struggles to generate high-fidelity images (See failure examples in Appendix C.5)."}, {"title": "A Algorithm", "content": "Algorithm 1 depicts the detailed procedure of DisMAE.\nAlgorithm 1 DisMAE: Disentangling Masked Autoencoder for Unsupervised Domain Generalization\nInput: \\(D_{s} = \\{(x_{1}, d_{1}), ..., (x_{n}, d_{n})\\}\\), maximum adaptive training epochs \\(E_{ad}\\), maximum training epoch E, adaptive training intervals \\(T_{ad}\\), batch size B, margin \\(\\gamma\\), coefficients \\(\\lambda_{1}\\), current training epoch e\nInitial: Parameters of DisMAE (i.e parameter \\(\\theta_{e}\\), \\(\\theta_{v}\\), \\(\\theta_{s}\\), and \\(\\theta_{cls}\\) for semantic encoder \\(\\phi_{s}\\), variation encoder \\(\\phi_{v}\\), decoder g, and domain classifier \\(f_{cls}\\)), e \u2190 1\nrepeat\nFreeze parameters of the domain classifier \\(f_{cls}\\)\nCompute \\(l(x_{i}, x'_{ii})\\) and \\(p(x_{i} \\in I_{d_{i}}|s_{i})\\) with \\(\\theta_{s}\\), \\(\\theta_{v}\\), \\(\\theta_{g}\\), \\(\\theta_{l}\\)\nCompute \\(L_{rec} = \\frac{1}{B} \\sum_{i=1}^{B} max\\{\\||x_{i} - g(s_{i}, \\bar{v_{i}})||_{L2} - \\gamma, 0\\}\\}\nCompute \\(L_{con} = \\frac{1}{B} \\sum_{i=1}^{B} \\frac{1}{p(x_{i} \\in I_{d_{i}} | s_{i})}  \\cdot l(x_{i}, x'_{ii})\\)\nCompute \\(L = L_{rec} + \\lambda_{1}L_{con}\\)\nUpdate \\(\\theta_{e}\\), \\(\\theta_{v}\\), \\(\\theta_{g}\\) by minimizing L\nif e mod \\(T_{ad}\\) == 0 and e < \\(E_{ad}\\) then\nFreeze parameters of backbones \\(\\theta_{e}\\), \\(\\theta_{v}\\) and \\(\\theta_{g}\\)\nCompute \\(L_{cls} = \\frac{1}{B} \\sum_{i=1}^{B} CE(d_{i}, d'_{i} = f_{cls}(s_{i}))\\)\nUpdate \\(\\theta_{cls}\\) by minimizing \\(L_{cls}\\)\nend if\ne \u2190 e+1\nuntil e == E"}, {"title": "B Discussion About Differences", "content": "We argue that DisMAE is novel and significantly different from prior studies w.r.t. three aspects. 1) Scope. Transitioning to UDG is non-trivial. Previous disentangled methods like DADA [38], DIDN [32], and DIR [19], while effective in DG, struggle with unsupervised data due to their high dependence on class labels to encapsulate semantic attributes. 2) Disentangled Targets. Without class label guidance, achieving a domain-invariant semantic encoder is challeng- ing. Many UDG methods, such as DiMAE [54] and CycleMAE [53], can only separate domain styles using multiple decoders but fall short in disentangling domain-invariant semantics from variations. 3) Disentangle Strategy. DisMAE is grounded in disentanglement and invariance principles, uniquely combining adaptive contrastive loss with reconstruction loss collaboratively. The adaptive contrastive loss, in particular, is designed by seamlessly leveraging the domain classifier and intra-domain negative sampling."}, {"title": "C Experiments", "content": "C.1 Experimental Settings\nBaseline Hyperparameter Tuning. For a fair comparison, we uniformly substitute the backbones of all baselines with the same ViT-B/16 and rerun the experiment using UDG and DG open-source codebases. And we provide the default hyperparameters for UDG baselines in Table 5. And the search distribution for each hyperparameter in each DG baseline is detailed in Table 6."}, {"title": "C.2 Overall performance", "content": "Unsupervised Domain Generalization. Due to limited space in the paper, we show the rest UDG results in Table 7. We employ Clipart, Infograph, and Quickdraw as training domains and Painting, Real, and Sketch as test domains. Following the same all correlated settings and protocols in DARLING, we find that our DisMAE could achieve 1.14%, 1.19%, 4.40%, and 5.45% gains for average accuracy over the second-best baselines across 1%, 5%, 10%, and 100% fraction setting respectively.\nDomain Generalization. Aligning with the training-domain validation setup in DomainBed, we achieve 0.50% gains for the average accuracy in PACS datasets, as shown in Table 8."}, {"title": "C.3 Discussion of the invariance principle", "content": "In Figure 2 and 4, we visualize the representation acquired through MAE, our semantics encoder, and the variational encoder via t-SNE. We find that: (1) The representations generated by MAE for each domain showcase a degree of overlap at the center of the picture, accompanied by slight variations within each distribution. This suggests that MAE captures both semantics and domain-variant information but fails to disentangle them effectively. (2) Our semantic representations in each domain distribute uniformly. This justifies that DisMAE could learn domain-invariant representations from each domain. (3) The variation representations in each domain has their specific distribution. Clusters of similar variation data further emphasize domain-specific characteristics."}, {"title": "C.4 More ablation study", "content": "Effects of decoder depth. The efficacy of the adaptive contrastive loss hinges on the output of decoders. This prompts the inquiry: how many decoder layers are optimal for achieving peak performance? As shown in Table 9, a deeper decoder may lead to overfitting in reconstruction and subsequently diminish the effect of our contrastive loss. Thus, adopting a lightweight decoder could both accelerate the training and guarantee robustness.\nEffects of mask ratios. In Table 10, we set different mask ratios to test the robustness of our model. And we found that the 80 percentile of the mask ratio reaches the optimal result. We set it as our default protocol."}, {"title": "C.5 Failure cases", "content": "Some failure cases of our proposed DisMAE are in Figure 6. Our approach strug- gles with reconstruction containing intricate details and lines. It frequently fails to generate images that possess sufficient detail while simultaneously providing clear augmented variations. We attribute these failures to two primary reasons: 1) The MAE backbone operates on patches, making pixel-level reconstruction difficult, and our method heavily relies on the MAE model's reconstruction outcomes. 2) Our disentanglement lacks granularity, often capturing broad color regions and background information rather than nuanced details. In the context of UDG, reconstructing images with fine granularity, high resolution, and authenticity remains a challenging and crucial research direction. We are also keenly interested in exploring the potential integration of the diffusion model within the UDG framework."}]}