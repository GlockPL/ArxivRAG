{"title": "Disentangling Masked Autoencoders for Unsupervised Domain Generalization", "authors": ["An Zhang", "Han Wang", "Xiang Wang", "Tat-Seng Chua"], "abstract": "Domain Generalization (DG), designed to enhance out-of- distribution (OOD) generalization, is all about learning invariance against domain shifts utilizing sufficient supervision signals. Yet, the scarcity of such labeled data has led to the rise of unsupervised domain generalization (UDG) a more important yet challenging task in that models are trained across diverse domains in an unsupervised manner and eventually tested on unseen domains. UDG is fast gaining attention but is still far from well-studied.\nTo close the research gap, we propose a novel learning framework designed for UDG, termed the Disentangled Masked AutoEncoder (DisMAE), aiming to discover the disentangled representations that faithfully reveal the intrinsic features and superficial variations without access to the class label. At its core is the distillation of domain-invariant semantic features, which can not be distinguished by domain classifier, while filtering out the domain-specific variations (for example, color schemes and texture patterns) that are unstable and redundant. Notably, DisMAE co-trains the asymmetric dual-branch architecture with semantic and lightweight variation encoders, offering dynamic data manipulation and representation level augmentation capabilities. Extensive experiments on four benchmark datasets (i.e. DomainNet, PACS, VLCS, Colored MNIST) with both DG and UDG tasks demonstrate that DisMAE can achieve competitive OOD performance compared with the state-of-the-art DG and UDG baselines, which shed light on potential research line in improving the generalization ability with large-scale unlabeled data.", "sections": [{"title": "1 Introduction", "content": "Domain Generalization (DG) strives to achieve robustness against domain shifts by leveraging high-quality supervision signals [46,55,56]. However, data is never"}, {"title": "2 Preliminary", "content": "The goal of unsupervised domain generalization (UDG) is to learn a domain- invariant feature extractor from diverse domains in a self-supervised manner, thereby enabling robust generalization to the target unseen domain [54]. Let\n\\(Dtrain = {x \u2208 Ia|d \u2208 {1,\u2026\u2026, M - 1}}\\) be the training set that involves M \u2013 1 source domains. Each training domain Ia comprises a collection of unlabeled images {x}. Let Dtest = {x \u2208 IM} represent the test set containing previously unseen images from the target domain IM.\nIn order to learn a domain-invariant feature extractor \u0444s in self-supervised learning, there is a foundation assumption [48]: any given sample x is generated from two disentangled features, x = g(sx, vx), where sx denotes the semantic rep- resentation that invariant to domain shifts, v signifies the superficial variations that change across domains. The generative function g(,) maps these two latent space representations back into the sample space. In particular, this assumption naturally encodes two principles: disentanglement and invariance principles.\nDisentanglement Principle. There exists a direct product decomposition of attributes as well as feature representations in real-world visual generation sce- narios.\nLet s be the semantic attribute of sample x, and vx denote its vari- ations, e.g. for an image in the Col- ored MNIST dataset [2], its semantic attribute sx signifies \"digit\", while the variation attributes v\u2082 is \"color\". There exists a direct product decomposition of attributes that could control each sample x's generation: SxXUx, e.g. gen- erating a digit \"5\" with \"green\" color (see examples in Figure 1). Thus, this decomposition implies a disentangled representation learning, s \u00d7 \u03c6\u03c5, that maps image pixels to a representation space. Here, sx := $s(x) captures the semantic aspects, and vx := \u03c6\u03c5(x) en- capsulates the variations of sample x.\nThe disentanglement principle is inherent in a desirable property. Mod- ifying attributes is essentially akin to directly altering representations in the feature space. For instance, consider"}, {"title": "3 Methodology", "content": "We now introduce our Disentangled Masked Autoencoder (DisMAE), designed to derive decomposed high-level representations for UDG, building on the scalable self-supervised MAE framework [23]. Grounded in disentanglement and invariance principles, our approach integrates reconstruction loss and adaptive contrastive"}, {"title": "3.1 DisMAE", "content": "Considering a multi-domain reconstruction task with the training set Dtrain =\n{(xi, di)}1, where xi \u2208 Id\u2081 denotes the input image, di signifies the correspond- ing domain category, and N represents the number of training data. DisMAE develops an asymmetric dual-branch architecture, visualized in Figure 3, with a main branch mapping the input image into the semantic feature space i.e. semantic encoder $s(.), along with a lightweight branch that extracts the input sample's variation attributes i.e. variations encoder (\u00b7). In alignment with MAE [23], we denote the masking mechanism as the function M and the input visible patches as M(xi) for the original image xi. Our approach utilizes two transformer-based encoders [16] to intensify the domain-invariant semantics and domain-specific variations as representations:\n\\((Si, Vi) = ($s(M(xi)), \u03c6\u03c5(\u039c(xi))).\\) (1)\nConsequently, the domain-invariant semantic representation si and domain- specific variation representation vi are to estimate the attributes sz\u2081 and vxi in feature space, respectively. We denote the [cls] patch embedding of si and vi as s and v, wheres, v \u2208 RB\u00d7H and B is the batch size and H is the hidden size. Their combination can parameterize image xi as:\n\\(Xii = Si||V,\\) (2)\nwhere || symbolizes the concatenation operation. Here, we utilize v since we only need overall information about variation features.\nIn addition to the encoders, we utilize a transformer-based decoder, denoted as g(,), mirroring the architecture of the MAE decoder. This enables the reconstruction xii = g(si, vi) of xi. Importantly, the decoder is operational only during training for reconstruction. During the UDG task's fine-tuning phase, solely the main branch, namely the semantic encoder os, is deployed.\nFor the effective optimization of both encoders and the decoder, the preva- lent reconstruction learning strategy is employed. Specifically, the risk function measures the quality of reconstructed pixel values, which can be formulated as the mean squared error (MSE) or y-constrained reconstruction loss [58] between the masked patches and the corresponding masked patches for the original image xi. Here we apply y-constrained reconstruction loss, which is defined as:\n\\(Crec =\n1\nN\n\u03a3max{||xi - g(si, vi)||12 \u2013 y, 0},\\) (3)\nwhere \u03b3 > 0 serves as a hyperparameter, indicating the bound of the allowable reconstruction error. However, solely minimizing the risks over the original data distribution fails to model domain-invariant information and suffers from poor domain generalization."}, {"title": "3.2 Implementation of Two Principles", "content": "To bring forth better generalizations w.r.t. domain shift, we advocate for dis- entanglement and invariance principles. To parameterize the disentanglement principle, we devise an augmentation operator on semantic and variation represen- tations, which preserves the estimated domain-invariant intrinsic attributes but intervenes in the estimated domain-specific information. Formally, the operator samples in-batch variation representations v from distinct sample x; to replace v of xi and later combine it with si to generate the augmented image xij. The augmentation process is:\n\\(Xij = Si||V, Xij = g(Si, vj).\\) (4)\nHaving established the augmented representations, we enforce the recon- structed sample with its original semantics and variations closer to the input sample while concurrently pushing the reconstructed samples possessing distinct intra-domain variations away from it. This can be captured by a contrastive loss between the original image xi and its reconstruction xii:\n\\(1(xi, xii) = -log\nexp(s(xi, x'ii)/T)\nEjeZau{i} exp(s(xi, Xij)/T)'\\) (5)\nwhere is the temperature hyperparameter in the contrastive loss [35], and s(,) is a similarity function quantifying the distance between the reconstructed and original images on the masked patches. For this, we adopt the negative y-constrained reconstruction loss s(xi, x'ii) =\nmax{||xi - Xii||12 - y, 0} as the\nsimilarity metric. We emphasize that due to the strong heterogeneity of different domain samples, intra-domain negatives j\u2208 Id, are exclusively selected in equation (5).\nTo further refine our approach, we instantiate the invariance principle, enabling DisMAE to learn domain-invariant representations that remain indistinguishable by the domain classifier f(\u00b7). We start by determining the degree of domain- specific information in the semantic representations s of image xi by employing a two-layer multilayer perception (MLP) as the domain classifier, with the cross- entropy loss serving as the objective function:\n\\(p(xi \u2208 Id\u2081|s) = Softmax(di, d\u2081 = f(s)).\\) (6)\nDrawing inspiration from Inverse Propensity Weighting (IPW) in causal inference [26, 36], we multiply the inverse of this degree to reweight the loss, leading us to our adaptive contrastive loss formulation:\n\\(Leon =\n1\nN\n\u03a31\np(xi \u2208 Idis)\n\u00b71(Xi, Xii).\\) (7)\nThis adaptive contrastive loss compels DisMAE to prioritize the refinement of domain-invariant semantic encoder learning, simultaneously mitigating the impact stemming from biased representation learning."}, {"title": "4 Experiments", "content": "We aim to answer the following research questions:\n\u2022 RQ1: How does DisMAE perform compared with prevailing UDG and DG approaches?\n\u2022 RQ2: Does DisMAE successfully derive disentangled representations and domain-invariant features in UDG scenarios?\n\u2022 RQ3: What are the impacts of the components on our DisMAE?\nDatasets. For a comprehensive comparison, we evaluate DisMAE across both unsupervised domain generalization (UDG) and domain generalization (DG) tasks. For UDG, our evaluation is on DomainNet [37], a dataset with 586,575 images spanning 345 object classes across six domains: Real, Painting, Sketch, Clipart, Infograph, and Quickdraw. Our DG evaluations center on two benchmark datasets: PACS [28] and VLCS [18]. PACS contains 9,991 images over 7 classes and 4 domains: Art, Cartoons, Photos, and Sketches. VLCS includes 10,729 images across 5 classes from four domains: Caltech101, LabelMe, SUN09, and VOC2007.\nImplementation Details. Contrary to the standard use of ResNet-18 [24], we employ the more powerful self-supervised learner ViT-B/16 [23] as our default backbone (for experiments in section 4.3, ViT-Tiny/16 serves as the backbone). We adopt a learning rate of le-4, a weight decay of 0.05, and a batch size of Na\u00d796, where Na denotes the number of domains in the training dataset. For UDG tasks, in alignment with the all correlated setting from DARLING [60], our model is pretrained on DomainBed, bypassing the ImageNet dataset. Conversely, for DG"}, {"title": "4.1 Overall Performance Comparison (RQ1)", "content": "Evaluations on UDG.\nSetting. For a balanced comparison, we strictly follow the all-correlated set- ting outlined by DARLING [60]. We select 20 classes from DomainNet's [37] 345 categories for both training and testing phases, using Painting, Real, and Sketch as source domains, while Clipart, Infograph, and Quickdraw act as target domains, and vice versa. Specifically, the UDG task is executed in three stages: 1. Unsupervised training on the source domains. 2. Depending on the labeled data proportion from the source domain, we either fit a linear classifier on the frozen"}, {"title": "Evaluations on DG.", "content": "Setting. We strictly follow DomainBed [21] evaluation protocols, utilizing training-domain validation set. One domain serves as the target (test), while"}, {"title": "4.2 Discussion about Two Principles (RQ2)", "content": "To visualize the latent representation space and evaluate the effectiveness of two principles in DisMAE, we conduct a comprehensive set of experiments in DomainNet, employing Painting, Real, and Sketch domains as training domains and Clipart, Infograph, and Quickdraw for testing.\nReconstruction Visualization. Figures 1 and more examples in Appendix C.6 demonstrate DisMAE's capability in image-level disentanglement.\nDomain-Invariance Assessment. Figure 5 emphasizes the domain-invariant proficiency of our semantic encoder, as evidenced by the prediction scores of so.\nRepresentation-level Visualization. Through t-SNE visualizations in Fig- ures 2 and 4, we validate that DisMAE adheres to invariance and disentangle- ment principles at the representation level.\nThe specific analysis is detailed below:\nDisentanglement principle: Semantic and variational features learned by DisMAE are fully disentangled. From an image perspective, as illustrated in Figure 1, DisMAE differentiates between the foreground and background of an image, exemplified by its capability to transform the background to a blue sky or a red hue without affecting the primary subject, the flower. Remarkably, DisMAE can discern domain styles and fuse domain-specific elements across them a notable instance is superimposing the sun from a sketch onto a painting. Such disentanglement ability endows DisMAE with the flexibility to generate controllable images by manipulating semantic and variation factors through swapping.\nInvariance Principle: Representations learned by semantic encoder are domain-invariant. As depicted in Figure 5, the initial prediction scores of Sketch for each domain (opaque lines), denoted as p(xi \u2208 Isketch si), exhibit randomness at the beginning. With the advancement of training, these prediction scores elevate for the Sketch domain (opaque green line), demonstrating that Sketch-specific features are extracted by the semantic encoder at first. As training progresses, the prediction scores gradually converge to 0.33 (domain-agnostic score), indicating domain-variant information excluded from our main encoder"}, {"title": "4.3 Study on DisMAE (RQ3)", "content": "To further evaluate each component of DisMAE, and its potential to enhance generalization ability, we conduct a comprehensive analysis using DomainBed in the UDG setting. This analysis contains three-fold: an ablation study of the adaptive contrastive loss components, the impact of decoder depth, and the effect of mask ratios. During the evaluation, we employ the Painting, Real, and Sketch domains as training domains and evaluate the models' performance on Clipart, Infograph, and Cartoon domains. Specifically, training is executed on ViT-Tiny/16 across 500 epochs, leveraging the full fraction of labeled data for fine-tuning. For thorough results and analysis, readers are referred to Appendix C.4 due to space constraints.\nEffect of adaptive contrastive loss. Table 3 shows that leveraging intra- domain negative samples significantly enhances generalization. The meticulously designed reweighting term aids in achieving the domain-invariance principle."}, {"title": "5 Related work", "content": "DisMAE is related to the literature on self-supervised learning (SSL), domain generalization (DG), and unsupervised domain generalization (UDG).\nSelf-supervised learning (SSL) employs a range of pretext tasks to derive meaningful representations from large amounts of unlabeled data, aiming to enhance performance in downstream tasks. Contemporary SSL methods can roughly fall into two distinct research lines: contrastive approaches and generative models. Recent advancements in contrastive methods [5,9,12, 14, 15, 20, 57] have excelled in instance discrimination. They strategically draw two augmented versions of an image closer in the feature space, recognized as positives, while simultaneously distancing them from negatives. While contrastive SSL methods have witnessed significant advancements, their efficacy often hinges on specific factors like high-quality data augmentation techniques and complicated strategies"}, {"title": "6 Conclusion", "content": "Despite the great success of domain generalization tasks, the challenge of unsu- pervised domain generalization remains relatively underexplored. In this work, we devised a principled disentangling approach, the disentangled mask autoen- coder (DisMAE), to learn domain-invariant features in an unsupervised manner. However, DisMAE does present a limitation meriting further exploration: its reconstruction accuracy is closely tied to the MAE backbone, which occasionally struggles to generate high-fidelity images (See failure examples in Appendix C.5)."}, {"title": "A Algorithm", "content": "Algorithm 1 depicts the detailed procedure of DisMAE.\nAlgorithm 1 DisMAE: Disentangling Masked Autoencoder for Unsupervised Domain Generalization\nInput: Ds = {(x1, d\u2081), ..., (xn, dn)}, maximum adaptive training epochs Ead, max- imum training epoch E, adaptive training intervals Tad, batch size B, margin y, coefficients \u51651, current training epoch e\nInitial: Parameters of DisMAE (i.e parameter \u03b8\u03b5, \u03b8\u03c5, \u03b8\u03c2, and Ocis for semantic encoder os, variation encoder \u03c6u, decoder g, and domain classifier fcls), e \u2190 1 repeat\nFreeze parameters of the domain classifier 01\nCompute l(xi, xii) and p(xi \u2208 Id\u2081|si) with 0s, \u03b8\u03c5,\u03b8\u03c2, \u03b8\u03b9\nCompute Lrec =1 \u03a3=1 max{||xi - g(si, v\u00ba)||12 - y, 0}\nCompute Lcon =\u03a31 p(xiZa) \u2022l(Xi, Xii)\nCompute L = Lrec + A1Lcon\nUpdate \u03b8\u03b5, \u03b8\u03c5, \u03b8g by minimizing L\nif e mod Tad == 0 and e < Ead then\nFreeze parameters of backbones \u03b8\u03b5, \u03b8\u03c5 and \u03b8g\nCompute Lcls =\u03a3=1CE(di, di = fcls(si))\nUpdate Ocis by minimizing Lels\nend if\nee+1\nuntil eE\nB Discussion About Differences\nWe argue that DisMAE is novel and significantly different from prior studies w.r.t. three aspects. 1) Scope. Transitioning to UDG is non-trivial. Previous disentangled methods like DADA [38], DIDN [32], and DIR [19], while effective in DG, struggle with unsupervised data due to their high dependence on class labels to encapsulate semantic attributes. 2) Disentangled Targets. Without class label guidance, achieving a domain-invariant semantic encoder is challeng- ing. Many UDG methods, such as DiMAE [54] and CycleMAE [53], can only separate domain styles using multiple decoders but fall short in disentangling domain-invariant semantics from variations. 3) Disentangle Strategy. DisMAE is grounded in disentanglement and invariance principles, uniquely combining adaptive contrastive loss with reconstruction loss collaboratively. The adaptive contrastive loss, in particular, is designed by seamlessly leveraging the domain classifier and intra-domain negative sampling. The differences are summarized in Table 4."}, {"title": "C Experiments", "content": "C.1 Experimental Settings\nBaseline Hyperparameter Tuning. For a fair comparison, we uniformly substitute the backbones of all baselines with the same ViT-B/16 and rerun the experiment using UDG and DG open-source codebases. And we provide the default hyperparameters for UDG baselines in Table 5. And the search distribution for each hyperparameter in each DG baseline is detailed in Table 6.\nDefault hyperparameters\nMoCo V2 lr=5e-4, BS=96, WD=0.05, K=65536, m=0.999, T=0.07\nBYOL lr=5e-4, BS=64, WD=0.05, m=0.996\nMAE lr=1e-3, BS=96, WD=0.00, mask ratio=0.75\nDARLING lr=5e-4, BS=96, WD=0.05, K=65536, m=0.995, T=0.07\nCycleMAE lr=7e-4, BS-96, WD=0.05, m=0.999, a = 2, \u03b2 = 1\nC.2 Overall performance\nUnsupervised Domain Generalization. Due to limited space in the paper, we show the rest UDG results in Table 7. We employ Clipart, Infograph, and Quickdraw as training domains and Painting, Real, and Sketch as test domains. Following the same all correlated settings and protocols in DARLING, we find that our DisMAE could achieve 1.14%, 1.19%, 4.40%, and 5.45% gains for average accuracy over the second-best baselines across 1%, 5%, 10%, and 100% fraction setting respectively.\nDomain Generalization. Aligning with the training-domain validation setup in DomainBed, we achieve 0.50% gains for the average accuracy in PACS datasets, as shown in Table 8.\nIn Figure 2 and 4, we visualize the representation acquired through MAE, our semantics encoder, and the variational encoder via t-SNE. We find that: (1) The representations generated by MAE for each domain showcase a degree of overlap at the center of the picture, accompanied by slight variations within each distribution. This suggests that MAE captures both semantics and domain- variant information but fails to disentangle them effectively. (2) Our semantic representations in each domain distribute uniformly. This justifies that DisMAE could learn domain-invariant representations from each domain. (3) The variation representations in each domain has their specific distribution. Clusters of similar variation data further emphasize domain-specific characteristics.\nC.4 More ablation study\nEffects of decoder depth. The efficacy of the adaptive contrastive loss hinges on the output of decoders. This prompts the inquiry: how many decoder layers are optimal for achieving peak performance? As shown in Table 9, a deeper decoder may lead to overfitting in reconstruction and subsequently diminish the"}, {"title": "C.5 Failure cases", "content": "Some failure cases of our proposed DisMAE are in Figure 6. Our approach strug- gles with reconstruction containing intricate details and lines. It frequently fails to generate images that possess sufficient detail while simultaneously providing clear augmented variations. We attribute these failures to two primary reasons: 1) The MAE backbone operates on patches, making pixel-level reconstruction difficult, and our method heavily relies on the MAE model's reconstruction outcomes. 2) Our disentanglement lacks granularity, often capturing broad color regions and background information rather than nuanced details. In the context of UDG, reconstructing images with fine granularity, high resolution, and authenticity remains a challenging and crucial research direction. We are also keenly interested in exploring the potential integration of the diffusion model within the UDG framework."}, {"title": "C.6 Qualitative reconstructions", "content": "Additional visualization results of image reconstruction, spanning both colored MNIST and DomainNet, can be observed in Figure 7.\nDisMAE differentiates between the foreground and background of an image. Remarkably, DisMAE can discern domain styles and fuse domain-specific elements across them a notable instance is superimposing the sun from a sketch onto a painting. Such disentanglement ability endows DisMAE with the flexibility to generate controllable images by manipulating semantic and variation factors through swapping."}, {"title": "C.7 Detailed implementation of DisMAE", "content": "We conduct all the experiments in Pytorch on a cluster of 8 NVIDIA Tesla A100 GPUs with 40GB each. Our default backbone consists of 12 blocks of semantic encoder, 6 blocks of variation encoder, and a transform-based decoder. We utilize ViT-B/16 as our default backbone for both visualization and main experiments. And we use ViT-Tiny/16 in our ablation study. We let margin y = 0.008 and T = 0.4. In UDG, we choose the AdamW optimizer for the main branch and set the learning rate as 1e-4 and betas as (0.9, 0.95) for pre-training. As for finetuning, we adopt the learning rate as 0.025, 0.05, 5e-5, 5e-5 and batch size as 96, 192, 36, and 36 in the label fraction 1%, 5%, 10%, and 100% experiments respectively. And we finetune all the checkpoints for 50 epochs. In DG, A\u2081 is selected within {5e-4, 1e-3, 5e-3, 1e-2} and A2 is selected within {0.1, 0.5, 1.0, 2.0}. The detailed hyperparameters for UDG and DG are listed in Table 11."}]}