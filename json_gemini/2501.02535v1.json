{"title": "A completely uniform transformer for parity", "authors": ["Alexander Kozachinskiy", "Tomasz Steifer"], "abstract": "We construct a 3-layer constant-dimension transformer, recognizing the parity language, where neither parameter matrices nor the positional encoding depend on the input length. This improves upon a construction of Chiang and Cholak who use a positional encoding, depending on the input length (but their construction has 2 layers).", "sections": [{"title": "Introduction", "content": "One of the ways to do mathematical analysis of the capabilities and limitations of the transformer architecture [8] is to study formal languages, recognizable by them. Namely, for a given formal language L, we study, if there exists a choice of parameters in the transformer architecture, for which words from L are accepted and words not from L are rejected by the resulting transformer. A seminal work of Hahn [5] performed such analysis for a number of formal languages, including the parity language, consisting of binary words with even number of 1s. Hahn have shown that transformers, recognizing this language, must have low confidence, partially explaining an empirically observed struggle of transformers in learning this language [1, 4].\nHowever, this does not exclude the possibility of existence of a theoretical solution for this language. And indeed, as was shown by Chiang and Cholak [3], there exists a 2-layer transformer with constant embedding dimension, recognizing the parity language. Their construction is uniform in a sense that parameter matrices in it do no depend on the input length. However, there is one aspect of their construction which is not completely uniform \u2013 the positional encoding. At position i, they use i/n, where n is the input length. This means that their positional encoding has to be reset each time we want to use their transformer for a larger input length.\nIn this paper, we get rid of this disadvantage of their construction by giving a completely uniform 3-layer transformer, recognizing parity. That is, in our construction, neither parameter matrices nor positional encoding depend on n. We do not use neither positional masks nor layer norm.\nIt remains open if the parity language can be recognized by a 1-layer transformer with constant embedding dimension, even non-uniformly. Current lower bound methods against 1-layer transformers [2, 6, 7] do not seem to work against parity."}, {"title": "Preliminaries", "content": "An attention layer of dimension d is a length-preserving function A: (\\mathbb{R}^d)^* \\rightarrow (\\mathbb{R}^d)^*, given by three matrices K,Q,O \u2208 \\mathbb{R}^{d\u00d7d} and a neural network N: \\mathbb{R}^d \\rightarrow \\mathbb{R}^d with ReLU activation. On input (f_1,..., f_n) \u2208 (\\mathbb{R}^d)^n,"}, {"title": "Construction", "content": "Lemma 1. For any function f: \\mathbb{N} \\rightarrow \\mathbb{R}, there exists a completely uniform transformer that, for any input length n, computes f(n) in every position in one layer.\nProof. We need a positional encoding p: \\mathbb{N} \\rightarrow \\mathbb{R} such that:\n$\\frac{p(1) + ... + p(n)}{n} = f(n)$,\nfor every n (this average is computable via softmax with uniform weights), which is achievable by setting p(1) = f(1) and $p(i) = if(i) - (i - 1)f(i - 1)$ for $i \\geq 2$.\nTheorem 1. There exists a 3-layer completely uniform transformer, recognizing the parity language.\nProof. Assume that on input we get x = x_1 ... x_n \u2208 {0,1}^n. Denote $\\Sigma = x_1 + ... + x_n$. Let us first give a construction, assuming that $\\Sigma \\geq 1$, that is, that not all input bits are 0. We explain how to get rid of this assumption in the end of the proof.\nThe plan of the proof is as follows. At the first two layers, we need to compute a sequence of numbers\n$a_1, a_2,..., a_n$,\nwith the property that $a_{\\Sigma}$ is strictly larger than any other number in the sequence. Assuming we have done this, at the last layer we can compute the following:\n$\\frac{\\sum_{i=1}^n \\exp{a_i f(n)}(-1)^i}{\\sum_{i=1}^n \\exp{a_i f(n)}}$                                                                                                                                    (1)"}, {"title": "", "content": "using a positional encoding i\u2194 (-1) and Lemma 1 for a function f: N \u2192 {0,1} of our choice. Our goal for (1) is to encode parity, i.e., (1) must be positive if \u2211 is even and negative otherwise. Indeed, the limit of (1) as f(n) \u2192 +\u221e is $(-1)^{\\Sigma}$, because $a_{\\Sigma} > a_i$ for $i \\neq \\Sigma$. For any fixed n, there are finitely many inputs of length n. Hence, we can take f(n) large enough so that, for any input x of length n, the difference between (1) for this x and $(-1)^{\\Sigma}$ is at most 1/3.\nIt remains to calculate a sequence $a_1,..., a_n$ with this property at the first two layers. By Lemma 1, we compute Inn at every position in the first layer. Then, for an absolute constant \u03b4\u2208 R, to be specified later, we set $a_i = e^{\\delta i/n}$ and use the second layer to compute the following expression at every position:\n$\\gamma = \\frac{\\sum_{i=1}^n e^{(-\\ln(n)+\\delta) \\cdot (1-x_i)} \\cdot x_i}{\\sum_{i=1}^n e^{(-\\ln(n)+\\delta) \\cdot (1-x_i)}} = \\frac{\\sum}{\\sum + (n - \\sum) e^{-\\delta}} = \\frac{\\sum}{\\sum + \\alpha (n - \\sum)} = \\frac{1}{\\alpha \\frac{n}{\\sum} + (1-\\frac{1}{\\alpha})}$(2)\nIt now suffices to prove the following lemma.\nLemma 2. There exists a \u2208 (0,1) such that for all n and $\\Sigma\u2208 \\{1, ..., n\\}$, the maximum of the expression:\n$a_i = - |1 + \\gamma| + (\\frac{i}{n} - \\frac{1}{2})^2 - (\\frac{a^2}{\\sum^2} + \\frac{a^2}{n^2})^2$,\nover i \u2208 {1,...,n} is attained uniquely at i = \u03a3, where \u03b3 is defined by (2).\nIndeed, once we have computed \u03b3, we can express $a_i$ by computing $\\frac{i}{n} - \\frac{1}{2}$ in every position by Lemma 1, and using a positional encoding i\u2192 $\\frac{i}{n} - \\frac{1}{2}$. We can then calculate the absolute value using a constant-size ReLU network.\nProof of Lemma 2. Observe that:\n$\\frac{1}{1+z} = (1 - z + z^2) - \\frac{z^3}{1+z} < z^2$\nfor z \u2265 0. Applying this to $z = \\frac{n}{\\Sigma} - 1$, we get:\n$\\gamma = 1 - (\\alpha \\frac{n}{\\Sigma} - 1) + (\\alpha \\frac{n}{\\Sigma} - 1)^2 + \\rho$,\nfor some \u03c1 with|\u03c1|\u2264 $\\frac{1}{3}$. We now can write:\n$a_i = - |1 + \\gamma| + (\\frac{i}{n} - \\frac{1}{2})^2 - (\\frac{a^2}{\\sum^2} + \\frac{a^2}{n^2})^2 + \\rho$\nDenoting $b_i = (\\frac{i}{n} - \\frac{1}{2})^2$, $C_i = (\\frac{a^2}{\\sum^2})$ and \u03bb = $\\frac{a^2}{n^2}$, we get:\n$a_i = - |b_i + C_i + \u03bb + \u03c1|$\nFor i = \u2211, we get $a_{\\sum} = - |\u03bb + \u03c1|$. To prove that the maximum of $a_i$ is attained at i = \u2211, we show that:\n$(\\frac{1}{10}) |b_i| > |C_i|$, ($\\frac{1}{10}) |b_i| > |\u03bb|$, ($\\frac{1}{10}) |b_i| > |\u03c1|$, for i \u2260 \u03a3,\nwhich implies that:\n$-a_i = |b_i + C_i + \u03bb + \u03c1| > |b_i| - |C_i| - |\u03bb| - |\u03c1| \u2265 (\\frac{7}{10}) |b_i| > |\u03bb| + |\u03c1| \u2265 -a_{\\Sigma}$.\nFirstly, for i \u2260 \u03a3, observe:\n$\\frac{b_i}{C_i} > \\frac{\\sum^2}{a^2 (i + \\sum)^2} > \\frac{1}{2a}$                                                                                                                               (3)"}, {"title": "", "content": "where the last inequality is due to the fact that i, \u03a3 > 1. Next, for i \u2260 \u03a3, observe:\n$\\frac{b_i}{\u03bb} = \\frac{\\frac{i}{n} - \\frac{1}{2}}{(\\frac{a}{n})^2} > \\frac{1}{2a^2}$                                                                                                                                                                          (4)\nFinally, for i \u2260 \u03a3, write:\n$\\frac{b_i}{\u03c1} > \\frac{(\\frac{i}{n} - \\frac{1}{2})^2}{\\frac{1}{3}} > \\frac{1}{3} (\\frac{i}{n} - \\frac{i - \u03a3}{n})^2$\nIf \u03a3 \u2265 i/2, since i - \u03a3 \u2265 1, we obtain $\\frac{b_i}{\u03c1} \\geq \\frac{1}{3n^2}$. If \u03a3 < i/2, we obtain i - \u03a3\u2265 i/2, implying the same bound\n$\\frac{b_i}{\u03c1} > \\frac{1}{12a^2}$                                                                                                                                                                                                   (5)\nsince \u03a3 > 1. Taking a = 1/100, we obtain that the right-hand sides of the inequalities in (3), (4), (5) are all larger than 10, as required.\nFinally, we explain how to get rid of the assumption \u03a3 > 1. Let us denote the value of the expression (1) by 0. We have that 0 is positive for even \u2211 \u2265 1, and is negative for odd \u2211 > 1. Now, at the first layer, we can compute the quantity 1/(2n) \u2013 (x1 + . . . +xn)/n by taking the arithmetic mean of the input bits using sotftmax with uniform weights, and computing 1/(2n) by Lemma 1. Observe this quantity is positive for \u03a3 = 0 and negative otherwise. It remains to output max{0, 1/(2n) \u2013 (x1 + . . . + xn)/n} in the third layer."}]}