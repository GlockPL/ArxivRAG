{"title": "A completely uniform transformer for parity", "authors": ["Alexander Kozachinskiy", "Tomasz Steifer"], "abstract": "We construct a 3-layer constant-dimension transformer, recognizing the parity language, where neither parameter matrices nor the positional encoding depend on the input length. This improves upon a construction of Chiang and Cholak who use a positional encoding, depending on the input length (but their construction has 2 layers).", "sections": [{"title": "Introduction", "content": "One of the ways to do mathematical analysis of the capabilities and limitations of the transformer architecture [8] is to study formal languages, recognizable by them. Namely, for a given formal language L, we study, if there exists a choice of parameters in the transformer architecture, for which words from L are accepted and words not from L are rejected by the resulting transformer. A seminal work of Hahn [5] performed such analysis for a number of formal languages, including the parity language, consisting of binary words with even number of 1s. Hahn have shown that transformers, recognizing this language, must have low confidence, partially explaining an empirically observed struggle of transformers in learning this language [1, 4].\nHowever, this does not exclude the possibility of existence of a theoretical solution for this language. And indeed, as was shown by Chiang and Cholak [3], there exists a 2-layer transformer with constant embedding dimension, recognizing the parity language. Their construction is uniform in a sense that parameter matrices in it do no depend on the input length. However, there is one aspect of their construction which is not completely uniform \u2013 the positional encoding. At position i, they use i/n, where n is the input length. This means that their positional encoding has to be reset each time we want to use their transformer for a larger input length.\nIn this paper, we get rid of this disadvantage of their construction by giving a completely uniform 3-layer transformer, recognizing parity. That is, in our construction, neither parameter matrices nor positional encoding depend on n. We do not use neither positional masks nor layer norm.\nIt remains open if the parity language can be recognized by a 1-layer transformer with constant embedding dimension, even non-uniformly. Current lower bound methods against 1-layer transformers [2, 6, 7] do not seem to work against parity."}, {"title": "Preliminaries", "content": "An attention layer of dimension d is a length-preserving function A: $(\\mathbb{R}^d)^* \\rightarrow (\\mathbb{R}^d)^*$, given by three matrices $K,Q,O \\in \\mathbb{R}^{d\\times d}$ and a neural network $N: \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ with ReLU activation. On input $(f_1,..., f_n) \\in (\\mathbb{R}^d)^n$,"}, {"title": "Construction", "content": "Lemma 1. For any function f: N \u2192 R, there exists a completely uniform transformer that, for any input length n, computes f(n) in every position in one layer.\nProof. We need a positional encoding p: N \u2192 R such that:\n$\\frac{p(1) + ... + p(n)}{n} = f(n)$,\nfor every n (this average is computable via softmax with uniform weights), which is achievable by setting p(1) = f(1) and p(i) = if (i) \u2013 (i \u2013 1)f(i \u2013 1) for i \u2265 2.\nTheorem 1. There exists a 3-layer completely uniform transformer, recognizing the parity language.\nProof. Assume that on input we get $x = x_1 ... x_n \\in {0,1}^n$. Denote $\u03a3 = x_1 + ... + x_n$. Let us first give a construction, assuming that \u2211 \u2265 1, that is, that not all input bits are 0. We explain how to get rid of this assumption in the end of the proof.\nThe plan of the proof is as follows. At the first two layers, we need to compute a sequence of numbers\n$a_1, a_2,..., a_n,$\nwith the property that $a_\u03a3$ is strictly larger than any other number in the sequence. Assuming we have done this, at the last layer we can compute the following:\n$\\frac{\\sum_{i=1}^n exp{a_if(n)}(-1)^i}{\\sum_{i=1}^n exp{a_if(n)}}$"}]}