{"title": "Enhance Reasoning by Learning from Mistakes: Peer-Review Knowledge Distillation from Multiple Large Language Models", "authors": ["Zhuochun Li", "Yuelyu Ji", "Rui Meng", "Daqing He"], "abstract": "Large language models (LLMs) have exhibited complex reasoning abilities by generating question rationales and demonstrated exceptional performance in natural language processing (NLP) tasks. However, these reasoning capabilities generally emerge in models with tens of billions of parameters, creating significant computational challenges for real-world deployment. Recent research has concentrated on improving open-source smaller models through knowledge distillation (KD) from commercial LLMs. Nevertheless, most of these studies rely solely on the responses from one single LLM as the gold rationale for training. In this paper, we introduce a novel Mistake-Aware Peer-Review Distillation (MAPD) approach: 1) Instead of merely obtaining gold rationales from teachers, our method asks teachers to identify and explain the student's mistakes, providing customized instruction learning data. 2) We design a simulated peer-review process between teacher LLMs, which selects only the generated rationales above the acceptance threshold. This reduces the chance of teachers guessing correctly with flawed rationale, improving instructional data quality. Comprehensive experiments and analysis on mathematical, commonsense, and logical reasoning tasks demonstrate the effectiveness of our method.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have proven to be highly effective in addressing a wide range of complex tasks (Ni et al., 2024; Fan and Tao, 2024), including mathematical reasoning (Lewkowycz et al., 2022; Imani et al., 2023), commonsense reasoning (Zhao et al., 2024; Achiam et al., 2023), and logical reasoning (Liu et al., 2023; Xu et al., 2023b). However, these emergent reasoning abilities tend to manifest only in LLMs with more than 100 billion parameters, while smaller models struggle to exhibit such capabilities (Wei et al., 2022a). Despite this, recent research (Touvron et al., 2023; Zeng et al., 2022) has shown that smaller language models, particularly those with fewer than 10 billion parameters like LLama2-7B, can perform similarly to larger models in terms of following human instructions. However, it is challenging to prompt smaller Language Models (LMs) to generate reasoning steps by Chain-of-Thought (CoT) prompts (Wang et al., 2023). Moreover, most existing reasoning datasets lack high-quality rationale, which is defined as justifying a model's output by providing a natural language explanation for the final correct answer (Gurrapu et al., 2023), due to the high cost of manual annotations.\nTo address these challenges, distilling the capabilities of LLMs emerges as a resource-friendly and effective strategy. Through collecting rationales generated by LLMs for instruction tuning, previous studies have been able to distill the private LLMs' reasoning abilities into smaller models (Wang et al., 2022; Ho et al., 2023; Magister et al., 2022; Fu et al., 2023). However, most of these efforts fall within the scope of Labeling Knowledge Distillation (Xu et al., 2024b), where LLMs are primarily used to annotate data for training smaller models, without utilizing smaller model's output as feedback to generate customized instruction data to improve the LM in return. As a result, LLMs remain unaware of the limitations of smaller models, which hampers their ability to provide targeted analysis and feedback, reducing the effectiveness of the reasoning distillation process.\nFurthermore, prior research typically employs only one LLM as the teacher, which can introduce more biased training data compared to using multiple teacher LLMs during distillation. Therefore, we propose using multiple LLMs from different organizations as teachers to provide more impartial and diverse training data for the student LM. Additionally, we designed a simulated peer-review process between the teacher LLMs, where the rationale generated by one LLM is reviewed by other LLMs. Only the rationales that pass this peer-review process are included in the training dataset. This method reduces the likelihood of flawed rationales, even when a correct answer is provided, thereby improving the overall quality of the training data used for instruction tuning.\nTo this end, we propose a Mistake-Aware Peer-Review Distillation (MAPD) method via Peer-Review knowledge distillation from multiple LLMs, as briefly shown in Figure 1. Inspired by the natural human learning process (Konold et al., 2004), we argue that students should not only know what is the correct answer but also learn why they made mistakes. Therefore, in addition to providing the correct rationale generated by the teacher LLMs, we also present the student model's mistakes to the teacher LLMs and return mistake-specific feedback to the student model. This enables the student to learn both the \"what\" and the \"why,\" enhancing its reasoning abilities and equipping it to solve similar problems, even if it has not encountered them before. Furthermore, inspired by the multi-agent evaluation framework of Nan et al. (2023), we employ multiple LLMs as teachers and ask them the same question. Each teacher LLM's answer is reviewed by the other teachers, and only the responses that pass this peer-review process are included in the instruction training dataset. We believe this peer-review mechanism between teacher LLMs can significantly reduce biased or flawed rationales, leading to improved distillation performance. In summary, the contributions of our work are as follows:\n1.  The Mistake-Aware Peer-Review Distillation (MAPD) approach is introduced to help student LM learn not only from the gold-standard rationale but also from feedback on their own mistakes provided by teacher LLMs, which builds a comprehensive instruction tuning method aimed at enhancing the student LM's general reasoning abilities.\n2.  We design a simulated Peer-Review mechanism between teacher LLMs to filter out flawed rationales and improve the confidence of instruction tuning data.\n3.  Our work provides a comprehensive benchmark on the mathematical, commonsense, and logical reasoning tasks. Experiments and comparisons with other concurrent works demonstrate the effectiveness of our method in distilling the reasoning ability of teacher LLMs."}, {"title": "2 Related Work", "content": "LLM Reasoning Recent studies have focused on provoking the thought processes of LLMs, validating their effectiveness in reasoning tasks (Wei et al., 2022b; Imani et al., 2023; Fu et al., 2023), such as GSM8K (Cobbe et al., 2021), SVAMP (Patel et al., 2021), and StrategyQA (Geva et al., 2021). Various techniques have been developed to enhance LLM reasoning abilities (Chu et al., 2023; Xu et al., 2024a). For instance, Chain-of-Thought (CoT) (Wei et al., 2022b) improves reasoning by prompting LLMs to generate intermediate natural language thought processes. Huang et al. (2022) proves that LLMs can self-improve reasoning capability through self-training by collecting data using majority voting techniques. Chung et al. (2024) demonstrated that smaller LMs can partially acquire CoT skills by training on data with rationales. In this paper, we further show that the CoT performance of smaller LMs can be improved through integrated instruction learning using CoT data selected by majority voting from LLMs.\nKnowledge Distillation from LLMs Distilling knowledge from LLMs by fine-tuning smaller language models to follow instructions using high-quality data collected from LLMs has become a prominent research direction (Xu et al., 2023a; Fu et al., 2023; Li et al., 2024). This approach serves as an effective method for transferring the emergent abilities of black-box LLMs to smaller open-source models. However, while recent works (Ho et al., 2023; Shridhar et al., 2022; Guo et al., 2024) use LLM-generated reasoning rationales as supervisory signals to train smaller task-specific models, they often overlook providing student models with feedback on their mistakes when their answers are incorrect. To address this, we collect both the correct rationale and mistake-specific feedback for student models' wrong answers from LLMs, integrating them into instruction tuning to enhance the overall reasoning capabilities of the student models. Moreover, unlike most studies that rely on a single teacher LLM (Wang et al., 2023; Chenglin et al., 2023; Zhu et al., 2024), we employ multiple LLMs as teachers to increase the diversity of generated data. Finally, compared to studies of the peer-review in LLMs for evaluation (Ning et al., 2024; Chu et al., 2024), we design a simulated peer-review process to ensure high-quality instruction training data, thereby improving the distillation performance."}, {"title": "3 Method", "content": "As illustrated in Figure 3, we introduce a Mistake-Aware Peer-Review Distillation (MAPD) knowledge distillation method that empowers the student model to improve by learning from its own mistakes and the correct answers generated by multiple teacher models. Specifically, our instruction learning procedure involves four major steps: (1) The student LM takes an \u201cexam\" on the training set to identify mistakes that are incorrectly generated rationales. (2) We then craft various prompts that incorporate the question and the student's wrong rationale to prompt the teacher LLMs to generate the gold answers and provide feedback on the student's errors respectively. (3) Subsequently, a simulated peer-review process is conducted among the teacher LLMs to produce highly confident instructional data. (4) Finally, the student model learns to reason through instruction learning based on the peer-reviewed correct answers and tailored corrections on its mistakes provided by the teacher LLMs."}, {"title": "3.1 Exam on Student Model", "content": "We aim to gather samples from reasoning benchmarks where the student model incorrectly answers questions. These samples will be used to create customized instructional data from the teacher models. To achieve this, the student model undergoes an \"exam\" on the training set \\(D_{train}\\) to assess its reasoning ability and collect the mistake set \\(D_{mistake}\\), which are the samples containing incorrect rationales and answers. Specifically, given a dataset \\(D = \\{x, y\\}\\), where \\(x\\) is the question and \\(y\\) is the gold answer, we propose to input the question \\(x\\) into the student model to generate the output \\(f(x) = [r', y']\\). Here, the square brackets denote the concatenation of the student model's rationale \\(r'\\) and answer \\(y'\\), with the answer typically at the end of the output. Since the correct rationale \\(r\\) is often not provided in \\(D_{train}\\), we follow Wang et al. (2023)'s work by considering \\(r'\\) as the wrong rationale if \\(y' \\neq y\\). Finally, the mistake set \\(D_{mistake}\\) is collected as follows:\n\\[D_{mistake} = \\{(x, r', y') | y' \\neq y \\cap (x, y) \\in D_{train}\\} \\quad (1)\\]\nThe collected mistake set \\(D_{mistake}\\) highlights the student's reasoning weaknesses and will be utilized for the following purposes:\n1) Providing the incorrectly answered questions for the teacher LLMs to generate correct rationales.\n2) Using the student's incorrect rationales to prompt the teacher LLMs to identify errors and create customized mistakes feedback."}, {"title": "3.2 Inquiry Teacher LLMs with the Mistakes", "content": "We expect the teacher LLM to function as a reasoning instructor who can identify student's mistakes and provide tailored feedback, rather than merely an answer provider. Therefore, we query the teacher LLMs with the student's incorrectly answered questions, aiming for them to generate the correct rationale and identify specific errors in the student's mistakes. We believe that customized training data, which includes both \"what\" the correct answer is and \"why\" the mistakes were made, can effectively address the student's weaknesses. We use the same prompt \\(P_r\\) employed to conduct exams on the student model to obtain rationales from the teachers. For prompt \\(P_f\\) to gather feedback on the student's mistakes, we follow Zelikman et al. (2022) by adding a hint that explicitly provides the correct answer to the question, ensuring more accurate responses. The detailed prompt templates are shown in Figure 2. In detail, for each sample \\((x, r', y') \\in D_{mistake}\\), we request each teacher \\(T(x)\\) to generate rationale \\(r_t\\) and feedback \\(f_t\\) which will be collected as the feedback set \\(D_f\\):\n\\[r_t = T(P_r(x))\\]\\[f_t = T(P_f(x,r', y))\\]\\[D_f = \\{(x,r', f_t) | x \\in D_{train}\\} \\quad (2)\\]"}, {"title": "3.3 Simulated Peer-Review Between Teacher Models", "content": "During our experiments, we observed that the rationales provided by teacher LLMs are not always accurate, even when the final answer matches the gold answer. This discrepancy is rare in common mathematical tasks, where there is often a strict correlation between the correctness of the rationale and the final answer number due to the inherent nature of mathematics. However, for multiple-choice questions, such as those in the commonsense StrategyQA (Geva et al., 2021) (True or False) and logic LogiQA (Liu et al., 2020) (A, B, C, D) benchmarks, there are instances where a correct rationale may lead to an incorrect final choice, or a wrong rationale might result in a correct final choice. See Appendix C for more peer-review examples on different benchmarks.\nTo address this issue and avoid having teacher LLMs \"guess\" the correct answer, we propose a simulated peer-review process among teacher LLMs. Since most relevant datasets do not provide gold rationales, we assume that each LLM's rationale should be reviewed and scored by peer LLMs, which is inspired by the multi-agent evaluation framework of Nan et al. (2023). Only those rationales that pass this peer-review process with high confidence will be included in the final instructional tuning dataset. Figure 3 has explained the peer-review process. Specifically, assume we have three different teacher LLMs \\(T_1, T_2, T_3\\). We obtain their rationales based on Equation 2, resulting in \\(r_{t1}, r_{t2}, r_{t3}\\). For each rationale, we incorporate it into the designed peer-review prompt \\(P_{pr}\\) shown in Figure 4 and request the other LLMs to score the rationale as \\(S_t(r_t)\\). Only the rationale with an average score exceeding the acceptance threshold \\(T_h\\) will be included in the rationale set \\(D_r\\). The peer-review process can be described as follows:\n\\[S_{t_2}(r_{t_1})=T_2(P_{pr}(x, r_{t_1},y)), S_{t_3}(r_{t_1}) = T_3(P_{pr}(x, r_{t_1}, y))\\]\\[S_{t_1}(r_{t_2}) = T_1(P_{pr}(x, r_{t_2},y)), S_{t_3}(r_{t_2}) = T_3(P_{pr}(x, r_{t_2}, y))\\]\\[S_{t_1}(r_{t_3}) = T_1(P_{pr}(x, r_{t_3},y)), S_{t_2}(r_{t_3}) = T_2(P_{pr}(x, r_{t_3}, y))\\]\\[D_r = \\{(x, r_{t_1}) | if (S_{t_2}(r_{t_1}) + S_{t_3}(r_{t_1}))/2 \\geq T_h\\}\\]\\[\\cup \\{(x, r_{t_2}) | if (S_{t_1}(r_{t_2}) + S_{t_3}(r_{t_2}))/2 \\geq T_h\\}\\]\\[\\cup \\{(x, r_{t_3}) | if (S_{t_1}(r_{t_3}) + S_{t_2}(r_{t_3}))/2 \\geq T_h\\} \\quad (3)\\]"}, {"title": "3.4 Instruction Tuning on Student Model", "content": "The reasoning ability of the student LM can be enhanced through instruction tuning (Wei et al., 2021), which incorporates both gold answers and customized mistake corrections provided by the teacher models. See Appendix B for explicit instruction tuning templates on different benchmarks."}, {"title": "4 Experiments", "content": "4.1 Datasets\nMathematical Reasoning We focused on the two most popular math problem datasets to evaluate mathematical reasoning ability. GSM8K (Cobbe et al., 2021) is a dataset of 8.5K high-quality, linguistically diverse grade school math word problems created by human problem writers. SVAMP (Patel et al., 2021) is a challenge set for elementary-level Math Word Problems (MWP), consisting of short natural language narratives that describe a state of the world and pose questions about unknown quantities.\nCommonsense Reasoning StrategyQA (Geva et al., 2021) is a question answering benchmark where the required reasoning steps are implicit within the question and should be inferred using a commonsense strategy.\nLogical Reasoning LogiQA (Liu et al., 2020) is a dataset constructed from logical comprehension problems sourced from publicly available questions of the National Civil Servants Examination of China. These questions are designed to assess civil servant candidates' critical thinking and problem-solving abilities. For our experiments, we used only the English version of the dataset.\n4.2 Models & Baselines\nModels For teacher LLMs, considering the expense and accessibility of our diverse LLMs, we select GPT-3.5-turbo\u00b9, Gemini-1.0-pro (Team et al., 2023), and Mixtral-8x7B-Instruct-v0.1 (Jiang et al., 2024) as teacher models. These three LLMs were built by different organizations but all proved powerful NLP abilities. Furthermore, Mixtral-8x7B-Instruct-v0.1 is an open-source model that is different than the other two private ones, we assume all these variations will make our multiple-teacher distillation more impartial. In addition, we choose the open-source LM Llama2-7B-chat (Touvron et al., 2023) for its leading performance among similar-size models and active community to compare our work.\nBaselines To demonstrate the effectiveness of our method, we compare it against the following baselines: (1) The teacher LLMs and student LM without fine-tuning, to highlight the impact of distilling reasoning abilities from the teachers. (2) Sophisticated distillation methods applied to GPT-series and T5 open-source models with fewer parameters (Shridhar et al., 2022; Wang et al., 2023; Zhu et al., 2024). (3) Three relevant works that utilize LLMs to enhance reasoning capabilities of Llama-7B (Li et al., 2024) and Llama2-7B (Guo et al., 2024; Mitra et al., 2023), with a particular focus on mathematical reasoning. (4) Two distillation approaches that use T5-XXL as the student LM, which has a larger parameter size (Fu et al., 2023; Magister et al., 2022). (5) Finally, our methods with individual-LLM and multiple-LLMs, demonstrate the advantage of multiple-teacher distillation in improving the reasoning ability compared to the single teacher."}, {"title": "4.3 Experimental Setup", "content": "All three teacher LLMs were configured with a Temperature of 0.8 and Max_tokens set to 512. The student model was instruction-tuned using a learning rate of 1e-5 over 10 epochs with AdamW (Loshchilov, 2017) as the optimizer in its default settings. The parameter a in Equation 6 was set to 0.5 to balance the impact of learning from mistakes. The datasets were downloaded from Huggingface, utilizing the standard train/test set split. All evaluation results are based on the zero-shot setting. Primary experiments were conducted on four Nvidia A100-80GB GPUs. Additional implementation details can be found in Appendix A."}, {"title": "5 Results and Analysis", "content": "The evaluation results are demonstrated in Table 1.\n5.1 Advantage of Distillation\nThe inference results of the student LM showed significant improvement after applying knowledge distillation. The test accuracy after instruction tuning on Llama2-7B-chat has improved from 16.55% to 36.24% on GSM8K, 44.71% to 59.50% on SVAMP, 48.53% to 67.67% on StrategyQA, and 16.50% to 36.27% on LogiQA. While there remains a noticeable gap between the student LM and teacher LLMs in mathematical reasoning, the fine-tuned Llama2-7B-chat demonstrated comparable performance in other reasoning tasks. Considering that we used only a subset of the training data that didn't pass the exam, it is notable that the results still outperformed the weakest LLMs in commonsense and logical reasoning tasks, despite the student models being significantly smaller in size. The exam result on the original Llama2-7B-chat can be found in Table 2."}, {"title": "5.2 Comparison of Concurrent Methods", "content": "Table 1 presents the results of our approach alongside other relevant distillation methods. When compared to models with fewer parameters, such as GPT2-Large and GPT-J with advanced distillation techniques, our method consistently outperforms them. However, this improvement could be attributed to the enhanced reasoning capabilities provided by the larger parameter size. To address this, we also compared our approach with different distillation methods based on the same student LM. For the GSM8K benchmark, our performance (36.24%) lags behind Llama-7B+NCE (41.93%) and ReversalMath (52.10%), likely because these models were exclusively fine-tuned on mathematical tasks, with GSM8K being a key and difficult benchmark in this domain. The other trained mathematical datasets improved student LM's overall mathematical reasoning capability. In addition, we utilized only a subset of the training data that did not pass the exam, which is significantly smaller compared to the training data used in other studies. Nevertheless, our approach still yields better performance on another easier and smaller mathematical benchmark, SVAMP (59.50%). Additionally, our superior results on LogiQA (36.27%) compared to ORCA2-7B (35.02%) highlight the effectiveness of our peer-reviewed distillation method in enhancing logical reasoning. Finally, to assess the importance of model size and distillation method, we compared our approach with a larger model, T5-XXL. Despite having fewer parameters, our well-designed reasoning distillation method enables us to achieve better results than those based on T5-XXL."}, {"title": "5.3 Effectiveness of Distillation via Multiple Teacher LLMS", "content": "As shown in Table 1, our multiple-teacher distillation with peer-review method improves average accuracy by 6.16% across all four benchmarks compared to single teacher distillation methods using Mistral, Gemini, and GPT respectively. This improvement indicates that the rationale flaws present in a single teacher's output can be mitigated by peer-review between multiple teachers. Consequently, our distillation enables the student model to learn from more convincing and solid rationales, leading to enhanced reasoning abilities.\nAdditionally, our findings reveal that different teacher LLMs possess varying levels of rationale capability, resulting in performance differences on the same benchmark. For instance, GPT-3.5-turbo demonstrates superior 78.01% accuracy on mathematical reasoning compared to Mistral (74.40%) and Gemini (76.42%), while Mistral excels in commonsense reasoning with 72.83% and Gemini performs better in logical reasoning tasks with 40.55%. Detailed comparisons of the student LM's output before and after distillation are provided in Appendix D."}, {"title": "5.4 Analysis about Necessity of Peer-Review", "content": "To assess the importance of the peer-review process further, we compare the evaluation results with and without peer-review, keeping all other settings constant, as shown in Table 3. When peer-review is absent, the test accuracy across all benchmarks will decrease by 7.84% on average. It strengthens that answers generated by multiple teachers may contain varying rationales, potentially confusing the student model during instruction tuning.\nIn addition, the experiments without peer-review even fall behind the best single teacher-GPT distillation outcomes on GSM8K (29.65%<30.71%). This pattern is particularly pronounced in commonsense and logical reasoning tasks, where the absence of peer-review leads to the poorest performance: 56.52% and 29.63% respectively. These findings align with our assumption that peer-review may have a smaller impact on mathematical reasoning tasks, where the rationale and final result are highly correlated, but significantly improves the quality of instruction data in commonsense and logical reasoning tasks."}, {"title": "5.5 Abalation Study of Learning from Mistakes", "content": "As a key component of our MAPD method, we initially set the proportion of learning from mistakes to 0.5 in previous experiments for simplicity. To explore the influence of balancing learning from gold rationales and learning from mistakes, we adjusted the value of a in Equation 6 across different extents of the two learning approaches. Specifically, a was varied from [0, 0.25, 0.5, 0.75, 1], and experiments were conducted on all benchmarks for 5 epochs, while keeping other parameters constant. Figure 5 visualizes how learning from mistakes affects instruction-tuning. Our findings support the hypothesis that learning from mistakes positively impacts instruction tuning. However, the relationship is not uniformly positive across all a values on the four benchmarks.\nFor GSM8K and LogiQA, the benefits of learning from mistakes increase when a < 0.25, but start to decrease when a exceeds 0.25. Conversely, for StrategyQA and SVAMP, the advantages of learning from mistakes consistently grow and reach their peak when a = 0.75. These results suggest that placing too much emphasis on learning from mistakes (i.e., a higher a value) can lead to diminished performance and increased instability. Consequently, it is important to evaluate and optimize the a value for different tasks to effectively balance the learning of \"what\" (correct answers) and \"why\" (understanding mistakes) during training."}, {"title": "6 Conclusion", "content": "In this paper, we introduce a novel distillation approach called the Mistake-Aware Peer-Review Distillation (MAPD) method. First, we implement a simulated peer-review process between multiple teacher LLMs to gather highly reliable and less biased outputs, which refines the quality of instruction tuning dataset. Additionally, we developed an integrated instruction tuning method that allows the student LM to learn from both the gold rationale and feedback on its mistakes provided by the teacher LLMs. Comprehensive results from mathematical, commonsense, and logical reasoning tasks highlight the success of the MAPD method in unlocking the reasoning potential of smaller LMs. We hope that our findings will encourage further investigations into distillation LLMs' reasoning capabilities."}, {"title": "Limitations", "content": "Although our method has demonstrated effectiveness in the reasoning ability distillation from teacher models to the student model, this technique has several limitations. First, our experiments primarily rely on GPT-3.5-turbo, Gemini-1.0-pro, and Mixtral-8x7B-Instruct-v0.1 as teacher LLMs due to considerations of availability and cost. Future research could benefit from using more powerful models like GPT-4, OpenAI 01, and Claude-3 Opus. Additionally, we selected Llama2-7B as the student LM for its training availability and the robust open-source community that allows us to benchmark our results against related work. Future studies might explore more advanced models like Llama3 to further validate the approach. Secondly, due to time and cost constraints, our method does not collect the student LM's incorrect rationales and update the instruction dataset after each epoch. The potential benefits of continuously incorporating fresh data throughout training remain unexplored. Lastly, we employed the default cross-entropy loss function for instruction tuning. It would be worthwhile to explore more sophisticated methods, such as Reinforcement Learning with Human Feedback (RLHF), and to integrate additional techniques into the joint learning framework."}, {"title": "A Experimental Setup Details", "content": "A.1 Datasets Statistics\nWe downloaded datasets GSM8K, SVAMP, StrategyQA, and LogiQA from Huggingface. All datasets are split according to the official original split ratio. The dataset statistics are shown in Table 4."}, {"title": "A.2 Teacher LLMs Parameters", "content": "Table 5 shows the unified parameters setting for GPT-3.5-turbo, Gemini-1.0-pro, and Mixtral-8x7B-Instruct-v0.1 LLMs to generate answers for the student LM. All inferences from teacher LLMs are acquired by APIs."}, {"title": "A.3 Student LM Parameters", "content": "Experiments are performed with the Huggingface Trainer framework and Flash Attention (Dao et al., 2022). We use four Nvidia A100-80GB GPUs with FP16 for training and evaluation. The inference parameter settings across all datasets are shown in Table 6. The adopted training hyperparameter settings across all datasets are shown in Table 7."}, {"title": "B Instruction Tuning Templates", "content": "Instruction tuning templates for learning from mistakes.\nFor all benchmarks:\n\"### Instruction: Imagine you are a teacher, I will give you one student's incorrect answer to a question. You should point out the mistakes in the student's answer.\n### Input: {}\n### Response: {}\"\nInstruction tuning templates for learning from gold rationale.\nFor benchmarks GSM8K and SVAMP:\n\"### Instruction: Answer the following question. Let's think step by step.\n### Input: {}\n### Response: {}\"\nFor benchmark strategyQA:\n\"### Instruction: Answer the following question. Let's think step by step. First, you should answer \"true\" or \"false\". Then, you should explain how you draw this conclusion.\n### Input: {}\n### Response: {}\"\nFor benchmark logiQA:\n\"### Instruction: Answer the following question based on the given context, query, and options. Let's think step by step.\n### Input: {}\n### Response: {}\""}, {"title": "CPeer-Review Examples", "content": "Table 8 provides detailed examples of the peer-review process on GSM8K and StrategyQA. It highlights instances where the causality between the teacher LLM's rationale and the final answer may be insufficient, and demonstrates how our peer-review mechanism effectively identifies the most confident rationales."}, {"title": "D Case Study of Distillation Impact on Student LM's Output", "content": "Table 9 provides the comparisons of student LM's behavior difference before and after the instruction tuning across four benchmarks ."}]}