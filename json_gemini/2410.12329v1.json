{"title": "UNDERSTANDING THE ROLE OF LLMS IN MULTI-MODAL EVALUATION BENCHMARKS", "authors": ["Botian Jiang", "Lei Li", "Xiaonan Li", "Zhaowei Li", "Xiachong Feng", "Lingpeng Kong", "Qi Liu", "Xipeng Qiu"], "abstract": "The rapid advancement of Multimodal Large Language Models (MLLMs) has been accompanied by the development of various benchmarks to evaluate their capabilities. However, the true nature of these evaluations and the extent to which they assess multimodal reasoning versus merely leveraging the underlying Large Language Model (LLM) backbone remain unclear. This paper presents a comprehensive investigation into the role of LLM backbones in MLLM evaluation, focusing on two critical aspects: the degree to which current benchmarks truly assess multimodal reasoning and the influence of LLM prior knowledge on performance. Specifically, we introduce a modified evaluation protocol to disentangle the contributions of the LLM backbone from multimodal integration, and an automatic knowledge identification technique for diagnosing whether LLMs equip the necessary knowledge for corresponding multimodal questions. Our study encompasses four diverse MLLM benchmarks and eight state-of-the-art MLLMs. Key findings reveal that some benchmarks allow high performance even without visual inputs and up to 50% of error rates can be attributed to insufficient world knowledge in the LLM backbone, indicating a heavy reliance on language capabilities. To address knowledge deficiencies, we propose a knowledge augmentation pipeline that achieves significant performance gains, with improvements of up to 60% on certain datasets, resulting in a approximately 4x increase in performance. Our work provides crucial insights into the role of the LLM backbone in MLLMs, and highlights the need for more nuanced benchmarking approaches.", "sections": [{"title": "1 INTRODUCTION", "content": "The rapid development of Large Language Models (LLMs) (Touvron et al., 2023; Bai et al., 2023a), combined with advancements in visual encoders (Radford et al., 2021; Zhai et al., 2023) and modality bridge techniques (Liu et al., 2023a; Dai et al., 2023), has catalyzed the evolution of Multimodal Large Language Models (MLLMs) capable of comprehending diverse multi-modal inputs. Concurrently, diverse benchmarks and leaderboards have emerged to evaluate various multimodal perception and reasoning capabilities (Lu et al., 2022b; Lerner et al., 2022; Yue et al., 2024a).\nWhile these benchmarks aim to assess multimodal capabilities, the role of the underlying LLM backbone in MLLM performance remains poorly understood. Recent studies (Tong et al., 2024; Yue et al., 2024c) have highlighted that some benchmarks demonstrate an excessive dependence on the language model component, allowing MLLMs to achieve high scores even without visual inputs. This observation raises critical questions about the true nature of multimodal reasoning in these models and the extent to which performance is driven by the LLM backbone rather than multimodal integration. Furthermore, as different MLLMs utilize LLM backbones with distinct knowledge priors learned from various pre-training corpora (Gao et al., 2020; Penedo et al., 2023), this knowledge inconsistency leads to incomparable evaluation scores when comparing MLLMs"}, {"title": "2 METHOD", "content": "In this section, we perform an approach to better understand the role of LLM in multi-modal evaluation benchmarks. Figure 1 provides a comprehensive overview of the method. We begin by formally introducing the key notations essential for setting up our framework (\u00a72.1). Following this, we delve into the specifics of how we measure the significance of vision and knowledge. First we outline the methodologies for evaluating the role of vision (\u00a72.2). We then explore the methodologies for gauging the impact of factual knowledge $2.3 and develop a knowledge-augmented framework to assist the MMLMs (\u00a72.4)."}, {"title": "2.1 PROBLEM NOTATIONS", "content": "VQA involves providing a model with visual input and a related question, and then requiring the model to generate an appropriate answer. Let D be a given multi-modal dataset. For any data entry d in D, we define d as a triple (I, Q, A), where I denotes the visual input (a single image in our work), Q represents the textual question, and A is the corresponding answer. We posit that MLLMs process VQA tasks through two primary stages: visual perception and knowledge reasoning. In the visual perception stage, the model extracts key information from the image input. Subsequently, we hypothesize that the model internally reformulates the original VQA question into a cognate knowledge reasoning query, denoted as K. This query K integrates both the textual input and the extracted visual information, forming the basis for the ensuing reasoning process to derive the answer.\nRegarding the representation of the multi-modal large language model (MLLM), we employ the function form f, where f(I,Q), f(\u00d8, Q), and f(\u00d8, K) denote the model's responses to visual questions with images, without images, and to knowledge reasoning queries respectively. The visual perception step is described by P, with ~ used instead of = to reflect the non-deterministic nature of visual conception. For evaluation, we define a combination C as one of these three input types. Given a dataset D, we calculate the Score Rate (SR) as:\n$SR_D^C = \\frac{1}{|D|} \\sum_{d \\in D} 1[f(C_d) == A_d].$                                                                                                                                                            (1)\nFor instance, $SR_\\text{Viquae}^{(\\emptyset,K)}$ represents the model's performance on knowledge reasoning questions in the Viquae dataset. Empirically, we believe that a higher SR of a model indicates stronger performance, and vice versa."}, {"title": "2.2 IS VISUAL CAPABILITY NECESSARY?", "content": "Previous research has demonstrated that the absence of visual input often does not significantly impact model performance on certain visual evaluation datasets (Goyal et al., 2017; Tong et al., 2024; Huang et al., 2024). To elucidate this phenomenon, we extend prior work by systematically modifying the VQA task paradigm to assess the role of visual information under varied conditions. Our methodology involves presenting identical questions to models in image-present and image-absent contexts. Furthermore, we introduce two critical modifications to the multiple-choice format: (1) randomization of multiple-choice option order and (2) reformulation of questions into open-ended queries. These alterations serve dual purposes: randomization of options mitigates potential biases towards specific answer types that may align with training data distributions, while open-ended reformulation allows us to evaluate whether the apparent diminished reliance on visual information is an artifact of constrained multiple-choice setups, where some options may be trivially eliminable. Our findings indicate that the presence of multiple-choice options significantly reduces both task difficulty and the necessity for visual information processing. This insight offers a nuanced perspective"}, {"title": "2.3 Do MLLMS HAVE SUFFICIENT PRIOR KNOWLEDGE?", "content": "Based on our hypothesis that the resolution of visual tasks could be delineated into two distinct steps, upon receiving an image I and a textual question Q, the model implicitly engages in a visual perception process P to generate a corresponding knowledge reasoning problem K, then subsequently utilized it to make response. We formalize the entire process as follows:\n$1 [f(I, Q) == a] = 1 [P(I, Q) \\sim K] \\cdot 1 [f(\\emptyset, K) == a] .$                                                                                                                                                                                                                             (3)\nThe formula suggests that language prior knowledge and visual perception capabilities are equally crucial and indispensable. Therefore, the reason for models' poor evaluation results may not only stem from insufficient visual capabilities but also from a lack of knowledge.\nTo determine whether the model's knowledge is sufficient, we use knowledge reasoning questions corresponding to visual questions in each dataset as models' inputs and then evaluate their performance. For datasets that do not provide corresponding knowledge reasoning questions, we directly replace image-referenced content in visual questions with given entities or invoke GPT-42 (Achiam et al., 2023) to convert the original visual questions (specific prompts employed are detailed in the Appendix A.1).\nWe also perform a statistical analysis about model's correctness and errors in each visual question and its corresponding knowledge reasoning question. To quantify the analysis results, we introduce the following two indicators, Sufficiency Ratio (SuR) and Necessity Ratio (NeR), defined as follows:\n$SuR_D = \\frac{\\sum_{d \\in D} 1 [f(I_d, Q_d) == A_d | f(\\emptyset, K_d) == A_d]}{\\sum_{d \\in D} 1 [f(I_d, Q_d) == A_d]}$                                                                                                                                (4)\n$NeR_D = \\frac{\\sum_{d \\in D} 1 [f(I_d, Q_d) \\neq A_d | f(\\emptyset, K_d) \\neq A_d]}{\\sum_{d \\in D} 1 [f(I_d, Q_d) \\neq A_d]}$                                                                                                                                (5)\nThese ratios serve to elucidate the sufficiency and necessity relationship between prior knowledge and visual capability, where higher values signify a more robust relationship."}, {"title": "2.4 CAN KNOWLEDGE AUGMENTATION IMPROVE MULTIMODAL CAPABILITIES?", "content": "In real-world scenarios, models often encounter the issue of insufficient knowledge due to their smaller scale or outdated information (Gao et al., 2023). To mitigate the limitation caused by the absence of prior knowledge, we adopt a straightforward idea here, using the Retrieval-Augmented Generation (RAG) approach to effectively enhance the model's knowledge and then design proper experiments for effectiveness evaluation (Weston et al., 2018; Cai et al., 2019).\nWe evaluate the relevance between the knowledge reasoning problem and the paragraph using cosine similarity, and then rank the paragraphs accordingly. Ultimately, the highest-ranked paragraphs are incorporated into the input to enhance the model's knowledge base. Within the framework of RAG,"}, {"title": "3 EXPERIMENTS", "content": "As described in the preceding sections, we conduct extensive experiments to investigate the role of LLMs in MLLM evaluation. We first introduce the experimental settings (\u00a73.1. We then discuss our findings regarding the shortcuts used by LLMs during evaluation (\u00a73.2) and the knowledge deficiency (\u00a73.3). Finally, we illustrate interesting cases during our investigation (\u00a73.4) and evaluate the effectiveness of our knowledge augmented method (\u00a72.4)."}, {"title": "3.1 EXPERIMENTAL SETUP", "content": "Benchmarks. The datasets we use primarily assess the models' knowledge, generalizability, and reasoning abilities, and do not include datasets that primarily rely on visual recognition capabilities such as OCR. The specifics are as follows:\n\u2022 Viquae (Lerner et al., 2022): Viquae comprises a test set of 1257 questions, is a visual version of the Named Entity Question Answering dataset, requiring the identification of named entities in images and then reasoning to answer questions based on the model's inherent knowledge.\n\u2022 ScienceQA (Lu et al., 2022a): SQA consists of multimodal multiple-choice questions on various topics which is sourced from elementary and secondary school science curricula. We selected questions from the test set that have visual context for testing, totaling 2017 items.\n\u2022 InfoSeek (Chen et al., 2023): InfoSeek is a dataset composed of visual information-seeking questions necessitating the model to draw upon fine-grained knowledge learned from pretraining instead of commonsense knowledge to formulate responses. We sampled 3000 questions from its validation set for testing purposes.\n\u2022 MMMU (Yue et al., 2024b): MMMU is composed of multimodal questions collected from university exams, quizzes, and textbooks, requiring the model to possess university-level subject knowledge and excellent reasoning abilities. We selected 648 single-image questions from a subset of its validation set for testing (further details of selected subset are available in Appendix B.1).\nTest Models and Setup. We conduct experiments using open-source multi-modal large models from different sources, ranging in scale from 4.2 billion to 76 billion parameters, including Qwen-VL (Qwen2-VL-7B-Instruct)(Bai et al., 2023b), Idefics (I defics2-8B) (Lauren\u00e7on et al., 2024), LLaVA (LLAMA3-LLAVA-Next-8B, LLaVA-Next-Yi-34B) (Li et al., 2024), Phi-3 (Phi-3.5-vision-instruct) (Abdin et al., 2024), ChatGLM (GLM-4V-9B) (GLM et al., 2024), InternVL (InternVL2-Llama3-76B) (Chen et al., 2024b) and MiniCPM (MiniCPM-V-2.6) (Yao et al., 2024). We use a temperature of 0 for all models for deterministic results. To ensure more accurate evaluation results, we employed different evaluation methods for open-ended questions and multiple-choice questions. On open-ended tasks, we evaluate the correctness of models' responses by determining whether the candidate answers are present in the output of models through rule-matching. As to multiple-choice questions, we use DeepSeek-AI (2024) to assess whether the model's reasoning results are correct. The specific prompt used for determining the correctness of the model's outputs can be found in Appendix A.2\nPrompts. For open-ended problems from Viquae and InfoSeek, we simply use the questions as input into the models. For multiple-choice questions from ScienceQA and MMMU, we concatenate the questions and options as the example shown in the Appendix A.3 to form the model's prompt without appending any additional information such as the topic in MMMU or the hint in SQA."}, {"title": "3.2 LLMS EXPLOIT SHORTCUTS IN VISION TASKS", "content": "We conduct comparative experiments using eight models on four datasets, comparing the performance of the image-included setup with the image-excluded setup to enhance the broad applicability and reliability of the analysis. We also calculate the expected score for multiple-choice questions via randomly guessing. The outcomes are shown in Figure 2. Nearly all GR values below 0.8 suggest that visual information is not always essential, echoing previous findings (Yue et al., 2024b; Tong et al., 2024). Even on open-ended question-answering tasks like Viquae and InfoSeek, models still achieve average SRs of approximately 0.25 and 0.07 without using visual inputs. This could be due to the model having learned similar data during its training process, as the data for these datasets is sourced from Wikipedia, which is widely used in pre-training or supervised fine-tuning stages. As to multiple-choice questions like SQA and MMMU, models' average GR on these two datasets is only 0.18 and 0.15, respectively. Such low GR values indicate a negligible role of visual input in performance.\nOn MMMU, we explore the correlation between question setup and the role of vision by shuffling or removing the initial options of each question, with the results visualized in Table 1. Obviously, our changes to the question setup pose greater challenges to the MLLMs, as almost all models' SRs have decreased to some extent. Analyzing from the perspective of GR, shuffling the initial options has a relatively minor overall impact. However, the removal of options leads to a significant increase in the maximum GR, rising from 24.6 to 59.7 percent, representing an over 100% enhancement. Combin-"}, {"title": "3.3 MLLMS SUFFERS FROM LLMS' KNOWLEDGE DEFICIENCY", "content": "Knowledge deficiency of Large Language Models (LLMs) in Visual Question Answering (VQA) tasks are evident, even for state-of-the-art systems. As demonstrated in Table 2, InternVL, despite being equipped with the powerful LLaMA3-70B, achieves an average SR not exceeding 90% across various datasets. This performance ceiling is even more pronounced in smaller models, which exhibit average SRs of approximately 70% across diverse datasets. These findings suggest that all models used in our experiments face the challenge of inadequate knowledge.\nFurthermore, our analysis of SuR and NeR also substantiates the significant impact of prior knowledge on visual capability, as presented in the Figure 3. Taking Phi-3 as an example, it possesses relevant knowledge for over 85% of the visual questions it correctly answered on Viquae. At the"}, {"title": "3.4 CASE STUDY", "content": "We present specific cases in Figure 4 that challenge our initial assumption regarding the decomposition of visual tasks into perception and reasoning steps. An example involves a question about the venue of The Beatles' last ever live concert. In the VQA context, the model correctly identifies The Beatles in the image and subsequently deduces that Candlestick Park was the venue for their last concert. Paradoxically, when presented with the same query as a pure knowledge reasoning question without visual input, the model fails to provide the correct answer. This observed performance disparity may be attributed to the knowledge representation within the model's architecture. We hypothesize that the relevant information is encoded in the model's parameters in a manner that is more closely aligned with visual question-answering paradigms. Consequently, the presence of this image in the input potentially serves as a more effective retrieval cue, facilitating the model's access to pertinent knowledge.\nThe model sometimes demonstrates proficiency in accurately answering knowledge reasoning queries, exemplified by its correct responses regarding Barack Obama. However, when confronted with visual questions, it exhibits a propensity for hallucination during the reasoning process, despite accurately identifying the Westminster Hall. This discrepancy suggests a misalignment between visual and textual modalities. While the model possesses the requisite knowledge, as evidenced by its performance on purely text-based queries, it struggles to effectively apply this prior knowledge to visual task resolution."}, {"title": "3.5 RETRIEVED KNOWLEDGE BOOSTS MULTIMODAL ABILITIES", "content": "Since the lack of knowledge is inevitable, to compensate for this deficiency, it is natural to consider using the RAG approach to enhance the model's knowledge. We employ the embedding model to retrieve the most relevant content from Wikipedia (June 2024 Wikipedia dump) for each knowledge reasoning question on InfoSeek and Viquae, and incorporate the information into the corresponding input. The recall on this two datasets is presented in Table 3. The performance of all models in solving visual tasks has significantly improved after knowledge enhancement, as shown in the Figure 5. Nevertheless, in contrast to the recall that increases with the number of relevant documents, the model's SR demonstrates a trend of initially rising and then slightly decreasing.,"}, {"title": "4 RELATED WORK", "content": "Multimodal Large Language Models. Multi-modal Large Language Models (MLLMs) have made remarkable strides in recent years (OpenAI, 2023a; Reid et al., 2024; Ormazabal et al., 2024), demonstrating an unprecedented ability to understand and generate content that seamlessly integrates visual and textual information (Fu et al., 2023). Representative proprietary commercial models, such as OpenAI's GPT-40 (Achiam et al., 2023), Google's Gemini 1.5 Pro (Reid et al., 2024), and Anthropic's Claude 3.5 Sonnet (Anthropic, 2024), have showcased impressive capabilities in various tasks. On the open-source front, models like LLaVA (Liu et al., 2023a), Qwen-VL (Bai et al., 2023b) and Phi-Vision (Abdin et al., 2024), have also demonstrated remarkable progress, particularly in their ability to comprehend multiple images or video simultaneously, expanding the scope of MLLMs from static single images to dynamic multi-frame visual content. Our research aims to gain a deeper understanding of MLLM's performance and limitations, with a special focus on the role of the LLM backbone. Our experimental results show that current MLLMs rely on the LLM backbone heavily on certain benchmarks, and suffer from knowledge deficiency when facing VQA tasks demanding rich world knowledge. Based on our findings, we introduce a RAG-based method that significantly enhances model performance.\nMultimodal Understanding Benchmarks. The rapid advancement of MLLMs has spurred the development of diverse evaluation benchmarks. These range from specialized tasks like OCR (e.g., InfogrpahicsVQA (Mathew et al., 2022), ChartVQA (Masry et al., 2022), DocVQA (Mathew et al., 2021)), knowledge integration (e.g., Viquae (Lerner et al., 2022) and Infoseek (Chen et al., 2023)), and mathematical reasoning (e.g., ScienceQA (Lu et al., 2022b) and MathVista (Lu et al., 2023)), to comprehensive frameworks such as MMMU (Yue et al., 2024b), MME (Fu et al., 2023), MM-Bench (Liu et al., 2023b), and MMVet (Yu et al., 2023). Our work contributes to this landscape by critically examining these evaluation frameworks, echoing previous findings that visual inputs may contribute less significantly in these benchmarks (Yue et al., 2024c; Chen et al., 2024a; Tong et al., 2024). Additionally, we show that the multiple-choice format could become a shortcut that"}, {"title": "5 CONCLUSION", "content": "This study provides a comprehensive analysis of the role of LLM backbones in Multimodal Large Language Model (MLLM) evaluation, shedding light on critical aspects that have been largely overlooked in previous research. Our investigation reveals several key insights that have significant implications for the development and evaluation of MLLMs. Our experimental findings first show that LLMs could exploit shortcuts by relying on inappropriate options in visual tasks, and that open-ended questions could offer more robust assessments. Secondly, we identify substantial knowledge deficiencies across various datasets, where models fail to provide correct answers despite accurate visual perception. To mitigate this, we implement a Retrieval-Augmented Generation (RAG) approach, which significantly improved performance on visual tasks by enhancing the models' factual knowledge. Further analysis reveals a phenomenon of knowledge misalignment between visual and textual modalities."}, {"title": "LIMITATIONS", "content": "Since only a portion of the models used in our experiments support multi-image input, and some questions are difficult to accurately convert into corresponding knowledge inference tasks, we selected only a subset of the MMMU dataset. Additionally, due to the scarcity of multi-modal embedding models, we opted to use knowledge inference questions instead of visual questions for retrieval during the RAG process. In future work, we plan to employ multi-modal retrievers to identify the most relevant paragraphs for VQA questions and evaluate the effectiveness."}, {"title": "A PROMPT", "content": "A.1 CONVERT VQA QUESTIONS\nSince some datasets do not provide knowledge reasoning questions corresponding to visual questions, we have designed a sophisticated prompt that inputs the original visual input, text question, and corresponding answer into GPT-4 to transform the question. The specific prompt is as Table 4."}, {"title": "B DATASET SETUP", "content": "B.1 MMMU SUBSET\nIn the Table 7, we present the specific subsets of MMMU that we selected, along with the number of questions in each subset. Moreover, we provide the Success Rate (SR) using GPT-4 on its transformed knowledge reasoning questions."}]}