{"title": "SpatialCoT: Advancing Spatial Reasoning through Coordinate Alignment and Chain-of-Thought for Embodied Task Planning", "authors": ["Yuecheng Liu", "Dafeng Chi", "Shiguang Wu", "Zhanguang Zhang", "Yaochen Hu", "Lingfeng Zhang", "Yingxue Zhang", "Shuang Wu", "Tongtong Cao", "Guowei Huang", "Guangjian Tian", "Xingyue Quan", "Jianye Hao", "Yuzheng Zhuang"], "abstract": "Spatial reasoning is an essential problem in embodied Al research. Efforts to enhance spatial reasoning abilities through supplementary spatial data and fine-tuning have proven limited and ineffective when addressing complex embodied tasks, largely due to their dependence on language-based outputs. While some approaches have introduced a point-based action space to mitigate this issue, they fall short in managing more intricate tasks within complex environments. This deficiency arises from their failure to fully exploit the inherent thinking and reasoning capabilities that are fundamental strengths of Vision-Language Models (VLMs). To address these limitations, we propose a novel approach named SpatialCoT, specifically designed to bolster the spatial reasoning capabilities of VLMs. Our approach comprises two stages: spatial coordinate bi-directional alignment, which aligns vision-language inputs with spatial coordinates, and chain-of-thought spatial grounding, which harnesses the reasoning capabilities of language models for advanced spatial reasoning. We evaluate SpatialCoT on challenging navigation and manipulation tasks, both in simulation and real-world settings. Experimental results demonstrate that our method significantly outperforms previous state-of-the-art approaches in both tasks.", "sections": [{"title": "1. Introduction", "content": "Spatial reasoning, defined as the cognitive ability to visualize, manipulate, and comprehend the relationships between objects, is fundamental for performing everyday tasks such as navigating environments, assembling furniture, and organizing items on a table. Recent advancements in Large Language Models (LLMs) [19] and Vision Language Models (VLMs) [34] have established spatial reasoning as a crucial tool for embodied AI researchers in completing embodied tasks. However, most VLMs [5, 10, 17, 25, 36] are trained on standard 2D images and text datasets, which lacks the information necessary for understanding spatial relationships, thereby limiting their spatial reasoning abilities. Some works have attempted to enhance these capabilities by integrating additional spatial data and refining the models accordingly [3, 6, 12]. Nevertheless, these efforts primarily focus on language-based reasoning, resulting in models that produce only coarse-grained reasoning results. This constraint significantly limits the range of embodied applications, particularly within tasks that demand sophisticated action decisions. For example, when a robot receives an instruction to set up a table and the model generates subtasks such as \"1) Put the cup on the top left to the right of the plate., 2) Put ...\", these commands are easy for humans to interpret. However, for a low-level policy, often implemented using a smaller model such as decision transformer [4] or diffusion model [7, 11], determining the correct placement of the cup while avoiding collisions with other objects presents a significant challenge, rendering the command ambiguous.\nRecent work, such as RoboPoint [31] and RoboSpatial [23], addresses this issue by introducing a point-based action space. For instance, given a spatially related instruction like \"left of bowl and on the tarp\", the model generates one or several points on the input image to indicate the location or region described. While RoboPoint performs satisfactorily on several basic spatial reasoning tasks, such as object reference and free space reference, it exhibits notable limitations. The model's approach of directly translating language instructions into points bypasses the inherent language-based reasoning capabilities of VLMs. As a result, this method overlooks the core strengths of large language models, thereby constraining the model's ability to manage complex tasks. Particularly, it struggles with those tasks requiring detailed or multi-step reasoning in intricate environments.\nSimultaneously, chain-of-thought (CoT) prompting [24] and its extensions have emerged as a prevalent methodology for researchers to tackle complex tasks using large language models [18, 28, 29] or vison-language models [33, 35]. This approach is also being explored within the domain of embodied AI [14, 32]. In these studies, models are instructed to articulate their thought processes prior to arriving at a final answer. While these works focus on language-based planning, the challenge of leveraging these thought processes to generate fine-grained actions remains largely unaddressed.\nTo overcome the limitations of existing methods, in this work, we propose a novel approach, termed Spatial-CoT, to enhance the spatial reasoning capabilities of vision-language models for embodied task planning. The approach comprises two stages: 1) spatial coordinate bi-directional alignment: This stage involves the explicit alignment between vision-language inputs and coordinates, thereby enabling the model to better comprehend and generate coordinate-based responses. We introduce a bi-directional alignment mechanism to further reinforce this process. 2) chain-of-thought spatial grounding: In this phase, we enhance spatial reasoning by explicitly utilizing the language-based reasoning capabilities of vision-language models, rather than directly generating coordinate-based actions in an end-to-end manner. This approach significantly improves the model's ability to handle more complex tasks in intricate environments. Additionally, we introduce a pipeline to automatically generate data with high-quality rationales for model fine-tuning, which substantially reduces data acquisition costs.\nDiffering from prior studies in spatial reasoning research, which typically conduct open-loop evaluations on offline visual question answering (VQA) datasets, this paper adopts a more challenging setting by performing closed-loop evaluations within simulators and real world. The evaluation tasks encompass both navigation and manipulation, each presenting greater challenges compared to those in earlier works. The experimental results demonstrate that our model significantly outperforms previous state-of-the-art methods in both tasks.\nIn summary, the key contributions of this paper are:\n\u2022 A novel approach, SpatialCoT, designed to enhance the spatial reasoning abilities of vision language models for fine-grained action generation, comprising two stages: spatial coordinate bi-directional alignment and chain-of-thought spatial grounding. The approach explicitly leverages the inherent language-based reasoning capabilities of vision language models, significantly improving performance on complex embodied tasks.\n\u2022 A pipeline that enables the automatic collection of data with high-quality rationale, substantially reducing data acquisition costs for model fine-tuning.\n\u2022 State-of-the-art results on challenging embodied planning tasks, including both navigation and manipulation."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Spatial Reasoning", "content": "Spatial reasoning is a crucial capability for vision language models and is included in numerous VQA benchmarks [15, 22, 27, 30]. However, most VLMs [5, 10, 17, 25, 36] are ppredominantly trained on 2D images paired with text, which lack sufficient spatial data. As a result, their spatial reasoning abilities are limited. To address this issue, some works, such as SpatialVLM [3] and SpatialRGPT [6] have been developed to enhance the spatial reasoning of VLMs by collecting spatially-related question-answering data and fine-tuning the models on them. Recent works has further extended spatial reasoning to generate more fine-grained actions by introducing a point-based action space [23, 31]. Given a spatially related instruction, RoboPoint [31] outputs one or several points located on the input image to indicate the location or region described in the instruction, a process the authors call \"spatial affordance prediction.\" Following the idea of RoboPoint, RoboSpatial [23] makes further improvements by introducing more types of data. However, these studies primarily focus on establishing a direct mapping between language instructions and corre-"}, {"title": "3. Our Method", "content": "Our method consists of two fundamental stages, as illustrated in Figure 2. The first stage, termed spatial coordinate bi-directional alignment, equips the vision-language models with the capability to understand and generate coordinates. The second stage, chain-of-thought spatial grounding, enables the model to engage in comprehensive reasoning and to translate this reasoning into coordinate-based actions, leveraging the alignment ability developed in the first stage. The following sections will provide a detailed explanation of each stage."}, {"title": "3.1. Spatial Coordinate Bi-directional Alignment", "content": "Previous studies [23, 31] have attempted to leverage additional VQA data, such as object references, as co-training data. However, the organization of this data in these works is often lacking, and its potential to enhance the model's spatial reasoning capabilities remains underutilized. In this work, we propose an explicit alignment of vision-language data with coordinates, which will significantly aid the model in understanding and generating coordinate-based inputs and outputs. Unlike previous studies, we introduce a bi-directional alignment framework to strengthen this process. Let \\(X\\) represent an image, \\(X_{lang}\\) represent language-only text (without coordinates), \\(X_{coor}\\) represent"}, {"title": "3.2. Chain-of-Thought Spatial Grounding", "content": "Unlike previous works [23, 31] that directly output coordinate-based actions given the language instruction (equation 3), this work aims to explicitly utilize the language-based reasoning abilities of VLMs to address complex spatial reasoning tasks. Drawing inspiration from ReAct [28], we generate additional data where the output is divided into two components (equation 4): 1) Rationale: the model's thinking process given the task. In this part, the model takes advantage of spatial and commonsense reasoning abilities in language-space to provide guidance for task completion. 2) Action: Based on the provided rationale, the model generates appropriate coordinate-based actions. By aligning language and coordinates in the preceding stage, the rationale (articulated in language) can be effectively translated into coordinates (illustrated by the yellow to blue gradient dotted line in Figure 2-b) without the need for extensive fine-tuning data.\n\\[[X, X_{lang}] f_\\theta()  X_{coor}  \\text{without rationale} \\]\n\\[ \\ (3) \\]\n\\[[X, X_{lang}] f_\\theta()  \\begin{bmatrix} \\ Rationale \\ \\ X_{coor} \\end{bmatrix},  \\begin{bmatrix}X_{lang} \\ X_{coor} \\end{bmatrix} \\text{with rationale} \\ (4) \\]\nGiven this approach, a significant challenge lies in efficiently collecting high-quality rationale-action data pairs. This challenge arises from the need to generate data in the rationale-action sequence while maintaining consistency between them and ensuring the optimality of the generated actions. To address this, we designed a pipeline to automatically generate high-quality rationale-action data, as illustrated in Figure 4. Initially, given an image and a task instruction, a ground truth action is acquired from the simulator in a rule-based manner, ensuring the action's optimality."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Tasks", "content": "Previous studies on spatial reasoning typically conduct open-loop evaluations on offline VQA datasets, which restricts the examination of their impact on downstream embodied tasks. To address these limitations, this paper adopts a more challenging setting by performing closed-loop evaluations within simulators. Additionally, we conduct offline evaluations of the fundamental capabilities of VLMS to investigate the relationship between these capabilities and end-to-end task planning abilities.\nClosed-loop Embodied Task Planning Inspired by the Goal-conditioned Markov Decision Process, we utilize this framework to break down the embodied task planning problem into varying levels of complexity:\n\u2022 State: We consider occlusion as a primary factor, including visual, stacked, and encased occlusion. Other factors include object properties such as geometry and movability.\n\u2022 Goal: This involves the number of objects, spatial constraints, and the abstraction level of the goal description.\n\u2022 Action: The action space impacts the task's difficulty, including the format of actions and the number of required skills.\n\u2022 Transition: This component addresses environmental transitions, encompassing uncertainty in dynamics.\nIn this work, we focus on a subset of the dimensions outlined above for simplicity. A comprehensive benchmark encompassing all dimensions will be provided in the future. This paper addresses two primary tasks: navigation and manipulation. For navigation tasks, unlike previous works, which treat navigation tasks as a region-localization problem (i.e., the model is asked to generate a position in the current view, such as \u201cGo to the position between the table and the chair\"), which do not require complex thinking and reasoning capabilities, this work adopts a more challenging setting. We use object-goal navigation as the evaluation task, where the agent must find an object not currently in view. The model is prompted to generate the best sub-"}, {"title": "4.2. Experimental Setup", "content": "Data Collection We collect data using two primary scene datasets. For navigation tasks, we utilize the Habitat Synthetic Scenes Dataset (HSSD) [16] for data collection and employed Habitat [21] as the simulator for closed-loop model evaluation. For manipulation tasks, we use Sapien [26] as the simulator and generate diverse tabletop rearrangement tasks and data. With the power of large language models, the data construction process of tabletop functional rearrangement is semi-automated. Additionally, to improve visual fidelity and reduce the sim-to-real gap, we use Blender [9] as the renderer to obtain high-quality images for data collection.\nModel Training For model training, we employed the Llama3.2-Vision 11B [1] as the backbone of the vision-language model. In both stages, we fine-tuned the model using LoRA [13] on the collected datasets. The training process spanned 2 epochs for each stage. All experiments were conducted on a single machine equipped with 8 NVIDIA L40 GPUs.\nBaselines We compare SpatialCoT with several baselines, including the specialized spatial reasoning model, RoboPoint, open-source VLMs such as LLaMA3.2V, and closed-source VLMs such as GPT-40. The baselines include:\n\u2022 RoboPoint: While the vanilla RoboPoint is trained from a Vicuna-v1.5-13B [8] base model, for a fair comparison, we reproduce RoboPoint by fine-tuning a Llama3.2V-11B model (which shares the same backbone as our work) on the dataset provided in the original work."}, {"title": "4.3. Experimental Results", "content": "Our analysis aims to address the following questions: 1) Is the two-stage training process effective in enhancing spatial reasoning ability of VLMs? 2) Which types of embodied planning tasks benefit most from improvements by SpatialCoT? 3) Is there a positive correlation between the fundamental capabilities of VLMs and their downstream performance in embodied planning tasks? 4) How does chain-of-thought contribute to improving the spatial reasoning capabilities of VLMs?\nQuestion 1: Is the two-stage training process effective in enhancing spatial reasoning ability of VLMs? We compare SpatialCoT with baseline models across navigation and manipulation tasks, as described in the preceding sections. Additionally, we conduct ablation experiments to verify the effectiveness of the two-stage approach.\nResults on Navigation Tasks For navigation tasks, we introduce two metrics as following:\n\u2022 Distance Gain (DG): This metric evaluates the quality of generated actions at each step by calculating the negative traverse distance from the generated position to the nearest target object, using a path planning algorithm. We also perform mean normalization across all possible"}, {"title": "Question 2: Which types of embodied planning tasks benefit most from improvements by SpatialCoT?", "content": "Question 2: Which types of embodied planning tasks benefit most from improvements by SpatialCoT? We analyzed the results by categorizing the tasks into several levels, based on the principle described in Section 4.1. The results are presented in Table 2 and Table 3. Table 2 reveals that the majority of failures in manipulation tasks stem from non-unique objects (level-3) and a high number of objects (level-4). This results in crowded scenes and an increased likelihood of collisions. Our model demonstrated significant improvements in these tasks, indicating an enhanced understanding of object relationships and physical cognitive abilities, such as collision avoidance. In navigation tasks, SpatialCoT exhibited superior performance at levels 1, 2, and 4. The most notable improvement was at the most challenging level (+ 11.19%), level 4, characterized by fewer goals and greater distances. These results suggest that SpatialCoT effectively enhances the model's capability to manage tasks with sparse reward signals, requiring advanced spatial understanding and reasoning."}, {"title": "Question 3: Is there a positive correlation between the fundamental capabilities of VLMs and their downstream performance in embodied planning tasks?", "content": "Question 3: Is there a positive correlation between the fundamental capabilities of VLMs and their downstream performance in embodied planning tasks? In our evaluation of the fundamental capabilities of Vision-Language Models (VLMs), we found that SpatialCoT consistently outperforms other models across all evaluated categories (see Figure 6a). To further explore the correlation between each category of fundamental capability and downstream performance, we presented these correlations in Figure 6b. The horizontal axis represents the scores of fundamental capabilities, while the vertical axis shows the success rates in embodied planning tasks. Each line corresponds to a specific category of fundamental capability. The results reveal a clear positive relationship between object understanding and spatial relationships (orange and red lines). The other two categories also display a positive correlation trend, though not entirely monotonic. These findings demonstrate that there is a positive correlation between the fundamental capabilities of VLMs and their downstream performance, providing a valuable basis for further research in this field."}, {"title": "Question 4: How does chain-of-thought contribute to improving the spatial reasoning capabilities of VLMs?", "content": "Question 4: How does chain-of-thought contribute to improving the spatial reasoning capabilities of VLMs? Experimental results demonstrate that the chain-of-thought process significantly enhances the model's ability to utilize spatial and contextual information, such as room layout and commonsense knowledge, to arrive at the correct answer. To illustrate this, we present a case study (see Figure 7). In this task, the model is instructed to locate an alarm clock within a house. The SpatialCoT model first considers the"}, {"title": "5. Limitations", "content": "SpatialCoT employs coordinate-based actions for embodied task planning; however, it does not account for complex actions such as rotations, rendering it unable to manage tasks that require object rotation. Moreover, as a vision-language model, SpatialCoT relies on 2D images for visual input, thereby necessitating future research to explore the potential of 3D inputs, especially in large spaces."}, {"title": "6. Conclusion", "content": "In this paper, we propose a novel approach, Spatial-CoT, designed to enhance the spatial reasoning capabilities of vision-language models through a two-stage training paradigm: spatial coordinate bi-directional alignment and chain-of-thought spatial grounding. By explicitly leveraging the language-based reasoning abilities of vision-language models and anchoring them in coordinate-based actions, our approach significantly improves the model's performance in handling complex embodied tasks. Experimental results demonstrate that SpatialCoT outperforms previous methods in challenging embodied tasks, including navigation and manipulation."}]}