{"title": "RELIABILITY, RESILIENCE AND HUMAN FACTORS\nENGINEERING FOR TRUSTWORTHY AI SYSTEMS", "authors": ["Saurabh Mishra", "Anand Rao", "Ramayya Krishnan", "Bilal Ayyub", "Amin Aria", "Enrico Zio"], "abstract": "As AI systems become integral to critical operations across industries and services, ensuring their\nreliability and safety is essential. We offer a framework that integrates established reliability and\nresilience engineering principles into AI systems. By applying traditional metrics such as failure rate\nand Mean Time Between Failures (MTBF) along with resilience engineering and human reliability\nanalysis, we propose an integrate framework to manage AI system performance, and prevent or\nefficiently recover from failures. Our work adapts classical engineering methods to AI systems and\noutlines a research agenda for future technical studies. We apply our framework to a real-world\nAl system, using system status data from platforms such as openAI, to demonstrate its practical\napplicability. This framework aligns with emerging global standards and regulatory frameworks,\nproviding a methodology to enhance the trustworthiness of AI systems. Our aim is to guide policy,\nregulation, and the development of reliable, safe, and adaptable AI technologies capable of consistent\nperformance in real-world environments.", "sections": [{"title": "1 Introduction", "content": "Ensuring the safety of AI systems has become a central concern as these systems are increasingly integrated into our\ndaily lives. At the heart of this discourse is reliability engineering, a field that has guided the reliability of complex\nengineering systems for decades. Reliability engineering involves applying scientific know-how to ensure that systems\nperform their intended functions without failure over a specified period. In this paper, following system reliability\nconventions, failure refers to any deviation from expected performance. We propose a framework that integrates\nReliability Engineering, Resilience Engineering, and Human Factors Engineering, underpinned by Prognostics and\nHealth Management (PHM), to enhance the trustworthiness of AI systems. In our framework, reliability engineering\naddresses pre-deployment considerations to prevent failures once the system is operational while resilience engineering\nfocuses on post-deployment to ensure the system's ability to recover from failures. Human factors are also integral\nthroughout both phases. Central to our approach is the Human-Centric AI Reliability Model (HC-AIRM), which\nemphasizes the role of human factors in both reliability and resilience engineering, ensuring that human interactions are\nconsidered throughout the AI system lifecycle (see Section 5.5)."}, {"title": "", "content": "An AI system is composed of multiple subsystems, including Data, Model, Computing Infrastructure, Code and\nSoftware, and Human interaction. Each subsystem contains components that contribute to the overall functionality\nof the AI system. Understanding the interplay between these subsystems and their components is essential for PHM\nof AI lifecycle and ensuring system reliability and resilience. In the meantime, it should be recognized that technical\nreliability and human reliability are tightly intertwined and cannot be considered separately.\nA recent real-world example illustrates the critical need for integrating human factors into technical reliability. In January\n2024, news outlets reported on the potential political influence of public figures like Taylor Swift, highlighting her sway\nover young social media followers [16] through rapid dissemination of deepfake images across social networks like X\n(formerly known as Twitter) [67]. These deepfakes raised immediate concerns about the misuse and weaponization\nof AI technologies, prompting swift actions such as restricting specific search queries, as part of post-deployment\nresilience, to mitigate the harm.\nAl systems are diverse, with diverse components/modules each associated with specific failure modes that are domain-\nand context- specific. These can be broadly categorized into intentional (adversarial) and unintentional (design flaws,\nhuman errors) failures [39]. Intentional failures occur when external actors exploit vulnerabilities through tactics like\nmodel poisoning or data manipulation. For instance, in autonomous vehicles adversarial attackers can alter road signs or\ndisplay adversarial patches on moving vehicles to deceive object detection models, leading to incorrect decision-making\nprocesses that exploit model's perception capabilities and data vulnerabilities [68, 12]. Unintentional failures generally\nstem from design flaws, incomplete testing, or distributional shifts during the AI lifecycle. In the field of radiology,\nas an example, AI systems interpreting medical images can produce false diagnoses or miss critical conditions due to\ninadequate training data, biases in the dataset, or limitations in the model architecture [72]. Such errors not only affect\nthe reliability of the AI system but also have the potential to negatively influence human decision-making. Users often\nexpect lower error rates for AI systems compared to human counterparts, underscoring the demand for higher reliability\nin AI applications [41]. As such, AI systems must be designed with a deep understanding of both technical limitations\nand human expectations.\nIncidents and examples explain above underscores how quickly AI can be exploited in unforeseen ways or cause\ncatastrophic failures, highlighting the importance of considering misuse and malfunction at the design stage, which can\nbe addressed using the Human-centric PHM approach proposed in this paper. By integrating human factors into the\ndesign process, we can implement remedial solutions or fail-safe mechanisms from the outset to prevent such failures.\nFor instance, incorporating watermarking techniques into generative AI models can help identify synthetic content and\ndeter misuse, exemplifying how pre-deployment reliability measures can enhance post-deployment resilience [18].\nAnalogous to how IKEA designs its furniture components to prevent incorrect assembly\u2014where parts are shaped so\nthat they cannot be put together improperly-we advocate for and propose a human-centric PHM framework that uses\nreliability engineering best practices to design AI systems with built-in safeguards. By anticipating worst-case scenarios\nand ensuring that both components and higher-level systems can handle them acceptably, we move towards trustworthy\nAI systems capable of consistent performance in real-world environments.\nIntroducing metrics from reliability engineering, such as failure rate and Mean Time Between Failures (MTBF)\noffers a structured approach to measuring and managing AI system performance over time. By understanding these\nfailures within a reliability engineering framework and incorporating cost-benefit analysis, we can propose targeted\ninterventions\u2014such as improved algorithms or better system controls\u2014that balance the costs and benefits of ensuring\nAI system safety [28, 37]. Defining what constitutes an AI failure across sectors or tasks in finance, healthcare, or\ncritical infrastructure is domain-specific and is key to developing a comprehensive approach to AI reliability and\nresilience. Moreover, the societal impact-whether measured in economic terms, number of people affected, or\ncritical infrastructure compromised\u2014varies widely, reinforcing the importance of applying these metrics across various\ndomains.\nFigure 1 illustrates a high-level taxonomy of AI failure modes and relates these modes to various sectors, impacts, and\ntasks, emphasizing the dynamic interlinkage between application-specific sectors and the critical need for resilient AI\nsystems across industries. By visualizing these connections, we can better understand the trade-offs and areas requiring\nfocused interventions to ensure reliable performance.\nWhile reliability is the cornerstone of system performance, trustworthiness, which is critical is Al systems as it influences\nuser confidence and the societal acceptance of AI technologies, encompasses a broader spectrum, including aspects like\ntransparency, explainability, fairness and security. Trustworthiness . Unlike traditional systems, where trust is often\nbuilt through consistent performance, AI systems must also demonstrate their ability to adapt, learn, and improve while\nmaintaining reliability [20, 15, 4].\nConsidering the dynamic and complex environment and architecture of AI System and the need for trustworthiness,\nreliability of such systems should not only refer to the consistent performance over time but also account for the system's"}, {"title": "", "content": "ability to handle new data, adapt to changing environments, and maintain performance under varying conditions. In the\ncontext of reliability, AI systems are considered \u201cbetter than new\" repairable systems systems that have varied\nversion updates with the objective that new releases outperform prior versions. Al systems are dynamic, meaning they\ncan \"age\" differently compared to traditional systems. Traditional systems may degrade slowly, but AI models can\nexperience rapid \u201caging\u201d through model drift, where shifts in data distributions lead to decreased performance [8]. For\nexample, a sentiment analysis model trained on a particular social media platform may misclassify sentiments if slang\nor language use evolves over time. Addressing this requires continuous adaptation and retraining [2, 14]. AI reliability\nmust consider these factors, making the concept more complex and intertwined with continuous learning and adaptation\n[71, 19]. Additionally, understanding the reliability of AI systems is crucial for calculating their Return on Investment\n(ROI), given the financial implications of both Type 1 and Type 2 errors in varied business contexts [54].\nTo address the concerned discussed above, the proposed PHM framework of this paper considers system architecture\nand interplays through systematic review of the AI system using human-centric reliability and resilience engineering\nbest practices. Additionally, it account for trustworthiness through insightful and holistic integration of traditional\nreliability and resilience engineering methodologies with AI systems. Doing so, the presented PHM framework tries to\nredefine reliability for AI systems and measure trustworthiness through ensuring both consistent reliability and fostering\nuser confidence through transparent, ethical, and resilient AI practices. Thus, AI Reliability in the proposed framework\nincludes guidelines for building models that can detect and adapt to data drifts and trends, reducing the likelihood of\nerrors and providing the ability to calculate the ROI given the financial implications in varied business contexts [54].\nType I (false positive) and Type II (false negative) errors occur when the system either incorrectly identifies or misses\nan event, such as a medical diagnosis or a loan approval [2]. Both errors can stem from machine predictions or human\ninteractions, especially when AI systems are used as recommendation engines [14]. A finance example would be\napproval of an unqualified applicant (false positive) or rejection of a qualified one (false negative). While preventing\nthese errors requires adaptive reliability engineering, managing them requires resilience\u2014ensuring systems can recover\nfrom disruptions and human factors engineering, which focuses on how humans interact with the system, including\ndesigning interfaces that help users understand and correct AI outputs [8, 2]. The proposed PHM framework for\nAI systems integrate these layers by considering not only the model's performance but also the human elements of\nuncertainty, decision-making, and error handling. This holistic approach ensures robust performance, particularly in\ncritical sectors like healthcare, finance, and autonomous systems, where reliability and resilience are paramount for\nsafety and trust.\nIn sum, this study presents a conceptual framework for improving safety of AI systems through the integration of\nhuman-centric reliability and resilience engineering principles, underpinned by PHM. Central to our methodology\nare concepts like the \u201cbathtub curve\", failure rate, MTBF, probabilistic risk, and resilient failure recovery, which we\nleverage to proactively manage AI system failures. Following a system level review, we focus on failure modes and\nPHM at the subsystem and component levels, emphasizing the importance of addressing failures in different subsystems\nof an AI system upon the release of new versions. By applying PHM methodologies at these granular levels, we aim to\nenhance both pre-deployment reliability and post-deployment resilience within the AI system lifecycle.\nOur analysis using the proposed framework also acknowledges the importance and urgency of emerging frameworks\nsuch as the NIST AI Risk Management Framework, Singapore's Model Governance Framework & AI Verify, the EU\nAI Act, Blueprint for an AI Bill of Rights, United Nations AI Advisory Body, Foundational Model Transparency Index,\nthe OECD's catalogue of tools and metrics, all underlining the relevance and urgency of such approaches. Economic\nmetrics, such as costs, revenues, and error rates, are pivotal in AI system reliability engineering, influencing financial\noutcomes and guiding policy and insurance frameworks [37].\nIncorporating PHM into AI systems provides a critical safety layer by enabling proactive maintenance and anomaly\ndetection through predictive analytics. This continuous monitoring approach allows for early identification of issues, pre-\nventing minor faults from escalating into catastrophic failures [30, 60, 55]. By applying PHM methodologies\u2014originally\ndeveloped for machinery monitoring-to AI, we can ensure ongoing assessments of model accuracy, data integrity, and\ninfrastructure stability, akin to scheduling predictive maintenance in engineering. This integration is especially vital\ngiven the EU AI Act's emphasis on transparency and regulatory compliance, further underscoring the need for robust\nmonitoring frameworks [42, 25]. The EU AI Act, as detailed in the COMPL-AI Framework [25] (compl-ai.org), high-\nlights the necessity for actionable technical interpretations and benchmarking to align AI development with regulatory\nstandards. Furthermore, resources like the AI Incident Database [45] and the AI Risk Repository [24] emphasize the\nimportance of understanding AI harms and managing systemic risks through comprehensive evaluation frameworks.\nThe rest of the paper is organized as follows. Section 2 introduces our systematic framework for PHM of AI systems,\nproviding an overview of the AI lifecycle and how it integrates PHM concepts and principles for reliability and resilience,\nwith specific attention to AI-specific failure modes in Subsection 2.1.1. Section 3 delves into the AI System Reliability:\nKey Metrics and Engineering Approaches, establishing the foundational metrics essential for assessing reliability in"}, {"title": "2 Systematic Framework for PHM of AI Systems", "content": "The technical performance of AI systems is both diverse and evolving, encompassing various performance metrics\nacross different tasks such as object detection, language modeling, and image recognition. Notable benchmarks, such\nas the aggregations conducted by Stanford AI Index's technical performance section [43], and platforms like State-\nof-the-Art Papers with Code, which maintains a free and open resource of Machine Learning papers, accompanying\ncode, datasets, evaluation methods, and tables, provide systematic evaluations across models,capturing the nuanced\nperformance criteria for different AI applications.\nReliability metrics in machine learning (ML) and deep learning (DL) involve concepts related to traditional performance\nmeasures such as precision, recall, and F-scores, often mapped to statistical Type I and Type II errors. These metrics\nplay a critical role in improving MTBF, crucial indicators for robust model deployment. Prior to production deployment,\nmodels undergo rigorous reliability testing, a practice aligned with software testing but specifically adapted for AI\nmodel behavior under diverse conditions. This establishes an upfront focus on model reliability during testing and\ndevelopment stages, helping distinguish AI reliability from traditional software reliability frameworks.\nTo address the full spectrum of trustworthiness requirements, the OECD.AI Catalogue of Tools & Metrics for Trustworthy\nAI (Table 1) outlines key principles such as transparency, explainability, and robustness, mapping specific metrics to\nvarious AI actors, including data scientists, developers, and system integrators. These principles align well with our\nsystematic framework, as they guide AI reliability and resilience by establishing performance standards that consider\nboth technical and human-centric factors.\nAI reliability is closely related to software reliability but introduces unique complexities due to the dynamic operating\nenvironment in which AI models function. Traditional software reliability often relies on Software Reliability Growth\nModels (SRGM) based on Non-Homogeneous Poisson Processes (NHPP), such as the S-shaped Gompertz model or the\nconcave Weibull model, as discussed in classic studies [70, 58]. Unlike traditional software, where the environment is\nrelatively stable, AI model performance significantly depends on environmental factors, resulting in inherent, sometimes\nirreducible errors. For example, dual poisson processes has been used to model AI reliability, incorporating both fixed\nand decreasing failure rates, a framework extendable to more complex AI reliability challenges [9].\nRecent developments in software reliability, such as PHM and NHPP-based models [62], have begun to adapt to\nAl reliability. For instance, studies ([44, 34]) applied SRGMs to analyze AI-related datasets, such as California AV\ndisengagement data, while [47] developed models with bathtub-shaped fault intensity functions to balance model\ncomplexity and predictive accuracy in AI reliability analysis. These advancements in reliability modeling for AI\nextend traditional software reliability frameworks, accommodating AI's sensitivity to changing environments and\nnon-removable intrinsic errors.\nReliability and resilience assessments of AI systems are traditionally conducted in corporate environments by product\nmanagers and internal governance teams. However, this paper seeks to codify these reliability concepts systematically\nfor PHM of AI systems, establishing a framework akin to that of software reliability engineering. By consolidating\nthese practices, this work aims to provide a structured approach to AI reliability and resilience engineering, addressing\ntraditional software reliability assumptions and extending them to encompass the unique challenges posed by AI\nsystems."}, {"title": "2.1 Reliability-Resilience Framework for AI System Lifecycle", "content": "AI systems are complex systems that one can break down into distinct 'subsystems' composed of 'components' (or\nmodules), each having a specific role in contributing to the system's overall functionality. These subsystems include\ndata, models, compute infrastructure (cloud), code+software and human. Human errors can occur at multiple\nstages\nduring development (developer), deployment (deployer) or in production (end-user or DevOps engineer)."}, {"title": "", "content": "Failures can also be classified based on when they occur. In this regard, a number of typical concepts and principles of\nreliability and resilience engineering need to be considered:\n\u2022 Pre-deployment reliability refers to the mitigation of failures during design and development to prevent their\noccurrence to the extent possible. This includes ensuring that components are tested, verified and validated\nbefore deployment with respect to aspects like data integrity, model accuracy or code stability. These methods\nand concepts should be applied at the component level, allowing for aggregation to the subsystem and overall\nAI system level.\n\u2022 Post-deployment resilience is the ability to sustain failure events in production, recover from them and\nmaintain functionality. This often involves handling computing resource scaling, as well as mitigating\ndata/model drift or other issues that arise once the AI system is deployed and operates.\nFailures in Al systems can range from small failures that involve minor errors, latency issues or model prediction\ninaccuracies with limited consequences to catastrophic failures such as the downtime of critical systems like critical\ngovernment operations or healthcare platforms. Reliability and Resilience Engineering try to prevent failures and plan\nfailure recovery, respectively. Ideally, if reliability is established early on, the need for resilience interventions (such as\nin-the-field fixes) should be minimal. However, resilience becomes especially crucial when deploying updates, patches,\nor new versions of a system.\nTable 2 provides an overview of the different AI subsystems, highlighting the key considerations for pre-deployment\nreliability and post-deployment resilience for each. The Table also offers examples of specific failure scenarios that\ncan occur within each subsystem. This helps classifying and understanding the failure modes that AI systems may\nexperience throughout their lifecycle. Figure 2 illustrates a high-level view of an AI system, where the cloud and\ncomputing infrastructure envelopes the other subsystems, showcasing how these subsystems interconnect.\nThe development and management of AI systems necessitate a nuanced understanding of the lifecycle that spans from\nideation in development environments to stable deployment in production settings. In developing AI systems, value\nscoping is crucial to ensure that business and data understanding align with solution design, a process described on\nevolving constructs related to operationalizing AI [56]. This lifecycle is characterized by constant iterations, where\nmodels are continuously refined and updated to adapt to new data or changing conditions. These updates are crucial in\nenvironments where data distributions can shift unpredictably, often referred to as data drift, which may necessitate\nadjustments in the model (model drift). For instance, in cloud environments, the variability in data input can be\nsignificant due to diverse user interactions, trends, seasonality or changing external factors, underscoring the need for\nrobust model management strategies that accommodate such shifts without compromising the system's reliability or\nperformance."}, {"title": "2.1.1 Failure Modes in an AI System", "content": "Failure modes in an AI system are specific to distinct components of its subsystems (e.g., Data, Model, Computing, and\nCode + Software). Proactive maintenance under PHM also occurs at the component level. Once the Fault Tree for the\nAI system is established upon FMEA, all failure modes and corresponding PHM actions are identified and addressed at\nthis granular level. This approach ensures that each component is continuously monitored and maintained, enhancing\nthe overall reliability and resilience of the AI system.\nFor example, pre-deployment data failures can involve issues such as data integrity problems (e.g., missing values,\nduplicates, or corrupted files), challenges in data acquisition (e.g., insufficient sample sizes or biased datasets that don't\nrepresent the target population), and governance issues (e.g., non-compliance with data privacy regulations or lack of\nproper data anonymization). In contrast, post-deployment data failures often relate to mitigating data drift, where the\nstatistical properties of incoming data change over time due to evolving user behavior or external factors, leading to a\ndecline in model or AI system performance unless continuously monitored and updated.\nTo provide a comprehensive understanding of failure modes, Table 3 outlines a detailed breakdown of subsystems,\ntheir components, failure types, and metrics for data collection. The table serves as a guide for deeper exploration\nof system-specific failures, highlighting how each component requires unique approaches for failure detection and\nmitigation."}, {"title": "2.2 AI System Architecture: Managing Risk and Performance", "content": "As illustrated in Figure 4, the lifecycle of AI systems is complex, encompassing various stages from development to\ndeployment, and involves multiple stakeholders, including developers, deployers and users. This lifecycle also reflects\nthe continuous interaction between AI systems and their environment, where AI not only functions within specific\nboundaries but can also influence or give rise to new AI systems through iterative processes, for example, multi-agent\nsystems or agents building new agents. In contrast to fully autonomous AI systems like multi-agent systems, where\nagents operate independently and make decisions without human intervention, there are also human-in-the-loop AI\nsystems that require human oversight and interaction, especially in business and industrial applications. Managing\nthese dynamics effectively requires a strong focus on risk management. Risk, in this context, can be understood as the\ncombination of variability in performance and uncertainty in outcomes. Reliability engineering plays a crucial role in\nminimizing performance inconsistency by ensuring consistent and predictable system behavior, thereby enabling risk\nassessment and management. Addressing these aspects is essential for building robust, scalable and sustainable AI\nsystems that can withstand and adapt to both expected and unforeseen challenges.\nThe AI systems discussed encompass not only the data lakes and AI models, but also the user interfaces, micro-services,\ncloud infrastructure and the human elements involved-humans as developers, maintainers and engineers. This\nintegrated perspective is crucial, as it reflects the reality that modern AI systems consist of layered software architectures\nthat combine monolithic models with micro-services. These systems require continuous updates and refinements,\nincluding decisions on whether and when to train or retrain models based on their performance and the operational\nneeds of the business.\nIn our approach, we emphasize the importance of considering the stages of the lifecycle, specifically pre-processing\n(data preparation for model training), during computation (model training and inference) and post-computation (testing,\ndeployment and monitoring). This approach ensures that the AI architecture can synchronize with micro-service\nframeworks, supporting moteraction between AI systems and their environment, where AI not only functions within\nspecific boundaries but can also influence or give rise to new AI systems through iterative processes, for example,\nmulti-agent systems or agents building new agents. In contrast to fully autonomous AI systems like multi-agent systems,\nwhere agents operate independently and make decisions without human intervention, there are also human-in-the-loop\nAI systems that require human oversight and interaction, especially in business and industrial applications. Managing\nthese dynamics effectively requires a strong focus on risk management. Risk, in this context, can be understood as the\ncombination of variability in performance and uncertainty in outcomes. Reliability engineering plays a crucial role in\nminimizing performance inconsistency by ensuring consistent and predictable system behavior, thereby enabling risk\nassessment and management. Addressing these aspects is essential for building robust, scalable and sustainable AI\nsystems that can withstand and adapt to both expected and unforeseen challenges.\nMoreover, a view of subsystems and components with associated failure types illustrated in Figure 4 also aims to\nincorporate the portrayal of the code and software systems as core components of the AI system. This inclusion not\nonly highlights their role but also underlines the need for continuous assessment of the wear and tear on these systems.\nIt is important to evaluate the marginal improvements in performance against the costs and risks of frequent updates,\nwhich can be substantial. This requires an enterprise-wide cost estimation approach that captures both hard costs (such\nas labor and infrastructure) and soft costs (such as compliance and ongoing training), providing a holistic view of\nthe financial implications of maintaining and upgrading AI systems. The graph also brings into focus the economic\naspects of maintaining and updating AI systems, suggesting a need for a detailed cost analysis to balance performance\nimprovements against potential risks and expenditures.\nThe abstraction of AI systems should integrate the dynamic interaction between the system and the real world, as\nillustrated in Figure 4, distinguishing between human roles in development and user interaction in production end\nuse. Central to this abstraction are the key subcomponents that constitute an AI system: Data, Model, Computing\nInfrastructure, and Code + Software. These subcomponents each play a critical role in ensuring the system's continuous\nadaptation, resilience and performance in real-world conditions.\n\u2022 Data: Includes how the system accesses and uses real-world data in real-time, adapting to changes and\nensuring data integrity continuously. This encompasses data acquisition, preprocessing, storage and real-time\nupdating mechanisms that ensure the data remains relevant and accurate for model consumption.\n\u2022 Model: Considers not only the internal workings of the model but also how it interacts and adapts to real-world\nfeedbacks and inputs. This involves monitoring model performance for issues like drift or bias, regularly\nretraining with updated data, and incorporating mechanisms for continuous learning and adaptation based on\nuser interactions and changing environmental conditions."}, {"title": "", "content": "\u2022 Computing Infrastructure: Reflects on the computing resources in real-time operation, scaling to meet\ndemand without degradation in performance. This includes optimizing computational efficiency, managing\nresource allocation to handle peak loads, and ensuring redundancy and fault tolerance to prevent downtime\nand ensure smooth operation. While the term Computing Infrastructure refers primarily to computational\nresources, it also implicitly covers the broader Cloud Infrastructure, which encompasses not just computing\npower but also memory, storage, networking ports, and all other cloud services critical to maintaining system\navailability, performance and scalability. By managing these resources holistically, AI systems can ensure\nrobust performance, even under variable and demanding workloads.\n\u2022 Code + Software: Encompasses the development, integration and maintenance of software components that\nform the backbone of AI systems. This includes the implementation of algorithms, libraries and frameworks,\nensuring compatibility and performance optimization. It also involves version control, testing and debugging\nto maintain software reliability and security. Continuous integration and deployment (CI/CD) practices are\nessential to quickly adapt to new updates and patches. Moreover, software architecture needs to support\nmodularity and microservices, enabling scalability and ease of updates without disrupting system operations.\nSecurity protocols must be embedded to prevent vulnerabilities, and rigorous documentation is required to\nfacilitate effective maintenance and governance.\n\u2022 Human-in-the-loop Systems: Human-in-the-loop dynamics, particularly when coupled with online or\ncontinuous learning strategies, introduce a unique dimension to AI reliability. As the system learns from\nhuman interactions, its reliability might fluctuate, especially when input predominantly comes from novice\nusers rather than from expertly curated training data. This phenomenon suggests that reliability in AI systems\nis not merely a function of the machine's performance but is significantly influenced by the human-model\ninteraction [66]. Human errors are a significant contributor to failures in both software systems and AI during\nthe production and development lifecycle. However, AI systems can be strategically designed to monitor and\nmitigate human errors effectively, enhancing system robustness and reducing the likelihood of failure due to\nhuman oversight [36].\n\u2022 Pre-Deployment (Design and Development): Involves developers in the loop for initial training,\nparameter tuning and model verification and validation before deployment. In the case of large language\nmodels (LLMs), the development phase is not solely the realm of technical personnel such as data\nscientists; it crucially involves domain experts or subject matter experts (SMEs) who provide essential\nhuman feedback. This feedback, which its incorporated through Reinforcement Learning from Human\nFeedback (RLHF) is the main advantage of generative over tradition AI, plays a pivotal role in refining\nthe models to ensure they align more closely with real-world complexities and domain-specific nuances.\nIncorporating domain experts in the loop enhances the model's relevance and accuracy, allowing for a\nmore nuanced understanding and integration of practical, field-specific knowledge.\n\u2022 Post-Deployment (Production): Users interact with the AI system, providing feedback that may be used\nto refine and adapt the system. This feedback mechanism is critical not only during the training phase but\nalso extends into production. In production, real-time user interactions generate continuous data streams\nthat can be leveraged to further train and fine-tune the AI system. This ongoing learning process helps in\ndynamically adapting the AI models to evolving real-world conditions and user needs, thereby enhancing\nthe system's responsiveness and accuracy over time. The integration of user feedback into both training\nand production phases exemplifies a continuous improvement cycle where AI systems learn from each\ninteraction, gradually improving their performance and utility. As discussed before, performance of an\nAI system could also deteriorate due to data drift or model drift triggered by changes in the environment\nor changes in human behavior. In human-in-the-loop AI systems automation bias or over-reliance on AI\ncould result in deteriorating performance of the AI system as humans fail to exercise appropriate control."}, {"title": "3 AI System Reliability: Key Metrics and Engineering Approaches", "content": "Reliability in AI systems is a critical factor in their effectiveness and trustworthiness, especially as these systems\nbecome integrated into our daily lives. Traditional predictive models and recommendation systems, which are designed\nto provide consistent and accurate results over time, can often be analyzed using established reliability engineering\nconcepts like the \"bathtub curve\" (see Appendix A). This approach allows us to model and predict failure rates at\ndifferent stages of the system's lifecycle, ensuring sustained performance and guiding maintenance strategies.\nHowever, AI systems such as voice assistants, like Siri and Alexa, Large Language Models (LLMs) like ChatGPT,\nintroduce unique challenges. Unlike traditional systems, these AI models have foundational issues related to the\ncorrectness and consistency of their outputs. These inconsistencies stem from their probabilistic nature and the\ncomplexities involved in natural language processing. As a result, reliability in these systems cannot be solely measured"}, {"title": "", "content": "by traditional methods but requires a broader approach that accounts for their inherent variability and evolving behavior.\nThis distinction highlights the need for new metrics and regulatory frameworks to effectively assess and manage the\nreliability of AI systems that differ fundamentally from more deterministic models.\nInheriting from reliability theory [3, 73] the reliability function, $R(t)$, represents the probability that a system or\ncomponent will function without failure over time t. For AI systems, these components include data pipelines, machine\nlearning models, computing infrastructure and software codebases. Mathematically, the reliability function is defined\nas:\n$R(t) = P(T > t) = 1 \u2013 F(t),$\nwhere $F(t)$ is the cumulative distribution function (CDF) of the failure time distribution, indicating the probability\nthat a failure occurs by time t. The probability density function (PDF), $f(t)$, describes the shape of the failure\ndistribution:\n$f(t) = \\frac{dF(t)}{dt}$\n$F(t) = \\int_0^t f(u) du,$\nThe hazard function, $h(t)$, provides the instantaneous failure rate, a critical measure for AI systems that rely on\ncontinuous monitoring and predictive maintenance:\n$h(t) = \\frac{f(t)}{R(t)}$\nThe hazard rate can fluctuate in AI systems based on factors like evolving data, system updates, or changes in model\nperformance. These metrics allow for deeper insights into the performance and operational reliability of AI systems.\nFor more details on parametric distributions to represent stochastic failure process, such as Exponential, Weibull,\nNormal, Lognormal and Gamma distributions, please refer to Appendix B, which outlines their application to AI\nreliability analysis."}, {"title": "3.1 Application to AI Lifecycle Components", "content": "As shown in Figure 4, AI systems can be decomposed into the following key components, each having distinct reliability\nconsiderations:\n\u2022 Data: The reliability of the data component, $R_{data}(t)$, involves real-time data integrity, completeness and\nquality. Failures can occur due to data corruption, missing data or erroneous inputs. The reliability of data\nhandling is critical for maintaining the performance of AI models.\n\u2022 Model: The reliability of AI models, $R_{model}(t)$, relates to the robustness of model parameters and structures\nagainst adversarial inputs, data drift and concept drift. This is particularly relevant for online learning\nenvironments where models continually adapt based on real-time data [26].\n\u2022 Computing Infrastructure: Computing infrastructure reliability, $R_{compute}(t)$, encompasses the availability\nand fault tolerance of hardware and software resources. In cloud and edge computing setups, scaling strategies\nand redundancy mechanisms ensure computing reliability [32].\n\u2022 Code + Software: The reliability of code and software, $R_{software}(t)$, concerns the stability of the codebase,\nversion control and error management. Failures can arise from bugs, vulnerabilities and integration issues\nacross the AI pipeline [27].\n\u2022 Human Error: Human reliability, $R_{human}(t)$, considers the range of errors from developers, deployers,\nDevOps teams, and end users across the AI lifecycle. Developer errors typically arise during coding, training,\nand testing, leading to bugs, biased models, or misconfigured parameters [8]. Deployers may introduce\nerrors during system rollout, especially under pressure to meet deadlines, which can lead to incorrect setups\nor neglected monitoring [2]. DevOps teams face challenges in managing updates, scaling, and integration,\nwhere missteps may compromise system stability and security. End users, especially in human-in-the-loop"}, {"title": "", "content": "systems", "14": ".", "reliabilities": "n$R_{AI"}, "t) = R_{data}(t) \\cdot R_{model}(t) \\cdot R_{compute}(t) \\cdot R_{software}(t) \\cdot R_{human}(t).$\nThis product assumes independence between the individual components, which may require further in-depth considera-\ntion for highly integrated systems.\nThe Mean Time Between Failures (MTBF): If the system (or component) can be repaired (renewed), the Mean Time\nBetween Failures (MTBF) is the average time between one failure and the next. Since AI systems with version updates\ncan be considered as 'Better than New Reparaible' systems, this metric intuitively represents the average lifespan of an\nAI system (subsystem or a component) before it slips up. This metric can guide the planning of new developments\n,version updating, or devOps efforts. Appendix Figure 10b provides an illustration of the concept.\nAny kind of underperformance in an AI system, considering the initially defined intended functionality, can be\nconsidered as a failure of an AI system. In reliability engineering, a failure point is typically a deviation from expected\nbehaviour, whether that is producing an incorrect output or function crashing entirely. Engineers usually gather data on\nthe frequency, nature and conditions of these failure incidents to get a clear picture of system reliability. One pressing\nquestion for the global policy community is finding consistent ways to collect and share data on AI failure"]}