{"title": "Improving Visual Object Tracking through Visual Prompting", "authors": ["Shih-Fang Chen", "Jun-Cheng Chen", "I-Hong Jhuo", "Yen-Yu Lin"], "abstract": "Abstract-Learning a discriminative model to distinguish a target from its surrounding distractors is essential to generic visual object tracking. Dynamic target representation adaptation against distractors is challenging due to the limited discrim-inative capabilities of prevailing trackers. We present a new visual Prompting mechanism for generic Visual Object Track-ing (PIVOT) to address this issue. PiVOT proposes a prompt generation network with the pre-trained foundation model CLIP to automatically generate and refine visual prompts, enabling the transfer of foundation model knowledge for tracking. While CLIP offers broad category-level knowledge, the tracker, trained on instance-specific data, excels at recognizing unique object instances. Thus, PiVOT first compiles a visual prompt highlighting potential target locations. To transfer the knowledge of CLIP to the tracker, PiVOT leverages CLIP to refine the visual prompt based on the similarities between candidate objects and the reference templates across potential targets. Once the visual prompt is refined, it can better highlight potential target locations, thereby reducing irrelevant prompt information. With the proposed prompting mechanism, the tracker can generate improved instance-aware feature maps through the guidance of the visual prompt, thus effectively reducing distractors. The proposed method does not involve CLIP during training, thereby keeping the same training complexity and preserving the general-ization capability of the pretrained foundation model. Extensive experiments across multiple benchmarks indicate that PiVOT, using the proposed prompting method can suppress distracting objects and enhance the tracker. The code is publicly available\u00b9.", "sections": [{"title": "I. INTRODUCTION", "content": "GENERIC visual object tracking (GOT) estimates the state of a target object in every video sequence frame, given its initial state in the first frame. Learning a discriminative representation for this target object is essential to alleviate the interference from other distracting objects. Despite great"}, {"title": "II. RELATED WORK", "content": "Generic Object Tracking. Generic object tracking (GOT) aims at estimating the state of an arbitrary target in a video sequence, given its initial state in the first frame. Building a robust GOT model is a significant challenge despite extensive research. The literature on GOT [15] is extensive. We focus on relevant paradigms, such as Siamese network-based trackers, DCF-based trackers, and their emerging transformer variants.\nSiamese trackers such as SiamRPN [16] and SiamRPN++ [2], take a target template and a search image, compute features, and use cross-correlation to create a response map. These trackers learn features offline on a large volume of data without online adaptation during inference. Despite their effectiveness, they struggle to generalize to new targets and search images dissimilar to training data.\nDCF trackers, particularly those based on model prediction, such as DiMP [1] and ToMP [17], use the paired support and query images to generate discriminative correlation filters online through meta-learning [18]\u2013[20]. The resultant filters can better identify the target from the background, resulting in higher generalization capabilities of these trackers. For instance, DiMP adopts this DCF paradigm and outperforms most Siamese trackers. Based on model prediction-based DCF trackers, our tracker further leverages a foundation model by the proposed prompting mechanism to explore the target areas, which helps derive more discriminative filters.\nTracking models like STARK [21], TransT [22], and TrDiMP [23] employ the attention mechanism in Transformers to construct Tracking Heads and showcase superior perfor-mance. The follow-up research efforts leverage Transformers for Tracking Head construction and image feature extraction.\nMixformer [24] employs a Vision Transformer (ViT) equipped with a Mixed Attention Module as its backbone for feature extraction, while SeqTrack [9] employs a masked autoencoder (MAE) pre-trained ViT-L backbone. They cast regression as a sequence prediction task, predicting boxes sequentially and autoregressively. These methods that use Transformers for feature extraction have high computation costs during training since they need to fine-tune heavy Transformers.\nOur method builds on ToMP [17] because its paradigm dy-namically predicts a model that can prevent overfitting to train-ing data more effectively than other paradigms. ToMP lever-ages ResNet and incorporates a Transformers-based model predictor for Tracking Head construction. Unlike ToMP, we develop a visual prompting mechanism where CLIP is lever-aged to compute visual features for arbitrary tracking objects, While the model predictor is adopted to make those objects with indistinguishable appearance instance-aware. Our method can derive more adaptive capability and discriminative features for Generic Object Tracking task based on the proposed prompting mechanism with CLIP-refined visual prompts.\nUnlike prior works [9], [13], [25], [26] that fine-tune their backbones with tracking datasets or with both tracking data and additional data such as object segmentation [27], [28], action recognition [29], etc., our approach leverages the foun-dation model characteristics of DINOv2 [14]. We construct a feature extractor by integrating the frozen backbone with a"}, {"title": "III. METHOD", "content": "An overview of our method is shown in Figure 2. Given the current frame and several reference frames, a backbone network is used for feature extraction. The Tracking Head is used to identify the target position in the current frame. To make the tracker promptable, we introduce a Prompt Gener-ation Network (PGN) and a Relation Modeling (RM) module before the Tracking Head. The PGN is a weak version of the Tracking Head to generate a score map where the potential target locations in the current frame are highlighted. The RM utilizes the resultant score map as the visual prompt to refine the feature map, which serves as the input to the Tracking Head to complete tracking. This is the procedure adopted during training. During inference, one additional module, Test-time Prompt Refinement (TPR), is inserted between PGN and RM. TPR, shown in the dashed box, leverages CLIP [3] to compile features. Hence, the features of the tracked object become more reliable, particularly for objects unseen during training, allowing RM to generate a more reliable feature.\nA. Revisiting DCF Tracking Paradigm\nIn this study, we use Transformer tracker ToMP [17], as our foundational tracker for its generality and discrimina-tive capability, though our method PiVOT can work with other trackers. In ToMP, its Tracking Head is a Transformer-based model predictor, consisting of a model predictor for predicting convolution filter weights and a target model for score map generation. ToMP maintains two reference frames: {Sref1, Srefe}. Sref\u2081 contains the initial tracking template specified by the user and remains unchanged during tracking. Sref is derived from the result in the previous frame. The reference frames are cropped larger than the template. They encompass both the template and its surrounding area in order to establish a filter for better target-background discrimina-tion. The templates are denoted as {Stem1, Stem2}, with one marked within a red box in the reference frame of Figure 2.\nGiven the reference frames {Sref1, Sref2} with their labels {Y1, Y2}, and the current frame Scur, we compute and re-spectively denote the frame features by {Vref1, Vrefe, Ucur}, each of which is of resolution RH\u00d7W\u00d7C where H \u00d7 W is the spatial resolution and C is the number of channels. The model predictor of the Tracking Head takes both the frame features and labels as input, and generates the enhanced feature maps for the current frame zcur \u2208 RH\u00d7W\u00d7C as well as a weight for the filter w \u2208 R1\u00d7C. This filter w is derived to discriminate the target from the background, especially distinguishing similar instances and also pinpointing the target location in the current frame. Specifically, convolving the current frame features with this filter yields the score map, namely\n$hcls = w * Zcur$. (1)\nIt follows that performing bounding box regressions gen-erates a dense location prediction map d \u2208 RH\u00d7W\u00d74 in the ltrb bounding box representation [38]. The coordinates with the highest confidence in the score map is mapped to the regression score map for bounding box prediction. The filter weights can also be used to predict the regression score map since the reference labels contain both classification and"}, {"title": "B. Promptable Visual Object Tracking", "content": "In the following, we make the tracker promptable by intro-ducing the Prompt Generation Network (PGN) and Relation Modeling (RM) so that we can leverage the strong zero-shot capability of CLIP to guide and improve the tracker.\nPrompt Generation Network (PGN): PGN is derived to generate a score map hcan \u2208 RH\u00d7W, where the centers of the candidate targets in the current frame are highlighted. Namely, once hean is resized to the frame resolution; the highlighted locations are expected to be close to the target center. The relationship between the current frame Scur and the score map hean is shown in Figure 2. The output score map of PGN is treated as an initial visual prompt for refining the feature map of the current frame before feeding it into the Tracking Head for tracking. With the features of two templates {tem1, Utem2} and the current frame Ucur, each of which is of size RH\u00d7W\u00d7C, the score map is computed by feeding the concatenated features of size [RH\u00d7W\u00d73C into a convolutional neural network $(\u00b7), where\n$hcan = $([Utem\u2081, Utem2, Ucur]). (2)\nThe input Utem to PGN is different from the input Vref to the Tracking Head. Utem encodes the exact bounding box area of the template, specifically the red box Stem in the reference frame depicted in Figure 2. This is because PGN is designed to identify target candidates that match the template in the current frame. Additionally, Utem2 is updated by comparing the CLIP similarity among the templates. If the similarity between a new template and the initial template surpasses that of the existing template, we update Utem2. Conversely, Vref encompasses both the template and its surrounding area. This is because the Tracking Head is designed to produce a filter to distinguish the target from the background, even if the targets have indistinguishable appearances. Uref2 is updated based on the confidence score of hels following previous work [1], [17]. Consequently, the network input to the backbone initially comprises five images: one current frame, two reference frames, and two templates for tracking where the updates occur only for the current frame, the second reference frame, and the second template.\nRelation Modeling (RM): Once the score map hean is obtained, it serves as the initial visual prompt and is channel-wise concatenated with the feature map of the current frame Ucur to generate the prompted feature map via\n$Ucurp = g$([hcan, Ucur]), (3)\nwhere g() is the relation network classifier [39] consisting of a Conv-BN-GeLU-based architecture. Then, we feed the prompted feature Ucurp to the Tracking Head to compute the final target score map hels.\nThe original relation network is designed for few-shot learning. Given a few examples and a query, the classifier learns to analyze the relationship for each query-example pair."}, {"title": "C. Offline Training", "content": "In this work, we adapt the relation network for tracking tasks, making go to learn to distinguish the relationship between the visual prompt and the image features. It is worth noting that the PGN and RM, being composed of lightweight and simple networks, introduce little to no additional cost for the tracker while maintaining the same complexity.\nBefore presenting the details of how to refine the visual prompt hcan by CLIP during the test time, we describe our loss function used in the offline training procedure. Similar to other recent end-to-end trained discriminative trackers [1], [17], we sample the current and reference frames from a video sequence to form training sub-sequences. The target classification loss is employed from DiMP [1], which is a discriminative loss for distinguishing background and foreground regions. The regres-sion loss is a generalized Intersection over Union loss [40]. The total objective function for the proposed method is:\n$Ltot = \\chi clsLcls(h, hcls) + \\chi canLcts(h, hcan) +\\chi regLreg(d, d)$, (4)\nwhere \\chi cls, \\chi can, and \\chi reg weight the corresponding losses. hels and d are the predicted classification and bounding box maps while h and d are the ground-truth labels. hcls and hcan share the same label h, which is similar to the ground-truth y (shown in Figure 2) with a Gaussian kernel process to the center of the target. Using this label with the DiMP loss addresses the data imbalance between the target and the background, as stated in DiMP."}, {"title": "D. Test-time Prompt Refinement", "content": "In the following, we show the details of how to leverage CLIP to refine the visual prompt, i.e., the score map hcan, dur-ing the test time for tracking improvement. Once the score map hcan is obtained, we can identify N target candidates where their positions in the score map are denoted by P = {pi}=1N and satisfy the following requirements:\n$\u0444\u0442\u0430\u0445 (hcan, Pi) = 1 and hcan(pi) \u2265 \u03c4, for 1 \u2264 i \u2264 N$, (5)\nwhere T represents the confidence threshold. Omax is an indicator function that returns 1 if the score at pi is a local maximum in hean, and 0 otherwise. The local maxima of hean are identified using the max-pooling operation in a 3 \u00d7 3 local neighborhood with a stride of 3.\nIn addition, we retrieve the corresponding bounding box for each target candidate from the bounding box regression map from the Tracking Head of the tracker at the last iteration since the scale changes between two frames are typically not signif-icant [15]. We avoid using PGN for target regression since the input Utem provides coarse information. It is utilized to predict the positions of multiple candidates. Utilizing its features to predict the regression map may not produce optimal results. Hence, we choose to use the predictions from the Tracking Head instead. With the bounding boxes, we can crop the N corresponding candidate RoIs {Scan;}1 from the current frame and extract their features using the image encoder of CLIP along with two reference templates {Stem1, Stem2}, as illustrated in Figure 2. Then, we compute an importance score"}, {"title": "IV. EXPERIMENTS", "content": "Our method is evaluated in this section. We detail the implementation, training, and testing setups. Next, we compare our method with state-of-the-art methods and analyze their performances. Finally, we perform ablation studies to validate the contributions of individual components.\nImplementation Details. Our method was developed using PyTorch 1.10 and CUDA 11.3 for PiVOT-50 and PyTorch 2.0.0 with CUDA 11.7 for PiVOT-L, all within the PyTracking framework [44]. We sample 200k sub-sequences and train the model for a total of 100 epochs using NVIDIA RTX 3090 GPUs. AdamW [45] is used as the optimization solver. There are two stages in the training process. For PiVOT-L, the backbone is frozen during both training stages since it leverages ViT-L/14 from DINOv2 [8] as its backbone, a vision foundation model with strong generalization capability. PiVOT-50 uses the ResNet-50 backbone and is fine-tuned in the first stage but frozen in the second stage.\nSpecifically, in the first stage, we train the tracker with-out the prompting components for 60 epochs, excluding the prompting components, for 60 epochs with a learning rate of 10-4. This rate decays by 0.2 after 30 and 50 epochs, producing a pre-trained tracking model. Following this, in the second stage, we integrate the pre-trained tracking model and fine-tune our prompting components with the pre-trained tracking model for an additional 40 epochs. The learning rate for the prompting components is initiated as 5 \u00d7 10-3, while the learning rate for fine-tuning the pre-trained tracking model is set to 4 \u00d7 10-6. This low value ensures that the pre-trained model maintains its discriminative capability while adapting to the refined features from the prompting components. The learning rates decay in the last 10 epochs. The difference between training PiVOT-50 and PiVOT-L depends mainly on the backbones they adopt. We set Areg = 1, Acls = 100, and Acan = 10 in the experiments. For prompt refinement, we use the official ViT-L/14@336px CLIP model [3]."}, {"title": "A. Comparisons with the State-of-the-Art Methods", "content": "Like previous methods [1], [17], We evaluate PiVOT with success (Suc), precision (Pr), and normalized precision (NPr) AUC scores. The precision score measures the center location distance between the predicted and ground truth targets, while the success score calculates their Intersection over Union (IoU). Detailed metric descriptions can be found in the ap-pendix of our supplementary material. To ensure consistency, we recalculated these metrics for all trackers using their raw predictions when available or the results reported in their papers. If both are missing, we refer to the survey paper [64]. In the absence of data, we omit reporting the results.\nPerformance comparisons for NfS, OTB-100, and UAV123 are in Table I. LaSOT results are detailed in Table II, with GOT-10K and TrackingNet in Table IV. AVisT and VOT2022 performances are in Tables III and V, respectively. Further"}, {"title": "V. ATTRIBUTE ANALYSIS FOR GOT", "content": "We analyze the attributes of the datasets through radar plots. By conducting a detailed attribute analysis of these datasets using radar plots, we can enhance the understanding of our method relative to others and identify areas for improvement in future work. Figure 8 and Figure 9 provides details of this extensive evaluation, including many competing trackers [2],"}, {"title": "VI. CONCLUSION", "content": "We introduce a promptable generic visual object tracker leveraging knowledge from foundation models like CLIP [3] and DINOv2 [8]. The proposed Prompt Generation Network (PGN) and Relation Modeling (RM) modules render the tracker promptable. Our method leverages CLIP for zero-shot knowledge transfer. The visual prompt can be automatically generated through RM and PGN and refined online by CLIP, guiding the tracker to focus more on the target indicated by the visual prompt. We also extend PiVOT using the frozen ViT backbone from DINOv2 for feature extraction. which reduces inductive bias and computational costs while still achieving performance improvements without fine-tuning the large ViT backbone with tracking data. We conduct a comprehensive experimental validation and analysis of PiVOT on several challenging datasets, revealing enhanced tracking performance."}, {"title": "D. Test-time Prompt Refinement", "content": "Di = 1/2 * [exp(cossim (Ecan, Etem;))/ N(k=1 to N) exp(cossim (Ecank, Etem;)))"}]}