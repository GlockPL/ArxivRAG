{"title": "Module-wise Adaptive Adversarial Training for End-to-end Autonomous Driving", "authors": ["Tianyuan Zhang", "Lu Wang", "Jiaqi Kang", "Xinwei Zhang", "Siyuan Liang", "Yuwei Chen", "Aishan Liu", "Xianglong Liu"], "abstract": "Recent advances in deep learning have markedly improved autonomous driving (AD) models, particularly end-to-end systems that integrate perception, prediction, and planning stages, achieving state-of-the-art performance. However, these models remain vulnerable to adversarial attacks, where human-imperceptible perturbations can disrupt decision-making processes. While adversarial training is an effective method for enhancing model robustness against such attacks, no prior studies have focused on its application to end-to-end AD models. In this paper, we take the first step in adversarial training for end-to-end AD models and present a novel Module-wise Adaptive Adversarial Training (MA2T). However, extending conventional adversarial training to this context is highly non-trivial, as different stages within the model have distinct objectives and are strongly interconnected. To address these challenges, MA2T first introduces Module-wise Noise Injection, which injects noise before the input of different modules, targeting training models with the guidance of overall objectives rather than each independent module loss. Additionally, we introduce Dynamic Weight Accumulation Adaptation, which incorporates accumulated weight changes to adaptively learn and adjust the loss weights of each module based on their contributions (accumulated reduction rates) for better balance and robust training. To demonstrate the efficacy of our defense, we conduct extensive experiments on the widely-used nuScenes dataset across several end-to-end AD models under both white-box and black-box attacks, where our method outperforms other baselines by large margins (+5-10%). Moreover, we validate the robustness of our defense through closed-loop evaluation in the CARLA simulation environment, showing improved resilience even against natural corruption.", "sections": [{"title": "I. INTRODUCTION", "content": "RECENT advancements in deep learning have driven significant progress in autonomous driving (AD) models. These models typically involve a series of interconnected tasks, including perception [1], prediction [2], and planning [3]. Traditional approaches often focus on addressing individual tasks in isolation [4]-[6], which can lead to issues such as information loss across modules, error accumulation, and feature misalignment [7]\u2013[9]. To address these challenges, end-to-end AD models, which unify all components from perception to planning, were proposed to offer a more holistic solution and achieve state-of-the-art performance [10], [11]. Despite the promising performance, deep-learning-based AD models are highly vulnerable to adversarial attacks, where imperceptible perturbations can significantly degrade the model performance [12]\u2013[19]. Previous studies have extensively assessed the adversarial robustness of AD systems in a wide range of sub-tasks (e.g., object detection [20]- [24] and trajectory prediction [25]-[28]) and even the end-to-end AD models [29]-[31]. To improve the robustness against adversarial attacks, various defense approaches have been proposed for mitigation [32], [33]. Among these, adversarial training [13], [34] has proven particularly effective by incorporating adversarial examples for data augmentation. While there has been considerable research on adversarial training in the context of autonomous driving such as 3D object detection [35], [36], end-to-end AD models have received relatively little attention. This sparsity of research presents a severe risk to the safety of end-to-end autonomous driving, as it increases their vulnerability to attack.\nTherefore, this paper takes the first step in studying adversarial training in the context of end-to-end AD. However, simply extending the existing adversarial training baselines to the context of end-to-end AD is non-trivial owing to the different learning paradigms. In particular, we identify two key challenges impeding robust adversarial training in this scenario: (1) diverse training objectives: designing effective adversarial training targets is complex due to the differing objectives of each module in the whole end-to-end pipeline. (2) different module contributions: different modules have varying impacts and contributions on the model's final decision robustness. To address these challenges, this paper proposes Module-wise Adaptive Adversarial Training (MA2T). As for the issue of diverse training objectives across modules, we design Module-wise Noise Injection, which targets training models with the guidance of overall objectives rather than each independent module loss. This approach ensures that noise is generated with a holistic view of the model, i.e., using the overall loss for backpropagation instead of focusing on individual module losses that may be contradictory and pose negative impacts on overall decision robustness. To manage the different contributions of modules during training, we introduce Dynamic Weight Accumulation Adaptation, which adaptively adjusts the loss weights of each module to the overall objectives based on their contributions during noise injection. In particular, this method incorporates a weight"}, {"title": "II. PRELIMINARIES AND BACKGROUNDS", "content": "Adversarial attacks introduce slight perturbations to inputs, causing deep learning models to make incorrect predictions. Recent studies have explored various aspects of these attacks [24], [34], [37]\u2013[44]. These attacks are generally categorized into white-box and black-box types. White-box attacks, first introduced by Szegedy et al. [45], assume the attacker has full access to the model's internal structure. Goodfellow et al. [46] later developed the Fast Gradient Sign Method (FGSM), while Kurakin et al. [47] introduced iterative techniques with the Basic Iterative Method (BIM). Madry et al. [48] further refined these methods, leading to the widely recognized Projected Gradient Descent (PGD) attack. In contrast, black-box attacks assume limited access to the model's internal details and utilize methods like Zeroth Order Optimization (ZOO) [49], which estimates gradients from model outputs, or the Momentum Iterative Fast Gradient Sign Method (MI-FGSM) [50], where adversarial examples generated on a substitute model are used to attack the target model. AutoAttack [51], a comprehensive attack framework, offers both white-box and black-box modes and is currently considered one of the most effective attacks.\nGiven an input $x \\in X$, where $X$ represents the input space (e.g., images), and a corresponding output $y \\in Y$, where $y$ denotes the output space (e.g., driving actions), we define a model $f_{\\theta} : X \\rightarrow Y$ parameterized by $\\theta$, that maps inputs to outputs. The model's objective is to minimize a loss function $L(y, f_{\\theta}(x))$ over a dataset $D = \\{(x_i, y_i)\\}_{i=1}^{N}$, where $N$ is the number of samples. This objective can be expressed as:\n$ \\theta^* = \\arg \\min_{\\theta} \\frac{1}{N} \\sum_{i=1}^{N} L(y_i, f_{\\theta}(x_i)), $"}, {"title": "A. Adversarial Attack"}, {"title": "B. Adversarial Training", "content": "Adversarial training is currently one of the most effective methods for improving neural network robustness. Initially proposed by Goodfellow et al. [12], this approach incorpo- rates adversarial examples during training to enhance model resilience. Since then, a variety of adversarial training methods have been developed to further strengthen model defenses. Traditional adversarial training typically blends adversarial examples with standard training data, but this may not pro- vide sufficient protection against a wide range of adversarial attacks. To address this limitation, some researchers have proposed introducing adversarial noise into the intermediate layers of neural networks. For example, Sankaranarayanan et al. [52] presented a hierarchical adversarial training method that perturbs activations in intermediate layers and computes adversarial gradients based on the previous batch, resulting in stronger regularization. Similarly, Liu et al. [34] introduced the Adversarial Noise Propagation (ANP) method, which injects diversified noise into hidden layers during training. By generating adversarial noise within the same batch, ANP substantially enhances model robustness. Adversarial training can be expressed as a min-max optimization problem:\n$ \\theta^* = \\arg \\min_{\\theta} \\min_{x^{adv}: ||x^{adv}-x_i||_p \\leq \\epsilon} \\frac{1}{N} \\sum_{i=1}^{N} L(y_i, f_{\\theta}(x^{adv})), $"}, {"title": "C. End-to-end Autonomous Driving Models", "content": "End-to-end models offer distinct advantages by simplify- ing the integration of perception and decision-making into a unified framework, thereby minimizing the intricacies of conventional segmented systems. Early end-to-end models seek to consolidate various AD tasks into a single framework to enhance interpretability. A notable example is the P3 series: P3 [9] innovatively aligns the motion planning costs con- sistently with perception and prediction estimates, following which, MP3 [53] and ST-P3 [54] emerge, further advancing end-to-end AD performance by integrating mapping task and learning spatiotemporal features respectively. ST-P3 [54], in particular, outperforms previous methods in individual stages. Subsequent models continue to improve learning strategies for AD. For instance, LAV [55] not only integrates multi- stage tasks but also incorporates driving experiences from all surrounding vehicles to refine driving strategies. With the advent of Transformer [56], the field of AD experienced a notable surge in interest in their application. UniAD [10], the first end-to-end network covering the entire AD stack, employs query-based interactions to facilitate information exchange across tasks and has achieved superior results across the entire stack of tasks. Following this, VAD [11], also based on Transformer [56], uses vectorized scene representations for learning across driving tasks, achieving new state-of-the-art end-to-end planning performance with high efficiency.\nEnd-to-end AD models map sensory inputs $x \\in X$ directly to driving actions $y \\in y$. These models typically include M perception modules $\\{f_m\\}_{m=1}^{M}$, each extracting features $f_m \\in F_m$ from $x$. These features are then processed by K prediction modules $\\{f_k^2\\}_{k=1}^{K}$ to generate predicted states $p_k \\in P_k$. Finally, a planning module $f^3$ combines these predictions to determine the optimal action $y$. The model can be expressed as:\n$ f_{\\theta}(x) = f^3 (\\{f_k^2(\\{f_m^1(x)\\}_{m=1}^{M})\\}_{k=1}^{K}), $"}, {"title": "III. THREAT MODEL", "content": "Existing adversarial training techniques primarily focus on single-module tasks, where adversarial inputs are generated at the image level. However, applying these methods directly to advanced module-wise end-to-end autonomous driving (AD) models introduces significant challenges. We identify two key challenges that must be addressed to enhance the robustness of these complex models effectively:\nChallenge : Diverse Training Objectives. Designing effective adversarial training targets is complex due to the differing objectives of each module. In single-module tasks, using the module's specific loss function to train the model adversarially is generally effective. This approach directly targets the module's weaknesses, making it easier to enhance its robustness. However, in end-to-end AD models, where multiple interconnected modules work together to achieve a final outcome, this strategy falls short. Each module, from perception to planning, has a unique objective and operates under different constraints. Simply focusing on individual module losses during adversarial training may not lead to meaningful improvements in the model's overall robustness. Instead, it is essential to design training targets that consider the collective impact of all modules, ensuring that the adversarial training enhances the resilience of the entire model rather than just isolated components.\nChallenge : Different Module Contributions. Different modules have varying impacts on the model's final robustness."}, {"title": "B. Adversarial Goals", "content": "In end-to-end AD models, the defender's primary objec- tive is to ensure the model remains robust under the most challenging adversarial conditions, as represented by Eq. (4). The defender seeks to minimize the worst-case loss function $L(y, f_{\\theta^{adv}})$ by:\n$ \\theta^* \\arg \\min_{\\theta} \\frac{1}{N} \\sum_{i=1}^{N} \\max_{||\\delta_i|| \\leq \\epsilon} L(y_i, f_{\\theta}(x_i + \\delta_i)), $"}, {"title": "IV. METHOD", "content": "In this section, we present our proposed approach, Module- wise Adaptive Adversarial Training (MA2T), which is com- posed of two key components: Module-wise Noise Injection and Dynamic Weight Accumulation Adaptation."}, {"title": "A. Module-wise Noise Injection", "content": "To address the challenges posed by diverse training objec- tives, we designed the Module-wise Noise Injection method. Instead of applying noise at the image level, we inject it directly into each module's input (e.g., adding noise to the image in the perception module) to ensure comprehensive training across all modules. While the typical approach targets each module's loss, this can lead to inconsistent impacts on the model. To mitigate this, we adopt a unified objective to guide training. Our Ablation Study highlights the effectiveness of this method, offering a novel solution to challenge 0.\nSpecifically, as shown in Eq. (6), we introduce noise $\\delta_i$ at the input of each module to achieve module-wise noise injection:\n$ f^{adv} (x_i, \\delta_i) = f^3 (\\{f_k^2(\\{f_m^1(x_i, \\delta_m^1)\\}_{m=1}^{M}, \\delta_k^2)\\}_{k=1}^{K}, \\delta^3), $"}, {"title": "B. Dynamic Weight Accumulation Adaptation", "content": "To address the varying module contributions mentioned in challenge, we introduced Dynamic Weight Accumulation Adaptation, which adaptively adjusts the loss weights of each module by incorporating accumulated weight changes. This approach leverages the modules' contribution (accumulated reduction rates) to ensure better balance and more robust training. The loss for each module during forward propagation can be described as $L_j^t$, which represents the loss of module $j$ at time step $t$.\nIn multi-task learning, Dynamic Weight Accumulation (DWA) [57] adjusts the relative size of losses before and after each task to balance the overall loss. Extend the concept of multitasking to multiple modules, the ratio of each module's loss $L_j$ at the current time step $t$ relative to its previous value is calculated as:\n$ r_j^t = \\frac{L_j^t}{L_j^{t-1}}, $"}, {"title": "C. Overall Training"}, {"title": "V. EXPERIMENTS", "content": "We evaluate the effectiveness of our proposed MA2T against various adversarial attacks. Following the guidelines from [58], we compare MA\u00b2T with several commonly-used adversarial defense methods, assessing its performance against both ad- versarial noise and natural corruption."}, {"title": "A. Experimental Setup", "content": "Datasets. Following [10], [11], we conduct our experiments on the nuScenes dataset [59], a comprehensive AD benchmark that provides richly annotated data across multiple sensor modalities, including cameras from six perspectives, LiDAR, and radar. This dataset is widely used as a standard baseline for evaluating AD tasks, covering a broad spectrum of driving conditions and environments.\nModels. We select two representative end-to-end au- tonomous driving models to validate the effectiveness of MA2T, i.e., UniAD [10] and VAD [11]. UniAD is the first model to integrate full-stack autonomous driving tasks, achiev- ing excellent performance on all tasks through plan-oriented collaboration. Unlike the complex construction method of UniAD, VAD learns autonomous driving tasks using vector- ized representations, even surpassing UniAD in planning.\nMetrics. In our experiments, we employ metrics from [10] to ensure consistency and facilitate direct comparison. We evaluate tracking performance using Average Multi-Object Tracking Accuracy (AMOTA \u2191) and assess map alignment through Intersection over Union (IOU \u2191) between predicted and ground-truth maps. Motion forecasting precision is mea- sured by Minimum Average Displacement Error (minADE \u2193), while occupancy accuracy is also evaluated using IOU \u2191. Finally, planning safety and reliability are assessed by Average L2 Error (Avg. L2 Error\u2193) over the next 3 seconds.\nAdversarial Attacks. For comprehensive robustness eval- uation, we consider setting up the model under both white- box and black-box attacks. White-box means that attackers can obtain model information from the victim models, such as gradients, while this information is invisible to black- box attackers. White-box settings. we follow established guidelines [60], [61], incorporating multiple adversarial attacks with various perturbation types and adapting these settings to end-to-end models. We constrain the perturbation under the l\u221e norm to 0.2, aligning with common practices on ImageNet. Based on this l\u221e setting, we calculate the corresponding l1 and l2 perturbation constraints\u2013240 and 288,000, respec- tively\u2014according to the image size of nuScenes. For l\u2081 and l2 attacks, we use PGD, while for l\u221e attacks, we employ PGD, FGSM, and MI-FGSM. The number of iteration steps is set to 5. Black-box settings. We apply the 5 attack methods to generate adversarial examples from the attack models and transfer them to the victim models for testing, selecting the method with the strongest attack effect for reporting.\nAdversarial Training Baselines. We compare four basic adversarial training methods with MA2T. The first method is the origin of adversarial training, [12], which introduces adversarial losses obtained from FGSM adversarial samples in normal training. [13] proposed a more powerful adversarial training method, namely PGD adversarial training. Based on this, we used 11, 12 and l\u221e norm adversarial training as the other three comparative methods. We use FGSM, P1, P2 and Po to represent the above four methods in the experiments.\nImplementation Details. For UniAD, we inject noise at five modules during training, corresponding to the five module interaction paths in the model. The l\u221e perturbation constraints for the noise in each module (i.e., Track, Map, Motion, Occ, Plan) are set to 0.8, 0.1, 0.1, 0.1, and 0.1. During the internal maximization, the number of attack iterations is 5, with the training limited to 3 epochs. For VAD, which differs from UniAD in that it uses vectorized representations to infer various driving tasks without clear module divisions, we inject noise into the vectorized representations of detection, mapping, motion, and planning. The corresponding l\u221e per- turbation constraints for these are all set to 0.1. The iteration count remains at 5, and the training spans 10 epochs. The time decay factor r for each model is set to 0.2. All training and testing are conducted on 8 NVIDIA A800 GPUs (with 80GB of memory each), and the parameters mentioned above are optimized based on multiple training comparisons to achieve the best results."}, {"title": "B. Main Results", "content": "In this section, we evaluate the robustness of the UniAD and VAD models against various perturbation types under both white-box and black-box settings. We perform five random restarts for attacks for each input to ensure reliability. Our proposed method, MA\u00b2T, is trained across five independent runs, and we report the average results from each run. Given that AD tasks ultimately map raw sensor data to planned trajectory results, we primarily present the results for the Planning task. The results for other tasks are provided in the supplementary materials."}, {"title": "C. Ablation Study", "content": "In this section, we provide some ablation studies to inves- tigate our MA2T approach further.\nDifferent Noise Budgets and Epochs in MAT. This section details our approach to selecting parameter settings in MA2T adversarial training, balancing resource constraints with performance outcomes. We use PGD-l as the evaluation method and report the reduction in planning errors under various training configurations.\nFirst, we address the issue of training epochs. Given the large size of end-to-end models and the time-intensive nature of adversarial training, it's crucial to strike a balance between training time and model performance. For UniAD, one epoch of adversarial training requires an entire day, while the original training process takes 20 epochs. We conducted training for 1 to 5 epochs, with the planning performance results shown in Tab. III. By the 3rd epoch, the model already demonstrates strong defense capabilities. As training continues, additional epochs result in diminishing returns, with minimal perfor- mance improvement despite significant time investment.\nRegarding the perturbation budget, we adjusted and com- pared the noise at each stage by controlling variables, keeping other perturbation settings fixed. Using UniAD as an example, we evaluated the impact of noise at five different stages on model robustness under varying perturbation constraints during training, with planning results as the evaluation metric. Specifically, the baseline l\u221e perturbation constraints for the noise in each module (i.e., Track, Map, Motion, Occ, Plan) were set to 0.8, 0.1, 0.1, 0.1, and 0.1, respectively. We then ad- justed the size of only one noise at a time and observed the re- sulting changes in the trained model. As shown in Tab. IV, the contribution of perturbation constraints to model robustness generally follows a convex pattern\u2014both excessively small or large constraints lead to decreased performance. Based on these observations, we selected the optimal adversarial training settings for both models.\nEffectiveness of the Module-wise Noise Injection. As a core component of our MA2T, the Module-wise Noise Injec- tion is important in bolstering adversarial defense. To better understand its impact, we conduct a series of experiments to evaluate its effectiveness.\nWe specifically examine the effects of selectively injecting or omitting noise to various modules to assess the robustness of the final model. These experiments are performed using the UniAD model on nuScenes dataset, employing a white- box PGD-l attack with a perturbation budget of \u20ac = 0.2, 5 steps, and a step size of a = \u20ac/5. As depicted in Fig. 1, we experiment with five different types of noise. The default setting included injecting all five types, and we explore settings where only one type of noise is either removed or retained during adversarial training to gauge its effect.\nGiven the variability in results across different training sessions, we conducted 20 repeated experiments for each setting and visualized the Avg. L2 Error with a violin plot, as shown in Fig. 2a. Overall, the results indicate that the most effective defense is achieved when all types of noise are injected. Additionally, as the number of modules injected with noise increases, the defense effect significantly improves. Specifically, we find that when only a single noise is injected, the noise from the Track-Motion branch contributes the most to the robustness of the model, while when only a single noise is discarded, the planning error is maximized after discarding the noise from the Track-Motion branch. This implies the key and fragile characteristics of the Track-Motion interface.\nEffectiveness of Dynamic Weight Accumulation Adap- tation. To assess the effectiveness of Dynamic Weight Ac- cumulation Adaptation (DWAA), we conducted experiments by removing this component. Using the UniAD model on the nuScenes dataset, we applied a white-box PGD-l\u221e attack with a perturbation budget of \u20ac = 0.2, 5 steps, and a step size of \u03b1 = \u20ac/5. For training, we set l\u221e perturbation constraints for each module (i.e., Track, Map, Motion, Occ, Plan) at 0.8, 0.1, 0.1, 0.1, and 0.1, respectively. The attack steps were set to 5, with fine-tuning over 3 epochs."}, {"title": "D. Adaptive White-box Attacks for MA2T", "content": "Adaptive Attack Design. In addition to the commonly used adversarial attacks, we also evaluate the performance of our MA\u00b2T against white-box attacks specifically designed for it, intending to provide a more thorough analysis. Starting from MA2T, we also design specific attack methods based on modular noise injection. We customize three different types of attacks by utilizing the rich task characteristics of end-to-end autonomous driving models. Our attack is consistent with the PGD-l attack in the main experiments, which uses the lo norm constraint, with a perturbation constraint of 0.2 for each module and 5 iterations.\nModule-wise Attack. This attack method mirrors the internal maximization process used in our training approach, extracted as a distinct attack strategy. During model inference, noise is injected into each module, with the model's own training target serving as the attack objective.\nSub-loss Attack. In this approach, targeted attacks are conducted at different stages of the end-to-end model. Noise is injected module by module, with each module's noise specifically targeting its corresponding task. The sub-loss of the respective task is used as the optimization objective for the corresponding noise. Specifically, the objective function of each noise is the loss of the nearest neighbor task.\nPlan-targeted Attack. This attack focuses on the critical planning task in AD. Noise is injected into each subtask during model inference, but all noise is directed toward the final planning output, with the planning loss serving as the goal for all noise optimization.\nResults and Analysis. We evaluated MA\u00b2T's defense ef- fectiveness to three adaptive attack methods on UniAD and VAD models. The most critical plan Avg. L2 Error (m) \u2193 is shown in Fig. 4."}, {"title": "VI. CLOSED-LOOP SIMULATION EVALUATION", "content": "In this section, we conduct a closed-loop evaluation using the CARLA simulator [63], integrating the state-of-the-art UniAD and VAD models. Following the methodology outlined in [64], we input images directly into the models as perception data to assess their decision-making performance in realistic driving scenarios. To validate the real-world effectiveness of our MA2T, we performed black-box attack experiments."}, {"title": "A. Experimental Setup", "content": "Simulation Environment Setup. We set up the simulation environment using CARLA 0.9.15 and Leaderboard v2, fo-"}, {"title": "B. Closed-loop Simulation Results", "content": "To validate the robustness of the adversarially trained models, we tested the MA2T trained models on CARLA by applying the generated noise to the input images. The performance comparison between the (a) vanilla models under clean conditions, (b) vanilla models after the attack, and (c) MA2T trained models after the attack are illustrated in Fig. 5."}, {"title": "VII. DISCUSSION AND ANALYSIS", "content": "To better understand the contributions of different mod- ules, we selectively freeze specific module parameters during training. By keeping these modules' weights fixed, we can assess their impact on the model's overall performance. Using UniAD as the baseline, we freeze five modules and apply loo perturbation constraints for each (i.e., Track, Map, Motion, Occ, Plan) with values set to 0.8, 0.1, 0.1, 0.1, and 0.1, respectively. The attack steps are set to 5, and fine-tuning is performed over 3 epochs. The results are shown in Fig. 6a."}, {"title": "B. Dynamic Weight Analysis", "content": "We tracked the weight changes of five modules during MA2T training process of UniAD, as shown in Fig. 6b, where the horizontal axis represents the number of batches. Initially, all module weights are set to 1.0, but they adjust over time. The weights of the Plan and Track modules increase, reaching 1.023 and 1.009, respectively, while the Map module drops to 0.981. This indicates that the Map module's performance declines more rapidly than the others, prompting the Dynamic Weight Accumulation Adaptation to lower its weight for balance. Additionally, module weights fluctuate, with significant changes observed between batches 6500, reflecting the varying contributions of different modules throughout training. Specifically, the Occ module increases rapidly at first, then decreases in later process."}, {"title": "C. Natural Corruption Analysis", "content": "Natural corruption refers to noise commonly encountered in the real world, which differs significantly from adversarial noise. We conducted additional experiments focusing on nat- ural corruption to assess our model's robustness against such disturbances. Following the methodology in [65], we evaluated our model under various natural corruption scenarios. Specif- ically, we selected six types of natural corruptions\u2014Contrast, Frost, Snow, Gaussian Noise, Shot Noise, and Spatter\u2014which represent the four distinct corruption categories defined in [65]. These evaluations were performed using the model trained in our main experiment. The results, shown in Fig. 6c, demonstrate that our MA2T method consistently performs well against natural corruptions, outperforming both the vanilla model and other adversarial training methods (average drop in Avg. L2 Error of 0.15). This underscores the effectiveness of our approach in defending against natural corruption."}, {"title": "D. Visualization", "content": "We also report the visualization comparison of key driving scenarios. Fig. 8 illustrates (a) vanilla UniAD under clean conditions, (b) vanilla UniAD after the PGD-l\u221e attack, and (c) MA2T trained UniAD after the PGD-l\u221e attack. In this scenario, UniAD makes critical errors in predicting the trajec- tories of surrounding vehicles, incorrectly identifying many stationary vehicles as being in motion (as indicated by the red arrow in the BEV view on the right side of Fig. 8b). This leads to the ego vehicle mistakenly avoiding future vehicles that don't actually exist. Consequently, UniAD's final trajectory collides with a stationary vehicle on the left side (as shown by the red circle in Fig. 8b). However, after applying MA2T, the model accurately predicts the future motion states of surrounding vehicles, closely resembling the scenario without an attack. The resulting decision trajectory remains centered on the road (as indicated by the blue circle in Fig. 8c)."}, {"title": "VIII. RELATED WORKS", "content": "Adversarial attacks present significant challenges in AD, particularly when targeting advanced end-to-end models. Re- cent studies aim at evaluating and enhancing the robustness of AD systems against such attacks. For instance, Zhang et al. [26] demonstrate how minor adversarial attacks can severely impact trajectory prediction. Similarly, Cao et al. [29] highlight that even robust end-to-end models are susceptible to adversarial perturbations, which can significantly compromise safety. Furthermore, Zhang et al. [66] investigate dynamic adversarial attacks that adapt to environmental changes, em- phasizing the challenges of defending against these threats in real-time scenarios.\nAdversarial training is currently one of the most effective methods for enhancing the robustness of neural networks. As autonomous driving technology advances, the role of adversarial training in fortifying model resilience within this field becomes increasingly critical. Li et al. [35] introduced a depth-aware adversarial training method (AdvMono3D) for monocular 3D object detection, which improves model ro- bustness in complex scenarios by incorporating multi-scale adversarial perturbations. Similarly, Li et al. [67] employed an adversarial training strategy that strengthens collaborative per- ception systems against adversarial attacks through a consen- sus mechanism, thereby enhancing the safety of autonomous driving. Zhang et al. [36] conducted a comprehensive analysis of the vulnerability of LiDAR 3D object detectors to adver- sarial attacks in autonomous driving, integrating adversarial training methods to bolster model robustness. Wang et al. [68] enhanced the resilience of multi-sensor fusion systems through adversarial training, enabling these systems to more effectively resist various adversarial attacks, thereby improving the overall robustness and safety of autonomous driving systems.\nDifferences. As discussed above, current adversarial train- ing research for autonomous driving models primarily focuses on single tasks or modules, such as 3D object detection and other sub-tasks. This paper focuses on adversarial training in end-to-end autonomous driving models and proposes Module- wise Adaptive Adversarial Training (MA2T), the first adver- sarial training method specifically designed for end-to-end autonomous driving models, addressing a gap in existing re- search. Additionally, we innovatively introduce modular noise injection and a dynamic weight accumulation adaptive mech- anism, addressing the challenges of inconsistent objectives across modules and strong inter-module dependencies in end-to-end models, significantly enhancing the model's robustness in various adversarial attack scenarios."}, {"title": "IX. CONCLUSION AND OUTLOOK", "content": "End-to-end autonomous driving (AD) models, which inte- grate perception, prediction, and planning into a unified frame- work, offer significant advantages in simplifying decision- making processes. However, their tightly coupled nature also makes them particularly susceptible to adversarial perturba- tions, and the lack of comprehensive adversarial training methods leaves these models vulnerable to attacks. Existing defenses typically focus on individual tasks within the AD pipeline and are often limited to specific types of perturbations, failing to address the complexity and interconnectedness of end-to-end AD systems.\nIn this paper, we introduced Module-wise Adaptive Adver- sarial Training (MA2T), a novel approach specifically designed to enhance the robustness of end-to-end AD models against a wide range of adversarial attacks. MA\u00b2T addresses the unique challenges of these models by incorporating module-wise noise injection and dynamic weight accumulation adaptation, ensuring balanced and effective training across all stages of the AD pipeline.\nWe demonstrated the effectiveness of MA2T through ex- tensive experiments on the nuScenes dataset, where it sig- nificantly outperformed existing adversarial training methods across multiple tasks. Furthermore, closed-loop evaluations in the CARLA simulator confirmed that MA2T improves the robustness of end-to-end AD models in closed-loop evaluation."}]}