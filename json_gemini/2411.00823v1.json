{"title": "Mobility-LLM: Learning Visiting Intentions and Travel Preferences from Human Mobility Data with Large Language Models", "authors": ["Letian Gong", "Yan Lin", "Xinyue Zhang", "Yiwen Lu", "Xuedi Han", "Yichen Liu", "Shengnan Guo", "Youfang Lin", "Huaiyu Wan"], "abstract": "Location-based services (LBS) have accumulated extensive human mobility data\non diverse behaviors through check-in sequences. These sequences offer valuable\ninsights into users' intentions and preferences. Yet, existing models analyzing\ncheck-in sequences fail to consider the semantics contained in these sequences,\nwhich closely reflect human visiting intentions and travel preferences, leading to\nan incomplete comprehension. Drawing inspiration from the exceptional semantic\nunderstanding and contextual information processing capabilities of large language\nmodels (LLMs) across various domains, we present Mobility-LLM, a novel frame-\nwork that leverages LLMs to analyze check-in sequences for multiple tasks. Since\nLLMs cannot directly interpret check-ins, we reprogram these sequences to help\nLLMs comprehensively understand the semantics of human visiting intentions and\ntravel preferences. Specifically, we introduce a visiting intention memory network\n(VIMN) to capture the visiting intentions at each record, along with a shared pool\nof human travel preference prompts (HTPP) to guide the LLM in understanding\nusers' travel preferences. These components enhance the model's ability to extract\nand leverage semantic information from human mobility data effectively. Extensive\nexperiments on four benchmark datasets and three downstream tasks demonstrate\nthat our approach significantly outperforms existing models, underscoring the\neffectiveness of Mobility-LLM in advancing our understanding of human mobility\ndata within LBS contexts.", "sections": [{"title": "1 Introduction", "content": "Location-based services (LBS) such as Gowalla, Weeplace, and Foursquare enable users to share and\ndiscover location information and nearby services. This results in the collection of extensive human\nmobility data, often presented in the form of check-in sequences. These sequences record users'\nvisits to different points of interest (POIs) like restaurants and hospitals at various times, reflecting\nsignificant semantics about their intentions and preferences. Analyzing these check-in sequences\nis crucial as it offers valuable information on human mobility data, which can positively impact\nindividuals, businesses, and urban management."}, {"title": "2 Related Works", "content": "In this section, we provide short reviews of literature in the areas of mobility data mining and cross-\ndomain applications of LLMs. We postpone the detailed discussion of works to the Appendix A, due\nto limited space.\nMobility Data Mining has emerged as a promising research area due to the proliferation of location-\nbased services. This has led to the development of three significant tasks that enhance service\nquality: next location prediction (LP), next time prediction (TP), and trajectory user link (TUL).\nThe LP task aims to anticipate a user's future location based on their historical movement. Several\nnotable models have emerged as leading approaches in LP [10, 58, 50, 47, 5, 22, 57]. The TUL task\nfocuses on establishing connections between different trajectories and users, facilitating the analysis\nof user movement patterns and uncovering valuable insights into their behavior [13, 7, 59]. The\nTP task focuses on estimating the time at which a user is likely to visit their next location. Various\nmodels have been developed to model the intensity function representing the rate or density of event\noccurrences, effectively making accurate time predictions [56, 16, 52].\nCross-domain Application of LLMs has gained attention in recent studies, which adopt large\nlanguage models to address the challenge of limited training data. In the field of time series analysis,\nLLM4TS [4] is a pioneering method that aligns pre-trained large language models with temporal\ncharacteristics, introducing a two-level aggregation method to effectively incorporate multi-scale\ntemporal data into pre-trained LLMs. One-Fits-All [62] is a unified framework that leverages a frozen"}, {"title": "3 Preliminaries", "content": "POI Visiting Record In the check-in datasets, a user's visit to a certain place is represented by a\nPOI visiting record $R = (L_p, t)$ generated by the user $u$. $L_p$ indicates the visited POI at time $t$. $L_p$ is\nrepresented by $(L_{id}, L_{lon}, L_{lat}, L_{category})$, comprising $L_{id}$ as a POI index, and accurate longitude\n$L_{lon}$ and latitude $L_{lat}$. $L_{category}$ denotes the category of the visited POI (e.g., hospital, restaurant).\nCheck-in Sequence A user's movement during a specific period can be represented by sequential\nPOI visiting records, which we refer to as a check-in sequence. We denote a check-in sequence as\n$C =< R_1, R_2,\uff65\uff65\uff65, R_n >$, where the POI visit records are ordered by their visited time, and $n$ is the\nlength of the sequence.\nProblem Statement Given a check-in sequence $C$, our objective is to encode this sequence into a\nmeaningful representation. This representation can be used for various tasks. In this paper, we choose\nthree typical check-in prediction tasks: 1) Identifying the user $\u00fb$ who generated the check-in sequence\n(TUL task). 2) Predicting the next location $\\hat{s}_{n+1}$ the user will arrive at (LP task). 3) Predicting the\narrival time $\\hat{t}_{n+1}$ at this location (TP task)."}, {"title": "4 Methodology", "content": "We introduce a novel unified framework, Mobility-LLM, designed to address a variety of check-in\nsequence tasks. The overall structure of this framework is illustrated in Fig. 1. 1) Initially, to\nembed POIs by incorporating category semantics, we introduce the POI Point-wise Embedding Layer\n(PPEL) to generate the embedding of POIs in the current check-in record (referred to as PPE). 2)\nSubsequently, we feed the PPEs and timestamps of a check-in sequence into the Visiting Intention\nMemory Network (VIMN) to capture the visiting intentions of users at each check-in record. 3)\nA Human Travel Preference Prompt (HTPP) pool is introduced to extract users' preferences from\ncheck-in sequences, which act as cues to assist the LLM in comprehending users' travel preferences\nmore effectively. 4) Finally, we use the different parts outputs of LLM (corresponding to VIMN,\nHTPP), each with its own projection head, to forecast the user's next location, the estimated arrival\ntime, and the user who generates the check-in sequence.\nPOI Point-wise Embedding Layer (PPEL) is designed to generate the semantic information\nembedding for each POI in a check-in record. This is especially important since POI categories\ncontain a wealth of semantic information. It has been observed that the category descriptions of\nPOIs in the original check-in datasets are often vague or unidentified. When we refer to \"vague,\" we\nare indicating that the descriptions are too broad or contain abbreviations, making it challenging to\ndetermine the specific POI categories accurately. To address this issue, we have developed a category\nword pool (see Appendix I for the whole categories list) that allows each POI to match with the\nmost appropriate categories automatically. We reindex the POI IDs and word IDs in each dataset,\nmaking it convenient for subsequent encoding of these IDs. As shown in Fig. 1a, we use the learnable\nembedding $E_{L_{id}}$ of each POI ID as the query, the learnable embedding $E_{c_{id}}$ of the category word ID\nas the key, and the corresponding category word token $E_{c_{token}}$ from the LLM tokenizer as the value.\nA Point-wise attention mechanism is used to calculate the i-th PPE $s_i$ of POIs:\n$s_i = Softmax(\\frac{Que(E_{L_{id}}) Key (E_{c_{id}})^T}{\\sqrt{d}}) Val(E_{c_{token}}) + GeoHash(L_{lon}, L_{lat}),$ (1)\nwhere Que(), Key(), and Val(\u00b7) denote linear projections, $d$ is the dimension of $E_{c_{token}}$,\nGeoHash(,) encodes geographic coordinates (latitude and longitude) into an embedding vec-"}, {"title": "Visiting Intention Memory Network (VIMN)", "content": "Visiting Intention Memory Network (VIMN) is proposed to capture the visiting intentions of\nusers at each check-in record by prioritizing relevant check-in records. As shown in Fig. 1b, we\nfeed the timestamp of each check-in record and the time interval between the adjacent records into\nthe Imminent GRU layer. We adopt a dual encoding approach for time representation: 1) Periodic\nencoding for timestamps t as $T(t) = [cos(w_1t), sin(w_1t), ..., cos(w_kt), sin(w_kt)]$, where {$w_k$} are\nfrequencies determined to capture periodicity across various temporal scales. 2) Logarithmic encoding\nfor time intervals $\\Delta t$ represented as $\\Delta T = log(1 + \\Delta t)$, which adjusts the GRU's forget gate based\non the time interval $\\Delta t$. This adjusted factor, denoted as $\\Delta T$, affects the forget gate of the GRU unit\nthrough $G_{forget}(\\Delta T) = \\sigma(W_f\\Delta T + b_f)$. The other components like $G_{update}$ remain unchanged\nfrom the original GRU configuration. The Imminent GRU layer can be depicted as:\n$z_i = \\sigma(W_inT(t_i) + G_{update}(H_{i-1}) \\times G_{forget}(\\Delta T))$, (2)\nwhere $z_i$ represents the output of the Imminent GRU at time step $t_i$, and $H_{i-1}$ denotes the hidden\nstate at time step i \u2013 1. This setup allows for filtering out less relevant, temporally distant data.\nSubsequently, the outputs $[Z_{i-r+1},\u00b7\u00b7\u00b7, Z_{i-1}, Z_i]$ from the most recent $r$ cycles of the Imminent GRU,\nalong with the latest $r$ PPEs, are forwarded to the feed-forward layer [43] (further elaborated in\nAppendix G) to refine the representation of the user's visiting intentions $h_i$."}, {"title": "Human Travel Preference Prompt (HTPP)", "content": "Human Travel Preference Prompt (HTPP). This paragraph introduces a method for extracting\nusers' travel preferences to help the LLM enable a comprehensive understanding of human travel\npreferences and match appropriate prompts from multiple domains. Prompting strategies have\ndemonstrated encouraging outcomes in various applications that aid model predictions [3]. Previous\nworks primarily focus on utilizing a fixed prompt to boost the pre-trained models' performance\nthrough fine-tuning [29] or learnable prompts lacking reality semantic meaning [40, 3]. User behavior\nis diverse and cannot be accurately summarized as a single fixed prompt to describe the user's travel"}, {"title": "5 Experiments", "content": "To evaluate the performance of our proposed model, we carry out extensive experiments on four\nreal-world check-in sequence datasets, targeting three different types of downstream tasks: Next\nLocation Prediction (LP), Trajectory User Link (TUL), and Time Prediction (TP). We use TinyLlama-\n1B [55] as the default backbone unless stated otherwise. The code of Mobility-LLM is released at\nhttps://github.com/LetianGong/Mobility-LLM.\nBaselines: For the LP task, we cover five state-of-the-art LP models to demonstrate the superiority\nof our model: DeepMove [10], LightMove [21], PLSPL [46], HMT-LSTM [24], LSTPM [58].\nFor the TUL task, we select four end-to-end models for comparison: TULER [13], TULVAE [61],\nMoveSim [11], S2TUL [7]. For time prediction methods, we select four SOTA models for comparison:\nIFLTPP [39], THP [63], NSTPP [16], DSTPP [52]. NSTPP and DSTPP can also be applied to the LP\ntask. For sequence representation methods ReMVC [54], VaSCL [53], SML [60], CACSR [15], we\napply them to learn the representation of the check-in sequence and serve different downstream tasks.\nMore details are in Appendix D.\nDatasets: In our experiments, we use four real-world datasets derived from Gowalla 3, WeePlace [27,\n30], Brightkite 4, and FourSquare [48, 49]. To ensure data consistency, we set a maximum historical\ntime limit of 120 days and filter out users with fewer than 10 records and places visited fewer than\n10 times. Appendix C provides statistical information for each processed dataset. We shuffle all the\nsamples and then split the datasets into training, validation, and test sets in a 6:2:2 ratio based on the\nnumber of samples."}, {"title": "Implement Details", "content": "Implement Details: During the training phase, we adopt a partially frozen strategy to fine-tune\nthe pre-trained LLM. We apply different parameter freezing strategies to the 1 - $L_F$ layers and the\n$L_F$ - $L_{F+U}$ layers. In order to enhance the model's performance on trajectory data, we employ the\nLow-Rank Adaptation (LoRA) algorithm [20] to incorporate additional parameters into the LLMs.\nDetails of the partially frozen strategy and other settings can be found in Appendix B. We run each\nset of experiments 5 times and reported their mean values"}, {"title": "5.1 Next Location Prediction", "content": "Setups: We consider the LP task as a multi-classification problem. Given a check-in sequence\n$C_{U_i}$ from a specific user $U_i$, we feed it to our framework $\\mathcal{G}_{LLM}$ to obtain the check-in sequence\nrepresentation $\\mathcal{G}_{LLM}(C_{U_i})$. As shown in Fig. 1e, a multi-class projection head $f_{multi}(\\beta) =$\n$softmax (W\\beta + b))$ is used to predict the next location $\\hat{s}_{n+1}$ with the corresponding output $\\beta$\nof VIMN. We maximize the conditional log-likelihood for a given N observations as follows:\n$\\mathcal{L}_{MLE}(\\theta) = \\sum_{i=1}^{N} log f_{multi} (\\hat{I}_{n+1} | \\beta)$, where N is the number of POIs. We maintain all baseline\nuser embeddings at 256 dimensions and POI embeddings at 128 dimensions. The evaluation metrics\ninclude Acc@k and mean reciprocal rank (MRR). The details of the implementation and metric can\nbe found in Appendix B.2.\nResults: Our brief results are shown in Tab. 1, and consistently surpass all baselines. The comparison\nwith CACSR is particularly noteworthy since it is the latest check-in sequence learning model. We\nnote average performance gains of 17.19% and 7.49% over CACSR and ReMVC, respectively.\nCompared with the SOTA task-specific models, PLSPL and LSTPM realized an average MRR\nimprovement of 9.32% and 19.88%. Relative to the time point process models, e.g., NSTPP and\nDSTPP, our improvements are also pronounced, exceeding 14.86% and 13.22%."}, {"title": "5.2 Trajectory User Link", "content": "Setups: Unlike the LP task, the TUL task requires predicting which user generated a given check-in\nsequence. Therefore, the input information cannot contain any details about the user. We use the"}, {"title": "5.3 Time Prediction", "content": "Setups: For the time prediction task, we follow the method of IFLTPP [39] using an intensity-free\nmethod to model the interaction time as a mixture distribution. We first obtain the mixture weights\n$w$, means $\\mu$ and standard deviations $s$ from the corresponding output $\\beta$ of LLM with linear layer.\nThen we use the TPP projection head built by a mixed distribution function and sample to get the\nprediction time $t_{n+1}$ as follows:\n$\\rho(\\tau | w, \\mu, s) = \\sum_{k=1}^{K} \\frac{w_k}{\\tau s_k \\sqrt{2\\pi}} exp -(\\frac{log \\tau - \\mu_k}{2s_k^2}), t_{n+1} = \\sum_{k=1}^{K} w_k exp (a\\mu_k + b + \\frac{a^2 s_k}{2}),$ (5)\nwhere $k$ represents the number of independent Gaussian distributions in the mixed distribution, $a\ndenotes the mean of the whole set and $b$ denotes the standard deviation of the whole set. We sample\nfrom the mixture model in the parsing solution. The evaluation metrics include root mean square\nerror (RMSE) and mean absolute error (MAE).\nResults: Our brief results are shown in Tab. 3. Due to the diversity of user behaviors and the\nunpredictability of activity timing, even the state-of-the-art (SOTA) models for Temporal Point"}, {"title": "5.4 Few-shot Forecasting", "content": "Setups: LLMs have recently demonstrated remarkable few-shot learning capabilities [28]. This\nsection assesses whether our reprogrammed LLM retains this ability in different tasks. In our\nexperiments, we maintain consistent splits for training, validation, and test sets in both standard\nlearning (where the full training set is used) and few-shot learning. For few-shot scenarios, we\nintentionally limit the training data percentage of sample number (i.e., using first 20%, 5%, 1%\nsamples of the training dataset).\nResults: Our 5% few-shot learning results are in Tab. 4 remarkably excel over all baseline methods,\nand we attribute this to the successful knowledge activation in our reprogrammed LLM. Interestingly,\nour approach on both LP and TUL tasks consistently surpasses other competitive baselines, further\nunderscoring the potential prowess of language models as proficient human behavior analysis ma-\nchines. Concerning recent SOTA models such as NSTPP, DSTPP, S2TUL, ReMVC, and CACSR, our\naverage enhancements surpass 21.4%, 21.7%, 86.6%, 46.2%, and 45.2% w.r.t. average on all the\nmetrics. Even with only 5% of the training dataset, our model achieves results comparable to other\nmodels using 100% of the training dataset. This is particularly significant for privacy-protected and\ntypically smaller Check-in datasets, as our model can effectively understand the distribution patterns\nof human behaviors with minimal data."}, {"title": "5.5 Model Analysis", "content": "Language Model Variants. We compare eight representative backbones with varying capacities\n(A.1-8 in Tab. 5). We find that the TinyLlama (A.1 in Tab. 5) backbone model performed the best\noverall for our task, while its Chat version (A.2) is relatively less suitable for reprogramming. Our\nresults indicate that the scaling law is not strictly retained after the LLM reprogramming. Even"}, {"title": "6 Conclusion", "content": "In conclusion, our work presents Mobility-LLM, a unified framework leveraging large language\nmodels (LLMs) to analyze check-in sequences and understand human mobility behaviors. By\nincorporating the visiting intention memory network (VIMN) and the human travel preference\nprompts (HTPP), our model excels in various tasks. Moreover, our model exhibits robust few-\nshot learning capabilities, outperforming existing methods by an average of 23.6% to 38.3%. Our\nwork paves the way for a more comprehensive and accurate analysis of human mobility, benefiting\nindividuals, businesses, and urban management.\nLimitations The sets of POIs in different datasets (which usually cover different regions) are unique.\nTherefore, our proposed model is trained on one dataset, its learned information about the set of POIs\nis not easily transferable to another dataset. Different sets of POIs have different functionalities and\nusually have a different number of POIs, making many modules (such as embedding and predictor)\ntechnically untransferable in a zero-shot setting. Future work will focus on developing universal user\nand POI embeddings to enhance cross-dataset migration and improve model versatility."}, {"title": "A.1 Mobility Data Mining", "content": "Location-based services have given rise to a new and promising research topic known as mobility\ndata mining, which has led to the emergence of three significant tasks that contribute to enhancing\nthe quality of services: next location prediction (LP), next time prediction (TP), and trajectory user\nlink (TUL). Recent studies have confirmed that deep learning techniques, specifically recurrent\nneural networks (RNNs) and attention mechanisms, are highly effective in capturing sequential and\nperiodic patterns of human mobility. By combining deep learning techniques, researchers have made\nsignificant advancements in capturing both the sequential and periodic patterns of human mobility.\nThe core of these models is the modeling of check-in sequences, which leads to improved accuracy\nin location prediction and trajectory analysis.\nLP aims to anticipate a user's future location based on their historical movement. Several notable\nmodels have emerged as prominent approaches in LP. DeepMove [10] leverages RNNs and attention\nmechanisms to capture the spatial-temporal intentions in users' location data and predict their next\ndestination. STAN [32] introduces a spatial-temporal attention network that incorporates spatial and\ntemporal contexts for accurate prediction. LSTPM [58] focuses on long and short-term patterns in\nuser trajectory using an attention-based LSTM [19] model. SERM [51] utilizes an encoder-decoder\narchitecture with a spatial-temporal residual network to capture user preferences and predict future\nlocations. PLSPL [46] trains two LSTM models for location- and category-based sequences to\ncapture the user's preference. LightMove [21] designs neural ordinary differential equations to\nenhance robustness against sparse or incorrect inputs. HMT-GRN [24] alleviates the data sparsity\nproblem by learning different User-Region matrices of lower sparsities in a multitask setting. Graph-\nFlashback [37] constructs a spatial-temporal knowledge graph to enhance the representation of\nPOIs, having a great advantage when dealing with the historical sequence input of the same length.\nGETNext [50] introduces a user-agnostic global trajectory flow map as a means to leverage the\nabundant collaborative signals.\nTUL is a significant task that focuses on establishing connections between different trajectories,\nfacilitating the analysis of user movement patterns, and uncovering valuable insights about their\nbehavior. Notable models have been specifically developed to address the challenge of predicting\ntrajectory links. TULER [13] takes advantage of advanced algorithms to establish links between\ntrajectories, allowing for a comprehensive understanding of user movement patterns. TULVAE [61]\nuses latent variables to model the variability in trajectories, capturing hierarchical and structural\nsemantics and improving the identification and linking performance. MoveSim [11] simulates\nhuman mobility using a generative adversarial framework that incorporates attention mechanisms\nto capture complex spatial-temporal transitions in human mobility. DeepTUL [33] utilizes deep\nlearning techniques to extract representations from trajectory data and facilitate the prediction of\ntrajectory links. S2TUL [7] utilizes graph convolutional networks and sequential neural networks\nto capture trajectory relationships and intra-trajectory information. GNNTUL [59] employs graph\nneural networks for human mobility and associates the traces with users on social networks.\nTP focuses on estimating the time at which a user is likely to visit their next location. To accomplish\nthis, it is common practice to use intensity functions to represent the rate or density of event\noccurrences, various models have been developed to model the intensity function and make accurate\ntime predictions effectively. Modeling the intensity function using RNNs or attention mechanisms\nis a common approach for predicting the occurrence of events. IFLTPP [39] approximates any\ndistribution of inter-event times using normalizing flows and mixture distributions. RMTPP [9]\nutilizes RNNs to model the intensity function. SAHP [56] combines the Hawkes process with self-"}, {"title": "A.2 Cross-domain Application of LLMs", "content": "We have witnessed the great success of Large Language Models (LLMs) in natural language process-\nsing [36], and some cross-domain (such as computer vision [35], time series, and graph-related tasks.\nIn the field of graph theory, To tackle graph-related tasks, Graph Neural Networks (GNNs) have\nemerged as one of the most popular choices for processing and analyzing graph data. While GNNs\nexcel at capturing structural information, their reliance on semantically constrained embeddings as\nnode features limits their ability to fully express the complexities of the nodes. By incorporating\nLLMS, GNNs can be augmented with stronger node features that effectively capture both structural\nand contextual aspects. TAPE utilizes semantic knowledge that is pertinent to the nodes (e.g., papers)\ngenerated by LLMs to enhance the quality of initial node embeddings in GNNs. Furthermore,\nInstructGLM replaces the predictor in GNNs with LLMs, harnessing the expressive capabilities\nof natural language through techniques like graph flattening and prompt-based instruction design.\nMoleculeSTM aligns GNNs and LLMs within a shared vector space, integrating textual knowledge\ninto graphs (e.g., molecules) to enhance reasoning capabilities.\nIn the field of Computer Vision (CV), ViT is an image classification model based on the Transformer\narchitecture, which has achieved excellent performance on multiple benchmark image classification\ndatasets. CLIP is a large-scale pre-trained model developed by OpenAI that aligns images and text,\nenabling simultaneous learning of representations for both modalities, which allows for interactive\nunderstanding between images and text. DALL-E utilizes a large-scale pre-trained language model\nto transform text into images through a generative approach, showcasing remarkable capabilities in\nimage generation. TEST [40], as a time series (TS) embedding method tailored for Large Language\nModels (LLMs), generates embeddings that capture similarity, instance-wise, feature-wise, and\ntext-prototype alignment for TS tokens. Time-LLM [23] introduces a reprogramming framework.\nGiven the challenge of limited training data, recent studies turn to adopting large language models\n(LLMs) to address cross-domain applications. In the field of time series analysis, LLM4TS [4] is\nthe pioneering method that aligns pre-trained Large Language Models with temporal characteristics,\nintroducing a two-level aggregation method to effectively incorporate multi-scale temporal data"}, {"title": "B.1 Settings", "content": "The Mobility-LLM model was constructed using the PyTorch. The loss function is a cross-entropy\nloss for the LP and TUL tasks and an MAE loss for the TP task. The performance on the validation\nsets determines the hyper-parameters and the best models. All experiments are performed five times,\nand the means and standard deviations are calculated. To make a fair comparison, for all methods, the\nembedding dimension $d$ is 256, while the hidden state $h$ has a size of 256. The learning rate is 0.001.\nThe model is pre-trained for 100 epochs on the training sets with the early-stopping mechanism of 10\npatients. All trials have been conducted on Intel Xeon E5-2620 CPUs and NVIDIA RTX A40 GPUs."}, {"title": "B.2 Evaluation Metrics", "content": "Four metrics are used for evaluating the models: mean absolute error (MAE), root mean squared error\n(RMSE), Accuracy at K (ACC@K), and Mean Reciprocal Rank (MRR). MAE and RMSE quantify\nabsolute errors. ACC@Kmeasures the accuracy of the predictions within the top K ranks. MRR\ncalculates the average of the reciprocal ranks, where a higher rank indicates a better performance.\n$MAE = \\frac{1}{m}\\sum_{i=1}^{m} |\\hat{Y_i} - Y_i|, RMSE = \\sqrt{\\frac{1}{m}\\sum_{i=1}^{m} (\\hat{Y_i} - Y_i)^2},$ (6)\nwhere $m$ is the number of predicted values, $\\hat{Y_i}$ is the predicted value, $Y_i$ is the target value.\n$ACC@K = \\frac{1}{m} \\sum_{i=1}^{K} I(\\hat{Y_i} == Y_i), MRR = \\frac{1}{m} \\sum_{i=1}^{m} \\frac{1}{rank_i},$ (7)\nwhere $m$ is the total number of queries, $rank_i$ is the rank position of the first relevant result for the\ni - th query."}, {"title": "C.1 Datasets Introduction", "content": "Gowalla was a location-based social networking service where users shared their locations by\nchecking-in. The dataset includes 6,442,890 check-ins made by 196,591 users from February 2009 to"}, {"title": "C.2 The Statics of Processed Datasets", "content": "We can see that the dataset we selected includes the Gowalla dataset, which has a large number of\nsamples, users, and Points of Interest (POI), as well as the Foursquare dataset, which has a small\nnumber of users but a large number of POIs. Additionally, we have the Brightkite and WeePlace\ndatasets, which have a small number of both users and POIs. These datasets cover different scenarios\nand can fully validate the model's comprehensive capabilities."}, {"title": "C.3 Partially Frozen Attention (PFA) LLM", "content": "The frozen pre-trained transformer (FPT) has demonstrated effectiveness in a variety of downstream\ntasks across non-language modalities [31]. We adopt a partially frozen attention (PFA) LLM,\nspecifically designed to enhance prediction accuracy in check-in sequences prediction.\nThe difference between the FPT and our PFA primarily lies in the configuration of frozen attention\nlayers. In the FPT framework, both the multi-head attention and feed-forward layers are frozen during"}, {"title": "D Overview of Baselines", "content": "DeepMove [10] is an attentional recurrent network designed for predicting human mobility from\nlengthy and sparse trajectories. It introduces a multi-modal embedding recurrent neural network that\ncaptures complex sequential transitions by embedding multiple factors influencing human mobility.\nDeepMove further incorporates a historical attention model with dual mechanisms to effectively\ncapture multi-level periodicity. This historical attention model enhances the recurrent neural network's\nability to predict mobility patterns by leveraging the inherent periodic nature of human movement.\nLightMove [21] is a lightweight and accurate deep learning-based method developed for predicting\nthe next locations of taxicabs in order to enhance targeted advertising on taxicab rooftop devices.\nThe paper focuses on Motov, a leading company in South Korea's taxicab rooftop advertising market,\nand aims to leverage demographic information of locations to improve the preparation of targeted\nadvertising campaigns.\nPLSPL [46] addresses the task of recommending the next Point of Interest (POI) for users based\non their historical check-in data. The main objective is to capture both the users' general taste and\ntheir recent sequential behaviors, as these factors are crucial in making accurate recommendations.\nHowever, existing methods often assume the same dependencies for all users, disregarding the fact\nthat different users may have varying preferences and dependencies on these two aspects.\nHMT-LSTM [24] addresses the challenging task of predicting the next Point-of-Interest (POI) that a\nuser is likely to visit in personalized recommender systems. One of the main difficulties in this task is\nthe large search space of possible POIs in the region, which leads to data sparsity issues in existing\nworks and hampers performance.\nLSTPM [58] proposes a method for POI recommendation by modeling long-term and short-term\npreferences. It uses a nonlocal network for capturing long-term dependencies and a geo-dilated RNN\nfor considering geographical relationships. This approach improves the reliability of recommendation\nresults compared to existing methods.\nTULER [13] addresses the task of understanding human trajectory patterns in Location-Based Social\nNetworks (LBSNs) applications. This task is crucial for various applications like personalized\nrecommendation and preference-based route planning. Existing methods often classify trajectories\nor their segments into predefined categories based on spatial-temporal values and activities, such as\nwalking or jogging. However, the paper focuses on a novel problem called Trajectory-User Linking\n(TUL), which aims to identify and link trajectories to the users who generated them in LBSNs.\nTULVAE [61] addresses the important task of Trajectory-User Linking (TUL) in Geo-tagged social\nmedia (GTSM) applications. It utilizes a neural generative architecture with stochastic latent variables\nthat span hidden states in an RNN. By incorporating variational autoencoder techniques, TULVAE\ncan capture the hierarchical and structural semantics of trajectories using high-dimensional latent\nvariables. Moreover, TULVAE addresses the data sparsity challenge by leveraging large-scale\nunlabeled data, providing more comprehensive representations of user mobility patterns."}, {"title": "E.2 Variants of Ablation", "content": "To further evaluate the effects of different components in Mobility-LLM, we conduct ablation\nexperiments and analyze experimental results on all datasets. We compare these four variants on\nthree downstream tasks.\n\u2022 w/o HTPP: We remove the HTPP Module. The rest of the settings are the same as Mobility-LLM.\nWe use this setting to evaluate the function of the HTPP Module.\n\u2022 w/o VIMN: We use full-connection to replace the VIMN module. The rest of the settings are the\nsame as Mobility-LLM. We use this setting to evaluate the function of the VIMN module.\n\u2022 w/o PPEL: We use a learnable parameter to represent a POI. We use this setting to evaluate the\nfunction of the PPEL."}, {"title": "E.3 Rusults of Variants on Different Datasets", "content": "E.3.1 Rusults of Variants on WeePlace\nLanguage Model Variants. We compared eight representative backbones on WeePlace dataset\nwith varying capacities in Tab. 8. We found that the TinyLlama backbone model performed the best\noverall for our task. In"}]}