{"title": "An Empirical Study of Validating Synthetic Data for Formula Generation", "authors": ["Usneek Singh", "Aditya Kanade", "Anirudh Khatry", "Jos\u00e9 Cambronero", "Sumit Gulwani", "Vu Le", "Mukul Singh", "Gust Verbruggen"], "abstract": "Large language models (LLMs) can be lever- aged to help with writing formulas in spread- sheets, but resources on these formulas are scarce, impacting both the base performance of pre-trained models and limiting the ability to fine-tune them. Given a corpus of formu- las, we can use a(nother) model to generate synthetic natural language utterances for fine- tuning. However, it is important to validate whether the NL generated by the LLM is in- deed accurate to be beneficial for fine-tuning. In this paper, we provide empirical results on the impact of validating these synthetic training examples with surrogate objectives that evalu- ate the accuracy of the synthetic annotations. We demonstrate that validation improves per- formance over raw data across four models (2 open and 2 closed weight). Interestingly, we show that although validation tends to prune more challenging examples, it increases the complexity of problems that models can solve after being fine-tuned on validated data.", "sections": [{"title": "1 Introduction", "content": "Derived-column formulas in spreadsheets generate a new column by transforming existing columns in a table, and they have been shown challenging to write (Gulwani et al., 2012). To aid users in writing such formulas, we can ask for a description in natural language (Zhao et al., 2024). Unfortu- nately, since such formulas are sparse, therefore pre-trained language models (especially smaller) struggle in generating them without fine-tuning (0.03 for phi-2 in pass@10). To construct a dataset for fine-tuning, public spreadsheet workbooks can be used but they con- tain only tables and formulas, whereas a fine-tuning dataset also requires paired natural language (NL) descriptions corresponding to each (Table, For- mula). Traditionally datasets for NL-to-code tasks have been manually annotated (Zhou et al., 2024; Austin et al., 2021). This is a time-consuming and expensive process. Leveraging LLMs, known for their text generation capabilities, is a viable alter- native (Tan et al., 2024) given that the synthetic NL generated by LLMs is accurate, as recent studies have shown that quality is more important than quantity (Zhou et al., 2024; Li et al., 2023). In this paper, we leverage LLMs to predict the ac- curacy of synthetic NL using 3 surrogate objectives, and show empirical results of fine-tuning models on subsets of synthetic data that are accepted by these objectives. Fine-tuning models on validated subsets shows better performance in predicting for- mulas compared to using raw data. For example, GPT-4 fine-tuned on data validated by alternative code generation objective saw up to a 25% im- provement in evaluation scores along with a 23% reduction in training time. Additionally, we ob- serve that the models fine-tuned on validated data perform better on more complex problems. Fur- ther, we release the synthetic dataset to seed future research in this area.\nOur key contributions are as follows.\n\u2022 We define three surrogate objectives (output prediction, alternative code generation, and classification) to predict accuracy of synthetic natural language in the NL-to-Formula task.\n\u2022 We empirically analyze the effect of validating synthetic data using these objectives on fine- tuning performance of different models."}, {"title": "2 Related work", "content": "Formula generation FlashFill (Gulwani, 2011; Gulwani et al., 2012) generates derived-column formulas by example, as users struggle with this task. SpreadsheetCoder (Chen et al., 2021b) sug- gests formulas from surrounding context in spread- sheets. FLAME (Joshi et al., 2024) is a small lan- guage model that understands formulas for tasks like repair and retrieval, but does not handle natu- ral language. The NL-to-Formula (NL2F) task is"}, {"title": "LLMs for synthetic data", "content": "Tan et al. (2024) dis- cusses the applications of LLMs in data annotation for classification tasks. Goel et al. (2023) demon- strates the use of LLMs in the medical domain, where they assist in labeling data with expert veri- fication. Wang et al. (2024) and Kim et al. (2024) explore human-LLM collaborative approaches for annotation and verification. There has been no com- parison of NL validation techniques on synthetic NL for NL2F."}, {"title": "Data quality for LLM fine-tuning", "content": "Chen and Mueller (2024) proposed an approach for auto- mated filtering and verification of datasets to en- sure high quality for LLM fine-tuning, leveraging the BSDetector (Chen and Mueller, 2023) to ob- tain confidence scores from LLM outputs. These techniques require existing ground truth labels (ut- terances) which are not available in our case. Zhou et al. (2024) and Li et al. (2023) manually curate data to demonstrate that instruction tuning with a small (< 1000) set of high-quality examples yields competitive results. While their work focuses on selecting examples based on alignment (already as- suming correctness), our work evaluates technique- based selection on accuracy of NL instructions."}, {"title": "3 Validating synthetic data", "content": "Let $T = [C_i]^n_{i=1}$ be a table with n columns uniquely identified by $h_i$. A derived-column formula $F$ is a formula where each leaf node is either a constant value or a column identifier $h_i$. Let $U$ be an ut- terance in natural language that describes how to derive a column from $T$. An derived-column task is specified by $(U,T, F)$. Given $U$ and $T$ the goal is to find a formula $F'$ such that $F'(T) = F(T)$.\nTo fine-tune a model, we therefore need exam- ples of the form $(U,T, F)$. $T$ and $F$ can be mined from large spreadsheet corpora (Singh et al., 2023; Joshi et al., 2024) and we can use an LLM to gen- erate an utterance $\\hat{U} = LLM(T, F)$.\nA validator $V(\\hat{U}, T, F) \\rightarrow B$ is a function that predicts whether $U$ accurately describes the for- mula $F$ operating on table $T$. These validators can be defined in any way even using human anno- tators. To reduce manual effort, we define three validators using an LLM."}, {"title": "Output prediction (Vo)", "content": "This validator asks the LLM to directly predict the output values $F(T)$ from $(\\hat{U}, T)$ and uses an element-wise row com- parison to evaluate correctness. For numbers, we allow an absolute difference of 0.05. For strings, we use a longest common sub-sequence ratio of 0.8 as passing criterion. This approach leverages nat- ural language to emulate the computation directly. It is inspired from the alternate task of output pre- diction discussed in Khatry et al. (2023)."}, {"title": "Alternate code generation (Vp)", "content": "This validator asks the LLM to predict a program $P$ in another language (we use Python) from $(\\hat{U}, T)$ and com- pares $P(T)$ (execution of P on T) with $F(T)$ using element-wise comparison with the same relaxations for strings and numbers. This leverages the abil- ities of LLMs to generate popular programming languages (Ni et al., 2023)."}, {"title": "Classification (Vc)", "content": "This validator directly asks the model to classify whether $U$ accurately de- scribes $F$ over $T$. It is based on the self-reflection certainty objective from BSDetector (Chen and Mueller, 2023)."}, {"title": "4 Experimental setup", "content": "We describe training data and models, and the test- ing benchmark.\nTraining data We mine $(T, F)$ pairs that satisfy our derived-column definition from publicly avail- able Excel workbooks (Singh et al., 2023). We"}, {"title": "5 Results and Discussion", "content": "We perform experiments to empirically explore the following research questions.\nRQ1 How do different validators compare?\nRQ2 What is the impact of validating data on fine- tuning performance?\nRQ3 What are the differences in cases solved by models trained on validated NL and raw dataset?"}, {"title": "5.1 RQ1: Comparing validators", "content": "We apply our three validation approaches to our initial set of 7833 points. This produces the data subsets described in Table 1. We shows properties of the formulas accepted by each validator. Since $V_O$ is bottle-necked on numerical operations, it succeeds for fewer unique functions and operators. Similarly, $V_P$ struggles with more functions than $V_C$ as there might not be an easy Python equivalent.\nFigure 2 shows overlap in examples accepted by different validators. Each validator uniquely accepts at least some examples. 1403 (18%) exam- ples does not pass any validator."}, {"title": "5.1.1 Attention weights comparison", "content": "We understand the alignment of NL with formula using average attention weights of the formula to- kens originating from the NL tokens in the last layer of two open-source models (Phi-2, Mistral). Let $a_{ij}$ denote the attention weight from the $i$-th NL token to the $j$-th formula token in the last layer. The average attention weight $\\bar{a_j}$ for the $j$-th for- mula token from all NL token is given by:\n$\\bar{a_j} = \\frac{1}{N_{NL}} \\sum_{i=1}^{N_{NL}} a_{ij}$\n(1)\nThe overall average attention weight for the en- tire formula $a_{total}$ is given by:\n$a_{total} = \\frac{1}{N_{Formula}} \\sum_{j=1}^{N_{Formula}} \\bar{a_j}$\n(2)\nBy calculating $a_{total}$, we gain a measure of the interaction between NL tokens and formula tokens, highlighting the overall influence of natural lan- guage on the generation of formula tokens within the base model (Yuksekgonul et al., 2023). We compare $a_{total}$ across different data subsets. We observe that the average $a_{total}$ is highest for subset with NL validated across all techniques, followed by subsets with validated NL from each technique,, then the raw data, and finally, with data subsets where NL is rejected by each technique (refer Fig- ure 3). This analysis suggests that validated NL"}, {"title": "5.2 RQ2: Effect on fine-tuning performance", "content": "We compare the impact of validated data versus raw (unvalidated) data, as well as the impact of val- idated data versus rejected cases by each validator, on the downstream performance of the NL2F task.\nVersus raw Table 2 shows base model (few-shot) and fine-tuning performance on different subsets of data. For the smaller models, Phi-2 and Mistral, the performance increase with fine-tuning is more sig- nificant. Except for Mistral in pass@10 and GPT- 35-turbo in pass@3, a smaller, validated dataset yields better performance than raw data. $V_P$ yields the best performance on average with nearly half the size of raw data. GPT-4 improves only when fine-tuned on validated data. Surprisingly,"}, {"title": "Versus invalidated", "content": "Table 3 compares the perfor- mance of fine-tuning on the accepted ($\\subset$) and rejected ($\\not\\subset$) examples for each valida- tor. We sub-sample the accepted sets to 2266\u2013the number of examples in the smallest set ($V_O$). Re- sults of pairs ($C \\subset V$, $\\not\\subset V$) are marked in green if ($C \\subset V > \\not\\subset V$), blue if ($C \\subset V = \\not\\subset V$) and red if ($C \\subset V < \\not\\subset V$). We observe that, despite the smaller size of the validated data subset (subsampled), it outperforms its larger invalidated (rejected) coun- terpart in most (28/36) comparisons. The only de- crease in performance happens for $V_O$ on GPT-4, likely due to the many functions (51) that were eliminated from the training data."}, {"title": "5.3 RQ3: Analysing solved cases", "content": "Figure 4 shows properties of the solved cases (where pass@10 = 1) after fine-tuning different models on raw data and validated subsets. We see that fine-tuning on datasets with fewer unique func- tions still enables all models (except for Mistral) to use more functions.\nThe average function call count increases for val- idated subsets compared to the raw data, indicating more complex formulas are solved by models fine- tuned on validated data. For GPT-4 and GPT-35- turbo, average operator count also increases with fine-tuning on validated data."}, {"title": "6 Conclusion", "content": "We empirically evaluate the effect of automated validation of synthetic data using LLMs on the fine-tuning performance of derived-column NL-to- formula. We validate synthetic NL annotations with three surrogate tasks (classification, code gen- eration in Python, and output prediction) and fine- tune different models on the examples accepted by each of these methods. In general, fine-tuning on smaller, validated datasets improves performance. Despite validation resulting in datasets with sim- pler formulas, that does not cause the fine-tuned models to only solve simpler problems. Further, we release our dataset to seed research in this area."}, {"title": "7 Limitations", "content": "Although we have focused on validating the cor- rectness of natural language instructions, we have not addressed techniques for correcting them. Ex- ploring methods for correcting instructions could be beneficial, as it would prevent the loss of data points. While having a smaller set of high-quality data can be advantageous for efficient training, achieving the best results may require maintaining a larger dataset by correcting invalid instructions.\nIn our study, the distribution of training data for fine-tuning is different than the testing data, which might not fully reflect the potential of fine- tuning. Additionally, our research has concentrated on formulas that expect a single, well-structured (formatted) input table. We aim to extend our work to include formulas that involve multiple tables and unstructured input. Furthermore, we have explored the potential of our technique in one language (En- glish). We believe it will be valuable to investigate multilingual systems for validation setups."}, {"title": "8 Appendix", "content": "8.1 Training Data Characteristics\nIn this section, we summarise important formula properties for the training data extracted from ex- cel workbooks (see Table 5). From the original corpus, we remove any formulas that have depre- cated functions to produce a set of 10,389 (table, formula) pairs. We then remove any pairs where the formula results in a missing/empty value for all output rows or uses multiple tables. After the pro- cess of filtering, our final dataset consists of 7,833 (table, formula) pairs. This dataset has formulas which use 122 distinct built-in functions. The most popular functions match those typically employed by Excel spreadsheet users: IF, SUM, IFERROR, CONCATENATE, AND. The other properties are sum- marised in Table 5). The function call count refers to the frequency of Excel function calls within a formula. The depth of formulas denotes the ex- tent of nested function calls within them. Operator count is the number of arithmetic operators (+, -, *, /) in a formula."}, {"title": "8.2 Model hyper-parameters used while Fine-tuning", "content": "Phi-2 For the Phi-2 model, fine-tuning was per- formed for 10 epochs with a batch size of 8. The learning rate was set to le-6, and the Adam op- timizer was used along with a cross-entropy loss function.\nMistral The Mistral model was fine-tuned for 15 epochs using the LoRA technique (Hu et al., 2021). The specific parameters for LoRA included a LoRA rank (Lora_r) of 64, a LoRA alpha (Lora_alpha) of 16, and a LoRA dropout (Lora_dropout) of 0.1. The target modules for LoRA adaptation were \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\", and \"lm_head\". No bias configuration was used, and the task type was Causal Language Modeling (CAUSAL_LM). The learning rate for this model was set to 2e-4, and the batch size was 8. Optimization was carried out using the PagedAdamW 32-bit optimizer."}]}