{"title": "An Empirical Study of Validating Synthetic Data for Formula Generation", "authors": ["Usneek Singh", "Aditya Kanade", "Anirudh Khatry", "Jos\u00e9 Cambronero", "Sumit Gulwani", "Vu Le", "Mukul Singh", "Gust Verbruggen"], "abstract": "Large language models (LLMs) can be leveraged to help with writing formulas in spreadsheets, but resources on these formulas are scarce, impacting both the base performance of pre-trained models and limiting the ability to fine-tune them. Given a corpus of formulas, we can use a(nother) model to generate synthetic natural language utterances for fine-tuning. However, it is important to validate whether the NL generated by the LLM is indeed accurate to be beneficial for fine-tuning. In this paper, we provide empirical results on the impact of validating these synthetic training examples with surrogate objectives that evaluate the accuracy of the synthetic annotations. We demonstrate that validation improves performance over raw data across four models (2 open and 2 closed weight). Interestingly, we show that although validation tends to prune more challenging examples, it increases the complexity of problems that models can solve after being fine-tuned on validated data.", "sections": [{"title": "Introduction", "content": "Derived-column formulas in spreadsheets generate a new column by transforming existing columns in a table, and they have been shown challenging to write (Gulwani et al., 2012). To aid users in writing such formulas, we can ask for a description in natural language (Zhao et al., 2024). Unfortunately, since such formulas are sparse, therefore pre-trained language models (especially smaller) struggle in generating them without fine-tuning (0.03 for phi-2 in pass@10). To construct a dataset for fine-tuning, public spreadsheet workbooks can be used but they contain only tables and formulas, whereas a fine-tuning dataset also requires paired natural language (NL) descriptions corresponding to each (Table, Formula). Traditionally datasets for NL-to-code tasks have been manually annotated (Zhou et al., 2024; Austin et al., 2021). This is a time-consuming and expensive process. Leveraging LLMs, known for their text generation capabilities, is a viable alternative (Tan et al., 2024) given that the synthetic NL generated by LLMs is accurate, as recent studies have shown that quality is more important than quantity (Zhou et al., 2024; Li et al., 2023). In this paper, we leverage LLMs to predict the accuracy of synthetic NL using 3 surrogate objectives, and show empirical results of fine-tuning models on subsets of synthetic data that are accepted by these objectives. Fine-tuning models on validated subsets shows better performance in predicting formulas compared to using raw data. For example, GPT-4 fine-tuned on data validated by alternative code generation objective saw up to a 25% improvement in evaluation scores along with a 23% reduction in training time. Additionally, we observe that the models fine-tuned on validated data perform better on more complex problems. Further, we release the synthetic dataset to seed future research in this area. Our key contributions are as follows.\n\u2022 We define three surrogate objectives (output prediction, alternative code generation, and classification) to predict accuracy of synthetic natural language in the NL-to-Formula task.\n\u2022 We empirically analyze the effect of validating synthetic data using these objectives on fine-tuning performance of different models."}, {"title": "Related work", "content": "Formula generation FlashFill (Gulwani, 2011; Gulwani et al., 2012) generates derived-column formulas by example, as users struggle with this task. SpreadsheetCoder (Chen et al., 2021b) suggests formulas from surrounding context in spreadsheets. FLAME (Joshi et al., 2024) is a small language model that understands formulas for tasks like repair and retrieval, but does not handle natural language. The NL-to-Formula (NL2F) task is introduced with a dataset obtained by converting the TEXT2SQL dataset to spreadsheet formulas (Zhao et al., 2024). Unlike (Zhao et al., 2024), our work centres on empirically evaluating different NL validation strategies.\nLLMs for synthetic data Tan et al. (2024) discusses the applications of LLMs in data annotation for classification tasks. Goel et al. (2023) demonstrates the use of LLMs in the medical domain, where they assist in labeling data with expert verification. Wang et al. (2024) and Kim et al. (2024) explore human-LLM collaborative approaches for annotation and verification. There has been no comparison of NL validation techniques on synthetic NL for NL2F.\nData quality for LLM fine-tuning Chen and Mueller (2024) proposed an approach for automated filtering and verification of datasets to ensure high quality for LLM fine-tuning, leveraging the BSDetector (Chen and Mueller, 2023) to obtain confidence scores from LLM outputs. These techniques require existing ground truth labels (utterances) which are not available in our case. Zhou et al. (2024) and Li et al. (2023) manually curate data to demonstrate that instruction tuning with a small (< 1000) set of high-quality examples yields competitive results. While their work focuses on selecting examples based on alignment (already assuming correctness), our work evaluates technique-based selection on accuracy of NL instructions."}, {"title": "Validating synthetic data", "content": "Let $T = [C_1^n]$ be a table with n columns uniquely identified by $h_i$. A derived-column formula $F$ is a formula where each leaf node is either a constant value or a column identifier $h_i$. Let $U$ be an utterance in natural language that describes how to derive a column from $T$. An derived-column task is specified by $(U,T, F)$. Given $U$ and $T$ the goal is to find a formula $F'$ such that $F'(T) = F(T)$.\nTo fine-tune a model, we therefore need examples of the form $(U,T, F)$. $T$ and $F$ can be mined from large spreadsheet corpora (Singh et al., 2023; Joshi et al., 2024) and we can use an LLM to generate an utterance $\\hat{U} = \\text{LLM}(T, F)$.\nA validator $V (\\hat{U}, T, F) \\rightarrow B$ is a function that predicts whether $\\hat{U}$ accurately describes the formula $F$ operating on table $T$. These validators can be defined in any way even using human annotators. To reduce manual effort, we define three validators using an LLM. An overview of these three validators is shown in Figure 1.\nOutput prediction ($V_o$) This validator asks the LLM to directly predict the output values $F(T)$ from $(\\hat{U}, T)$ and uses an element-wise row comparison to evaluate correctness. For numbers, we allow an absolute difference of 0.05. For strings, we use a longest common sub-sequence ratio of 0.8 as passing criterion. This approach leverages natural language to emulate the computation directly. It is inspired from the alternate task of output prediction discussed in Khatry et al. (2023)\nAlternate code generation ($V_p$) This validator asks the LLM to predict a program $P$ in another language (we use Python) from $(\\hat{U}, T)$ and compares $P(T)$ (execution of $P$ on $T$) with $F(T)$ using element-wise comparison with the same relaxations for strings and numbers. This leverages the abilities of LLMs to generate popular programming languages (Ni et al., 2023).\nClassification ($V_c$) This validator directly asks the model to classify whether $\\hat{U}$ accurately describes $F$ over $T$. It is based on the self-reflection certainty objective from BSDetector (Chen and Mueller, 2023)."}, {"title": "Experimental setup", "content": "We describe training data and models, and the testing benchmark.\nTraining data We mine $(T, F)$ pairs that satisfy our derived-column definition from publicly available Excel workbooks (Singh et al., 2023). We create a training set and validation set of size 7833 and 422 respectively. Each (T, F) pair is annotated with an utterance U using GPT-4.\nModels We use two open (Phi-2 and Mistral-7b-instruct) and two closed-weight (GPT-35-turbo and GPT-4) models. Phi-2 (8 \u00d7 V100) and Mistral (1 \u00d7 A100) were fine-tuned for 10 and 15 epochs respectively. We selected the best checkpoint using validation loss. GPT-35-turbo (16 \u00d7 A100) and GPT-4 (24 \u00d7 A100) were fine-tuned using the Azure API. Mistral, GPT-35-turbo, GPT-4 were fine-tuned using LoRA (Hu et al., 2021).\nTesting data The SOFSET dataset (Barke et al., 2024) consists of 201 spreadsheet formula tasks from StackOverflow. Of these, we filter the 139 tasks that satisfy our derived-column definition.\nMetric We use the pass@k metric (Chen et al., 2021a) based on execution match of formula, were k represents the number of predictions considered out of the total number of predictions provided. In our evaluation system, we generate n = 10 predictions at temperature 0.6 and consider pass@1, pass@3 and pass@10."}, {"title": "Results and Discussion", "content": "We perform experiments to empirically explore the following research questions.\nRQ1 How do different validators compare?\nRQ2 What is the impact of validating data on fine-tuning performance?\nRQ3 What are the differences in cases solved by models trained on validated NL and raw dataset?"}, {"title": "RQ1: Comparing validators", "content": "We apply our three validation approaches to our initial set of 7833 points. This produces the data subsets described in Table 1. We shows properties of the formulas accepted by each validator. Since $V_o$ is bottle-necked on numerical operations, it succeeds for fewer unique functions and operators. Similarly, $V_p$ struggles with more functions than $V_c$ as there might not be an easy Python equivalent.\nFigure 2 shows overlap in examples accepted by different validators. Each validator uniquely accepts at least some examples. 1403 (18%) examples does not pass any validator."}, {"title": "Attention weights comparison", "content": "We understand the alignment of NL with formula using average attention weights of the formula tokens originating from the NL tokens in the last layer of two open-source models (Phi-2, Mistral). Let $a_{ij}$ denote the attention weight from the $i$-th NL token to the $j$-th formula token in the last layer. The average attention weight $\\bar{a_j}$ for the $j$-th formula token from all NL token is given by:\n$\\bar{a_j} = \\frac{1}{N_{NL}} \\sum_{i=1}^{N_{NL}} a_{ij}$   (1)\nThe overall average attention weight for the entire formula $\\alpha_{total}$ is given by:\n$\\alpha_{total} = \\frac{1}{N_{Formula}} \\sum_{j=1}^{N_{Formula}} \\bar{a_j}$  (2)\nBy calculating $\\alpha_{total}$, we gain a measure of the interaction between NL tokens and formula tokens, highlighting the overall influence of natural language on the generation of formula tokens within the base model (Yuksekgonul et al., 2023). We compare $\\alpha_{total}$ across different data subsets. We observe that the average $\\alpha_{total}$ is highest for subset with NL validated across all techniques, followed by subsets with validated NL from each technique,, then the raw data, and finally, with data subsets where NL is rejected by each technique (refer Figure 3). This analysis suggests that validated NL provides a more relevant context for generating formula tokens."}, {"title": "RQ2: Effect on fine-tuning performance", "content": "We compare the impact of validated data versus raw (unvalidated) data, as well as the impact of validated data versus rejected cases by each validator, on the downstream performance of the NL2F task.\nVersus raw Table 2 shows base model (few-shot) and fine-tuning performance on different subsets of data. For the smaller models, Phi-2 and Mistral, the performance increase with fine-tuning is more significant. Except for Mistral in pass@10 and GPT-35-turbo in pass@3, a smaller, validated dataset yields better performance than raw data. $V_p$ yields the best performance on average with nearly half the size of raw data. GPT-4 improves only when fine-tuned on validated data. Surprisingly, GPT-35-turbo without fine-tuning outperforms the fine-tuned version, likely due to differences in data distribution between training and testing benchmarks. Besides performance, fine-tuning with validated data also reduces training time significantly, as shown in Table 4.\nVersus invalidated Table 3 compares the performance of fine-tuning on the accepted (\u2282 subsampled) and rejected (\u00ac) examples for each validator. We sub-sample the accepted sets to 2266\u2013the number of examples in the smallest set ($V_o$). Results of pairs (\u2282 V, \u00acV) are marked in green if (\u2282 V > \u00acV), blue if (\u2282 V = \u00acV) and red if (\u2282 V < \u00acV). We observe that, despite the smaller size of the validated data subset (subsampled), it outperforms its larger invalidated (rejected) counterpart in most (28/36) comparisons. The only decrease in performance happens for $V_o$ on GPT-4, likely due to the many functions (51) that were eliminated from the training data."}, {"title": "RQ3: Analysing solved cases", "content": "Figure 4 shows properties of the solved cases (where pass@10 = 1) after fine-tuning different models on raw data and validated subsets. We see that fine-tuning on datasets with fewer unique functions still enables all models (except for Mistral) to use more functions.\nThe average function call count increases for validated subsets compared to the raw data, indicating more complex formulas are solved by models fine-tuned on validated data. For GPT-4 and GPT-35-turbo, average operator count also increases with fine-tuning on validated data."}, {"title": "Conclusion", "content": "We empirically evaluate the effect of automated validation of synthetic data using LLMs on the fine-tuning performance of derived-column NL-to-formula. We validate synthetic NL annotations with three surrogate tasks (classification, code generation in Python, and output prediction) and fine-tune different models on the examples accepted by each of these methods. In general, fine-tuning on smaller, validated datasets improves performance. Despite validation resulting in datasets with simpler formulas, that does not cause the fine-tuned models to only solve simpler problems. Further, we release our dataset to seed research in this area."}, {"title": "Limitations", "content": "Although we have focused on validating the correctness of natural language instructions, we have not addressed techniques for correcting them. Exploring methods for correcting instructions could be beneficial, as it would prevent the loss of data points. While having a smaller set of high-quality data can be advantageous for efficient training, achieving the best results may require maintaining a larger dataset by correcting invalid instructions.\nIn our study, the distribution of training data for fine-tuning is different than the testing data, which might not fully reflect the potential of fine-tuning. Additionally, our research has concentrated on formulas that expect a single, well-structured (formatted) input table. We aim to extend our work to include formulas that involve multiple tables and unstructured input. Furthermore, we have explored the potential of our technique in one language (English). We believe it will be valuable to investigate multilingual systems for validation setups."}, {"title": "Appendix", "content": "Training Data Characteristics\nIn this section, we summarise important formula properties for the training data extracted from excel workbooks (see Table 5). From the original corpus, we remove any formulas that have deprecated functions to produce a set of 10,389 (table, formula) pairs. We then remove any pairs where the formula results in a missing/empty value for all output rows or uses multiple tables. After the process of filtering, our final dataset consists of 7,833 (table, formula) pairs. This dataset has formulas which use 122 distinct built-in functions. The most popular functions match those typically employed by Excel spreadsheet users: IF, SUM, IFERROR, CONCATENATE, AND. The other properties are summarised in Table 5). The function call count refers to the frequency of Excel function calls within a formula. The depth of formulas denotes the extent of nested function calls within them. Operator count is the number of arithmetic operators (+, -, *, /) in a formula.\nModel hyper-parameters used while Fine-tuning\nPhi-2 For the Phi-2 model, fine-tuning was performed for 10 epochs with a batch size of 8. The learning rate was set to le-6, and the Adam optimizer was used along with a cross-entropy loss function.\nMistral The Mistral model was fine-tuned for 15 epochs using the LoRA technique (Hu et al., 2021). The specific parameters for LoRA included a LoRA rank (Lora_r) of 64, a LoRA alpha (Lora_alpha) of 16, and a LoRA dropout (Lora_dropout) of 0.1. The target modules for LoRA adaptation were \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\", and \"lm_head\". No bias configuration was used, and the task type was Causal Language Modeling (CAUSAL_LM). The learning rate for this model was set to 2e-4, and the batch size was 8. Optimization was carried out using the PagedAdamW 32-bit optimizer."}]}