{"title": "ExPO: Explainable Phonetic Trait-Oriented Network for Speaker Verification", "authors": ["Yi Ma", "Shuai Wang", "Tianchi Liu", "Haizhou Li"], "abstract": "In speaker verification, we use computational method to verify if an utterance matches the identity of an enrolled speaker. This task is similar to the manual task of forensic voice comparison, where linguistic analysis is combined with auditory measurements to compare and evaluate voice samples. Despite much success, we have yet to develop a speaker verification system that offers explainable results comparable to those from manual forensic voice comparison. A novel approach, Explainable Phonetic Trait-Oriented (EXPO) network, is proposed in this paper to introduce the speaker's phonetic trait which describes the speaker's characteristics at the phonetic level, resembling what forensic comparison does. ExPO not only generates utterance-level speaker embeddings but also allows for fine-grained analysis and visualization of phonetic traits, offering an explainable speaker verification process. Furthermore, we investigate phonetic traits from within-speaker and between-speaker variation perspectives to determine which trait is most effective for speaker verification, marking an important step towards explainable speaker verification. Our code is available at https://github.com/mmmmayi/ExPO.", "sections": [{"title": "I. INTRODUCTION", "content": "The speaker verification task involves determining computationally whether a test utterance matches the voice identity of a target speaker [1]\u2013[6]. This is similar to the manual task of forensic voice comparison, where we first extract features from the speech of test utterances as well as the target speaker, then make decision by comparing the features [7], [8], as illustrated in Fig. 1 (a). The feature extraction techniques have been intensively studied. The linguistic-auditory method is one of them [9]. This method breaks down an utterance into fine-grained constituent units, such as consonants, vowels, and intonation. The units are then compared by some analytical techniques from the perspective of phonetics, phonology, acoustics, and sociolinguistics.\nSuch two-step approach offers two benefits. First, it provides a fact-based reasoning process, that is explainable and trustworthy. Second, it only pays attention to the most prominent discriminative features, which may lead to a higher accuracy [10], [11]. As shown in Fig. 1 (a), one evidence is the"}, {"title": "II. SPEAKER MODEL WITH PHONETIC TRAIT", "content": "To explain how we derive the similarity score, we introduce phonetic trait layers into speaker models. While the phonetic trait layers are designed for any speaker models. Without loss of generality, we take the commonly used ECAPA-TDNN [12] as an example, which consists of frame-level and utterance-level layers. The trait layers seek to generate phonetic traits from the frame sequence and pool these phonetic traits to obtain an utterance-level speaker embedding as in Fig. 2 (a). This ensures that the output speaker embedding is derived from fine-to-coarse hierarchy explainable in human terms.\nWe prepare the training and test utterances by applying a pre-trained wav2vec2-based phone recognizer [31], that demarcates the phone boundaries. This allows us to compute the phonetic traits in the \u201cTrait Embedding' block as shown in Fig. 2 (b). We first derive a frame-level embedding sequence of T frames from the frame-level layers, where each embedding has D\u2081 dimensions. We then derive the phonetic trait by averaging the frame-level embeddings within each phone segment. Suppose we have a phonetic inventory of I phones, the phonetic trait is represented by I D\u2081-dimensional trait embeddings, which forms a phonetic trait set to characterize a speaker.\nConsidering that some phones might be absent in an utterance, we set the empty phonetic trait as 0, or a blank vector in the colored phonetic trait set in Fig. 2 (b). To reduce sparsity, such empty phonetic trait is removed by a 'Trait Filter' block before the utterance-level layers, leading to N < I trait embeddings. A statistics pooling layer aggregates all N trait embeddings and propagated through a linear layer. We finally obtain a speaker embedding of D2 dimensions as the output of the linear layer, following the same setting as in ECAPA-TDNN."}, {"title": "III. TRAINING AND LOSS FUNCTION", "content": "During training, each minibatch contains K randomly selected speakers. Each speaker contributes two randomly selected utterances: one for enrollment and another for test. For simplicity, as shown in Fig. 2, we denote the set of enrollment utterances within a minibatch as Ue = {Ue,1, Ue,2,..., Ue,K} and the set of test utterances as Ut = {Ut,1, Ut,2,..., Ut,K}.\nAn enrollment utterance Uek and a test utterance ut,k from the same speaker form a matched pair (Ue,k, Ut,k). Meanwhile, a randomly sampled utterance ut,h (where h\u2260 k) from a different speaker is selected to form an unmatched pair (Ue,k, Ut,h). We further denote the fine-grained phonetic trait set ek = {e,e,...,e} for the enrollment utterance Ue,k and tk = {t, t,...,t} for the test utterance ut,k\u00b7\nUsually, the speaker network is trained with a classification task, while is tested with a verification task [32]\u2013[34]. This mismatch may increase the opaqueness of the model. To reduce this mismatch, we define a verification loss function Lveri, that strengthens the trait consistency for the same speaker while discriminates the traits between speakers,\n$\\L_{veri} = \\frac{\\alpha}{N_1} \\sum_{i=1}^{I} \\sum_{k=1}^{K} II(e_{i}^{k} \\neq 0 \\land t_{i}^{k} \\neq 0) || e_{i}^{k} - t_{i}^{k} ||^{2}  + \\frac{\\beta}{N_2} \\sum_{i=1}^{I} \\sum_{k=1}^{K}  min_{h \\neq k}(II(e_{i}^{k} \\neq 0 \\land t_{i}^{h} \\neq 0) || e_{i}^{k} - t_{i}^{h} ||^{2} ,$\n(1)\nwhere I() is the indicator function, having 1 if the condition is true and 0 otherwise. The symbol A represents the logical AND operator. N\u2081 = $\\sum_{i=1}^{I} \\sum_{k=1}^{K}II(e_{i}^{k} \\neq 0 \\land t_{i}^{k} \\neq 0)$ and\nN2 = $\\sum_{i=1}^{I} \\sum_{k=1}^{K}II(e_{i}^{k} \\neq 0 \\land t_{i}^{h} \\neq 0)$ are two denominators.\n$\\alpha$ and $\\beta$ are hyper-parameters to balance the inter-speaker and intra-speaker trait loss. We only retain the least distance among all unmatched pairs, which corresponds to the most similar pair.\nThe trait verification loss Lveri only considers relationships involving the same phone from either the same or different speakers, without looking at different phones. In text-independent speaker verification, it is desirable that we compare speech samples between different phones. It is expected that the phonetic traits from one single utterance are close to another. To address this, we introduce a trait center loss Lcenter for it:\n$\\L_{center} = \\frac{\\gamma}{K} \\sum_{k=1}^{K}  [ \\frac{1}{N_{e,k}} \\sum_{i=1}^{I}II(e_{i}^{k} \\neq 0) || e_{i}^{k} - \\hat{e}_{k} ||^{2} + \\frac{1}{N_{t,k}} \\sum_{i=1}^{I}II(t_{i}^{k} \\neq 0) || t_{i}^{k} - \\hat{t}_{k} ||^{2} ], $\n(2)\n$\\hat{e}_{k} = \\frac{1}{N_{e,k}} \\sum_{i=1}^{I}I(e_{i}^{k} \\neq 0)e_{i}^{k},$\n(3)"}, {"title": "IV. EXPERIMENTS", "content": "We use VoxCeleb2 [36] for training. The hyperparameters are fine-tuned and decided based on Vox1-O, where a, \u1e9e and y are set to 0.0007, 0.00001 and 0.0001, respectively. We evaluate the model on Vox1-H, Vox1-E, The speakers in the wild (SITW) [37], and Librispeech [38]. We use the trial list of the core-core condition of SITW for both the development and evaluation parts. Since there is no official trial list for Librispeech, we randomly generated one target and one non-target trial for each utterance in the train-clean-100, train-clean-360, dev-clean, and test-clean subsets, resulting in a total of 275,752 trials. The phonetic inventory contains 39 units in the CMU phone set [39] and a non-verbal label [N-V], therefore, I = 40. Any frame that cannot be classified as a phoneme has been assigned to the non-verbal label.\nThe speaker verification system is an implementation of the WeSpeaker toolkit [40]. We follow the 'ecapa-tdnn' configuration while setting the number of speakers in each minibatch as 32. D\u2081 and D2 are set to 1,536 and 192 respectively. We trained an ECAPA-TDNN as the baseline of the traditional black-box model. Both the ECAPA-TDNN and EXPO are trained with the same formula.\nFor the speaker verification decision, the cosine similarity between two speaker embeddings, namely enrollment and test utterance, is adopted as the final score, with EER and minDCF serving as the evaluation metrics. As in manual voice comparison, we attempt to explain the speaker verification results by showing the similarity between phonetic traits. A positive correlation between this phonetic-level similarity and the final score serves as an evaluation criterion for explainability.\nSpecifically, for each trial, a I-dimensional phonetic trait similarity vector s is generated, of which an element s(i) represents the phone-wise cosine similarity between the i-th phonetic trait of the enrollment utterance and the test utterance. If the phone-wise similarity vector behaves similarly to the utterance-level final score, one is able to use the similarity vector to explain the final decision. Motivated by this idea, we propose an evidence score as the non-zero average of its values, that is not used to make speaker verification decision,\n$\\hat{t}_{k} = \\frac{1}{\u039d_{t,k}} \\sum_{i=1}^{I}I(t_{i}^{k} \\neq 0)t_{i}^{k},$\n(4)\nwhere Ne,k = $\\sum_{i=1}^{I}II(e_{i}^{k} \\neq 0)$ and Nt,k = $\\sum_{i=1}^{I}I(t_{i}^{k} \\neq 0)$ and y is the weight for Lcenter.\nThe final loss function L for ExPO optimization is:\n$\\L = L_{AAM} + L_{veri} + L_{center}.$\n(5)\nBesides the Lveri and Lcenter, the Additive angular margin loss (AAM)-Softmax [35] LAAM penalizes misclassification on the speaker identity."}, {"title": "V. RESULTS", "content": "Let us now understand the model's explainable results. Fig. 3 is a visualization for a positive and a negative trial. In the first trial, as in Fig. 3(a), the final score is 0.914, and the phonetic trait similarity vector\n$\\score =  \\frac{1}{N}  \\sum_{i=1}^{I}II(e_{i} \\neq 0 \\land t_{i} \\neq 0)S(i),$\n(6)\nwhere e and t is the enrollment and test utterance respectively, and N = $\\sum_{i=1}^{I} I(e_{i}^{k} \\neq 0 \\land t_{i}^{k} \\neq 0)$. The closer the evidence score is to the final score, the better the explainability is. A closer match between the evidence score and the final score suggests a better explanation of the decision.\nFig. 4 shows the F-ratio for the phonetic traits across multiple test sets. We observe that all phonetic traits achieve a ratio greater than 1.0, confirming their discriminability. Although the ratio varies across different test sets, they generally follow the same trend decreasing from left to right in terms of F-ratio. This suggests that the discriminativeness of the phonetic traits is consistent across test sets. We observe that the '[N-V]' values are unexpectedly high, which suggests that non-verbal segments may play a more substantial role than previously thought. This observation may be attributed to the potential speaker information in non-verbal voice, such as laughter, breathing, coughing, and also rhythmic speech patterns. The importance of non-verbal segments is also investigated in both automatic speaker verification [41]\u2013[43] and forensic speaker comparison [44], [45]. We believe our study provides a cue for understanding the role of non-verbal sounds in speaker verification."}, {"title": "VI. CONCLUSION", "content": "In this work, we validated an idea to integrate phonetic traits in a deep learning-based speaker network to enhance model explainability. We shown that, by comparing phonetic traits between enrollment and test utterances, the proposed EXPO model offers a better understanding of the decision-making"}]}