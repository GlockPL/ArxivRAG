{"title": "Multilingual Mathematical Reasoning: Advancing Open-Source LLMs in Hindi and English", "authors": ["Avinash Anand", "Kritarth Prasad", "Chhavi Kirtani", "Ashwin R Nair", "Manvendra Kumar Nema", "Raj Jaiswal", "Rajiv Ratn Shah"], "abstract": "Large Language Models (LLMs) excel in linguistic tasks but struggle with mathematical reasoning, particularly in non-English languages like Hindi. This research aims to enhance the mathematical reasoning skills of smaller, resource-efficient open-source LLMs in both Hindi and English. We evaluate models like OpenHathi 7B, LLaMA-2 7B, Wizard-Math 7B, Mistral 7B, LLeMMa 7B, MAmmoTH 7B, Gemini Pro, and GPT-4 using zero-shot, few-shot chain-of-thought (CoT) methods, and supervised fine-tuning. Our approach incorporates curriculum learning, progressively training models on increasingly difficult problems, a novel Decomposition Strategy to simplify complex arithmetic operations, and a Structured Solution Design that divides solutions into phases. Our experiments result in notable performance enhancements. WizardMath 7B exceeds Gemini's accuracy on English datasets by +6% and matches Gemini's performance on Hindi datasets. Adopting a bilingual approach that combines English and Hindi samples achieves results comparable to individual language models, demonstrating the capability to learn mathematical reasoning in both languages. This research highlights the potential for improving mathematical reasoning in open-source LLMs.", "sections": [{"title": "Introduction", "content": "Enhancing AI systems to solve complex problems has become a crucial objective within the AI research community, particularly in the realm of mathematical question-answering. While models like GPT-4 and Gemini have demonstrated their strengths in arithmetic (Zhang et al. 2024), algebra (Kao, Wang, and Hsieh 2024), scientific text generation (Anand et al. 2024d, 2023a), and symbolic manipulation (Dave et al. 2024), they are not without limitations. Our evaluations on the GSM8K (Cobbe et al. 2021) and MATH (Hendrycks et al. 2021) datasets reveal a stark contrast in their capabilities. These models perform well on the relatively straightforward GSM8K dataset, but their effectiveness significantly diminishes when tasked with the more challenging MATH dataset. This dataset includes high-school competition-level questions that require a deeper level of contextual understanding and more advanced reasoning skills. The discrepancies in performance highlight the current limitations of these models in handling complex mathematical problem-solving.\nIn addition to these challenges, there is a noticeable gap in the performance of large language models (LLMs) when applied to English versus non-English languages, particularly in natural language processing tasks such as question answering and classification. This gap is particularly evident in Hindi, India's predominant language, which is used by over 105 million students according to UDISE+ reports for 2019-202. Enhancing the capabilities of LLMs in Hindi is essential to make these tools more accessible and effective in subject-specific learning contexts. While research efforts such as OpenHathi-7B (AI 2023), Hi-NOLIN (Research 2023), and Airavata (Gala et al. 2024) have made strides in adapting LLMs to the Hindi language, these models were not originally optimized for domain-specific tasks like mathematical problem-solving.\nRecent advancements in open-source LLMs have shown promise in improving mathematical and physics problem-solving abilities (Anand et al. 2024a,c,b, 2023b), as evidenced by prominent models like WizardMath (Luo et al. 2023), Mistral (Jiang et al. 2023), LLeMMA (Azerbayev et al. 2023), and MAmmoTH (Yue et al. 2023). However, these advancements have largely focused on the English language, with limited performance gains observed in Hindi math datasets. Additionally, closed-source models such as GPT-4 and Gemini-Pro continue to outperform open-source models on established benchmarks like GSM8K and MATH, as well as on newly defined Hindi datasets. The significant performance disparity can be attributed to the vast difference in the number of parameters these models are trained on. While the open-source LLMs explored in this research have fewer than 10 billion parameters, well-known closed-source LLMs are trained on considerably large parameter counts. Given the constraints on computational resources, this research focuses on enhancing the performance of smaller open-source LLMs (SLLMs), acknowledging the limitations while seeking to optimize within these parameters.\nThis research introduces several key contributions aimed at improving the mathematical capabilities of SLLMs, particularly in Hindi:"}, {"title": "Related Work", "content": "Recent advances in large language models (LLMs) have significantly improved their ability to perform complex tasks, particularly in the areas of natural language processing and mathematical reasoning. However, one area that remains underexplored is the application of Curriculum Learning to these models. Originally proposed by Bengio et al. (Bengio et al. 2009), Curriculum Learning is a training strategy that mimics the way humans learn by gradually increasing the complexity of tasks presented to the model. Although widely used in deep learning, its application to LLMs has been limited, particularly in enhancing the models' capabilities in complex, multistep reasoning tasks. This section discusses various open-source and bilingual LLMs, their architectures, and the benchmark datasets used to evaluate their performance."}, {"title": "Open-Source Large Language Models", "content": "1. Llama Model (Touvron et al. 2023): It is a foundational model requiring less computational power, ideal for fine-tuning for various tasks as it is trained on vast unlabeled data.\n2. Wizard Math (Luo et al. 2023): It improved mathematical reasoning abilities by applying the \"Reinforcement Learning from Evolutionary Instruction Feedback (RLEIF) method\" (Xu et al. 2023) in math."}, {"title": "Bilingual Open-Source Large Language Models for English and Hindi", "content": "OpenHathi-7B Sarvam AI used Llama2-7B to create OpenHathi-Hi-v0.1 (AI 2023). It integrated a custom tokenizer to expand the embedding layer to 48K tokens and trained on Hindi-English translation and bilingual next-token prediction tasks.\nHi-NOLIN HI-NOLIN (Research 2023), a Bilingual LLM in the Pythia model suite, was trained on English and code datasets, then further pre-trained on combined Hindi and English data to improve Hindi understanding.\nAiravata AIRAVATA model (Gala et al. 2024) is derived from fine-tuning the OpenHathi model (AI 2023) with a Hindi instruction-tuned dataset, translated from high-quality English data via IndicTrans2 translation model (Gala et al. 2023)."}, {"title": "Benchmark Datasets", "content": "(Cobbe et al. 2021) introduced the GSM8K dataset, which consists of 8,500 grade school math problems that require basic arithmetic operations. These problems are designed to be solvable by proficient middle school students. Similarly, (Hendrycks et al. 2021) released the MATH dataset, containing 12,500 complex problems from high school competitions such as AMC 10 and AMC 12, intended for high school students. It covers topics: Algebra, Counting and Probability, Geometry, Intermediate Algebra, Number Theory, Prealgebra, and Precalculus. (Lightman et al. 2023) introduced PRM800K, a dataset with 800,000 step-level feedback labels for solutions to MATH (Hendrycks et al. 2021) problems, providing annotations (\"Positive,\u201d \"Negative,\u201d or \"Neutral\") to each solution step. For Hindi-speaking students, (Sharma, Mishra, and Sharma 2022) released HAWP (Hindi Arithmetic Word Problems), which is the only publicly available dataset of Hindi mathematical questions. It is for grades 1 to 6, and comprises 2,336 basic math word problems requiring a single operator solution."}, {"title": "Methodology", "content": "To improve the computational accuracy of large language models (LLMs) in arithmetic operations involving large numbers, we propose a Decomposition Strategy for multiplication and division tasks. For multiplication, this involves breaking down the multiplicand into place value components\u2014such as hundreds, tens, and ones\u2014and multiplying each by the other multiplicand. The products are then aggregated to obtain the final result. For division, the dividend is similarly decomposed into segments, each divided by the divisor, with the quotients summed to produce the final answer. This has been proposed to combat the poor calculation skills of open-source language models. In this paper, we focus on introducing and validating the Decomposition Strategy using the HAWP dataset, which contains basic mathematical word problems requiring single-operation calculations. This allows us to clearly demonstrate the strategy's effectiveness in a controlled, straightforward context. While exploring its application to more complex datasets is an exciting future direction, we have chosen to concentrate on HAWP for now to ensure a thorough and focused evaluation of this novel approach.\nWe utilized 2,336 Hindi arithmetic problems from the HAWP dataset, covering basic operations like addition, subtraction, multiplication, and division. Since the original dataset lacked solutions, we enhanced it by generating question-answer pairs using GPT-4, which were then carefully reviewed and corrected by five human experts, resulting in the Enhanced HAWP dataset.\nTo evaluate the Decomposition Strategy's effectiveness, we applied it to the Enhanced HAWP dataset (see Figure 2). Manually solved examples using this strategy were used in few-shot learning with GPT-4 to modify the remaining solutions in Enhanced HAWP. These refined solutions, with 70% and 30% training/testing split, were then used to fine-tune the models OpenHathi 7B, WizardMath-v1.1 7B, and LLeMMa 7B. The resulting accuracy improvements are shown in Table 1.\nIn our final phase, we focused on exploring the benefits of fine-tuning using an augmented version of the dataset that we previously prepared Decomposition Strategy-enhanced dataset. We expanded the original 2,000 problems to 10,000 using a one-shot prompting technique with GPT-4. These newly generated samples were carefully reviewed by five human experts for accuracy, resulting in the HMQA (Hindi Math Questions-Answers) dataset. We then used this augmented dataset to fine-tune the models OpenHathi 7B, WizardMath-v1.1 7B, and LLeMMa 7B, with the resulting accuracy compared to previous settings in Table 1."}, {"title": "IndiMathQA", "content": "We have meticulously curated our own comprehensive math problem dataset, referred to as IndiMathQA, sourcing problems from the official NCERT textbooks used in Indian schools. This dataset contains 598 manually curated math problems and their corresponding solutions. These problems are suited for students in grades 10, 11, and 12, and it encompasses a wide range of problems that vary in complexity and span 14 major mathematical domains, including sets, trigonometry, and the binomial theorem, among others. Appendix provides more details on topic distribution."}, {"title": "LLM Enhancement in Bilingual Mathematics", "content": "In this section we demonstrate the strategies used in improving mathematical reasoning skills in Bilingual settings. Our proposed strategies are namely Structured Solution Creation, Curriculum Learning, and Bilingual Training in Hindi and English. We explain the Bilingual Training Dataset Creation in two phases: (i) Classification based on Complexity (required for Curriculum Learning), (ii) Structured Solution Creation and Bilingual Translations. Finally, we demonstrate how we conducted curriculum learning based bilingual fine-tuning on our training datasets."}, {"title": "Classification based on Complexity", "content": "We have carefully curated a collection of mathematical problems categorized into easy, medium, and hard levels. This collection includes problems from our own dataset as well as from benchmark datasets, such as GSM8K and MATH. Below, we outline the methods we used to classify each problem by its complexity. We utilize additional datasets (GSM8K and MATH) for the sole reason of having more diversity of mathematical topics in our dataset.\nIndiMathQA: The IndiMathQA dataset was carefully annotated by a team of five human experts, resulting in 136 easy, 218 medium, and 244 hard questions. To ensure the reliability of these annotations, we calculated the Average Fleiss' Kappa score, which came out to 0.58, indicating low bias and substantial agreement among the annotators. Further details on the annotation process can be found in the Appendix. This dataset was then augmented to a total of 7823 questions with similar concepts (820 easy, 2,470 medium, and 4,533 hard) using the GPT-4 API, which were then reviewed by a team of 5 human experts to correct any errors"}, {"title": "Curriculum Learning based Fine-Tuning", "content": "We apply the technique of Curriculum Learning to SLLMs, hypothesizing that by incrementally increasing the complexity of problems during fine-tuning, we can simulate the natural process of human learning\u2014where mastering simpler tasks paves the way for tackling more challenging ones. Our approach utilizes the Easy and Medium datasets, carefully constructed to cover a diverse range of mathematical topics. Each dataset was divided into 70% for training and 30% for testing, ensuring this split was consistently applied across all problem categories: easy, medium, and hard.\nTo implement Curriculum Learning, we first train our SLLMs on the Easy dataset, producing a model checkpoint we refer to as SFT_Easy. This checkpoint is then further fine-tuned using the Medium dataset, resulting in the final checkpoint, SFT_Easy+Medium. We evaluate the performance difference between these two checkpoints using testing sets from both benchmark datasets and our curated dataset.\nWe propose a hypothesis that fine-tuning LLMs on a dataset combining identical question-answer pairs in both English and Hindi could enhance the model's ability to understand and reason through math problems in Hindi-a language where the LLM might not be as proficient. Our reasoning is grounded in the idea that by exposing the LLM to parallel data in English, a language it excels in, the model can leverage its strengths in English to build stronger associations and improve its performance in Hindi. To test this hypothesis, our Curriculum Learning-based fine-tuning is conducted in two distinct ways:\n1. Training the SLLMs separately on English and Hindi datasets, with results presented in Table 2\n2. Employing Bilingual Combined Training, where the model is trained on a combined dataset of both English and Hindi question-answer pairs. The outcomes of this bilingual training are detailed in Table 3 and 4.\nFor our evaluation of SLLMs in Hindi Math reasoning, we only evaluate performance on the Hindi version of Indi-MathQA and the HAWP dataset. For the purpose of clarity, we refer the Hindi version of IndiMathQA as HMKB and the English version as EMKB. (Table 2)"}, {"title": "Ablation Study", "content": "To comprehensively understand the results and significance of each novel methodology employed in our study, we evaluated performance at every stage. In our experiments with the Hindi dataset, Table 1 shows accuracy metrics attained by base models employing zero-shot and few-shot prompting. The findings underscore a substantial performance enhancement with few-shot prompting compared to zero-shot, demonstrating a notable increase of 20-50% across all operations. This highlights the effectiveness of providing task examples to LLMs. Further, fine-tuning on an enhanced HAWP dataset led to substantial improvements (20-30% in general) in OpenHathi's performance in addition and subtraction tasks, and in WizardMath's performance across all operations. However, LLeMMA-7B's performance declined after fine-tuning. After manual assessment of its responses, we found that it is exhibiting hallucinations in its solutions. This aligns with recent findings that fine-tuning on new knowledge can increase hallucinations (Gekhman et al. 2024). LLeMMA, primarily pre-trained on English mathematical data, showed hallucinations when provided with new Hindi mathematical knowledge. Our novel Decomposition Strategy significantly enhanced LLeMMA's performance, demonstrating that breaking down complex calculations can reduce hallucinations and enhance reasoning skills. Additionally, addressing hallucinations through augmentation of samples proved effective, as shown by the improvements from instruction-tuning on HMQA for both Open-Hathi and LLeMMA. A detailed analysis of the benefits of curriculum learning on both Hindi and English datasets is also provided in the following Results & Analysis section."}, {"title": "Result & Analysis", "content": "In this section, we first examine the impact of Curriculum Learning based fine-tuning in Hindi and English separately. The analysis then explores the results from bilingual combined training. Lastly, we compare the problem-solving capabilities of lightweight open-source models (SLLMs) against closed-source models (LLMs) across different languages and difficulty levels."}, {"title": "Curriculum Learning - English Training", "content": "We explore the impact of Curriculum Learning on English Dataset on the performance of SLLMs. In the base setting, the models were fine-tuned on the entire English dataset without distinguishing problem complexity. In this setting, WizardMath-7B demonstrated the highest performance, while LLeMMA-7B exhibited the lowest performance across all benchmarks and our English dataset, EMKB, as shown in Table 2. Following this, the models underwent fine-tuning on a subset of easy problems (SFT_easy), leading to a 4-6% improvement on easy problems and a 6-8% increase on the GSM8K benchmark, indicating effective learning of simpler questions during this phase. However, the improvements on more challenging benchmarks like MATH and PRM800K were modest, with only a 1-2% increase. In the next stage, models were fine-tuned on both easy and medium problems (SFT_easy+medium). This approach yielded a consistent 6% performance increase on medium problems and a 3% improvement on hard problems. These findings (Table 2), suggest that systematically increasing the difficulty of problems enables models to surpass their base setting performance."}, {"title": "Curriculum Learning - Hindi Training", "content": "When applying Curriculum Learning to the Hindi datasets, initially, WizardMath-7B led, while LLeMMA-7B lagged on the Enhanced HAWP Benchmark. Fine-tuning on easy problems (SFT_easy) improved performance by 3-5%, but gains on medium and hard problems were minimal. Introducing Curriculum Learning (SFT_easy+medium) led to an additional 2-4% improvement on the benchmark and 3-5% on more difficult problems (Table 2). This stepwise training approach effectively enhanced the models' ability to tackle increasingly complex tasks, demonstrating the value of a structured learning regimen in Hindi datasets."}, {"title": "Curriculum Learning - Bilingual Combined Training", "content": "Finally, we tested the performance of SLLMs on full Indi-MathQA dataset, covering both Hindi and English versions (Tables 3 and 4). SLLMs were fine-tuned using Curriculum Learning on a bilingual combined training set. As a general trend, all models that went through a combined bilingual training (Tables 3 and 4) performed better on Hindi Benchmarks in comparison to single language fine-tuning (Table 2). This is a remarkable enhancement achieved from our hypothesis that combined fine-tuning on English and Hindi can help improve model's Hindi Mathematical Reasoning.\nInitially, WizardMath-7B achieved the highest performance, while Airavata-7B had the lowest results (Base Settings: Table 2). Fine-tuning on easy problems (SFT_Easy: Table 3) in both languages led to a consistent 3-5% improvement on easy questions, enhancing the models' ability to generalize across different linguistic contexts. However, improvements on medium and hard problems were minimal, highlighting the limitations of focusing solely on easy problems.\nWhen fine-tuned on both easy and medium problems in both languages (SFT_Easy+Medium: Table 4), the models showed more significant gains, with medium problems improving by 11-18% and hard problems by around 2%. This demonstrates the effectiveness of Curriculum Learning in enhancing problem-solving abilities and leveraging bilingual training."}, {"title": "Fine-Tuning Open-Source Models (SLLMs)", "content": "In our evaluation of the performance of open-source models such as LLaMA-7B, LLeMMA-7B, Mistral-7B, MAmmoTH-7B, and WizardMath-7B when fine-tuned on combined both Hindi and English versions of IndiMathQA (HMKB and EMKB) (Tables 3 and 4), we observed that fine-tuning on both languages combined improves the performance on both the languages significantly compared to the gains when fine-tuning on a single language. As shown in Table 3 and 4, fine-tuning on easy problems from both languages led to a marginal 2-3% performance increase on easy problems in both Hindi and English. This improvement is better than the pre-trained models but less substantial than the improvements seen with single-language fine-tuning, as indicated in Table 2. However, Table 3 and 4 further demonstrate that fine-tuning easy and medium problems from both languages resulted in a significant major improvement of 11-18% accuracy."}, {"title": "SLLMs (Lightweight open-source) vs LLMS (closed-source)", "content": "WizardMath-7B is the best-performing SLLM in our research. Although GPT-4 performance exceeds even the enhanced performance of WizardMath (Table 2, 3 and 4), through Curriculum Learning (SFT_easy+medium) and Bilingual Parallel Training, WizardMath-7B outperforms Gemini 1.0 Pro in English datasets by about 5% (Table 2, 3 and 4). This improvement highlights the effectiveness of our methodology in enhancing SLLM's problem-solving abilities in English. However, in Hindi datasets, while WizardMath-7B performance is comparable to Gemini Pro, it still lags by approximately 3% across Medium and Hard difficulties, likely because WizardMath is more proficient in solving math problems in English than in Hindi."}, {"title": "English Models vs Bilingual Models", "content": "Finally, in this comparative analysis of bilingual models and other open-source models (Tables 2, 3 and 4), we observe that bilingual models perform consistently better across English and Hindi, unlike most open-source models, except for WizardMath-7B. This consistency is likely due to the language-independent nature of mathematical reasoning. However, bilingual models like OpenHathi-7B, which are not pre-trained on mathematical tasks, show only slight improvement after fine-tuning, suggesting limited learning efficiency. The superior performance of WizardMath-7B highlights the importance of pre-training models on mathematical tasks for robust performance across languages."}, {"title": "Conclusion", "content": "This research developed a Bilingual Math Problem Solver using curriculum learning, query decomposition, and structured solution generation. The Decomposition Strategy improved reasoning by breaking down complex queries, Structured Solution addressed the problem of Hallucinations, while curriculum learning enhanced performance on medium and hard problems. WizardMath-7B consistently outperformed other SLLMs (Lightweight open-source) models and often surpassed closed-source models like Gemini 1.0 Pro with these strategies. Our findings demonstrate that integrating these methodologies significantly enhances the problem-solving capabilities of LLMs. Bilingual Parallel Training (Training in multiple languages) provided diverse problem-solving perspectives, proving more effective than single-language training. This study shows how these diverse methodologies can be used to address issues with LLMs in math problem-solving, and can effectively enhance their performance in Hindi."}, {"title": "Explanation of Key Terms and Methodology Components", "content": "In our study, we developed a structured approach to provide solutions for each raw question-answer pair, consisting of the following steps:\nData Identification: Specifying the relevant data needed for the problem. This includes identifying variables, constants, and any conditions or parameters related to the problem.\nProblem Analysis: Examining the problem to understand its components and determine suitable methods for solving it. This includes understanding theoretical aspects such as set theory or integration rules.\nTheoretical Framework: Establishing the foundational theories and principles that guide the solution. Examples include using set operations for probability problems or antiderivative rules for integrals.\nMethodology Development: Creating a detailed, step-by-step plan to solve the problem. This includes developing necessary procedures and formulae for the solution.\nComputation: Performing the calculations and applying the developed methodology to derive the final solution.\nSolution: Presenting the final answer, ensuring it is clear and easy to understand."}, {"title": "Data Classification: Easy, Medium, and Hard", "content": "To classify data into easy, medium, and hard categories, we use the following criteria:\n1. Language Understanding: Assesses the complexity of the language used. Problems with straightforward, clear language are classified as easy, while those with technical or complex phrasing are considered medium or hard.\n2. Mathematical Complexity: Assesses the nature of mathematical operations required. Problems involving fundamental concepts are easy, whereas those requiring multiple or intricate operations are medium or hard.\n3. Reasoning Complexity: Evaluates the complexity of reasoning needed. Problems requiring straightforward application of concepts are easy, while those needing detailed logic or multi-step reasoning are medium or hard.\n4. Number of Variables: Evaluates the number of variables involved. Problems with a few variables are easy, whereas those with many or complex variables are medium or hard.\n5. Conceptual Complexity: Assesses the depth of understanding required. Basic conceptual understanding is easy, while advanced or abstract concepts are medium or hard."}, {"title": "Structured Problem Solving: Examples of Data Transformation", "content": "We have raw data in the format of Question and Final Answer, which is then converted into a Refined Solution. These examples span various topics, including but not limited to Probability, Linear Algebra, Integration, and many more. Following are examples of few instances from"}, {"title": "Example 1: Conic Sections", "content": "Question: Find the equation of the hyperbola with foci (0, \u00b13) and vertices (0,\u00b1$\\frac{\\sqrt{11}}{2}$). \nRaw Solution: Given the foci on the y-axis, the equation of the hyperbola is of the form\n$\\frac{y^2}{\u03b1^2} - \\frac{x^2}{b^2} = 1$.\nUsing the given vertices (0,\u00b1$\\frac{\\sqrt{11}}{2}$), we deduce that\na = $\\frac{\\sqrt{11}}{2}$"}, {"title": "Refined Solution:", "content": "Data Identification: Foci: (0,\u00b13) Vertices:(0,\u00b1$\\frac{\\sqrt{11}}{2}$)\nProblem Analysis: We need to determine the standard form of the equation of the hyperbola given its foci and vertices. For a hyperbola centered at the origin and oriented along the y-axis, its general equation is\n$\\frac{y^2}{\u03b1^2} - \\frac{x^2}{b^2} = 1$,\nwhere: - The distance from the center to a vertex is a. - The distance from the center to a focus is c. - The relationship $c^2 = a^2 + b^2$ holds true.\nTheoretical Framework: Key concepts and formulae for hyperbolas include: - Standard form of hyperbola equation: $\\frac{y^2}{\u03b1^2} - \\frac{x^2}{b^2} = 1$  - Distance to the vertices: \u00b1a - Distance to the foci: \u00b1c - Relationship: $c^2 = a^2 + b^2$\\nMethodology Development: From the given foci and vertices, we can identify a and c. Using the relationship $c^2 = a^2 + b^2$, we can find b and formulate the hyperbola's equation.\nComputation:\n1. Identify c:\nc=3\n2. Identify a:\na = $\\frac{\\sqrt{11}}{2}$\n3. Apply the relationship $c^2 = a^2 + b^2$:\n$c^2 = a^2 + b^2 \u21d2 3^2 = (\\frac{\\sqrt{11}}{2})^2 +b^2$\n4. Compute $a^2$:\n$a^2 = (\\frac{\\sqrt{11}}{2})^2  \u21d2 a^2 = \\frac{11}{4}$\n5. Solve for $b^2$:\n$\\frac{11}{4} + b^2 = 9 \u21d2 b^2 = 9-\\frac{11}{4} \u21d2 b^2 = \\frac{36-11}{4}$ \n$\u21d2 b^2 = \\frac{25}{4}$\nTherefore,\n$\\frac{36-11}{4}$\nTherefore,\n$b^2 = \\frac{25}{4}$\nSolution: Using the values of $a^2$ and $b^2$, the equation of the hyperbola is: Using the values of $a^2$ and $b^2$, the equation of the hyperbola is:\n$\\frac{y^2}{\\frac{11}{4}} - \\frac{x^2}{\\frac{25}{4}} = 1$\n$\\frac{4y^2}{11} - \\frac{4x^2}{25} = 1$\n$\\frac{25*4y^2}{11*25} - \\frac{4*11x^2}{25*11} = 1$\nSimplifying by multiplying through by 100 to clear the denominators:\n$100y^2- 44x^2 = 275$\nHence, the equation of the hyperbola is:\n$100y^2 - 44x^2 = 275$"}, {"title": "Example 2: Probability", "content": "Question: A committee of two persons is selected from two men and two women. What is the probability that the committee will have (a) no man? (b) one man? (c) two men?\nRaw Solution: The total number of persons = 2+ 2 = 4. Out of these four persons, two can be selected in $^4C_2$ ways.\n(a) No men in the committee of two means there will be two women in the committee.\nOut of two women, two can be selected in $^2C_2$ = 1 way.\nTherefore,\nP(no man) = $\\frac{^2C_2}{^4C_2}= \\frac{1\u00d72\u00d71}{4\u00d73} = \\frac{1}{6}$"}, {"title": "Refined Solution: Data Identification:", "content": "Total people: 2 men and 2 women.\nCommittee size: 2 persons.\nProblem Analysis:\nCalculate the total number of ways to form a committee of 2 persons from 4 people.\nDetermine the number of favorable outcomes for:\nNo man in the committee.\nOne man in the committee.\nTwo men in the committee.\nCalculate the probability for each case by dividing the number of favourable outcomes by the total number of ways to form a committee.\nTheoretical Framework:\nUse the combination formula C(n,r) which denotes the number of ways to choose r items from n items without regard to the order, given by C(n,r) = $\\frac{n!}{r!(n-r)!}$\nProbability of an event P(E) = $\\frac{Number  of favorable outcomes}{Total number of outcomes}$\nMethodology Development:\n1. Total number of ways to form a committee of 2 out of 4 people: C(4, 2).\n2. Number of ways to form a committee with:\nNo man: C(2, 2) (choosing 2 women out of 2).\nOne man: C(2, 1) \u00d7 C(2, 1) (choosing 1 man out of 2 and 1 woman out of 2).\nTwo men: C(2, 2) (choosing 2 men out of 2).\nComputation:\n1. Total number of ways to form the committee:\nC(4,2) = $\\frac{4!}{2!(4-2)!}  =\\frac{4\u00d73}{2\u00d71} = 6$"}, {"title": "Limitations", "content": "A limitation of our study is that we do not evaluate on all the available sources of data available. Future iterations of this research aim to expand the scope of evaluation, with the help of the research community to collect more comprehensive data from various available sources. Additionally, our study did not include the examination of romanized Hindi sentences, where Hindi word for elephant are written using the English alphabet as \"Hathi.\u201d This form of input is prevalent in India, particularly when typing on electronic devices. Future research could beneficially extend to enhancing model performance on such inputs as well. The Easy/Medium/Hard distribution of questions was done manually, which can add bias to the distribution as we increase the dataset size."}]}