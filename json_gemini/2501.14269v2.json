{"title": "Hierarchical Time-Aware Mixture of Experts for Multi-Modal Sequential Recommendation", "authors": ["Shengzhe Zhang", "Liyi Chen", "Dazhong Shen", "Chao Wang", "Hui Xiong"], "abstract": "Multi-modal sequential recommendation (SR) leverages multi-modal data to learn more comprehensive item features and user preferences than traditional SR methods, which has become a critical topic in both academia and industry. Existing methods typically focus on enhancing multi-modal information utility through adaptive modality fusion to capture the evolving of user preference from user-item interaction sequences. However, most of them overlook the interference caused by redundant interest-irrelevant information contained in rich multi-modal data. Additionally, they primarily rely on implicit temporal information based solely on chronological ordering, neglecting explicit temporal signals that could more effectively represent dynamic user interest over time. To address these limitations, we propose a Hierarchical time-aware Mixture of experts for multi-modal Sequential Recommendation (HM4SR) with a two-level Mixture of Experts (MoE) and a multi-task learning strategy. Specifically, the first MoE, named Interactive MoE, extracts essential user interest-related information from the multi-modal data of each item. Then, the second MoE, termed Temporal MoE, captures user dynamic interests by introducing explicit temporal embeddings from timestamps in modality encoding. To further address data sparsity, we propose three auxiliary supervision tasks: sequence-level category prediction (CP) for item feature understanding, contrastive learning on ID (IDCL) to align sequence context with user interests, and placeholder contrastive learning (PCL) to integrate temporal information with modalities for dynamic interest modeling. Extensive experiments on four public datasets verify the effectiveness of HM4SR compared to several state-of-the-art approaches. Our code is available at https://github.com/SStarCCat/HM4SR.", "sections": [{"title": "1 Introduction", "content": "Sequential recommendation (SR) aims to predict the subsequent interactions of users by analyzing their historical behaviors in chronological order [26, 49, 54]. Traditional SR methods rely on item-IDs solely to develop sequence encoders [25, 31, 37, 66], and the abundant multi-modal item descriptions are ignored and unused, which may encapsulate potential user interests. Recently, the multi-modal SR task has been proposed to integrate various types of multi-modal data like text and images as semantic supplements for user-item interactions [1, 22, 23, 33, 53]. In this context, these approaches significantly enhance recommendation quality by capturing more fine-grained item features and better modeling user interests, thereby attracting increasing attention from both academia and industry."}, {"title": "2 Related Work", "content": "In this section, we provide previous research relevant to our work, including traditional and multi-modal sequential recommendation."}, {"title": "2.1 Traditional Sequential Recommendation", "content": "Sequential recommendation targets to predict the next item users may be interested in based on their behavior sequences. Early works focus on different sequential encoding models to process user behavior sequences. For instance, GRU4Rec [20], Caser [41], SASRec [27] and BERT4Rec [40] utilize RNN, CNN, self-attention and BERT structures as their basic encoders, respectively. Since time information apparently represents the changes and transitions of user interest, many studies try to enhance SR methods with time-aware designs. For example, TiSASRec [30] exploits time intervals for self-attention-based models to capture evolving information in behavior sequences. MEANTIME [9] exploits multiple types of time embeddings to improve sequential interest modeling. TGCL4SR [62] devises temporal graph contrastive learning with time perturbation augmentation to enhance item temporal transition pattern encoding [13, 32, 38, 39]. Furthermore, FEARec [16] designs a frequency rump structure, and hybrids the time domain self-attention encoder with the frequency domain self-attention module to grasp both low-frequency and high-frequency patterns. TiCoSeRec [11] utilizes five time-aware sequence-level augmentation operations to unify the time distribution of user behaviors and conduct contrastive learning. However, the above methods model evolving behavior sequences without exploring multi-modal data, and thus cannot fully exploit multi-modal features which reflect user interests."}, {"title": "2.2 Mutli-Modal Sequential Recommendation", "content": "Multi-Modal SR has emerged to leverage multi-modal information of items to capture user interests, which significantly enhances recommendation quality [24, 29, 33, 59, 60]. Existing studies mainly concentrate on integrating modalities to augment recommendation accuracy. For example, MV-RNN [10] fuses modal data through addition, concatenation, and reconstruction. UniSRec [22] applies MoE to facilitate semantic transfer on text representations and add them to the ID embeddings. Successively, many works further design adaptive fusion modules to improve multi-modal fusion effectiveness. For instance, MISSRec [48] implements a lightweight fusion mechanism to discern user dynamic attention across modalities for adaptable fusion. MMSR [23] devises adaptive fusion with heterogeneous graph neural networks to flexibly utilize the interplay between modalities. M3SRec[1] uses modality-specific MoE and cross-modal MoE for multi-sided modality integration. In addition, TedRec [57] leverages Fast Fourier Transform to process the embedding sequences of IDs and textual contents for sequence-level semantic fusion in the frequency domain. While most methods rely solely on implicit temporal information, they often neglect dynamic explicit temporal data, redundant modal filtering, and the extraction of interest-relevant information. In contrast, our proposed HM4SR addresses these challenges by explicitly incorporating dynamic temporal information and introducing auxiliary supervision tasks to enhance dynamic user interest modeling."}, {"title": "3 Problem Definition", "content": "Multi-Modal sequential recommendation aims to exploit multi-modal item information and historical user behavior to make personalized recommendations for the next user interaction. Let us assume a set of users $U$ and a set of items $V$. Each item $v \\in V$ is represented as $v = (i_{id}, i_{txt}, i_{img})$, where $i_{id}$ denotes the item ID, while $i_{txt}$ and $i_{img}$ refer to the associated text and image content of $v$, respectively. We denote the historical behavior sequence $S$ of user $u \\in U$ as $S = \\{v_1, v_2, ..., v_{|S|} \\}$, where $v_i \\in V$ represents the interacted item at the $i$-th time step. The corresponding timestamp sequence is denoted as $T = \\{t_1, t_2, ..., t_{|S|} \\}$.\nGiven a user $u$ and $u$'s historical behavior sequence $S$ with multi-modal item information, the objective of multi-modal sequential recommendation is to predict an item $v \\in V$ that the user $u$ might be interested in at the $(|S|+1)$-th time step. This can be formulated as finding the item $v$ that maximizes the conditional probability given the sequence of interactions:\n$$v = \\operatorname{argmax}_{v \\in V} P(v_{|S|+1} = v | S).$$"}, {"title": "4 Methodology", "content": "In this section, we introduce the technical details of our proposed HM4SR. As illustrated in Figure 2, HM4SR consists of four main components: 1) Initial Item Representation module extracts the ID, text, and image features for items as the initial representations. 2) Hierarchical Mixture of Experts module designs two levels of MoE, named Interactive MoE and Temporal MoE, to mine key interest features from the multi-modal information interaction and harness explicit temporal information for multi-modal sequence modeling by inputting time embeddings to gating routers. 3) User Interest Learning module encodes user behavior sequences with different modalities to learn user preferences. The prediction is based on the sum of logits from three modalities. 4) Multi-Task Learning strategy is designed to enhance main recommendation task training, including sequence-level category prediction and contrastive learning on ID. Placeholder contrastive learning is further proposed to deepen relationship learning between item features and time information."}, {"title": "4.1 Initial Item Representation", "content": "Multi-Modal information contains rich semantic descriptions [4, 5, 7, 67], effectively characterizing the items. Therefore, we obtain item initial representations from three modalities in real-world recommendation scenarios: ID, text, and images. For ID, we initialize an ID embedding matrix $M_{id} \\in \\mathbb{R}^{|V|\\times d}$, where $d$ denotes the size of the hidden dimension. We let $x_{id}$ represent the initial ID representation of the item $v \\in V$. As for text and image data, we encode them with pre-trained models for better representation.\nFirst, for text, we apply a widely used pre-trained BERT [12] to extract text features to capture user preference from textual semantics [8]. Given the words $\\{W_1, W_2, ..., W_L \\}$ of $v$ in textual information of items, we first concatenate them with a special symbol $[\\text{CLS}]_{txt}$ to form the input sentence. Since $[\\text{CLS}]_{txt}$ can convey the semantics of the whole sentence, we use the embedding of $[\\text{CLS}]_{txt}$ to represent the text features. Here, we input the combined sentence into pre-trained BERT to obtain the text feature as follows:\n$$f_{txt} = \\text{BERT}([[\\text{CLS}]_{txt}; W_1; W_2;...; W_L]),$$\nwhere $f_{txt} \\in \\mathbb{R}^{d_{txt}}$ is the final hidden vector for $[\\text{CLS}]_{txt}$, and $[;]$ denotes the concatenation operation.\nSecond, for images, we process the visual information by the pre-trained visual model ViT [14]. The image $w_{img}$ of item $v$ is divided into several patches $\\{p_1, p_2, ..., p_N\\}$, and then these patches are transformed into a sequence. Next, we input the patch sequence into ViT [14] as follows:\n$$f_{img} = \\text{ViT}([[\\text{CLS}]_{img}; p_1; p_2;...;p_N]),$$\nwhere $f_{img} \\in \\mathbb{R}^{d_{img}}$ is the final hidden vector for $[\\text{CLS}]_{img}$.\nMoreover, to convert the dimensionality of text embeddings and image embeddings into the same dimension size as ID embeddings, we employ two respective linear layers to change their dimensions:\n$$x_{txt} = W_{txt}f_{txt} + b_{txt},$$ \n$$x_{img} = W_{img}f_{img} + b_{img},$$ \nwhere $x_{txt}$ is the initial text representation of $v$, $x_{img}$ is the initial image representation of $v$, $W_{txt} \\in \\mathbb{R}^{d_{txt} \\times d}$, $W_{img} \\in \\mathbb{R}^{d_{img} \\times d}$, $b_{txt} \\in \\mathbb{R}^d$ and $b_{img} \\in \\mathbb{R}^d$ are trainable parameters, while $f_{txt}$, $f_{img}$ are frozen in the training process."}, {"title": "4.2 Hierarchical Mixture of Experts", "content": "To model users' interests for recommendations from multi-modal information, we design the two-level hierarchical Mixture of Experts including Interactive MoE and Temporal MoE. Interactive MoE extracts user interest-related item features to facilitate modality data learning, while Temporal MoE introduces explicit temporal information for dynamic interest modeling with the merit of specialization. In previous studies, the effect of MoE has been demonstrated in many recommendation scenarios, because it can augment the learning ability and flexibility of recommendation models by obtaining specialized item representations and user preferences from multiple aspects [2, 36, 56, 63]. MoE in multi-modal SR typically employs multiple expert networks to process modality semantic information and achieves adaptive combination by a gating network [6, 22]. The following are details of our Interactive MoE and Temporal MoE."}, {"title": "4.2.1 Interactive MoE", "content": "The target of Interactive MoE is to enhance item key feature extraction and avoid redundant information of each modality. The MoE for multi-modal SR usually processes information from one modality [1, 22, 57], which lacks interactions between different modalities and thus hinges on more informative item feature learning. Different from them, we let each expert in Interactive MoE process all modality information of items and route experts based on a target modality to achieve effective modality interaction learning. In specific, given the ID, text, and image representations of an item $e_{id}$, $e_{txt}$ and $e_{img}$ respectively, experts on the target modality $m\\in \\{id, txt, img\\}$ process them as follows:\n$$e_{inter}^m = [e_{id}; e_{txt}; e_{img}],$$ \n$$x_{inter}^m = \\alpha^m + \\sum_{i=1}^{k_1} g_{i,m}(W_{inter}^{i,m}e_{inter}^m + b_{inter}^{i,m}),$$\nwhere $x_{inter}^m$ is the fused expert output, $\\alpha^m$ is a trainable parameter to control modality interaction intensity on the modality $m$, $k_1$ denotes the number of experts in Interactive MoE, $W_{inter} \\in \\mathbb{R}^{3d \\times d}$ and $b_{inter} \\in \\mathbb{R}^d$ are the learnable weight and bias of the $i$-th expert. $g_{i,m}$ represents the routing weight of the $i$-th expert on the modality $m$ from the routing vector $g^m$ by the following gating router:\n$$g^m = \\text{Softmax} (W_{1,m}e_m + b_{1,m}),$$\nwhere $W_{1,m} \\in \\mathbb{R}^{d \\times k_1}$ and $b_{1,m} \\in \\mathbb{R}^{k_1}$ are the learnable weight and bias of the gating routers. After obtaining $x_{inter}^m$, we take the residual connection to get the outputs of Interactive MoE:\n$$e'_m = e_m + x_{inter}^m.$$\nWith the procedure above, item interest-related key information is semantically enhanced by modality interaction learning."}, {"title": "4.2.2 Temporal MoE", "content": "User interest usually changes dynamically and complicatedly, which can be more expressly indicated by explicit temporal information. Therefore, we further propose Temporal MoE as the second level to incorporate explicit temporal information into multi-modal learning, including intervals between user behaviors and absolute timestamps. To begin with, given the timestamp sequence $T = \\{t_1, t_2, ..., t_{|S|} \\}$ of the sequence $S$, the corresponding time interval sequence is formulated as follows:\n$$\\{a_1, a_2, ..., a_{|S|} \\} = \\{0, t_2 - t_1, ..., t_{|S|} - t_{|S|-1} \\},$$\nwhere $a_i$ denotes the corresponding interval of the item $v_i \\in S$. Considering that different time intervals indicate different user interest transition [44, 45, 58], we apply the following function to calculate the corresponding position of $a_i$:\n$$pos_i = [\\mu \\log (a_i + 1)],$$\nwhere $\\mu$ is a scaling parameter to control the total number of interval positions. We maintain an embedding matrix $M_t$ for intervals, and then we can get the interval embedding $r_{1,i} \\in \\mathbb{R}^d$ corresponding to $pos_i$. For absolute timestamps, inspired by the positional embedding in Transformer [43], we use the following function to obtain the time embedding of the timestamp $t$ for direct time information:\n$$r^i_{2,i} = cos(\\frac{l_i}{freq}),$$ \n$$r^i_{2,i} = sin(\\frac{l_it + z_i}{freq}).$$\nwhere $r_i^1$ is the $i$-th value of $r_{2,i} \\in \\mathbb{R}^d$, $freq$ is an adjustable hyper-parameter, $l_i$ and $z_i$ are trainable parameters. After that, the temporal embeddings are inputted to the gating router of Temporal MoE to obtain the routing weight:\n$$g' = \\text{Softmax} (W_2[r_{1,i}; r_{2,i}] + b_2),$$\nwhere $W_2 \\in \\mathbb{R}^{2d \\times k_2}$ and $b_2 \\in \\mathbb{R}^{k_2}$ are the learnable weight and bias of the gating router respectively, $k_2$ is the number of experts. Then given the outputs from Interactive MoE $e'_{id}$, $e'_{txt}$ and $e'_{img}$ of $v_i$, the output calculation of Temporal MoE is defined as below:\n$$e_{temp} = [e'_{id}; e'_{txt}; e'_{img}],$$ \n$$x_{id}^{temp} = \\sum_{i=1}^{k_2} g'_i \\odot (W_{i}^{temp}e_{id}),$$ \n$$x_{txt}^{temp} = \\sum_{i=1}^{k_2} g'_i \\odot (W_{i}^{temp}e_{txt}),$$ \n$$x_{img}^{temp} = \\sum_{i=1}^{k_2} g'_i \\odot (W_{i}^{temp}e_{img}),$$\nwhere $W_{i}^{temp} \\in \\mathbb{R}^{1 \\times 3d}$ is the learnable weight of the $i$-th expert, $\\odot$ is element-wise multiply, and $g'_i$ represents the routing weight of the $i$-th expert. By this means, we can model user dynamic interest changes via two forms of time, interval and absolute time, to embed explicit temporal data into multi-modal learning.\nThrough the two-level hierarchical MoE, we can improve item key feature extraction by modality interactions and also introduce explicit temporal information to multi-modal learning for user dynamic interest modeling."}, {"title": "4.3 User Interest Learning and Prediction", "content": "To learn user evolving interest, we choose Transformer [43] to encode the information within behavior sequences effectively, which is widely applied in SR methods. Specifically, for the behavior sequence $S$ of the user $u$, we organize the outputs of all items from Temporal MoE as three types of sequence representations $S_{id} = \\{x_{id,1}^{temp}, x_{id,2}^{temp}, ..., x_{id,|S|}^{temp} \\}$, $S_{txt} = \\{x_{txt,1}^{temp}, x_{txt,2}^{temp}, ..., x_{txt,|S|}^{temp} \\}$, $S_{img} = \\{x_{img,1}^{temp}, x_{img,2}^{temp}, ..., x_{img,|S|}^{temp} \\}$, where $m \\in \\{id, txt, img\\}$ represents the specific modality. Then, these sequences are fed into their corresponding Transformer models as follows:\n$$S'_m = Dropout(LayerNorm(S_m)),$$\n$$H_m = \\text{Transformer}_m(S'_m),$$\nwhere $H_m$ is the final hidden vector corresponding to the last position as user interest under modality $m$. Next, to predict the interaction probability between the user $u$ and the item $v$, we calculate the score on each modality and then sum up these scores as the final prediction result $\\hat{y}_v$, which can be formulated as below:\n$$\\hat{y}_v = H_{id} \\cdot x_{id} + H_{txt} \\cdot x_{txt} + H_{img} \\cdot x_{img} .$$"}, {"title": "4.4 Multi-Task Learning", "content": "To alleviate information loss owing to data sparsity in multi-modal SR, we design the multi-task training strategy besides the main SR task to provide multi-faceted and informative supervision signals as complements. First, given a user behavior sequence $S$, the main target of SR is to predict which item would this user be interested in at the $(n+1)$-th time step. Through the proposed network above, we get the prediction score $\\hat{y}_v$, which is the possibility that this user would interact with $u$ at the next time step. Then the cross-entropy loss function is utilized as the main training objective to measure the differences between our prediction and the ground truth $y_0$:\n$$L_{main} = -\\sum_{v \\in V} y_0 \\log (\\hat{y}_v).$$\nNext, considering that item category information is helpful for item feature understanding [46, 47, 65], we design the sequence-level category prediction task (CP) to provide category signals on multi-modal learning. To be specific, we take the categories of items as classification labels to supervise the model to distinguish which classes these items belong to based on all modality information. To generate signals more related to user sequence, we compute this task at the sequence level. For the item $v \\in S$, we employ the initial representations of $v$ to calculate its probability to belong to each class, which is formulated as:\n$$\\hat{y}_{CP} = W_{CP}[x_{id}; x_{txt}; x_{img}] + b_{CP},$$\nwhere $W_{CP} \\in \\mathbb{R}^{3d \\times |C|}$ and $b_{Cp} \\in \\mathbb{R}^{|C|}$ are the learnable weight and bias for CP, $C$ is the set of the categories. Let $\\hat{y}^{CP}_c$ denote the likelihood that $v$ belongs to $c \\in C$, then the binary cross entropy loss is utilized to facilitate multi-label category information [15, 55]:\n$$L_{CP} (v) = - \\sum_{c \\in C} [y_c^{CP} \\log (\\hat{y}_c^{CP}) + (1-y_c^{CP}) \\log (1 - \\hat{y}_c^{CP})],$$\nwhere $y_c^{CP}$ is the ground truth. Then the loss of the CP task on $S$ is $L_{CP} = \\sum_{v \\in S} L_{CP} (v)$, which provides sequence-level category signals to enhance multi-modal learning.\nTo obtain supervision signals from user sequence context and user interests, we also design the objective of contrastive learning on ID (IDCL) to improve alignments between user sequence representations and the ground truth. Since the embeddings of text and images are frozen in the training process, we only utilize this task on ID. Specifically, within a training batch $B$, we organize the representations of users and their ground truth items on ID as $\\{(H^u_{id}, x^u_{id}), (H^u_{id}, x^u_{id}), ..., (H^u_{id}, x^u_{id})\\}$. We aim to make the vectors of user sequences and their ground truth be more similar, while forcing this user to be dissimilar with other items. Therefore, the loss of IDCL is formulated as:\n$$L_{IDCL} = -log \\frac{\\sum_{i \\in B} \\exp (\\text{sim} (H^u_{id}, x^u_{id}) / \\tau)}{\\sum_{i' \\in B} \\sum_{j \\in B} \\exp (\\text{sim} (H^u_{id}, x^u_{id}) / \\tau)},$$\nwhere sim(,) is the cosine similarity function, and $\\tau$ denotes the temperature parameter.\nLast, to deepen correlation learning between explicit temporal information and multi-modal data, we further propose a sequence-level augmentation contrastive learning method, called Placeholder Contrastive Learning (PCL). In particular, for the sequence $S = \\{v_1, v_2, ..., v_{|S|} \\}$, we randomly replace items with the Time Placeholder \"[TP]\" with the proportion $\\beta$ to get the augmented sequence. Next, to integrate explicit temporal information to multi-modal data, for the representation sequence $S_m = \\{x_{m,1}^{temp}, x_{m,2}^{temp}, ..., x_{m,|S|}^{temp} \\}$ on the modality $m$ after Temporal MoE, if the $i$-th item is replaced by \"[TP]\" in $S'$, then $x_{m,i}^{temp}$ will be replaced with the operation below:\n$$x_{m,i}^{temp} \\leftarrow W_{P,m}[r_{1,i}; r_{2,i}] + b_{P,m}$$\nwhere $r_{1,i}$ and $r_{2,i}$ are the $i$-th time embeddings from Temporal MoE, and $W_{P,m} \\in \\mathbb{R}^{2d \\times d}$ and $b_{P,m} \\in \\mathbb{R}^d$ are trainable parameters. After that, the augmented sequence $S'_m$ is inputted into $\\text{Transformer}_m$, and we can get the augmented representation $H'_{m,i}$ corresponding to $H_m$. Subsequently, within a training batch $B$, we can obtain $\\{H_{m,1}^u, H_{m,1}^u,..., H_{m,1}^u\\}$ from $\\{H_m^u, H_m^u,...,H_m^u \\}$, and we treat $(H_m^u, H_{m,1}^u)$ as the positive pair, while $\\{(H_m^u, H_{m,1}^u)|i \\neq j\\}$ are regarded as negative pairs. We aim to enhance consistency between positive sequence pairs while make negative pairs less similar, thus PCL is formulated as follows:\n$$L_{PCL} = -log \\frac{\\exp (\\text{sim} (H_m^u, H_{m,1}^u))}{\\sum_{j=1}^{|B|^2} \\exp (\\text{sim} (H_m^u, H_{m,1}^u))}.$$\nThen the total loss of PCL is added up as:\n$$L_{PCL} = (L_{PCL} + L_{PCL})/2.$$\nFinally, we sum up these auxiliary training tasks with the main recommendation task to enhance the learning quality of our model:\n$$L = L_{main} + \\lambda_1L_{CP} + \\lambda_2L_{IDCL} + \\lambda_3L_{PCL},$$\nwhere $\\lambda_1, \\lambda_2$ and $\\lambda_3$ are weight hyper-parameters for CP, IDCL and PCL respectively. By above three tasks, we generate multi-faceted supervision signals to mitigate information loss from data sparsity."}, {"title": "5 Experiment", "content": "In this section, we introduce details of the experimental setup and compare HM4SR with state-of-the-art SR baselines. Then the ablation study and the hyper-parameter study are conducted to display the impact of designed modules and hyper-parameters on model performance. We further discuss the effect of time encoding methods for Temporal MoE. Last, we present a case study to show how explicit temporal information influences recommendation results."}, {"title": "5.1 Experimental Setup", "content": "5.  1.  1 Datasets. Four public datasets are chosen from Amazon Review Datasets [19] to evaluate SR models, including \"Toys and Games\" (Toys), \"Video Games\" (Games), \"Beauty\" and \"Home and Kitchen\" (Home). For each dataset, duplicated interactions are removed, and interactions of each user are sorted by timestamps chronologically to build behavior sequences. Following previous studies [22, 34, 50, 55], we filter out users and items that have fewer than five interactions to get the 5-core subset of each dataset. For text, we concatenate phrases from title, category and brand fields consistent with previous studies [22, 48]. For images, we directly download the first image of each item from provided product URLs. The statistics of the processed datasets are shown in Appendix A.1.\n5.  1.  2 Compared Methods. To verify the effectiveness of our method, we select the following representative and competitive baselines for sequential recommendation from three categories: (1) For traditional non-time-aware methods, GRU4Rec [20], SASRec [27] and LRURec [61] are included. (2) We compare traditional time-aware methods including TiSASRec [30], FEARec [16] and TiCoSeRec [11]. (3) HM4SR is also compared with multi-modal methods, including NOVA [35], DIF-SR [55], UniSRec [22], MISSRec [48], M3SRec [1], IISAN [18] and TedRec [57].\n6.  1.  3 Evaluating Metrics. Models are evaluated with Normalized Discounted Cumulative Gain (NDCG@K) and Mean Reciprocal Rank (MRR@K), where $K \\in \\{5, 10\\}$. We adopt the leave-one-out evaluation strategy to conduct the experiments. The ranking scores are computed on the whole item set without sampling.\n7.  1.  4 Implementation Details. We implement our method in Pytorch based on a widely used open-source library RecBole [64]. We set the training batch size as 1024, and the hidden size of all methods is 64. The maximum length of each behavior sequence is limited to 50. For the encoder structure, the number of multi-head and the number of self-attention layers for the Transformer are both empirically set as 2. For hyper-parameters, the expert number for Interactive MoE $k_1$ and Temporal MoE $k_2$ is selected from $\\{2, 4, 6, 8, 12\\}$, and $\\mu$ is set as 100. freq is empirically set as 10000. $\\tau$ is searched within $\\{0.1, 0.2, 0.3, 0.5, 0.7, 1.0\\}$. The loss weight $\\lambda_1$ is empirically set as 1.0, while $\\lambda_2$ and $\\lambda_3$ are tuned within [0.25, 1.5] stepping by 0.25 and $\\{0.1, 0.3, 0.5, 0.7, 1.0, 1.5\\}$ respectively. $\\beta$ is chosen from [0.1, 0.5] stepping by 0.1. In addition, we use Adam optimization and the learning rate is 1e-3. We apply grid search to find the best settings for HM4SR and baselines. All experiments are conducted on a server with Intel(R) Xeon(R) Gold 6226R 16-Core CPUs and NVIDIA GeForce RTX 3090 GPUs."}, {"title": "5.2 Overall Comparison", "content": "Table 1 shows the evaluation results of HM4SR and other compared methods. We have the following findings. (1) Overall, HM4SR outperforms both traditional and multi-modal approaches. It achieves improvements ranging from 2.13%~22.9% to the best multi-modal method. For the best time-aware traditional model, HM4SR gains relative improvements ranging from 13.6%~64.1%. (2) Multi-Modal methods outperform traditional models in most cases, indicating that modal information is important for richer user interest learning. Yet HM4SR performs the best among them, showing the advancement of incorporating explicit temporal information to multi-modal modeling for capturing user preference changes. (3) Time-aware approaches (including TedRec) generally perform better than non-time-aware traditional models. This implies that contextual information held in temporal information is useful for user dynamic interest learning. However, HM4SR displays further improvement with the utilization of both explicit temporal information and multi-modal data. (4) HM4SR improves relatively less on Games Dataset compared to other three datasets. A possible reason is that the time"}, {"title": "5.3 Ablation Study", "content": "To verify the effectiveness of each design for HM4SR, we compare our model with the following variants: (1) -IMoE removes the structure of Interactive MoE. (2) -TMoE removes Temporal MoE. Notably, $L_{PCL}$ is also removed simultaneously. (3) -CP removes the category prediction task. (4) -IDCL removes the objective of contrastive learning on ID. (5) -PCL removes the placeholder contrastive learning task. (6) -Text removes text modality. (7) -Image removes image modality. All the components and modalities contribute to the performance of HM4SR. We can also observe that Temporal MoE generally achieves the highest improvement among all components, indicating the crucial importance of the incorporation of explicit temporal information for multi-modal modeling. PCL further amplifies this enhancement by deepening correlation learning between modality and time embeddings. Interactive MoE is useful for preference learning by improving item key information extraction. CP and IDCL tasks also play a vital role in supervised signal generation. Notably, text information is usually more powerful than images since text portrays item features more directly."}, {"title": "5.4 Hyper-Parameter Study", "content": "In this part, we study the impact of important hyper-parameters, including the number of experts for Interactive MoE and Temporal MoE $k_1$ and $k_2$ respectively, the loss weights for IDCL and PCL $\\lambda_2$ and $\\lambda_3$, and the placeholder proportion $\\beta$. When changing one hyper-parameter, we keep other hyper-parameters fixed to control variables. The results demonstrate that reasonably setting them can effectively enhance the preference learning ability of HM4SR. Last, $\\beta$ is the proportion that items get replaced by temporal placeholders in PCL."}, {"title": "5.5 Impact of Temporal Embeddings", "content": "For a deeper analysis of the impact of explicit temporal information on multi-modal SR, we change the types of time embeddings inputted to Temporal MoE as five variants: Though all types of time embeddings can facilitate user preference learning, our time encoding method gets more advances."}, {"title": "5.6 Case Study", "content": "We select a typical case from the test set of Toys to expound the effect of designed components, is shown in Figure 6. We compare HM4SR with M3SRec, which demonstrates the effectiveness of our design towards multi-modal SR."}, {"title": "6 Conclusion", "content": "In this paper, we proposed a multi-modal sequential recommendation method called HM4SR, which improved the prediction accuracy by extracting item key information and introducing explicit temporal information to multi-modal learning."}, {"title": "A.2 Baseline Descriptions", "content": "We compare HM4SR with several representative and state-of-the-art methods. The descriptions for these baselines are as follows:\n\u2022 Traditional Non-Time-Aware Methods. GRU4Rec [20", "27": "adopt gated recurrent units and self-attention mechanisms respectively to learn user sequential interest. LRURec [61", "30": "designs a time interval-aware self-attention mechanism. FEARec [16", "11": "devises five time interval-related sequence-level augmentation methods to obtain uniform sequences and applies"}]}