{"title": "CPRM: A LLM-based Continual Pre-training Framework for Relevance Modeling in Commercial Search", "authors": ["Kaixin Wu", "Yixin Ji", "Zeyuan Chen", "Qiang Wang", "Cunxiang Wang", "Hong Liu", "Baijun Ji", "Jia Xu", "Zhongyi Liu", "Jinjie Gu", "Yuan Zhou", "Linjian Mo"], "abstract": "Relevance modeling between queries and items stands as a pivotal component in commercial search engines, directly affecting the user experience. Given the remarkable achievements of large language models (LLMs) in various natural language processing (NLP) tasks, LLM-based relevance modeling is gradually being adopted within industrial search systems. Nevertheless, foundational LLMs lack domain-specific knowledge and do not fully exploit the potential of in-context learning. Furthermore, structured item text remains underutilized, and there is a shortage in the supply of corresponding queries and background knowledge. We thereby propose CPRM (Continual Pre-training for Relevance Modeling), a framework designed for the continual pre-training of LLMs to address these issues. Our CPRM framework includes three modules: 1) employing both queries and multi-field item to jointly pre-train for enhancing domain knowledge, 2) applying in-context pre-training, a novel approach where LLMs are pre-trained on a sequence of related queries or items, and 3) conducting reading comprehension on items to produce associated domain knowledge and background information (e.g., generating summaries and corresponding queries) to further strengthen LLMs. Results on offline experiments and online A/B testing demonstrate that our model achieves convincing performance compared to strong baselines.", "sections": [{"title": "1 Introduction", "content": "Relevance modeling is designed to evaluate the correlation between queries and items, an essential component of commercial search engines and crucial for the user experience. Mini-app service search is a common search application scenario. Unlike traditional e-commerce searches that only provide product search functions, mini-app services encompasses numerous scenarios such as livelihoods, government affairs, transportation, healthcare and dining. Moreover, the item consists of structured data with multiple fields; for instance, a hospital mini-app might include fields like title, keywords, category and description. Considering the diverse scenes and the complexity of structured data across multiple fields, conducting relevance modeling within the such search scenario poses a significant challenge. The current relevance model in commercial search systems is a semantic matching model, leveraging LLMs combined with domain-annotated data through supervised fine-tuning (SFT) methods. These LLMs like GPT-3 (Brown et al., 2020), GLM (Du et al., 2022), LLaMA (Touvron et al., 2023a), Qwen (Bai et al., 2023; Yang et al., 2024), having more extensive parameters and utilizing a massive corpora of texts during training compared to previous pre-trained models such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019b) and XLNet (Yang et al., 2020), demonstrate superior performance in semantic matching tasks.\nDespite the great success of LLMs, there still remain certain limitations in their application to relevance modeling. Firstly, LLMs are pre-trained on a broad range of data sources (Brown et al., 2020; Du et al., 2022; Touvron et al., 2023a,b), which do not afford special attention to particular domains (Wu et al., 2023; Cui et al., 2023; Xiong et al., 2023), resulting in a lack of domain-specific knowledge. Besides, queries tend to be colloquial and present in short-text form, whereas items are typically expressed in a more formal long-text form, leading to a \u201csemantic gap\u201d (Lian et al., 2019; Qi et al., 2020; Kumar and Sarkar, 2021) between their representations. Secondly, the pre-training phase of LLMs is \u201ctask-agnostic\u201d (Brown et al., 2020), which impedes direct connection with downstream tasks and precludes the possibility of in-context pre-training enhancements tailored for these tasks (Min et al., 2022; Gu et al., 2023). Finally, the item tends to be highly structured and difficult to leverage, which prevents LLMs from fully realizing their potential with such data.\nTo address the above problems, we investigate a Continual Pre-training approach of LLMs for Relevance Modeling, CPRM for short. Initially, we introduce a pre-training technique using pairs of queries and multi-field item as inputs. This method enables the LLMs to explicitly model the semantic representations between queries and items, thus bridging the semantic gap between them. Subsequently, we collect sets of semantically similar queries and items based on user click logs, then further refine these sets through semantic modeling to filter out semantically irrelevant cases. Following that, we reorder these refined sets according to semantic similarity and ultimately construct in-context pre-training instances via prompting techniques. Utilizing this approach to data construction, LLMs are able to make better predictions within such contexts during the training process, which benefits the efficient learning current domain knowledge for LLMs. Lastly, we employ ChatGPT to conduct reading comprehension on structured item data, facilitating the generation of relevant domain knowledge for pre-training, which can be considered as a secondary development and exploitation of the item. More specifically, we leverage ChatGPT to summarize and paraphrase item to produce fluent domain knowledge, while also guiding ChatGPT to produce background information related to the items. Additionally, we prompt ChatGPT to create diverse queries and provide further reasons for their generation. In summary, the contributions of this paper are as follows:\n\u2022 To our knowledge, we are the first to systematically propose a continual pre-training approach of LLMs specifically designed for relevance modeling tasks.\n\u2022 We propose a CPRM framework with three components. Firstly, the joint pre-training of queries and multi-field item to enhance domain knowledge of LLMs. Secondly, in-context pre-training by constructing collections of semantically similar queries or items. And thirdly, reading comprehension of structured items employed to strengthen the capabilities of LLMs further.\n\u2022 Our approach has been validated on real-world industry data, outperforming strong baselines significantly in both offline experiments and online A/B testing."}, {"title": "2 Related Work", "content": "Relevance modeling in search corresponds to the semantic matching task in NLP. With the advancement of neural network and pre-trained models, deep semantic matching models have become mainstream. Deep semantic matching models are categorized into two types: representation-based (Shen et al., 2014; Palangi et al., 2015; Rao et al., 2019) and interaction-based methods (Chen et al., 2016; Hu et al., 2014; Pang et al., 2016; Parikh et al., 2016). The former focuses on learning low-dimensional representations, while the latter emphasizes capturing the interactions between inputs. The representation-based model with independently encoded inputs struggles to capture complex correlations, whereas interaction-based methods that concatenate the two inputs for semantic computation can alleviate this issue.\nIn recent years, pre-trained models like BERT (Devlin et al., 2019) has show its superiority on natural language understanding (NLU) tasks. Consequently, both representation-based and interaction-based methods have begun leveraging the capabilities of these pre-trained models for semantic modeling. Most recently, LLMs like GPT-3 (Brown et al., 2020), GLM (Du et al., 2022), LLaMA (Touvron et al., 2023a), Qwen (Bai et al., 2023; Yang et al., 2024) pre-trained extensive volumes of data with numerous parameters have garnered significant performance in language understanding, generation and reasoning tasks. Compared to traditional pre-trained models like BERT (Devlin et al., 2019), LLMs possess significant advantages in both the scale of pre-training data and the quantity of model parameters, leading to their evident superiority in performance across a variety of downstream tasks. Recent research work (Sun et al., 2023; Spatharioti et al., 2023; Zhu et al., 2024) indicates that combining LLMs with downstream applications presents significant potential, LLMs can achieve competitive or even superior results compared to traditional supervised methods on information retrieval benchmarks. As a result, we try to explore the application of LLM relevance modeling in search engines.\nLLMs are pre-trained on a wide variety of data sources (Brown et al., 2020; Du et al., 2022; Touvron et al., 2023a,b) without pay more attention on specific domains, resulting in a lack of domain knowledge. On the other hand, the pre-training phase of LLMs is task-agnostic (Brown"}, {"title": "3 Methodology", "content": "In this section, we present the details of the CPRM framework, including Domain Knowledge Enhancement (DKE), In-Context Pre-training (ICP) and Reading Comprehension Distillation (RCD)."}, {"title": "3.1 Domain Knowledge Enhancement (DKE)", "content": "Different from conventional pre-training methods, we jointly pre-train the structured item data with multiple queries as shown in Figure 1. Each item encompasses multiple fields, including title, keywords, category, description, etc., with the query being the most frequently searched top query for a given item. For each query or item, we employ segment embeddings to distinguish between different texts. For convenience, we add special tokens \u201c<lstartofpiece|>\u201d and \u201c<lendofpiece|>\" between the queries and item as segment embeddings to differentiate them. Furthermore, queries and item are combined for position encoding, thereby allowing LLMs to explicitly model the relationships between them during pre-training process. Due to constraints on response time for online services, when calculating relevance scores between queries and items, only a limited number of item fields (such as title and keywords) are considered. Consequently, domain knowledge from other unused item fields, such as description, can be incorporated through continual pre-training. Considering that relevance modeling is a NLU task, we adopt both token-level masked language modeling (MLM) (Devlin et al., 2018) and segment-level MLM pre-training strategies for LLMs. Therefore, the optimization objective is:\n$\\mathcal{L}(\\theta) = \\min_{\\theta} \\alpha \\mathcal{L}_{t-MLM}(\\theta) + (1-\\alpha)\\mathcal{L}_{s-MLM}(\\theta)$. (1)\nwhere $\\theta$ is the parameters of the model, $\\mathcal{L}_{t-MLM}(\\theta)$ and $\\mathcal{L}_{s-MLM}(\\theta)$ represent token-level MLM loss and segment-level MLM loss respectively, we set $\\alpha = 0.7$ in our experiments.\""}, {"title": "3.2 In-Context Pre-training (ICP)", "content": "We construct in-context pre-training instances using historical click logs from real-world business search scenario. The overall idea is to build collections of semantically similar queries and items as pre-training data to further stimulate in-context learning capabilities of LLMs. The detailed data construction methodology is as follows:\nCoarse Screening. Utilizing the click logs, we establish a mapping from Queries to Items (denoted as Q2I) and from Items to Queries (denoted as I2Q), sorting them by the number of clicks in descending order. Consequently, within the Q2I mapping, for a query $Query$ there is an associated set of items $I_{Query} = \\{I_1, I_2, ..., I_N \\}$. These items can be considered as a preliminarily semantically related collection under the specific constraint of query $Query$. Vice versa for I2Q mapping.\nFine Screening. Following described above, cases may be introduced that received clicks but are semantically unrelated. We employ Contriever (Izacard et al., 2022), a semantic model, to encode text into vectors, and then calculate the similarity between various text representations for semantic filtering. For set $I_{Query}$, when the following condition is met:\n$Sim(Query, I_k) < \\sigma$,   $k \\in [1, N]$ (2)\nit signifies that $Query$ and $I_k$ are semantically unrelated and require filtering, where $Sim(\\cdot)$ is similarity function, $\\sigma$ is a threshold.\nData Construction. As shown in the left of Figure 2, we subsequently obtain a collection of items that are semantically relevant to the query, then sort these items by semantic similarity in ascending order. Finally, we concatenate the query with the sorted collection of items to create an ICP instance via prompting. The right of Figure 2 illustrates how to construct ICP instances under I2Q mapping, namely, obtaining a set of semantically related queries given the constraint of an item.\nWhy adopt this construction manner? By assembling collections of items under a specified query or collections of queries under a specified item, LLMs can make better predictions based on the context during the pre-training process, enabling more efficient learning within the current domain. Moreover, the reordering in ICP instances implicitly indicates the strength of relevance between queries and items, enabling LLMs to model the degree of their association effectively. On the other hand, by linking queries and items in our ICP instances, we enable LLMs to model their semantic representations directly."}, {"title": "3.3 Reading Comprehension Distillation (RCD)", "content": "We employ ChatGPT for reading comprehension on items, with the prompt design shown in Figure 3. Assume that in a mini-app search scenario, we need to provide the mini-app's structured information like title, keywords, category, and description, and utilizing prompt template instructions to invoke ChatGPT. This generates the reading comprehension pre-training instances. Why design the prompt in this way? We have several reasons for this design choice. Firstly, item text is structured data and difficult to utilize, lacking in relevant background knowledge. Through summarizing and rephrasing with Prompt 1, fluent domain knowledge can be generated. Additionally, understanding and analyzing items can instruct ChatGPT in generating relevant background knowledge. Secondly, by using Prompt 2 and Prompt 3 guide ChatGPT to generate related and diversified queries, enriching the supply of suitable queries. We also instruct ChatGPT to provide further explanations for the generated queries. This approach not only facilitates the generation of relevant domain knowledge but also allows downstream models to significantly improve their understanding and handling of the item when utilizing these data. Employing ChatGPT for reading comprehension on items can be considered a secondary development and utilization of item data, enriching the domain knowledge further. Pre-training LLMs on the above data can also be seen as a process of knowledge transfer from ChatGPT to LLMs."}, {"title": "4 Experiments", "content": "4.1 Experimental Settings\nPre-training Dataset. We utilize the real-world mini-app search scenario data for verification. The pre-training data includes three parts: DKE data (4M), ICP data (4M) and RCD data (500K). The first part is sampled from the mini-app search scenarios and consist of structured items containing multiple fields. For top 500K most frequently visited items, we sample 5 top queries based on click logs for each item, which are then concatenated with multi-field item to serve as pre-training examples for adapting relevance tasks. The second part is in-context pre-training data, where we construct these examples based on the real-world search click logs using the method described in Section 3.2, and subsequently randomly sample 4M from them. The third part is reading comprehension data, for which we utilize ChatGPT to perform reading comprehension on item data.\nRelevance Dataset. The SFT data consists of three parts: train set (625K), valid set (35K) and test set (35K). These data are sourced from real mini-app search results and then are generated through manual annotation. The human-annotated data for relevance tasks are in format of triples <Query, Item, Label>, the data statistics as shown in Table 1. The annotated data have only two levels of relevance, #Related and #Not_Related denote label 1 and 0 respectively.\nEvaluation Metrics. We employ three widely used metrics Acc., F1 and AUC to evaluate model performance, with higher values indicating better performance. Note that AUC serves as the most important metric in relevance tasks while the others provide auxiliary supports for our analysis.\nImplementation Details. All our pre-training experiments are conducted on the GLM-2B model. The model configuration set to 36 layers, hidden size of 2048, FFN size of 8192, and 32 attention heads. We utilize adam (Kingma and Ba, 2017) optimizer and the warmup steps and learning rate set 28K and 1e-4. All models are pre-trained on 8 A100 GPUs for 2 epochs and the batch size set 64. During SFT, all models are trained for 5 epochs on 8 A100 GPUs and the batch size is 8. The adam optimizer is employed and warmup steps and learning rate set to 5K and 2e-5 respectively. When constructing the ICP instances, we utilize facebook's open-source multilingual Contriever\u00b9 (Izacard et al., 2022) model for semantic filtering. This model encodes text inputs into a 768-dimensional vector, and then calculates cosine similarity based on this vector."}, {"title": "4.2 Baselines", "content": "We compare our proposed CPRM model with the following baselines:\n\u2022 DSSM (Shen et al., 2014) is a classic two-tower structure text matching model that constructs representations for the query and item independently, using cosine similarity to measure their relevance.\n\u2022 ReprBERT (Yao et al., 2022) is a representation-based BERT model that utilizes novel interaction strategies to balance performance and latency.\n\u2022 BERT (Devlin et al., 2018) has achieved great success on NLP tasks as an interaction-based model. Here, we concatenate the query and item as the model input for relevance modeling.\n\u2022 GLM (Du et al., 2022) is a powerful LLM architecture with various parameter sizes to suit different business scenarios. Our LLM online system is developed based on the GLM, thus all our experiments are mainly conducted on the GLM.\n\u2022 Qwen2 (Yang et al., 2024) is currently one of the newest and the state-of-the-art (SOTA) open-source LLMs for Chinese NLP tasks.\n\u2022 ChatGPT & GPT-4 (OpenAI, 2024) are the SOTA closed-source LLMs. They cannot perform SFT and employ the direct generation approach for relevance task evaluation."}, {"title": "4.3 Offline Experimental Results", "content": "Performance Comparison. Table 2 presents the performance of different baselines and various continual pre-training models on the relevance task. From the experimental results, GLM demonstrates strong competitiveness, achieving superior performance even with similar parameter numbers compared to BERT-Large (line 7 vs. line 4). We also conducts experiments on the GLM of various parameter sizes, and the results show that as the model size increases, its performance gradually improves. However, with further increases in model size, the performance gains become progressively smaller. Specifically, GLM-10B achieves only a 0.21% improvement in AUC over GLM-5B (line 10 vs. line 9). Compared to other latest LLMs such as Qwen2, our GLM also shows impressive performance at a similar parameter scale (line 7 vs. line 5, line 8 vs. line 4). ChatGPT and GPT-4 performed poorly compared to other SFT-based baseline systems; this is because these models are more suited for NLG tasks and are not capable of supporting SFT. Additionally, the task data is biased towards a specific domain. We conducts continued pre-training experiments on GLM-2B, and the experimental results demonstrate that all three different methods result in performance improvements compared to the baseline model. Notably, the DKE and ICP methods achieve significant performance enhancements, with respective gains of 1.11% and 1.54% in AUC. This is because both methods are constructed based on domain-specific data and jointly training semantically related queries or items can further enhance model performance. The experimental results also indicate that integrating different continued pre-training methods can further strength model performance (line 17 and line 18), with the combination of all three methods leading to the greatest performance gain, making it comparable to that of GLM-10B (line 18 vs. line 10). Our CPRM model achieves the highest F1 score (94.42%) among all models compared.\nAnalysis on Query Length. We compare the performance of different models at various query lengths on test set. As shown in Figure 4, our CPRM model outperforms the baseline model across all length intervals, especially on longer queries (when the length greater than 15), where the CPRM model demonstrates a significant improvement performance gains, surpassing the baseline model by 15.85% in AUC (92.27% vs. 76.42%). This suggests that the CPRM model possesses a superior ability to understand and deal with long queries. We speculate that this advantage may be attributable to the ICP and RCD methods. Since the ICP method semantically aggregates historical search queries, allowing the model have the possibility encountered related long queries and to understand their semantics in the in-context pre-training process. On the other hand, the RCD method generates diverse queries, thereby enriching the model's understanding of various long query types.\nImpact of Training Steps. As shown Figure 5, we compare models' performance with different pre-training methods across various training steps. The experimental results show that models trained with all three different pre-training methods surpasse the baseline across various training steps. The CPRM model, which combines all three methods, achieves the best performance at each step. These evidence highlights the robustness of our proposed approach. Interestingly, an phenomenon observed from the figure is that the baseline model's performance significantly decreases at the 16K training step before it gradually increases thereafter. The reason is due to the significant difference between the current task data and the data previously seen by the LLM, resulting in challenges for the LLM in fitting this domain-specific data. None of the other pre-training methods exhibits this phenomenon; instead, the performance of these models steadily improved at each training step. This indicates that our proposed methods are beneficial for the domain adaptation of LLM."}, {"title": "5 Model Deployment", "content": "LLMs have achieved significant performance improvements in relevance tasks, but their large parameter size leads to low inference efficiency, thus affecting their deployment online. We have designed a solution that allows for the real-time use of LLMs' relevance scores. As shown in Figure 6, the online relevance model for search consists of two parts: the GLM-0.3B model serves as the online model to respond to search queries in real-time, while the GLM-2B model employ a T+1 update strategy to score historical Q-I pairs and cache them offline. The online relevance service gives priority to using the cached scores from GLM-2B; if the cache does not exist, it calls on the GLM-0.3B online model. Currently, using GLM-2B's offline caching scoring has covered over 60% of mini-app search requests, significantly alleviating the request pressure on the online model."}, {"title": "6 Online A/B Testing", "content": "We deploy the proposed model on the online search platform to provide search services for mini-apps, and conduct a two-week online A/B testing with 5% proportion of the experiment traffic. The experimental results show that, compared to the baseline system (GLM-2B), our CPRM method yields a statistically significant increase of 0.32% in valid PVCTR2 at a 95% confidence level. Human evaluation indicates a 0.75% reduction in Badcase@10 metric and a 4.71% decrease in the Error Filtering Rate\u00b3. The model has now been serving search functions to mini-apps for over nine months. These results suggest that our proposed method can effectively enhance relevance models' performance in real-world search systems."}, {"title": "7 Conclusion", "content": "In this paper, we have investigated CPRM framework, a continued pre-training approach of LLMs tailored to relevance modeling tasks, which comprises three methods: DKE, ICP and RCD. Both offline experiments and online A/B testing results demonstrate that our proposed method boosts the search relevance of LLMs effectively. Our model has been successfully deployed online search platform."}]}