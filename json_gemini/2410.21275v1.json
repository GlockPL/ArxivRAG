{"title": "Enhancing Action Recognition by Leveraging the Hierarchical Structure of Actions and Textual Context", "authors": ["Manuel Benavent-Lledo", "David Mulero-P\u00e9rez", "David Ortiz-Perez", "Jose Garcia-Rodriguez", "Antonis Argyros"], "abstract": "The sequential execution of actions and their hierarchical structure consisting of different levels of abstraction, provide features that remain unexplored in the task of action recognition. In this study, we present a novel approach to improve action recognition by exploiting the hierarchical organization of actions and by incorporating contextualized textual information, including location and prior actions to reflect the sequential context. To achieve this goal, we introduce a novel transformer architecture tailored for action recognition that utilizes both visual and textual features. Visual features are obtained from RGB and optical flow data, while text embeddings represent contextual information. Furthermore, we define a joint loss function to simultaneously train the model for both coarse and fine-grained action recognition, thereby exploiting the hierarchical nature of actions. To demonstrate the effectiveness of our method, we extend the Toyota Smarthome Untrimmed (TSU) dataset to introduce action hierarchies, introducing the Hierarchical TSU dataset. We also conduct an ablation study to assess the impact of different methods for integrating contextual and hierarchical data on action recognition performance. Results show that the proposed approach outperforms pre-trained SOTA methods when trained with the same hyperparameters. Moreover, they also show a 17.12% improvement in top-1 accuracy over the equivalent", "sections": [{"title": "1. Introduction", "content": "Action recognition has become a fundamental task in a wide range of real-world applications. It involves the understanding of actions occurring within video sequences, allowing for diverse applications in everyday scenarios such as video surveillance [1], autonomous driving [2, 3], and human-robot interaction [4].\nIn some of these cases, the effectiveness of the provided solutions depends on their ability to operate online and in real-time. However, in several other cases (e.g. routine analysis [5, 6]), offline analysis of behavior is sufficient. In such cases, the consideration of the broader context provided by the entire video, may lead to improved model performance.\nIn the offline setting, action recognition and action detection emerge as the primary tasks in analyzing trimmed and untrimmed videos, respectively. Action recognition involves categorizing videos into predefined classes [7, 8], whereas action detection extends this by identifying the temporal boundaries of each action within the video [9, 10]. While the former pursues a straightforward goal, the latter benefits from the sequential nature of actions to enrich the temporal context, thereby reinforcing the robustness of the model.\nThe definition of the different action categories is typically left to the discretion of dataset authors, resulting in varying levels of abstraction. For example, the task of making coffee may be decomposed into several steps involving different objects: open coffee maker, place filter, etc. While the coarse granularity is sufficient in some cases, certain applications may require additional refinement to achieve specific goals. For example, robotic applications often need more detailed annotations, including per-joint instructions.\nThis work presents an action recognition architecture that exploits both the hierarchical structure and the sequential execution of actions. To this end, we adopt a transformer encoder built upon state-of-the-art methods [11, 12, 13] that use this architecture to model temporal dependencies. By exploiting the self-attention mechanism inherent in transformer architectures,"}, {"title": "2. Related Work", "content": "We present an overview of pertinent literature concerning action recognition, focusing on the integration of language models and hierarchical structures of actions as tools to enhance performance."}, {"title": "2.1. Action Recognition", "content": "Action recognition involves the classification of the action class performed within a video clip. Over time, various methodologies have been devised, with 2D CNN methods relying on single-frame human-object interaction being deemed the least effective due to their lack of temporal context [31]. Temporal modeling stands as a crucial aspect of action recognition. While recurrent neural networks and 3D CNN based methods held sway for several years [14, 15, 16, 17, 18, 19, 32], the advent of video transformers [11, 12, 13] has significantly enhanced temporal modeling capabilities.\nTwo primary categories of architectures emerge. The first one comprises end-to-end models such as ViVit [33] which uses a pure transformer architecture for video classification inspired by the advances in the image domain. Embeddings are extracted as non-overlapping tubelets that span both the spatial and temporal dimensions. In TimeSformer [34], a convolution-free approach was presented with a detailed study on self-attention schemes. Results from this study suggest that divided attention for spatial and temporal features leads to the best performance. Video Swin [35] explores the inductive bias of locality in video transformers by adapting the Swin transformer designed for the image domain [36]. MViT [37] employs multiscale feature hierarchies with a pyramid of feature activations, allowing effective modeling of simple low-level, and complex high-level visual information. MViTv2 [38] enhances its predecessor with decomposed relative positional embeddings and residual pooling connections.\nSecond, temporal modeling transformers that use pre-trained feature extractors from large datasets such as Kinetics-400 [14] and ImageNet [39]. OadTR [11] focus on temporal modeling using decoded RGB frames and frozen frame-level feature extractors. In addition, optical flow is computed from the RGB data to improve accuracy. Similarly, TIM [40] uses frozen visual and audio encoders for feature extraction. Features include a timestamp provided by a Time Interval MLP, so that the model can be queried about the events at a given interval in a specific modality. Other approaches incorporate the vision transformer, ViT [41], to fine-tune the feature extractor on the corresponding datasets [7, 8, 42], yielding better results with a remarkable increase in terms of computing cost.\nFeature extractors usually consist of 2D or 3D CNNs such as ResNet [43], InceptionV3 [44] or I3D [14]. On the contrary, MM-Vit [13] diverges by operating on multimodal features extracted from compressed videos, including I-frames, motion vectors and audio features. Similar strategies are observed in [45, 46, 47]."}, {"title": "2.2. Language Models for Action Recognition", "content": "The remarkable capabilities of large language models in temporal modeling and feature representation have been thoroughly investigated [21, 22, 23]. These capabilities have found application in various vision tasks, notably in image and video captioning. While these approaches focus on generating textual descriptions of videos [48], recent research has explored the use of language models to improve action recognition results. For example, ActionCLIP [7] uses a contrastive learning approach inspired by CLIP [49] to improve action recognition. Instead of image captions, action classes are converted to prompts, which are then compared to the aggregated representation of a video. Spatial features from frames are extracted from a fine-tuned version of CLIP's visual encoder, based on ViT [41]. Similarly,"}, {"title": "2.3. Action Hierarchies", "content": "While current action recognition techniques have made significant progress, they often fall short in segmenting actions into distinct phases, which is required for many real-world applications. To address this gap, the authors of the FineGym dataset [56] introduced a sports video dataset with a three-level semantic hierarchy. Previous research [26, 27] has explored this dataset using the premise that hyperbolic geometry inherently encodes hierarchical structures. Using the Poincar\u00e9 ball, these studies define a distance metric between predictions and observations, where points closer to the center of the ball represent abstract embeddings, while those near the edge denote specific ones. Essentially, edge proximity indicates higher model confidence.\nIn addition, Timeception [57] redefines the notion of activity, restricting it to \"complex actions\" characterized by: (1) composition - consisting of several simpler actions, (2) temporal order of these actions, and (3) extent - recognizing the variability in temporal length between actions. By introducing the Timeception layer, the architecture tolerates both long-range temporal dependencies and variations in the temporal extent."}, {"title": "3. Methodology", "content": "We present the proposed method for action recognition using hierarchical action structures and textual context."}, {"title": "3.1. Video Encoder", "content": "We adopt a standard Transformer's encoder to model temporal dependencies as in [11]. Given a trimmed video $V = \\{f_t\\}_{t=-r}$ associated with a coarse-grained action, and one or more fine-grained actions, the frames $f_t$ are grouped into blocks of size $B$, resulting in $(T + 1)/B$ chunks. For each block, we use a frozen feature extractor to obtain spatial features from RGB and optical flow frames, and employ separate video encoders with the same structure for each modality. We use the central frame of the block for RGB features, and the $B$ frames sequence for optical flow features. These features are mapped into a $D$ dimensional feature space, which we can formalize as $F = \\{O_t\\}_{t=0}^{T} \\in R^{(T+1)\\times D}$, where $O_t$ represents the output token of the transformer encoder at time $t$.\nWe extend the feature vector with an additional learnable class token with $O_{CLS} \\in R^D$. This token is used to learn the aggregated representation of the entire input, and is combined with the feature sequence as $F = Stack(\\{O_t\\}_{t=0}^{T}, O_{CLS}) \\in R^{(T+2)\\times D}$. Alternatively, the aggregated representation of the input can be obtained as the mean of the tokens in the input sequence as in [59]."}, {"title": "3.2. Contextualized Textual Information", "content": "Large language models have shown exceptional abilities in temporal modeling and understanding context. To leverage these capabilities, we propose an approach that models past information to enhance the contextualization of current action recognition. Specifically, given the location and $N$ previous actions, we use a prompt (as illustrated in Figure 1) to provide context about prior events, thus modeling longer temporal dependencies. From the generated sentences, we extract feature vectors using DistilBERT [25], a distilled variant of BERT [23], which employs deep bidirectional transformer encoders for text comprehension. As for the video encoder, we utilize the class token to represent the entire input sequence, denoted as $m_{Txt}$.\nIn addition to DistilBERT, we compare its performance against BERT and explore the use of LLMs for rephrasing to increase variability and enrich the data. Section 5 presents results for different setups, including no rephrasing, and rephrasing using GPT-3.5 [21] and Llama 3 [24], as well as an analysis of the optimal number of past actions and the impact of location information on contextualization."}, {"title": "3.2.1. Exploiting Hierarchical Information from Context", "content": "Contextual information derived from past actions and location provides a general understanding of the ongoing action, effectively yielding a coarse-grained representation. Likewise, this coarse-grained action can serve as supplementary supervision, enhancing fine-grained action recognition performance. To leverage this relationship, we introduce a coarse-grained classifier, which, in conjunction with a joint loss function, improves the accuracy of fine-grained action recognition."}, {"title": "3.2.2. Fusion of Visual and Textual Embeddings", "content": "In addition to their capabilities in temporal aggregation, transformer architectures excel at fusing features from different modalities [60]. To leverage this advantage, we employ a transformer encoder to combine visual and textual embeddings. Specifically, we concatenate RGB and optical flow features, as this approach has shown improved performance compared to feeding them separately into the fusion transformer. The encoder layers are structured similarly to the video encoder, but without the class token or positional encoding. Formally,\n$m_{VIS} = m_{RGB} + m_{Flow},\\\\$\n$m_{M} = FusionTransformer(m_{VIS}, m_{Txt}),$"}, {"title": "3.3. Training", "content": "The model is trained on a dual classification objective. First, a fine-grained classifier uses $m_{Fus}$ as input. Second, a coarse-grained classifier leverages contextual information from $m_{Txt}$. The resulting logits from both classifiers are used for multi-class classification, given the existing overlaps between composite actions, as discussed in the next section.\nThe supervision of both fine-grained and coarse-grained actions is performed through a joint loss function, defined as:\n$\\mathcal{L} = BCE(\\sigma(z_{coarse}), y_{coarse}) + BCE(\\sigma(z_{fine}), y_{fine}),$"}, {"title": "4. Experiments", "content": "We now present the experimental setup on the Hierarchical TSU dataset. The results demonstrate the effectiveness of the proposed method compared to the equivalent visual-only approach, and state-of-the-art methods."}, {"title": "4.1. Hierarchical TSU Dataset", "content": "The experimental framework in this work is built upon the Hierarchical TSU dataset, a hierarchical dataset for activities of daily living, which extends the Toyota Smarthome Untrimmed (TSU) dataset [30]. The proposed dataset emphasizes the use of contextual information for action recognition,"}, {"title": "4.2. Experimental Setup", "content": "Evaluation: We evaluate the results of our experiments on top-k accuracy following previous work on action recognition [7, 8, 61]. Top-k accuracy measures how often the correct action label appears among the top-k labels predicted by the model. We use the cross-subject evaluation introduced in the TSU dataset [30], which uses 11 subjects for training and the remaining 7 for testing. Unless specified otherwise, in all tables, the best performance is highlighted in bold, and the best performance within each subgroup, if applicable, is underlined.\nImplementation details: After exhaustive ablation experiments to determine the best configuration, we adopt ViT-H [41] with a patch size of 14 as RGB feature extractor and Inception v3 [44] from the two-stream network TSN [62] for optical flow features. TLV1 algorithm is used to compute optical flow frames. To extract embedding vectors from textual data we use DistilBERT [25].\nThe video encoder is composed of 4 attention layers with a single attention head, and an embedding size of 2048. The input vector is supplemented with learned position encoding and a CLS token. The resulting RGB and optical flow video embeddings are concatenated, and this visual representation is fused with text embeddings using the fusion transformer composed of 2 encoder layers, with 2 attention heads per layer and an embedding size of 768. The best performance is obtained using 32 blocks as input, which corresponds to 6.4 seconds of video. The prompt for contextual information contains 5 past actions along with location data.\nTraining details: The proposed method is implemented using PyTorch, and all experiments are conducted on Nvidia RTX 4090 GPUs. We employ AdamW optimizer with a weight decay of 0.1 and a base learning rate of 5\u00d710-5. Models are trained for 100 epochs with a batch size of 32, employing an early stopping mechanism with a patience of 20 epochs. We use gradient clipping to avoid exploding gradients and warm up the learning rate for 5 epochs. We set the block size to 5, which implies a downsampling rate of 5 on the 25 fps videos from the TSU dataset."}, {"title": "4.3. Results", "content": "To determine the best combination of visual input with contextual data that provides the best top-k accuracy, Table 1 contains the results for the best settings obtained after extensive ablation experiments (see Section 5). For each experiment, we evaluate using both a fine-grained only training objective (-), and a dual training objective with the proposed joint loss (\u221a). The results lead to the conclusion that the proposed method outperforms the standard approach relying only on visual inputs. Moreover, for every input combination it is observed that predicting both fine- and coarse-grained actions results in better performance. Note that the observed decrease in top-5 accuracy in some cases is primarily due to the model's high confidence in the top classes.\nImpact of optical flow on video analytics: Optical flow is considered an auxiliary modality that complements RGB data, providing a notable performance boost of 1.97% when used in combination with RGB, and even out-performing in the uni-modal approach (1.26%). Although, when combined with contextual information, its effectiveness diminishes, underperforming compared to the corresponding RGB-based method. When all three modalities (RGB, optical flow, and contextual information) are combined, optical"}, {"title": "4.4. Comparison with State-of-the-art Methods", "content": "To evaluate the reliability and robustness of our proposal, we compare our method with existing state-of-the-art architectures for action recognition. To this end, we train different models initialized to pre-trained Kinetics 400 [14]"}, {"title": "5. Ablation Experiments", "content": "We present an ablation study to evaluate the contributions of various components of the proposed method. In addition to identifying the optimal RGB feature extractor and determining the best configuration values for the transformers used in the architecture, we compare different methodologies for incorporating hierarchical information. We also conduct an ablation analysis on the influence of the number of past actions and location data within the contextual information. Furthermore, we experiment with GPT-3.5 and"}, {"title": "5.1. Contextual Data Analysis", "content": "The best performing model considers the last 5 actions and the location provided by fixed cameras as contextual data. Text embeddings are obtained using DistilBERT [25] for feature extraction and the prompt used to generate the descriptions is a fixed template. Table 4 compares the use of DistilBERT and BERT as feature extractors as well as the use of rephrasing to enhance textual descriptions exploiting two well known large language models: GPT3.5 and Llama 3.\nResults indicate that DistilBERT outperforms its predecessor for feature"}, {"title": "5.2. Strategies for Hierarchical Action Recognition", "content": "We compare various strategies to integrate fine-grained and coarse-grained action recognition into a joint learning scheme. Table 6 contains the results using 4 different mechanisms depending on the features used to disambiguate fine- and coarse-grained information. Specifically, (1) using only contextual information for coarse-grained action recognition. Another set of options is to use both visual and contextual information from the vision transformer, by (2) using two separate fusion transformers and (3) using two separate classifiers from the same fused embeddings. Another option (4) is to share partial layers of the classifier, separating only the last layers for classification.\nResults indicate that the degree of differentiation between fine-grained and coarse-grained features significantly affects performance. Specifically, using only contextual information for coarse-grained action recognition (1) achieves the best performance, with a Top-1 accuracy of 54.95% for fine-grained actions and competitive results for coarse-grained actions. This confirms that contextual data alone is optimal for coarse-grained recognition and enhances fine-grained performance."}, {"title": "5.3. Data Fusion", "content": "Effective fusion of data modalities is crucial for achieving strong model performance. To this end, we evaluate three different fusion strategies and conduct an ablation study on the most effective one: the fusion transformer. RGB and optical flow features are obtained separately, based on the finding that late fusion is the best strategy for the video encoder, as discussed in the next section. We assess the impact of three approaches: (1) concatenating all three modalities (RGB, optical flow, and text), (2) concatenating the visual modalities (RGB and optical flow) prior to the fusion transformer, and (3) inputting the three modalities separately. Table 7 contains the results, with visual concatenation prior to the fusion transformer performing best on top-1 accuracy.\nSince the fusion transformer outperforms the concatenation approach, we conduct an ablation study on the number of attention heads and encoder layers, while keeping the embedding size fixed at 768, which corresponds to the text modality. Table 8 presents the results for different numbers of encoder layers, with two layers yielding the best performance. Similarly, Table 9 shows the results for varying numbers of attention heads, with two heads being the optimal configuration for our method."}, {"title": "5.4. Video Transformer", "content": "We determine the optimal configuration for the video encoder by evaluating several factors: input length, RGB feature extractor, number of layers and attention heads, fusion strategy, embedding size, and the use of positional encoding and CLS tokens.\nInput Length: In theory, longer input sequences should improve action recognition accuracy, provided there is no data loss due to excessively long sequences. Table 10 shows the results of our experiments with different sequence lengths. Our findings indicate that an input length of 6.4 seconds (32 input blocks) yields the best performance. Longer sequences tend to miss specific actions within the dataset, while shorter sequences lack the temporal context needed for accurate recognition.\nRGB Feature Extractor: Since transformer models for feature extraction are typically trained on static images rather than videos, they may encounter challenges such as motion blur caused by moving objects or people. To address this, we experiment with ViT-H/14 [41] as well as RGB feature extractors from the two-stream network TSN [62], both using pre-trained weights from the Kinetics dataset [14]. Specifically, we evaluate Inception v3 [44] and BN-Inception [65]. As shown in Table 11, despite being trained on static images, the transformer-based method achieves the best accuracy.\nPositional Information: As pointed out in Section 3, extracted frame"}, {"title": "6. Conclusion", "content": "In this paper, we present a novel approach for action recognition by leveraging the capabilities of language models to generate contextualized representations of previously performed actions and their locations. This enables us to capture the sequential and contextual properties of actions effectively. Additionally, we introduce a joint loss function for both coarse-grained and fine-grained action recognition, benefiting from the hierarchical structure of actions. Our method is built on a transformer encoder for video feature extraction, which incorporates a class token for enhanced representation and positional encoding to preserve the temporal order of input video blocks. We fuse text and video features using a transformer encoder, trained with a dual objective to compute the joint loss.\nFor our experiments, we extend the TSU dataset by introducing the Hierarchical TSU dataset, which focuses on activities of daily living and emphasizes the hierarchical nature of actions. Despite its simplicity, our method outperforms comparable visual-only methods, including those with the same architecture and pre-trained state-of-the-art models, when trained under the same hyperparameters.\nWhile future work may explore alternative video aggregation and feature fusion mechanisms, this study focuses on the hierarchical properties of actions by analyzing the effects of various textual prompts, such as the number of previous actions and the relevance of location, using different text encoders. We also evaluate different strategies for incorporating hierarchical information into action recognition."}]}