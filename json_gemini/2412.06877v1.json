{"title": "LLMS FOR GENERALIZABLE\nLANGUAGE-CONDITIONED POLICY LEARNING\nUNDER MINIMAL DATA REQUIREMENTS", "authors": ["Thomas Pouplin", "Katarzyna Kobalczyk", "Hao Sun", "Mihaela van der Schaar"], "abstract": "To develop autonomous agents capable of executing complex, multi-step decision-\nmaking tasks as specified by humans in natural language, existing reinforce-\nment learning approaches typically require expensive labeled datasets or access\nto real-time experimentation. Moreover, conventional methods often face diffi-\nculties in generalizing to unseen goals and states, thereby limiting their practical\napplicability. This paper presents TEDUO, a novel training pipeline for offline\nlanguage-conditioned policy learning. TEDUO operates on easy-to-obtain, unla-\nbeled datasets and is suited for the so-called in-the-wild evaluation, wherein the\nagent encounters previously unseen goals and states. To address the challenges\nposed by such data and evaluation settings, our method leverages the prior knowl-\nedge and instruction-following capabilities of large language models (LLMs) to\nenhance the fidelity of pre-collected offline data and enable flexible generalization\nto new goals and states. Empirical results demonstrate that the dual role of LLMs\nin our framework-as data enhancers and generalizers-facilitates both effective\nand data-efficient learning of generalizable language-conditioned policies.", "sections": [{"title": "1 INTRODUCTION", "content": "Motivation. A central aim of AI research is to develop autonomous agents capable of solving com-\nplex, multi-step decision-making tasks based on human-provided instructions articulated in natural\nlanguage. Current approaches, which rely on traditional reinforcement learning (RL) methods, re-\nquire either vast amounts of data in the form of offline expert demonstrations or data collected from\nreal-time interactions with the environment. Such data is expensive to gather and online experi-\nmentation may be impractical in many real-world scenarios. Moreover, even with substantial data,\nRL agents are often constrained to a limited set of previously attained goals, or their performance\ndeteriorates significantly when tested on new goal-reaching tasks (Yang et al., 2023).\nProblem setting. Given the above, this paper approaches the problem of learning generalizable\nlanguage-conditioned policies under minimal data requirements. Specifically, we consider a fully\noffline setup with access to a pre-collected dataset of state-action transitions, D, alongside an un-\npaired set of natural language commands, Gtr. The dataset D consists of triplets (x, a, x'), where x\nand x' belong to a high-dimensional state space X, and a represents actions within an action space\nA. The natural language goals g \u2208 Gtr describe a subset of tasks achievable within the environ-\nment. We posit that such data is often easy to obtain by simply recording agents interact with the\nenvironment and creating a list of natural language commands corresponding to the tasks typically\nperformed within that environment. For instance, in household robotics, D may be derived from\nrandom exploration of the environment, while Gtr may describe tasks such as \u201cGo and open the\nwindow\" or \"Fetch me a cup of tea.\" In the case of personal assistants, D might be collected by\nrecording the daily activities of a human interacting with their mobile device, while Gtr would de-\nscribe goals like \"Book a restaurant for 7 PM\" or \"Send an email to John.\u201d With no assumptions"}, {"title": "2 PROBLEM FORMALISM", "content": "Inputs. We are given a dataset D of past interactions of an agent acting according to a data collection\npolicy \\( \\pi^\\beta \\). This dataset is represented as a collection of trajectories:\n\\(D = \\{T_i\\}_{i \\in I}, T_i = \\{(x_t, a_t, x_{t+1})\\}_{t=0}^{T_i}, x_0 \\sim p, x_{t+1} \\sim P(\\cdot|x_t, a_t), a_t \\sim \\pi^\\beta(\\cdot|x_t),\\) (1)\nwhere P is the state transition function determining the next state given an action \\( a_t \\in A \\) and state\n\\( x_t \\in X \\) and p represents a distribution of initial states. Alongside D, we are provided with an\nunpaired set of training goals Gtr describing a subset of tasks an agent may attempt to solve within\nthe environment. Each goal g is expressed in a goal representations space G. In this paper, we focus\non learning language-conditioned policies, taking G to be the space of natural language.\nModeling assumptions. Denoting by P(X) the powerset of X, we assume there exists a ground-\ntruth mapping \\( \\phi : G \\rightarrow P(X) \\) associating each goal g with a subset of the state space, \\(\\phi(g) = X_g \\subseteq\nX\\). We say that g is achieved at time step t, if \\( x_t \\) lies in \\( X_g \\). Then, the cumulative discounted\nreward: \\( \\sum_{t=0}^{\\infty} \\gamma^t R_\\phi(x_t, a_t, x_{t+1}; g) \\), with \\( R_\\phi(x_t,a_t,x_{t+1};g) = \\mathbb{1}\\{x_{t+1} \\in \\phi(g)\\} \\) measures the\noptimality of actions taken by an agent with respect to achieving the goal g, where \\( \\gamma \\in [0,1) \\)\nis the discount factor penalizing long sequences of actions. We make no assumptions regarding the\noptimality of the data collection policy \\( \\pi^\\beta \\) with respect to Gtr and thus, in what follows, we will view\nour pre-collected data as an un-ordered collection of state-action-state transitions, in short denoted\nas \\( D = \\{(x, a, x')\\} \\). We also do not assume access to either of the ground-truth state-transition\ndynamics P or the goal-to-state mapping \\(\\phi\\), and consequently the reward \\( R_\\phi \\). We only require that\nGtr contains goals corresponding to states that have been visited in D .\nThe goal. Given D and Gtr, our objective is to learn a language-conditioned policy \\( \\pi^* \\), where\n\\(\\pi^* = \\underset{\\pi}{argmax} E_{g \\in G, x_0 \\sim \\rho} \\sum_{t=0}^{\\infty} \\gamma^t E_{a_t \\sim \\pi(\\cdot|x_t;g), x_{t+1} \\sim P(\\cdot|x_t,a_t)} [\\mathbb{1}(x_{t+1} \\in \\phi(g))], \\) (2)\nCrucially, \\( \\pi^* \\) should generalize to novel goals \\( g \\notin Gtr \\) and previously unseen states \\( x \\notin D \\). We\nalso require that \\( \\pi^* \\) not only generalizes to synonymous language commands, but also to goals\ncorresponding to new goal-states, emphasizing our focus on evaluation in the wild."}, {"title": "3 THE METHOD: TEDUO", "content": "To address the problem of learning language-conditioned policies solely based on the inputs D and\nGtr, we must overcome the challenges C1-C4 outlined in the introduction. While conventional RL\nmethods are successful at learning optimal policies within well-explored environments, they typi-\ncally require additional data labeling and are limited in generalization to new, previously unseen lan-\nguage commands and states. In contrast, although LLMs can understand the meaning of sentences\nin natural language describing each goal, their skills lack grounding in relation to the environment's\ndynamics. Our pipeline, TEDUO, employs LLMs to enhance conventional RL, effectively address-\ning challenges C1-C4. TEDUO consists of three main steps:\nConstruction of abstract MDPs. For each goal, \\( g \\in G_{tr} \\), we construct an ab-\nstract MDP by employing LLM-automated hindsight labeling and state abstraction,\naddressing C1 and C2, respectively.\nGoal-conditioned policy learning with offline RL After obtaining a labeled dataset\nfor each goal in \\( G_{tr} \\), we solve the set of abstract MDPs using an out-of-the-box\noffline RL algorithm. As a result, we obtain a set of learned policies \\(\\{ \\pi_g : g \\in G_{tr} \\}\\).\nThe learned policies improve on naive imitation learning, addressing C3.\nLLM supervised fine-tuning. We distill the knowledge about the environment dy-\nnamics and optimal actions into a pre-trained LLM with supervised instruction fine-\ntuning (SFT). This step grounds the prior knowledge of the base LLM in the en-\nvironment dynamics, thus enabling generalization to new, previously unseen states\nand goals, addressing both challenges C2 and C4.\nIn the following paragraphs we explain in detail individual steps of TEDUO."}, {"title": "3.1 STEP 1. CONSTRUCTION OF ABSTRACT MDPS", "content": "Starting with the dataset of unlabeled observations D and the training goals Gtr, we first construct\na set of abstract MDPs \\(\\{M_g : g \\in G_{tr}\\}\\), where \\( M_g := (S_g, A, P_g, R_g, \\rho, \\gamma) \\), with \\( S_g \\) being the\nabstract state space for the goal g, \\( P_g \\) the induced transition operator, and \\( R_g \\) the reward function\nwith respect to the goal g. To define our abstract MDPs, we employ hindsight labeling and state\nabstraction with LLM-based operators."}, {"title": "3.1.1 STATE ABSTRACTION", "content": "The goal of state abstraction is to reduce the size of the environmental state space by grouping\ntogether similar states in a way that reduces the complexity of the underlying problem being solved\n(Li et al., 2006). With a well-designed state abstraction, RL algorithms can learn more efficiently,\nrequiring fewer samples of data, which is particularly relevant in our offline setup. Formally, let\n\\( F : X \\times G \\rightarrow S_g \\) be an abstraction operator that given a goal g maps a single observation \\( x \\in X \\) to\nan abstract state \\( s_g = F(x; g) \\). The abstraction map, F, should be such that |X| \\( \\gg \\) |\\( S_g \\)|."}, {"title": "3.1.2 GOAL-CONDITIONED HINDSIGHT LABELING.", "content": "Following recent works (Kwon et al., 2023), we hypothesize that the existing abilities of LLMs in\nnatural language understanding are sufficient to perform the simple task of identifying whether a\nparticular state belongs to the set of goal states \\(\\phi(g)\\) associated with a given goal description g.\nIn order to perform hindsight labeling of our dataset D, we wish to approximate \\(\\phi\\), and thus the\nreward \\(R_\\phi(\\cdot; g)\\), with a prompted language model \\( LLM^{rwrd} \\approx \\mathbb{1}(g \\text{ is achieved in } s_g) \\). Note, we assign\nthe rewards in the abstracted spaces \\( S_g \\) and not the original state space X. Given the large number\nof goals and states, to reduce the number of LLM calls needed, instead of using language models\ndirectly, we train proxy reward models-lightweight neural networks trained to predict the labels\ngenerated by the prompted language model, \\( LLM^{rwrd} \\). For each goal g, we build a supervised\ndataset, \\(\\{(s_g,r_g) : r_g = LLM^{rwrd}(s_g;g), s_g \\in \\hat{S}_g\\}\\), where \\( \\hat{S}_g \\) is a small, diverse subset of the\nabstract space \\( S_g \\) and \\( r_g \\in \\{0, 1\\} \\). We train a neural network \\( R_g(\\cdot ;g) : S_g \\rightarrow \\{0, 1\\} \\) to predict\nbinary rewards for the entire abstract state space \\( S_g \\). The subset \\( \\hat{S}_g \\) is chosen so that for any two\nabstract states, the set of features relevant for goal-identifications is distinct, i.e. \\( \\forall s_1 \\neq s_2 \\in\n\\hat{S}_g, \\hat{s}_{1,g} \\neq \\hat{s}_{2,g} \\). This maximizes the chances of including goal-states in \\( \\hat{S}_g \\), mitigating the potential\nissue of generating a highly-imbalanced dataset for training our proxy neural networks. These proxy\nreward functions provide a much more cost-effective way to perform hindsight labeling compared\nto labeling all states from D for all goals from Gtr directly by LLM prompting or with human\nannotators. Appendix D shows that for the BabyAI environment, depending on the goal, proxy\nrewards reach near 100% accuracy in comparison to the ground truth rewards of the environment."}, {"title": "3.2 STEP 2. SOLVING THE ABSTRACT MDPS", "content": "After applying state abstractions and hindsight labeling to our offline dataset D, for each goal g \u2208\nGtr, we obtain an offline dataset \\( D_g := \\{(s_g, a, s'_g, r_g)\\} \\). Given these data, we can apply any offline\nreinforcement learning method to learn optimal policies \\( \\pi_g \\), for each goal g \u2208 Gtr. In practice,\nhowever, to learn the goal-conditioned policies, the chosen RL method should be scalable, as we\nmust solve multiple MDPs, one for each goal in Gtr. Therefore, in our instantiation, we discard\ncomputationally intensive methods. Furthermore, as the generalization to new states is tackled by\nthe next step, we do not require at this stage that the learned policies generalize to unseen states.\nGiven these considerations, we simply choose tabular Q-learning (Watkins & Dayan, 1992) to solve\nthe set of abstract MDPs. At the end of this stage, we obtain a set of learned policies \\(\\{\\pi_g : g \\in Gtr\\}\\).\nThese policies are limited to the set of training goals and the set of states observed in D. The final\nstep of our pipeline addresses these limitations."}, {"title": "3.3 STEP 3. TRAINING THE LLM AS A GOAL-CONDITIONED POLICY", "content": "To enable generalization to previously unseen states, and more importantly, generalization to novel\ngoals, the final step of our method distills the knowledge of the optimal actions per each abstract\nstate and goal into a pre-trained LLM. We build a supervised dataset DSFT consisting of goal\ncommands, initial abstract states and the sequence of optimal actions with respect to the learned"}, {"title": "4 RELATED WORK", "content": "LLMs for decision making. There is growing interest in using general-purpose LLMs directly as\ndecision-making agents (Yao et al., 2023). Various prompting techniques, such as chain of thought\n(Wei et al., 2023) and self-reflection (Ji et al., 2023), have been developed to enhance LLMs' abilities\nin long-term planning tasks. However, as demonstrated in previous works (Szot et al., 2024; Finn,\n2024), prompting techniques alone are insufficient for solving complex decision-making tasks in\ndynamic environments. To effectively utilize the knowledge embedded in LLMs for RL problems,\nthese models must be grounded in the dynamics of the environment. This grounding can be achieved\neither through in-context learning (Wang et al., 2023; Wu et al., 2023) or fine-tuning (Carta et al.,\n2023; Tan et al., 2024; Brohan et al., 2023a). A key limitation of in-context learning is its restricted\nwindow size. In this work, we focus on fine-tuning; however, unlike prior studies, we significantly\nreduce the requirements on input data for fine-tuning the decision-making agent.\nLLMs as data enhancers. To apply conventional RL methods in search of optimal goal-conditioned\npolicies, we must augment our dataset of state-action transitions with goal-dependent rewards. This\nprocess, known as hindsight labeling, has traditionally been performed manually by human anno-\ntators or through learning the reward function from expert demonstrations (Ziebart et al., 2008; Fu\net al., 2018; Bahdanau et al., 2018). Recent studies, however, have demonstrated that task-specific\nrewards can be effectively generated using pre-trained LLMs (Yu et al., 2023b; Ma et al., 2023; Xie\net al., 2023). While successful, most LLM-based approaches rely on iterative prompting strategies,\nwhich are costly in terms of LLM calls. Our approach to hindsight labeling reduces this cost by ap-\nproximating the LLM-induced reward function with a lightweight neural network. Furthermore, we\nassign rewards in abstracted state spaces, significantly reducing the number of states to be labeled.\nSimilar to the work of Peng et al. (2023), our state abstraction function uses the language command\nto guide the elimination of irrelevant state features.\nLanguage-conditioned RL. Numerous previous studies have explored learning language-\nconditioned policies by assuming access to ground-truth environment rewards (Jiang et al., 2019;\nCo-Reyes et al., 2018), real-time experimentation (Fu et al., 2018; Bahdanau et al., 2018; Mirchan-\ndani et al., 2021), or expert demonstrations paired with language annotations (Stepputtis et al., 2020;\nLynch & Sermanet, 2021; Xiao et al., 2023; Brohan et al., 2023b;a). In contrast, our approach aims\nto learn language-conditioned policies from entirely offline datasets, which may be highly subopti-\nmal and which are unlabeled, with no environment- or human-provided reward signals. Regarding\npolicy evaluation, much of the prior work in language-conditioned RL and IL tests agents on new\nlanguage commands synonymous with those seen during training (Lynch & Sermanet, 2021; Nair\net al., 2022). Similar to the works of (Xiao et al., 2023; Brohan et al., 2023a; Shridhar et al., 2021a;\nJang et al., 2022), our focus lies on novel instructions corresponding to previously unsolved goals.\nRefer to Appendix A for an extended discussion of the related work."}, {"title": "5 EXPERIMENTS", "content": "Questions. In our experiments we aim to answer the following questions: (Q1) Does the use of a\npre-trained language model enable generalization to new language commands and new states? (Q2)"}, {"title": "5.1 Q1: ONLINE EVALUATION: GENERALIZATION BENCHMARK", "content": "Setup. We choose the collection of Synth environments from BabyAI as the main test bed for\nTEDUO. All environments are constructed as a 22x22 grid and containing 9 rooms. They dif-\nfer in the type, position, and color of the distractors. The tasks include goals such as \"go to the\n{color} {object}\u201d, \u201cpick up the {color} {object}", "put the {color} {object} next to the {color}\n{object}": "We use a list of 500 goals as Gtr. The set of testing goals contains 100 goals that are\nsemantically distinct from those in Gtr. We also augment the set of testing goals by asking GPT-4 to\nparaphrase the original commands provided by BabyAI. We train a Llama-3-8B model with TEDUO\nbased on a dataset D containing 800k non-unique state-action-state triplets generated according to a\npolicy that is a random mixture of default policies from BabyAI (see Appendix B.1 for details).\nBaselines. We compare our fine-tuned Llama-3-8B agent with non-fine-tuned LLMs: Llama-3-8B\nand Llama-3-70B using a) vanilla and b) chain-of-thought prompting Wei et al. (2023) with addi-\ntional demonstrations provided in-context (in-context+CoT). The latter integrates expert demonstra-\ntions generated during step 2 of TEDUO to test the in-context learning ability of the LLM. Following\nrecent works Mezghani et al. (2023); Li et al. (2022); Cao et al. (2023), we also compare against\nBabyAI-IL-bot, the baseline proposed by the authors of BabyAI (Chevalier-Boisvert et al., 2018),\nwhich is the combination of a GRU to encode the instruction, CNN+FILM layers to encode the\ngrid and an LSTM memory. We train this method via imitation learning on the policy generated by\nTEDUO, steps 1&2. Implementation details of the baselines can be found in Appendix B.6."}, {"title": "5.2 Q2: ONLINE EVALUATION: ABLATION STUDY", "content": "Setup. Using the same experimental setup as in the previous section, we compare our full fine-tuning\npipeline with its ablations. After obtaining the abstract datasets D\u00ba with the first step of TEDUO,\nwe generate goal-conditioned policies with naive behavioral cloning (step 1 + GCBC). We also\ncompare our fine-tuned Llama-3-8B against the performance of the GCRL policies obtained with\noffline Q-learning in step 2. Note, neither of GCBC nor GCRL can generalize to new, previously\nunseen language commands. Therefore, in this study, we are only looking at performance on goals\nfrom Gtr. Ablation of the abstraction function is delayed to the next section."}, {"title": "5.3 Q3: LEARNING AND EXPLOITING CORE SKILLS", "content": "The aim of the first part of our experiments is to investigate the generalization abilities of the LLM\nfine-tuned with our pipeline. We wish to investigate if by learning the optimal policies for diverse\ngoals and environments, the LLM can integrate the core skills required to achieve these goals and\nhow such skills can be transferred across tasks. We also investigate the aspect of skill composition-\nality. Does the prior knowledge of the LLM, now grounded in the environment dynamics, suffice to\ncompose together learned skills to solve novel tasks?"}, {"title": "5.3.1 SKILL TRANSFER AND COMPOSITIONALITY.", "content": "Setup. We are working with the following three types of illustrative environments:\nThe position and color of the door and box vary across different instantiations of the environments.\nWe use type A and B environments for training and type C environments for testing. We note that\ntasks from type C environments require the internalization of three core skills: moving to a given\nlocation, opening a door, picking up a box. The skill of moving to a location can be obtained from\nboth environments A and B, but the skill of picking up a box or opening the door can only be\nobtained from one of the environments, A or B, respectively. This setup allows us to investigate the\ntransferability of learned skills across environments and their compositionality."}, {"title": "5.3.2 INTERNALIZATION OF CORE SKILLS.", "content": "One of the core skills required to successfully solve tasks from the BabyAI environments is to\nidentify whether the agent at its given location is facing an object or a wall, or it is free to move\nforwards. This section provides additional insights into the behaviour and internal representation of\nstates of the LLM fine-tuned with TEDUO in comparison to a base LLM.\nSetup. We operate within the Synth BabyAI environments as in the main evaluation benchmark\nand generate a dataset consisting of 10 random goals and 512 states per each goal. We embed each\ngoal-state pair into our prompt template for eliciting actions and fine-tuning the language models\nand pass them through both the base and fine-tuned Llama-3-8B from experiments 5.1 and 5.2. We\nrecord the logprobabilities of the tokens [0, 1, ..., 6] as well as the hidden representation of states at\neach layer. We label our dataset according to whether at the given state the agent is facing a wall, an\nobject, or it is free to move forwards. For each layer, we fit two linear probes on top of the hidden\nrepresentations: one to predict if the agent is facing a wall and the other if it is facing an object."}, {"title": "5.4 Q4: DATA EFFICIENCY AND SCALING OF TEDUO", "content": "Impact of state abstraction. In Figure 4, we look at the performance of the learned Q-learning\npolicies (i.e. the policies \\(\\{\\pi_g\\}_{g\\in gtr}\\) obtained at the end of TEDUO step 1+2 for different sizes of\nobservational dataset D. This experiment is realized with and without the ablation of the abstrac-\ntion function during step 1. As anticipated, the efficacy of the learned policies improves with the\nincreasing size of the dataset D until reaching a plateau. On average, our LLM-based state abstrac-\ntion function reduces the number of unique states within each abstract MDP by 10% (Fig. C.8 in\nthe Appendix). Due to the reduction in state space size, the abstraction function significantly en-\nhances the data efficiency of our training method across all three performance metrics. Furthermore,\nthe size of the state spaces S, corresponding to the subset of features relevant for identifying the\ncompletion of the goal is reduced to just around 20% of the original state space size (Fig. C.8 in\nthe Appendix). This reduces the size of the goal detection datasets for training and the subsequent\ngoal-identification described in section 3.1.2 5-fold.\nCompute power is the new bottleneck. Given a fixed observational dataset D, we can expand at no\nextra cost the fine-tuning dataset DSFT by introducing more training goals in Gtr. Yet, larger DSFT\nnecessitates more compute power for training the LLM agent."}, {"title": "6 DISCUSSION", "content": "Limitations. Leveraging LLMs' prior knowledge enables efficient policy generation with minimal\ndata. However, some applications may benefit more than others. First, certain scenarios may be\nout of distribution even for LLMs trained on extensive Internet data. Second, we assume that the\nenvironment state can be represented textually, which, although feasible for many applications due\nto language's expressiveness, may not be ideal in all cases. Third, due to the discrete nature of LLM\ntokenization, using fine-tuned LLMs to directly output actions requires discretization of the action\nspace, which can hinder performance in continuous control tasks. Lastly, while data requirements\nare minimal, they still assume some practitioner knowledge of the environment and the data D to\npropose training goals Gtr likely achievable in D (see Appendix C.1 for details).\nConclusions. We introduced a novel framework for training natural language instruction-following\nagents capable of generalizing to new states and instructions in a zero-shot setting. This is the\nfirst method to learn natural language goal-conditioned policies in an offline setting using unlabeled\ndata and it surpasses the current state-of-the-art in zero-shot generalization for both goal and do-\nmain adaptation. To achieve this, TEDUO teaches environment dynamics to an LLM, enabling it\nto interact effectively within the given environment\u2014a task where out-of-the-box LLMs typically\nunderperform. Given the demonstrated flexibility of LLMs in generalizing to new goals and en-\nvironments, future work could explore the potential of this approach in learning across multiple\nenvironments with distinct action and state spaces."}, {"title": "A EXTENDED RELATED WORK", "content": "A.1 GENERALIZATION IN OFFLINE REINFORCEMENT LEARNING\nFollowing the work of Mediratta et al. (2024)", "categories": "new instruction following and adaptation to\nnew states or environments.\nGoal-conditioned RL. Goal-conditioned Reinforcement Learning (GCRL) is a subfield of RL ded-\nicated to developing policies capable of achieving multiple goals within the same environment dy-\nnamics. These policies are conditioned on an additional input, g, indicating the goal that the next\na"}]}