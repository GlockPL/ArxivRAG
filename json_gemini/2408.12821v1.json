{"title": "EXAMINING THE COMMITMENTS AND DIFFICULTIES INHERENT\nIN MULTIMODAL FOUNDATION MODELS FOR STREET VIEW\nIMAGERY", "authors": ["Zhenyuan Yang", "Xuhui Lin", "Qinyi He", "Ziye Huang", "Zhengliang Liu", "Hanqi Jiang", "Peng Shu", "Zihao Wu", "Yiwei Li", "Stephen Law", "Gengchen Mai", "Tianming Liu", "Tao Yang"], "abstract": "The emergence of Large Language Models (LLMs) and multimodal foundation\nmodels (FMs) has generated heightened interest in their applications that integrate\nvision and language. This paper investigates the capabilities of ChatGPT-4V and\nGemini Pro for Street View Imagery, Built Environment, and Interior by evaluating\ntheir performance across various tasks. The assessments include street furniture\nidentification, pedestrian and car counts, and road width measurement in Street View\nImagery; building function classification, building age analysis, building height\nanalysis, and building structure classification in the Built Environment; and interior\nroom classification, interior design style analysis, interior furniture counts, and\ninterior length measurement in Interior. The results reveal proficiency in length\nmeasurement, style analysis, question answering, and basic image understanding,\nbut highlight limitations in detailed recognition and counting tasks. While zero-shot\nlearning shows potential, performance varies depending on the problem domains\nand image complexities. This study provides new insights into the strengths and\nweaknesses of multimodal foundation models for practical challenges in Street\nView Imagery, Built Environment, and Interior. Overall, the findings demonstrate", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) and multimodal foundation models (FMs) have demonstrated\nconsiderable promise in the realm of generalized intelligence across a spectrum of tasks and\nmodalities. [1, 2, 3, 4] Through the strategic utilization of substantial datasets and computational\nresources, exemplified by models such as GPT-4V [5] and Gemini Pro [6], these models attain\nexpansive capabilities through large-scale pretraining, facilitating seamless adaptation to novel data\nand tasks without prior exposure. [7, 8, 3] Nonetheless, the exploration of their capacity to interpret\nand utilize visual information has been relatively limited, particularly within specialized domains\nsuch as streetscape. This paper undertakes a comprehensive evaluation to scrutinize the capabilities\nand constraints of GPT-4V and Gemini Pro in the context of Street View Imagery, architecture, and\nurban planning ."}, {"title": "2 Background", "content": null}, {"title": "2.1 The Rise of LLMs and Multimodal Models", "content": "The transformative transformer architecture [9], foundational to LLMs, significantly advanced\nmultiple domains, including natural language processing (NLP) and computer vision (CV). The\nintroduction of transformers marked a breakthrough in NLP, paving the way for natural language\nprocessing by overcoming limitations inherent in earlier architectures such as Recurrent Neural\nNetworks (RNNs) and Long Short-Term Memory networks (LSTMs). The attention mechanisms [9]\nallow for the processing of long-range dependencies in text without encountering the aforementioned\nlimitations, thereby significantly enhancing the efficiency and scalability of LLMs, providing a\nfoundational basis for subsequent advancements. GPT series, adopted a probabilistic approach to\ngenerating text one token at a time, facilitating flexible and coherent text generation.\nIn the domain of computer vision, models like Vision Transformer (ViT) [10] and Masked Au-\ntoencoders (MAE) [11] played pivotal roles in advancing image classification tasks. The Segment\nAnything Model (SAM) [12] showcased remarkable generalization capabilities and zero-shot learn-\ning, particularly in adapting to diverse aerial and orbital images. Despite challenges, SAM's\nadaptability holds promise for remote sensing image processing.\nThe growing demand for multimodal models, integrating tasks from computer vision and NLP, is\ndriven by the aspiration for Artificial General Intelligence (AGI). There emerged a necessity to\nextend LLMs to encompass multimodal intelligence by leveraging pretrained ViT as image encoders\nand LLMs as interfaces for vision-language inputs, aligning with human multimodal instructions.\nModels such as GPT-4V and Gemini Pro were specifically designed to process both images and\ntext. Pre-trained on diverse datasets containing both text and images, these models demonstrate\nrobust visual comprehension abilities, thereby expanding the horizons at the intersection of natural\nlanguage processing, computer vision, and human-AI interaction."}, {"title": "2.2 Street View Imagery", "content": "Street view imagery, serving as a vital data source for capturing urban environmental details, plays\na significant role across various fields such as urban planning, traffic management, and public safety.\nWith the rapid advancement of Large Language Models (LLMs) and computer vision technologies,\nthe application of street view imagery in conjunction with LLMs has begun to demonstrate immense\npotential, offering novel perspectives and tools for urban management and planning. Initially, street\nview imagery provides rich visual information on urban layouts, architectural features, and the\nutilization of public spaces. The application of LLMs and computer vision technologies enables the\nautomatic identification and analysis of key elements within these images, such as road signs, traffic\nflow, and pedestrian density, thereby obtaining real-time data on the operational status of cities. This\napplication significantly enhances the efficiency of urban data collection and analysis, providing\naccurate data support for urban planning and management decisions. Furthermore, the analysis of\nstreet view imagery combined with LLMs supports complex spatial analyses and pattern recognition\ntasks. For instance, by analyzing the architectural styles and spatial layouts in street view images,\nLLMs can assist urban planners in identifying historical and cultural areas within cities, evaluating\nthe impact of urban renewal projects on the preservation of historical buildings. Additionally, by\nrecognizing and tracking changes in street view imagery, such as variations in green coverage or the\nemergence of new constructions, LLMs can provide substantial evidence for urban development\ntrends, supporting sustainable development planning. In the realm of traffic management, the\napplication of street view imagery in conjunction with LLMs also reveals substantial potential.\nReal-time analysis of street images can monitor traffic volumes, identify congestion points, and even\npredict potential risk areas for traffic accidents, offering real-time data support for traffic planning\nand management. Moreover, street view imagery can be utilized for the automatic detection of road\nand traffic infrastructure damage, aiding city management departments in timely maintenance and\nrepair. However, the application of street view imagery combined with LLMs faces challenges,\nincluding how to process and analyze the vast amount of street view image data, ensure the accuracy\nand reliability of analysis results, and address data privacy and security concerns. Future research\nneeds to focus not only on enhancing technical performance but also on addressing these challenges.\nIn summary, the integration of street view imagery with Large Language Models provides robust\ntechnical support for urban planning and management, making the collection and analysis of urban\ndata more efficient and accurate. As technology continues to evolve and its application scope\nexpands, the future is poised to unlock more possibilities for smarter and more sustainable urban\ndevelopment."}, {"title": "2.3 Built Environment", "content": "The built environment refers to human-made surroundings that provide the setting for human activity,\nincluding buildings, infrastructure, and public spaces. It has significant impacts on human life,\nhealth, and sustainable development. Traditional research on the built environment primarily relies\non field surveys, manual drawing, and statistical analysis, which often suffer from inefficiencies\nand subjectivity [13]. With the advancement of technologies such as remote sensing, Geographic\nInformation Systems (GIS), and Building Information Modeling (BIM), digital and information-\nbased methods have started to be applied to the analysis and planning of the built environment\n[14]. Particularly in the past decade, deep learning methods have achieved significant progress in"}, {"title": "2.4 Interior", "content": "In the field of interior design, computer vision and machine learning technologies have been widely\napplied. Traditional methods primarily rely on handcrafted features or shallow learning models to\nanalyze and understand interior images[28]. However, these methods often show limitations when\ndealing with the complexity and diversity of indoor scenes. In recent years, the advent of deep\nlearning, particularly convolutional neural networks (CNNs), has significantly advanced the under-\nstanding of indoor scenes. CNNs can automatically learn hierarchical feature representations of\nimages, achieving remarkable performance improvements in tasks such as image classification, ob-\nject detection, and semantic segmentation[29]. A series of CNN-based methods have been proposed\nfor indoor scene classification[30], room type recognition [31], and indoor layout estimation[32].\nBeyond analyzing interior images, the use of deep learning to generate realistic indoor scene\nimages has also attracted attention. Several studies have explored the use of generative adversarial\nnetworks (GANs) to generate photorealistic indoor images from noise vectors or semantic layout\nmaps [33, 34]. This provides a new approach for automated and diverse interior design generation.\nHowever, current methods for generating indoor images still need improvement in terms of semantic\naccuracy and detail realism.\nThe emergence of large models, particularly vision-language pre-trained models, has opened new\npossibilities for interior design. These models are pre-trained on vast amounts of image-text data,"}, {"title": "3 Experiments and Observation of FMs for Street Furniture", "content": null}, {"title": "3.1 Brand Recognition", "content": null}, {"title": "3.1.1 Data Source", "content": "In the architectural style and architectural logo identification, the information of famous brand retail\nstores can be extracted. In this experiment, we selected three famous brands, namely Burger King,\nApple, Starbucks and KFC.In the process of logo recognition, it is necessary to extract elements\nof architectural style and architectural features. The image identified this time comes from the\nstorefront of the four major brands in Google Street View, and the image also includes the logo\nitself. The dataset can be accessed at https://github.com/fqhwas/architecture."}, {"title": "3.1.2 Evaluation and analysis", "content": "In the brand recognition task, the main task of the model is to recognise the pattern and text of\nthe store logo and accurately locate it with the brand name. GPT-4V's recognition of the store\nfocuses more on the materials and architectural style of the building and the description of the\nscene. However, even for common brands, GPT-4V has difficulty in recognising them. In contrast,\nGPT-40 not only accurately recognises brands and logos, but also analyses slogans, design elements,\narchitectural styles, lighting, etc. Similar to GPT-40, Gemini can also accurately identify common\nstore brands, including the text and logo of the brand. Unlike GPT-4V, Gemini does not further\nanalyse the light, architectural style, etc. in the picture after identifying the brand, but instead gives\na brief introduction to the business of the brand, which is not in the prompt requirements, showing\nthat Gemini has some divergent thinking and seems to have a better understanding of the real world.\nIn sum, GPT-40 and Gemini both have good zero-shot performance in brand recognition. The latter\nseems to have stronger divergent thinking, while the former focuses more on the prompt task. The\ndifference may be related to their training data and fine-tuning strategies."}, {"title": "3.2 Counts of Pedestrians", "content": null}, {"title": "3.2.1 Data Source", "content": "The task of counting pedestrians in street view images primarily aims to assess the fine-grained\ndiscrimination capabilities of multimodal models. Pedestrians are a significant component of street\nscenes, and accurately counting the number of people provides a measure of the model's proficiency.\nIn this section, we utilize data from a public dataset to evaluate this capability. The dataset can be\naccessed at https://github.com/fqhwas/architecture."}, {"title": "3.2.2 Evaluation and analysis", "content": "In the pedestrian counting task, GPT-4V tends to give interval estimates, such as \"at least 20 people,\"\nand can further update its interval estimates during the description. GPT-40 and Gemini tend not to\ndescribe the picture in detail and give point estimates instead of interval estimates. The interval\nboundaries of GPT-4v's answers are usually more deviated from the correct answer than the point\nestimates of GPT-4o and Gemini, but because it gives a larger possible range, its answer is also\nreasonable. Among them, Gemini is completely correct in a pedestrian counting task with less\nnoise in the foreground, but the answer given in a scene with more noise and a complex background\ndeviates from the correct value relatively far. GPT-40, on the other hand, performed more evenly in\nboth the near and far-field pedestrian counting tasks. This difference between the two models may\nbe related to their training data and fine-tuning strategies.\nAll three models showed good zero-shot performance, indicating that all three models have the\nability to identify people in complex scenes and fine-grained recognition capabilities."}, {"title": "3.3 Counts of Cars", "content": null}, {"title": "3.3.1 Data Source", "content": "The task of counting vehicles in street view images is designed to evaluate the fine-grained discrimi-\nnation capabilities of multimodal models. Vehicles are a crucial component of street scenes, and\naccurately counting the number of vehicles provides a measure of the model's proficiency. In this\nsection, we utilize data from a public dataset to conduct this evaluation. The dataset is available at\nhttps://github.com/fqhwas/architecture."}, {"title": "3.3.2 Evaluation and analysis", "content": "In the car counting task, GPT-4V refuses to give an answer when there are many cars in the image.\nFor tasks with fewer vehicles, it can add the number of different types and colours of cars in the\nimage to get an answer. The analysis process is reasonable, but it often ignores some of the vehicles\nin the image. This shows that GPT-4V has the ability to recognise the characteristics of vehicles\non the street, but when there are too many vehicles or some of them are partially in the image, it\nwill not be able to given an answer. In contrast, GPT-40 has improved its feature recognition ability\nin large-scale counting ranges and has a stronger ability to recognise partially occluded objects.\nFor tasks with many vehicles in the image, GPT-40 no longer refuses to answer the question, but\ninstead gives a reference value that is closer to the correct answer. At the same time, when there are\nrelatively few vehicles, GPT-40 can count the vehicles with different characteristics and identify the"}, {"title": "3.4 Road Width Measurement", "content": null}, {"title": "3.4.1 Data Source", "content": "The estimation of street width plays a crucial role in assessing the ability of multimodal models\nto identify important parameters in urban streetscapes. When processing streetscape images,\nstreets are integral components, and estimating their scale measures the capability of large-\nscale models. In this section, the data utilized is sourced from public datasets, accessible at\nhttps://github.com/fqhwas/architecture."}, {"title": "3.4.2 Evaluation and analysis", "content": "In the road width measurement task, we gave the general length of a single lane, the general height\nof a floor, and the general height of a person as a reference in the prompt, and asked the three\nmodels to infer the length of the single-sided motor lane based on the information in the figure.\nGPT-4V often refused to answer and ignored the prompts about the length of the lane. Based on the\nanswers and reasoning process of GPT-4V, it has certain difficulties in identifying and inferring the\ndirection of traffic. Gemini tends to give answers directly without reasoning, and its answers are\nusually 1-2 times higher than the real ones. According to the information about the traffic in the\nfigure, this is also because it does not correctly identify the direction of traffic, thus adding up the\nlanes of different lanes together. GPT-4o showed excellent ability in this task. It was able to identify\nlanes that were partially blocked by cars in the foreground and partially obscured in the background,\nand it was able to integrate information such as the direction of the vehicles in the picture and road\nsigns to determine the lane direction, thus giving answers that were close to the real values in each\ntask. Although in the case of ambiguous vehicle information and no obvious road signs, it may\nmisjudge the lane direction and thus give incorrect measurements, this is reasonable based on the\ndata in the picture.\nIn sum, GPT-4o can use the information in the figure to the greatest extent to eliminate the\ninterference caused by the occlusion of the near field and the blurring of the far field, and give\nmore accurate answers. Gemini's application of comprehensive information is still insufficient,\nand GPT-4V's comprehensive information ability is even more lacking. GPT-40 has better zero-\nshot performance and achieved high quantitative accuracy in terms of road width measurement\nwhen given a reference for road length. However, its own quantitative accuracy regarding width\nmeasurement needs to be further tested when there is no reference for road length."}, {"title": "4 Experiments and Observation of FMs for Built Environment", "content": null}, {"title": "4.1 Buildings Functions Classification", "content": null}, {"title": "4.1.1 Data Source", "content": "The classification of building functions is primarily aimed at evaluating the ability of multimodal\nmodels to process and classify large volumes of building information. Buildings constitute the\nprimary elements of streetscapes. In this section, functional recognition involves the computer's\nidentification of the architectural style, building materials, and other major features of buildings. The\ndata utilized is sourced from public datasets accessible at https://github.com/fqhwas/architecture."}, {"title": "4.1.2 Evaluation and analysis", "content": "In the buildings functions classification task, we gave six types of buildings in the prompt that\nthe buildings in the figure might belong to, and asked the three models to classify the buildings\nin the figure to the six categories. All three models completed the identification of all buildings\nperfectly, and GPT-4V and GPT-40 both gave detailed reasoning processes, which showed that they\nhad already been able to make comprehensive inferences from the architectural style, architectural\nelements, logos, text outside the building, and the possible functions of the components in the\nbuilding. Gemini did not give a detailed reasoning process, but its correct answers show that it\nshould have similar capabilities.\nIn sum, all three models seem to have a high ability to classify buildings, and they can use the\nbuildings themselves and surrounding elements to make inferences. Their zero-shot performance is\nvery good."}, {"title": "4.2 Buildings Age Analysis", "content": null}, {"title": "4.2.1 Data Source", "content": "While building age is an important parameter in building specifications, the data is not always\navailable or complete[38]. Based on the first attempt for estimating building age from Google\nStreet View images by using deep learning techniques, we made experiments on the picture of their\nattempt to testify whether GPT-4V, GPT-4o and Gemini can estimate the age of buildings. We chose\nfour buildings of different styles, which were built in different eras, 10 to 20 years apart."}, {"title": "4.2.2 Evaluation and analysis", "content": "In the building age prediction task, GPT-4V is able to give an approximate age of a building, GPT-4V\nnot only describes the building materials, architectural styles, and exterior spaces of a building, but\nalso gives accurate answers up to ten years. These answers are based on statistical data of different\narchitectural styles in different periods. In addition, the quality of the images helps to confirm the\nprobable age of the building. Similarly, the GPT-40 is able to give approximate building dates. In\ncontrast to the GPT-4V, the GPT-40 provided categorisation and description of architectural details\nsuch as roof shape, windows, elevations, building materials, layout and landscaping. Answers were\nmore organised, dating the building in terms of architectural style and referencing answers closer to"}, {"title": "4.3 Building Height Analysis", "content": null}, {"title": "4.3.1 Data Source", "content": "The task of estimating building height is primarily aimed at evaluating the ability of multimodal\nmodels to identify important parameters in urban streetscapes. This assessment involves scale\ncomparisons with human reference points and estimations based on perspective, thereby measuring\nthe capability of large-scale models. In this section, the data utilized is sourced from public datasets\naccessible at https://github.com/fqhwas/architecture. This provides a comprehensive foundation for\nthe analysis and evaluation of building height estimation."}, {"title": "4.3.2 Evaluation and analysis", "content": "In the building height analysis task, GPT-4V carefully described the inference process. Judging\nfrom the output results of the three selected images, the accuracy of identifying building heights is\nnot high, and there is a large gap with the real values. GPT-4V mainly calculates the height of the\nbuilding by referring to the height and number of floors of common objects around the building.\nWhen the view of the building in the picture is not clear, there is partial occlusion, or the direction\nof the building is not clear, and the building itself cannot be correctly identified, the answer often\nvaries greatly. Therefore, in the final answer, GPT-4V was unable to answer. Compared to GPT-4V,\nGPT-40 no longer refuses to answer questions. Even if the final answer is somewhat different from\nthe correct answer, GPT-4o tries to give the calculation process and estimation method. In larger\nscale photos, the error of GPT-40 will be greater. Gemini explains the calculation method in detail,\nand its estimation results are far from the correct answer.\nIn sum, the zero-shot performance of the three models in estimating the height of the building is\naverage, which is mainly related to the fact that the building is partially obscured, the perspective\neffect is too strong, and the shooting angle is somewhat distorted, which makes it impossible for the\nmodel to correctly count the number of floors of the building. To improve the model's ability in this\nregard, it is necessary to increase the training data accordingly to help the model develop the ability\nto estimate the number of floors in the presence of occlusion and perspective effects."}, {"title": "4.4 Building Structure Classification", "content": null}, {"title": "4.4.1 Data Source", "content": "Rapid and accurate identification of potential structural deficiencies is a crucial task in evaluating\nseismic vulnerability of large building inventories in a region[39].For the observer, it is easy to see\nwith the eye whether there is an open space on the ground floor of the building. Using street View\nimages which had already been classified by deep learning, we further tested whether Gemini and"}, {"title": "4.4.2 Evaluation and analysis", "content": "In the building structure classification task, the model needs to identify whether the building in\nthe picture has open spaces such as garages on the ground. GPT-4V can describe the important\nappearance and structural features of soft-story buildings and make judgments accordingly. In\naddition to giving a definitive conclusion, GPT-4V can also describe the appearance and possible\nfunctions of the building. As shown in the figure, if the building in the picture has obvious ground-\nlevel opening features, GPT-4V can correctly identify the building type. Similarly, GPT-40 can also\ndetermine whether the building in the picture has open space and whether the upper floor of the\nbuilding is a rigid structure. GPT-4V and GPT-40 are both accurate in their judgments of soft-story\nbuildings. In contrast, Gemini not only provides an answer for the building type, but also helps to\nconfirm the building function and the possible risk during an earthquake. It tends to identify the\nbuilding as a soft-story building, but two of the four answers are incorrect. Based on the inference\nprocess it gives, it is not very accurate in inferring the use of the ground floor of the building based\non the picture, always tending to think that the ground floor is a garage or commercial space, which\nis a misjudgment.\nIn sum, GPT-4V and GPT-40 both show strong one-shot performance in soft-story building classifi-\ncation, while Gemini is relatively poor in this regard. Perhaps providing a closer or clearer image\ncan help improve its task performance."}, {"title": "5 Experiments and Observation of FMs for Interior", "content": null}, {"title": "5.1 Interior Room Classification", "content": null}, {"title": "5.1.1 Data Source", "content": "The task of interior building function recognition is primarily aimed at evaluating the suc-\ncess of multimodal models in identifying architectural styles and landmarks. When processing\nstreetscape images, large-scale models need to be capable of recognizing specific functional spaces\nwithin buildings. In this section, the data utilized is sourced from public datasets accessible at\nhttps://github.com/fqhwas/architecture. This provides a robust foundation for assessing the recogni-\ntion of interior building functions with respect to architectural styles and landmarks"}, {"title": "5.2 Evaluation and analysis", "content": "In the indoor room classification task, the performance of models needs to be evaluated across\nmultiple dimensions, including the precise recognition of objects within the room, understanding of\nspatial layout, and the ability to distinguish between different functional areas.\nGPT-4V has demonstrated robust capabilities in both recognition and classification, accurately\ncategorizing rooms by identifying the core elements present. For example, when the image includes\nobjects such as a sink, mirror, and bathtub, which are typically associated with a bathroom, GPT-4V\nswiftly identifies the room as a bathroom. Additionally, it goes further by describing the arrangement\nof these objects, such as the placement of the mirror and towels, thus offering not only a justified"}, {"title": "5.2.1 GPT-4V Results and Analysis", "content": "In this section, the task of the GPT is to identify the type of room in the house. The results showed\nthat GPT was able to accurately describe the main furniture in the room and make judgments based\non this, and all judgments were accurate."}, {"title": "5.2.2 GPT-40 Results and Analysis", "content": "In this section, the task of the GPT is to identify the type of room in the house.GPT-4o is also a\ndescription of the indoor furnishings. Within the test range, the indoor function judgment is correct."}, {"title": "5.2.3 Gemini Pro Results and Analysis", "content": "Compared to GPT, Gemini tends to give a definitive answer without further explanation. All of the\nanswers are correct, showing Gemini's ability to judge the function of interior Spaces."}, {"title": "5.3 Interior Design Style Analysis", "content": null}, {"title": "5.3.1 Data Source", "content": "The task of architectural style recognition, encompassing interior style recognition, is primarily\naimed at evaluating the success of multimodal models in classifying buildings. When process-\ning streetscape images, large-scale models require extensive training based on a wide range of\narchitectural styles. In this section, the data utilized is sourced from public datasets accessible at\nhttps://github.com/fqhwas/architecture. This encompasses a comprehensive collection of architec-\ntural features for model training and evaluation."}, {"title": "5.3.2 Evaluation and analysis", "content": "In the interior design style classification task, models must evaluate a variety of visual elements to\ndetermine the style of the space accurately. This requires an understanding of design principles,\nincluding color schemes, furniture types, materials used, and the overall aesthetic feel.\nGPT-4V excels in identifying design styles that combine multiple elements or blend aesthetics. In\none instance, it accurately identified a Scandinavian-style interior based on the use of natural light,\nneutral color palettes, minimalist furniture, and the presence of organic materials like wood. The\nmodel's analysis goes beyond mere surface-level observation, as it provides a detailed reasoning\nprocess by explaining the use of space, light, and materials. This demonstrates GPT-4V's capacity\nfor nuanced understanding in style classification, especially when rooms feature design elements\ntypical of multiple styles.\nIn contrast, GPT-40 tends to offer more straightforward classifications, often focusing on the\ndominant design elements. While it is competent in recognizing the primary style, it lacks the\ndeeper contextual analysis that GPT-4V provides. For example, when analyzing a room with a bold\nand playful color scheme, GPT-40 correctly identified it as modern, but GPT-4V took this analysis\nfurther by highlighting specific elements like the unique design of furniture and color contrasts,\nwhich added depth to the classification.\nGemini-pro-vision, while capable of recognizing some design elements, often mis-classifies the\noverall style. For instance, in a room featuring classic antique furniture, Gemini-pro-vision incor-\nrectly classified the style as \"Korea Asia style.\" This suggests that Gemini-pro-vision struggles to\naccurately combine the various visual clues into a coherent style classification, especially when\nfaced with subtle variations or blended styles. This model may rely too heavily on a specific set of\nfeatures, leading to misinterpretation of the overall aesthetic.\nOverall, GPT-4V consistently outperforms the other models by offering not only accurate clas-\nsifications but also well-supported reasoning that considers multiple layers of design elements.\nGPT-40 is reliable in identifying dominant styles, but it lacks the depth of analysis provided by\nGPT-4V. Gemini-pro-vision, on the other hand, exhibits weaknesses in distinguishing between\nstyles, particularly when faced with complex or blended designs."}, {"title": "5.3.3 GPT-4V Results and Analysis", "content": "In this section, GPT-4V is tasked with identifying 8 styles of interior architecture. GPT-4V describes\nin detail the furniture styles and architectural details identified in the pictures, and gives the answers\nidentified.However, because of the beautiful curves and elegant style of the furniture in the pictures,\nGPT-4V had difficulty recognizing the difference between different interior decoration styles. At the\nsame time, GPT-4V also tries to distinguish different styles through interior colors and materials.\nHowever, except for the classical style, all the other judgments are wrong."}, {"title": "5.3.4 GPT-40 Results and Analysis", "content": "In this section, GPT-4o is tasked with identifying 8 styles of interior architecture. Compared with\nGPT-4V, GPT-40 has more comprehensive interior design style data and more accurate style type\njudgment. In the answer content, GPT-4o analyzes the color, line, furniture, decoration, etc., in the\npicture."}, {"title": "5.3.5 Gemini Pro Results and Analysis", "content": "Gemini also looks at interior furniture for style recognition. However, Gemini gives the same wrong\nanswer for all interior styles. It seems that Gemini has less knowledge of interior style."}, {"title": "5.4 Counts of Interior Furniture", "content": null}, {"title": "5.4.1 Data Source", "content": "The counting of indoor furniture is primarily aimed at evaluating the ability of multimodal models\nto make fine-grained differentiations. When processing streetscape images, large-scale models need\nto identify various types of furniture and related spatial elements. In this section, the data utilized is\nsourced from public datasets accessible at https://github.com/fqhwas/architecture."}, {"title": "5.4.2 Evaluation and analysis", "content": "In the task of counting interior furniture, specifically the number of chairs, the model's performance\ndepends on its ability to accurately detect and quantify the objects within a given scene. This\nrequires not only recognizing what qualifies as a chair but also correctly assessing the total number\npresent in the image.\nFor example, when tasked with identifying the number of indoor chairs in a bathroom scene, GPT-\n4V correctly assessed that there were no chairs present, aligning with the reference answer of \"0.\"\nHowever, in a kitchen and dining area scene, GPT-4V identified \"three chairs,\" while the reference\nanswer indicated \"four.\" This suggests that while GPT-4V is generally reliable, it may occasionally\nmiss objects, particularly if they are partially out of view or arranged in a way that makes them\ndifficult to distinguish clearly. GPT-4o also demonstrated strong performance in recognizing the\nabsence of chairs in the bathroom scene but, like GPT-4V, showed limitations in scenarios involving\nmultiple chairs, as evidenced by a similar miscount in the kitchen and dining area example.\nOn the other hand, Gemini-pro-vision displayed inconsistent performance. In some instances, it\nmiscounted the number of chairs present, such as when it incorrectly identified two chairs in the\nbathroom scene or three chairs in the kitchen scene, where the reference answer indicated four."}, {"title": "5.4.3 GPT-4V Results and Analysis", "content": "In this section, the GPT-4V needs to identify the interior chairs. As you can see, if the chair is not\nrecognized in the picture, GPT-4V will reply that it cannot help. If the chairs appear partly, GPT-4V\nis also difficult to count the number of indoor seats."}, {"title": "5.4.4 GPT-40 Results and Analysis", "content": "In this section, the GPT-4o needs to identify the interior chairs. Compared to GPT-4V, GPT-40 also\nhas fewer cases of refusal and inability to answer. The answers were also more accurate, even when\nthe chairs are partly shown in the pictures."}, {"title": "5.4.5 Gemini Pro Results and Analysis", "content": "Gemini also gives definite answer to count the interior furniture. Compared to GPT, Gemini tries\nto identify the chairs which are only partially shown in the pictures, though the answers might be\nwrong. Some of the furniture are mistakenly identified as chairs in the first picture."}, {"title": "5.5 Interior Length Measurement", "content": null}, {"title": "5.5.1 Data Source", "content": "The estimation of indoor width serves as a crucial task aimed at evaluating the capability of\nmultimodal models to recognize important parameters in"}]}