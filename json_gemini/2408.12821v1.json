{"title": "EXAMINING THE COMMITMENTS AND DIFFICULTIES INHERENT IN MULTIMODAL FOUNDATION MODELS FOR STREET VIEW IMAGERY", "authors": ["Zhenyuan Yang", "Xuhui Lin", "Qinyi He", "Ziye Huang", "Zhengliang Liu", "Hanqi Jiang", "Peng Shu", "Zihao Wu", "Yiwei Li", "Stephen Law", "Gengchen Mai", "Tianming Liu", "Tao Yang"], "abstract": "The emergence of Large Language Models (LLMs) and multimodal foundation models (FMs) has generated heightened interest in their applications that integrate vision and language. This paper investigates the capabilities of ChatGPT-4V and Gemini Pro for Street View Imagery, Built Environment, and Interior by evaluating their performance across various tasks. The assessments include street furniture identification, pedestrian and car counts, and road width measurement in Street View Imagery; building function classification, building age analysis, building height analysis, and building structure classification in the Built Environment; and interior room classification, interior design style analysis, interior furniture counts, and interior length measurement in Interior. The results reveal proficiency in length measurement, style analysis, question answering, and basic image understanding, but highlight limitations in detailed recognition and counting tasks. While zero-shot learning shows potential, performance varies depending on the problem domains and image complexities. This study provides new insights into the strengths and weaknesses of multimodal foundation models for practical challenges in Street View Imagery, Built Environment, and Interior. Overall, the findings demonstrate foundational multimodal intelligence, emphasizing the potential of FMs to drive forward interdisciplinary applications at the intersection of computer vision and language.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) and multimodal foundation models (FMs) have demonstrated considerable promise in the realm of generalized intelligence across a spectrum of tasks and modalities. [1, 2, 3, 4] Through the strategic utilization of substantial datasets and computational resources, exemplified by models such as GPT-4V [5] and Gemini Pro [6], these models attain expansive capabilities through large-scale pretraining, facilitating seamless adaptation to novel data and tasks without prior exposure. [7, 8, 3] Nonetheless, the exploration of their capacity to interpret and utilize visual information has been relatively limited, particularly within specialized domains such as streetscape. This paper undertakes a comprehensive evaluation to scrutinize the capabilities and constraints of GPT-4V and Gemini Pro in the context of Street View Imagery, architecture, and urban planning ."}, {"title": "2 Background", "content": ""}, {"title": "2.1 The Rise of LLMs and Multimodal Models", "content": "The transformative transformer architecture [9], foundational to LLMs, significantly advanced multiple domains, including natural language processing (NLP) and computer vision (CV). The introduction of transformers marked a breakthrough in NLP, paving the way for natural language processing by overcoming limitations inherent in earlier architectures such as Recurrent Neural Networks (RNNs) and Long Short-Term Memory networks (LSTMs). The attention mechanisms [9] allow for the processing of long-range dependencies in text without encountering the aforementioned limitations, thereby significantly enhancing the efficiency and scalability of LLMs, providing a foundational basis for subsequent advancements. GPT series, adopted a probabilistic approach to generating text one token at a time, facilitating flexible and coherent text generation.\nIn the domain of computer vision, models like Vision Transformer (ViT) [10] and Masked Au-toencoders (MAE) [11] played pivotal roles in advancing image classification tasks. The Segment Anything Model (SAM) [12] showcased remarkable generalization capabilities and zero-shot learn-ing, particularly in adapting to diverse aerial and orbital images. Despite challenges, SAM's adaptability holds promise for remote sensing image processing.\nThe growing demand for multimodal models, integrating tasks from computer vision and NLP, is driven by the aspiration for Artificial General Intelligence (AGI). There emerged a necessity to extend LLMs to encompass multimodal intelligence by leveraging pretrained ViT as image encoders and LLMs as interfaces for vision-language inputs, aligning with human multimodal instructions. Models such as GPT-4V and Gemini Pro were specifically designed to process both images and text. Pre-trained on diverse datasets containing both text and images, these models demonstrate robust visual comprehension abilities, thereby expanding the horizons at the intersection of natural language processing, computer vision, and human-AI interaction."}, {"title": "2.2 Street View Imagery", "content": "Street view imagery, serving as a vital data source for capturing urban environmental details, plays a significant role across various fields such as urban planning, traffic management, and public safety. With the rapid advancement of Large Language Models (LLMs) and computer vision technologies, the application of street view imagery in conjunction with LLMs has begun to demonstrate immense potential, offering novel perspectives and tools for urban management and planning. Initially, street view imagery provides rich visual information on urban layouts, architectural features, and the utilization of public spaces. The application of LLMs and computer vision technologies enables the automatic identification and analysis of key elements within these images, such as road signs, traffic flow, and pedestrian density, thereby obtaining real-time data on the operational status of cities. This application significantly enhances the efficiency of urban data collection and analysis, providing accurate data support for urban planning and management decisions. Furthermore, the analysis of street view imagery combined with LLMs supports complex spatial analyses and pattern recognition tasks. For instance, by analyzing the architectural styles and spatial layouts in street view images, LLMs can assist urban planners in identifying historical and cultural areas within cities, evaluating the impact of urban renewal projects on the preservation of historical buildings. Additionally, by recognizing and tracking changes in street view imagery, such as variations in green coverage or the emergence of new constructions, LLMs can provide substantial evidence for urban development trends, supporting sustainable development planning. In the realm of traffic management, the application of street view imagery in conjunction with LLMs also reveals substantial potential. Real-time analysis of street images can monitor traffic volumes, identify congestion points, and even predict potential risk areas for traffic accidents, offering real-time data support for traffic planning and management. Moreover, street view imagery can be utilized for the automatic detection of road and traffic infrastructure damage, aiding city management departments in timely maintenance and repair. However, the application of street view imagery combined with LLMs faces challenges, including how to process and analyze the vast amount of street view image data, ensure the accuracy and reliability of analysis results, and address data privacy and security concerns. Future research needs to focus not only on enhancing technical performance but also on addressing these challenges.\nIn summary, the integration of street view imagery with Large Language Models provides robust technical support for urban planning and management, making the collection and analysis of urban data more efficient and accurate. As technology continues to evolve and its application scope expands, the future is poised to unlock more possibilities for smarter and more sustainable urban development."}, {"title": "2.3 Built Environment", "content": "The built environment refers to human-made surroundings that provide the setting for human activity, including buildings, infrastructure, and public spaces. It has significant impacts on human life, health, and sustainable development. Traditional research on the built environment primarily relies on field surveys, manual drawing, and statistical analysis, which often suffer from inefficiencies and subjectivity [13]. With the advancement of technologies such as remote sensing, Geographic Information Systems (GIS), and Building Information Modeling (BIM), digital and information-based methods have started to be applied to the analysis and planning of the built environment [14]. Particularly in the past decade, deep learning methods have achieved significant progress in fields like computer vision and remote sensing image analysis, bringing new breakthroughs to built environment research [15].\nOne major direction is using convolutional neural networks (CNNs) to extract information about the built environment from remote sensing images and street view images, such as building detection [16], road extraction [17], and land use classification[18]. These methods can automatically and efficiently generate extensive data on the built environment, providing crucial foundational information for urban planning and resource management. Another direction is using generative models (such as GANs) to generate or simulate urban landscapes and street designs [19], aiding in the formulation and evaluation of planning and design schemes.\nLarge models, especially multimodal vision-language pre-trained models, open new possibilities for built environment research. These models can jointly process data from multiple modalities, such as text and images, enabling more comprehensive and semantically rich scene understanding and generation [20]. Preliminary attempts in the built environment field include: using large models for cross-modal land use classification[21], urban functional area recognition[22], generating urban design scheme images from textual descriptions[23], and using visual question answering to analyze urban facilities and public spaces in street view images[24, 25]. The language understanding capabilities of large models can also be used to handle unstructured data, such as planning texts and design guidelines, assisting in tasks like planning approval and compliance checks[26, 27].\nDespite the potential shown by large models in built environment research, the current work is still in its early stages and faces several challenges in practical application. For instance, how to integrate large models with existing planning and design processes and tools; how to evaluate and validate the feasibility and rationality of model-generated schemes; and how to address issues related to data privacy and ethics."}, {"title": "2.4 Interior", "content": "In the field of interior design, computer vision and machine learning technologies have been widely applied. Traditional methods primarily rely on handcrafted features or shallow learning models to analyze and understand interior images[28]. However, these methods often show limitations when dealing with the complexity and diversity of indoor scenes. In recent years, the advent of deep learning, particularly convolutional neural networks (CNNs), has significantly advanced the under-standing of indoor scenes. CNNs can automatically learn hierarchical feature representations of images, achieving remarkable performance improvements in tasks such as image classification, ob-ject detection, and semantic segmentation[29]. A series of CNN-based methods have been proposed for indoor scene classification[30], room type recognition [31], and indoor layout estimation[32].\nBeyond analyzing interior images, the use of deep learning to generate realistic indoor scene images has also attracted attention. Several studies have explored the use of generative adversarial networks (GANs) to generate photorealistic indoor images from noise vectors or semantic layout maps [33, 34]. This provides a new approach for automated and diverse interior design generation. However, current methods for generating indoor images still need improvement in terms of semantic accuracy and detail realism.\nThe emergence of large models, particularly vision-language pre-trained models, has opened new possibilities for interior design. These models are pre-trained on vast amounts of image-text data, learning cross-modal alignments and fusion representations, thereby possessing strong cross-modal understanding and generation capabilities [35]. Applying pre-trained large models to interior design tasks is expected to enable more intelligent and interactive design generation. For example, users can control the style and layout of generated interior design images through text descriptions, or use the visual question-answering capabilities of large models to edit and modify design images by asking questions[36]. Another interesting direction is using visual language navigation models to automatically generate guided tours and explanations for indoor spaces [37].\nAlthough large models have shown promising potential in interior design, current research is still in the exploratory stage and faces several challenges in practical application. For instance, how to convert design images generated by large models into executable design plans; how to evaluate the rationality and usability of generated designs; and whether the generated designs comply with ergonomics and building codes. Moreover, the interpretability and controllability of large models need enhancement, necessitating research on incorporating more prior knowledge and constraints to guide the design generation process."}, {"title": "3 Experiments and Observation of FMs for Street Furniture", "content": ""}, {"title": "3.1 Brand Recognition", "content": ""}, {"title": "3.1.1 Data Source", "content": "In the architectural style and architectural logo identification, the information of famous brand retail stores can be extracted. In this experiment, we selected three famous brands, namely Burger King, Apple, Starbucks and KFC.In the process of logo recognition, it is necessary to extract elements of architectural style and architectural features. The image identified this time comes from the storefront of the four major brands in Google Street View, and the image also includes the logo itself. The dataset can be accessed at https://github.com/fqhwas/architecture."}, {"title": "3.1.2 Evaluation and analysis", "content": "In the brand recognition task, the main task of the model is to recognise the pattern and text of the store logo and accurately locate it with the brand name. GPT-4V's recognition of the store focuses more on the materials and architectural style of the building and the description of the scene. However, even for common brands, GPT-4V has difficulty in recognising them. In contrast, GPT-40 not only accurately recognises brands and logos, but also analyses slogans, design elements, architectural styles, lighting, etc. Similar to GPT-40, Gemini can also accurately identify common store brands, including the text and logo of the brand. Unlike GPT-4V, Gemini does not further analyse the light, architectural style, etc. in the picture after identifying the brand, but instead gives a brief introduction to the business of the brand, which is not in the prompt requirements, showing that Gemini has some divergent thinking and seems to have a better understanding of the real world. In sum, GPT-40 and Gemini both have good zero-shot performance in brand recognition. The latter seems to have stronger divergent thinking, while the former focuses more on the prompt task. The difference may be related to their training data and fine-tuning strategies."}, {"title": "3.2 Counts of Pedestrians", "content": ""}, {"title": "3.2.1 Data Source", "content": "The task of counting pedestrians in street view images primarily aims to assess the fine-grained discrimination capabilities of multimodal models. Pedestrians are a significant component of street scenes, and accurately counting the number of people provides a measure of the model's proficiency. In this section, we utilize data from a public dataset to evaluate this capability. The dataset can be accessed at https://github.com/fqhwas/architecture."}, {"title": "3.2.2 Evaluation and analysis", "content": "In the pedestrian counting task, GPT-4V tends to give interval estimates, such as \"at least 20 people,\" and can further update its interval estimates during the description. GPT-40 and Gemini tend not to describe the picture in detail and give point estimates instead of interval estimates. The interval boundaries of GPT-4v's answers are usually more deviated from the correct answer than the point estimates of GPT-4o and Gemini, but because it gives a larger possible range, its answer is also reasonable. Among them, Gemini is completely correct in a pedestrian counting task with less noise in the foreground, but the answer given in a scene with more noise and a complex background deviates from the correct value relatively far. GPT-40, on the other hand, performed more evenly in both the near and far-field pedestrian counting tasks. This difference between the two models may be related to their training data and fine-tuning strategies.\nAll three models showed good zero-shot performance, indicating that all three models have the ability to identify people in complex scenes and fine-grained recognition capabilities."}, {"title": "3.3 Counts of Cars", "content": ""}, {"title": "3.3.1 Data Source", "content": "The task of counting vehicles in street view images is designed to evaluate the fine-grained discrimi-nation capabilities of multimodal models. Vehicles are a crucial component of street scenes, and accurately counting the number of vehicles provides a measure of the model's proficiency. In this section, we utilize data from a public dataset to conduct this evaluation. The dataset is available at https://github.com/fqhwas/architecture."}, {"title": "3.3.2 Evaluation and analysis", "content": "In the car counting task, GPT-4V refuses to give an answer when there are many cars in the image. For tasks with fewer vehicles, it can add the number of different types and colours of cars in the image to get an answer. The analysis process is reasonable, but it often ignores some of the vehicles in the image. This shows that GPT-4V has the ability to recognise the characteristics of vehicles on the street, but when there are too many vehicles or some of them are partially in the image, it will not be able to given an answer. In contrast, GPT-40 has improved its feature recognition ability in large-scale counting ranges and has a stronger ability to recognise partially occluded objects. For tasks with many vehicles in the image, GPT-40 no longer refuses to answer the question, but instead gives a reference value that is closer to the correct answer. At the same time, when there are relatively few vehicles, GPT-40 can count the vehicles with different characteristics and identify the vehicles that are partially visible in the picture, so the total result given is also more accurate than GPT-40. When there are many vehicles, Gemini is able to give an answer, and the answer is usually accurate, which shows that it has the ability to recognise vehicles. When there are few vehicles, even partially occluded vehicles seem to be accurately included, but it does not provide a specific explanation of the estimation process, so this needs to be verified.\nIn sum, in the car counting task, Gemini and GPT-4o have shown high-level fine-grained recognition capabilities, especially in scenarios with large-scale counting and partially occluded vehicles."}, {"title": "3.4 Road Width Measurement", "content": ""}, {"title": "3.4.1 Data Source", "content": "The estimation of street width plays a crucial role in assessing the ability of multimodal models to identify important parameters in urban streetscapes. When processing streetscape images, streets are integral components, and estimating their scale measures the capability of large-scale models. In this section, the data utilized is sourced from public datasets, accessible at https://github.com/fqhwas/architecture."}, {"title": "3.4.2 Evaluation and analysis", "content": "In the road width measurement task, we gave the general length of a single lane, the general height of a floor, and the general height of a person as a reference in the prompt, and asked the three models to infer the length of the single-sided motor lane based on the information in the figure. GPT-4V often refused to answer and ignored the prompts about the length of the lane. Based on the answers and reasoning process of GPT-4V, it has certain difficulties in identifying and inferring the direction of traffic. Gemini tends to give answers directly without reasoning, and its answers are usually 1-2 times higher than the real ones. According to the information about the traffic in the figure, this is also because it does not correctly identify the direction of traffic, thus adding up the lanes of different lanes together. GPT-4o showed excellent ability in this task. It was able to identify lanes that were partially blocked by cars in the foreground and partially obscured in the background, and it was able to integrate information such as the direction of the vehicles in the picture and road signs to determine the lane direction, thus giving answers that were close to the real values in each task. Although in the case of ambiguous vehicle information and no obvious road signs, it may misjudge the lane direction and thus give incorrect measurements, this is reasonable based on the data in the picture.\nIn sum, GPT-4o can use the information in the figure to the greatest extent to eliminate the interference caused by the occlusion of the near field and the blurring of the far field, and give more accurate answers. Gemini's application of comprehensive information is still insufficient, and GPT-4V's comprehensive information ability is even more lacking. GPT-40 has better zero-shot performance and achieved high quantitative accuracy in terms of road width measurement when given a reference for road length. However, its own quantitative accuracy regarding width measurement needs to be further tested when there is no reference for road length."}, {"title": "4 Experiments and Observation of FMs for Built Environment", "content": ""}, {"title": "4.1 Buildings Functions Classification", "content": ""}, {"title": "4.1.1 Data Source", "content": "The classification of building functions is primarily aimed at evaluating the ability of multimodal models to process and classify large volumes of building information. Buildings constitute the primary elements of streetscapes. In this section, functional recognition involves the computer's identification of the architectural style, building materials, and other major features of buildings. The data utilized is sourced from public datasets accessible at https://github.com/fqhwas/architecture."}, {"title": "4.1.2 Evaluation and analysis", "content": "In the buildings functions classification task, we gave six types of buildings in the prompt that the buildings in the figure might belong to, and asked the three models to classify the buildings in the figure to the six categories. All three models completed the identification of all buildings perfectly, and GPT-4V and GPT-40 both gave detailed reasoning processes, which showed that they had already been able to make comprehensive inferences from the architectural style, architectural elements, logos, text outside the building, and the possible functions of the components in the building. Gemini did not give a detailed reasoning process, but its correct answers show that it should have similar capabilities.\nIn sum, all three models seem to have a high ability to classify buildings, and they can use the buildings themselves and surrounding elements to make inferences. Their zero-shot performance is very good."}, {"title": "4.2 Buildings Age Analysis", "content": ""}, {"title": "4.2.1 Data Source", "content": "While building age is an important parameter in building specifications, the data is not always available or complete[38]. Based on the first attempt for estimating building age from Google Street View images by using deep learning techniques, we made experiments on the picture of their attempt to testify whether GPT-4V, GPT-4o and Gemini can estimate the age of buildings. We chose four buildings of different styles, which were built in different eras, 10 to 20 years apart."}, {"title": "4.2.2 Evaluation and analysis", "content": "In the building age prediction task, GPT-4V is able to give an approximate age of a building, GPT-4V not only describes the building materials, architectural styles, and exterior spaces of a building, but also gives accurate answers up to ten years. These answers are based on statistical data of different architectural styles in different periods. In addition, the quality of the images helps to confirm the probable age of the building. Similarly, the GPT-40 is able to give approximate building dates. In contrast to the GPT-4V, the GPT-40 provided categorisation and description of architectural details such as roof shape, windows, elevations, building materials, layout and landscaping. Answers were more organised, dating the building in terms of architectural style and referencing answers closer to the correct answer. Most of the answers given by Gemini were correct and only one answer was far from the truth. The reference answer also provides a description of the building's appearance and structure, which helps to confirm the possible age of the building.\nIn sum, all three models were able to speculate on the date of completion of the building based on factors such as image quality, architectural style, architectural details and landscaping, the zero-shot performance is quite good among all three models."}, {"title": "4.3 Building Height Analysis", "content": ""}, {"title": "4.3.1 Data Source", "content": "The task of estimating building height is primarily aimed at evaluating the ability of multimodal models to identify important parameters in urban streetscapes. This assessment involves scale comparisons with human reference points and estimations based on perspective, thereby measuring the capability of large-scale models. In this section, the data utilized is sourced from public datasets accessible at https://github.com/fqhwas/architecture. This provides a comprehensive foundation for the analysis and evaluation of building height estimation."}, {"title": "4.3.2 Evaluation and analysis", "content": "In the building height analysis task, GPT-4V carefully described the inference process. Judging from the output results of the three selected images, the accuracy of identifying building heights is not high, and there is a large gap with the real values. GPT-4V mainly calculates the height of the building by referring to the height and number of floors of common objects around the building. When the view of the building in the picture is not clear, there is partial occlusion, or the direction of the building is not clear, and the building itself cannot be correctly identified, the answer often varies greatly. Therefore, in the final answer, GPT-4V was unable to answer. Compared to GPT-4V, GPT-40 no longer refuses to answer questions. Even if the final answer is somewhat different from the correct answer, GPT-4o tries to give the calculation process and estimation method. In larger scale photos, the error of GPT-40 will be greater. Gemini explains the calculation method in detail, and its estimation results are far from the correct answer.\nIn sum, the zero-shot performance of the three models in estimating the height of the building is average, which is mainly related to the fact that the building is partially obscured, the perspective effect is too strong, and the shooting angle is somewhat distorted, which makes it impossible for the model to correctly count the number of floors of the building. To improve the model's ability in this regard, it is necessary to increase the training data accordingly to help the model develop the ability to estimate the number of floors in the presence of occlusion and perspective effects."}, {"title": "4.4 Building Structure Classification", "content": ""}, {"title": "4.4.1 Data Source", "content": "Rapid and accurate identification of potential structural deficiencies is a crucial task in evaluating seismic vulnerability of large building inventories in a region[39].For the observer, it is easy to see with the eye whether there is an open space on the ground floor of the building. Using street View images which had already been classified by deep learning, we further tested whether Gemini and GPT could successfully identify soft-story buildings. In the three examples, only one architecure is a soft-story building with the open ground."}, {"title": "4.4.2 Evaluation and analysis", "content": "In the building structure classification task, the model needs to identify whether the building in the picture has open spaces such as garages on the ground. GPT-4V can describe the important appearance and structural features of soft-story buildings and make judgments accordingly. In addition to giving a definitive conclusion, GPT-4V can also describe the appearance and possible functions of the building. As shown in the figure, if the building in the picture has obvious ground-level opening features, GPT-4V can correctly identify the building type. Similarly, GPT-40 can also determine whether the building in the picture has open space and whether the upper floor of the building is a rigid structure. GPT-4V and GPT-40 are both accurate in their judgments of soft-story buildings. In contrast, Gemini not only provides an answer for the building type, but also helps to confirm the building function and the possible risk during an earthquake. It tends to identify the building as a soft-story building, but two of the four answers are incorrect. Based on the inference process it gives, it is not very accurate in inferring the use of the ground floor of the building based on the picture, always tending to think that the ground floor is a garage or commercial space, which is a misjudgment.\nIn sum, GPT-4V and GPT-40 both show strong one-shot performance in soft-story building classifi-cation, while Gemini is relatively poor in this regard. Perhaps providing a closer or clearer image can help improve its task performance."}, {"title": "5 Experiments and Observation of FMs for Interior", "content": ""}, {"title": "5.1 Interior Room Classification", "content": ""}, {"title": "5.1.1 Data Source", "content": "The task of interior building function recognition is primarily aimed at evaluating the suc-cess of multimodal models in identifying architectural styles and landmarks. When processing streetscape images, large-scale models need to be capable of recognizing specific functional spaces within buildings. In this section, the data utilized is sourced from public datasets accessible at https://github.com/fqhwas/architecture. This provides a robust foundation for assessing the recogni-tion of interior building functions with respect to architectural styles and landmarks"}, {"title": "5.2 Evaluation and analysis", "content": "In the indoor room classification task, the performance of models needs to be evaluated across multiple dimensions, including the precise recognition of objects within the room, understanding of spatial layout, and the ability to distinguish between different functional areas.\nGPT-4V has demonstrated robust capabilities in both recognition and classification, accurately categorizing rooms by identifying the core elements present. For example, when the image includes objects such as a sink, mirror, and bathtub, which are typically associated with a bathroom, GPT-4V swiftly identifies the room as a bathroom. Additionally, it goes further by describing the arrangement of these objects, such as the placement of the mirror and towels, thus offering not only a justified conclusion but also contextual explanations that enhance its classification. GPT-4V's strength lies not only in its ability to classify single-function rooms but also in its outstanding performance in handling complex, multi-functional areas. For instance, when an image depicts a space that combines both a kitchen and a dining area, GPT-4V not only recognizes the primary features of the kitchen\u2014such as the refrigerator, cabinets, and stove\u2014but also notes the presence of dining-related elements like a table and chairs. This allows GPT-4V to classify the room as a combined kitchen and dining space, rather than limiting it to a single function. This nuanced spatial awareness significantly improves its performance in complex scenarios.\nIn comparison, GPT-4o also performs reliably but exhibits less detail when processing multi-functional areas. For example, in an image featuring a combined kitchen and dining space, GPT-40 may correctly identify the primary characteristics of the kitchen, but it might fall short when describing the room's combined functions. This suggests that GPT-40 tends to favor single-function classifications and may lack the detailed perception needed to capture all functional areas in more complex scenes.\nGemini-pro-vision, on the other hand, shows relatively average accuracy in classification, especially when dealing with multi-functional rooms. Although it can recognize some of the key features in an image, it may focus on a single area while overlooking other functional spaces. For instance, in an image depicting both a kitchen and dining area, Gemini-pro-vision might be more inclined to classify the space as a kitchen, neglecting the presence of the dining area. This issue could stem from limitations in its understanding of the functional elements within the image, or from its weaker ability to perceive the overall spatial layout. As a result, Gemini-pro-vision's classification accuracy tends to decline when faced with complex scenarios.\nOverall, GPT-4V performs the best in indoor room classification tasks, particularly excelling in the identification and description of multi-functional areas. It not only provides accurate classifications but also offers thorough contextual explanations that enhance the reliability of its decisions. While GPT-40 demonstrates stable performance, its ability to capture fine details in complex room settings is slightly inferior to that of GPT-4V. Gemini-pro-vision, however, performs relatively weaker, especially when dealing with multi-functional areas, where its judgment accuracy is more prone to errors."}, {"title": "5.2.1 GPT-4V Results and Analysis", "content": "In this section, the task of the GPT is to identify the type of room in the house. The results showed that GPT was able to accurately describe the main furniture in the room and make judgments based on this, and all judgments were accurate."}, {"title": "5.2.2 GPT-40 Results and Analysis", "content": "In this section, the task of the GPT is to identify the type of room in the house.GPT-4o is also a description of the indoor furnishings. Within the test range, the indoor function judgment is correct."}, {"title": "5.2.3 Gemini Pro Results and Analysis", "content": "Compared to GPT, Gemini tends to give a definitive answer without further explanation. All of the answers are correct, showing Gemini's ability to judge the function of interior Spaces."}, {"title": "5.3 Interior Design Style Analysis", "content": ""}, {"title": "5.3.1 Data Source", "content": "The task of architectural style recognition, encompassing interior style recognition, is primarily aimed at evaluating the success of multimodal models in classifying buildings. When process-ing streetscape images, large-scale models require extensive training based on a wide range of architectural styles. In this section, the data utilized is sourced from public datasets accessible at https://github.com/fqhwas/architecture. This encompasses a comprehensive collection of architec-tural features for model training and evaluation."}, {"title": "5.3.2 Evaluation and analysis", "content": "In the interior design style classification task, models must evaluate a variety of visual elements to determine the style of the space accurately. This requires an understanding of design principles, including color schemes, furniture types, materials used, and the overall aesthetic feel.\nGPT-4V excels in identifying design styles that combine multiple elements or blend aesthetics. In one instance, it accurately identified a Scandinavian-style interior based on the use of natural light, neutral color palettes, minimalist furniture, and the presence of organic materials like wood. The model's analysis goes beyond mere surface-level observation, as it provides a detailed reasoning process by explaining the use of space, light, and materials. This demonstrates GPT-4V's capacity for nuanced understanding in style classification, especially when rooms feature design elements typical of multiple styles.\nIn contrast, GPT-40 tends to offer more straightforward classifications, often focusing on the dominant design elements. While it is competent in recognizing the primary style, it lacks the deeper contextual analysis that GPT-4V provides. For example, when analyzing a room with a bold and playful color scheme, GPT-40 correctly identified it as modern, but GPT-4V took this analysis further by highlighting specific elements like the unique design of furniture and color contrasts, which added depth to the classification.\nGemini-pro-vision, while capable of recognizing some design elements, often mis-classifies the overall style. For instance, in a room featuring classic antique furniture, Gemini-pro-vision incor-rectly classified the style as \"Korea Asia style.\" This suggests that Gemini-pro-vision struggles to accurately combine the various visual clues into a coherent style classification, especially when faced with subtle variations or blended styles. This model may rely too heavily on a specific set of features, leading to misinterpretation of the overall aesthetic.\nOverall, GPT-4V consistently outperforms the other models by offering not only accurate clas-sifications but also well-supported reasoning that considers multiple layers of design elements. GPT-40 is reliable in identifying dominant styles, but it lacks the depth of analysis provided by GPT-4V. Gemini-pro-vision, on the other hand, exhibits weaknesses in distinguishing between styles, particularly when faced with complex or blended designs."}, {"title": "5.3.3 GPT-4V Results and Analysis", "content": "In this section, GPT-4V is tasked with identifying 8 styles of interior architecture. GPT-4V describes in detail the furniture styles and architectural details identified in the pictures, and gives the answers identified.However, because of the beautiful curves and elegant style of the furniture in the pictures, GPT-4V had difficulty recognizing the difference between different interior decoration styles. At the same time, GPT-4V also tries to distinguish different styles through interior colors and materials. However, except for the classical style, all the other judgments are wrong."}, {"title": "5.3.4 GPT-40 Results and Analysis", "content": "In this section, GPT-4o is tasked with identifying 8 styles of interior architecture. Compared with GPT-4V, GPT-40 has more comprehensive interior design style data and more accurate style type judgment. In the answer content, GPT-4o analyzes the color, line, furniture, decoration, etc., in the picture."}, {"title": "5.3.5 Gemini Pro Results and Analysis", "content": ""}]}