{"title": "AnyAttack: Towards Large-scale Self-supervised Generation of Targeted Adversarial Examples for Vision-Language Models", "authors": ["Jiaming Zhang", "Junhong Ye", "Xingjun Ma", "Yige Li", "Yunfan Yang", "Jitao Sang", "Dit-Yan Yeung"], "abstract": "Due to their multimodal capabilities, Vision-Language Models (VLMs) have found numerous impactful applications in real-world scenarios. However, recent studies have revealed that VLMs are vulnerable to image-based adversarial attacks, particularly targeted adversarial images that manipulate the model to generate harmful content specified by the adversary. Current attack methods rely on predefined target labels to create targeted adversarial attacks, which limits their scalability and applicability for large-scale robustness evaluations. In this paper, we propose AnyAttack, a self-supervised framework that generates targeted adversarial images for VLMs without label supervision, allowing any image to serve as a target for the attack. To address the limitation of existing methods that require label supervision, we introduce a contrastive loss that trains a generator on a large-scale unlabeled image dataset, LAION-400M dataset, for generating targeted adversarial noise. This large-scale pre-training endows our method with powerful transferability across a wide range of VLMs. Extensive experiments on five mainstream open-source VLMs (CLIP, BLIP, BLIP2, InstructBLIP, and MiniGPT-4) across three multimodal tasks (image-text retrieval, multimodal classification, and image captioning) demonstrate the effectiveness of our attack. Additionally, we successfully transfer AnyAttack to multiple commercial VLMs, including Google's Gemini, Claude's Sonnet, and Microsoft's Copilot. These results reveal an unprecedented risk to VLMs, highlighting the need for effective countermeasures.", "sections": [{"title": "1 Introduction", "content": "Vision-Language Models (VLMs) have exhibited remarkable performance across a diverse array of tasks, primarily attributed to the scale of training data and model size (Radford et al., 2021; Li et al., 2023; Zhu et al., 2024). Despite their remarkable performance, these models, heavily reliant on visual inputs, remain vulnerable to image-based adversarial attacks\u00b9, which are carefully crafted input images designed to mislead the model into making incorrect predictions (Szegedy et al., 2013). While general, untargeted adversarial attacks only aim to induce incorrect outputs, targeted adversarial attacks present a more insidious threat, manipulating the model's output to yield an adversary-specified, predetermined response. For instance, a benign image such as a landscape could be subtly altered to elicit harmful text descriptions such as \u201cviolence\" or \"explicit content\""}, {"title": "2 Related Work", "content": "Targeted Adversarial Attacks A number of works have been proposed to enhance the effectiveness and transferability of targeted adversarial attacks against vision models. Input augmentation techniques like image translation (Dong et al., 2019), cropping (Wei et al., 2023), mixup (Wang et al., 2021; Liu & Lyu, 2024), and resizing (Xie et al., 2019), have been employed to increase the"}, {"title": "Jailbreak Attacks on VLMs", "content": "VLMs have revolutionized DNNs by leveraging large-scale pre-training on diverse image-text datasets. These models learn to integrate visual and textual information effectively, enabling superb performance across a wide range of tasks. Broadly, VLMs can be categorized into two types: the first offers multimodal functionalities built on large language models (LLMs), complemented by visual models, such as BLIP2 (Li et al., 2023), InstructBLIP (Dai et al., 2023), and MiniGPT-4 (Zhu et al., 2024). The second type provides a more balanced approach, bridging textual and visual modalities efficiently, as seen in models like CLIP (Radford et al., 2021), ALIGN (Jia et al., 2021), and BLIP (Li et al., 2022). Recent advancements in VLMs have spurred parallel research into their vulnerabilities, with jailbreaks and adversarial attacks emerging as distinct threat vectors. Multimodal jailbreaks primarily exploit cross-modal interaction vulnerabilities in VLMs, with the intention of influencing LLMs (Zou et al., 2023). These attacks manipulate inputs of text (Wu et al., 2023), images (Carlini et al., 2024; Gong et al., 2023; Qi et al., 2024; Niu et al., 2024), or both simultaneously (Ying et al., 2024; Wang et al., 2024), aiming to elicit harmful but non-predefined responses. In contrast, image-based adversarial attacks focus on manipulating the image encoder of VLMs, typically leaving the LLM component largely undisturbed. The objective is to induce adversary-specified, predetermined responses through precise visual manipulations. Understanding these differences is crucial for explaining our methodology."}, {"title": "Adversarial Attacks on VLMs", "content": "Adversarial research on VLMs is relatively limited compared to the extensive studies on vision models, with the majority of existing attacks focusing primarily on untargeted attacks. Co-Attack (Zhang et al., 2022) was among the first to perform white-box untargeted attacks on several VLMs. Following this, more approaches have been proposed to enhance adversarial transferability for black-box untargeted attacks (Lu et al., 2023; Zhou et al., 2023; Yin et al., 2024; Xu et al., 2024). Cross-Prompt Attack (Luo et al., 2024) investigates a novel setup for adversarial transferability based on the prompts of language models. AttackVLM (Zhao et al., 2024) is the most closely related work, using a combination of text inputs and popular text-to-image models to generate guided images for creating targeted adversarial images. Although their approach shares a similar objective with our work, our method distinguishes itself by being self-supervised and independent of any text-based guidance."}, {"title": "3 Proposed Attack", "content": "In this section, we first present the preliminaries on targeted adversarial attacks and then introduce our proposed AnyAttack and its two phases (i.e., pre-training and fine-tuning)."}, {"title": "3.1 Problem Formulation", "content": "Threat Model This work focuses on transfer-based black-box attacks, where the adversary generates an adversarial image x' using a fully accessible pre-trained surrogate model fs. The adversary has no knowledge of the target VLMs ft, including its architecture and parameters, nor can they leverage the outputs of ft to reconstruct adversarial images. The adversary's objective is to cause the target VLM ft to incorrectly match the adversarial image x' with the target text description yt.\nWe begin by formulating the problem of targeted adversarial attacks. Let fs represent a pre-trained surrogate model, and D = {(x,y)} denote the image dataset, where x is the original image and y is the corresponding label (description). The attacker's objective is to craft an adversarial example x' = x + d that misleads the target model ft into predicting a predefined target label yt. In the context of VLMs, this objective requires that x' aligns with yt as a valid image-text pair. The process of generating targeted adversarial images typically involves finding a perturbation d using the surrogate model fs. Existing strategies can be approached through two primary strategies: target label supervision and target image supervision.\nThe first approach utilizes the target label yt as supervision, directing the embedding of the adversarial image x' to align with that of yt, as demonstrated in AttackVLM-it Zhao et al. (2024). The second approach employs the target image xt, which corresponds to yt, as supervision to encourage the embedding of x' to replicate that of xt. This is illustrated in AttackVLM-ii (Zhao et al., 2024) and certain image-based attacks (Wei et al., 2023; Wu et al., 2024). Both methods depend on explicit target supervision, as summarized in Table 1. In these approaches, L denotes a distance-based loss function, such as Euclidean distance or cosine similarity. In contrast, our method, AnyAttack, employs the input image itself to guide the attack and thus is unsupervised. In this context, x, is a random image that is unrelated to x, while the adversarial noise d is designed to align with the original image x within the surrogate model's embedding space. In summary, existing methods generate adversarial noise that mimics other images, whereas our approach produces adversarial noise that closely resembles the original image itself."}, {"title": "3.2 AnyAttack", "content": "Framework Overview Our proposed framework, AnyAttack, employs a two-stage training paradigm: pre-training and fine-tuning. Figure 2 provides a framework overview of AnyAttack.\nIn the pre-training stage, we train a decoder F, to produce adversarial noise d on large-scale datasets Dp. Given a batch of images x, we extract their embeddings using a frozen image encoder E. These normalized embeddings z are then fed into the decoder F, which generates adversarial noise \u03b4 corresponding to the images x. To enhance generalization and computational efficiency, we introduce a K-augmentation strategy that creates multiple shuffled versions of the original images within each mini-batch. During this process, adversarial noise is added to the shuffled original images (unrelated images) to produce the adversarial images. After passing through E, we employ a contrastive loss to maximize the cosine similarity between positive sample pairs (the i-th elements of the adversarial and original embeddings) while minimizing the similarity between negative sample pairs (the remaining elements). This approach trains the decoder F to ensure that the perturbed images resemble the original images in the embedding space of encoder E, while distinguishing them from the shuffled versions.\nIn the fine-tuning stage, we adapt the pre-trained decoder F to a specific downstream dataset Df. The frozen encoder E continues to provide embeddings that guide the generation of adversarial noise \u03b4. We use an unrelated random image xr from an external dataset De as the clean image to synthesize the adversarial image xr + \u03b4. Unlike the pre-training stage, where only the encoder is"}, {"title": "3.2.1 Pre-training Stage", "content": "The pre-training phase of AnyAttack aims to train the generator on large-scale datasets, enabling it to handle a diverse array of input images as potential targets. Given a batch of n images x \u2208 Rn\u00d7H\u00d7W\u00d73 from the large-scale training dataset Dp, we employ the CLIP ViT-B/32 image encoder, which is frozen during training, as the encoder E, to obtain the normalized embeddings E(x) = z \u2208 Rn\u00d7d corresponding to the original images x, where d represents the embedding dimension (i.e., 512 for CLIP ViT-B/32). Subsequently, we deploy an initialized decoder F, which maps the embeddings z to adversarial noise D(z) = \u03b4 \u2208 Rn\u00d7H\u00d7W\u00d73 corresponding to the original images x. We expect the generated noises \u03b4 to serve as adversarial noise representative of the original images x. Our goal is for the generated noises \u03b4, when added to random images xr, to be interpreted by the encoder E as the original images x, i.e., E(xr + \u03b4) = E(x).\nHowever, when the number of random images is smaller than the training dataset, the generated noises \u03b4 may overfit to this limited set, leading to poor generalization for F. To address this, we propose the K-augmentation strategy, which expands the set of random images to match the size of the training dataset Dp. This strategy increases the number of sample pairs within each batch by a factor of K, thereby improving computational efficiency. Specially, K-augmentation duplicates both adversarial noises \u03b4 and the original images x K times, forming K mini-batches. For each mini-batch, the order of the adversarial noises remain consistent, while the order of the original images is shuffled within the mini-batch, referred to as shuffled images. These shuffled images are then added to the corresponding adversarial noise, resulting in adversarial images x'. Next, the adversarial images are fed into F to produce adversarial embeddings z(adv), which are then used for subsequent calculations against the original embeddings z. We introduce a contrastive loss that maximizes the cosine similarity between positive sample pairs, defined by the i-th elements of adversarial and original embeddings in each mini-batch, while minimizing the similarity between the negative pairs, which consist of all other elements. This setup creates n positive pairs and n(n - 1) negative pairs in every mini-batch, with gradients accumulated to update F:\n\nLCon = 1/n \u03a3i=1n log(exp((zi \u00b7 zi^(adv))/\u03c4) / \u03a3j=1n exp((zi \u00b7 zj^(adv))/\u03c4)) (1)\nwhere zi and zi^(adv) are the l2-normalized embeddings of the i-th sample from original images x and adversarial images x'. \u03c4(t) is the temperature at step t, enabling the model to dynamically adjust the hardness of negative samples during training. To facilitate learning and convergence in early training, we set a relatively large initial temperature \u03c40 at the beginning of training and gradually"}, {"title": "3.2.1 Fine-tuning Stage", "content": "In the fine-tuning stage, we refine the pre-trained decoder F on downstream vision-language datasets using task-specific objective functions, facilitating its adaptation to particular domains and multimodal tasks. The motivation for fine-tuning arises from scenarios where well-defined multimodal tasks and in-domain images are available. Given a batch of n images x \u2208 Rn\u00d7H\u00d7W\u00d73 from the downstream dataset Df, the encoder E remains frozen and outputs the embeddings z, which are then fed into the decoder F to generate the noise \u03b4. We randomly select images from an external dataset De as unrelated images xr, which are then added to the generated noise \u03b4 to create adversarial images. To improve transferability, we incorporate auxiliary models alongside the encoder E, forming an ensemble surrogate. Drawing on research in ensemble learning for adversarial attacks (Liu et al., 2017), we select auxiliary models based on model diversity, as greater differences between models are known to improve complementarity. This ensures that both adversarial and original images maintain consistency across the embedding spaces of multiple models.\nDepending on the downstream tasks, we employ two different fine-tuning objectives. The first strategy is tailored for the image-text retrieval task, which imposes stricter requirements for distinguishing between similar samples. It demands robust retrieval performance in both directions: from z(adv) to z and from z to z(adv). This motivates the adoption of a bidirectional InfoNCE loss:\n\nLBi = 1/2n \u03a3i=1n(- log(exp(zi \u00b7 z_i^(adv)/\u03c4) / \u03a3j=1n exp(zi \u00b7 z_j^(adv)/\u03c4)) - log(exp(z_i^(adv) \u00b7 zi/\u03c4) / \u03a3j=1n exp(z_i^(adv) \u00b7 zj/\u03c4))) (3)\nThe second strategy is suited for general tasks, such as image captioning, multimodal classification, and other broad vision-language applications. It requires zadu) to match zi, so we employ cosine similarity to align z z(adv) with zi, denoting this objective as Lcos."}, {"title": "4 Experiments", "content": "In this section, we evaluate the performance of our proposed attack across multiple datasets, tasks, and VLMs. We evaluate the effectiveness of targeted adversarial attacks first in image-text retrieval tasks, then multimodal classification tasks, and finally image captioning tasks. Additionally, we analyze the performance of targeted adversarial images on commercial VLMs."}, {"title": "4.1 Experimental Setup", "content": "Baselines We first employed the state-of-the-art (SOTA) targeted adversarial attack for VLMs, referred to as AttackVLM (Zhao et al., 2024). This method includes two variations: AttackVLM-ii and AttackVLM-it, which are based on different attack objectives. Both methods utilize the CLIP ViT-B/32 image encoder as the surrogate model, consistent with our approach. Additionally, we incorporated two targeted adversarial attacks designed for visual classification models: SU (Wei et al., 2023) and SASD-WS (Wu et al., 2024). Since the original cross-entropy loss used in these methods is not suitable for vision-language tasks, we modified them to employ cosine loss and mean squared error (MSE) loss to match targeted images. These modified methods are denoted as SU-Cos/SASD-WS-Cos and SU-MSE/SASD-WS-MSE, respectively. For the SU attack, the surrogate model is configurable, and we set it to align with our proposed method, namely, CLIP ViT-B/32. For the SASD-WS attack, we utilized the officially released weights, as its surrogate model includes a self-enhancement component. We denote our proposed methods as AnyAttack-Cos, AnyAttack-Bi, AnyAttack-Cos w/ Aux, and AnyAttack-Bi w/ Aux. These represent AnyAttack fine-tuned with LCos, fine-tuned with L\u00dfi, fine-tuned with Lcos using auxiliary models, and fine-tuned with L\u00dfi using auxiliary models, respectively.\nDatasets, Models, and Tasks For the downstream datasets, we utilize the MSCOCO, Flickr30K, and SNLI-VE datasets. We employ a variety of target models, including CLIP, BLIP, BLIP2, InstructBLIP, and MiniGPT-4. The downstream tasks we focus on are image-text retrieval, multimodal"}, {"title": "Metric", "content": "In this work, we examine perturbations constrained by the l\u221e norm, ensuring that the perturbation \u03b4 satisfies the condition ||\u03b4||\u221e \u2264 \u03b5, where \u03b5 defines the maximum allowable magnitude of the perturbation. We use the attack success rate (ASR) as the primary evaluation metric to assess the performance of targeted adversarial attacks. The calculation of ASR varies slightly depending on the specific task. For instance, in image-text retrieval tasks, ASR represents the recall rate between adversarial images and their corresponding ground-truth text descriptions. In multimodal classification tasks, ASR refers to the accuracy of correctly classifying pairs of \"adversarial image and ground-truth description.\" Essentially, ASR is calculated by replacing clean images with their adversarial counterparts and then computing the relevant task-specific evaluation metric."}, {"title": "Implementation Details", "content": "We pre-trained the decoder for 520,000 steps on the LAION-400M dataset (Schuhmann et al., 2021), using a batch size of 600 per GPU on three NVIDIA A100 80GB GPUs. The optimizer used was AdamW, with an initial learning rate of 1\u00d710\u22124, which was adjusted using cosine annealing. For the downstream datasets, we fine-tuned the decoder for 20 epochs using the same optimizer, initial learning rate, and cosine annealing schedule. We deployed two auxiliary"}, {"title": "4.2 Evaluation on Image-Text Retrieval", "content": "In this subsection, we compare the performance of our method against baseline approaches on the image-text retrieval task. Table 2 presents the results on the MSCOCO dataset, while results on the Flickr30K dataset are detailed in the Appendix. The following key observations can be made:\n\u2022 Performance of AnyAttack-Bi w/ Auxiliary: This variant achieves significantly superior performance compared to all baselines, surpassing the best-performing baseline by 15.02%, 18.44%, and 18.54% on ViT-B/16, ViT-B/32, and ViT-L/14, respectively. All AnyAttack methods consistently deliver competitive results, outperforming most baselines. This highlights the effectiveness of our proposed method.\n\u2022 Effectiveness of the Auxiliary Module: The Auxiliary module demonstrates its effectiveness, providing improvements of 6.455%, 13.75%, and 15.875% on ViT-B/16, ViT-B/32, and ViT-L/14, respectively, when comparing AnyAttack w/ Auxiliary to AnyAttack.\n\u2022 Advantages of Bidirectional InfoNCE Loss: The bidirectional InfoNCE loss L\u00dfi shows clear advantages for retrieval tasks, with AnyAttack-Bi consistently outperforming AnyAttack-Cos."}, {"title": "4.3 Evaluation on Multimodal Classification", "content": "Here, we compare the performance of our attack with the baselines on the multimodal classification task. Table 3 presents the results on the SNLI-VE dataset. Our method, AnyAttack-Cos w/ Auxiliary, achieves the highest performance, surpassing the strongest baseline, SASD-WS-MSE, by 20.0%. This underscores the effectiveness of our attack in multimodal classification tasks."}, {"title": "4.4 Evaluation on Image Captioning", "content": "Here, we evaluate the performance of our attack on the image captioning task using the MSCOCO dataset. The VLMs take adversarial images as input and generate text descriptions, which are then assessed against the ground-truth captions using standard metrics. Table 4 presents the results across four VLMs: InstructBLIP, BLIP2, BLIP, and MiniGPT-4. Our attack AnyAttack-Cos w/ Auxiliary consistently demonstrates superior performance across all evaluation metrics, outperforming the baseline attacks on each VLM."}, {"title": "4.5 Transfer to Commercial VLMS", "content": "Here, we transfer the targeted adversarial images generated by the pre-trained decoder (referred to as AnyAttack-Pre) to three commercial VLMs, including Claude's Sonnet, Microsoft's Copilot, and Google's Gemini. We utilized the publicly available web interfaces of these models. Figure 3 illustrates the example responses of the three commercial models, with more examples are provided in the Appendix. No prior conversation context or constraints were applied; the only prompt used was \"Describe this image.\u201d The portions of the VLM responses highlighted in red correspond to the target images, showcasing the effectiveness of our attack."}, {"title": "4.6 Further Analysis", "content": "Ablation Study We perform an ablation study on the MSCOCO dataset for the image-text retrieval task to evaluate the impact of three key components in our approach: 1) Training approach: Pre-"}, {"title": "Efficiency Analysis", "content": "In this subsection, we compare the efficiency of our method with SU, SASD, and AttackVLM. Figure 5 presents the results for generating 1,000 adversarial images on a single NVIDIA A100 80GB GPU with a batch size of 250, showing both memory usage and time consumption. The results demonstrate"}, {"title": "5 Conclusion", "content": "In this paper, we introduced AnyAttack, a novel self-supervised framework for generating targeted adversarial attacks on VLMs. Our approach overcomes the scalability limitations of previous methods by enabling the use of any image to serve as a target for attack target without label supervision. Through extensive experiments, we demonstrated the effectiveness of AnyAttack across multiple VLMs and vision-language tasks, revealing significant vulnerabilities in state-of-the-art models. Notably, our method showed considerable transferability, even to commercial VLMs, highlighting the broad implications of our findings.\nThese results underscore the urgent need for robust defense mechanisms in VLM systems. As VLMs become increasingly prevalent in real-world applications, our work opens new avenues for research in VLM security, particularly considering that this is the first time pre-training has been conducted on a large-scale dataset like LAION-400M. This emphasizes the critical importance of addressing these challenges. Future work should focus on developing resilient VLMs and exploring potential mitigation strategies against such targeted attacks."}, {"title": "A Appendix", "content": "In this appendix, we describe more implementation details and additional experiment results."}, {"title": "A.1 Implementation Details", "content": "In this section, we provide additional details regarding the experimental setup. For AttackVLM, we utilized the official code\u00b2, while for SU and SASD-WS, we employed the TransferAttack tool\u00b3. For VLMs integrated with LLMs, we used the LAVIS library and the MiniGPT-4 repository5. More details are provided in Table 5. Regarding MiniGPT-4, it tends to generate detailed responses, even when the prompt \u201cDescribe this image in one short sentence only\u201d. Occasionally, it outputs multiple sentences, which affects its scoring in the image captioning task."}, {"title": "A.2 Additional Experiment Results", "content": "Additional Results on Image-text Retrieval We also report the retrieval performance of our method and the baseline methods on the Flickr30k dataset, with results shown in Table 6. The conclusions are consistent with those obtained from the MSCOCO dataset.\nAdditional Results on Image Captioning We present additional examples of the Image Captioning task in Figures 6, 7, and 8. The visualized results further demonstrate the effectiveness of our approach. Interestingly, MiniGPT-4 models (including MiniGPT-v2) tend to generate longer responses. While this behavior slightly affects their quantitative performance on image captioning metrics (as shown in Table 4), the qualitative results suggest that their output remains quite effective.\nAdditional Results on Commercial VLMs To further demonstrate the effectiveness of our method, we present additional examples in Figures 9, 10, and 11, where we transfer our attack to commercial VLMs, including Claude's Sonnet, Microsoft's Copilot, and Google's Gemini. The"}]}