{"title": "Curriculum-based Sample Efficient Reinforcement Learning for Robust Stabilization of a Quadrotor", "authors": ["Fausto Mauricio Lagos Suarez", "Akshit Saradagi", "Vidya Sumathy", "Shruti Kotpalliwar", "George Nikolakopoulos"], "abstract": "This article introduces a curriculum learning approach to develop a reinforcement learning-based robust stabilizing controller for a Quadrotor that meets predefined performance criteria. The learning objective is to achieve desired positions from random initial conditions while adhering to both transient and steady-state performance specifications. This objective is challenging for conventional one-stage end-to-end reinforcement learning, due to the strong coupling between position and orientation dynamics, the complexity in designing and tuning the reward function, and poor sample efficiency, which necessitates substantial computational resources and leads to extended convergence times. To address these challenges, this work decomposes the learning objective into a three-stage curriculum that incrementally increases task complexity. The curriculum begins with learning to achieve stable hovering from a fixed initial condition, followed by progressively introducing randomization in initial positions, orientations and velocities. A novel additive reward function is proposed, to incorporate transient and steady-state performance specifications. The results demonstrate that the Proximal Policy Optimization (PPO)-based curriculum learning approach, coupled with the proposed reward structure, achieves superior performance compared to a single-stage PPO-trained policy with the same reward function, while significantly reducing computational resource requirements and convergence time. The curriculum-trained policy's performance and robustness are thoroughly validated under random initial conditions and in the presence of disturbances.", "sections": [{"title": "I. INTRODUCTION", "content": "In the recent years, artificial intelligence methodologies have significantly influenced the development of controllers for Unmanned Aerial Vehicles (UAVs), with Reinforcement Learning (RL) emerging as a prominent approach, particularly in path planning, navigation, and control of Quadrotors [1][2]. In RL, an agent learns a specific behavior by interacting with its environment, by taking actions, and receiving numeric rewards that indicate the effectiveness of the actions in achieving desired outcomes [3].\nRL has proven particularly effective in tackling complex control problems that are challenging for classical system-theoretic approaches [4][5]. This makes RL well-suited for complex learning tasks involving UAVs, where precise modeling of the interaction between the UAV's nonlinear dynamics and the surrounding aerial flows is highly challenging.\nHowever, RL training for UAVs is computationally intensive and time-consuming, underscoring the need for research into sample-efficient RL training methods. Although, using modern GPUs and extreme parallelization in training, millions of time steps (samples) of training can be achieved in minutes [6] [7], it is crucial to investigate the fundamental challenge of sample-efficiency in the training process.\nThis article addresses the challenge of designing a sample-efficient RL training process to achieve robust stabilization for a Quadrotor starting from random initialization in position, orientation, and velocities. The control objective under consideration and the training methodology aim to advance the use of RL in developing robust, high-accuracy low-level controllers for Quadrotors, contrasting with standard single-stage training approaches commonly used in UAV control, that require several hundred million time steps in one-stage training [8].\nExisting literature in RL controller design for Quadrotors has explored diverse aspects, such as reward function design to achieve specific aerial objectives, performance enhancements of RL algorithms, and reduction of computational resources required during training. Notable contributions include [9], which introduced a Bio-inspiered Flight Controller (BFC) that mimics natural flying behaviors and [10], which proposed enhancements to the Proximal Policy Optimization (PPO) algorithm for improved learning stability. The work in [4] developed the GymFC training environment, demonstrating that RL controllers employing state-of-the-art algorithms such as Deep Deterministic Policy Gradient (DDPG) [11], Trust Region Policy Optimization (TRPO) [12], and PPO [13] can surpass the performance of traditional PID controllers in Quadrotor attitude control. Additionally, [14] introduced an RL-based controller capable of stabilizing a Quadrotor from extreme initial conditions, such as manual tossing. It is worth noting that, in the literature alluded to so far, the sample efficiency of the training process is rarely given any attention.\nContributions: This work presents a novel three-stage Curriculum Learning (CL) methodology for training an RL agent to achieve robust stabilization of a Quadrotor, that significantly enhances the sample efficiency of the training process (Section III). To the best of the authors' knowledge, existing RL methodologies for designing low-level Quadrotor controllers, have not explored curriculum learning, and all reported approaches rely on single-stage training. Our results demonstrate that the proposed three-stage CL approach requires significantly less training time and computational"}, {"title": "II. PROBLEM FORMULATION", "content": "Quadrotor system. A Quadrotor is an underactuated aerial system with six degrees-of-freedom (DOF), three translational $((x, y, z) \\in R^3)$ and three rotational $(\\phi, \\theta, \\psi \\in S^1 \\times S^1 \\times S^1)$. The Quadrotor is influenced by four control inputs provided by the motors M1-M4. The motors produce upward thrust and the roll, pitch, and yaw rotations necessary for aerial mobility. In this article, a Crazyflie 2.0 (shown in Figure 1) in $\\times$ configuration is considered. Mathematical modeling of the Quadrotor dynamics can be found in [15][16].\nProblem statement. Despite the availability of high-performance computational resources such as Graphics Processing Units (GPUs), training an RL policy to achieve complex control tasks for a Quadrotor, with acceptable performance levels, requires millions of interactions [17] with the training environment. This high demand for interactions makes the training process computationally expensive and time-consuming. To address this challenge, this work looks to propose a methodology for sample-efficient RL training, aimed at achieving robust stabilization of a Quadrotor, from complex initial states and under the influence of external disturbances."}, {"title": "III. CURRICULUM LEARNING METHODOLOGY", "content": "This section introduces a sequenced curriculum, designed to enhance the sample efficiency of the RL training process for Quadrotor control. Curriculum learning involves three key components: sub-task generation, sequencing, and transfer learning. This approach decomposes a complex target task into a series of sub-tasks, progressively transferring knowledge through multiple learning stages until the target is achieved [18].\nIn this work, a three-stage curriculum is proposed, which is structured using domain knowledge in aerial robotics. In synthesizing a sequence of three sub-tasks, the Quadrotor's under-actuated nature and the coupling between different degrees-of-freedom of a Quadrotor are taken into account.\nThe training initially focuses on the task of achieving stable hovering from a fixed position. The task difficulty gradually increases in two additional stages, where random initialization in positions/orientations and velocities (both linear and angular) respectively are introduced.\nIn general, sub-tasks may differ from the final task in terms of state/action space, reward function, or transition dynamics [19] [20]. In this work, we maintain a consistent reward function structure across all sub-tasks, but sequentially expand the domain of initialization of the Quadrotor, thereby altering the environment within the Markov Decision Process (MDP). To develop an RL policy that yields high accuracy, stability, and robustness, while having good sample efficiency in training, we design a compounded reward function (detailed in Section IV-B) that penalizes excessive exploration, instability, and imprecision in reaching the target position and attitude.\nThe first sub-task focuses on achieving a target hover position at $(x(0),y(0), z(0)) = (0,0,1)$, starting from the fixed position $(x, y, z) = (0,0,0)$. In this stage, the policy is trained to control the drone to take off and maintain the target position, which essentially involves learning to use the same RPM values for all four motors.\nThe second sub-task increases task complexity by requiring the drone to reach $(x(0),y(0), z(0)) = (0,0,1)$ from random initial positions within a cylinder of 2 meters radius and 2 meters height, and random initial attitudes with roll and pitch angles in a safe range of $[-15\u00b0, 15\u00b0]$ and yaw angle in a range of $[-180\u00b0, 180\u00b0]$. This task emphasizes learning the coupling between the three translational degrees of freedom and the roll $(\\phi)$ and pitch $(\\theta)$ rotations.\nThe third sub-task further increases difficulty by introducing random initial linear velocities within the range of [-1,1] meters per second and angular velocities within the range of [-1,1] radians per second, in addition to random initial positions and orientations. Here, the policy learns to manage the motor commands to correct for deviations in position and attitude caused by random initial velocities.\nTo evaluate the sample efficiency and convergence time of RL training, number of time steps and wall-clock time required to achieve the target task are used as metrics. The proposed curriculum learning is compared against a baseline policy trained in a single stage. The evaluation focuses on the ability of the curriculum-trained policy to meet the final task objectives with reduced training time and enhanced sample efficiency."}, {"title": "IV. REINFORCEMENT LEARNING SETUP", "content": "In the reinforcement learning framework, the control of a Quadrotor is modeled as a Markov Decision Process and represented as a tuple of five elements $(\\mathbb{E}, \\pi, \\mathbb{A}, \\mathbb{S}, \\mathbb{R})$, where $\\mathbb{E}$ is the environment, $\\mathbb{A}$ is the set of actions, $\\mathbb{S}$ and $\\mathbb{R}$ are the current state and rewards returned by the environment, and $\\pi$ is the policy in charge of taking control decisions.\n\nA. Observation and action spaces\nIn this work, we consider a 12-dimensional observation space consisting of the position $[x \\; y \\; z] \\in \\mathbb{R}^3$, the Euler"}, {"title": "A. Observation and action spaces", "content": "angles $[\\,\\phi \\; \\theta \\; \\psi \\,] \\in S^1 \\times S^1 \\times S^1$, the linear velocities $[\\,\\dot{x} \\; \\dot{y} \\; \\dot{z} \\,] \\in \\mathbb{R}^3$ and the angular velocities $[\\,\\dot{\\phi} \\; \\dot{\\theta} \\; \\dot{\\psi}\\,] \\in \\mathbb{R}^3$. The rotor speeds $R_1, ..., R_4$ (in RPM) of the propellers constitute the action space. The reinforcement learning policy $\\pi$ maps the observation space to the action space. Although the rotor speeds are an unconventional choice for the action space, this article intends to find an end-to-end policy that directly maps the states to the raw control inputs. The Crazyflie experimental setup considered in this work allows for direct control with the RPMs of the motors (through the PWMs).\nB. Reward shaping\nThe goal of RL algorithms is to train an agent to obtain the most cumulative reward over time. Although several works in literature (such as [21]) suggest having a simple reward function based on the error in position and velocities, simple rewards leads to sample inefficiency and are not suitable for problems that go beyond hovering. In this paper, a multi-component reward function is designed to consider a wide range of aspects that are crucial for both robust stabilization of a Quadrotor and sample efficiency of the training process. In this article, we propose the following reward function:\n$R(t) = 25 - 20T_e - 100E + 20S - 18w_e$, (1)\nwhere the individual components are defined below. Although some of the terms are in fact penalties and not rewards, we use the term reward, without loss of generality. \nTarget reward ($T_e$) : The target reward is a penalty proportional to the distance between the Quadrotor and the target position (T), and the difference between the target yaw and the current yaw. The penalty decreases as the Quadrotor nears the target configuration. The penalty is computed as:\n$T_e = ||[x_T \\; y_T \\; z_T] - [x \\; y \\; z]|| + |\\psi_T - \\psi|$, (2)\nwhere the target values are indicated through the subscript T.\nExploration reward (E): The exploration space for the policy is designed to be a cylinder, centered at the target position (T). In each episode, the cylinder's radius is set to the distance between the center and the starting position plus a tolerance $\\delta_r$, and the height is the target height $z$ plus a tolerance $\\delta_H$. This bounded exploration space, along with the truncation conditions for each training episode, help in lowering the overall training time, without compromising the agent's ability to explore adequately. The exploration reward (E) is defined as:\n$E = \\begin{cases} 1 & d(C,T) > d(i,T) + \\delta_r \\; V \\; C_z > T_z + \\delta_H \\\\ -0.2 & otherwise \\end{cases}$, (3)\nwhere $i$, $C$ and $T$ refer to the initial, current and target positions respectively.\nStability reward (S): To ensure that the drone achieves stability at the target position, we provide a positive reward when the drone is within $\\Delta_p$ cm of the target position and the sum-of-squares of the roll and pitch angles are within $\\Delta_a$.\nOtherwise, the drone receives a penalty proportional to the sum of deviations in roll and pitch from zero. The stability reward (S) is defined as:\n$S = \\begin{cases} 2 & d(C,T) < \\Delta_p \\; \\land \\; \\phi^2 + \\theta^2 < \\Delta_a \\\\ -(\\phi^2 + \\theta^2) & \\end{cases}$, (4)\nThe parameter $\\Delta_p$ represents the radius of a tolerance sphere centered at the target position, while $\\Delta_a$ evaluates stability in roll $(\\phi)$ and pitch $(\\theta)$ angles.\nNavigation reward ($w_e$): To ensure smooth navigation from the starting position to the target position, we penalize significant changes in angular velocities. The penalty is proportional to the difference between the angular velocity in the previous time step and the current time step. The navigation reward ($w_e$) is calculated as\n$w_e = \\sum_{i=1}^{3} (w_{i,t-1} - w_{i,t})^2$, (5)\nwhere $w \\in {\\dot{\\phi}, \\dot{\\theta}, \\dot{\\psi}}$. Although we do not impose specific penalties for high roll or pitch angles, we employ truncation conditions to limit extreme behavior, such as when the roll or pitch angle exceeds $\\pm 40\u00b0$."}, {"title": "V. PROCEDURE FOR SEQUENTIAL TRAINING", "content": "In this article, the standard implementation of the Proximal Policy Optimization (PPO) algorithm, considered a state-of-the-art RL algorithm [10], is used to train the curriculum learning policy."}, {"title": "VI. TESTING OF THE RL POLICY", "content": "To evaluate the robustness and stability achieved by the curriculum-trained policy, we design two tests to evaluate: i) the performance of the policy in achieving a target position and orientation, starting from randomized position, orientation, and velocities, and ii) stability in the presence of external disturbances, where the policy's capacity to recover the desired position and attitude is assessed.\nRobust stabilization: The performance of the trained policy was evaluated by randomly setting the initial states of the drone and testing its ability to achieve the target configuration $(x,y,z,\\phi, \\theta, \\psi) = (0,0,1,0,0,0)$ and $(\\dot{x}, \\dot{y}, \\dot{z}, \\dot{\\phi}, \\dot{\\theta}, \\dot{\\psi}) =(0,0,0,0,0,0)$. We conducted 30 trials, each lasting 8 seconds, starting from randomized initial positions, orientations, linear velocities and angular velocities. The 30 initial conditions were split among three regions shown in Figure 4: Test region A, which is a cylinder with radius 1.5 m and height 1.5 m; Test region B, which is an"}, {"title": "VII. DISCUSSION", "content": "Training process. The implementation of the PPO algorithm followed the standard implementation provided by the Stable-Baselines3 library. Both the curriculum learning and single-stage training approaches were trained utilizing the same neural network architecture and reward function structure. The evolution of training depicted in Figure 3 clearly shows the effectiveness of curriculum learning in improving sample efficiency. Curriculum learning allows the agent to progressively learn increasingly difficult stabilizing maneuvers, resulting in faster convergence and higher cumulative rewards compared to single-stage training. In contrast, single-stage training approach struggles to achieve the target learning objective for the Quadrotor, with the training remaining unstable and the ECR unable to reach higher reward levels, as can be seen in Figure 3.\nResults. The results from section VI demonstrate that the proposed curriculum learning approach, combined with the compounded reward function, allows the RL agent to achieve high degree of stability and robustness under complex initial conditions, including random linear and angular velocities. Figure 7 reveals that the trained policy efficiently handles disturbances, recovering the target position and attitude within a few seconds of being perturbed, showcasing the robustness of the curriculum-trained policy. This suggest that the trained agent has the potential to function as a reliable, general purpose low-level controller for Quadrotors, even in challenging operating conditions.\nDespite these promising results, there are areas where further improvements can be made. In some instances, particularly when the Quadrotor is initialized at extreme configurations that go beyond those encountered during training, the drone exhibits large transients. This observation points to the need for refining the curriculum learning design to better generalize the agent's performance across a broader ranger of initial states. The occasional large transients suggest that additional training stages or more diverse initial conditions could improve the policy's robustness in extreme scenarios.\nMoreover, while the current curriculum-trained policy demonstrated desirable performance in simulation, bridging the gap between simulation and real-world (sim2real) deployment remains a critical challenge. Real-world environments introduce unmodelled dynamics, disturbances and noise that may not be fully captured in the simulation engine. Thus, incorporating specific measures to address sim2real transfer is essential. This would allow the trained policy to be deployed effectively in controlling real Quadrotor systems."}, {"title": "VIII. CONCLUSION AND FUTURE DIRECTIONS", "content": "This work proposed a sample-efficient curriculum learning methodology for efficiently and effectively training a reinforcement learning agent to accomplish robust stabilization for Quadrotors. A compounded reward function was proposed to capture performance specifications related to transient performance, and steady-state accuracy in robust stabilization of a Quadrotor. The performance of the curriculum-trained RL policy was thoroughly tested in a physics based simulator, with different initial conditions and in the presence of disturbances. The results revealed that curriculum training is more sample-efficient than one-stage training, while yielding superior performance. The future work involves automating the curriculum setup and transferring the policy to real drones, thereby closing the simulation-to-reality gap."}]}