{"title": "More Text, Less Point: Towards 3D Data-Efficient Point-Language Understanding", "authors": ["Yuan Tang", "Xu Han", "Xianzhi Li", "Qiao Yu", "Jinfeng Xu", "Yixue Hao", "Long Hu", "Min Chen"], "abstract": "Enabling Large Language Models (LLMs) to comprehend the 3D physical world remains a significant challenge. Due to the lack of large-scale 3D-text pair datasets, the success of LLMS has yet to be replicated in 3D understanding. In this paper, we rethink this issue and propose a new task: 3D Data-Efficient Point-Language Understanding. The goal is to enable LLMs to achieve robust 3D object understanding with minimal 3D point cloud and text data pairs. To address this task, we introduce GreenPLM, which leverages more text data to compensate for the lack of 3D data. First, inspired by using CLIP to align images and text, we utilize a pre-trained point cloud-text en-coder to map the 3D point cloud space to the text space. This mapping leaves us to seamlessly connect the text space with LLMs. Once the point-text-LLM connection is established, we further enhance text-LLM alignment by expanding the intermediate text space, thereby reducing the reliance on 3D point cloud data. Specifically, we generate 6M free-text descriptions of 3D objects, and design a three-stage training strategy to help LLMs better explore the intrinsic connections between different modalities. To achieve efficient modality alignment, we design a zero-parameter cross-attention module for token pooling. Extensive experimental results show that GreenPLM requires only 12% of the 3D training data used by existing state-of-the-art models to achieve superior 3D understanding. Remarkably, GreenPLM also achieves competitive performance using text-only data. The code and weights are available at: https://github.com/TangYuan96/GreenPLM.", "sections": [{"title": "Introduction", "content": "Recent advancements in large language models (LLMs) have revolutionized natural language processing, demonstrating emergent intelligence and exceptional capabilities in lan-guage understanding and generation (OpenAI 2023; Yang et al. 2024a; Dubey et al. 2024; Team et al. 2023). However, LLMs are blind to the 3D physical world because they lack the ability to capture and understand 3D objects. Solving this challenging multimodal 3D-language understanding task could benefit many applications, such as autonomous driving, robotics and embodied AI (Driess et al. 2023; Fu et al. 2024; Brohan et al. 2023).\nInspired by CLIP (Radford et al. 2021), multimodal large language models (MLLMs) can map different modality in-puts to a text space closer to LLMs using pre-trained multi-modal encoders, enabling LLMs to understand data beyond just language. Existing 3D point-language models follow a similar approach, applying LLMs to 3D understanding by learning from 3D point-text data pairs (Luo et al. 2024; Qi et al. 2024b). For example, PointLLM (Xu et al. 2023) and ShapeLLM (Qi et al. 2024a) employ pre-trained multimodal point cloud encoders (Xue et al. 2024; Qi et al. 2024a), map-ping the point cloud space to the text space. This leaves the alignment of point cloud with LLMs to only align the text space with LLMs, which is relatively easier for LLMs. Fi-"}, {"title": "Related Work", "content": "3D Point-Language Understanding\nTo enable LLMs to understand the 3D physical world, early approaches project 3D point clouds into 2D images, rely-ing on 2D-LLMs for comprehension (Hong et al. 2023). Point-Bind LLM (Guo et al. 2023) establishes a 3D-2D-LLM connection for better performance. However, these 2D-based methods lose crucial 3D information, leading to issues like occlusion, ambiguity, and hallucination. Recently, with the availability of large-scale 3D-text data (Luo et al. 2024; Qi et al. 2024b) and multimodal encoders, methods like PointLLM (Xu et al. 2023) and ShapeLLM (Qi et al. 2024a) connect point encoders with LLMs and fine-tune the 3D Point Cloud-LLMs (3D-LLMs) using vast amounts of 3D-text data. Unfortunately, comparing to images, 3D-text data remains extremely scarce (Objaverse-1M vs. LAION-5B) (Deitke et al. 2023; Schuhmann et al. 2022) and expensive, let alone the near infinite and free text data, making it challenging"}, {"title": "Method", "content": "To enable LLMs to achieve robust 3D understanding with minimal 3D data, we propose using more text data to reduce reliance on 3D data. First, we generate a 6M text dataset of 3D object descriptions and conversations. Then, to better uncover connections between different modalities, we design a 3-stage training strategy. Finally, we introduce a parameter-free token pooling module to efficiently utilize information from the encoder's output token sequence. The details of these three parts are as follows.\n3D Object Description and Conversation Dataset\nLeveraging multimodal pre-trained encoders, we propose using large amounts of text data to compensate for the lack of 3D data pairs. Specifically, we first align the text encoder with the LLM using extensive text data. Since the text encoder is already aligned with the point encoder, we then only need a small amount of 3D data for point encoder-LLM alignment.\nTo achieve this, we bring T3D, a 6M text dataset of 3D object descriptions and conversations. Fig. 3 shows the verb-noun distribution and a visualized word cloud. In-stead of using the closed-source GPT-4 (OpenAI 2023), we use the equally powerful open-source model Qwen2-72B-Instruct (Yang et al. 2024a) to construct this dataset. We select object categories from Cap3D (Luo et al. 2024) and DiffuRank (Luo, Johnson, and Lee 2024), and we design prompts to generate 4 types of data: 1M brief descriptions, 1M detailed descriptions, 3M single-round conversations, and 1M multi-round conversations. The object descriptions help the LLMs learn rich semantic knowledge, while the conversations enable the LLMs to extract useful informa-tion from the context to improve 3D understanding. Notably, this dataset is constructed without any manual annotation or post-processing, requiring only minimal model inference cost. Each type of data follows the Caption-Question-Answer format, as shown in Tab. 1. During training, we input the Cap-tion into the text encoder, pass the encoded tokens through a projector, and then input them along with the Question into the LLM, which outputs a response to calculate the loss against the Answer. More detailed prompts and distributions are in Appendix.\n3-Stage Training Strategy\nFor better multimodal encoder-LLM alignment and minimiz-ing the use of 3D point-text data pairs, we propose a 3-stage training strategy, as shown in Fig. 4. Our design principle is to first use a large amount of text data to align the text encoder with the LLM via a MLP projector (Stage I and II). Then, using only a small amount of 3D point-text data pairs, we align the point cloud encoder with the LLM via the same projector (Stage III). Specifically, for each stage, we will introduce the pipeline, trainable layers, and data aspects as follows.\nStage I is shown in Fig. 4(a). First, we input a text caption D of a 3D object into the pre-trained text encoder $f_{text}$, obtaining the global text embedding $C_t$ as the encoder output. $C_t$ is then passed through a learnable MLP projector $f_{proj}$ to connect with the LLM $f_{LLM}$. The LLM input consists of the projector output $f_{proj}(C_t)$, and the text tokens of an instruction prompt I such as \"What is this?\". Finally, the LLM outputs a brief description $R_{brief}$ of the 3D object, which can be used to calculate the loss with the ground-truth"}, {"title": "descrip. The formulas are as follows:", "content": "$C_t = f_{text}(D),$\n$R_{brief} = f_{LLM}(f_{proj}(C_t), h(I)),$\nwhere, h is the LLM's tokenizer.\nTrainable Layers & Data: Note that, only the projector $f_{proj}$ is a trainable MLP, while the rest, including the text encoder $f_{text}$ and LLM $f_{LLM}$, have frozen weights. We train the model using a large dataset of brief descriptions (1M) from our T3D dataset, as shown in Tab. 1."}, {"title": "Stage II", "content": "is shown in Fig. 4(b), Stage II is similar to Stage I. We also first input a caption of a 3D object into the text encoder $f_{text}$, then extract the global text embedding and pass it to the projector $f_{proj}$. The projector output, along with a complex instruction, is then fed to the LLM $f_{LLM}$. Fi-nally, the LLM outputs detailed description and conversation results, which are then used to calculate the loss.\nTrainable Layers & Data: The differences from Stage I are as follows: (1) The weights of the projector $f_{proj}$ are copied from Stage I for initialization and remain trainable. (2) We use LoRA (Hu et al. 2021) to train the LLM $F_{LLM}$ in this stage to achieve better multimodal alignment. The text encoder $f_{text}$ remains frozen. We use only 210K detailed descriptions and conversation data for 3D objects from our T3D dataset, such as describing an object in ~50 words and engaging in multi-turn conversations, as shown in Tab. 1.\nNotably, to enhance the perception robustness of the LLM, we add Gaussian noise to the encoder's output features to sim-ulate the semantic discrepancies between different modalities, inspired by Chen et al. (2024b). After two stages of pure text training, our GreenPLM acquires the ability to comprehend raw 3D point clouds by directly replacing the text encoder $f_{text}$ with a point encoder $f_{pe}$ without weight tuning."}, {"title": "Stage III", "content": "is shown in Fig. 4(c), we use 3D point cloud as input. The 3D point cloud P is fed into the point cloud en-coder $f_{pe}$ to output a token sequence. Unlike previous stages that use only the global text embedding (corresponding to the class token in the point encoder) for the projector, in this stage, we extract representations from all tokens $T_{pe}$ to more effectively leverage information from the point en-coder. To reduce the token sequence length for efficiency, we introduce a parameter-free token pooling module based on cross-attention, namely 0M-Pooling, which compresses the token length from 512 to 32. The pooled point tokens $T_e$, along with three tokens from Mix-pooling and the class to-ken $C_{pc}$, are input to the projector. Thus, the projector $f_{proj}$ receives 32+3+1=36 tokens. We then feed the projector's output, along with the instruction I, into $f_{LLM}$ to generate the predict responses $R_{pred}$ of descriptions or conversations. The responses will be used to compute loss with the ground truth. This stage can be formulated as:\n$[C_{pc}, T_{pc}] = f_{pc}(P), T_e = 0M-Pooling(T_{pc}),$\n$R_{pred} = f_{LLM}(f_{proj} (C_{pc}, Mix (T_{pc}), T_e), h(I)),$\nwhere Mix represents Mix-pooling of max, mean, and sum.\nTrainable Layers & Data: Similar to Stage II, the weights of projector $f_{proj}$ here are copied from the previous stage and then still kept trainable. We continue using LoRA (Hu et al. 2021) to train $f_{LLM}$ for efficient point-LLM alignment. The point cloud encoder $f_{pc}$ remain frozen. In this stage, we train using only a small amount of 3D-text pairs (90K)."}, {"title": "Loss Function", "content": "For all training stages, given a pair of LLM output R and text ground truth y, GreenPLM is optimized under a causal language modeling objective (Liu et al. 2018):\n$L = CrossEntropyLoss (R, h (y)),$"}, {"title": "0M-Pooling", "content": "To fully leverage the output of the point cloud encoder, we extract information from all output tokens $T_{pe}$, not just the class token, while reducing computational load. As shown in Fig. 5, we design a zero-parameter token pooling module based on cross-attention, namely 0M-Pooling, which com-presses the 512 output tokens down to 32 tokens, without introducing any learnable parameters, defined as:\n$T_c = FPS(T_{pc}), T_p = KNN(T_c, T_{pc}),$\n$T_m = MaxPool(T_p), T_e = SoftMax(TT)T_p,$\nwhere $T_{pc} \\in R^{N \\times 1 \\times C}$ is the output point token sequence of the point cloud encoder (N = 512), $T_c \\in R^{M \\times 1 \\times C}$ is the central token gained via farthest point sampling (FPS) from $T_{pc}$ (M = 32), and $T_p \\in R^{M \\times R \\times C}$ represents the K-Nearest Neighborhood (KNN) tokens of $T_c$ within $T_{pc}$ (K = 8). Then, we pass $T_p$ to Max Pooling on the K dimension to get $T_m \\in R^{M \\times 1 \\times C}$. Finally, we use cross-attention in Equ.(6) to aggregate information from $T_{pc} \\in [][R^{512 \\times 1 \\times C -> TPE R^{32 \\times 1 \\times C}$. Finally, we obtain the compressed token $T_e$ using zero trainable parameters. Notely, the $T_{pe}$ input to 0M-Pooling is from the point encoder's second-to-last layer."}, {"title": "Experiment", "content": "Implementation details. We use Phi-3 (Abdin et al. 2024) as the LLM backbone, with EVA-CLIP-E (Sun et al. 2023) and ViT (Dosovitskiy et al. 2020) both trained by Uni3D (Zhou et al. 2023) as the text encoder and point encoder, respectively. The point encoder outputs 512+1 tokens, each with C =\n1024. The MLP projector consists of two linear layers and a GeLU activation, mapping the encoder's output tokens to tokens with 3072 dimensions of Phi-3. Our GreenPLM has 63.3M trainable parameters and requires only 26.6 hours of training on a single 3090 GPU. Besides the standard 3-stage training of GreenPLM, we also train GreenPLM-0 with text-only data, utilizing only Stages I and II. During inference, we simply replace the text encoder in GreenPLM-0 with the point encoder from Uni3D without weight tuning. More detailed training settings are included in Appendix.\nBaselines. To validate our 3D data-free capability, we com-pared GreenPLM-0 with the SOTA 2D-LLMs, InstructBLIP and LLaVA, as well as the 3D-2D-LLM model Point-Bind LLM (Guo et al. 2023). To evaluate GreenPLM with limited 3D data, we choose the SoTA models PointLLM (Xu et al. 2023) and MiniGPT-3D (Tang et al. 2024). For fairness, we train both using the same 90K limited 3D point-text datas.\nEvaluation Settings. An efficient and accurate model eval-uation method is a shared goal in the MLLM community. We observe that existing evaluation approaches often rely on GPT-4 and GPT-3.5 to assess the similarity between gener-ated results and ground truth sentences. While this method provides accurate evaluations, it has two major drawbacks: inconsistent API versions and high evaluation costs. For in-stance, the GPT-3.5-turbo-0613 model used in PointLLM and MiniGPT-3D is no longer maintained, making it difficult to replicate the results. To address these issues, we propose a new benchmark based on open-source models and intro-duce a new metric to evaluate data efficiency. Specifically, we use two prompts for the classification task: an Instruction-type (I) prompt, \u201cWhat is this?\u201d, and a Completion-type (C) prompt, \u201cThis is an object of.\u201d. For the captioning task, we use a single prompt: \u201cCaption this 3D model in detail.\u201d. We then replace GPT-4 and GPT-3.5 with the open-source Qwen2-72B-Instruct (Yang et al. 2024a) (Qwen2 for short) to evaluate the model's output. We introduce the Accuracy-to-3D-Data Ratio (A3DR) metric to assess a model's efficiency in utilizing 3D data, defined as follows:\n$A3DR(Acc) = \\frac{1}{1 + exp(- \\frac{Acc}{Size} )},$\nwhere Size is the size of 3D data (K), Acc is the accuracy, $\\epsilon$=1e-5 to avoid zero division.\nGenerative 3D Object Classification\nWe validate the model's recognition ability by performing the generative 3D object classification task on the ModelNet40 dataset (Wu et al. 2015) and Objaverse dataset (Deitke et al. 2023), using I-type and C-type prompts, with results shown in Tab. 2. For close-set zero-shot classification on ModelNet40, we let Qwen2 select the closest matching category in the 40 classes as the model's output. For open-vocabulary classifica-tion on Objaverse, we use Qwen2 to evaluate if the model's output describes the category of ground truth sentence.\nAs shown in Tab. 2, our GreenPLM-0 achieves an av-erage classification accuracy (AvgAcc) of 54.57% without using any 3D data, outperforming all 2D-based models. It surpasses LLaVA-1.5-13B by +21.95 and Point-Bind LLM by +27.89 in AvgAcc. Remarkably, our model also exceeds GPT4Point (660K), which is trained with 660K 3D data, by +20.08 and performs on par with PointLLM-7B (730K). With only a small amount of 3D data (90K), GreenPLM achieves an average accuracy of 60.08%, surpassing PointLLM and MiniGPT by +10.95 and +11.06 in AvgAcc, respectively. GreenPLM even outperforms PointLLM-13B (730K) while using a smaller LLM, and obtains results comparable to SOTA model MiniGPT-3D (730K). Additionally, GreenPLM (90K) outperforms MiniGPT-3D (90K) and MiniGPT-3D"}, {"title": "3D Object Captioning", "content": "We evaluate the ability to understand 3D context through a 3D object captioning task, as shown in Tab. 3. Following pre-vious works (Xu et al. 2023; Tang et al. 2024), we assess the similarity between the model's response and the ground truth caption using an LLM, and also evaluate embedding simi-larity using Sentence-BERT (Reimers and Gurevych 2019) (S-BERT) and SimCSE (Gao, Yao, and Chen 2021).\nIt is evident that all models without 3D data underperform compared to those trained with 3D data, as they lose signif-icant 3D information. However, our GreenPLM-0 can still outperforms Point-Bind LLM and achieves comparable re-sults to powerful 2D-LLMs by a large margin. When using a small amount of 3D data (90K), our Qwen2 score surpasses MiniGPT-3D (90K) by +7.50, with S-BERT and SimCSE scores also exceeding by +2.72 and +1.61, respectively. Sim-ilarly, GreenPLM (90K) achieves a Qwen2 score higher than PointLLM-13B (730K) by +2.15, with S-BERT and Sim-CSE scores comparable to MiniGPT-3D (730K) while using only 12% of 3D data. These results again demonstrate Green-PLM's ability to efficiently extract 3D information from even small amounts of 3D data or purely text data."}, {"title": "Qualitative Results", "content": "Fig. 1 and Tab. 5 present the qualitative results. As shown in Fig. 1, whether trained on text-only or with minimal 3D data, GreenPLM provides accurate, context-aware responses in multi-turn conversations. Tab. 5 shows that our GreenPLM-"}, {"title": "Ablation Study", "content": "We conduct ablation experiments on the generative 3D object classification task and report the average accuracy.\nTraining stages. As shown in Tab. 4, removing any stage reduces performance, with the biggest drop when Stage I is removed. This is because Stage I trains the MLP projector to align the encoder with the LLM. Comparing rows #4 and #7, we observe that Stage II helps the LLM better align with the semantic space. The results of rows #6 and #7 indicate that Stage III injects 3D information into the LLM, significantly enhancing the model's 3D understanding.\n0M-Pooling. As shown in Fig. 6, when we replace OM-Pooling with Max Pooling or Mean Pooling, the accuracy drops by 1.96 and 1.58, respectively, even though the learn-able parameters remain zero. This demonstrates that our OM-Pooling module effectively and efficiently captures point"}, {"title": "Conclusion", "content": "To enable LLMs to achieve strong 3D understanding with minimal 3D data, we introduce a new task: 3D Data-Efficient Point-Language Understanding. We propose GreenPLM, which employs a 3-stage training strategy that increases text data in Stage I & II to reduce the need for 3D data in Stage III. We create a 6M T3D dataset and an unified benchmark. Results show that GreenPLM achieves performance com-"}, {"title": "Appendix", "content": "Here in the Appendix, we present the detailed distribution of the T3D dataset, along with the prompts and instructions used to create it, and provide several data examples. We also showcase more visual result comparisons. Additionally, we provide additional ablation results and more detailed training parameters. Finally, we include illustrations of the model architecture used during training and inference."}, {"title": "Our 6M T3D Dataset", "content": "Distributions We show the detailed distributions of our 6M T3D dataset in Fig. 11 and Fig. 12. Specifically, in Fig. 11, we show word clouds for captions and responses. Following Wang et al. (2022), we also present the distribution of verb-noun pairs in the dataset, highlighting its diverse attributes. Additionally, in Fig. 12, we display the length distribution for different data types; for instance, most brief and detailed descriptions range from about 18 to 42 words.\nPrompts and Instructions Here, we show one example of the data generation pipeline using Qwen2-72B-Instruct(Yang et al. 2024b) in Fig. 13, also the instruction list for description data in Fig. 7 and Fig. 8.\nData Samples We give several samples of four types of data from our T3D dataset, shown in Fig. 14-17. Please see supplementary material for more data samples of our T3D dataset."}, {"title": "Qualitative Results", "content": "We show more qualitive results of our GreenPLM-0 and GreenPLM in Fig. 18 and Fig. 19. Trained with only text data, our GreenPLM-0 accurately identifies the shape, color, and usage of 3D objects. For example, in the bottom right of Fig. 18, the model not only correctly recognizes the shoe's category and purpose but also accurately distinguishes be-tween the left and right shoe. When using only a small amount of 3D data, our GreenPLM can accurately and thor-oughly describe 3D objects, as shown in the first row of Fig. 19."}, {"title": "Detailed Training Settings", "content": "We report more detailed training settings in Tab. 9."}, {"title": "Ablation on the size of point encoder", "content": "parameters ranging from 6.2M to 1016.5M, as shown in Tab. 6. The results indicate that as the point encoder size increases, the model's performance first improves and then declines, achieving the best accuracy with 22.6M parameters. Notably, even with just 6.2M parameters, GreenPLM still demonstrates strong 3D understanding, further proving the efficiency of our model."}, {"title": "Training and Inference Architecture", "content": "We show the difference of architectures between training and inference in Fig. 20. Note that each stage of our 3-stage strategy can be used for inference. Specifically, if using the Stage I or Stage II for inference, simply replace the text encoder with the aligned point encoder."}]}