{"title": "More Text, Less Point: Towards 3D Data-Efficient Point-Language Understanding", "authors": ["Yuan Tang", "Xu Han", "Xianzhi Li", "Qiao Yu", "Jinfeng Xu", "Yixue Hao", "Long Hu", "Min Chen"], "abstract": "Enabling Large Language Models (LLMs) to comprehend the 3D physical world remains a significant challenge. Due to the lack of large-scale 3D-text pair datasets, the success of LLMS has yet to be replicated in 3D understanding. In this paper, we rethink this issue and propose a new task: 3D Data-Efficient Point-Language Understanding. The goal is to enable LLMs to achieve robust 3D object understanding with minimal 3D point cloud and text data pairs. To address this task, we introduce GreenPLM, which leverages more text data to compensate for the lack of 3D data. First, inspired by using CLIP to align images and text, we utilize a pre-trained point cloud-text en-coder to map the 3D point cloud space to the text space. This mapping leaves us to seamlessly connect the text space with LLMs. Once the point-text-LLM connection is established, we further enhance text-LLM alignment by expanding the intermediate text space, thereby reducing the reliance on 3D point cloud data. Specifically, we generate 6M free-text de-scriptions of 3D objects, and design a three-stage training strategy to help LLMs better explore the intrinsic connections between different modalities. To achieve efficient modality alignment, we design a zero-parameter cross-attention module for token pooling. Extensive experimental results show that GreenPLM requires only 12% of the 3D training data used by existing state-of-the-art models to achieve superior 3D under-standing. Remarkably, GreenPLM also achieves competitive performance using text-only data. The code and weights are available at: https://github.com/TangYuan96/GreenPLM.", "sections": [{"title": "Introduction", "content": "Recent advancements in large language models (LLMs) have revolutionized natural language processing, demonstrating emergent intelligence and exceptional capabilities in lan-guage understanding and generation (OpenAI 2023; Yang et al. 2024a; Dubey et al. 2024; Team et al. 2023). However, LLMs are blind to the 3D physical world because they lack the ability to capture and understand 3D objects. Solving this challenging multimodal 3D-language understanding task could benefit many applications, such as autonomous driving, robotics and embodied AI (Driess et al. 2023; Fu et al. 2024; Brohan et al. 2023).\nInspired by CLIP (Radford et al. 2021), multimodal large language models (MLLMs) can map different modality in-puts to a text space closer to LLMs using pre-trained multi-modal encoders, enabling LLMs to understand data beyond just language. Existing 3D point-language models follow a similar approach, applying LLMs to 3D understanding by learning from 3D point-text data pairs (Luo et al. 2024; Qi et al. 2024b). For example, PointLLM (Xu et al. 2023) and ShapeLLM (Qi et al. 2024a) employ pre-trained multimodal point cloud encoders (Xue et al. 2024; Qi et al. 2024a), map-ping the point cloud space to the text space. This leaves the alignment of point cloud with LLMs to only align the text space with LLMs, which is relatively easier for LLMs. Fi-nally, they propose to fine-tune the whole model with large amount of 3D-text data pairs, thus enhancing the LLMs' 3D understanding capabilities. However, this field remains under-explored. The primary reason is that training LLMs requires billions of text data from the internet, while 3D-text pair data is scarce since 3D data itself is hard to acquire and requires expensive annotations. Consequently, the scaling law that drives LLMs success are difficult to achieve in the 3D do-main, directly limiting the development of 3D foundation models.\nIn this paper, we revisit the 3D data bottleneck and pose a question: Can we achieve robust 3D understanding with minimal 3D data? To answer this question, we propose a new task: 3D Data-Efficient Point-Language Understanding (3DEPL). The goal is to enable LLMs to achieve robust 3D understanding using as little 3D point cloud-text data pairs as possible. This requires the model to explore the intrinsic connections between different modalities, and effectively leverage the powerful language comprehension capabilities of LLMs to achieve data-efficient 3D understanding.\nTo address this data-limited multimodal alignment prob-lem, we propose GreenPLM. Intuitively, as shown in Fig. 2, we observe that after establishing the point-text-LLM connec-tion, instead of increasing point-text data pairs to optimize the point-text mapping like in existing methods (Xu et al. 2023; Qi et al. 2024a), we can also enhance the text-LLM alignment by simply adding more text data. This approach can also improve the point-LLM alignment and, more impor-tantly, reduce the reliance on point-text data pairs, shifting the data bottleneck from expensive and scarce 3D-text data to abundant and cheap text data. That is, the text-LLM align-ment approach fits perfectly with the goal of 3D data-efficient point-language understanding, also provides an alternative so-lution for aligning point clouds with LLMs, enabling Green-PLM to achieve robust 3D understanding even with limited 3D data.\nIn detail, GreenPLM solves the 3DEPL task with key tech-niques across three perspectives: data, training strategy, and model architecture. (1) We bring T3D dataset, a 6M text dataset of 3D object descriptions and conversations for free, the largest to our knowledge, to expand the text space for better text-LLM alignment and compensate for the scarcity of expensive 3D data. (2) We propose a 3-stage training strategy designed to help LLMs better uncover the intrinsic connec-tions between different modalities. Specifically, we propose a coarse-to-fine training approach, progressing from data to model. The first two stages fine-tune the LLMs with text-only data, while the final stage uses minimal 3D data for further point-LLMs alignment. (3) From the architecture's perspec-tive, we design a parameter-free cross-attention module for token pooling, namely 0M-Pooling, which better utilizes the encoder's output tokens, thereby aligning point clouds with LLMs more effectively. This, we can achieve excellent per-formance with only an efficient LLM (Abdin et al. 2024). Together, we can complete training in just 26.6 hours using a single 3090 GPU (24GB), leaving opportunities for efficient end-side deployment.\nTo fairly and reasonably evaluate the models, we introduce a new metric to measure the efficiency of 3D data usage, and establish a new evaluation benchmark based on open-source LLMs. Experimental results show that our GreenPLM outperforms previous models using only 12% of the 3D data. It even surpasses GPT4Point (Qi et al. 2024b) without any 3D data, maintaining extremely 3D data-efficient point-language understanding, which demonstrates the effectiveness of our approach. The contributions of this paper are as follows:\n\u2022 We introduce a new task of 3D data-efficient point-language understanding, aiming to enable LLMs to achieve robust 3D understanding with minimal 3D data.\n\u2022 We propose GreenPLM to tackle this 3D data-limited task from a novel perspective, enhancing point-LLM align-ment with more free-text data. Specifically, we introduce a 6M T3D dataset, design a 3-stage training strategy, and present a OM-Pooling module for token pooling.\n\u2022 We introduce the Accuracy-to-3D-Data Ratio (A3DR) to measure the efficiency of 3D data usage and establish an evaluation benchmark based on open-source LLMs.\n\u2022 GreenPLM outperforms previous models using only 12% of 3D data and even surpasses GPT4Point (660K 3D data) using only text, demonstrating superior 3D data efficiency."}, {"title": "Related Work", "content": "To enable LLMs to understand the 3D physical world, early approaches project 3D point clouds into 2D images, rely-ing on 2D-LLMs for comprehension (Hong et al. 2023). Point-Bind LLM (Guo et al. 2023) establishes a 3D-2D-LLM connection for better performance. However, these 2D-based methods lose crucial 3D information, leading to issues like occlusion, ambiguity, and hallucination. Recently, with the availability of large-scale 3D-text data (Luo et al. 2024; Qi et al. 2024b) and multimodal encoders, methods like PointLLM (Xu et al. 2023) and ShapeLLM (Qi et al. 2024a) connect point encoders with LLMs and fine-tune the 3D Point Cloud-LLMs (3D-LLMs) using vast amounts of 3D-text data. Unfortunately, comparing to images, 3D-text data remains extremely scarce (Objaverse-1M vs. LAION-5B) (Deitke et al. 2023; Schuhmann et al. 2022) and expensive, let alone the near infinite and free text data, making it challenging to build powerful 3D foundation models according to the scaling law. Also, training 3D-LLMs is resource-intensive, often requiring 8xA100 GPUs for hundreds of hours. Although MiniGPT-3D (Tang et al. 2024) reduces training time to 26.8h on a single GPU, the 3D data bottleneck persists. Our GreenPLM proposes to solve this 3D data bottleneck by leveraging extensive text data to compensate for the lack of 3D data, and introducing a 3-stage training strategy for effective and efficient alignment."}, {"title": "Multimodal Encoders in 3D-LLM", "content": "The encoder maps raw data into a more compact embedding space, which can then be aligned with LLMs. To reduce the training cost, one can intuitively employ a multimodal pre-trained encoder, such as CLIP (Radford et al. 2021), which has been trained on text-image pairs, for aligning 2D images with LLMs. This makes it easier to align data from different modalities with LLMs. Similarly, some existing 3D-LLMs use multimodal pre-trained encoders (Huang et al. 2023; Xue et al. 2023; Qi et al. 2023; Gao et al. 2024; Lei et al. 2023; Chen et al. 2024a; Han et al. 2024) to map point clouds into embedding space, followed by fine-tuning the 3D-LLM. However, even without training the encoder, constructing the 3D-LLM still requires a vast amount of point-text data (Xu et al. 2023; Zhou et al. 2023; Qi et al. 2024a; Tang et al. 2024). We observe that existing methods underutilize the potential of the text encoder, only focusing on aligning point encoder with LLM. In contrast, we propose leveraging the cost-efficient text space and the text encoder to reduce the dependency on 3D data."}, {"title": "Method", "content": "To enable LLMs to achieve robust 3D understanding with minimal 3D data, we propose using more text data to reduce reliance on 3D data. First, we generate a 6M text dataset of 3D object descriptions and conversations. Then, to better uncover connections between different modalities, we design a 3-stage training strategy. Finally, we introduce a parameter-free token pooling module to efficiently utilize information from the encoder's output token sequence. The details of these three parts are as follows."}, {"title": "3D Object Description and Conversation Dataset", "content": "Leveraging multimodal pre-trained encoders, we propose using large amounts of text data to compensate for the lack of 3D data pairs. Specifically, we first align the text encoder with the LLM using extensive text data. Since the text encoder is already aligned with the point encoder, we then only need a small amount of 3D data for point encoder-LLM alignment. To achieve this, we bring T3D, a 6M text dataset of 3D object descriptions and conversations. Fig. 3 shows the verb-noun distribution and a visualized word cloud. Instead of using the closed-source GPT-4 (OpenAI 2023), we use the equally powerful open-source model Qwen2-72B-Instruct (Yang et al. 2024a) to construct this dataset. We select object categories from Cap3D (Luo et al. 2024) and DiffuRank (Luo, Johnson, and Lee 2024), and we design prompts to generate 4 types of data: 1M brief descriptions, 1M detailed descriptions, 3M single-round conversations, and 1M multi-round conversations. The object descriptions help the LLMs learn rich semantic knowledge, while the conversations enable the LLMs to extract useful informa-tion from the context to improve 3D understanding. Notably, this dataset is constructed without any manual annotation or post-processing, requiring only minimal model inference cost. Each type of data follows the Caption-Question-Answer format, as shown in Tab. 1. During training, we input the Cap-tion into the text encoder, pass the encoded tokens through a projector, and then input them along with the Question into the LLM, which outputs a response to calculate the loss against the Answer. More detailed prompts and distributions are in Appendix."}, {"title": "3-Stage Training Strategy", "content": "For better multimodal encoder-LLM alignment and minimiz-ing the use of 3D point-text data pairs, we propose a 3-stage training strategy, as shown in Fig. 4. Our design principle is to first use a large amount of text data to align the text encoder with the LLM via a MLP projector (Stage I and II). Then, using only a small amount of 3D point-text data pairs, we align the point cloud encoder with the LLM via the same projector (Stage III). Specifically, for each stage, we will introduce the pipeline, trainable layers, and data aspects as follows.\nStage I is shown in Fig. 4(a). First, we input a text caption D of a 3D object into the pre-trained text encoder \\(f_{\text{text}}\\), obtaining the global text embedding \\(C_t\\) as the encoder output. \\(C_t\\) is then passed through a learnable MLP projector \\(f_{\text{proj}}\\) to connect with the LLM \\(f_{\text{LLM}}\\). The LLM input consists of the projector output \\(f_{\text{proj}}(C_t)\\), and the text tokens of an instruction prompt \\(I\\) such as \"What is this?\". Finally, the LLM outputs a brief description \\(R_{\text{brief}}\\) of the 3D object, which can be used to calculate the loss with the ground-truth description. The formulas are as follows:\n\\(C_t = f_{\text{text}}(D),\\)   (1)\n\\(R_{\text{brief}} = f_{\text{LLM}}(f_{\text{proj}} (C_t), h(I)),\\)   (2)\nwhere, \\(h\\) is the LLM's tokenizer.\nTrainable Layers & Data: Note that, only the projector \\(f_{\text{proj}}\\) is a trainable MLP, while the rest, including the text encoder \\(f_{\text{text}}\\) and LLM \\(f_{\text{LLM}}\\), have frozen weights. We train the model using a large dataset of brief descriptions (1M) from our T3D dataset, as shown in Tab. 1.\nStage II is shown in Fig. 4(b), Stage II is similar to Stage I. We also first input a caption of a 3D object into the text encoder \\(f_{\text{text}}\\), then extract the global text embedding and pass it to the projector \\(f_{\text{proj}}\\). The projector output, along with a complex instruction, is then fed to the LLM \\(f_{\text{LLM}}\\). Finally, the LLM outputs detailed description and conversation results, which are then used to calculate the loss.\nTrainable Layers & Data: The differences from Stage I are as follows: (1) The weights of the projector \\(f_{\text{proj}}\\) are copied from Stage I for initialization and remain trainable. (2) We use LoRA (Hu et al. 2021) to train the LLM \\(F_{\text{LLM}}\\) in this stage to achieve better multimodal alignment. The text encoder \\(f_{\text{text}}\\) remains frozen. We use only 210K detailed descriptions and conversation data for 3D objects from our T3D dataset, such as describing an object in ~50 words and engaging in multi-turn conversations, as shown in Tab. 1.\nNotably, to enhance the perception robustness of the LLM, we add Gaussian noise to the encoder's output features to sim-ulate the semantic discrepancies between different modalities, inspired by Chen et al. (2024b). After two stages of pure text training, our GreenPLM acquires the ability to comprehend raw 3D point clouds by directly replacing the text encoder \\(f_{\text{text}}\\) with a point encoder \\(f_{\text{pe}}\\) without weight tuning.\nStage III is shown in Fig. 4(c), we use 3D point cloud as input. The 3D point cloud \\(P\\) is fed into the point cloud en-coder \\(f_{\text{pe}}\\) to output a token sequence. Unlike previous stages that use only the global text embedding (corresponding to the class token in the point encoder) for the projector, in this stage, we extract representations from all tokens \\(T_{\text{pe}}\\) to more effectively leverage information from the point en-coder. To reduce the token sequence length for efficiency, we introduce a parameter-free token pooling module based on cross-attention, namely 0M-Pooling, which compresses the token length from 512 to 32. The pooled point tokens \\(T_e\\), along with three tokens from Mix-pooling and the class to-ken \\(C_{\text{pc}}\\), are input to the projector. Thus, the projector \\(f_{\text{proj}}\\) receives 32+3+1=36 tokens. We then feed the projector's output, along with the instruction \\(I\\), into \\(f_{\text{LLM}}\\) to generate the predict responses \\(R_{\text{pred}}\\) of descriptions or conversations. The responses will be used to compute loss with the ground truth. This stage can be formulated as:\n\\([C_{\text{pc}}, T_{\text{pc}}] = f_{\text{pc}}(P), T_e = 0\text{M-Pooling}(T_{\text{pc}}),\\)   (3)\n\\(R_{\text{pred}} = f_{\text{LLM}}(f_{\text{proj}} (C_{\text{pc}}, \text{Mix} (T_{\text{pc}}), T_e), h(I)),\\)   (4)\nwhere Mix represents Mix-pooling of max, mean, and sum.\nTrainable Layers & Data: Similar to Stage II, the weights of projector \\(f_{\text{proj}}\\) here are copied from the previous stage and then still kept trainable. We continue using LoRA (Hu et al. 2021) to train \\(f_{\text{LLM}}\\) for efficient point-LLM alignment. The point cloud encoder \\(f_{\text{pc}}\\) remain frozen. In this stage, we train using only a small amount of 3D-text pairs (90K).\nLoss Function For all training stages, given a pair of LLM output R and text ground truth y, GreenPLM is optimized under a causal language modeling objective (Liu et al. 2018):\n\\(L = \text{CrossEntropyLoss} (R, h (y)),\\)   (5)"}, {"title": "0M-Pooling", "content": "To fully leverage the output of the point cloud encoder, we extract information from all output tokens \\(T_{\text{pe}}\\), not just the class token, while reducing computational load. As shown in Fig. 5, we design a zero-parameter token pooling module based on cross-attention, namely 0M-Pooling, which com-presses the 512 output tokens down to 32 tokens, without introducing any learnable parameters, defined as:\nT_c = \text{FPS}(T_{\text{pc}}),  (6)\nT_m = \text{MaxPool}(T_p), (6)\nT_p = \text{KNN}(T_c, T_{\text{pc}}), (6)\nT_e = \text{SoftMax}(T_p^T)T_p,  (6)\nwhere \\(T_{\text{pc}} \\in R^{N \\times 1 \\times C}\\) is the output point token sequence of the point cloud encoder (N = 512), \\(T_c \\in R^{M \\times 1 \\times C}\\) is the central token gained via farthest point sampling (FPS) from \\(T_{\text{pc}}\\) (M = 32), and \\(T_p \\in R^{M \\times R \\times C}\\) represents the K-Nearest Neighborhood (KNN) tokens of \\(T_c\\) within \\(T_{\text{pc}}\\) (K = 8). Then, we pass \\(T_p\\) to Max Pooling on the K dimension to get \\(T_m \\in R^{M \\times 1 \\times C}\\). Finally, we use cross-attention in Equ.(6) to aggregate information from \\(T_{\text{pc}} \\in [][R^{512 \\times 1 \\times C} \\rightarrow TPE R^{32 \\times 1 \\times C}\\). Finally, we obtain the compressed token \\(T_e\\) using zero trainable parameters. Notely, the \\(T_{\text{pe}}\\) input to 0M-Pooling is from the point encoder's second-to-last layer."}, {"title": "Experiment", "content": "Implementation details. We use Phi-3 (Abdin et al. 2024) as the LLM backbone, with EVA-CLIP-E (Sun et al. 2023) and ViT (Dosovitskiy et al. 2020) both trained by Uni3D (Zhou et al. 2023) as the text encoder and point encoder, respectively. The point encoder outputs 512+1 tokens, each with C = 1024. The MLP projector consists of two linear layers and a GeLU activation, mapping the encoder's output tokens to tokens with 3072 dimensions of Phi-3. Our GreenPLM has 63.3M trainable parameters and requires only 26.6 hours of training on a single 3090 GPU. Besides the standard 3-stage training of GreenPLM, we also train GreenPLM-0 with text-only data, utilizing only Stages I and II. During inference, we simply replace the text encoder in GreenPLM-0 with the point encoder from Uni3D without weight tuning. More detailed training settings are included in Appendix.\nBaselines. To validate our 3D data-free capability, we com-pared GreenPLM-0 with the SOTA 2D-LLMs, InstructBLIP and LLaVA, as well as the 3D-2D-LLM model Point-Bind LLM (Guo et al. 2023). To evaluate GreenPLM with limited 3D data, we choose the SoTA models PointLLM (Xu et al. 2023) and MiniGPT-3D (Tang et al. 2024). For fairness, we train both using the same 90K limited 3D point-text datas.\nEvaluation Settings. An efficient and accurate model eval-uation method is a shared goal in the MLLM community. We observe that existing evaluation approaches often rely on GPT-4 and GPT-3.5 to assess the similarity between gener-ated results and ground truth sentences. While this method provides accurate evaluations, it has two major drawbacks: inconsistent API versions and high evaluation costs. For in-stance, the GPT-3.5-turbo-0613 model used in PointLLM and MiniGPT-3D is no longer maintained, making it difficult to replicate the results. To address these issues, we propose a new benchmark based on open-source models and intro-duce a new metric to evaluate data efficiency. Specifically, we use two prompts for the classification task: an Instruction-type (I) prompt, \u201cWhat is this?\u201d, and a Completion-type (C) prompt, \u201cThis is an object of.\u201d. For the captioning task, we use a single prompt: \u201cCaption this 3D model in detail.\u201d. We then replace GPT-4 and GPT-3.5 with the open-source Qwen2-72B-Instruct (Yang et al. 2024a) (Qwen2 for short) to evaluate the model's output. We introduce the Accuracy-to-3D-Data Ratio (A3DR) metric to assess a model's efficiency in utilizing 3D data, defined as follows:\n\\(A3DR(Acc) = \\frac{1}{1 + \\exp(-x)}, \\)   (7)\nwhere Size is the size of 3D data (K), Acc is the accuracy, \\(\\epsilon=1e-5\\) to avoid zero division."}, {"title": "Generative 3D Object Classification", "content": "We validate the model's recognition ability by performing the generative 3D object classification task on the ModelNet40 dataset (Wu et al. 2015) and Objaverse dataset (Deitke et al. 2023), using I-type and C-type prompts, with results shown in Tab. 2. For close-set zero-shot classification on ModelNet40, we let Qwen2 select the closest matching category in the 40 classes as the model's output. For open-vocabulary classifica-tion on Objaverse, we use Qwen2 to evaluate if the model's output describes the category of ground truth sentence.\nAs shown in Tab. 2, our GreenPLM-0 achieves an av-erage classification accuracy (AvgAcc) of 54.57% without using any 3D data, outperforming all 2D-based models. It surpasses LLaVA-1.5-13B by +21.95 and Point-Bind LLM by +27.89 in AvgAcc. Remarkably, our model also exceeds GPT4Point (660K), which is trained with 660K 3D data, by +20.08 and performs on par with PointLLM-7B (730K). With only a small amount of 3D data (90K), GreenPLM achieves an average accuracy of 60.08%, surpassing PointLLM and MiniGPT by +10.95 and +11.06 in AvgAcc, respectively. GreenPLM even outperforms PointLLM-13B (730K) while using a smaller LLM, and obtains results comparable to SOTA model MiniGPT-3D (730K). Additionally, GreenPLM (90K) outperforms MiniGPT-3D (90K) and MiniGPT-3D (730K) on the A3DR (average accuracy) by +5.9% and 45.6%, respectively. These results demonstrate the high 3D data-efficiency of our model."}, {"title": "3D Object Captioning", "content": "We evaluate the ability to understand 3D context through a 3D object captioning task, as shown in Tab. 3. Following pre-vious works (Xu et al. 2023; Tang et al. 2024), we assess the similarity between the model's response and the ground truth caption using an LLM, and also evaluate embedding simi-larity using Sentence-BERT (Reimers and Gurevych 2019) (S-BERT) and SimCSE (Gao, Yao, and Chen 2021).\nIt is evident that all models without 3D data underperform compared to those trained with 3D data, as they lose signif-icant 3D information. However, our GreenPLM-0 can still outperforms Point-Bind LLM and achieves comparable re-sults to powerful 2D-LLMs by a large margin. When using a small amount of 3D data (90K), our Qwen2 score surpasses MiniGPT-3D (90K) by +7.50, with S-BERT and SimCSE scores also exceeding by +2.72 and +1.61, respectively. Sim-ilarly, GreenPLM (90K) achieves a Qwen2 score higher than PointLLM-13B (730K) by +2.15, with S-BERT and Sim-CSE scores comparable to MiniGPT-3D (730K) while using only 12% of 3D data. These results again demonstrate Green-PLM's ability to efficiently extract 3D information from even small amounts of 3D data or purely text data."}, {"title": "Qualitative Results", "content": "Fig. 1 and Tab. 5 present the qualitative results. As shown in Fig. 1, whether trained on text-only or with minimal 3D data, GreenPLM provides accurate, context-aware responses in multi-turn conversations. Tab. 5 shows that our GreenPLM-"}, {"title": "Conclusion", "content": "To enable LLMs to achieve strong 3D understanding with minimal 3D data, we introduce a new task: 3D Data-Efficient Point-Language Understanding. We propose GreenPLM, which employs a 3-stage training strategy that increases text data in Stage I & II to reduce the need for 3D data in Stage III. We create a 6M T3D dataset and an unified benchmark. Results show that GreenPLM achieves performance com-parable to state-of-the-arts using only 12% of the 3D data. Remarkably, our model performs well even without 3D data.\nLimitations. Our approach has limitations. Due to time and resource constraints, we couldn't explore all text and 3D data combinations. We believe scaling up either could further improve performance. Additionally, we only test feasibility on small objects, and will explore GreenPLM's potential for larger scenes in future work."}, {"title": "Appendix", "content": "Here in the Appendix, we present the detailed distribution of the T3D dataset, along with the prompts and instructions used to create it, and provide several data examples. We also showcase more visual result comparisons. Additionally, we provide additional ablation results and more detailed training parameters. Finally, we include illustrations of the model architecture used during training and inference."}, {"title": "Our 6M T3D Dataset", "content": "Distributions We show the detailed distributions of our 6M T3D dataset in Fig. 11 and Fig. 12. Specifically, in Fig. 11, we show word clouds for captions and responses. Following Wang et al. (2022), we also present the distribution of verb-noun pairs in the dataset, highlighting its diverse attributes. Additionally, in Fig. 12, we display the length distribution for different data types; for instance, most brief and detailed descriptions range from about 18 to 42 words.\nPrompts and Instructions Here, we show one example of the data generation pipeline using Qwen2-72B-Instruct(Yang et al. 2024b) in Fig. 13, also the instruction list for description data in Fig. 7 and Fig. 8.\nData Samples We give several samples of four types of data from our T3D dataset, shown in Fig. 14-17. Please see supplementary material for more data samples of our T3D dataset.\nQualitative Results We show more qualitive results of our GreenPLM-0 and GreenPLM in Fig. 18 and Fig. 19. Trained with only text data, our GreenPLM-0 accurately identifies the shape, color, and usage of 3D objects. For example, in the bottom right of Fig. 18, the model not only correctly recognizes the shoe's category and purpose but also accurately distinguishes be-tween the left and right shoe. When using only a small amount of 3D data, our GreenPLM can accurately and thor-oughly describe 3D objects, as shown in the first row of Fig. 19."}, {"title": "Detailed Training Settings", "content": "We report more detailed training settings in Tab. 9."}, {"title": "Ablation on the size of point encoder", "content": "parameters ranging from 6.2M to 1016.5M, as shown in Tab. 6. The results indicate that as the point encoder size increases, the model's performance first improves and then declines, achieving the best accuracy with 22.6M parameters. Notably, even with just 6.2M parameters, GreenPLM still demonstrates strong 3D understanding, further proving the efficiency of our model."}, {"title": "Training and Inference Architecture", "content": "We show the difference of architectures between training and inference in Fig. 20. Note that each stage of our 3-stage strategy can be used for inference. Specifically, if using the Stage I or Stage II for inference, simply replace the text encoder with the aligned point encoder."}]}