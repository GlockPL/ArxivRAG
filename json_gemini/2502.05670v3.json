{"title": "Language Models Largely Exhibit Human-like Constituent Ordering Preferences", "authors": ["Ada Defne Tur", "Gaurav Kamath", "Siva Reddy"], "abstract": "Though English sentences are typically inflexible vis-\u00e0-vis word order, constituents often show far more variability in ordering. One prominent theory presents the notion that constituent ordering is directly correlated with constituent weight: a measure of the constituent's length or complexity. Such theories are interesting in the context of natural language processing (NLP), because while recent advances in NLP have led to significant gains in the performance of large language models (LLMs), much remains unclear about how these models process language, and how this compares to human language processing. In particular, the question remains whether LLMs display the same patterns with constituent movement, and may provide insights into existing theories on when and how the shift occurs in human language. We compare a variety of LLMs with diverse properties to evaluate broad LLM performance on four types of constituent movement: heavy NP shift, particle movement, dative alternation, and multiple PPs. Despite performing unexpectedly around particle movement, LLMs generally align with human preferences around constituent ordering.", "sections": [{"title": "1 Introduction", "content": "Despite the fact that word order in English is typically strict, constituents in post-verbal positions can be highly flexible in their ordering (Chomsky, 2002; Wasow and Arnold, 2003). A number of specific phenomena are prime examples of this movement; we show these in Figure 1.\nThe cause of this movement has been attributed to a variety of factors, including lexical bias (Baars et al., 1975; Hartsuiker et al., 2005; Dell and Reich, 1981), semantic connectedness (Kayne, 1983; Behaghel, 1932), and information structure (Lambrecht, 1994; Chafe, 1976; Behaghel, 1932; Gundel, 1988). However, one of the most prominent factors, as prior research has suggested, is constituent weight and complexity (Quirk et al., 1975; Behaghel, 1909; Wasow, 1997a).\nAs the following examples show, we tend to prefer sentences where longer and more complex constituents are moved to the end of the sentence:\n(5)\na.\tI met [the man]NP [at the park]PP.\nb.\t*I met [at the park]PP [the man]NP.\nc.\tI met [at the park]PP [the tall man selling water to marathon runners]NP.\nd.\t?I met [the tall man selling water to marathon runners]NP [at the park]PP.\nThe typical constituent order shown in (5-a), for example, is not readily perturbed to the constituent order shown in (5-b). In sentences (5-c) and (5-d), however, where the NP is considerably longer and more complex than the PP, the reverse is true. Linguistic theory thus suggests that phrases and constituents are specifically ordered to be presented in increasing complexity, or weight; essentially, the larger the constituent, the further to the end of the sentence we expect it to appear (Quirk et al., 1975; Behaghel, 1909; Wasow, 1997a; Futrell et al., 2015). Consider the example in Figure 3:\nOther orderings of this sentence, if greatly violating this principle of weight, would likely be considered undesirable. This relationship between the complexity of post-verbal constituents and their ordering raises several questions:\n\u2022\tWhat are the exact effects of weight on con-"}, {"title": "2 Background", "content": "At a high level, English follows a subject-verb-object (SVO) ordering; beyond this basic structure, other objects, modifiers, constituents, and clauses can be added to form more complex sentences (Hengeveld, 1992). The organization and format of how and when each constituent in a sentence is delivered, or its ordering, can be highly flexible (Bakker, 1998; Namboodiripad, 2019, 2017).\nConstituent shifting is the process of reordering the constituents of a sentence, such that the original meaning of the sentence is maintained, and all semantic truth conditions are unchanged. This work focuses on four specific types of shift, with a prime commonality: each shift involves the movement of constituents from a post-verbal position (i.e. appearing after the verb of a sentence) to another post-verbal position (Wasow and Arnold, 2003; Wasow, 2002). Table 1 demonstrates how we define shifted/unshifted sentences."}, {"title": "3 Related Work", "content": "Constituent movement has been the subject of considerable linguistic literature; we categorize relevant contributions by our three research questions."}, {"title": "3.1 What are the exact effects of weight on constituent ordering?", "content": "Significant prior work has focused on investigating the effects of weight on constituent ordering (Arnold et al., 2000; Wasow and Arnold, 2003; Wasow, 1997a; Arnold et al., 2004; Hawkins, 1995; Behaghel, 1909; Hawkins, 2004); many contributions find gradient effects by which the shift becomes more frequent in examined language corpora as the relative weight of the relative constituent changes, suggesting that weight is the predominant factor in triggering the shift, even cross-linguistically (Wasow, 1997b; Wasow and Arnold, 2003; Faghiri and Thuilier, 2018; Wang and Liu, 2014; Hawkins, 1999; Quirk et al., 1975; Manetta, 2012). Furthermore, studies with human participants find similar results, where weight presents a primary role in the shift (Medeiros et al., 2021). The study, however, also suggests that this movement is constrained by ceiling effects, by which the efficacy of additional weight and complexity plateaus. More closely relevant to this work, Futrell and Levy (2018) conduct a similar analysis on post-verbal constituent movement using weight as a binary feature (i.e. 'long' vs 'short'), and find similar trends with LSTMs."}, {"title": "3.2 What measure of weight best explains effects of constituent ordering?", "content": "Weight is a measure of a constituent's complexity or size, but how best to measure it is less straightforward (Chomsky, 2008; Haegeman, 1991; Wasow, 1997a). Related research categorizes and analyzes the effects of three primary measures of weight on constituent movement, particularly with HNPS: the word length of the NP, the number of nodes in the NP's syntactic structure, and the number of modifiers applied to the NP (Wasow and Arnold, 2003; Medeiros et al., 2021; Wasow, 1997b). The analyses found that, although the word length was statistically the strongest predictor for HNPS, \"no single factor can account for observed constituent order alternation\u201d (Medeiros et al., 2021, pg.6). Similarly, Wasow and Arnold (2003) find in a corpus study that for HNPS and DA, constituent movement was best accounted for when considering both word length and modifier weight together, as opposed to either on its own."}, {"title": "3.3 How exactly do LLM preferences around constituent shifting align with human constituent shifting preferences?", "content": "Research concerning the behaviors of computational models has also shown that models exhibit human-like preferences (Fujihara et al., 2022; Linzen et al., 2016; Marvin and Linzen, 2018; Kamath et al., 2024). Notably, prior work shows models learn syntactic alternations (Wilcox et al., 2019; Lau et al., 2017); more directly relevant to us, Futrell and Levy (2018) finds that behaviors of LSTMs appear to correlate closely with observed judgements of humans on corresponding data, suggesting that constituent movement is motivated similarly in both humans and models. This work, however, predates preference-aligned models (Ouyang et al., 2022); we hypothesize that the behaviors of such models will align even more closely with human preferences around constituent ordering."}, {"title": "4 Models and Data", "content": "We both generate synthetic data using a template modified for the various shifts and structures and mine natural data from the Penn Treebank-2 corpus (Marcus et al., 1995). Each shift we consider has a standardized form that we can utilize for both processes, noted in Table 1. We also annotate for the aforementioned weight measures; syllable weight is computed using Syllapy; token weight is retrieved simultaneously with the model scoring process; modifier weight is counted when constructing modifier chunks on constituents."}, {"title": "4.1.1 Synthetic Data", "content": "We synthetically generate data using the process shown in Figure 4 in order to accumulate large amounts of iteratively more complex data for model evaluations. Using an overall frame for the sentence, we alternate subjects, verbs, moving constituents, and their modifiers; we do this for all variables on a variety of constituents."}, {"title": "4.1.2 Mined Data", "content": "Our synthetic data, however, contains limited syntactic variation, and may not represent naturally occurring data; thus, we mine from existing data to ground our results. We use the Penn Treebank-2 to retrieve sentences following the structure of each"}, {"title": "5 Shifting Preference of Models", "content": "To observe the effects of constituent weight on ordering preferences, we compute the difference in log probabilities assigned by models to shifted and unshifted sentences at a range of constituent weight ratios (see Section 2). We begin by extracting log probabilities assigned to each sentence by the aforementioned models, using the minicons library (Misra, 2022). This score corresponds to the model's judgement of the sentence's linguistic plausibility, computed with the following equation:\n$Mscore(w) = \\frac{1}{T}\\sum_{t=1}^{T} log P_M(w_t|w_1, w_2, ..., w_{t-1}; \\theta).$\n$Mscore$ is the log probability score of the sequence $w = [W_1,W_2,..., w_t,...,w_T]^T$, where $w_t$ is the token at position $t$. The term $P_M$ is the conditional probability from the model $M$ of token $w_t$ given the preceding tokens, while $\\theta$ are the model parameters. The output of this formula is the sum of the probabilities of all tokens in the sequence, given previous tokens and model parameters, which equates to the overall sequence probability. The closer $Mscore$ is to 0, the more strongly the model judges the sequence to likely occur in human language.\nUpon computing this score for each minimal pair of shifted-unshifted sentences (denoted as $U$ and $S$), we calculate the difference of the $Mscore$ for each sentence:\n$M_{preference} = M_{score}(U) \u2013 M_{score}(S).$\nThis metric aligns closely with the surprisal-based measure used by Futrell and Levy (2018),"}, {"title": "5.2 Results: Are models motivated by weight to shift?", "content": "We analyze model $M_{preference}$ scores by weight ratios (see Section 2) considering various metric types in the analysis. Figure 5 shows OLMO 7B performance on HNPS; see Section A.2.1 for remaining models and variables. We observe similar trends overall across models.\nFor HNPS, we find that model $M_{preference}$ scores initially start positive, i.e., models prefer unshifted sentence, but converge above 0 as constituent weight ratios increase, indicate some continued preference for unshifted constituent orderings beyond a given point. In the case of DA and MPP, we see a similar effect of weight $M_{preference}$ scores, but with a plateau below zero, indicating that models eventually converge upon a relative preference for the shifted version of a sentence. In the case of PM, however, we see scores initially drop sharply from around 0, but later somewhat rise."}, {"title": "6 Motivating Factors of Model Preference", "content": "Literature regarding human language finds varying levels of importance correlated with different measures of constituent weight, particularly in corpus studies and human judgement tasks (Wasow, 1997a; Wasow and Arnold, 2003; Medeiros et al., 2021). Whether or not this same trend is seen with language models is unknown. We conduct a regression analysis on the data collected for the first experiment, using a generalized additive mixed model (Wood, 2017; S\u00f3skuthy, 2017), with the goal of measuring how significantly each weight measure both impacts the models' judgements and serves to fit regression lines on the data."}, {"title": "6.1 Approach", "content": "To compare how well different measures of weight explain shifting preferences, we fit Generalized Additive Mixed Models, or GAMMs (Wood, 2017; S\u00f3skuthy, 2017), on our $M_{preference}$ scores (see Section 5.1) as a function of various weight measures: word length, token length, syllable weight, and modifier weight. GAMMs allow for the fitting of highly non-linear relationships as the sum of multiple predictor-wise smooth functions: basis functions that allow for an arbitrary degree of smoothness. Crucially, aside from providing interpretable measures of goodness of fit, GAMMS also allow for grouping structures in the data to be captured as random effects (Wood, 2017, ch.6).\nBearing in mind that multiple measures of weight may jointly determine the accessibility of a shift (Wasow, 1997b; Wasow and Arnold, 2003), we analyze the relative importance of each weight measure in the following manner. For each model, first, we fit a GAMM on the model's $M_{preference}$ scores as a function of all weight predictors, with verb-wise random intercepts and slopes. We then iteratively ablate each weight predictor while retaining all others, and compare the quality of fit yielded by the full model with that of the ablated model. Intuitively, this provides an indication of how important the dropped predictor is for the LLM: it captures how much less of the LLM's be-"}, {"title": "6.2 Results: Which measure of weight best explains shifting?", "content": "Table 3 presents a subset of the results of our regression analysis (full results in Appendix Table 5). Crucially, we find that syllable weight is often the most important predictor of LLM behavior around constituent ordering preference, since the drop in R-squared scores is highest when syllable weight is not used. For DA, word length seems to be the best predictor.\nWe find that GPT-2 Medium achieves the highest R-squared overall on both HNPS and MPP. Further, contrary to our hypotheses, across almost all shift types, instruction-tuned models consistently achieve lower R-squared scores than their base model counterparts. Table 3, for example, shows these results for Llama-3 and Llama-3 Instruct; see Appendix A.2.1 for remaining plots on synthetic data. The BabyLM models also present high R-squared scores on both PM and DA, with BabyLlama yielding the highest R-squared values (see Table 3 for BabyLlama results and Appendix A.2.1 for BabyOPT results). Finally, despite the high performance of GPT-2 Medium, we do not observe consistent improvements in R-squared values as model sizes scale."}, {"title": "7 Human-Model Preference Correlation", "content": "To adequately compare the behaviors of LLMs with those of humans, a direct study of preferences on identical data points is necessary. We collect human judgements on a subset of data presented to models; though human judgements and model scores are not identical metrics, they can act as proxies when comparing relative trends.\nWe conduct a crowdsourced study through Prolific, collecting judgments from 126 native English speakers residing in Anglophone countries, on 500 sentence pairs. Each participant is presented 25 sentence pairs and asked to judge how natural they sound in relation to each other, assigning a score between 1 and 7; 1 corresponds to the first sentence presented appearing far more natural than the second, and 7 the reverse. We exclude datapoints with low inter-annotator agreement to minimize noise."}, {"title": "7.2 Results: Do LLM and human preferences correlate?", "content": "To analyze how this human judgment data compares with model results, we compute the Spearman correlation between the average human score for each data point and the model's $M_{preference}$ score. We present these scores in Table 4. We also plot model scores against human judgment data to observe the correlation visually; these plots are presented in Figure 6 and Section A.3.\nComparing preferences of humans and models on a statistical level introduces interesting find-"}, {"title": "8 Discussion", "content": "In Figure 5 and Appendix A.2.1, we observe converging effects as weight increases, suggesting that prior theories defining weight as a prime factor in motivating the shift hold with computational models as well. Further, we observe, specifically with PM, that weight, beyond a certain threshold, begins to detriment motivation for shifting.\nWe observe similar effects on the scraped corpus data, in Appendix A.2.2, though with more noise. Given the specificity of the data itself, being rooted in financial reports and news articles, some noise and outliers were expected, and in some cases, observable trends remain. Similar to the synthetic data plots, we see convergence on HNPS, and some on MPP, as well as an initial drop followed by a rise in $M_{preference}$ scores for PM."}, {"title": "8.2 What measure of weight best explains effects of constituent ordering?", "content": "The syllable weight was the best measure of weight for explaining motivations to shift in LLMs. This raises an obvious question-why is syllable weight a more effective predictor of model behavior than token weight, which would intuitively be most aligned with a model's processing of weight and complexity? This finding acts as initial evidence that models may induce linguistic information not just at the token level, but also implicitly at the level of syllables."}, {"title": "8.3 How exactly do LLM preferences around constituent shifting align with human constituent shifting preferences?", "content": "Broadly speaking, a clear trend is maintained across humans and models, following what was presented by Futrell and Levy (2018) in their analysis. Where human language sees motivation for movement with increasing weight, model behavior follows closely. Our experiment, which includes graded data beyond binary weight categorizations, and a wider range of models, yields relatively high correlation effects between preferences of models and humans, as presented in Table 4-suggesting noticeably similar behaviors between the two on particular linguistic tendencies, with the notable exception of particle movement.\nInterestingly, we observe an unexpected trend where instruction-tuned models, which consistently correlate less with human data than their corresponding base model, as well as, quite often, yield lower R-squared scores. This runs against our initial hypothesis around instruction-tuned models, and suggests inadequacy in providing consistent and explainable trends compared to base models."}, {"title": "8.4 Future Work", "content": "Our findings suggest that even though newer models are equipped with more parameters, training data, and the human-feedback mechanism, they fail to align better with human linguistic preferences than their earlier counterparts, raising ques-"}, {"title": "9 Conclusions", "content": "In this work, we present a thorough analysis of the behaviors of LLMs in response to constituent movement, using both a novel set of nearly 400K minimal pairs of variably ordered sentences, as well as naturally occurring data. We collect human judgements and model preference scores and observe comparable trends between the behaviors of humans and models. Such comparisons indicate that humans and LLMs largely hold similar linguistic preferences around constituent ordering, with the exception of particle movement. Our findings and in particular, the surprising gap we find between instruction-tuned models and their vanilla counterparts-invite further research into when and how linguistic preferences of models and humans align."}, {"title": "10 Limitations", "content": "We only focus on constituent movement in English, even though this phenomena is known to manifest cross-linguistically (Faghiri and Thuilier, 2018; Wang and Liu, 2014; Hawkins, 1999; Quirk et al., 1975; Manetta, 2012; Fujihara et al., 2022)."}, {"title": "11 Ethics Statement", "content": "Our experimentation poses no risks or harms for any participants involved. Human participants from our data study through Prolific were compensated on average US$12 per hour. Anonymous participants were informed of the purpose of the study and how their responses would be used, as well as their rights regarding submitted data."}]}