{"title": "Why Do We Laugh? Annotation and Taxonomy Generation for Laughable Contexts in Spontaneous Text Conversation", "authors": ["Koji Inoue", "Mikey Elmers", "Divesh Lala", "Tatsuya Kawahara"], "abstract": "Laughter serves as a multifaceted communicative signal in human interaction, yet its identification within dialogue presents a significant challenge for conversational AI systems. This study addresses this challenge by annotating laughable contexts in Japanese spontaneous text conversation data and developing a taxonomy to classify the underlying reasons for such contexts. Initially, multiple annotators manually labeled laughable contexts using a binary decision (laughable or non-laughable). Subsequently, an LLM was used to generate explanations for the binary annotations of laughable contexts, which were then categorized into a taxonomy comprising ten categories, including \"Empathy and Affinity\" and \"Humor and Surprise,\" highlighting the diverse range of laughter-inducing scenarios. The study also evaluated GPT-4's performance in recognizing the majority labels of laughable contexts, achieving an F1 score of 43.14%. These findings contribute to the advancement of conversational AI by establishing a foundation for more nuanced recognition and generation of laughter, ultimately fostering more natural and engaging human-AI interactions.", "sections": [{"title": "Introduction", "content": "In human dialogue, laughter serves as a communicative signal conveying humor, empathy, surprise, or social bonding (Norrick, 1993; Glenn, 2003; Attardo, 2009). However, its mechanisms are complex and multifaceted, and understanding them remains a long-term challenge for dialogue systems aiming to achieve human-like interaction (Tian et al., 2016; T\u00fcrker et al., 2017; Inoue et al., 2022; Ludusan and Wagner, 2023; Perkins Booker et al., 2024). Furthermore, traditional approaches to modeling laughter and humor have often been limited to scenarios involving explicit auditory or visual stimuli, with few addressing the subtle contextual nuances present in spontaneous dialogue (Bertero and Fung, 2016; Choube and Soleymani, 2020; Jentzsch and Kersting, 2023; Ko et al., 2023; Hessel et al., 2023). Therefore, elucidating the underlying reasons for laughter in spontaneous dialogue data can contribute to making large language model (LLM)-based dialogue more natural and empathetic. However, annotating the reasons for laughter in any formalized manner has been prohibitively time- and labor-intensive, leaving the field largely reliant on qualitative approaches through conversational analysis.\nIn this study, we address the question of \u201cwhy do we laugh?\u201d from an informatics perspective by proposing a semi-automated approach to constructing taxonomy labels for the reasons of laughter. First, to identify target segments, multiple annotators were asked to perform a simple binary classification on each utterance in a dialogue data, determining whether it was \u201claughable\u201d or not, as shown in Table 1. Subsequently, for contexts labeled as \"laughable\" based on the majority voting, we used an LLM (GPT-4o) to generate the reasoning sentence behind this judgment and further classified these generated reasons into distinct categories (taxonomy labels). This semi-automated taxonomy generation approach is generalizable and can be particularly effective in scenarios where manual annotation is limited to simpler labels, such as emotion labeling.\nThe purpose of this research is to contribute to-ward more nuanced conversational AI systems that can recognize and even anticipate moments for laughter, ultimately fostering more natural interactions between humans and machines. Ideally, such systems should be able to respond with the correct acoustics, delay, and consider group size for different laughter types (Truong and Trouvain, 2012). Our findings reveal that AI can improve our understanding of laughter and offer a foundation for future research in AI context-sensitive recognition."}, {"title": "Annotation of Laughable Context", "content": "We annotated laughable contexts in the RealPersonaChat dataset (Yamashita et al., 2023). This textual data contains one-on-one Japanese spontaneous conversation where participants chat without assuming assigned personas. It includes approximately 30 utterances per conversation, totaling around 14,000 dialogues. We annotated 900 dialogues, with plans to annotate the remainder in future work.\nDuring the annotation process, each annotator reviewed each dialogue and, after the initial two greeting utterances, made a binary decision for whether the next person would laugh (laughable) or not. Five annotators assigned these binary labels to each utterance. While some samples showed clear agreement (either all or none of the annotators marked them as laughable), there were also numerous split samples, highlighting the subjectivity and complexity of the task. If we applied a majority voting process, 3,739 contexts (14.8%) were labeled as laughable, and 21,550 contexts (85.2%) as non-laughable.\nTable 1 illustrates a laughable context example. In this dialogue, person A's final utterance is self-contradictory, requiring high-level comprehension of the dialogue context. These annotations underscore the significance of cultural context and conversational flow in interpreting laughter cues."}, {"title": "Generating Taxonomy of Laughable Reason", "content": "Towards developing human-like laughter behaviors in LLMs, we investigated the reasons behind human annotators' recognition of laughable contexts. In this section, we used only samples with majority labels marked as laughable (3,739 samples). Since manual annotations are costly, we utilized GPT-4o to generate explanations for the human judgments. For example, a generated reason for the example context in Table 1 is:\nIf we were to speculate on the reasons a third party might judge that Person A laughed at Person B's final remark in this conversation, the following points can be considered:\nElement of Humor: Person B's comment, \u201cMy husband doesn't seem to listen to me. Huh, that's strange.,\" contains a touch of self-deprecating humor. This lighthearted tone, making fun of their own situation, can be amusing to the listener. (...)\nRelaxed Atmosphere: The overall tone of the conversation seems light and relaxed, and Person B's comment might have been perceived as a playful joke in line with this mood.\nA combination of these factors may have led the third party to interpret that Person A laughed in response to Person B's remark.\nNext, we attempted to summarize the generated reasoning text for laughable contexts by following a taxonomy generation approach using LLMs (Wan et al., 2024). First, we randomly divided the generated reasons into smaller portions, each representing 5% of the data. Starting with the initial one portion, we used GPT-4o again to generate taxonomy labels and explanations, manually validating them as needed. We then iteratively updated the taxonomy by having the LLM re-generate the taxonomy information based on the previous taxonomy information and the portion data, until all data was processed. This resulted in the generation of ten taxonomy labels, summarized in Table 3, including categories like (1) Empathy and Affinity and (2) Humor and Surprise.\nAfter generating the taxonomy labels, we again used the LLM to assign labels to each reason sample, allowing for multiple labels per sample. The results of this labeling are summarized on the right side of Table 3. While dominant categories like (1)\""}, {"title": "LLM's Performance on Laughable Context Recognition", "content": "We then examined how much LLMs, specifically GPT-4o, can recognize the laughable contexts in spontaneous text conversation. The model was tested in a zero-shot setting, instructed to first analyze the conversational context and then determine its laughability as a binary. The provided prompt included a task description for laughable context recognition, followed by a Chain-of-Thought (CoT) reasoning approach to encourage the model to consider the reasoning behind its decision step by step. We evaluated GPT-4o's performance against the majority labels, achieving an F1 score of 43.14%, with a precision of 41.66% and recall of 44.72%. While this score was significantly above the chance level (14.8%), capturing the nuanced subtleties of conversational humor remains challenging.\nWe then further examined the LLM's performance on each generated taxonomy label.  First, the primary labels, from (1) to (3), showed similar accuracy rates, ranging from 40% to 50%. Additionally, we observed comparatively higher scores for (5) Cultural Background and Shared Understanding, (7) Self-Deprecating Humor, and (8) Defying Expectations, suggesting that the current LLM may effectively capture these contexts. In contrast, categories like (6) Nostalgia and Fondness and (9) Positive Energy displayed lower accuracy, potentially highlighting limitations in the LLM's understanding.\nTable 5 presents an example dialogue context where the LLM marked non-laughable for the final utterance from person A, despite a positive majority label with a (6) Nostalgia and Fondness reason. This context was also assigned the (2) Humor and Surprise and (3) Relaxed Atmosphere labels. In this example, the participants discuss a nostalgic memory of drinking a powdered beverage with milk. The last utterance evokes nostalgia, implicitly inviting laughter. Here, capturing person A's sentiment seems to be difficult for the current LLM, but is essential for appropriate laughter response.\nTable 6 provides an example for (9) Positive Energy label. This context was also assigned the (1) Empathy and Affinity and (2) Humor and Surprise labels. The participants discussed a challenging experience with childcare, but in the final utterance, person A reflects positively on the experience after some time has passed. Although the story itself recounts a difficult time, it is now viewed positively, making it laughable. This example suggests that the LLM needs to comprehend the temporal structure of the story and the person's current feelings to accurately interpret the context as laughable."}, {"title": "Correlation Among Taxonomy Labels", "content": "Figure 1 presents the correlation matrix of the assigned labels discussed in Section 3, where multiple labels can be assigned to the same laughable context. For instance, \u201cEmpathy and Affinity\u201d shows a weak positive correlation with \u201cRelaxed Atmosphere.\u201d Conversely, \u201cEmpathy and Affinity\u201d exhibits a negative correlation with \u201cDefying Expressions.\u201d We also find a negative correlation between \"Humor and Surprise\u201d and \u201cPositive Energy,\u201d despite both being associated with positive sentiment. This may be attributed to different expressive styles, with the former implicit and the latter explicit. To gain deeper insight into the relationships between these labels, further qualitative analysis will be conducted in future work."}]}